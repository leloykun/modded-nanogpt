====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import flex_attention, create_block_mask, BlockMask, _score_mod_signature
from torch._inductor.lowering import make_pointwise, register_lowering
# Some internal torch.compile details
from torch._inductor.virtualized import ops
from functools import partial
flex_attention = torch.compile(flex_attention, dynamic=False)
create_block_mask = torch.compile(create_block_mask, dynamic=False)

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# Attention Tanh softcapping

@torch.library.custom_op("approx::tanh", mutates_args=())
def _tanh_approx(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

@_tanh_approx.register_fake
def _(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

def _tanh_approx_lowering(inp):
    fn = partial(ops.inline_asm_elementwise, asm="tanh.approx.f32 $0, $1;")
    return make_pointwise(fn)(inp)

register_lowering(torch.ops.approx.tanh)(_tanh_approx_lowering)

class _TanhApprox(torch.autograd.Function):
    @staticmethod
    def forward(x):
        return torch.ops.approx.tanh(x)

    @staticmethod
    def setup_context(ctx, inputs, output):
        (x,) = inputs
        result = output
        ctx.save_for_backward(result)

    @staticmethod
    def backward(ctx, grad_output):
        (result,) = ctx.saved_tensors
        return grad_output * (1 - result * result)

    @staticmethod
    def vmap(info, in_dims, x):
        return torch.tanh(x), 0

_tanh_approx = _TanhApprox.apply

def generate_tanh_softcap(soft_cap: int, approx: bool=True) -> _score_mod_signature:
    tanh = _tanh_approx if approx else torch.tanh

    def tanh_softcap(score, b, h, q_idx, kv_idx):
        return soft_cap * tanh(score / soft_cap)

    prefix = "tanh_softcap_approx" if approx else "tanh_softcap"
    tanh_softcap.__name__ = f"{prefix}_{soft_cap}"

    return tanh_softcap

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1, block_mask: BlockMask, score_mod: _score_mod_signature):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=score_mod, block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, block_mask: BlockMask, score_mod: _score_mod_signature):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1, block_mask, score_mod)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attention_soft_cap : int = 50
    lm_head_soft_cap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.attention_soft_cap = config.attention_soft_cap
        self.lm_head_soft_cap = config.lm_head_soft_cap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target, attn_blocksize: int):

        docs = (idx == 50256).cumsum(0)
        def document_causal_mask(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < attn_blocksize
            return causal_mask & document_mask & window_mask

        softcap_mod = generate_tanh_softcap(self.attention_soft_cap, approx=True)  # @leloykun

        S = len(idx)
        block_mask = create_block_mask(document_causal_mask, None, None, S, S, device="cuda", _compile=True)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x, v1 = self.transformer.h[i](x, v1, x0, block_mask, softcap_mod)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x, v1 = self.transformer.h[self.num_encoder_layers + i](x, v1, x0, block_mask, softcap_mod)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = self.lm_head_soft_cap * torch.tanh(logits / self.lm_head_soft_cap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        batch_size = self.B * self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.B*self.T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = buf[:-1] # inputs
        y = buf[1:] # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    device_batch_size : int = 1 # batch size, in sequences, per device
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1750 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 640 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # Set the attention blocksize for the current step, in chunks of 64
    attn_blocksize = torch.tensor(64*((step/args.num_iterations * (1792 - 64) + 64)//64), dtype=torch.int, device='cuda')
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Set the attention blocksize for the current step, in chunks of 64. By @fernbear.bsky.social
    attn_blocksize = torch.tensor(64*((step/args.num_iterations * (1792 - 64) + 64)//64), dtype=torch.int, device='cuda')

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, attn_blocksize=attn_blocksize)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        if master_process:
            print("============== Weight norms: ==============")
            with open(logfile, "a") as f:
                f.write("============== Weight norms: ==============\n")
                for name, p in model.named_parameters():
                    if p.ndim == 2:
                        frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
                        spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
                        nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
                        print(f"{name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
                        f.write(f"{name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
                f.write("===========================================\n")
            print("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y, attn_blocksize=attn_blocksize)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241124+cu124 compiled for CUDA 12.4
nvidia-smi:
Mon Nov 25 08:40:51 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:4E:00.0 Off |                    0 |
| N/A   31C    P0             68W /  400W |   32907MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:87:00.0 Off |                    0 |
| N/A   47C    P0             74W /  400W |      21MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:90:00.0 Off |                    0 |
| N/A   33C    P0             68W /  400W |      33MiB /  81920MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   34C    P0             69W /  400W |      21MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1100000000 across 11 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:nanms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 6215.50146 | spectral_norm = 251.72810 | nuclear_norm = 171939.53125
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 16.02217 | spectral_norm = 1.14396 | nuclear_norm = 376.95203
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 15.99791 | spectral_norm = 1.15398 | nuclear_norm = 376.27350
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 15.99917 | spectral_norm = 1.14908 | nuclear_norm = 376.11551
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 32.00344 | spectral_norm = 1.72197 | nuclear_norm = 858.44006
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 16.00389 | spectral_norm = 1.14705 | nuclear_norm = 376.35153
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 15.99507 | spectral_norm = 1.14149 | nuclear_norm = 376.45667
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 15.99536 | spectral_norm = 1.14779 | nuclear_norm = 376.27878
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 32.00713 | spectral_norm = 1.72893 | nuclear_norm = 858.43494
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 16.02249 | spectral_norm = 1.15283 | nuclear_norm = 377.03418
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 15.99394 | spectral_norm = 1.14462 | nuclear_norm = 376.25021
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 15.98911 | spectral_norm = 1.14441 | nuclear_norm = 376.06036
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 31.99533 | spectral_norm = 1.72281 | nuclear_norm = 858.20520
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 15.99225 | spectral_norm = 1.14339 | nuclear_norm = 376.22931
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 16.00647 | spectral_norm = 1.14479 | nuclear_norm = 376.56342
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 15.99015 | spectral_norm = 1.15315 | nuclear_norm = 376.17230
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 32.00573 | spectral_norm = 1.72714 | nuclear_norm = 858.49219
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 16.00044 | spectral_norm = 1.15686 | nuclear_norm = 376.20667
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 15.99785 | spectral_norm = 1.14406 | nuclear_norm = 376.08350
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 15.99219 | spectral_norm = 1.14480 | nuclear_norm = 376.27261
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 31.98920 | spectral_norm = 1.72537 | nuclear_norm = 858.01886
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 16.00175 | spectral_norm = 1.14768 | nuclear_norm = 376.45770
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 16.00027 | spectral_norm = 1.15057 | nuclear_norm = 376.30774
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 15.99256 | spectral_norm = 1.14028 | nuclear_norm = 376.29730
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 32.00664 | spectral_norm = 1.72302 | nuclear_norm = 858.49792
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 15.99012 | spectral_norm = 1.14592 | nuclear_norm = 376.15704
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 16.00598 | spectral_norm = 1.14466 | nuclear_norm = 376.59949
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 16.00557 | spectral_norm = 1.13649 | nuclear_norm = 376.57208
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 31.99481 | spectral_norm = 1.72782 | nuclear_norm = 858.22900
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 15.99902 | spectral_norm = 1.15168 | nuclear_norm = 376.18643
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 16.00691 | spectral_norm = 1.14351 | nuclear_norm = 376.61151
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 15.99859 | spectral_norm = 1.14256 | nuclear_norm = 376.50714
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 32.01469 | spectral_norm = 1.73631 | nuclear_norm = 858.54468
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 15.99503 | spectral_norm = 1.16031 | nuclear_norm = 376.49896
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 15.99989 | spectral_norm = 1.15856 | nuclear_norm = 376.39935
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 16.00294 | spectral_norm = 1.14269 | nuclear_norm = 376.41437
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 32.01484 | spectral_norm = 1.72425 | nuclear_norm = 858.67017
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 16.00228 | spectral_norm = 1.14490 | nuclear_norm = 376.26233
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 16.01271 | spectral_norm = 1.14818 | nuclear_norm = 376.60562
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 16.00100 | spectral_norm = 1.14706 | nuclear_norm = 376.55743
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 32.00416 | spectral_norm = 1.72424 | nuclear_norm = 858.33704
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 16.01552 | spectral_norm = 1.14451 | nuclear_norm = 376.65262
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 15.98162 | spectral_norm = 1.14583 | nuclear_norm = 376.10336
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 15.99741 | spectral_norm = 1.14123 | nuclear_norm = 376.25134
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 31.99707 | spectral_norm = 1.71972 | nuclear_norm = 858.08881
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 16.00373 | spectral_norm = 1.14976 | nuclear_norm = 376.28802
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 16.00785 | spectral_norm = 1.14845 | nuclear_norm = 376.63040
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 15.99623 | spectral_norm = 1.13672 | nuclear_norm = 376.04266
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 32.00352 | spectral_norm = 1.72199 | nuclear_norm = 858.29309
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 0.00000 | spectral_norm = 0.00000 | nuclear_norm = 0.00000
===========================================
step:1/1750 train_loss:10.8258 train_time:31222ms step_avg:nanms
step:2/1750 train_loss:10.1069 train_time:31847ms step_avg:nanms
step:3/1750 train_loss:8.3523 train_time:32502ms step_avg:nanms
step:4/1750 train_loss:7.6213 train_time:33159ms step_avg:nanms
step:5/1750 train_loss:7.4418 train_time:33816ms step_avg:nanms
step:6/1750 train_loss:7.0091 train_time:34473ms step_avg:nanms
step:7/1750 train_loss:7.0044 train_time:35133ms step_avg:nanms
step:8/1750 train_loss:6.4573 train_time:35793ms step_avg:nanms
step:9/1750 train_loss:6.7831 train_time:36451ms step_avg:nanms
step:10/1750 train_loss:6.5297 train_time:37110ms step_avg:nanms
step:11/1750 train_loss:6.4247 train_time:648ms step_avg:nanms
step:12/1750 train_loss:6.3063 train_time:1307ms step_avg:nanms
step:13/1750 train_loss:6.2941 train_time:1966ms step_avg:655.42ms
step:14/1750 train_loss:6.2966 train_time:2625ms step_avg:656.31ms
step:15/1750 train_loss:6.2413 train_time:3284ms step_avg:656.87ms
step:16/1750 train_loss:6.0398 train_time:3946ms step_avg:657.64ms
step:17/1750 train_loss:5.9752 train_time:4605ms step_avg:657.88ms
step:18/1750 train_loss:6.5355 train_time:5264ms step_avg:658.05ms
step:19/1750 train_loss:5.9668 train_time:5924ms step_avg:658.18ms
step:20/1750 train_loss:6.1149 train_time:6585ms step_avg:658.50ms
step:21/1750 train_loss:6.0383 train_time:7246ms step_avg:658.72ms
step:22/1750 train_loss:5.7862 train_time:7906ms step_avg:658.87ms
step:23/1750 train_loss:5.8555 train_time:8566ms step_avg:658.91ms
step:24/1750 train_loss:5.9092 train_time:9226ms step_avg:659.01ms
step:25/1750 train_loss:5.6644 train_time:9886ms step_avg:659.03ms
step:26/1750 train_loss:5.7971 train_time:10546ms step_avg:659.15ms
step:27/1750 train_loss:5.7477 train_time:11207ms step_avg:659.25ms
step:28/1750 train_loss:5.7191 train_time:11867ms step_avg:659.30ms
step:29/1750 train_loss:5.7497 train_time:12527ms step_avg:659.29ms
step:30/1750 train_loss:5.7621 train_time:13186ms step_avg:659.30ms
step:31/1750 train_loss:6.1115 train_time:13845ms step_avg:659.27ms
step:32/1750 train_loss:5.6327 train_time:14506ms step_avg:659.36ms
step:33/1750 train_loss:5.4927 train_time:15165ms step_avg:659.36ms
step:34/1750 train_loss:5.4962 train_time:15825ms step_avg:659.38ms
step:35/1750 train_loss:5.7284 train_time:16485ms step_avg:659.42ms
step:36/1750 train_loss:5.6147 train_time:17145ms step_avg:659.42ms
step:37/1750 train_loss:5.6168 train_time:17805ms step_avg:659.44ms
step:38/1750 train_loss:5.4534 train_time:18465ms step_avg:659.47ms
step:39/1750 train_loss:5.5480 train_time:19125ms step_avg:659.49ms
step:40/1750 train_loss:5.3549 train_time:19785ms step_avg:659.52ms
step:41/1750 train_loss:5.5087 train_time:20444ms step_avg:659.49ms
step:42/1750 train_loss:5.4041 train_time:21102ms step_avg:659.43ms
step:43/1750 train_loss:5.4239 train_time:21762ms step_avg:659.46ms
step:44/1750 train_loss:5.3070 train_time:22422ms step_avg:659.48ms
step:45/1750 train_loss:5.1942 train_time:23082ms step_avg:659.48ms
step:46/1750 train_loss:5.2699 train_time:23741ms step_avg:659.46ms
step:47/1750 train_loss:5.1995 train_time:24401ms step_avg:659.48ms
step:48/1750 train_loss:5.3256 train_time:25060ms step_avg:659.48ms
step:49/1750 train_loss:5.1553 train_time:25720ms step_avg:659.49ms
step:50/1750 train_loss:5.1990 train_time:26378ms step_avg:659.46ms
step:51/1750 train_loss:5.2368 train_time:27038ms step_avg:659.46ms
step:52/1750 train_loss:5.3470 train_time:27697ms step_avg:659.44ms
step:53/1750 train_loss:5.1644 train_time:28357ms step_avg:659.47ms
step:54/1750 train_loss:5.2266 train_time:29017ms step_avg:659.47ms
step:55/1750 train_loss:5.1063 train_time:29675ms step_avg:659.44ms
step:56/1750 train_loss:5.1201 train_time:30335ms step_avg:659.46ms
step:57/1750 train_loss:5.1493 train_time:30993ms step_avg:659.43ms
step:58/1750 train_loss:5.1694 train_time:31652ms step_avg:659.41ms
step:59/1750 train_loss:5.1792 train_time:32310ms step_avg:659.38ms
step:60/1750 train_loss:5.0448 train_time:32967ms step_avg:659.35ms
step:61/1750 train_loss:5.1801 train_time:33626ms step_avg:659.34ms
step:62/1750 train_loss:5.1917 train_time:34284ms step_avg:659.32ms
step:63/1750 train_loss:5.1134 train_time:34943ms step_avg:659.31ms
step:64/1750 train_loss:5.0713 train_time:35602ms step_avg:659.29ms
step:65/1750 train_loss:4.9099 train_time:36261ms step_avg:659.28ms
step:66/1750 train_loss:4.9873 train_time:36925ms step_avg:659.38ms
step:67/1750 train_loss:5.1051 train_time:37579ms step_avg:659.29ms
step:68/1750 train_loss:5.0570 train_time:38238ms step_avg:659.27ms
step:69/1750 train_loss:5.0921 train_time:38896ms step_avg:659.25ms
step:70/1750 train_loss:4.9524 train_time:39553ms step_avg:659.22ms
step:71/1750 train_loss:5.0312 train_time:40213ms step_avg:659.22ms
step:72/1750 train_loss:5.0110 train_time:40869ms step_avg:659.18ms
step:73/1750 train_loss:4.9955 train_time:41527ms step_avg:659.17ms
step:74/1750 train_loss:4.8354 train_time:42185ms step_avg:659.14ms
step:75/1750 train_loss:4.8804 train_time:42843ms step_avg:659.12ms
step:76/1750 train_loss:4.7763 train_time:43500ms step_avg:659.10ms
step:77/1750 train_loss:4.9860 train_time:44158ms step_avg:659.08ms
step:78/1750 train_loss:4.9372 train_time:44816ms step_avg:659.06ms
step:79/1750 train_loss:4.6480 train_time:45473ms step_avg:659.02ms
step:80/1750 train_loss:4.9199 train_time:46130ms step_avg:659.00ms
step:81/1750 train_loss:4.8675 train_time:46787ms step_avg:658.97ms
step:82/1750 train_loss:4.9117 train_time:47444ms step_avg:658.95ms
step:83/1750 train_loss:4.8992 train_time:48101ms step_avg:658.92ms
step:84/1750 train_loss:4.7919 train_time:48759ms step_avg:658.90ms
step:85/1750 train_loss:4.7865 train_time:49416ms step_avg:658.88ms
step:86/1750 train_loss:4.8746 train_time:50072ms step_avg:658.85ms
step:87/1750 train_loss:4.8852 train_time:50731ms step_avg:658.84ms
step:88/1750 train_loss:4.7260 train_time:51387ms step_avg:658.81ms
step:89/1750 train_loss:4.7221 train_time:52044ms step_avg:658.79ms
step:90/1750 train_loss:4.6654 train_time:52702ms step_avg:658.78ms
step:91/1750 train_loss:4.8055 train_time:53359ms step_avg:658.75ms
step:92/1750 train_loss:4.7690 train_time:54016ms step_avg:658.73ms
step:93/1750 train_loss:4.8601 train_time:54672ms step_avg:658.69ms
step:94/1750 train_loss:4.9827 train_time:55329ms step_avg:658.68ms
step:95/1750 train_loss:4.7185 train_time:55988ms step_avg:658.68ms
step:96/1750 train_loss:4.6237 train_time:56645ms step_avg:658.66ms
step:97/1750 train_loss:4.7782 train_time:57301ms step_avg:658.63ms
step:98/1750 train_loss:4.6320 train_time:57957ms step_avg:658.60ms
step:99/1750 train_loss:4.6089 train_time:58615ms step_avg:658.59ms
step:100/1750 train_loss:4.6542 train_time:59271ms step_avg:658.57ms
step:101/1750 train_loss:4.5100 train_time:59929ms step_avg:658.56ms
step:102/1750 train_loss:4.6702 train_time:60586ms step_avg:658.54ms
step:103/1750 train_loss:4.5614 train_time:61242ms step_avg:658.51ms
step:104/1750 train_loss:4.6656 train_time:61899ms step_avg:658.50ms
step:105/1750 train_loss:4.6598 train_time:62556ms step_avg:658.48ms
step:106/1750 train_loss:4.8217 train_time:63211ms step_avg:658.45ms
step:107/1750 train_loss:4.5876 train_time:63868ms step_avg:658.44ms
step:108/1750 train_loss:4.4553 train_time:64524ms step_avg:658.41ms
step:109/1750 train_loss:4.8435 train_time:65182ms step_avg:658.40ms
step:110/1750 train_loss:4.6392 train_time:65838ms step_avg:658.38ms
step:111/1750 train_loss:4.5363 train_time:66495ms step_avg:658.36ms
step:112/1750 train_loss:4.7559 train_time:67150ms step_avg:658.34ms
step:113/1750 train_loss:4.4130 train_time:67806ms step_avg:658.31ms
step:114/1750 train_loss:4.6297 train_time:68461ms step_avg:658.28ms
step:115/1750 train_loss:4.5513 train_time:69117ms step_avg:658.26ms
step:116/1750 train_loss:4.5929 train_time:69772ms step_avg:658.23ms
step:117/1750 train_loss:4.3696 train_time:70429ms step_avg:658.21ms
step:118/1750 train_loss:4.6017 train_time:71084ms step_avg:658.19ms
step:119/1750 train_loss:4.4220 train_time:71740ms step_avg:658.16ms
step:120/1750 train_loss:4.5252 train_time:72396ms step_avg:658.15ms
step:121/1750 train_loss:4.5149 train_time:73051ms step_avg:658.12ms
step:122/1750 train_loss:4.3943 train_time:73707ms step_avg:658.10ms
step:123/1750 train_loss:4.4697 train_time:74365ms step_avg:658.09ms
step:124/1750 train_loss:4.3411 train_time:75021ms step_avg:658.08ms
step:125/1750 train_loss:4.3562 train_time:75678ms step_avg:658.07ms
step:125/1750 val_loss:4.4602 train_time:75689ms step_avg:658.17ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 26583.32031 | spectral_norm = 6969.14648 | nuclear_norm = 640765.87500
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 35.39754 | spectral_norm = 3.28696 | nuclear_norm = 830.17566
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 34.39336 | spectral_norm = 3.97046 | nuclear_norm = 802.67969
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 44.87763 | spectral_norm = 3.48196 | nuclear_norm = 988.32568
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 37.14672 | spectral_norm = 2.96399 | nuclear_norm = 866.09497
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 66.57104 | spectral_norm = 5.83006 | nuclear_norm = 1764.30676
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 39.19942 | spectral_norm = 2.82251 | nuclear_norm = 1038.78174
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 33.11990 | spectral_norm = 3.15822 | nuclear_norm = 771.52643
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 32.75362 | spectral_norm = 3.79575 | nuclear_norm = 761.87769
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 38.49383 | spectral_norm = 3.34066 | nuclear_norm = 844.93256
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 38.85631 | spectral_norm = 3.06638 | nuclear_norm = 886.79047
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 75.22683 | spectral_norm = 5.56415 | nuclear_norm = 1998.99316
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 42.29836 | spectral_norm = 2.86719 | nuclear_norm = 1122.82666
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 33.32526 | spectral_norm = 3.22433 | nuclear_norm = 773.08582
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 33.30205 | spectral_norm = 3.30353 | nuclear_norm = 771.19751
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 39.79858 | spectral_norm = 3.56156 | nuclear_norm = 865.70050
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 40.21073 | spectral_norm = 3.27391 | nuclear_norm = 920.60754
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 78.36749 | spectral_norm = 5.44191 | nuclear_norm = 2080.37988
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 42.86510 | spectral_norm = 2.91092 | nuclear_norm = 1136.24170
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 32.90127 | spectral_norm = 2.96477 | nuclear_norm = 761.47046
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 33.37801 | spectral_norm = 3.17168 | nuclear_norm = 771.40057
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 39.24607 | spectral_norm = 3.49660 | nuclear_norm = 848.11926
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 39.13457 | spectral_norm = 3.26588 | nuclear_norm = 871.25317
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 79.69272 | spectral_norm = 5.44558 | nuclear_norm = 2113.18530
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 42.83147 | spectral_norm = 2.89725 | nuclear_norm = 1133.90430
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 32.28389 | spectral_norm = 3.03107 | nuclear_norm = 744.44104
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 32.91098 | spectral_norm = 3.00047 | nuclear_norm = 758.53174
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 39.33131 | spectral_norm = 3.64840 | nuclear_norm = 848.53821
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 40.40839 | spectral_norm = 3.27315 | nuclear_norm = 911.74438
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 79.14028 | spectral_norm = 5.44880 | nuclear_norm = 2096.62988
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 42.03777 | spectral_norm = 2.88808 | nuclear_norm = 1111.82227
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 32.08815 | spectral_norm = 2.81659 | nuclear_norm = 737.69629
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 32.75682 | spectral_norm = 2.86244 | nuclear_norm = 752.85950
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 39.31826 | spectral_norm = 3.64076 | nuclear_norm = 848.52490
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 41.18845 | spectral_norm = 3.22644 | nuclear_norm = 945.52179
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 77.70790 | spectral_norm = 5.62381 | nuclear_norm = 2057.76172
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 41.11296 | spectral_norm = 2.78282 | nuclear_norm = 1087.03369
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 31.31102 | spectral_norm = 2.86648 | nuclear_norm = 711.35803
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 32.37205 | spectral_norm = 2.84828 | nuclear_norm = 737.54395
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 40.05597 | spectral_norm = 3.90942 | nuclear_norm = 861.00378
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 42.43610 | spectral_norm = 3.26665 | nuclear_norm = 976.01013
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 76.77745 | spectral_norm = 5.59465 | nuclear_norm = 2026.89514
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 40.59353 | spectral_norm = 2.81156 | nuclear_norm = 1067.48010
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 31.30742 | spectral_norm = 2.83075 | nuclear_norm = 711.02856
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 32.11709 | spectral_norm = 2.81946 | nuclear_norm = 731.29797
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 34.63653 | spectral_norm = 3.38511 | nuclear_norm = 763.59875
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 43.92260 | spectral_norm = 3.38722 | nuclear_norm = 1010.46570
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 76.78836 | spectral_norm = 5.62609 | nuclear_norm = 2026.14282
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 40.50090 | spectral_norm = 2.84149 | nuclear_norm = 1064.98621
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 32.02138 | spectral_norm = 3.01642 | nuclear_norm = 732.04736
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 32.65760 | spectral_norm = 2.99879 | nuclear_norm = 747.54272
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 42.93400 | spectral_norm = 4.01497 | nuclear_norm = 934.55835
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 43.21196 | spectral_norm = 3.29506 | nuclear_norm = 999.50470
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 77.72030 | spectral_norm = 5.71967 | nuclear_norm = 2050.88086
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 40.61621 | spectral_norm = 2.91412 | nuclear_norm = 1067.01184
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 32.07331 | spectral_norm = 3.01660 | nuclear_norm = 732.45782
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 32.77151 | spectral_norm = 2.91857 | nuclear_norm = 748.08313
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 39.34922 | spectral_norm = 3.84956 | nuclear_norm = 848.35736
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 43.12039 | spectral_norm = 3.51936 | nuclear_norm = 995.22559
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 78.74837 | spectral_norm = 5.79266 | nuclear_norm = 2079.34277
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 40.66465 | spectral_norm = 2.87089 | nuclear_norm = 1068.46594
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 31.98635 | spectral_norm = 2.93234 | nuclear_norm = 730.65527
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 32.48567 | spectral_norm = 2.95794 | nuclear_norm = 740.91968
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 37.78283 | spectral_norm = 3.31894 | nuclear_norm = 828.30688
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 44.56953 | spectral_norm = 3.23401 | nuclear_norm = 1032.58472
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 76.26479 | spectral_norm = 5.77083 | nuclear_norm = 2014.56592
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 39.34085 | spectral_norm = 2.88474 | nuclear_norm = 1030.48193
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 31.83027 | spectral_norm = 3.11382 | nuclear_norm = 727.34229
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 32.58222 | spectral_norm = 3.09741 | nuclear_norm = 743.28510
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 37.90317 | spectral_norm = 3.48005 | nuclear_norm = 817.56250
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 41.49687 | spectral_norm = 3.14333 | nuclear_norm = 944.37299
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 75.60629 | spectral_norm = 5.95120 | nuclear_norm = 1994.70801
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 38.60538 | spectral_norm = 2.86297 | nuclear_norm = 1007.96631
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 380.55688 | spectral_norm = 132.33919 | nuclear_norm = 9014.56055
===========================================
step:126/1750 train_loss:4.3236 train_time:76327ms step_avg:657.99ms
step:127/1750 train_loss:4.5186 train_time:76980ms step_avg:657.95ms
step:128/1750 train_loss:4.5126 train_time:77634ms step_avg:657.92ms
step:129/1750 train_loss:4.4974 train_time:78289ms step_avg:657.89ms
step:130/1750 train_loss:4.4693 train_time:78943ms step_avg:657.85ms
step:131/1750 train_loss:4.6042 train_time:79617ms step_avg:657.99ms
step:132/1750 train_loss:4.3510 train_time:80292ms step_avg:658.13ms
step:133/1750 train_loss:4.3160 train_time:80966ms step_avg:658.26ms
step:134/1750 train_loss:4.4735 train_time:81640ms step_avg:658.39ms
step:135/1750 train_loss:4.3071 train_time:82316ms step_avg:658.53ms
step:136/1750 train_loss:4.3193 train_time:82992ms step_avg:658.67ms
step:137/1750 train_loss:4.3693 train_time:83666ms step_avg:658.79ms
step:138/1750 train_loss:4.3990 train_time:84341ms step_avg:658.91ms
step:139/1750 train_loss:4.5031 train_time:85016ms step_avg:659.04ms
step:140/1750 train_loss:4.3785 train_time:85691ms step_avg:659.16ms
step:141/1750 train_loss:4.2636 train_time:86366ms step_avg:659.28ms
step:142/1750 train_loss:4.3917 train_time:87043ms step_avg:659.41ms
step:143/1750 train_loss:4.4880 train_time:87717ms step_avg:659.52ms
step:144/1750 train_loss:4.5542 train_time:88391ms step_avg:659.63ms
step:145/1750 train_loss:4.3201 train_time:89066ms step_avg:659.75ms
step:146/1750 train_loss:4.3486 train_time:89740ms step_avg:659.85ms
step:147/1750 train_loss:4.3722 train_time:90415ms step_avg:659.96ms
step:148/1750 train_loss:4.1630 train_time:91090ms step_avg:660.07ms
step:149/1750 train_loss:4.3265 train_time:91764ms step_avg:660.17ms
step:150/1750 train_loss:4.2918 train_time:92439ms step_avg:660.28ms
step:151/1750 train_loss:4.2972 train_time:93114ms step_avg:660.38ms
step:152/1750 train_loss:4.1901 train_time:93790ms step_avg:660.49ms
step:153/1750 train_loss:4.3770 train_time:94465ms step_avg:660.59ms
step:154/1750 train_loss:4.1838 train_time:95139ms step_avg:660.69ms
step:155/1750 train_loss:4.1765 train_time:95815ms step_avg:660.80ms
step:156/1750 train_loss:4.2997 train_time:96492ms step_avg:660.90ms
step:157/1750 train_loss:4.3764 train_time:97168ms step_avg:661.01ms
step:158/1750 train_loss:4.2907 train_time:97843ms step_avg:661.10ms
step:159/1750 train_loss:4.2350 train_time:98517ms step_avg:661.19ms
step:160/1750 train_loss:4.1868 train_time:99194ms step_avg:661.29ms
step:161/1750 train_loss:4.2484 train_time:99870ms step_avg:661.39ms
step:162/1750 train_loss:4.2632 train_time:100544ms step_avg:661.48ms
step:163/1750 train_loss:4.2377 train_time:101221ms step_avg:661.57ms
step:164/1750 train_loss:4.1677 train_time:101896ms step_avg:661.66ms
step:165/1750 train_loss:4.2374 train_time:102572ms step_avg:661.75ms
step:166/1750 train_loss:4.3719 train_time:103248ms step_avg:661.85ms
step:167/1750 train_loss:4.3075 train_time:103924ms step_avg:661.93ms
step:168/1750 train_loss:4.2294 train_time:104600ms step_avg:662.02ms
step:169/1750 train_loss:4.2785 train_time:105276ms step_avg:662.11ms
step:170/1750 train_loss:4.3249 train_time:105953ms step_avg:662.21ms
step:171/1750 train_loss:3.8207 train_time:106630ms step_avg:662.30ms
step:172/1750 train_loss:4.1565 train_time:107308ms step_avg:662.40ms
step:173/1750 train_loss:4.1836 train_time:107984ms step_avg:662.48ms
step:174/1750 train_loss:4.3716 train_time:108660ms step_avg:662.56ms
step:175/1750 train_loss:4.1876 train_time:109336ms step_avg:662.64ms
step:176/1750 train_loss:4.2432 train_time:110013ms step_avg:662.73ms
step:177/1750 train_loss:4.3877 train_time:110687ms step_avg:662.80ms
step:178/1750 train_loss:4.2591 train_time:111363ms step_avg:662.87ms
step:179/1750 train_loss:4.2005 train_time:112039ms step_avg:662.95ms
step:180/1750 train_loss:4.2515 train_time:112715ms step_avg:663.03ms
step:181/1750 train_loss:4.1252 train_time:113391ms step_avg:663.10ms
step:182/1750 train_loss:4.1804 train_time:114066ms step_avg:663.18ms
step:183/1750 train_loss:4.1572 train_time:114743ms step_avg:663.25ms
step:184/1750 train_loss:4.3101 train_time:115419ms step_avg:663.33ms
step:185/1750 train_loss:4.1973 train_time:116094ms step_avg:663.40ms
step:186/1750 train_loss:4.3220 train_time:116771ms step_avg:663.47ms
step:187/1750 train_loss:4.2161 train_time:117448ms step_avg:663.55ms
step:188/1750 train_loss:4.1903 train_time:118123ms step_avg:663.61ms
step:189/1750 train_loss:4.0216 train_time:118799ms step_avg:663.68ms
step:190/1750 train_loss:4.1363 train_time:119682ms step_avg:664.90ms
step:191/1750 train_loss:4.1181 train_time:120358ms step_avg:664.96ms
step:192/1750 train_loss:4.0667 train_time:121035ms step_avg:665.03ms
step:193/1750 train_loss:4.2800 train_time:121711ms step_avg:665.08ms
step:194/1750 train_loss:4.2093 train_time:122386ms step_avg:665.14ms
step:195/1750 train_loss:4.3950 train_time:123061ms step_avg:665.19ms
step:196/1750 train_loss:4.2200 train_time:123733ms step_avg:665.23ms
step:197/1750 train_loss:4.0892 train_time:124406ms step_avg:665.27ms
step:198/1750 train_loss:4.1998 train_time:125077ms step_avg:665.30ms
step:199/1750 train_loss:4.0527 train_time:125749ms step_avg:665.34ms
step:200/1750 train_loss:4.1379 train_time:126420ms step_avg:665.37ms
step:201/1750 train_loss:4.0244 train_time:127092ms step_avg:665.40ms
step:202/1750 train_loss:4.2742 train_time:127764ms step_avg:665.44ms
step:203/1750 train_loss:4.0881 train_time:128436ms step_avg:665.47ms
step:204/1750 train_loss:4.2173 train_time:129107ms step_avg:665.50ms
step:205/1750 train_loss:4.2747 train_time:129779ms step_avg:665.53ms
step:206/1750 train_loss:3.9588 train_time:130451ms step_avg:665.57ms
step:207/1750 train_loss:4.1115 train_time:131123ms step_avg:665.60ms
step:208/1750 train_loss:4.1164 train_time:131796ms step_avg:665.63ms
step:209/1750 train_loss:4.2593 train_time:132467ms step_avg:665.66ms
step:210/1750 train_loss:4.1873 train_time:133139ms step_avg:665.69ms
step:211/1750 train_loss:4.0662 train_time:133812ms step_avg:665.73ms
step:212/1750 train_loss:4.1370 train_time:134484ms step_avg:665.76ms
step:213/1750 train_loss:4.0774 train_time:135157ms step_avg:665.80ms
step:214/1750 train_loss:4.1230 train_time:135829ms step_avg:665.83ms
step:215/1750 train_loss:3.9711 train_time:136501ms step_avg:665.86ms
step:216/1750 train_loss:4.0241 train_time:137173ms step_avg:665.89ms
step:217/1750 train_loss:4.0355 train_time:137844ms step_avg:665.91ms
step:218/1750 train_loss:4.1101 train_time:138516ms step_avg:665.94ms
step:219/1750 train_loss:4.0849 train_time:139186ms step_avg:665.96ms
step:220/1750 train_loss:4.1068 train_time:139858ms step_avg:665.99ms
step:221/1750 train_loss:4.1126 train_time:140530ms step_avg:666.02ms
step:222/1750 train_loss:4.0115 train_time:141202ms step_avg:666.05ms
step:223/1750 train_loss:4.0049 train_time:141874ms step_avg:666.08ms
step:224/1750 train_loss:4.3152 train_time:142547ms step_avg:666.11ms
step:225/1750 train_loss:3.9236 train_time:143219ms step_avg:666.14ms
step:226/1750 train_loss:4.0005 train_time:143891ms step_avg:666.16ms
step:227/1750 train_loss:4.0041 train_time:144564ms step_avg:666.19ms
step:228/1750 train_loss:4.1640 train_time:145236ms step_avg:666.22ms
step:229/1750 train_loss:3.9566 train_time:145908ms step_avg:666.24ms
step:230/1750 train_loss:4.0778 train_time:146580ms step_avg:666.27ms
step:231/1750 train_loss:3.9420 train_time:147251ms step_avg:666.30ms
step:232/1750 train_loss:4.0088 train_time:147922ms step_avg:666.32ms
step:233/1750 train_loss:4.1231 train_time:148594ms step_avg:666.34ms
step:234/1750 train_loss:4.0584 train_time:149266ms step_avg:666.37ms
step:235/1750 train_loss:3.9436 train_time:149937ms step_avg:666.39ms
step:236/1750 train_loss:4.1267 train_time:150608ms step_avg:666.41ms
step:237/1750 train_loss:4.1046 train_time:151280ms step_avg:666.43ms
step:238/1750 train_loss:3.9701 train_time:151951ms step_avg:666.45ms
step:239/1750 train_loss:4.1024 train_time:152623ms step_avg:666.48ms
step:240/1750 train_loss:4.1450 train_time:153295ms step_avg:666.50ms
step:241/1750 train_loss:3.9979 train_time:153967ms step_avg:666.53ms
step:242/1750 train_loss:4.1780 train_time:154639ms step_avg:666.55ms
step:243/1750 train_loss:4.0458 train_time:155310ms step_avg:666.56ms
step:244/1750 train_loss:4.0993 train_time:155981ms step_avg:666.58ms
step:245/1750 train_loss:4.1741 train_time:156653ms step_avg:666.61ms
step:246/1750 train_loss:4.0921 train_time:157325ms step_avg:666.63ms
step:247/1750 train_loss:4.0349 train_time:157998ms step_avg:666.66ms
step:248/1750 train_loss:4.1393 train_time:158668ms step_avg:666.67ms
step:249/1750 train_loss:3.9424 train_time:159340ms step_avg:666.69ms
step:250/1750 train_loss:3.9973 train_time:160012ms step_avg:666.72ms
step:250/1750 val_loss:4.0345 train_time:160023ms step_avg:666.76ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 35547.85938 | spectral_norm = 7729.11426 | nuclear_norm = 868519.75000
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 56.37825 | spectral_norm = 6.00807 | nuclear_norm = 1318.02136
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 54.70770 | spectral_norm = 7.66587 | nuclear_norm = 1269.13037
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 73.40329 | spectral_norm = 5.03411 | nuclear_norm = 1690.15356
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 58.69942 | spectral_norm = 5.07333 | nuclear_norm = 1379.43225
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 109.16529 | spectral_norm = 9.51572 | nuclear_norm = 2900.40137
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 60.06755 | spectral_norm = 4.52840 | nuclear_norm = 1593.81042
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 53.94980 | spectral_norm = 5.19622 | nuclear_norm = 1256.74817
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 53.06163 | spectral_norm = 6.29918 | nuclear_norm = 1236.14648
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 58.91351 | spectral_norm = 5.02232 | nuclear_norm = 1321.69531
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 60.22593 | spectral_norm = 4.79917 | nuclear_norm = 1393.43469
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 119.57018 | spectral_norm = 8.28138 | nuclear_norm = 3188.42261
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 63.35928 | spectral_norm = 4.80945 | nuclear_norm = 1680.17969
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 54.60933 | spectral_norm = 5.46284 | nuclear_norm = 1268.44141
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 54.04506 | spectral_norm = 5.33299 | nuclear_norm = 1253.22876
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 63.35728 | spectral_norm = 5.55948 | nuclear_norm = 1403.70715
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 64.69484 | spectral_norm = 5.26928 | nuclear_norm = 1504.08484
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 124.12603 | spectral_norm = 8.48532 | nuclear_norm = 3309.46216
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 63.92658 | spectral_norm = 4.96051 | nuclear_norm = 1691.80139
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 54.09912 | spectral_norm = 4.96827 | nuclear_norm = 1254.74597
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 53.90664 | spectral_norm = 5.33756 | nuclear_norm = 1251.34961
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 61.57607 | spectral_norm = 5.44212 | nuclear_norm = 1336.22266
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 62.97443 | spectral_norm = 5.18876 | nuclear_norm = 1420.79663
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 126.78632 | spectral_norm = 8.81779 | nuclear_norm = 3376.23291
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 64.11669 | spectral_norm = 5.01915 | nuclear_norm = 1694.37964
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 53.40513 | spectral_norm = 4.96978 | nuclear_norm = 1234.02087
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 53.88636 | spectral_norm = 4.75104 | nuclear_norm = 1243.75684
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 63.01888 | spectral_norm = 5.56526 | nuclear_norm = 1376.95752
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 65.88253 | spectral_norm = 5.21459 | nuclear_norm = 1504.19995
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 126.73658 | spectral_norm = 8.76482 | nuclear_norm = 3373.30420
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 63.03641 | spectral_norm = 4.98247 | nuclear_norm = 1666.44055
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 53.84950 | spectral_norm = 4.67535 | nuclear_norm = 1236.00708
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 54.10642 | spectral_norm = 4.59132 | nuclear_norm = 1246.68079
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 62.63434 | spectral_norm = 5.62103 | nuclear_norm = 1371.83740
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 65.89995 | spectral_norm = 5.05556 | nuclear_norm = 1535.81982
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 128.19981 | spectral_norm = 8.61286 | nuclear_norm = 3414.10669
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 62.50845 | spectral_norm = 4.73176 | nuclear_norm = 1656.11426
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 53.52724 | spectral_norm = 4.70934 | nuclear_norm = 1215.12939
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 53.80730 | spectral_norm = 4.60023 | nuclear_norm = 1225.01123
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 65.56734 | spectral_norm = 6.49785 | nuclear_norm = 1416.53052
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 69.66647 | spectral_norm = 5.29281 | nuclear_norm = 1631.81250
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 125.33748 | spectral_norm = 9.00767 | nuclear_norm = 3335.16943
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 62.29847 | spectral_norm = 4.94711 | nuclear_norm = 1646.65234
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 54.42016 | spectral_norm = 4.62040 | nuclear_norm = 1242.69458
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 54.21370 | spectral_norm = 4.84455 | nuclear_norm = 1240.31104
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 54.54043 | spectral_norm = 4.68747 | nuclear_norm = 1224.93213
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 69.22926 | spectral_norm = 4.98090 | nuclear_norm = 1623.88965
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 123.98103 | spectral_norm = 9.29105 | nuclear_norm = 3296.52002
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 62.62164 | spectral_norm = 5.17790 | nuclear_norm = 1656.08032
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 54.95425 | spectral_norm = 4.90497 | nuclear_norm = 1267.87231
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 54.11092 | spectral_norm = 5.10780 | nuclear_norm = 1248.65039
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 67.14749 | spectral_norm = 6.33274 | nuclear_norm = 1487.03955
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 69.98788 | spectral_norm = 5.10772 | nuclear_norm = 1646.73120
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 125.03794 | spectral_norm = 9.27122 | nuclear_norm = 3323.09814
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 63.00754 | spectral_norm = 5.19678 | nuclear_norm = 1664.70679
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 54.15041 | spectral_norm = 5.14824 | nuclear_norm = 1246.48865
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 54.17124 | spectral_norm = 4.98893 | nuclear_norm = 1246.71802
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 64.60757 | spectral_norm = 6.60226 | nuclear_norm = 1418.35791
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 71.29256 | spectral_norm = 5.80940 | nuclear_norm = 1673.87463
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 126.71156 | spectral_norm = 9.20136 | nuclear_norm = 3367.16016
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 63.31259 | spectral_norm = 5.30970 | nuclear_norm = 1672.29028
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 54.94719 | spectral_norm = 4.95794 | nuclear_norm = 1260.33142
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 54.35044 | spectral_norm = 5.02810 | nuclear_norm = 1240.42737
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 54.84707 | spectral_norm = 4.45312 | nuclear_norm = 1246.43958
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 73.05181 | spectral_norm = 5.01951 | nuclear_norm = 1716.17505
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 123.31540 | spectral_norm = 9.05956 | nuclear_norm = 3279.64038
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 62.57032 | spectral_norm = 5.45844 | nuclear_norm = 1649.81726
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 54.56630 | spectral_norm = 5.64096 | nuclear_norm = 1255.55884
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 55.18935 | spectral_norm = 5.25872 | nuclear_norm = 1268.76978
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 61.35753 | spectral_norm = 5.91545 | nuclear_norm = 1344.99231
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 68.69946 | spectral_norm = 5.15356 | nuclear_norm = 1578.43787
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 124.88492 | spectral_norm = 9.03659 | nuclear_norm = 3317.41406
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 62.98818 | spectral_norm = 5.57874 | nuclear_norm = 1653.98804
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 545.38000 | spectral_norm = 151.17458 | nuclear_norm = 13522.95508
===========================================
step:251/1750 train_loss:4.0998 train_time:160676ms step_avg:666.71ms
step:252/1750 train_loss:4.1826 train_time:161343ms step_avg:666.70ms
step:253/1750 train_loss:3.9589 train_time:162011ms step_avg:666.71ms
step:254/1750 train_loss:3.9160 train_time:162681ms step_avg:666.72ms
step:255/1750 train_loss:4.1131 train_time:163352ms step_avg:666.74ms
step:256/1750 train_loss:4.0189 train_time:164023ms step_avg:666.76ms
step:257/1750 train_loss:4.0244 train_time:164692ms step_avg:666.77ms
step:258/1750 train_loss:4.0124 train_time:165362ms step_avg:666.78ms
step:259/1750 train_loss:4.0595 train_time:166032ms step_avg:666.80ms
step:260/1750 train_loss:4.1029 train_time:166703ms step_avg:666.81ms
step:261/1750 train_loss:4.0422 train_time:167391ms step_avg:666.90ms
step:262/1750 train_loss:4.0164 train_time:168081ms step_avg:666.99ms
step:263/1750 train_loss:3.9206 train_time:168771ms step_avg:667.08ms
step:264/1750 train_loss:4.0155 train_time:169459ms step_avg:667.16ms
step:265/1750 train_loss:3.8929 train_time:170148ms step_avg:667.25ms
step:266/1750 train_loss:3.9389 train_time:170834ms step_avg:667.32ms
step:267/1750 train_loss:3.9448 train_time:171522ms step_avg:667.40ms
step:268/1750 train_loss:3.9812 train_time:172210ms step_avg:667.48ms
step:269/1750 train_loss:3.8821 train_time:172899ms step_avg:667.57ms
step:270/1750 train_loss:4.1427 train_time:173586ms step_avg:667.64ms
step:271/1750 train_loss:3.9987 train_time:174273ms step_avg:667.71ms
step:272/1750 train_loss:3.9447 train_time:174962ms step_avg:667.80ms
step:273/1750 train_loss:3.9681 train_time:175651ms step_avg:667.87ms
step:274/1750 train_loss:4.0628 train_time:176340ms step_avg:667.95ms
step:275/1750 train_loss:4.0841 train_time:177030ms step_avg:668.04ms
step:276/1750 train_loss:4.2403 train_time:177720ms step_avg:668.12ms
step:277/1750 train_loss:4.0530 train_time:178407ms step_avg:668.19ms
step:278/1750 train_loss:4.1046 train_time:179095ms step_avg:668.27ms
step:279/1750 train_loss:4.0180 train_time:179783ms step_avg:668.34ms
step:280/1750 train_loss:4.1834 train_time:180475ms step_avg:668.43ms
step:281/1750 train_loss:3.9894 train_time:181166ms step_avg:668.51ms
step:282/1750 train_loss:3.9783 train_time:181857ms step_avg:668.59ms
step:283/1750 train_loss:3.9362 train_time:182546ms step_avg:668.67ms
step:284/1750 train_loss:4.0705 train_time:183236ms step_avg:668.74ms
step:285/1750 train_loss:4.0911 train_time:183924ms step_avg:668.81ms
step:286/1750 train_loss:4.1133 train_time:184614ms step_avg:668.89ms
step:287/1750 train_loss:3.9401 train_time:185303ms step_avg:668.96ms
step:288/1750 train_loss:4.0471 train_time:185989ms step_avg:669.03ms
step:289/1750 train_loss:3.9081 train_time:186678ms step_avg:669.10ms
step:290/1750 train_loss:3.8862 train_time:187367ms step_avg:669.17ms
step:291/1750 train_loss:3.9566 train_time:188055ms step_avg:669.23ms
step:292/1750 train_loss:3.8864 train_time:188742ms step_avg:669.30ms
step:293/1750 train_loss:3.9382 train_time:189430ms step_avg:669.36ms
step:294/1750 train_loss:3.9682 train_time:190119ms step_avg:669.43ms
step:295/1750 train_loss:3.8613 train_time:190808ms step_avg:669.50ms
step:296/1750 train_loss:3.8973 train_time:191498ms step_avg:669.57ms
step:297/1750 train_loss:3.8942 train_time:192188ms step_avg:669.65ms
step:298/1750 train_loss:4.0045 train_time:192877ms step_avg:669.71ms
step:299/1750 train_loss:3.8526 train_time:193567ms step_avg:669.78ms
step:300/1750 train_loss:3.9947 train_time:194256ms step_avg:669.85ms
step:301/1750 train_loss:3.9966 train_time:194946ms step_avg:669.92ms
step:302/1750 train_loss:3.9606 train_time:195634ms step_avg:669.98ms
step:303/1750 train_loss:4.0064 train_time:196321ms step_avg:670.04ms
step:304/1750 train_loss:3.9983 train_time:197010ms step_avg:670.10ms
step:305/1750 train_loss:4.4955 train_time:197697ms step_avg:670.16ms
step:306/1750 train_loss:3.9637 train_time:198387ms step_avg:670.23ms
step:307/1750 train_loss:3.8641 train_time:199077ms step_avg:670.29ms
step:308/1750 train_loss:4.0150 train_time:199766ms step_avg:670.35ms
step:309/1750 train_loss:3.8985 train_time:200454ms step_avg:670.41ms
step:310/1750 train_loss:4.1140 train_time:201141ms step_avg:670.47ms
step:311/1750 train_loss:3.9573 train_time:201831ms step_avg:670.54ms
step:312/1750 train_loss:3.8908 train_time:202519ms step_avg:670.59ms
step:313/1750 train_loss:3.9675 train_time:203211ms step_avg:670.66ms
step:314/1750 train_loss:4.0990 train_time:203899ms step_avg:670.72ms
step:315/1750 train_loss:3.9749 train_time:204587ms step_avg:670.78ms
step:316/1750 train_loss:3.8298 train_time:205275ms step_avg:670.83ms
step:317/1750 train_loss:3.9122 train_time:205964ms step_avg:670.89ms
step:318/1750 train_loss:3.9538 train_time:206656ms step_avg:670.96ms
step:319/1750 train_loss:3.9114 train_time:207343ms step_avg:671.01ms
step:320/1750 train_loss:4.0473 train_time:208034ms step_avg:671.08ms
step:321/1750 train_loss:3.9925 train_time:208722ms step_avg:671.13ms
step:322/1750 train_loss:3.9657 train_time:209411ms step_avg:671.19ms
step:323/1750 train_loss:4.0347 train_time:210099ms step_avg:671.24ms
step:324/1750 train_loss:3.9770 train_time:210788ms step_avg:671.30ms
step:325/1750 train_loss:4.0466 train_time:211477ms step_avg:671.36ms
step:326/1750 train_loss:3.9157 train_time:212161ms step_avg:671.39ms
step:327/1750 train_loss:4.4219 train_time:212845ms step_avg:671.44ms
step:328/1750 train_loss:4.0960 train_time:213532ms step_avg:671.49ms
step:329/1750 train_loss:3.8288 train_time:214221ms step_avg:671.54ms
step:330/1750 train_loss:3.7642 train_time:214905ms step_avg:671.58ms
step:331/1750 train_loss:4.0076 train_time:215590ms step_avg:671.62ms
step:332/1750 train_loss:3.9379 train_time:216273ms step_avg:671.66ms
step:333/1750 train_loss:3.9134 train_time:216958ms step_avg:671.70ms
step:334/1750 train_loss:3.8722 train_time:217643ms step_avg:671.74ms
step:335/1750 train_loss:4.0312 train_time:218326ms step_avg:671.77ms
step:336/1750 train_loss:3.9815 train_time:219010ms step_avg:671.81ms
step:337/1750 train_loss:4.4468 train_time:219696ms step_avg:671.85ms
step:338/1750 train_loss:3.9645 train_time:220382ms step_avg:671.90ms
step:339/1750 train_loss:3.8762 train_time:221069ms step_avg:671.94ms
step:340/1750 train_loss:3.9597 train_time:221755ms step_avg:671.98ms
step:341/1750 train_loss:3.8798 train_time:222439ms step_avg:672.02ms
step:342/1750 train_loss:3.8398 train_time:223124ms step_avg:672.06ms
step:343/1750 train_loss:3.8703 train_time:223810ms step_avg:672.10ms
step:344/1750 train_loss:4.0217 train_time:224496ms step_avg:672.14ms
step:345/1750 train_loss:3.8472 train_time:225181ms step_avg:672.18ms
step:346/1750 train_loss:3.7910 train_time:225866ms step_avg:672.22ms
step:347/1750 train_loss:3.8289 train_time:226550ms step_avg:672.26ms
step:348/1750 train_loss:3.8852 train_time:227235ms step_avg:672.29ms
step:349/1750 train_loss:3.8594 train_time:227919ms step_avg:672.33ms
step:350/1750 train_loss:3.5887 train_time:228604ms step_avg:672.36ms
step:351/1750 train_loss:3.8484 train_time:229288ms step_avg:672.40ms
step:352/1750 train_loss:4.2114 train_time:229971ms step_avg:672.43ms
step:353/1750 train_loss:3.6857 train_time:230658ms step_avg:672.47ms
step:354/1750 train_loss:3.9602 train_time:231343ms step_avg:672.51ms
step:355/1750 train_loss:3.8188 train_time:232028ms step_avg:672.55ms
step:356/1750 train_loss:3.9117 train_time:232712ms step_avg:672.58ms
step:357/1750 train_loss:3.7845 train_time:233397ms step_avg:672.61ms
step:358/1750 train_loss:3.8843 train_time:234084ms step_avg:672.65ms
step:359/1750 train_loss:3.8688 train_time:234770ms step_avg:672.69ms
step:360/1750 train_loss:3.4627 train_time:235453ms step_avg:672.72ms
step:361/1750 train_loss:4.0509 train_time:236139ms step_avg:672.76ms
step:362/1750 train_loss:3.9431 train_time:236823ms step_avg:672.79ms
step:363/1750 train_loss:3.8726 train_time:237507ms step_avg:672.83ms
step:364/1750 train_loss:3.7715 train_time:238193ms step_avg:672.86ms
step:365/1750 train_loss:3.9475 train_time:238876ms step_avg:672.89ms
step:366/1750 train_loss:3.8978 train_time:239560ms step_avg:672.92ms
step:367/1750 train_loss:3.8821 train_time:240245ms step_avg:672.96ms
step:368/1750 train_loss:3.8748 train_time:240930ms step_avg:672.99ms
step:369/1750 train_loss:3.7689 train_time:241615ms step_avg:673.02ms
step:370/1750 train_loss:3.9146 train_time:242298ms step_avg:673.05ms
step:371/1750 train_loss:3.7707 train_time:242983ms step_avg:673.08ms
step:372/1750 train_loss:3.7184 train_time:243667ms step_avg:673.11ms
step:373/1750 train_loss:3.9450 train_time:244351ms step_avg:673.14ms
step:374/1750 train_loss:3.8555 train_time:245036ms step_avg:673.17ms
step:375/1750 train_loss:3.8369 train_time:245720ms step_avg:673.20ms
step:375/1750 val_loss:3.8546 train_time:245731ms step_avg:673.24ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 42083.59375 | spectral_norm = 8466.57227 | nuclear_norm = 1036568.81250
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 81.22605 | spectral_norm = 9.71831 | nuclear_norm = 1896.39990
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 79.01964 | spectral_norm = 12.06475 | nuclear_norm = 1829.42139
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 104.35901 | spectral_norm = 6.85341 | nuclear_norm = 2449.96387
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 84.41692 | spectral_norm = 7.59092 | nuclear_norm = 1983.26050
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 163.51540 | spectral_norm = 13.21218 | nuclear_norm = 4349.27295
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 87.07665 | spectral_norm = 6.76074 | nuclear_norm = 2314.17041
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 78.85995 | spectral_norm = 7.64347 | nuclear_norm = 1840.06812
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 77.65071 | spectral_norm = 9.02284 | nuclear_norm = 1811.18115
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 84.27238 | spectral_norm = 7.11781 | nuclear_norm = 1926.68652
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 85.25583 | spectral_norm = 6.87783 | nuclear_norm = 1983.27393
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 170.78557 | spectral_norm = 11.25457 | nuclear_norm = 4557.86670
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 88.89560 | spectral_norm = 7.30147 | nuclear_norm = 2356.45850
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 79.83851 | spectral_norm = 8.04509 | nuclear_norm = 1855.33081
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 78.74832 | spectral_norm = 7.79707 | nuclear_norm = 1827.15991
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 92.03313 | spectral_norm = 7.81799 | nuclear_norm = 2082.23682
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 93.13716 | spectral_norm = 7.41984 | nuclear_norm = 2174.00073
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 174.68175 | spectral_norm = 12.20896 | nuclear_norm = 4663.38379
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 88.08088 | spectral_norm = 7.52226 | nuclear_norm = 2326.14844
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 79.28452 | spectral_norm = 7.53130 | nuclear_norm = 1840.95935
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 78.77094 | spectral_norm = 7.78122 | nuclear_norm = 1831.55176
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 87.97330 | spectral_norm = 7.65617 | nuclear_norm = 1933.51135
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 89.63820 | spectral_norm = 7.43224 | nuclear_norm = 2033.95044
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 178.34160 | spectral_norm = 12.94852 | nuclear_norm = 4755.70801
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 88.67013 | spectral_norm = 7.69268 | nuclear_norm = 2337.97095
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 78.27057 | spectral_norm = 7.24656 | nuclear_norm = 1807.72241
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 78.52496 | spectral_norm = 6.91202 | nuclear_norm = 1811.32617
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 90.03847 | spectral_norm = 7.74167 | nuclear_norm = 1993.28210
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 93.31824 | spectral_norm = 7.44778 | nuclear_norm = 2133.71191
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 179.30475 | spectral_norm = 13.08074 | nuclear_norm = 4778.47266
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 88.37772 | spectral_norm = 7.49803 | nuclear_norm = 2333.61426
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 78.67226 | spectral_norm = 6.98543 | nuclear_norm = 1799.50781
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 78.57776 | spectral_norm = 6.90537 | nuclear_norm = 1808.20251
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 89.90271 | spectral_norm = 8.00460 | nuclear_norm = 2001.86865
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 93.32081 | spectral_norm = 7.15811 | nuclear_norm = 2177.31299
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 185.04999 | spectral_norm = 12.55081 | nuclear_norm = 4937.23389
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 88.59604 | spectral_norm = 7.69919 | nuclear_norm = 2347.03491
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 78.23685 | spectral_norm = 7.08860 | nuclear_norm = 1775.34619
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 78.23969 | spectral_norm = 6.83298 | nuclear_norm = 1774.72437
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 94.77128 | spectral_norm = 9.61766 | nuclear_norm = 2062.92969
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 100.62885 | spectral_norm = 8.06378 | nuclear_norm = 2364.37183
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 180.77681 | spectral_norm = 13.16864 | nuclear_norm = 4823.19434
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 88.96103 | spectral_norm = 7.56130 | nuclear_norm = 2356.95679
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 80.54216 | spectral_norm = 7.30504 | nuclear_norm = 1839.35107
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 79.87775 | spectral_norm = 7.23753 | nuclear_norm = 1820.05383
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 81.07767 | spectral_norm = 7.20059 | nuclear_norm = 1826.80273
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 95.83822 | spectral_norm = 6.67382 | nuclear_norm = 2260.75879
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 179.51855 | spectral_norm = 13.50302 | nuclear_norm = 4786.54688
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 90.77090 | spectral_norm = 7.82813 | nuclear_norm = 2407.76172
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 81.51216 | spectral_norm = 7.49733 | nuclear_norm = 1883.04395
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 79.58781 | spectral_norm = 7.86061 | nuclear_norm = 1835.56226
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 97.76722 | spectral_norm = 9.37010 | nuclear_norm = 2193.36938
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 100.09711 | spectral_norm = 7.17325 | nuclear_norm = 2360.64502
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 179.71455 | spectral_norm = 13.48658 | nuclear_norm = 4788.98877
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 90.98167 | spectral_norm = 7.87017 | nuclear_norm = 2411.76660
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 79.59263 | spectral_norm = 7.92655 | nuclear_norm = 1830.25708
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 79.34592 | spectral_norm = 7.37427 | nuclear_norm = 1827.12329
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 95.90776 | spectral_norm = 9.57923 | nuclear_norm = 2135.82227
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 102.70897 | spectral_norm = 8.31912 | nuclear_norm = 2420.45435
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 181.70992 | spectral_norm = 13.51396 | nuclear_norm = 4843.46973
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 90.30885 | spectral_norm = 8.09917 | nuclear_norm = 2391.31470
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 81.29391 | spectral_norm = 7.68230 | nuclear_norm = 1864.69238
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 79.81470 | spectral_norm = 7.61712 | nuclear_norm = 1816.97485
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 80.72871 | spectral_norm = 6.86834 | nuclear_norm = 1859.22449
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 100.46921 | spectral_norm = 6.87201 | nuclear_norm = 2366.12695
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 179.88708 | spectral_norm = 12.43683 | nuclear_norm = 4799.31396
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 88.45673 | spectral_norm = 8.51001 | nuclear_norm = 2336.05615
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 81.04003 | spectral_norm = 8.71250 | nuclear_norm = 1864.50061
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 81.17697 | spectral_norm = 7.80638 | nuclear_norm = 1869.66809
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 91.32877 | spectral_norm = 8.93596 | nuclear_norm = 2025.16553
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 99.17510 | spectral_norm = 7.53772 | nuclear_norm = 2287.05322
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 183.37593 | spectral_norm = 13.16723 | nuclear_norm = 4882.69287
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 90.10944 | spectral_norm = 8.62483 | nuclear_norm = 2367.62158
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 684.74976 | spectral_norm = 159.71442 | nuclear_norm = 17332.53906
===========================================
step:376/1750 train_loss:3.8985 train_time:246396ms step_avg:673.21ms
step:377/1750 train_loss:3.8218 train_time:247079ms step_avg:673.24ms
step:378/1750 train_loss:3.8664 train_time:247761ms step_avg:673.26ms
step:379/1750 train_loss:3.8886 train_time:248442ms step_avg:673.29ms
step:380/1750 train_loss:3.9769 train_time:249334ms step_avg:673.87ms
step:381/1750 train_loss:3.7251 train_time:250215ms step_avg:674.43ms
step:382/1750 train_loss:3.7901 train_time:250904ms step_avg:674.47ms
step:383/1750 train_loss:3.8044 train_time:251587ms step_avg:674.50ms
step:384/1750 train_loss:3.9089 train_time:252269ms step_avg:674.52ms
step:385/1750 train_loss:3.7031 train_time:252953ms step_avg:674.54ms
step:386/1750 train_loss:3.8874 train_time:253636ms step_avg:674.56ms
step:387/1750 train_loss:3.8145 train_time:254320ms step_avg:674.59ms
step:388/1750 train_loss:3.9952 train_time:255004ms step_avg:674.61ms
step:389/1750 train_loss:3.8282 train_time:255687ms step_avg:674.63ms
step:390/1750 train_loss:3.8992 train_time:256387ms step_avg:674.70ms
step:391/1750 train_loss:3.7354 train_time:257088ms step_avg:674.77ms
step:392/1750 train_loss:3.8050 train_time:257786ms step_avg:674.83ms
step:393/1750 train_loss:3.8379 train_time:258482ms step_avg:674.89ms
step:394/1750 train_loss:3.8219 train_time:259179ms step_avg:674.95ms
step:395/1750 train_loss:3.8344 train_time:259878ms step_avg:675.01ms
step:396/1750 train_loss:3.7421 train_time:260579ms step_avg:675.08ms
step:397/1750 train_loss:3.6012 train_time:261280ms step_avg:675.14ms
step:398/1750 train_loss:3.8402 train_time:261981ms step_avg:675.21ms
step:399/1750 train_loss:3.8079 train_time:262682ms step_avg:675.27ms
step:400/1750 train_loss:3.7329 train_time:263379ms step_avg:675.33ms
step:401/1750 train_loss:3.8361 train_time:264078ms step_avg:675.39ms
step:402/1750 train_loss:3.7130 train_time:264777ms step_avg:675.45ms
step:403/1750 train_loss:4.0081 train_time:265480ms step_avg:675.52ms
step:404/1750 train_loss:3.9311 train_time:266180ms step_avg:675.59ms
step:405/1750 train_loss:3.8672 train_time:266879ms step_avg:675.64ms
step:406/1750 train_loss:3.8818 train_time:267579ms step_avg:675.70ms
step:407/1750 train_loss:3.8613 train_time:268276ms step_avg:675.76ms
step:408/1750 train_loss:3.7691 train_time:268980ms step_avg:675.83ms
step:409/1750 train_loss:3.8368 train_time:269679ms step_avg:675.89ms
step:410/1750 train_loss:3.7773 train_time:270377ms step_avg:675.94ms
step:411/1750 train_loss:3.7877 train_time:271075ms step_avg:676.00ms
step:412/1750 train_loss:3.7965 train_time:271773ms step_avg:676.05ms
step:413/1750 train_loss:3.8019 train_time:272473ms step_avg:676.11ms
step:414/1750 train_loss:3.8952 train_time:273174ms step_avg:676.17ms
step:415/1750 train_loss:3.7435 train_time:273874ms step_avg:676.23ms
step:416/1750 train_loss:3.8260 train_time:274573ms step_avg:676.29ms
step:417/1750 train_loss:3.9160 train_time:275273ms step_avg:676.35ms
step:418/1750 train_loss:3.6965 train_time:275973ms step_avg:676.40ms
step:419/1750 train_loss:3.9510 train_time:276670ms step_avg:676.46ms
step:420/1750 train_loss:4.0202 train_time:277371ms step_avg:676.51ms
step:421/1750 train_loss:3.7845 train_time:278070ms step_avg:676.57ms
step:422/1750 train_loss:3.9167 train_time:278769ms step_avg:676.62ms
step:423/1750 train_loss:3.6122 train_time:279472ms step_avg:676.69ms
step:424/1750 train_loss:3.7993 train_time:280170ms step_avg:676.74ms
step:425/1750 train_loss:3.6808 train_time:280870ms step_avg:676.80ms
step:426/1750 train_loss:3.8901 train_time:281570ms step_avg:676.85ms
step:427/1750 train_loss:3.8546 train_time:282269ms step_avg:676.90ms
step:428/1750 train_loss:3.7839 train_time:282971ms step_avg:676.96ms
step:429/1750 train_loss:3.9019 train_time:283669ms step_avg:677.01ms
step:430/1750 train_loss:3.7161 train_time:284369ms step_avg:677.07ms
step:431/1750 train_loss:3.6538 train_time:285070ms step_avg:677.13ms
step:432/1750 train_loss:3.8594 train_time:285769ms step_avg:677.18ms
step:433/1750 train_loss:3.8864 train_time:286465ms step_avg:677.22ms
step:434/1750 train_loss:3.8722 train_time:287163ms step_avg:677.27ms
step:435/1750 train_loss:3.7764 train_time:287864ms step_avg:677.33ms
step:436/1750 train_loss:3.8528 train_time:288561ms step_avg:677.37ms
step:437/1750 train_loss:3.8532 train_time:289259ms step_avg:677.42ms
step:438/1750 train_loss:3.8272 train_time:289958ms step_avg:677.47ms
step:439/1750 train_loss:3.8873 train_time:290659ms step_avg:677.53ms
step:440/1750 train_loss:3.7239 train_time:291362ms step_avg:677.59ms
step:441/1750 train_loss:3.8291 train_time:292062ms step_avg:677.64ms
step:442/1750 train_loss:3.7444 train_time:292762ms step_avg:677.69ms
step:443/1750 train_loss:3.6424 train_time:293461ms step_avg:677.74ms
step:444/1750 train_loss:3.7936 train_time:294157ms step_avg:677.78ms
step:445/1750 train_loss:4.0496 train_time:294858ms step_avg:677.83ms
step:446/1750 train_loss:3.6603 train_time:295555ms step_avg:677.88ms
step:447/1750 train_loss:3.8554 train_time:296254ms step_avg:677.93ms
step:448/1750 train_loss:3.9117 train_time:296955ms step_avg:677.98ms
step:449/1750 train_loss:3.7308 train_time:297655ms step_avg:678.03ms
step:450/1750 train_loss:3.6979 train_time:298356ms step_avg:678.08ms
step:451/1750 train_loss:3.7468 train_time:299061ms step_avg:678.14ms
step:452/1750 train_loss:4.0791 train_time:299763ms step_avg:678.20ms
step:453/1750 train_loss:3.9796 train_time:300462ms step_avg:678.24ms
step:454/1750 train_loss:3.8260 train_time:301162ms step_avg:678.29ms
step:455/1750 train_loss:3.7281 train_time:301860ms step_avg:678.34ms
step:456/1750 train_loss:3.8385 train_time:302558ms step_avg:678.38ms
step:457/1750 train_loss:3.7697 train_time:303252ms step_avg:678.42ms
step:458/1750 train_loss:3.7803 train_time:303945ms step_avg:678.45ms
step:459/1750 train_loss:3.8783 train_time:304641ms step_avg:678.49ms
step:460/1750 train_loss:3.6928 train_time:305338ms step_avg:678.53ms
step:461/1750 train_loss:3.8052 train_time:306037ms step_avg:678.57ms
step:462/1750 train_loss:3.7690 train_time:306732ms step_avg:678.61ms
step:463/1750 train_loss:3.6147 train_time:307427ms step_avg:678.65ms
step:464/1750 train_loss:3.7596 train_time:308122ms step_avg:678.68ms
step:465/1750 train_loss:3.8454 train_time:308817ms step_avg:678.72ms
step:466/1750 train_loss:3.7434 train_time:309514ms step_avg:678.76ms
step:467/1750 train_loss:3.7478 train_time:310210ms step_avg:678.80ms
step:468/1750 train_loss:3.7256 train_time:310907ms step_avg:678.84ms
step:469/1750 train_loss:3.9342 train_time:311600ms step_avg:678.87ms
step:470/1750 train_loss:3.7661 train_time:312294ms step_avg:678.90ms
step:471/1750 train_loss:3.6273 train_time:312994ms step_avg:678.95ms
step:472/1750 train_loss:3.8450 train_time:313688ms step_avg:678.98ms
step:473/1750 train_loss:3.6983 train_time:314380ms step_avg:679.01ms
step:474/1750 train_loss:3.8239 train_time:315077ms step_avg:679.04ms
step:475/1750 train_loss:3.8894 train_time:315771ms step_avg:679.08ms
step:476/1750 train_loss:4.0676 train_time:316468ms step_avg:679.12ms
step:477/1750 train_loss:3.8426 train_time:317162ms step_avg:679.15ms
step:478/1750 train_loss:3.8137 train_time:317858ms step_avg:679.18ms
step:479/1750 train_loss:3.7614 train_time:318553ms step_avg:679.22ms
step:480/1750 train_loss:3.7085 train_time:319251ms step_avg:679.26ms
step:481/1750 train_loss:3.7454 train_time:319946ms step_avg:679.29ms
step:482/1750 train_loss:3.8628 train_time:320641ms step_avg:679.32ms
step:483/1750 train_loss:3.7797 train_time:321337ms step_avg:679.36ms
step:484/1750 train_loss:3.8490 train_time:322034ms step_avg:679.40ms
step:485/1750 train_loss:3.7867 train_time:322729ms step_avg:679.43ms
step:486/1750 train_loss:3.6204 train_time:323423ms step_avg:679.46ms
step:487/1750 train_loss:3.7558 train_time:324118ms step_avg:679.49ms
step:488/1750 train_loss:3.7686 train_time:324815ms step_avg:679.53ms
step:489/1750 train_loss:3.7688 train_time:325506ms step_avg:679.55ms
step:490/1750 train_loss:3.9768 train_time:326201ms step_avg:679.59ms
step:491/1750 train_loss:3.7164 train_time:326896ms step_avg:679.62ms
step:492/1750 train_loss:3.6717 train_time:327590ms step_avg:679.65ms
step:493/1750 train_loss:3.8036 train_time:328283ms step_avg:679.67ms
step:494/1750 train_loss:3.5725 train_time:328980ms step_avg:679.71ms
step:495/1750 train_loss:3.7368 train_time:329678ms step_avg:679.75ms
step:496/1750 train_loss:3.9221 train_time:330375ms step_avg:679.78ms
step:497/1750 train_loss:3.7667 train_time:331071ms step_avg:679.82ms
step:498/1750 train_loss:3.7455 train_time:331768ms step_avg:679.85ms
step:499/1750 train_loss:3.7194 train_time:332462ms step_avg:679.88ms
step:500/1750 train_loss:3.8298 train_time:333157ms step_avg:679.91ms
step:500/1750 val_loss:3.7360 train_time:333168ms step_avg:679.93ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 47276.83203 | spectral_norm = 9104.77051 | nuclear_norm = 1170945.62500
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 102.94405 | spectral_norm = 13.59384 | nuclear_norm = 2393.28955
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 99.35332 | spectral_norm = 16.26548 | nuclear_norm = 2288.85449
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 130.76082 | spectral_norm = 8.47891 | nuclear_norm = 3088.31836
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 106.13601 | spectral_norm = 10.04615 | nuclear_norm = 2488.93652
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 209.70651 | spectral_norm = 16.45832 | nuclear_norm = 5575.98779
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 110.85636 | spectral_norm = 9.50069 | nuclear_norm = 2944.39966
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 100.11620 | spectral_norm = 9.78061 | nuclear_norm = 2328.07593
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 98.40694 | spectral_norm = 11.17752 | nuclear_norm = 2287.89990
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 106.18714 | spectral_norm = 9.01555 | nuclear_norm = 2439.85547
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 106.83569 | spectral_norm = 8.72622 | nuclear_norm = 2485.60132
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 214.77492 | spectral_norm = 14.10090 | nuclear_norm = 5732.93848
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 111.43418 | spectral_norm = 9.92702 | nuclear_norm = 2951.64941
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 101.09368 | spectral_norm = 10.28304 | nuclear_norm = 2339.30225
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 99.47218 | spectral_norm = 9.92227 | nuclear_norm = 2297.64795
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 117.33292 | spectral_norm = 9.80563 | nuclear_norm = 2670.62793
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 117.93179 | spectral_norm = 9.32728 | nuclear_norm = 2753.78223
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 217.82924 | spectral_norm = 16.04615 | nuclear_norm = 5815.46924
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 109.21902 | spectral_norm = 9.92540 | nuclear_norm = 2880.15771
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 100.34108 | spectral_norm = 9.81438 | nuclear_norm = 2317.67090
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 99.63440 | spectral_norm = 9.92428 | nuclear_norm = 2308.02710
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 111.69072 | spectral_norm = 9.71924 | nuclear_norm = 2465.26416
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 113.00436 | spectral_norm = 9.43660 | nuclear_norm = 2567.61646
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 222.36731 | spectral_norm = 17.01991 | nuclear_norm = 5929.49219
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 110.18138 | spectral_norm = 10.23199 | nuclear_norm = 2900.78198
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 99.09129 | spectral_norm = 9.48490 | nuclear_norm = 2275.36621
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 99.22327 | spectral_norm = 8.83091 | nuclear_norm = 2279.36328
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 113.98993 | spectral_norm = 9.86190 | nuclear_norm = 2530.30518
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 117.07713 | spectral_norm = 9.29099 | nuclear_norm = 2673.63452
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 223.41014 | spectral_norm = 17.54525 | nuclear_norm = 5948.65625
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 110.30103 | spectral_norm = 10.15682 | nuclear_norm = 2906.74951
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 99.11002 | spectral_norm = 9.08454 | nuclear_norm = 2248.23071
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 99.16389 | spectral_norm = 8.96255 | nuclear_norm = 2269.00244
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 113.83138 | spectral_norm = 10.16691 | nuclear_norm = 2545.17432
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 116.98479 | spectral_norm = 8.92186 | nuclear_norm = 2721.42676
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 233.62711 | spectral_norm = 16.80403 | nuclear_norm = 6227.00000
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 110.76178 | spectral_norm = 10.55651 | nuclear_norm = 2928.23071
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 98.39578 | spectral_norm = 9.34721 | nuclear_norm = 2217.68555
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 98.55493 | spectral_norm = 8.83157 | nuclear_norm = 2219.31226
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 120.43893 | spectral_norm = 12.39878 | nuclear_norm = 2625.41455
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 127.41151 | spectral_norm = 10.35057 | nuclear_norm = 2991.23901
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 227.97995 | spectral_norm = 17.30444 | nuclear_norm = 6083.03955
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 112.05486 | spectral_norm = 10.49449 | nuclear_norm = 2967.83008
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 101.91045 | spectral_norm = 10.08848 | nuclear_norm = 2307.68848
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 101.42921 | spectral_norm = 9.47397 | nuclear_norm = 2287.96753
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 104.33315 | spectral_norm = 9.75218 | nuclear_norm = 2344.90210
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 118.49962 | spectral_norm = 8.22705 | nuclear_norm = 2800.52710
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 228.79897 | spectral_norm = 18.05978 | nuclear_norm = 6098.07031
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 115.68715 | spectral_norm = 10.59962 | nuclear_norm = 3067.98291
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 103.54723 | spectral_norm = 10.04037 | nuclear_norm = 2375.45947
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 101.06115 | spectral_norm = 10.44012 | nuclear_norm = 2318.33887
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 125.63667 | spectral_norm = 12.47828 | nuclear_norm = 2817.75513
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 127.22250 | spectral_norm = 9.13023 | nuclear_norm = 2997.49854
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 226.73033 | spectral_norm = 17.79669 | nuclear_norm = 6041.75928
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 115.83289 | spectral_norm = 10.62584 | nuclear_norm = 3072.33716
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 100.71397 | spectral_norm = 10.32505 | nuclear_norm = 2299.02686
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 100.35761 | spectral_norm = 9.77805 | nuclear_norm = 2294.58887
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 124.05520 | spectral_norm = 12.23668 | nuclear_norm = 2766.33374
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 130.70184 | spectral_norm = 10.76708 | nuclear_norm = 3076.95703
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 228.06505 | spectral_norm = 17.70815 | nuclear_norm = 6079.32422
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 114.72214 | spectral_norm = 10.80782 | nuclear_norm = 3041.60059
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 103.13928 | spectral_norm = 10.54063 | nuclear_norm = 2349.03906
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 101.28040 | spectral_norm = 10.18248 | nuclear_norm = 2286.35352
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 103.24456 | spectral_norm = 8.87600 | nuclear_norm = 2380.54395
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 123.35177 | spectral_norm = 8.49094 | nuclear_norm = 2904.00049
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 229.73328 | spectral_norm = 16.25308 | nuclear_norm = 6131.77002
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 111.45232 | spectral_norm = 11.57323 | nuclear_norm = 2943.51758
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 102.80120 | spectral_norm = 11.52727 | nuclear_norm = 2348.46289
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 102.58958 | spectral_norm = 10.34931 | nuclear_norm = 2348.10449
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 119.09637 | spectral_norm = 11.98126 | nuclear_norm = 2639.41943
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 127.32681 | spectral_norm = 9.73634 | nuclear_norm = 2942.06152
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 234.07658 | spectral_norm = 17.97925 | nuclear_norm = 6231.43848
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 113.49664 | spectral_norm = 11.77287 | nuclear_norm = 2979.77954
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 803.72815 | spectral_norm = 164.42535 | nuclear_norm = 20586.13281
===========================================
step:501/1750 train_loss:3.7264 train_time:333846ms step_avg:679.93ms
step:502/1750 train_loss:3.6706 train_time:334538ms step_avg:679.96ms
step:503/1750 train_loss:3.7706 train_time:335229ms step_avg:679.98ms
step:504/1750 train_loss:3.6427 train_time:335923ms step_avg:680.01ms
step:505/1750 train_loss:4.0522 train_time:336615ms step_avg:680.03ms
step:506/1750 train_loss:3.7024 train_time:337311ms step_avg:680.06ms
step:507/1750 train_loss:3.7962 train_time:338005ms step_avg:680.09ms
step:508/1750 train_loss:3.9737 train_time:338697ms step_avg:680.12ms
step:509/1750 train_loss:3.6738 train_time:339392ms step_avg:680.15ms
step:510/1750 train_loss:3.7615 train_time:340085ms step_avg:680.17ms
step:511/1750 train_loss:3.7689 train_time:340782ms step_avg:680.20ms
step:512/1750 train_loss:3.5969 train_time:341477ms step_avg:680.23ms
step:513/1750 train_loss:3.5858 train_time:342173ms step_avg:680.26ms
step:514/1750 train_loss:3.8146 train_time:342867ms step_avg:680.29ms
step:515/1750 train_loss:3.9362 train_time:343563ms step_avg:680.32ms
step:516/1750 train_loss:3.8088 train_time:344258ms step_avg:680.35ms
step:517/1750 train_loss:3.6472 train_time:344954ms step_avg:680.38ms
step:518/1750 train_loss:3.8123 train_time:345648ms step_avg:680.41ms
step:519/1750 train_loss:3.5596 train_time:346341ms step_avg:680.43ms
step:520/1750 train_loss:3.8080 train_time:347052ms step_avg:680.49ms
step:521/1750 train_loss:3.6782 train_time:347763ms step_avg:680.55ms
step:522/1750 train_loss:3.5817 train_time:348471ms step_avg:680.61ms
step:523/1750 train_loss:3.8323 train_time:349180ms step_avg:680.66ms
step:524/1750 train_loss:3.6307 train_time:349891ms step_avg:680.72ms
step:525/1750 train_loss:3.6922 train_time:350597ms step_avg:680.77ms
step:526/1750 train_loss:3.7306 train_time:351312ms step_avg:680.84ms
step:527/1750 train_loss:3.9890 train_time:352018ms step_avg:680.89ms
step:528/1750 train_loss:3.7099 train_time:352725ms step_avg:680.94ms
step:529/1750 train_loss:3.6900 train_time:353433ms step_avg:680.99ms
step:530/1750 train_loss:3.6930 train_time:354141ms step_avg:681.04ms
step:531/1750 train_loss:3.7996 train_time:354850ms step_avg:681.09ms
step:532/1750 train_loss:3.7682 train_time:355568ms step_avg:681.16ms
step:533/1750 train_loss:3.7663 train_time:356275ms step_avg:681.21ms
step:534/1750 train_loss:3.8432 train_time:356981ms step_avg:681.26ms
step:535/1750 train_loss:3.7837 train_time:357699ms step_avg:681.33ms
step:536/1750 train_loss:3.6533 train_time:358408ms step_avg:681.38ms
step:537/1750 train_loss:3.7184 train_time:359116ms step_avg:681.43ms
step:538/1750 train_loss:3.6594 train_time:359826ms step_avg:681.49ms
step:539/1750 train_loss:3.6532 train_time:360539ms step_avg:681.55ms
step:540/1750 train_loss:3.7252 train_time:361256ms step_avg:681.61ms
step:541/1750 train_loss:3.6503 train_time:361963ms step_avg:681.66ms
step:542/1750 train_loss:3.6809 train_time:362674ms step_avg:681.72ms
step:543/1750 train_loss:3.7440 train_time:363384ms step_avg:681.77ms
step:544/1750 train_loss:3.7237 train_time:364093ms step_avg:681.82ms
step:545/1750 train_loss:3.7692 train_time:364802ms step_avg:681.87ms
step:546/1750 train_loss:3.7854 train_time:365507ms step_avg:681.92ms
step:547/1750 train_loss:3.6334 train_time:366219ms step_avg:681.97ms
step:548/1750 train_loss:3.8784 train_time:366928ms step_avg:682.02ms
step:549/1750 train_loss:3.2985 train_time:367639ms step_avg:682.08ms
step:550/1750 train_loss:3.7693 train_time:368355ms step_avg:682.14ms
step:551/1750 train_loss:3.7650 train_time:369061ms step_avg:682.18ms
step:552/1750 train_loss:3.6960 train_time:369768ms step_avg:682.23ms
step:553/1750 train_loss:3.7829 train_time:370482ms step_avg:682.29ms
step:554/1750 train_loss:3.7038 train_time:371191ms step_avg:682.34ms
step:555/1750 train_loss:3.7018 train_time:371902ms step_avg:682.39ms
step:556/1750 train_loss:3.8120 train_time:372609ms step_avg:682.43ms
step:557/1750 train_loss:3.7235 train_time:373319ms step_avg:682.48ms
step:558/1750 train_loss:3.6484 train_time:374028ms step_avg:682.53ms
step:559/1750 train_loss:3.7427 train_time:374734ms step_avg:682.58ms
step:560/1750 train_loss:3.6429 train_time:375440ms step_avg:682.62ms
step:561/1750 train_loss:3.6924 train_time:376149ms step_avg:682.67ms
step:562/1750 train_loss:3.6958 train_time:376856ms step_avg:682.71ms
step:563/1750 train_loss:3.5059 train_time:377564ms step_avg:682.76ms
step:564/1750 train_loss:3.7646 train_time:378269ms step_avg:682.80ms
step:565/1750 train_loss:3.6348 train_time:378978ms step_avg:682.84ms
step:566/1750 train_loss:3.6772 train_time:379690ms step_avg:682.90ms
step:567/1750 train_loss:3.7466 train_time:380400ms step_avg:682.94ms
step:568/1750 train_loss:3.6966 train_time:381109ms step_avg:682.99ms
step:569/1750 train_loss:4.0194 train_time:381816ms step_avg:683.03ms
step:570/1750 train_loss:3.7243 train_time:382735ms step_avg:683.46ms
step:571/1750 train_loss:3.6867 train_time:383457ms step_avg:683.52ms
step:572/1750 train_loss:3.7879 train_time:384164ms step_avg:683.57ms
step:573/1750 train_loss:3.7609 train_time:384874ms step_avg:683.61ms
step:574/1750 train_loss:3.7602 train_time:385584ms step_avg:683.66ms
step:575/1750 train_loss:3.8138 train_time:386305ms step_avg:683.73ms
step:576/1750 train_loss:3.7666 train_time:387015ms step_avg:683.77ms
step:577/1750 train_loss:3.7925 train_time:387725ms step_avg:683.82ms
step:578/1750 train_loss:3.7032 train_time:388429ms step_avg:683.85ms
step:579/1750 train_loss:3.7030 train_time:389137ms step_avg:683.90ms
step:580/1750 train_loss:3.6992 train_time:389847ms step_avg:683.94ms
step:581/1750 train_loss:3.6280 train_time:390556ms step_avg:683.99ms
step:582/1750 train_loss:3.6679 train_time:391265ms step_avg:684.03ms
step:583/1750 train_loss:3.8833 train_time:391975ms step_avg:684.08ms
step:584/1750 train_loss:3.6586 train_time:392682ms step_avg:684.11ms
step:585/1750 train_loss:3.6183 train_time:393387ms step_avg:684.15ms
step:586/1750 train_loss:3.8169 train_time:394091ms step_avg:684.19ms
step:587/1750 train_loss:3.5402 train_time:394798ms step_avg:684.23ms
step:588/1750 train_loss:3.6970 train_time:395502ms step_avg:684.26ms
step:589/1750 train_loss:3.6793 train_time:396206ms step_avg:684.29ms
step:590/1750 train_loss:4.0241 train_time:396914ms step_avg:684.33ms
step:591/1750 train_loss:3.8053 train_time:397617ms step_avg:684.37ms
step:592/1750 train_loss:3.5343 train_time:398319ms step_avg:684.40ms
step:593/1750 train_loss:3.5606 train_time:399025ms step_avg:684.43ms
step:594/1750 train_loss:3.5271 train_time:399733ms step_avg:684.47ms
step:595/1750 train_loss:3.5855 train_time:400441ms step_avg:684.51ms
step:596/1750 train_loss:3.9496 train_time:401151ms step_avg:684.56ms
step:597/1750 train_loss:3.6755 train_time:401861ms step_avg:684.60ms
step:598/1750 train_loss:3.6179 train_time:402563ms step_avg:684.63ms
step:599/1750 train_loss:3.6876 train_time:403266ms step_avg:684.66ms
step:600/1750 train_loss:3.5017 train_time:403974ms step_avg:684.70ms
step:601/1750 train_loss:3.6249 train_time:404688ms step_avg:684.75ms
step:602/1750 train_loss:3.6669 train_time:405393ms step_avg:684.78ms
step:603/1750 train_loss:3.6933 train_time:406098ms step_avg:684.82ms
step:604/1750 train_loss:3.8083 train_time:406807ms step_avg:684.86ms
step:605/1750 train_loss:3.6430 train_time:407511ms step_avg:684.89ms
step:606/1750 train_loss:3.6409 train_time:408220ms step_avg:684.93ms
step:607/1750 train_loss:3.6045 train_time:408936ms step_avg:684.98ms
step:608/1750 train_loss:3.8634 train_time:409637ms step_avg:685.01ms
step:609/1750 train_loss:3.6735 train_time:410343ms step_avg:685.05ms
step:610/1750 train_loss:3.6413 train_time:411045ms step_avg:685.07ms
step:611/1750 train_loss:3.7403 train_time:411751ms step_avg:685.11ms
step:612/1750 train_loss:3.6256 train_time:412460ms step_avg:685.15ms
step:613/1750 train_loss:3.6072 train_time:413164ms step_avg:685.18ms
step:614/1750 train_loss:3.7909 train_time:413872ms step_avg:685.22ms
step:615/1750 train_loss:3.7344 train_time:414576ms step_avg:685.25ms
step:616/1750 train_loss:3.7169 train_time:415280ms step_avg:685.28ms
step:617/1750 train_loss:3.6596 train_time:415983ms step_avg:685.31ms
step:618/1750 train_loss:3.5944 train_time:416689ms step_avg:685.34ms
step:619/1750 train_loss:3.7164 train_time:417393ms step_avg:685.37ms
step:620/1750 train_loss:3.5866 train_time:418101ms step_avg:685.41ms
step:621/1750 train_loss:3.6120 train_time:418810ms step_avg:685.45ms
step:622/1750 train_loss:3.9490 train_time:419517ms step_avg:685.49ms
step:623/1750 train_loss:3.5986 train_time:420225ms step_avg:685.52ms
step:624/1750 train_loss:3.6317 train_time:420928ms step_avg:685.55ms
step:625/1750 train_loss:3.7187 train_time:421634ms step_avg:685.58ms
step:625/1750 val_loss:3.6496 train_time:421645ms step_avg:685.60ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 51642.89844 | spectral_norm = 9678.00488 | nuclear_norm = 1284381.50000
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 120.59190 | spectral_norm = 17.16911 | nuclear_norm = 2794.49219
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 115.65932 | spectral_norm = 19.96251 | nuclear_norm = 2656.88623
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 151.94667 | spectral_norm = 9.81761 | nuclear_norm = 3601.44092
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 122.79762 | spectral_norm = 12.11461 | nuclear_norm = 2875.49854
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 247.08705 | spectral_norm = 19.26107 | nuclear_norm = 6571.25049
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 130.32535 | spectral_norm = 12.02799 | nuclear_norm = 3460.27295
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 117.61593 | spectral_norm = 11.46682 | nuclear_norm = 2729.23584
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 115.30379 | spectral_norm = 12.71535 | nuclear_norm = 2674.62109
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 123.65437 | spectral_norm = 10.45807 | nuclear_norm = 2846.45459
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 124.07327 | spectral_norm = 10.19659 | nuclear_norm = 2886.07080
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 250.74046 | spectral_norm = 16.79732 | nuclear_norm = 6695.04785
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 129.89668 | spectral_norm = 12.36580 | nuclear_norm = 3437.82324
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 118.48718 | spectral_norm = 12.18272 | nuclear_norm = 2731.11621
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 116.48186 | spectral_norm = 11.66440 | nuclear_norm = 2682.40918
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 138.13120 | spectral_norm = 11.61879 | nuclear_norm = 3148.82861
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 138.12219 | spectral_norm = 10.88905 | nuclear_norm = 3225.40186
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 253.47632 | spectral_norm = 19.77712 | nuclear_norm = 6768.33105
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 126.57161 | spectral_norm = 12.07340 | nuclear_norm = 3334.27637
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 117.43365 | spectral_norm = 11.77709 | nuclear_norm = 2702.11841
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 116.77849 | spectral_norm = 11.66587 | nuclear_norm = 2697.65625
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 131.33206 | spectral_norm = 11.39964 | nuclear_norm = 2901.44458
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 132.27188 | spectral_norm = 11.18187 | nuclear_norm = 3007.98828
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 258.39514 | spectral_norm = 20.97316 | nuclear_norm = 6886.87891
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 127.83070 | spectral_norm = 12.49369 | nuclear_norm = 3360.85620
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 115.84631 | spectral_norm = 11.47959 | nuclear_norm = 2646.12476
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 116.06181 | spectral_norm = 10.66007 | nuclear_norm = 2653.71045
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 134.08978 | spectral_norm = 11.67494 | nuclear_norm = 2975.89453
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 137.00876 | spectral_norm = 10.95605 | nuclear_norm = 3125.23926
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 259.35196 | spectral_norm = 21.63268 | nuclear_norm = 6898.37061
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 128.13646 | spectral_norm = 12.50163 | nuclear_norm = 3369.44092
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 115.46809 | spectral_norm = 10.93598 | nuclear_norm = 2600.84375
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 115.59746 | spectral_norm = 10.67415 | nuclear_norm = 2633.19873
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 134.11072 | spectral_norm = 12.05734 | nuclear_norm = 2999.53052
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 136.95406 | spectral_norm = 10.39709 | nuclear_norm = 3180.25659
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 273.50482 | spectral_norm = 21.00029 | nuclear_norm = 7280.53223
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 128.70920 | spectral_norm = 12.95989 | nuclear_norm = 3396.37451
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 114.50873 | spectral_norm = 11.34820 | nuclear_norm = 2565.25781
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 114.83310 | spectral_norm = 10.41699 | nuclear_norm = 2571.27734
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 141.88637 | spectral_norm = 14.41654 | nuclear_norm = 3088.88867
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 149.33774 | spectral_norm = 12.20535 | nuclear_norm = 3498.87549
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 267.97391 | spectral_norm = 21.01980 | nuclear_norm = 7145.95605
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 130.87132 | spectral_norm = 13.06763 | nuclear_norm = 3461.84302
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 119.01996 | spectral_norm = 12.44768 | nuclear_norm = 2673.12695
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 118.79693 | spectral_norm = 11.18551 | nuclear_norm = 2654.75781
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 123.69212 | spectral_norm = 12.17972 | nuclear_norm = 2768.51489
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 137.00052 | spectral_norm = 9.55915 | nuclear_norm = 3240.64258
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 270.61932 | spectral_norm = 22.30202 | nuclear_norm = 7206.45850
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 136.25809 | spectral_norm = 13.22382 | nuclear_norm = 3611.55908
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 121.36059 | spectral_norm = 12.04829 | nuclear_norm = 2768.27734
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 118.17033 | spectral_norm = 12.77859 | nuclear_norm = 2695.13672
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 149.24344 | spectral_norm = 15.27116 | nuclear_norm = 3336.15088
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 149.89029 | spectral_norm = 10.72846 | nuclear_norm = 3528.91748
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 265.69131 | spectral_norm = 21.56959 | nuclear_norm = 7076.19824
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 136.19821 | spectral_norm = 13.14834 | nuclear_norm = 3612.91479
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 117.86240 | spectral_norm = 12.47494 | nuclear_norm = 2672.34937
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 117.33259 | spectral_norm = 11.76840 | nuclear_norm = 2666.83545
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 147.77463 | spectral_norm = 14.53514 | nuclear_norm = 3289.11353
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 154.56543 | spectral_norm = 12.67648 | nuclear_norm = 3636.50415
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 265.70380 | spectral_norm = 21.29482 | nuclear_norm = 7078.81396
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 135.07831 | spectral_norm = 13.22337 | nuclear_norm = 3583.25195
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 120.64250 | spectral_norm = 12.97055 | nuclear_norm = 2727.20557
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 118.60477 | spectral_norm = 12.39359 | nuclear_norm = 2657.73242
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 121.05112 | spectral_norm = 10.55287 | nuclear_norm = 2788.43140
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 142.24806 | spectral_norm = 9.74102 | nuclear_norm = 3345.04980
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 272.44592 | spectral_norm = 19.66496 | nuclear_norm = 7272.87891
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 130.58655 | spectral_norm = 14.34958 | nuclear_norm = 3449.00684
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 120.03286 | spectral_norm = 13.88370 | nuclear_norm = 2722.95190
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 119.66727 | spectral_norm = 12.49110 | nuclear_norm = 2725.29834
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 142.49672 | spectral_norm = 14.77127 | nuclear_norm = 3147.32397
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 151.85568 | spectral_norm = 11.53755 | nuclear_norm = 3513.74146
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 276.35818 | spectral_norm = 22.15062 | nuclear_norm = 7353.34961
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 133.50165 | spectral_norm = 14.75810 | nuclear_norm = 3503.22437
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 908.28552 | spectral_norm = 167.42929 | nuclear_norm = 23447.22266
===========================================
step:626/1750 train_loss:3.7341 train_time:422331ms step_avg:685.60ms
step:627/1750 train_loss:3.7710 train_time:423034ms step_avg:685.63ms
step:628/1750 train_loss:3.7437 train_time:423738ms step_avg:685.66ms
step:629/1750 train_loss:3.8001 train_time:424439ms step_avg:685.68ms
step:630/1750 train_loss:3.6201 train_time:425143ms step_avg:685.72ms
step:631/1750 train_loss:3.7502 train_time:425843ms step_avg:685.74ms
step:632/1750 train_loss:3.7735 train_time:426548ms step_avg:685.77ms
step:633/1750 train_loss:3.6754 train_time:427250ms step_avg:685.79ms
step:634/1750 train_loss:3.6294 train_time:427955ms step_avg:685.83ms
step:635/1750 train_loss:3.7273 train_time:428661ms step_avg:685.86ms
step:636/1750 train_loss:3.9778 train_time:429364ms step_avg:685.89ms
step:637/1750 train_loss:3.5753 train_time:430068ms step_avg:685.91ms
step:638/1750 train_loss:3.3824 train_time:430770ms step_avg:685.94ms
step:639/1750 train_loss:3.6232 train_time:431474ms step_avg:685.97ms
step:640/1750 train_loss:3.6666 train_time:432179ms step_avg:686.00ms
step:641/1750 train_loss:3.6009 train_time:432884ms step_avg:686.03ms
step:642/1750 train_loss:3.6063 train_time:433588ms step_avg:686.06ms
step:643/1750 train_loss:3.6566 train_time:434294ms step_avg:686.09ms
step:644/1750 train_loss:3.6318 train_time:435000ms step_avg:686.12ms
step:645/1750 train_loss:3.5920 train_time:435704ms step_avg:686.15ms
step:646/1750 train_loss:3.8125 train_time:436408ms step_avg:686.18ms
step:647/1750 train_loss:3.7092 train_time:437115ms step_avg:686.21ms
step:648/1750 train_loss:3.6929 train_time:437821ms step_avg:686.24ms
step:649/1750 train_loss:3.7381 train_time:438532ms step_avg:686.28ms
step:650/1750 train_loss:3.7970 train_time:439248ms step_avg:686.33ms
step:651/1750 train_loss:3.6483 train_time:439975ms step_avg:686.39ms
step:652/1750 train_loss:3.7919 train_time:440698ms step_avg:686.45ms
step:653/1750 train_loss:3.6077 train_time:441414ms step_avg:686.49ms
step:654/1750 train_loss:3.6870 train_time:442126ms step_avg:686.53ms
step:655/1750 train_loss:3.4542 train_time:442845ms step_avg:686.58ms
step:656/1750 train_loss:3.6022 train_time:443558ms step_avg:686.62ms
step:657/1750 train_loss:3.6064 train_time:444278ms step_avg:686.67ms
step:658/1750 train_loss:3.5301 train_time:445004ms step_avg:686.73ms
step:659/1750 train_loss:3.7133 train_time:445734ms step_avg:686.80ms
step:660/1750 train_loss:3.6153 train_time:446457ms step_avg:686.86ms
step:661/1750 train_loss:3.6998 train_time:447173ms step_avg:686.90ms
step:662/1750 train_loss:3.7781 train_time:447893ms step_avg:686.95ms
step:663/1750 train_loss:3.6930 train_time:448608ms step_avg:686.99ms
step:664/1750 train_loss:3.5723 train_time:449322ms step_avg:687.04ms
step:665/1750 train_loss:3.6477 train_time:450042ms step_avg:687.09ms
step:666/1750 train_loss:3.5201 train_time:450761ms step_avg:687.13ms
step:667/1750 train_loss:3.8071 train_time:451476ms step_avg:687.18ms
step:668/1750 train_loss:3.6420 train_time:452206ms step_avg:687.24ms
step:669/1750 train_loss:3.6735 train_time:452924ms step_avg:687.29ms
step:670/1750 train_loss:3.5086 train_time:453649ms step_avg:687.35ms
step:671/1750 train_loss:3.6257 train_time:454370ms step_avg:687.40ms
step:672/1750 train_loss:3.5828 train_time:455089ms step_avg:687.45ms
step:673/1750 train_loss:3.6017 train_time:455808ms step_avg:687.49ms
step:674/1750 train_loss:3.8863 train_time:456527ms step_avg:687.54ms
step:675/1750 train_loss:3.6611 train_time:457248ms step_avg:687.59ms
step:676/1750 train_loss:3.7408 train_time:457969ms step_avg:687.64ms
step:677/1750 train_loss:3.5132 train_time:458686ms step_avg:687.69ms
step:678/1750 train_loss:3.6241 train_time:459400ms step_avg:687.72ms
step:679/1750 train_loss:3.5781 train_time:460117ms step_avg:687.77ms
step:680/1750 train_loss:3.7072 train_time:460839ms step_avg:687.82ms
step:681/1750 train_loss:3.6147 train_time:461572ms step_avg:687.89ms
step:682/1750 train_loss:3.6446 train_time:462287ms step_avg:687.93ms
step:683/1750 train_loss:3.6932 train_time:463007ms step_avg:687.98ms
step:684/1750 train_loss:3.7654 train_time:463727ms step_avg:688.02ms
step:685/1750 train_loss:3.6765 train_time:464462ms step_avg:688.09ms
step:686/1750 train_loss:3.7243 train_time:465181ms step_avg:688.14ms
step:687/1750 train_loss:3.6594 train_time:465906ms step_avg:688.19ms
step:688/1750 train_loss:3.6914 train_time:466626ms step_avg:688.24ms
step:689/1750 train_loss:3.2375 train_time:467363ms step_avg:688.31ms
step:690/1750 train_loss:3.4319 train_time:468079ms step_avg:688.35ms
step:691/1750 train_loss:3.5743 train_time:468799ms step_avg:688.40ms
step:692/1750 train_loss:3.4395 train_time:469520ms step_avg:688.45ms
step:693/1750 train_loss:3.6534 train_time:470235ms step_avg:688.49ms
step:694/1750 train_loss:3.6790 train_time:470953ms step_avg:688.53ms
step:695/1750 train_loss:3.5761 train_time:471675ms step_avg:688.58ms
step:696/1750 train_loss:3.5586 train_time:472389ms step_avg:688.61ms
step:697/1750 train_loss:3.8890 train_time:473114ms step_avg:688.67ms
step:698/1750 train_loss:3.6180 train_time:473831ms step_avg:688.71ms
step:699/1750 train_loss:3.6715 train_time:474547ms step_avg:688.75ms
step:700/1750 train_loss:3.8029 train_time:475271ms step_avg:688.80ms
step:701/1750 train_loss:3.5991 train_time:475983ms step_avg:688.83ms
step:702/1750 train_loss:3.5740 train_time:476697ms step_avg:688.87ms
step:703/1750 train_loss:3.5534 train_time:477415ms step_avg:688.91ms
step:704/1750 train_loss:3.5212 train_time:478135ms step_avg:688.96ms
step:705/1750 train_loss:3.6010 train_time:478867ms step_avg:689.02ms
step:706/1750 train_loss:3.5876 train_time:479587ms step_avg:689.06ms
step:707/1750 train_loss:3.6061 train_time:480313ms step_avg:689.11ms
step:708/1750 train_loss:3.6759 train_time:481033ms step_avg:689.16ms
step:709/1750 train_loss:3.6312 train_time:481761ms step_avg:689.21ms
step:710/1750 train_loss:3.6145 train_time:482478ms step_avg:689.25ms
step:711/1750 train_loss:3.5707 train_time:483200ms step_avg:689.30ms
step:712/1750 train_loss:3.6237 train_time:483929ms step_avg:689.36ms
step:713/1750 train_loss:3.6827 train_time:484647ms step_avg:689.40ms
step:714/1750 train_loss:3.6831 train_time:485367ms step_avg:689.44ms
step:715/1750 train_loss:3.5887 train_time:486082ms step_avg:689.48ms
step:716/1750 train_loss:3.6044 train_time:486795ms step_avg:689.51ms
step:717/1750 train_loss:3.6148 train_time:487508ms step_avg:689.54ms
step:718/1750 train_loss:3.7355 train_time:488220ms step_avg:689.58ms
step:719/1750 train_loss:3.6272 train_time:488935ms step_avg:689.61ms
step:720/1750 train_loss:3.7059 train_time:489646ms step_avg:689.64ms
step:721/1750 train_loss:3.8766 train_time:490369ms step_avg:689.69ms
step:722/1750 train_loss:3.4906 train_time:491087ms step_avg:689.73ms
step:723/1750 train_loss:3.7586 train_time:491804ms step_avg:689.77ms
step:724/1750 train_loss:3.8025 train_time:492514ms step_avg:689.80ms
step:725/1750 train_loss:3.5938 train_time:493235ms step_avg:689.84ms
step:726/1750 train_loss:3.6803 train_time:493968ms step_avg:689.90ms
step:727/1750 train_loss:3.5657 train_time:494685ms step_avg:689.94ms
step:728/1750 train_loss:3.6007 train_time:495399ms step_avg:689.97ms
step:729/1750 train_loss:3.7633 train_time:496110ms step_avg:690.00ms
step:730/1750 train_loss:3.6952 train_time:496829ms step_avg:690.04ms
step:731/1750 train_loss:3.7019 train_time:497553ms step_avg:690.09ms
step:732/1750 train_loss:3.5949 train_time:498265ms step_avg:690.12ms
step:733/1750 train_loss:3.6247 train_time:498973ms step_avg:690.14ms
step:734/1750 train_loss:3.8653 train_time:499685ms step_avg:690.17ms
step:735/1750 train_loss:3.5896 train_time:500398ms step_avg:690.20ms
step:736/1750 train_loss:3.6367 train_time:501117ms step_avg:690.24ms
step:737/1750 train_loss:3.7667 train_time:501831ms step_avg:690.28ms
step:738/1750 train_loss:3.7026 train_time:502545ms step_avg:690.31ms
step:739/1750 train_loss:3.6227 train_time:503256ms step_avg:690.34ms
step:740/1750 train_loss:3.5364 train_time:503967ms step_avg:690.37ms
step:741/1750 train_loss:4.1446 train_time:504693ms step_avg:690.41ms
step:742/1750 train_loss:3.5209 train_time:505405ms step_avg:690.44ms
step:743/1750 train_loss:3.5889 train_time:506127ms step_avg:690.49ms
step:744/1750 train_loss:3.6104 train_time:506857ms step_avg:690.54ms
step:745/1750 train_loss:3.6753 train_time:507581ms step_avg:690.59ms
step:746/1750 train_loss:3.6211 train_time:508293ms step_avg:690.61ms
step:747/1750 train_loss:3.6269 train_time:509006ms step_avg:690.65ms
step:748/1750 train_loss:3.6730 train_time:509720ms step_avg:690.68ms
step:749/1750 train_loss:3.5973 train_time:510441ms step_avg:690.72ms
step:750/1750 train_loss:3.5932 train_time:511164ms step_avg:690.76ms
step:750/1750 val_loss:3.5967 train_time:511176ms step_avg:690.78ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 55448.68359 | spectral_norm = 10199.35645 | nuclear_norm = 1383644.12500
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 135.81691 | spectral_norm = 20.41273 | nuclear_norm = 3139.53223
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 129.62358 | spectral_norm = 23.51321 | nuclear_norm = 2969.25269
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 170.34009 | spectral_norm = 10.97068 | nuclear_norm = 4050.92920
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 136.93747 | spectral_norm = 13.92851 | nuclear_norm = 3200.49609
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 280.17844 | spectral_norm = 21.74793 | nuclear_norm = 7456.32275
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 147.39493 | spectral_norm = 14.35966 | nuclear_norm = 3912.02393
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 132.49490 | spectral_norm = 13.24415 | nuclear_norm = 3067.15186
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 129.72063 | spectral_norm = 14.04808 | nuclear_norm = 3003.17041
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 138.93776 | spectral_norm = 11.70329 | nuclear_norm = 3202.25732
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 139.12006 | spectral_norm = 11.44807 | nuclear_norm = 3235.60474
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 282.82611 | spectral_norm = 18.99271 | nuclear_norm = 7555.45166
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 146.14920 | spectral_norm = 14.60427 | nuclear_norm = 3865.83838
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 133.67801 | spectral_norm = 13.95784 | nuclear_norm = 3073.28784
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 131.19333 | spectral_norm = 13.09323 | nuclear_norm = 3013.11987
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 156.31264 | spectral_norm = 13.09652 | nuclear_norm = 3567.12207
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 155.70399 | spectral_norm = 12.16208 | nuclear_norm = 3638.13867
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 285.20581 | spectral_norm = 23.27131 | nuclear_norm = 7617.89990
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 141.72577 | spectral_norm = 13.97665 | nuclear_norm = 3729.87695
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 132.06929 | spectral_norm = 13.71130 | nuclear_norm = 3026.59717
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 131.64740 | spectral_norm = 13.15456 | nuclear_norm = 3033.33252
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 148.95978 | spectral_norm = 12.80356 | nuclear_norm = 3294.75415
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 149.43369 | spectral_norm = 12.73323 | nuclear_norm = 3401.19409
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 290.08630 | spectral_norm = 24.67001 | nuclear_norm = 7729.40479
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 143.30498 | spectral_norm = 14.47119 | nuclear_norm = 3763.62134
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 129.92149 | spectral_norm = 13.36710 | nuclear_norm = 2953.84399
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 130.64569 | spectral_norm = 12.11537 | nuclear_norm = 2975.75464
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 152.17346 | spectral_norm = 13.31190 | nuclear_norm = 3376.16479
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 154.87849 | spectral_norm = 12.50051 | nuclear_norm = 3531.50049
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 290.88730 | spectral_norm = 25.64381 | nuclear_norm = 7727.45215
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 143.58447 | spectral_norm = 14.53012 | nuclear_norm = 3768.23242
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 129.73604 | spectral_norm = 12.60906 | nuclear_norm = 2907.45117
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 129.98769 | spectral_norm = 12.28017 | nuclear_norm = 2948.63184
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 152.36879 | spectral_norm = 13.70806 | nuclear_norm = 3408.47412
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 154.83725 | spectral_norm = 11.72941 | nuclear_norm = 3592.15527
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 308.16177 | spectral_norm = 25.15229 | nuclear_norm = 8193.02832
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 144.22284 | spectral_norm = 15.13053 | nuclear_norm = 3799.58789
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 128.46223 | spectral_norm = 13.08837 | nuclear_norm = 2864.70483
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 128.82933 | spectral_norm = 11.80582 | nuclear_norm = 2875.52295
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 160.71725 | spectral_norm = 16.31253 | nuclear_norm = 3496.50342
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 168.06302 | spectral_norm = 13.79409 | nuclear_norm = 3929.15552
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 303.20236 | spectral_norm = 24.56145 | nuclear_norm = 8080.39111
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 147.04199 | spectral_norm = 15.36051 | nuclear_norm = 3884.99170
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 133.74326 | spectral_norm = 14.46945 | nuclear_norm = 2984.10864
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 133.79062 | spectral_norm = 12.72385 | nuclear_norm = 2965.67407
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 141.03789 | spectral_norm = 14.40521 | nuclear_norm = 3145.76465
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 153.37343 | spectral_norm = 10.61089 | nuclear_norm = 3631.80884
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 308.78961 | spectral_norm = 26.29482 | nuclear_norm = 8215.96875
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 154.48946 | spectral_norm = 15.62643 | nuclear_norm = 4092.10327
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 136.65726 | spectral_norm = 13.75417 | nuclear_norm = 3100.58496
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 133.02133 | spectral_norm = 14.80453 | nuclear_norm = 3021.64307
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 170.37425 | spectral_norm = 17.90927 | nuclear_norm = 3794.39111
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 169.79071 | spectral_norm = 12.03487 | nuclear_norm = 3994.57715
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 299.40881 | spectral_norm = 25.20353 | nuclear_norm = 7970.73096
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 153.70393 | spectral_norm = 15.39818 | nuclear_norm = 4076.97217
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 132.74898 | spectral_norm = 14.28194 | nuclear_norm = 2994.25684
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 131.94470 | spectral_norm = 13.39321 | nuclear_norm = 2986.48096
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 168.68845 | spectral_norm = 16.47661 | nuclear_norm = 3747.28467
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 175.59821 | spectral_norm = 14.37824 | nuclear_norm = 4128.59375
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 298.49646 | spectral_norm = 24.55364 | nuclear_norm = 7949.71191
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 152.77992 | spectral_norm = 15.40428 | nuclear_norm = 4055.31348
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 135.53030 | spectral_norm = 15.13256 | nuclear_norm = 3044.66333
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 133.29457 | spectral_norm = 14.46500 | nuclear_norm = 2968.16870
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 136.45384 | spectral_norm = 12.30201 | nuclear_norm = 3137.88623
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 159.00557 | spectral_norm = 10.94889 | nuclear_norm = 3736.40820
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 310.29059 | spectral_norm = 22.70802 | nuclear_norm = 8282.63477
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 147.28934 | spectral_norm = 16.82695 | nuclear_norm = 3890.50049
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 134.72841 | spectral_norm = 15.91841 | nuclear_norm = 3039.68896
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 134.32042 | spectral_norm = 14.30785 | nuclear_norm = 3045.13232
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 163.34987 | spectral_norm = 17.34366 | nuclear_norm = 3597.18408
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 174.22852 | spectral_norm = 13.20219 | nuclear_norm = 4036.40234
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 313.34586 | spectral_norm = 25.87585 | nuclear_norm = 8334.37891
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 151.86249 | spectral_norm = 17.45455 | nuclear_norm = 3984.84521
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 1001.70477 | spectral_norm = 169.88976 | nuclear_norm = 26009.87500
===========================================
step:751/1750 train_loss:3.6285 train_time:511870ms step_avg:690.78ms
step:752/1750 train_loss:3.5940 train_time:512589ms step_avg:690.82ms
step:753/1750 train_loss:3.6445 train_time:513302ms step_avg:690.85ms
step:754/1750 train_loss:3.6517 train_time:514013ms step_avg:690.88ms
step:755/1750 train_loss:3.6193 train_time:514726ms step_avg:690.91ms
step:756/1750 train_loss:3.7078 train_time:515442ms step_avg:690.94ms
step:757/1750 train_loss:3.4919 train_time:516155ms step_avg:690.97ms
step:758/1750 train_loss:3.7481 train_time:516882ms step_avg:691.02ms
step:759/1750 train_loss:3.6863 train_time:517592ms step_avg:691.04ms
step:760/1750 train_loss:3.6230 train_time:518512ms step_avg:691.35ms
step:761/1750 train_loss:3.7312 train_time:519223ms step_avg:691.38ms
step:762/1750 train_loss:3.6390 train_time:520153ms step_avg:691.69ms
step:763/1750 train_loss:3.4831 train_time:520871ms step_avg:691.73ms
step:764/1750 train_loss:3.4611 train_time:521583ms step_avg:691.76ms
step:765/1750 train_loss:3.5712 train_time:522296ms step_avg:691.78ms
step:766/1750 train_loss:3.5787 train_time:523006ms step_avg:691.81ms
step:767/1750 train_loss:4.5953 train_time:523725ms step_avg:691.84ms
step:768/1750 train_loss:3.5748 train_time:524442ms step_avg:691.88ms
step:769/1750 train_loss:3.6237 train_time:525152ms step_avg:691.90ms
step:770/1750 train_loss:3.7002 train_time:525865ms step_avg:691.93ms
step:771/1750 train_loss:4.2097 train_time:526579ms step_avg:691.96ms
step:772/1750 train_loss:3.6277 train_time:527297ms step_avg:691.99ms
step:773/1750 train_loss:3.6287 train_time:528009ms step_avg:692.02ms
step:774/1750 train_loss:3.5886 train_time:528722ms step_avg:692.04ms
step:775/1750 train_loss:3.7241 train_time:529431ms step_avg:692.07ms
step:776/1750 train_loss:3.5262 train_time:530143ms step_avg:692.09ms
step:777/1750 train_loss:3.6487 train_time:530855ms step_avg:692.12ms
step:778/1750 train_loss:3.6490 train_time:531571ms step_avg:692.15ms
step:779/1750 train_loss:3.6115 train_time:532302ms step_avg:692.20ms
step:780/1750 train_loss:3.6110 train_time:533024ms step_avg:692.24ms
step:781/1750 train_loss:3.5038 train_time:533756ms step_avg:692.29ms
step:782/1750 train_loss:3.6553 train_time:534478ms step_avg:692.33ms
step:783/1750 train_loss:3.5999 train_time:535207ms step_avg:692.38ms
step:784/1750 train_loss:3.5621 train_time:535928ms step_avg:692.41ms
step:785/1750 train_loss:3.5652 train_time:536653ms step_avg:692.46ms
step:786/1750 train_loss:3.5962 train_time:537376ms step_avg:692.49ms
step:787/1750 train_loss:3.5459 train_time:538105ms step_avg:692.54ms
step:788/1750 train_loss:3.6182 train_time:538826ms step_avg:692.58ms
step:789/1750 train_loss:3.5786 train_time:539552ms step_avg:692.62ms
step:790/1750 train_loss:3.5059 train_time:540275ms step_avg:692.66ms
step:791/1750 train_loss:3.5616 train_time:541010ms step_avg:692.71ms
step:792/1750 train_loss:3.6262 train_time:541728ms step_avg:692.75ms
step:793/1750 train_loss:3.6330 train_time:542454ms step_avg:692.79ms
step:794/1750 train_loss:3.6608 train_time:543189ms step_avg:692.84ms
step:795/1750 train_loss:3.5986 train_time:543915ms step_avg:692.89ms
step:796/1750 train_loss:3.7137 train_time:544651ms step_avg:692.94ms
step:797/1750 train_loss:3.6094 train_time:545379ms step_avg:692.98ms
step:798/1750 train_loss:3.4201 train_time:546110ms step_avg:693.03ms
step:799/1750 train_loss:3.4939 train_time:546843ms step_avg:693.08ms
step:800/1750 train_loss:4.3420 train_time:547584ms step_avg:693.14ms
step:801/1750 train_loss:3.7239 train_time:548305ms step_avg:693.18ms
step:802/1750 train_loss:3.5775 train_time:549032ms step_avg:693.22ms
step:803/1750 train_loss:3.6301 train_time:549765ms step_avg:693.27ms
step:804/1750 train_loss:3.6078 train_time:550494ms step_avg:693.32ms
step:805/1750 train_loss:3.5570 train_time:551215ms step_avg:693.35ms
step:806/1750 train_loss:3.5586 train_time:551947ms step_avg:693.40ms
step:807/1750 train_loss:3.5862 train_time:552672ms step_avg:693.44ms
step:808/1750 train_loss:3.6552 train_time:553395ms step_avg:693.48ms
step:809/1750 train_loss:3.8666 train_time:554118ms step_avg:693.51ms
step:810/1750 train_loss:3.7115 train_time:554838ms step_avg:693.55ms
step:811/1750 train_loss:3.5132 train_time:555566ms step_avg:693.59ms
step:812/1750 train_loss:3.6355 train_time:556289ms step_avg:693.63ms
step:813/1750 train_loss:3.6490 train_time:557012ms step_avg:693.66ms
step:814/1750 train_loss:3.5868 train_time:557733ms step_avg:693.70ms
step:815/1750 train_loss:3.4465 train_time:558463ms step_avg:693.74ms
step:816/1750 train_loss:3.7920 train_time:559189ms step_avg:693.78ms
step:817/1750 train_loss:3.6029 train_time:559915ms step_avg:693.82ms
step:818/1750 train_loss:3.5669 train_time:560642ms step_avg:693.86ms
step:819/1750 train_loss:3.5781 train_time:561366ms step_avg:693.90ms
step:820/1750 train_loss:3.5570 train_time:562085ms step_avg:693.93ms
step:821/1750 train_loss:3.4482 train_time:562810ms step_avg:693.97ms
step:822/1750 train_loss:3.5777 train_time:563532ms step_avg:694.01ms
step:823/1750 train_loss:3.6761 train_time:564253ms step_avg:694.04ms
step:824/1750 train_loss:3.4076 train_time:564977ms step_avg:694.08ms
step:825/1750 train_loss:3.6131 train_time:565703ms step_avg:694.11ms
step:826/1750 train_loss:3.7078 train_time:566430ms step_avg:694.15ms
step:827/1750 train_loss:3.4661 train_time:567171ms step_avg:694.21ms
step:828/1750 train_loss:3.5302 train_time:567905ms step_avg:694.26ms
step:829/1750 train_loss:3.5416 train_time:568629ms step_avg:694.30ms
step:830/1750 train_loss:3.6417 train_time:569347ms step_avg:694.33ms
step:831/1750 train_loss:3.4755 train_time:570074ms step_avg:694.37ms
step:832/1750 train_loss:3.6022 train_time:570807ms step_avg:694.41ms
step:833/1750 train_loss:3.6369 train_time:571541ms step_avg:694.46ms
step:834/1750 train_loss:3.6434 train_time:572263ms step_avg:694.49ms
step:835/1750 train_loss:3.4845 train_time:573008ms step_avg:694.56ms
step:836/1750 train_loss:3.7117 train_time:573733ms step_avg:694.59ms
step:837/1750 train_loss:3.4919 train_time:574459ms step_avg:694.63ms
step:838/1750 train_loss:3.4098 train_time:575177ms step_avg:694.66ms
step:839/1750 train_loss:3.6457 train_time:575900ms step_avg:694.69ms
step:840/1750 train_loss:3.5674 train_time:576623ms step_avg:694.73ms
step:841/1750 train_loss:3.6547 train_time:577343ms step_avg:694.76ms
step:842/1750 train_loss:3.5387 train_time:578060ms step_avg:694.78ms
step:843/1750 train_loss:3.5868 train_time:578784ms step_avg:694.82ms
step:844/1750 train_loss:3.5524 train_time:579505ms step_avg:694.85ms
step:845/1750 train_loss:3.5659 train_time:580231ms step_avg:694.89ms
step:846/1750 train_loss:3.5833 train_time:580953ms step_avg:694.92ms
step:847/1750 train_loss:3.6225 train_time:581679ms step_avg:694.96ms
step:848/1750 train_loss:3.5746 train_time:582402ms step_avg:694.99ms
step:849/1750 train_loss:3.4067 train_time:583123ms step_avg:695.02ms
step:850/1750 train_loss:3.6178 train_time:583843ms step_avg:695.05ms
step:851/1750 train_loss:3.5053 train_time:584565ms step_avg:695.08ms
step:852/1750 train_loss:3.6184 train_time:585298ms step_avg:695.13ms
step:853/1750 train_loss:3.3851 train_time:586018ms step_avg:695.16ms
step:854/1750 train_loss:3.6844 train_time:586740ms step_avg:695.19ms
step:855/1750 train_loss:3.6137 train_time:587457ms step_avg:695.22ms
step:856/1750 train_loss:3.3963 train_time:588181ms step_avg:695.25ms
step:857/1750 train_loss:3.6908 train_time:588911ms step_avg:695.29ms
step:858/1750 train_loss:3.6899 train_time:589639ms step_avg:695.33ms
step:859/1750 train_loss:3.4051 train_time:590365ms step_avg:695.36ms
step:860/1750 train_loss:3.5831 train_time:591088ms step_avg:695.40ms
step:861/1750 train_loss:3.6455 train_time:591812ms step_avg:695.43ms
step:862/1750 train_loss:3.4496 train_time:592533ms step_avg:695.46ms
step:863/1750 train_loss:3.5292 train_time:593261ms step_avg:695.50ms
step:864/1750 train_loss:3.8173 train_time:593994ms step_avg:695.54ms
step:865/1750 train_loss:3.7710 train_time:594735ms step_avg:695.60ms
step:866/1750 train_loss:3.5919 train_time:595455ms step_avg:695.62ms
step:867/1750 train_loss:3.5399 train_time:596174ms step_avg:695.65ms
step:868/1750 train_loss:3.7349 train_time:596901ms step_avg:695.69ms
step:869/1750 train_loss:3.4718 train_time:597627ms step_avg:695.72ms
step:870/1750 train_loss:3.4193 train_time:598350ms step_avg:695.76ms
step:871/1750 train_loss:3.6002 train_time:599068ms step_avg:695.78ms
step:872/1750 train_loss:3.5492 train_time:599788ms step_avg:695.81ms
step:873/1750 train_loss:3.4934 train_time:600512ms step_avg:695.84ms
step:874/1750 train_loss:3.6367 train_time:601235ms step_avg:695.87ms
step:875/1750 train_loss:3.5460 train_time:601956ms step_avg:695.90ms
step:875/1750 val_loss:3.5505 train_time:601967ms step_avg:695.92ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 58834.68359 | spectral_norm = 10686.67188 | nuclear_norm = 1472179.00000
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 149.59160 | spectral_norm = 23.36004 | nuclear_norm = 3451.43359
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 142.35040 | spectral_norm = 27.06227 | nuclear_norm = 3253.21289
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 186.98160 | spectral_norm = 11.93954 | nuclear_norm = 4457.85303
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 149.67816 | spectral_norm = 15.59956 | nuclear_norm = 3495.87305
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 311.34659 | spectral_norm = 24.19224 | nuclear_norm = 8293.08008
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 162.52533 | spectral_norm = 16.45300 | nuclear_norm = 4311.69482
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 146.22304 | spectral_norm = 14.70758 | nuclear_norm = 3377.49951
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 142.87325 | spectral_norm = 15.35515 | nuclear_norm = 3302.07617
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 152.75578 | spectral_norm = 12.81587 | nuclear_norm = 3521.49805
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 152.70012 | spectral_norm = 12.53136 | nuclear_norm = 3551.53223
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 313.86334 | spectral_norm = 21.18898 | nuclear_norm = 8392.28418
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 160.86931 | spectral_norm = 16.73053 | nuclear_norm = 4253.28760
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 147.72285 | spectral_norm = 15.46268 | nuclear_norm = 3389.91772
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 144.80756 | spectral_norm = 14.51070 | nuclear_norm = 3319.54736
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 172.90594 | spectral_norm = 14.53363 | nuclear_norm = 3948.31885
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 171.53174 | spectral_norm = 13.44457 | nuclear_norm = 4007.92041
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 315.08075 | spectral_norm = 26.55828 | nuclear_norm = 8419.78906
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 155.58527 | spectral_norm = 15.62842 | nuclear_norm = 4091.83813
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 145.50484 | spectral_norm = 15.66087 | nuclear_norm = 3322.61328
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 145.30104 | spectral_norm = 14.54723 | nuclear_norm = 3340.74805
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 165.32372 | spectral_norm = 14.27624 | nuclear_norm = 3659.15063
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 165.22441 | spectral_norm = 14.32026 | nuclear_norm = 3764.45752
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 319.87231 | spectral_norm = 28.06223 | nuclear_norm = 8522.20898
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 157.49715 | spectral_norm = 16.31947 | nuclear_norm = 4132.62744
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 143.10359 | spectral_norm = 15.01959 | nuclear_norm = 3242.90161
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 144.15491 | spectral_norm = 13.52867 | nuclear_norm = 3272.75000
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 168.94258 | spectral_norm = 14.77834 | nuclear_norm = 3747.10229
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 171.42224 | spectral_norm = 13.95164 | nuclear_norm = 3906.94849
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 320.42749 | spectral_norm = 29.50107 | nuclear_norm = 8504.83203
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 157.54846 | spectral_norm = 16.32231 | nuclear_norm = 4128.71338
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 142.54105 | spectral_norm = 14.22818 | nuclear_norm = 3176.09766
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 143.06654 | spectral_norm = 13.66794 | nuclear_norm = 3234.70923
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 169.62746 | spectral_norm = 15.34394 | nuclear_norm = 3795.17310
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 171.72662 | spectral_norm = 13.05451 | nuclear_norm = 3980.26489
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 339.90106 | spectral_norm = 29.01546 | nuclear_norm = 9029.15430
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 158.25179 | spectral_norm = 17.02344 | nuclear_norm = 4164.14404
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 141.17932 | spectral_norm = 14.60049 | nuclear_norm = 3134.90991
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 141.53857 | spectral_norm = 13.04556 | nuclear_norm = 3149.86572
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 178.11746 | spectral_norm = 17.93593 | nuclear_norm = 3876.26465
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 185.15150 | spectral_norm = 15.37827 | nuclear_norm = 4321.02246
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 335.72208 | spectral_norm = 27.59992 | nuclear_norm = 8943.10156
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 161.52522 | spectral_norm = 17.40463 | nuclear_norm = 4263.06152
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 147.02728 | spectral_norm = 16.26625 | nuclear_norm = 3257.91602
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 147.46019 | spectral_norm = 14.10858 | nuclear_norm = 3245.97876
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 157.19238 | spectral_norm = 16.72294 | nuclear_norm = 3496.13281
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 168.44923 | spectral_norm = 11.76650 | nuclear_norm = 3988.55225
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 344.31613 | spectral_norm = 30.08897 | nuclear_norm = 9154.42773
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 170.95532 | spectral_norm = 17.84515 | nuclear_norm = 4525.77539
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 150.65321 | spectral_norm = 15.27494 | nuclear_norm = 3405.24365
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 146.52853 | spectral_norm = 16.52510 | nuclear_norm = 3314.95264
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 189.59581 | spectral_norm = 20.25060 | nuclear_norm = 4209.52734
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 188.04883 | spectral_norm = 13.34479 | nuclear_norm = 4423.18359
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 330.19168 | spectral_norm = 28.54448 | nuclear_norm = 8785.79492
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 169.42885 | spectral_norm = 17.48456 | nuclear_norm = 4493.03809
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 146.17790 | spectral_norm = 16.02911 | nuclear_norm = 3283.14160
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 145.24707 | spectral_norm = 15.09689 | nuclear_norm = 3277.23999
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 187.95769 | spectral_norm = 18.31884 | nuclear_norm = 4168.80176
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 194.93611 | spectral_norm = 15.92995 | nuclear_norm = 4580.00586
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 328.55057 | spectral_norm = 27.51944 | nuclear_norm = 8745.53125
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 168.67822 | spectral_norm = 17.47234 | nuclear_norm = 4478.50488
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 149.04553 | spectral_norm = 17.34203 | nuclear_norm = 3329.77588
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 146.60464 | spectral_norm = 16.38717 | nuclear_norm = 3244.56396
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 150.21855 | spectral_norm = 14.06170 | nuclear_norm = 3450.12720
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 174.21524 | spectral_norm = 12.03176 | nuclear_norm = 4091.11670
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 345.39630 | spectral_norm = 25.59577 | nuclear_norm = 9220.23047
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 162.59904 | spectral_norm = 19.21363 | nuclear_norm = 4294.58643
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 148.03088 | spectral_norm = 17.72898 | nuclear_norm = 3322.45312
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 147.61253 | spectral_norm = 16.09058 | nuclear_norm = 3336.40112
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 182.92329 | spectral_norm = 19.79135 | nuclear_norm = 4018.06934
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 195.52654 | spectral_norm = 14.86972 | nuclear_norm = 4534.48047
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 347.02371 | spectral_norm = 29.24623 | nuclear_norm = 9225.34082
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 169.35474 | spectral_norm = 19.97347 | nuclear_norm = 4444.55371
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 1086.27295 | spectral_norm = 171.99574 | nuclear_norm = 28331.83398
===========================================
step:876/1750 train_loss:3.6554 train_time:602678ms step_avg:695.93ms
step:877/1750 train_loss:3.4419 train_time:603396ms step_avg:695.96ms
step:878/1750 train_loss:3.6586 train_time:604116ms step_avg:695.99ms
step:879/1750 train_loss:3.5225 train_time:604833ms step_avg:696.01ms
step:880/1750 train_loss:3.8825 train_time:605553ms step_avg:696.04ms
step:881/1750 train_loss:3.5950 train_time:606272ms step_avg:696.06ms
step:882/1750 train_loss:3.4104 train_time:606985ms step_avg:696.08ms
step:883/1750 train_loss:3.7334 train_time:607705ms step_avg:696.11ms
step:884/1750 train_loss:3.4404 train_time:608429ms step_avg:696.14ms
step:885/1750 train_loss:3.6975 train_time:609144ms step_avg:696.16ms
step:886/1750 train_loss:3.5603 train_time:609877ms step_avg:696.21ms
step:887/1750 train_loss:3.6345 train_time:610605ms step_avg:696.24ms
step:888/1750 train_loss:3.6058 train_time:611324ms step_avg:696.27ms
step:889/1750 train_loss:3.6364 train_time:612045ms step_avg:696.30ms
step:890/1750 train_loss:3.5972 train_time:612778ms step_avg:696.34ms
step:891/1750 train_loss:3.4408 train_time:613498ms step_avg:696.37ms
step:892/1750 train_loss:3.6174 train_time:614216ms step_avg:696.39ms
step:893/1750 train_loss:3.5307 train_time:614943ms step_avg:696.42ms
step:894/1750 train_loss:3.5868 train_time:615660ms step_avg:696.45ms
step:895/1750 train_loss:3.4172 train_time:616379ms step_avg:696.47ms
step:896/1750 train_loss:3.3506 train_time:617108ms step_avg:696.51ms
step:897/1750 train_loss:3.4914 train_time:617832ms step_avg:696.54ms
step:898/1750 train_loss:3.6812 train_time:618558ms step_avg:696.57ms
step:899/1750 train_loss:3.5405 train_time:619278ms step_avg:696.60ms
step:900/1750 train_loss:3.5969 train_time:620004ms step_avg:696.63ms
step:901/1750 train_loss:3.7581 train_time:620723ms step_avg:696.66ms
step:902/1750 train_loss:3.5378 train_time:621443ms step_avg:696.69ms
step:903/1750 train_loss:3.4721 train_time:622161ms step_avg:696.71ms
step:904/1750 train_loss:3.7013 train_time:622898ms step_avg:696.75ms
step:905/1750 train_loss:3.6498 train_time:623616ms step_avg:696.78ms
step:906/1750 train_loss:3.4979 train_time:624339ms step_avg:696.81ms
step:907/1750 train_loss:3.5010 train_time:625057ms step_avg:696.83ms
step:908/1750 train_loss:3.7903 train_time:625794ms step_avg:696.88ms
step:909/1750 train_loss:3.5063 train_time:626526ms step_avg:696.91ms
step:910/1750 train_loss:3.6931 train_time:627255ms step_avg:696.95ms
step:911/1750 train_loss:3.8886 train_time:628003ms step_avg:697.01ms
step:912/1750 train_loss:3.3540 train_time:628733ms step_avg:697.04ms
step:913/1750 train_loss:3.6657 train_time:629463ms step_avg:697.08ms
step:914/1750 train_loss:3.5307 train_time:630205ms step_avg:697.13ms
step:915/1750 train_loss:3.5915 train_time:630946ms step_avg:697.18ms
step:916/1750 train_loss:3.7398 train_time:631689ms step_avg:697.23ms
step:917/1750 train_loss:3.5182 train_time:632421ms step_avg:697.27ms
step:918/1750 train_loss:3.4934 train_time:633166ms step_avg:697.32ms
step:919/1750 train_loss:3.5804 train_time:633898ms step_avg:697.36ms
step:920/1750 train_loss:3.4753 train_time:634642ms step_avg:697.41ms
step:921/1750 train_loss:3.5276 train_time:635386ms step_avg:697.46ms
step:922/1750 train_loss:3.4810 train_time:636115ms step_avg:697.49ms
step:923/1750 train_loss:3.6482 train_time:636844ms step_avg:697.53ms
step:924/1750 train_loss:3.5232 train_time:637567ms step_avg:697.56ms
step:925/1750 train_loss:3.5059 train_time:638298ms step_avg:697.59ms
step:926/1750 train_loss:3.6178 train_time:639039ms step_avg:697.64ms
step:927/1750 train_loss:3.4944 train_time:639768ms step_avg:697.68ms
step:928/1750 train_loss:3.6835 train_time:640510ms step_avg:697.72ms
step:929/1750 train_loss:3.5546 train_time:641234ms step_avg:697.75ms
step:930/1750 train_loss:3.4014 train_time:641971ms step_avg:697.79ms
step:931/1750 train_loss:3.7110 train_time:642702ms step_avg:697.83ms
step:932/1750 train_loss:3.4167 train_time:643432ms step_avg:697.87ms
step:933/1750 train_loss:3.3883 train_time:644161ms step_avg:697.90ms
step:934/1750 train_loss:3.6093 train_time:644902ms step_avg:697.95ms
step:935/1750 train_loss:3.5656 train_time:645628ms step_avg:697.98ms
step:936/1750 train_loss:3.4114 train_time:646383ms step_avg:698.04ms
step:937/1750 train_loss:3.3687 train_time:647120ms step_avg:698.08ms
step:938/1750 train_loss:3.5323 train_time:647847ms step_avg:698.11ms
step:939/1750 train_loss:3.3367 train_time:648575ms step_avg:698.14ms
step:940/1750 train_loss:3.6107 train_time:649300ms step_avg:698.17ms
step:941/1750 train_loss:3.4495 train_time:650048ms step_avg:698.23ms
step:942/1750 train_loss:3.4507 train_time:650787ms step_avg:698.27ms
step:943/1750 train_loss:3.5805 train_time:651530ms step_avg:698.32ms
step:944/1750 train_loss:3.4758 train_time:652259ms step_avg:698.35ms
step:945/1750 train_loss:3.4797 train_time:652990ms step_avg:698.39ms
step:946/1750 train_loss:3.6440 train_time:653734ms step_avg:698.43ms
step:947/1750 train_loss:3.5515 train_time:654466ms step_avg:698.47ms
step:948/1750 train_loss:3.6168 train_time:655211ms step_avg:698.52ms
step:949/1750 train_loss:3.7833 train_time:655950ms step_avg:698.56ms
step:950/1750 train_loss:3.4136 train_time:656892ms step_avg:698.82ms
step:951/1750 train_loss:3.4801 train_time:657620ms step_avg:698.85ms
step:952/1750 train_loss:3.7338 train_time:658360ms step_avg:698.90ms
step:953/1750 train_loss:3.4432 train_time:659085ms step_avg:698.92ms
step:954/1750 train_loss:3.5059 train_time:659827ms step_avg:698.97ms
step:955/1750 train_loss:3.6018 train_time:660575ms step_avg:699.02ms
step:956/1750 train_loss:3.4776 train_time:661309ms step_avg:699.06ms
step:957/1750 train_loss:3.5106 train_time:662031ms step_avg:699.08ms
step:958/1750 train_loss:3.4754 train_time:662784ms step_avg:699.14ms
step:959/1750 train_loss:3.5342 train_time:663527ms step_avg:699.19ms
step:960/1750 train_loss:3.5390 train_time:664263ms step_avg:699.22ms
step:961/1750 train_loss:3.5448 train_time:664995ms step_avg:699.26ms
step:962/1750 train_loss:3.4355 train_time:665746ms step_avg:699.31ms
step:963/1750 train_loss:3.6814 train_time:666483ms step_avg:699.35ms
step:964/1750 train_loss:3.6361 train_time:667209ms step_avg:699.38ms
step:965/1750 train_loss:3.4259 train_time:667944ms step_avg:699.42ms
step:966/1750 train_loss:3.4627 train_time:668684ms step_avg:699.46ms
step:967/1750 train_loss:3.5066 train_time:669411ms step_avg:699.49ms
step:968/1750 train_loss:3.7430 train_time:670143ms step_avg:699.52ms
step:969/1750 train_loss:3.5565 train_time:670871ms step_avg:699.55ms
step:970/1750 train_loss:3.5516 train_time:671596ms step_avg:699.58ms
step:971/1750 train_loss:3.6165 train_time:672337ms step_avg:699.62ms
step:972/1750 train_loss:3.4075 train_time:673069ms step_avg:699.66ms
step:973/1750 train_loss:3.5677 train_time:673803ms step_avg:699.69ms
step:974/1750 train_loss:3.5148 train_time:674529ms step_avg:699.72ms
step:975/1750 train_loss:3.5709 train_time:675263ms step_avg:699.75ms
step:976/1750 train_loss:3.6271 train_time:675989ms step_avg:699.78ms
step:977/1750 train_loss:3.5134 train_time:676719ms step_avg:699.81ms
step:978/1750 train_loss:3.7050 train_time:677446ms step_avg:699.84ms
step:979/1750 train_loss:3.6073 train_time:678170ms step_avg:699.87ms
step:980/1750 train_loss:3.3968 train_time:678899ms step_avg:699.90ms
step:981/1750 train_loss:3.6633 train_time:679626ms step_avg:699.92ms
step:982/1750 train_loss:3.4516 train_time:680350ms step_avg:699.95ms
step:983/1750 train_loss:3.6122 train_time:681072ms step_avg:699.97ms
step:984/1750 train_loss:3.5798 train_time:681806ms step_avg:700.01ms
step:985/1750 train_loss:3.5511 train_time:682545ms step_avg:700.05ms
step:986/1750 train_loss:3.5371 train_time:683275ms step_avg:700.08ms
step:987/1750 train_loss:3.6160 train_time:684012ms step_avg:700.11ms
step:988/1750 train_loss:3.4538 train_time:684736ms step_avg:700.14ms
step:989/1750 train_loss:3.5280 train_time:685463ms step_avg:700.17ms
step:990/1750 train_loss:3.5351 train_time:686189ms step_avg:700.19ms
step:991/1750 train_loss:3.4529 train_time:686929ms step_avg:700.23ms
step:992/1750 train_loss:3.6963 train_time:687672ms step_avg:700.28ms
step:993/1750 train_loss:3.5110 train_time:688396ms step_avg:700.30ms
step:994/1750 train_loss:3.4818 train_time:689123ms step_avg:700.33ms
step:995/1750 train_loss:3.5478 train_time:689873ms step_avg:700.38ms
step:996/1750 train_loss:3.6351 train_time:690598ms step_avg:700.40ms
step:997/1750 train_loss:3.5724 train_time:691325ms step_avg:700.43ms
step:998/1750 train_loss:3.4981 train_time:692052ms step_avg:700.46ms
step:999/1750 train_loss:3.8128 train_time:692775ms step_avg:700.48ms
step:1000/1750 train_loss:3.4845 train_time:693502ms step_avg:700.51ms
step:1000/1750 val_loss:3.5104 train_time:693513ms step_avg:700.52ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 61885.18750 | spectral_norm = 11149.70215 | nuclear_norm = 1552132.87500
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 162.09650 | spectral_norm = 26.12424 | nuclear_norm = 3734.71558
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 154.29243 | spectral_norm = 30.44764 | nuclear_norm = 3519.71582
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 202.06573 | spectral_norm = 12.84875 | nuclear_norm = 4830.53906
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 161.12308 | spectral_norm = 17.20209 | nuclear_norm = 3758.64062
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 338.36996 | spectral_norm = 26.20720 | nuclear_norm = 9017.24219
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 176.60565 | spectral_norm = 18.55935 | nuclear_norm = 4683.92676
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 159.16292 | spectral_norm = 16.05901 | nuclear_norm = 3671.77100
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 155.18396 | spectral_norm = 16.46373 | nuclear_norm = 3581.84546
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 164.76746 | spectral_norm = 13.80483 | nuclear_norm = 3801.39136
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 164.36432 | spectral_norm = 13.46337 | nuclear_norm = 3823.98267
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 342.00177 | spectral_norm = 23.31554 | nuclear_norm = 9150.60547
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 174.54666 | spectral_norm = 18.76800 | nuclear_norm = 4613.28418
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 160.71477 | spectral_norm = 17.06402 | nuclear_norm = 3682.51416
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 157.30592 | spectral_norm = 15.86989 | nuclear_norm = 3600.64526
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 187.90768 | spectral_norm = 15.84895 | nuclear_norm = 4293.66748
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 185.64816 | spectral_norm = 14.54545 | nuclear_norm = 4339.44482
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 342.09897 | spectral_norm = 29.70120 | nuclear_norm = 9145.24805
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 168.31084 | spectral_norm = 17.17016 | nuclear_norm = 4423.47559
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 157.77559 | spectral_norm = 17.59870 | nuclear_norm = 3593.13916
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 157.79749 | spectral_norm = 15.88297 | nuclear_norm = 3623.38721
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 180.43170 | spectral_norm = 15.49103 | nuclear_norm = 3996.68018
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 179.88460 | spectral_norm = 15.70217 | nuclear_norm = 4103.39355
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 346.62680 | spectral_norm = 31.27156 | nuclear_norm = 9232.03418
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 170.60513 | spectral_norm = 18.01976 | nuclear_norm = 4472.88770
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 155.00879 | spectral_norm = 16.66818 | nuclear_norm = 3502.56396
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 156.46765 | spectral_norm = 14.82855 | nuclear_norm = 3542.52734
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 184.67349 | spectral_norm = 16.17011 | nuclear_norm = 4094.17896
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 186.87250 | spectral_norm = 15.26898 | nuclear_norm = 4260.55664
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 347.37299 | spectral_norm = 33.21728 | nuclear_norm = 9210.50000
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 170.52184 | spectral_norm = 18.03183 | nuclear_norm = 4462.95996
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 154.45802 | spectral_norm = 15.78516 | nuclear_norm = 3427.94141
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 155.17337 | spectral_norm = 14.99601 | nuclear_norm = 3500.83350
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 185.50337 | spectral_norm = 16.91353 | nuclear_norm = 4151.30469
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 187.36952 | spectral_norm = 14.33880 | nuclear_norm = 4342.79590
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 368.78552 | spectral_norm = 32.71078 | nuclear_norm = 9786.48633
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 171.08083 | spectral_norm = 18.81807 | nuclear_norm = 4496.95410
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 152.87912 | spectral_norm = 16.12288 | nuclear_norm = 3386.34424
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 153.05751 | spectral_norm = 14.18012 | nuclear_norm = 3399.55859
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 194.04260 | spectral_norm = 19.54524 | nuclear_norm = 4226.14160
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 200.65636 | spectral_norm = 16.82988 | nuclear_norm = 4677.24707
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 365.36813 | spectral_norm = 30.61396 | nuclear_norm = 9729.09375
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 175.01033 | spectral_norm = 19.32376 | nuclear_norm = 4615.49316
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 159.35393 | spectral_norm = 17.96096 | nuclear_norm = 3511.57129
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 159.78218 | spectral_norm = 15.45494 | nuclear_norm = 3494.49243
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 172.75383 | spectral_norm = 18.99803 | nuclear_norm = 3829.76318
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 182.60263 | spectral_norm = 12.76465 | nuclear_norm = 4326.42236
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 377.10062 | spectral_norm = 33.74393 | nuclear_norm = 10018.62207
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 186.08444 | spectral_norm = 19.93848 | nuclear_norm = 4924.04102
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 163.55077 | spectral_norm = 16.71883 | nuclear_norm = 3684.47241
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 158.92363 | spectral_norm = 18.08489 | nuclear_norm = 3586.08081
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 207.46402 | spectral_norm = 22.37000 | nuclear_norm = 4594.17773
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 204.72864 | spectral_norm = 14.48676 | nuclear_norm = 4812.72412
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 358.19943 | spectral_norm = 31.74693 | nuclear_norm = 9526.77734
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 183.68309 | spectral_norm = 19.36793 | nuclear_norm = 4870.94971
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 158.46095 | spectral_norm = 17.58747 | nuclear_norm = 3545.85840
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 157.41823 | spectral_norm = 16.68933 | nuclear_norm = 3540.35083
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 205.76097 | spectral_norm = 19.93648 | nuclear_norm = 4558.50879
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 212.88058 | spectral_norm = 17.29928 | nuclear_norm = 4997.96484
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 355.83743 | spectral_norm = 30.20517 | nuclear_norm = 9466.46875
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 183.15335 | spectral_norm = 19.30219 | nuclear_norm = 4863.63086
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 161.65669 | spectral_norm = 19.34919 | nuclear_norm = 3594.82471
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 158.75540 | spectral_norm = 18.18444 | nuclear_norm = 3497.57275
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 162.50406 | spectral_norm = 15.52862 | nuclear_norm = 3728.57007
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 188.11739 | spectral_norm = 12.95422 | nuclear_norm = 4415.18164
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 378.09653 | spectral_norm = 28.27559 | nuclear_norm = 10092.26367
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 176.76738 | spectral_norm = 21.39085 | nuclear_norm = 4668.94043
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 160.10016 | spectral_norm = 19.57606 | nuclear_norm = 3577.95386
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 159.55772 | spectral_norm = 17.72573 | nuclear_norm = 3594.10205
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 201.41505 | spectral_norm = 22.26287 | nuclear_norm = 4411.82031
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 215.97282 | spectral_norm = 16.34407 | nuclear_norm = 5012.57324
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 377.87112 | spectral_norm = 32.25031 | nuclear_norm = 10042.35840
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 186.06918 | spectral_norm = 22.48785 | nuclear_norm = 4884.88184
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 1163.44080 | spectral_norm = 174.06818 | nuclear_norm = 30454.29688
===========================================
step:1001/1750 train_loss:3.6287 train_time:694220ms step_avg:700.53ms
step:1002/1750 train_loss:3.4842 train_time:694942ms step_avg:700.55ms
step:1003/1750 train_loss:3.5443 train_time:695663ms step_avg:700.57ms
step:1004/1750 train_loss:3.4192 train_time:696399ms step_avg:700.60ms
step:1005/1750 train_loss:3.6029 train_time:697138ms step_avg:700.64ms
step:1006/1750 train_loss:3.6457 train_time:697878ms step_avg:700.68ms
step:1007/1750 train_loss:3.4323 train_time:698605ms step_avg:700.71ms
step:1008/1750 train_loss:3.5046 train_time:699332ms step_avg:700.73ms
step:1009/1750 train_loss:3.4816 train_time:700059ms step_avg:700.76ms
step:1010/1750 train_loss:3.6102 train_time:700795ms step_avg:700.80ms
step:1011/1750 train_loss:3.7082 train_time:701540ms step_avg:700.84ms
step:1012/1750 train_loss:3.5988 train_time:702267ms step_avg:700.87ms
step:1013/1750 train_loss:3.5777 train_time:702990ms step_avg:700.89ms
step:1014/1750 train_loss:3.4360 train_time:703728ms step_avg:700.92ms
step:1015/1750 train_loss:3.5798 train_time:704452ms step_avg:700.95ms
step:1016/1750 train_loss:3.6693 train_time:705177ms step_avg:700.97ms
step:1017/1750 train_loss:3.3785 train_time:705914ms step_avg:701.01ms
step:1018/1750 train_loss:3.4551 train_time:706658ms step_avg:701.05ms
step:1019/1750 train_loss:3.4492 train_time:707397ms step_avg:701.09ms
step:1020/1750 train_loss:3.4405 train_time:708121ms step_avg:701.11ms
step:1021/1750 train_loss:3.5656 train_time:708853ms step_avg:701.14ms
step:1022/1750 train_loss:3.4365 train_time:709586ms step_avg:701.17ms
step:1023/1750 train_loss:3.4014 train_time:710321ms step_avg:701.21ms
step:1024/1750 train_loss:3.5280 train_time:711055ms step_avg:701.24ms
step:1025/1750 train_loss:3.5502 train_time:711796ms step_avg:701.28ms
step:1026/1750 train_loss:3.5267 train_time:712535ms step_avg:701.31ms
step:1027/1750 train_loss:3.5263 train_time:713272ms step_avg:701.35ms
step:1028/1750 train_loss:3.6785 train_time:713994ms step_avg:701.37ms
step:1029/1750 train_loss:3.3656 train_time:714733ms step_avg:701.41ms
step:1030/1750 train_loss:3.4423 train_time:715489ms step_avg:701.46ms
step:1031/1750 train_loss:3.3722 train_time:716230ms step_avg:701.50ms
step:1032/1750 train_loss:3.5829 train_time:716951ms step_avg:701.52ms
step:1033/1750 train_loss:3.5637 train_time:717682ms step_avg:701.55ms
step:1034/1750 train_loss:3.7511 train_time:718415ms step_avg:701.58ms
step:1035/1750 train_loss:3.5444 train_time:719152ms step_avg:701.61ms
step:1036/1750 train_loss:3.4672 train_time:719891ms step_avg:701.65ms
step:1037/1750 train_loss:3.4981 train_time:720623ms step_avg:701.68ms
step:1038/1750 train_loss:3.5388 train_time:721354ms step_avg:701.71ms
step:1039/1750 train_loss:3.8449 train_time:722093ms step_avg:701.74ms
step:1040/1750 train_loss:3.6714 train_time:722832ms step_avg:701.78ms
step:1041/1750 train_loss:3.5621 train_time:723570ms step_avg:701.81ms
step:1042/1750 train_loss:3.4671 train_time:724310ms step_avg:701.85ms
step:1043/1750 train_loss:3.5364 train_time:725057ms step_avg:701.89ms
step:1044/1750 train_loss:3.5786 train_time:725805ms step_avg:701.94ms
step:1045/1750 train_loss:3.4923 train_time:726535ms step_avg:701.97ms
step:1046/1750 train_loss:3.5098 train_time:727269ms step_avg:702.00ms
step:1047/1750 train_loss:3.5708 train_time:728019ms step_avg:702.04ms
step:1048/1750 train_loss:3.4748 train_time:728764ms step_avg:702.08ms
step:1049/1750 train_loss:3.6860 train_time:729506ms step_avg:702.12ms
step:1050/1750 train_loss:3.5527 train_time:730254ms step_avg:702.17ms
step:1051/1750 train_loss:3.4547 train_time:730992ms step_avg:702.20ms
step:1052/1750 train_loss:3.4430 train_time:731734ms step_avg:702.24ms
step:1053/1750 train_loss:3.5521 train_time:732474ms step_avg:702.28ms
step:1054/1750 train_loss:3.4095 train_time:733213ms step_avg:702.31ms
step:1055/1750 train_loss:3.7551 train_time:733944ms step_avg:702.34ms
step:1056/1750 train_loss:3.5975 train_time:734687ms step_avg:702.38ms
step:1057/1750 train_loss:3.4320 train_time:735426ms step_avg:702.41ms
step:1058/1750 train_loss:3.5578 train_time:736166ms step_avg:702.45ms
step:1059/1750 train_loss:3.6362 train_time:736898ms step_avg:702.48ms
step:1060/1750 train_loss:3.3559 train_time:737651ms step_avg:702.53ms
step:1061/1750 train_loss:3.4209 train_time:738406ms step_avg:702.57ms
step:1062/1750 train_loss:3.4969 train_time:739142ms step_avg:702.61ms
step:1063/1750 train_loss:3.4705 train_time:739879ms step_avg:702.64ms
step:1064/1750 train_loss:3.4388 train_time:740614ms step_avg:702.67ms
step:1065/1750 train_loss:3.5225 train_time:741352ms step_avg:702.70ms
step:1066/1750 train_loss:3.4421 train_time:742084ms step_avg:702.73ms
step:1067/1750 train_loss:3.4195 train_time:742828ms step_avg:702.77ms
step:1068/1750 train_loss:3.4679 train_time:743574ms step_avg:702.81ms
step:1069/1750 train_loss:3.3363 train_time:744326ms step_avg:702.86ms
step:1070/1750 train_loss:3.4891 train_time:745056ms step_avg:702.88ms
step:1071/1750 train_loss:3.3573 train_time:745809ms step_avg:702.93ms
step:1072/1750 train_loss:3.6244 train_time:746543ms step_avg:702.96ms
step:1073/1750 train_loss:3.5673 train_time:747307ms step_avg:703.02ms
step:1074/1750 train_loss:3.4993 train_time:748038ms step_avg:703.04ms
step:1075/1750 train_loss:3.5796 train_time:748770ms step_avg:703.07ms
step:1076/1750 train_loss:3.4997 train_time:749521ms step_avg:703.12ms
step:1077/1750 train_loss:3.4602 train_time:750264ms step_avg:703.15ms
step:1078/1750 train_loss:3.8536 train_time:750999ms step_avg:703.18ms
step:1079/1750 train_loss:3.4987 train_time:751734ms step_avg:703.21ms
step:1080/1750 train_loss:3.1547 train_time:752503ms step_avg:703.27ms
step:1081/1750 train_loss:3.5932 train_time:753241ms step_avg:703.31ms
step:1082/1750 train_loss:3.4899 train_time:753985ms step_avg:703.34ms
step:1083/1750 train_loss:3.5714 train_time:754742ms step_avg:703.39ms
step:1084/1750 train_loss:3.6557 train_time:755484ms step_avg:703.43ms
step:1085/1750 train_loss:3.5661 train_time:756215ms step_avg:703.46ms
step:1086/1750 train_loss:3.5355 train_time:756956ms step_avg:703.49ms
step:1087/1750 train_loss:3.4975 train_time:757700ms step_avg:703.53ms
step:1088/1750 train_loss:3.6931 train_time:758454ms step_avg:703.58ms
step:1089/1750 train_loss:3.5783 train_time:759207ms step_avg:703.62ms
step:1090/1750 train_loss:3.4288 train_time:759942ms step_avg:703.65ms
step:1091/1750 train_loss:3.4428 train_time:760690ms step_avg:703.69ms
step:1092/1750 train_loss:3.5526 train_time:761434ms step_avg:703.73ms
step:1093/1750 train_loss:3.3478 train_time:762172ms step_avg:703.76ms
step:1094/1750 train_loss:3.5569 train_time:762903ms step_avg:703.79ms
step:1095/1750 train_loss:3.6685 train_time:763639ms step_avg:703.82ms
step:1096/1750 train_loss:3.5078 train_time:764389ms step_avg:703.86ms
step:1097/1750 train_loss:3.4819 train_time:765125ms step_avg:703.89ms
step:1098/1750 train_loss:3.4938 train_time:765872ms step_avg:703.93ms
step:1099/1750 train_loss:3.5562 train_time:766607ms step_avg:703.95ms
step:1100/1750 train_loss:3.6204 train_time:767366ms step_avg:704.01ms
step:1101/1750 train_loss:3.5976 train_time:768113ms step_avg:704.04ms
step:1102/1750 train_loss:3.5043 train_time:768857ms step_avg:704.08ms
step:1103/1750 train_loss:3.3557 train_time:769601ms step_avg:704.12ms
step:1104/1750 train_loss:3.3777 train_time:770341ms step_avg:704.15ms
step:1105/1750 train_loss:3.5234 train_time:771084ms step_avg:704.19ms
step:1106/1750 train_loss:3.3869 train_time:771818ms step_avg:704.21ms
step:1107/1750 train_loss:4.1397 train_time:772569ms step_avg:704.26ms
step:1108/1750 train_loss:3.2996 train_time:773306ms step_avg:704.29ms
step:1109/1750 train_loss:3.6398 train_time:774042ms step_avg:704.31ms
step:1110/1750 train_loss:3.4113 train_time:774773ms step_avg:704.34ms
step:1111/1750 train_loss:3.5671 train_time:775499ms step_avg:704.36ms
step:1112/1750 train_loss:3.4963 train_time:776234ms step_avg:704.39ms
step:1113/1750 train_loss:3.5525 train_time:776978ms step_avg:704.42ms
step:1114/1750 train_loss:3.6305 train_time:777713ms step_avg:704.45ms
step:1115/1750 train_loss:3.5076 train_time:778437ms step_avg:704.47ms
step:1116/1750 train_loss:3.4374 train_time:779173ms step_avg:704.50ms
step:1117/1750 train_loss:3.3154 train_time:779940ms step_avg:704.55ms
step:1118/1750 train_loss:3.4941 train_time:780664ms step_avg:704.57ms
step:1119/1750 train_loss:3.6573 train_time:781422ms step_avg:704.62ms
step:1120/1750 train_loss:3.6953 train_time:782151ms step_avg:704.64ms
step:1121/1750 train_loss:3.5499 train_time:782881ms step_avg:704.66ms
step:1122/1750 train_loss:3.5632 train_time:783614ms step_avg:704.69ms
step:1123/1750 train_loss:3.4603 train_time:784351ms step_avg:704.72ms
step:1124/1750 train_loss:3.5245 train_time:785075ms step_avg:704.74ms
step:1125/1750 train_loss:3.6568 train_time:785822ms step_avg:704.77ms
step:1125/1750 val_loss:3.4826 train_time:785834ms step_avg:704.78ms
============== Weight norms: ==============
name = 'module._orig_mod.transformer.wte.weight' | frobenius_norm = 64674.05469 | spectral_norm = 11586.58887 | nuclear_norm = 1625438.50000
name = 'module._orig_mod.transformer.h.0.attn.c_q.weight' | frobenius_norm = 173.67824 | spectral_norm = 28.68294 | nuclear_norm = 3995.54346
name = 'module._orig_mod.transformer.h.0.attn.c_k.weight' | frobenius_norm = 165.42725 | spectral_norm = 33.54998 | nuclear_norm = 3767.15698
name = 'module._orig_mod.transformer.h.0.attn.c_v.weight' | frobenius_norm = 215.75235 | spectral_norm = 13.86971 | nuclear_norm = 5169.09180
name = 'module._orig_mod.transformer.h.0.attn.c_proj.weight' | frobenius_norm = 171.28255 | spectral_norm = 18.66597 | nuclear_norm = 3994.04004
name = 'module._orig_mod.transformer.h.0.mlp.c_fc.weight' | frobenius_norm = 363.37781 | spectral_norm = 28.13690 | nuclear_norm = 9690.07422
name = 'module._orig_mod.transformer.h.0.mlp.c_proj.weight' | frobenius_norm = 189.51204 | spectral_norm = 20.53606 | nuclear_norm = 5025.32520
name = 'module._orig_mod.transformer.h.1.attn.c_q.weight' | frobenius_norm = 171.09732 | spectral_norm = 17.40425 | nuclear_norm = 3941.09717
name = 'module._orig_mod.transformer.h.1.attn.c_k.weight' | frobenius_norm = 166.76347 | spectral_norm = 17.58487 | nuclear_norm = 3845.65527
name = 'module._orig_mod.transformer.h.1.attn.c_v.weight' | frobenius_norm = 175.45219 | spectral_norm = 14.83492 | nuclear_norm = 4049.27783
name = 'module._orig_mod.transformer.h.1.attn.c_proj.weight' | frobenius_norm = 174.70430 | spectral_norm = 14.35028 | nuclear_norm = 4065.53711
name = 'module._orig_mod.transformer.h.1.mlp.c_fc.weight' | frobenius_norm = 368.41882 | spectral_norm = 25.41815 | nuclear_norm = 9862.71094
name = 'module._orig_mod.transformer.h.1.mlp.c_proj.weight' | frobenius_norm = 187.16290 | spectral_norm = 20.57403 | nuclear_norm = 4945.84082
name = 'module._orig_mod.transformer.h.2.attn.c_q.weight' | frobenius_norm = 172.55328 | spectral_norm = 18.40129 | nuclear_norm = 3945.74121
name = 'module._orig_mod.transformer.h.2.attn.c_k.weight' | frobenius_norm = 168.97517 | spectral_norm = 17.02384 | nuclear_norm = 3864.65967
name = 'module._orig_mod.transformer.h.2.attn.c_v.weight' | frobenius_norm = 201.50104 | spectral_norm = 17.05416 | nuclear_norm = 4604.67773
name = 'module._orig_mod.transformer.h.2.attn.c_proj.weight' | frobenius_norm = 198.33446 | spectral_norm = 15.51485 | nuclear_norm = 4636.79395
name = 'module._orig_mod.transformer.h.2.mlp.c_fc.weight' | frobenius_norm = 367.59402 | spectral_norm = 32.54762 | nuclear_norm = 9830.57812
name = 'module._orig_mod.transformer.h.2.mlp.c_proj.weight' | frobenius_norm = 180.04929 | spectral_norm = 18.60305 | nuclear_norm = 4729.00781
name = 'module._orig_mod.transformer.h.3.attn.c_q.weight' | frobenius_norm = 169.04100 | spectral_norm = 19.30609 | nuclear_norm = 3839.38232
name = 'module._orig_mod.transformer.h.3.attn.c_k.weight' | frobenius_norm = 169.39787 | spectral_norm = 17.16230 | nuclear_norm = 3882.69458
name = 'module._orig_mod.transformer.h.3.attn.c_v.weight' | frobenius_norm = 194.48239 | spectral_norm = 16.76010 | nuclear_norm = 4310.82422
name = 'module._orig_mod.transformer.h.3.attn.c_proj.weight' | frobenius_norm = 193.48540 | spectral_norm = 16.96922 | nuclear_norm = 4416.82080
name = 'module._orig_mod.transformer.h.3.mlp.c_fc.weight' | frobenius_norm = 371.44232 | spectral_norm = 34.33378 | nuclear_norm = 9890.49219
name = 'module._orig_mod.transformer.h.3.mlp.c_proj.weight' | frobenius_norm = 182.59987 | spectral_norm = 19.54404 | nuclear_norm = 4784.56641
name = 'module._orig_mod.transformer.h.4.attn.c_q.weight' | frobenius_norm = 165.91960 | spectral_norm = 18.21762 | nuclear_norm = 3737.01929
name = 'module._orig_mod.transformer.h.4.attn.c_k.weight' | frobenius_norm = 167.86055 | spectral_norm = 16.12291 | nuclear_norm = 3791.90869
name = 'module._orig_mod.transformer.h.4.attn.c_v.weight' | frobenius_norm = 199.52739 | spectral_norm = 17.56026 | nuclear_norm = 4423.20703
name = 'module._orig_mod.transformer.h.4.attn.c_proj.weight' | frobenius_norm = 201.48837 | spectral_norm = 16.51749 | nuclear_norm = 4595.14111
name = 'module._orig_mod.transformer.h.4.mlp.c_fc.weight' | frobenius_norm = 372.14664 | spectral_norm = 36.82418 | nuclear_norm = 9857.78320
name = 'module._orig_mod.transformer.h.4.mlp.c_proj.weight' | frobenius_norm = 182.33835 | spectral_norm = 19.58829 | nuclear_norm = 4766.80664
name = 'module._orig_mod.transformer.h.5.attn.c_q.weight' | frobenius_norm = 165.49899 | spectral_norm = 17.02678 | nuclear_norm = 3660.36768
name = 'module._orig_mod.transformer.h.5.attn.c_k.weight' | frobenius_norm = 166.26190 | spectral_norm = 16.16629 | nuclear_norm = 3742.88257
name = 'module._orig_mod.transformer.h.5.attn.c_v.weight' | frobenius_norm = 200.47255 | spectral_norm = 18.41412 | nuclear_norm = 4486.88770
name = 'module._orig_mod.transformer.h.5.attn.c_proj.weight' | frobenius_norm = 202.23599 | spectral_norm = 15.62138 | nuclear_norm = 4686.86084
name = 'module._orig_mod.transformer.h.5.mlp.c_fc.weight' | frobenius_norm = 395.15155 | spectral_norm = 36.38944 | nuclear_norm = 10476.04395
name = 'module._orig_mod.transformer.h.5.mlp.c_proj.weight' | frobenius_norm = 182.85637 | spectral_norm = 20.39266 | nuclear_norm = 4802.89502
name = 'module._orig_mod.transformer.h.6.attn.c_q.weight' | frobenius_norm = 163.72571 | spectral_norm = 17.34651 | nuclear_norm = 3615.73975
name = 'module._orig_mod.transformer.h.6.attn.c_k.weight' | frobenius_norm = 163.93768 | spectral_norm = 15.42577 | nuclear_norm = 3635.68164
name = 'module._orig_mod.transformer.h.6.attn.c_v.weight' | frobenius_norm = 208.76479 | spectral_norm = 21.10615 | nuclear_norm = 4551.26660
name = 'module._orig_mod.transformer.h.6.attn.c_proj.weight' | frobenius_norm = 214.89955 | spectral_norm = 18.21393 | nuclear_norm = 5004.88184
name = 'module._orig_mod.transformer.h.6.mlp.c_fc.weight' | frobenius_norm = 392.77579 | spectral_norm = 33.37726 | nuclear_norm = 10454.37500
name = 'module._orig_mod.transformer.h.6.mlp.c_proj.weight' | frobenius_norm = 187.45505 | spectral_norm = 21.04558 | nuclear_norm = 4939.41504
name = 'module._orig_mod.transformer.h.7.attn.c_q.weight' | frobenius_norm = 170.50613 | spectral_norm = 19.47735 | nuclear_norm = 3740.81885
name = 'module._orig_mod.transformer.h.7.attn.c_k.weight' | frobenius_norm = 171.13173 | spectral_norm = 16.68417 | nuclear_norm = 3725.27686
name = 'module._orig_mod.transformer.h.7.attn.c_v.weight' | frobenius_norm = 187.08484 | spectral_norm = 21.07383 | nuclear_norm = 4136.76807
name = 'module._orig_mod.transformer.h.7.attn.c_proj.weight' | frobenius_norm = 195.52672 | spectral_norm = 13.93884 | nuclear_norm = 4632.50049
name = 'module._orig_mod.transformer.h.7.mlp.c_fc.weight' | frobenius_norm = 408.03589 | spectral_norm = 37.16397 | nuclear_norm = 10833.76660
name = 'module._orig_mod.transformer.h.7.mlp.c_proj.weight' | frobenius_norm = 200.28552 | spectral_norm = 21.90647 | nuclear_norm = 5298.35498
name = 'module._orig_mod.transformer.h.8.attn.c_q.weight' | frobenius_norm = 175.46362 | spectral_norm = 18.11778 | nuclear_norm = 3941.93921
name = 'module._orig_mod.transformer.h.8.attn.c_k.weight' | frobenius_norm = 170.36003 | spectral_norm = 19.67647 | nuclear_norm = 3833.63379
name = 'module._orig_mod.transformer.h.8.attn.c_v.weight' | frobenius_norm = 223.88768 | spectral_norm = 24.42959 | nuclear_norm = 4947.87793
name = 'module._orig_mod.transformer.h.8.attn.c_proj.weight' | frobenius_norm = 220.15637 | spectral_norm = 15.82643 | nuclear_norm = 5174.54004
name = 'module._orig_mod.transformer.h.8.mlp.c_fc.weight' | frobenius_norm = 383.96188 | spectral_norm = 34.85655 | nuclear_norm = 10207.00586
name = 'module._orig_mod.transformer.h.8.mlp.c_proj.weight' | frobenius_norm = 196.72069 | spectral_norm = 21.15875 | nuclear_norm = 5215.69580
name = 'module._orig_mod.transformer.h.9.attn.c_q.weight' | frobenius_norm = 170.06830 | spectral_norm = 19.18931 | nuclear_norm = 3794.89771
name = 'module._orig_mod.transformer.h.9.attn.c_k.weight' | frobenius_norm = 168.89809 | spectral_norm = 18.09949 | nuclear_norm = 3790.66846
name = 'module._orig_mod.transformer.h.9.attn.c_v.weight' | frobenius_norm = 222.13676 | spectral_norm = 21.55631 | nuclear_norm = 4918.86035
name = 'module._orig_mod.transformer.h.9.attn.c_proj.weight' | frobenius_norm = 229.51544 | spectral_norm = 18.50440 | nuclear_norm = 5385.34717
name = 'module._orig_mod.transformer.h.9.mlp.c_fc.weight' | frobenius_norm = 380.92429 | spectral_norm = 32.81624 | nuclear_norm = 10129.12012
name = 'module._orig_mod.transformer.h.9.mlp.c_proj.weight' | frobenius_norm = 196.39427 | spectral_norm = 20.98534 | nuclear_norm = 5215.56592
name = 'module._orig_mod.transformer.h.10.attn.c_q.weight' | frobenius_norm = 173.09380 | spectral_norm = 21.27051 | nuclear_norm = 3831.90991
name = 'module._orig_mod.transformer.h.10.attn.c_k.weight' | frobenius_norm = 169.82925 | spectral_norm = 19.91320 | nuclear_norm = 3725.88794
name = 'module._orig_mod.transformer.h.10.attn.c_v.weight' | frobenius_norm = 173.98178 | spectral_norm = 17.09907 | nuclear_norm = 3987.64258
name = 'module._orig_mod.transformer.h.10.attn.c_proj.weight' | frobenius_norm = 201.18420 | spectral_norm = 13.89691 | nuclear_norm = 4719.30176
name = 'module._orig_mod.transformer.h.10.mlp.c_fc.weight' | frobenius_norm = 408.27295 | spectral_norm = 30.83450 | nuclear_norm = 10896.42383
name = 'module._orig_mod.transformer.h.10.mlp.c_proj.weight' | frobenius_norm = 189.89183 | spectral_norm = 23.47239 | nuclear_norm = 5015.75195
name = 'module._orig_mod.transformer.h.11.attn.c_q.weight' | frobenius_norm = 171.33269 | spectral_norm = 21.25131 | nuclear_norm = 3814.43115
name = 'module._orig_mod.transformer.h.11.attn.c_k.weight' | frobenius_norm = 170.63762 | spectral_norm = 19.35591 | nuclear_norm = 3831.61133
name = 'module._orig_mod.transformer.h.11.attn.c_v.weight' | frobenius_norm = 218.96036 | spectral_norm = 24.65911 | nuclear_norm = 4783.87598
name = 'module._orig_mod.transformer.h.11.attn.c_proj.weight' | frobenius_norm = 235.70557 | spectral_norm = 17.88773 | nuclear_norm = 5473.41699
name = 'module._orig_mod.transformer.h.11.mlp.c_fc.weight' | frobenius_norm = 405.87836 | spectral_norm = 35.04447 | nuclear_norm = 10783.22070
name = 'module._orig_mod.transformer.h.11.mlp.c_proj.weight' | frobenius_norm = 201.72397 | spectral_norm = 24.75407 | nuclear_norm = 5297.31738
name = 'module._orig_mod.lm_head.weight' | frobenius_norm = 1234.82007 | spectral_norm = 175.93045 | nuclear_norm = 32423.56250
===========================================
step:1126/1750 train_loss:3.4174 train_time:786557ms step_avg:704.80ms
step:1127/1750 train_loss:3.2891 train_time:787300ms step_avg:704.83ms
step:1128/1750 train_loss:3.5483 train_time:788044ms step_avg:704.87ms
