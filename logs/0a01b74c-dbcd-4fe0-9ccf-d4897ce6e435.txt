====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.layer_id != 0:
            self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        else:
            v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        if self.layer_id != 0:
            self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        if self.layer_id != 0:
            x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.zeros(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 3000 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.01, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.06, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.06, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 13:41:56 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   34C    P0             69W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             75W /  400W |    3481MiB /  81920MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   33C    P0             74W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   39C    P0             74W /  400W |    3337MiB /  81920MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/3000 val_loss:10.8258 train_time:296ms step_avg:nanms
step:1/3000 train_loss:10.8258 train_time:14192ms step_avg:nanms
step:2/3000 train_loss:10.1741 train_time:14831ms step_avg:nanms
step:3/3000 train_loss:8.4496 train_time:15493ms step_avg:nanms
step:4/3000 train_loss:7.8146 train_time:16159ms step_avg:nanms
step:5/3000 train_loss:7.7563 train_time:16825ms step_avg:nanms
step:6/3000 train_loss:7.2657 train_time:17491ms step_avg:nanms
step:7/3000 train_loss:7.4622 train_time:18158ms step_avg:nanms
step:8/3000 train_loss:6.9741 train_time:18826ms step_avg:nanms
step:9/3000 train_loss:6.8180 train_time:19494ms step_avg:nanms
step:10/3000 train_loss:6.7639 train_time:20163ms step_avg:nanms
step:11/3000 train_loss:6.7526 train_time:658ms step_avg:nanms
step:12/3000 train_loss:6.5619 train_time:1326ms step_avg:nanms
step:13/3000 train_loss:6.5037 train_time:1994ms step_avg:664.53ms
step:14/3000 train_loss:6.4312 train_time:2663ms step_avg:665.70ms
step:15/3000 train_loss:6.3952 train_time:3332ms step_avg:666.36ms
step:16/3000 train_loss:6.3318 train_time:4003ms step_avg:667.16ms
step:17/3000 train_loss:6.4371 train_time:4674ms step_avg:667.67ms
step:18/3000 train_loss:6.2223 train_time:5344ms step_avg:667.94ms
step:19/3000 train_loss:6.2152 train_time:6016ms step_avg:668.48ms
step:20/3000 train_loss:5.9092 train_time:6690ms step_avg:669.00ms
step:21/3000 train_loss:6.2234 train_time:7360ms step_avg:669.12ms
step:22/3000 train_loss:6.4499 train_time:8030ms step_avg:669.19ms
step:23/3000 train_loss:6.1466 train_time:8700ms step_avg:669.26ms
step:24/3000 train_loss:6.2837 train_time:9372ms step_avg:669.44ms
step:25/3000 train_loss:5.9856 train_time:10041ms step_avg:669.41ms
step:26/3000 train_loss:5.8993 train_time:10713ms step_avg:669.59ms
step:27/3000 train_loss:6.1313 train_time:11386ms step_avg:669.74ms
step:28/3000 train_loss:5.7237 train_time:12058ms step_avg:669.86ms
step:29/3000 train_loss:5.9835 train_time:12729ms step_avg:669.95ms
step:30/3000 train_loss:5.7950 train_time:13400ms step_avg:670.00ms
step:31/3000 train_loss:5.7768 train_time:14071ms step_avg:670.05ms
step:32/3000 train_loss:5.6154 train_time:14743ms step_avg:670.12ms
step:33/3000 train_loss:5.8948 train_time:15414ms step_avg:670.17ms
step:34/3000 train_loss:5.8070 train_time:16086ms step_avg:670.27ms
step:35/3000 train_loss:5.9538 train_time:16757ms step_avg:670.29ms
step:36/3000 train_loss:5.8851 train_time:17431ms step_avg:670.41ms
step:37/3000 train_loss:5.7529 train_time:18102ms step_avg:670.46ms
step:38/3000 train_loss:5.6227 train_time:18774ms step_avg:670.51ms
step:39/3000 train_loss:5.6414 train_time:19445ms step_avg:670.52ms
step:40/3000 train_loss:5.5723 train_time:20117ms step_avg:670.58ms
step:41/3000 train_loss:5.5548 train_time:20788ms step_avg:670.57ms
step:42/3000 train_loss:5.4662 train_time:21460ms step_avg:670.64ms
step:43/3000 train_loss:5.5671 train_time:22133ms step_avg:670.70ms
step:44/3000 train_loss:5.5449 train_time:22804ms step_avg:670.70ms
step:45/3000 train_loss:5.6707 train_time:23476ms step_avg:670.75ms
step:46/3000 train_loss:5.4741 train_time:24149ms step_avg:670.79ms
step:47/3000 train_loss:5.3375 train_time:24820ms step_avg:670.81ms
step:48/3000 train_loss:5.5068 train_time:25492ms step_avg:670.83ms
step:49/3000 train_loss:5.4192 train_time:26163ms step_avg:670.85ms
step:50/3000 train_loss:5.5358 train_time:26836ms step_avg:670.91ms
step:51/3000 train_loss:5.4316 train_time:27509ms step_avg:670.94ms
step:52/3000 train_loss:5.2826 train_time:28181ms step_avg:670.98ms
step:53/3000 train_loss:5.4120 train_time:28854ms step_avg:671.01ms
step:54/3000 train_loss:5.2698 train_time:29526ms step_avg:671.05ms
step:55/3000 train_loss:5.6594 train_time:30198ms step_avg:671.06ms
step:56/3000 train_loss:5.2710 train_time:30870ms step_avg:671.09ms
step:57/3000 train_loss:5.1614 train_time:31541ms step_avg:671.09ms
step:58/3000 train_loss:5.2740 train_time:32214ms step_avg:671.13ms
step:59/3000 train_loss:5.2507 train_time:32886ms step_avg:671.15ms
step:60/3000 train_loss:5.3866 train_time:33560ms step_avg:671.20ms
step:61/3000 train_loss:5.1087 train_time:34231ms step_avg:671.20ms
step:62/3000 train_loss:5.2260 train_time:34904ms step_avg:671.23ms
step:63/3000 train_loss:5.2045 train_time:35575ms step_avg:671.22ms
step:64/3000 train_loss:5.2078 train_time:36247ms step_avg:671.25ms
step:65/3000 train_loss:5.0511 train_time:36919ms step_avg:671.26ms
step:66/3000 train_loss:5.1925 train_time:37591ms step_avg:671.27ms
step:67/3000 train_loss:5.0502 train_time:38264ms step_avg:671.29ms
step:68/3000 train_loss:5.3650 train_time:38936ms step_avg:671.32ms
step:69/3000 train_loss:4.9441 train_time:39608ms step_avg:671.32ms
step:70/3000 train_loss:5.0325 train_time:40281ms step_avg:671.35ms
step:71/3000 train_loss:5.2048 train_time:40952ms step_avg:671.35ms
step:72/3000 train_loss:5.1325 train_time:41625ms step_avg:671.38ms
step:73/3000 train_loss:4.9944 train_time:42298ms step_avg:671.39ms
step:74/3000 train_loss:5.1345 train_time:42969ms step_avg:671.39ms
step:75/3000 train_loss:5.0944 train_time:43641ms step_avg:671.39ms
step:76/3000 train_loss:5.0411 train_time:44312ms step_avg:671.40ms
step:77/3000 train_loss:5.1519 train_time:44984ms step_avg:671.41ms
step:78/3000 train_loss:5.2928 train_time:45656ms step_avg:671.41ms
step:79/3000 train_loss:5.0337 train_time:46328ms step_avg:671.42ms
step:80/3000 train_loss:5.0758 train_time:47001ms step_avg:671.44ms
step:81/3000 train_loss:4.8674 train_time:47672ms step_avg:671.43ms
step:82/3000 train_loss:5.0475 train_time:48343ms step_avg:671.43ms
step:83/3000 train_loss:4.9990 train_time:49014ms step_avg:671.43ms
step:84/3000 train_loss:4.9911 train_time:49686ms step_avg:671.43ms
step:85/3000 train_loss:4.8484 train_time:50357ms step_avg:671.42ms
step:86/3000 train_loss:5.0541 train_time:51031ms step_avg:671.46ms
step:87/3000 train_loss:4.9642 train_time:51703ms step_avg:671.47ms
step:88/3000 train_loss:4.9825 train_time:52376ms step_avg:671.48ms
step:89/3000 train_loss:4.9398 train_time:53048ms step_avg:671.50ms
step:90/3000 train_loss:4.8937 train_time:53720ms step_avg:671.50ms
step:91/3000 train_loss:4.8701 train_time:54390ms step_avg:671.48ms
step:92/3000 train_loss:5.0069 train_time:55061ms step_avg:671.48ms
step:93/3000 train_loss:4.8127 train_time:55733ms step_avg:671.48ms
step:94/3000 train_loss:4.8361 train_time:56404ms step_avg:671.48ms
step:95/3000 train_loss:4.8643 train_time:57076ms step_avg:671.48ms
step:96/3000 train_loss:4.7789 train_time:57747ms step_avg:671.48ms
step:97/3000 train_loss:4.8277 train_time:58418ms step_avg:671.47ms
step:98/3000 train_loss:4.7503 train_time:59088ms step_avg:671.46ms
step:99/3000 train_loss:4.8554 train_time:59759ms step_avg:671.45ms
step:100/3000 train_loss:4.8535 train_time:60430ms step_avg:671.44ms
step:101/3000 train_loss:4.7556 train_time:61101ms step_avg:671.43ms
step:102/3000 train_loss:4.8618 train_time:61772ms step_avg:671.43ms
step:103/3000 train_loss:4.7428 train_time:62443ms step_avg:671.43ms
step:104/3000 train_loss:4.6889 train_time:63114ms step_avg:671.42ms
step:105/3000 train_loss:4.7353 train_time:63784ms step_avg:671.41ms
step:106/3000 train_loss:4.7586 train_time:64454ms step_avg:671.39ms
step:107/3000 train_loss:4.6843 train_time:65124ms step_avg:671.38ms
step:108/3000 train_loss:4.5119 train_time:65794ms step_avg:671.36ms
step:109/3000 train_loss:4.6489 train_time:66465ms step_avg:671.36ms
step:110/3000 train_loss:4.6444 train_time:67135ms step_avg:671.35ms
step:111/3000 train_loss:4.5707 train_time:67807ms step_avg:671.35ms
step:112/3000 train_loss:4.7124 train_time:68476ms step_avg:671.33ms
step:113/3000 train_loss:4.6097 train_time:69146ms step_avg:671.32ms
step:114/3000 train_loss:4.4791 train_time:69817ms step_avg:671.32ms
step:115/3000 train_loss:4.6263 train_time:70487ms step_avg:671.30ms
step:116/3000 train_loss:4.5769 train_time:71158ms step_avg:671.30ms
step:117/3000 train_loss:4.5119 train_time:71828ms step_avg:671.29ms
step:118/3000 train_loss:4.7228 train_time:72499ms step_avg:671.29ms
step:119/3000 train_loss:4.5739 train_time:73170ms step_avg:671.29ms
step:120/3000 train_loss:4.4367 train_time:73841ms step_avg:671.28ms
step:121/3000 train_loss:4.4341 train_time:74510ms step_avg:671.26ms
step:122/3000 train_loss:4.5885 train_time:75181ms step_avg:671.26ms
step:123/3000 train_loss:4.3857 train_time:75851ms step_avg:671.25ms
step:124/3000 train_loss:4.6907 train_time:76521ms step_avg:671.24ms
step:125/3000 train_loss:4.5728 train_time:77192ms step_avg:671.24ms
step:125/3000 val_loss:4.5162 train_time:77205ms step_avg:671.34ms
step:126/3000 train_loss:4.5262 train_time:77866ms step_avg:671.26ms
step:127/3000 train_loss:4.5524 train_time:78536ms step_avg:671.25ms
step:128/3000 train_loss:4.4882 train_time:79206ms step_avg:671.24ms
step:129/3000 train_loss:4.7789 train_time:79878ms step_avg:671.24ms
step:130/3000 train_loss:4.4523 train_time:80549ms step_avg:671.24ms
step:131/3000 train_loss:4.5123 train_time:81219ms step_avg:671.24ms
step:132/3000 train_loss:4.4520 train_time:81890ms step_avg:671.23ms
step:133/3000 train_loss:4.5746 train_time:82562ms step_avg:671.24ms
step:134/3000 train_loss:4.3925 train_time:83232ms step_avg:671.22ms
step:135/3000 train_loss:4.5447 train_time:83904ms step_avg:671.23ms
step:136/3000 train_loss:4.2841 train_time:84575ms step_avg:671.23ms
step:137/3000 train_loss:4.4740 train_time:85246ms step_avg:671.23ms
step:138/3000 train_loss:4.3743 train_time:85916ms step_avg:671.22ms
step:139/3000 train_loss:4.4676 train_time:86587ms step_avg:671.22ms
step:140/3000 train_loss:4.5625 train_time:87257ms step_avg:671.21ms
step:141/3000 train_loss:4.4251 train_time:87928ms step_avg:671.21ms
step:142/3000 train_loss:4.3880 train_time:88598ms step_avg:671.20ms
step:143/3000 train_loss:4.3291 train_time:89269ms step_avg:671.19ms
step:144/3000 train_loss:4.4401 train_time:89938ms step_avg:671.18ms
step:145/3000 train_loss:4.3837 train_time:90608ms step_avg:671.17ms
step:146/3000 train_loss:4.2658 train_time:91278ms step_avg:671.16ms
step:147/3000 train_loss:4.4034 train_time:91948ms step_avg:671.15ms
step:148/3000 train_loss:4.4461 train_time:92619ms step_avg:671.15ms
step:149/3000 train_loss:4.3946 train_time:93289ms step_avg:671.14ms
step:150/3000 train_loss:4.5071 train_time:93960ms step_avg:671.14ms
step:151/3000 train_loss:4.3456 train_time:94630ms step_avg:671.13ms
step:152/3000 train_loss:4.3519 train_time:95300ms step_avg:671.12ms
step:153/3000 train_loss:4.4431 train_time:95970ms step_avg:671.12ms
step:154/3000 train_loss:4.4209 train_time:96640ms step_avg:671.11ms
step:155/3000 train_loss:4.3446 train_time:97311ms step_avg:671.11ms
step:156/3000 train_loss:4.4252 train_time:97981ms step_avg:671.10ms
step:157/3000 train_loss:4.5145 train_time:98652ms step_avg:671.10ms
step:158/3000 train_loss:4.3216 train_time:99321ms step_avg:671.09ms
step:159/3000 train_loss:4.3896 train_time:99992ms step_avg:671.08ms
step:160/3000 train_loss:4.1978 train_time:100661ms step_avg:671.07ms
step:161/3000 train_loss:4.4295 train_time:101331ms step_avg:671.07ms
step:162/3000 train_loss:4.4267 train_time:102002ms step_avg:671.06ms
step:163/3000 train_loss:4.4049 train_time:102673ms step_avg:671.07ms
step:164/3000 train_loss:4.2666 train_time:103343ms step_avg:671.06ms
step:165/3000 train_loss:4.3556 train_time:104015ms step_avg:671.06ms
step:166/3000 train_loss:4.4181 train_time:104685ms step_avg:671.05ms
step:167/3000 train_loss:4.2580 train_time:105356ms step_avg:671.05ms
step:168/3000 train_loss:4.3248 train_time:106027ms step_avg:671.06ms
step:169/3000 train_loss:4.2206 train_time:106697ms step_avg:671.05ms
step:170/3000 train_loss:4.1026 train_time:107367ms step_avg:671.04ms
step:171/3000 train_loss:4.2533 train_time:108037ms step_avg:671.03ms
step:172/3000 train_loss:4.2885 train_time:108706ms step_avg:671.03ms
step:173/3000 train_loss:4.3363 train_time:109376ms step_avg:671.02ms
step:174/3000 train_loss:4.5083 train_time:110047ms step_avg:671.02ms
step:175/3000 train_loss:4.3361 train_time:110717ms step_avg:671.01ms
step:176/3000 train_loss:4.1876 train_time:111389ms step_avg:671.02ms
step:177/3000 train_loss:4.1447 train_time:112058ms step_avg:671.00ms
step:178/3000 train_loss:4.2610 train_time:112728ms step_avg:671.00ms
step:179/3000 train_loss:4.2044 train_time:113398ms step_avg:670.99ms
step:180/3000 train_loss:4.1823 train_time:114069ms step_avg:670.99ms
step:181/3000 train_loss:4.3699 train_time:114739ms step_avg:670.99ms
step:182/3000 train_loss:4.2391 train_time:115410ms step_avg:670.99ms
step:183/3000 train_loss:4.2001 train_time:116080ms step_avg:670.98ms
step:184/3000 train_loss:4.2119 train_time:116751ms step_avg:670.98ms
step:185/3000 train_loss:4.2873 train_time:117420ms step_avg:670.97ms
step:186/3000 train_loss:4.2491 train_time:118091ms step_avg:670.97ms
step:187/3000 train_loss:4.3062 train_time:118762ms step_avg:670.97ms
step:188/3000 train_loss:4.2362 train_time:119432ms step_avg:670.97ms
step:189/3000 train_loss:4.1735 train_time:120102ms step_avg:670.96ms
step:190/3000 train_loss:4.2840 train_time:120781ms step_avg:671.00ms
step:191/3000 train_loss:4.3643 train_time:121629ms step_avg:671.98ms
step:192/3000 train_loss:4.1478 train_time:122297ms step_avg:671.96ms
step:193/3000 train_loss:4.2228 train_time:122967ms step_avg:671.95ms
step:194/3000 train_loss:4.1749 train_time:123638ms step_avg:671.95ms
step:195/3000 train_loss:4.1456 train_time:124308ms step_avg:671.93ms
step:196/3000 train_loss:4.1420 train_time:124978ms step_avg:671.93ms
step:197/3000 train_loss:4.1952 train_time:125648ms step_avg:671.91ms
step:198/3000 train_loss:4.0937 train_time:126318ms step_avg:671.91ms
step:199/3000 train_loss:4.3274 train_time:126989ms step_avg:671.90ms
step:200/3000 train_loss:4.2454 train_time:127658ms step_avg:671.89ms
step:201/3000 train_loss:5.0133 train_time:128328ms step_avg:671.88ms
step:202/3000 train_loss:4.4542 train_time:128998ms step_avg:671.86ms
step:203/3000 train_loss:4.2194 train_time:129669ms step_avg:671.86ms
step:204/3000 train_loss:4.1502 train_time:130339ms step_avg:671.85ms
step:205/3000 train_loss:4.1830 train_time:131008ms step_avg:671.84ms
step:206/3000 train_loss:4.1496 train_time:131678ms step_avg:671.83ms
step:207/3000 train_loss:4.1500 train_time:132349ms step_avg:671.82ms
step:208/3000 train_loss:4.1082 train_time:133019ms step_avg:671.81ms
step:209/3000 train_loss:4.2789 train_time:133687ms step_avg:671.80ms
step:210/3000 train_loss:4.0784 train_time:134358ms step_avg:671.79ms
step:211/3000 train_loss:4.1953 train_time:135028ms step_avg:671.78ms
step:212/3000 train_loss:4.1908 train_time:135697ms step_avg:671.77ms
step:213/3000 train_loss:4.1405 train_time:136367ms step_avg:671.76ms
step:214/3000 train_loss:4.0836 train_time:137038ms step_avg:671.75ms
step:215/3000 train_loss:4.2203 train_time:137707ms step_avg:671.74ms
step:216/3000 train_loss:4.1561 train_time:138376ms step_avg:671.73ms
step:217/3000 train_loss:4.1545 train_time:139046ms step_avg:671.72ms
step:218/3000 train_loss:4.1640 train_time:139716ms step_avg:671.71ms
step:219/3000 train_loss:4.0870 train_time:140385ms step_avg:671.70ms
step:220/3000 train_loss:4.2039 train_time:141054ms step_avg:671.69ms
step:221/3000 train_loss:4.1019 train_time:141724ms step_avg:671.68ms
step:222/3000 train_loss:4.1919 train_time:142394ms step_avg:671.67ms
step:223/3000 train_loss:4.1098 train_time:143063ms step_avg:671.66ms
step:224/3000 train_loss:4.1175 train_time:143733ms step_avg:671.65ms
step:225/3000 train_loss:4.1134 train_time:144403ms step_avg:671.64ms
step:226/3000 train_loss:4.1266 train_time:145073ms step_avg:671.63ms
step:227/3000 train_loss:4.1092 train_time:145742ms step_avg:671.62ms
step:228/3000 train_loss:4.0378 train_time:146411ms step_avg:671.61ms
step:229/3000 train_loss:4.2612 train_time:147080ms step_avg:671.60ms
step:230/3000 train_loss:4.1241 train_time:147749ms step_avg:671.59ms
step:231/3000 train_loss:4.0086 train_time:148419ms step_avg:671.58ms
step:232/3000 train_loss:4.2345 train_time:149091ms step_avg:671.58ms
step:233/3000 train_loss:4.0596 train_time:149761ms step_avg:671.57ms
step:234/3000 train_loss:4.0907 train_time:150430ms step_avg:671.56ms
step:235/3000 train_loss:4.2544 train_time:151098ms step_avg:671.55ms
step:236/3000 train_loss:4.1490 train_time:151767ms step_avg:671.53ms
step:237/3000 train_loss:4.2400 train_time:152436ms step_avg:671.53ms
step:238/3000 train_loss:4.1833 train_time:153106ms step_avg:671.52ms
step:239/3000 train_loss:4.1683 train_time:153775ms step_avg:671.50ms
step:240/3000 train_loss:4.1361 train_time:154444ms step_avg:671.49ms
step:241/3000 train_loss:4.3342 train_time:155114ms step_avg:671.49ms
step:242/3000 train_loss:4.1623 train_time:155782ms step_avg:671.48ms
step:243/3000 train_loss:4.5059 train_time:156452ms step_avg:671.47ms
step:244/3000 train_loss:4.1128 train_time:157120ms step_avg:671.45ms
step:245/3000 train_loss:4.1421 train_time:157790ms step_avg:671.45ms
step:246/3000 train_loss:4.1657 train_time:158459ms step_avg:671.44ms
step:247/3000 train_loss:4.0751 train_time:159129ms step_avg:671.43ms
step:248/3000 train_loss:4.0833 train_time:159798ms step_avg:671.42ms
step:249/3000 train_loss:4.0118 train_time:160467ms step_avg:671.41ms
step:250/3000 train_loss:4.1747 train_time:161136ms step_avg:671.40ms
step:250/3000 val_loss:4.0822 train_time:161147ms step_avg:671.45ms
step:251/3000 train_loss:4.0677 train_time:161807ms step_avg:671.40ms
step:252/3000 train_loss:3.9919 train_time:162477ms step_avg:671.39ms
step:253/3000 train_loss:4.0165 train_time:163146ms step_avg:671.38ms
step:254/3000 train_loss:4.0873 train_time:163816ms step_avg:671.38ms
step:255/3000 train_loss:4.0278 train_time:164485ms step_avg:671.37ms
step:256/3000 train_loss:3.9669 train_time:165154ms step_avg:671.36ms
step:257/3000 train_loss:4.0331 train_time:165822ms step_avg:671.35ms
step:258/3000 train_loss:4.1574 train_time:166492ms step_avg:671.34ms
step:259/3000 train_loss:3.9657 train_time:167160ms step_avg:671.33ms
step:260/3000 train_loss:4.0669 train_time:167830ms step_avg:671.32ms
step:261/3000 train_loss:4.0225 train_time:168499ms step_avg:671.31ms
step:262/3000 train_loss:4.0627 train_time:169169ms step_avg:671.31ms
step:263/3000 train_loss:4.1219 train_time:169838ms step_avg:671.30ms
step:264/3000 train_loss:4.1618 train_time:170508ms step_avg:671.29ms
step:265/3000 train_loss:4.5401 train_time:171178ms step_avg:671.28ms
step:266/3000 train_loss:4.0639 train_time:171846ms step_avg:671.27ms
step:267/3000 train_loss:4.2520 train_time:172516ms step_avg:671.27ms
step:268/3000 train_loss:4.0098 train_time:173185ms step_avg:671.26ms
step:269/3000 train_loss:4.0149 train_time:173853ms step_avg:671.25ms
step:270/3000 train_loss:4.1179 train_time:174520ms step_avg:671.23ms
step:271/3000 train_loss:4.1681 train_time:175188ms step_avg:671.22ms
step:272/3000 train_loss:4.0546 train_time:175856ms step_avg:671.21ms
step:273/3000 train_loss:4.0042 train_time:176524ms step_avg:671.19ms
step:274/3000 train_loss:4.0049 train_time:177194ms step_avg:671.19ms
step:275/3000 train_loss:3.9898 train_time:177863ms step_avg:671.18ms
step:276/3000 train_loss:4.0965 train_time:178532ms step_avg:671.17ms
step:277/3000 train_loss:4.1194 train_time:179202ms step_avg:671.17ms
step:278/3000 train_loss:3.9090 train_time:179871ms step_avg:671.16ms
step:279/3000 train_loss:4.2150 train_time:180540ms step_avg:671.15ms
step:280/3000 train_loss:4.0685 train_time:181209ms step_avg:671.15ms
step:281/3000 train_loss:4.0786 train_time:181879ms step_avg:671.14ms
step:282/3000 train_loss:4.0204 train_time:182548ms step_avg:671.13ms
step:283/3000 train_loss:4.2268 train_time:183217ms step_avg:671.12ms
step:284/3000 train_loss:3.9844 train_time:183887ms step_avg:671.12ms
step:285/3000 train_loss:3.9517 train_time:184558ms step_avg:671.12ms
step:286/3000 train_loss:4.1395 train_time:185226ms step_avg:671.11ms
step:287/3000 train_loss:4.0547 train_time:185894ms step_avg:671.10ms
step:288/3000 train_loss:4.0836 train_time:186562ms step_avg:671.09ms
step:289/3000 train_loss:4.1307 train_time:187231ms step_avg:671.08ms
step:290/3000 train_loss:4.1215 train_time:187899ms step_avg:671.07ms
step:291/3000 train_loss:3.9538 train_time:188568ms step_avg:671.06ms
step:292/3000 train_loss:3.9463 train_time:189236ms step_avg:671.05ms
step:293/3000 train_loss:4.1261 train_time:189906ms step_avg:671.05ms
step:294/3000 train_loss:3.9314 train_time:190575ms step_avg:671.04ms
step:295/3000 train_loss:4.2201 train_time:191244ms step_avg:671.03ms
step:296/3000 train_loss:4.1078 train_time:191913ms step_avg:671.03ms
step:297/3000 train_loss:3.9430 train_time:192583ms step_avg:671.02ms
step:298/3000 train_loss:4.0430 train_time:193252ms step_avg:671.01ms
step:299/3000 train_loss:3.9562 train_time:193920ms step_avg:671.00ms
step:300/3000 train_loss:4.1541 train_time:194588ms step_avg:670.99ms
step:301/3000 train_loss:3.9452 train_time:195258ms step_avg:670.99ms
step:302/3000 train_loss:4.0575 train_time:195927ms step_avg:670.98ms
step:303/3000 train_loss:3.9806 train_time:196596ms step_avg:670.98ms
step:304/3000 train_loss:3.9905 train_time:197265ms step_avg:670.97ms
step:305/3000 train_loss:3.9904 train_time:197934ms step_avg:670.96ms
step:306/3000 train_loss:4.0880 train_time:198604ms step_avg:670.96ms
step:307/3000 train_loss:4.0324 train_time:199273ms step_avg:670.95ms
step:308/3000 train_loss:3.9330 train_time:199944ms step_avg:670.95ms
step:309/3000 train_loss:4.1134 train_time:200613ms step_avg:670.95ms
step:310/3000 train_loss:3.9782 train_time:201283ms step_avg:670.94ms
step:311/3000 train_loss:4.0370 train_time:201954ms step_avg:670.94ms
step:312/3000 train_loss:4.0288 train_time:202622ms step_avg:670.93ms
step:313/3000 train_loss:3.9298 train_time:203291ms step_avg:670.93ms
step:314/3000 train_loss:4.0172 train_time:203961ms step_avg:670.92ms
step:315/3000 train_loss:4.0120 train_time:204630ms step_avg:670.92ms
step:316/3000 train_loss:3.7442 train_time:205298ms step_avg:670.91ms
step:317/3000 train_loss:3.9925 train_time:205967ms step_avg:670.90ms
step:318/3000 train_loss:3.9831 train_time:206636ms step_avg:670.90ms
step:319/3000 train_loss:3.9065 train_time:207304ms step_avg:670.89ms
step:320/3000 train_loss:3.9355 train_time:207974ms step_avg:670.88ms
step:321/3000 train_loss:3.9834 train_time:208643ms step_avg:670.88ms
step:322/3000 train_loss:3.9990 train_time:209312ms step_avg:670.87ms
step:323/3000 train_loss:3.8690 train_time:209981ms step_avg:670.87ms
step:324/3000 train_loss:3.9529 train_time:210651ms step_avg:670.86ms
step:325/3000 train_loss:4.0045 train_time:211318ms step_avg:670.85ms
step:326/3000 train_loss:4.0173 train_time:211987ms step_avg:670.84ms
step:327/3000 train_loss:3.9780 train_time:212655ms step_avg:670.84ms
step:328/3000 train_loss:4.2431 train_time:213324ms step_avg:670.83ms
step:329/3000 train_loss:4.0167 train_time:213994ms step_avg:670.83ms
step:330/3000 train_loss:4.0556 train_time:214662ms step_avg:670.82ms
step:331/3000 train_loss:3.9194 train_time:215332ms step_avg:670.82ms
step:332/3000 train_loss:4.5026 train_time:216002ms step_avg:670.81ms
step:333/3000 train_loss:3.9158 train_time:216671ms step_avg:670.81ms
step:334/3000 train_loss:3.8319 train_time:217340ms step_avg:670.80ms
step:335/3000 train_loss:3.9965 train_time:218009ms step_avg:670.80ms
step:336/3000 train_loss:4.0094 train_time:218678ms step_avg:670.79ms
step:337/3000 train_loss:4.0078 train_time:219349ms step_avg:670.79ms
step:338/3000 train_loss:3.9887 train_time:220018ms step_avg:670.79ms
step:339/3000 train_loss:4.0392 train_time:220687ms step_avg:670.78ms
step:340/3000 train_loss:3.9100 train_time:221357ms step_avg:670.78ms
step:341/3000 train_loss:3.8974 train_time:222025ms step_avg:670.77ms
step:342/3000 train_loss:3.9175 train_time:222695ms step_avg:670.77ms
step:343/3000 train_loss:3.9464 train_time:223363ms step_avg:670.76ms
step:344/3000 train_loss:4.0480 train_time:224031ms step_avg:670.75ms
step:345/3000 train_loss:3.9697 train_time:224700ms step_avg:670.75ms
step:346/3000 train_loss:3.9387 train_time:225369ms step_avg:670.74ms
step:347/3000 train_loss:3.9910 train_time:226038ms step_avg:670.74ms
step:348/3000 train_loss:3.9204 train_time:226707ms step_avg:670.73ms
step:349/3000 train_loss:3.9144 train_time:227377ms step_avg:670.73ms
step:350/3000 train_loss:3.9859 train_time:228045ms step_avg:670.72ms
step:351/3000 train_loss:4.2189 train_time:228715ms step_avg:670.72ms
step:352/3000 train_loss:3.8547 train_time:229384ms step_avg:670.71ms
step:353/3000 train_loss:3.9089 train_time:230052ms step_avg:670.71ms
step:354/3000 train_loss:3.9092 train_time:230721ms step_avg:670.70ms
step:355/3000 train_loss:3.8064 train_time:231389ms step_avg:670.69ms
step:356/3000 train_loss:3.9537 train_time:232060ms step_avg:670.70ms
step:357/3000 train_loss:3.8699 train_time:232729ms step_avg:670.69ms
step:358/3000 train_loss:3.9976 train_time:233398ms step_avg:670.69ms
step:359/3000 train_loss:3.9762 train_time:234068ms step_avg:670.68ms
step:360/3000 train_loss:3.8949 train_time:234736ms step_avg:670.68ms
step:361/3000 train_loss:4.1617 train_time:235404ms step_avg:670.67ms
step:362/3000 train_loss:4.1435 train_time:236074ms step_avg:670.66ms
step:363/3000 train_loss:3.9585 train_time:236743ms step_avg:670.66ms
step:364/3000 train_loss:3.8517 train_time:237412ms step_avg:670.65ms
step:365/3000 train_loss:4.0317 train_time:238081ms step_avg:670.65ms
step:366/3000 train_loss:3.8654 train_time:238749ms step_avg:670.64ms
step:367/3000 train_loss:3.9607 train_time:239418ms step_avg:670.64ms
step:368/3000 train_loss:3.8867 train_time:240087ms step_avg:670.63ms
step:369/3000 train_loss:3.9773 train_time:240756ms step_avg:670.63ms
step:370/3000 train_loss:3.9501 train_time:241425ms step_avg:670.63ms
step:371/3000 train_loss:3.7562 train_time:242095ms step_avg:670.62ms
step:372/3000 train_loss:3.8777 train_time:242765ms step_avg:670.62ms
step:373/3000 train_loss:3.9388 train_time:243435ms step_avg:670.62ms
step:374/3000 train_loss:3.8964 train_time:244104ms step_avg:670.62ms
step:375/3000 train_loss:3.9830 train_time:244773ms step_avg:670.61ms
step:375/3000 val_loss:3.9183 train_time:244784ms step_avg:670.64ms
step:376/3000 train_loss:3.8979 train_time:245444ms step_avg:670.61ms
step:377/3000 train_loss:3.7990 train_time:246116ms step_avg:670.62ms
step:378/3000 train_loss:3.3900 train_time:246785ms step_avg:670.61ms
step:379/3000 train_loss:3.7658 train_time:247455ms step_avg:670.61ms
step:380/3000 train_loss:3.7753 train_time:248130ms step_avg:670.62ms
step:381/3000 train_loss:3.8816 train_time:248799ms step_avg:670.62ms
step:382/3000 train_loss:3.9344 train_time:249468ms step_avg:670.61ms
step:383/3000 train_loss:3.9017 train_time:250138ms step_avg:670.61ms
step:384/3000 train_loss:3.8841 train_time:250807ms step_avg:670.61ms
step:385/3000 train_loss:3.9599 train_time:251477ms step_avg:670.61ms
step:386/3000 train_loss:3.8852 train_time:252146ms step_avg:670.60ms
step:387/3000 train_loss:3.9826 train_time:252815ms step_avg:670.60ms
step:388/3000 train_loss:4.1592 train_time:253484ms step_avg:670.59ms
step:389/3000 train_loss:3.8868 train_time:254153ms step_avg:670.59ms
step:390/3000 train_loss:3.8798 train_time:254824ms step_avg:670.59ms
step:391/3000 train_loss:3.9856 train_time:255492ms step_avg:670.58ms
step:392/3000 train_loss:3.9048 train_time:256162ms step_avg:670.58ms
step:393/3000 train_loss:4.0135 train_time:256831ms step_avg:670.58ms
step:394/3000 train_loss:3.8458 train_time:257500ms step_avg:670.57ms
step:395/3000 train_loss:3.9798 train_time:258169ms step_avg:670.57ms
step:396/3000 train_loss:3.7235 train_time:258838ms step_avg:670.56ms
step:397/3000 train_loss:3.9362 train_time:259506ms step_avg:670.56ms
step:398/3000 train_loss:3.9674 train_time:260175ms step_avg:670.55ms
step:399/3000 train_loss:3.9798 train_time:260843ms step_avg:670.55ms
step:400/3000 train_loss:3.8703 train_time:261511ms step_avg:670.54ms
step:401/3000 train_loss:3.9284 train_time:262179ms step_avg:670.54ms
step:402/3000 train_loss:4.0169 train_time:262849ms step_avg:670.53ms
step:403/3000 train_loss:3.9378 train_time:263518ms step_avg:670.53ms
step:404/3000 train_loss:4.0565 train_time:264187ms step_avg:670.52ms
step:405/3000 train_loss:3.7929 train_time:264855ms step_avg:670.52ms
step:406/3000 train_loss:3.8909 train_time:265524ms step_avg:670.51ms
step:407/3000 train_loss:4.1995 train_time:266194ms step_avg:670.51ms
step:408/3000 train_loss:3.8842 train_time:266862ms step_avg:670.51ms
step:409/3000 train_loss:3.9187 train_time:267530ms step_avg:670.50ms
step:410/3000 train_loss:3.9578 train_time:268200ms step_avg:670.50ms
step:411/3000 train_loss:3.8496 train_time:268870ms step_avg:670.50ms
step:412/3000 train_loss:3.8653 train_time:269540ms step_avg:670.50ms
step:413/3000 train_loss:4.2926 train_time:270209ms step_avg:670.49ms
step:414/3000 train_loss:3.7498 train_time:270878ms step_avg:670.49ms
step:415/3000 train_loss:4.1119 train_time:271548ms step_avg:670.49ms
step:416/3000 train_loss:3.8511 train_time:272217ms step_avg:670.48ms
step:417/3000 train_loss:3.8685 train_time:272886ms step_avg:670.48ms
step:418/3000 train_loss:4.0436 train_time:273555ms step_avg:670.48ms
step:419/3000 train_loss:3.7799 train_time:274224ms step_avg:670.48ms
step:420/3000 train_loss:3.9057 train_time:274893ms step_avg:670.47ms
step:421/3000 train_loss:3.8065 train_time:275561ms step_avg:670.47ms
step:422/3000 train_loss:3.7470 train_time:276230ms step_avg:670.46ms
step:423/3000 train_loss:3.8820 train_time:276899ms step_avg:670.46ms
step:424/3000 train_loss:3.9670 train_time:277568ms step_avg:670.45ms
step:425/3000 train_loss:3.7219 train_time:278237ms step_avg:670.45ms
step:426/3000 train_loss:3.9087 train_time:278907ms step_avg:670.45ms
step:427/3000 train_loss:3.7688 train_time:279576ms step_avg:670.45ms
step:428/3000 train_loss:3.9979 train_time:280246ms step_avg:670.44ms
step:429/3000 train_loss:3.9156 train_time:280916ms step_avg:670.44ms
step:430/3000 train_loss:3.8534 train_time:281584ms step_avg:670.44ms
step:431/3000 train_loss:3.8182 train_time:282252ms step_avg:670.43ms
step:432/3000 train_loss:3.7276 train_time:282921ms step_avg:670.43ms
step:433/3000 train_loss:3.8641 train_time:283590ms step_avg:670.43ms
step:434/3000 train_loss:3.9207 train_time:284259ms step_avg:670.42ms
step:435/3000 train_loss:3.8708 train_time:284928ms step_avg:670.42ms
step:436/3000 train_loss:3.9122 train_time:285599ms step_avg:670.42ms
step:437/3000 train_loss:3.9307 train_time:286268ms step_avg:670.42ms
step:438/3000 train_loss:3.8079 train_time:286937ms step_avg:670.41ms
step:439/3000 train_loss:3.8202 train_time:287606ms step_avg:670.41ms
step:440/3000 train_loss:3.8126 train_time:288276ms step_avg:670.41ms
step:441/3000 train_loss:3.9881 train_time:288944ms step_avg:670.40ms
step:442/3000 train_loss:3.8629 train_time:289614ms step_avg:670.40ms
step:443/3000 train_loss:3.8463 train_time:290282ms step_avg:670.40ms
step:444/3000 train_loss:3.7436 train_time:290950ms step_avg:670.39ms
step:445/3000 train_loss:4.0341 train_time:291620ms step_avg:670.39ms
step:446/3000 train_loss:3.9485 train_time:292288ms step_avg:670.39ms
step:447/3000 train_loss:3.9339 train_time:292957ms step_avg:670.38ms
step:448/3000 train_loss:3.8547 train_time:293627ms step_avg:670.38ms
step:449/3000 train_loss:3.9671 train_time:294295ms step_avg:670.38ms
step:450/3000 train_loss:3.7947 train_time:294964ms step_avg:670.37ms
step:451/3000 train_loss:3.8330 train_time:295634ms step_avg:670.37ms
step:452/3000 train_loss:3.6766 train_time:296303ms step_avg:670.37ms
step:453/3000 train_loss:3.8093 train_time:296971ms step_avg:670.36ms
step:454/3000 train_loss:3.7860 train_time:297640ms step_avg:670.36ms
step:455/3000 train_loss:3.7406 train_time:298308ms step_avg:670.36ms
step:456/3000 train_loss:3.9567 train_time:298977ms step_avg:670.35ms
step:457/3000 train_loss:3.8372 train_time:299647ms step_avg:670.35ms
step:458/3000 train_loss:3.9043 train_time:300316ms step_avg:670.35ms
step:459/3000 train_loss:3.9404 train_time:300985ms step_avg:670.35ms
step:460/3000 train_loss:3.7411 train_time:301654ms step_avg:670.34ms
step:461/3000 train_loss:3.9031 train_time:302323ms step_avg:670.34ms
step:462/3000 train_loss:3.8028 train_time:302992ms step_avg:670.34ms
step:463/3000 train_loss:3.8292 train_time:303662ms step_avg:670.33ms
step:464/3000 train_loss:3.8882 train_time:304330ms step_avg:670.33ms
step:465/3000 train_loss:3.8288 train_time:305000ms step_avg:670.33ms
step:466/3000 train_loss:3.8329 train_time:305668ms step_avg:670.33ms
step:467/3000 train_loss:3.9166 train_time:306337ms step_avg:670.32ms
step:468/3000 train_loss:3.9335 train_time:307006ms step_avg:670.32ms
step:469/3000 train_loss:3.9124 train_time:307676ms step_avg:670.32ms
step:470/3000 train_loss:3.7953 train_time:308345ms step_avg:670.31ms
step:471/3000 train_loss:3.8720 train_time:309013ms step_avg:670.31ms
step:472/3000 train_loss:3.9364 train_time:309682ms step_avg:670.31ms
step:473/3000 train_loss:3.8880 train_time:310352ms step_avg:670.31ms
step:474/3000 train_loss:3.8287 train_time:311021ms step_avg:670.30ms
step:475/3000 train_loss:3.6990 train_time:311692ms step_avg:670.31ms
step:476/3000 train_loss:4.1268 train_time:312361ms step_avg:670.30ms
step:477/3000 train_loss:3.8815 train_time:313031ms step_avg:670.30ms
step:478/3000 train_loss:3.6988 train_time:313699ms step_avg:670.30ms
step:479/3000 train_loss:3.9281 train_time:314369ms step_avg:670.30ms
step:480/3000 train_loss:3.8847 train_time:315037ms step_avg:670.29ms
step:481/3000 train_loss:4.0282 train_time:315707ms step_avg:670.29ms
step:482/3000 train_loss:3.8419 train_time:316375ms step_avg:670.29ms
step:483/3000 train_loss:3.6446 train_time:317045ms step_avg:670.29ms
step:484/3000 train_loss:3.9203 train_time:317714ms step_avg:670.28ms
step:485/3000 train_loss:3.7771 train_time:318383ms step_avg:670.28ms
step:486/3000 train_loss:3.7860 train_time:319052ms step_avg:670.28ms
step:487/3000 train_loss:3.7164 train_time:319722ms step_avg:670.28ms
step:488/3000 train_loss:3.7848 train_time:320392ms step_avg:670.28ms
step:489/3000 train_loss:3.9837 train_time:321063ms step_avg:670.28ms
step:490/3000 train_loss:3.8331 train_time:321732ms step_avg:670.27ms
step:491/3000 train_loss:3.7156 train_time:322400ms step_avg:670.27ms
step:492/3000 train_loss:3.7319 train_time:323069ms step_avg:670.27ms
step:493/3000 train_loss:3.8491 train_time:323738ms step_avg:670.27ms
step:494/3000 train_loss:3.6915 train_time:324407ms step_avg:670.26ms
step:495/3000 train_loss:3.8271 train_time:325076ms step_avg:670.26ms
step:496/3000 train_loss:3.7777 train_time:325744ms step_avg:670.26ms
step:497/3000 train_loss:3.6465 train_time:326413ms step_avg:670.25ms
step:498/3000 train_loss:3.8471 train_time:327082ms step_avg:670.25ms
step:499/3000 train_loss:3.9172 train_time:327751ms step_avg:670.25ms
step:500/3000 train_loss:3.9466 train_time:328420ms step_avg:670.25ms
step:500/3000 val_loss:3.8225 train_time:328432ms step_avg:670.27ms
step:501/3000 train_loss:3.8637 train_time:329094ms step_avg:670.25ms
step:502/3000 train_loss:3.9153 train_time:329764ms step_avg:670.25ms
step:503/3000 train_loss:3.8693 train_time:330432ms step_avg:670.25ms
step:504/3000 train_loss:3.8962 train_time:331101ms step_avg:670.25ms
step:505/3000 train_loss:3.8360 train_time:331771ms step_avg:670.24ms
step:506/3000 train_loss:3.9397 train_time:332440ms step_avg:670.24ms
step:507/3000 train_loss:3.7675 train_time:333110ms step_avg:670.24ms
step:508/3000 train_loss:3.8791 train_time:333780ms step_avg:670.24ms
step:509/3000 train_loss:3.9474 train_time:334449ms step_avg:670.24ms
step:510/3000 train_loss:3.8890 train_time:335119ms step_avg:670.24ms
step:511/3000 train_loss:3.7083 train_time:335787ms step_avg:670.23ms
step:512/3000 train_loss:3.9011 train_time:336456ms step_avg:670.23ms
step:513/3000 train_loss:3.8406 train_time:337125ms step_avg:670.23ms
step:514/3000 train_loss:3.7982 train_time:337793ms step_avg:670.22ms
step:515/3000 train_loss:3.8940 train_time:338462ms step_avg:670.22ms
step:516/3000 train_loss:3.8532 train_time:339131ms step_avg:670.22ms
step:517/3000 train_loss:4.2184 train_time:339804ms step_avg:670.23ms
step:518/3000 train_loss:3.8131 train_time:340473ms step_avg:670.22ms
step:519/3000 train_loss:3.9124 train_time:341142ms step_avg:670.22ms
step:520/3000 train_loss:3.7961 train_time:341811ms step_avg:670.22ms
step:521/3000 train_loss:3.8182 train_time:342480ms step_avg:670.22ms
step:522/3000 train_loss:3.7691 train_time:343149ms step_avg:670.21ms
step:523/3000 train_loss:3.7799 train_time:343817ms step_avg:670.21ms
step:524/3000 train_loss:4.4197 train_time:344486ms step_avg:670.21ms
step:525/3000 train_loss:3.8699 train_time:345155ms step_avg:670.20ms
step:526/3000 train_loss:3.8042 train_time:345824ms step_avg:670.20ms
step:527/3000 train_loss:3.8199 train_time:346492ms step_avg:670.20ms
step:528/3000 train_loss:3.7786 train_time:347161ms step_avg:670.19ms
step:529/3000 train_loss:3.7514 train_time:347829ms step_avg:670.19ms
step:530/3000 train_loss:3.9686 train_time:348499ms step_avg:670.19ms
step:531/3000 train_loss:3.7692 train_time:349167ms step_avg:670.19ms
step:532/3000 train_loss:4.0386 train_time:349836ms step_avg:670.18ms
step:533/3000 train_loss:3.8580 train_time:350505ms step_avg:670.18ms
step:534/3000 train_loss:3.7848 train_time:351175ms step_avg:670.18ms
step:535/3000 train_loss:3.8054 train_time:351843ms step_avg:670.18ms
step:536/3000 train_loss:3.7344 train_time:352513ms step_avg:670.18ms
step:537/3000 train_loss:3.8692 train_time:353181ms step_avg:670.17ms
step:538/3000 train_loss:3.8575 train_time:353850ms step_avg:670.17ms
step:539/3000 train_loss:3.7494 train_time:354520ms step_avg:670.17ms
step:540/3000 train_loss:4.2439 train_time:355189ms step_avg:670.17ms
step:541/3000 train_loss:3.7881 train_time:355858ms step_avg:670.17ms
step:542/3000 train_loss:3.9053 train_time:356527ms step_avg:670.16ms
step:543/3000 train_loss:3.7290 train_time:357196ms step_avg:670.16ms
step:544/3000 train_loss:3.7111 train_time:357865ms step_avg:670.16ms
step:545/3000 train_loss:3.7826 train_time:358535ms step_avg:670.16ms
step:546/3000 train_loss:3.7072 train_time:359204ms step_avg:670.16ms
step:547/3000 train_loss:3.7661 train_time:359872ms step_avg:670.15ms
step:548/3000 train_loss:3.7669 train_time:360541ms step_avg:670.15ms
step:549/3000 train_loss:3.7456 train_time:361210ms step_avg:670.15ms
step:550/3000 train_loss:3.8515 train_time:361877ms step_avg:670.14ms
step:551/3000 train_loss:3.7312 train_time:362545ms step_avg:670.14ms
step:552/3000 train_loss:3.7569 train_time:363214ms step_avg:670.14ms
step:553/3000 train_loss:4.0837 train_time:363884ms step_avg:670.14ms
step:554/3000 train_loss:3.8821 train_time:364553ms step_avg:670.13ms
step:555/3000 train_loss:3.8322 train_time:365221ms step_avg:670.13ms
step:556/3000 train_loss:3.7699 train_time:365892ms step_avg:670.13ms
step:557/3000 train_loss:3.8208 train_time:366560ms step_avg:670.13ms
step:558/3000 train_loss:3.4511 train_time:367229ms step_avg:670.13ms
step:559/3000 train_loss:3.7307 train_time:367898ms step_avg:670.12ms
step:560/3000 train_loss:3.7751 train_time:368567ms step_avg:670.12ms
step:561/3000 train_loss:3.8223 train_time:369235ms step_avg:670.12ms
step:562/3000 train_loss:3.7388 train_time:369903ms step_avg:670.11ms
step:563/3000 train_loss:3.6718 train_time:370571ms step_avg:670.11ms
step:564/3000 train_loss:3.8901 train_time:371239ms step_avg:670.11ms
step:565/3000 train_loss:3.6936 train_time:371908ms step_avg:670.10ms
step:566/3000 train_loss:3.8121 train_time:372578ms step_avg:670.10ms
step:567/3000 train_loss:3.7539 train_time:373247ms step_avg:670.10ms
step:568/3000 train_loss:3.7236 train_time:373915ms step_avg:670.10ms
step:569/3000 train_loss:3.8045 train_time:374584ms step_avg:670.10ms
step:570/3000 train_loss:3.7778 train_time:375253ms step_avg:670.10ms
step:571/3000 train_loss:3.8062 train_time:375922ms step_avg:670.09ms
step:572/3000 train_loss:3.7016 train_time:376774ms step_avg:670.42ms
step:573/3000 train_loss:3.6503 train_time:377444ms step_avg:670.42ms
step:574/3000 train_loss:3.6588 train_time:378113ms step_avg:670.41ms
step:575/3000 train_loss:4.1359 train_time:378783ms step_avg:670.41ms
step:576/3000 train_loss:3.6812 train_time:379450ms step_avg:670.41ms
step:577/3000 train_loss:3.7574 train_time:380119ms step_avg:670.40ms
step:578/3000 train_loss:3.9055 train_time:380788ms step_avg:670.40ms
step:579/3000 train_loss:3.7019 train_time:381457ms step_avg:670.40ms
step:580/3000 train_loss:3.7398 train_time:382125ms step_avg:670.39ms
step:581/3000 train_loss:3.8475 train_time:382793ms step_avg:670.39ms
step:582/3000 train_loss:3.8606 train_time:383462ms step_avg:670.39ms
step:583/3000 train_loss:3.7961 train_time:384131ms step_avg:670.39ms
step:584/3000 train_loss:3.9819 train_time:384800ms step_avg:670.38ms
step:585/3000 train_loss:3.7659 train_time:385470ms step_avg:670.38ms
step:586/3000 train_loss:3.7739 train_time:386139ms step_avg:670.38ms
step:587/3000 train_loss:3.8111 train_time:386808ms step_avg:670.38ms
step:588/3000 train_loss:4.2069 train_time:387478ms step_avg:670.38ms
step:589/3000 train_loss:3.6375 train_time:388147ms step_avg:670.37ms
step:590/3000 train_loss:4.0017 train_time:388815ms step_avg:670.37ms
step:591/3000 train_loss:3.8175 train_time:389484ms step_avg:670.37ms
step:592/3000 train_loss:3.6573 train_time:390152ms step_avg:670.36ms
step:593/3000 train_loss:3.7575 train_time:390821ms step_avg:670.36ms
step:594/3000 train_loss:3.8580 train_time:391490ms step_avg:670.36ms
step:595/3000 train_loss:3.7394 train_time:392159ms step_avg:670.36ms
step:596/3000 train_loss:3.7684 train_time:392829ms step_avg:670.36ms
step:597/3000 train_loss:3.7962 train_time:393498ms step_avg:670.35ms
step:598/3000 train_loss:3.8093 train_time:394167ms step_avg:670.35ms
step:599/3000 train_loss:3.8146 train_time:394837ms step_avg:670.35ms
step:600/3000 train_loss:3.9225 train_time:395506ms step_avg:670.35ms
step:601/3000 train_loss:3.7749 train_time:396176ms step_avg:670.35ms
step:602/3000 train_loss:3.7284 train_time:396845ms step_avg:670.35ms
step:603/3000 train_loss:3.8803 train_time:397515ms step_avg:670.34ms
step:604/3000 train_loss:3.6872 train_time:398183ms step_avg:670.34ms
step:605/3000 train_loss:3.7961 train_time:398854ms step_avg:670.34ms
step:606/3000 train_loss:4.0961 train_time:399528ms step_avg:670.35ms
step:607/3000 train_loss:3.8269 train_time:400198ms step_avg:670.35ms
step:608/3000 train_loss:3.8323 train_time:400866ms step_avg:670.34ms
step:609/3000 train_loss:3.7847 train_time:401535ms step_avg:670.34ms
step:610/3000 train_loss:3.6964 train_time:402204ms step_avg:670.34ms
step:611/3000 train_loss:3.7773 train_time:402874ms step_avg:670.34ms
step:612/3000 train_loss:3.7297 train_time:403543ms step_avg:670.34ms
step:613/3000 train_loss:3.8593 train_time:404213ms step_avg:670.34ms
step:614/3000 train_loss:3.6850 train_time:404882ms step_avg:670.33ms
step:615/3000 train_loss:3.7382 train_time:405552ms step_avg:670.33ms
step:616/3000 train_loss:3.7062 train_time:406221ms step_avg:670.33ms
step:617/3000 train_loss:3.7678 train_time:406890ms step_avg:670.33ms
step:618/3000 train_loss:3.8273 train_time:407560ms step_avg:670.33ms
step:619/3000 train_loss:3.5967 train_time:408229ms step_avg:670.33ms
step:620/3000 train_loss:3.8833 train_time:408898ms step_avg:670.32ms
step:621/3000 train_loss:3.7827 train_time:409568ms step_avg:670.32ms
step:622/3000 train_loss:3.7848 train_time:410236ms step_avg:670.32ms
step:623/3000 train_loss:3.6049 train_time:410906ms step_avg:670.32ms
step:624/3000 train_loss:3.7176 train_time:411576ms step_avg:670.32ms
step:625/3000 train_loss:3.8358 train_time:412245ms step_avg:670.32ms
step:625/3000 val_loss:3.7507 train_time:412257ms step_avg:670.34ms
step:626/3000 train_loss:3.7776 train_time:412916ms step_avg:670.32ms
step:627/3000 train_loss:3.7454 train_time:413586ms step_avg:670.32ms
step:628/3000 train_loss:3.6598 train_time:414254ms step_avg:670.31ms
step:629/3000 train_loss:3.7947 train_time:414924ms step_avg:670.31ms
step:630/3000 train_loss:3.6377 train_time:415595ms step_avg:670.31ms
