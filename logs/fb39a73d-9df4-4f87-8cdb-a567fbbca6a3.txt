====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.zeros(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 2900 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]+[raw_model.v_skip_weights]
optimizer3 = Muon(matrix_params, lr=0.04, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    print(f"{logfile = }")
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # log lambdas
        if master_process:
            skip_weights_str = str(model.module.skip_weights)
            v_skip_weights_str = str(model.module.v_skip_weights)
            print(f"{skip_weights_str = }")
            print(f"{v_skip_weights_str = }")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 16:41:43 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   31C    P0             68W /  400W |    3337MiB /  81920MiB |      7%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   34C    P0             88W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   31C    P0             75W /  400W |    3407MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   34C    P0             73W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2900 val_loss:10.8258 train_time:268ms step_avg:nanms
step:1/2900 train_loss:10.8258 train_time:16812ms step_avg:nanms
step:2/2900 train_loss:10.1704 train_time:17448ms step_avg:nanms
step:3/2900 train_loss:8.4736 train_time:18113ms step_avg:nanms
step:4/2900 train_loss:7.6502 train_time:18779ms step_avg:nanms
step:5/2900 train_loss:7.3917 train_time:19447ms step_avg:nanms
step:6/2900 train_loss:7.0406 train_time:20115ms step_avg:nanms
step:7/2900 train_loss:7.2311 train_time:20783ms step_avg:nanms
step:8/2900 train_loss:6.8236 train_time:21450ms step_avg:nanms
step:9/2900 train_loss:6.6763 train_time:22121ms step_avg:nanms
step:10/2900 train_loss:6.6015 train_time:22790ms step_avg:nanms
step:11/2900 train_loss:6.5879 train_time:658ms step_avg:nanms
step:12/2900 train_loss:6.4631 train_time:1327ms step_avg:nanms
step:13/2900 train_loss:6.3844 train_time:1997ms step_avg:665.58ms
step:14/2900 train_loss:6.3455 train_time:2665ms step_avg:666.35ms
step:15/2900 train_loss:6.2999 train_time:3335ms step_avg:667.06ms
step:16/2900 train_loss:6.2840 train_time:4006ms step_avg:667.59ms
step:17/2900 train_loss:6.3682 train_time:4676ms step_avg:668.03ms
step:18/2900 train_loss:6.1801 train_time:5347ms step_avg:668.36ms
step:19/2900 train_loss:6.1758 train_time:6018ms step_avg:668.62ms
step:20/2900 train_loss:5.8977 train_time:6688ms step_avg:668.81ms
step:21/2900 train_loss:6.1824 train_time:7358ms step_avg:668.91ms
step:22/2900 train_loss:6.4219 train_time:8030ms step_avg:669.19ms
step:23/2900 train_loss:6.0839 train_time:8702ms step_avg:669.39ms
step:24/2900 train_loss:6.2386 train_time:9374ms step_avg:669.54ms
step:25/2900 train_loss:5.9267 train_time:10045ms step_avg:669.63ms
step:26/2900 train_loss:5.8465 train_time:10716ms step_avg:669.73ms
step:27/2900 train_loss:6.0386 train_time:11387ms step_avg:669.84ms
step:28/2900 train_loss:5.6920 train_time:12060ms step_avg:670.01ms
step:29/2900 train_loss:5.9507 train_time:12732ms step_avg:670.08ms
step:30/2900 train_loss:5.7641 train_time:13404ms step_avg:670.22ms
step:31/2900 train_loss:5.7180 train_time:14076ms step_avg:670.29ms
step:32/2900 train_loss:5.5858 train_time:14750ms step_avg:670.44ms
step:33/2900 train_loss:5.8741 train_time:15421ms step_avg:670.47ms
step:34/2900 train_loss:5.7979 train_time:16094ms step_avg:670.58ms
step:35/2900 train_loss:5.9393 train_time:16767ms step_avg:670.68ms
step:36/2900 train_loss:5.8715 train_time:17440ms step_avg:670.78ms
step:37/2900 train_loss:5.7383 train_time:18114ms step_avg:670.88ms
step:38/2900 train_loss:5.6182 train_time:18786ms step_avg:670.94ms
step:39/2900 train_loss:5.6149 train_time:19460ms step_avg:671.02ms
step:40/2900 train_loss:5.5379 train_time:20133ms step_avg:671.10ms
step:41/2900 train_loss:5.5288 train_time:20806ms step_avg:671.16ms
step:42/2900 train_loss:5.4343 train_time:21480ms step_avg:671.25ms
step:43/2900 train_loss:5.5399 train_time:22153ms step_avg:671.31ms
step:44/2900 train_loss:5.5045 train_time:22827ms step_avg:671.39ms
step:45/2900 train_loss:5.6399 train_time:23500ms step_avg:671.44ms
step:46/2900 train_loss:5.4510 train_time:24174ms step_avg:671.50ms
step:47/2900 train_loss:5.3097 train_time:24850ms step_avg:671.61ms
step:48/2900 train_loss:5.4963 train_time:25523ms step_avg:671.65ms
step:49/2900 train_loss:5.4020 train_time:26196ms step_avg:671.70ms
step:50/2900 train_loss:5.5189 train_time:26870ms step_avg:671.76ms
step:51/2900 train_loss:5.4055 train_time:27544ms step_avg:671.81ms
step:52/2900 train_loss:5.2645 train_time:28219ms step_avg:671.88ms
step:53/2900 train_loss:5.3885 train_time:28893ms step_avg:671.93ms
step:54/2900 train_loss:5.2426 train_time:29567ms step_avg:671.98ms
step:55/2900 train_loss:5.6345 train_time:30241ms step_avg:672.01ms
step:56/2900 train_loss:5.2591 train_time:30915ms step_avg:672.07ms
step:57/2900 train_loss:5.1114 train_time:31588ms step_avg:672.09ms
step:58/2900 train_loss:5.2453 train_time:32262ms step_avg:672.12ms
step:59/2900 train_loss:5.2429 train_time:32935ms step_avg:672.15ms
step:60/2900 train_loss:5.3474 train_time:33610ms step_avg:672.20ms
step:61/2900 train_loss:5.0859 train_time:34284ms step_avg:672.24ms
step:62/2900 train_loss:5.2033 train_time:34958ms step_avg:672.27ms
step:63/2900 train_loss:5.1919 train_time:35632ms step_avg:672.30ms
step:64/2900 train_loss:5.1992 train_time:36307ms step_avg:672.34ms
step:65/2900 train_loss:5.0128 train_time:36980ms step_avg:672.36ms
step:66/2900 train_loss:5.1392 train_time:37655ms step_avg:672.40ms
step:67/2900 train_loss:5.0253 train_time:38329ms step_avg:672.44ms
step:68/2900 train_loss:5.3111 train_time:39002ms step_avg:672.45ms
step:69/2900 train_loss:4.9368 train_time:39674ms step_avg:672.45ms
step:70/2900 train_loss:5.0106 train_time:40348ms step_avg:672.47ms
step:71/2900 train_loss:5.1922 train_time:41023ms step_avg:672.50ms
step:72/2900 train_loss:5.1103 train_time:41696ms step_avg:672.52ms
step:73/2900 train_loss:4.9674 train_time:42370ms step_avg:672.54ms
step:74/2900 train_loss:5.1014 train_time:43045ms step_avg:672.57ms
step:75/2900 train_loss:5.0777 train_time:43718ms step_avg:672.58ms
step:76/2900 train_loss:5.0022 train_time:44392ms step_avg:672.61ms
step:77/2900 train_loss:5.1182 train_time:45066ms step_avg:672.62ms
step:78/2900 train_loss:5.2925 train_time:45739ms step_avg:672.63ms
step:79/2900 train_loss:5.0316 train_time:46414ms step_avg:672.66ms
step:80/2900 train_loss:5.0569 train_time:47087ms step_avg:672.67ms
step:81/2900 train_loss:4.8357 train_time:47761ms step_avg:672.69ms
step:82/2900 train_loss:5.0192 train_time:48436ms step_avg:672.72ms
step:83/2900 train_loss:4.9737 train_time:49109ms step_avg:672.72ms
step:84/2900 train_loss:4.9709 train_time:49784ms step_avg:672.75ms
step:85/2900 train_loss:4.8110 train_time:50457ms step_avg:672.77ms
step:86/2900 train_loss:5.0217 train_time:51131ms step_avg:672.78ms
step:87/2900 train_loss:4.9324 train_time:51805ms step_avg:672.79ms
step:88/2900 train_loss:4.9736 train_time:52478ms step_avg:672.79ms
step:89/2900 train_loss:4.9285 train_time:53153ms step_avg:672.82ms
step:90/2900 train_loss:4.8459 train_time:53827ms step_avg:672.83ms
step:91/2900 train_loss:4.8411 train_time:54500ms step_avg:672.84ms
step:92/2900 train_loss:4.9897 train_time:55174ms step_avg:672.85ms
step:93/2900 train_loss:4.7960 train_time:55847ms step_avg:672.86ms
step:94/2900 train_loss:4.8168 train_time:56520ms step_avg:672.86ms
step:95/2900 train_loss:4.8654 train_time:57193ms step_avg:672.86ms
step:96/2900 train_loss:4.7592 train_time:57867ms step_avg:672.87ms
step:97/2900 train_loss:4.8317 train_time:58541ms step_avg:672.89ms
step:98/2900 train_loss:4.7558 train_time:59214ms step_avg:672.89ms
step:99/2900 train_loss:4.8531 train_time:59889ms step_avg:672.90ms
step:100/2900 train_loss:4.8611 train_time:60561ms step_avg:672.90ms
step:101/2900 train_loss:4.7495 train_time:61235ms step_avg:672.91ms
step:102/2900 train_loss:4.8601 train_time:61908ms step_avg:672.91ms
step:103/2900 train_loss:4.7436 train_time:62582ms step_avg:672.92ms
step:104/2900 train_loss:4.6832 train_time:63256ms step_avg:672.93ms
step:105/2900 train_loss:4.7127 train_time:63929ms step_avg:672.93ms
step:106/2900 train_loss:4.8035 train_time:64602ms step_avg:672.94ms
step:107/2900 train_loss:4.6665 train_time:65276ms step_avg:672.95ms
step:108/2900 train_loss:4.5096 train_time:65951ms step_avg:672.97ms
step:109/2900 train_loss:4.6499 train_time:66624ms step_avg:672.97ms
step:110/2900 train_loss:4.6235 train_time:67298ms step_avg:672.98ms
step:111/2900 train_loss:4.5796 train_time:67971ms step_avg:672.98ms
step:112/2900 train_loss:4.7203 train_time:68645ms step_avg:672.99ms
step:113/2900 train_loss:4.6170 train_time:69318ms step_avg:672.99ms
step:114/2900 train_loss:4.4729 train_time:69991ms step_avg:672.99ms
step:115/2900 train_loss:4.6243 train_time:70666ms step_avg:673.01ms
step:116/2900 train_loss:4.5881 train_time:71340ms step_avg:673.02ms
step:117/2900 train_loss:4.4877 train_time:72013ms step_avg:673.02ms
step:118/2900 train_loss:4.7058 train_time:72688ms step_avg:673.03ms
step:119/2900 train_loss:4.5950 train_time:73361ms step_avg:673.04ms
step:120/2900 train_loss:4.4691 train_time:74035ms step_avg:673.04ms
step:121/2900 train_loss:4.4195 train_time:74709ms step_avg:673.05ms
step:122/2900 train_loss:4.5586 train_time:75382ms step_avg:673.05ms
step:123/2900 train_loss:4.3968 train_time:76055ms step_avg:673.06ms
step:124/2900 train_loss:4.6939 train_time:76730ms step_avg:673.07ms
step:125/2900 train_loss:4.5559 train_time:77405ms step_avg:673.08ms
step:125/2900 val_loss:4.5105 train_time:77416ms step_avg:673.18ms
step:126/2900 train_loss:4.5121 train_time:78082ms step_avg:673.12ms
step:127/2900 train_loss:4.5478 train_time:78754ms step_avg:673.11ms
step:128/2900 train_loss:4.4699 train_time:79429ms step_avg:673.13ms
step:129/2900 train_loss:4.7716 train_time:80103ms step_avg:673.13ms
step:130/2900 train_loss:4.4514 train_time:80776ms step_avg:673.14ms
step:131/2900 train_loss:4.4930 train_time:81450ms step_avg:673.14ms
step:132/2900 train_loss:4.4189 train_time:82123ms step_avg:673.14ms
step:133/2900 train_loss:4.5269 train_time:82797ms step_avg:673.15ms
step:134/2900 train_loss:4.3549 train_time:83470ms step_avg:673.14ms
step:135/2900 train_loss:4.5251 train_time:84144ms step_avg:673.15ms
step:136/2900 train_loss:4.2760 train_time:84818ms step_avg:673.16ms
step:137/2900 train_loss:4.4564 train_time:85492ms step_avg:673.17ms
step:138/2900 train_loss:4.3606 train_time:86166ms step_avg:673.17ms
step:139/2900 train_loss:4.4489 train_time:86839ms step_avg:673.17ms
step:140/2900 train_loss:4.5399 train_time:87514ms step_avg:673.18ms
step:141/2900 train_loss:4.3883 train_time:88186ms step_avg:673.18ms
step:142/2900 train_loss:4.3854 train_time:88860ms step_avg:673.18ms
step:143/2900 train_loss:4.3269 train_time:89535ms step_avg:673.19ms
step:144/2900 train_loss:4.4214 train_time:90208ms step_avg:673.20ms
step:145/2900 train_loss:4.3739 train_time:90881ms step_avg:673.19ms
step:146/2900 train_loss:4.2566 train_time:91555ms step_avg:673.20ms
step:147/2900 train_loss:4.3910 train_time:92228ms step_avg:673.20ms
step:148/2900 train_loss:4.4209 train_time:92900ms step_avg:673.19ms
step:149/2900 train_loss:4.3754 train_time:93575ms step_avg:673.20ms
step:150/2900 train_loss:4.4963 train_time:94249ms step_avg:673.21ms
step:151/2900 train_loss:4.3301 train_time:94923ms step_avg:673.21ms
step:152/2900 train_loss:4.3330 train_time:95597ms step_avg:673.22ms
step:153/2900 train_loss:4.4164 train_time:96270ms step_avg:673.22ms
step:154/2900 train_loss:4.4171 train_time:96943ms step_avg:673.22ms
step:155/2900 train_loss:4.3242 train_time:97616ms step_avg:673.22ms
step:156/2900 train_loss:4.3906 train_time:98291ms step_avg:673.22ms
step:157/2900 train_loss:4.4517 train_time:98965ms step_avg:673.23ms
step:158/2900 train_loss:4.2862 train_time:99640ms step_avg:673.24ms
step:159/2900 train_loss:4.3769 train_time:100313ms step_avg:673.24ms
step:160/2900 train_loss:4.1686 train_time:100987ms step_avg:673.25ms
step:161/2900 train_loss:4.4039 train_time:101660ms step_avg:673.24ms
step:162/2900 train_loss:4.4062 train_time:102333ms step_avg:673.24ms
step:163/2900 train_loss:4.3933 train_time:103007ms step_avg:673.25ms
step:164/2900 train_loss:4.2611 train_time:103680ms step_avg:673.25ms
step:165/2900 train_loss:4.3253 train_time:104355ms step_avg:673.26ms
step:166/2900 train_loss:4.4055 train_time:105027ms step_avg:673.25ms
step:167/2900 train_loss:4.2564 train_time:105701ms step_avg:673.25ms
step:168/2900 train_loss:4.3121 train_time:106375ms step_avg:673.26ms
step:169/2900 train_loss:4.2145 train_time:107048ms step_avg:673.26ms
step:170/2900 train_loss:4.0822 train_time:107721ms step_avg:673.26ms
step:171/2900 train_loss:4.2342 train_time:108396ms step_avg:673.27ms
step:172/2900 train_loss:4.2635 train_time:109070ms step_avg:673.27ms
step:173/2900 train_loss:4.3095 train_time:109742ms step_avg:673.27ms
step:174/2900 train_loss:4.4759 train_time:110418ms step_avg:673.28ms
step:175/2900 train_loss:4.2861 train_time:111091ms step_avg:673.28ms
step:176/2900 train_loss:4.1571 train_time:111765ms step_avg:673.28ms
step:177/2900 train_loss:4.1220 train_time:112438ms step_avg:673.28ms
step:178/2900 train_loss:4.2353 train_time:113112ms step_avg:673.29ms
step:179/2900 train_loss:4.1788 train_time:113787ms step_avg:673.29ms
step:180/2900 train_loss:4.1636 train_time:114460ms step_avg:673.29ms
step:181/2900 train_loss:4.3451 train_time:115134ms step_avg:673.30ms
step:182/2900 train_loss:4.2069 train_time:115808ms step_avg:673.30ms
step:183/2900 train_loss:4.1771 train_time:116481ms step_avg:673.30ms
step:184/2900 train_loss:4.1799 train_time:117153ms step_avg:673.29ms
step:185/2900 train_loss:4.2547 train_time:117827ms step_avg:673.30ms
step:186/2900 train_loss:4.2372 train_time:118501ms step_avg:673.30ms
step:187/2900 train_loss:4.2877 train_time:119173ms step_avg:673.29ms
step:188/2900 train_loss:4.2211 train_time:119846ms step_avg:673.29ms
step:189/2900 train_loss:4.1483 train_time:120520ms step_avg:673.30ms
step:190/2900 train_loss:4.2555 train_time:121193ms step_avg:673.30ms
step:191/2900 train_loss:4.3284 train_time:122044ms step_avg:674.28ms
step:192/2900 train_loss:4.1320 train_time:122716ms step_avg:674.26ms
step:193/2900 train_loss:4.1919 train_time:123389ms step_avg:674.26ms
step:194/2900 train_loss:4.1525 train_time:124062ms step_avg:674.25ms
step:195/2900 train_loss:4.1255 train_time:124735ms step_avg:674.24ms
step:196/2900 train_loss:4.1276 train_time:125408ms step_avg:674.23ms
step:197/2900 train_loss:4.1629 train_time:126081ms step_avg:674.23ms
step:198/2900 train_loss:4.0771 train_time:126754ms step_avg:674.22ms
step:199/2900 train_loss:4.3072 train_time:127427ms step_avg:674.22ms
step:200/2900 train_loss:4.2375 train_time:128102ms step_avg:674.22ms
step:201/2900 train_loss:4.9823 train_time:128774ms step_avg:674.21ms
step:202/2900 train_loss:4.4291 train_time:129449ms step_avg:674.21ms
step:203/2900 train_loss:4.1932 train_time:130122ms step_avg:674.21ms
step:204/2900 train_loss:4.1174 train_time:130795ms step_avg:674.20ms
step:205/2900 train_loss:4.1644 train_time:131468ms step_avg:674.20ms
step:206/2900 train_loss:4.1309 train_time:132141ms step_avg:674.19ms
step:207/2900 train_loss:4.1281 train_time:132813ms step_avg:674.18ms
step:208/2900 train_loss:4.0738 train_time:133487ms step_avg:674.18ms
step:209/2900 train_loss:4.2447 train_time:134160ms step_avg:674.17ms
step:210/2900 train_loss:4.0578 train_time:134834ms step_avg:674.17ms
step:211/2900 train_loss:4.1771 train_time:135508ms step_avg:674.17ms
step:212/2900 train_loss:4.1666 train_time:136181ms step_avg:674.16ms
step:213/2900 train_loss:4.1083 train_time:136854ms step_avg:674.16ms
step:214/2900 train_loss:4.0589 train_time:137528ms step_avg:674.15ms
step:215/2900 train_loss:4.1917 train_time:138202ms step_avg:674.16ms
step:216/2900 train_loss:4.1259 train_time:138875ms step_avg:674.15ms
step:217/2900 train_loss:4.1132 train_time:139548ms step_avg:674.15ms
step:218/2900 train_loss:4.1415 train_time:140222ms step_avg:674.14ms
step:219/2900 train_loss:4.0621 train_time:140895ms step_avg:674.14ms
step:220/2900 train_loss:4.1799 train_time:141568ms step_avg:674.13ms
step:221/2900 train_loss:4.0814 train_time:142242ms step_avg:674.13ms
step:222/2900 train_loss:4.1526 train_time:142915ms step_avg:674.13ms
step:223/2900 train_loss:4.0775 train_time:143589ms step_avg:674.13ms
step:224/2900 train_loss:4.0990 train_time:144263ms step_avg:674.12ms
step:225/2900 train_loss:4.0775 train_time:144935ms step_avg:674.12ms
step:226/2900 train_loss:4.0989 train_time:145608ms step_avg:674.11ms
step:227/2900 train_loss:4.0903 train_time:146283ms step_avg:674.11ms
step:228/2900 train_loss:4.0173 train_time:146956ms step_avg:674.11ms
step:229/2900 train_loss:4.2420 train_time:147630ms step_avg:674.11ms
step:230/2900 train_loss:4.1136 train_time:148304ms step_avg:674.11ms
step:231/2900 train_loss:3.9899 train_time:148977ms step_avg:674.10ms
step:232/2900 train_loss:4.1986 train_time:149651ms step_avg:674.10ms
step:233/2900 train_loss:4.0326 train_time:150324ms step_avg:674.10ms
step:234/2900 train_loss:4.0605 train_time:150998ms step_avg:674.10ms
step:235/2900 train_loss:4.2274 train_time:151671ms step_avg:674.09ms
step:236/2900 train_loss:4.1286 train_time:152345ms step_avg:674.09ms
step:237/2900 train_loss:4.2072 train_time:153018ms step_avg:674.09ms
step:238/2900 train_loss:4.1467 train_time:153691ms step_avg:674.09ms
step:239/2900 train_loss:4.1471 train_time:154365ms step_avg:674.08ms
step:240/2900 train_loss:4.1204 train_time:155038ms step_avg:674.08ms
step:241/2900 train_loss:4.3045 train_time:155711ms step_avg:674.08ms
step:242/2900 train_loss:4.1278 train_time:156385ms step_avg:674.07ms
step:243/2900 train_loss:4.4669 train_time:157059ms step_avg:674.07ms
step:244/2900 train_loss:4.0917 train_time:157733ms step_avg:674.07ms
step:245/2900 train_loss:4.1094 train_time:158406ms step_avg:674.07ms
step:246/2900 train_loss:4.1451 train_time:159081ms step_avg:674.07ms
step:247/2900 train_loss:4.0511 train_time:159753ms step_avg:674.06ms
step:248/2900 train_loss:4.0633 train_time:160427ms step_avg:674.07ms
step:249/2900 train_loss:3.9933 train_time:161101ms step_avg:674.06ms
step:250/2900 train_loss:4.1511 train_time:161775ms step_avg:674.06ms
step:250/2900 val_loss:4.0616 train_time:161786ms step_avg:674.11ms
step:251/2900 train_loss:4.0455 train_time:162449ms step_avg:674.06ms
step:252/2900 train_loss:3.9604 train_time:163121ms step_avg:674.06ms
step:253/2900 train_loss:3.9938 train_time:163795ms step_avg:674.05ms
step:254/2900 train_loss:4.0547 train_time:164467ms step_avg:674.05ms
step:255/2900 train_loss:3.9890 train_time:165141ms step_avg:674.04ms
step:256/2900 train_loss:3.9344 train_time:165814ms step_avg:674.04ms
step:257/2900 train_loss:4.0058 train_time:166486ms step_avg:674.03ms
step:258/2900 train_loss:4.1329 train_time:167159ms step_avg:674.03ms
step:259/2900 train_loss:3.9440 train_time:167833ms step_avg:674.03ms
step:260/2900 train_loss:4.0546 train_time:168506ms step_avg:674.02ms
step:261/2900 train_loss:4.0069 train_time:169178ms step_avg:674.02ms
step:262/2900 train_loss:4.0284 train_time:169851ms step_avg:674.01ms
step:263/2900 train_loss:4.0926 train_time:170524ms step_avg:674.01ms
step:264/2900 train_loss:4.1388 train_time:171197ms step_avg:674.01ms
step:265/2900 train_loss:4.4980 train_time:171871ms step_avg:674.00ms
step:266/2900 train_loss:4.0336 train_time:172544ms step_avg:674.00ms
step:267/2900 train_loss:4.2323 train_time:173218ms step_avg:674.00ms
step:268/2900 train_loss:3.9783 train_time:173891ms step_avg:673.99ms
step:269/2900 train_loss:3.9899 train_time:174564ms step_avg:673.99ms
step:270/2900 train_loss:4.0975 train_time:175236ms step_avg:673.98ms
step:271/2900 train_loss:4.1542 train_time:175910ms step_avg:673.98ms
step:272/2900 train_loss:4.0367 train_time:176584ms step_avg:673.99ms
step:273/2900 train_loss:3.9834 train_time:177257ms step_avg:673.98ms
step:274/2900 train_loss:3.9814 train_time:177930ms step_avg:673.98ms
step:275/2900 train_loss:3.9584 train_time:178603ms step_avg:673.97ms
step:276/2900 train_loss:4.0795 train_time:179275ms step_avg:673.97ms
step:277/2900 train_loss:4.0981 train_time:179949ms step_avg:673.96ms
step:278/2900 train_loss:3.8888 train_time:180622ms step_avg:673.96ms
step:279/2900 train_loss:4.1986 train_time:181295ms step_avg:673.96ms
step:280/2900 train_loss:4.0446 train_time:181968ms step_avg:673.96ms
step:281/2900 train_loss:4.0503 train_time:182642ms step_avg:673.95ms
step:282/2900 train_loss:3.9940 train_time:183313ms step_avg:673.95ms
step:283/2900 train_loss:4.2100 train_time:183987ms step_avg:673.95ms
step:284/2900 train_loss:3.9499 train_time:184660ms step_avg:673.94ms
step:285/2900 train_loss:3.9228 train_time:185333ms step_avg:673.94ms
step:286/2900 train_loss:4.1063 train_time:186005ms step_avg:673.93ms
step:287/2900 train_loss:4.0332 train_time:186678ms step_avg:673.93ms
step:288/2900 train_loss:4.0650 train_time:187351ms step_avg:673.92ms
step:289/2900 train_loss:4.1038 train_time:188024ms step_avg:673.92ms
step:290/2900 train_loss:4.0878 train_time:188697ms step_avg:673.92ms
step:291/2900 train_loss:3.9299 train_time:189372ms step_avg:673.92ms
step:292/2900 train_loss:3.9001 train_time:190044ms step_avg:673.92ms
step:293/2900 train_loss:4.1086 train_time:190717ms step_avg:673.91ms
step:294/2900 train_loss:3.9040 train_time:191389ms step_avg:673.91ms
step:295/2900 train_loss:4.2034 train_time:192062ms step_avg:673.90ms
step:296/2900 train_loss:4.0894 train_time:192736ms step_avg:673.90ms
step:297/2900 train_loss:3.9129 train_time:193410ms step_avg:673.90ms
step:298/2900 train_loss:4.0225 train_time:194084ms step_avg:673.90ms
step:299/2900 train_loss:3.9378 train_time:194758ms step_avg:673.90ms
step:300/2900 train_loss:4.1260 train_time:195431ms step_avg:673.90ms
step:301/2900 train_loss:3.9131 train_time:196105ms step_avg:673.90ms
step:302/2900 train_loss:4.0278 train_time:196778ms step_avg:673.90ms
step:303/2900 train_loss:3.9603 train_time:197451ms step_avg:673.89ms
step:304/2900 train_loss:3.9586 train_time:198125ms step_avg:673.90ms
step:305/2900 train_loss:3.9668 train_time:198798ms step_avg:673.89ms
step:306/2900 train_loss:4.0655 train_time:199471ms step_avg:673.89ms
step:307/2900 train_loss:4.0087 train_time:200144ms step_avg:673.88ms
step:308/2900 train_loss:3.9206 train_time:200817ms step_avg:673.88ms
step:309/2900 train_loss:4.0874 train_time:201490ms step_avg:673.88ms
step:310/2900 train_loss:3.9491 train_time:202163ms step_avg:673.88ms
step:311/2900 train_loss:4.0287 train_time:202837ms step_avg:673.88ms
step:312/2900 train_loss:4.0169 train_time:203510ms step_avg:673.87ms
step:313/2900 train_loss:3.9084 train_time:204182ms step_avg:673.87ms
step:314/2900 train_loss:4.0033 train_time:204856ms step_avg:673.87ms
step:315/2900 train_loss:3.9900 train_time:205529ms step_avg:673.86ms
step:316/2900 train_loss:3.7245 train_time:206202ms step_avg:673.86ms
step:317/2900 train_loss:3.9703 train_time:206875ms step_avg:673.86ms
step:318/2900 train_loss:3.9649 train_time:207550ms step_avg:673.86ms
step:319/2900 train_loss:3.8829 train_time:208223ms step_avg:673.86ms
step:320/2900 train_loss:3.9191 train_time:208895ms step_avg:673.86ms
step:321/2900 train_loss:3.9564 train_time:209569ms step_avg:673.85ms
step:322/2900 train_loss:3.9768 train_time:210240ms step_avg:673.85ms
step:323/2900 train_loss:3.8473 train_time:210914ms step_avg:673.85ms
step:324/2900 train_loss:3.9336 train_time:211588ms step_avg:673.85ms
step:325/2900 train_loss:3.9776 train_time:212260ms step_avg:673.84ms
step:326/2900 train_loss:3.9940 train_time:212933ms step_avg:673.84ms
step:327/2900 train_loss:3.9621 train_time:213607ms step_avg:673.84ms
step:328/2900 train_loss:4.2188 train_time:214280ms step_avg:673.84ms
step:329/2900 train_loss:3.9853 train_time:214952ms step_avg:673.83ms
step:330/2900 train_loss:4.0359 train_time:215626ms step_avg:673.83ms
step:331/2900 train_loss:3.8993 train_time:216299ms step_avg:673.83ms
step:332/2900 train_loss:4.4929 train_time:216971ms step_avg:673.82ms
step:333/2900 train_loss:3.8937 train_time:217644ms step_avg:673.82ms
step:334/2900 train_loss:3.8048 train_time:218317ms step_avg:673.82ms
step:335/2900 train_loss:3.9680 train_time:218990ms step_avg:673.82ms
step:336/2900 train_loss:3.9879 train_time:219664ms step_avg:673.82ms
step:337/2900 train_loss:3.9885 train_time:220338ms step_avg:673.82ms
step:338/2900 train_loss:3.9616 train_time:221010ms step_avg:673.81ms
step:339/2900 train_loss:4.0186 train_time:221683ms step_avg:673.81ms
step:340/2900 train_loss:3.8745 train_time:222355ms step_avg:673.80ms
step:341/2900 train_loss:3.8760 train_time:223028ms step_avg:673.80ms
step:342/2900 train_loss:3.8862 train_time:223701ms step_avg:673.80ms
step:343/2900 train_loss:3.9269 train_time:224372ms step_avg:673.79ms
step:344/2900 train_loss:4.0195 train_time:225046ms step_avg:673.79ms
step:345/2900 train_loss:3.9508 train_time:225718ms step_avg:673.79ms
step:346/2900 train_loss:3.9168 train_time:226390ms step_avg:673.78ms
step:347/2900 train_loss:3.9687 train_time:227063ms step_avg:673.78ms
step:348/2900 train_loss:3.8967 train_time:227737ms step_avg:673.78ms
step:349/2900 train_loss:3.8887 train_time:228410ms step_avg:673.78ms
step:350/2900 train_loss:3.9560 train_time:229083ms step_avg:673.77ms
step:351/2900 train_loss:4.1930 train_time:229755ms step_avg:673.77ms
step:352/2900 train_loss:3.8339 train_time:230428ms step_avg:673.76ms
step:353/2900 train_loss:3.8886 train_time:231099ms step_avg:673.76ms
step:354/2900 train_loss:3.8858 train_time:231773ms step_avg:673.76ms
step:355/2900 train_loss:3.7924 train_time:232446ms step_avg:673.76ms
step:356/2900 train_loss:3.9230 train_time:233117ms step_avg:673.75ms
step:357/2900 train_loss:3.8481 train_time:233789ms step_avg:673.74ms
step:358/2900 train_loss:3.9766 train_time:234461ms step_avg:673.74ms
step:359/2900 train_loss:3.9549 train_time:235133ms step_avg:673.73ms
step:360/2900 train_loss:3.8759 train_time:235805ms step_avg:673.73ms
step:361/2900 train_loss:4.1305 train_time:236477ms step_avg:673.72ms
step:362/2900 train_loss:4.1207 train_time:237150ms step_avg:673.72ms
step:363/2900 train_loss:3.9323 train_time:237822ms step_avg:673.72ms
step:364/2900 train_loss:3.8298 train_time:238495ms step_avg:673.71ms
step:365/2900 train_loss:4.0039 train_time:239167ms step_avg:673.71ms
step:366/2900 train_loss:3.8333 train_time:239839ms step_avg:673.71ms
step:367/2900 train_loss:3.9364 train_time:240510ms step_avg:673.70ms
step:368/2900 train_loss:3.8708 train_time:241182ms step_avg:673.69ms
step:369/2900 train_loss:3.9534 train_time:241854ms step_avg:673.69ms
step:370/2900 train_loss:3.9279 train_time:242525ms step_avg:673.68ms
step:371/2900 train_loss:3.7335 train_time:243198ms step_avg:673.68ms
step:372/2900 train_loss:3.8534 train_time:243871ms step_avg:673.68ms
step:373/2900 train_loss:3.9224 train_time:244543ms step_avg:673.67ms
step:374/2900 train_loss:3.8733 train_time:245215ms step_avg:673.67ms
step:375/2900 train_loss:3.9616 train_time:245886ms step_avg:673.66ms
step:375/2900 val_loss:3.8975 train_time:245898ms step_avg:673.69ms
step:376/2900 train_loss:3.8755 train_time:246561ms step_avg:673.67ms
step:377/2900 train_loss:3.7767 train_time:247234ms step_avg:673.66ms
step:378/2900 train_loss:3.3723 train_time:247905ms step_avg:673.66ms
step:379/2900 train_loss:3.7488 train_time:248580ms step_avg:673.66ms
step:380/2900 train_loss:3.7623 train_time:249253ms step_avg:673.66ms
step:381/2900 train_loss:3.8696 train_time:249925ms step_avg:673.65ms
step:382/2900 train_loss:3.9181 train_time:250596ms step_avg:673.65ms
step:383/2900 train_loss:3.8747 train_time:251270ms step_avg:673.65ms
step:384/2900 train_loss:3.8550 train_time:251943ms step_avg:673.65ms
step:385/2900 train_loss:3.9391 train_time:252616ms step_avg:673.64ms
step:386/2900 train_loss:3.8614 train_time:253288ms step_avg:673.64ms
step:387/2900 train_loss:3.9614 train_time:253961ms step_avg:673.64ms
step:388/2900 train_loss:4.1310 train_time:254635ms step_avg:673.64ms
step:389/2900 train_loss:3.8719 train_time:255307ms step_avg:673.63ms
step:390/2900 train_loss:3.8549 train_time:255981ms step_avg:673.63ms
step:391/2900 train_loss:3.9637 train_time:256654ms step_avg:673.63ms
step:392/2900 train_loss:3.8825 train_time:257327ms step_avg:673.63ms
step:393/2900 train_loss:3.9884 train_time:258000ms step_avg:673.63ms
step:394/2900 train_loss:3.8328 train_time:258673ms step_avg:673.63ms
step:395/2900 train_loss:3.9536 train_time:259345ms step_avg:673.62ms
step:396/2900 train_loss:3.7026 train_time:260019ms step_avg:673.62ms
step:397/2900 train_loss:3.9171 train_time:260690ms step_avg:673.62ms
step:398/2900 train_loss:3.9434 train_time:261362ms step_avg:673.61ms
step:399/2900 train_loss:3.9510 train_time:262035ms step_avg:673.61ms
step:400/2900 train_loss:3.8502 train_time:262707ms step_avg:673.61ms
step:401/2900 train_loss:3.9050 train_time:263381ms step_avg:673.61ms
step:402/2900 train_loss:3.9896 train_time:264054ms step_avg:673.61ms
step:403/2900 train_loss:3.9090 train_time:264726ms step_avg:673.60ms
step:404/2900 train_loss:4.0346 train_time:265398ms step_avg:673.60ms
step:405/2900 train_loss:3.7675 train_time:266071ms step_avg:673.60ms
step:406/2900 train_loss:3.8687 train_time:266744ms step_avg:673.60ms
step:407/2900 train_loss:4.1729 train_time:267417ms step_avg:673.59ms
step:408/2900 train_loss:3.8594 train_time:268089ms step_avg:673.59ms
step:409/2900 train_loss:3.9001 train_time:268763ms step_avg:673.59ms
step:410/2900 train_loss:3.9269 train_time:269436ms step_avg:673.59ms
step:411/2900 train_loss:3.8320 train_time:270109ms step_avg:673.59ms
step:412/2900 train_loss:3.8457 train_time:270781ms step_avg:673.59ms
step:413/2900 train_loss:4.2739 train_time:271454ms step_avg:673.58ms
step:414/2900 train_loss:3.7052 train_time:272126ms step_avg:673.58ms
step:415/2900 train_loss:4.0871 train_time:272799ms step_avg:673.58ms
step:416/2900 train_loss:3.8301 train_time:273472ms step_avg:673.58ms
step:417/2900 train_loss:3.8343 train_time:274145ms step_avg:673.58ms
step:418/2900 train_loss:4.0252 train_time:274818ms step_avg:673.57ms
step:419/2900 train_loss:3.7654 train_time:275492ms step_avg:673.57ms
step:420/2900 train_loss:3.8808 train_time:276164ms step_avg:673.57ms
step:421/2900 train_loss:3.7893 train_time:276836ms step_avg:673.57ms
step:422/2900 train_loss:3.7259 train_time:277509ms step_avg:673.57ms
step:423/2900 train_loss:3.8572 train_time:278181ms step_avg:673.56ms
step:424/2900 train_loss:3.9513 train_time:278855ms step_avg:673.56ms
step:425/2900 train_loss:3.6948 train_time:279527ms step_avg:673.56ms
step:426/2900 train_loss:3.8842 train_time:280200ms step_avg:673.56ms
step:427/2900 train_loss:3.7584 train_time:280872ms step_avg:673.56ms
step:428/2900 train_loss:3.9865 train_time:281545ms step_avg:673.55ms
step:429/2900 train_loss:3.8950 train_time:282218ms step_avg:673.55ms
step:430/2900 train_loss:3.8345 train_time:282891ms step_avg:673.55ms
step:431/2900 train_loss:3.8026 train_time:283564ms step_avg:673.55ms
step:432/2900 train_loss:3.7062 train_time:284237ms step_avg:673.55ms
step:433/2900 train_loss:3.8436 train_time:284910ms step_avg:673.55ms
step:434/2900 train_loss:3.9038 train_time:285582ms step_avg:673.54ms
step:435/2900 train_loss:3.8522 train_time:286255ms step_avg:673.54ms
step:436/2900 train_loss:3.8963 train_time:286927ms step_avg:673.54ms
step:437/2900 train_loss:3.9160 train_time:287599ms step_avg:673.53ms
step:438/2900 train_loss:3.7826 train_time:288271ms step_avg:673.53ms
step:439/2900 train_loss:3.7976 train_time:288943ms step_avg:673.53ms
step:440/2900 train_loss:3.7938 train_time:289615ms step_avg:673.52ms
step:441/2900 train_loss:3.9669 train_time:290287ms step_avg:673.52ms
step:442/2900 train_loss:3.8375 train_time:290959ms step_avg:673.52ms
step:443/2900 train_loss:3.8296 train_time:291633ms step_avg:673.52ms
step:444/2900 train_loss:3.7231 train_time:292305ms step_avg:673.51ms
step:445/2900 train_loss:4.0075 train_time:292978ms step_avg:673.51ms
step:446/2900 train_loss:3.9295 train_time:293650ms step_avg:673.51ms
step:447/2900 train_loss:3.9197 train_time:294323ms step_avg:673.51ms
step:448/2900 train_loss:3.8355 train_time:294996ms step_avg:673.51ms
step:449/2900 train_loss:3.9421 train_time:295668ms step_avg:673.50ms
step:450/2900 train_loss:3.7822 train_time:296340ms step_avg:673.50ms
step:451/2900 train_loss:3.7984 train_time:297013ms step_avg:673.50ms
step:452/2900 train_loss:3.6694 train_time:297685ms step_avg:673.50ms
step:453/2900 train_loss:3.7937 train_time:298358ms step_avg:673.49ms
step:454/2900 train_loss:3.7592 train_time:299029ms step_avg:673.49ms
step:455/2900 train_loss:3.7207 train_time:299701ms step_avg:673.49ms
step:456/2900 train_loss:3.9285 train_time:300373ms step_avg:673.48ms
step:457/2900 train_loss:3.8165 train_time:301046ms step_avg:673.48ms
step:458/2900 train_loss:3.8871 train_time:301717ms step_avg:673.48ms
step:459/2900 train_loss:3.9222 train_time:302388ms step_avg:673.47ms
step:460/2900 train_loss:3.7222 train_time:303062ms step_avg:673.47ms
step:461/2900 train_loss:3.8864 train_time:303733ms step_avg:673.47ms
step:462/2900 train_loss:3.7888 train_time:304404ms step_avg:673.46ms
step:463/2900 train_loss:3.8217 train_time:305076ms step_avg:673.46ms
step:464/2900 train_loss:3.8692 train_time:305748ms step_avg:673.45ms
step:465/2900 train_loss:3.8102 train_time:306421ms step_avg:673.45ms
step:466/2900 train_loss:3.8137 train_time:307093ms step_avg:673.45ms
step:467/2900 train_loss:3.8942 train_time:307765ms step_avg:673.45ms
step:468/2900 train_loss:3.9167 train_time:308436ms step_avg:673.44ms
step:469/2900 train_loss:3.8901 train_time:309109ms step_avg:673.44ms
step:470/2900 train_loss:3.7812 train_time:309781ms step_avg:673.44ms
step:471/2900 train_loss:3.8631 train_time:310454ms step_avg:673.44ms
step:472/2900 train_loss:3.9154 train_time:311125ms step_avg:673.43ms
step:473/2900 train_loss:3.8675 train_time:311797ms step_avg:673.43ms
step:474/2900 train_loss:3.8087 train_time:312470ms step_avg:673.43ms
step:475/2900 train_loss:3.6779 train_time:313142ms step_avg:673.42ms
step:476/2900 train_loss:4.1066 train_time:313815ms step_avg:673.42ms
step:477/2900 train_loss:3.8589 train_time:314487ms step_avg:673.42ms
step:478/2900 train_loss:3.6796 train_time:315158ms step_avg:673.41ms
step:479/2900 train_loss:3.9151 train_time:315830ms step_avg:673.41ms
step:480/2900 train_loss:3.8625 train_time:316503ms step_avg:673.41ms
step:481/2900 train_loss:4.0064 train_time:317175ms step_avg:673.41ms
step:482/2900 train_loss:3.8214 train_time:317847ms step_avg:673.40ms
step:483/2900 train_loss:3.6216 train_time:318518ms step_avg:673.40ms
step:484/2900 train_loss:3.9042 train_time:319191ms step_avg:673.40ms
step:485/2900 train_loss:3.7503 train_time:319862ms step_avg:673.39ms
step:486/2900 train_loss:3.7706 train_time:320533ms step_avg:673.39ms
step:487/2900 train_loss:3.6916 train_time:321206ms step_avg:673.39ms
step:488/2900 train_loss:3.7646 train_time:321879ms step_avg:673.39ms
step:489/2900 train_loss:3.9703 train_time:322551ms step_avg:673.38ms
step:490/2900 train_loss:3.8062 train_time:323223ms step_avg:673.38ms
step:491/2900 train_loss:3.6893 train_time:323895ms step_avg:673.38ms
step:492/2900 train_loss:3.7175 train_time:324567ms step_avg:673.38ms
step:493/2900 train_loss:3.8362 train_time:325238ms step_avg:673.37ms
step:494/2900 train_loss:3.6770 train_time:325910ms step_avg:673.37ms
step:495/2900 train_loss:3.8015 train_time:326582ms step_avg:673.36ms
step:496/2900 train_loss:3.7546 train_time:327254ms step_avg:673.36ms
step:497/2900 train_loss:3.6318 train_time:327927ms step_avg:673.36ms
step:498/2900 train_loss:3.8291 train_time:328598ms step_avg:673.36ms
step:499/2900 train_loss:3.8963 train_time:329271ms step_avg:673.36ms
step:500/2900 train_loss:3.9295 train_time:329943ms step_avg:673.35ms
step:500/2900 val_loss:3.8053 train_time:329955ms step_avg:673.38ms
step:501/2900 train_loss:3.8457 train_time:330617ms step_avg:673.35ms
