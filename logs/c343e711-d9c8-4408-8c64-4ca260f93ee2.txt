====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.ones(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 2900 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]+[raw_model.v_skip_weights]
optimizer3 = Muon(matrix_params, lr=0.04, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.08, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    print(f"{logfile = }")
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # log lambdas
        if master_process:
            skip_weights_str = str(model.module.skip_weights)
            v_skip_weights_str = str(model.module.v_skip_weights)
            print(f"{skip_weights_str = }")
            print(f"{v_skip_weights_str = }")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 14:40:43 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   37C    P0             71W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   41C    P0             98W /  400W |    3407MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   37C    P0             74W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   48C    P0             78W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2900 val_loss:10.8258 train_time:304ms step_avg:nanms
step:1/2900 train_loss:10.8258 train_time:17911ms step_avg:nanms
step:2/2900 train_loss:10.1509 train_time:18548ms step_avg:nanms
step:3/2900 train_loss:8.5243 train_time:19214ms step_avg:nanms
step:4/2900 train_loss:7.5400 train_time:19884ms step_avg:nanms
step:5/2900 train_loss:7.4393 train_time:20553ms step_avg:nanms
step:6/2900 train_loss:7.1240 train_time:21224ms step_avg:nanms
step:7/2900 train_loss:7.3407 train_time:21895ms step_avg:nanms
step:8/2900 train_loss:6.9555 train_time:22566ms step_avg:nanms
step:9/2900 train_loss:6.7833 train_time:23236ms step_avg:nanms
step:10/2900 train_loss:6.6652 train_time:23907ms step_avg:nanms
step:11/2900 train_loss:6.6036 train_time:659ms step_avg:nanms
step:12/2900 train_loss:6.4572 train_time:1330ms step_avg:nanms
step:13/2900 train_loss:6.3778 train_time:2002ms step_avg:667.41ms
step:14/2900 train_loss:6.3802 train_time:2674ms step_avg:668.38ms
step:15/2900 train_loss:6.3440 train_time:3345ms step_avg:668.97ms
step:16/2900 train_loss:6.3011 train_time:4017ms step_avg:669.42ms
step:17/2900 train_loss:6.4007 train_time:4688ms step_avg:669.75ms
step:18/2900 train_loss:6.1926 train_time:5362ms step_avg:670.27ms
step:19/2900 train_loss:6.2001 train_time:6036ms step_avg:670.63ms
step:20/2900 train_loss:5.8901 train_time:6709ms step_avg:670.87ms
step:21/2900 train_loss:6.2104 train_time:7382ms step_avg:671.12ms
step:22/2900 train_loss:6.4489 train_time:8055ms step_avg:671.25ms
step:23/2900 train_loss:6.1046 train_time:8729ms step_avg:671.50ms
step:24/2900 train_loss:6.2602 train_time:9404ms step_avg:671.73ms
step:25/2900 train_loss:5.9531 train_time:10078ms step_avg:671.87ms
step:26/2900 train_loss:5.8659 train_time:10751ms step_avg:671.95ms
step:27/2900 train_loss:6.0858 train_time:11427ms step_avg:672.15ms
step:28/2900 train_loss:5.6977 train_time:12100ms step_avg:672.25ms
step:29/2900 train_loss:5.9552 train_time:12774ms step_avg:672.34ms
step:30/2900 train_loss:5.7784 train_time:13450ms step_avg:672.51ms
step:31/2900 train_loss:5.7332 train_time:14125ms step_avg:672.61ms
step:32/2900 train_loss:5.5882 train_time:14800ms step_avg:672.73ms
step:33/2900 train_loss:5.8865 train_time:15474ms step_avg:672.78ms
step:34/2900 train_loss:5.8257 train_time:16149ms step_avg:672.87ms
step:35/2900 train_loss:5.9197 train_time:16823ms step_avg:672.93ms
step:36/2900 train_loss:5.8776 train_time:17498ms step_avg:672.99ms
step:37/2900 train_loss:5.7557 train_time:18172ms step_avg:673.03ms
step:38/2900 train_loss:5.6387 train_time:18847ms step_avg:673.11ms
step:39/2900 train_loss:5.6316 train_time:19521ms step_avg:673.15ms
step:40/2900 train_loss:5.5700 train_time:20196ms step_avg:673.21ms
step:41/2900 train_loss:5.5537 train_time:20869ms step_avg:673.19ms
step:42/2900 train_loss:5.4537 train_time:21545ms step_avg:673.28ms
step:43/2900 train_loss:5.5723 train_time:22219ms step_avg:673.30ms
step:44/2900 train_loss:5.5181 train_time:22894ms step_avg:673.34ms
step:45/2900 train_loss:5.6389 train_time:23569ms step_avg:673.40ms
step:46/2900 train_loss:5.4500 train_time:24243ms step_avg:673.41ms
step:47/2900 train_loss:5.3271 train_time:24918ms step_avg:673.46ms
step:48/2900 train_loss:5.4990 train_time:25592ms step_avg:673.48ms
step:49/2900 train_loss:5.3971 train_time:26268ms step_avg:673.54ms
step:50/2900 train_loss:5.5338 train_time:26944ms step_avg:673.60ms
step:51/2900 train_loss:5.4028 train_time:27618ms step_avg:673.62ms
step:52/2900 train_loss:5.2682 train_time:28292ms step_avg:673.63ms
step:53/2900 train_loss:5.3879 train_time:28967ms step_avg:673.64ms
step:54/2900 train_loss:5.2435 train_time:29641ms step_avg:673.65ms
step:55/2900 train_loss:5.6194 train_time:30315ms step_avg:673.67ms
step:56/2900 train_loss:5.2588 train_time:30991ms step_avg:673.71ms
step:57/2900 train_loss:5.1101 train_time:31665ms step_avg:673.73ms
step:58/2900 train_loss:5.2440 train_time:32339ms step_avg:673.73ms
step:59/2900 train_loss:5.2285 train_time:33015ms step_avg:673.78ms
step:60/2900 train_loss:5.3152 train_time:33688ms step_avg:673.77ms
step:61/2900 train_loss:5.0788 train_time:34363ms step_avg:673.78ms
step:62/2900 train_loss:5.2073 train_time:35036ms step_avg:673.77ms
step:63/2900 train_loss:5.1719 train_time:35711ms step_avg:673.80ms
step:64/2900 train_loss:5.1496 train_time:36385ms step_avg:673.80ms
step:65/2900 train_loss:4.9938 train_time:37059ms step_avg:673.80ms
step:66/2900 train_loss:5.1243 train_time:37732ms step_avg:673.79ms
step:67/2900 train_loss:5.0004 train_time:38408ms step_avg:673.82ms
step:68/2900 train_loss:5.2968 train_time:39081ms step_avg:673.82ms
step:69/2900 train_loss:4.9095 train_time:39756ms step_avg:673.84ms
step:70/2900 train_loss:4.9938 train_time:40431ms step_avg:673.85ms
step:71/2900 train_loss:5.1461 train_time:41106ms step_avg:673.86ms
step:72/2900 train_loss:5.0671 train_time:41780ms step_avg:673.86ms
step:73/2900 train_loss:4.9229 train_time:42454ms step_avg:673.87ms
step:74/2900 train_loss:5.0698 train_time:43127ms step_avg:673.87ms
step:75/2900 train_loss:5.0303 train_time:43801ms step_avg:673.86ms
step:76/2900 train_loss:4.9858 train_time:44477ms step_avg:673.89ms
step:77/2900 train_loss:5.0986 train_time:45151ms step_avg:673.89ms
step:78/2900 train_loss:5.2536 train_time:45825ms step_avg:673.90ms
step:79/2900 train_loss:4.9622 train_time:46499ms step_avg:673.90ms
step:80/2900 train_loss:5.0162 train_time:47173ms step_avg:673.90ms
step:81/2900 train_loss:4.8122 train_time:47846ms step_avg:673.89ms
step:82/2900 train_loss:4.9722 train_time:48519ms step_avg:673.88ms
step:83/2900 train_loss:4.9305 train_time:49193ms step_avg:673.88ms
step:84/2900 train_loss:4.9182 train_time:49867ms step_avg:673.88ms
step:85/2900 train_loss:4.7751 train_time:50539ms step_avg:673.86ms
step:86/2900 train_loss:4.9806 train_time:51215ms step_avg:673.89ms
step:87/2900 train_loss:4.8820 train_time:51889ms step_avg:673.89ms
step:88/2900 train_loss:4.8830 train_time:52563ms step_avg:673.88ms
step:89/2900 train_loss:4.8510 train_time:53236ms step_avg:673.87ms
step:90/2900 train_loss:4.7990 train_time:53911ms step_avg:673.89ms
step:91/2900 train_loss:4.7645 train_time:54584ms step_avg:673.88ms
step:92/2900 train_loss:4.8949 train_time:55259ms step_avg:673.89ms
step:93/2900 train_loss:4.7192 train_time:55933ms step_avg:673.90ms
step:94/2900 train_loss:4.7616 train_time:56608ms step_avg:673.90ms
step:95/2900 train_loss:4.7803 train_time:57281ms step_avg:673.90ms
step:96/2900 train_loss:4.6772 train_time:57954ms step_avg:673.89ms
step:97/2900 train_loss:4.7137 train_time:58627ms step_avg:673.87ms
step:98/2900 train_loss:4.6802 train_time:59301ms step_avg:673.88ms
step:99/2900 train_loss:4.7599 train_time:59974ms step_avg:673.86ms
step:100/2900 train_loss:4.7818 train_time:60648ms step_avg:673.86ms
step:101/2900 train_loss:4.6599 train_time:61322ms step_avg:673.87ms
step:102/2900 train_loss:4.7798 train_time:61995ms step_avg:673.85ms
step:103/2900 train_loss:4.6558 train_time:62668ms step_avg:673.85ms
step:104/2900 train_loss:4.6588 train_time:63344ms step_avg:673.87ms
step:105/2900 train_loss:4.6540 train_time:64017ms step_avg:673.86ms
step:106/2900 train_loss:4.6433 train_time:64691ms step_avg:673.87ms
step:107/2900 train_loss:4.5995 train_time:65365ms step_avg:673.87ms
step:108/2900 train_loss:4.4298 train_time:66039ms step_avg:673.87ms
step:109/2900 train_loss:4.5607 train_time:66713ms step_avg:673.87ms
step:110/2900 train_loss:4.5464 train_time:67388ms step_avg:673.88ms
step:111/2900 train_loss:4.4872 train_time:68061ms step_avg:673.88ms
step:112/2900 train_loss:4.6401 train_time:68737ms step_avg:673.89ms
step:113/2900 train_loss:4.5444 train_time:69412ms step_avg:673.90ms
step:114/2900 train_loss:4.4233 train_time:70086ms step_avg:673.90ms
step:115/2900 train_loss:4.5768 train_time:70759ms step_avg:673.90ms
step:116/2900 train_loss:4.5481 train_time:71433ms step_avg:673.89ms
step:117/2900 train_loss:4.4394 train_time:72107ms step_avg:673.90ms
step:118/2900 train_loss:4.6573 train_time:72781ms step_avg:673.90ms
step:119/2900 train_loss:4.5282 train_time:73455ms step_avg:673.90ms
step:120/2900 train_loss:4.3880 train_time:74128ms step_avg:673.89ms
step:121/2900 train_loss:4.3777 train_time:74801ms step_avg:673.89ms
step:122/2900 train_loss:4.5241 train_time:75475ms step_avg:673.88ms
step:123/2900 train_loss:4.3421 train_time:76149ms step_avg:673.88ms
step:124/2900 train_loss:4.6482 train_time:76824ms step_avg:673.90ms
step:125/2900 train_loss:4.4995 train_time:77497ms step_avg:673.89ms
step:125/2900 val_loss:4.4656 train_time:77509ms step_avg:673.99ms
step:126/2900 train_loss:4.4718 train_time:78172ms step_avg:673.89ms
step:127/2900 train_loss:4.4968 train_time:78845ms step_avg:673.89ms
step:128/2900 train_loss:4.4283 train_time:79520ms step_avg:673.90ms
step:129/2900 train_loss:4.7356 train_time:80194ms step_avg:673.90ms
step:130/2900 train_loss:4.4097 train_time:80868ms step_avg:673.90ms
step:131/2900 train_loss:4.4634 train_time:81542ms step_avg:673.90ms
step:132/2900 train_loss:4.3968 train_time:82217ms step_avg:673.91ms
step:133/2900 train_loss:4.5033 train_time:82891ms step_avg:673.91ms
step:134/2900 train_loss:4.3156 train_time:83565ms step_avg:673.91ms
step:135/2900 train_loss:4.4878 train_time:84237ms step_avg:673.90ms
step:136/2900 train_loss:4.2495 train_time:84912ms step_avg:673.91ms
step:137/2900 train_loss:4.4364 train_time:85587ms step_avg:673.91ms
step:138/2900 train_loss:4.3466 train_time:86261ms step_avg:673.91ms
step:139/2900 train_loss:4.4425 train_time:86933ms step_avg:673.90ms
step:140/2900 train_loss:4.5179 train_time:87607ms step_avg:673.90ms
step:141/2900 train_loss:4.3672 train_time:88282ms step_avg:673.91ms
step:142/2900 train_loss:4.3524 train_time:88955ms step_avg:673.90ms
step:143/2900 train_loss:4.2998 train_time:89628ms step_avg:673.90ms
step:144/2900 train_loss:4.4151 train_time:90302ms step_avg:673.90ms
step:145/2900 train_loss:4.3607 train_time:90975ms step_avg:673.89ms
step:146/2900 train_loss:4.2408 train_time:91649ms step_avg:673.89ms
step:147/2900 train_loss:4.3777 train_time:92324ms step_avg:673.90ms
step:148/2900 train_loss:4.4074 train_time:92998ms step_avg:673.90ms
step:149/2900 train_loss:4.3479 train_time:93671ms step_avg:673.89ms
step:150/2900 train_loss:4.4723 train_time:94344ms step_avg:673.88ms
step:151/2900 train_loss:4.3185 train_time:95017ms step_avg:673.88ms
step:152/2900 train_loss:4.3166 train_time:95690ms step_avg:673.88ms
step:153/2900 train_loss:4.4024 train_time:96365ms step_avg:673.88ms
step:154/2900 train_loss:4.4112 train_time:97039ms step_avg:673.88ms
step:155/2900 train_loss:4.3371 train_time:97713ms step_avg:673.88ms
step:156/2900 train_loss:4.3883 train_time:98387ms step_avg:673.88ms
step:157/2900 train_loss:4.4616 train_time:99060ms step_avg:673.88ms
step:158/2900 train_loss:4.2864 train_time:99734ms step_avg:673.88ms
step:159/2900 train_loss:4.3603 train_time:100407ms step_avg:673.87ms
step:160/2900 train_loss:4.1658 train_time:101081ms step_avg:673.87ms
step:161/2900 train_loss:4.3941 train_time:101753ms step_avg:673.86ms
step:162/2900 train_loss:4.4057 train_time:102427ms step_avg:673.86ms
step:163/2900 train_loss:4.3868 train_time:103100ms step_avg:673.85ms
step:164/2900 train_loss:4.2457 train_time:103774ms step_avg:673.86ms
step:165/2900 train_loss:4.3133 train_time:104447ms step_avg:673.85ms
step:166/2900 train_loss:4.3907 train_time:105121ms step_avg:673.85ms
step:167/2900 train_loss:4.2291 train_time:105795ms step_avg:673.85ms
step:168/2900 train_loss:4.3130 train_time:106468ms step_avg:673.85ms
step:169/2900 train_loss:4.1925 train_time:107142ms step_avg:673.85ms
step:170/2900 train_loss:4.0792 train_time:107815ms step_avg:673.84ms
step:171/2900 train_loss:4.2460 train_time:108489ms step_avg:673.84ms
step:172/2900 train_loss:4.2584 train_time:109161ms step_avg:673.83ms
step:173/2900 train_loss:4.3034 train_time:109836ms step_avg:673.84ms
step:174/2900 train_loss:4.4615 train_time:110509ms step_avg:673.83ms
step:175/2900 train_loss:4.2890 train_time:111182ms step_avg:673.83ms
step:176/2900 train_loss:4.1557 train_time:111856ms step_avg:673.83ms
step:177/2900 train_loss:4.1204 train_time:112530ms step_avg:673.83ms
step:178/2900 train_loss:4.2363 train_time:113203ms step_avg:673.82ms
step:179/2900 train_loss:4.1855 train_time:113877ms step_avg:673.83ms
step:180/2900 train_loss:4.1627 train_time:114549ms step_avg:673.82ms
step:181/2900 train_loss:4.3443 train_time:115223ms step_avg:673.82ms
step:182/2900 train_loss:4.2108 train_time:115896ms step_avg:673.81ms
step:183/2900 train_loss:4.1816 train_time:116571ms step_avg:673.82ms
step:184/2900 train_loss:4.1887 train_time:117243ms step_avg:673.81ms
step:185/2900 train_loss:4.2646 train_time:117916ms step_avg:673.81ms
step:186/2900 train_loss:4.2364 train_time:118590ms step_avg:673.81ms
step:187/2900 train_loss:4.2989 train_time:119263ms step_avg:673.80ms
step:188/2900 train_loss:4.2204 train_time:119936ms step_avg:673.80ms
step:189/2900 train_loss:4.1498 train_time:120609ms step_avg:673.79ms
step:190/2900 train_loss:4.2490 train_time:121316ms step_avg:673.98ms
step:191/2900 train_loss:4.3259 train_time:122146ms step_avg:674.84ms
step:192/2900 train_loss:4.1315 train_time:122819ms step_avg:674.83ms
step:193/2900 train_loss:4.2008 train_time:123492ms step_avg:674.82ms
step:194/2900 train_loss:4.1600 train_time:124167ms step_avg:674.82ms
step:195/2900 train_loss:4.1182 train_time:124840ms step_avg:674.81ms
step:196/2900 train_loss:4.1329 train_time:125514ms step_avg:674.80ms
step:197/2900 train_loss:4.1788 train_time:126186ms step_avg:674.79ms
step:198/2900 train_loss:4.0764 train_time:126861ms step_avg:674.79ms
step:199/2900 train_loss:4.3108 train_time:127534ms step_avg:674.79ms
step:200/2900 train_loss:4.2386 train_time:128207ms step_avg:674.77ms
step:201/2900 train_loss:5.0223 train_time:128881ms step_avg:674.77ms
step:202/2900 train_loss:4.4464 train_time:129555ms step_avg:674.76ms
step:203/2900 train_loss:4.1992 train_time:130228ms step_avg:674.76ms
step:204/2900 train_loss:4.1155 train_time:130903ms step_avg:674.76ms
step:205/2900 train_loss:4.1661 train_time:131575ms step_avg:674.75ms
step:206/2900 train_loss:4.1325 train_time:132251ms step_avg:674.75ms
step:207/2900 train_loss:4.1382 train_time:132923ms step_avg:674.74ms
step:208/2900 train_loss:4.0884 train_time:133597ms step_avg:674.73ms
step:209/2900 train_loss:4.2612 train_time:134270ms step_avg:674.73ms
step:210/2900 train_loss:4.0679 train_time:134945ms step_avg:674.72ms
step:211/2900 train_loss:4.1865 train_time:135619ms step_avg:674.72ms
step:212/2900 train_loss:4.1833 train_time:136293ms step_avg:674.72ms
step:213/2900 train_loss:4.1247 train_time:136967ms step_avg:674.72ms
step:214/2900 train_loss:4.0752 train_time:137641ms step_avg:674.71ms
step:215/2900 train_loss:4.2043 train_time:138315ms step_avg:674.71ms
step:216/2900 train_loss:4.1366 train_time:138988ms step_avg:674.70ms
step:217/2900 train_loss:4.1214 train_time:139661ms step_avg:674.69ms
step:218/2900 train_loss:4.1433 train_time:140336ms step_avg:674.69ms
step:219/2900 train_loss:4.0662 train_time:141010ms step_avg:674.69ms
step:220/2900 train_loss:4.1886 train_time:141684ms step_avg:674.68ms
step:221/2900 train_loss:4.0847 train_time:142358ms step_avg:674.68ms
step:222/2900 train_loss:4.1682 train_time:143033ms step_avg:674.69ms
step:223/2900 train_loss:4.0888 train_time:143706ms step_avg:674.68ms
step:224/2900 train_loss:4.1004 train_time:144380ms step_avg:674.67ms
step:225/2900 train_loss:4.0938 train_time:145053ms step_avg:674.67ms
step:226/2900 train_loss:4.1112 train_time:145727ms step_avg:674.66ms
step:227/2900 train_loss:4.1077 train_time:146400ms step_avg:674.66ms
step:228/2900 train_loss:4.0270 train_time:147074ms step_avg:674.65ms
step:229/2900 train_loss:4.2403 train_time:147747ms step_avg:674.64ms
step:230/2900 train_loss:4.1220 train_time:148421ms step_avg:674.64ms
step:231/2900 train_loss:4.0055 train_time:149094ms step_avg:674.63ms
step:232/2900 train_loss:4.2213 train_time:149768ms step_avg:674.63ms
step:233/2900 train_loss:4.0404 train_time:150442ms step_avg:674.63ms
step:234/2900 train_loss:4.0822 train_time:151115ms step_avg:674.62ms
step:235/2900 train_loss:4.2391 train_time:151789ms step_avg:674.62ms
step:236/2900 train_loss:4.1311 train_time:152463ms step_avg:674.61ms
step:237/2900 train_loss:4.2180 train_time:153137ms step_avg:674.61ms
step:238/2900 train_loss:4.1591 train_time:153810ms step_avg:674.60ms
step:239/2900 train_loss:4.1669 train_time:154483ms step_avg:674.60ms
step:240/2900 train_loss:4.1233 train_time:155156ms step_avg:674.59ms
step:241/2900 train_loss:4.3034 train_time:155830ms step_avg:674.59ms
step:242/2900 train_loss:4.1394 train_time:156503ms step_avg:674.58ms
step:243/2900 train_loss:4.4823 train_time:157176ms step_avg:674.58ms
step:244/2900 train_loss:4.0987 train_time:157849ms step_avg:674.57ms
step:245/2900 train_loss:4.1206 train_time:158524ms step_avg:674.57ms
step:246/2900 train_loss:4.1508 train_time:159198ms step_avg:674.57ms
step:247/2900 train_loss:4.0533 train_time:159871ms step_avg:674.56ms
step:248/2900 train_loss:4.0631 train_time:160545ms step_avg:674.56ms
step:249/2900 train_loss:4.0004 train_time:161219ms step_avg:674.56ms
step:250/2900 train_loss:4.1593 train_time:161892ms step_avg:674.55ms
step:250/2900 val_loss:4.0685 train_time:161904ms step_avg:674.60ms
step:251/2900 train_loss:4.0476 train_time:162566ms step_avg:674.55ms
step:252/2900 train_loss:3.9756 train_time:163240ms step_avg:674.55ms
step:253/2900 train_loss:4.0015 train_time:163914ms step_avg:674.54ms
step:254/2900 train_loss:4.0728 train_time:164588ms step_avg:674.54ms
step:255/2900 train_loss:4.0075 train_time:165262ms step_avg:674.54ms
step:256/2900 train_loss:3.9458 train_time:165936ms step_avg:674.54ms
step:257/2900 train_loss:4.0190 train_time:166610ms step_avg:674.53ms
step:258/2900 train_loss:4.1394 train_time:167285ms step_avg:674.54ms
step:259/2900 train_loss:3.9486 train_time:167958ms step_avg:674.53ms
step:260/2900 train_loss:4.0553 train_time:168632ms step_avg:674.53ms
step:261/2900 train_loss:4.0153 train_time:169306ms step_avg:674.53ms
step:262/2900 train_loss:4.0392 train_time:169980ms step_avg:674.52ms
step:263/2900 train_loss:4.0997 train_time:170652ms step_avg:674.51ms
step:264/2900 train_loss:4.1406 train_time:171326ms step_avg:674.51ms
step:265/2900 train_loss:4.4954 train_time:172000ms step_avg:674.51ms
step:266/2900 train_loss:4.0497 train_time:172675ms step_avg:674.51ms
step:267/2900 train_loss:4.2358 train_time:173348ms step_avg:674.51ms
step:268/2900 train_loss:3.9944 train_time:174022ms step_avg:674.50ms
step:269/2900 train_loss:3.9991 train_time:174695ms step_avg:674.50ms
step:270/2900 train_loss:4.1115 train_time:175369ms step_avg:674.50ms
step:271/2900 train_loss:4.1663 train_time:176043ms step_avg:674.49ms
step:272/2900 train_loss:4.0499 train_time:176716ms step_avg:674.49ms
step:273/2900 train_loss:4.0005 train_time:177390ms step_avg:674.49ms
step:274/2900 train_loss:3.9940 train_time:178064ms step_avg:674.48ms
step:275/2900 train_loss:3.9643 train_time:178737ms step_avg:674.48ms
step:276/2900 train_loss:4.0799 train_time:179411ms step_avg:674.48ms
step:277/2900 train_loss:4.1137 train_time:180083ms step_avg:674.47ms
step:278/2900 train_loss:3.9004 train_time:180757ms step_avg:674.47ms
step:279/2900 train_loss:4.2046 train_time:181431ms step_avg:674.47ms
step:280/2900 train_loss:4.0582 train_time:182107ms step_avg:674.47ms
step:281/2900 train_loss:4.0616 train_time:182780ms step_avg:674.46ms
step:282/2900 train_loss:4.0065 train_time:183453ms step_avg:674.46ms
step:283/2900 train_loss:4.2221 train_time:184128ms step_avg:674.46ms
step:284/2900 train_loss:3.9656 train_time:184801ms step_avg:674.45ms
step:285/2900 train_loss:3.9355 train_time:185475ms step_avg:674.45ms
step:286/2900 train_loss:4.1213 train_time:186148ms step_avg:674.45ms
step:287/2900 train_loss:4.0397 train_time:186821ms step_avg:674.44ms
step:288/2900 train_loss:4.0743 train_time:187494ms step_avg:674.44ms
step:289/2900 train_loss:4.1198 train_time:188167ms step_avg:674.43ms
step:290/2900 train_loss:4.1072 train_time:188841ms step_avg:674.43ms
step:291/2900 train_loss:3.9502 train_time:189513ms step_avg:674.42ms
step:292/2900 train_loss:3.9202 train_time:190187ms step_avg:674.42ms
step:293/2900 train_loss:4.1194 train_time:190861ms step_avg:674.42ms
step:294/2900 train_loss:3.9172 train_time:191534ms step_avg:674.42ms
step:295/2900 train_loss:4.2031 train_time:192208ms step_avg:674.41ms
step:296/2900 train_loss:4.0913 train_time:192881ms step_avg:674.41ms
step:297/2900 train_loss:3.9302 train_time:193554ms step_avg:674.41ms
step:298/2900 train_loss:4.0382 train_time:194228ms step_avg:674.40ms
step:299/2900 train_loss:3.9426 train_time:194901ms step_avg:674.40ms
step:300/2900 train_loss:4.1386 train_time:195574ms step_avg:674.39ms
step:301/2900 train_loss:3.9295 train_time:196248ms step_avg:674.39ms
step:302/2900 train_loss:4.0417 train_time:196920ms step_avg:674.38ms
step:303/2900 train_loss:3.9803 train_time:197594ms step_avg:674.38ms
step:304/2900 train_loss:3.9761 train_time:198268ms step_avg:674.38ms
step:305/2900 train_loss:3.9709 train_time:198941ms step_avg:674.37ms
step:306/2900 train_loss:4.0840 train_time:199614ms step_avg:674.37ms
step:307/2900 train_loss:4.0261 train_time:200286ms step_avg:674.37ms
step:308/2900 train_loss:3.9215 train_time:200960ms step_avg:674.36ms
step:309/2900 train_loss:4.0994 train_time:201634ms step_avg:674.36ms
step:310/2900 train_loss:3.9568 train_time:202307ms step_avg:674.36ms
step:311/2900 train_loss:4.0335 train_time:202980ms step_avg:674.35ms
step:312/2900 train_loss:4.0173 train_time:203653ms step_avg:674.35ms
step:313/2900 train_loss:3.9225 train_time:204327ms step_avg:674.35ms
step:314/2900 train_loss:4.0091 train_time:205000ms step_avg:674.34ms
step:315/2900 train_loss:4.0036 train_time:205673ms step_avg:674.34ms
step:316/2900 train_loss:3.7386 train_time:206347ms step_avg:674.34ms
step:317/2900 train_loss:3.9876 train_time:207020ms step_avg:674.33ms
step:318/2900 train_loss:3.9747 train_time:207696ms step_avg:674.34ms
step:319/2900 train_loss:3.8939 train_time:208369ms step_avg:674.33ms
step:320/2900 train_loss:3.9238 train_time:209042ms step_avg:674.33ms
step:321/2900 train_loss:3.9685 train_time:209716ms step_avg:674.33ms
step:322/2900 train_loss:3.9938 train_time:210390ms step_avg:674.33ms
step:323/2900 train_loss:3.8564 train_time:211064ms step_avg:674.33ms
step:324/2900 train_loss:3.9429 train_time:211738ms step_avg:674.32ms
step:325/2900 train_loss:3.9937 train_time:212412ms step_avg:674.32ms
step:326/2900 train_loss:4.0026 train_time:213084ms step_avg:674.32ms
step:327/2900 train_loss:3.9606 train_time:213758ms step_avg:674.32ms
step:328/2900 train_loss:4.2286 train_time:214431ms step_avg:674.31ms
step:329/2900 train_loss:4.0001 train_time:215103ms step_avg:674.30ms
step:330/2900 train_loss:4.0535 train_time:215778ms step_avg:674.31ms
step:331/2900 train_loss:3.9159 train_time:216451ms step_avg:674.30ms
step:332/2900 train_loss:4.4971 train_time:217125ms step_avg:674.30ms
step:333/2900 train_loss:3.9107 train_time:217799ms step_avg:674.30ms
step:334/2900 train_loss:3.8190 train_time:218472ms step_avg:674.30ms
step:335/2900 train_loss:3.9796 train_time:219146ms step_avg:674.29ms
step:336/2900 train_loss:4.0034 train_time:219820ms step_avg:674.29ms
step:337/2900 train_loss:3.9982 train_time:220493ms step_avg:674.29ms
step:338/2900 train_loss:3.9810 train_time:221167ms step_avg:674.29ms
step:339/2900 train_loss:4.0273 train_time:221841ms step_avg:674.29ms
step:340/2900 train_loss:3.8966 train_time:222516ms step_avg:674.29ms
step:341/2900 train_loss:3.8948 train_time:223189ms step_avg:674.29ms
step:342/2900 train_loss:3.9057 train_time:223862ms step_avg:674.28ms
step:343/2900 train_loss:3.9411 train_time:224535ms step_avg:674.28ms
step:344/2900 train_loss:4.0298 train_time:225209ms step_avg:674.28ms
step:345/2900 train_loss:3.9603 train_time:225883ms step_avg:674.28ms
step:346/2900 train_loss:3.9246 train_time:226557ms step_avg:674.28ms
step:347/2900 train_loss:3.9831 train_time:227231ms step_avg:674.27ms
step:348/2900 train_loss:3.9120 train_time:227904ms step_avg:674.27ms
step:349/2900 train_loss:3.9040 train_time:228577ms step_avg:674.27ms
step:350/2900 train_loss:3.9775 train_time:229251ms step_avg:674.27ms
step:351/2900 train_loss:4.2038 train_time:229925ms step_avg:674.27ms
step:352/2900 train_loss:3.8481 train_time:230600ms step_avg:674.27ms
step:353/2900 train_loss:3.9004 train_time:231272ms step_avg:674.26ms
step:354/2900 train_loss:3.8944 train_time:231946ms step_avg:674.26ms
step:355/2900 train_loss:3.7960 train_time:232620ms step_avg:674.26ms
step:356/2900 train_loss:3.9389 train_time:233295ms step_avg:674.26ms
step:357/2900 train_loss:3.8691 train_time:233968ms step_avg:674.26ms
step:358/2900 train_loss:3.9931 train_time:234643ms step_avg:674.26ms
step:359/2900 train_loss:3.9721 train_time:235317ms step_avg:674.26ms
step:360/2900 train_loss:3.8848 train_time:235991ms step_avg:674.26ms
step:361/2900 train_loss:4.1468 train_time:236664ms step_avg:674.26ms
step:362/2900 train_loss:4.1296 train_time:237337ms step_avg:674.25ms
step:363/2900 train_loss:3.9453 train_time:238011ms step_avg:674.25ms
step:364/2900 train_loss:3.8387 train_time:238685ms step_avg:674.25ms
step:365/2900 train_loss:4.0099 train_time:239359ms step_avg:674.25ms
step:366/2900 train_loss:3.8494 train_time:240033ms step_avg:674.25ms
step:367/2900 train_loss:3.9501 train_time:240706ms step_avg:674.25ms
step:368/2900 train_loss:3.8780 train_time:241379ms step_avg:674.24ms
step:369/2900 train_loss:3.9699 train_time:242053ms step_avg:674.24ms
step:370/2900 train_loss:3.9387 train_time:242726ms step_avg:674.24ms
step:371/2900 train_loss:3.7536 train_time:243399ms step_avg:674.24ms
step:372/2900 train_loss:3.8623 train_time:244073ms step_avg:674.24ms
step:373/2900 train_loss:3.9271 train_time:244747ms step_avg:674.23ms
step:374/2900 train_loss:3.8908 train_time:245421ms step_avg:674.23ms
step:375/2900 train_loss:3.9736 train_time:246094ms step_avg:674.23ms
step:375/2900 val_loss:3.9131 train_time:246106ms step_avg:674.26ms
step:376/2900 train_loss:3.8843 train_time:246770ms step_avg:674.23ms
step:377/2900 train_loss:3.7786 train_time:247444ms step_avg:674.23ms
step:378/2900 train_loss:3.3722 train_time:248116ms step_avg:674.23ms
step:379/2900 train_loss:3.7572 train_time:248790ms step_avg:674.23ms
step:380/2900 train_loss:3.7710 train_time:249464ms step_avg:674.23ms
step:381/2900 train_loss:3.8790 train_time:250137ms step_avg:674.22ms
step:382/2900 train_loss:3.9214 train_time:250811ms step_avg:674.22ms
step:383/2900 train_loss:3.8900 train_time:251484ms step_avg:674.22ms
step:384/2900 train_loss:3.8706 train_time:252158ms step_avg:674.22ms
step:385/2900 train_loss:3.9527 train_time:252831ms step_avg:674.22ms
step:386/2900 train_loss:3.8738 train_time:253505ms step_avg:674.22ms
step:387/2900 train_loss:3.9694 train_time:254179ms step_avg:674.21ms
step:388/2900 train_loss:4.1447 train_time:254853ms step_avg:674.22ms
step:389/2900 train_loss:3.8782 train_time:255526ms step_avg:674.21ms
step:390/2900 train_loss:3.8685 train_time:256200ms step_avg:674.21ms
step:391/2900 train_loss:3.9741 train_time:256874ms step_avg:674.21ms
step:392/2900 train_loss:3.8969 train_time:257548ms step_avg:674.21ms
step:393/2900 train_loss:4.0108 train_time:258222ms step_avg:674.21ms
step:394/2900 train_loss:3.8434 train_time:258897ms step_avg:674.21ms
step:395/2900 train_loss:3.9688 train_time:259571ms step_avg:674.21ms
step:396/2900 train_loss:3.7184 train_time:260245ms step_avg:674.21ms
step:397/2900 train_loss:3.9332 train_time:260918ms step_avg:674.21ms
step:398/2900 train_loss:3.9594 train_time:261592ms step_avg:674.21ms
step:399/2900 train_loss:3.9725 train_time:262266ms step_avg:674.21ms
step:400/2900 train_loss:3.8625 train_time:262940ms step_avg:674.21ms
step:401/2900 train_loss:3.9188 train_time:263614ms step_avg:674.20ms
step:402/2900 train_loss:4.0080 train_time:264287ms step_avg:674.20ms
step:403/2900 train_loss:3.9267 train_time:264963ms step_avg:674.21ms
step:404/2900 train_loss:4.0456 train_time:265637ms step_avg:674.21ms
step:405/2900 train_loss:3.7824 train_time:266311ms step_avg:674.21ms
step:406/2900 train_loss:3.8867 train_time:266984ms step_avg:674.20ms
step:407/2900 train_loss:4.1764 train_time:267658ms step_avg:674.20ms
step:408/2900 train_loss:3.8744 train_time:268331ms step_avg:674.20ms
step:409/2900 train_loss:3.9049 train_time:269005ms step_avg:674.20ms
step:410/2900 train_loss:3.9488 train_time:269679ms step_avg:674.20ms
step:411/2900 train_loss:3.8432 train_time:270353ms step_avg:674.20ms
step:412/2900 train_loss:3.8563 train_time:271026ms step_avg:674.19ms
step:413/2900 train_loss:4.2824 train_time:271700ms step_avg:674.19ms
step:414/2900 train_loss:3.7564 train_time:272374ms step_avg:674.19ms
step:415/2900 train_loss:4.0949 train_time:273048ms step_avg:674.19ms
step:416/2900 train_loss:3.8423 train_time:273721ms step_avg:674.19ms
step:417/2900 train_loss:3.8494 train_time:274396ms step_avg:674.19ms
step:418/2900 train_loss:4.0365 train_time:275070ms step_avg:674.19ms
step:419/2900 train_loss:3.7777 train_time:275743ms step_avg:674.19ms
step:420/2900 train_loss:3.9004 train_time:276417ms step_avg:674.19ms
step:421/2900 train_loss:3.8055 train_time:277091ms step_avg:674.19ms
step:422/2900 train_loss:3.7334 train_time:277765ms step_avg:674.19ms
step:423/2900 train_loss:3.8683 train_time:278439ms step_avg:674.19ms
step:424/2900 train_loss:3.9610 train_time:279115ms step_avg:674.19ms
step:425/2900 train_loss:3.7111 train_time:279789ms step_avg:674.19ms
step:426/2900 train_loss:3.8920 train_time:280463ms step_avg:674.19ms
step:427/2900 train_loss:3.7744 train_time:281138ms step_avg:674.19ms
step:428/2900 train_loss:3.9925 train_time:281813ms step_avg:674.19ms
step:429/2900 train_loss:3.9032 train_time:282486ms step_avg:674.19ms
step:430/2900 train_loss:3.8470 train_time:283162ms step_avg:674.19ms
step:431/2900 train_loss:3.8185 train_time:283836ms step_avg:674.19ms
step:432/2900 train_loss:3.7194 train_time:284510ms step_avg:674.19ms
step:433/2900 train_loss:3.8547 train_time:285184ms step_avg:674.19ms
step:434/2900 train_loss:3.9113 train_time:285859ms step_avg:674.20ms
step:435/2900 train_loss:3.8677 train_time:286532ms step_avg:674.19ms
step:436/2900 train_loss:3.9103 train_time:287207ms step_avg:674.19ms
step:437/2900 train_loss:3.9254 train_time:287881ms step_avg:674.19ms
step:438/2900 train_loss:3.7984 train_time:288555ms step_avg:674.19ms
step:439/2900 train_loss:3.8187 train_time:289228ms step_avg:674.19ms
step:440/2900 train_loss:3.8018 train_time:289902ms step_avg:674.19ms
step:441/2900 train_loss:3.9806 train_time:290578ms step_avg:674.20ms
step:442/2900 train_loss:3.8550 train_time:291252ms step_avg:674.20ms
step:443/2900 train_loss:3.8386 train_time:291926ms step_avg:674.19ms
step:444/2900 train_loss:3.7393 train_time:292600ms step_avg:674.19ms
step:445/2900 train_loss:4.0273 train_time:293273ms step_avg:674.19ms
step:446/2900 train_loss:3.9378 train_time:293949ms step_avg:674.19ms
step:447/2900 train_loss:3.9273 train_time:294622ms step_avg:674.19ms
step:448/2900 train_loss:3.8469 train_time:295296ms step_avg:674.19ms
step:449/2900 train_loss:3.9570 train_time:295970ms step_avg:674.19ms
step:450/2900 train_loss:3.7821 train_time:296644ms step_avg:674.19ms
step:451/2900 train_loss:3.8265 train_time:297317ms step_avg:674.19ms
step:452/2900 train_loss:3.6751 train_time:297991ms step_avg:674.19ms
step:453/2900 train_loss:3.7995 train_time:298665ms step_avg:674.19ms
step:454/2900 train_loss:3.7741 train_time:299339ms step_avg:674.19ms
step:455/2900 train_loss:3.7294 train_time:300014ms step_avg:674.19ms
step:456/2900 train_loss:3.9454 train_time:300688ms step_avg:674.19ms
step:457/2900 train_loss:3.8330 train_time:301362ms step_avg:674.19ms
step:458/2900 train_loss:3.8966 train_time:302036ms step_avg:674.19ms
step:459/2900 train_loss:3.9318 train_time:302711ms step_avg:674.19ms
step:460/2900 train_loss:3.7378 train_time:303385ms step_avg:674.19ms
step:461/2900 train_loss:3.9002 train_time:304060ms step_avg:674.19ms
step:462/2900 train_loss:3.8107 train_time:304733ms step_avg:674.19ms
step:463/2900 train_loss:3.8316 train_time:305408ms step_avg:674.19ms
step:464/2900 train_loss:3.8749 train_time:306082ms step_avg:674.19ms
step:465/2900 train_loss:3.8190 train_time:306756ms step_avg:674.19ms
step:466/2900 train_loss:3.8201 train_time:307430ms step_avg:674.19ms
step:467/2900 train_loss:3.9132 train_time:308104ms step_avg:674.19ms
step:468/2900 train_loss:3.9287 train_time:308778ms step_avg:674.19ms
step:469/2900 train_loss:3.9059 train_time:309451ms step_avg:674.19ms
step:470/2900 train_loss:3.7912 train_time:310125ms step_avg:674.19ms
step:471/2900 train_loss:3.8737 train_time:310800ms step_avg:674.19ms
step:472/2900 train_loss:3.9241 train_time:311475ms step_avg:674.19ms
step:473/2900 train_loss:3.8821 train_time:312148ms step_avg:674.19ms
step:474/2900 train_loss:3.8240 train_time:312822ms step_avg:674.19ms
step:475/2900 train_loss:3.6900 train_time:313496ms step_avg:674.19ms
step:476/2900 train_loss:4.1287 train_time:314169ms step_avg:674.18ms
step:477/2900 train_loss:3.8712 train_time:314844ms step_avg:674.18ms
step:478/2900 train_loss:3.6996 train_time:315517ms step_avg:674.18ms
step:479/2900 train_loss:3.9250 train_time:316191ms step_avg:674.18ms
step:480/2900 train_loss:3.8713 train_time:316865ms step_avg:674.18ms
step:481/2900 train_loss:4.0175 train_time:317539ms step_avg:674.18ms
step:482/2900 train_loss:3.8286 train_time:318212ms step_avg:674.18ms
step:483/2900 train_loss:3.6335 train_time:318887ms step_avg:674.18ms
step:484/2900 train_loss:3.9042 train_time:319559ms step_avg:674.18ms
step:485/2900 train_loss:3.7751 train_time:320233ms step_avg:674.17ms
step:486/2900 train_loss:3.7807 train_time:320907ms step_avg:674.17ms
step:487/2900 train_loss:3.7014 train_time:321581ms step_avg:674.18ms
step:488/2900 train_loss:3.7808 train_time:322256ms step_avg:674.17ms
step:489/2900 train_loss:3.9754 train_time:322930ms step_avg:674.17ms
step:490/2900 train_loss:3.8182 train_time:323603ms step_avg:674.17ms
step:491/2900 train_loss:3.7044 train_time:324277ms step_avg:674.17ms
step:492/2900 train_loss:3.7263 train_time:324951ms step_avg:674.17ms
step:493/2900 train_loss:3.8421 train_time:325626ms step_avg:674.17ms
step:494/2900 train_loss:3.6874 train_time:326298ms step_avg:674.17ms
step:495/2900 train_loss:3.8194 train_time:326972ms step_avg:674.17ms
step:496/2900 train_loss:3.7688 train_time:327647ms step_avg:674.17ms
step:497/2900 train_loss:3.6381 train_time:328320ms step_avg:674.17ms
step:498/2900 train_loss:3.8395 train_time:328993ms step_avg:674.17ms
step:499/2900 train_loss:3.9175 train_time:329667ms step_avg:674.16ms
step:500/2900 train_loss:3.9444 train_time:330340ms step_avg:674.16ms
step:500/2900 val_loss:3.8222 train_time:330351ms step_avg:674.19ms
step:501/2900 train_loss:3.8631 train_time:331016ms step_avg:674.17ms
step:502/2900 train_loss:3.9193 train_time:331690ms step_avg:674.17ms
step:503/2900 train_loss:3.8558 train_time:332363ms step_avg:674.17ms
step:504/2900 train_loss:3.8893 train_time:333038ms step_avg:674.17ms
step:505/2900 train_loss:3.8377 train_time:333712ms step_avg:674.16ms
step:506/2900 train_loss:3.9337 train_time:334386ms step_avg:674.16ms
step:507/2900 train_loss:3.7523 train_time:335060ms step_avg:674.16ms
step:508/2900 train_loss:3.8688 train_time:335736ms step_avg:674.17ms
step:509/2900 train_loss:3.9459 train_time:336409ms step_avg:674.17ms
step:510/2900 train_loss:3.8790 train_time:337083ms step_avg:674.17ms
step:511/2900 train_loss:3.6938 train_time:337756ms step_avg:674.16ms
step:512/2900 train_loss:3.8938 train_time:338430ms step_avg:674.16ms
step:513/2900 train_loss:3.8282 train_time:339104ms step_avg:674.16ms
step:514/2900 train_loss:3.7888 train_time:339778ms step_avg:674.16ms
step:515/2900 train_loss:3.8889 train_time:340451ms step_avg:674.16ms
step:516/2900 train_loss:3.8532 train_time:341125ms step_avg:674.16ms
step:517/2900 train_loss:4.2060 train_time:341800ms step_avg:674.16ms
step:518/2900 train_loss:3.8057 train_time:342474ms step_avg:674.16ms
step:519/2900 train_loss:3.9000 train_time:343149ms step_avg:674.16ms
step:520/2900 train_loss:3.7920 train_time:343822ms step_avg:674.16ms
step:521/2900 train_loss:3.8150 train_time:344497ms step_avg:674.16ms
step:522/2900 train_loss:3.7725 train_time:345171ms step_avg:674.16ms
step:523/2900 train_loss:3.7810 train_time:345844ms step_avg:674.16ms
step:524/2900 train_loss:4.4052 train_time:346517ms step_avg:674.16ms
step:525/2900 train_loss:3.8634 train_time:347191ms step_avg:674.16ms
step:526/2900 train_loss:3.7932 train_time:347864ms step_avg:674.15ms
step:527/2900 train_loss:3.8066 train_time:348538ms step_avg:674.15ms
step:528/2900 train_loss:3.7676 train_time:349212ms step_avg:674.15ms
step:529/2900 train_loss:3.7391 train_time:349886ms step_avg:674.15ms
step:530/2900 train_loss:3.9655 train_time:350560ms step_avg:674.15ms
step:531/2900 train_loss:3.7624 train_time:351235ms step_avg:674.15ms
step:532/2900 train_loss:4.0300 train_time:351908ms step_avg:674.15ms
step:533/2900 train_loss:3.8444 train_time:352582ms step_avg:674.15ms
step:534/2900 train_loss:3.7783 train_time:353255ms step_avg:674.15ms
step:535/2900 train_loss:3.7949 train_time:353929ms step_avg:674.15ms
step:536/2900 train_loss:3.7278 train_time:354603ms step_avg:674.15ms
step:537/2900 train_loss:3.8645 train_time:355276ms step_avg:674.15ms
step:538/2900 train_loss:3.8482 train_time:355951ms step_avg:674.15ms
step:539/2900 train_loss:3.7470 train_time:356624ms step_avg:674.15ms
step:540/2900 train_loss:4.2444 train_time:357298ms step_avg:674.15ms
step:541/2900 train_loss:3.7907 train_time:357972ms step_avg:674.15ms
step:542/2900 train_loss:3.8965 train_time:358646ms step_avg:674.15ms
step:543/2900 train_loss:3.7191 train_time:359320ms step_avg:674.15ms
step:544/2900 train_loss:3.7019 train_time:359995ms step_avg:674.15ms
step:545/2900 train_loss:3.7741 train_time:360668ms step_avg:674.15ms
step:546/2900 train_loss:3.7052 train_time:361342ms step_avg:674.14ms
step:547/2900 train_loss:3.7608 train_time:362017ms step_avg:674.15ms
step:548/2900 train_loss:3.7624 train_time:362691ms step_avg:674.15ms
step:549/2900 train_loss:3.7340 train_time:363365ms step_avg:674.15ms
step:550/2900 train_loss:3.8398 train_time:364038ms step_avg:674.14ms
step:551/2900 train_loss:3.7297 train_time:364711ms step_avg:674.14ms
step:552/2900 train_loss:3.7422 train_time:365383ms step_avg:674.14ms
step:553/2900 train_loss:4.0814 train_time:366056ms step_avg:674.14ms
step:554/2900 train_loss:3.8657 train_time:366730ms step_avg:674.14ms
step:555/2900 train_loss:3.8244 train_time:367403ms step_avg:674.13ms
step:556/2900 train_loss:3.7646 train_time:368077ms step_avg:674.13ms
step:557/2900 train_loss:3.8064 train_time:368748ms step_avg:674.13ms
step:558/2900 train_loss:3.4583 train_time:369423ms step_avg:674.13ms
step:559/2900 train_loss:3.7309 train_time:370097ms step_avg:674.13ms
step:560/2900 train_loss:3.7704 train_time:370772ms step_avg:674.13ms
step:561/2900 train_loss:3.8121 train_time:371445ms step_avg:674.13ms
step:562/2900 train_loss:3.7336 train_time:372119ms step_avg:674.13ms
step:563/2900 train_loss:3.6726 train_time:372793ms step_avg:674.13ms
step:564/2900 train_loss:3.8768 train_time:373467ms step_avg:674.13ms
step:565/2900 train_loss:3.6839 train_time:374140ms step_avg:674.13ms
step:566/2900 train_loss:3.7983 train_time:374814ms step_avg:674.13ms
step:567/2900 train_loss:3.7471 train_time:375487ms step_avg:674.12ms
step:568/2900 train_loss:3.7193 train_time:376160ms step_avg:674.12ms
step:569/2900 train_loss:3.8000 train_time:376832ms step_avg:674.12ms
step:570/2900 train_loss:3.7685 train_time:377529ms step_avg:674.16ms
step:571/2900 train_loss:3.8026 train_time:378203ms step_avg:674.16ms
step:572/2900 train_loss:3.6961 train_time:379039ms step_avg:674.45ms
step:573/2900 train_loss:3.6424 train_time:379713ms step_avg:674.45ms
step:574/2900 train_loss:3.6603 train_time:380386ms step_avg:674.44ms
step:575/2900 train_loss:4.1312 train_time:381059ms step_avg:674.44ms
step:576/2900 train_loss:3.6701 train_time:381733ms step_avg:674.44ms
step:577/2900 train_loss:3.7441 train_time:382406ms step_avg:674.44ms
step:578/2900 train_loss:3.8941 train_time:383079ms step_avg:674.44ms
step:579/2900 train_loss:3.6882 train_time:383753ms step_avg:674.43ms
step:580/2900 train_loss:3.7351 train_time:384427ms step_avg:674.43ms
step:581/2900 train_loss:3.8369 train_time:385100ms step_avg:674.43ms
step:582/2900 train_loss:3.8516 train_time:385775ms step_avg:674.43ms
step:583/2900 train_loss:3.7923 train_time:386448ms step_avg:674.43ms
step:584/2900 train_loss:3.9687 train_time:387122ms step_avg:674.43ms
step:585/2900 train_loss:3.7544 train_time:387795ms step_avg:674.43ms
step:586/2900 train_loss:3.7690 train_time:388468ms step_avg:674.42ms
step:587/2900 train_loss:3.8034 train_time:389142ms step_avg:674.42ms
step:588/2900 train_loss:4.1982 train_time:389814ms step_avg:674.42ms
step:589/2900 train_loss:3.6328 train_time:390488ms step_avg:674.42ms
step:590/2900 train_loss:3.9947 train_time:391162ms step_avg:674.42ms
step:591/2900 train_loss:3.8080 train_time:391834ms step_avg:674.41ms
step:592/2900 train_loss:3.6548 train_time:392509ms step_avg:674.41ms
step:593/2900 train_loss:3.7490 train_time:393182ms step_avg:674.41ms
step:594/2900 train_loss:3.8475 train_time:393855ms step_avg:674.41ms
step:595/2900 train_loss:3.7346 train_time:394526ms step_avg:674.40ms
step:596/2900 train_loss:3.7615 train_time:395199ms step_avg:674.40ms
step:597/2900 train_loss:3.7914 train_time:395873ms step_avg:674.40ms
step:598/2900 train_loss:3.7966 train_time:396544ms step_avg:674.40ms
step:599/2900 train_loss:3.8041 train_time:397218ms step_avg:674.39ms
step:600/2900 train_loss:3.9118 train_time:397891ms step_avg:674.39ms
step:601/2900 train_loss:3.7647 train_time:398564ms step_avg:674.39ms
step:602/2900 train_loss:3.7218 train_time:399236ms step_avg:674.39ms
step:603/2900 train_loss:3.8753 train_time:399910ms step_avg:674.38ms
step:604/2900 train_loss:3.6786 train_time:400582ms step_avg:674.38ms
step:605/2900 train_loss:3.7850 train_time:401256ms step_avg:674.38ms
step:606/2900 train_loss:4.0915 train_time:401929ms step_avg:674.38ms
step:607/2900 train_loss:3.8234 train_time:402602ms step_avg:674.38ms
step:608/2900 train_loss:3.8257 train_time:403274ms step_avg:674.37ms
step:609/2900 train_loss:3.7790 train_time:403946ms step_avg:674.37ms
step:610/2900 train_loss:3.6904 train_time:404619ms step_avg:674.37ms
step:611/2900 train_loss:3.7719 train_time:405292ms step_avg:674.36ms
step:612/2900 train_loss:3.7265 train_time:405964ms step_avg:674.36ms
step:613/2900 train_loss:3.8518 train_time:406637ms step_avg:674.36ms
step:614/2900 train_loss:3.6807 train_time:407309ms step_avg:674.35ms
step:615/2900 train_loss:3.7323 train_time:407982ms step_avg:674.35ms
step:616/2900 train_loss:3.6944 train_time:408655ms step_avg:674.35ms
step:617/2900 train_loss:3.7616 train_time:409328ms step_avg:674.35ms
step:618/2900 train_loss:3.8250 train_time:410000ms step_avg:674.34ms
step:619/2900 train_loss:3.5944 train_time:410672ms step_avg:674.34ms
step:620/2900 train_loss:3.8767 train_time:411346ms step_avg:674.34ms
step:621/2900 train_loss:3.7722 train_time:412018ms step_avg:674.33ms
step:622/2900 train_loss:3.7713 train_time:412691ms step_avg:674.33ms
step:623/2900 train_loss:3.6018 train_time:413363ms step_avg:674.33ms
step:624/2900 train_loss:3.7157 train_time:414035ms step_avg:674.32ms
step:625/2900 train_loss:3.8307 train_time:414708ms step_avg:674.32ms
step:625/2900 val_loss:3.7417 train_time:414720ms step_avg:674.34ms
step:626/2900 train_loss:3.7671 train_time:415383ms step_avg:674.32ms
step:627/2900 train_loss:3.7374 train_time:416056ms step_avg:674.32ms
step:628/2900 train_loss:3.6503 train_time:416728ms step_avg:674.32ms
step:629/2900 train_loss:3.7829 train_time:417401ms step_avg:674.31ms
step:630/2900 train_loss:3.6310 train_time:418073ms step_avg:674.31ms
step:631/2900 train_loss:3.8154 train_time:418746ms step_avg:674.31ms
step:632/2900 train_loss:3.6057 train_time:419418ms step_avg:674.31ms
step:633/2900 train_loss:3.7480 train_time:420090ms step_avg:674.30ms
step:634/2900 train_loss:3.9037 train_time:420762ms step_avg:674.30ms
step:635/2900 train_loss:3.7320 train_time:421435ms step_avg:674.30ms
step:636/2900 train_loss:3.6482 train_time:422107ms step_avg:674.29ms
step:637/2900 train_loss:3.7603 train_time:422779ms step_avg:674.29ms
step:638/2900 train_loss:3.6653 train_time:423451ms step_avg:674.28ms
step:639/2900 train_loss:3.7227 train_time:424124ms step_avg:674.28ms
step:640/2900 train_loss:3.7684 train_time:424795ms step_avg:674.28ms
step:641/2900 train_loss:3.6801 train_time:425467ms step_avg:674.27ms
step:642/2900 train_loss:3.6642 train_time:426138ms step_avg:674.27ms
step:643/2900 train_loss:3.8517 train_time:426810ms step_avg:674.27ms
step:644/2900 train_loss:3.5318 train_time:427481ms step_avg:674.26ms
step:645/2900 train_loss:3.7319 train_time:428155ms step_avg:674.26ms
step:646/2900 train_loss:3.6960 train_time:428826ms step_avg:674.25ms
step:647/2900 train_loss:3.8431 train_time:429499ms step_avg:674.25ms
step:648/2900 train_loss:3.7580 train_time:430170ms step_avg:674.25ms
step:649/2900 train_loss:3.9336 train_time:430843ms step_avg:674.25ms
step:650/2900 train_loss:3.7240 train_time:431514ms step_avg:674.24ms
step:651/2900 train_loss:3.5766 train_time:432186ms step_avg:674.24ms
step:652/2900 train_loss:3.6520 train_time:432856ms step_avg:674.23ms
step:653/2900 train_loss:3.7399 train_time:433529ms step_avg:674.23ms
step:654/2900 train_loss:3.7196 train_time:434201ms step_avg:674.23ms
step:655/2900 train_loss:3.8750 train_time:434873ms step_avg:674.22ms
step:656/2900 train_loss:3.8787 train_time:435545ms step_avg:674.22ms
step:657/2900 train_loss:3.8017 train_time:436217ms step_avg:674.22ms
step:658/2900 train_loss:3.7170 train_time:436888ms step_avg:674.21ms
step:659/2900 train_loss:3.5679 train_time:437560ms step_avg:674.21ms
step:660/2900 train_loss:3.7548 train_time:438231ms step_avg:674.20ms
step:661/2900 train_loss:3.8318 train_time:438905ms step_avg:674.20ms
step:662/2900 train_loss:3.8225 train_time:439576ms step_avg:674.20ms
step:663/2900 train_loss:3.6275 train_time:440248ms step_avg:674.19ms
step:664/2900 train_loss:3.7703 train_time:440919ms step_avg:674.19ms
step:665/2900 train_loss:3.7802 train_time:441590ms step_avg:674.18ms
step:666/2900 train_loss:3.6659 train_time:442262ms step_avg:674.18ms
step:667/2900 train_loss:3.7668 train_time:442934ms step_avg:674.18ms
step:668/2900 train_loss:3.8668 train_time:443605ms step_avg:674.17ms
step:669/2900 train_loss:3.8473 train_time:444277ms step_avg:674.17ms
step:670/2900 train_loss:3.6557 train_time:444950ms step_avg:674.17ms
step:671/2900 train_loss:3.8359 train_time:445622ms step_avg:674.16ms
step:672/2900 train_loss:3.7528 train_time:446295ms step_avg:674.16ms
step:673/2900 train_loss:3.5567 train_time:446965ms step_avg:674.16ms
step:674/2900 train_loss:4.0249 train_time:447637ms step_avg:674.15ms
step:675/2900 train_loss:3.7963 train_time:448309ms step_avg:674.15ms
step:676/2900 train_loss:3.7376 train_time:448981ms step_avg:674.15ms
step:677/2900 train_loss:3.7926 train_time:449652ms step_avg:674.14ms
step:678/2900 train_loss:3.7014 train_time:450322ms step_avg:674.14ms
step:679/2900 train_loss:3.5761 train_time:450995ms step_avg:674.13ms
step:680/2900 train_loss:3.6763 train_time:451667ms step_avg:674.13ms
step:681/2900 train_loss:3.7826 train_time:452338ms step_avg:674.13ms
step:682/2900 train_loss:3.8917 train_time:453009ms step_avg:674.12ms
step:683/2900 train_loss:3.6470 train_time:453682ms step_avg:674.12ms
step:684/2900 train_loss:3.7501 train_time:454353ms step_avg:674.11ms
step:685/2900 train_loss:3.7885 train_time:455023ms step_avg:674.11ms
step:686/2900 train_loss:3.7128 train_time:455695ms step_avg:674.11ms
step:687/2900 train_loss:3.7919 train_time:456367ms step_avg:674.10ms
step:688/2900 train_loss:3.6973 train_time:457040ms step_avg:674.10ms
step:689/2900 train_loss:3.6584 train_time:457712ms step_avg:674.10ms
step:690/2900 train_loss:3.7382 train_time:458383ms step_avg:674.09ms
step:691/2900 train_loss:3.5919 train_time:459054ms step_avg:674.09ms
step:692/2900 train_loss:3.7867 train_time:459725ms step_avg:674.08ms
step:693/2900 train_loss:3.7879 train_time:460397ms step_avg:674.08ms
step:694/2900 train_loss:3.5493 train_time:461069ms step_avg:674.08ms
step:695/2900 train_loss:3.7918 train_time:461740ms step_avg:674.07ms
step:696/2900 train_loss:3.7306 train_time:462413ms step_avg:674.07ms
step:697/2900 train_loss:3.4844 train_time:463086ms step_avg:674.07ms
step:698/2900 train_loss:3.8299 train_time:463757ms step_avg:674.07ms
step:699/2900 train_loss:3.7072 train_time:464430ms step_avg:674.06ms
step:700/2900 train_loss:3.6977 train_time:465101ms step_avg:674.06ms
step:701/2900 train_loss:3.6669 train_time:465773ms step_avg:674.06ms
step:702/2900 train_loss:3.7026 train_time:466444ms step_avg:674.05ms
step:703/2900 train_loss:3.2271 train_time:467116ms step_avg:674.05ms
step:704/2900 train_loss:3.7112 train_time:467788ms step_avg:674.05ms
step:705/2900 train_loss:3.5423 train_time:468460ms step_avg:674.04ms
step:706/2900 train_loss:3.8318 train_time:469132ms step_avg:674.04ms
step:707/2900 train_loss:3.6737 train_time:469803ms step_avg:674.04ms
step:708/2900 train_loss:3.7112 train_time:470475ms step_avg:674.03ms
step:709/2900 train_loss:3.7167 train_time:471148ms step_avg:674.03ms
step:710/2900 train_loss:3.7542 train_time:471821ms step_avg:674.03ms
step:711/2900 train_loss:3.7884 train_time:472493ms step_avg:674.03ms
step:712/2900 train_loss:3.6483 train_time:473164ms step_avg:674.02ms
step:713/2900 train_loss:3.6678 train_time:473835ms step_avg:674.02ms
step:714/2900 train_loss:3.6568 train_time:474508ms step_avg:674.02ms
step:715/2900 train_loss:3.6887 train_time:475180ms step_avg:674.01ms
step:716/2900 train_loss:3.5450 train_time:475851ms step_avg:674.01ms
step:717/2900 train_loss:3.9747 train_time:476522ms step_avg:674.01ms
step:718/2900 train_loss:3.8410 train_time:477193ms step_avg:674.00ms
step:719/2900 train_loss:3.8270 train_time:477864ms step_avg:674.00ms
step:720/2900 train_loss:3.7943 train_time:478536ms step_avg:673.99ms
step:721/2900 train_loss:3.7388 train_time:479208ms step_avg:673.99ms
step:722/2900 train_loss:3.6159 train_time:479878ms step_avg:673.99ms
step:723/2900 train_loss:3.7744 train_time:480550ms step_avg:673.98ms
step:724/2900 train_loss:3.6933 train_time:481221ms step_avg:673.98ms
step:725/2900 train_loss:3.7030 train_time:481893ms step_avg:673.98ms
step:726/2900 train_loss:3.6771 train_time:482565ms step_avg:673.97ms
step:727/2900 train_loss:3.5933 train_time:483235ms step_avg:673.97ms
step:728/2900 train_loss:3.7674 train_time:483907ms step_avg:673.96ms
step:729/2900 train_loss:3.7068 train_time:484578ms step_avg:673.96ms
step:730/2900 train_loss:3.5142 train_time:485251ms step_avg:673.96ms
step:731/2900 train_loss:3.6754 train_time:485922ms step_avg:673.96ms
step:732/2900 train_loss:3.7873 train_time:486593ms step_avg:673.95ms
step:733/2900 train_loss:3.7680 train_time:487264ms step_avg:673.95ms
step:734/2900 train_loss:3.8037 train_time:487935ms step_avg:673.94ms
step:735/2900 train_loss:3.6311 train_time:488607ms step_avg:673.94ms
step:736/2900 train_loss:3.6424 train_time:489277ms step_avg:673.94ms
step:737/2900 train_loss:3.6937 train_time:489949ms step_avg:673.93ms
step:738/2900 train_loss:3.8288 train_time:490619ms step_avg:673.93ms
step:739/2900 train_loss:3.7379 train_time:491290ms step_avg:673.92ms
step:740/2900 train_loss:3.7075 train_time:491960ms step_avg:673.92ms
step:741/2900 train_loss:3.7612 train_time:492632ms step_avg:673.91ms
step:742/2900 train_loss:3.6990 train_time:493303ms step_avg:673.91ms
step:743/2900 train_loss:3.7041 train_time:493974ms step_avg:673.91ms
step:744/2900 train_loss:3.6096 train_time:494645ms step_avg:673.90ms
step:745/2900 train_loss:3.8639 train_time:495316ms step_avg:673.90ms
step:746/2900 train_loss:3.6482 train_time:495987ms step_avg:673.90ms
step:747/2900 train_loss:3.6775 train_time:496659ms step_avg:673.89ms
step:748/2900 train_loss:3.7680 train_time:497331ms step_avg:673.89ms
step:749/2900 train_loss:3.6914 train_time:498003ms step_avg:673.89ms
step:750/2900 train_loss:3.7609 train_time:498674ms step_avg:673.88ms
step:750/2900 val_loss:3.6835 train_time:498686ms step_avg:673.90ms
step:751/2900 train_loss:3.6686 train_time:499347ms step_avg:673.88ms
step:752/2900 train_loss:3.7856 train_time:500018ms step_avg:673.88ms
step:753/2900 train_loss:3.6504 train_time:500690ms step_avg:673.88ms
step:754/2900 train_loss:3.6816 train_time:501360ms step_avg:673.87ms
step:755/2900 train_loss:3.6439 train_time:502032ms step_avg:673.87ms
step:756/2900 train_loss:3.7088 train_time:502703ms step_avg:673.86ms
