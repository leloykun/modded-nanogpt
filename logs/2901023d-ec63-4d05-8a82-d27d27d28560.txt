====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.zeros(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 2900 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 950 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]+[raw_model.v_skip_weights]
optimizer3 = Muon(matrix_params, lr=0.04, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    print(f"{logfile = }")
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # log lambdas
        if master_process:
            skip_weights_str = str(model.module.skip_weights)
            v_skip_weights_str = str(model.module.v_skip_weights)
            print(f"{skip_weights_str = }")
            print(f"{v_skip_weights_str = }")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 17:52:52 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   34C    P0             83W /  400W |    3337MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   37C    P0             77W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   34C    P0            101W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   43C    P0             76W /  400W |    3263MiB /  81920MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2900 val_loss:10.8258 train_time:271ms step_avg:nanms
step:1/2900 train_loss:10.8258 train_time:16862ms step_avg:nanms
step:2/2900 train_loss:10.1811 train_time:17515ms step_avg:nanms
step:3/2900 train_loss:8.4866 train_time:18180ms step_avg:nanms
step:4/2900 train_loss:7.5608 train_time:18849ms step_avg:nanms
step:5/2900 train_loss:7.3583 train_time:19518ms step_avg:nanms
step:6/2900 train_loss:7.0012 train_time:20187ms step_avg:nanms
step:7/2900 train_loss:7.2020 train_time:20857ms step_avg:nanms
step:8/2900 train_loss:6.7778 train_time:21528ms step_avg:nanms
step:9/2900 train_loss:6.6835 train_time:22200ms step_avg:nanms
step:10/2900 train_loss:6.6515 train_time:22870ms step_avg:nanms
step:11/2900 train_loss:6.5834 train_time:661ms step_avg:nanms
step:12/2900 train_loss:6.4178 train_time:1333ms step_avg:nanms
step:13/2900 train_loss:6.3998 train_time:2005ms step_avg:668.41ms
step:14/2900 train_loss:6.3311 train_time:2678ms step_avg:669.58ms
step:15/2900 train_loss:6.2985 train_time:3351ms step_avg:670.28ms
step:16/2900 train_loss:6.2629 train_time:4024ms step_avg:670.62ms
step:17/2900 train_loss:6.3538 train_time:4696ms step_avg:670.92ms
step:18/2900 train_loss:6.1584 train_time:5369ms step_avg:671.18ms
step:19/2900 train_loss:6.1980 train_time:6042ms step_avg:671.30ms
step:20/2900 train_loss:5.8754 train_time:6715ms step_avg:671.55ms
step:21/2900 train_loss:6.1721 train_time:7388ms step_avg:671.66ms
step:22/2900 train_loss:6.4179 train_time:8062ms step_avg:671.80ms
step:23/2900 train_loss:6.0922 train_time:8736ms step_avg:672.00ms
step:24/2900 train_loss:6.2330 train_time:9409ms step_avg:672.08ms
step:25/2900 train_loss:5.9263 train_time:10084ms step_avg:672.27ms
step:26/2900 train_loss:5.8485 train_time:10758ms step_avg:672.38ms
step:27/2900 train_loss:6.0915 train_time:11433ms step_avg:672.51ms
step:28/2900 train_loss:5.7133 train_time:12107ms step_avg:672.61ms
step:29/2900 train_loss:5.9427 train_time:12781ms step_avg:672.70ms
step:30/2900 train_loss:5.7715 train_time:13455ms step_avg:672.76ms
step:31/2900 train_loss:5.7401 train_time:14129ms step_avg:672.81ms
step:32/2900 train_loss:5.6130 train_time:14804ms step_avg:672.93ms
step:33/2900 train_loss:5.8741 train_time:15479ms step_avg:672.98ms
step:34/2900 train_loss:5.7983 train_time:16153ms step_avg:673.06ms
step:35/2900 train_loss:5.9280 train_time:16828ms step_avg:673.10ms
step:36/2900 train_loss:5.8627 train_time:17502ms step_avg:673.17ms
step:37/2900 train_loss:5.7537 train_time:18177ms step_avg:673.22ms
step:38/2900 train_loss:5.6231 train_time:18851ms step_avg:673.26ms
step:39/2900 train_loss:5.6173 train_time:19525ms step_avg:673.28ms
step:40/2900 train_loss:5.5486 train_time:20200ms step_avg:673.32ms
step:41/2900 train_loss:5.5496 train_time:20874ms step_avg:673.34ms
step:42/2900 train_loss:5.4487 train_time:21548ms step_avg:673.38ms
step:43/2900 train_loss:5.5399 train_time:22223ms step_avg:673.43ms
step:44/2900 train_loss:5.5118 train_time:22899ms step_avg:673.49ms
step:45/2900 train_loss:5.6424 train_time:23574ms step_avg:673.53ms
step:46/2900 train_loss:5.4628 train_time:24248ms step_avg:673.55ms
step:47/2900 train_loss:5.3301 train_time:24922ms step_avg:673.57ms
step:48/2900 train_loss:5.5052 train_time:25597ms step_avg:673.59ms
step:49/2900 train_loss:5.3984 train_time:26271ms step_avg:673.62ms
step:50/2900 train_loss:5.5342 train_time:26947ms step_avg:673.67ms
step:51/2900 train_loss:5.4049 train_time:27621ms step_avg:673.69ms
step:52/2900 train_loss:5.2649 train_time:28296ms step_avg:673.71ms
step:53/2900 train_loss:5.3946 train_time:28970ms step_avg:673.71ms
step:54/2900 train_loss:5.2511 train_time:29645ms step_avg:673.74ms
step:55/2900 train_loss:5.6359 train_time:30320ms step_avg:673.77ms
step:56/2900 train_loss:5.2569 train_time:30993ms step_avg:673.77ms
step:57/2900 train_loss:5.1194 train_time:31668ms step_avg:673.79ms
step:58/2900 train_loss:5.2474 train_time:32344ms step_avg:673.82ms
step:59/2900 train_loss:5.2467 train_time:33017ms step_avg:673.82ms
step:60/2900 train_loss:5.3782 train_time:33694ms step_avg:673.87ms
step:61/2900 train_loss:5.1064 train_time:34368ms step_avg:673.88ms
step:62/2900 train_loss:5.1966 train_time:35043ms step_avg:673.90ms
step:63/2900 train_loss:5.1810 train_time:35718ms step_avg:673.92ms
step:64/2900 train_loss:5.1996 train_time:36392ms step_avg:673.93ms
step:65/2900 train_loss:5.0056 train_time:37067ms step_avg:673.94ms
step:66/2900 train_loss:5.1657 train_time:37742ms step_avg:673.97ms
step:67/2900 train_loss:5.0257 train_time:38418ms step_avg:674.00ms
step:68/2900 train_loss:5.3357 train_time:39091ms step_avg:673.99ms
step:69/2900 train_loss:4.9397 train_time:39766ms step_avg:674.00ms
step:70/2900 train_loss:5.0249 train_time:40442ms step_avg:674.03ms
step:71/2900 train_loss:5.1936 train_time:41117ms step_avg:674.05ms
step:72/2900 train_loss:5.1069 train_time:41793ms step_avg:674.08ms
step:73/2900 train_loss:4.9594 train_time:42467ms step_avg:674.07ms
step:74/2900 train_loss:5.0957 train_time:43141ms step_avg:674.08ms
step:75/2900 train_loss:5.0845 train_time:43815ms step_avg:674.08ms
step:76/2900 train_loss:5.0070 train_time:44491ms step_avg:674.10ms
step:77/2900 train_loss:5.1206 train_time:45166ms step_avg:674.11ms
step:78/2900 train_loss:5.2890 train_time:45840ms step_avg:674.11ms
step:79/2900 train_loss:5.0107 train_time:46514ms step_avg:674.12ms
step:80/2900 train_loss:5.0583 train_time:47190ms step_avg:674.14ms
step:81/2900 train_loss:4.8371 train_time:47864ms step_avg:674.14ms
step:82/2900 train_loss:5.0265 train_time:48539ms step_avg:674.15ms
step:83/2900 train_loss:4.9699 train_time:49212ms step_avg:674.14ms
step:84/2900 train_loss:4.9581 train_time:49887ms step_avg:674.14ms
step:85/2900 train_loss:4.8255 train_time:50561ms step_avg:674.14ms
step:86/2900 train_loss:5.0231 train_time:51235ms step_avg:674.14ms
step:87/2900 train_loss:4.9236 train_time:51909ms step_avg:674.14ms
step:88/2900 train_loss:4.9444 train_time:52583ms step_avg:674.14ms
step:89/2900 train_loss:4.8957 train_time:53257ms step_avg:674.14ms
step:90/2900 train_loss:4.8321 train_time:53931ms step_avg:674.14ms
step:91/2900 train_loss:4.8248 train_time:54605ms step_avg:674.14ms
step:92/2900 train_loss:4.9694 train_time:55279ms step_avg:674.13ms
step:93/2900 train_loss:4.7824 train_time:55952ms step_avg:674.12ms
step:94/2900 train_loss:4.8144 train_time:56627ms step_avg:674.13ms
step:95/2900 train_loss:4.8480 train_time:57300ms step_avg:674.12ms
step:96/2900 train_loss:4.7367 train_time:57974ms step_avg:674.12ms
step:97/2900 train_loss:4.7986 train_time:58648ms step_avg:674.11ms
step:98/2900 train_loss:4.7322 train_time:59322ms step_avg:674.11ms
step:99/2900 train_loss:4.8405 train_time:59995ms step_avg:674.10ms
step:100/2900 train_loss:4.8519 train_time:60670ms step_avg:674.11ms
step:101/2900 train_loss:4.7394 train_time:61343ms step_avg:674.10ms
step:102/2900 train_loss:4.8588 train_time:62017ms step_avg:674.10ms
step:103/2900 train_loss:4.7403 train_time:62691ms step_avg:674.09ms
step:104/2900 train_loss:4.6577 train_time:63363ms step_avg:674.08ms
step:105/2900 train_loss:4.6922 train_time:64038ms step_avg:674.08ms
step:106/2900 train_loss:4.7692 train_time:64711ms step_avg:674.08ms
step:107/2900 train_loss:4.6534 train_time:65386ms step_avg:674.08ms
step:108/2900 train_loss:4.4781 train_time:66059ms step_avg:674.07ms
step:109/2900 train_loss:4.6186 train_time:66735ms step_avg:674.09ms
step:110/2900 train_loss:4.6176 train_time:67407ms step_avg:674.07ms
step:111/2900 train_loss:4.5549 train_time:68082ms step_avg:674.08ms
step:112/2900 train_loss:4.7075 train_time:68755ms step_avg:674.07ms
step:113/2900 train_loss:4.5923 train_time:69428ms step_avg:674.06ms
step:114/2900 train_loss:4.4738 train_time:70102ms step_avg:674.06ms
step:115/2900 train_loss:4.6072 train_time:70776ms step_avg:674.05ms
step:116/2900 train_loss:4.5655 train_time:71449ms step_avg:674.05ms
step:117/2900 train_loss:4.4757 train_time:72123ms step_avg:674.04ms
step:118/2900 train_loss:4.6821 train_time:72797ms step_avg:674.05ms
step:119/2900 train_loss:4.5714 train_time:73470ms step_avg:674.04ms
step:120/2900 train_loss:4.4441 train_time:74146ms step_avg:674.05ms
step:121/2900 train_loss:4.3964 train_time:74819ms step_avg:674.05ms
step:122/2900 train_loss:4.5490 train_time:75496ms step_avg:674.07ms
step:123/2900 train_loss:4.3714 train_time:76169ms step_avg:674.06ms
step:124/2900 train_loss:4.6862 train_time:76842ms step_avg:674.05ms
step:125/2900 train_loss:4.5571 train_time:77516ms step_avg:674.05ms
step:125/2900 val_loss:4.4953 train_time:77528ms step_avg:674.16ms
step:126/2900 train_loss:4.5008 train_time:78191ms step_avg:674.06ms
step:127/2900 train_loss:4.5344 train_time:78865ms step_avg:674.06ms
step:128/2900 train_loss:4.4585 train_time:79539ms step_avg:674.06ms
step:129/2900 train_loss:4.7725 train_time:80212ms step_avg:674.05ms
step:130/2900 train_loss:4.4311 train_time:80886ms step_avg:674.05ms
step:131/2900 train_loss:4.4745 train_time:81559ms step_avg:674.04ms
step:132/2900 train_loss:4.4126 train_time:82232ms step_avg:674.04ms
step:133/2900 train_loss:4.5337 train_time:82906ms step_avg:674.03ms
step:134/2900 train_loss:4.3406 train_time:83580ms step_avg:674.03ms
step:135/2900 train_loss:4.5246 train_time:84253ms step_avg:674.03ms
step:136/2900 train_loss:4.2871 train_time:84926ms step_avg:674.02ms
step:137/2900 train_loss:4.4617 train_time:85600ms step_avg:674.02ms
step:138/2900 train_loss:4.3636 train_time:86274ms step_avg:674.02ms
step:139/2900 train_loss:4.4483 train_time:86947ms step_avg:674.01ms
step:140/2900 train_loss:4.5442 train_time:87621ms step_avg:674.01ms
step:141/2900 train_loss:4.3988 train_time:88295ms step_avg:674.01ms
step:142/2900 train_loss:4.3749 train_time:88968ms step_avg:674.00ms
step:143/2900 train_loss:4.3213 train_time:89644ms step_avg:674.01ms
step:144/2900 train_loss:4.4366 train_time:90317ms step_avg:674.01ms
step:145/2900 train_loss:4.3739 train_time:90993ms step_avg:674.02ms
step:146/2900 train_loss:4.2368 train_time:91666ms step_avg:674.01ms
step:147/2900 train_loss:4.3772 train_time:92340ms step_avg:674.01ms
step:148/2900 train_loss:4.4186 train_time:93014ms step_avg:674.02ms
step:149/2900 train_loss:4.3713 train_time:93687ms step_avg:674.01ms
step:150/2900 train_loss:4.4829 train_time:94361ms step_avg:674.01ms
step:151/2900 train_loss:4.3327 train_time:95036ms step_avg:674.01ms
step:152/2900 train_loss:4.3334 train_time:95709ms step_avg:674.01ms
step:153/2900 train_loss:4.4105 train_time:96384ms step_avg:674.01ms
step:154/2900 train_loss:4.4116 train_time:97058ms step_avg:674.01ms
step:155/2900 train_loss:4.3381 train_time:97732ms step_avg:674.01ms
step:156/2900 train_loss:4.4078 train_time:98405ms step_avg:674.01ms
step:157/2900 train_loss:4.4616 train_time:99078ms step_avg:674.00ms
step:158/2900 train_loss:4.2989 train_time:99752ms step_avg:674.00ms
step:159/2900 train_loss:4.3702 train_time:100426ms step_avg:674.00ms
step:160/2900 train_loss:4.1626 train_time:101100ms step_avg:674.00ms
step:161/2900 train_loss:4.4077 train_time:101774ms step_avg:674.00ms
step:162/2900 train_loss:4.4173 train_time:102448ms step_avg:674.00ms
step:163/2900 train_loss:4.3913 train_time:103121ms step_avg:674.00ms
step:164/2900 train_loss:4.2527 train_time:103795ms step_avg:673.99ms
step:165/2900 train_loss:4.3365 train_time:104469ms step_avg:674.00ms
step:166/2900 train_loss:4.4043 train_time:105144ms step_avg:674.00ms
step:167/2900 train_loss:4.2410 train_time:105819ms step_avg:674.00ms
step:168/2900 train_loss:4.3132 train_time:106493ms step_avg:674.01ms
step:169/2900 train_loss:4.2089 train_time:107167ms step_avg:674.00ms
step:170/2900 train_loss:4.1016 train_time:107841ms step_avg:674.00ms
step:171/2900 train_loss:4.2445 train_time:108514ms step_avg:674.00ms
step:172/2900 train_loss:4.2621 train_time:109189ms step_avg:674.00ms
step:173/2900 train_loss:4.3059 train_time:109862ms step_avg:674.00ms
step:174/2900 train_loss:4.4742 train_time:110535ms step_avg:674.00ms
step:175/2900 train_loss:4.2928 train_time:111209ms step_avg:673.99ms
step:176/2900 train_loss:4.1524 train_time:111883ms step_avg:673.99ms
step:177/2900 train_loss:4.1298 train_time:112556ms step_avg:673.99ms
step:178/2900 train_loss:4.2496 train_time:113231ms step_avg:673.99ms
step:179/2900 train_loss:4.1988 train_time:113904ms step_avg:673.99ms
step:180/2900 train_loss:4.1688 train_time:114577ms step_avg:673.98ms
step:181/2900 train_loss:4.3499 train_time:115251ms step_avg:673.98ms
step:182/2900 train_loss:4.1978 train_time:115925ms step_avg:673.98ms
step:183/2900 train_loss:4.1860 train_time:116599ms step_avg:673.98ms
step:184/2900 train_loss:4.1789 train_time:117271ms step_avg:673.97ms
step:185/2900 train_loss:4.2562 train_time:117946ms step_avg:673.98ms
step:186/2900 train_loss:4.2305 train_time:118619ms step_avg:673.97ms
step:187/2900 train_loss:4.2891 train_time:119293ms step_avg:673.97ms
step:188/2900 train_loss:4.2229 train_time:119966ms step_avg:673.97ms
step:189/2900 train_loss:4.1535 train_time:120640ms step_avg:673.96ms
step:190/2900 train_loss:4.2436 train_time:121321ms step_avg:674.01ms
step:191/2900 train_loss:4.3349 train_time:122175ms step_avg:675.00ms
step:192/2900 train_loss:4.1319 train_time:122849ms step_avg:674.99ms
step:193/2900 train_loss:4.2055 train_time:123522ms step_avg:674.98ms
step:194/2900 train_loss:4.1557 train_time:124197ms step_avg:674.98ms
step:195/2900 train_loss:4.1200 train_time:124870ms step_avg:674.97ms
step:196/2900 train_loss:4.1292 train_time:125543ms step_avg:674.96ms
step:197/2900 train_loss:4.1706 train_time:126217ms step_avg:674.96ms
step:198/2900 train_loss:4.0660 train_time:126891ms step_avg:674.95ms
step:199/2900 train_loss:4.3010 train_time:127565ms step_avg:674.95ms
step:200/2900 train_loss:4.2315 train_time:128239ms step_avg:674.94ms
step:201/2900 train_loss:4.9509 train_time:128914ms step_avg:674.94ms
step:202/2900 train_loss:4.4265 train_time:129588ms step_avg:674.94ms
step:203/2900 train_loss:4.1884 train_time:130263ms step_avg:674.94ms
step:204/2900 train_loss:4.1195 train_time:130937ms step_avg:674.93ms
step:205/2900 train_loss:4.1634 train_time:131610ms step_avg:674.92ms
step:206/2900 train_loss:4.1325 train_time:132285ms step_avg:674.92ms
step:207/2900 train_loss:4.1320 train_time:132958ms step_avg:674.91ms
step:208/2900 train_loss:4.0780 train_time:133631ms step_avg:674.91ms
step:209/2900 train_loss:4.2563 train_time:134304ms step_avg:674.90ms
step:210/2900 train_loss:4.0638 train_time:134979ms step_avg:674.89ms
step:211/2900 train_loss:4.1794 train_time:135652ms step_avg:674.89ms
step:212/2900 train_loss:4.1777 train_time:136325ms step_avg:674.87ms
step:213/2900 train_loss:4.1188 train_time:136999ms step_avg:674.87ms
step:214/2900 train_loss:4.0558 train_time:137674ms step_avg:674.87ms
step:215/2900 train_loss:4.2017 train_time:138348ms step_avg:674.87ms
step:216/2900 train_loss:4.1302 train_time:139020ms step_avg:674.85ms
step:217/2900 train_loss:4.1231 train_time:139695ms step_avg:674.85ms
step:218/2900 train_loss:4.1456 train_time:140367ms step_avg:674.84ms
step:219/2900 train_loss:4.0625 train_time:141042ms step_avg:674.84ms
step:220/2900 train_loss:4.1797 train_time:141716ms step_avg:674.84ms
step:221/2900 train_loss:4.0790 train_time:142391ms step_avg:674.84ms
step:222/2900 train_loss:4.1671 train_time:143064ms step_avg:674.83ms
step:223/2900 train_loss:4.0798 train_time:143738ms step_avg:674.83ms
step:224/2900 train_loss:4.1000 train_time:144411ms step_avg:674.82ms
step:225/2900 train_loss:4.0815 train_time:145085ms step_avg:674.81ms
step:226/2900 train_loss:4.1075 train_time:145758ms step_avg:674.81ms
step:227/2900 train_loss:4.0912 train_time:146431ms step_avg:674.80ms
step:228/2900 train_loss:4.0145 train_time:147106ms step_avg:674.80ms
step:229/2900 train_loss:4.2404 train_time:147779ms step_avg:674.79ms
step:230/2900 train_loss:4.1142 train_time:148453ms step_avg:674.79ms
step:231/2900 train_loss:3.9823 train_time:149127ms step_avg:674.78ms
step:232/2900 train_loss:4.2117 train_time:149803ms step_avg:674.79ms
step:233/2900 train_loss:4.0359 train_time:150476ms step_avg:674.78ms
step:234/2900 train_loss:4.0600 train_time:151151ms step_avg:674.78ms
step:235/2900 train_loss:4.2313 train_time:151827ms step_avg:674.79ms
step:236/2900 train_loss:4.1255 train_time:152501ms step_avg:674.78ms
step:237/2900 train_loss:4.2220 train_time:153174ms step_avg:674.77ms
step:238/2900 train_loss:4.1549 train_time:153848ms step_avg:674.77ms
step:239/2900 train_loss:4.1562 train_time:154521ms step_avg:674.76ms
step:240/2900 train_loss:4.1144 train_time:155195ms step_avg:674.76ms
step:241/2900 train_loss:4.3091 train_time:155871ms step_avg:674.77ms
step:242/2900 train_loss:4.1365 train_time:156544ms step_avg:674.76ms
step:243/2900 train_loss:4.4782 train_time:157217ms step_avg:674.75ms
step:244/2900 train_loss:4.0849 train_time:157890ms step_avg:674.74ms
step:245/2900 train_loss:4.1168 train_time:158564ms step_avg:674.74ms
step:246/2900 train_loss:4.1455 train_time:159239ms step_avg:674.74ms
step:247/2900 train_loss:4.0446 train_time:159913ms step_avg:674.74ms
step:248/2900 train_loss:4.0653 train_time:160587ms step_avg:674.73ms
step:249/2900 train_loss:4.0002 train_time:161261ms step_avg:674.73ms
step:250/2900 train_loss:4.1664 train_time:161935ms step_avg:674.73ms
step:250/2900 val_loss:4.0654 train_time:161946ms step_avg:674.78ms
step:251/2900 train_loss:4.0506 train_time:162608ms step_avg:674.72ms
step:252/2900 train_loss:3.9678 train_time:163283ms step_avg:674.72ms
step:253/2900 train_loss:3.9927 train_time:163956ms step_avg:674.72ms
step:254/2900 train_loss:4.0700 train_time:164630ms step_avg:674.71ms
step:255/2900 train_loss:4.0031 train_time:165304ms step_avg:674.71ms
step:256/2900 train_loss:3.9404 train_time:165977ms step_avg:674.70ms
step:257/2900 train_loss:4.0072 train_time:166650ms step_avg:674.70ms
step:258/2900 train_loss:4.1348 train_time:167324ms step_avg:674.69ms
step:259/2900 train_loss:3.9455 train_time:167997ms step_avg:674.69ms
step:260/2900 train_loss:4.0527 train_time:168671ms step_avg:674.68ms
step:261/2900 train_loss:4.0037 train_time:169344ms step_avg:674.68ms
step:262/2900 train_loss:4.0338 train_time:170017ms step_avg:674.67ms
step:263/2900 train_loss:4.0937 train_time:170690ms step_avg:674.66ms
step:264/2900 train_loss:4.1347 train_time:171363ms step_avg:674.66ms
step:265/2900 train_loss:4.5143 train_time:172036ms step_avg:674.65ms
step:266/2900 train_loss:4.0377 train_time:172709ms step_avg:674.64ms
step:267/2900 train_loss:4.2348 train_time:173383ms step_avg:674.64ms
step:268/2900 train_loss:3.9756 train_time:174055ms step_avg:674.63ms
step:269/2900 train_loss:3.9927 train_time:174729ms step_avg:674.63ms
step:270/2900 train_loss:4.1028 train_time:175403ms step_avg:674.63ms
step:271/2900 train_loss:4.1531 train_time:176075ms step_avg:674.62ms
step:272/2900 train_loss:4.0263 train_time:176749ms step_avg:674.62ms
step:273/2900 train_loss:3.9841 train_time:177423ms step_avg:674.61ms
step:274/2900 train_loss:3.9858 train_time:178095ms step_avg:674.60ms
step:275/2900 train_loss:3.9625 train_time:178769ms step_avg:674.60ms
step:276/2900 train_loss:4.0723 train_time:179443ms step_avg:674.60ms
step:277/2900 train_loss:4.1092 train_time:180115ms step_avg:674.59ms
step:278/2900 train_loss:3.8948 train_time:180789ms step_avg:674.59ms
step:279/2900 train_loss:4.2020 train_time:181462ms step_avg:674.58ms
step:280/2900 train_loss:4.0468 train_time:182135ms step_avg:674.57ms
step:281/2900 train_loss:4.0508 train_time:182810ms step_avg:674.58ms
step:282/2900 train_loss:4.0023 train_time:183483ms step_avg:674.57ms
step:283/2900 train_loss:4.2208 train_time:184156ms step_avg:674.57ms
step:284/2900 train_loss:3.9538 train_time:184831ms step_avg:674.56ms
step:285/2900 train_loss:3.9269 train_time:185503ms step_avg:674.56ms
step:286/2900 train_loss:4.1061 train_time:186176ms step_avg:674.55ms
step:287/2900 train_loss:4.0389 train_time:186849ms step_avg:674.54ms
step:288/2900 train_loss:4.0624 train_time:187521ms step_avg:674.54ms
step:289/2900 train_loss:4.1072 train_time:188194ms step_avg:674.53ms
step:290/2900 train_loss:4.0979 train_time:188868ms step_avg:674.53ms
step:291/2900 train_loss:3.9362 train_time:189540ms step_avg:674.52ms
step:292/2900 train_loss:3.9177 train_time:190213ms step_avg:674.52ms
step:293/2900 train_loss:4.1080 train_time:190885ms step_avg:674.51ms
step:294/2900 train_loss:3.9067 train_time:191559ms step_avg:674.50ms
step:295/2900 train_loss:4.1983 train_time:192233ms step_avg:674.50ms
step:296/2900 train_loss:4.0833 train_time:192906ms step_avg:674.50ms
step:297/2900 train_loss:3.9122 train_time:193579ms step_avg:674.49ms
step:298/2900 train_loss:4.0333 train_time:194251ms step_avg:674.48ms
step:299/2900 train_loss:3.9401 train_time:194923ms step_avg:674.47ms
step:300/2900 train_loss:4.1225 train_time:195596ms step_avg:674.47ms
step:301/2900 train_loss:3.9107 train_time:196269ms step_avg:674.46ms
step:302/2900 train_loss:4.0316 train_time:196941ms step_avg:674.46ms
step:303/2900 train_loss:3.9662 train_time:197614ms step_avg:674.45ms
step:304/2900 train_loss:3.9612 train_time:198286ms step_avg:674.44ms
step:305/2900 train_loss:3.9560 train_time:198959ms step_avg:674.44ms
step:306/2900 train_loss:4.0725 train_time:199632ms step_avg:674.43ms
step:307/2900 train_loss:4.0141 train_time:200305ms step_avg:674.43ms
step:308/2900 train_loss:3.9202 train_time:200978ms step_avg:674.42ms
step:309/2900 train_loss:4.0943 train_time:201650ms step_avg:674.42ms
step:310/2900 train_loss:3.9494 train_time:202324ms step_avg:674.41ms
step:311/2900 train_loss:4.0276 train_time:202997ms step_avg:674.41ms
step:312/2900 train_loss:4.0127 train_time:203670ms step_avg:674.40ms
step:313/2900 train_loss:3.9165 train_time:204344ms step_avg:674.40ms
step:314/2900 train_loss:3.9985 train_time:205016ms step_avg:674.40ms
step:315/2900 train_loss:3.9953 train_time:205689ms step_avg:674.39ms
step:316/2900 train_loss:3.7288 train_time:206362ms step_avg:674.39ms
step:317/2900 train_loss:3.9804 train_time:207034ms step_avg:674.38ms
step:318/2900 train_loss:3.9632 train_time:207707ms step_avg:674.37ms
step:319/2900 train_loss:3.8813 train_time:208379ms step_avg:674.37ms
step:320/2900 train_loss:3.9140 train_time:209053ms step_avg:674.37ms
step:321/2900 train_loss:3.9626 train_time:209725ms step_avg:674.36ms
step:322/2900 train_loss:3.9772 train_time:210400ms step_avg:674.36ms
step:323/2900 train_loss:3.8510 train_time:211073ms step_avg:674.35ms
step:324/2900 train_loss:3.9292 train_time:211744ms step_avg:674.35ms
step:325/2900 train_loss:3.9860 train_time:212418ms step_avg:674.34ms
step:326/2900 train_loss:3.9964 train_time:213091ms step_avg:674.34ms
step:327/2900 train_loss:3.9676 train_time:213763ms step_avg:674.33ms
step:328/2900 train_loss:4.2363 train_time:214435ms step_avg:674.33ms
step:329/2900 train_loss:3.9887 train_time:215108ms step_avg:674.32ms
step:330/2900 train_loss:4.0333 train_time:215780ms step_avg:674.31ms
step:331/2900 train_loss:3.9024 train_time:216453ms step_avg:674.31ms
step:332/2900 train_loss:4.5307 train_time:217125ms step_avg:674.30ms
step:333/2900 train_loss:3.8965 train_time:217799ms step_avg:674.30ms
step:334/2900 train_loss:3.8035 train_time:218471ms step_avg:674.29ms
step:335/2900 train_loss:3.9744 train_time:219143ms step_avg:674.29ms
step:336/2900 train_loss:3.9958 train_time:219815ms step_avg:674.28ms
step:337/2900 train_loss:3.9906 train_time:220488ms step_avg:674.28ms
step:338/2900 train_loss:3.9628 train_time:221161ms step_avg:674.27ms
step:339/2900 train_loss:4.0135 train_time:221834ms step_avg:674.27ms
step:340/2900 train_loss:3.8809 train_time:222506ms step_avg:674.26ms
step:341/2900 train_loss:3.8745 train_time:223178ms step_avg:674.25ms
step:342/2900 train_loss:3.8891 train_time:223850ms step_avg:674.25ms
step:343/2900 train_loss:3.9296 train_time:224524ms step_avg:674.25ms
step:344/2900 train_loss:4.0170 train_time:225195ms step_avg:674.24ms
step:345/2900 train_loss:3.9448 train_time:225867ms step_avg:674.23ms
step:346/2900 train_loss:3.9187 train_time:226538ms step_avg:674.22ms
step:347/2900 train_loss:3.9727 train_time:227211ms step_avg:674.22ms
step:348/2900 train_loss:3.8974 train_time:227884ms step_avg:674.21ms
step:349/2900 train_loss:3.8942 train_time:228556ms step_avg:674.21ms
step:350/2900 train_loss:3.9578 train_time:229230ms step_avg:674.21ms
step:351/2900 train_loss:4.1949 train_time:229902ms step_avg:674.20ms
step:352/2900 train_loss:3.8388 train_time:230574ms step_avg:674.19ms
step:353/2900 train_loss:3.8864 train_time:231246ms step_avg:674.19ms
step:354/2900 train_loss:3.8803 train_time:231920ms step_avg:674.19ms
step:355/2900 train_loss:3.7973 train_time:232592ms step_avg:674.18ms
step:356/2900 train_loss:3.9270 train_time:233265ms step_avg:674.18ms
step:357/2900 train_loss:3.8525 train_time:233937ms step_avg:674.17ms
step:358/2900 train_loss:3.9765 train_time:234608ms step_avg:674.16ms
step:359/2900 train_loss:3.9573 train_time:235281ms step_avg:674.16ms
step:360/2900 train_loss:3.8731 train_time:235953ms step_avg:674.15ms
step:361/2900 train_loss:4.1369 train_time:236624ms step_avg:674.14ms
step:362/2900 train_loss:4.1217 train_time:237296ms step_avg:674.14ms
step:363/2900 train_loss:3.9424 train_time:237968ms step_avg:674.13ms
step:364/2900 train_loss:3.8323 train_time:238640ms step_avg:674.12ms
step:365/2900 train_loss:4.0064 train_time:239313ms step_avg:674.12ms
step:366/2900 train_loss:3.8469 train_time:239985ms step_avg:674.12ms
step:367/2900 train_loss:3.9408 train_time:240657ms step_avg:674.11ms
step:368/2900 train_loss:3.8702 train_time:241329ms step_avg:674.10ms
step:369/2900 train_loss:3.9571 train_time:242002ms step_avg:674.10ms
step:370/2900 train_loss:3.9350 train_time:242674ms step_avg:674.09ms
step:371/2900 train_loss:3.7409 train_time:243345ms step_avg:674.09ms
step:372/2900 train_loss:3.8551 train_time:244018ms step_avg:674.08ms
step:373/2900 train_loss:3.9179 train_time:244692ms step_avg:674.08ms
step:374/2900 train_loss:3.8775 train_time:245363ms step_avg:674.08ms
step:375/2900 train_loss:3.9651 train_time:246036ms step_avg:674.07ms
step:375/2900 val_loss:3.9021 train_time:246048ms step_avg:674.10ms
step:376/2900 train_loss:3.8759 train_time:246710ms step_avg:674.07ms
step:377/2900 train_loss:3.7806 train_time:247382ms step_avg:674.07ms
step:378/2900 train_loss:3.3765 train_time:248056ms step_avg:674.06ms
step:379/2900 train_loss:3.7465 train_time:248728ms step_avg:674.06ms
step:380/2900 train_loss:3.7619 train_time:249401ms step_avg:674.06ms
step:381/2900 train_loss:3.8640 train_time:250074ms step_avg:674.06ms
step:382/2900 train_loss:3.9173 train_time:250746ms step_avg:674.05ms
step:383/2900 train_loss:3.8807 train_time:251419ms step_avg:674.05ms
step:384/2900 train_loss:3.8636 train_time:252092ms step_avg:674.04ms
step:385/2900 train_loss:3.9402 train_time:252763ms step_avg:674.04ms
step:386/2900 train_loss:3.8633 train_time:253437ms step_avg:674.03ms
step:387/2900 train_loss:3.9632 train_time:254108ms step_avg:674.03ms
step:388/2900 train_loss:4.1455 train_time:254780ms step_avg:674.02ms
step:389/2900 train_loss:3.8768 train_time:255452ms step_avg:674.02ms
step:390/2900 train_loss:3.8654 train_time:256125ms step_avg:674.01ms
step:391/2900 train_loss:3.9622 train_time:256797ms step_avg:674.01ms
step:392/2900 train_loss:3.8803 train_time:257470ms step_avg:674.01ms
step:393/2900 train_loss:3.9955 train_time:258142ms step_avg:674.00ms
step:394/2900 train_loss:3.8326 train_time:258815ms step_avg:674.00ms
step:395/2900 train_loss:3.9631 train_time:259488ms step_avg:674.00ms
step:396/2900 train_loss:3.7040 train_time:260161ms step_avg:673.99ms
step:397/2900 train_loss:3.9176 train_time:260832ms step_avg:673.99ms
step:398/2900 train_loss:3.9510 train_time:261505ms step_avg:673.98ms
step:399/2900 train_loss:3.9505 train_time:262178ms step_avg:673.98ms
step:400/2900 train_loss:3.8533 train_time:262851ms step_avg:673.98ms
step:401/2900 train_loss:3.9018 train_time:263524ms step_avg:673.98ms
step:402/2900 train_loss:3.9919 train_time:264196ms step_avg:673.97ms
step:403/2900 train_loss:3.9153 train_time:264869ms step_avg:673.97ms
step:404/2900 train_loss:4.0396 train_time:265541ms step_avg:673.96ms
step:405/2900 train_loss:3.7688 train_time:266213ms step_avg:673.96ms
step:406/2900 train_loss:3.8691 train_time:266886ms step_avg:673.95ms
step:407/2900 train_loss:4.1731 train_time:267558ms step_avg:673.95ms
step:408/2900 train_loss:3.8659 train_time:268230ms step_avg:673.94ms
step:409/2900 train_loss:3.8988 train_time:268903ms step_avg:673.94ms
step:410/2900 train_loss:3.9360 train_time:269576ms step_avg:673.94ms
step:411/2900 train_loss:3.8291 train_time:270249ms step_avg:673.94ms
step:412/2900 train_loss:3.8458 train_time:270922ms step_avg:673.94ms
step:413/2900 train_loss:4.2741 train_time:271595ms step_avg:673.93ms
step:414/2900 train_loss:3.7245 train_time:272268ms step_avg:673.93ms
step:415/2900 train_loss:4.0888 train_time:272940ms step_avg:673.93ms
step:416/2900 train_loss:3.8328 train_time:273612ms step_avg:673.92ms
step:417/2900 train_loss:3.8464 train_time:274284ms step_avg:673.92ms
step:418/2900 train_loss:4.0313 train_time:274958ms step_avg:673.92ms
step:419/2900 train_loss:3.7671 train_time:275631ms step_avg:673.92ms
step:420/2900 train_loss:3.8912 train_time:276303ms step_avg:673.91ms
step:421/2900 train_loss:3.7934 train_time:276976ms step_avg:673.91ms
step:422/2900 train_loss:3.7278 train_time:277648ms step_avg:673.90ms
step:423/2900 train_loss:3.8587 train_time:278320ms step_avg:673.90ms
step:424/2900 train_loss:3.9508 train_time:278992ms step_avg:673.89ms
step:425/2900 train_loss:3.7038 train_time:279665ms step_avg:673.89ms
step:426/2900 train_loss:3.8856 train_time:280337ms step_avg:673.89ms
step:427/2900 train_loss:3.7584 train_time:281011ms step_avg:673.89ms
step:428/2900 train_loss:3.9854 train_time:281682ms step_avg:673.88ms
step:429/2900 train_loss:3.8982 train_time:282353ms step_avg:673.87ms
step:430/2900 train_loss:3.8399 train_time:283026ms step_avg:673.87ms
step:431/2900 train_loss:3.7996 train_time:283699ms step_avg:673.87ms
step:432/2900 train_loss:3.7068 train_time:284370ms step_avg:673.86ms
step:433/2900 train_loss:3.8507 train_time:285043ms step_avg:673.86ms
step:434/2900 train_loss:3.9000 train_time:285716ms step_avg:673.86ms
step:435/2900 train_loss:3.8524 train_time:286387ms step_avg:673.85ms
step:436/2900 train_loss:3.8944 train_time:287061ms step_avg:673.85ms
step:437/2900 train_loss:3.9156 train_time:287733ms step_avg:673.85ms
step:438/2900 train_loss:3.7859 train_time:288406ms step_avg:673.85ms
step:439/2900 train_loss:3.7942 train_time:289078ms step_avg:673.84ms
step:440/2900 train_loss:3.7893 train_time:289751ms step_avg:673.84ms
step:441/2900 train_loss:3.9708 train_time:290422ms step_avg:673.83ms
step:442/2900 train_loss:3.8419 train_time:291095ms step_avg:673.83ms
step:443/2900 train_loss:3.8303 train_time:291767ms step_avg:673.83ms
step:444/2900 train_loss:3.7286 train_time:292440ms step_avg:673.82ms
step:445/2900 train_loss:4.0086 train_time:293112ms step_avg:673.82ms
step:446/2900 train_loss:3.9297 train_time:293784ms step_avg:673.82ms
step:447/2900 train_loss:3.9195 train_time:294456ms step_avg:673.81ms
step:448/2900 train_loss:3.8395 train_time:295129ms step_avg:673.81ms
step:449/2900 train_loss:3.9513 train_time:295801ms step_avg:673.81ms
step:450/2900 train_loss:3.7714 train_time:296473ms step_avg:673.80ms
step:451/2900 train_loss:3.8066 train_time:297145ms step_avg:673.80ms
step:452/2900 train_loss:3.6735 train_time:297817ms step_avg:673.79ms
step:453/2900 train_loss:3.7901 train_time:298489ms step_avg:673.79ms
step:454/2900 train_loss:3.7609 train_time:299161ms step_avg:673.79ms
step:455/2900 train_loss:3.7153 train_time:299833ms step_avg:673.78ms
step:456/2900 train_loss:3.9367 train_time:300506ms step_avg:673.78ms
step:457/2900 train_loss:3.8171 train_time:301178ms step_avg:673.78ms
step:458/2900 train_loss:3.8856 train_time:301850ms step_avg:673.77ms
step:459/2900 train_loss:3.9251 train_time:302522ms step_avg:673.77ms
step:460/2900 train_loss:3.7228 train_time:303196ms step_avg:673.77ms
step:461/2900 train_loss:3.8874 train_time:303868ms step_avg:673.76ms
step:462/2900 train_loss:3.7922 train_time:304540ms step_avg:673.76ms
step:463/2900 train_loss:3.8163 train_time:305212ms step_avg:673.76ms
step:464/2900 train_loss:3.8627 train_time:305884ms step_avg:673.75ms
step:465/2900 train_loss:3.8089 train_time:306556ms step_avg:673.75ms
step:466/2900 train_loss:3.8097 train_time:307227ms step_avg:673.74ms
step:467/2900 train_loss:3.9011 train_time:307899ms step_avg:673.74ms
step:468/2900 train_loss:3.9158 train_time:308570ms step_avg:673.73ms
step:469/2900 train_loss:3.8948 train_time:309242ms step_avg:673.73ms
step:470/2900 train_loss:3.7814 train_time:309913ms step_avg:673.72ms
step:471/2900 train_loss:3.8605 train_time:310584ms step_avg:673.72ms
step:472/2900 train_loss:3.9173 train_time:311255ms step_avg:673.71ms
step:473/2900 train_loss:3.8702 train_time:311927ms step_avg:673.71ms
step:474/2900 train_loss:3.8063 train_time:312598ms step_avg:673.70ms
step:475/2900 train_loss:3.6745 train_time:313270ms step_avg:673.70ms
step:476/2900 train_loss:4.1136 train_time:313941ms step_avg:673.69ms
step:477/2900 train_loss:3.8593 train_time:314613ms step_avg:673.69ms
step:478/2900 train_loss:3.6906 train_time:315284ms step_avg:673.68ms
step:479/2900 train_loss:3.9138 train_time:315956ms step_avg:673.68ms
step:480/2900 train_loss:3.8643 train_time:316627ms step_avg:673.67ms
step:481/2900 train_loss:4.0085 train_time:317299ms step_avg:673.67ms
step:482/2900 train_loss:3.8185 train_time:317971ms step_avg:673.67ms
step:483/2900 train_loss:3.6222 train_time:318642ms step_avg:673.66ms
step:484/2900 train_loss:3.9044 train_time:319314ms step_avg:673.66ms
step:485/2900 train_loss:3.7582 train_time:319987ms step_avg:673.66ms
step:486/2900 train_loss:3.7633 train_time:320659ms step_avg:673.65ms
step:487/2900 train_loss:3.6931 train_time:321329ms step_avg:673.65ms
step:488/2900 train_loss:3.7672 train_time:322002ms step_avg:673.64ms
step:489/2900 train_loss:3.9687 train_time:322673ms step_avg:673.64ms
step:490/2900 train_loss:3.8123 train_time:323345ms step_avg:673.64ms
step:491/2900 train_loss:3.6957 train_time:324017ms step_avg:673.63ms
step:492/2900 train_loss:3.7187 train_time:324689ms step_avg:673.63ms
step:493/2900 train_loss:3.8333 train_time:325361ms step_avg:673.63ms
step:494/2900 train_loss:3.6716 train_time:326032ms step_avg:673.62ms
step:495/2900 train_loss:3.8100 train_time:326704ms step_avg:673.62ms
step:496/2900 train_loss:3.7617 train_time:327375ms step_avg:673.61ms
step:497/2900 train_loss:3.6335 train_time:328046ms step_avg:673.61ms
step:498/2900 train_loss:3.8279 train_time:328718ms step_avg:673.60ms
step:499/2900 train_loss:3.8963 train_time:329390ms step_avg:673.60ms
step:500/2900 train_loss:3.9350 train_time:330060ms step_avg:673.59ms
step:500/2900 val_loss:3.8080 train_time:330072ms step_avg:673.62ms
step:501/2900 train_loss:3.8461 train_time:330732ms step_avg:673.59ms
step:502/2900 train_loss:3.9029 train_time:331404ms step_avg:673.59ms
step:503/2900 train_loss:3.8480 train_time:332075ms step_avg:673.58ms
step:504/2900 train_loss:3.8836 train_time:332747ms step_avg:673.58ms
step:505/2900 train_loss:3.8230 train_time:333419ms step_avg:673.57ms
step:506/2900 train_loss:3.9218 train_time:334091ms step_avg:673.57ms
step:507/2900 train_loss:3.7483 train_time:334763ms step_avg:673.57ms
step:508/2900 train_loss:3.8565 train_time:335435ms step_avg:673.56ms
step:509/2900 train_loss:3.9321 train_time:336108ms step_avg:673.56ms
step:510/2900 train_loss:3.8720 train_time:336780ms step_avg:673.56ms
step:511/2900 train_loss:3.6914 train_time:337451ms step_avg:673.55ms
step:512/2900 train_loss:3.8805 train_time:338121ms step_avg:673.55ms
step:513/2900 train_loss:3.8240 train_time:338794ms step_avg:673.55ms
step:514/2900 train_loss:3.7754 train_time:339466ms step_avg:673.54ms
step:515/2900 train_loss:3.8828 train_time:340137ms step_avg:673.54ms
step:516/2900 train_loss:3.8365 train_time:340809ms step_avg:673.54ms
step:517/2900 train_loss:4.1870 train_time:341482ms step_avg:673.53ms
step:518/2900 train_loss:3.7941 train_time:342154ms step_avg:673.53ms
step:519/2900 train_loss:3.8923 train_time:342826ms step_avg:673.53ms
step:520/2900 train_loss:3.7796 train_time:343498ms step_avg:673.52ms
step:521/2900 train_loss:3.7961 train_time:344168ms step_avg:673.52ms
step:522/2900 train_loss:3.7570 train_time:344840ms step_avg:673.51ms
step:523/2900 train_loss:3.7649 train_time:345511ms step_avg:673.51ms
step:524/2900 train_loss:4.3943 train_time:346184ms step_avg:673.51ms
step:525/2900 train_loss:3.8510 train_time:346854ms step_avg:673.50ms
step:526/2900 train_loss:3.7854 train_time:347527ms step_avg:673.50ms
step:527/2900 train_loss:3.7979 train_time:348199ms step_avg:673.50ms
step:528/2900 train_loss:3.7565 train_time:348871ms step_avg:673.50ms
step:529/2900 train_loss:3.7280 train_time:349543ms step_avg:673.49ms
step:530/2900 train_loss:3.9589 train_time:350214ms step_avg:673.49ms
step:531/2900 train_loss:3.7529 train_time:350886ms step_avg:673.48ms
step:532/2900 train_loss:4.0225 train_time:351558ms step_avg:673.48ms
step:533/2900 train_loss:3.8396 train_time:352230ms step_avg:673.48ms
step:534/2900 train_loss:3.7621 train_time:352902ms step_avg:673.48ms
step:535/2900 train_loss:3.7815 train_time:353574ms step_avg:673.47ms
step:536/2900 train_loss:3.7225 train_time:354246ms step_avg:673.47ms
step:537/2900 train_loss:3.8579 train_time:354919ms step_avg:673.47ms
step:538/2900 train_loss:3.8382 train_time:355591ms step_avg:673.47ms
step:539/2900 train_loss:3.7352 train_time:356264ms step_avg:673.47ms
step:540/2900 train_loss:4.2369 train_time:356936ms step_avg:673.46ms
step:541/2900 train_loss:3.7752 train_time:357608ms step_avg:673.46ms
step:542/2900 train_loss:3.8904 train_time:358281ms step_avg:673.46ms
step:543/2900 train_loss:3.7103 train_time:358952ms step_avg:673.46ms
step:544/2900 train_loss:3.6867 train_time:359624ms step_avg:673.45ms
step:545/2900 train_loss:3.7695 train_time:360295ms step_avg:673.45ms
step:546/2900 train_loss:3.6978 train_time:360969ms step_avg:673.45ms
step:547/2900 train_loss:3.7502 train_time:361641ms step_avg:673.45ms
step:548/2900 train_loss:3.7496 train_time:362313ms step_avg:673.44ms
step:549/2900 train_loss:3.7252 train_time:362985ms step_avg:673.44ms
step:550/2900 train_loss:3.8293 train_time:363658ms step_avg:673.44ms
step:551/2900 train_loss:3.7201 train_time:364330ms step_avg:673.44ms
step:552/2900 train_loss:3.7376 train_time:365005ms step_avg:673.44ms
step:553/2900 train_loss:4.0819 train_time:365675ms step_avg:673.44ms
step:554/2900 train_loss:3.8598 train_time:366348ms step_avg:673.43ms
step:555/2900 train_loss:3.8158 train_time:367020ms step_avg:673.43ms
step:556/2900 train_loss:3.7529 train_time:367692ms step_avg:673.43ms
step:557/2900 train_loss:3.7968 train_time:368363ms step_avg:673.42ms
step:558/2900 train_loss:3.4490 train_time:369035ms step_avg:673.42ms
step:559/2900 train_loss:3.7221 train_time:369707ms step_avg:673.42ms
step:560/2900 train_loss:3.7550 train_time:370378ms step_avg:673.42ms
step:561/2900 train_loss:3.8078 train_time:371051ms step_avg:673.41ms
step:562/2900 train_loss:3.7149 train_time:371724ms step_avg:673.41ms
step:563/2900 train_loss:3.6568 train_time:372397ms step_avg:673.41ms
step:564/2900 train_loss:3.8678 train_time:373068ms step_avg:673.41ms
step:565/2900 train_loss:3.6796 train_time:373741ms step_avg:673.41ms
step:566/2900 train_loss:3.7926 train_time:374414ms step_avg:673.41ms
step:567/2900 train_loss:3.7417 train_time:375087ms step_avg:673.41ms
step:568/2900 train_loss:3.7104 train_time:375760ms step_avg:673.40ms
step:569/2900 train_loss:3.7844 train_time:376431ms step_avg:673.40ms
step:570/2900 train_loss:3.7625 train_time:377120ms step_avg:673.43ms
step:571/2900 train_loss:3.7983 train_time:377792ms step_avg:673.43ms
step:572/2900 train_loss:3.6876 train_time:378641ms step_avg:673.74ms
step:573/2900 train_loss:3.6321 train_time:379312ms step_avg:673.73ms
step:574/2900 train_loss:3.6483 train_time:379984ms step_avg:673.73ms
step:575/2900 train_loss:4.1301 train_time:380656ms step_avg:673.73ms
step:576/2900 train_loss:3.6603 train_time:381328ms step_avg:673.73ms
step:577/2900 train_loss:3.7367 train_time:382000ms step_avg:673.72ms
step:578/2900 train_loss:3.8870 train_time:382673ms step_avg:673.72ms
step:579/2900 train_loss:3.6768 train_time:383345ms step_avg:673.72ms
step:580/2900 train_loss:3.7295 train_time:384019ms step_avg:673.72ms
step:581/2900 train_loss:3.8337 train_time:384690ms step_avg:673.71ms
step:582/2900 train_loss:3.8476 train_time:385364ms step_avg:673.71ms
step:583/2900 train_loss:3.7801 train_time:386036ms step_avg:673.71ms
step:584/2900 train_loss:3.9572 train_time:386708ms step_avg:673.71ms
step:585/2900 train_loss:3.7478 train_time:387380ms step_avg:673.70ms
step:586/2900 train_loss:3.7539 train_time:388052ms step_avg:673.70ms
step:587/2900 train_loss:3.7925 train_time:388724ms step_avg:673.70ms
step:588/2900 train_loss:4.1955 train_time:389397ms step_avg:673.70ms
step:589/2900 train_loss:3.6294 train_time:390070ms step_avg:673.70ms
step:590/2900 train_loss:3.9842 train_time:390742ms step_avg:673.69ms
step:591/2900 train_loss:3.8024 train_time:391413ms step_avg:673.69ms
step:592/2900 train_loss:3.6433 train_time:392086ms step_avg:673.69ms
step:593/2900 train_loss:3.7462 train_time:392758ms step_avg:673.68ms
step:594/2900 train_loss:3.8424 train_time:393430ms step_avg:673.68ms
step:595/2900 train_loss:3.7205 train_time:394103ms step_avg:673.68ms
step:596/2900 train_loss:3.7570 train_time:394776ms step_avg:673.68ms
step:597/2900 train_loss:3.7798 train_time:395448ms step_avg:673.68ms
step:598/2900 train_loss:3.7910 train_time:396122ms step_avg:673.68ms
step:599/2900 train_loss:3.7932 train_time:396794ms step_avg:673.67ms
step:600/2900 train_loss:3.9033 train_time:397466ms step_avg:673.67ms
step:601/2900 train_loss:3.7545 train_time:398138ms step_avg:673.67ms
step:602/2900 train_loss:3.7110 train_time:398809ms step_avg:673.66ms
step:603/2900 train_loss:3.8634 train_time:399483ms step_avg:673.66ms
step:604/2900 train_loss:3.6744 train_time:400155ms step_avg:673.66ms
step:605/2900 train_loss:3.7813 train_time:400827ms step_avg:673.66ms
step:606/2900 train_loss:4.0806 train_time:401500ms step_avg:673.66ms
step:607/2900 train_loss:3.8081 train_time:402174ms step_avg:673.66ms
step:608/2900 train_loss:3.8113 train_time:402845ms step_avg:673.65ms
step:609/2900 train_loss:3.7695 train_time:403519ms step_avg:673.65ms
step:610/2900 train_loss:3.6805 train_time:404191ms step_avg:673.65ms
step:611/2900 train_loss:3.7593 train_time:404863ms step_avg:673.65ms
step:612/2900 train_loss:3.7123 train_time:405535ms step_avg:673.65ms
step:613/2900 train_loss:3.8396 train_time:406206ms step_avg:673.64ms
step:614/2900 train_loss:3.6698 train_time:406878ms step_avg:673.64ms
step:615/2900 train_loss:3.7176 train_time:407550ms step_avg:673.64ms
step:616/2900 train_loss:3.6829 train_time:408222ms step_avg:673.63ms
step:617/2900 train_loss:3.7588 train_time:408895ms step_avg:673.63ms
step:618/2900 train_loss:3.8176 train_time:409568ms step_avg:673.63ms
step:619/2900 train_loss:3.5866 train_time:410240ms step_avg:673.63ms
step:620/2900 train_loss:3.8654 train_time:410912ms step_avg:673.63ms
step:621/2900 train_loss:3.7629 train_time:411583ms step_avg:673.62ms
step:622/2900 train_loss:3.7678 train_time:412256ms step_avg:673.62ms
step:623/2900 train_loss:3.5891 train_time:412928ms step_avg:673.62ms
step:624/2900 train_loss:3.7040 train_time:413600ms step_avg:673.62ms
step:625/2900 train_loss:3.8139 train_time:414273ms step_avg:673.61ms
step:625/2900 val_loss:3.7332 train_time:414285ms step_avg:673.63ms
step:626/2900 train_loss:3.7651 train_time:414946ms step_avg:673.61ms
step:627/2900 train_loss:3.7298 train_time:415619ms step_avg:673.61ms
step:628/2900 train_loss:3.6448 train_time:416289ms step_avg:673.61ms
step:629/2900 train_loss:3.7786 train_time:416962ms step_avg:673.61ms
step:630/2900 train_loss:3.6172 train_time:417633ms step_avg:673.60ms
step:631/2900 train_loss:3.8023 train_time:418305ms step_avg:673.60ms
step:632/2900 train_loss:3.6014 train_time:418978ms step_avg:673.60ms
step:633/2900 train_loss:3.7410 train_time:419650ms step_avg:673.60ms
step:634/2900 train_loss:3.8968 train_time:420323ms step_avg:673.59ms
step:635/2900 train_loss:3.7229 train_time:420995ms step_avg:673.59ms
step:636/2900 train_loss:3.6446 train_time:421667ms step_avg:673.59ms
step:637/2900 train_loss:3.7501 train_time:422339ms step_avg:673.59ms
step:638/2900 train_loss:3.6636 train_time:423011ms step_avg:673.59ms
step:639/2900 train_loss:3.7209 train_time:423684ms step_avg:673.58ms
step:640/2900 train_loss:3.7632 train_time:424355ms step_avg:673.58ms
step:641/2900 train_loss:3.6647 train_time:425027ms step_avg:673.58ms
step:642/2900 train_loss:3.6556 train_time:425699ms step_avg:673.57ms
step:643/2900 train_loss:3.8432 train_time:426371ms step_avg:673.57ms
step:644/2900 train_loss:3.5237 train_time:427043ms step_avg:673.57ms
step:645/2900 train_loss:3.7254 train_time:427716ms step_avg:673.57ms
step:646/2900 train_loss:3.6829 train_time:428387ms step_avg:673.56ms
step:647/2900 train_loss:3.8371 train_time:429060ms step_avg:673.56ms
step:648/2900 train_loss:3.7433 train_time:429732ms step_avg:673.56ms
step:649/2900 train_loss:3.9308 train_time:430404ms step_avg:673.56ms
step:650/2900 train_loss:3.6998 train_time:431076ms step_avg:673.56ms
step:651/2900 train_loss:3.5634 train_time:431748ms step_avg:673.55ms
step:652/2900 train_loss:3.6454 train_time:432420ms step_avg:673.55ms
step:653/2900 train_loss:3.7307 train_time:433093ms step_avg:673.55ms
step:654/2900 train_loss:3.7110 train_time:433765ms step_avg:673.55ms
step:655/2900 train_loss:3.8579 train_time:434437ms step_avg:673.55ms
step:656/2900 train_loss:3.8712 train_time:435108ms step_avg:673.54ms
step:657/2900 train_loss:3.7885 train_time:435781ms step_avg:673.54ms
step:658/2900 train_loss:3.7106 train_time:436454ms step_avg:673.54ms
step:659/2900 train_loss:3.5634 train_time:437127ms step_avg:673.54ms
step:660/2900 train_loss:3.7435 train_time:437799ms step_avg:673.54ms
step:661/2900 train_loss:3.8236 train_time:438472ms step_avg:673.54ms
step:662/2900 train_loss:3.8111 train_time:439145ms step_avg:673.53ms
step:663/2900 train_loss:3.6065 train_time:439817ms step_avg:673.53ms
step:664/2900 train_loss:3.7668 train_time:440489ms step_avg:673.53ms
step:665/2900 train_loss:3.7761 train_time:441162ms step_avg:673.53ms
step:666/2900 train_loss:3.6599 train_time:441834ms step_avg:673.53ms
step:667/2900 train_loss:3.7616 train_time:442507ms step_avg:673.53ms
step:668/2900 train_loss:3.8611 train_time:443179ms step_avg:673.52ms
step:669/2900 train_loss:3.8361 train_time:443851ms step_avg:673.52ms
step:670/2900 train_loss:3.6476 train_time:444523ms step_avg:673.52ms
step:671/2900 train_loss:3.8233 train_time:445195ms step_avg:673.52ms
step:672/2900 train_loss:3.7470 train_time:445867ms step_avg:673.52ms
step:673/2900 train_loss:3.5559 train_time:446540ms step_avg:673.51ms
step:674/2900 train_loss:4.0115 train_time:447212ms step_avg:673.51ms
step:675/2900 train_loss:3.7848 train_time:447885ms step_avg:673.51ms
step:676/2900 train_loss:3.7347 train_time:448558ms step_avg:673.51ms
step:677/2900 train_loss:3.7892 train_time:449230ms step_avg:673.51ms
step:678/2900 train_loss:3.6912 train_time:449903ms step_avg:673.51ms
step:679/2900 train_loss:3.5605 train_time:450575ms step_avg:673.51ms
step:680/2900 train_loss:3.6632 train_time:451247ms step_avg:673.50ms
step:681/2900 train_loss:3.7761 train_time:451920ms step_avg:673.50ms
step:682/2900 train_loss:3.8807 train_time:452594ms step_avg:673.50ms
step:683/2900 train_loss:3.6427 train_time:453266ms step_avg:673.50ms
step:684/2900 train_loss:3.7419 train_time:453939ms step_avg:673.50ms
step:685/2900 train_loss:3.7773 train_time:454612ms step_avg:673.50ms
step:686/2900 train_loss:3.6996 train_time:455284ms step_avg:673.50ms
step:687/2900 train_loss:3.7878 train_time:455956ms step_avg:673.50ms
step:688/2900 train_loss:3.6870 train_time:456629ms step_avg:673.49ms
step:689/2900 train_loss:3.6506 train_time:457301ms step_avg:673.49ms
step:690/2900 train_loss:3.7250 train_time:457974ms step_avg:673.49ms
step:691/2900 train_loss:3.5829 train_time:458646ms step_avg:673.49ms
step:692/2900 train_loss:3.7750 train_time:459318ms step_avg:673.49ms
step:693/2900 train_loss:3.7787 train_time:459990ms step_avg:673.48ms
step:694/2900 train_loss:3.5359 train_time:460663ms step_avg:673.48ms
step:695/2900 train_loss:3.7773 train_time:461336ms step_avg:673.48ms
step:696/2900 train_loss:3.7259 train_time:462008ms step_avg:673.48ms
step:697/2900 train_loss:3.4811 train_time:462681ms step_avg:673.48ms
step:698/2900 train_loss:3.8219 train_time:463354ms step_avg:673.48ms
step:699/2900 train_loss:3.7044 train_time:464027ms step_avg:673.48ms
step:700/2900 train_loss:3.6882 train_time:464698ms step_avg:673.48ms
step:701/2900 train_loss:3.6526 train_time:465369ms step_avg:673.47ms
step:702/2900 train_loss:3.6890 train_time:466043ms step_avg:673.47ms
step:703/2900 train_loss:3.2290 train_time:466715ms step_avg:673.47ms
step:704/2900 train_loss:3.6992 train_time:467388ms step_avg:673.47ms
step:705/2900 train_loss:3.5346 train_time:468061ms step_avg:673.47ms
step:706/2900 train_loss:3.8247 train_time:468733ms step_avg:673.47ms
step:707/2900 train_loss:3.6700 train_time:469406ms step_avg:673.47ms
step:708/2900 train_loss:3.7140 train_time:470077ms step_avg:673.46ms
step:709/2900 train_loss:3.7038 train_time:470750ms step_avg:673.46ms
step:710/2900 train_loss:3.7559 train_time:471423ms step_avg:673.46ms
step:711/2900 train_loss:3.7801 train_time:472095ms step_avg:673.46ms
step:712/2900 train_loss:3.6437 train_time:472768ms step_avg:673.46ms
step:713/2900 train_loss:3.6656 train_time:473441ms step_avg:673.46ms
step:714/2900 train_loss:3.6503 train_time:474114ms step_avg:673.46ms
step:715/2900 train_loss:3.6772 train_time:474787ms step_avg:673.46ms
step:716/2900 train_loss:3.5367 train_time:475458ms step_avg:673.45ms
step:717/2900 train_loss:3.9554 train_time:476130ms step_avg:673.45ms
step:718/2900 train_loss:3.8281 train_time:476803ms step_avg:673.45ms
step:719/2900 train_loss:3.8217 train_time:477475ms step_avg:673.45ms
step:720/2900 train_loss:3.7862 train_time:478147ms step_avg:673.45ms
step:721/2900 train_loss:3.7341 train_time:478820ms step_avg:673.45ms
step:722/2900 train_loss:3.6090 train_time:479492ms step_avg:673.44ms
step:723/2900 train_loss:3.7614 train_time:480166ms step_avg:673.44ms
step:724/2900 train_loss:3.6839 train_time:480838ms step_avg:673.44ms
step:725/2900 train_loss:3.6937 train_time:481510ms step_avg:673.44ms
step:726/2900 train_loss:3.6681 train_time:482183ms step_avg:673.44ms
step:727/2900 train_loss:3.5805 train_time:482855ms step_avg:673.44ms
step:728/2900 train_loss:3.7598 train_time:483528ms step_avg:673.44ms
step:729/2900 train_loss:3.6982 train_time:484201ms step_avg:673.44ms
step:730/2900 train_loss:3.5014 train_time:484873ms step_avg:673.43ms
step:731/2900 train_loss:3.6750 train_time:485545ms step_avg:673.43ms
step:732/2900 train_loss:3.7755 train_time:486216ms step_avg:673.43ms
step:733/2900 train_loss:3.7629 train_time:486891ms step_avg:673.43ms
step:734/2900 train_loss:3.7954 train_time:487562ms step_avg:673.43ms
step:735/2900 train_loss:3.6230 train_time:488235ms step_avg:673.43ms
step:736/2900 train_loss:3.6335 train_time:488908ms step_avg:673.43ms
step:737/2900 train_loss:3.6813 train_time:489582ms step_avg:673.43ms
step:738/2900 train_loss:3.8190 train_time:490254ms step_avg:673.43ms
step:739/2900 train_loss:3.7361 train_time:490927ms step_avg:673.42ms
step:740/2900 train_loss:3.7094 train_time:491599ms step_avg:673.42ms
step:741/2900 train_loss:3.7599 train_time:492272ms step_avg:673.42ms
step:742/2900 train_loss:3.6907 train_time:492945ms step_avg:673.42ms
step:743/2900 train_loss:3.6987 train_time:493618ms step_avg:673.42ms
step:744/2900 train_loss:3.6016 train_time:494290ms step_avg:673.42ms
step:745/2900 train_loss:3.8594 train_time:494963ms step_avg:673.42ms
step:746/2900 train_loss:3.6396 train_time:495637ms step_avg:673.42ms
step:747/2900 train_loss:3.6768 train_time:496310ms step_avg:673.42ms
step:748/2900 train_loss:3.7636 train_time:496981ms step_avg:673.42ms
step:749/2900 train_loss:3.6863 train_time:497654ms step_avg:673.42ms
step:750/2900 train_loss:3.7524 train_time:498327ms step_avg:673.42ms
step:750/2900 val_loss:3.6765 train_time:498339ms step_avg:673.43ms
step:751/2900 train_loss:3.6587 train_time:499001ms step_avg:673.42ms
step:752/2900 train_loss:3.7900 train_time:499673ms step_avg:673.41ms
step:753/2900 train_loss:3.6398 train_time:500346ms step_avg:673.41ms
step:754/2900 train_loss:3.6717 train_time:501017ms step_avg:673.41ms
step:755/2900 train_loss:3.6378 train_time:501690ms step_avg:673.41ms
step:756/2900 train_loss:3.7007 train_time:502362ms step_avg:673.41ms
step:757/2900 train_loss:3.6107 train_time:503035ms step_avg:673.41ms
step:758/2900 train_loss:3.8495 train_time:503708ms step_avg:673.41ms
step:759/2900 train_loss:3.9521 train_time:504381ms step_avg:673.41ms
step:760/2900 train_loss:3.6901 train_time:505054ms step_avg:673.41ms
step:761/2900 train_loss:3.6116 train_time:505727ms step_avg:673.40ms
step:762/2900 train_loss:3.7602 train_time:506400ms step_avg:673.40ms
step:763/2900 train_loss:3.5142 train_time:507071ms step_avg:673.40ms
step:764/2900 train_loss:3.6606 train_time:507745ms step_avg:673.40ms
step:765/2900 train_loss:3.7696 train_time:508418ms step_avg:673.40ms
step:766/2900 train_loss:3.4274 train_time:509090ms step_avg:673.40ms
step:767/2900 train_loss:3.8456 train_time:509763ms step_avg:673.40ms
step:768/2900 train_loss:3.6890 train_time:510436ms step_avg:673.40ms
step:769/2900 train_loss:3.6667 train_time:511109ms step_avg:673.40ms
step:770/2900 train_loss:3.6901 train_time:511781ms step_avg:673.40ms
step:771/2900 train_loss:3.7032 train_time:512455ms step_avg:673.40ms
step:772/2900 train_loss:3.7572 train_time:513128ms step_avg:673.40ms
step:773/2900 train_loss:3.9903 train_time:513800ms step_avg:673.39ms
step:774/2900 train_loss:3.5703 train_time:514472ms step_avg:673.39ms
step:775/2900 train_loss:3.7508 train_time:515144ms step_avg:673.39ms
step:776/2900 train_loss:3.7445 train_time:515816ms step_avg:673.39ms
step:777/2900 train_loss:3.7204 train_time:516489ms step_avg:673.39ms
step:778/2900 train_loss:3.5068 train_time:517161ms step_avg:673.39ms
step:779/2900 train_loss:3.5187 train_time:517834ms step_avg:673.39ms
step:780/2900 train_loss:3.5821 train_time:518506ms step_avg:673.38ms
step:781/2900 train_loss:3.6802 train_time:519180ms step_avg:673.38ms
step:782/2900 train_loss:3.7020 train_time:519852ms step_avg:673.38ms
step:783/2900 train_loss:3.7697 train_time:520525ms step_avg:673.38ms
step:784/2900 train_loss:3.6912 train_time:521197ms step_avg:673.38ms
step:785/2900 train_loss:3.6756 train_time:521868ms step_avg:673.38ms
step:786/2900 train_loss:3.6875 train_time:522541ms step_avg:673.38ms
step:787/2900 train_loss:3.6615 train_time:523215ms step_avg:673.38ms
step:788/2900 train_loss:3.5547 train_time:523886ms step_avg:673.38ms
step:789/2900 train_loss:3.8033 train_time:524558ms step_avg:673.37ms
step:790/2900 train_loss:3.6063 train_time:525230ms step_avg:673.37ms
step:791/2900 train_loss:3.6618 train_time:525903ms step_avg:673.37ms
step:792/2900 train_loss:3.7308 train_time:526576ms step_avg:673.37ms
step:793/2900 train_loss:3.8658 train_time:527249ms step_avg:673.37ms
step:794/2900 train_loss:3.8691 train_time:527922ms step_avg:673.37ms
step:795/2900 train_loss:3.5941 train_time:528594ms step_avg:673.37ms
step:796/2900 train_loss:3.7078 train_time:529266ms step_avg:673.37ms
step:797/2900 train_loss:3.7682 train_time:529938ms step_avg:673.36ms
step:798/2900 train_loss:3.8787 train_time:530612ms step_avg:673.37ms
step:799/2900 train_loss:3.6258 train_time:531284ms step_avg:673.36ms
step:800/2900 train_loss:3.7628 train_time:531959ms step_avg:673.37ms
step:801/2900 train_loss:3.6544 train_time:532631ms step_avg:673.36ms
step:802/2900 train_loss:3.6459 train_time:533303ms step_avg:673.36ms
step:803/2900 train_loss:3.7341 train_time:533976ms step_avg:673.36ms
step:804/2900 train_loss:3.6067 train_time:534649ms step_avg:673.36ms
step:805/2900 train_loss:3.6340 train_time:535322ms step_avg:673.36ms
step:806/2900 train_loss:3.7370 train_time:535994ms step_avg:673.36ms
step:807/2900 train_loss:3.6370 train_time:536666ms step_avg:673.36ms
step:808/2900 train_loss:3.6629 train_time:537339ms step_avg:673.36ms
step:809/2900 train_loss:3.7530 train_time:538012ms step_avg:673.36ms
step:810/2900 train_loss:3.6710 train_time:538685ms step_avg:673.36ms
step:811/2900 train_loss:3.5949 train_time:539357ms step_avg:673.35ms
step:812/2900 train_loss:3.6692 train_time:540029ms step_avg:673.35ms
step:813/2900 train_loss:3.6904 train_time:540701ms step_avg:673.35ms
step:814/2900 train_loss:3.7044 train_time:541375ms step_avg:673.35ms
step:815/2900 train_loss:3.7375 train_time:542046ms step_avg:673.35ms
step:816/2900 train_loss:3.6772 train_time:542718ms step_avg:673.35ms
step:817/2900 train_loss:3.6717 train_time:543391ms step_avg:673.35ms
step:818/2900 train_loss:3.7772 train_time:544063ms step_avg:673.35ms
step:819/2900 train_loss:3.8719 train_time:544736ms step_avg:673.35ms
step:820/2900 train_loss:3.6343 train_time:545408ms step_avg:673.34ms
step:821/2900 train_loss:3.8298 train_time:546081ms step_avg:673.34ms
step:822/2900 train_loss:3.6092 train_time:546753ms step_avg:673.34ms
step:823/2900 train_loss:3.6502 train_time:547425ms step_avg:673.34ms
step:824/2900 train_loss:3.7857 train_time:548098ms step_avg:673.34ms
step:825/2900 train_loss:3.6853 train_time:548771ms step_avg:673.34ms
step:826/2900 train_loss:3.6178 train_time:549443ms step_avg:673.34ms
step:827/2900 train_loss:3.7172 train_time:550115ms step_avg:673.34ms
step:828/2900 train_loss:3.6082 train_time:550788ms step_avg:673.34ms
step:829/2900 train_loss:3.8444 train_time:551460ms step_avg:673.33ms
step:830/2900 train_loss:3.7270 train_time:552134ms step_avg:673.33ms
step:831/2900 train_loss:3.7767 train_time:552806ms step_avg:673.33ms
step:832/2900 train_loss:3.6399 train_time:553478ms step_avg:673.33ms
step:833/2900 train_loss:3.6946 train_time:554151ms step_avg:673.33ms
step:834/2900 train_loss:3.6291 train_time:554823ms step_avg:673.33ms
step:835/2900 train_loss:3.7550 train_time:555495ms step_avg:673.33ms
step:836/2900 train_loss:3.5823 train_time:556168ms step_avg:673.33ms
step:837/2900 train_loss:3.5681 train_time:556841ms step_avg:673.33ms
step:838/2900 train_loss:3.8285 train_time:557512ms step_avg:673.32ms
step:839/2900 train_loss:3.5150 train_time:558187ms step_avg:673.33ms
step:840/2900 train_loss:3.6996 train_time:558860ms step_avg:673.33ms
step:841/2900 train_loss:3.5327 train_time:559534ms step_avg:673.33ms
step:842/2900 train_loss:3.5767 train_time:560207ms step_avg:673.33ms
step:843/2900 train_loss:3.6655 train_time:560881ms step_avg:673.33ms
step:844/2900 train_loss:3.6910 train_time:561553ms step_avg:673.33ms
step:845/2900 train_loss:3.6790 train_time:562226ms step_avg:673.32ms
step:846/2900 train_loss:3.5338 train_time:562900ms step_avg:673.33ms
step:847/2900 train_loss:3.7692 train_time:563573ms step_avg:673.32ms
step:848/2900 train_loss:3.6312 train_time:564244ms step_avg:673.32ms
step:849/2900 train_loss:3.6013 train_time:564917ms step_avg:673.32ms
step:850/2900 train_loss:3.7379 train_time:565590ms step_avg:673.32ms
step:851/2900 train_loss:3.6015 train_time:566263ms step_avg:673.32ms
step:852/2900 train_loss:3.5534 train_time:566935ms step_avg:673.32ms
step:853/2900 train_loss:3.8425 train_time:567607ms step_avg:673.32ms
step:854/2900 train_loss:3.5540 train_time:568279ms step_avg:673.32ms
step:855/2900 train_loss:3.6738 train_time:568952ms step_avg:673.32ms
step:856/2900 train_loss:3.7532 train_time:569624ms step_avg:673.31ms
step:857/2900 train_loss:3.6222 train_time:570297ms step_avg:673.31ms
step:858/2900 train_loss:3.6534 train_time:570970ms step_avg:673.31ms
step:859/2900 train_loss:3.7067 train_time:571643ms step_avg:673.31ms
step:860/2900 train_loss:3.5907 train_time:572316ms step_avg:673.31ms
step:861/2900 train_loss:3.6733 train_time:572990ms step_avg:673.31ms
step:862/2900 train_loss:3.7005 train_time:573662ms step_avg:673.31ms
step:863/2900 train_loss:3.7391 train_time:574334ms step_avg:673.31ms
step:864/2900 train_loss:3.7009 train_time:575008ms step_avg:673.31ms
step:865/2900 train_loss:3.6786 train_time:575681ms step_avg:673.31ms
step:866/2900 train_loss:3.4963 train_time:576352ms step_avg:673.31ms
step:867/2900 train_loss:3.6936 train_time:577024ms step_avg:673.31ms
step:868/2900 train_loss:3.9764 train_time:577696ms step_avg:673.31ms
step:869/2900 train_loss:3.5612 train_time:578369ms step_avg:673.30ms
step:870/2900 train_loss:3.7397 train_time:579040ms step_avg:673.30ms
step:871/2900 train_loss:3.7254 train_time:579714ms step_avg:673.30ms
step:872/2900 train_loss:3.5544 train_time:580386ms step_avg:673.30ms
step:873/2900 train_loss:3.5389 train_time:581059ms step_avg:673.30ms
step:874/2900 train_loss:3.7673 train_time:581731ms step_avg:673.30ms
step:875/2900 train_loss:3.5596 train_time:582404ms step_avg:673.30ms
step:875/2900 val_loss:3.6329 train_time:582416ms step_avg:673.31ms
step:876/2900 train_loss:3.3013 train_time:583078ms step_avg:673.30ms
step:877/2900 train_loss:3.7495 train_time:583752ms step_avg:673.30ms
step:878/2900 train_loss:3.5516 train_time:584424ms step_avg:673.30ms
step:879/2900 train_loss:3.7292 train_time:585096ms step_avg:673.30ms
step:880/2900 train_loss:3.5910 train_time:585769ms step_avg:673.30ms
step:881/2900 train_loss:3.7663 train_time:586443ms step_avg:673.30ms
step:882/2900 train_loss:3.4284 train_time:587116ms step_avg:673.30ms
step:883/2900 train_loss:3.5948 train_time:587789ms step_avg:673.30ms
step:884/2900 train_loss:3.7959 train_time:588463ms step_avg:673.30ms
step:885/2900 train_loss:3.9488 train_time:589135ms step_avg:673.30ms
step:886/2900 train_loss:3.6666 train_time:589809ms step_avg:673.30ms
step:887/2900 train_loss:3.5950 train_time:590481ms step_avg:673.30ms
step:888/2900 train_loss:3.6781 train_time:591155ms step_avg:673.30ms
step:889/2900 train_loss:4.1862 train_time:591828ms step_avg:673.30ms
step:890/2900 train_loss:3.9528 train_time:592501ms step_avg:673.30ms
step:891/2900 train_loss:3.6254 train_time:593174ms step_avg:673.30ms
step:892/2900 train_loss:3.6389 train_time:593847ms step_avg:673.30ms
step:893/2900 train_loss:3.4678 train_time:594519ms step_avg:673.29ms
step:894/2900 train_loss:3.8237 train_time:595191ms step_avg:673.29ms
step:895/2900 train_loss:3.5327 train_time:595865ms step_avg:673.29ms
step:896/2900 train_loss:3.7939 train_time:596537ms step_avg:673.29ms
step:897/2900 train_loss:3.7975 train_time:597210ms step_avg:673.29ms
step:898/2900 train_loss:3.5978 train_time:597883ms step_avg:673.29ms
step:899/2900 train_loss:3.6542 train_time:598556ms step_avg:673.29ms
step:900/2900 train_loss:3.7016 train_time:599230ms step_avg:673.29ms
step:901/2900 train_loss:3.5791 train_time:599902ms step_avg:673.29ms
step:902/2900 train_loss:3.5179 train_time:600575ms step_avg:673.29ms
step:903/2900 train_loss:3.7284 train_time:601248ms step_avg:673.29ms
step:904/2900 train_loss:3.7345 train_time:601920ms step_avg:673.29ms
step:905/2900 train_loss:3.6410 train_time:602593ms step_avg:673.29ms
step:906/2900 train_loss:3.6196 train_time:603266ms step_avg:673.29ms
step:907/2900 train_loss:3.5974 train_time:603938ms step_avg:673.29ms
step:908/2900 train_loss:3.8291 train_time:604612ms step_avg:673.29ms
step:909/2900 train_loss:3.6183 train_time:605285ms step_avg:673.29ms
step:910/2900 train_loss:3.6602 train_time:605958ms step_avg:673.29ms
step:911/2900 train_loss:3.5637 train_time:606631ms step_avg:673.29ms
step:912/2900 train_loss:3.6461 train_time:607304ms step_avg:673.29ms
step:913/2900 train_loss:3.7276 train_time:607977ms step_avg:673.29ms
step:914/2900 train_loss:3.7115 train_time:608650ms step_avg:673.29ms
step:915/2900 train_loss:3.5909 train_time:609322ms step_avg:673.28ms
step:916/2900 train_loss:3.8433 train_time:609996ms step_avg:673.28ms
step:917/2900 train_loss:3.6467 train_time:610668ms step_avg:673.28ms
step:918/2900 train_loss:3.7340 train_time:611341ms step_avg:673.28ms
step:919/2900 train_loss:3.7053 train_time:612013ms step_avg:673.28ms
step:920/2900 train_loss:4.9271 train_time:612687ms step_avg:673.28ms
step:921/2900 train_loss:3.6259 train_time:613359ms step_avg:673.28ms
step:922/2900 train_loss:3.6851 train_time:614032ms step_avg:673.28ms
step:923/2900 train_loss:3.6443 train_time:614705ms step_avg:673.28ms
step:924/2900 train_loss:3.6913 train_time:615378ms step_avg:673.28ms
step:925/2900 train_loss:3.7073 train_time:616050ms step_avg:673.28ms
step:926/2900 train_loss:3.7872 train_time:616723ms step_avg:673.28ms
step:927/2900 train_loss:3.7632 train_time:617396ms step_avg:673.28ms
step:928/2900 train_loss:3.6599 train_time:618070ms step_avg:673.28ms
step:929/2900 train_loss:3.6441 train_time:618742ms step_avg:673.28ms
step:930/2900 train_loss:3.8941 train_time:619416ms step_avg:673.28ms
step:931/2900 train_loss:3.7209 train_time:620089ms step_avg:673.28ms
step:932/2900 train_loss:3.5097 train_time:620762ms step_avg:673.28ms
step:933/2900 train_loss:3.6040 train_time:621434ms step_avg:673.28ms
step:934/2900 train_loss:3.7792 train_time:622106ms step_avg:673.28ms
step:935/2900 train_loss:3.4991 train_time:622780ms step_avg:673.28ms
step:936/2900 train_loss:3.6798 train_time:623453ms step_avg:673.28ms
step:937/2900 train_loss:3.5589 train_time:624125ms step_avg:673.27ms
step:938/2900 train_loss:3.6230 train_time:624798ms step_avg:673.27ms
step:939/2900 train_loss:3.7181 train_time:625472ms step_avg:673.27ms
step:940/2900 train_loss:3.6439 train_time:626144ms step_avg:673.27ms
step:941/2900 train_loss:3.8040 train_time:626816ms step_avg:673.27ms
step:942/2900 train_loss:3.5852 train_time:627488ms step_avg:673.27ms
step:943/2900 train_loss:3.6567 train_time:628160ms step_avg:673.27ms
step:944/2900 train_loss:3.4528 train_time:628833ms step_avg:673.27ms
step:945/2900 train_loss:3.8086 train_time:629506ms step_avg:673.27ms
step:946/2900 train_loss:3.5186 train_time:630178ms step_avg:673.27ms
step:947/2900 train_loss:3.5321 train_time:630850ms step_avg:673.27ms
step:948/2900 train_loss:5.1807 train_time:631522ms step_avg:673.26ms
step:949/2900 train_loss:3.7120 train_time:632195ms step_avg:673.26ms
step:950/2900 train_loss:3.6018 train_time:632890ms step_avg:673.29ms
step:951/2900 train_loss:3.4957 train_time:633563ms step_avg:673.29ms
step:952/2900 train_loss:3.5619 train_time:634235ms step_avg:673.29ms
step:953/2900 train_loss:3.7742 train_time:635086ms step_avg:673.47ms
step:954/2900 train_loss:3.6841 train_time:635758ms step_avg:673.47ms
step:955/2900 train_loss:3.3800 train_time:636430ms step_avg:673.47ms
step:956/2900 train_loss:3.5365 train_time:637103ms step_avg:673.47ms
step:957/2900 train_loss:3.5389 train_time:637774ms step_avg:673.47ms
step:958/2900 train_loss:3.5760 train_time:638448ms step_avg:673.47ms
step:959/2900 train_loss:3.5451 train_time:639120ms step_avg:673.47ms
step:960/2900 train_loss:3.6611 train_time:639792ms step_avg:673.47ms
step:961/2900 train_loss:3.7868 train_time:640465ms step_avg:673.46ms
step:962/2900 train_loss:3.7242 train_time:641138ms step_avg:673.46ms
step:963/2900 train_loss:3.5468 train_time:641810ms step_avg:673.46ms
step:964/2900 train_loss:3.4946 train_time:642483ms step_avg:673.46ms
step:965/2900 train_loss:3.6061 train_time:643154ms step_avg:673.46ms
step:966/2900 train_loss:3.6634 train_time:643827ms step_avg:673.46ms
step:967/2900 train_loss:3.7293 train_time:644499ms step_avg:673.46ms
step:968/2900 train_loss:3.6075 train_time:645170ms step_avg:673.45ms
step:969/2900 train_loss:3.6623 train_time:645842ms step_avg:673.45ms
step:970/2900 train_loss:3.6173 train_time:646515ms step_avg:673.45ms
step:971/2900 train_loss:3.6096 train_time:647186ms step_avg:673.45ms
step:972/2900 train_loss:3.5552 train_time:647859ms step_avg:673.45ms
step:973/2900 train_loss:3.4408 train_time:648531ms step_avg:673.45ms
step:974/2900 train_loss:3.6466 train_time:649203ms step_avg:673.45ms
step:975/2900 train_loss:3.4609 train_time:649875ms step_avg:673.45ms
step:976/2900 train_loss:3.5976 train_time:650547ms step_avg:673.44ms
step:977/2900 train_loss:3.7039 train_time:651218ms step_avg:673.44ms
step:978/2900 train_loss:3.5374 train_time:651890ms step_avg:673.44ms
step:979/2900 train_loss:3.5924 train_time:652562ms step_avg:673.44ms
step:980/2900 train_loss:3.6559 train_time:653234ms step_avg:673.44ms
step:981/2900 train_loss:3.4679 train_time:653906ms step_avg:673.44ms
step:982/2900 train_loss:3.6262 train_time:654578ms step_avg:673.43ms
step:983/2900 train_loss:3.5824 train_time:655250ms step_avg:673.43ms
step:984/2900 train_loss:3.5951 train_time:655922ms step_avg:673.43ms
step:985/2900 train_loss:3.7143 train_time:656593ms step_avg:673.43ms
step:986/2900 train_loss:3.5431 train_time:657265ms step_avg:673.43ms
step:987/2900 train_loss:3.5894 train_time:657937ms step_avg:673.43ms
step:988/2900 train_loss:3.6019 train_time:658609ms step_avg:673.42ms
step:989/2900 train_loss:3.6578 train_time:659280ms step_avg:673.42ms
step:990/2900 train_loss:3.4977 train_time:659952ms step_avg:673.42ms
step:991/2900 train_loss:3.5327 train_time:660623ms step_avg:673.42ms
step:992/2900 train_loss:3.6911 train_time:661294ms step_avg:673.42ms
step:993/2900 train_loss:3.5882 train_time:661966ms step_avg:673.41ms
step:994/2900 train_loss:3.6183 train_time:662638ms step_avg:673.41ms
step:995/2900 train_loss:3.5527 train_time:663308ms step_avg:673.41ms
step:996/2900 train_loss:3.5121 train_time:663981ms step_avg:673.41ms
step:997/2900 train_loss:3.5730 train_time:664652ms step_avg:673.41ms
step:998/2900 train_loss:3.5757 train_time:665325ms step_avg:673.41ms
step:999/2900 train_loss:3.7041 train_time:665997ms step_avg:673.40ms
step:1000/2900 train_loss:3.5273 train_time:666671ms step_avg:673.41ms
step:1000/2900 val_loss:3.5920 train_time:666683ms step_avg:673.42ms
step:1001/2900 train_loss:3.7808 train_time:667344ms step_avg:673.40ms
step:1002/2900 train_loss:3.7169 train_time:668016ms step_avg:673.40ms
step:1003/2900 train_loss:3.6516 train_time:668688ms step_avg:673.40ms
step:1004/2900 train_loss:3.6168 train_time:669360ms step_avg:673.40ms
step:1005/2900 train_loss:3.6100 train_time:670031ms step_avg:673.40ms
step:1006/2900 train_loss:3.6154 train_time:670703ms step_avg:673.40ms
step:1007/2900 train_loss:3.5900 train_time:671375ms step_avg:673.40ms
step:1008/2900 train_loss:3.5780 train_time:672046ms step_avg:673.39ms
step:1009/2900 train_loss:3.5614 train_time:672718ms step_avg:673.39ms
step:1010/2900 train_loss:3.5861 train_time:673390ms step_avg:673.39ms
step:1011/2900 train_loss:3.8283 train_time:674063ms step_avg:673.39ms
step:1012/2900 train_loss:3.7348 train_time:674734ms step_avg:673.39ms
step:1013/2900 train_loss:3.5268 train_time:675406ms step_avg:673.39ms
step:1014/2900 train_loss:3.6137 train_time:676079ms step_avg:673.39ms
step:1015/2900 train_loss:3.5612 train_time:676752ms step_avg:673.39ms
step:1016/2900 train_loss:3.6491 train_time:677425ms step_avg:673.38ms
step:1017/2900 train_loss:3.7074 train_time:678096ms step_avg:673.38ms
step:1018/2900 train_loss:3.5044 train_time:678768ms step_avg:673.38ms
step:1019/2900 train_loss:3.5892 train_time:679440ms step_avg:673.38ms
step:1020/2900 train_loss:3.5389 train_time:680112ms step_avg:673.38ms
step:1021/2900 train_loss:3.7640 train_time:680783ms step_avg:673.38ms
step:1022/2900 train_loss:3.4995 train_time:681455ms step_avg:673.37ms
step:1023/2900 train_loss:3.8549 train_time:682126ms step_avg:673.37ms
step:1024/2900 train_loss:3.7074 train_time:682798ms step_avg:673.37ms
step:1025/2900 train_loss:3.4858 train_time:683470ms step_avg:673.37ms
step:1026/2900 train_loss:3.6343 train_time:684143ms step_avg:673.37ms
step:1027/2900 train_loss:3.5477 train_time:684816ms step_avg:673.37ms
step:1028/2900 train_loss:3.6253 train_time:685489ms step_avg:673.37ms
step:1029/2900 train_loss:3.5839 train_time:686161ms step_avg:673.37ms
step:1030/2900 train_loss:3.4798 train_time:686834ms step_avg:673.37ms
step:1031/2900 train_loss:3.6178 train_time:687507ms step_avg:673.37ms
step:1032/2900 train_loss:4.0569 train_time:688178ms step_avg:673.36ms
step:1033/2900 train_loss:3.5175 train_time:688849ms step_avg:673.36ms
step:1034/2900 train_loss:3.5618 train_time:689522ms step_avg:673.36ms
step:1035/2900 train_loss:3.6009 train_time:690193ms step_avg:673.36ms
step:1036/2900 train_loss:3.6509 train_time:690864ms step_avg:673.36ms
step:1037/2900 train_loss:3.4553 train_time:691535ms step_avg:673.35ms
step:1038/2900 train_loss:3.4622 train_time:692207ms step_avg:673.35ms
step:1039/2900 train_loss:3.6656 train_time:692877ms step_avg:673.35ms
step:1040/2900 train_loss:3.4713 train_time:693549ms step_avg:673.35ms
step:1041/2900 train_loss:3.4882 train_time:694219ms step_avg:673.35ms
step:1042/2900 train_loss:3.6235 train_time:694892ms step_avg:673.34ms
step:1043/2900 train_loss:3.4431 train_time:695563ms step_avg:673.34ms
step:1044/2900 train_loss:3.5382 train_time:696236ms step_avg:673.34ms
step:1045/2900 train_loss:3.6311 train_time:696907ms step_avg:673.34ms
step:1046/2900 train_loss:3.7757 train_time:697580ms step_avg:673.34ms
step:1047/2900 train_loss:3.4586 train_time:698252ms step_avg:673.34ms
step:1048/2900 train_loss:3.8041 train_time:698924ms step_avg:673.34ms
step:1049/2900 train_loss:3.5070 train_time:699596ms step_avg:673.34ms
step:1050/2900 train_loss:4.0989 train_time:700268ms step_avg:673.33ms
step:1051/2900 train_loss:3.5922 train_time:700940ms step_avg:673.33ms
step:1052/2900 train_loss:3.5302 train_time:701611ms step_avg:673.33ms
step:1053/2900 train_loss:3.5454 train_time:702284ms step_avg:673.33ms
step:1054/2900 train_loss:3.6404 train_time:702954ms step_avg:673.33ms
step:1055/2900 train_loss:3.5634 train_time:703625ms step_avg:673.33ms
step:1056/2900 train_loss:3.6145 train_time:704296ms step_avg:673.32ms
step:1057/2900 train_loss:3.5324 train_time:704968ms step_avg:673.32ms
step:1058/2900 train_loss:3.5595 train_time:705640ms step_avg:673.32ms
step:1059/2900 train_loss:3.6055 train_time:706311ms step_avg:673.32ms
step:1060/2900 train_loss:3.8209 train_time:706982ms step_avg:673.32ms
step:1061/2900 train_loss:3.5742 train_time:707654ms step_avg:673.32ms
step:1062/2900 train_loss:3.7621 train_time:708325ms step_avg:673.31ms
step:1063/2900 train_loss:3.4631 train_time:708997ms step_avg:673.31ms
step:1064/2900 train_loss:3.5326 train_time:709668ms step_avg:673.31ms
step:1065/2900 train_loss:3.6902 train_time:710339ms step_avg:673.31ms
step:1066/2900 train_loss:3.5940 train_time:711011ms step_avg:673.31ms
step:1067/2900 train_loss:3.7664 train_time:711682ms step_avg:673.30ms
step:1068/2900 train_loss:3.2403 train_time:712354ms step_avg:673.30ms
step:1069/2900 train_loss:3.5121 train_time:713026ms step_avg:673.30ms
step:1070/2900 train_loss:3.6325 train_time:713698ms step_avg:673.30ms
step:1071/2900 train_loss:3.6185 train_time:714369ms step_avg:673.30ms
step:1072/2900 train_loss:3.5577 train_time:715041ms step_avg:673.30ms
step:1073/2900 train_loss:3.4212 train_time:715712ms step_avg:673.29ms
step:1074/2900 train_loss:3.5595 train_time:716384ms step_avg:673.29ms
step:1075/2900 train_loss:3.6175 train_time:717057ms step_avg:673.29ms
step:1076/2900 train_loss:3.5195 train_time:717729ms step_avg:673.29ms
step:1077/2900 train_loss:3.7134 train_time:718400ms step_avg:673.29ms
step:1078/2900 train_loss:3.6046 train_time:719072ms step_avg:673.29ms
step:1079/2900 train_loss:3.5849 train_time:719743ms step_avg:673.29ms
step:1080/2900 train_loss:3.5747 train_time:720416ms step_avg:673.29ms
step:1081/2900 train_loss:3.6371 train_time:721088ms step_avg:673.28ms
step:1082/2900 train_loss:3.6190 train_time:721758ms step_avg:673.28ms
step:1083/2900 train_loss:3.6113 train_time:722429ms step_avg:673.28ms
step:1084/2900 train_loss:3.5556 train_time:723102ms step_avg:673.28ms
step:1085/2900 train_loss:3.3723 train_time:723774ms step_avg:673.28ms
