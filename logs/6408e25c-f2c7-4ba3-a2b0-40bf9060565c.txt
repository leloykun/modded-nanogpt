====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.ones(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 3000 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.04, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 13:55:15 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   34C    P0             71W /  400W |    3263MiB /  81920MiB |      7%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   38C    P0             73W /  400W |    3481MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   34C    P0             75W /  400W |    3407MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   45C    P0             77W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/3000 val_loss:10.8258 train_time:309ms step_avg:nanms
step:1/3000 train_loss:10.8258 train_time:25292ms step_avg:nanms
step:2/3000 train_loss:10.1643 train_time:25942ms step_avg:nanms
step:3/3000 train_loss:8.4796 train_time:26608ms step_avg:nanms
step:4/3000 train_loss:7.6192 train_time:27276ms step_avg:nanms
step:5/3000 train_loss:7.3860 train_time:27945ms step_avg:nanms
step:6/3000 train_loss:7.0029 train_time:28614ms step_avg:nanms
step:7/3000 train_loss:7.2170 train_time:29282ms step_avg:nanms
step:8/3000 train_loss:6.7838 train_time:29951ms step_avg:nanms
step:9/3000 train_loss:6.6842 train_time:30620ms step_avg:nanms
step:10/3000 train_loss:6.6460 train_time:31290ms step_avg:nanms
step:11/3000 train_loss:6.5771 train_time:659ms step_avg:nanms
step:12/3000 train_loss:6.4372 train_time:1330ms step_avg:nanms
step:13/3000 train_loss:6.4001 train_time:2000ms step_avg:666.79ms
step:14/3000 train_loss:6.3262 train_time:2672ms step_avg:667.90ms
step:15/3000 train_loss:6.2912 train_time:3343ms step_avg:668.51ms
step:16/3000 train_loss:6.2654 train_time:4014ms step_avg:668.99ms
step:17/3000 train_loss:6.3489 train_time:4686ms step_avg:669.49ms
step:18/3000 train_loss:6.1496 train_time:5357ms step_avg:669.64ms
step:19/3000 train_loss:6.1808 train_time:6030ms step_avg:669.95ms
step:20/3000 train_loss:5.8574 train_time:6702ms step_avg:670.23ms
step:21/3000 train_loss:6.1867 train_time:7375ms step_avg:670.48ms
step:22/3000 train_loss:6.4105 train_time:8047ms step_avg:670.62ms
step:23/3000 train_loss:6.0821 train_time:8721ms step_avg:670.84ms
step:24/3000 train_loss:6.2402 train_time:9394ms step_avg:671.00ms
step:25/3000 train_loss:5.9358 train_time:10068ms step_avg:671.19ms
step:26/3000 train_loss:5.8569 train_time:10740ms step_avg:671.24ms
step:27/3000 train_loss:6.0744 train_time:11414ms step_avg:671.41ms
step:28/3000 train_loss:5.6861 train_time:12087ms step_avg:671.52ms
step:29/3000 train_loss:5.9480 train_time:12760ms step_avg:671.60ms
step:30/3000 train_loss:5.7699 train_time:13435ms step_avg:671.73ms
step:31/3000 train_loss:5.7395 train_time:14108ms step_avg:671.80ms
step:32/3000 train_loss:5.5822 train_time:14781ms step_avg:671.86ms
step:33/3000 train_loss:5.8717 train_time:15455ms step_avg:671.95ms
step:34/3000 train_loss:5.7880 train_time:16127ms step_avg:671.97ms
step:35/3000 train_loss:5.9012 train_time:16801ms step_avg:672.05ms
step:36/3000 train_loss:5.8533 train_time:17475ms step_avg:672.11ms
step:37/3000 train_loss:5.7489 train_time:18148ms step_avg:672.15ms
step:38/3000 train_loss:5.6120 train_time:18821ms step_avg:672.18ms
step:39/3000 train_loss:5.6109 train_time:19495ms step_avg:672.23ms
step:40/3000 train_loss:5.5599 train_time:20168ms step_avg:672.27ms
step:41/3000 train_loss:5.5487 train_time:20843ms step_avg:672.35ms
step:42/3000 train_loss:5.4437 train_time:21516ms step_avg:672.37ms
step:43/3000 train_loss:5.5286 train_time:22189ms step_avg:672.39ms
step:44/3000 train_loss:5.5140 train_time:22864ms step_avg:672.47ms
step:45/3000 train_loss:5.6613 train_time:23537ms step_avg:672.50ms
step:46/3000 train_loss:5.4518 train_time:24211ms step_avg:672.54ms
step:47/3000 train_loss:5.3215 train_time:24885ms step_avg:672.58ms
step:48/3000 train_loss:5.5001 train_time:25560ms step_avg:672.64ms
step:49/3000 train_loss:5.3972 train_time:26235ms step_avg:672.68ms
step:50/3000 train_loss:5.5102 train_time:26908ms step_avg:672.70ms
step:51/3000 train_loss:5.4019 train_time:27582ms step_avg:672.74ms
step:52/3000 train_loss:5.2642 train_time:28255ms step_avg:672.75ms
step:53/3000 train_loss:5.3758 train_time:28929ms step_avg:672.78ms
step:54/3000 train_loss:5.2489 train_time:29603ms step_avg:672.80ms
step:55/3000 train_loss:5.6487 train_time:30276ms step_avg:672.81ms
step:56/3000 train_loss:5.2748 train_time:30950ms step_avg:672.83ms
step:57/3000 train_loss:5.1251 train_time:31624ms step_avg:672.85ms
step:58/3000 train_loss:5.2330 train_time:32296ms step_avg:672.84ms
step:59/3000 train_loss:5.2155 train_time:32971ms step_avg:672.87ms
step:60/3000 train_loss:5.3431 train_time:33645ms step_avg:672.90ms
step:61/3000 train_loss:5.0707 train_time:34318ms step_avg:672.90ms
step:62/3000 train_loss:5.1785 train_time:34991ms step_avg:672.90ms
step:63/3000 train_loss:5.1620 train_time:35665ms step_avg:672.92ms
step:64/3000 train_loss:5.1840 train_time:36339ms step_avg:672.94ms
step:65/3000 train_loss:4.9782 train_time:37012ms step_avg:672.95ms
step:66/3000 train_loss:5.1466 train_time:37686ms step_avg:672.97ms
step:67/3000 train_loss:5.0159 train_time:38360ms step_avg:672.98ms
step:68/3000 train_loss:5.2760 train_time:39032ms step_avg:672.97ms
step:69/3000 train_loss:4.9010 train_time:39706ms step_avg:672.99ms
step:70/3000 train_loss:4.9897 train_time:40380ms step_avg:673.00ms
step:71/3000 train_loss:5.1579 train_time:41054ms step_avg:673.02ms
step:72/3000 train_loss:5.0587 train_time:41727ms step_avg:673.02ms
step:73/3000 train_loss:4.9342 train_time:42401ms step_avg:673.03ms
step:74/3000 train_loss:5.0763 train_time:43075ms step_avg:673.04ms
step:75/3000 train_loss:5.0438 train_time:43748ms step_avg:673.04ms
step:76/3000 train_loss:4.9596 train_time:44421ms step_avg:673.05ms
step:77/3000 train_loss:5.0838 train_time:45094ms step_avg:673.05ms
step:78/3000 train_loss:5.2601 train_time:45768ms step_avg:673.06ms
step:79/3000 train_loss:4.9755 train_time:46442ms step_avg:673.07ms
step:80/3000 train_loss:5.0245 train_time:47115ms step_avg:673.07ms
step:81/3000 train_loss:4.8231 train_time:47788ms step_avg:673.07ms
step:82/3000 train_loss:4.9909 train_time:48461ms step_avg:673.07ms
step:83/3000 train_loss:4.9415 train_time:49134ms step_avg:673.07ms
step:84/3000 train_loss:4.9137 train_time:49807ms step_avg:673.06ms
step:85/3000 train_loss:4.7779 train_time:50480ms step_avg:673.06ms
step:86/3000 train_loss:4.9733 train_time:51153ms step_avg:673.07ms
step:87/3000 train_loss:4.8857 train_time:51826ms step_avg:673.06ms
step:88/3000 train_loss:4.8962 train_time:52499ms step_avg:673.07ms
step:89/3000 train_loss:4.8563 train_time:53172ms step_avg:673.06ms
step:90/3000 train_loss:4.7977 train_time:53846ms step_avg:673.07ms
step:91/3000 train_loss:4.7659 train_time:54518ms step_avg:673.07ms
step:92/3000 train_loss:4.9181 train_time:55193ms step_avg:673.08ms
step:93/3000 train_loss:4.7343 train_time:55866ms step_avg:673.08ms
step:94/3000 train_loss:4.7800 train_time:56540ms step_avg:673.10ms
step:95/3000 train_loss:4.8132 train_time:57213ms step_avg:673.10ms
step:96/3000 train_loss:4.7140 train_time:57886ms step_avg:673.09ms
step:97/3000 train_loss:4.7573 train_time:58560ms step_avg:673.10ms
step:98/3000 train_loss:4.6939 train_time:59233ms step_avg:673.10ms
step:99/3000 train_loss:4.7776 train_time:59907ms step_avg:673.11ms
step:100/3000 train_loss:4.7867 train_time:60581ms step_avg:673.12ms
step:101/3000 train_loss:4.6595 train_time:61253ms step_avg:673.11ms
step:102/3000 train_loss:4.7959 train_time:61927ms step_avg:673.11ms
step:103/3000 train_loss:4.6827 train_time:62600ms step_avg:673.12ms
step:104/3000 train_loss:4.6332 train_time:63273ms step_avg:673.11ms
step:105/3000 train_loss:4.6449 train_time:63946ms step_avg:673.12ms
step:106/3000 train_loss:4.6789 train_time:64620ms step_avg:673.12ms
step:107/3000 train_loss:4.6051 train_time:65293ms step_avg:673.13ms
step:108/3000 train_loss:4.4454 train_time:65966ms step_avg:673.12ms
step:109/3000 train_loss:4.5754 train_time:66640ms step_avg:673.13ms
step:110/3000 train_loss:4.5532 train_time:67313ms step_avg:673.13ms
step:111/3000 train_loss:4.4996 train_time:67986ms step_avg:673.13ms
step:112/3000 train_loss:4.6660 train_time:68659ms step_avg:673.13ms
step:113/3000 train_loss:4.5574 train_time:69332ms step_avg:673.13ms
step:114/3000 train_loss:4.4272 train_time:70006ms step_avg:673.14ms
step:115/3000 train_loss:4.5846 train_time:70679ms step_avg:673.13ms
step:116/3000 train_loss:4.5482 train_time:71353ms step_avg:673.14ms
step:117/3000 train_loss:4.4639 train_time:72025ms step_avg:673.13ms
step:118/3000 train_loss:4.6667 train_time:72699ms step_avg:673.14ms
step:119/3000 train_loss:4.5309 train_time:73373ms step_avg:673.14ms
step:120/3000 train_loss:4.4030 train_time:74045ms step_avg:673.13ms
step:121/3000 train_loss:4.3741 train_time:74719ms step_avg:673.14ms
step:122/3000 train_loss:4.5272 train_time:75390ms step_avg:673.13ms
step:123/3000 train_loss:4.3497 train_time:76063ms step_avg:673.13ms
step:124/3000 train_loss:4.6543 train_time:76737ms step_avg:673.13ms
step:125/3000 train_loss:4.5221 train_time:77409ms step_avg:673.12ms
step:125/3000 val_loss:4.4707 train_time:77421ms step_avg:673.23ms
step:126/3000 train_loss:4.4791 train_time:78084ms step_avg:673.14ms
step:127/3000 train_loss:4.4944 train_time:78759ms step_avg:673.15ms
step:128/3000 train_loss:4.4348 train_time:79432ms step_avg:673.16ms
step:129/3000 train_loss:4.7352 train_time:80107ms step_avg:673.17ms
step:130/3000 train_loss:4.4097 train_time:80780ms step_avg:673.17ms
step:131/3000 train_loss:4.4667 train_time:81455ms step_avg:673.18ms
step:132/3000 train_loss:4.3989 train_time:82129ms step_avg:673.18ms
step:133/3000 train_loss:4.5148 train_time:82801ms step_avg:673.18ms
step:134/3000 train_loss:4.3314 train_time:83475ms step_avg:673.19ms
step:135/3000 train_loss:4.5070 train_time:84148ms step_avg:673.19ms
step:136/3000 train_loss:4.2546 train_time:84822ms step_avg:673.19ms
step:137/3000 train_loss:4.4406 train_time:85495ms step_avg:673.19ms
step:138/3000 train_loss:4.3400 train_time:86169ms step_avg:673.19ms
step:139/3000 train_loss:4.4324 train_time:86842ms step_avg:673.20ms
step:140/3000 train_loss:4.5324 train_time:87514ms step_avg:673.19ms
step:141/3000 train_loss:4.3673 train_time:88188ms step_avg:673.19ms
step:142/3000 train_loss:4.3488 train_time:88862ms step_avg:673.20ms
step:143/3000 train_loss:4.2999 train_time:89535ms step_avg:673.20ms
step:144/3000 train_loss:4.4033 train_time:90209ms step_avg:673.20ms
step:145/3000 train_loss:4.3543 train_time:90883ms step_avg:673.21ms
step:146/3000 train_loss:4.2276 train_time:91556ms step_avg:673.21ms
step:147/3000 train_loss:4.3720 train_time:92230ms step_avg:673.21ms
step:148/3000 train_loss:4.4049 train_time:92902ms step_avg:673.21ms
step:149/3000 train_loss:4.3544 train_time:93576ms step_avg:673.21ms
step:150/3000 train_loss:4.4706 train_time:94249ms step_avg:673.20ms
step:151/3000 train_loss:4.3073 train_time:94921ms step_avg:673.20ms
step:152/3000 train_loss:4.3236 train_time:95594ms step_avg:673.20ms
step:153/3000 train_loss:4.4032 train_time:96267ms step_avg:673.20ms
step:154/3000 train_loss:4.4041 train_time:96941ms step_avg:673.20ms
step:155/3000 train_loss:4.3112 train_time:97616ms step_avg:673.21ms
step:156/3000 train_loss:4.3812 train_time:98290ms step_avg:673.22ms
step:157/3000 train_loss:4.4541 train_time:98964ms step_avg:673.22ms
step:158/3000 train_loss:4.2759 train_time:99637ms step_avg:673.22ms
step:159/3000 train_loss:4.3638 train_time:100311ms step_avg:673.23ms
step:160/3000 train_loss:4.1524 train_time:100984ms step_avg:673.23ms
step:161/3000 train_loss:4.3950 train_time:101658ms step_avg:673.23ms
step:162/3000 train_loss:4.3840 train_time:102330ms step_avg:673.22ms
step:163/3000 train_loss:4.3623 train_time:103003ms step_avg:673.22ms
step:164/3000 train_loss:4.2416 train_time:103677ms step_avg:673.23ms
step:165/3000 train_loss:4.3372 train_time:104350ms step_avg:673.22ms
step:166/3000 train_loss:4.3948 train_time:105023ms step_avg:673.23ms
step:167/3000 train_loss:4.2305 train_time:105697ms step_avg:673.23ms
step:168/3000 train_loss:4.3082 train_time:106370ms step_avg:673.23ms
step:169/3000 train_loss:4.1876 train_time:107043ms step_avg:673.23ms
step:170/3000 train_loss:4.0570 train_time:107716ms step_avg:673.23ms
step:171/3000 train_loss:4.2260 train_time:108389ms step_avg:673.22ms
step:172/3000 train_loss:4.2535 train_time:109062ms step_avg:673.22ms
step:173/3000 train_loss:4.3000 train_time:109735ms step_avg:673.22ms
step:174/3000 train_loss:4.4654 train_time:110409ms step_avg:673.23ms
step:175/3000 train_loss:4.2873 train_time:111082ms step_avg:673.22ms
step:176/3000 train_loss:4.1466 train_time:111755ms step_avg:673.22ms
step:177/3000 train_loss:4.1128 train_time:112428ms step_avg:673.22ms
step:178/3000 train_loss:4.2328 train_time:113100ms step_avg:673.22ms
step:179/3000 train_loss:4.1823 train_time:113773ms step_avg:673.22ms
step:180/3000 train_loss:4.1654 train_time:114446ms step_avg:673.21ms
step:181/3000 train_loss:4.3353 train_time:115118ms step_avg:673.20ms
step:182/3000 train_loss:4.1967 train_time:115792ms step_avg:673.21ms
step:183/3000 train_loss:4.1743 train_time:116465ms step_avg:673.21ms
step:184/3000 train_loss:4.1741 train_time:117138ms step_avg:673.21ms
step:185/3000 train_loss:4.2571 train_time:117811ms step_avg:673.21ms
step:186/3000 train_loss:4.2295 train_time:118483ms step_avg:673.20ms
step:187/3000 train_loss:4.2932 train_time:119155ms step_avg:673.19ms
step:188/3000 train_loss:4.2137 train_time:119829ms step_avg:673.20ms
step:189/3000 train_loss:4.1540 train_time:120501ms step_avg:673.19ms
step:190/3000 train_loss:4.2501 train_time:121176ms step_avg:673.20ms
step:191/3000 train_loss:4.3170 train_time:122043ms step_avg:674.27ms
step:192/3000 train_loss:4.1267 train_time:122716ms step_avg:674.26ms
step:193/3000 train_loss:4.1955 train_time:123390ms step_avg:674.26ms
step:194/3000 train_loss:4.1516 train_time:124062ms step_avg:674.25ms
step:195/3000 train_loss:4.1191 train_time:124736ms step_avg:674.25ms
step:196/3000 train_loss:4.1190 train_time:125409ms step_avg:674.24ms
step:197/3000 train_loss:4.1496 train_time:126082ms step_avg:674.24ms
step:198/3000 train_loss:4.0539 train_time:126755ms step_avg:674.23ms
step:199/3000 train_loss:4.2983 train_time:127428ms step_avg:674.22ms
step:200/3000 train_loss:4.2195 train_time:128101ms step_avg:674.22ms
step:201/3000 train_loss:4.9860 train_time:128774ms step_avg:674.21ms
step:202/3000 train_loss:4.4196 train_time:129446ms step_avg:674.20ms
step:203/3000 train_loss:4.1968 train_time:130120ms step_avg:674.20ms
step:204/3000 train_loss:4.1140 train_time:130794ms step_avg:674.20ms
step:205/3000 train_loss:4.1665 train_time:131467ms step_avg:674.19ms
step:206/3000 train_loss:4.1321 train_time:132140ms step_avg:674.18ms
step:207/3000 train_loss:4.1213 train_time:132813ms step_avg:674.18ms
step:208/3000 train_loss:4.0692 train_time:133485ms step_avg:674.17ms
step:209/3000 train_loss:4.2467 train_time:134157ms step_avg:674.16ms
step:210/3000 train_loss:4.0670 train_time:134830ms step_avg:674.15ms
step:211/3000 train_loss:4.1820 train_time:135503ms step_avg:674.14ms
step:212/3000 train_loss:4.1699 train_time:136176ms step_avg:674.14ms
step:213/3000 train_loss:4.1100 train_time:136848ms step_avg:674.13ms
step:214/3000 train_loss:4.0583 train_time:137520ms step_avg:674.12ms
step:215/3000 train_loss:4.1987 train_time:138193ms step_avg:674.11ms
step:216/3000 train_loss:4.1278 train_time:138866ms step_avg:674.11ms
step:217/3000 train_loss:4.1125 train_time:139540ms step_avg:674.10ms
step:218/3000 train_loss:4.1462 train_time:140212ms step_avg:674.10ms
step:219/3000 train_loss:4.0680 train_time:140885ms step_avg:674.09ms
step:220/3000 train_loss:4.1832 train_time:141558ms step_avg:674.09ms
step:221/3000 train_loss:4.0805 train_time:142231ms step_avg:674.08ms
step:222/3000 train_loss:4.1599 train_time:142904ms step_avg:674.08ms
step:223/3000 train_loss:4.0805 train_time:143576ms step_avg:674.06ms
step:224/3000 train_loss:4.0935 train_time:144249ms step_avg:674.06ms
step:225/3000 train_loss:4.0778 train_time:144921ms step_avg:674.05ms
step:226/3000 train_loss:4.1015 train_time:145594ms step_avg:674.05ms
step:227/3000 train_loss:4.0910 train_time:146268ms step_avg:674.04ms
step:228/3000 train_loss:4.0097 train_time:146940ms step_avg:674.04ms
step:229/3000 train_loss:4.2322 train_time:147613ms step_avg:674.03ms
step:230/3000 train_loss:4.0984 train_time:148286ms step_avg:674.03ms
step:231/3000 train_loss:3.9905 train_time:148958ms step_avg:674.02ms
step:232/3000 train_loss:4.2020 train_time:149630ms step_avg:674.01ms
step:233/3000 train_loss:4.0340 train_time:150305ms step_avg:674.01ms
step:234/3000 train_loss:4.0587 train_time:150978ms step_avg:674.01ms
step:235/3000 train_loss:4.2295 train_time:151652ms step_avg:674.01ms
step:236/3000 train_loss:4.1255 train_time:152325ms step_avg:674.01ms
step:237/3000 train_loss:4.2103 train_time:152998ms step_avg:674.00ms
step:238/3000 train_loss:4.1548 train_time:153671ms step_avg:674.00ms
step:239/3000 train_loss:4.1546 train_time:154344ms step_avg:673.99ms
step:240/3000 train_loss:4.1080 train_time:155019ms step_avg:674.00ms
step:241/3000 train_loss:4.3066 train_time:155692ms step_avg:673.99ms
step:242/3000 train_loss:4.1327 train_time:156365ms step_avg:673.99ms
step:243/3000 train_loss:4.4791 train_time:157038ms step_avg:673.98ms
step:244/3000 train_loss:4.0825 train_time:157711ms step_avg:673.98ms
step:245/3000 train_loss:4.1103 train_time:158385ms step_avg:673.98ms
step:246/3000 train_loss:4.1422 train_time:159057ms step_avg:673.97ms
step:247/3000 train_loss:4.0449 train_time:159731ms step_avg:673.97ms
step:248/3000 train_loss:4.0609 train_time:160404ms step_avg:673.97ms
step:249/3000 train_loss:3.9898 train_time:161077ms step_avg:673.96ms
step:250/3000 train_loss:4.1546 train_time:161748ms step_avg:673.95ms
step:250/3000 val_loss:4.0591 train_time:161760ms step_avg:674.00ms
step:251/3000 train_loss:4.0401 train_time:162424ms step_avg:673.96ms
step:252/3000 train_loss:3.9590 train_time:163098ms step_avg:673.96ms
step:253/3000 train_loss:3.9933 train_time:163770ms step_avg:673.95ms
step:254/3000 train_loss:4.0526 train_time:164444ms step_avg:673.95ms
step:255/3000 train_loss:3.9980 train_time:165117ms step_avg:673.95ms
step:256/3000 train_loss:3.9409 train_time:165789ms step_avg:673.94ms
step:257/3000 train_loss:4.0074 train_time:166463ms step_avg:673.94ms
step:258/3000 train_loss:4.1280 train_time:167136ms step_avg:673.94ms
step:259/3000 train_loss:3.9433 train_time:167809ms step_avg:673.93ms
step:260/3000 train_loss:4.0524 train_time:168482ms step_avg:673.93ms
step:261/3000 train_loss:4.0062 train_time:169154ms step_avg:673.92ms
step:262/3000 train_loss:4.0393 train_time:169827ms step_avg:673.92ms
step:263/3000 train_loss:4.1084 train_time:170500ms step_avg:673.91ms
step:264/3000 train_loss:4.1521 train_time:171174ms step_avg:673.91ms
step:265/3000 train_loss:4.5269 train_time:171846ms step_avg:673.91ms
step:266/3000 train_loss:4.0325 train_time:172519ms step_avg:673.90ms
step:267/3000 train_loss:4.2311 train_time:173193ms step_avg:673.90ms
step:268/3000 train_loss:3.9751 train_time:173866ms step_avg:673.90ms
step:269/3000 train_loss:3.9901 train_time:174540ms step_avg:673.90ms
step:270/3000 train_loss:4.1003 train_time:175212ms step_avg:673.89ms
step:271/3000 train_loss:4.1509 train_time:175886ms step_avg:673.89ms
step:272/3000 train_loss:4.0341 train_time:176560ms step_avg:673.89ms
step:273/3000 train_loss:3.9921 train_time:177233ms step_avg:673.89ms
step:274/3000 train_loss:3.9867 train_time:177905ms step_avg:673.88ms
step:275/3000 train_loss:3.9613 train_time:178578ms step_avg:673.88ms
step:276/3000 train_loss:4.0712 train_time:179252ms step_avg:673.88ms
step:277/3000 train_loss:4.1037 train_time:179925ms step_avg:673.88ms
step:278/3000 train_loss:3.8898 train_time:180598ms step_avg:673.87ms
step:279/3000 train_loss:4.2027 train_time:181272ms step_avg:673.87ms
step:280/3000 train_loss:4.0484 train_time:181945ms step_avg:673.87ms
step:281/3000 train_loss:4.0537 train_time:182619ms step_avg:673.87ms
step:282/3000 train_loss:3.9990 train_time:183292ms step_avg:673.87ms
step:283/3000 train_loss:4.2122 train_time:183965ms step_avg:673.86ms
step:284/3000 train_loss:3.9489 train_time:184639ms step_avg:673.86ms
step:285/3000 train_loss:3.9234 train_time:185313ms step_avg:673.86ms
step:286/3000 train_loss:4.1063 train_time:185986ms step_avg:673.86ms
step:287/3000 train_loss:4.0341 train_time:186661ms step_avg:673.87ms
step:288/3000 train_loss:4.0637 train_time:187333ms step_avg:673.86ms
step:289/3000 train_loss:4.1028 train_time:188007ms step_avg:673.86ms
step:290/3000 train_loss:4.0983 train_time:188681ms step_avg:673.86ms
step:291/3000 train_loss:3.9370 train_time:189354ms step_avg:673.86ms
step:292/3000 train_loss:3.9119 train_time:190027ms step_avg:673.85ms
step:293/3000 train_loss:4.1110 train_time:190699ms step_avg:673.85ms
step:294/3000 train_loss:3.9125 train_time:191372ms step_avg:673.85ms
step:295/3000 train_loss:4.1986 train_time:192046ms step_avg:673.84ms
step:296/3000 train_loss:4.0880 train_time:192718ms step_avg:673.84ms
step:297/3000 train_loss:3.9179 train_time:193391ms step_avg:673.84ms
step:298/3000 train_loss:4.0235 train_time:194064ms step_avg:673.83ms
step:299/3000 train_loss:3.9419 train_time:194737ms step_avg:673.83ms
step:300/3000 train_loss:4.1226 train_time:195411ms step_avg:673.83ms
step:301/3000 train_loss:3.9189 train_time:196085ms step_avg:673.83ms
step:302/3000 train_loss:4.0308 train_time:196757ms step_avg:673.82ms
step:303/3000 train_loss:3.9630 train_time:197430ms step_avg:673.82ms
step:304/3000 train_loss:3.9651 train_time:198104ms step_avg:673.82ms
step:305/3000 train_loss:3.9676 train_time:198778ms step_avg:673.82ms
step:306/3000 train_loss:4.0806 train_time:199452ms step_avg:673.83ms
step:307/3000 train_loss:4.0133 train_time:200126ms step_avg:673.82ms
step:308/3000 train_loss:3.9179 train_time:200799ms step_avg:673.82ms
step:309/3000 train_loss:4.0951 train_time:201473ms step_avg:673.82ms
step:310/3000 train_loss:3.9546 train_time:202145ms step_avg:673.82ms
step:311/3000 train_loss:4.0152 train_time:202818ms step_avg:673.82ms
step:312/3000 train_loss:4.0053 train_time:203491ms step_avg:673.81ms
step:313/3000 train_loss:3.9176 train_time:204165ms step_avg:673.81ms
step:314/3000 train_loss:4.0059 train_time:204839ms step_avg:673.81ms
step:315/3000 train_loss:3.9904 train_time:205512ms step_avg:673.81ms
step:316/3000 train_loss:3.7276 train_time:206186ms step_avg:673.81ms
step:317/3000 train_loss:3.9719 train_time:206860ms step_avg:673.81ms
step:318/3000 train_loss:3.9660 train_time:207532ms step_avg:673.81ms
step:319/3000 train_loss:3.8857 train_time:208205ms step_avg:673.80ms
step:320/3000 train_loss:3.9126 train_time:208878ms step_avg:673.80ms
step:321/3000 train_loss:3.9666 train_time:209551ms step_avg:673.80ms
step:322/3000 train_loss:3.9782 train_time:210224ms step_avg:673.80ms
step:323/3000 train_loss:3.8480 train_time:210898ms step_avg:673.80ms
step:324/3000 train_loss:3.9358 train_time:211572ms step_avg:673.80ms
step:325/3000 train_loss:3.9819 train_time:212244ms step_avg:673.79ms
step:326/3000 train_loss:3.9885 train_time:212918ms step_avg:673.79ms
step:327/3000 train_loss:3.9471 train_time:213591ms step_avg:673.79ms
step:328/3000 train_loss:4.2194 train_time:214263ms step_avg:673.78ms
step:329/3000 train_loss:3.9906 train_time:214937ms step_avg:673.78ms
step:330/3000 train_loss:4.0411 train_time:215609ms step_avg:673.78ms
step:331/3000 train_loss:3.9042 train_time:216283ms step_avg:673.78ms
step:332/3000 train_loss:4.4934 train_time:216956ms step_avg:673.78ms
step:333/3000 train_loss:3.8974 train_time:217629ms step_avg:673.77ms
step:334/3000 train_loss:3.8054 train_time:218302ms step_avg:673.77ms
step:335/3000 train_loss:3.9776 train_time:218976ms step_avg:673.77ms
step:336/3000 train_loss:3.9896 train_time:219649ms step_avg:673.77ms
step:337/3000 train_loss:3.9913 train_time:220321ms step_avg:673.77ms
step:338/3000 train_loss:3.9676 train_time:220994ms step_avg:673.76ms
step:339/3000 train_loss:4.0211 train_time:221667ms step_avg:673.76ms
step:340/3000 train_loss:3.8841 train_time:222341ms step_avg:673.76ms
step:341/3000 train_loss:3.8720 train_time:223013ms step_avg:673.76ms
step:342/3000 train_loss:3.8937 train_time:223687ms step_avg:673.76ms
step:343/3000 train_loss:3.9254 train_time:224359ms step_avg:673.75ms
step:344/3000 train_loss:4.0202 train_time:225033ms step_avg:673.75ms
step:345/3000 train_loss:3.9433 train_time:225705ms step_avg:673.75ms
step:346/3000 train_loss:3.9111 train_time:226378ms step_avg:673.74ms
step:347/3000 train_loss:3.9633 train_time:227051ms step_avg:673.74ms
step:348/3000 train_loss:3.9002 train_time:227725ms step_avg:673.74ms
step:349/3000 train_loss:3.8917 train_time:228398ms step_avg:673.74ms
step:350/3000 train_loss:3.9712 train_time:229071ms step_avg:673.74ms
step:351/3000 train_loss:4.1962 train_time:229744ms step_avg:673.74ms
step:352/3000 train_loss:3.8435 train_time:230417ms step_avg:673.73ms
step:353/3000 train_loss:3.8913 train_time:231090ms step_avg:673.73ms
step:354/3000 train_loss:3.8851 train_time:231764ms step_avg:673.73ms
step:355/3000 train_loss:3.7932 train_time:232438ms step_avg:673.73ms
step:356/3000 train_loss:3.9344 train_time:233110ms step_avg:673.73ms
step:357/3000 train_loss:3.8501 train_time:233783ms step_avg:673.73ms
step:358/3000 train_loss:3.9819 train_time:234456ms step_avg:673.72ms
step:359/3000 train_loss:3.9602 train_time:235129ms step_avg:673.72ms
step:360/3000 train_loss:3.8731 train_time:235803ms step_avg:673.72ms
step:361/3000 train_loss:4.1382 train_time:236475ms step_avg:673.72ms
step:362/3000 train_loss:4.1227 train_time:237147ms step_avg:673.71ms
step:363/3000 train_loss:3.9322 train_time:237821ms step_avg:673.71ms
step:364/3000 train_loss:3.8301 train_time:238494ms step_avg:673.71ms
step:365/3000 train_loss:4.0129 train_time:239166ms step_avg:673.71ms
step:366/3000 train_loss:3.8556 train_time:239839ms step_avg:673.71ms
step:367/3000 train_loss:3.9382 train_time:240512ms step_avg:673.70ms
step:368/3000 train_loss:3.8712 train_time:241185ms step_avg:673.70ms
step:369/3000 train_loss:3.9545 train_time:241859ms step_avg:673.70ms
step:370/3000 train_loss:3.9312 train_time:242531ms step_avg:673.70ms
step:371/3000 train_loss:3.7431 train_time:243205ms step_avg:673.70ms
step:372/3000 train_loss:3.8607 train_time:243877ms step_avg:673.69ms
step:373/3000 train_loss:3.9177 train_time:244550ms step_avg:673.69ms
step:374/3000 train_loss:3.8734 train_time:245222ms step_avg:673.69ms
step:375/3000 train_loss:3.9585 train_time:245895ms step_avg:673.69ms
step:375/3000 val_loss:3.8972 train_time:245907ms step_avg:673.72ms
step:376/3000 train_loss:3.8757 train_time:246572ms step_avg:673.69ms
step:377/3000 train_loss:3.7784 train_time:247244ms step_avg:673.69ms
step:378/3000 train_loss:3.3754 train_time:247917ms step_avg:673.69ms
step:379/3000 train_loss:3.7481 train_time:248590ms step_avg:673.69ms
step:380/3000 train_loss:3.7630 train_time:249262ms step_avg:673.68ms
step:381/3000 train_loss:3.8671 train_time:249941ms step_avg:673.69ms
step:382/3000 train_loss:3.9150 train_time:250613ms step_avg:673.69ms
step:383/3000 train_loss:3.8839 train_time:251285ms step_avg:673.69ms
step:384/3000 train_loss:3.8660 train_time:251957ms step_avg:673.68ms
step:385/3000 train_loss:3.9436 train_time:252630ms step_avg:673.68ms
step:386/3000 train_loss:3.8594 train_time:253304ms step_avg:673.68ms
step:387/3000 train_loss:3.9568 train_time:253976ms step_avg:673.68ms
step:388/3000 train_loss:4.1440 train_time:254649ms step_avg:673.67ms
step:389/3000 train_loss:3.8716 train_time:255322ms step_avg:673.67ms
step:390/3000 train_loss:3.8624 train_time:255995ms step_avg:673.67ms
step:391/3000 train_loss:3.9686 train_time:256667ms step_avg:673.67ms
step:392/3000 train_loss:3.8910 train_time:257340ms step_avg:673.66ms
step:393/3000 train_loss:3.9960 train_time:258012ms step_avg:673.66ms
step:394/3000 train_loss:3.8242 train_time:258684ms step_avg:673.66ms
step:395/3000 train_loss:3.9654 train_time:259356ms step_avg:673.65ms
step:396/3000 train_loss:3.7138 train_time:260030ms step_avg:673.65ms
step:397/3000 train_loss:3.9188 train_time:260703ms step_avg:673.65ms
step:398/3000 train_loss:3.9486 train_time:261376ms step_avg:673.65ms
step:399/3000 train_loss:3.9503 train_time:262049ms step_avg:673.65ms
step:400/3000 train_loss:3.8554 train_time:262721ms step_avg:673.64ms
step:401/3000 train_loss:3.9162 train_time:263394ms step_avg:673.64ms
step:402/3000 train_loss:3.9931 train_time:264066ms step_avg:673.64ms
step:403/3000 train_loss:3.9139 train_time:264738ms step_avg:673.63ms
step:404/3000 train_loss:4.0275 train_time:265409ms step_avg:673.63ms
step:405/3000 train_loss:3.7759 train_time:266081ms step_avg:673.62ms
step:406/3000 train_loss:3.8718 train_time:266754ms step_avg:673.62ms
step:407/3000 train_loss:4.1701 train_time:267426ms step_avg:673.62ms
step:408/3000 train_loss:3.8681 train_time:268100ms step_avg:673.62ms
step:409/3000 train_loss:3.9042 train_time:268772ms step_avg:673.61ms
step:410/3000 train_loss:3.9362 train_time:269446ms step_avg:673.61ms
step:411/3000 train_loss:3.8365 train_time:270118ms step_avg:673.61ms
step:412/3000 train_loss:3.8456 train_time:270790ms step_avg:673.61ms
step:413/3000 train_loss:4.2758 train_time:271462ms step_avg:673.60ms
step:414/3000 train_loss:3.6998 train_time:272135ms step_avg:673.60ms
step:415/3000 train_loss:4.0936 train_time:272807ms step_avg:673.60ms
step:416/3000 train_loss:3.8366 train_time:273479ms step_avg:673.59ms
step:417/3000 train_loss:3.8478 train_time:274151ms step_avg:673.59ms
step:418/3000 train_loss:4.0334 train_time:274823ms step_avg:673.59ms
step:419/3000 train_loss:3.7693 train_time:275494ms step_avg:673.58ms
step:420/3000 train_loss:3.8973 train_time:276167ms step_avg:673.58ms
step:421/3000 train_loss:3.7961 train_time:276839ms step_avg:673.57ms
step:422/3000 train_loss:3.7326 train_time:277511ms step_avg:673.57ms
step:423/3000 train_loss:3.8645 train_time:278182ms step_avg:673.56ms
step:424/3000 train_loss:3.9618 train_time:278853ms step_avg:673.56ms
step:425/3000 train_loss:3.6955 train_time:279525ms step_avg:673.55ms
step:426/3000 train_loss:3.8848 train_time:280197ms step_avg:673.55ms
step:427/3000 train_loss:3.7573 train_time:280869ms step_avg:673.55ms
step:428/3000 train_loss:3.9807 train_time:281541ms step_avg:673.54ms
step:429/3000 train_loss:3.8950 train_time:282213ms step_avg:673.54ms
step:430/3000 train_loss:3.8366 train_time:282885ms step_avg:673.53ms
step:431/3000 train_loss:3.7986 train_time:283560ms step_avg:673.54ms
step:432/3000 train_loss:3.7086 train_time:284232ms step_avg:673.53ms
step:433/3000 train_loss:3.8471 train_time:284904ms step_avg:673.53ms
step:434/3000 train_loss:3.9046 train_time:285576ms step_avg:673.53ms
step:435/3000 train_loss:3.8555 train_time:286248ms step_avg:673.52ms
step:436/3000 train_loss:3.8995 train_time:286919ms step_avg:673.52ms
step:437/3000 train_loss:3.9172 train_time:287591ms step_avg:673.52ms
step:438/3000 train_loss:3.7885 train_time:288263ms step_avg:673.51ms
step:439/3000 train_loss:3.8003 train_time:288934ms step_avg:673.51ms
step:440/3000 train_loss:3.7853 train_time:289606ms step_avg:673.50ms
step:441/3000 train_loss:3.9718 train_time:290278ms step_avg:673.50ms
step:442/3000 train_loss:3.8480 train_time:290950ms step_avg:673.50ms
step:443/3000 train_loss:3.8292 train_time:291624ms step_avg:673.50ms
step:444/3000 train_loss:3.7276 train_time:292295ms step_avg:673.49ms
step:445/3000 train_loss:4.0190 train_time:292967ms step_avg:673.49ms
step:446/3000 train_loss:3.9325 train_time:293640ms step_avg:673.49ms
step:447/3000 train_loss:3.9121 train_time:294313ms step_avg:673.48ms
step:448/3000 train_loss:3.8283 train_time:294985ms step_avg:673.48ms
step:449/3000 train_loss:3.9439 train_time:295657ms step_avg:673.48ms
step:450/3000 train_loss:3.7762 train_time:296328ms step_avg:673.47ms
step:451/3000 train_loss:3.8254 train_time:297000ms step_avg:673.47ms
step:452/3000 train_loss:3.6624 train_time:297671ms step_avg:673.46ms
step:453/3000 train_loss:3.7931 train_time:298343ms step_avg:673.46ms
step:454/3000 train_loss:3.7637 train_time:299014ms step_avg:673.46ms
step:455/3000 train_loss:3.7251 train_time:299687ms step_avg:673.45ms
step:456/3000 train_loss:3.9321 train_time:300357ms step_avg:673.45ms
step:457/3000 train_loss:3.8227 train_time:301030ms step_avg:673.44ms
step:458/3000 train_loss:3.8875 train_time:301702ms step_avg:673.44ms
step:459/3000 train_loss:3.9223 train_time:302374ms step_avg:673.44ms
step:460/3000 train_loss:3.7283 train_time:303046ms step_avg:673.44ms
step:461/3000 train_loss:3.8864 train_time:303718ms step_avg:673.43ms
step:462/3000 train_loss:3.7950 train_time:304390ms step_avg:673.43ms
step:463/3000 train_loss:3.8173 train_time:305062ms step_avg:673.43ms
step:464/3000 train_loss:3.8660 train_time:305734ms step_avg:673.42ms
step:465/3000 train_loss:3.8083 train_time:306407ms step_avg:673.42ms
step:466/3000 train_loss:3.8159 train_time:307078ms step_avg:673.42ms
step:467/3000 train_loss:3.8971 train_time:307750ms step_avg:673.41ms
step:468/3000 train_loss:3.9190 train_time:308421ms step_avg:673.41ms
step:469/3000 train_loss:3.8999 train_time:309093ms step_avg:673.41ms
step:470/3000 train_loss:3.7860 train_time:309765ms step_avg:673.40ms
step:471/3000 train_loss:3.8650 train_time:310438ms step_avg:673.40ms
step:472/3000 train_loss:3.9192 train_time:311110ms step_avg:673.40ms
step:473/3000 train_loss:3.8728 train_time:311782ms step_avg:673.40ms
step:474/3000 train_loss:3.8137 train_time:312454ms step_avg:673.39ms
step:475/3000 train_loss:3.6780 train_time:313125ms step_avg:673.39ms
step:476/3000 train_loss:4.1180 train_time:313795ms step_avg:673.38ms
step:477/3000 train_loss:3.8583 train_time:314467ms step_avg:673.38ms
step:478/3000 train_loss:3.6863 train_time:315137ms step_avg:673.37ms
step:479/3000 train_loss:3.9146 train_time:315809ms step_avg:673.37ms
step:480/3000 train_loss:3.8675 train_time:316480ms step_avg:673.36ms
step:481/3000 train_loss:4.0092 train_time:317150ms step_avg:673.35ms
step:482/3000 train_loss:3.8272 train_time:317821ms step_avg:673.35ms
step:483/3000 train_loss:3.6252 train_time:318491ms step_avg:673.34ms
step:484/3000 train_loss:3.9025 train_time:319162ms step_avg:673.34ms
step:485/3000 train_loss:3.7619 train_time:319833ms step_avg:673.33ms
step:486/3000 train_loss:3.7688 train_time:320505ms step_avg:673.33ms
step:487/3000 train_loss:3.6905 train_time:321176ms step_avg:673.33ms
step:488/3000 train_loss:3.7652 train_time:321847ms step_avg:673.32ms
step:489/3000 train_loss:3.9784 train_time:322518ms step_avg:673.31ms
step:490/3000 train_loss:3.8199 train_time:323189ms step_avg:673.31ms
step:491/3000 train_loss:3.6886 train_time:323860ms step_avg:673.31ms
step:492/3000 train_loss:3.7193 train_time:324532ms step_avg:673.30ms
step:493/3000 train_loss:3.8378 train_time:325203ms step_avg:673.30ms
step:494/3000 train_loss:3.6713 train_time:325875ms step_avg:673.30ms
step:495/3000 train_loss:3.8039 train_time:326547ms step_avg:673.29ms
step:496/3000 train_loss:3.7621 train_time:327217ms step_avg:673.29ms
step:497/3000 train_loss:3.6299 train_time:327888ms step_avg:673.28ms
step:498/3000 train_loss:3.8335 train_time:328559ms step_avg:673.28ms
step:499/3000 train_loss:3.9017 train_time:329230ms step_avg:673.27ms
step:500/3000 train_loss:3.9336 train_time:329902ms step_avg:673.27ms
step:500/3000 val_loss:3.8065 train_time:329913ms step_avg:673.29ms
step:501/3000 train_loss:3.8423 train_time:330575ms step_avg:673.27ms
step:502/3000 train_loss:3.9041 train_time:331246ms step_avg:673.26ms
step:503/3000 train_loss:3.8441 train_time:331917ms step_avg:673.26ms
step:504/3000 train_loss:3.8757 train_time:332588ms step_avg:673.25ms
step:505/3000 train_loss:3.8239 train_time:333259ms step_avg:673.25ms
step:506/3000 train_loss:3.9189 train_time:333932ms step_avg:673.25ms
step:507/3000 train_loss:3.7586 train_time:334603ms step_avg:673.24ms
step:508/3000 train_loss:3.8656 train_time:335273ms step_avg:673.24ms
step:509/3000 train_loss:3.9265 train_time:335945ms step_avg:673.24ms
step:510/3000 train_loss:3.8715 train_time:336615ms step_avg:673.23ms
step:511/3000 train_loss:3.6915 train_time:337286ms step_avg:673.23ms
step:512/3000 train_loss:3.8825 train_time:337957ms step_avg:673.22ms
step:513/3000 train_loss:3.8216 train_time:338628ms step_avg:673.22ms
step:514/3000 train_loss:3.7794 train_time:339298ms step_avg:673.21ms
step:515/3000 train_loss:3.8743 train_time:339969ms step_avg:673.21ms
step:516/3000 train_loss:3.8402 train_time:340639ms step_avg:673.20ms
step:517/3000 train_loss:4.1988 train_time:341310ms step_avg:673.20ms
step:518/3000 train_loss:3.7990 train_time:341981ms step_avg:673.19ms
step:519/3000 train_loss:3.8895 train_time:342652ms step_avg:673.19ms
step:520/3000 train_loss:3.7865 train_time:343321ms step_avg:673.18ms
step:521/3000 train_loss:3.8034 train_time:343993ms step_avg:673.18ms
step:522/3000 train_loss:3.7553 train_time:344664ms step_avg:673.17ms
step:523/3000 train_loss:3.7565 train_time:345333ms step_avg:673.16ms
step:524/3000 train_loss:4.3971 train_time:346005ms step_avg:673.16ms
step:525/3000 train_loss:3.8484 train_time:346676ms step_avg:673.16ms
step:526/3000 train_loss:3.7876 train_time:347347ms step_avg:673.15ms
step:527/3000 train_loss:3.8012 train_time:348018ms step_avg:673.15ms
step:528/3000 train_loss:3.7574 train_time:348689ms step_avg:673.14ms
step:529/3000 train_loss:3.7352 train_time:349360ms step_avg:673.14ms
step:530/3000 train_loss:3.9524 train_time:350031ms step_avg:673.14ms
step:531/3000 train_loss:3.7578 train_time:350701ms step_avg:673.13ms
step:532/3000 train_loss:4.0172 train_time:351372ms step_avg:673.13ms
step:533/3000 train_loss:3.8387 train_time:352044ms step_avg:673.12ms
step:534/3000 train_loss:3.7648 train_time:352716ms step_avg:673.12ms
step:535/3000 train_loss:3.7842 train_time:353387ms step_avg:673.12ms
step:536/3000 train_loss:3.7255 train_time:354057ms step_avg:673.11ms
step:537/3000 train_loss:3.8536 train_time:354729ms step_avg:673.11ms
step:538/3000 train_loss:3.8410 train_time:355400ms step_avg:673.11ms
step:539/3000 train_loss:3.7354 train_time:356071ms step_avg:673.10ms
step:540/3000 train_loss:4.2436 train_time:356743ms step_avg:673.10ms
step:541/3000 train_loss:3.7769 train_time:357413ms step_avg:673.09ms
step:542/3000 train_loss:3.8943 train_time:358083ms step_avg:673.09ms
step:543/3000 train_loss:3.7113 train_time:358754ms step_avg:673.08ms
step:544/3000 train_loss:3.6861 train_time:359423ms step_avg:673.08ms
step:545/3000 train_loss:3.7671 train_time:360094ms step_avg:673.07ms
step:546/3000 train_loss:3.7002 train_time:360764ms step_avg:673.07ms
step:547/3000 train_loss:3.7486 train_time:361434ms step_avg:673.06ms
step:548/3000 train_loss:3.7457 train_time:362104ms step_avg:673.05ms
step:549/3000 train_loss:3.7222 train_time:362773ms step_avg:673.05ms
step:550/3000 train_loss:3.8264 train_time:363444ms step_avg:673.05ms
step:551/3000 train_loss:3.7181 train_time:364116ms step_avg:673.04ms
step:552/3000 train_loss:3.7423 train_time:364786ms step_avg:673.04ms
step:553/3000 train_loss:4.0598 train_time:365457ms step_avg:673.03ms
step:554/3000 train_loss:3.8680 train_time:366128ms step_avg:673.03ms
step:555/3000 train_loss:3.8181 train_time:366798ms step_avg:673.02ms
step:556/3000 train_loss:3.7525 train_time:367470ms step_avg:673.02ms
step:557/3000 train_loss:3.8005 train_time:368142ms step_avg:673.02ms
step:558/3000 train_loss:3.4437 train_time:368812ms step_avg:673.01ms
step:559/3000 train_loss:3.7162 train_time:369483ms step_avg:673.01ms
step:560/3000 train_loss:3.7602 train_time:370153ms step_avg:673.01ms
step:561/3000 train_loss:3.8067 train_time:370825ms step_avg:673.00ms
step:562/3000 train_loss:3.7242 train_time:371495ms step_avg:673.00ms
step:563/3000 train_loss:3.6579 train_time:372165ms step_avg:672.99ms
step:564/3000 train_loss:3.8701 train_time:372836ms step_avg:672.99ms
step:565/3000 train_loss:3.6793 train_time:373508ms step_avg:672.99ms
step:566/3000 train_loss:3.7968 train_time:374178ms step_avg:672.98ms
step:567/3000 train_loss:3.7256 train_time:374848ms step_avg:672.98ms
step:568/3000 train_loss:3.7098 train_time:375519ms step_avg:672.97ms
step:569/3000 train_loss:3.7979 train_time:376189ms step_avg:672.97ms
step:570/3000 train_loss:3.7630 train_time:376861ms step_avg:672.97ms
step:571/3000 train_loss:3.7962 train_time:377532ms step_avg:672.96ms
step:572/3000 train_loss:3.6887 train_time:378417ms step_avg:673.34ms
step:573/3000 train_loss:3.6344 train_time:379087ms step_avg:673.33ms
step:574/3000 train_loss:3.6424 train_time:379758ms step_avg:673.33ms
step:575/3000 train_loss:4.1212 train_time:380428ms step_avg:673.32ms
step:576/3000 train_loss:3.6665 train_time:381100ms step_avg:673.32ms
step:577/3000 train_loss:3.7399 train_time:381770ms step_avg:673.31ms
step:578/3000 train_loss:3.8911 train_time:382441ms step_avg:673.31ms
step:579/3000 train_loss:3.6810 train_time:383111ms step_avg:673.31ms
step:580/3000 train_loss:3.7268 train_time:383781ms step_avg:673.30ms
step:581/3000 train_loss:3.8399 train_time:384451ms step_avg:673.30ms
step:582/3000 train_loss:3.8448 train_time:385122ms step_avg:673.29ms
step:583/3000 train_loss:3.7872 train_time:385792ms step_avg:673.28ms
step:584/3000 train_loss:3.9597 train_time:386464ms step_avg:673.28ms
step:585/3000 train_loss:3.7515 train_time:387134ms step_avg:673.28ms
step:586/3000 train_loss:3.7553 train_time:387804ms step_avg:673.27ms
step:587/3000 train_loss:3.7884 train_time:388474ms step_avg:673.27ms
step:588/3000 train_loss:4.1865 train_time:389146ms step_avg:673.26ms
step:589/3000 train_loss:3.6305 train_time:389816ms step_avg:673.26ms
step:590/3000 train_loss:3.9853 train_time:390487ms step_avg:673.25ms
step:591/3000 train_loss:3.7999 train_time:391159ms step_avg:673.25ms
step:592/3000 train_loss:3.6429 train_time:391830ms step_avg:673.25ms
step:593/3000 train_loss:3.7444 train_time:392500ms step_avg:673.24ms
step:594/3000 train_loss:3.8391 train_time:393170ms step_avg:673.24ms
step:595/3000 train_loss:3.7205 train_time:393840ms step_avg:673.23ms
step:596/3000 train_loss:3.7535 train_time:394510ms step_avg:673.22ms
step:597/3000 train_loss:3.7848 train_time:395180ms step_avg:673.22ms
step:598/3000 train_loss:3.7900 train_time:395852ms step_avg:673.22ms
step:599/3000 train_loss:3.7946 train_time:396522ms step_avg:673.21ms
step:600/3000 train_loss:3.9076 train_time:397192ms step_avg:673.21ms
step:601/3000 train_loss:3.7574 train_time:397863ms step_avg:673.20ms
step:602/3000 train_loss:3.7209 train_time:398532ms step_avg:673.20ms
step:603/3000 train_loss:3.8652 train_time:399203ms step_avg:673.19ms
step:604/3000 train_loss:3.6656 train_time:399874ms step_avg:673.19ms
step:605/3000 train_loss:3.7830 train_time:400544ms step_avg:673.18ms
step:606/3000 train_loss:4.0838 train_time:401215ms step_avg:673.18ms
step:607/3000 train_loss:3.8141 train_time:401885ms step_avg:673.17ms
step:608/3000 train_loss:3.8156 train_time:402556ms step_avg:673.17ms
step:609/3000 train_loss:3.7682 train_time:403227ms step_avg:673.17ms
step:610/3000 train_loss:3.6858 train_time:403897ms step_avg:673.16ms
step:611/3000 train_loss:3.7612 train_time:404567ms step_avg:673.16ms
step:612/3000 train_loss:3.7115 train_time:405238ms step_avg:673.15ms
step:613/3000 train_loss:3.8417 train_time:405909ms step_avg:673.15ms
step:614/3000 train_loss:3.6721 train_time:406579ms step_avg:673.14ms
step:615/3000 train_loss:3.7200 train_time:407250ms step_avg:673.14ms
step:616/3000 train_loss:3.6884 train_time:407921ms step_avg:673.14ms
step:617/3000 train_loss:3.7564 train_time:408593ms step_avg:673.14ms
step:618/3000 train_loss:3.8159 train_time:409263ms step_avg:673.13ms
step:619/3000 train_loss:3.5833 train_time:409934ms step_avg:673.13ms
step:620/3000 train_loss:3.8607 train_time:410605ms step_avg:673.12ms
step:621/3000 train_loss:3.7733 train_time:411275ms step_avg:673.12ms
step:622/3000 train_loss:3.7622 train_time:411945ms step_avg:673.11ms
step:623/3000 train_loss:3.5932 train_time:412615ms step_avg:673.11ms
step:624/3000 train_loss:3.7043 train_time:413287ms step_avg:673.11ms
step:625/3000 train_loss:3.8188 train_time:413958ms step_avg:673.10ms
step:625/3000 val_loss:3.7349 train_time:413969ms step_avg:673.12ms
step:626/3000 train_loss:3.7609 train_time:414630ms step_avg:673.10ms
step:627/3000 train_loss:3.7215 train_time:415302ms step_avg:673.10ms
step:628/3000 train_loss:3.6411 train_time:415972ms step_avg:673.09ms
step:629/3000 train_loss:3.7757 train_time:416642ms step_avg:673.09ms
step:630/3000 train_loss:3.6146 train_time:417313ms step_avg:673.09ms
step:631/3000 train_loss:3.8047 train_time:417983ms step_avg:673.08ms
step:632/3000 train_loss:3.6005 train_time:418655ms step_avg:673.08ms
step:633/3000 train_loss:3.7462 train_time:419326ms step_avg:673.08ms
step:634/3000 train_loss:3.9018 train_time:419997ms step_avg:673.07ms
step:635/3000 train_loss:3.7266 train_time:420667ms step_avg:673.07ms
step:636/3000 train_loss:3.6422 train_time:421338ms step_avg:673.06ms
step:637/3000 train_loss:3.7494 train_time:422007ms step_avg:673.06ms
step:638/3000 train_loss:3.6585 train_time:422680ms step_avg:673.06ms
step:639/3000 train_loss:3.7151 train_time:423350ms step_avg:673.05ms
step:640/3000 train_loss:3.7660 train_time:424021ms step_avg:673.05ms
step:641/3000 train_loss:3.6649 train_time:424691ms step_avg:673.04ms
step:642/3000 train_loss:3.6522 train_time:425362ms step_avg:673.04ms
step:643/3000 train_loss:3.8379 train_time:426033ms step_avg:673.04ms
step:644/3000 train_loss:3.5339 train_time:426703ms step_avg:673.03ms
step:645/3000 train_loss:3.7258 train_time:427375ms step_avg:673.03ms
step:646/3000 train_loss:3.6868 train_time:428046ms step_avg:673.03ms
step:647/3000 train_loss:3.8366 train_time:428717ms step_avg:673.02ms
step:648/3000 train_loss:3.7475 train_time:429388ms step_avg:673.02ms
step:649/3000 train_loss:3.9250 train_time:430058ms step_avg:673.02ms
step:650/3000 train_loss:3.7083 train_time:430729ms step_avg:673.01ms
step:651/3000 train_loss:3.5638 train_time:431400ms step_avg:673.01ms
step:652/3000 train_loss:3.6510 train_time:432071ms step_avg:673.01ms
step:653/3000 train_loss:3.7416 train_time:432742ms step_avg:673.01ms
step:654/3000 train_loss:3.7144 train_time:433413ms step_avg:673.00ms
step:655/3000 train_loss:3.8638 train_time:434084ms step_avg:673.00ms
step:656/3000 train_loss:3.8609 train_time:434755ms step_avg:672.99ms
step:657/3000 train_loss:3.7904 train_time:435425ms step_avg:672.99ms
step:658/3000 train_loss:3.7035 train_time:436096ms step_avg:672.99ms
step:659/3000 train_loss:3.5566 train_time:436769ms step_avg:672.99ms
step:660/3000 train_loss:3.7430 train_time:437439ms step_avg:672.98ms
step:661/3000 train_loss:3.8236 train_time:438110ms step_avg:672.98ms
step:662/3000 train_loss:3.8045 train_time:438780ms step_avg:672.98ms
step:663/3000 train_loss:3.6124 train_time:439452ms step_avg:672.97ms
step:664/3000 train_loss:3.7657 train_time:440124ms step_avg:672.97ms
step:665/3000 train_loss:3.7798 train_time:440795ms step_avg:672.97ms
step:666/3000 train_loss:3.6572 train_time:441466ms step_avg:672.97ms
step:667/3000 train_loss:3.7608 train_time:442138ms step_avg:672.96ms
step:668/3000 train_loss:3.8595 train_time:442808ms step_avg:672.96ms
step:669/3000 train_loss:3.8382 train_time:443480ms step_avg:672.96ms
step:670/3000 train_loss:3.6496 train_time:444150ms step_avg:672.95ms
step:671/3000 train_loss:3.8274 train_time:444821ms step_avg:672.95ms
step:672/3000 train_loss:3.7476 train_time:445492ms step_avg:672.95ms
step:673/3000 train_loss:3.5540 train_time:446163ms step_avg:672.95ms
step:674/3000 train_loss:4.0161 train_time:446835ms step_avg:672.94ms
step:675/3000 train_loss:3.7756 train_time:447506ms step_avg:672.94ms
step:676/3000 train_loss:3.7338 train_time:448178ms step_avg:672.94ms
step:677/3000 train_loss:3.7855 train_time:448849ms step_avg:672.94ms
step:678/3000 train_loss:3.6915 train_time:449520ms step_avg:672.93ms
step:679/3000 train_loss:3.5629 train_time:450190ms step_avg:672.93ms
step:680/3000 train_loss:3.6718 train_time:450862ms step_avg:672.93ms
step:681/3000 train_loss:3.7759 train_time:451534ms step_avg:672.93ms
step:682/3000 train_loss:3.8836 train_time:452204ms step_avg:672.92ms
step:683/3000 train_loss:3.6428 train_time:452875ms step_avg:672.92ms
step:684/3000 train_loss:3.7441 train_time:453547ms step_avg:672.92ms
step:685/3000 train_loss:3.7793 train_time:454217ms step_avg:672.91ms
step:686/3000 train_loss:3.6995 train_time:454888ms step_avg:672.91ms
step:687/3000 train_loss:3.7789 train_time:455558ms step_avg:672.91ms
step:688/3000 train_loss:3.6886 train_time:456229ms step_avg:672.90ms
step:689/3000 train_loss:3.6485 train_time:456899ms step_avg:672.90ms
step:690/3000 train_loss:3.7275 train_time:457571ms step_avg:672.90ms
step:691/3000 train_loss:3.5823 train_time:458241ms step_avg:672.89ms
step:692/3000 train_loss:3.7788 train_time:458912ms step_avg:672.89ms
step:693/3000 train_loss:3.7786 train_time:459584ms step_avg:672.89ms
step:694/3000 train_loss:3.5538 train_time:460255ms step_avg:672.89ms
step:695/3000 train_loss:3.7775 train_time:460925ms step_avg:672.88ms
step:696/3000 train_loss:3.7263 train_time:461595ms step_avg:672.88ms
step:697/3000 train_loss:3.4811 train_time:462267ms step_avg:672.88ms
step:698/3000 train_loss:3.8168 train_time:462938ms step_avg:672.88ms
step:699/3000 train_loss:3.7000 train_time:463610ms step_avg:672.87ms
step:700/3000 train_loss:3.6888 train_time:464281ms step_avg:672.87ms
step:701/3000 train_loss:3.6568 train_time:464953ms step_avg:672.87ms
step:702/3000 train_loss:3.6933 train_time:465624ms step_avg:672.87ms
step:703/3000 train_loss:3.2268 train_time:466296ms step_avg:672.87ms
step:704/3000 train_loss:3.7028 train_time:466966ms step_avg:672.86ms
step:705/3000 train_loss:3.5406 train_time:467638ms step_avg:672.86ms
step:706/3000 train_loss:3.8217 train_time:468309ms step_avg:672.86ms
step:707/3000 train_loss:3.6705 train_time:468980ms step_avg:672.86ms
step:708/3000 train_loss:3.7059 train_time:469652ms step_avg:672.85ms
step:709/3000 train_loss:3.7057 train_time:470323ms step_avg:672.85ms
step:710/3000 train_loss:3.7522 train_time:470994ms step_avg:672.85ms
step:711/3000 train_loss:3.7791 train_time:471665ms step_avg:672.85ms
step:712/3000 train_loss:3.6398 train_time:472337ms step_avg:672.84ms
step:713/3000 train_loss:3.6652 train_time:473008ms step_avg:672.84ms
step:714/3000 train_loss:3.6478 train_time:473679ms step_avg:672.84ms
step:715/3000 train_loss:3.6765 train_time:474351ms step_avg:672.84ms
step:716/3000 train_loss:3.5373 train_time:475020ms step_avg:672.83ms
step:717/3000 train_loss:3.9632 train_time:475693ms step_avg:672.83ms
step:718/3000 train_loss:3.8311 train_time:476364ms step_avg:672.83ms
step:719/3000 train_loss:3.8175 train_time:477035ms step_avg:672.83ms
step:720/3000 train_loss:3.7839 train_time:477704ms step_avg:672.82ms
step:721/3000 train_loss:3.7356 train_time:478376ms step_avg:672.82ms
step:722/3000 train_loss:3.6046 train_time:479047ms step_avg:672.82ms
step:723/3000 train_loss:3.7682 train_time:479719ms step_avg:672.82ms
step:724/3000 train_loss:3.6843 train_time:480390ms step_avg:672.82ms
step:725/3000 train_loss:3.6970 train_time:481061ms step_avg:672.81ms
step:726/3000 train_loss:3.6688 train_time:481731ms step_avg:672.81ms
step:727/3000 train_loss:3.5818 train_time:482402ms step_avg:672.81ms
step:728/3000 train_loss:3.7639 train_time:483073ms step_avg:672.80ms
step:729/3000 train_loss:3.6956 train_time:483743ms step_avg:672.80ms
step:730/3000 train_loss:3.5033 train_time:484414ms step_avg:672.80ms
step:731/3000 train_loss:3.6735 train_time:485085ms step_avg:672.80ms
step:732/3000 train_loss:3.7763 train_time:485756ms step_avg:672.79ms
step:733/3000 train_loss:3.7654 train_time:486427ms step_avg:672.79ms
step:734/3000 train_loss:3.7981 train_time:487098ms step_avg:672.79ms
step:735/3000 train_loss:3.6221 train_time:487769ms step_avg:672.79ms
step:736/3000 train_loss:3.6333 train_time:488440ms step_avg:672.78ms
step:737/3000 train_loss:3.6814 train_time:489111ms step_avg:672.78ms
step:738/3000 train_loss:3.8179 train_time:489782ms step_avg:672.78ms
step:739/3000 train_loss:3.7277 train_time:490453ms step_avg:672.78ms
step:740/3000 train_loss:3.7013 train_time:491124ms step_avg:672.77ms
step:741/3000 train_loss:3.7594 train_time:491796ms step_avg:672.77ms
step:742/3000 train_loss:3.6915 train_time:492465ms step_avg:672.77ms
step:743/3000 train_loss:3.6960 train_time:493138ms step_avg:672.77ms
step:744/3000 train_loss:3.6039 train_time:493808ms step_avg:672.76ms
step:745/3000 train_loss:3.8594 train_time:494479ms step_avg:672.76ms
step:746/3000 train_loss:3.6419 train_time:495149ms step_avg:672.76ms
step:747/3000 train_loss:3.6668 train_time:495820ms step_avg:672.75ms
step:748/3000 train_loss:3.7605 train_time:496489ms step_avg:672.75ms
step:749/3000 train_loss:3.6940 train_time:497160ms step_avg:672.75ms
step:750/3000 train_loss:3.7519 train_time:497830ms step_avg:672.74ms
step:750/3000 val_loss:3.6774 train_time:497842ms step_avg:672.76ms
step:751/3000 train_loss:3.6614 train_time:498505ms step_avg:672.75ms
step:752/3000 train_loss:3.7883 train_time:499176ms step_avg:672.74ms
step:753/3000 train_loss:3.6428 train_time:499848ms step_avg:672.74ms
step:754/3000 train_loss:3.6753 train_time:500520ms step_avg:672.74ms
step:755/3000 train_loss:3.6412 train_time:501191ms step_avg:672.74ms
step:756/3000 train_loss:3.6987 train_time:501863ms step_avg:672.74ms
step:757/3000 train_loss:3.6101 train_time:502533ms step_avg:672.73ms
step:758/3000 train_loss:3.8510 train_time:503204ms step_avg:672.73ms
step:759/3000 train_loss:3.9485 train_time:503876ms step_avg:672.73ms
step:760/3000 train_loss:3.6946 train_time:504548ms step_avg:672.73ms
step:761/3000 train_loss:3.6162 train_time:505220ms step_avg:672.73ms
step:762/3000 train_loss:3.7668 train_time:505893ms step_avg:672.73ms
step:763/3000 train_loss:3.5126 train_time:506565ms step_avg:672.73ms
step:764/3000 train_loss:3.6585 train_time:507237ms step_avg:672.73ms
step:765/3000 train_loss:3.7741 train_time:507910ms step_avg:672.73ms
step:766/3000 train_loss:3.4343 train_time:508581ms step_avg:672.73ms
step:767/3000 train_loss:3.8523 train_time:509253ms step_avg:672.73ms
step:768/3000 train_loss:3.6848 train_time:509925ms step_avg:672.72ms
step:769/3000 train_loss:3.6708 train_time:510597ms step_avg:672.72ms
step:770/3000 train_loss:3.6891 train_time:511268ms step_avg:672.72ms
step:771/3000 train_loss:3.7073 train_time:511940ms step_avg:672.72ms
step:772/3000 train_loss:3.7602 train_time:512611ms step_avg:672.72ms
step:773/3000 train_loss:3.9875 train_time:513284ms step_avg:672.72ms
step:774/3000 train_loss:3.5714 train_time:513955ms step_avg:672.72ms
step:775/3000 train_loss:3.7591 train_time:514626ms step_avg:672.71ms
step:776/3000 train_loss:3.7404 train_time:515298ms step_avg:672.71ms
step:777/3000 train_loss:3.7207 train_time:515970ms step_avg:672.71ms
step:778/3000 train_loss:3.5099 train_time:516642ms step_avg:672.71ms
step:779/3000 train_loss:3.5112 train_time:517314ms step_avg:672.71ms
step:780/3000 train_loss:3.5843 train_time:517985ms step_avg:672.71ms
step:781/3000 train_loss:3.6749 train_time:518657ms step_avg:672.71ms
step:782/3000 train_loss:3.7097 train_time:519329ms step_avg:672.71ms
step:783/3000 train_loss:3.7700 train_time:520001ms step_avg:672.70ms
step:784/3000 train_loss:3.6833 train_time:520673ms step_avg:672.70ms
step:785/3000 train_loss:3.6740 train_time:521344ms step_avg:672.70ms
step:786/3000 train_loss:3.6895 train_time:522017ms step_avg:672.70ms
step:787/3000 train_loss:3.6559 train_time:522689ms step_avg:672.70ms
step:788/3000 train_loss:3.5595 train_time:523361ms step_avg:672.70ms
step:789/3000 train_loss:3.8335 train_time:524032ms step_avg:672.70ms
step:790/3000 train_loss:3.6130 train_time:524705ms step_avg:672.70ms
step:791/3000 train_loss:3.6637 train_time:525377ms step_avg:672.70ms
step:792/3000 train_loss:3.7395 train_time:526050ms step_avg:672.70ms
step:793/3000 train_loss:3.8704 train_time:526720ms step_avg:672.69ms
step:794/3000 train_loss:3.8775 train_time:527393ms step_avg:672.70ms
step:795/3000 train_loss:3.5931 train_time:528066ms step_avg:672.70ms
step:796/3000 train_loss:3.7044 train_time:528737ms step_avg:672.69ms
step:797/3000 train_loss:3.7696 train_time:529409ms step_avg:672.69ms
step:798/3000 train_loss:3.8619 train_time:530080ms step_avg:672.69ms
step:799/3000 train_loss:3.6260 train_time:530753ms step_avg:672.69ms
step:800/3000 train_loss:3.7699 train_time:531425ms step_avg:672.69ms
step:801/3000 train_loss:3.6603 train_time:532097ms step_avg:672.69ms
step:802/3000 train_loss:3.6479 train_time:532769ms step_avg:672.69ms
step:803/3000 train_loss:3.7345 train_time:533441ms step_avg:672.69ms
step:804/3000 train_loss:3.6045 train_time:534114ms step_avg:672.69ms
step:805/3000 train_loss:3.6397 train_time:534786ms step_avg:672.69ms
step:806/3000 train_loss:3.7473 train_time:535457ms step_avg:672.69ms
step:807/3000 train_loss:3.6417 train_time:536130ms step_avg:672.69ms
step:808/3000 train_loss:3.6536 train_time:536803ms step_avg:672.68ms
step:809/3000 train_loss:3.7581 train_time:537475ms step_avg:672.68ms
step:810/3000 train_loss:3.6682 train_time:538148ms step_avg:672.68ms
step:811/3000 train_loss:3.5979 train_time:538819ms step_avg:672.68ms
step:812/3000 train_loss:3.6738 train_time:539491ms step_avg:672.68ms
step:813/3000 train_loss:3.7027 train_time:540163ms step_avg:672.68ms
step:814/3000 train_loss:3.7092 train_time:540835ms step_avg:672.68ms
step:815/3000 train_loss:3.7369 train_time:541507ms step_avg:672.68ms
step:816/3000 train_loss:3.6829 train_time:542180ms step_avg:672.68ms
step:817/3000 train_loss:3.6696 train_time:542852ms step_avg:672.68ms
step:818/3000 train_loss:3.7823 train_time:543525ms step_avg:672.68ms
step:819/3000 train_loss:3.8742 train_time:544197ms step_avg:672.68ms
step:820/3000 train_loss:3.6375 train_time:544870ms step_avg:672.68ms
step:821/3000 train_loss:3.8278 train_time:545541ms step_avg:672.68ms
step:822/3000 train_loss:3.6077 train_time:546213ms step_avg:672.68ms
step:823/3000 train_loss:3.6562 train_time:546885ms step_avg:672.68ms
step:824/3000 train_loss:3.7861 train_time:547559ms step_avg:672.68ms
step:825/3000 train_loss:3.6856 train_time:548230ms step_avg:672.67ms
step:826/3000 train_loss:3.6216 train_time:548903ms step_avg:672.68ms
step:827/3000 train_loss:3.7127 train_time:549574ms step_avg:672.67ms
step:828/3000 train_loss:3.6127 train_time:550247ms step_avg:672.67ms
step:829/3000 train_loss:3.8480 train_time:550919ms step_avg:672.67ms
step:830/3000 train_loss:3.7274 train_time:551590ms step_avg:672.67ms
step:831/3000 train_loss:3.7753 train_time:552263ms step_avg:672.67ms
step:832/3000 train_loss:3.6434 train_time:552935ms step_avg:672.67ms
step:833/3000 train_loss:3.6968 train_time:553607ms step_avg:672.67ms
step:834/3000 train_loss:3.6260 train_time:554279ms step_avg:672.67ms
step:835/3000 train_loss:3.7583 train_time:554951ms step_avg:672.67ms
step:836/3000 train_loss:3.5905 train_time:555622ms step_avg:672.67ms
step:837/3000 train_loss:3.5653 train_time:556295ms step_avg:672.67ms
step:838/3000 train_loss:3.8253 train_time:556967ms step_avg:672.67ms
step:839/3000 train_loss:3.5206 train_time:557638ms step_avg:672.66ms
step:840/3000 train_loss:3.7007 train_time:558309ms step_avg:672.66ms
step:841/3000 train_loss:3.5377 train_time:558981ms step_avg:672.66ms
step:842/3000 train_loss:3.5793 train_time:559652ms step_avg:672.66ms
step:843/3000 train_loss:3.6674 train_time:560325ms step_avg:672.66ms
step:844/3000 train_loss:3.6933 train_time:560996ms step_avg:672.66ms
step:845/3000 train_loss:3.6847 train_time:561669ms step_avg:672.66ms
step:846/3000 train_loss:3.5359 train_time:562342ms step_avg:672.66ms
step:847/3000 train_loss:3.7803 train_time:563014ms step_avg:672.66ms
step:848/3000 train_loss:3.6444 train_time:563686ms step_avg:672.66ms
step:849/3000 train_loss:3.6039 train_time:564358ms step_avg:672.66ms
step:850/3000 train_loss:3.7417 train_time:565031ms step_avg:672.66ms
step:851/3000 train_loss:3.5981 train_time:565703ms step_avg:672.65ms
step:852/3000 train_loss:3.5588 train_time:566375ms step_avg:672.65ms
step:853/3000 train_loss:3.8528 train_time:567047ms step_avg:672.65ms
step:854/3000 train_loss:3.5540 train_time:567719ms step_avg:672.65ms
step:855/3000 train_loss:3.6737 train_time:568391ms step_avg:672.65ms
step:856/3000 train_loss:3.7484 train_time:569063ms step_avg:672.65ms
step:857/3000 train_loss:3.6258 train_time:569734ms step_avg:672.65ms
step:858/3000 train_loss:3.6575 train_time:570407ms step_avg:672.65ms
step:859/3000 train_loss:3.7104 train_time:571078ms step_avg:672.65ms
step:860/3000 train_loss:3.5878 train_time:571750ms step_avg:672.65ms
step:861/3000 train_loss:3.6786 train_time:572423ms step_avg:672.65ms
step:862/3000 train_loss:3.7042 train_time:573094ms step_avg:672.65ms
step:863/3000 train_loss:3.7350 train_time:573767ms step_avg:672.65ms
step:864/3000 train_loss:3.7129 train_time:574439ms step_avg:672.64ms
step:865/3000 train_loss:3.6813 train_time:575110ms step_avg:672.64ms
step:866/3000 train_loss:3.5006 train_time:575782ms step_avg:672.64ms
step:867/3000 train_loss:3.7009 train_time:576454ms step_avg:672.64ms
step:868/3000 train_loss:3.9755 train_time:577125ms step_avg:672.64ms
step:869/3000 train_loss:3.5601 train_time:577798ms step_avg:672.64ms
step:870/3000 train_loss:3.7460 train_time:578471ms step_avg:672.64ms
step:871/3000 train_loss:3.7221 train_time:579142ms step_avg:672.64ms
step:872/3000 train_loss:3.5544 train_time:579814ms step_avg:672.64ms
step:873/3000 train_loss:3.5466 train_time:580486ms step_avg:672.64ms
step:874/3000 train_loss:3.7690 train_time:581158ms step_avg:672.64ms
step:875/3000 train_loss:3.5620 train_time:581830ms step_avg:672.64ms
step:875/3000 val_loss:3.6335 train_time:581842ms step_avg:672.65ms
step:876/3000 train_loss:3.3020 train_time:582506ms step_avg:672.64ms
step:877/3000 train_loss:3.7481 train_time:583178ms step_avg:672.64ms
step:878/3000 train_loss:3.5568 train_time:583850ms step_avg:672.64ms
step:879/3000 train_loss:3.7333 train_time:584522ms step_avg:672.64ms
step:880/3000 train_loss:3.5846 train_time:585195ms step_avg:672.64ms
step:881/3000 train_loss:3.7665 train_time:585867ms step_avg:672.64ms
step:882/3000 train_loss:3.4322 train_time:586540ms step_avg:672.64ms
step:883/3000 train_loss:3.5935 train_time:587212ms step_avg:672.64ms
step:884/3000 train_loss:3.8039 train_time:587885ms step_avg:672.64ms
step:885/3000 train_loss:3.9502 train_time:588558ms step_avg:672.64ms
step:886/3000 train_loss:3.6727 train_time:589230ms step_avg:672.64ms
step:887/3000 train_loss:3.5918 train_time:589902ms step_avg:672.64ms
step:888/3000 train_loss:3.6806 train_time:590574ms step_avg:672.64ms
step:889/3000 train_loss:4.1845 train_time:591246ms step_avg:672.63ms
step:890/3000 train_loss:3.9554 train_time:591918ms step_avg:672.63ms
step:891/3000 train_loss:3.6266 train_time:592589ms step_avg:672.63ms
step:892/3000 train_loss:3.6466 train_time:593261ms step_avg:672.63ms
step:893/3000 train_loss:3.4764 train_time:593934ms step_avg:672.63ms
step:894/3000 train_loss:3.8136 train_time:594606ms step_avg:672.63ms
step:895/3000 train_loss:3.5305 train_time:595279ms step_avg:672.63ms
step:896/3000 train_loss:3.7925 train_time:595950ms step_avg:672.63ms
step:897/3000 train_loss:3.8046 train_time:596622ms step_avg:672.63ms
step:898/3000 train_loss:3.5980 train_time:597295ms step_avg:672.63ms
step:899/3000 train_loss:3.6487 train_time:597968ms step_avg:672.63ms
step:900/3000 train_loss:3.6988 train_time:598639ms step_avg:672.63ms
step:901/3000 train_loss:3.5828 train_time:599312ms step_avg:672.63ms
step:902/3000 train_loss:3.5221 train_time:599983ms step_avg:672.63ms
step:903/3000 train_loss:3.7368 train_time:600655ms step_avg:672.63ms
step:904/3000 train_loss:3.7377 train_time:601327ms step_avg:672.63ms
step:905/3000 train_loss:3.6430 train_time:602001ms step_avg:672.63ms
step:906/3000 train_loss:3.6177 train_time:602674ms step_avg:672.63ms
step:907/3000 train_loss:3.5950 train_time:603346ms step_avg:672.63ms
step:908/3000 train_loss:3.8428 train_time:604019ms step_avg:672.63ms
step:909/3000 train_loss:3.6194 train_time:604691ms step_avg:672.63ms
step:910/3000 train_loss:3.6586 train_time:605364ms step_avg:672.63ms
step:911/3000 train_loss:3.5672 train_time:606036ms step_avg:672.63ms
step:912/3000 train_loss:3.6485 train_time:606709ms step_avg:672.63ms
step:913/3000 train_loss:3.7310 train_time:607380ms step_avg:672.62ms
step:914/3000 train_loss:3.7167 train_time:608053ms step_avg:672.63ms
step:915/3000 train_loss:3.5929 train_time:608725ms step_avg:672.62ms
step:916/3000 train_loss:3.8371 train_time:609398ms step_avg:672.63ms
step:917/3000 train_loss:3.6458 train_time:610070ms step_avg:672.62ms
step:918/3000 train_loss:3.7365 train_time:610744ms step_avg:672.63ms
step:919/3000 train_loss:3.7044 train_time:611416ms step_avg:672.62ms
step:920/3000 train_loss:4.9485 train_time:612088ms step_avg:672.62ms
step:921/3000 train_loss:3.6249 train_time:612759ms step_avg:672.62ms
step:922/3000 train_loss:3.6899 train_time:613432ms step_avg:672.62ms
step:923/3000 train_loss:3.6458 train_time:614104ms step_avg:672.62ms
step:924/3000 train_loss:3.6952 train_time:614776ms step_avg:672.62ms
step:925/3000 train_loss:3.7126 train_time:615448ms step_avg:672.62ms
step:926/3000 train_loss:3.7951 train_time:616120ms step_avg:672.62ms
step:927/3000 train_loss:3.7591 train_time:616792ms step_avg:672.62ms
step:928/3000 train_loss:3.6653 train_time:617466ms step_avg:672.62ms
step:929/3000 train_loss:3.6500 train_time:618137ms step_avg:672.62ms
step:930/3000 train_loss:3.9005 train_time:618811ms step_avg:672.62ms
step:931/3000 train_loss:3.7271 train_time:619483ms step_avg:672.62ms
step:932/3000 train_loss:3.5140 train_time:620155ms step_avg:672.62ms
step:933/3000 train_loss:3.6028 train_time:620828ms step_avg:672.62ms
step:934/3000 train_loss:3.7785 train_time:621500ms step_avg:672.62ms
step:935/3000 train_loss:3.4962 train_time:622173ms step_avg:672.62ms
step:936/3000 train_loss:3.6796 train_time:622845ms step_avg:672.62ms
step:937/3000 train_loss:3.5596 train_time:623517ms step_avg:672.62ms
step:938/3000 train_loss:3.6224 train_time:624189ms step_avg:672.62ms
step:939/3000 train_loss:3.7215 train_time:624861ms step_avg:672.62ms
step:940/3000 train_loss:3.6471 train_time:625534ms step_avg:672.62ms
step:941/3000 train_loss:3.7978 train_time:626205ms step_avg:672.62ms
step:942/3000 train_loss:3.5880 train_time:626878ms step_avg:672.62ms
step:943/3000 train_loss:3.6556 train_time:627549ms step_avg:672.61ms
step:944/3000 train_loss:3.4590 train_time:628223ms step_avg:672.62ms
step:945/3000 train_loss:3.8159 train_time:628896ms step_avg:672.62ms
step:946/3000 train_loss:3.5125 train_time:629568ms step_avg:672.62ms
step:947/3000 train_loss:3.5304 train_time:630240ms step_avg:672.61ms
step:948/3000 train_loss:5.1673 train_time:630911ms step_avg:672.61ms
step:949/3000 train_loss:3.7164 train_time:631584ms step_avg:672.61ms
step:950/3000 train_loss:3.6014 train_time:632257ms step_avg:672.61ms
step:951/3000 train_loss:3.4987 train_time:632929ms step_avg:672.61ms
step:952/3000 train_loss:3.5634 train_time:633601ms step_avg:672.61ms
step:953/3000 train_loss:3.7737 train_time:634498ms step_avg:672.85ms
step:954/3000 train_loss:3.6868 train_time:635169ms step_avg:672.85ms
step:955/3000 train_loss:3.3775 train_time:635842ms step_avg:672.85ms
step:956/3000 train_loss:3.5319 train_time:636515ms step_avg:672.85ms
step:957/3000 train_loss:3.5378 train_time:637187ms step_avg:672.85ms
step:958/3000 train_loss:3.5797 train_time:637859ms step_avg:672.85ms
step:959/3000 train_loss:3.5446 train_time:638531ms step_avg:672.85ms
step:960/3000 train_loss:3.6805 train_time:639203ms step_avg:672.85ms
step:961/3000 train_loss:3.7875 train_time:639875ms step_avg:672.84ms
step:962/3000 train_loss:3.7253 train_time:640547ms step_avg:672.84ms
step:963/3000 train_loss:3.5533 train_time:641220ms step_avg:672.84ms
step:964/3000 train_loss:3.5020 train_time:641892ms step_avg:672.84ms
step:965/3000 train_loss:3.6090 train_time:642564ms step_avg:672.84ms
step:966/3000 train_loss:3.6646 train_time:643236ms step_avg:672.84ms
step:967/3000 train_loss:3.7324 train_time:643909ms step_avg:672.84ms
step:968/3000 train_loss:3.6002 train_time:644580ms step_avg:672.84ms
step:969/3000 train_loss:3.6652 train_time:645254ms step_avg:672.84ms
step:970/3000 train_loss:3.6235 train_time:645924ms step_avg:672.84ms
step:971/3000 train_loss:3.6152 train_time:646598ms step_avg:672.84ms
step:972/3000 train_loss:3.5594 train_time:647269ms step_avg:672.84ms
step:973/3000 train_loss:3.4421 train_time:647942ms step_avg:672.84ms
step:974/3000 train_loss:3.6464 train_time:648616ms step_avg:672.84ms
step:975/3000 train_loss:3.4604 train_time:649288ms step_avg:672.84ms
step:976/3000 train_loss:3.6029 train_time:649960ms step_avg:672.84ms
step:977/3000 train_loss:3.7056 train_time:650633ms step_avg:672.84ms
step:978/3000 train_loss:3.5433 train_time:651305ms step_avg:672.84ms
step:979/3000 train_loss:3.6016 train_time:651978ms step_avg:672.84ms
step:980/3000 train_loss:3.6561 train_time:652650ms step_avg:672.83ms
step:981/3000 train_loss:3.4701 train_time:653322ms step_avg:672.83ms
step:982/3000 train_loss:3.6272 train_time:653995ms step_avg:672.83ms
step:983/3000 train_loss:3.5902 train_time:654667ms step_avg:672.83ms
step:984/3000 train_loss:3.5906 train_time:655338ms step_avg:672.83ms
step:985/3000 train_loss:3.7138 train_time:656011ms step_avg:672.83ms
step:986/3000 train_loss:3.5482 train_time:656683ms step_avg:672.83ms
step:987/3000 train_loss:3.5953 train_time:657356ms step_avg:672.83ms
step:988/3000 train_loss:3.6011 train_time:658029ms step_avg:672.83ms
step:989/3000 train_loss:3.6607 train_time:658701ms step_avg:672.83ms
step:990/3000 train_loss:3.4934 train_time:659373ms step_avg:672.83ms
step:991/3000 train_loss:3.5410 train_time:660046ms step_avg:672.83ms
step:992/3000 train_loss:3.6929 train_time:660720ms step_avg:672.83ms
step:993/3000 train_loss:3.5859 train_time:661392ms step_avg:672.83ms
step:994/3000 train_loss:3.6227 train_time:662064ms step_avg:672.83ms
step:995/3000 train_loss:3.5448 train_time:662737ms step_avg:672.83ms
step:996/3000 train_loss:3.5120 train_time:663409ms step_avg:672.83ms
step:997/3000 train_loss:3.5741 train_time:664082ms step_avg:672.83ms
step:998/3000 train_loss:3.5735 train_time:664755ms step_avg:672.83ms
step:999/3000 train_loss:3.7094 train_time:665426ms step_avg:672.83ms
step:1000/3000 train_loss:3.5324 train_time:666099ms step_avg:672.83ms
step:1000/3000 val_loss:3.5941 train_time:666110ms step_avg:672.84ms
step:1001/3000 train_loss:3.7833 train_time:666774ms step_avg:672.83ms
step:1002/3000 train_loss:3.7205 train_time:667447ms step_avg:672.83ms
step:1003/3000 train_loss:3.6558 train_time:668119ms step_avg:672.83ms
step:1004/3000 train_loss:3.6136 train_time:668791ms step_avg:672.83ms
step:1005/3000 train_loss:3.6176 train_time:669462ms step_avg:672.83ms
step:1006/3000 train_loss:3.6162 train_time:670136ms step_avg:672.83ms
step:1007/3000 train_loss:3.5940 train_time:670809ms step_avg:672.83ms
step:1008/3000 train_loss:3.5768 train_time:671481ms step_avg:672.83ms
step:1009/3000 train_loss:3.5656 train_time:672153ms step_avg:672.83ms
step:1010/3000 train_loss:3.5888 train_time:672826ms step_avg:672.83ms
step:1011/3000 train_loss:3.8251 train_time:673499ms step_avg:672.83ms
step:1012/3000 train_loss:3.7371 train_time:674171ms step_avg:672.83ms
step:1013/3000 train_loss:3.5283 train_time:674843ms step_avg:672.82ms
step:1014/3000 train_loss:3.6133 train_time:675516ms step_avg:672.82ms
step:1015/3000 train_loss:3.5537 train_time:676188ms step_avg:672.82ms
step:1016/3000 train_loss:3.6517 train_time:676859ms step_avg:672.82ms
step:1017/3000 train_loss:3.7071 train_time:677532ms step_avg:672.82ms
step:1018/3000 train_loss:3.5094 train_time:678204ms step_avg:672.82ms
step:1019/3000 train_loss:3.5854 train_time:678876ms step_avg:672.82ms
step:1020/3000 train_loss:3.5377 train_time:679548ms step_avg:672.82ms
step:1021/3000 train_loss:3.7726 train_time:680221ms step_avg:672.82ms
step:1022/3000 train_loss:3.5012 train_time:680893ms step_avg:672.82ms
step:1023/3000 train_loss:3.8529 train_time:681565ms step_avg:672.82ms
step:1024/3000 train_loss:3.7081 train_time:682238ms step_avg:672.82ms
step:1025/3000 train_loss:3.4844 train_time:682910ms step_avg:672.82ms
step:1026/3000 train_loss:3.6416 train_time:683583ms step_avg:672.82ms
step:1027/3000 train_loss:3.5494 train_time:684255ms step_avg:672.82ms
step:1028/3000 train_loss:3.6305 train_time:684928ms step_avg:672.82ms
step:1029/3000 train_loss:3.5931 train_time:685601ms step_avg:672.82ms
step:1030/3000 train_loss:3.4837 train_time:686273ms step_avg:672.82ms
step:1031/3000 train_loss:3.6175 train_time:686946ms step_avg:672.82ms
step:1032/3000 train_loss:4.0541 train_time:687619ms step_avg:672.82ms
step:1033/3000 train_loss:3.5224 train_time:688291ms step_avg:672.82ms
step:1034/3000 train_loss:3.5586 train_time:688963ms step_avg:672.82ms
step:1035/3000 train_loss:3.5957 train_time:689635ms step_avg:672.81ms
step:1036/3000 train_loss:3.6500 train_time:690307ms step_avg:672.81ms
step:1037/3000 train_loss:3.4577 train_time:690978ms step_avg:672.81ms
step:1038/3000 train_loss:3.4699 train_time:691652ms step_avg:672.81ms
step:1039/3000 train_loss:3.6722 train_time:692322ms step_avg:672.81ms
step:1040/3000 train_loss:3.4735 train_time:692994ms step_avg:672.81ms
step:1041/3000 train_loss:3.4884 train_time:693668ms step_avg:672.81ms
step:1042/3000 train_loss:3.6281 train_time:694341ms step_avg:672.81ms
step:1043/3000 train_loss:3.4449 train_time:695013ms step_avg:672.81ms
step:1044/3000 train_loss:3.5410 train_time:695684ms step_avg:672.81ms
step:1045/3000 train_loss:3.6404 train_time:696355ms step_avg:672.81ms
step:1046/3000 train_loss:3.7735 train_time:697028ms step_avg:672.81ms
step:1047/3000 train_loss:3.4579 train_time:697700ms step_avg:672.81ms
step:1048/3000 train_loss:3.8005 train_time:698374ms step_avg:672.81ms
step:1049/3000 train_loss:3.5119 train_time:699047ms step_avg:672.81ms
step:1050/3000 train_loss:4.0968 train_time:699719ms step_avg:672.81ms
step:1051/3000 train_loss:3.5979 train_time:700391ms step_avg:672.81ms
step:1052/3000 train_loss:3.5319 train_time:701064ms step_avg:672.81ms
step:1053/3000 train_loss:3.5459 train_time:701736ms step_avg:672.81ms
step:1054/3000 train_loss:3.6381 train_time:702407ms step_avg:672.80ms
step:1055/3000 train_loss:3.5636 train_time:703080ms step_avg:672.80ms
step:1056/3000 train_loss:3.6143 train_time:703752ms step_avg:672.80ms
step:1057/3000 train_loss:3.5366 train_time:704424ms step_avg:672.80ms
step:1058/3000 train_loss:3.5541 train_time:705096ms step_avg:672.80ms
step:1059/3000 train_loss:3.6070 train_time:705769ms step_avg:672.80ms
step:1060/3000 train_loss:3.8240 train_time:706441ms step_avg:672.80ms
step:1061/3000 train_loss:3.5780 train_time:707113ms step_avg:672.80ms
step:1062/3000 train_loss:3.7588 train_time:707787ms step_avg:672.80ms
step:1063/3000 train_loss:3.4678 train_time:708458ms step_avg:672.80ms
step:1064/3000 train_loss:3.5304 train_time:709130ms step_avg:672.80ms
step:1065/3000 train_loss:3.6996 train_time:709802ms step_avg:672.80ms
step:1066/3000 train_loss:3.6031 train_time:710474ms step_avg:672.80ms
step:1067/3000 train_loss:3.7701 train_time:711147ms step_avg:672.80ms
step:1068/3000 train_loss:3.2376 train_time:711819ms step_avg:672.80ms
step:1069/3000 train_loss:3.5153 train_time:712491ms step_avg:672.80ms
step:1070/3000 train_loss:3.6313 train_time:713163ms step_avg:672.80ms
step:1071/3000 train_loss:3.6150 train_time:713835ms step_avg:672.79ms
step:1072/3000 train_loss:3.5589 train_time:714508ms step_avg:672.79ms
step:1073/3000 train_loss:3.4218 train_time:715180ms step_avg:672.79ms
step:1074/3000 train_loss:3.5615 train_time:715852ms step_avg:672.79ms
step:1075/3000 train_loss:3.6204 train_time:716524ms step_avg:672.79ms
step:1076/3000 train_loss:3.5202 train_time:717197ms step_avg:672.79ms
step:1077/3000 train_loss:3.7170 train_time:717868ms step_avg:672.79ms
step:1078/3000 train_loss:3.6031 train_time:718539ms step_avg:672.79ms
step:1079/3000 train_loss:3.5837 train_time:719211ms step_avg:672.79ms
step:1080/3000 train_loss:3.5757 train_time:719883ms step_avg:672.79ms
step:1081/3000 train_loss:3.6375 train_time:720555ms step_avg:672.79ms
step:1082/3000 train_loss:3.6259 train_time:721226ms step_avg:672.79ms
step:1083/3000 train_loss:3.6102 train_time:721899ms step_avg:672.79ms
step:1084/3000 train_loss:3.5508 train_time:722572ms step_avg:672.79ms
step:1085/3000 train_loss:3.3661 train_time:723244ms step_avg:672.79ms
step:1086/3000 train_loss:3.5036 train_time:723916ms step_avg:672.78ms
step:1087/3000 train_loss:3.6394 train_time:724588ms step_avg:672.78ms
step:1088/3000 train_loss:3.4785 train_time:725259ms step_avg:672.78ms
step:1089/3000 train_loss:3.5019 train_time:725929ms step_avg:672.78ms
step:1090/3000 train_loss:3.5826 train_time:726600ms step_avg:672.78ms
step:1091/3000 train_loss:3.6167 train_time:727272ms step_avg:672.78ms
step:1092/3000 train_loss:3.6583 train_time:727943ms step_avg:672.78ms
step:1093/3000 train_loss:3.6107 train_time:728614ms step_avg:672.77ms
step:1094/3000 train_loss:3.5172 train_time:729287ms step_avg:672.77ms
step:1095/3000 train_loss:3.4850 train_time:729961ms step_avg:672.77ms
step:1096/3000 train_loss:3.7241 train_time:730632ms step_avg:672.77ms
step:1097/3000 train_loss:3.4815 train_time:731303ms step_avg:672.77ms
step:1098/3000 train_loss:3.5434 train_time:731974ms step_avg:672.77ms
step:1099/3000 train_loss:3.5568 train_time:732644ms step_avg:672.77ms
step:1100/3000 train_loss:3.6121 train_time:733316ms step_avg:672.77ms
step:1101/3000 train_loss:3.6900 train_time:733987ms step_avg:672.77ms
step:1102/3000 train_loss:3.5526 train_time:734658ms step_avg:672.76ms
step:1103/3000 train_loss:3.5695 train_time:735331ms step_avg:672.76ms
step:1104/3000 train_loss:3.5417 train_time:736001ms step_avg:672.76ms
step:1105/3000 train_loss:3.4870 train_time:736673ms step_avg:672.76ms
step:1106/3000 train_loss:3.6131 train_time:737344ms step_avg:672.76ms
step:1107/3000 train_loss:3.7151 train_time:738016ms step_avg:672.76ms
step:1108/3000 train_loss:3.5139 train_time:738688ms step_avg:672.76ms
step:1109/3000 train_loss:3.7819 train_time:739359ms step_avg:672.76ms
step:1110/3000 train_loss:3.5081 train_time:740030ms step_avg:672.75ms
step:1111/3000 train_loss:3.6140 train_time:740702ms step_avg:672.75ms
step:1112/3000 train_loss:3.6103 train_time:741373ms step_avg:672.75ms
step:1113/3000 train_loss:3.5973 train_time:742044ms step_avg:672.75ms
step:1114/3000 train_loss:3.5688 train_time:742715ms step_avg:672.75ms
step:1115/3000 train_loss:3.4729 train_time:743386ms step_avg:672.75ms
step:1116/3000 train_loss:3.6763 train_time:744057ms step_avg:672.75ms
step:1117/3000 train_loss:3.6697 train_time:744728ms step_avg:672.74ms
step:1118/3000 train_loss:3.4854 train_time:745399ms step_avg:672.74ms
step:1119/3000 train_loss:3.7705 train_time:746070ms step_avg:672.74ms
step:1120/3000 train_loss:3.5656 train_time:746742ms step_avg:672.74ms
step:1121/3000 train_loss:3.5339 train_time:747414ms step_avg:672.74ms
step:1122/3000 train_loss:3.6011 train_time:748084ms step_avg:672.74ms
step:1123/3000 train_loss:3.8534 train_time:748762ms step_avg:672.74ms
step:1124/3000 train_loss:3.5406 train_time:749434ms step_avg:672.74ms
step:1125/3000 train_loss:3.5695 train_time:750104ms step_avg:672.74ms
step:1125/3000 val_loss:3.5656 train_time:750115ms step_avg:672.75ms
step:1126/3000 train_loss:3.5500 train_time:750778ms step_avg:672.74ms
step:1127/3000 train_loss:3.6293 train_time:751448ms step_avg:672.74ms
step:1128/3000 train_loss:3.6760 train_time:752121ms step_avg:672.74ms
step:1129/3000 train_loss:3.4856 train_time:752792ms step_avg:672.74ms
step:1130/3000 train_loss:3.5303 train_time:753462ms step_avg:672.73ms
step:1131/3000 train_loss:3.6067 train_time:754133ms step_avg:672.73ms
step:1132/3000 train_loss:3.5140 train_time:754803ms step_avg:672.73ms
step:1133/3000 train_loss:3.6585 train_time:755475ms step_avg:672.73ms
step:1134/3000 train_loss:3.5951 train_time:756147ms step_avg:672.73ms
step:1135/3000 train_loss:3.6202 train_time:756817ms step_avg:672.73ms
step:1136/3000 train_loss:3.5296 train_time:757489ms step_avg:672.73ms
step:1137/3000 train_loss:3.8009 train_time:758160ms step_avg:672.72ms
step:1138/3000 train_loss:3.6648 train_time:758830ms step_avg:672.72ms
step:1139/3000 train_loss:3.5003 train_time:759501ms step_avg:672.72ms
step:1140/3000 train_loss:3.6645 train_time:760173ms step_avg:672.72ms
step:1141/3000 train_loss:3.5253 train_time:760844ms step_avg:672.72ms
step:1142/3000 train_loss:3.5334 train_time:761515ms step_avg:672.72ms
step:1143/3000 train_loss:3.6224 train_time:762192ms step_avg:672.72ms
step:1144/3000 train_loss:3.7188 train_time:762863ms step_avg:672.72ms
step:1145/3000 train_loss:3.6023 train_time:763533ms step_avg:672.72ms
step:1146/3000 train_loss:3.5171 train_time:764204ms step_avg:672.72ms
step:1147/3000 train_loss:3.6230 train_time:764876ms step_avg:672.71ms
step:1148/3000 train_loss:3.7402 train_time:765547ms step_avg:672.71ms
step:1149/3000 train_loss:3.7219 train_time:766218ms step_avg:672.71ms
step:1150/3000 train_loss:3.6469 train_time:766890ms step_avg:672.71ms
step:1151/3000 train_loss:3.6541 train_time:767561ms step_avg:672.71ms
step:1152/3000 train_loss:3.5016 train_time:768233ms step_avg:672.71ms
step:1153/3000 train_loss:3.5238 train_time:768903ms step_avg:672.71ms
step:1154/3000 train_loss:3.4854 train_time:769574ms step_avg:672.70ms
step:1155/3000 train_loss:3.6246 train_time:770245ms step_avg:672.70ms
step:1156/3000 train_loss:3.6078 train_time:770916ms step_avg:672.70ms
step:1157/3000 train_loss:3.6713 train_time:771587ms step_avg:672.70ms
step:1158/3000 train_loss:3.5131 train_time:772259ms step_avg:672.70ms
step:1159/3000 train_loss:3.6914 train_time:772930ms step_avg:672.70ms
step:1160/3000 train_loss:3.6478 train_time:773600ms step_avg:672.70ms
step:1161/3000 train_loss:3.4519 train_time:774272ms step_avg:672.69ms
