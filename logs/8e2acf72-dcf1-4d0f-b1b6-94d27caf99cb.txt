import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    X = torch.einsum("ij,ij->", G.type_as(X), X).clip(min=0) * X
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, use_attn=(i != 7))
                                     for i in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1390 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.6.0.dev20241231+cu126 compiled for CUDA 12.6
Tue Jan 14 17:18:33 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   28C    P0             118W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0             108W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1390 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1390 train_time:31482ms step_avg:nanms
step:2/1390 train_time:31864ms step_avg:nanms
step:3/1390 train_time:33235ms step_avg:nanms
step:4/1390 train_time:33370ms step_avg:nanms
step:5/1390 train_time:33504ms step_avg:nanms
step:6/1390 train_time:33640ms step_avg:nanms
step:7/1390 train_time:33774ms step_avg:nanms
step:8/1390 train_time:33908ms step_avg:nanms
step:9/1390 train_time:34042ms step_avg:nanms
step:10/1390 train_time:34182ms step_avg:nanms
step:11/1390 train_time:138ms step_avg:nanms
step:12/1390 train_time:273ms step_avg:nanms
step:13/1390 train_time:407ms step_avg:135.78ms
step:14/1390 train_time:542ms step_avg:135.41ms
step:15/1390 train_time:677ms step_avg:135.41ms
step:16/1390 train_time:811ms step_avg:135.25ms
step:17/1390 train_time:949ms step_avg:135.59ms
step:18/1390 train_time:1085ms step_avg:135.61ms
step:19/1390 train_time:1220ms step_avg:135.59ms
step:20/1390 train_time:1357ms step_avg:135.69ms
step:21/1390 train_time:1491ms step_avg:135.58ms
step:22/1390 train_time:1626ms step_avg:135.52ms
step:23/1390 train_time:1762ms step_avg:135.50ms
step:24/1390 train_time:1898ms step_avg:135.59ms
step:25/1390 train_time:2035ms step_avg:135.66ms
step:26/1390 train_time:2170ms step_avg:135.60ms
step:27/1390 train_time:2307ms step_avg:135.68ms
step:28/1390 train_time:2441ms step_avg:135.63ms
step:29/1390 train_time:2576ms step_avg:135.57ms
step:30/1390 train_time:2710ms step_avg:135.49ms
step:31/1390 train_time:2846ms step_avg:135.50ms
step:32/1390 train_time:2982ms step_avg:135.54ms
step:33/1390 train_time:3117ms step_avg:135.50ms
step:34/1390 train_time:3253ms step_avg:135.52ms
step:35/1390 train_time:3388ms step_avg:135.53ms
step:36/1390 train_time:3523ms step_avg:135.49ms
step:37/1390 train_time:3658ms step_avg:135.49ms
step:38/1390 train_time:3793ms step_avg:135.46ms
step:39/1390 train_time:3928ms step_avg:135.46ms
step:40/1390 train_time:4064ms step_avg:135.46ms
step:41/1390 train_time:4200ms step_avg:135.49ms
step:42/1390 train_time:4335ms step_avg:135.47ms
step:43/1390 train_time:4469ms step_avg:135.43ms
step:44/1390 train_time:4608ms step_avg:135.53ms
step:45/1390 train_time:4744ms step_avg:135.53ms
step:46/1390 train_time:4879ms step_avg:135.52ms
step:47/1390 train_time:5015ms step_avg:135.55ms
step:48/1390 train_time:5153ms step_avg:135.60ms
step:49/1390 train_time:5288ms step_avg:135.59ms
step:50/1390 train_time:5422ms step_avg:135.54ms
step:51/1390 train_time:5556ms step_avg:135.50ms
step:52/1390 train_time:5691ms step_avg:135.51ms
step:53/1390 train_time:5827ms step_avg:135.52ms
step:54/1390 train_time:5961ms step_avg:135.48ms
step:55/1390 train_time:6098ms step_avg:135.50ms
step:56/1390 train_time:6235ms step_avg:135.54ms
step:57/1390 train_time:6369ms step_avg:135.50ms
step:58/1390 train_time:6505ms step_avg:135.51ms
step:59/1390 train_time:6640ms step_avg:135.51ms
step:60/1390 train_time:6775ms step_avg:135.50ms
step:61/1390 train_time:6910ms step_avg:135.49ms
step:62/1390 train_time:7045ms step_avg:135.48ms
step:63/1390 train_time:7181ms step_avg:135.48ms
step:64/1390 train_time:7316ms step_avg:135.49ms
step:65/1390 train_time:7454ms step_avg:135.52ms
step:66/1390 train_time:7589ms step_avg:135.52ms
step:67/1390 train_time:7723ms step_avg:135.50ms
step:68/1390 train_time:7858ms step_avg:135.49ms
step:69/1390 train_time:7994ms step_avg:135.49ms
step:70/1390 train_time:8130ms step_avg:135.50ms
step:71/1390 train_time:8266ms step_avg:135.50ms
step:72/1390 train_time:8401ms step_avg:135.51ms
step:73/1390 train_time:8536ms step_avg:135.49ms
step:74/1390 train_time:8670ms step_avg:135.46ms
step:75/1390 train_time:8807ms step_avg:135.49ms
step:76/1390 train_time:8941ms step_avg:135.47ms
step:77/1390 train_time:9075ms step_avg:135.46ms
step:78/1390 train_time:9211ms step_avg:135.46ms
step:79/1390 train_time:9347ms step_avg:135.47ms
step:80/1390 train_time:9483ms step_avg:135.47ms
step:81/1390 train_time:9618ms step_avg:135.47ms
step:82/1390 train_time:9753ms step_avg:135.46ms
step:83/1390 train_time:9889ms step_avg:135.46ms
step:84/1390 train_time:10024ms step_avg:135.46ms
step:85/1390 train_time:10159ms step_avg:135.46ms
step:86/1390 train_time:10296ms step_avg:135.47ms
step:87/1390 train_time:10432ms step_avg:135.48ms
step:88/1390 train_time:10567ms step_avg:135.47ms
step:89/1390 train_time:10703ms step_avg:135.48ms
step:90/1390 train_time:10839ms step_avg:135.49ms
step:91/1390 train_time:10974ms step_avg:135.48ms
step:92/1390 train_time:11110ms step_avg:135.49ms
step:93/1390 train_time:11247ms step_avg:135.51ms
step:94/1390 train_time:11382ms step_avg:135.50ms
step:95/1390 train_time:11518ms step_avg:135.50ms
step:96/1390 train_time:11654ms step_avg:135.51ms
step:97/1390 train_time:11790ms step_avg:135.52ms
step:98/1390 train_time:11925ms step_avg:135.52ms
step:99/1390 train_time:12060ms step_avg:135.51ms
step:100/1390 train_time:12197ms step_avg:135.52ms
step:101/1390 train_time:12333ms step_avg:135.52ms
step:102/1390 train_time:12468ms step_avg:135.52ms
step:103/1390 train_time:12605ms step_avg:135.54ms
step:104/1390 train_time:12741ms step_avg:135.55ms
step:105/1390 train_time:12879ms step_avg:135.57ms
step:106/1390 train_time:13018ms step_avg:135.60ms
step:107/1390 train_time:13155ms step_avg:135.62ms
step:108/1390 train_time:13293ms step_avg:135.65ms
step:109/1390 train_time:13433ms step_avg:135.68ms
step:110/1390 train_time:13570ms step_avg:135.70ms
step:111/1390 train_time:13712ms step_avg:135.76ms
step:112/1390 train_time:13852ms step_avg:135.80ms
step:113/1390 train_time:13991ms step_avg:135.84ms
step:114/1390 train_time:14130ms step_avg:135.87ms
step:115/1390 train_time:14268ms step_avg:135.89ms
step:116/1390 train_time:14408ms step_avg:135.93ms
step:117/1390 train_time:14547ms step_avg:135.95ms
step:118/1390 train_time:14685ms step_avg:135.97ms
step:119/1390 train_time:14825ms step_avg:136.01ms
step:120/1390 train_time:14965ms step_avg:136.04ms
step:121/1390 train_time:15104ms step_avg:136.07ms
step:122/1390 train_time:15245ms step_avg:136.12ms
step:123/1390 train_time:15385ms step_avg:136.15ms
step:124/1390 train_time:15524ms step_avg:136.18ms
step:125/1390 train_time:15666ms step_avg:136.23ms
step:125/1390 val_loss:4.6194 train_time:15730ms step_avg:136.78ms
step:126/1390 train_time:15808ms step_avg:136.27ms
step:127/1390 train_time:15953ms step_avg:136.35ms
step:128/1390 train_time:16094ms step_avg:136.39ms
step:129/1390 train_time:16231ms step_avg:136.39ms
step:130/1390 train_time:16369ms step_avg:136.41ms
step:131/1390 train_time:16507ms step_avg:136.42ms
step:132/1390 train_time:16645ms step_avg:136.43ms
step:133/1390 train_time:16785ms step_avg:136.46ms
step:134/1390 train_time:16926ms step_avg:136.50ms
step:135/1390 train_time:17065ms step_avg:136.52ms
step:136/1390 train_time:17205ms step_avg:136.55ms
step:137/1390 train_time:17343ms step_avg:136.56ms
step:138/1390 train_time:17482ms step_avg:136.58ms
step:139/1390 train_time:17619ms step_avg:136.58ms
step:140/1390 train_time:17759ms step_avg:136.61ms
step:141/1390 train_time:17899ms step_avg:136.63ms
step:142/1390 train_time:18040ms step_avg:136.67ms
step:143/1390 train_time:18179ms step_avg:136.69ms
step:144/1390 train_time:18318ms step_avg:136.70ms
step:145/1390 train_time:18457ms step_avg:136.72ms
step:146/1390 train_time:18598ms step_avg:136.75ms
step:147/1390 train_time:18737ms step_avg:136.77ms
step:148/1390 train_time:18878ms step_avg:136.79ms
step:149/1390 train_time:19018ms step_avg:136.82ms
step:150/1390 train_time:19157ms step_avg:136.84ms
step:151/1390 train_time:19297ms step_avg:136.86ms
step:152/1390 train_time:19438ms step_avg:136.88ms
step:153/1390 train_time:19577ms step_avg:136.90ms
step:154/1390 train_time:19716ms step_avg:136.92ms
step:155/1390 train_time:19856ms step_avg:136.94ms
step:156/1390 train_time:19997ms step_avg:136.96ms
step:157/1390 train_time:20136ms step_avg:136.98ms
step:158/1390 train_time:20276ms step_avg:137.00ms
step:159/1390 train_time:20414ms step_avg:137.01ms
step:160/1390 train_time:20555ms step_avg:137.04ms
step:161/1390 train_time:20697ms step_avg:137.07ms
step:162/1390 train_time:20838ms step_avg:137.09ms
step:163/1390 train_time:20978ms step_avg:137.11ms
step:164/1390 train_time:21117ms step_avg:137.12ms
step:165/1390 train_time:21258ms step_avg:137.15ms
step:166/1390 train_time:21398ms step_avg:137.16ms
step:167/1390 train_time:21536ms step_avg:137.17ms
step:168/1390 train_time:21677ms step_avg:137.20ms
step:169/1390 train_time:21818ms step_avg:137.22ms
step:170/1390 train_time:21958ms step_avg:137.24ms
step:171/1390 train_time:22098ms step_avg:137.26ms
step:172/1390 train_time:22238ms step_avg:137.27ms
step:173/1390 train_time:22378ms step_avg:137.29ms
step:174/1390 train_time:22516ms step_avg:137.29ms
step:175/1390 train_time:22657ms step_avg:137.32ms
step:176/1390 train_time:22797ms step_avg:137.33ms
step:177/1390 train_time:22937ms step_avg:137.35ms
step:178/1390 train_time:23077ms step_avg:137.37ms
step:179/1390 train_time:23216ms step_avg:137.37ms
step:180/1390 train_time:23357ms step_avg:137.39ms
step:181/1390 train_time:23496ms step_avg:137.40ms
step:182/1390 train_time:23636ms step_avg:137.42ms
step:183/1390 train_time:23777ms step_avg:137.44ms
step:184/1390 train_time:23916ms step_avg:137.45ms
step:185/1390 train_time:24057ms step_avg:137.47ms
step:186/1390 train_time:24197ms step_avg:137.48ms
step:187/1390 train_time:24337ms step_avg:137.50ms
step:188/1390 train_time:24476ms step_avg:137.51ms
step:189/1390 train_time:24615ms step_avg:137.51ms
step:190/1390 train_time:24757ms step_avg:137.54ms
step:191/1390 train_time:24940ms step_avg:137.79ms
step:192/1390 train_time:25078ms step_avg:137.79ms
step:193/1390 train_time:25215ms step_avg:137.79ms
step:194/1390 train_time:25354ms step_avg:137.79ms
step:195/1390 train_time:25491ms step_avg:137.79ms
step:196/1390 train_time:25629ms step_avg:137.79ms
step:197/1390 train_time:25771ms step_avg:137.81ms
step:198/1390 train_time:25916ms step_avg:137.85ms
step:199/1390 train_time:26058ms step_avg:137.87ms
step:200/1390 train_time:26197ms step_avg:137.88ms
step:201/1390 train_time:26336ms step_avg:137.88ms
step:202/1390 train_time:26475ms step_avg:137.89ms
step:203/1390 train_time:26613ms step_avg:137.89ms
step:204/1390 train_time:26757ms step_avg:137.92ms
step:205/1390 train_time:26899ms step_avg:137.95ms
step:206/1390 train_time:27041ms step_avg:137.96ms
step:207/1390 train_time:27180ms step_avg:137.97ms
step:208/1390 train_time:27321ms step_avg:137.99ms
step:209/1390 train_time:27463ms step_avg:138.01ms
step:210/1390 train_time:27605ms step_avg:138.02ms
step:211/1390 train_time:27747ms step_avg:138.05ms
step:212/1390 train_time:27889ms step_avg:138.07ms
step:213/1390 train_time:28033ms step_avg:138.09ms
step:214/1390 train_time:28176ms step_avg:138.12ms
step:215/1390 train_time:28317ms step_avg:138.13ms
step:216/1390 train_time:28457ms step_avg:138.14ms
step:217/1390 train_time:28600ms step_avg:138.17ms
step:218/1390 train_time:28745ms step_avg:138.20ms
step:219/1390 train_time:28886ms step_avg:138.21ms
step:220/1390 train_time:29029ms step_avg:138.24ms
step:221/1390 train_time:29172ms step_avg:138.26ms
step:222/1390 train_time:29314ms step_avg:138.28ms
step:223/1390 train_time:29456ms step_avg:138.29ms
step:224/1390 train_time:29599ms step_avg:138.31ms
step:225/1390 train_time:29743ms step_avg:138.34ms
step:226/1390 train_time:29886ms step_avg:138.36ms
step:227/1390 train_time:30030ms step_avg:138.39ms
step:228/1390 train_time:30173ms step_avg:138.41ms
step:229/1390 train_time:30314ms step_avg:138.42ms
step:230/1390 train_time:30457ms step_avg:138.44ms
step:231/1390 train_time:30599ms step_avg:138.46ms
step:232/1390 train_time:30743ms step_avg:138.48ms
step:233/1390 train_time:30886ms step_avg:138.50ms
step:234/1390 train_time:31029ms step_avg:138.52ms
step:235/1390 train_time:31173ms step_avg:138.54ms
step:236/1390 train_time:31315ms step_avg:138.56ms
step:237/1390 train_time:31458ms step_avg:138.58ms
step:238/1390 train_time:31602ms step_avg:138.61ms
step:239/1390 train_time:31746ms step_avg:138.63ms
step:240/1390 train_time:31887ms step_avg:138.64ms
step:241/1390 train_time:32029ms step_avg:138.66ms
step:242/1390 train_time:32173ms step_avg:138.68ms
step:243/1390 train_time:32315ms step_avg:138.69ms
step:244/1390 train_time:32458ms step_avg:138.71ms
step:245/1390 train_time:32601ms step_avg:138.73ms
step:246/1390 train_time:32745ms step_avg:138.75ms
step:247/1390 train_time:32888ms step_avg:138.77ms
step:248/1390 train_time:33031ms step_avg:138.78ms
step:249/1390 train_time:33175ms step_avg:138.81ms
step:250/1390 train_time:33317ms step_avg:138.82ms
step:250/1390 val_loss:4.1456 train_time:33381ms step_avg:139.09ms
step:251/1390 train_time:33460ms step_avg:138.84ms
step:252/1390 train_time:33607ms step_avg:138.87ms
step:253/1390 train_time:33749ms step_avg:138.89ms
step:254/1390 train_time:33892ms step_avg:138.90ms
step:255/1390 train_time:34034ms step_avg:138.91ms
step:256/1390 train_time:34176ms step_avg:138.93ms
step:257/1390 train_time:34318ms step_avg:138.94ms
step:258/1390 train_time:34462ms step_avg:138.96ms
step:259/1390 train_time:34605ms step_avg:138.97ms
step:260/1390 train_time:34748ms step_avg:138.99ms
step:261/1390 train_time:34891ms step_avg:139.01ms
step:262/1390 train_time:35033ms step_avg:139.02ms
step:263/1390 train_time:35177ms step_avg:139.04ms
step:264/1390 train_time:35319ms step_avg:139.05ms
step:265/1390 train_time:35461ms step_avg:139.06ms
step:266/1390 train_time:35605ms step_avg:139.08ms
step:267/1390 train_time:35749ms step_avg:139.10ms
step:268/1390 train_time:35892ms step_avg:139.12ms
step:269/1390 train_time:36034ms step_avg:139.13ms
step:270/1390 train_time:36178ms step_avg:139.15ms
step:271/1390 train_time:36320ms step_avg:139.16ms
step:272/1390 train_time:36463ms step_avg:139.17ms
step:273/1390 train_time:36605ms step_avg:139.18ms
step:274/1390 train_time:36748ms step_avg:139.20ms
step:275/1390 train_time:36892ms step_avg:139.22ms
step:276/1390 train_time:37035ms step_avg:139.23ms
step:277/1390 train_time:37178ms step_avg:139.24ms
step:278/1390 train_time:37321ms step_avg:139.26ms
step:279/1390 train_time:37463ms step_avg:139.27ms
step:280/1390 train_time:37605ms step_avg:139.28ms
step:281/1390 train_time:37749ms step_avg:139.30ms
step:282/1390 train_time:37892ms step_avg:139.31ms
step:283/1390 train_time:38034ms step_avg:139.32ms
step:284/1390 train_time:38179ms step_avg:139.34ms
step:285/1390 train_time:38322ms step_avg:139.35ms
step:286/1390 train_time:38465ms step_avg:139.36ms
step:287/1390 train_time:38607ms step_avg:139.37ms
step:288/1390 train_time:38751ms step_avg:139.39ms
step:289/1390 train_time:38894ms step_avg:139.41ms
step:290/1390 train_time:39038ms step_avg:139.42ms
step:291/1390 train_time:39182ms step_avg:139.44ms
step:292/1390 train_time:39326ms step_avg:139.45ms
step:293/1390 train_time:39471ms step_avg:139.47ms
step:294/1390 train_time:39613ms step_avg:139.48ms
step:295/1390 train_time:39755ms step_avg:139.49ms
step:296/1390 train_time:39899ms step_avg:139.51ms
step:297/1390 train_time:40040ms step_avg:139.51ms
step:298/1390 train_time:40184ms step_avg:139.53ms
step:299/1390 train_time:40330ms step_avg:139.55ms
step:300/1390 train_time:40475ms step_avg:139.57ms
step:301/1390 train_time:40615ms step_avg:139.57ms
step:302/1390 train_time:40757ms step_avg:139.58ms
step:303/1390 train_time:40900ms step_avg:139.59ms
step:304/1390 train_time:41042ms step_avg:139.60ms
step:305/1390 train_time:41185ms step_avg:139.61ms
step:306/1390 train_time:41329ms step_avg:139.62ms
step:307/1390 train_time:41473ms step_avg:139.64ms
step:308/1390 train_time:41615ms step_avg:139.65ms
step:309/1390 train_time:41758ms step_avg:139.66ms
step:310/1390 train_time:41901ms step_avg:139.67ms
step:311/1390 train_time:42044ms step_avg:139.68ms
step:312/1390 train_time:42190ms step_avg:139.70ms
step:313/1390 train_time:42335ms step_avg:139.72ms
step:314/1390 train_time:42481ms step_avg:139.74ms
step:315/1390 train_time:42626ms step_avg:139.76ms
step:316/1390 train_time:42771ms step_avg:139.77ms
step:317/1390 train_time:42916ms step_avg:139.79ms
step:318/1390 train_time:43059ms step_avg:139.80ms
step:319/1390 train_time:43204ms step_avg:139.82ms
step:320/1390 train_time:43348ms step_avg:139.83ms
step:321/1390 train_time:43494ms step_avg:139.85ms
step:322/1390 train_time:43639ms step_avg:139.87ms
step:323/1390 train_time:43784ms step_avg:139.88ms
step:324/1390 train_time:43928ms step_avg:139.90ms
step:325/1390 train_time:44072ms step_avg:139.91ms
step:326/1390 train_time:44218ms step_avg:139.93ms
step:327/1390 train_time:44362ms step_avg:139.94ms
step:328/1390 train_time:44508ms step_avg:139.96ms
step:329/1390 train_time:44655ms step_avg:139.98ms
step:330/1390 train_time:44800ms step_avg:140.00ms
step:331/1390 train_time:44944ms step_avg:140.01ms
step:332/1390 train_time:45089ms step_avg:140.03ms
step:333/1390 train_time:45234ms step_avg:140.04ms
step:334/1390 train_time:45381ms step_avg:140.06ms
step:335/1390 train_time:45527ms step_avg:140.08ms
step:336/1390 train_time:45671ms step_avg:140.10ms
step:337/1390 train_time:45817ms step_avg:140.11ms
step:338/1390 train_time:45962ms step_avg:140.13ms
step:339/1390 train_time:46108ms step_avg:140.15ms
step:340/1390 train_time:46253ms step_avg:140.16ms
step:341/1390 train_time:46398ms step_avg:140.18ms
step:342/1390 train_time:46543ms step_avg:140.19ms
step:343/1390 train_time:46689ms step_avg:140.21ms
step:344/1390 train_time:46834ms step_avg:140.22ms
step:345/1390 train_time:46980ms step_avg:140.24ms
step:346/1390 train_time:47126ms step_avg:140.25ms
step:347/1390 train_time:47272ms step_avg:140.27ms
step:348/1390 train_time:47417ms step_avg:140.29ms
step:349/1390 train_time:47563ms step_avg:140.30ms
step:350/1390 train_time:47708ms step_avg:140.32ms
step:351/1390 train_time:47853ms step_avg:140.33ms
step:352/1390 train_time:47998ms step_avg:140.34ms
step:353/1390 train_time:48143ms step_avg:140.36ms
step:354/1390 train_time:48288ms step_avg:140.37ms
step:355/1390 train_time:48434ms step_avg:140.39ms
step:356/1390 train_time:48580ms step_avg:140.40ms
step:357/1390 train_time:48724ms step_avg:140.42ms
step:358/1390 train_time:48869ms step_avg:140.43ms
step:359/1390 train_time:49017ms step_avg:140.45ms
step:360/1390 train_time:49162ms step_avg:140.46ms
step:361/1390 train_time:49307ms step_avg:140.48ms
step:362/1390 train_time:49454ms step_avg:140.50ms
step:363/1390 train_time:49600ms step_avg:140.51ms
step:364/1390 train_time:49745ms step_avg:140.52ms
step:365/1390 train_time:49890ms step_avg:140.54ms
step:366/1390 train_time:50036ms step_avg:140.55ms
step:367/1390 train_time:50182ms step_avg:140.56ms
step:368/1390 train_time:50325ms step_avg:140.57ms
step:369/1390 train_time:50470ms step_avg:140.59ms
step:370/1390 train_time:50616ms step_avg:140.60ms
step:371/1390 train_time:50760ms step_avg:140.61ms
step:372/1390 train_time:50907ms step_avg:140.63ms
step:373/1390 train_time:51053ms step_avg:140.64ms
step:374/1390 train_time:51199ms step_avg:140.66ms
step:375/1390 train_time:51342ms step_avg:140.66ms
step:375/1390 val_loss:3.9716 train_time:51407ms step_avg:140.84ms
step:376/1390 train_time:51489ms step_avg:140.68ms
step:377/1390 train_time:51636ms step_avg:140.70ms
step:378/1390 train_time:51781ms step_avg:140.71ms
step:379/1390 train_time:51924ms step_avg:140.72ms
step:380/1390 train_time:52068ms step_avg:140.73ms
step:381/1390 train_time:52259ms step_avg:140.86ms
step:382/1390 train_time:52402ms step_avg:140.87ms
step:383/1390 train_time:52546ms step_avg:140.87ms
step:384/1390 train_time:52690ms step_avg:140.88ms
step:385/1390 train_time:52832ms step_avg:140.89ms
step:386/1390 train_time:52976ms step_avg:140.89ms
step:387/1390 train_time:53124ms step_avg:140.91ms
step:388/1390 train_time:53275ms step_avg:140.94ms
step:389/1390 train_time:53420ms step_avg:140.95ms
step:390/1390 train_time:53564ms step_avg:140.96ms
step:391/1390 train_time:53710ms step_avg:140.97ms
step:392/1390 train_time:53853ms step_avg:140.98ms
step:393/1390 train_time:53998ms step_avg:140.99ms
step:394/1390 train_time:54144ms step_avg:141.00ms
step:395/1390 train_time:54293ms step_avg:141.02ms
step:396/1390 train_time:54438ms step_avg:141.03ms
step:397/1390 train_time:54582ms step_avg:141.04ms
step:398/1390 train_time:54728ms step_avg:141.05ms
step:399/1390 train_time:54874ms step_avg:141.06ms
step:400/1390 train_time:55017ms step_avg:141.07ms
step:401/1390 train_time:55163ms step_avg:141.08ms
step:402/1390 train_time:55309ms step_avg:141.09ms
step:403/1390 train_time:55454ms step_avg:141.10ms
step:404/1390 train_time:55599ms step_avg:141.11ms
step:405/1390 train_time:55744ms step_avg:141.12ms
step:406/1390 train_time:55890ms step_avg:141.14ms
step:407/1390 train_time:56034ms step_avg:141.14ms
step:408/1390 train_time:56179ms step_avg:141.15ms
step:409/1390 train_time:56324ms step_avg:141.16ms
step:410/1390 train_time:56470ms step_avg:141.18ms
step:411/1390 train_time:56616ms step_avg:141.19ms
step:412/1390 train_time:56761ms step_avg:141.20ms
step:413/1390 train_time:56906ms step_avg:141.21ms
step:414/1390 train_time:57053ms step_avg:141.22ms
step:415/1390 train_time:57199ms step_avg:141.23ms
step:416/1390 train_time:57346ms step_avg:141.25ms
step:417/1390 train_time:57495ms step_avg:141.26ms
step:418/1390 train_time:57641ms step_avg:141.28ms
step:419/1390 train_time:57789ms step_avg:141.29ms
step:420/1390 train_time:57935ms step_avg:141.31ms
step:421/1390 train_time:58082ms step_avg:141.32ms
step:422/1390 train_time:58229ms step_avg:141.33ms
step:423/1390 train_time:58377ms step_avg:141.35ms
step:424/1390 train_time:58525ms step_avg:141.36ms
step:425/1390 train_time:58674ms step_avg:141.38ms
step:426/1390 train_time:58821ms step_avg:141.40ms
step:427/1390 train_time:58967ms step_avg:141.41ms
step:428/1390 train_time:59114ms step_avg:141.42ms
step:429/1390 train_time:59262ms step_avg:141.44ms
step:430/1390 train_time:59411ms step_avg:141.46ms
step:431/1390 train_time:59559ms step_avg:141.47ms
step:432/1390 train_time:59706ms step_avg:141.48ms
step:433/1390 train_time:59854ms step_avg:141.50ms
step:434/1390 train_time:60000ms step_avg:141.51ms
step:435/1390 train_time:60147ms step_avg:141.52ms
step:436/1390 train_time:60294ms step_avg:141.54ms
step:437/1390 train_time:60440ms step_avg:141.55ms
step:438/1390 train_time:60588ms step_avg:141.56ms
step:439/1390 train_time:60734ms step_avg:141.57ms
step:440/1390 train_time:60881ms step_avg:141.58ms
step:441/1390 train_time:61027ms step_avg:141.59ms
step:442/1390 train_time:61175ms step_avg:141.61ms
step:443/1390 train_time:61324ms step_avg:141.62ms
step:444/1390 train_time:61472ms step_avg:141.64ms
step:445/1390 train_time:61618ms step_avg:141.65ms
step:446/1390 train_time:61766ms step_avg:141.67ms
step:447/1390 train_time:61914ms step_avg:141.68ms
step:448/1390 train_time:62061ms step_avg:141.69ms
step:449/1390 train_time:62208ms step_avg:141.70ms
step:450/1390 train_time:62356ms step_avg:141.72ms
step:451/1390 train_time:62503ms step_avg:141.73ms
step:452/1390 train_time:62650ms step_avg:141.74ms
step:453/1390 train_time:62798ms step_avg:141.76ms
step:454/1390 train_time:62946ms step_avg:141.77ms
step:455/1390 train_time:63094ms step_avg:141.79ms
step:456/1390 train_time:63240ms step_avg:141.79ms
step:457/1390 train_time:63388ms step_avg:141.81ms
step:458/1390 train_time:63534ms step_avg:141.82ms
step:459/1390 train_time:63681ms step_avg:141.83ms
step:460/1390 train_time:63828ms step_avg:141.84ms
step:461/1390 train_time:63976ms step_avg:141.85ms
step:462/1390 train_time:64123ms step_avg:141.86ms
step:463/1390 train_time:64271ms step_avg:141.88ms
step:464/1390 train_time:64416ms step_avg:141.89ms
step:465/1390 train_time:64563ms step_avg:141.90ms
step:466/1390 train_time:64713ms step_avg:141.91ms
step:467/1390 train_time:64859ms step_avg:141.92ms
step:468/1390 train_time:65005ms step_avg:141.93ms
step:469/1390 train_time:65153ms step_avg:141.95ms
step:470/1390 train_time:65300ms step_avg:141.96ms
step:471/1390 train_time:65447ms step_avg:141.97ms
step:472/1390 train_time:65595ms step_avg:141.98ms
step:473/1390 train_time:65742ms step_avg:141.99ms
step:474/1390 train_time:65889ms step_avg:142.00ms
step:475/1390 train_time:66034ms step_avg:142.01ms
step:476/1390 train_time:66182ms step_avg:142.02ms
step:477/1390 train_time:66329ms step_avg:142.03ms
step:478/1390 train_time:66476ms step_avg:142.04ms
step:479/1390 train_time:66623ms step_avg:142.05ms
step:480/1390 train_time:66772ms step_avg:142.07ms
step:481/1390 train_time:66919ms step_avg:142.08ms
step:482/1390 train_time:67065ms step_avg:142.09ms
step:483/1390 train_time:67213ms step_avg:142.10ms
step:484/1390 train_time:67361ms step_avg:142.11ms
step:485/1390 train_time:67507ms step_avg:142.12ms
step:486/1390 train_time:67654ms step_avg:142.13ms
step:487/1390 train_time:67800ms step_avg:142.14ms
step:488/1390 train_time:67947ms step_avg:142.15ms
step:489/1390 train_time:68094ms step_avg:142.16ms
step:490/1390 train_time:68241ms step_avg:142.17ms
step:491/1390 train_time:68388ms step_avg:142.18ms
step:492/1390 train_time:68536ms step_avg:142.19ms
step:493/1390 train_time:68683ms step_avg:142.20ms
step:494/1390 train_time:68829ms step_avg:142.21ms
step:495/1390 train_time:68977ms step_avg:142.22ms
step:496/1390 train_time:69125ms step_avg:142.23ms
step:497/1390 train_time:69272ms step_avg:142.24ms
step:498/1390 train_time:69419ms step_avg:142.25ms
step:499/1390 train_time:69567ms step_avg:142.26ms
step:500/1390 train_time:69715ms step_avg:142.27ms
step:500/1390 val_loss:3.8749 train_time:69782ms step_avg:142.41ms
step:501/1390 train_time:69863ms step_avg:142.29ms
step:502/1390 train_time:70012ms step_avg:142.30ms
step:503/1390 train_time:70160ms step_avg:142.31ms
step:504/1390 train_time:70307ms step_avg:142.32ms
step:505/1390 train_time:70454ms step_avg:142.33ms
step:506/1390 train_time:70599ms step_avg:142.34ms
step:507/1390 train_time:70747ms step_avg:142.35ms
step:508/1390 train_time:70897ms step_avg:142.36ms
step:509/1390 train_time:71044ms step_avg:142.37ms
step:510/1390 train_time:71192ms step_avg:142.38ms
step:511/1390 train_time:71339ms step_avg:142.39ms
step:512/1390 train_time:71485ms step_avg:142.40ms
step:513/1390 train_time:71633ms step_avg:142.41ms
step:514/1390 train_time:71780ms step_avg:142.42ms
step:515/1390 train_time:71928ms step_avg:142.43ms
step:516/1390 train_time:72076ms step_avg:142.44ms
step:517/1390 train_time:72225ms step_avg:142.46ms
step:518/1390 train_time:72374ms step_avg:142.47ms
step:519/1390 train_time:72522ms step_avg:142.48ms
step:520/1390 train_time:72671ms step_avg:142.49ms
step:521/1390 train_time:72818ms step_avg:142.50ms
step:522/1390 train_time:72968ms step_avg:142.52ms
step:523/1390 train_time:73116ms step_avg:142.53ms
step:524/1390 train_time:73265ms step_avg:142.54ms
step:525/1390 train_time:73414ms step_avg:142.55ms
step:526/1390 train_time:73564ms step_avg:142.57ms
step:527/1390 train_time:73713ms step_avg:142.58ms
step:528/1390 train_time:73862ms step_avg:142.59ms
step:529/1390 train_time:74011ms step_avg:142.60ms
step:530/1390 train_time:74161ms step_avg:142.62ms
step:531/1390 train_time:74309ms step_avg:142.63ms
step:532/1390 train_time:74458ms step_avg:142.64ms
step:533/1390 train_time:74607ms step_avg:142.65ms
step:534/1390 train_time:74755ms step_avg:142.66ms
step:535/1390 train_time:74904ms step_avg:142.68ms
step:536/1390 train_time:75055ms step_avg:142.69ms
step:537/1390 train_time:75204ms step_avg:142.70ms
step:538/1390 train_time:75354ms step_avg:142.72ms
step:539/1390 train_time:75504ms step_avg:142.73ms
step:540/1390 train_time:75653ms step_avg:142.74ms
step:541/1390 train_time:75800ms step_avg:142.75ms
step:542/1390 train_time:75950ms step_avg:142.76ms
step:543/1390 train_time:76099ms step_avg:142.78ms
step:544/1390 train_time:76248ms step_avg:142.79ms
step:545/1390 train_time:76395ms step_avg:142.79ms
step:546/1390 train_time:76545ms step_avg:142.81ms
step:547/1390 train_time:76694ms step_avg:142.82ms
step:548/1390 train_time:76844ms step_avg:142.83ms
step:549/1390 train_time:76994ms step_avg:142.85ms
step:550/1390 train_time:77142ms step_avg:142.86ms
step:551/1390 train_time:77291ms step_avg:142.87ms
step:552/1390 train_time:77439ms step_avg:142.88ms
step:553/1390 train_time:77587ms step_avg:142.89ms
step:554/1390 train_time:77735ms step_avg:142.90ms
step:555/1390 train_time:77887ms step_avg:142.91ms
step:556/1390 train_time:78036ms step_avg:142.92ms
step:557/1390 train_time:78185ms step_avg:142.93ms
step:558/1390 train_time:78333ms step_avg:142.94ms
step:559/1390 train_time:78482ms step_avg:142.95ms
step:560/1390 train_time:78631ms step_avg:142.97ms
step:561/1390 train_time:78778ms step_avg:142.97ms
step:562/1390 train_time:78927ms step_avg:142.98ms
step:563/1390 train_time:79077ms step_avg:143.00ms
step:564/1390 train_time:79226ms step_avg:143.01ms
step:565/1390 train_time:79375ms step_avg:143.02ms
step:566/1390 train_time:79525ms step_avg:143.03ms
step:567/1390 train_time:79674ms step_avg:143.04ms
step:568/1390 train_time:79821ms step_avg:143.05ms
step:569/1390 train_time:79972ms step_avg:143.06ms
step:570/1390 train_time:80120ms step_avg:143.07ms
step:571/1390 train_time:80307ms step_avg:143.15ms
step:572/1390 train_time:80454ms step_avg:143.16ms
step:573/1390 train_time:80602ms step_avg:143.16ms
step:574/1390 train_time:80751ms step_avg:143.18ms
step:575/1390 train_time:80898ms step_avg:143.18ms
step:576/1390 train_time:81047ms step_avg:143.19ms
step:577/1390 train_time:81197ms step_avg:143.21ms
step:578/1390 train_time:81349ms step_avg:143.22ms
step:579/1390 train_time:81497ms step_avg:143.23ms
step:580/1390 train_time:81645ms step_avg:143.24ms
step:581/1390 train_time:81794ms step_avg:143.25ms
step:582/1390 train_time:81941ms step_avg:143.25ms
step:583/1390 train_time:82090ms step_avg:143.26ms
step:584/1390 train_time:82239ms step_avg:143.27ms
step:585/1390 train_time:82389ms step_avg:143.28ms
step:586/1390 train_time:82537ms step_avg:143.29ms
step:587/1390 train_time:82687ms step_avg:143.30ms
step:588/1390 train_time:82835ms step_avg:143.31ms
step:589/1390 train_time:82985ms step_avg:143.33ms
step:590/1390 train_time:83135ms step_avg:143.34ms
step:591/1390 train_time:83284ms step_avg:143.35ms
step:592/1390 train_time:83433ms step_avg:143.36ms
step:593/1390 train_time:83582ms step_avg:143.37ms
step:594/1390 train_time:83732ms step_avg:143.38ms
step:595/1390 train_time:83879ms step_avg:143.38ms
step:596/1390 train_time:84028ms step_avg:143.39ms
step:597/1390 train_time:84177ms step_avg:143.40ms
step:598/1390 train_time:84325ms step_avg:143.41ms
step:599/1390 train_time:84475ms step_avg:143.42ms
step:600/1390 train_time:84623ms step_avg:143.43ms
step:601/1390 train_time:84774ms step_avg:143.44ms
step:602/1390 train_time:84921ms step_avg:143.45ms
step:603/1390 train_time:85070ms step_avg:143.46ms
step:604/1390 train_time:85217ms step_avg:143.46ms
step:605/1390 train_time:85366ms step_avg:143.47ms
step:606/1390 train_time:85516ms step_avg:143.48ms
step:607/1390 train_time:85666ms step_avg:143.49ms
step:608/1390 train_time:85815ms step_avg:143.50ms
step:609/1390 train_time:85964ms step_avg:143.51ms
step:610/1390 train_time:86113ms step_avg:143.52ms
step:611/1390 train_time:86260ms step_avg:143.53ms
step:612/1390 train_time:86410ms step_avg:143.54ms
step:613/1390 train_time:86559ms step_avg:143.55ms
step:614/1390 train_time:86708ms step_avg:143.56ms
step:615/1390 train_time:86856ms step_avg:143.56ms
step:616/1390 train_time:87006ms step_avg:143.57ms
step:617/1390 train_time:87155ms step_avg:143.58ms
step:618/1390 train_time:87304ms step_avg:143.59ms
step:619/1390 train_time:87454ms step_avg:143.60ms
step:620/1390 train_time:87605ms step_avg:143.61ms
step:621/1390 train_time:87755ms step_avg:143.63ms
step:622/1390 train_time:87906ms step_avg:143.64ms
step:623/1390 train_time:88057ms step_avg:143.65ms
step:624/1390 train_time:88206ms step_avg:143.66ms
step:625/1390 train_time:88355ms step_avg:143.67ms
step:625/1390 val_loss:3.7969 train_time:88425ms step_avg:143.78ms
step:626/1390 train_time:88506ms step_avg:143.68ms
step:627/1390 train_time:88658ms step_avg:143.69ms
step:628/1390 train_time:88807ms step_avg:143.70ms
step:629/1390 train_time:88957ms step_avg:143.71ms
step:630/1390 train_time:89105ms step_avg:143.72ms
step:631/1390 train_time:89253ms step_avg:143.72ms
step:632/1390 train_time:89403ms step_avg:143.74ms
step:633/1390 train_time:89557ms step_avg:143.75ms
step:634/1390 train_time:89706ms step_avg:143.76ms
step:635/1390 train_time:89857ms step_avg:143.77ms
step:636/1390 train_time:90006ms step_avg:143.78ms
step:637/1390 train_time:90157ms step_avg:143.79ms
step:638/1390 train_time:90305ms step_avg:143.80ms
step:639/1390 train_time:90456ms step_avg:143.81ms
step:640/1390 train_time:90607ms step_avg:143.82ms
step:641/1390 train_time:90756ms step_avg:143.83ms
step:642/1390 train_time:90906ms step_avg:143.84ms
step:643/1390 train_time:91058ms step_avg:143.85ms
step:644/1390 train_time:91207ms step_avg:143.86ms
step:645/1390 train_time:91359ms step_avg:143.87ms
step:646/1390 train_time:91511ms step_avg:143.88ms
step:647/1390 train_time:91660ms step_avg:143.89ms
step:648/1390 train_time:91813ms step_avg:143.91ms
step:649/1390 train_time:91964ms step_avg:143.92ms
step:650/1390 train_time:92116ms step_avg:143.93ms
step:651/1390 train_time:92266ms step_avg:143.94ms
step:652/1390 train_time:92416ms step_avg:143.95ms
step:653/1390 train_time:92565ms step_avg:143.96ms
step:654/1390 train_time:92718ms step_avg:143.97ms
step:655/1390 train_time:92866ms step_avg:143.98ms
step:656/1390 train_time:93017ms step_avg:143.99ms
step:657/1390 train_time:93165ms step_avg:143.99ms
step:658/1390 train_time:93316ms step_avg:144.01ms
step:659/1390 train_time:93466ms step_avg:144.01ms
step:660/1390 train_time:93616ms step_avg:144.03ms
step:661/1390 train_time:93766ms step_avg:144.03ms
step:662/1390 train_time:93915ms step_avg:144.04ms
step:663/1390 train_time:94064ms step_avg:144.05ms
step:664/1390 train_time:94217ms step_avg:144.06ms
step:665/1390 train_time:94366ms step_avg:144.07ms
step:666/1390 train_time:94517ms step_avg:144.08ms
step:667/1390 train_time:94665ms step_avg:144.09ms
step:668/1390 train_time:94817ms step_avg:144.10ms
step:669/1390 train_time:94966ms step_avg:144.11ms
step:670/1390 train_time:95118ms step_avg:144.12ms
step:671/1390 train_time:95268ms step_avg:144.13ms
step:672/1390 train_time:95420ms step_avg:144.14ms
step:673/1390 train_time:95571ms step_avg:144.15ms
step:674/1390 train_time:95722ms step_avg:144.16ms
step:675/1390 train_time:95873ms step_avg:144.17ms
step:676/1390 train_time:96024ms step_avg:144.18ms
step:677/1390 train_time:96174ms step_avg:144.19ms
step:678/1390 train_time:96324ms step_avg:144.20ms
step:679/1390 train_time:96474ms step_avg:144.21ms
step:680/1390 train_time:96625ms step_avg:144.22ms
step:681/1390 train_time:96775ms step_avg:144.23ms
step:682/1390 train_time:96925ms step_avg:144.23ms
step:683/1390 train_time:97077ms step_avg:144.25ms
step:684/1390 train_time:97227ms step_avg:144.25ms
step:685/1390 train_time:97377ms step_avg:144.26ms
step:686/1390 train_time:97526ms step_avg:144.27ms
step:687/1390 train_time:97677ms step_avg:144.28ms
step:688/1390 train_time:97829ms step_avg:144.29ms
step:689/1390 train_time:97980ms step_avg:144.30ms
step:690/1390 train_time:98132ms step_avg:144.31ms
step:691/1390 train_time:98282ms step_avg:144.32ms
step:692/1390 train_time:98432ms step_avg:144.33ms
step:693/1390 train_time:98581ms step_avg:144.34ms
step:694/1390 train_time:98731ms step_avg:144.34ms
step:695/1390 train_time:98881ms step_avg:144.35ms
step:696/1390 train_time:99033ms step_avg:144.36ms
step:697/1390 train_time:99183ms step_avg:144.37ms
step:698/1390 train_time:99332ms step_avg:144.38ms
step:699/1390 train_time:99482ms step_avg:144.39ms
step:700/1390 train_time:99632ms step_avg:144.39ms
step:701/1390 train_time:99781ms step_avg:144.40ms
step:702/1390 train_time:99933ms step_avg:144.41ms
step:703/1390 train_time:100084ms step_avg:144.42ms
step:704/1390 train_time:100234ms step_avg:144.43ms
step:705/1390 train_time:100386ms step_avg:144.44ms
step:706/1390 train_time:100540ms step_avg:144.45ms
step:707/1390 train_time:100689ms step_avg:144.46ms
step:708/1390 train_time:100839ms step_avg:144.47ms
step:709/1390 train_time:100988ms step_avg:144.48ms
step:710/1390 train_time:101141ms step_avg:144.49ms
step:711/1390 train_time:101292ms step_avg:144.50ms
step:712/1390 train_time:101443ms step_avg:144.51ms
step:713/1390 train_time:101595ms step_avg:144.52ms
step:714/1390 train_time:101745ms step_avg:144.52ms
step:715/1390 train_time:101896ms step_avg:144.53ms
step:716/1390 train_time:102046ms step_avg:144.54ms
step:717/1390 train_time:102198ms step_avg:144.55ms
step:718/1390 train_time:102347ms step_avg:144.56ms
step:719/1390 train_time:102498ms step_avg:144.57ms
step:720/1390 train_time:102648ms step_avg:144.57ms
step:721/1390 train_time:102799ms step_avg:144.58ms
step:722/1390 train_time:102950ms step_avg:144.59ms
step:723/1390 train_time:103102ms step_avg:144.60ms
step:724/1390 train_time:103254ms step_avg:144.61ms
step:725/1390 train_time:103405ms step_avg:144.62ms
step:726/1390 train_time:103558ms step_avg:144.63ms
step:727/1390 train_time:103712ms step_avg:144.65ms
step:728/1390 train_time:103863ms step_avg:144.66ms
step:729/1390 train_time:104015ms step_avg:144.67ms
step:730/1390 train_time:104167ms step_avg:144.68ms
step:731/1390 train_time:104319ms step_avg:144.69ms
step:732/1390 train_time:104471ms step_avg:144.70ms
step:733/1390 train_time:104622ms step_avg:144.71ms
step:734/1390 train_time:104773ms step_avg:144.71ms
step:735/1390 train_time:104927ms step_avg:144.73ms
step:736/1390 train_time:105079ms step_avg:144.74ms
step:737/1390 train_time:105231ms step_avg:144.75ms
step:738/1390 train_time:105382ms step_avg:144.76ms
step:739/1390 train_time:105534ms step_avg:144.77ms
step:740/1390 train_time:105687ms step_avg:144.78ms
step:741/1390 train_time:105839ms step_avg:144.79ms
step:742/1390 train_time:105991ms step_avg:144.80ms
step:743/1390 train_time:106141ms step_avg:144.80ms
step:744/1390 train_time:106292ms step_avg:144.81ms
step:745/1390 train_time:106447ms step_avg:144.83ms
step:746/1390 train_time:106599ms step_avg:144.84ms
step:747/1390 train_time:106751ms step_avg:144.85ms
step:748/1390 train_time:106904ms step_avg:144.86ms
step:749/1390 train_time:107056ms step_avg:144.87ms
step:750/1390 train_time:107207ms step_avg:144.87ms
step:750/1390 val_loss:3.7492 train_time:107277ms step_avg:144.97ms
step:751/1390 train_time:107360ms step_avg:144.88ms
step:752/1390 train_time:107513ms step_avg:144.90ms
step:753/1390 train_time:107664ms step_avg:144.90ms
step:754/1390 train_time:107814ms step_avg:144.91ms
step:755/1390 train_time:107965ms step_avg:144.92ms
step:756/1390 train_time:108116ms step_avg:144.93ms
step:757/1390 train_time:108270ms step_avg:144.94ms
step:758/1390 train_time:108423ms step_avg:144.95ms
step:759/1390 train_time:108575ms step_avg:144.96ms
step:760/1390 train_time:108725ms step_avg:144.97ms
step:761/1390 train_time:108925ms step_avg:145.04ms
step:762/1390 train_time:109075ms step_avg:145.05ms
step:763/1390 train_time:109226ms step_avg:145.05ms
step:764/1390 train_time:109376ms step_avg:145.06ms
step:765/1390 train_time:109526ms step_avg:145.07ms
step:766/1390 train_time:109680ms step_avg:145.08ms
step:767/1390 train_time:109834ms step_avg:145.09ms
step:768/1390 train_time:109988ms step_avg:145.10ms
step:769/1390 train_time:110141ms step_avg:145.11ms
step:770/1390 train_time:110291ms step_avg:145.12ms
step:771/1390 train_time:110442ms step_avg:145.13ms
step:772/1390 train_time:110592ms step_avg:145.13ms
step:773/1390 train_time:110744ms step_avg:145.14ms
step:774/1390 train_time:110897ms step_avg:145.15ms
step:775/1390 train_time:111050ms step_avg:145.16ms
step:776/1390 train_time:111202ms step_avg:145.17ms
step:777/1390 train_time:111354ms step_avg:145.18ms
step:778/1390 train_time:111506ms step_avg:145.19ms
step:779/1390 train_time:111656ms step_avg:145.20ms
step:780/1390 train_time:111810ms step_avg:145.21ms
step:781/1390 train_time:111962ms step_avg:145.22ms
step:782/1390 train_time:112113ms step_avg:145.22ms
step:783/1390 train_time:112264ms step_avg:145.23ms
step:784/1390 train_time:112417ms step_avg:145.24ms
step:785/1390 train_time:112570ms step_avg:145.25ms
step:786/1390 train_time:112721ms step_avg:145.26ms
step:787/1390 train_time:112872ms step_avg:145.27ms
step:788/1390 train_time:113025ms step_avg:145.28ms
step:789/1390 train_time:113176ms step_avg:145.28ms
step:790/1390 train_time:113325ms step_avg:145.29ms
step:791/1390 train_time:113477ms step_avg:145.30ms
step:792/1390 train_time:113629ms step_avg:145.31ms
step:793/1390 train_time:113780ms step_avg:145.31ms
step:794/1390 train_time:113933ms step_avg:145.32ms
step:795/1390 train_time:114088ms step_avg:145.33ms
step:796/1390 train_time:114239ms step_avg:145.34ms
step:797/1390 train_time:114390ms step_avg:145.35ms
step:798/1390 train_time:114545ms step_avg:145.36ms
step:799/1390 train_time:114699ms step_avg:145.37ms
step:800/1390 train_time:114851ms step_avg:145.38ms
step:801/1390 train_time:115002ms step_avg:145.39ms
step:802/1390 train_time:115154ms step_avg:145.40ms
step:803/1390 train_time:115306ms step_avg:145.40ms
step:804/1390 train_time:115457ms step_avg:145.41ms
step:805/1390 train_time:115611ms step_avg:145.42ms
step:806/1390 train_time:115763ms step_avg:145.43ms
step:807/1390 train_time:115912ms step_avg:145.44ms
step:808/1390 train_time:116064ms step_avg:145.44ms
step:809/1390 train_time:116214ms step_avg:145.45ms
step:810/1390 train_time:116367ms step_avg:145.46ms
step:811/1390 train_time:116519ms step_avg:145.47ms
step:812/1390 train_time:116671ms step_avg:145.48ms
step:813/1390 train_time:116821ms step_avg:145.48ms
step:814/1390 train_time:116972ms step_avg:145.49ms
step:815/1390 train_time:117123ms step_avg:145.49ms
step:816/1390 train_time:117278ms step_avg:145.51ms
step:817/1390 train_time:117429ms step_avg:145.51ms
step:818/1390 train_time:117582ms step_avg:145.52ms
step:819/1390 train_time:117732ms step_avg:145.53ms
step:820/1390 train_time:117885ms step_avg:145.54ms
step:821/1390 train_time:118036ms step_avg:145.54ms
step:822/1390 train_time:118189ms step_avg:145.55ms
step:823/1390 train_time:118339ms step_avg:145.56ms
step:824/1390 train_time:118491ms step_avg:145.57ms
step:825/1390 train_time:118646ms step_avg:145.58ms
step:826/1390 train_time:118799ms step_avg:145.59ms
step:827/1390 train_time:118952ms step_avg:145.60ms
step:828/1390 train_time:119105ms step_avg:145.61ms
step:829/1390 train_time:119256ms step_avg:145.61ms
step:830/1390 train_time:119409ms step_avg:145.62ms
step:831/1390 train_time:119562ms step_avg:145.63ms
step:832/1390 train_time:119715ms step_avg:145.64ms
step:833/1390 train_time:119868ms step_avg:145.65ms
step:834/1390 train_time:120021ms step_avg:145.66ms
step:835/1390 train_time:120175ms step_avg:145.67ms
step:836/1390 train_time:120329ms step_avg:145.68ms
step:837/1390 train_time:120480ms step_avg:145.68ms
step:838/1390 train_time:120632ms step_avg:145.69ms
step:839/1390 train_time:120786ms step_avg:145.70ms
step:840/1390 train_time:120938ms step_avg:145.71ms
step:841/1390 train_time:121093ms step_avg:145.72ms
step:842/1390 train_time:121245ms step_avg:145.73ms
step:843/1390 train_time:121397ms step_avg:145.73ms
step:844/1390 train_time:121550ms step_avg:145.74ms
step:845/1390 train_time:121701ms step_avg:145.75ms
step:846/1390 train_time:121855ms step_avg:145.76ms
step:847/1390 train_time:122010ms step_avg:145.77ms
step:848/1390 train_time:122161ms step_avg:145.78ms
step:849/1390 train_time:122314ms step_avg:145.79ms
step:850/1390 train_time:122470ms step_avg:145.80ms
step:851/1390 train_time:122623ms step_avg:145.81ms
step:852/1390 train_time:122775ms step_avg:145.81ms
step:853/1390 train_time:122925ms step_avg:145.82ms
step:854/1390 train_time:123077ms step_avg:145.83ms
step:855/1390 train_time:123228ms step_avg:145.83ms
step:856/1390 train_time:123381ms step_avg:145.84ms
step:857/1390 train_time:123533ms step_avg:145.85ms
step:858/1390 train_time:123689ms step_avg:145.86ms
step:859/1390 train_time:123842ms step_avg:145.87ms
step:860/1390 train_time:123995ms step_avg:145.88ms
step:861/1390 train_time:124150ms step_avg:145.89ms
step:862/1390 train_time:124303ms step_avg:145.90ms
step:863/1390 train_time:124457ms step_avg:145.90ms
step:864/1390 train_time:124610ms step_avg:145.91ms
step:865/1390 train_time:124761ms step_avg:145.92ms
step:866/1390 train_time:124917ms step_avg:145.93ms
step:867/1390 train_time:125070ms step_avg:145.94ms
step:868/1390 train_time:125221ms step_avg:145.94ms
step:869/1390 train_time:125372ms step_avg:145.95ms
step:870/1390 train_time:125528ms step_avg:145.96ms
step:871/1390 train_time:125682ms step_avg:145.97ms
step:872/1390 train_time:125833ms step_avg:145.98ms
step:873/1390 train_time:125985ms step_avg:145.98ms
step:874/1390 train_time:126138ms step_avg:145.99ms
step:875/1390 train_time:126292ms step_avg:146.00ms
step:875/1390 val_loss:3.6986 train_time:126362ms step_avg:146.08ms
step:876/1390 train_time:126444ms step_avg:146.01ms
step:877/1390 train_time:126601ms step_avg:146.02ms
step:878/1390 train_time:126753ms step_avg:146.03ms
step:879/1390 train_time:126907ms step_avg:146.04ms
step:880/1390 train_time:127058ms step_avg:146.04ms
step:881/1390 train_time:127209ms step_avg:146.05ms
step:882/1390 train_time:127365ms step_avg:146.06ms
step:883/1390 train_time:127518ms step_avg:146.07ms
step:884/1390 train_time:127671ms step_avg:146.08ms
step:885/1390 train_time:127823ms step_avg:146.08ms
step:886/1390 train_time:127978ms step_avg:146.09ms
step:887/1390 train_time:128130ms step_avg:146.10ms
step:888/1390 train_time:128283ms step_avg:146.11ms
step:889/1390 train_time:128437ms step_avg:146.12ms
step:890/1390 train_time:128590ms step_avg:146.12ms
step:891/1390 train_time:128742ms step_avg:146.13ms
step:892/1390 train_time:128898ms step_avg:146.14ms
step:893/1390 train_time:129049ms step_avg:146.15ms
step:894/1390 train_time:129202ms step_avg:146.16ms
step:895/1390 train_time:129358ms step_avg:146.17ms
step:896/1390 train_time:129510ms step_avg:146.17ms
step:897/1390 train_time:129663ms step_avg:146.18ms
step:898/1390 train_time:129817ms step_avg:146.19ms
step:899/1390 train_time:129970ms step_avg:146.20ms
step:900/1390 train_time:130122ms step_avg:146.20ms
step:901/1390 train_time:130275ms step_avg:146.21ms
step:902/1390 train_time:130425ms step_avg:146.22ms
step:903/1390 train_time:130580ms step_avg:146.23ms
step:904/1390 train_time:130734ms step_avg:146.24ms
step:905/1390 train_time:130886ms step_avg:146.24ms
step:906/1390 train_time:131041ms step_avg:146.25ms
step:907/1390 train_time:131196ms step_avg:146.26ms
step:908/1390 train_time:131350ms step_avg:146.27ms
step:909/1390 train_time:131504ms step_avg:146.28ms
step:910/1390 train_time:131664ms step_avg:146.29ms
step:911/1390 train_time:131816ms step_avg:146.30ms
step:912/1390 train_time:131969ms step_avg:146.31ms
step:913/1390 train_time:132125ms step_avg:146.32ms
step:914/1390 train_time:132277ms step_avg:146.32ms
step:915/1390 train_time:132431ms step_avg:146.33ms
step:916/1390 train_time:132585ms step_avg:146.34ms
step:917/1390 train_time:132737ms step_avg:146.35ms
step:918/1390 train_time:132892ms step_avg:146.36ms
step:919/1390 train_time:133049ms step_avg:146.37ms
step:920/1390 train_time:133200ms step_avg:146.37ms
step:921/1390 train_time:133354ms step_avg:146.38ms
step:922/1390 train_time:133511ms step_avg:146.39ms
step:923/1390 train_time:133662ms step_avg:146.40ms
step:924/1390 train_time:133816ms step_avg:146.41ms
step:925/1390 train_time:133970ms step_avg:146.42ms
step:926/1390 train_time:134123ms step_avg:146.42ms
step:927/1390 train_time:134276ms step_avg:146.43ms
step:928/1390 train_time:134432ms step_avg:146.44ms
step:929/1390 train_time:134587ms step_avg:146.45ms
step:930/1390 train_time:134743ms step_avg:146.46ms
step:931/1390 train_time:134896ms step_avg:146.47ms
step:932/1390 train_time:135049ms step_avg:146.47ms
step:933/1390 train_time:135204ms step_avg:146.48ms
step:934/1390 train_time:135357ms step_avg:146.49ms
step:935/1390 train_time:135513ms step_avg:146.50ms
step:936/1390 train_time:135669ms step_avg:146.51ms
step:937/1390 train_time:135827ms step_avg:146.52ms
step:938/1390 train_time:135983ms step_avg:146.53ms
step:939/1390 train_time:136138ms step_avg:146.54ms
step:940/1390 train_time:136293ms step_avg:146.55ms
step:941/1390 train_time:136447ms step_avg:146.56ms
step:942/1390 train_time:136599ms step_avg:146.57ms
step:943/1390 train_time:136756ms step_avg:146.58ms
step:944/1390 train_time:136914ms step_avg:146.59ms
step:945/1390 train_time:137069ms step_avg:146.60ms
step:946/1390 train_time:137227ms step_avg:146.61ms
step:947/1390 train_time:137381ms step_avg:146.62ms
step:948/1390 train_time:137534ms step_avg:146.62ms
step:949/1390 train_time:137690ms step_avg:146.64ms
step:950/1390 train_time:137846ms step_avg:146.64ms
step:951/1390 train_time:138043ms step_avg:146.70ms
step:952/1390 train_time:138195ms step_avg:146.70ms
step:953/1390 train_time:138352ms step_avg:146.71ms
step:954/1390 train_time:138506ms step_avg:146.72ms
step:955/1390 train_time:138658ms step_avg:146.73ms
step:956/1390 train_time:138813ms step_avg:146.74ms
step:957/1390 train_time:138969ms step_avg:146.75ms
step:958/1390 train_time:139128ms step_avg:146.76ms
step:959/1390 train_time:139284ms step_avg:146.77ms
step:960/1390 train_time:139440ms step_avg:146.78ms
step:961/1390 train_time:139593ms step_avg:146.79ms
step:962/1390 train_time:139745ms step_avg:146.79ms
step:963/1390 train_time:139904ms step_avg:146.80ms
step:964/1390 train_time:140058ms step_avg:146.81ms
step:965/1390 train_time:140211ms step_avg:146.82ms
step:966/1390 train_time:140364ms step_avg:146.82ms
step:967/1390 train_time:140517ms step_avg:146.83ms
step:968/1390 train_time:140671ms step_avg:146.84ms
step:969/1390 train_time:140827ms step_avg:146.85ms
step:970/1390 train_time:140979ms step_avg:146.85ms
step:971/1390 train_time:141133ms step_avg:146.86ms
step:972/1390 train_time:141286ms step_avg:146.87ms
step:973/1390 train_time:141439ms step_avg:146.87ms
step:974/1390 train_time:141595ms step_avg:146.88ms
step:975/1390 train_time:141751ms step_avg:146.89ms
step:976/1390 train_time:141908ms step_avg:146.90ms
step:977/1390 train_time:142062ms step_avg:146.91ms
step:978/1390 train_time:142216ms step_avg:146.92ms
step:979/1390 train_time:142369ms step_avg:146.92ms
step:980/1390 train_time:142523ms step_avg:146.93ms
step:981/1390 train_time:142675ms step_avg:146.94ms
step:982/1390 train_time:142830ms step_avg:146.94ms
step:983/1390 train_time:142985ms step_avg:146.95ms
step:984/1390 train_time:143138ms step_avg:146.96ms
step:985/1390 train_time:143292ms step_avg:146.97ms
step:986/1390 train_time:143450ms step_avg:146.98ms
step:987/1390 train_time:143602ms step_avg:146.98ms
step:988/1390 train_time:143756ms step_avg:146.99ms
step:989/1390 train_time:143909ms step_avg:147.00ms
step:990/1390 train_time:144064ms step_avg:147.00ms
step:991/1390 train_time:144218ms step_avg:147.01ms
step:992/1390 train_time:144377ms step_avg:147.02ms
step:993/1390 train_time:144539ms step_avg:147.04ms
step:994/1390 train_time:144691ms step_avg:147.04ms
step:995/1390 train_time:144844ms step_avg:147.05ms
step:996/1390 train_time:144997ms step_avg:147.06ms
step:997/1390 train_time:145151ms step_avg:147.06ms
step:998/1390 train_time:145303ms step_avg:147.07ms
step:999/1390 train_time:145457ms step_avg:147.07ms
step:1000/1390 train_time:145612ms step_avg:147.08ms
step:1000/1390 val_loss:3.6410 train_time:145682ms step_avg:147.15ms
step:1001/1390 train_time:145767ms step_avg:147.09ms
step:1002/1390 train_time:145924ms step_avg:147.10ms
step:1003/1390 train_time:146079ms step_avg:147.11ms
step:1004/1390 train_time:146235ms step_avg:147.12ms
step:1005/1390 train_time:146386ms step_avg:147.12ms
step:1006/1390 train_time:146538ms step_avg:147.13ms
step:1007/1390 train_time:146693ms step_avg:147.13ms
step:1008/1390 train_time:146850ms step_avg:147.14ms
step:1009/1390 train_time:147014ms step_avg:147.16ms
step:1010/1390 train_time:147168ms step_avg:147.17ms
step:1011/1390 train_time:147321ms step_avg:147.17ms
step:1012/1390 train_time:147474ms step_avg:147.18ms
step:1013/1390 train_time:147630ms step_avg:147.19ms
step:1014/1390 train_time:147783ms step_avg:147.19ms
step:1015/1390 train_time:147939ms step_avg:147.20ms
step:1016/1390 train_time:148094ms step_avg:147.21ms
step:1017/1390 train_time:148251ms step_avg:147.22ms
step:1018/1390 train_time:148405ms step_avg:147.23ms
step:1019/1390 train_time:148560ms step_avg:147.23ms
step:1020/1390 train_time:148715ms step_avg:147.24ms
step:1021/1390 train_time:148867ms step_avg:147.25ms
step:1022/1390 train_time:149022ms step_avg:147.25ms
step:1023/1390 train_time:149176ms step_avg:147.26ms
step:1024/1390 train_time:149329ms step_avg:147.27ms
step:1025/1390 train_time:149482ms step_avg:147.27ms
step:1026/1390 train_time:149636ms step_avg:147.28ms
step:1027/1390 train_time:149789ms step_avg:147.28ms
step:1028/1390 train_time:149943ms step_avg:147.29ms
step:1029/1390 train_time:150099ms step_avg:147.30ms
step:1030/1390 train_time:150256ms step_avg:147.31ms
step:1031/1390 train_time:150409ms step_avg:147.32ms
step:1032/1390 train_time:150565ms step_avg:147.32ms
step:1033/1390 train_time:150719ms step_avg:147.33ms
step:1034/1390 train_time:150875ms step_avg:147.34ms
step:1035/1390 train_time:151033ms step_avg:147.35ms
step:1036/1390 train_time:151192ms step_avg:147.36ms
step:1037/1390 train_time:151348ms step_avg:147.37ms
step:1038/1390 train_time:151503ms step_avg:147.38ms
step:1039/1390 train_time:151658ms step_avg:147.38ms
step:1040/1390 train_time:151811ms step_avg:147.39ms
step:1041/1390 train_time:151966ms step_avg:147.40ms
step:1042/1390 train_time:152118ms step_avg:147.40ms
step:1043/1390 train_time:152274ms step_avg:147.41ms
step:1044/1390 train_time:152431ms step_avg:147.42ms
step:1045/1390 train_time:152589ms step_avg:147.43ms
step:1046/1390 train_time:152744ms step_avg:147.44ms
step:1047/1390 train_time:152898ms step_avg:147.44ms
step:1048/1390 train_time:153057ms step_avg:147.45ms
step:1049/1390 train_time:153212ms step_avg:147.46ms
step:1050/1390 train_time:153372ms step_avg:147.47ms
step:1051/1390 train_time:153532ms step_avg:147.48ms
step:1052/1390 train_time:153686ms step_avg:147.49ms
step:1053/1390 train_time:153840ms step_avg:147.50ms
step:1054/1390 train_time:153996ms step_avg:147.51ms
step:1055/1390 train_time:154151ms step_avg:147.51ms
step:1056/1390 train_time:154306ms step_avg:147.52ms
step:1057/1390 train_time:154463ms step_avg:147.53ms
step:1058/1390 train_time:154621ms step_avg:147.54ms
step:1059/1390 train_time:154778ms step_avg:147.55ms
step:1060/1390 train_time:154935ms step_avg:147.56ms
step:1061/1390 train_time:155089ms step_avg:147.56ms
step:1062/1390 train_time:155245ms step_avg:147.57ms
step:1063/1390 train_time:155401ms step_avg:147.58ms
step:1064/1390 train_time:155556ms step_avg:147.59ms
step:1065/1390 train_time:155711ms step_avg:147.59ms
step:1066/1390 train_time:155869ms step_avg:147.60ms
step:1067/1390 train_time:156027ms step_avg:147.61ms
step:1068/1390 train_time:156181ms step_avg:147.62ms
step:1069/1390 train_time:156339ms step_avg:147.63ms
step:1070/1390 train_time:156494ms step_avg:147.64ms
step:1071/1390 train_time:156651ms step_avg:147.64ms
step:1072/1390 train_time:156805ms step_avg:147.65ms
step:1073/1390 train_time:156959ms step_avg:147.66ms
step:1074/1390 train_time:157113ms step_avg:147.66ms
step:1075/1390 train_time:157270ms step_avg:147.67ms
step:1076/1390 train_time:157426ms step_avg:147.68ms
step:1077/1390 train_time:157580ms step_avg:147.69ms
step:1078/1390 train_time:157739ms step_avg:147.70ms
step:1079/1390 train_time:157898ms step_avg:147.71ms
step:1080/1390 train_time:158054ms step_avg:147.71ms
step:1081/1390 train_time:158208ms step_avg:147.72ms
step:1082/1390 train_time:158363ms step_avg:147.73ms
step:1083/1390 train_time:158516ms step_avg:147.73ms
step:1084/1390 train_time:158675ms step_avg:147.74ms
step:1085/1390 train_time:158830ms step_avg:147.75ms
step:1086/1390 train_time:158987ms step_avg:147.76ms
step:1087/1390 train_time:159143ms step_avg:147.76ms
step:1088/1390 train_time:159298ms step_avg:147.77ms
step:1089/1390 train_time:159457ms step_avg:147.78ms
step:1090/1390 train_time:159616ms step_avg:147.79ms
step:1091/1390 train_time:159773ms step_avg:147.80ms
step:1092/1390 train_time:159926ms step_avg:147.81ms
step:1093/1390 train_time:160083ms step_avg:147.81ms
step:1094/1390 train_time:160238ms step_avg:147.82ms
step:1095/1390 train_time:160395ms step_avg:147.83ms
step:1096/1390 train_time:160554ms step_avg:147.84ms
step:1097/1390 train_time:160709ms step_avg:147.85ms
step:1098/1390 train_time:160865ms step_avg:147.85ms
step:1099/1390 train_time:161018ms step_avg:147.86ms
step:1100/1390 train_time:161175ms step_avg:147.87ms
step:1101/1390 train_time:161328ms step_avg:147.87ms
step:1102/1390 train_time:161484ms step_avg:147.88ms
step:1103/1390 train_time:161639ms step_avg:147.89ms
step:1104/1390 train_time:161793ms step_avg:147.89ms
step:1105/1390 train_time:161951ms step_avg:147.90ms
step:1106/1390 train_time:162105ms step_avg:147.91ms
step:1107/1390 train_time:162259ms step_avg:147.91ms
step:1108/1390 train_time:162416ms step_avg:147.92ms
step:1109/1390 train_time:162573ms step_avg:147.93ms
step:1110/1390 train_time:162728ms step_avg:147.93ms
step:1111/1390 train_time:162884ms step_avg:147.94ms
step:1112/1390 train_time:163039ms step_avg:147.95ms
step:1113/1390 train_time:163196ms step_avg:147.96ms
step:1114/1390 train_time:163352ms step_avg:147.96ms
step:1115/1390 train_time:163508ms step_avg:147.97ms
step:1116/1390 train_time:163662ms step_avg:147.98ms
step:1117/1390 train_time:163817ms step_avg:147.98ms
step:1118/1390 train_time:163977ms step_avg:147.99ms
step:1119/1390 train_time:164135ms step_avg:148.00ms
step:1120/1390 train_time:164291ms step_avg:148.01ms
step:1121/1390 train_time:164444ms step_avg:148.01ms
step:1122/1390 train_time:164599ms step_avg:148.02ms
step:1123/1390 train_time:164754ms step_avg:148.03ms
step:1124/1390 train_time:164911ms step_avg:148.03ms
step:1125/1390 train_time:165068ms step_avg:148.04ms
step:1125/1390 val_loss:3.5947 train_time:165140ms step_avg:148.11ms
step:1126/1390 train_time:165223ms step_avg:148.05ms
step:1127/1390 train_time:165382ms step_avg:148.06ms
step:1128/1390 train_time:165537ms step_avg:148.07ms
step:1129/1390 train_time:165696ms step_avg:148.08ms
step:1130/1390 train_time:165850ms step_avg:148.08ms
step:1131/1390 train_time:166008ms step_avg:148.09ms
step:1132/1390 train_time:166160ms step_avg:148.09ms
step:1133/1390 train_time:166315ms step_avg:148.10ms
step:1134/1390 train_time:166471ms step_avg:148.11ms
step:1135/1390 train_time:166627ms step_avg:148.11ms
step:1136/1390 train_time:166789ms step_avg:148.13ms
step:1137/1390 train_time:166942ms step_avg:148.13ms
step:1138/1390 train_time:167099ms step_avg:148.14ms
step:1139/1390 train_time:167255ms step_avg:148.14ms
step:1140/1390 train_time:167411ms step_avg:148.15ms
step:1141/1390 train_time:167615ms step_avg:148.20ms
step:1142/1390 train_time:167770ms step_avg:148.21ms
step:1143/1390 train_time:167930ms step_avg:148.22ms
step:1144/1390 train_time:168087ms step_avg:148.22ms
step:1145/1390 train_time:168239ms step_avg:148.23ms
step:1146/1390 train_time:168397ms step_avg:148.24ms
step:1147/1390 train_time:168555ms step_avg:148.25ms
step:1148/1390 train_time:168712ms step_avg:148.25ms
step:1149/1390 train_time:168867ms step_avg:148.26ms
step:1150/1390 train_time:169021ms step_avg:148.26ms
step:1151/1390 train_time:169177ms step_avg:148.27ms
step:1152/1390 train_time:169334ms step_avg:148.28ms
step:1153/1390 train_time:169492ms step_avg:148.29ms
step:1154/1390 train_time:169649ms step_avg:148.29ms
step:1155/1390 train_time:169806ms step_avg:148.30ms
step:1156/1390 train_time:169967ms step_avg:148.31ms
step:1157/1390 train_time:170125ms step_avg:148.32ms
step:1158/1390 train_time:170281ms step_avg:148.33ms
step:1159/1390 train_time:170438ms step_avg:148.34ms
step:1160/1390 train_time:170594ms step_avg:148.34ms
step:1161/1390 train_time:170750ms step_avg:148.35ms
step:1162/1390 train_time:170907ms step_avg:148.36ms
step:1163/1390 train_time:171063ms step_avg:148.36ms
step:1164/1390 train_time:171219ms step_avg:148.37ms
step:1165/1390 train_time:171374ms step_avg:148.38ms
step:1166/1390 train_time:171530ms step_avg:148.38ms
step:1167/1390 train_time:171686ms step_avg:148.39ms
step:1168/1390 train_time:171842ms step_avg:148.40ms
step:1169/1390 train_time:171999ms step_avg:148.40ms
step:1170/1390 train_time:172154ms step_avg:148.41ms
step:1171/1390 train_time:172311ms step_avg:148.42ms
step:1172/1390 train_time:172467ms step_avg:148.42ms
step:1173/1390 train_time:172624ms step_avg:148.43ms
step:1174/1390 train_time:172787ms step_avg:148.44ms
step:1175/1390 train_time:172949ms step_avg:148.45ms
step:1176/1390 train_time:173110ms step_avg:148.46ms
step:1177/1390 train_time:173273ms step_avg:148.48ms
step:1178/1390 train_time:173429ms step_avg:148.48ms
step:1179/1390 train_time:173587ms step_avg:148.49ms
step:1180/1390 train_time:173749ms step_avg:148.50ms
step:1181/1390 train_time:173905ms step_avg:148.51ms
step:1182/1390 train_time:174059ms step_avg:148.51ms
step:1183/1390 train_time:174215ms step_avg:148.52ms
step:1184/1390 train_time:174372ms step_avg:148.53ms
step:1185/1390 train_time:174532ms step_avg:148.54ms
step:1186/1390 train_time:174687ms step_avg:148.54ms
step:1187/1390 train_time:174852ms step_avg:148.56ms
step:1188/1390 train_time:175008ms step_avg:148.56ms
step:1189/1390 train_time:175168ms step_avg:148.57ms
step:1190/1390 train_time:175328ms step_avg:148.58ms
step:1191/1390 train_time:175486ms step_avg:148.59ms
step:1192/1390 train_time:175640ms step_avg:148.60ms
step:1193/1390 train_time:175796ms step_avg:148.60ms
step:1194/1390 train_time:175952ms step_avg:148.61ms
step:1195/1390 train_time:176108ms step_avg:148.61ms
step:1196/1390 train_time:176265ms step_avg:148.62ms
step:1197/1390 train_time:176423ms step_avg:148.63ms
step:1198/1390 train_time:176587ms step_avg:148.64ms
step:1199/1390 train_time:176743ms step_avg:148.65ms
step:1200/1390 train_time:176898ms step_avg:148.65ms
step:1201/1390 train_time:177053ms step_avg:148.66ms
step:1202/1390 train_time:177222ms step_avg:148.68ms
step:1203/1390 train_time:177382ms step_avg:148.69ms
step:1204/1390 train_time:177538ms step_avg:148.69ms
step:1205/1390 train_time:177693ms step_avg:148.70ms
step:1206/1390 train_time:177850ms step_avg:148.70ms
step:1207/1390 train_time:178006ms step_avg:148.71ms
step:1208/1390 train_time:178163ms step_avg:148.72ms
step:1209/1390 train_time:178320ms step_avg:148.72ms
step:1210/1390 train_time:178480ms step_avg:148.73ms
step:1211/1390 train_time:178636ms step_avg:148.74ms
step:1212/1390 train_time:178794ms step_avg:148.75ms
step:1213/1390 train_time:178949ms step_avg:148.75ms
step:1214/1390 train_time:179106ms step_avg:148.76ms
step:1215/1390 train_time:179262ms step_avg:148.76ms
step:1216/1390 train_time:179415ms step_avg:148.77ms
step:1217/1390 train_time:179573ms step_avg:148.78ms
step:1218/1390 train_time:179730ms step_avg:148.78ms
step:1219/1390 train_time:179885ms step_avg:148.79ms
step:1220/1390 train_time:180039ms step_avg:148.79ms
step:1221/1390 train_time:180194ms step_avg:148.80ms
step:1222/1390 train_time:180351ms step_avg:148.80ms
step:1223/1390 train_time:180509ms step_avg:148.81ms
step:1224/1390 train_time:180667ms step_avg:148.82ms
step:1225/1390 train_time:180823ms step_avg:148.83ms
step:1226/1390 train_time:180980ms step_avg:148.83ms
step:1227/1390 train_time:181138ms step_avg:148.84ms
step:1228/1390 train_time:181293ms step_avg:148.84ms
step:1229/1390 train_time:181448ms step_avg:148.85ms
step:1230/1390 train_time:181609ms step_avg:148.86ms
step:1231/1390 train_time:181767ms step_avg:148.87ms
step:1232/1390 train_time:181927ms step_avg:148.88ms
step:1233/1390 train_time:182082ms step_avg:148.88ms
step:1234/1390 train_time:182238ms step_avg:148.89ms
step:1235/1390 train_time:182393ms step_avg:148.89ms
step:1236/1390 train_time:182548ms step_avg:148.90ms
step:1237/1390 train_time:182703ms step_avg:148.90ms
step:1238/1390 train_time:182868ms step_avg:148.92ms
step:1239/1390 train_time:183026ms step_avg:148.92ms
step:1240/1390 train_time:183187ms step_avg:148.93ms
step:1241/1390 train_time:183350ms step_avg:148.94ms
step:1242/1390 train_time:183505ms step_avg:148.95ms
step:1243/1390 train_time:183665ms step_avg:148.96ms
step:1244/1390 train_time:183819ms step_avg:148.96ms
step:1245/1390 train_time:183975ms step_avg:148.97ms
step:1246/1390 train_time:184131ms step_avg:148.97ms
step:1247/1390 train_time:184289ms step_avg:148.98ms
step:1248/1390 train_time:184444ms step_avg:148.99ms
step:1249/1390 train_time:184600ms step_avg:148.99ms
step:1250/1390 train_time:184756ms step_avg:149.00ms
step:1250/1390 val_loss:3.5550 train_time:184831ms step_avg:149.06ms
step:1251/1390 train_time:184918ms step_avg:149.01ms
step:1252/1390 train_time:185073ms step_avg:149.01ms
step:1253/1390 train_time:185229ms step_avg:149.02ms
step:1254/1390 train_time:185385ms step_avg:149.02ms
step:1255/1390 train_time:185551ms step_avg:149.04ms
step:1256/1390 train_time:185708ms step_avg:149.04ms
step:1257/1390 train_time:185864ms step_avg:149.05ms
step:1258/1390 train_time:186026ms step_avg:149.06ms
step:1259/1390 train_time:186184ms step_avg:149.07ms
step:1260/1390 train_time:186338ms step_avg:149.07ms
step:1261/1390 train_time:186496ms step_avg:149.08ms
step:1262/1390 train_time:186657ms step_avg:149.09ms
step:1263/1390 train_time:186814ms step_avg:149.09ms
step:1264/1390 train_time:186969ms step_avg:149.10ms
step:1265/1390 train_time:187126ms step_avg:149.10ms
step:1266/1390 train_time:187284ms step_avg:149.11ms
step:1267/1390 train_time:187441ms step_avg:149.12ms
step:1268/1390 train_time:187601ms step_avg:149.13ms
step:1269/1390 train_time:187761ms step_avg:149.14ms
step:1270/1390 train_time:187920ms step_avg:149.14ms
step:1271/1390 train_time:188076ms step_avg:149.15ms
step:1272/1390 train_time:188231ms step_avg:149.15ms
step:1273/1390 train_time:188386ms step_avg:149.16ms
step:1274/1390 train_time:188542ms step_avg:149.16ms
step:1275/1390 train_time:188700ms step_avg:149.17ms
step:1276/1390 train_time:188854ms step_avg:149.17ms
step:1277/1390 train_time:189013ms step_avg:149.18ms
step:1278/1390 train_time:189168ms step_avg:149.19ms
step:1279/1390 train_time:189326ms step_avg:149.19ms
step:1280/1390 train_time:189489ms step_avg:149.20ms
step:1281/1390 train_time:189648ms step_avg:149.21ms
step:1282/1390 train_time:189804ms step_avg:149.22ms
step:1283/1390 train_time:189962ms step_avg:149.22ms
step:1284/1390 train_time:190123ms step_avg:149.23ms
step:1285/1390 train_time:190279ms step_avg:149.24ms
step:1286/1390 train_time:190433ms step_avg:149.24ms
step:1287/1390 train_time:190590ms step_avg:149.25ms
step:1288/1390 train_time:190749ms step_avg:149.26ms
step:1289/1390 train_time:190910ms step_avg:149.27ms
step:1290/1390 train_time:191071ms step_avg:149.27ms
step:1291/1390 train_time:191234ms step_avg:149.29ms
step:1292/1390 train_time:191392ms step_avg:149.29ms
step:1293/1390 train_time:191552ms step_avg:149.30ms
step:1294/1390 train_time:191709ms step_avg:149.31ms
step:1295/1390 train_time:191867ms step_avg:149.31ms
step:1296/1390 train_time:192026ms step_avg:149.32ms
step:1297/1390 train_time:192184ms step_avg:149.33ms
step:1298/1390 train_time:192341ms step_avg:149.33ms
step:1299/1390 train_time:192498ms step_avg:149.34ms
step:1300/1390 train_time:192654ms step_avg:149.34ms
step:1301/1390 train_time:192809ms step_avg:149.35ms
step:1302/1390 train_time:192967ms step_avg:149.36ms
step:1303/1390 train_time:193127ms step_avg:149.36ms
step:1304/1390 train_time:193289ms step_avg:149.37ms
step:1305/1390 train_time:193444ms step_avg:149.38ms
step:1306/1390 train_time:193606ms step_avg:149.39ms
step:1307/1390 train_time:193762ms step_avg:149.39ms
step:1308/1390 train_time:193919ms step_avg:149.40ms
step:1309/1390 train_time:194073ms step_avg:149.40ms
step:1310/1390 train_time:194229ms step_avg:149.41ms
step:1311/1390 train_time:194385ms step_avg:149.41ms
step:1312/1390 train_time:194540ms step_avg:149.42ms
step:1313/1390 train_time:194694ms step_avg:149.42ms
step:1314/1390 train_time:194854ms step_avg:149.43ms
step:1315/1390 train_time:195011ms step_avg:149.43ms
step:1316/1390 train_time:195165ms step_avg:149.44ms
step:1317/1390 train_time:195322ms step_avg:149.44ms
step:1318/1390 train_time:195485ms step_avg:149.45ms
step:1319/1390 train_time:195642ms step_avg:149.46ms
step:1320/1390 train_time:195801ms step_avg:149.47ms
step:1321/1390 train_time:195961ms step_avg:149.47ms
step:1322/1390 train_time:196121ms step_avg:149.48ms
step:1323/1390 train_time:196275ms step_avg:149.49ms
step:1324/1390 train_time:196431ms step_avg:149.49ms
step:1325/1390 train_time:196588ms step_avg:149.50ms
step:1326/1390 train_time:196750ms step_avg:149.51ms
step:1327/1390 train_time:196905ms step_avg:149.51ms
step:1328/1390 train_time:197063ms step_avg:149.52ms
step:1329/1390 train_time:197236ms step_avg:149.53ms
step:1330/1390 train_time:197396ms step_avg:149.54ms
step:1331/1390 train_time:197589ms step_avg:149.58ms
step:1332/1390 train_time:197751ms step_avg:149.58ms
step:1333/1390 train_time:197908ms step_avg:149.59ms
step:1334/1390 train_time:198063ms step_avg:149.59ms
step:1335/1390 train_time:198216ms step_avg:149.60ms
step:1336/1390 train_time:198378ms step_avg:149.61ms
step:1337/1390 train_time:198536ms step_avg:149.61ms
step:1338/1390 train_time:198693ms step_avg:149.62ms
step:1339/1390 train_time:198852ms step_avg:149.63ms
step:1340/1390 train_time:199009ms step_avg:149.63ms
step:1341/1390 train_time:199165ms step_avg:149.64ms
step:1342/1390 train_time:199328ms step_avg:149.65ms
step:1343/1390 train_time:199484ms step_avg:149.65ms
step:1344/1390 train_time:199641ms step_avg:149.66ms
step:1345/1390 train_time:199799ms step_avg:149.66ms
step:1346/1390 train_time:199956ms step_avg:149.67ms
step:1347/1390 train_time:200116ms step_avg:149.68ms
step:1348/1390 train_time:200272ms step_avg:149.68ms
step:1349/1390 train_time:200429ms step_avg:149.69ms
step:1350/1390 train_time:200585ms step_avg:149.69ms
step:1351/1390 train_time:200746ms step_avg:149.70ms
step:1352/1390 train_time:200910ms step_avg:149.71ms
step:1353/1390 train_time:201070ms step_avg:149.72ms
step:1354/1390 train_time:201230ms step_avg:149.72ms
step:1355/1390 train_time:201388ms step_avg:149.73ms
step:1356/1390 train_time:201545ms step_avg:149.74ms
step:1357/1390 train_time:201704ms step_avg:149.74ms
step:1358/1390 train_time:201866ms step_avg:149.75ms
step:1359/1390 train_time:202024ms step_avg:149.76ms
step:1360/1390 train_time:202186ms step_avg:149.77ms
step:1361/1390 train_time:202348ms step_avg:149.78ms
step:1362/1390 train_time:202510ms step_avg:149.79ms
step:1363/1390 train_time:202673ms step_avg:149.80ms
step:1364/1390 train_time:202831ms step_avg:149.80ms
step:1365/1390 train_time:202986ms step_avg:149.81ms
step:1366/1390 train_time:203144ms step_avg:149.81ms
step:1367/1390 train_time:203306ms step_avg:149.82ms
step:1368/1390 train_time:203466ms step_avg:149.83ms
step:1369/1390 train_time:203633ms step_avg:149.84ms
step:1370/1390 train_time:203794ms step_avg:149.85ms
step:1371/1390 train_time:203950ms step_avg:149.85ms
step:1372/1390 train_time:204114ms step_avg:149.86ms
step:1373/1390 train_time:204269ms step_avg:149.87ms
step:1374/1390 train_time:204430ms step_avg:149.88ms
step:1375/1390 train_time:204588ms step_avg:149.88ms
step:1375/1390 val_loss:3.5314 train_time:204660ms step_avg:149.93ms
step:1376/1390 train_time:204745ms step_avg:149.89ms
step:1377/1390 train_time:204903ms step_avg:149.89ms
step:1378/1390 train_time:205059ms step_avg:149.90ms
step:1379/1390 train_time:205219ms step_avg:149.90ms
step:1380/1390 train_time:205377ms step_avg:149.91ms
step:1381/1390 train_time:205540ms step_avg:149.92ms
step:1382/1390 train_time:205701ms step_avg:149.93ms
step:1383/1390 train_time:205859ms step_avg:149.93ms
step:1384/1390 train_time:206022ms step_avg:149.94ms
step:1385/1390 train_time:206178ms step_avg:149.95ms
step:1386/1390 train_time:206336ms step_avg:149.95ms
step:1387/1390 train_time:206500ms step_avg:149.96ms
step:1388/1390 train_time:206658ms step_avg:149.97ms
step:1389/1390 train_time:206820ms step_avg:149.98ms
step:1390/1390 train_time:206978ms step_avg:149.98ms
step:1390/1390 val_loss:3.5307 train_time:207049ms step_avg:150.04ms
peak memory consumption: 31565 MiB
