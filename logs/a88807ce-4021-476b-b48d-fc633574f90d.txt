import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import contextlib
from dataclasses import dataclass
from pathlib import Path

import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

torch._dynamo.config.capture_scalar_outputs = True

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params = list(params)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [
            {
                'params': [p for p in params if p.numel() == size],
                'update_buffer': [
                    torch.empty(size, device='cuda', dtype=torch.bfloat16)
                    for _ in range(self.world_size)
                ],
            }
            for size in sizes
        ]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            assert len(params) % self.world_size == 0
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                p = params[base_i + self.rank]
                g = p.grad
                assert g is not None
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.lerp_(g, 1 - momentum)
                g = g.lerp_(buf, momentum) if nesterov else buf
                g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                update_prev()
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.register_buffer('inv_freq', (1 / base) ** (torch.arange(0, dim, 2) / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            t = torch.arange(seq_len, device=x.device)
            freqs = torch.outer(t, self.inv_freq)
            self.seq_len_cached = seq_len
            self.cos_cached = freqs.cos()
            self.sin_cached = freqs.sin()
        cos, sin = self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]
        # apply_rotary_emb(x, cos, sin)
        x1, x2 = x.chunk(2, dim=3)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x, vi, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        v = self.lambdas[0] * v + self.lambdas[1] * vi.view_as(v) # @KoszarskyB & @Grad62304977
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc   = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config.model_dim, config.num_heads)
        self.mlp = MLP(config.model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, vi, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x = x + self.attn(norm(x), vi, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    num_layers : int = 12
    num_heads : int = 6 # head dim 128 suggested by @Grad62304977
    model_dim : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.num_layers = config.num_layers

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.embed = nn.Embedding(config.vocab_size, config.model_dim)
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = nn.Embedding(config.vocab_size, config.model_dim*self.num_encoder_layers)
        self.lm_head = CastedLinear(config.model_dim, config.vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, inputs, targets, sliding_window_size):

        BLOCK_SIZE = 128
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.reshape(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.reshape(-1, BLOCK_SIZE)[:, -1].contiguous()
        def document_sliding_window_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < sliding_window_size
            return causal_mask & document_mask & window_mask

        S = len(inputs)
        def create_sliding_window_causal_mask(S, sliding_window_size):
            kv_idx = block_idx = torch.arange(S // BLOCK_SIZE, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_mask = q_idx >= kv_idx
            document_mask = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            window_mask = q_idx - kv_idx < ((sliding_window_size + BLOCK_SIZE - 1) // BLOCK_SIZE)
            dense_mask = causal_mask & document_mask & window_mask
            dense_mask = dense_mask.to(torch.int32)
            num_blocks = dense_mask.sum(dim=-1).to(torch.int32)
            indices = torch.argsort(dense_mask, dim=-1, descending=True, stable=True).to(torch.int32)
            num_blocks = num_blocks[None, None, :].contiguous()
            indices = indices[None, None, :].contiguous()
            return BlockMask.from_kv_blocks(num_blocks, indices, BLOCK_SIZE=BLOCK_SIZE, mask_mod=document_sliding_window_causal)
        block_mask = create_sliding_window_causal_mask(S, sliding_window_size)

        # forward the GPT model itself
        x = self.embed(inputs[None]) # token embeddings of shape (b, t, model_dim)
        x = norm(x) # @Grad62304977
        x0 = x
        vi = self.value_embeds(inputs[None]).chunk(self.num_encoder_layers, dim=-1)

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, vi[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, vi[self.num_encoder_layers-1-i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(file: Path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32)
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    return int(header[2]) # number of tokens (claimed)

def _load_data_shard(path, num_tokens):
    with path.open("rb") as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True)
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy())
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, seq_len, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.seq_len = seq_len

        # glob files that match the pattern
        self.files = sorted(Path.cwd().glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        self.files_num_tokens = [_peek_data_shard(file) for file in self.files]
        assert min(self.files_num_tokens) >= num_processes * seq_len + 1
        self.total_num_tokens = sum(self.files_num_tokens)

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.seq_len
        self.tokens = _load_data_shard(self.files[self.current_shard], self.files_num_tokens[self.current_shard])

    def next_batch(self):
        batch_size = self.seq_len * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.seq_len+1]
        # host side async is sufficient;
        # no performance improvement was observed when introducing a separate stream.
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # inputs
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1480 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 600 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    log_norms: bool = False # log the norms of the weights and gradients
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running python {sys.version}")
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# calculate the number of steps to take in the val loop.
assert args.val_tokens % (args.sequence_length * ddp_world_size) == 0
val_steps = args.val_tokens // (args.sequence_length * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (ddp_world_size) == 0
train_accumulation_steps = args.batch_size // ddp_world_size

# load tokens
train_loader = DistributedDataLoader(args.input_bin, args.sequence_length, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, args.sequence_length, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.total_num_tokens} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.total_num_tokens} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
inputs_train, targets_train = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, num_layers=12, num_heads=6, model_dim=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.embed.weight, raw_model.value_embeds.weight], lr=0.6, betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight], lr=0.008, betas=(0.8, 0.95), fused=True)
params = list(raw_model.blocks.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True)
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

sliding_window_size = torch.tensor(64, dtype=torch.int32, device="cuda")
sw_size_prev = 64
# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the sliding window size over training in chunks of 64 from 64 -> 1792. By @fernbear.bsky.social
    frac_done = step / args.num_iterations # training progress
    sw_size = 64 * (((1 - frac_done) * 64 + frac_done * 1792) // 64)
    if sw_size != sw_size_prev:
        sliding_window_size.copy_(sw_size, non_blocking=True)
        sw_size_prev = sw_size

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch()
                val_loss += model(inputs_val, targets_val, sliding_window_size)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        if args.log_norms and master_process:
            print0("============== Weight norms: ==============")
            for name, p in model.named_parameters():
                if p.ndim != 2:
                    continue
                if "embed" in name:
                    l1_to_l2_norm = torch.norm(p.data.float(), p=2, dim=1).mean().item()
                    print0(f"W {name = } | {l1_to_l2_norm = :.7f}")
                else:
                    frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
                    spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
                    median_sv = torch.median(p.data.float().svd(compute_uv=False).S).item() + 1e-8
                    print0(f"W {name = } | {median_sv = :.7f} | {spectral_norm = :.7f} | {frobenius_norm = :.7f}")
            print0("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    # if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
    #     # stop the clock
    #     torch.cuda.synchronize()
    #     training_time_ms += 1000 * (time.perf_counter() - t0)
    #     # save the state of the training process
    #     log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
    #     torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
    #     # start the clock again
    #     torch.cuda.synchronize()
    #     t0 = time.perf_counter()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        ctx = model.no_sync() if i < train_accumulation_steps else contextlib.nullcontext()
        with ctx: # there's no need to sync gradients every accumulation step
            loss = model(inputs_train, targets_train, sliding_window_size)
            loss.backward()
            del loss
            inputs_train, targets_train = train_loader.next_batch()
    if train_accumulation_steps != 1:
        for p in model.parameters():
            p.grad /= train_accumulation_steps
    if args.log_norms and master_process and step != 0 and (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        print0("============== Gradient norms: ==============")
        log = dict()
        for name, p in model.named_parameters():
            if p.ndim != 2:
                continue
            if p.grad is None:
                continue
            log[name] = p.grad.detach()
            if "embed" in name:
                l1_to_l2_norm = torch.norm(p.grad.float(), p=2, dim=1).mean().item()
                print0(f"G {name = } | {l1_to_l2_norm = :.7f}")
            else:
                frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
                spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
                median_sv = torch.median(p.grad.float().svd(compute_uv=False).S).item() + 1e-8
                gram1 = p.grad.float() if p.size(0) <= p.size(1) else p.grad.float().T
                gram2 = gram1 @ gram1.T
                frobenius_norm2 = torch.linalg.norm(gram2, ord='fro').item()
                spectral_norm_est_t2 = frobenius_norm2 ** (0.5)
                gram4 = gram2 @ gram2.T
                frobenius_norm4 = torch.linalg.norm(gram4, ord='fro').item()
                spectral_norm_est_t4 = frobenius_norm4 ** (0.25)
                print0(f"G {name = } | {median_sv = :.7f} | {spectral_norm = :.7f} | {frobenius_norm = :.7f} | {spectral_norm_est_t2 = :.7f} | {spectral_norm_est_t4 = :.7f}")
        torch.save(log, "logs/%s/grad_state_step%06d.pt" % (run_id, step))
        print0("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer3.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

print0(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running pytorch 2.6.0.dev20241203+cu124 compiled for CUDA 12.4
nvidia-smi:
Wed Dec 11 13:26:04 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   24C    P0             69W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:0F:00.0 Off |                    0 |
| N/A   23C    P0             87W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:10:00.0 Off |                    0 |
| N/A   24C    P0            101W /  700W |      24MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:11:00.0 Off |                    0 |
| N/A   25C    P0             97W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 900000000 across 9 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1480 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1480 train_time:313934ms step_avg:nanms
step:2/1480 train_time:315307ms step_avg:nanms
step:3/1480 train_time:315586ms step_avg:nanms
step:4/1480 train_time:315855ms step_avg:nanms
step:5/1480 train_time:316132ms step_avg:nanms
step:6/1480 train_time:316411ms step_avg:nanms
step:7/1480 train_time:316685ms step_avg:nanms
step:8/1480 train_time:316961ms step_avg:nanms
step:9/1480 train_time:317239ms step_avg:nanms
step:10/1480 train_time:317516ms step_avg:nanms
step:11/1480 train_time:276ms step_avg:nanms
step:12/1480 train_time:554ms step_avg:nanms
step:13/1480 train_time:827ms step_avg:275.70ms
step:14/1480 train_time:1104ms step_avg:275.95ms
step:15/1480 train_time:1380ms step_avg:275.99ms
step:16/1480 train_time:1658ms step_avg:276.36ms
step:17/1480 train_time:1938ms step_avg:276.86ms
step:18/1480 train_time:2210ms step_avg:276.28ms
step:19/1480 train_time:2486ms step_avg:276.27ms
step:20/1480 train_time:2763ms step_avg:276.29ms
step:21/1480 train_time:3039ms step_avg:276.30ms
step:22/1480 train_time:3318ms step_avg:276.48ms
step:23/1480 train_time:3595ms step_avg:276.52ms
step:24/1480 train_time:3871ms step_avg:276.47ms
step:25/1480 train_time:4148ms step_avg:276.55ms
step:26/1480 train_time:4425ms step_avg:276.55ms
step:27/1480 train_time:4700ms step_avg:276.46ms
step:28/1480 train_time:4976ms step_avg:276.46ms
step:29/1480 train_time:5256ms step_avg:276.66ms
step:30/1480 train_time:5531ms step_avg:276.54ms
step:31/1480 train_time:5808ms step_avg:276.56ms
step:32/1480 train_time:6084ms step_avg:276.54ms
step:33/1480 train_time:6362ms step_avg:276.60ms
step:34/1480 train_time:6638ms step_avg:276.59ms
step:35/1480 train_time:6915ms step_avg:276.59ms
step:36/1480 train_time:7192ms step_avg:276.62ms
step:37/1480 train_time:7471ms step_avg:276.70ms
step:38/1480 train_time:7744ms step_avg:276.57ms
step:39/1480 train_time:8021ms step_avg:276.59ms
step:40/1480 train_time:8299ms step_avg:276.64ms
step:41/1480 train_time:8577ms step_avg:276.67ms
step:42/1480 train_time:8852ms step_avg:276.64ms
step:43/1480 train_time:9128ms step_avg:276.60ms
step:44/1480 train_time:9405ms step_avg:276.62ms
step:45/1480 train_time:9682ms step_avg:276.63ms
step:46/1480 train_time:9959ms step_avg:276.64ms
step:47/1480 train_time:10235ms step_avg:276.63ms
step:48/1480 train_time:10512ms step_avg:276.62ms
step:49/1480 train_time:10788ms step_avg:276.61ms
step:50/1480 train_time:11065ms step_avg:276.61ms
step:51/1480 train_time:11341ms step_avg:276.62ms
step:52/1480 train_time:11622ms step_avg:276.70ms
step:53/1480 train_time:11898ms step_avg:276.69ms
step:54/1480 train_time:12178ms step_avg:276.77ms
step:55/1480 train_time:12456ms step_avg:276.81ms
step:56/1480 train_time:12731ms step_avg:276.77ms
step:57/1480 train_time:13008ms step_avg:276.76ms
step:58/1480 train_time:13286ms step_avg:276.79ms
step:59/1480 train_time:13562ms step_avg:276.78ms
step:60/1480 train_time:13839ms step_avg:276.77ms
step:61/1480 train_time:14114ms step_avg:276.74ms
step:62/1480 train_time:14392ms step_avg:276.76ms
step:63/1480 train_time:14667ms step_avg:276.74ms
step:64/1480 train_time:14944ms step_avg:276.74ms
step:65/1480 train_time:15221ms step_avg:276.75ms
step:66/1480 train_time:15498ms step_avg:276.75ms
step:67/1480 train_time:15777ms step_avg:276.79ms
step:68/1480 train_time:16053ms step_avg:276.78ms
step:69/1480 train_time:16329ms step_avg:276.76ms
step:70/1480 train_time:16605ms step_avg:276.75ms
step:71/1480 train_time:16884ms step_avg:276.78ms
step:72/1480 train_time:17162ms step_avg:276.80ms
step:73/1480 train_time:17439ms step_avg:276.81ms
step:74/1480 train_time:17715ms step_avg:276.79ms
step:75/1480 train_time:17992ms step_avg:276.80ms
step:76/1480 train_time:18268ms step_avg:276.79ms
step:77/1480 train_time:18546ms step_avg:276.80ms
step:78/1480 train_time:18823ms step_avg:276.81ms
step:79/1480 train_time:19099ms step_avg:276.79ms
step:80/1480 train_time:19376ms step_avg:276.80ms
step:81/1480 train_time:19655ms step_avg:276.83ms
step:82/1480 train_time:19929ms step_avg:276.80ms
step:83/1480 train_time:20206ms step_avg:276.79ms
step:84/1480 train_time:20482ms step_avg:276.79ms
step:85/1480 train_time:20760ms step_avg:276.80ms
step:86/1480 train_time:21035ms step_avg:276.78ms
step:87/1480 train_time:21311ms step_avg:276.76ms
step:88/1480 train_time:21589ms step_avg:276.78ms
step:89/1480 train_time:21865ms step_avg:276.78ms
step:90/1480 train_time:22143ms step_avg:276.78ms
step:91/1480 train_time:22421ms step_avg:276.80ms
step:92/1480 train_time:22699ms step_avg:276.82ms
step:93/1480 train_time:22976ms step_avg:276.82ms
step:94/1480 train_time:23251ms step_avg:276.80ms
step:95/1480 train_time:23527ms step_avg:276.79ms
step:96/1480 train_time:23804ms step_avg:276.79ms
step:97/1480 train_time:24081ms step_avg:276.79ms
step:98/1480 train_time:24358ms step_avg:276.80ms
step:99/1480 train_time:24633ms step_avg:276.78ms
step:100/1480 train_time:24911ms step_avg:276.79ms
step:101/1480 train_time:25187ms step_avg:276.78ms
step:102/1480 train_time:25464ms step_avg:276.78ms
step:103/1480 train_time:25741ms step_avg:276.78ms
step:104/1480 train_time:26017ms step_avg:276.78ms
step:105/1480 train_time:26294ms step_avg:276.78ms
step:106/1480 train_time:26571ms step_avg:276.78ms
step:107/1480 train_time:26847ms step_avg:276.78ms
step:108/1480 train_time:27123ms step_avg:276.76ms
step:109/1480 train_time:27400ms step_avg:276.77ms
step:110/1480 train_time:27678ms step_avg:276.78ms
step:111/1480 train_time:27964ms step_avg:276.87ms
step:112/1480 train_time:28249ms step_avg:276.95ms
step:113/1480 train_time:28536ms step_avg:277.04ms
step:114/1480 train_time:28820ms step_avg:277.12ms
step:115/1480 train_time:29105ms step_avg:277.19ms
step:116/1480 train_time:29392ms step_avg:277.29ms
step:117/1480 train_time:29677ms step_avg:277.35ms
step:118/1480 train_time:29964ms step_avg:277.45ms
step:119/1480 train_time:30252ms step_avg:277.54ms
step:120/1480 train_time:30536ms step_avg:277.60ms
step:121/1480 train_time:30826ms step_avg:277.71ms
step:122/1480 train_time:31108ms step_avg:277.75ms
step:123/1480 train_time:31393ms step_avg:277.82ms
step:124/1480 train_time:31678ms step_avg:277.87ms
step:125/1480 train_time:31965ms step_avg:277.96ms
step:125/1480 val_loss:4.4196 train_time:31979ms step_avg:278.07ms
step:126/1480 train_time:32260ms step_avg:278.10ms
step:127/1480 train_time:32545ms step_avg:278.16ms
step:128/1480 train_time:32831ms step_avg:278.22ms
step:129/1480 train_time:33118ms step_avg:278.30ms
step:130/1480 train_time:33406ms step_avg:278.38ms
step:131/1480 train_time:33692ms step_avg:278.45ms
step:132/1480 train_time:33976ms step_avg:278.49ms
step:133/1480 train_time:34261ms step_avg:278.55ms
step:134/1480 train_time:34549ms step_avg:278.62ms
step:135/1480 train_time:34833ms step_avg:278.67ms
step:136/1480 train_time:35120ms step_avg:278.73ms
step:137/1480 train_time:35407ms step_avg:278.79ms
step:138/1480 train_time:35692ms step_avg:278.84ms
step:139/1480 train_time:35977ms step_avg:278.89ms
step:140/1480 train_time:36264ms step_avg:278.95ms
step:141/1480 train_time:36551ms step_avg:279.01ms
step:142/1480 train_time:36837ms step_avg:279.07ms
step:143/1480 train_time:37122ms step_avg:279.11ms
step:144/1480 train_time:37409ms step_avg:279.17ms
step:145/1480 train_time:37695ms step_avg:279.22ms
step:146/1480 train_time:37979ms step_avg:279.26ms
step:147/1480 train_time:38265ms step_avg:279.30ms
step:148/1480 train_time:38551ms step_avg:279.35ms
step:149/1480 train_time:38836ms step_avg:279.40ms
step:150/1480 train_time:39121ms step_avg:279.44ms
step:151/1480 train_time:39408ms step_avg:279.49ms
step:152/1480 train_time:39693ms step_avg:279.53ms
step:153/1480 train_time:39979ms step_avg:279.57ms
step:154/1480 train_time:40264ms step_avg:279.61ms
step:155/1480 train_time:40550ms step_avg:279.66ms
step:156/1480 train_time:40838ms step_avg:279.71ms
step:157/1480 train_time:41123ms step_avg:279.75ms
step:158/1480 train_time:41410ms step_avg:279.80ms
step:159/1480 train_time:41696ms step_avg:279.84ms
step:160/1480 train_time:41984ms step_avg:279.90ms
step:161/1480 train_time:42267ms step_avg:279.91ms
step:162/1480 train_time:42552ms step_avg:279.95ms
step:163/1480 train_time:42839ms step_avg:279.99ms
step:164/1480 train_time:43124ms step_avg:280.02ms
step:165/1480 train_time:43410ms step_avg:280.06ms
step:166/1480 train_time:43696ms step_avg:280.10ms
step:167/1480 train_time:43983ms step_avg:280.15ms
step:168/1480 train_time:44270ms step_avg:280.19ms
step:169/1480 train_time:44554ms step_avg:280.21ms
step:170/1480 train_time:44840ms step_avg:280.25ms
step:171/1480 train_time:45126ms step_avg:280.29ms
step:172/1480 train_time:45412ms step_avg:280.32ms
step:173/1480 train_time:45698ms step_avg:280.35ms
step:174/1480 train_time:45983ms step_avg:280.39ms
step:175/1480 train_time:46269ms step_avg:280.42ms
step:176/1480 train_time:46555ms step_avg:280.45ms
step:177/1480 train_time:46840ms step_avg:280.48ms
step:178/1480 train_time:47127ms step_avg:280.52ms
step:179/1480 train_time:47411ms step_avg:280.54ms
step:180/1480 train_time:47698ms step_avg:280.58ms
step:181/1480 train_time:47984ms step_avg:280.61ms
step:182/1480 train_time:48269ms step_avg:280.63ms
step:183/1480 train_time:48555ms step_avg:280.66ms
step:184/1480 train_time:48842ms step_avg:280.70ms
step:185/1480 train_time:49128ms step_avg:280.73ms
step:186/1480 train_time:49416ms step_avg:280.77ms
step:187/1480 train_time:49701ms step_avg:280.80ms
step:188/1480 train_time:49987ms step_avg:280.82ms
step:189/1480 train_time:50273ms step_avg:280.85ms
step:190/1480 train_time:50559ms step_avg:280.88ms
step:191/1480 train_time:50844ms step_avg:280.90ms
step:192/1480 train_time:51130ms step_avg:280.93ms
step:193/1480 train_time:51416ms step_avg:280.96ms
step:194/1480 train_time:51702ms step_avg:280.99ms
step:195/1480 train_time:51987ms step_avg:281.01ms
step:196/1480 train_time:52273ms step_avg:281.04ms
step:197/1480 train_time:52559ms step_avg:281.06ms
step:198/1480 train_time:52845ms step_avg:281.09ms
step:199/1480 train_time:53131ms step_avg:281.12ms
step:200/1480 train_time:53417ms step_avg:281.14ms
step:201/1480 train_time:53703ms step_avg:281.17ms
step:202/1480 train_time:53992ms step_avg:281.21ms
step:203/1480 train_time:54274ms step_avg:281.21ms
step:204/1480 train_time:54559ms step_avg:281.23ms
step:205/1480 train_time:54846ms step_avg:281.26ms
step:206/1480 train_time:55131ms step_avg:281.28ms
step:207/1480 train_time:55416ms step_avg:281.30ms
step:208/1480 train_time:55705ms step_avg:281.34ms
step:209/1480 train_time:55987ms step_avg:281.34ms
step:210/1480 train_time:56274ms step_avg:281.37ms
step:211/1480 train_time:56560ms step_avg:281.39ms
step:212/1480 train_time:56846ms step_avg:281.42ms
step:213/1480 train_time:57132ms step_avg:281.44ms
step:214/1480 train_time:57417ms step_avg:281.45ms
step:215/1480 train_time:57703ms step_avg:281.48ms
step:216/1480 train_time:57987ms step_avg:281.49ms
step:217/1480 train_time:58273ms step_avg:281.51ms
step:218/1480 train_time:58558ms step_avg:281.53ms
step:219/1480 train_time:58844ms step_avg:281.55ms
step:220/1480 train_time:59130ms step_avg:281.57ms
step:221/1480 train_time:59424ms step_avg:281.63ms
step:222/1480 train_time:59718ms step_avg:281.69ms
step:223/1480 train_time:60012ms step_avg:281.75ms
step:224/1480 train_time:60305ms step_avg:281.80ms
step:225/1480 train_time:60599ms step_avg:281.85ms
step:226/1480 train_time:60892ms step_avg:281.91ms
step:227/1480 train_time:61183ms step_avg:281.95ms
step:228/1480 train_time:61478ms step_avg:282.01ms
step:229/1480 train_time:61771ms step_avg:282.06ms
step:230/1480 train_time:62066ms step_avg:282.12ms
step:231/1480 train_time:62359ms step_avg:282.17ms
step:232/1480 train_time:62653ms step_avg:282.22ms
step:233/1480 train_time:62945ms step_avg:282.27ms
step:234/1480 train_time:63238ms step_avg:282.31ms
step:235/1480 train_time:63532ms step_avg:282.37ms
step:236/1480 train_time:63824ms step_avg:282.41ms
step:237/1480 train_time:64117ms step_avg:282.46ms
step:238/1480 train_time:64411ms step_avg:282.50ms
step:239/1480 train_time:64704ms step_avg:282.55ms
step:240/1480 train_time:64998ms step_avg:282.60ms
step:241/1480 train_time:65291ms step_avg:282.64ms
step:242/1480 train_time:65584ms step_avg:282.69ms
step:243/1480 train_time:65877ms step_avg:282.73ms
step:244/1480 train_time:66171ms step_avg:282.78ms
step:245/1480 train_time:66464ms step_avg:282.83ms
step:246/1480 train_time:66758ms step_avg:282.87ms
step:247/1480 train_time:67050ms step_avg:282.91ms
step:248/1480 train_time:67343ms step_avg:282.96ms
step:249/1480 train_time:67638ms step_avg:283.00ms
step:250/1480 train_time:67930ms step_avg:283.04ms
step:250/1480 val_loss:3.9920 train_time:67946ms step_avg:283.11ms
step:251/1480 train_time:68232ms step_avg:283.12ms
step:252/1480 train_time:68527ms step_avg:283.17ms
step:253/1480 train_time:68821ms step_avg:283.21ms
step:254/1480 train_time:69114ms step_avg:283.25ms
step:255/1480 train_time:69407ms step_avg:283.29ms
step:256/1480 train_time:69702ms step_avg:283.34ms
step:257/1480 train_time:69996ms step_avg:283.38ms
step:258/1480 train_time:70289ms step_avg:283.42ms
step:259/1480 train_time:70584ms step_avg:283.47ms
step:260/1480 train_time:70875ms step_avg:283.50ms
step:261/1480 train_time:71169ms step_avg:283.54ms
step:262/1480 train_time:71464ms step_avg:283.59ms
step:263/1480 train_time:71758ms step_avg:283.63ms
step:264/1480 train_time:72050ms step_avg:283.66ms
step:265/1480 train_time:72346ms step_avg:283.71ms
step:266/1480 train_time:72637ms step_avg:283.74ms
step:267/1480 train_time:72930ms step_avg:283.78ms
step:268/1480 train_time:73223ms step_avg:283.81ms
step:269/1480 train_time:73516ms step_avg:283.85ms
step:270/1480 train_time:73809ms step_avg:283.88ms
step:271/1480 train_time:74105ms step_avg:283.93ms
step:272/1480 train_time:74396ms step_avg:283.95ms
step:273/1480 train_time:74687ms step_avg:283.98ms
step:274/1480 train_time:74982ms step_avg:284.02ms
step:275/1480 train_time:75276ms step_avg:284.06ms
step:276/1480 train_time:75570ms step_avg:284.10ms
step:277/1480 train_time:75861ms step_avg:284.13ms
step:278/1480 train_time:76154ms step_avg:284.16ms
step:279/1480 train_time:76448ms step_avg:284.19ms
step:280/1480 train_time:76741ms step_avg:284.22ms
step:281/1480 train_time:77035ms step_avg:284.26ms
step:282/1480 train_time:77328ms step_avg:284.30ms
step:283/1480 train_time:77625ms step_avg:284.34ms
step:284/1480 train_time:77919ms step_avg:284.37ms
step:285/1480 train_time:78208ms step_avg:284.39ms
step:286/1480 train_time:78502ms step_avg:284.43ms
step:287/1480 train_time:78796ms step_avg:284.46ms
step:288/1480 train_time:79088ms step_avg:284.49ms
step:289/1480 train_time:79382ms step_avg:284.52ms
step:290/1480 train_time:79676ms step_avg:284.56ms
step:291/1480 train_time:79968ms step_avg:284.59ms
step:292/1480 train_time:80262ms step_avg:284.62ms
step:293/1480 train_time:80555ms step_avg:284.65ms
step:294/1480 train_time:80847ms step_avg:284.67ms
step:295/1480 train_time:81141ms step_avg:284.70ms
step:296/1480 train_time:81435ms step_avg:284.74ms
step:297/1480 train_time:81727ms step_avg:284.76ms
step:298/1480 train_time:82022ms step_avg:284.80ms
step:299/1480 train_time:82315ms step_avg:284.83ms
step:300/1480 train_time:82608ms step_avg:284.85ms
step:301/1480 train_time:82901ms step_avg:284.88ms
step:302/1480 train_time:83195ms step_avg:284.92ms
step:303/1480 train_time:83488ms step_avg:284.94ms
step:304/1480 train_time:83782ms step_avg:284.97ms
step:305/1480 train_time:84075ms step_avg:285.00ms
step:306/1480 train_time:84369ms step_avg:285.03ms
step:307/1480 train_time:84662ms step_avg:285.06ms
step:308/1480 train_time:84956ms step_avg:285.09ms
step:309/1480 train_time:85247ms step_avg:285.11ms
step:310/1480 train_time:85541ms step_avg:285.14ms
step:311/1480 train_time:85835ms step_avg:285.17ms
step:312/1480 train_time:86129ms step_avg:285.19ms
step:313/1480 train_time:86422ms step_avg:285.22ms
step:314/1480 train_time:86715ms step_avg:285.25ms
step:315/1480 train_time:87010ms step_avg:285.28ms
step:316/1480 train_time:87302ms step_avg:285.30ms
step:317/1480 train_time:87596ms step_avg:285.33ms
step:318/1480 train_time:87888ms step_avg:285.35ms
step:319/1480 train_time:88182ms step_avg:285.38ms
step:320/1480 train_time:88475ms step_avg:285.40ms
step:321/1480 train_time:88768ms step_avg:285.43ms
step:322/1480 train_time:89062ms step_avg:285.45ms
step:323/1480 train_time:89354ms step_avg:285.48ms
step:324/1480 train_time:89648ms step_avg:285.50ms
step:325/1480 train_time:89941ms step_avg:285.53ms
step:326/1480 train_time:90235ms step_avg:285.55ms
step:327/1480 train_time:90529ms step_avg:285.58ms
step:328/1480 train_time:90822ms step_avg:285.60ms
step:329/1480 train_time:91116ms step_avg:285.63ms
step:330/1480 train_time:91415ms step_avg:285.67ms
step:331/1480 train_time:91714ms step_avg:285.71ms
step:332/1480 train_time:92014ms step_avg:285.76ms
step:333/1480 train_time:92313ms step_avg:285.80ms
step:334/1480 train_time:92612ms step_avg:285.84ms
step:335/1480 train_time:92913ms step_avg:285.89ms
step:336/1480 train_time:93212ms step_avg:285.93ms
step:337/1480 train_time:93514ms step_avg:285.98ms
step:338/1480 train_time:93813ms step_avg:286.01ms
step:339/1480 train_time:94112ms step_avg:286.05ms
step:340/1480 train_time:94411ms step_avg:286.09ms
step:341/1480 train_time:94712ms step_avg:286.14ms
step:342/1480 train_time:95014ms step_avg:286.19ms
step:343/1480 train_time:95313ms step_avg:286.23ms
step:344/1480 train_time:95613ms step_avg:286.27ms
step:345/1480 train_time:95912ms step_avg:286.31ms
step:346/1480 train_time:96211ms step_avg:286.34ms
step:347/1480 train_time:96511ms step_avg:286.38ms
step:348/1480 train_time:96812ms step_avg:286.43ms
step:349/1480 train_time:97111ms step_avg:286.46ms
step:350/1480 train_time:97413ms step_avg:286.51ms
step:351/1480 train_time:97712ms step_avg:286.54ms
step:352/1480 train_time:98011ms step_avg:286.58ms
step:353/1480 train_time:98310ms step_avg:286.62ms
step:354/1480 train_time:98609ms step_avg:286.65ms
step:355/1480 train_time:98911ms step_avg:286.70ms
step:356/1480 train_time:99210ms step_avg:286.73ms
step:357/1480 train_time:99509ms step_avg:286.77ms
step:358/1480 train_time:99811ms step_avg:286.81ms
step:359/1480 train_time:100112ms step_avg:286.85ms
step:360/1480 train_time:100413ms step_avg:286.89ms
step:361/1480 train_time:100712ms step_avg:286.93ms
step:362/1480 train_time:101011ms step_avg:286.96ms
step:363/1480 train_time:101310ms step_avg:287.00ms
step:364/1480 train_time:101610ms step_avg:287.03ms
step:365/1480 train_time:101911ms step_avg:287.07ms
step:366/1480 train_time:102212ms step_avg:287.11ms
step:367/1480 train_time:102510ms step_avg:287.14ms
step:368/1480 train_time:102809ms step_avg:287.18ms
step:369/1480 train_time:103110ms step_avg:287.22ms
step:370/1480 train_time:103410ms step_avg:287.25ms
step:371/1480 train_time:103710ms step_avg:287.28ms
step:372/1480 train_time:104011ms step_avg:287.32ms
step:373/1480 train_time:104310ms step_avg:287.36ms
step:374/1480 train_time:104610ms step_avg:287.39ms
step:375/1480 train_time:104911ms step_avg:287.43ms
step:375/1480 val_loss:3.8031 train_time:104926ms step_avg:287.47ms
step:376/1480 train_time:105221ms step_avg:287.49ms
step:377/1480 train_time:105523ms step_avg:287.53ms
step:378/1480 train_time:105825ms step_avg:287.57ms
step:379/1480 train_time:106124ms step_avg:287.60ms
step:380/1480 train_time:106425ms step_avg:287.64ms
step:381/1480 train_time:106726ms step_avg:287.67ms
step:382/1480 train_time:107029ms step_avg:287.71ms
step:383/1480 train_time:107329ms step_avg:287.75ms
step:384/1480 train_time:107630ms step_avg:287.78ms
step:385/1480 train_time:107932ms step_avg:287.82ms
step:386/1480 train_time:108233ms step_avg:287.85ms
step:387/1480 train_time:108534ms step_avg:287.89ms
step:388/1480 train_time:108835ms step_avg:287.92ms
step:389/1480 train_time:109134ms step_avg:287.95ms
step:390/1480 train_time:109436ms step_avg:287.99ms
step:391/1480 train_time:109735ms step_avg:288.02ms
step:392/1480 train_time:110035ms step_avg:288.05ms
step:393/1480 train_time:110335ms step_avg:288.08ms
step:394/1480 train_time:110636ms step_avg:288.11ms
step:395/1480 train_time:110935ms step_avg:288.14ms
step:396/1480 train_time:111236ms step_avg:288.18ms
step:397/1480 train_time:111537ms step_avg:288.21ms
step:398/1480 train_time:111838ms step_avg:288.24ms
step:399/1480 train_time:112136ms step_avg:288.27ms
step:400/1480 train_time:112436ms step_avg:288.30ms
step:401/1480 train_time:112738ms step_avg:288.33ms
step:402/1480 train_time:113037ms step_avg:288.36ms
step:403/1480 train_time:113338ms step_avg:288.39ms
step:404/1480 train_time:113639ms step_avg:288.42ms
step:405/1480 train_time:113938ms step_avg:288.45ms
step:406/1480 train_time:114237ms step_avg:288.48ms
step:407/1480 train_time:114537ms step_avg:288.51ms
step:408/1480 train_time:114837ms step_avg:288.53ms
step:409/1480 train_time:115136ms step_avg:288.56ms
step:410/1480 train_time:115435ms step_avg:288.59ms
step:411/1480 train_time:115736ms step_avg:288.62ms
step:412/1480 train_time:116036ms step_avg:288.65ms
step:413/1480 train_time:116336ms step_avg:288.67ms
step:414/1480 train_time:116636ms step_avg:288.70ms
step:415/1480 train_time:116937ms step_avg:288.73ms
step:416/1480 train_time:117236ms step_avg:288.76ms
step:417/1480 train_time:117537ms step_avg:288.79ms
step:418/1480 train_time:117837ms step_avg:288.82ms
step:419/1480 train_time:118138ms step_avg:288.85ms
step:420/1480 train_time:118438ms step_avg:288.87ms
step:421/1480 train_time:118737ms step_avg:288.90ms
step:422/1480 train_time:119037ms step_avg:288.92ms
step:423/1480 train_time:119338ms step_avg:288.96ms
step:424/1480 train_time:119636ms step_avg:288.98ms
step:425/1480 train_time:119938ms step_avg:289.01ms
step:426/1480 train_time:120238ms step_avg:289.03ms
step:427/1480 train_time:120538ms step_avg:289.06ms
step:428/1480 train_time:120839ms step_avg:289.09ms
step:429/1480 train_time:121139ms step_avg:289.11ms
step:430/1480 train_time:121439ms step_avg:289.14ms
step:431/1480 train_time:121740ms step_avg:289.17ms
step:432/1480 train_time:122040ms step_avg:289.19ms
step:433/1480 train_time:122339ms step_avg:289.22ms
step:434/1480 train_time:122639ms step_avg:289.24ms
step:435/1480 train_time:122938ms step_avg:289.27ms
step:436/1480 train_time:123237ms step_avg:289.29ms
step:437/1480 train_time:123539ms step_avg:289.32ms
step:438/1480 train_time:123840ms step_avg:289.35ms
step:439/1480 train_time:124139ms step_avg:289.37ms
step:440/1480 train_time:124446ms step_avg:289.41ms
step:441/1480 train_time:124755ms step_avg:289.45ms
step:442/1480 train_time:125062ms step_avg:289.49ms
step:443/1480 train_time:125369ms step_avg:289.54ms
step:444/1480 train_time:125675ms step_avg:289.57ms
step:445/1480 train_time:125982ms step_avg:289.61ms
step:446/1480 train_time:126287ms step_avg:289.65ms
step:447/1480 train_time:126593ms step_avg:289.69ms
step:448/1480 train_time:126902ms step_avg:289.73ms
step:449/1480 train_time:127209ms step_avg:289.77ms
step:450/1480 train_time:127518ms step_avg:289.81ms
step:451/1480 train_time:127825ms step_avg:289.85ms
step:452/1480 train_time:128131ms step_avg:289.89ms
step:453/1480 train_time:128439ms step_avg:289.93ms
step:454/1480 train_time:128746ms step_avg:289.97ms
step:455/1480 train_time:129054ms step_avg:290.01ms
step:456/1480 train_time:129358ms step_avg:290.04ms
step:457/1480 train_time:129664ms step_avg:290.08ms
step:458/1480 train_time:129968ms step_avg:290.11ms
step:459/1480 train_time:130274ms step_avg:290.14ms
step:460/1480 train_time:130582ms step_avg:290.18ms
step:461/1480 train_time:130887ms step_avg:290.22ms
step:462/1480 train_time:131195ms step_avg:290.25ms
step:463/1480 train_time:131500ms step_avg:290.29ms
step:464/1480 train_time:131805ms step_avg:290.32ms
step:465/1480 train_time:132110ms step_avg:290.35ms
step:466/1480 train_time:132418ms step_avg:290.39ms
step:467/1480 train_time:132723ms step_avg:290.42ms
step:468/1480 train_time:133031ms step_avg:290.46ms
step:469/1480 train_time:133335ms step_avg:290.49ms
step:470/1480 train_time:133640ms step_avg:290.52ms
step:471/1480 train_time:133949ms step_avg:290.56ms
step:472/1480 train_time:134256ms step_avg:290.60ms
step:473/1480 train_time:134561ms step_avg:290.63ms
step:474/1480 train_time:134866ms step_avg:290.66ms
step:475/1480 train_time:135172ms step_avg:290.69ms
step:476/1480 train_time:135479ms step_avg:290.73ms
step:477/1480 train_time:135786ms step_avg:290.76ms
step:478/1480 train_time:136090ms step_avg:290.79ms
step:479/1480 train_time:136397ms step_avg:290.82ms
step:480/1480 train_time:136706ms step_avg:290.86ms
step:481/1480 train_time:137010ms step_avg:290.89ms
step:482/1480 train_time:137317ms step_avg:290.93ms
step:483/1480 train_time:137622ms step_avg:290.96ms
step:484/1480 train_time:137927ms step_avg:290.99ms
step:485/1480 train_time:138234ms step_avg:291.02ms
step:486/1480 train_time:138542ms step_avg:291.05ms
step:487/1480 train_time:138846ms step_avg:291.08ms
step:488/1480 train_time:139153ms step_avg:291.12ms
step:489/1480 train_time:139459ms step_avg:291.15ms
step:490/1480 train_time:139766ms step_avg:291.18ms
step:491/1480 train_time:140071ms step_avg:291.21ms
step:492/1480 train_time:140377ms step_avg:291.24ms
step:493/1480 train_time:140682ms step_avg:291.27ms
step:494/1480 train_time:140991ms step_avg:291.30ms
step:495/1480 train_time:141300ms step_avg:291.34ms
step:496/1480 train_time:141606ms step_avg:291.37ms
step:497/1480 train_time:141912ms step_avg:291.40ms
step:498/1480 train_time:142219ms step_avg:291.43ms
step:499/1480 train_time:142524ms step_avg:291.46ms
step:500/1480 train_time:142834ms step_avg:291.50ms
step:500/1480 val_loss:3.6827 train_time:142845ms step_avg:291.52ms
step:501/1480 train_time:143147ms step_avg:291.54ms
step:502/1480 train_time:143452ms step_avg:291.57ms
step:503/1480 train_time:143760ms step_avg:291.60ms
step:504/1480 train_time:144065ms step_avg:291.63ms
step:505/1480 train_time:144371ms step_avg:291.66ms
step:506/1480 train_time:144677ms step_avg:291.69ms
step:507/1480 train_time:144986ms step_avg:291.72ms
step:508/1480 train_time:145291ms step_avg:291.75ms
step:509/1480 train_time:145597ms step_avg:291.78ms
step:510/1480 train_time:145901ms step_avg:291.80ms
step:511/1480 train_time:146210ms step_avg:291.84ms
step:512/1480 train_time:146517ms step_avg:291.87ms
step:513/1480 train_time:146825ms step_avg:291.90ms
step:514/1480 train_time:147132ms step_avg:291.93ms
step:515/1480 train_time:147442ms step_avg:291.96ms
step:516/1480 train_time:147747ms step_avg:291.99ms
step:517/1480 train_time:148053ms step_avg:292.02ms
step:518/1480 train_time:148359ms step_avg:292.04ms
step:519/1480 train_time:148664ms step_avg:292.07ms
step:520/1480 train_time:148970ms step_avg:292.10ms
step:521/1480 train_time:149277ms step_avg:292.13ms
step:522/1480 train_time:149583ms step_avg:292.15ms
step:523/1480 train_time:149890ms step_avg:292.18ms
step:524/1480 train_time:150199ms step_avg:292.22ms
step:525/1480 train_time:150504ms step_avg:292.24ms
step:526/1480 train_time:150811ms step_avg:292.27ms
step:527/1480 train_time:151118ms step_avg:292.30ms
step:528/1480 train_time:151423ms step_avg:292.32ms
step:529/1480 train_time:151728ms step_avg:292.35ms
step:530/1480 train_time:152036ms step_avg:292.38ms
step:531/1480 train_time:152343ms step_avg:292.40ms
step:532/1480 train_time:152649ms step_avg:292.43ms
step:533/1480 train_time:152958ms step_avg:292.46ms
step:534/1480 train_time:153266ms step_avg:292.49ms
step:535/1480 train_time:153577ms step_avg:292.53ms
step:536/1480 train_time:153884ms step_avg:292.56ms
step:537/1480 train_time:154191ms step_avg:292.58ms
step:538/1480 train_time:154499ms step_avg:292.61ms
step:539/1480 train_time:154807ms step_avg:292.64ms
step:540/1480 train_time:155115ms step_avg:292.67ms
step:541/1480 train_time:155422ms step_avg:292.70ms
step:542/1480 train_time:155728ms step_avg:292.72ms
step:543/1480 train_time:156034ms step_avg:292.75ms
step:544/1480 train_time:156339ms step_avg:292.77ms
step:545/1480 train_time:156647ms step_avg:292.80ms
step:546/1480 train_time:156954ms step_avg:292.83ms
step:547/1480 train_time:157261ms step_avg:292.85ms
step:548/1480 train_time:157568ms step_avg:292.88ms
step:549/1480 train_time:157877ms step_avg:292.91ms
step:550/1480 train_time:158188ms step_avg:292.94ms
step:551/1480 train_time:158500ms step_avg:292.98ms
step:552/1480 train_time:158810ms step_avg:293.01ms
step:553/1480 train_time:159121ms step_avg:293.04ms
step:554/1480 train_time:159433ms step_avg:293.08ms
step:555/1480 train_time:159745ms step_avg:293.11ms
step:556/1480 train_time:160055ms step_avg:293.14ms
step:557/1480 train_time:160365ms step_avg:293.17ms
step:558/1480 train_time:160677ms step_avg:293.21ms
step:559/1480 train_time:160986ms step_avg:293.24ms
step:560/1480 train_time:161298ms step_avg:293.27ms
step:561/1480 train_time:161608ms step_avg:293.30ms
step:562/1480 train_time:161918ms step_avg:293.33ms
step:563/1480 train_time:162228ms step_avg:293.36ms
step:564/1480 train_time:162540ms step_avg:293.39ms
step:565/1480 train_time:162855ms step_avg:293.43ms
step:566/1480 train_time:163163ms step_avg:293.46ms
step:567/1480 train_time:163477ms step_avg:293.50ms
step:568/1480 train_time:163789ms step_avg:293.53ms
step:569/1480 train_time:164099ms step_avg:293.56ms
step:570/1480 train_time:164411ms step_avg:293.59ms
step:571/1480 train_time:164724ms step_avg:293.63ms
step:572/1480 train_time:165036ms step_avg:293.66ms
step:573/1480 train_time:165348ms step_avg:293.69ms
step:574/1480 train_time:165660ms step_avg:293.72ms
step:575/1480 train_time:165977ms step_avg:293.76ms
step:576/1480 train_time:166288ms step_avg:293.80ms
step:577/1480 train_time:166597ms step_avg:293.82ms
step:578/1480 train_time:166909ms step_avg:293.85ms
step:579/1480 train_time:167222ms step_avg:293.89ms
step:580/1480 train_time:167534ms step_avg:293.92ms
step:581/1480 train_time:167846ms step_avg:293.95ms
step:582/1480 train_time:168161ms step_avg:293.99ms
step:583/1480 train_time:168471ms step_avg:294.02ms
step:584/1480 train_time:168782ms step_avg:294.05ms
step:585/1480 train_time:169094ms step_avg:294.08ms
step:586/1480 train_time:169407ms step_avg:294.11ms
step:587/1480 train_time:169721ms step_avg:294.14ms
step:588/1480 train_time:170033ms step_avg:294.17ms
step:589/1480 train_time:170344ms step_avg:294.20ms
step:590/1480 train_time:170659ms step_avg:294.24ms
step:591/1480 train_time:170971ms step_avg:294.27ms
step:592/1480 train_time:171282ms step_avg:294.30ms
step:593/1480 train_time:171595ms step_avg:294.33ms
step:594/1480 train_time:171908ms step_avg:294.36ms
step:595/1480 train_time:172223ms step_avg:294.40ms
step:596/1480 train_time:172539ms step_avg:294.44ms
step:597/1480 train_time:172848ms step_avg:294.46ms
step:598/1480 train_time:173157ms step_avg:294.48ms
step:599/1480 train_time:173468ms step_avg:294.51ms
step:600/1480 train_time:173780ms step_avg:294.54ms
step:601/1480 train_time:174089ms step_avg:294.57ms
step:602/1480 train_time:174402ms step_avg:294.60ms
step:603/1480 train_time:174713ms step_avg:294.63ms
step:604/1480 train_time:175029ms step_avg:294.66ms
step:605/1480 train_time:175340ms step_avg:294.69ms
step:606/1480 train_time:175652ms step_avg:294.72ms
step:607/1480 train_time:175967ms step_avg:294.75ms
step:608/1480 train_time:176278ms step_avg:294.78ms
step:609/1480 train_time:176589ms step_avg:294.81ms
step:610/1480 train_time:176899ms step_avg:294.83ms
step:611/1480 train_time:177211ms step_avg:294.86ms
step:612/1480 train_time:177523ms step_avg:294.89ms
step:613/1480 train_time:177833ms step_avg:294.91ms
step:614/1480 train_time:178146ms step_avg:294.94ms
step:615/1480 train_time:178461ms step_avg:294.98ms
step:616/1480 train_time:178768ms step_avg:295.00ms
step:617/1480 train_time:179081ms step_avg:295.03ms
step:618/1480 train_time:179390ms step_avg:295.05ms
step:619/1480 train_time:179701ms step_avg:295.08ms
step:620/1480 train_time:180014ms step_avg:295.10ms
step:621/1480 train_time:180326ms step_avg:295.13ms
step:622/1480 train_time:180638ms step_avg:295.16ms
step:623/1480 train_time:180953ms step_avg:295.19ms
step:624/1480 train_time:181266ms step_avg:295.22ms
step:625/1480 train_time:181578ms step_avg:295.25ms
step:625/1480 val_loss:3.6015 train_time:181593ms step_avg:295.27ms
step:626/1480 train_time:181896ms step_avg:295.29ms
step:627/1480 train_time:182210ms step_avg:295.32ms
step:628/1480 train_time:182523ms step_avg:295.34ms
step:629/1480 train_time:182832ms step_avg:295.37ms
step:630/1480 train_time:183143ms step_avg:295.39ms
step:631/1480 train_time:183455ms step_avg:295.42ms
step:632/1480 train_time:183766ms step_avg:295.44ms
step:633/1480 train_time:184077ms step_avg:295.47ms
step:634/1480 train_time:184389ms step_avg:295.49ms
step:635/1480 train_time:184701ms step_avg:295.52ms
step:636/1480 train_time:185012ms step_avg:295.55ms
step:637/1480 train_time:185322ms step_avg:295.57ms
step:638/1480 train_time:185635ms step_avg:295.60ms
step:639/1480 train_time:185946ms step_avg:295.62ms
step:640/1480 train_time:186258ms step_avg:295.65ms
step:641/1480 train_time:186569ms step_avg:295.67ms
step:642/1480 train_time:186879ms step_avg:295.70ms
step:643/1480 train_time:187190ms step_avg:295.72ms
step:644/1480 train_time:187501ms step_avg:295.74ms
step:645/1480 train_time:187813ms step_avg:295.77ms
step:646/1480 train_time:188124ms step_avg:295.79ms
step:647/1480 train_time:188437ms step_avg:295.82ms
step:648/1480 train_time:188748ms step_avg:295.84ms
step:649/1480 train_time:189067ms step_avg:295.88ms
step:650/1480 train_time:189378ms step_avg:295.90ms
step:651/1480 train_time:189692ms step_avg:295.93ms
step:652/1480 train_time:190004ms step_avg:295.96ms
step:653/1480 train_time:190315ms step_avg:295.98ms
step:654/1480 train_time:190626ms step_avg:296.00ms
step:655/1480 train_time:190938ms step_avg:296.03ms
step:656/1480 train_time:191251ms step_avg:296.05ms
step:657/1480 train_time:191558ms step_avg:296.07ms
step:658/1480 train_time:191868ms step_avg:296.09ms
step:659/1480 train_time:192186ms step_avg:296.13ms
step:660/1480 train_time:192503ms step_avg:296.16ms
step:661/1480 train_time:192819ms step_avg:296.19ms
step:662/1480 train_time:193140ms step_avg:296.23ms
step:663/1480 train_time:193455ms step_avg:296.26ms
step:664/1480 train_time:193770ms step_avg:296.28ms
step:665/1480 train_time:194089ms step_avg:296.32ms
step:666/1480 train_time:194408ms step_avg:296.35ms
step:667/1480 train_time:194721ms step_avg:296.38ms
step:668/1480 train_time:195039ms step_avg:296.41ms
step:669/1480 train_time:195359ms step_avg:296.45ms
step:670/1480 train_time:195673ms step_avg:296.47ms
step:671/1480 train_time:195993ms step_avg:296.51ms
step:672/1480 train_time:196309ms step_avg:296.54ms
step:673/1480 train_time:196624ms step_avg:296.57ms
step:674/1480 train_time:196942ms step_avg:296.60ms
step:675/1480 train_time:197260ms step_avg:296.63ms
step:676/1480 train_time:197576ms step_avg:296.66ms
step:677/1480 train_time:197897ms step_avg:296.70ms
step:678/1480 train_time:198208ms step_avg:296.72ms
step:679/1480 train_time:198523ms step_avg:296.75ms
step:680/1480 train_time:198844ms step_avg:296.78ms
step:681/1480 train_time:199163ms step_avg:296.81ms
step:682/1480 train_time:199479ms step_avg:296.84ms
step:683/1480 train_time:199798ms step_avg:296.88ms
step:684/1480 train_time:200117ms step_avg:296.91ms
step:685/1480 train_time:200437ms step_avg:296.94ms
step:686/1480 train_time:200755ms step_avg:296.97ms
step:687/1480 train_time:201069ms step_avg:297.00ms
step:688/1480 train_time:201388ms step_avg:297.03ms
step:689/1480 train_time:201714ms step_avg:297.07ms
step:690/1480 train_time:202027ms step_avg:297.10ms
step:691/1480 train_time:202344ms step_avg:297.13ms
step:692/1480 train_time:202659ms step_avg:297.15ms
step:693/1480 train_time:202974ms step_avg:297.18ms
step:694/1480 train_time:203290ms step_avg:297.21ms
step:695/1480 train_time:203604ms step_avg:297.23ms
step:696/1480 train_time:203918ms step_avg:297.26ms
step:697/1480 train_time:204239ms step_avg:297.29ms
step:698/1480 train_time:204552ms step_avg:297.31ms
step:699/1480 train_time:204868ms step_avg:297.34ms
step:700/1480 train_time:205186ms step_avg:297.37ms
step:701/1480 train_time:205500ms step_avg:297.39ms
step:702/1480 train_time:205814ms step_avg:297.42ms
step:703/1480 train_time:206131ms step_avg:297.45ms
step:704/1480 train_time:206447ms step_avg:297.47ms
step:705/1480 train_time:206771ms step_avg:297.51ms
step:706/1480 train_time:207088ms step_avg:297.54ms
step:707/1480 train_time:207406ms step_avg:297.57ms
step:708/1480 train_time:207727ms step_avg:297.60ms
step:709/1480 train_time:208046ms step_avg:297.63ms
step:710/1480 train_time:208365ms step_avg:297.66ms
step:711/1480 train_time:208681ms step_avg:297.69ms
step:712/1480 train_time:208999ms step_avg:297.72ms
step:713/1480 train_time:209316ms step_avg:297.75ms
step:714/1480 train_time:209635ms step_avg:297.78ms
step:715/1480 train_time:209951ms step_avg:297.80ms
step:716/1480 train_time:210269ms step_avg:297.83ms
step:717/1480 train_time:210585ms step_avg:297.86ms
step:718/1480 train_time:210901ms step_avg:297.88ms
step:719/1480 train_time:211217ms step_avg:297.91ms
step:720/1480 train_time:211532ms step_avg:297.93ms
step:721/1480 train_time:211852ms step_avg:297.96ms
step:722/1480 train_time:212166ms step_avg:297.99ms
step:723/1480 train_time:212483ms step_avg:298.01ms
step:724/1480 train_time:212797ms step_avg:298.03ms
step:725/1480 train_time:213113ms step_avg:298.06ms
step:726/1480 train_time:213436ms step_avg:298.10ms
step:727/1480 train_time:213751ms step_avg:298.12ms
step:728/1480 train_time:214068ms step_avg:298.14ms
step:729/1480 train_time:214384ms step_avg:298.17ms
step:730/1480 train_time:214698ms step_avg:298.19ms
step:731/1480 train_time:215020ms step_avg:298.22ms
step:732/1480 train_time:215336ms step_avg:298.25ms
step:733/1480 train_time:215649ms step_avg:298.27ms
step:734/1480 train_time:215965ms step_avg:298.29ms
step:735/1480 train_time:216281ms step_avg:298.32ms
step:736/1480 train_time:216602ms step_avg:298.35ms
step:737/1480 train_time:216923ms step_avg:298.38ms
step:738/1480 train_time:217236ms step_avg:298.40ms
step:739/1480 train_time:217551ms step_avg:298.42ms
step:740/1480 train_time:217868ms step_avg:298.45ms
step:741/1480 train_time:218190ms step_avg:298.48ms
step:742/1480 train_time:218504ms step_avg:298.50ms
step:743/1480 train_time:218822ms step_avg:298.53ms
step:744/1480 train_time:219141ms step_avg:298.56ms
step:745/1480 train_time:219458ms step_avg:298.58ms
step:746/1480 train_time:219775ms step_avg:298.61ms
step:747/1480 train_time:220089ms step_avg:298.63ms
step:748/1480 train_time:220404ms step_avg:298.65ms
step:749/1480 train_time:220721ms step_avg:298.68ms
step:750/1480 train_time:221040ms step_avg:298.70ms
step:750/1480 val_loss:3.5484 train_time:221057ms step_avg:298.73ms
step:751/1480 train_time:221365ms step_avg:298.74ms
step:752/1480 train_time:221682ms step_avg:298.76ms
step:753/1480 train_time:221997ms step_avg:298.79ms
step:754/1480 train_time:222316ms step_avg:298.81ms
step:755/1480 train_time:222629ms step_avg:298.83ms
step:756/1480 train_time:222945ms step_avg:298.85ms
step:757/1480 train_time:223266ms step_avg:298.88ms
step:758/1480 train_time:223590ms step_avg:298.92ms
step:759/1480 train_time:223904ms step_avg:298.94ms
step:760/1480 train_time:224219ms step_avg:298.96ms
step:761/1480 train_time:224532ms step_avg:298.98ms
step:762/1480 train_time:224848ms step_avg:299.00ms
step:763/1480 train_time:225165ms step_avg:299.02ms
step:764/1480 train_time:225484ms step_avg:299.05ms
step:765/1480 train_time:225804ms step_avg:299.08ms
step:766/1480 train_time:226118ms step_avg:299.10ms
step:767/1480 train_time:226443ms step_avg:299.13ms
step:768/1480 train_time:226760ms step_avg:299.16ms
step:769/1480 train_time:227080ms step_avg:299.18ms
step:770/1480 train_time:227403ms step_avg:299.21ms
step:771/1480 train_time:227721ms step_avg:299.24ms
step:772/1480 train_time:228047ms step_avg:299.27ms
step:773/1480 train_time:228367ms step_avg:299.30ms
step:774/1480 train_time:228691ms step_avg:299.33ms
step:775/1480 train_time:229010ms step_avg:299.36ms
step:776/1480 train_time:229333ms step_avg:299.39ms
step:777/1480 train_time:229653ms step_avg:299.42ms
step:778/1480 train_time:229978ms step_avg:299.45ms
step:779/1480 train_time:230298ms step_avg:299.48ms
step:780/1480 train_time:230619ms step_avg:299.51ms
step:781/1480 train_time:230943ms step_avg:299.54ms
step:782/1480 train_time:231263ms step_avg:299.56ms
step:783/1480 train_time:231586ms step_avg:299.59ms
step:784/1480 train_time:231908ms step_avg:299.62ms
step:785/1480 train_time:232231ms step_avg:299.65ms
step:786/1480 train_time:232555ms step_avg:299.68ms
step:787/1480 train_time:232874ms step_avg:299.71ms
step:788/1480 train_time:233196ms step_avg:299.74ms
step:789/1480 train_time:233516ms step_avg:299.76ms
step:790/1480 train_time:233835ms step_avg:299.79ms
step:791/1480 train_time:234158ms step_avg:299.82ms
step:792/1480 train_time:234480ms step_avg:299.85ms
step:793/1480 train_time:234801ms step_avg:299.87ms
step:794/1480 train_time:235126ms step_avg:299.91ms
step:795/1480 train_time:235452ms step_avg:299.94ms
step:796/1480 train_time:235779ms step_avg:299.97ms
step:797/1480 train_time:236099ms step_avg:300.00ms
step:798/1480 train_time:236421ms step_avg:300.03ms
step:799/1480 train_time:236742ms step_avg:300.05ms
step:800/1480 train_time:237071ms step_avg:300.09ms
step:801/1480 train_time:237391ms step_avg:300.12ms
step:802/1480 train_time:237709ms step_avg:300.14ms
step:803/1480 train_time:238032ms step_avg:300.17ms
step:804/1480 train_time:238360ms step_avg:300.20ms
step:805/1480 train_time:238686ms step_avg:300.23ms
step:806/1480 train_time:239011ms step_avg:300.26ms
step:807/1480 train_time:239334ms step_avg:300.29ms
step:808/1480 train_time:239650ms step_avg:300.31ms
step:809/1480 train_time:239973ms step_avg:300.34ms
step:810/1480 train_time:240291ms step_avg:300.36ms
step:811/1480 train_time:240614ms step_avg:300.39ms
step:812/1480 train_time:240935ms step_avg:300.42ms
step:813/1480 train_time:241254ms step_avg:300.44ms
step:814/1480 train_time:241576ms step_avg:300.47ms
step:815/1480 train_time:241901ms step_avg:300.50ms
step:816/1480 train_time:242220ms step_avg:300.52ms
step:817/1480 train_time:242539ms step_avg:300.54ms
step:818/1480 train_time:242861ms step_avg:300.57ms
step:819/1480 train_time:243184ms step_avg:300.60ms
step:820/1480 train_time:243499ms step_avg:300.62ms
step:821/1480 train_time:243822ms step_avg:300.64ms
step:822/1480 train_time:244141ms step_avg:300.67ms
step:823/1480 train_time:244459ms step_avg:300.69ms
step:824/1480 train_time:244782ms step_avg:300.72ms
step:825/1480 train_time:245107ms step_avg:300.74ms
step:826/1480 train_time:245431ms step_avg:300.77ms
step:827/1480 train_time:245755ms step_avg:300.80ms
step:828/1480 train_time:246077ms step_avg:300.83ms
step:829/1480 train_time:246398ms step_avg:300.85ms
step:830/1480 train_time:246714ms step_avg:300.87ms
step:831/1480 train_time:247036ms step_avg:300.90ms
step:832/1480 train_time:247359ms step_avg:300.92ms
step:833/1480 train_time:247681ms step_avg:300.95ms
step:834/1480 train_time:248000ms step_avg:300.97ms
step:835/1480 train_time:248323ms step_avg:301.00ms
step:836/1480 train_time:248646ms step_avg:301.02ms
step:837/1480 train_time:248964ms step_avg:301.05ms
step:838/1480 train_time:249283ms step_avg:301.07ms
step:839/1480 train_time:249604ms step_avg:301.09ms
step:840/1480 train_time:249924ms step_avg:301.11ms
step:841/1480 train_time:250249ms step_avg:301.14ms
step:842/1480 train_time:250569ms step_avg:301.16ms
step:843/1480 train_time:250892ms step_avg:301.19ms
step:844/1480 train_time:251213ms step_avg:301.21ms
step:845/1480 train_time:251535ms step_avg:301.24ms
step:846/1480 train_time:251860ms step_avg:301.27ms
step:847/1480 train_time:252179ms step_avg:301.29ms
step:848/1480 train_time:252504ms step_avg:301.32ms
step:849/1480 train_time:252827ms step_avg:301.34ms
step:850/1480 train_time:253146ms step_avg:301.36ms
step:851/1480 train_time:253469ms step_avg:301.39ms
step:852/1480 train_time:253791ms step_avg:301.41ms
step:853/1480 train_time:254109ms step_avg:301.43ms
step:854/1480 train_time:254429ms step_avg:301.46ms
step:855/1480 train_time:254746ms step_avg:301.47ms
step:856/1480 train_time:255063ms step_avg:301.49ms
step:857/1480 train_time:255388ms step_avg:301.52ms
step:858/1480 train_time:255714ms step_avg:301.55ms
step:859/1480 train_time:256041ms step_avg:301.58ms
step:860/1480 train_time:256362ms step_avg:301.60ms
step:861/1480 train_time:256685ms step_avg:301.63ms
step:862/1480 train_time:257001ms step_avg:301.64ms
step:863/1480 train_time:257326ms step_avg:301.67ms
step:864/1480 train_time:257652ms step_avg:301.70ms
step:865/1480 train_time:257983ms step_avg:301.73ms
step:866/1480 train_time:258305ms step_avg:301.76ms
step:867/1480 train_time:258622ms step_avg:301.78ms
step:868/1480 train_time:258950ms step_avg:301.81ms
step:869/1480 train_time:259272ms step_avg:301.83ms
step:870/1480 train_time:259595ms step_avg:301.85ms
step:871/1480 train_time:259916ms step_avg:301.88ms
step:872/1480 train_time:260236ms step_avg:301.90ms
step:873/1480 train_time:260556ms step_avg:301.92ms
step:874/1480 train_time:260873ms step_avg:301.94ms
step:875/1480 train_time:261195ms step_avg:301.96ms
step:875/1480 val_loss:3.5011 train_time:261211ms step_avg:301.98ms
step:876/1480 train_time:261526ms step_avg:301.99ms
step:877/1480 train_time:261847ms step_avg:302.02ms
step:878/1480 train_time:262169ms step_avg:302.04ms
step:879/1480 train_time:262493ms step_avg:302.06ms
step:880/1480 train_time:262820ms step_avg:302.09ms
step:881/1480 train_time:263143ms step_avg:302.12ms
step:882/1480 train_time:263462ms step_avg:302.14ms
step:883/1480 train_time:263790ms step_avg:302.16ms
step:884/1480 train_time:264113ms step_avg:302.19ms
step:885/1480 train_time:264436ms step_avg:302.21ms
step:886/1480 train_time:264766ms step_avg:302.24ms
step:887/1480 train_time:265095ms step_avg:302.28ms
step:888/1480 train_time:265416ms step_avg:302.30ms
step:889/1480 train_time:265744ms step_avg:302.33ms
step:890/1480 train_time:266081ms step_avg:302.37ms
step:891/1480 train_time:266404ms step_avg:302.39ms
step:892/1480 train_time:266729ms step_avg:302.41ms
step:893/1480 train_time:267059ms step_avg:302.44ms
step:894/1480 train_time:267383ms step_avg:302.47ms
step:895/1480 train_time:267705ms step_avg:302.49ms
step:896/1480 train_time:268038ms step_avg:302.53ms
step:897/1480 train_time:268366ms step_avg:302.55ms
step:898/1480 train_time:268694ms step_avg:302.58ms
step:899/1480 train_time:269021ms step_avg:302.61ms
step:900/1480 train_time:269346ms step_avg:302.64ms
step:901/1480 train_time:269667ms step_avg:302.66ms
step:902/1480 train_time:269994ms step_avg:302.68ms
step:903/1480 train_time:270316ms step_avg:302.71ms
step:904/1480 train_time:270645ms step_avg:302.73ms
step:905/1480 train_time:270974ms step_avg:302.76ms
step:906/1480 train_time:271304ms step_avg:302.80ms
step:907/1480 train_time:271624ms step_avg:302.81ms
step:908/1480 train_time:271961ms step_avg:302.85ms
step:909/1480 train_time:272288ms step_avg:302.88ms
step:910/1480 train_time:272615ms step_avg:302.91ms
step:911/1480 train_time:272954ms step_avg:302.95ms
step:912/1480 train_time:273274ms step_avg:302.96ms
step:913/1480 train_time:273599ms step_avg:302.99ms
step:914/1480 train_time:273929ms step_avg:303.02ms
step:915/1480 train_time:274254ms step_avg:303.04ms
step:916/1480 train_time:274588ms step_avg:303.08ms
step:917/1480 train_time:274910ms step_avg:303.10ms
step:918/1480 train_time:275243ms step_avg:303.13ms
step:919/1480 train_time:275575ms step_avg:303.16ms
step:920/1480 train_time:275905ms step_avg:303.19ms
step:921/1480 train_time:276237ms step_avg:303.22ms
step:922/1480 train_time:276561ms step_avg:303.25ms
step:923/1480 train_time:276886ms step_avg:303.27ms
step:924/1480 train_time:277209ms step_avg:303.29ms
step:925/1480 train_time:277531ms step_avg:303.31ms
step:926/1480 train_time:277862ms step_avg:303.34ms
step:927/1480 train_time:278188ms step_avg:303.37ms
step:928/1480 train_time:278515ms step_avg:303.39ms
step:929/1480 train_time:278835ms step_avg:303.41ms
step:930/1480 train_time:279166ms step_avg:303.44ms
step:931/1480 train_time:279488ms step_avg:303.46ms
step:932/1480 train_time:279810ms step_avg:303.48ms
step:933/1480 train_time:280135ms step_avg:303.51ms
step:934/1480 train_time:280466ms step_avg:303.53ms
step:935/1480 train_time:280788ms step_avg:303.55ms
step:936/1480 train_time:281119ms step_avg:303.58ms
step:937/1480 train_time:281453ms step_avg:303.62ms
step:938/1480 train_time:281779ms step_avg:303.64ms
step:939/1480 train_time:282105ms step_avg:303.67ms
step:940/1480 train_time:282428ms step_avg:303.69ms
step:941/1480 train_time:282764ms step_avg:303.72ms
step:942/1480 train_time:283093ms step_avg:303.75ms
step:943/1480 train_time:283426ms step_avg:303.78ms
step:944/1480 train_time:283751ms step_avg:303.80ms
step:945/1480 train_time:284079ms step_avg:303.83ms
step:946/1480 train_time:284412ms step_avg:303.86ms
step:947/1480 train_time:284736ms step_avg:303.88ms
step:948/1480 train_time:285065ms step_avg:303.91ms
step:949/1480 train_time:285391ms step_avg:303.93ms
step:950/1480 train_time:285722ms step_avg:303.96ms
step:951/1480 train_time:286048ms step_avg:303.98ms
step:952/1480 train_time:286375ms step_avg:304.01ms
step:953/1480 train_time:286693ms step_avg:304.02ms
step:954/1480 train_time:287020ms step_avg:304.05ms
step:955/1480 train_time:287355ms step_avg:304.08ms
step:956/1480 train_time:287682ms step_avg:304.10ms
step:957/1480 train_time:288002ms step_avg:304.12ms
step:958/1480 train_time:288332ms step_avg:304.15ms
step:959/1480 train_time:288664ms step_avg:304.18ms
step:960/1480 train_time:288994ms step_avg:304.20ms
step:961/1480 train_time:289320ms step_avg:304.23ms
step:962/1480 train_time:289653ms step_avg:304.26ms
step:963/1480 train_time:289982ms step_avg:304.28ms
step:964/1480 train_time:290302ms step_avg:304.30ms
step:965/1480 train_time:290631ms step_avg:304.33ms
step:966/1480 train_time:290961ms step_avg:304.35ms
step:967/1480 train_time:291281ms step_avg:304.37ms
step:968/1480 train_time:291609ms step_avg:304.39ms
step:969/1480 train_time:291933ms step_avg:304.41ms
step:970/1480 train_time:292255ms step_avg:304.43ms
step:971/1480 train_time:292584ms step_avg:304.46ms
step:972/1480 train_time:292910ms step_avg:304.48ms
step:973/1480 train_time:293240ms step_avg:304.51ms
step:974/1480 train_time:293560ms step_avg:304.52ms
step:975/1480 train_time:293883ms step_avg:304.54ms
step:976/1480 train_time:294209ms step_avg:304.56ms
step:977/1480 train_time:294537ms step_avg:304.59ms
step:978/1480 train_time:294860ms step_avg:304.61ms
step:979/1480 train_time:295183ms step_avg:304.63ms
step:980/1480 train_time:295508ms step_avg:304.65ms
step:981/1480 train_time:295829ms step_avg:304.66ms
step:982/1480 train_time:296149ms step_avg:304.68ms
step:983/1480 train_time:296466ms step_avg:304.69ms
step:984/1480 train_time:296792ms step_avg:304.71ms
step:985/1480 train_time:297127ms step_avg:304.75ms
step:986/1480 train_time:297456ms step_avg:304.77ms
step:987/1480 train_time:297780ms step_avg:304.79ms
step:988/1480 train_time:298109ms step_avg:304.81ms
step:989/1480 train_time:298433ms step_avg:304.83ms
step:990/1480 train_time:298762ms step_avg:304.86ms
step:991/1480 train_time:299097ms step_avg:304.89ms
step:992/1480 train_time:299438ms step_avg:304.93ms
step:993/1480 train_time:299760ms step_avg:304.94ms
step:994/1480 train_time:300088ms step_avg:304.97ms
step:995/1480 train_time:300435ms step_avg:305.01ms
step:996/1480 train_time:300762ms step_avg:305.03ms
step:997/1480 train_time:301085ms step_avg:305.05ms
step:998/1480 train_time:301412ms step_avg:305.07ms
step:999/1480 train_time:301738ms step_avg:305.09ms
step:1000/1480 train_time:302067ms step_avg:305.12ms
step:1000/1480 val_loss:3.4381 train_time:302083ms step_avg:305.13ms
step:1001/1480 train_time:302398ms step_avg:305.14ms
step:1002/1480 train_time:302728ms step_avg:305.17ms
step:1003/1480 train_time:303052ms step_avg:305.19ms
step:1004/1480 train_time:303382ms step_avg:305.21ms
step:1005/1480 train_time:303719ms step_avg:305.24ms
step:1006/1480 train_time:304057ms step_avg:305.28ms
step:1007/1480 train_time:304385ms step_avg:305.30ms
step:1008/1480 train_time:304716ms step_avg:305.33ms
step:1009/1480 train_time:305048ms step_avg:305.35ms
step:1010/1480 train_time:305377ms step_avg:305.38ms
step:1011/1480 train_time:305723ms step_avg:305.42ms
step:1012/1480 train_time:306052ms step_avg:305.44ms
step:1013/1480 train_time:306378ms step_avg:305.46ms
step:1014/1480 train_time:306708ms step_avg:305.49ms
step:1015/1480 train_time:307031ms step_avg:305.50ms
step:1016/1480 train_time:307358ms step_avg:305.52ms
step:1017/1480 train_time:307686ms step_avg:305.55ms
step:1018/1480 train_time:308022ms step_avg:305.58ms
step:1019/1480 train_time:308354ms step_avg:305.60ms
step:1020/1480 train_time:308678ms step_avg:305.62ms
step:1021/1480 train_time:309006ms step_avg:305.64ms
step:1022/1480 train_time:309333ms step_avg:305.67ms
step:1023/1480 train_time:309660ms step_avg:305.69ms
step:1024/1480 train_time:309990ms step_avg:305.71ms
step:1025/1480 train_time:310317ms step_avg:305.73ms
step:1026/1480 train_time:310649ms step_avg:305.76ms
step:1027/1480 train_time:310979ms step_avg:305.78ms
step:1028/1480 train_time:311304ms step_avg:305.80ms
step:1029/1480 train_time:311634ms step_avg:305.82ms
step:1030/1480 train_time:311977ms step_avg:305.86ms
step:1031/1480 train_time:312314ms step_avg:305.89ms
step:1032/1480 train_time:312637ms step_avg:305.91ms
step:1033/1480 train_time:312966ms step_avg:305.93ms
step:1034/1480 train_time:313303ms step_avg:305.96ms
step:1035/1480 train_time:313634ms step_avg:305.98ms
step:1036/1480 train_time:313966ms step_avg:306.01ms
step:1037/1480 train_time:314302ms step_avg:306.04ms
step:1038/1480 train_time:314632ms step_avg:306.06ms
step:1039/1480 train_time:314958ms step_avg:306.08ms
step:1040/1480 train_time:315283ms step_avg:306.10ms
step:1041/1480 train_time:315612ms step_avg:306.12ms
step:1042/1480 train_time:315940ms step_avg:306.14ms
step:1043/1480 train_time:316275ms step_avg:306.17ms
step:1044/1480 train_time:316608ms step_avg:306.20ms
step:1045/1480 train_time:316934ms step_avg:306.22ms
step:1046/1480 train_time:317261ms step_avg:306.24ms
step:1047/1480 train_time:317591ms step_avg:306.26ms
step:1048/1480 train_time:317917ms step_avg:306.28ms
step:1049/1480 train_time:318247ms step_avg:306.30ms
step:1050/1480 train_time:318580ms step_avg:306.33ms
step:1051/1480 train_time:318911ms step_avg:306.35ms
step:1052/1480 train_time:319236ms step_avg:306.37ms
step:1053/1480 train_time:319565ms step_avg:306.39ms
step:1054/1480 train_time:319894ms step_avg:306.41ms
step:1055/1480 train_time:320216ms step_avg:306.43ms
step:1056/1480 train_time:320550ms step_avg:306.45ms
step:1057/1480 train_time:320880ms step_avg:306.48ms
step:1058/1480 train_time:321210ms step_avg:306.50ms
step:1059/1480 train_time:321537ms step_avg:306.52ms
step:1060/1480 train_time:321871ms step_avg:306.54ms
step:1061/1480 train_time:322207ms step_avg:306.57ms
step:1062/1480 train_time:322534ms step_avg:306.59ms
step:1063/1480 train_time:322861ms step_avg:306.61ms
step:1064/1480 train_time:323190ms step_avg:306.63ms
step:1065/1480 train_time:323515ms step_avg:306.65ms
step:1066/1480 train_time:323837ms step_avg:306.66ms
step:1067/1480 train_time:324169ms step_avg:306.69ms
step:1068/1480 train_time:324499ms step_avg:306.71ms
step:1069/1480 train_time:324834ms step_avg:306.74ms
step:1070/1480 train_time:325155ms step_avg:306.75ms
step:1071/1480 train_time:325497ms step_avg:306.78ms
step:1072/1480 train_time:325820ms step_avg:306.80ms
step:1073/1480 train_time:326157ms step_avg:306.83ms
step:1074/1480 train_time:326488ms step_avg:306.85ms
step:1075/1480 train_time:326816ms step_avg:306.87ms
step:1076/1480 train_time:327144ms step_avg:306.89ms
step:1077/1480 train_time:327473ms step_avg:306.91ms
step:1078/1480 train_time:327799ms step_avg:306.93ms
step:1079/1480 train_time:328123ms step_avg:306.94ms
step:1080/1480 train_time:328472ms step_avg:306.98ms
step:1081/1480 train_time:328797ms step_avg:307.00ms
step:1082/1480 train_time:329132ms step_avg:307.03ms
step:1083/1480 train_time:329468ms step_avg:307.05ms
step:1084/1480 train_time:329802ms step_avg:307.08ms
step:1085/1480 train_time:330129ms step_avg:307.10ms
step:1086/1480 train_time:330456ms step_avg:307.12ms
step:1087/1480 train_time:330782ms step_avg:307.13ms
step:1088/1480 train_time:331118ms step_avg:307.16ms
step:1089/1480 train_time:331457ms step_avg:307.19ms
step:1090/1480 train_time:331785ms step_avg:307.21ms
step:1091/1480 train_time:332117ms step_avg:307.23ms
step:1092/1480 train_time:332448ms step_avg:307.25ms
step:1093/1480 train_time:332777ms step_avg:307.27ms
step:1094/1480 train_time:333099ms step_avg:307.29ms
step:1095/1480 train_time:333429ms step_avg:307.31ms
step:1096/1480 train_time:333755ms step_avg:307.32ms
step:1097/1480 train_time:334081ms step_avg:307.34ms
step:1098/1480 train_time:334419ms step_avg:307.37ms
step:1099/1480 train_time:334744ms step_avg:307.39ms
step:1100/1480 train_time:335085ms step_avg:307.42ms
step:1101/1480 train_time:335426ms step_avg:307.45ms
step:1102/1480 train_time:335765ms step_avg:307.48ms
step:1103/1480 train_time:336100ms step_avg:307.50ms
step:1104/1480 train_time:336441ms step_avg:307.53ms
step:1105/1480 train_time:336779ms step_avg:307.56ms
step:1106/1480 train_time:337109ms step_avg:307.58ms
step:1107/1480 train_time:337453ms step_avg:307.61ms
step:1108/1480 train_time:337788ms step_avg:307.64ms
step:1109/1480 train_time:338123ms step_avg:307.66ms
step:1110/1480 train_time:338452ms step_avg:307.68ms
step:1111/1480 train_time:338775ms step_avg:307.70ms
step:1112/1480 train_time:339105ms step_avg:307.72ms
step:1113/1480 train_time:339442ms step_avg:307.74ms
step:1114/1480 train_time:339774ms step_avg:307.77ms
step:1115/1480 train_time:340102ms step_avg:307.78ms
step:1116/1480 train_time:340436ms step_avg:307.81ms
step:1117/1480 train_time:340780ms step_avg:307.84ms
step:1118/1480 train_time:341115ms step_avg:307.87ms
step:1119/1480 train_time:341454ms step_avg:307.89ms
step:1120/1480 train_time:341786ms step_avg:307.92ms
step:1121/1480 train_time:342118ms step_avg:307.94ms
step:1122/1480 train_time:342451ms step_avg:307.96ms
step:1123/1480 train_time:342779ms step_avg:307.98ms
step:1124/1480 train_time:343102ms step_avg:307.99ms
step:1125/1480 train_time:343434ms step_avg:308.01ms
step:1125/1480 val_loss:3.3840 train_time:343455ms step_avg:308.03ms
step:1126/1480 train_time:343782ms step_avg:308.05ms
step:1127/1480 train_time:344121ms step_avg:308.08ms
step:1128/1480 train_time:344457ms step_avg:308.10ms
step:1129/1480 train_time:344790ms step_avg:308.12ms
step:1130/1480 train_time:345126ms step_avg:308.15ms
step:1131/1480 train_time:345465ms step_avg:308.18ms
step:1132/1480 train_time:345798ms step_avg:308.20ms
step:1133/1480 train_time:346129ms step_avg:308.22ms
step:1134/1480 train_time:346455ms step_avg:308.23ms
step:1135/1480 train_time:346806ms step_avg:308.27ms
step:1136/1480 train_time:347141ms step_avg:308.30ms
step:1137/1480 train_time:347469ms step_avg:308.31ms
step:1138/1480 train_time:347808ms step_avg:308.34ms
step:1139/1480 train_time:348142ms step_avg:308.36ms
step:1140/1480 train_time:348479ms step_avg:308.39ms
step:1141/1480 train_time:348810ms step_avg:308.41ms
step:1142/1480 train_time:349138ms step_avg:308.43ms
step:1143/1480 train_time:349469ms step_avg:308.45ms
step:1144/1480 train_time:349798ms step_avg:308.46ms
step:1145/1480 train_time:350135ms step_avg:308.49ms
step:1146/1480 train_time:350469ms step_avg:308.51ms
step:1147/1480 train_time:350800ms step_avg:308.53ms
step:1148/1480 train_time:351128ms step_avg:308.55ms
step:1149/1480 train_time:351457ms step_avg:308.57ms
step:1150/1480 train_time:351787ms step_avg:308.58ms
step:1151/1480 train_time:352116ms step_avg:308.60ms
step:1152/1480 train_time:352449ms step_avg:308.62ms
step:1153/1480 train_time:352791ms step_avg:308.65ms
step:1154/1480 train_time:353124ms step_avg:308.68ms
step:1155/1480 train_time:353462ms step_avg:308.70ms
step:1156/1480 train_time:353797ms step_avg:308.72ms
step:1157/1480 train_time:354124ms step_avg:308.74ms
step:1158/1480 train_time:354467ms step_avg:308.77ms
step:1159/1480 train_time:354808ms step_avg:308.80ms
step:1160/1480 train_time:355141ms step_avg:308.82ms
step:1161/1480 train_time:355469ms step_avg:308.84ms
step:1162/1480 train_time:355799ms step_avg:308.85ms
step:1163/1480 train_time:356130ms step_avg:308.87ms
step:1164/1480 train_time:356470ms step_avg:308.90ms
step:1165/1480 train_time:356801ms step_avg:308.92ms
step:1166/1480 train_time:357137ms step_avg:308.94ms
step:1167/1480 train_time:357467ms step_avg:308.96ms
step:1168/1480 train_time:357793ms step_avg:308.97ms
step:1169/1480 train_time:358126ms step_avg:309.00ms
step:1170/1480 train_time:358458ms step_avg:309.02ms
step:1171/1480 train_time:358788ms step_avg:309.03ms
step:1172/1480 train_time:359119ms step_avg:309.05ms
step:1173/1480 train_time:359453ms step_avg:309.07ms
step:1174/1480 train_time:359781ms step_avg:309.09ms
step:1175/1480 train_time:360110ms step_avg:309.11ms
step:1176/1480 train_time:360462ms step_avg:309.14ms
step:1177/1480 train_time:360799ms step_avg:309.17ms
step:1178/1480 train_time:361134ms step_avg:309.19ms
step:1179/1480 train_time:361481ms step_avg:309.22ms
step:1180/1480 train_time:361817ms step_avg:309.25ms
step:1181/1480 train_time:362149ms step_avg:309.26ms
step:1182/1480 train_time:362481ms step_avg:309.28ms
step:1183/1480 train_time:362821ms step_avg:309.31ms
step:1184/1480 train_time:363150ms step_avg:309.33ms
step:1185/1480 train_time:363485ms step_avg:309.35ms
step:1186/1480 train_time:363823ms step_avg:309.37ms
step:1187/1480 train_time:364155ms step_avg:309.39ms
step:1188/1480 train_time:364490ms step_avg:309.41ms
step:1189/1480 train_time:364840ms step_avg:309.45ms
step:1190/1480 train_time:365177ms step_avg:309.47ms
step:1191/1480 train_time:365508ms step_avg:309.49ms
step:1192/1480 train_time:365842ms step_avg:309.51ms
step:1193/1480 train_time:366170ms step_avg:309.53ms
step:1194/1480 train_time:366501ms step_avg:309.54ms
step:1195/1480 train_time:366828ms step_avg:309.56ms
step:1196/1480 train_time:367169ms step_avg:309.59ms
step:1197/1480 train_time:367510ms step_avg:309.61ms
step:1198/1480 train_time:367845ms step_avg:309.63ms
step:1199/1480 train_time:368174ms step_avg:309.65ms
step:1200/1480 train_time:368510ms step_avg:309.67ms
step:1201/1480 train_time:368866ms step_avg:309.71ms
step:1202/1480 train_time:369209ms step_avg:309.74ms
step:1203/1480 train_time:369540ms step_avg:309.76ms
step:1204/1480 train_time:369879ms step_avg:309.78ms
step:1205/1480 train_time:370225ms step_avg:309.81ms
step:1206/1480 train_time:370557ms step_avg:309.83ms
step:1207/1480 train_time:370898ms step_avg:309.86ms
step:1208/1480 train_time:371233ms step_avg:309.88ms
step:1209/1480 train_time:371579ms step_avg:309.91ms
step:1210/1480 train_time:371917ms step_avg:309.93ms
step:1211/1480 train_time:372251ms step_avg:309.95ms
step:1212/1480 train_time:372605ms step_avg:309.99ms
step:1213/1480 train_time:372937ms step_avg:310.01ms
step:1214/1480 train_time:373271ms step_avg:310.03ms
step:1215/1480 train_time:373605ms step_avg:310.05ms
step:1216/1480 train_time:373945ms step_avg:310.07ms
step:1217/1480 train_time:374286ms step_avg:310.10ms
step:1218/1480 train_time:374614ms step_avg:310.11ms
step:1219/1480 train_time:374951ms step_avg:310.13ms
step:1220/1480 train_time:375286ms step_avg:310.15ms
step:1221/1480 train_time:375620ms step_avg:310.17ms
step:1222/1480 train_time:375956ms step_avg:310.19ms
step:1223/1480 train_time:376286ms step_avg:310.21ms
step:1224/1480 train_time:376632ms step_avg:310.24ms
step:1225/1480 train_time:376977ms step_avg:310.27ms
step:1226/1480 train_time:377314ms step_avg:310.29ms
step:1227/1480 train_time:377654ms step_avg:310.32ms
step:1228/1480 train_time:377978ms step_avg:310.33ms
step:1229/1480 train_time:378319ms step_avg:310.35ms
step:1230/1480 train_time:378652ms step_avg:310.37ms
step:1231/1480 train_time:378986ms step_avg:310.39ms
step:1232/1480 train_time:379350ms step_avg:310.43ms
step:1233/1480 train_time:379688ms step_avg:310.46ms
step:1234/1480 train_time:380024ms step_avg:310.48ms
step:1235/1480 train_time:380357ms step_avg:310.50ms
step:1236/1480 train_time:380687ms step_avg:310.51ms
step:1237/1480 train_time:381027ms step_avg:310.54ms
step:1238/1480 train_time:381364ms step_avg:310.56ms
step:1239/1480 train_time:381704ms step_avg:310.58ms
step:1240/1480 train_time:382042ms step_avg:310.60ms
step:1241/1480 train_time:382392ms step_avg:310.64ms
step:1242/1480 train_time:382736ms step_avg:310.66ms
step:1243/1480 train_time:383075ms step_avg:310.69ms
step:1244/1480 train_time:383406ms step_avg:310.70ms
step:1245/1480 train_time:383742ms step_avg:310.72ms
step:1246/1480 train_time:384074ms step_avg:310.74ms
step:1247/1480 train_time:384406ms step_avg:310.76ms
step:1248/1480 train_time:384740ms step_avg:310.78ms
step:1249/1480 train_time:385077ms step_avg:310.80ms
step:1250/1480 train_time:385417ms step_avg:310.82ms
step:1250/1480 val_loss:3.3340 train_time:385430ms step_avg:310.83ms
step:1251/1480 train_time:385756ms step_avg:310.84ms
step:1252/1480 train_time:386100ms step_avg:310.87ms
step:1253/1480 train_time:386434ms step_avg:310.89ms
step:1254/1480 train_time:386786ms step_avg:310.92ms
step:1255/1480 train_time:387128ms step_avg:310.95ms
step:1256/1480 train_time:387456ms step_avg:310.96ms
step:1257/1480 train_time:387792ms step_avg:310.98ms
step:1258/1480 train_time:388131ms step_avg:311.00ms
step:1259/1480 train_time:388468ms step_avg:311.02ms
step:1260/1480 train_time:388800ms step_avg:311.04ms
step:1261/1480 train_time:389148ms step_avg:311.07ms
step:1262/1480 train_time:389484ms step_avg:311.09ms
step:1263/1480 train_time:389818ms step_avg:311.11ms
step:1264/1480 train_time:390156ms step_avg:311.13ms
step:1265/1480 train_time:390490ms step_avg:311.15ms
step:1266/1480 train_time:390827ms step_avg:311.17ms
step:1267/1480 train_time:391164ms step_avg:311.19ms
step:1268/1480 train_time:391505ms step_avg:311.21ms
step:1269/1480 train_time:391843ms step_avg:311.23ms
step:1270/1480 train_time:392180ms step_avg:311.25ms
step:1271/1480 train_time:392506ms step_avg:311.27ms
step:1272/1480 train_time:392852ms step_avg:311.29ms
step:1273/1480 train_time:393181ms step_avg:311.31ms
step:1274/1480 train_time:393519ms step_avg:311.33ms
step:1275/1480 train_time:393848ms step_avg:311.34ms
step:1276/1480 train_time:394177ms step_avg:311.36ms
step:1277/1480 train_time:394508ms step_avg:311.37ms
step:1278/1480 train_time:394841ms step_avg:311.39ms
step:1279/1480 train_time:395180ms step_avg:311.41ms
step:1280/1480 train_time:395529ms step_avg:311.44ms
step:1281/1480 train_time:395858ms step_avg:311.45ms
step:1282/1480 train_time:396196ms step_avg:311.47ms
step:1283/1480 train_time:396529ms step_avg:311.49ms
step:1284/1480 train_time:396864ms step_avg:311.51ms
step:1285/1480 train_time:397198ms step_avg:311.53ms
step:1286/1480 train_time:397547ms step_avg:311.56ms
step:1287/1480 train_time:397873ms step_avg:311.57ms
step:1288/1480 train_time:398203ms step_avg:311.58ms
step:1289/1480 train_time:398541ms step_avg:311.60ms
step:1290/1480 train_time:398885ms step_avg:311.63ms
step:1291/1480 train_time:399229ms step_avg:311.65ms
step:1292/1480 train_time:399580ms step_avg:311.68ms
step:1293/1480 train_time:399933ms step_avg:311.72ms
step:1294/1480 train_time:400257ms step_avg:311.73ms
step:1295/1480 train_time:400600ms step_avg:311.75ms
step:1296/1480 train_time:400943ms step_avg:311.78ms
step:1297/1480 train_time:401286ms step_avg:311.80ms
step:1298/1480 train_time:401615ms step_avg:311.81ms
step:1299/1480 train_time:401951ms step_avg:311.83ms
step:1300/1480 train_time:402281ms step_avg:311.85ms
step:1301/1480 train_time:402613ms step_avg:311.86ms
step:1302/1480 train_time:402949ms step_avg:311.88ms
step:1303/1480 train_time:403282ms step_avg:311.90ms
step:1304/1480 train_time:403611ms step_avg:311.91ms
step:1305/1480 train_time:403951ms step_avg:311.93ms
step:1306/1480 train_time:404303ms step_avg:311.96ms
step:1307/1480 train_time:404643ms step_avg:311.98ms
step:1308/1480 train_time:404972ms step_avg:312.00ms
step:1309/1480 train_time:405314ms step_avg:312.02ms
step:1310/1480 train_time:405641ms step_avg:312.03ms
step:1311/1480 train_time:405972ms step_avg:312.05ms
step:1312/1480 train_time:406311ms step_avg:312.07ms
step:1313/1480 train_time:406646ms step_avg:312.08ms
step:1314/1480 train_time:406976ms step_avg:312.10ms
step:1315/1480 train_time:407312ms step_avg:312.12ms
step:1316/1480 train_time:407645ms step_avg:312.13ms
step:1317/1480 train_time:407995ms step_avg:312.16ms
step:1318/1480 train_time:408328ms step_avg:312.18ms
step:1319/1480 train_time:408660ms step_avg:312.19ms
step:1320/1480 train_time:408991ms step_avg:312.21ms
step:1321/1480 train_time:409341ms step_avg:312.24ms
step:1322/1480 train_time:409672ms step_avg:312.25ms
step:1323/1480 train_time:410011ms step_avg:312.27ms
step:1324/1480 train_time:410358ms step_avg:312.30ms
step:1325/1480 train_time:410701ms step_avg:312.32ms
step:1326/1480 train_time:411050ms step_avg:312.35ms
step:1327/1480 train_time:411383ms step_avg:312.36ms
step:1328/1480 train_time:411721ms step_avg:312.38ms
step:1329/1480 train_time:412067ms step_avg:312.41ms
step:1330/1480 train_time:412412ms step_avg:312.43ms
step:1331/1480 train_time:412778ms step_avg:312.47ms
step:1332/1480 train_time:413121ms step_avg:312.50ms
step:1333/1480 train_time:413461ms step_avg:312.52ms
step:1334/1480 train_time:413793ms step_avg:312.53ms
step:1335/1480 train_time:414142ms step_avg:312.56ms
step:1336/1480 train_time:414481ms step_avg:312.58ms
step:1337/1480 train_time:414821ms step_avg:312.60ms
step:1338/1480 train_time:415159ms step_avg:312.62ms
step:1339/1480 train_time:415501ms step_avg:312.64ms
step:1340/1480 train_time:415849ms step_avg:312.67ms
step:1341/1480 train_time:416188ms step_avg:312.69ms
step:1342/1480 train_time:416529ms step_avg:312.71ms
step:1343/1480 train_time:416861ms step_avg:312.72ms
step:1344/1480 train_time:417199ms step_avg:312.74ms
step:1345/1480 train_time:417538ms step_avg:312.76ms
step:1346/1480 train_time:417874ms step_avg:312.78ms
step:1347/1480 train_time:418211ms step_avg:312.80ms
step:1348/1480 train_time:418549ms step_avg:312.82ms
step:1349/1480 train_time:418887ms step_avg:312.84ms
step:1350/1480 train_time:419222ms step_avg:312.85ms
step:1351/1480 train_time:419563ms step_avg:312.87ms
step:1352/1480 train_time:419911ms step_avg:312.90ms
step:1353/1480 train_time:420251ms step_avg:312.92ms
step:1354/1480 train_time:420585ms step_avg:312.93ms
step:1355/1480 train_time:420917ms step_avg:312.95ms
step:1356/1480 train_time:421259ms step_avg:312.97ms
step:1357/1480 train_time:421607ms step_avg:313.00ms
step:1358/1480 train_time:421943ms step_avg:313.01ms
step:1359/1480 train_time:422280ms step_avg:313.03ms
step:1360/1480 train_time:422617ms step_avg:313.05ms
step:1361/1480 train_time:422954ms step_avg:313.07ms
step:1362/1480 train_time:423297ms step_avg:313.09ms
step:1363/1480 train_time:423642ms step_avg:313.11ms
step:1364/1480 train_time:423995ms step_avg:313.14ms
step:1365/1480 train_time:424331ms step_avg:313.16ms
step:1366/1480 train_time:424685ms step_avg:313.19ms
step:1367/1480 train_time:425013ms step_avg:313.20ms
step:1368/1480 train_time:425354ms step_avg:313.22ms
step:1369/1480 train_time:425688ms step_avg:313.24ms
step:1370/1480 train_time:426025ms step_avg:313.25ms
step:1371/1480 train_time:426376ms step_avg:313.28ms
step:1372/1480 train_time:426726ms step_avg:313.31ms
step:1373/1480 train_time:427070ms step_avg:313.33ms
step:1374/1480 train_time:427402ms step_avg:313.34ms
step:1375/1480 train_time:427745ms step_avg:313.37ms
step:1375/1480 val_loss:3.2956 train_time:427759ms step_avg:313.38ms
step:1376/1480 train_time:428084ms step_avg:313.39ms
step:1377/1480 train_time:428423ms step_avg:313.40ms
step:1378/1480 train_time:428770ms step_avg:313.43ms
step:1379/1480 train_time:429099ms step_avg:313.44ms
step:1380/1480 train_time:429442ms step_avg:313.46ms
step:1381/1480 train_time:429787ms step_avg:313.48ms
step:1382/1480 train_time:430130ms step_avg:313.51ms
step:1383/1480 train_time:430472ms step_avg:313.53ms
step:1384/1480 train_time:430828ms step_avg:313.56ms
step:1385/1480 train_time:431161ms step_avg:313.57ms
step:1386/1480 train_time:431497ms step_avg:313.59ms
step:1387/1480 train_time:431838ms step_avg:313.61ms
step:1388/1480 train_time:432166ms step_avg:313.62ms
step:1389/1480 train_time:432504ms step_avg:313.64ms
step:1390/1480 train_time:432846ms step_avg:313.66ms
step:1391/1480 train_time:433184ms step_avg:313.67ms
step:1392/1480 train_time:433527ms step_avg:313.70ms
step:1393/1480 train_time:433858ms step_avg:313.71ms
step:1394/1480 train_time:434193ms step_avg:313.72ms
step:1395/1480 train_time:434533ms step_avg:313.74ms
step:1396/1480 train_time:434875ms step_avg:313.76ms
step:1397/1480 train_time:435210ms step_avg:313.78ms
step:1398/1480 train_time:435549ms step_avg:313.80ms
step:1399/1480 train_time:435884ms step_avg:313.81ms
step:1400/1480 train_time:436212ms step_avg:313.82ms
step:1401/1480 train_time:436547ms step_avg:313.84ms
step:1402/1480 train_time:436881ms step_avg:313.85ms
step:1403/1480 train_time:437231ms step_avg:313.88ms
step:1404/1480 train_time:437559ms step_avg:313.89ms
step:1405/1480 train_time:437895ms step_avg:313.90ms
step:1406/1480 train_time:438242ms step_avg:313.93ms
step:1407/1480 train_time:438568ms step_avg:313.94ms
step:1408/1480 train_time:438915ms step_avg:313.96ms
step:1409/1480 train_time:439253ms step_avg:313.98ms
step:1410/1480 train_time:439582ms step_avg:313.99ms
step:1411/1480 train_time:439910ms step_avg:314.00ms
step:1412/1480 train_time:440266ms step_avg:314.03ms
step:1413/1480 train_time:440600ms step_avg:314.04ms
step:1414/1480 train_time:440928ms step_avg:314.05ms
step:1415/1480 train_time:441265ms step_avg:314.07ms
step:1416/1480 train_time:441599ms step_avg:314.08ms
step:1417/1480 train_time:441932ms step_avg:314.10ms
step:1418/1480 train_time:442271ms step_avg:314.11ms
step:1419/1480 train_time:442629ms step_avg:314.14ms
step:1420/1480 train_time:442969ms step_avg:314.16ms
step:1421/1480 train_time:443310ms step_avg:314.18ms
step:1422/1480 train_time:443656ms step_avg:314.20ms
step:1423/1480 train_time:443998ms step_avg:314.22ms
step:1424/1480 train_time:444338ms step_avg:314.24ms
step:1425/1480 train_time:444678ms step_avg:314.26ms
step:1426/1480 train_time:445009ms step_avg:314.27ms
step:1427/1480 train_time:445350ms step_avg:314.29ms
step:1428/1480 train_time:445713ms step_avg:314.32ms
step:1429/1480 train_time:446047ms step_avg:314.34ms
step:1430/1480 train_time:446391ms step_avg:314.36ms
step:1431/1480 train_time:446730ms step_avg:314.38ms
step:1432/1480 train_time:447060ms step_avg:314.39ms
step:1433/1480 train_time:447403ms step_avg:314.41ms
step:1434/1480 train_time:447752ms step_avg:314.43ms
step:1435/1480 train_time:448095ms step_avg:314.45ms
step:1436/1480 train_time:448455ms step_avg:314.48ms
step:1437/1480 train_time:448806ms step_avg:314.51ms
step:1438/1480 train_time:449141ms step_avg:314.52ms
step:1439/1480 train_time:449480ms step_avg:314.54ms
step:1440/1480 train_time:449826ms step_avg:314.56ms
step:1441/1480 train_time:450166ms step_avg:314.58ms
step:1442/1480 train_time:450513ms step_avg:314.60ms
step:1443/1480 train_time:450852ms step_avg:314.62ms
step:1444/1480 train_time:451187ms step_avg:314.64ms
step:1445/1480 train_time:451538ms step_avg:314.66ms
step:1446/1480 train_time:451891ms step_avg:314.69ms
step:1447/1480 train_time:452228ms step_avg:314.70ms
step:1448/1480 train_time:452569ms step_avg:314.72ms
step:1449/1480 train_time:452912ms step_avg:314.74ms
step:1450/1480 train_time:453254ms step_avg:314.76ms
step:1451/1480 train_time:453596ms step_avg:314.78ms
step:1452/1480 train_time:453931ms step_avg:314.79ms
step:1453/1480 train_time:454269ms step_avg:314.81ms
step:1454/1480 train_time:454612ms step_avg:314.83ms
step:1455/1480 train_time:454954ms step_avg:314.85ms
step:1456/1480 train_time:455291ms step_avg:314.86ms
step:1457/1480 train_time:455632ms step_avg:314.88ms
step:1458/1480 train_time:455988ms step_avg:314.91ms
step:1459/1480 train_time:456327ms step_avg:314.93ms
step:1460/1480 train_time:456666ms step_avg:314.94ms
step:1461/1480 train_time:457003ms step_avg:314.96ms
step:1462/1480 train_time:457342ms step_avg:314.97ms
step:1463/1480 train_time:457683ms step_avg:314.99ms
step:1464/1480 train_time:458017ms step_avg:315.00ms
step:1465/1480 train_time:458354ms step_avg:315.02ms
step:1466/1480 train_time:458698ms step_avg:315.04ms
step:1467/1480 train_time:459032ms step_avg:315.05ms
step:1468/1480 train_time:459369ms step_avg:315.07ms
step:1469/1480 train_time:459709ms step_avg:315.08ms
step:1470/1480 train_time:460051ms step_avg:315.10ms
step:1471/1480 train_time:460389ms step_avg:315.12ms
step:1472/1480 train_time:460730ms step_avg:315.14ms
step:1473/1480 train_time:461072ms step_avg:315.16ms
step:1474/1480 train_time:461423ms step_avg:315.18ms
step:1475/1480 train_time:461785ms step_avg:315.21ms
step:1476/1480 train_time:462117ms step_avg:315.22ms
step:1477/1480 train_time:462458ms step_avg:315.24ms
step:1478/1480 train_time:462797ms step_avg:315.26ms
step:1479/1480 train_time:463140ms step_avg:315.28ms
step:1480/1480 train_time:463490ms step_avg:315.30ms
step:1480/1480 val_loss:3.2773 train_time:463520ms step_avg:315.32ms
peak memory consumption: 35247 MiB
