====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

PRINT_GRAD_STATS = False

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        torch.nn.init.orthogonal_(self.c_q.weight.data)
        torch.nn.init.orthogonal_(self.c_k.weight.data)
        torch.nn.init.orthogonal_(self.c_v.weight.data)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1=None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        torch.nn.init.orthogonal_(self.c_fc.weight.data)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x) * torch.tensor(0.5, dtype=x.dtype, device=x.device)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x) * torch.tensor(2.0, dtype=x.dtype, device=x.device)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None
        for block in self.transformer.h:
            x, v1 = block(x, v1, x0)
        x = F.rms_norm(x, (x.size(-1),))

        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 3125 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.3,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.002, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]
optimizer3 = Muon(matrix_params,           lr=0.02,  momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.02, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    if master_process and (step % 500) == 0 and PRINT_GRAD_STATS:
        print("============== Gradient norms: ==============")
        with open(logfile, "a") as f:
            f.write("============== Gradient norms: ==============\n")
            for name, p in model.named_parameters():
                if p.grad is not None and p.ndim == 2:
                    spectral_norm = torch.linalg.matrix_norm(p.grad.data.float(), ord=2).item()
                    nuclear_norm = torch.linalg.matrix_norm(p.grad.data.float(), ord="nuc").item()
                    print(f"{name = } | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
                    f.write(f"{name = } | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
            f.write("===========================================\n")
        print("===========================================")
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Mon Nov 11 01:48:47 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:11:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   5155MiB / 81559MiB |      4%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:12:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   5203MiB / 81559MiB |      3%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:43:00.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |   5203MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:46:00.0 Off |                    0 |
| N/A   35C    P0             122W / 700W |   5277MiB / 81559MiB |      6%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:86:00.0 Off |                    0 |
| N/A   35C    P0             119W / 700W |   5277MiB / 81559MiB |      3%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:87:00.0 Off |                    0 |
| N/A   39C    P0             119W / 700W |   5277MiB / 81559MiB |      6%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   39C    P0             119W / 700W |   5277MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:BE:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   5037MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/3125 val_loss:10.8258 train_time:262ms step_avg:nanms
step:1/3125 train_loss:10.8258 train_time:14221ms step_avg:nanms
step:2/3125 train_loss:10.4283 train_time:14327ms step_avg:nanms
step:3/3125 train_loss:10.0129 train_time:14466ms step_avg:nanms
step:4/3125 train_loss:9.1698 train_time:14607ms step_avg:nanms
step:5/3125 train_loss:8.1960 train_time:14750ms step_avg:nanms
step:6/3125 train_loss:7.6536 train_time:14892ms step_avg:nanms
step:7/3125 train_loss:7.1440 train_time:15034ms step_avg:nanms
step:8/3125 train_loss:7.2520 train_time:15181ms step_avg:nanms
step:9/3125 train_loss:6.9341 train_time:15331ms step_avg:nanms
step:10/3125 train_loss:6.8190 train_time:15475ms step_avg:nanms
step:11/3125 train_loss:6.7125 train_time:106ms step_avg:nanms
step:12/3125 train_loss:6.6971 train_time:248ms step_avg:nanms
step:13/3125 train_loss:6.5149 train_time:392ms step_avg:130.60ms
step:14/3125 train_loss:6.4770 train_time:534ms step_avg:133.54ms
step:15/3125 train_loss:6.4389 train_time:681ms step_avg:136.22ms
step:16/3125 train_loss:6.3933 train_time:829ms step_avg:138.16ms
step:17/3125 train_loss:6.3939 train_time:976ms step_avg:139.38ms
step:18/3125 train_loss:6.4459 train_time:1118ms step_avg:139.77ms
step:19/3125 train_loss:6.2764 train_time:1263ms step_avg:140.28ms
step:20/3125 train_loss:6.2939 train_time:1406ms step_avg:140.63ms
step:21/3125 train_loss:6.0077 train_time:1550ms step_avg:140.88ms
step:22/3125 train_loss:6.3186 train_time:1695ms step_avg:141.23ms
step:23/3125 train_loss:6.5422 train_time:1841ms step_avg:141.58ms
step:24/3125 train_loss:6.2187 train_time:1988ms step_avg:142.03ms
step:25/3125 train_loss:6.3602 train_time:2134ms step_avg:142.26ms
step:26/3125 train_loss:6.0721 train_time:2277ms step_avg:142.31ms
step:27/3125 train_loss:5.9918 train_time:2420ms step_avg:142.38ms
step:28/3125 train_loss:6.1770 train_time:2565ms step_avg:142.48ms
step:29/3125 train_loss:5.8403 train_time:2711ms step_avg:142.67ms
step:30/3125 train_loss:6.0933 train_time:2856ms step_avg:142.80ms
step:31/3125 train_loss:5.9318 train_time:3000ms step_avg:142.88ms
step:32/3125 train_loss:5.8987 train_time:3146ms step_avg:142.98ms
step:33/3125 train_loss:5.7507 train_time:3291ms step_avg:143.08ms
step:34/3125 train_loss:6.0170 train_time:3435ms step_avg:143.13ms
step:35/3125 train_loss:5.9593 train_time:3580ms step_avg:143.18ms
step:36/3125 train_loss:6.0900 train_time:3725ms step_avg:143.26ms
step:37/3125 train_loss:6.0183 train_time:3872ms step_avg:143.39ms
step:38/3125 train_loss:5.9198 train_time:4017ms step_avg:143.46ms
step:39/3125 train_loss:5.8194 train_time:4161ms step_avg:143.48ms
step:40/3125 train_loss:5.8447 train_time:4305ms step_avg:143.51ms
step:41/3125 train_loss:5.7589 train_time:4450ms step_avg:143.54ms
step:42/3125 train_loss:5.7515 train_time:4594ms step_avg:143.57ms
step:43/3125 train_loss:5.6589 train_time:4739ms step_avg:143.60ms
step:44/3125 train_loss:5.7413 train_time:4884ms step_avg:143.64ms
step:45/3125 train_loss:5.7225 train_time:5029ms step_avg:143.69ms
step:46/3125 train_loss:5.8600 train_time:5175ms step_avg:143.74ms
step:47/3125 train_loss:5.6740 train_time:5318ms step_avg:143.74ms
step:48/3125 train_loss:5.5365 train_time:5462ms step_avg:143.73ms
step:49/3125 train_loss:5.7355 train_time:5608ms step_avg:143.80ms
step:50/3125 train_loss:5.6123 train_time:5754ms step_avg:143.84ms
step:51/3125 train_loss:5.7559 train_time:5899ms step_avg:143.87ms
step:52/3125 train_loss:5.6237 train_time:6043ms step_avg:143.89ms
step:53/3125 train_loss:5.4709 train_time:6190ms step_avg:143.95ms
step:54/3125 train_loss:5.5950 train_time:6335ms step_avg:143.98ms
step:55/3125 train_loss:5.4741 train_time:6478ms step_avg:143.97ms
step:56/3125 train_loss:5.8146 train_time:6623ms step_avg:143.98ms
step:57/3125 train_loss:5.4748 train_time:6770ms step_avg:144.04ms
step:58/3125 train_loss:5.3626 train_time:6916ms step_avg:144.07ms
step:59/3125 train_loss:5.4717 train_time:7060ms step_avg:144.08ms
step:60/3125 train_loss:5.4494 train_time:7204ms step_avg:144.08ms
step:61/3125 train_loss:5.5488 train_time:7351ms step_avg:144.14ms
step:62/3125 train_loss:5.3202 train_time:7496ms step_avg:144.15ms
step:63/3125 train_loss:5.4170 train_time:7640ms step_avg:144.14ms
step:64/3125 train_loss:5.3872 train_time:7785ms step_avg:144.16ms
step:65/3125 train_loss:5.2253 train_time:7931ms step_avg:144.21ms
step:66/3125 train_loss:5.2022 train_time:8076ms step_avg:144.21ms
step:67/3125 train_loss:5.3639 train_time:8220ms step_avg:144.21ms
step:68/3125 train_loss:5.2279 train_time:8365ms step_avg:144.22ms
step:69/3125 train_loss:5.4672 train_time:8511ms step_avg:144.25ms
step:70/3125 train_loss:5.1343 train_time:8655ms step_avg:144.25ms
step:71/3125 train_loss:5.1941 train_time:8800ms step_avg:144.27ms
step:72/3125 train_loss:5.3629 train_time:8945ms step_avg:144.28ms
step:73/3125 train_loss:5.2774 train_time:9091ms step_avg:144.30ms
step:74/3125 train_loss:5.1738 train_time:9237ms step_avg:144.33ms
step:75/3125 train_loss:5.2834 train_time:9381ms step_avg:144.33ms
step:76/3125 train_loss:5.2698 train_time:9527ms step_avg:144.34ms
step:77/3125 train_loss:5.2101 train_time:9673ms step_avg:144.37ms
step:78/3125 train_loss:5.2965 train_time:9817ms step_avg:144.37ms
step:79/3125 train_loss:5.3914 train_time:9961ms step_avg:144.37ms
step:80/3125 train_loss:5.1504 train_time:10107ms step_avg:144.39ms
step:81/3125 train_loss:5.2415 train_time:10253ms step_avg:144.41ms
step:82/3125 train_loss:5.0186 train_time:10398ms step_avg:144.41ms
step:83/3125 train_loss:5.1918 train_time:10543ms step_avg:144.43ms
step:84/3125 train_loss:5.1399 train_time:10690ms step_avg:144.45ms
step:85/3125 train_loss:5.1279 train_time:10835ms step_avg:144.46ms
step:86/3125 train_loss:4.9916 train_time:10979ms step_avg:144.46ms
step:87/3125 train_loss:5.1908 train_time:11123ms step_avg:144.46ms
step:88/3125 train_loss:5.0989 train_time:11270ms step_avg:144.49ms
step:89/3125 train_loss:5.1499 train_time:11417ms step_avg:144.52ms
step:90/3125 train_loss:5.1231 train_time:11561ms step_avg:144.51ms
step:91/3125 train_loss:5.0322 train_time:11707ms step_avg:144.53ms
step:92/3125 train_loss:5.0337 train_time:11853ms step_avg:144.54ms
step:93/3125 train_loss:5.1486 train_time:11997ms step_avg:144.54ms
step:94/3125 train_loss:4.9777 train_time:12141ms step_avg:144.54ms
step:95/3125 train_loss:4.9873 train_time:12287ms step_avg:144.56ms
step:96/3125 train_loss:5.0330 train_time:12434ms step_avg:144.58ms
step:97/3125 train_loss:4.9352 train_time:12578ms step_avg:144.57ms
step:98/3125 train_loss:5.0092 train_time:12723ms step_avg:144.58ms
step:99/3125 train_loss:4.9401 train_time:12870ms step_avg:144.60ms
step:100/3125 train_loss:5.0442 train_time:13015ms step_avg:144.62ms
step:101/3125 train_loss:5.0122 train_time:13158ms step_avg:144.59ms
step:102/3125 train_loss:4.9002 train_time:13304ms step_avg:144.60ms
step:103/3125 train_loss:5.0239 train_time:13450ms step_avg:144.62ms
step:104/3125 train_loss:4.9731 train_time:13596ms step_avg:144.64ms
step:105/3125 train_loss:4.8428 train_time:13741ms step_avg:144.64ms
step:106/3125 train_loss:4.9064 train_time:13886ms step_avg:144.64ms
step:107/3125 train_loss:5.0820 train_time:14031ms step_avg:144.65ms
step:108/3125 train_loss:4.8772 train_time:14177ms step_avg:144.66ms
step:109/3125 train_loss:4.6888 train_time:14320ms step_avg:144.65ms
step:110/3125 train_loss:4.8560 train_time:14467ms step_avg:144.67ms
step:111/3125 train_loss:4.8611 train_time:14614ms step_avg:144.69ms
step:112/3125 train_loss:4.8007 train_time:14758ms step_avg:144.68ms
step:113/3125 train_loss:4.9337 train_time:14903ms step_avg:144.69ms
step:114/3125 train_loss:4.8325 train_time:15050ms step_avg:144.71ms
step:115/3125 train_loss:4.6958 train_time:15195ms step_avg:144.72ms
step:116/3125 train_loss:4.8480 train_time:15338ms step_avg:144.70ms
step:117/3125 train_loss:4.7669 train_time:15484ms step_avg:144.71ms
step:118/3125 train_loss:4.7145 train_time:15630ms step_avg:144.72ms
step:119/3125 train_loss:4.8833 train_time:15777ms step_avg:144.74ms
step:120/3125 train_loss:4.8106 train_time:15921ms step_avg:144.74ms
step:121/3125 train_loss:4.7277 train_time:16067ms step_avg:144.75ms
step:122/3125 train_loss:4.6457 train_time:16214ms step_avg:144.77ms
step:123/3125 train_loss:4.7655 train_time:16358ms step_avg:144.76ms
step:124/3125 train_loss:4.6237 train_time:16504ms step_avg:144.77ms
step:125/3125 train_loss:4.9225 train_time:16651ms step_avg:144.79ms
step:125/3125 val_loss:4.7411 train_time:16689ms step_avg:145.12ms
step:126/3125 train_loss:4.7854 train_time:16803ms step_avg:144.85ms
step:127/3125 train_loss:4.7445 train_time:16952ms step_avg:144.89ms
step:128/3125 train_loss:4.7836 train_time:17095ms step_avg:144.87ms
step:129/3125 train_loss:4.6734 train_time:17238ms step_avg:144.86ms
step:130/3125 train_loss:4.9710 train_time:17380ms step_avg:144.83ms
step:131/3125 train_loss:4.7061 train_time:17521ms step_avg:144.81ms
step:132/3125 train_loss:4.7195 train_time:17672ms step_avg:144.85ms
step:133/3125 train_loss:4.6645 train_time:17821ms step_avg:144.89ms
step:134/3125 train_loss:4.7169 train_time:17966ms step_avg:144.89ms
step:135/3125 train_loss:4.5925 train_time:18111ms step_avg:144.89ms
step:136/3125 train_loss:4.7276 train_time:18254ms step_avg:144.87ms
step:137/3125 train_loss:4.5063 train_time:18397ms step_avg:144.86ms
step:138/3125 train_loss:4.6637 train_time:18540ms step_avg:144.84ms
step:139/3125 train_loss:4.6062 train_time:18687ms step_avg:144.86ms
step:140/3125 train_loss:4.6648 train_time:18836ms step_avg:144.89ms
step:141/3125 train_loss:4.7127 train_time:18981ms step_avg:144.89ms
step:142/3125 train_loss:4.5876 train_time:19125ms step_avg:144.88ms
step:143/3125 train_loss:4.6247 train_time:19268ms step_avg:144.88ms
step:144/3125 train_loss:4.5096 train_time:19410ms step_avg:144.85ms
step:145/3125 train_loss:4.6188 train_time:19555ms step_avg:144.85ms
step:146/3125 train_loss:4.5779 train_time:19701ms step_avg:144.86ms
step:147/3125 train_loss:4.4608 train_time:19846ms step_avg:144.86ms
step:148/3125 train_loss:4.5954 train_time:19994ms step_avg:144.88ms
step:149/3125 train_loss:4.6036 train_time:20139ms step_avg:144.89ms
step:150/3125 train_loss:4.5806 train_time:20284ms step_avg:144.89ms
step:151/3125 train_loss:4.6671 train_time:20428ms step_avg:144.88ms
step:152/3125 train_loss:4.5284 train_time:20572ms step_avg:144.88ms
step:153/3125 train_loss:4.5160 train_time:20718ms step_avg:144.88ms
step:154/3125 train_loss:4.6028 train_time:20863ms step_avg:144.88ms
step:155/3125 train_loss:4.5693 train_time:21010ms step_avg:144.90ms
step:156/3125 train_loss:4.5028 train_time:21155ms step_avg:144.90ms
step:157/3125 train_loss:4.5558 train_time:21298ms step_avg:144.89ms
step:158/3125 train_loss:4.6459 train_time:21441ms step_avg:144.87ms
step:159/3125 train_loss:4.4537 train_time:21586ms step_avg:144.87ms
step:160/3125 train_loss:4.5203 train_time:21732ms step_avg:144.88ms
step:161/3125 train_loss:4.3398 train_time:21877ms step_avg:144.88ms
step:162/3125 train_loss:4.5392 train_time:22022ms step_avg:144.88ms
step:163/3125 train_loss:4.5536 train_time:22168ms step_avg:144.89ms
step:164/3125 train_loss:4.5404 train_time:22313ms step_avg:144.89ms
step:165/3125 train_loss:4.3858 train_time:22457ms step_avg:144.88ms
step:166/3125 train_loss:4.4749 train_time:22601ms step_avg:144.88ms
step:167/3125 train_loss:4.5583 train_time:22746ms step_avg:144.88ms
step:168/3125 train_loss:4.3901 train_time:22893ms step_avg:144.89ms
step:169/3125 train_loss:4.4651 train_time:23039ms step_avg:144.90ms
step:170/3125 train_loss:4.3458 train_time:23184ms step_avg:144.90ms
step:171/3125 train_loss:4.2384 train_time:23328ms step_avg:144.90ms
step:172/3125 train_loss:4.3742 train_time:23473ms step_avg:144.89ms
step:173/3125 train_loss:4.3768 train_time:23618ms step_avg:144.90ms
step:174/3125 train_loss:4.4259 train_time:23762ms step_avg:144.89ms
step:175/3125 train_loss:4.5976 train_time:23906ms step_avg:144.89ms
step:176/3125 train_loss:4.4311 train_time:24051ms step_avg:144.89ms
step:177/3125 train_loss:4.2821 train_time:24197ms step_avg:144.89ms
step:178/3125 train_loss:4.2412 train_time:24340ms step_avg:144.88ms
step:179/3125 train_loss:4.3404 train_time:24485ms step_avg:144.88ms
step:180/3125 train_loss:4.3027 train_time:24630ms step_avg:144.88ms
step:181/3125 train_loss:4.2741 train_time:24774ms step_avg:144.88ms
step:182/3125 train_loss:4.4520 train_time:24918ms step_avg:144.87ms
step:183/3125 train_loss:4.3192 train_time:25064ms step_avg:144.88ms
step:184/3125 train_loss:4.3002 train_time:25210ms step_avg:144.88ms
step:185/3125 train_loss:4.2887 train_time:25355ms step_avg:144.89ms
step:186/3125 train_loss:4.3730 train_time:25498ms step_avg:144.88ms
step:187/3125 train_loss:4.3331 train_time:25642ms step_avg:144.87ms
step:188/3125 train_loss:4.4120 train_time:25789ms step_avg:144.88ms
step:189/3125 train_loss:4.3363 train_time:26085ms step_avg:145.73ms
step:190/3125 train_loss:4.2679 train_time:26421ms step_avg:146.78ms
step:191/3125 train_loss:4.3562 train_time:26562ms step_avg:146.75ms
step:192/3125 train_loss:4.2358 train_time:26704ms step_avg:146.73ms
step:193/3125 train_loss:4.1673 train_time:26849ms step_avg:146.71ms
step:194/3125 train_loss:4.3913 train_time:26993ms step_avg:146.70ms
step:195/3125 train_loss:4.3049 train_time:27136ms step_avg:146.68ms
step:196/3125 train_loss:4.5094 train_time:27284ms step_avg:146.69ms
step:197/3125 train_loss:4.3382 train_time:27433ms step_avg:146.70ms
step:198/3125 train_loss:4.1843 train_time:27579ms step_avg:146.70ms
step:199/3125 train_loss:4.3192 train_time:27724ms step_avg:146.69ms
step:200/3125 train_loss:4.1679 train_time:27869ms step_avg:146.68ms
step:201/3125 train_loss:4.2598 train_time:28013ms step_avg:146.67ms
step:202/3125 train_loss:4.1456 train_time:28156ms step_avg:146.65ms
step:203/3125 train_loss:4.3824 train_time:28301ms step_avg:146.64ms
step:204/3125 train_loss:4.2131 train_time:28449ms step_avg:146.65ms
step:205/3125 train_loss:4.3249 train_time:28597ms step_avg:146.65ms
step:206/3125 train_loss:4.3824 train_time:28740ms step_avg:146.63ms
step:207/3125 train_loss:4.0810 train_time:28885ms step_avg:146.63ms
step:208/3125 train_loss:4.2220 train_time:29030ms step_avg:146.62ms
step:209/3125 train_loss:4.2106 train_time:29174ms step_avg:146.61ms
step:210/3125 train_loss:4.3680 train_time:29319ms step_avg:146.59ms
step:211/3125 train_loss:4.3010 train_time:29465ms step_avg:146.59ms
step:212/3125 train_loss:4.1890 train_time:29613ms step_avg:146.60ms
step:213/3125 train_loss:4.2307 train_time:29757ms step_avg:146.59ms
step:214/3125 train_loss:4.1679 train_time:29900ms step_avg:146.57ms
step:215/3125 train_loss:4.2427 train_time:30044ms step_avg:146.56ms
step:216/3125 train_loss:4.0604 train_time:30190ms step_avg:146.55ms
step:217/3125 train_loss:4.1274 train_time:30336ms step_avg:146.55ms
step:218/3125 train_loss:4.1340 train_time:30481ms step_avg:146.54ms
step:219/3125 train_loss:4.2046 train_time:30627ms step_avg:146.54ms
step:220/3125 train_loss:4.1921 train_time:30773ms step_avg:146.54ms
step:221/3125 train_loss:4.2081 train_time:30917ms step_avg:146.53ms
step:222/3125 train_loss:4.2256 train_time:31061ms step_avg:146.51ms
step:223/3125 train_loss:4.1385 train_time:31205ms step_avg:146.50ms
step:224/3125 train_loss:4.0985 train_time:31351ms step_avg:146.50ms
step:225/3125 train_loss:4.4031 train_time:31496ms step_avg:146.50ms
step:226/3125 train_loss:4.0269 train_time:31640ms step_avg:146.48ms
step:227/3125 train_loss:4.1023 train_time:31785ms step_avg:146.47ms
step:228/3125 train_loss:4.1013 train_time:31931ms step_avg:146.47ms
step:229/3125 train_loss:4.2462 train_time:32075ms step_avg:146.46ms
step:230/3125 train_loss:4.0398 train_time:32219ms step_avg:146.45ms
step:231/3125 train_loss:4.1644 train_time:32363ms step_avg:146.44ms
step:232/3125 train_loss:4.0238 train_time:32510ms step_avg:146.44ms
step:233/3125 train_loss:4.0867 train_time:32655ms step_avg:146.44ms
step:234/3125 train_loss:4.2122 train_time:32799ms step_avg:146.42ms
step:235/3125 train_loss:4.1313 train_time:32943ms step_avg:146.41ms
step:236/3125 train_loss:4.0159 train_time:33089ms step_avg:146.41ms
step:237/3125 train_loss:4.1826 train_time:33235ms step_avg:146.41ms
step:238/3125 train_loss:4.1856 train_time:33378ms step_avg:146.39ms
step:239/3125 train_loss:4.0463 train_time:33523ms step_avg:146.39ms
step:240/3125 train_loss:4.1913 train_time:33668ms step_avg:146.38ms
step:241/3125 train_loss:4.2218 train_time:33815ms step_avg:146.38ms
step:242/3125 train_loss:4.0711 train_time:33959ms step_avg:146.37ms
step:243/3125 train_loss:4.2379 train_time:34103ms step_avg:146.36ms
step:244/3125 train_loss:4.1195 train_time:34247ms step_avg:146.35ms
step:245/3125 train_loss:4.1836 train_time:34393ms step_avg:146.35ms
step:246/3125 train_loss:4.2534 train_time:34539ms step_avg:146.35ms
step:247/3125 train_loss:4.1687 train_time:34682ms step_avg:146.34ms
step:248/3125 train_loss:4.1105 train_time:34827ms step_avg:146.33ms
step:249/3125 train_loss:4.2113 train_time:34972ms step_avg:146.33ms
step:250/3125 train_loss:4.0223 train_time:35116ms step_avg:146.32ms
step:250/3125 val_loss:4.1075 train_time:35154ms step_avg:146.48ms
step:251/3125 train_loss:4.0718 train_time:35270ms step_avg:146.35ms
step:252/3125 train_loss:4.1734 train_time:35414ms step_avg:146.34ms
step:253/3125 train_loss:4.2524 train_time:35557ms step_avg:146.32ms
step:254/3125 train_loss:4.0447 train_time:35700ms step_avg:146.31ms
step:255/3125 train_loss:3.9873 train_time:35844ms step_avg:146.30ms
step:256/3125 train_loss:4.1653 train_time:35988ms step_avg:146.29ms
step:257/3125 train_loss:4.0739 train_time:36133ms step_avg:146.29ms
step:258/3125 train_loss:4.0829 train_time:36281ms step_avg:146.29ms
step:259/3125 train_loss:4.0608 train_time:36428ms step_avg:146.30ms
step:260/3125 train_loss:4.1188 train_time:36572ms step_avg:146.29ms
step:261/3125 train_loss:4.1523 train_time:36715ms step_avg:146.27ms
step:262/3125 train_loss:4.1234 train_time:36859ms step_avg:146.27ms
step:263/3125 train_loss:4.0831 train_time:37002ms step_avg:146.25ms
step:264/3125 train_loss:3.9957 train_time:37149ms step_avg:146.26ms
step:265/3125 train_loss:4.0865 train_time:37294ms step_avg:146.25ms
step:266/3125 train_loss:3.9567 train_time:37441ms step_avg:146.25ms
step:267/3125 train_loss:4.0135 train_time:37587ms step_avg:146.25ms
step:268/3125 train_loss:4.0188 train_time:37730ms step_avg:146.24ms
step:269/3125 train_loss:4.0496 train_time:37873ms step_avg:146.23ms
step:270/3125 train_loss:3.9585 train_time:38015ms step_avg:146.21ms
step:271/3125 train_loss:4.1916 train_time:38162ms step_avg:146.21ms
step:272/3125 train_loss:4.0748 train_time:38307ms step_avg:146.21ms
step:273/3125 train_loss:4.0059 train_time:38454ms step_avg:146.21ms
step:274/3125 train_loss:4.0534 train_time:38599ms step_avg:146.21ms
step:275/3125 train_loss:4.1278 train_time:38745ms step_avg:146.21ms
step:276/3125 train_loss:4.1556 train_time:38889ms step_avg:146.20ms
step:277/3125 train_loss:4.3194 train_time:39031ms step_avg:146.18ms
step:278/3125 train_loss:4.1199 train_time:39176ms step_avg:146.18ms
step:279/3125 train_loss:4.1731 train_time:39322ms step_avg:146.18ms
step:280/3125 train_loss:4.0826 train_time:39470ms step_avg:146.18ms
step:281/3125 train_loss:4.2239 train_time:39613ms step_avg:146.17ms
step:282/3125 train_loss:4.0422 train_time:39757ms step_avg:146.16ms
step:283/3125 train_loss:4.0346 train_time:39901ms step_avg:146.16ms
step:284/3125 train_loss:4.0012 train_time:40046ms step_avg:146.15ms
step:285/3125 train_loss:4.1357 train_time:40191ms step_avg:146.15ms
step:286/3125 train_loss:4.1425 train_time:40334ms step_avg:146.14ms
step:287/3125 train_loss:4.1776 train_time:40481ms step_avg:146.14ms
step:288/3125 train_loss:3.9993 train_time:40626ms step_avg:146.14ms
step:289/3125 train_loss:4.1055 train_time:40771ms step_avg:146.13ms
step:290/3125 train_loss:3.9588 train_time:40914ms step_avg:146.12ms
step:291/3125 train_loss:3.9531 train_time:41058ms step_avg:146.12ms
step:292/3125 train_loss:4.0216 train_time:41203ms step_avg:146.11ms
step:293/3125 train_loss:3.9536 train_time:41348ms step_avg:146.11ms
step:294/3125 train_loss:3.9942 train_time:41492ms step_avg:146.10ms
step:295/3125 train_loss:4.0420 train_time:41637ms step_avg:146.09ms
step:296/3125 train_loss:3.9250 train_time:41782ms step_avg:146.09ms
step:297/3125 train_loss:3.9467 train_time:41927ms step_avg:146.09ms
step:298/3125 train_loss:3.9528 train_time:42071ms step_avg:146.08ms
step:299/3125 train_loss:4.0572 train_time:42214ms step_avg:146.07ms
step:300/3125 train_loss:3.9137 train_time:42359ms step_avg:146.07ms
step:301/3125 train_loss:4.0538 train_time:42504ms step_avg:146.06ms
step:302/3125 train_loss:4.0634 train_time:42649ms step_avg:146.06ms
step:303/3125 train_loss:4.0174 train_time:42794ms step_avg:146.05ms
step:304/3125 train_loss:4.0618 train_time:42938ms step_avg:146.05ms
step:305/3125 train_loss:4.0440 train_time:43083ms step_avg:146.04ms
step:306/3125 train_loss:4.5347 train_time:43227ms step_avg:146.04ms
step:307/3125 train_loss:4.0212 train_time:43372ms step_avg:146.03ms
step:308/3125 train_loss:3.9232 train_time:43515ms step_avg:146.02ms
step:309/3125 train_loss:4.0724 train_time:43660ms step_avg:146.02ms
step:310/3125 train_loss:3.9394 train_time:43806ms step_avg:146.02ms
step:311/3125 train_loss:4.1689 train_time:43951ms step_avg:146.02ms
step:312/3125 train_loss:4.0116 train_time:44095ms step_avg:146.01ms
step:313/3125 train_loss:3.9582 train_time:44239ms step_avg:146.00ms
step:314/3125 train_loss:4.0308 train_time:44386ms step_avg:146.01ms
step:315/3125 train_loss:4.1611 train_time:44531ms step_avg:146.00ms
step:316/3125 train_loss:4.0326 train_time:44675ms step_avg:146.00ms
step:317/3125 train_loss:3.8720 train_time:44820ms step_avg:145.99ms
step:318/3125 train_loss:3.9574 train_time:44966ms step_avg:145.99ms
step:319/3125 train_loss:3.9923 train_time:45111ms step_avg:145.99ms
step:320/3125 train_loss:3.9715 train_time:45255ms step_avg:145.98ms
step:321/3125 train_loss:4.0876 train_time:45399ms step_avg:145.98ms
step:322/3125 train_loss:4.0293 train_time:45545ms step_avg:145.98ms
step:323/3125 train_loss:4.0167 train_time:45690ms step_avg:145.98ms
step:324/3125 train_loss:4.0917 train_time:45834ms step_avg:145.97ms
step:325/3125 train_loss:4.0336 train_time:45978ms step_avg:145.96ms
step:326/3125 train_loss:4.1035 train_time:46122ms step_avg:145.96ms
step:327/3125 train_loss:3.9721 train_time:46268ms step_avg:145.96ms
step:328/3125 train_loss:4.4674 train_time:46414ms step_avg:145.95ms
step:329/3125 train_loss:4.1538 train_time:46557ms step_avg:145.95ms
step:330/3125 train_loss:3.8926 train_time:46701ms step_avg:145.94ms
step:331/3125 train_loss:3.8342 train_time:46847ms step_avg:145.94ms
step:332/3125 train_loss:4.0632 train_time:46993ms step_avg:145.94ms
step:333/3125 train_loss:3.9819 train_time:47136ms step_avg:145.93ms
step:334/3125 train_loss:3.9554 train_time:47280ms step_avg:145.93ms
step:335/3125 train_loss:3.9226 train_time:47427ms step_avg:145.93ms
step:336/3125 train_loss:4.0939 train_time:47571ms step_avg:145.92ms
step:337/3125 train_loss:4.0444 train_time:47716ms step_avg:145.92ms
step:338/3125 train_loss:4.5011 train_time:47860ms step_avg:145.91ms
step:339/3125 train_loss:4.0138 train_time:48005ms step_avg:145.91ms
step:340/3125 train_loss:3.9640 train_time:48150ms step_avg:145.91ms
step:341/3125 train_loss:4.0121 train_time:48293ms step_avg:145.90ms
step:342/3125 train_loss:3.9264 train_time:48436ms step_avg:145.89ms
step:343/3125 train_loss:3.8911 train_time:48583ms step_avg:145.89ms
step:344/3125 train_loss:3.9270 train_time:48728ms step_avg:145.89ms
step:345/3125 train_loss:4.0757 train_time:48874ms step_avg:145.89ms
step:346/3125 train_loss:3.9126 train_time:49017ms step_avg:145.88ms
step:347/3125 train_loss:3.8503 train_time:49163ms step_avg:145.88ms
step:348/3125 train_loss:3.8833 train_time:49308ms step_avg:145.88ms
step:349/3125 train_loss:3.9431 train_time:49453ms step_avg:145.88ms
step:350/3125 train_loss:3.9093 train_time:49596ms step_avg:145.87ms
step:351/3125 train_loss:3.6479 train_time:49742ms step_avg:145.87ms
step:352/3125 train_loss:3.8994 train_time:49888ms step_avg:145.87ms
step:353/3125 train_loss:4.2482 train_time:50032ms step_avg:145.87ms
step:354/3125 train_loss:3.7389 train_time:50176ms step_avg:145.86ms
step:355/3125 train_loss:4.0083 train_time:50320ms step_avg:145.85ms
step:356/3125 train_loss:3.8654 train_time:50465ms step_avg:145.85ms
step:357/3125 train_loss:3.9711 train_time:50610ms step_avg:145.85ms
step:358/3125 train_loss:3.8817 train_time:50755ms step_avg:145.85ms
step:359/3125 train_loss:3.9326 train_time:50899ms step_avg:145.84ms
step:360/3125 train_loss:3.9312 train_time:51046ms step_avg:145.85ms
step:361/3125 train_loss:3.5231 train_time:51192ms step_avg:145.85ms
step:362/3125 train_loss:4.1080 train_time:51335ms step_avg:145.84ms
step:363/3125 train_loss:4.0015 train_time:51479ms step_avg:145.83ms
step:364/3125 train_loss:3.9266 train_time:51625ms step_avg:145.83ms
step:365/3125 train_loss:3.8229 train_time:51770ms step_avg:145.83ms
step:366/3125 train_loss:3.9955 train_time:51914ms step_avg:145.83ms
step:367/3125 train_loss:3.9489 train_time:52059ms step_avg:145.82ms
step:368/3125 train_loss:3.9413 train_time:52204ms step_avg:145.82ms
step:369/3125 train_loss:3.9292 train_time:52350ms step_avg:145.82ms
step:370/3125 train_loss:3.8285 train_time:52493ms step_avg:145.81ms
step:371/3125 train_loss:3.9686 train_time:52639ms step_avg:145.81ms
step:372/3125 train_loss:3.8299 train_time:52786ms step_avg:145.82ms
step:373/3125 train_loss:3.7779 train_time:52930ms step_avg:145.81ms
step:374/3125 train_loss:4.0003 train_time:53074ms step_avg:145.81ms
step:375/3125 train_loss:3.9239 train_time:53220ms step_avg:145.81ms
step:375/3125 val_loss:3.9147 train_time:53260ms step_avg:145.92ms
step:376/3125 train_loss:3.8919 train_time:53377ms step_avg:145.84ms
step:377/3125 train_loss:3.9542 train_time:53524ms step_avg:145.84ms
step:378/3125 train_loss:3.8789 train_time:53819ms step_avg:146.25ms
step:379/3125 train_loss:3.9351 train_time:53975ms step_avg:146.27ms
step:380/3125 train_loss:3.9588 train_time:54292ms step_avg:146.74ms
step:381/3125 train_loss:4.0300 train_time:54434ms step_avg:146.72ms
step:382/3125 train_loss:3.9317 train_time:54578ms step_avg:146.72ms
step:383/3125 train_loss:3.8959 train_time:54720ms step_avg:146.70ms
step:384/3125 train_loss:3.8785 train_time:54862ms step_avg:146.69ms
step:385/3125 train_loss:3.9540 train_time:55005ms step_avg:146.68ms
step:386/3125 train_loss:3.8662 train_time:55155ms step_avg:146.69ms
step:387/3125 train_loss:3.9714 train_time:55302ms step_avg:146.69ms
step:388/3125 train_loss:4.1642 train_time:55448ms step_avg:146.69ms
step:389/3125 train_loss:3.8816 train_time:55592ms step_avg:146.68ms
step:390/3125 train_loss:3.8795 train_time:55736ms step_avg:146.67ms
step:391/3125 train_loss:3.9809 train_time:55879ms step_avg:146.66ms
step:392/3125 train_loss:3.9018 train_time:56022ms step_avg:146.65ms
step:393/3125 train_loss:4.0102 train_time:56167ms step_avg:146.65ms
step:394/3125 train_loss:3.8451 train_time:56316ms step_avg:146.66ms
step:395/3125 train_loss:3.9790 train_time:56460ms step_avg:146.65ms
step:396/3125 train_loss:3.7190 train_time:56605ms step_avg:146.65ms
step:397/3125 train_loss:3.9297 train_time:56749ms step_avg:146.64ms
step:398/3125 train_loss:3.9554 train_time:56893ms step_avg:146.63ms
step:399/3125 train_loss:3.9633 train_time:57037ms step_avg:146.62ms
step:400/3125 train_loss:3.8721 train_time:57182ms step_avg:146.62ms
step:401/3125 train_loss:3.9131 train_time:57327ms step_avg:146.62ms
step:402/3125 train_loss:3.9979 train_time:57473ms step_avg:146.61ms
step:403/3125 train_loss:3.9244 train_time:57619ms step_avg:146.61ms
step:404/3125 train_loss:4.0363 train_time:57762ms step_avg:146.60ms
step:405/3125 train_loss:3.7791 train_time:57906ms step_avg:146.60ms
step:406/3125 train_loss:3.8852 train_time:58050ms step_avg:146.59ms
step:407/3125 train_loss:4.1833 train_time:58195ms step_avg:146.59ms
step:408/3125 train_loss:3.8819 train_time:58341ms step_avg:146.58ms
step:409/3125 train_loss:3.9061 train_time:58484ms step_avg:146.58ms
step:410/3125 train_loss:3.9502 train_time:58629ms step_avg:146.57ms
step:411/3125 train_loss:3.8415 train_time:58773ms step_avg:146.57ms
step:412/3125 train_loss:3.8473 train_time:58918ms step_avg:146.56ms
step:413/3125 train_loss:4.2836 train_time:59062ms step_avg:146.55ms
step:414/3125 train_loss:3.7147 train_time:59205ms step_avg:146.55ms
step:415/3125 train_loss:4.0901 train_time:59351ms step_avg:146.55ms
step:416/3125 train_loss:3.8476 train_time:59496ms step_avg:146.54ms
step:417/3125 train_loss:3.8556 train_time:59641ms step_avg:146.54ms
step:418/3125 train_loss:4.0404 train_time:59786ms step_avg:146.53ms
step:419/3125 train_loss:3.7780 train_time:59931ms step_avg:146.53ms
step:420/3125 train_loss:3.9002 train_time:60076ms step_avg:146.53ms
step:421/3125 train_loss:3.8162 train_time:60220ms step_avg:146.52ms
step:422/3125 train_loss:3.7431 train_time:60364ms step_avg:146.52ms
step:423/3125 train_loss:3.8710 train_time:60511ms step_avg:146.51ms
step:424/3125 train_loss:3.9645 train_time:60656ms step_avg:146.51ms
step:425/3125 train_loss:3.7130 train_time:60800ms step_avg:146.51ms
step:426/3125 train_loss:3.9024 train_time:60945ms step_avg:146.50ms
step:427/3125 train_loss:3.7798 train_time:61090ms step_avg:146.50ms
step:428/3125 train_loss:3.9926 train_time:61236ms step_avg:146.50ms
step:429/3125 train_loss:3.9114 train_time:61380ms step_avg:146.49ms
step:430/3125 train_loss:3.8492 train_time:61525ms step_avg:146.49ms
step:431/3125 train_loss:3.8185 train_time:61670ms step_avg:146.48ms
step:432/3125 train_loss:3.7184 train_time:61816ms step_avg:146.48ms
step:433/3125 train_loss:3.8583 train_time:61960ms step_avg:146.48ms
step:434/3125 train_loss:3.9126 train_time:62104ms step_avg:146.47ms
step:435/3125 train_loss:3.8572 train_time:62249ms step_avg:146.47ms
step:436/3125 train_loss:3.9031 train_time:62395ms step_avg:146.47ms
step:437/3125 train_loss:3.9216 train_time:62540ms step_avg:146.46ms
step:438/3125 train_loss:3.7938 train_time:62685ms step_avg:146.46ms
step:439/3125 train_loss:3.8085 train_time:62829ms step_avg:146.46ms
step:440/3125 train_loss:3.7928 train_time:62974ms step_avg:146.45ms
step:441/3125 train_loss:3.9799 train_time:63118ms step_avg:146.44ms
step:442/3125 train_loss:3.8556 train_time:63262ms step_avg:146.44ms
step:443/3125 train_loss:3.8361 train_time:63406ms step_avg:146.43ms
step:444/3125 train_loss:3.7379 train_time:63553ms step_avg:146.44ms
step:445/3125 train_loss:4.0143 train_time:63698ms step_avg:146.43ms
step:446/3125 train_loss:3.9341 train_time:63842ms step_avg:146.43ms
step:447/3125 train_loss:3.9227 train_time:63988ms step_avg:146.42ms
step:448/3125 train_loss:3.8494 train_time:64135ms step_avg:146.43ms
step:449/3125 train_loss:3.9497 train_time:64279ms step_avg:146.42ms
step:450/3125 train_loss:3.7834 train_time:64422ms step_avg:146.41ms
step:451/3125 train_loss:3.8149 train_time:64567ms step_avg:146.41ms
step:452/3125 train_loss:3.6815 train_time:64714ms step_avg:146.41ms
step:453/3125 train_loss:3.7988 train_time:64859ms step_avg:146.41ms
step:454/3125 train_loss:3.7768 train_time:65002ms step_avg:146.40ms
step:455/3125 train_loss:3.7326 train_time:65146ms step_avg:146.40ms
step:456/3125 train_loss:3.9393 train_time:65291ms step_avg:146.39ms
step:457/3125 train_loss:3.8204 train_time:65436ms step_avg:146.39ms
step:458/3125 train_loss:3.8876 train_time:65579ms step_avg:146.38ms
step:459/3125 train_loss:3.9263 train_time:65724ms step_avg:146.38ms
step:460/3125 train_loss:3.7324 train_time:65869ms step_avg:146.38ms
step:461/3125 train_loss:3.9028 train_time:66016ms step_avg:146.38ms
step:462/3125 train_loss:3.7979 train_time:66161ms step_avg:146.37ms
step:463/3125 train_loss:3.8246 train_time:66304ms step_avg:146.37ms
step:464/3125 train_loss:3.8665 train_time:66448ms step_avg:146.36ms
step:465/3125 train_loss:3.8114 train_time:66594ms step_avg:146.36ms
step:466/3125 train_loss:3.8191 train_time:66739ms step_avg:146.36ms
step:467/3125 train_loss:3.9047 train_time:66883ms step_avg:146.35ms
step:468/3125 train_loss:3.9206 train_time:67028ms step_avg:146.35ms
step:469/3125 train_loss:3.9012 train_time:67173ms step_avg:146.35ms
step:470/3125 train_loss:3.7903 train_time:67318ms step_avg:146.34ms
step:471/3125 train_loss:3.8690 train_time:67462ms step_avg:146.34ms
step:472/3125 train_loss:3.9142 train_time:67605ms step_avg:146.33ms
step:473/3125 train_loss:3.8719 train_time:67750ms step_avg:146.33ms
step:474/3125 train_loss:3.8183 train_time:67895ms step_avg:146.33ms
step:475/3125 train_loss:3.6832 train_time:68039ms step_avg:146.32ms
step:476/3125 train_loss:4.1124 train_time:68183ms step_avg:146.32ms
step:477/3125 train_loss:3.8667 train_time:68327ms step_avg:146.31ms
step:478/3125 train_loss:3.6877 train_time:68474ms step_avg:146.31ms
step:479/3125 train_loss:3.9166 train_time:68618ms step_avg:146.31ms
step:480/3125 train_loss:3.8715 train_time:68762ms step_avg:146.30ms
step:481/3125 train_loss:4.0171 train_time:68907ms step_avg:146.30ms
step:482/3125 train_loss:3.8280 train_time:69052ms step_avg:146.30ms
step:483/3125 train_loss:3.6312 train_time:69196ms step_avg:146.29ms
step:484/3125 train_loss:3.9135 train_time:69340ms step_avg:146.29ms
step:485/3125 train_loss:3.7672 train_time:69484ms step_avg:146.28ms
step:486/3125 train_loss:3.7723 train_time:69629ms step_avg:146.28ms
step:487/3125 train_loss:3.7029 train_time:69775ms step_avg:146.28ms
step:488/3125 train_loss:3.7781 train_time:69920ms step_avg:146.28ms
step:489/3125 train_loss:3.9745 train_time:70063ms step_avg:146.27ms
step:490/3125 train_loss:3.8164 train_time:70208ms step_avg:146.27ms
step:491/3125 train_loss:3.7003 train_time:70354ms step_avg:146.27ms
step:492/3125 train_loss:3.7217 train_time:70498ms step_avg:146.26ms
step:493/3125 train_loss:3.8353 train_time:70643ms step_avg:146.26ms
step:494/3125 train_loss:3.6799 train_time:70787ms step_avg:146.25ms
step:495/3125 train_loss:3.8168 train_time:70934ms step_avg:146.25ms
step:496/3125 train_loss:3.7580 train_time:71077ms step_avg:146.25ms
step:497/3125 train_loss:3.6295 train_time:71221ms step_avg:146.24ms
step:498/3125 train_loss:3.8322 train_time:71367ms step_avg:146.24ms
step:499/3125 train_loss:3.9035 train_time:71513ms step_avg:146.24ms
step:500/3125 train_loss:3.9341 train_time:71660ms step_avg:146.24ms
step:500/3125 val_loss:3.8121 train_time:71698ms step_avg:146.32ms
step:501/3125 train_loss:3.8486 train_time:71811ms step_avg:146.26ms
step:502/3125 train_loss:3.9086 train_time:71961ms step_avg:146.26ms
step:503/3125 train_loss:3.8500 train_time:72103ms step_avg:146.25ms
step:504/3125 train_loss:3.8787 train_time:72245ms step_avg:146.25ms
step:505/3125 train_loss:3.8289 train_time:72387ms step_avg:146.24ms
step:506/3125 train_loss:3.9192 train_time:72530ms step_avg:146.23ms
step:507/3125 train_loss:3.7534 train_time:72680ms step_avg:146.24ms
step:508/3125 train_loss:3.8677 train_time:72827ms step_avg:146.24ms
step:509/3125 train_loss:3.9392 train_time:72972ms step_avg:146.24ms
step:510/3125 train_loss:3.8751 train_time:73116ms step_avg:146.23ms
step:511/3125 train_loss:3.6917 train_time:73259ms step_avg:146.23ms
step:512/3125 train_loss:3.8882 train_time:73402ms step_avg:146.22ms
step:513/3125 train_loss:3.8268 train_time:73544ms step_avg:146.21ms
step:514/3125 train_loss:3.7912 train_time:73690ms step_avg:146.21ms
step:515/3125 train_loss:3.8914 train_time:73838ms step_avg:146.21ms
step:516/3125 train_loss:3.8443 train_time:73983ms step_avg:146.21ms
step:517/3125 train_loss:4.2031 train_time:74127ms step_avg:146.21ms
step:518/3125 train_loss:3.7990 train_time:74270ms step_avg:146.20ms
step:519/3125 train_loss:3.8901 train_time:74414ms step_avg:146.20ms
step:520/3125 train_loss:3.7796 train_time:74559ms step_avg:146.19ms
step:521/3125 train_loss:3.8054 train_time:74706ms step_avg:146.19ms
step:522/3125 train_loss:3.7560 train_time:74850ms step_avg:146.19ms
step:523/3125 train_loss:3.7611 train_time:74995ms step_avg:146.19ms
step:524/3125 train_loss:4.3958 train_time:75142ms step_avg:146.19ms
step:525/3125 train_loss:3.8576 train_time:75285ms step_avg:146.19ms
step:526/3125 train_loss:3.7928 train_time:75429ms step_avg:146.18ms
step:527/3125 train_loss:3.8030 train_time:75574ms step_avg:146.18ms
step:528/3125 train_loss:3.7677 train_time:75719ms step_avg:146.18ms
step:529/3125 train_loss:3.7307 train_time:75865ms step_avg:146.18ms
step:530/3125 train_loss:3.9557 train_time:76009ms step_avg:146.17ms
step:531/3125 train_loss:3.7544 train_time:76153ms step_avg:146.17ms
step:532/3125 train_loss:4.0224 train_time:76298ms step_avg:146.17ms
step:533/3125 train_loss:3.8357 train_time:76442ms step_avg:146.16ms
step:534/3125 train_loss:3.7675 train_time:76586ms step_avg:146.16ms
step:535/3125 train_loss:3.7908 train_time:76730ms step_avg:146.15ms
step:536/3125 train_loss:3.7258 train_time:76876ms step_avg:146.15ms
step:537/3125 train_loss:3.8556 train_time:77020ms step_avg:146.15ms
step:538/3125 train_loss:3.8392 train_time:77164ms step_avg:146.14ms
step:539/3125 train_loss:3.7373 train_time:77309ms step_avg:146.14ms
step:540/3125 train_loss:4.2385 train_time:77453ms step_avg:146.14ms
step:541/3125 train_loss:3.7788 train_time:77597ms step_avg:146.13ms
step:542/3125 train_loss:3.8937 train_time:77742ms step_avg:146.13ms
step:543/3125 train_loss:3.7089 train_time:77886ms step_avg:146.13ms
step:544/3125 train_loss:3.6902 train_time:78031ms step_avg:146.13ms
step:545/3125 train_loss:3.7721 train_time:78177ms step_avg:146.12ms
step:546/3125 train_loss:3.6987 train_time:78321ms step_avg:146.12ms
step:547/3125 train_loss:3.7452 train_time:78466ms step_avg:146.12ms
step:548/3125 train_loss:3.7515 train_time:78610ms step_avg:146.11ms
step:549/3125 train_loss:3.7351 train_time:78754ms step_avg:146.11ms
step:550/3125 train_loss:3.8315 train_time:78899ms step_avg:146.11ms
step:551/3125 train_loss:3.7198 train_time:79043ms step_avg:146.11ms
step:552/3125 train_loss:3.7355 train_time:79187ms step_avg:146.10ms
step:553/3125 train_loss:4.0651 train_time:79333ms step_avg:146.10ms
step:554/3125 train_loss:3.8618 train_time:79479ms step_avg:146.10ms
step:555/3125 train_loss:3.8202 train_time:79623ms step_avg:146.10ms
step:556/3125 train_loss:3.7622 train_time:79767ms step_avg:146.09ms
step:557/3125 train_loss:3.7981 train_time:79911ms step_avg:146.09ms
step:558/3125 train_loss:3.4615 train_time:80058ms step_avg:146.09ms
step:559/3125 train_loss:3.7198 train_time:80202ms step_avg:146.09ms
step:560/3125 train_loss:3.7623 train_time:80346ms step_avg:146.08ms
step:561/3125 train_loss:3.8071 train_time:80490ms step_avg:146.08ms
step:562/3125 train_loss:3.7171 train_time:80634ms step_avg:146.08ms
step:563/3125 train_loss:3.6627 train_time:80780ms step_avg:146.08ms
step:564/3125 train_loss:3.8668 train_time:80925ms step_avg:146.07ms
step:565/3125 train_loss:3.6833 train_time:81070ms step_avg:146.07ms
step:566/3125 train_loss:3.7932 train_time:81215ms step_avg:146.07ms
step:567/3125 train_loss:3.7349 train_time:81511ms step_avg:146.34ms
step:568/3125 train_loss:3.7098 train_time:81664ms step_avg:146.35ms
step:569/3125 train_loss:3.7894 train_time:81807ms step_avg:146.35ms
step:570/3125 train_loss:3.7668 train_time:82127ms step_avg:146.66ms
step:571/3125 train_loss:3.7991 train_time:82268ms step_avg:146.65ms
step:572/3125 train_loss:3.8760 train_time:82412ms step_avg:146.64ms
step:573/3125 train_loss:3.8320 train_time:82556ms step_avg:146.64ms
step:574/3125 train_loss:3.8394 train_time:82698ms step_avg:146.63ms
step:575/3125 train_loss:3.8861 train_time:82841ms step_avg:146.62ms
step:576/3125 train_loss:3.8413 train_time:82988ms step_avg:146.62ms
step:577/3125 train_loss:3.8682 train_time:83138ms step_avg:146.63ms
step:578/3125 train_loss:3.7872 train_time:83284ms step_avg:146.63ms
step:579/3125 train_loss:3.7917 train_time:83426ms step_avg:146.62ms
step:580/3125 train_loss:3.7773 train_time:83570ms step_avg:146.61ms
step:581/3125 train_loss:3.7058 train_time:83713ms step_avg:146.61ms
step:582/3125 train_loss:3.7386 train_time:83857ms step_avg:146.60ms
step:583/3125 train_loss:3.9627 train_time:84004ms step_avg:146.60ms
step:584/3125 train_loss:3.7300 train_time:84150ms step_avg:146.60ms
step:585/3125 train_loss:3.6966 train_time:84295ms step_avg:146.60ms
step:586/3125 train_loss:3.8864 train_time:84441ms step_avg:146.60ms
step:587/3125 train_loss:3.6388 train_time:84585ms step_avg:146.59ms
step:588/3125 train_loss:3.7740 train_time:84727ms step_avg:146.59ms
step:589/3125 train_loss:3.7548 train_time:84872ms step_avg:146.58ms
step:590/3125 train_loss:4.1018 train_time:85018ms step_avg:146.58ms
step:591/3125 train_loss:3.8897 train_time:85164ms step_avg:146.58ms
step:592/3125 train_loss:3.6261 train_time:85308ms step_avg:146.58ms
step:593/3125 train_loss:3.6445 train_time:85452ms step_avg:146.57ms
step:594/3125 train_loss:3.6280 train_time:85596ms step_avg:146.57ms
step:595/3125 train_loss:3.6769 train_time:85741ms step_avg:146.57ms
step:596/3125 train_loss:4.0432 train_time:85885ms step_avg:146.56ms
step:597/3125 train_loss:3.7557 train_time:86029ms step_avg:146.56ms
step:598/3125 train_loss:3.6914 train_time:86176ms step_avg:146.56ms
step:599/3125 train_loss:3.7694 train_time:86322ms step_avg:146.56ms
step:600/3125 train_loss:3.5900 train_time:86466ms step_avg:146.55ms
step:601/3125 train_loss:3.7058 train_time:86610ms step_avg:146.55ms
step:602/3125 train_loss:3.7466 train_time:86754ms step_avg:146.54ms
step:603/3125 train_loss:3.7679 train_time:86899ms step_avg:146.54ms
step:604/3125 train_loss:3.8944 train_time:87043ms step_avg:146.54ms
step:605/3125 train_loss:3.7361 train_time:87188ms step_avg:146.54ms
step:606/3125 train_loss:3.7259 train_time:87334ms step_avg:146.53ms
step:607/3125 train_loss:3.6822 train_time:87480ms step_avg:146.53ms
step:608/3125 train_loss:3.9298 train_time:87623ms step_avg:146.53ms
step:609/3125 train_loss:3.7542 train_time:87768ms step_avg:146.52ms
step:610/3125 train_loss:3.7215 train_time:87912ms step_avg:146.52ms
step:611/3125 train_loss:3.8211 train_time:88056ms step_avg:146.52ms
step:612/3125 train_loss:3.7191 train_time:88202ms step_avg:146.51ms
step:613/3125 train_loss:3.7126 train_time:88345ms step_avg:146.51ms
step:614/3125 train_loss:3.8734 train_time:88490ms step_avg:146.51ms
step:615/3125 train_loss:3.8222 train_time:88637ms step_avg:146.51ms
step:616/3125 train_loss:3.8028 train_time:88782ms step_avg:146.51ms
step:617/3125 train_loss:3.7289 train_time:88925ms step_avg:146.50ms
step:618/3125 train_loss:3.6809 train_time:89070ms step_avg:146.50ms
step:619/3125 train_loss:3.7921 train_time:89215ms step_avg:146.49ms
step:620/3125 train_loss:3.6942 train_time:89361ms step_avg:146.49ms
step:621/3125 train_loss:3.7004 train_time:89505ms step_avg:146.49ms
step:622/3125 train_loss:4.0166 train_time:89650ms step_avg:146.49ms
step:623/3125 train_loss:3.6941 train_time:89795ms step_avg:146.48ms
step:624/3125 train_loss:3.7212 train_time:89940ms step_avg:146.48ms
step:625/3125 train_loss:3.8073 train_time:90085ms step_avg:146.48ms
step:625/3125 val_loss:3.7372 train_time:90123ms step_avg:146.54ms
step:626/3125 train_loss:3.8258 train_time:90243ms step_avg:146.50ms
step:627/3125 train_loss:3.8507 train_time:90390ms step_avg:146.50ms
step:628/3125 train_loss:3.8423 train_time:90533ms step_avg:146.49ms
step:629/3125 train_loss:3.8796 train_time:90675ms step_avg:146.49ms
step:630/3125 train_loss:3.7047 train_time:90817ms step_avg:146.48ms
step:631/3125 train_loss:3.8336 train_time:90960ms step_avg:146.47ms
step:632/3125 train_loss:3.8570 train_time:91105ms step_avg:146.47ms
step:633/3125 train_loss:3.7630 train_time:91254ms step_avg:146.48ms
step:634/3125 train_loss:3.6962 train_time:91399ms step_avg:146.47ms
step:635/3125 train_loss:3.7925 train_time:91545ms step_avg:146.47ms
step:636/3125 train_loss:4.0499 train_time:91688ms step_avg:146.47ms
step:637/3125 train_loss:3.6486 train_time:91831ms step_avg:146.46ms
step:638/3125 train_loss:3.4720 train_time:91974ms step_avg:146.45ms
step:639/3125 train_loss:3.6924 train_time:92118ms step_avg:146.45ms
step:640/3125 train_loss:3.7290 train_time:92264ms step_avg:146.45ms
step:641/3125 train_loss:3.6769 train_time:92412ms step_avg:146.45ms
step:642/3125 train_loss:3.6902 train_time:92556ms step_avg:146.45ms
step:643/3125 train_loss:3.7310 train_time:92699ms step_avg:146.44ms
step:644/3125 train_loss:3.7234 train_time:92843ms step_avg:146.44ms
step:645/3125 train_loss:3.6633 train_time:92987ms step_avg:146.44ms
step:646/3125 train_loss:3.8773 train_time:93132ms step_avg:146.43ms
step:647/3125 train_loss:3.7840 train_time:93279ms step_avg:146.43ms
step:648/3125 train_loss:3.7786 train_time:93424ms step_avg:146.43ms
step:649/3125 train_loss:3.8119 train_time:93569ms step_avg:146.43ms
step:650/3125 train_loss:3.8712 train_time:93713ms step_avg:146.43ms
step:651/3125 train_loss:3.7329 train_time:93856ms step_avg:146.42ms
step:652/3125 train_loss:3.8649 train_time:93999ms step_avg:146.42ms
step:653/3125 train_loss:3.6882 train_time:94144ms step_avg:146.41ms
step:654/3125 train_loss:3.7696 train_time:94290ms step_avg:146.41ms
step:655/3125 train_loss:3.5389 train_time:94436ms step_avg:146.41ms
step:656/3125 train_loss:3.6822 train_time:94580ms step_avg:146.41ms
step:657/3125 train_loss:3.6896 train_time:94727ms step_avg:146.41ms
step:658/3125 train_loss:3.6161 train_time:94871ms step_avg:146.41ms
step:659/3125 train_loss:3.7992 train_time:95014ms step_avg:146.40ms
step:660/3125 train_loss:3.6973 train_time:95158ms step_avg:146.40ms
step:661/3125 train_loss:3.7869 train_time:95303ms step_avg:146.40ms
step:662/3125 train_loss:3.8623 train_time:95450ms step_avg:146.40ms
step:663/3125 train_loss:3.7780 train_time:95594ms step_avg:146.39ms
step:664/3125 train_loss:3.6568 train_time:95739ms step_avg:146.39ms
step:665/3125 train_loss:3.7311 train_time:95883ms step_avg:146.39ms
step:666/3125 train_loss:3.6072 train_time:96029ms step_avg:146.39ms
step:667/3125 train_loss:3.8874 train_time:96173ms step_avg:146.38ms
step:668/3125 train_loss:3.7289 train_time:96317ms step_avg:146.38ms
step:669/3125 train_loss:3.7429 train_time:96462ms step_avg:146.38ms
step:670/3125 train_loss:3.5880 train_time:96609ms step_avg:146.38ms
step:671/3125 train_loss:3.7046 train_time:96754ms step_avg:146.38ms
step:672/3125 train_loss:3.6637 train_time:96897ms step_avg:146.37ms
step:673/3125 train_loss:3.6838 train_time:97041ms step_avg:146.37ms
step:674/3125 train_loss:3.9569 train_time:97187ms step_avg:146.37ms
step:675/3125 train_loss:3.7480 train_time:97333ms step_avg:146.36ms
step:676/3125 train_loss:3.8203 train_time:97477ms step_avg:146.36ms
step:677/3125 train_loss:3.6028 train_time:97622ms step_avg:146.36ms
step:678/3125 train_loss:3.7083 train_time:97768ms step_avg:146.36ms
step:679/3125 train_loss:3.6560 train_time:97913ms step_avg:146.36ms
step:680/3125 train_loss:3.7944 train_time:98057ms step_avg:146.35ms
step:681/3125 train_loss:3.6910 train_time:98201ms step_avg:146.35ms
step:682/3125 train_loss:3.7224 train_time:98346ms step_avg:146.35ms
step:683/3125 train_loss:3.8099 train_time:98491ms step_avg:146.35ms
step:684/3125 train_loss:3.8430 train_time:98637ms step_avg:146.35ms
step:685/3125 train_loss:3.7518 train_time:98780ms step_avg:146.34ms
step:686/3125 train_loss:3.8137 train_time:98924ms step_avg:146.34ms
step:687/3125 train_loss:3.7431 train_time:99069ms step_avg:146.34ms
step:688/3125 train_loss:3.7912 train_time:99214ms step_avg:146.33ms
step:689/3125 train_loss:3.3862 train_time:99357ms step_avg:146.33ms
step:690/3125 train_loss:3.5176 train_time:99501ms step_avg:146.33ms
step:691/3125 train_loss:3.6622 train_time:99648ms step_avg:146.33ms
step:692/3125 train_loss:3.5400 train_time:99793ms step_avg:146.32ms
step:693/3125 train_loss:3.7499 train_time:99937ms step_avg:146.32ms
step:694/3125 train_loss:3.7665 train_time:100082ms step_avg:146.32ms
step:695/3125 train_loss:3.6537 train_time:100227ms step_avg:146.32ms
step:696/3125 train_loss:3.6497 train_time:100371ms step_avg:146.31ms
step:697/3125 train_loss:3.9639 train_time:100515ms step_avg:146.31ms
step:698/3125 train_loss:3.7070 train_time:100659ms step_avg:146.31ms
step:699/3125 train_loss:3.7543 train_time:100804ms step_avg:146.30ms
step:700/3125 train_loss:3.9093 train_time:100950ms step_avg:146.30ms
step:701/3125 train_loss:3.6885 train_time:101094ms step_avg:146.30ms
step:702/3125 train_loss:3.6531 train_time:101238ms step_avg:146.30ms
step:703/3125 train_loss:3.6366 train_time:101382ms step_avg:146.29ms
step:704/3125 train_loss:3.5949 train_time:101528ms step_avg:146.29ms
step:705/3125 train_loss:3.6709 train_time:101674ms step_avg:146.29ms
step:706/3125 train_loss:3.6716 train_time:101817ms step_avg:146.29ms
step:707/3125 train_loss:3.6856 train_time:101962ms step_avg:146.29ms
step:708/3125 train_loss:3.7549 train_time:102110ms step_avg:146.29ms
step:709/3125 train_loss:3.7037 train_time:102254ms step_avg:146.29ms
step:710/3125 train_loss:3.6834 train_time:102397ms step_avg:146.28ms
step:711/3125 train_loss:3.6518 train_time:102542ms step_avg:146.28ms
step:712/3125 train_loss:3.6981 train_time:102689ms step_avg:146.28ms
step:713/3125 train_loss:3.7534 train_time:102833ms step_avg:146.28ms
step:714/3125 train_loss:3.7608 train_time:102977ms step_avg:146.27ms
step:715/3125 train_loss:3.6737 train_time:103121ms step_avg:146.27ms
step:716/3125 train_loss:3.6798 train_time:103267ms step_avg:146.27ms
step:717/3125 train_loss:3.6894 train_time:103412ms step_avg:146.27ms
step:718/3125 train_loss:3.8406 train_time:103556ms step_avg:146.27ms
step:719/3125 train_loss:3.6976 train_time:103701ms step_avg:146.26ms
step:720/3125 train_loss:3.7804 train_time:103845ms step_avg:146.26ms
step:721/3125 train_loss:3.9433 train_time:103990ms step_avg:146.26ms
step:722/3125 train_loss:3.5723 train_time:104135ms step_avg:146.26ms
step:723/3125 train_loss:3.8374 train_time:104278ms step_avg:146.25ms
step:724/3125 train_loss:3.8861 train_time:104422ms step_avg:146.25ms
step:725/3125 train_loss:3.6714 train_time:104568ms step_avg:146.25ms
step:726/3125 train_loss:3.7565 train_time:104713ms step_avg:146.25ms
step:727/3125 train_loss:3.6459 train_time:104857ms step_avg:146.24ms
step:728/3125 train_loss:3.6762 train_time:105002ms step_avg:146.24ms
step:729/3125 train_loss:3.8437 train_time:105148ms step_avg:146.24ms
step:730/3125 train_loss:3.7835 train_time:105292ms step_avg:146.24ms
step:731/3125 train_loss:3.7761 train_time:105436ms step_avg:146.24ms
step:732/3125 train_loss:3.6739 train_time:105581ms step_avg:146.23ms
step:733/3125 train_loss:3.6975 train_time:105730ms step_avg:146.24ms
step:734/3125 train_loss:3.9282 train_time:105873ms step_avg:146.23ms
step:735/3125 train_loss:3.6684 train_time:106016ms step_avg:146.23ms
step:736/3125 train_loss:3.7299 train_time:106161ms step_avg:146.23ms
step:737/3125 train_loss:3.8460 train_time:106306ms step_avg:146.23ms
step:738/3125 train_loss:3.7719 train_time:106451ms step_avg:146.22ms
step:739/3125 train_loss:3.7089 train_time:106595ms step_avg:146.22ms
step:740/3125 train_loss:3.6010 train_time:106739ms step_avg:146.22ms
step:741/3125 train_loss:4.2334 train_time:106883ms step_avg:146.21ms
step:742/3125 train_loss:3.6025 train_time:107029ms step_avg:146.21ms
step:743/3125 train_loss:3.6815 train_time:107175ms step_avg:146.21ms
step:744/3125 train_loss:3.6913 train_time:107319ms step_avg:146.21ms
step:745/3125 train_loss:3.7534 train_time:107462ms step_avg:146.21ms
step:746/3125 train_loss:3.7201 train_time:107610ms step_avg:146.21ms
step:747/3125 train_loss:3.7069 train_time:107755ms step_avg:146.21ms
step:748/3125 train_loss:3.7408 train_time:107897ms step_avg:146.20ms
step:749/3125 train_loss:3.6698 train_time:108043ms step_avg:146.20ms
step:750/3125 train_loss:3.6695 train_time:108189ms step_avg:146.20ms
step:750/3125 val_loss:3.6804 train_time:108226ms step_avg:146.25ms
step:751/3125 train_loss:3.7085 train_time:108341ms step_avg:146.21ms
step:752/3125 train_loss:3.6721 train_time:108489ms step_avg:146.21ms
step:753/3125 train_loss:3.7079 train_time:108632ms step_avg:146.21ms
step:754/3125 train_loss:3.7277 train_time:108775ms step_avg:146.20ms
step:755/3125 train_loss:3.6963 train_time:108919ms step_avg:146.20ms
step:756/3125 train_loss:3.7736 train_time:109215ms step_avg:146.40ms
step:757/3125 train_loss:3.5964 train_time:109367ms step_avg:146.41ms
step:758/3125 train_loss:3.8379 train_time:109510ms step_avg:146.40ms
step:759/3125 train_loss:3.7576 train_time:109653ms step_avg:146.40ms
step:760/3125 train_loss:3.6974 train_time:109972ms step_avg:146.63ms
step:761/3125 train_loss:3.8015 train_time:110114ms step_avg:146.62ms
step:762/3125 train_loss:3.5112 train_time:110258ms step_avg:146.62ms
step:763/3125 train_loss:3.6580 train_time:110400ms step_avg:146.61ms
step:764/3125 train_loss:3.7696 train_time:110542ms step_avg:146.61ms
step:765/3125 train_loss:3.4319 train_time:110685ms step_avg:146.60ms
step:766/3125 train_loss:3.8484 train_time:110833ms step_avg:146.60ms
step:767/3125 train_loss:3.6928 train_time:110984ms step_avg:146.61ms
step:768/3125 train_loss:3.6674 train_time:111129ms step_avg:146.61ms
step:769/3125 train_loss:3.6878 train_time:111273ms step_avg:146.60ms
step:770/3125 train_loss:3.7064 train_time:111416ms step_avg:146.60ms
step:771/3125 train_loss:3.7624 train_time:111558ms step_avg:146.59ms
step:772/3125 train_loss:3.9907 train_time:111701ms step_avg:146.59ms
step:773/3125 train_loss:3.5677 train_time:111850ms step_avg:146.59ms
step:774/3125 train_loss:3.7601 train_time:112000ms step_avg:146.60ms
step:775/3125 train_loss:3.7455 train_time:112143ms step_avg:146.59ms
step:776/3125 train_loss:3.7133 train_time:112287ms step_avg:146.59ms
step:777/3125 train_loss:3.5126 train_time:112431ms step_avg:146.59ms
step:778/3125 train_loss:3.5190 train_time:112576ms step_avg:146.58ms
step:779/3125 train_loss:3.5878 train_time:112719ms step_avg:146.58ms
step:780/3125 train_loss:3.6787 train_time:112863ms step_avg:146.58ms
step:781/3125 train_loss:3.7076 train_time:113011ms step_avg:146.58ms
step:782/3125 train_loss:3.7659 train_time:113160ms step_avg:146.58ms
step:783/3125 train_loss:3.6867 train_time:113304ms step_avg:146.58ms
step:784/3125 train_loss:3.6786 train_time:113449ms step_avg:146.57ms
step:785/3125 train_loss:3.6917 train_time:113593ms step_avg:146.57ms
step:786/3125 train_loss:3.6604 train_time:113737ms step_avg:146.57ms
step:787/3125 train_loss:3.5609 train_time:113882ms step_avg:146.57ms
step:788/3125 train_loss:3.8153 train_time:114028ms step_avg:146.57ms
step:789/3125 train_loss:3.6122 train_time:114177ms step_avg:146.57ms
step:790/3125 train_loss:3.6724 train_time:114320ms step_avg:146.56ms
step:791/3125 train_loss:3.7325 train_time:114464ms step_avg:146.56ms
step:792/3125 train_loss:3.8681 train_time:114607ms step_avg:146.56ms
step:793/3125 train_loss:3.8736 train_time:114751ms step_avg:146.55ms
step:794/3125 train_loss:3.5854 train_time:114898ms step_avg:146.55ms
step:795/3125 train_loss:3.7045 train_time:115042ms step_avg:146.55ms
step:796/3125 train_loss:3.7667 train_time:115189ms step_avg:146.55ms
step:797/3125 train_loss:3.8737 train_time:115336ms step_avg:146.55ms
step:798/3125 train_loss:3.6257 train_time:115481ms step_avg:146.55ms
step:799/3125 train_loss:3.7757 train_time:115625ms step_avg:146.55ms
step:800/3125 train_loss:3.6615 train_time:115770ms step_avg:146.54ms
step:801/3125 train_loss:3.6407 train_time:115915ms step_avg:146.54ms
step:802/3125 train_loss:3.7370 train_time:116060ms step_avg:146.54ms
step:803/3125 train_loss:3.6081 train_time:116207ms step_avg:146.54ms
step:804/3125 train_loss:3.6324 train_time:116354ms step_avg:146.54ms
step:805/3125 train_loss:3.7433 train_time:116498ms step_avg:146.54ms
step:806/3125 train_loss:3.6337 train_time:116641ms step_avg:146.53ms
step:807/3125 train_loss:3.6573 train_time:116785ms step_avg:146.53ms
step:808/3125 train_loss:3.7514 train_time:116929ms step_avg:146.53ms
step:809/3125 train_loss:3.6704 train_time:117077ms step_avg:146.53ms
step:810/3125 train_loss:3.5957 train_time:117222ms step_avg:146.53ms
step:811/3125 train_loss:3.6707 train_time:117366ms step_avg:146.52ms
step:812/3125 train_loss:3.7046 train_time:117510ms step_avg:146.52ms
step:813/3125 train_loss:3.6988 train_time:117656ms step_avg:146.52ms
step:814/3125 train_loss:3.7396 train_time:117800ms step_avg:146.52ms
step:815/3125 train_loss:3.6846 train_time:117944ms step_avg:146.51ms
step:816/3125 train_loss:3.6694 train_time:118089ms step_avg:146.51ms
step:817/3125 train_loss:3.7761 train_time:118235ms step_avg:146.51ms
step:818/3125 train_loss:3.8718 train_time:118380ms step_avg:146.51ms
step:819/3125 train_loss:3.6284 train_time:118524ms step_avg:146.51ms
step:820/3125 train_loss:3.8267 train_time:118668ms step_avg:146.50ms
step:821/3125 train_loss:3.6159 train_time:118813ms step_avg:146.50ms
step:822/3125 train_loss:3.6565 train_time:118958ms step_avg:146.50ms
step:823/3125 train_loss:3.7888 train_time:119103ms step_avg:146.50ms
step:824/3125 train_loss:3.6921 train_time:119249ms step_avg:146.50ms
step:825/3125 train_loss:3.6205 train_time:119395ms step_avg:146.50ms
step:826/3125 train_loss:3.7197 train_time:119539ms step_avg:146.49ms
step:827/3125 train_loss:3.6114 train_time:119683ms step_avg:146.49ms
step:828/3125 train_loss:3.8400 train_time:119828ms step_avg:146.49ms
step:829/3125 train_loss:3.7259 train_time:119976ms step_avg:146.49ms
step:830/3125 train_loss:3.7738 train_time:120121ms step_avg:146.49ms
step:831/3125 train_loss:3.6449 train_time:120264ms step_avg:146.48ms
step:832/3125 train_loss:3.6985 train_time:120408ms step_avg:146.48ms
step:833/3125 train_loss:3.6227 train_time:120554ms step_avg:146.48ms
step:834/3125 train_loss:3.7540 train_time:120699ms step_avg:146.48ms
step:835/3125 train_loss:3.5868 train_time:120843ms step_avg:146.48ms
step:836/3125 train_loss:3.5698 train_time:120987ms step_avg:146.47ms
step:837/3125 train_loss:3.8234 train_time:121132ms step_avg:146.47ms
step:838/3125 train_loss:3.5229 train_time:121277ms step_avg:146.47ms
step:839/3125 train_loss:3.7022 train_time:121423ms step_avg:146.47ms
step:840/3125 train_loss:3.5327 train_time:121566ms step_avg:146.47ms
step:841/3125 train_loss:3.5798 train_time:121710ms step_avg:146.46ms
step:842/3125 train_loss:3.6640 train_time:121856ms step_avg:146.46ms
step:843/3125 train_loss:3.6882 train_time:122002ms step_avg:146.46ms
step:844/3125 train_loss:3.6844 train_time:122145ms step_avg:146.46ms
step:845/3125 train_loss:3.5383 train_time:122290ms step_avg:146.46ms
step:846/3125 train_loss:3.7693 train_time:122436ms step_avg:146.45ms
