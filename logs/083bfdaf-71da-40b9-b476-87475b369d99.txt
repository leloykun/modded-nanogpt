====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import flex_attention, create_block_mask, BlockMask, _score_mod_signature
from torch._inductor.lowering import make_pointwise, register_lowering
# Some internal torch.compile details
from torch._inductor.virtualized import ops
from functools import partial
flex_attention = torch.compile(flex_attention, dynamic=False)
create_block_mask = torch.compile(create_block_mask, dynamic=False)

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                p.grad = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(p.grad, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# Attention Tanh softcapping

@torch.library.custom_op("approx::tanh", mutates_args=())
def _tanh_approx(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

@_tanh_approx.register_fake
def _(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

def _tanh_approx_lowering(inp):
    fn = partial(ops.inline_asm_elementwise, asm="tanh.approx.f32 $0, $1;")
    return make_pointwise(fn)(inp)

register_lowering(torch.ops.approx.tanh)(_tanh_approx_lowering)

class _TanhApprox(torch.autograd.Function):
    @staticmethod
    def forward(x):
        return torch.ops.approx.tanh(x)

    @staticmethod
    def setup_context(ctx, inputs, output):
        (x,) = inputs
        result = output
        ctx.save_for_backward(result)

    @staticmethod
    def backward(ctx, grad_output):
        (result,) = ctx.saved_tensors
        return grad_output * (1 - result * result)

    @staticmethod
    def vmap(info, in_dims, x):
        return torch.tanh(x), 0

_tanh_approx = _TanhApprox.apply

def generate_tanh_softcap(soft_cap: int, approx: bool=True) -> _score_mod_signature:
    tanh = _tanh_approx if approx else torch.tanh

    def tanh_softcap(score, b, h, q_idx, kv_idx):
        return soft_cap * tanh(score / soft_cap)

    prefix = "tanh_softcap_approx" if approx else "tanh_softcap"
    tanh_softcap.__name__ = f"{prefix}_{soft_cap}"

    return tanh_softcap

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1, block_mask: BlockMask, score_mod: _score_mod_signature):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=score_mod, block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, block_mask: BlockMask, score_mod: _score_mod_signature):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1, block_mask, score_mod)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attention_soft_cap : int = 50
    lm_head_soft_cap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.attention_soft_cap = config.attention_soft_cap
        self.lm_head_soft_cap = config.lm_head_soft_cap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target, attn_blocksize):

        docs = (idx == 50256).cumsum(0)
        def document_causal_mask(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < attn_blocksize
            return causal_mask & document_mask & window_mask

        softcap_mod = generate_tanh_softcap(self.attention_soft_cap, approx=True)  # @leloykun

        S = len(idx)
        block_mask = create_block_mask(document_causal_mask, None, None, S, S, device="cuda", _compile=True)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x, v1 = self.transformer.h[i](x, v1, x0, block_mask, softcap_mod)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x, v1 = self.transformer.h[self.num_encoder_layers + i](x, v1, x0, block_mask, softcap_mod)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = self.lm_head_soft_cap * torch.tanh(logits / self.lm_head_soft_cap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        batch_size = self.B * self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.B*self.T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = buf[:-1] # inputs
        y = buf[1:] # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    device_batch_size : int = 1 # batch size, in sequences, per device
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1700 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 622 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    block_size_warmup_iters : int = 1600
    block_size_warmup_step : int = 8
    block_size_warmup_max : int = 1792
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
gpt_config = GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768)
model = GPT(gpt_config)
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.8, 0.95), fused=True)
param_names = [name for name, _ in raw_model.named_parameters()]
params = list(raw_model.transformer.h.parameters())
qk_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and ("c_q" in n or "c_k" in n)]
matrix_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and "c_q" not in n and "c_k" not in n]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer5 = Muon(qk_params, lr=0.08, momentum=0.95)
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4, optimizer5]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # Set the attention blocksize for the current step, in chunks of 64
    attn_blocksize = torch.tensor(
        args.block_size_warmup_step
        * (
            1 +
            (min(step/args.block_size_warmup_iters, 1) * (args.block_size_warmup_max - args.block_size_warmup_step))
            // args.block_size_warmup_step
        ),
        dtype=torch.int,
        device='cuda',
    )
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, attn_blocksize=attn_blocksize)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # if master_process:
        #     print("============== Weight norms: ==============")
        #     with open(logfile, "a") as f:
        #         f.write("============== Weight norms: ==============\n")
        #         for name, p in model.named_parameters():
        #             if p.ndim != 2:
        #                 continue
        #             if "transformer.wte" in name:
        #                 l1_to_l2_norm = torch.norm(p.data.float(), p=2, dim=1).mean().item()
        #                 l1_to_rms_norm = (1/p.data.size(1))**0.5 * l1_to_l2_norm
        #                 print(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
        #                 f.write(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
        #             elif "attn.c_q" in name or "attn.c_k" in name:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #                 for h in range(gpt_config.n_head):
        #                     head_dim = gpt_config.n_embd // gpt_config.n_head
        #                     frobenius_norm = torch.linalg.norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
        #                     spectral_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
        #                     nuclear_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
        #                     print(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                     f.write(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #             else:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #         f.write("===========================================\n")
        #     print("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y, attn_blocksize=attn_blocksize)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # if master_process and (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
    #     print("============== Gradient norms: ==============")
    #     with open(logfile, "a") as f:
    #         f.write("============== Gradient norms: ==============\n")
    #         for name, p in model.named_parameters():
    #             if p.ndim != 2:
    #                 continue
    #             if p.grad is None:
    #                 continue
    #             if "transformer.wte" in name:
    #                 l1_to_l2_norm = torch.norm(p.grad.float(), p=2, dim=1).mean().item()
    #                 l1_to_rms_norm = (1/p.grad.size(1))**0.5 * l1_to_l2_norm
    #                 print(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
    #                 f.write(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
    #             elif "attn.c_q" in name or "attn.c_k" in name:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #                 for h in range(gpt_config.n_head):
    #                     head_dim = gpt_config.n_embd // gpt_config.n_head
    #                     frobenius_norm = torch.linalg.norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
    #                     spectral_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
    #                     nuclear_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
    #                     print(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                     f.write(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #             else:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #         f.write("===========================================\n")
    #     print("===========================================")
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241126+cu124 compiled for CUDA 12.4
nvidia-smi:
Wed Nov 27 00:48:32 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:34:00.0 Off |                    0 |
| N/A   31C    P0            101W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:48:00.0 Off |                    0 |
| N/A   34C    P0            104W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:AE:00.0 Off |                    0 |
| N/A   30C    P0             99W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:C2:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |      26MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1100000000 across 11 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1700 train_loss:10.8258 train_time:22617ms step_avg:nanms
step:2/1700 train_loss:10.1114 train_time:23200ms step_avg:nanms
step:3/1700 train_loss:8.3847 train_time:23490ms step_avg:nanms
step:4/1700 train_loss:7.6591 train_time:23783ms step_avg:nanms
step:5/1700 train_loss:7.3931 train_time:24074ms step_avg:nanms
step:6/1700 train_loss:7.0128 train_time:24367ms step_avg:nanms
step:7/1700 train_loss:6.9608 train_time:24657ms step_avg:nanms
step:8/1700 train_loss:6.4519 train_time:24948ms step_avg:nanms
step:9/1700 train_loss:6.7231 train_time:25240ms step_avg:nanms
step:10/1700 train_loss:6.4991 train_time:25530ms step_avg:nanms
step:11/1700 train_loss:6.4424 train_time:284ms step_avg:nanms
step:12/1700 train_loss:6.2953 train_time:576ms step_avg:nanms
step:13/1700 train_loss:6.2027 train_time:869ms step_avg:289.54ms
step:14/1700 train_loss:6.1339 train_time:1160ms step_avg:290.07ms
step:15/1700 train_loss:6.0790 train_time:1452ms step_avg:290.31ms
step:16/1700 train_loss:5.9163 train_time:1746ms step_avg:290.98ms
step:17/1700 train_loss:5.8649 train_time:2038ms step_avg:291.08ms
step:18/1700 train_loss:6.4371 train_time:2329ms step_avg:291.18ms
step:19/1700 train_loss:5.8415 train_time:2623ms step_avg:291.40ms
step:20/1700 train_loss:5.9953 train_time:2914ms step_avg:291.37ms
step:21/1700 train_loss:5.9453 train_time:3207ms step_avg:291.57ms
step:22/1700 train_loss:5.6465 train_time:3499ms step_avg:291.56ms
step:23/1700 train_loss:5.7499 train_time:3791ms step_avg:291.63ms
step:24/1700 train_loss:5.7626 train_time:4085ms step_avg:291.79ms
step:25/1700 train_loss:5.5483 train_time:4377ms step_avg:291.80ms
step:26/1700 train_loss:5.6657 train_time:4670ms step_avg:291.88ms
step:27/1700 train_loss:5.6426 train_time:4962ms step_avg:291.87ms
step:28/1700 train_loss:5.6451 train_time:5254ms step_avg:291.87ms
step:29/1700 train_loss:5.6835 train_time:5547ms step_avg:291.97ms
step:30/1700 train_loss:5.6807 train_time:5840ms step_avg:292.01ms
step:31/1700 train_loss:6.0272 train_time:6132ms step_avg:291.98ms
step:32/1700 train_loss:5.4991 train_time:6426ms step_avg:292.09ms
step:33/1700 train_loss:5.3367 train_time:6719ms step_avg:292.11ms
step:34/1700 train_loss:5.3773 train_time:7011ms step_avg:292.11ms
step:35/1700 train_loss:5.6099 train_time:7303ms step_avg:292.12ms
step:36/1700 train_loss:5.5006 train_time:7595ms step_avg:292.11ms
step:37/1700 train_loss:5.5377 train_time:7889ms step_avg:292.18ms
step:38/1700 train_loss:5.3571 train_time:8183ms step_avg:292.25ms
step:39/1700 train_loss:5.4308 train_time:8475ms step_avg:292.23ms
step:40/1700 train_loss:5.2346 train_time:8768ms step_avg:292.27ms
step:41/1700 train_loss:5.3979 train_time:9060ms step_avg:292.25ms
step:42/1700 train_loss:5.2902 train_time:9352ms step_avg:292.26ms
step:43/1700 train_loss:5.3124 train_time:9647ms step_avg:292.33ms
step:44/1700 train_loss:5.2165 train_time:9939ms step_avg:292.33ms
step:45/1700 train_loss:5.0807 train_time:10231ms step_avg:292.32ms
step:46/1700 train_loss:5.1795 train_time:10526ms step_avg:292.39ms
step:47/1700 train_loss:5.0978 train_time:10818ms step_avg:292.38ms
step:48/1700 train_loss:5.2241 train_time:11111ms step_avg:292.39ms
step:49/1700 train_loss:5.0596 train_time:11403ms step_avg:292.39ms
step:50/1700 train_loss:5.1187 train_time:11697ms step_avg:292.41ms
step:51/1700 train_loss:5.1199 train_time:11990ms step_avg:292.43ms
step:52/1700 train_loss:5.2545 train_time:12284ms step_avg:292.48ms
step:53/1700 train_loss:5.0510 train_time:12577ms step_avg:292.49ms
step:54/1700 train_loss:5.1088 train_time:12870ms step_avg:292.49ms
step:55/1700 train_loss:5.0176 train_time:13163ms step_avg:292.52ms
step:56/1700 train_loss:5.0415 train_time:13455ms step_avg:292.51ms
step:57/1700 train_loss:5.0662 train_time:13749ms step_avg:292.53ms
step:58/1700 train_loss:5.0915 train_time:14043ms step_avg:292.56ms
step:59/1700 train_loss:5.0996 train_time:14335ms step_avg:292.55ms
step:60/1700 train_loss:4.9503 train_time:14629ms step_avg:292.57ms
step:61/1700 train_loss:5.0825 train_time:14921ms step_avg:292.57ms
step:62/1700 train_loss:5.0672 train_time:15215ms step_avg:292.59ms
step:63/1700 train_loss:5.0212 train_time:15508ms step_avg:292.61ms
step:64/1700 train_loss:4.9718 train_time:15802ms step_avg:292.63ms
step:65/1700 train_loss:4.8160 train_time:16094ms step_avg:292.62ms
step:66/1700 train_loss:4.8537 train_time:16388ms step_avg:292.65ms
step:67/1700 train_loss:4.9893 train_time:16681ms step_avg:292.64ms
step:68/1700 train_loss:4.9654 train_time:16973ms step_avg:292.64ms
step:69/1700 train_loss:5.0009 train_time:17267ms step_avg:292.66ms
step:70/1700 train_loss:4.8421 train_time:17559ms step_avg:292.65ms
step:71/1700 train_loss:4.9390 train_time:17851ms step_avg:292.65ms
step:72/1700 train_loss:4.9122 train_time:18144ms step_avg:292.65ms
step:73/1700 train_loss:4.8789 train_time:18436ms step_avg:292.64ms
step:74/1700 train_loss:4.7391 train_time:18729ms step_avg:292.64ms
step:75/1700 train_loss:4.7832 train_time:19022ms step_avg:292.64ms
step:76/1700 train_loss:4.6708 train_time:19313ms step_avg:292.62ms
step:77/1700 train_loss:4.8995 train_time:19607ms step_avg:292.64ms
step:78/1700 train_loss:4.8309 train_time:19899ms step_avg:292.63ms
step:79/1700 train_loss:4.5552 train_time:20190ms step_avg:292.61ms
step:80/1700 train_loss:4.8222 train_time:20484ms step_avg:292.62ms
step:81/1700 train_loss:4.7782 train_time:20777ms step_avg:292.63ms
step:82/1700 train_loss:4.8107 train_time:21069ms step_avg:292.63ms
step:83/1700 train_loss:4.7901 train_time:21362ms step_avg:292.62ms
step:84/1700 train_loss:4.6924 train_time:21652ms step_avg:292.60ms
step:85/1700 train_loss:4.6840 train_time:21947ms step_avg:292.63ms
step:86/1700 train_loss:4.7946 train_time:22240ms step_avg:292.63ms
step:87/1700 train_loss:4.7935 train_time:22531ms step_avg:292.61ms
step:88/1700 train_loss:4.6528 train_time:22824ms step_avg:292.62ms
step:89/1700 train_loss:4.6622 train_time:23117ms step_avg:292.62ms
step:90/1700 train_loss:4.5896 train_time:23409ms step_avg:292.61ms
step:91/1700 train_loss:4.7360 train_time:23700ms step_avg:292.60ms
step:92/1700 train_loss:4.7052 train_time:23993ms step_avg:292.60ms
step:93/1700 train_loss:4.7656 train_time:24287ms step_avg:292.62ms
step:94/1700 train_loss:4.9061 train_time:24579ms step_avg:292.61ms
step:95/1700 train_loss:4.6484 train_time:24871ms step_avg:292.61ms
step:96/1700 train_loss:4.5451 train_time:25165ms step_avg:292.61ms
step:97/1700 train_loss:4.6966 train_time:25457ms step_avg:292.61ms
step:98/1700 train_loss:4.5397 train_time:25750ms step_avg:292.62ms
step:99/1700 train_loss:4.5371 train_time:26044ms step_avg:292.63ms
step:100/1700 train_loss:4.5960 train_time:26337ms step_avg:292.63ms
step:101/1700 train_loss:4.4491 train_time:26629ms step_avg:292.63ms
step:102/1700 train_loss:4.5980 train_time:26921ms step_avg:292.62ms
step:103/1700 train_loss:4.5346 train_time:27213ms step_avg:292.61ms
step:104/1700 train_loss:4.6026 train_time:27508ms step_avg:292.63ms
step:105/1700 train_loss:4.6078 train_time:27799ms step_avg:292.62ms
step:106/1700 train_loss:4.7530 train_time:28090ms step_avg:292.60ms
step:107/1700 train_loss:4.5484 train_time:28385ms step_avg:292.62ms
step:108/1700 train_loss:4.3938 train_time:28676ms step_avg:292.61ms
step:109/1700 train_loss:4.7664 train_time:28969ms step_avg:292.62ms
step:110/1700 train_loss:4.5568 train_time:29261ms step_avg:292.61ms
step:111/1700 train_loss:4.4575 train_time:29552ms step_avg:292.59ms
step:112/1700 train_loss:4.7000 train_time:29847ms step_avg:292.62ms
step:113/1700 train_loss:4.3711 train_time:30139ms step_avg:292.61ms
step:114/1700 train_loss:4.5566 train_time:30430ms step_avg:292.60ms
step:115/1700 train_loss:4.4859 train_time:30723ms step_avg:292.60ms
step:116/1700 train_loss:4.5211 train_time:31022ms step_avg:292.66ms
step:117/1700 train_loss:4.3060 train_time:31321ms step_avg:292.72ms
step:118/1700 train_loss:4.5386 train_time:31620ms step_avg:292.78ms
step:119/1700 train_loss:4.3728 train_time:31919ms step_avg:292.83ms
step:120/1700 train_loss:4.4651 train_time:32218ms step_avg:292.89ms
step:121/1700 train_loss:4.4438 train_time:32517ms step_avg:292.95ms
step:122/1700 train_loss:4.3453 train_time:32816ms step_avg:293.00ms
step:123/1700 train_loss:4.4214 train_time:33116ms step_avg:293.06ms
step:124/1700 train_loss:4.3029 train_time:33416ms step_avg:293.12ms
step:125/1700 train_loss:4.2975 train_time:33716ms step_avg:293.18ms
step:125/1700 val_loss:4.4003 train_time:33724ms step_avg:293.25ms
step:126/1700 train_loss:4.2620 train_time:34026ms step_avg:293.33ms
step:127/1700 train_loss:4.4644 train_time:34327ms step_avg:293.39ms
step:128/1700 train_loss:4.4597 train_time:34627ms step_avg:293.45ms
step:129/1700 train_loss:4.4601 train_time:34927ms step_avg:293.50ms
step:130/1700 train_loss:4.4034 train_time:35228ms step_avg:293.57ms
step:131/1700 train_loss:4.5459 train_time:35528ms step_avg:293.62ms
step:132/1700 train_loss:4.3058 train_time:35829ms step_avg:293.68ms
step:133/1700 train_loss:4.2837 train_time:36129ms step_avg:293.73ms
step:134/1700 train_loss:4.4480 train_time:36430ms step_avg:293.79ms
step:135/1700 train_loss:4.2797 train_time:36729ms step_avg:293.83ms
step:136/1700 train_loss:4.2832 train_time:37029ms step_avg:293.88ms
step:137/1700 train_loss:4.3511 train_time:37329ms step_avg:293.93ms
step:138/1700 train_loss:4.3598 train_time:37629ms step_avg:293.98ms
step:139/1700 train_loss:4.4734 train_time:37929ms step_avg:294.02ms
step:140/1700 train_loss:4.3740 train_time:38230ms step_avg:294.08ms
step:141/1700 train_loss:4.2584 train_time:38530ms step_avg:294.12ms
step:142/1700 train_loss:4.3708 train_time:38832ms step_avg:294.18ms
step:143/1700 train_loss:4.4644 train_time:39132ms step_avg:294.23ms
step:144/1700 train_loss:4.5275 train_time:39432ms step_avg:294.27ms
step:145/1700 train_loss:4.2792 train_time:39731ms step_avg:294.31ms
step:146/1700 train_loss:4.3176 train_time:40032ms step_avg:294.35ms
step:147/1700 train_loss:4.3461 train_time:40331ms step_avg:294.39ms
step:148/1700 train_loss:4.1523 train_time:40631ms step_avg:294.43ms
step:149/1700 train_loss:4.2986 train_time:40931ms step_avg:294.47ms
step:150/1700 train_loss:4.2847 train_time:41231ms step_avg:294.51ms
step:151/1700 train_loss:4.2783 train_time:41530ms step_avg:294.54ms
step:152/1700 train_loss:4.1648 train_time:41831ms step_avg:294.58ms
step:153/1700 train_loss:4.3492 train_time:42132ms step_avg:294.63ms
step:154/1700 train_loss:4.1481 train_time:42432ms step_avg:294.66ms
step:155/1700 train_loss:4.1508 train_time:42732ms step_avg:294.70ms
step:156/1700 train_loss:4.2960 train_time:43032ms step_avg:294.74ms
step:157/1700 train_loss:4.3620 train_time:43332ms step_avg:294.78ms
step:158/1700 train_loss:4.2667 train_time:43632ms step_avg:294.81ms
step:159/1700 train_loss:4.2176 train_time:43932ms step_avg:294.85ms
step:160/1700 train_loss:4.1703 train_time:44233ms step_avg:294.89ms
step:161/1700 train_loss:4.2151 train_time:44532ms step_avg:294.92ms
step:162/1700 train_loss:4.2440 train_time:44832ms step_avg:294.95ms
step:163/1700 train_loss:4.2060 train_time:45132ms step_avg:294.98ms
step:164/1700 train_loss:4.1327 train_time:45432ms step_avg:295.01ms
step:165/1700 train_loss:4.2171 train_time:45733ms step_avg:295.05ms
step:166/1700 train_loss:4.3464 train_time:46032ms step_avg:295.08ms
step:167/1700 train_loss:4.2646 train_time:46333ms step_avg:295.12ms
step:168/1700 train_loss:4.1896 train_time:46632ms step_avg:295.14ms
step:169/1700 train_loss:4.2550 train_time:46932ms step_avg:295.17ms
step:170/1700 train_loss:4.3011 train_time:47232ms step_avg:295.20ms
step:171/1700 train_loss:3.7875 train_time:47532ms step_avg:295.23ms
step:172/1700 train_loss:4.1290 train_time:47832ms step_avg:295.26ms
step:173/1700 train_loss:4.1402 train_time:48132ms step_avg:295.29ms
step:174/1700 train_loss:4.3352 train_time:48432ms step_avg:295.32ms
step:175/1700 train_loss:4.1596 train_time:48732ms step_avg:295.34ms
step:176/1700 train_loss:4.2119 train_time:49030ms step_avg:295.36ms
step:177/1700 train_loss:4.3490 train_time:49331ms step_avg:295.39ms
step:178/1700 train_loss:4.2154 train_time:49632ms step_avg:295.43ms
step:179/1700 train_loss:4.1798 train_time:49931ms step_avg:295.45ms
step:180/1700 train_loss:4.2280 train_time:50232ms step_avg:295.48ms
step:181/1700 train_loss:4.1120 train_time:50532ms step_avg:295.51ms
step:182/1700 train_loss:4.1532 train_time:50832ms step_avg:295.53ms
step:183/1700 train_loss:4.1199 train_time:51132ms step_avg:295.56ms
step:184/1700 train_loss:4.2708 train_time:51433ms step_avg:295.59ms
step:185/1700 train_loss:4.1703 train_time:51732ms step_avg:295.61ms
step:186/1700 train_loss:4.2660 train_time:52033ms step_avg:295.64ms
step:187/1700 train_loss:4.1798 train_time:52333ms step_avg:295.66ms
step:188/1700 train_loss:4.1557 train_time:52633ms step_avg:295.69ms
step:189/1700 train_loss:3.9922 train_time:52933ms step_avg:295.71ms
step:190/1700 train_loss:4.1060 train_time:53409ms step_avg:296.71ms
step:191/1700 train_loss:4.0909 train_time:53710ms step_avg:296.74ms
step:192/1700 train_loss:4.0329 train_time:54011ms step_avg:296.76ms
step:193/1700 train_loss:4.2416 train_time:54312ms step_avg:296.79ms
step:194/1700 train_loss:4.1723 train_time:54612ms step_avg:296.81ms
step:195/1700 train_loss:4.3618 train_time:54912ms step_avg:296.82ms
step:196/1700 train_loss:4.1936 train_time:55212ms step_avg:296.84ms
step:197/1700 train_loss:4.0471 train_time:55513ms step_avg:296.86ms
step:198/1700 train_loss:4.1739 train_time:55813ms step_avg:296.88ms
step:199/1700 train_loss:4.0389 train_time:56112ms step_avg:296.89ms
step:200/1700 train_loss:4.1238 train_time:56412ms step_avg:296.91ms
step:201/1700 train_loss:3.9935 train_time:56714ms step_avg:296.93ms
step:202/1700 train_loss:4.2469 train_time:57014ms step_avg:296.95ms
step:203/1700 train_loss:4.0610 train_time:57314ms step_avg:296.97ms
step:204/1700 train_loss:4.1859 train_time:57614ms step_avg:296.98ms
step:205/1700 train_loss:4.2539 train_time:57914ms step_avg:297.00ms
step:206/1700 train_loss:3.9489 train_time:58215ms step_avg:297.01ms
step:207/1700 train_loss:4.0816 train_time:58514ms step_avg:297.03ms
step:208/1700 train_loss:4.0881 train_time:58815ms step_avg:297.04ms
step:209/1700 train_loss:4.2350 train_time:59114ms step_avg:297.06ms
step:210/1700 train_loss:4.1865 train_time:59415ms step_avg:297.07ms
step:211/1700 train_loss:4.0548 train_time:59715ms step_avg:297.09ms
step:212/1700 train_loss:4.1128 train_time:60016ms step_avg:297.11ms
step:213/1700 train_loss:4.0400 train_time:60317ms step_avg:297.13ms
step:214/1700 train_loss:4.1089 train_time:60618ms step_avg:297.15ms
step:215/1700 train_loss:3.9509 train_time:60919ms step_avg:297.17ms
step:216/1700 train_loss:4.0043 train_time:61221ms step_avg:297.19ms
step:217/1700 train_loss:4.0113 train_time:61522ms step_avg:297.21ms
step:218/1700 train_loss:4.0929 train_time:61823ms step_avg:297.22ms
step:219/1700 train_loss:4.0759 train_time:62125ms step_avg:297.25ms
step:220/1700 train_loss:4.0827 train_time:62428ms step_avg:297.28ms
step:221/1700 train_loss:4.0955 train_time:62730ms step_avg:297.30ms
step:222/1700 train_loss:3.9907 train_time:63031ms step_avg:297.32ms
step:223/1700 train_loss:3.9796 train_time:63332ms step_avg:297.33ms
step:224/1700 train_loss:4.2865 train_time:63630ms step_avg:297.34ms
step:225/1700 train_loss:3.8869 train_time:63928ms step_avg:297.34ms
step:226/1700 train_loss:3.9776 train_time:64227ms step_avg:297.35ms
step:227/1700 train_loss:3.9767 train_time:64527ms step_avg:297.36ms
step:228/1700 train_loss:4.1361 train_time:64826ms step_avg:297.37ms
step:229/1700 train_loss:3.9257 train_time:65127ms step_avg:297.38ms
step:230/1700 train_loss:4.0504 train_time:65426ms step_avg:297.39ms
step:231/1700 train_loss:3.9089 train_time:65734ms step_avg:297.44ms
step:232/1700 train_loss:3.9711 train_time:66039ms step_avg:297.47ms
step:233/1700 train_loss:4.0822 train_time:66345ms step_avg:297.51ms
step:234/1700 train_loss:4.0275 train_time:66651ms step_avg:297.55ms
step:235/1700 train_loss:3.9230 train_time:66958ms step_avg:297.59ms
step:236/1700 train_loss:4.0911 train_time:67263ms step_avg:297.62ms
step:237/1700 train_loss:4.0856 train_time:67570ms step_avg:297.67ms
step:238/1700 train_loss:3.9431 train_time:67877ms step_avg:297.71ms
step:239/1700 train_loss:4.0754 train_time:68184ms step_avg:297.75ms
step:240/1700 train_loss:4.1215 train_time:68490ms step_avg:297.78ms
step:241/1700 train_loss:3.9677 train_time:68796ms step_avg:297.82ms
step:242/1700 train_loss:4.1507 train_time:69103ms step_avg:297.86ms
step:243/1700 train_loss:4.0257 train_time:69410ms step_avg:297.90ms
step:244/1700 train_loss:4.0780 train_time:69715ms step_avg:297.93ms
step:245/1700 train_loss:4.1441 train_time:70022ms step_avg:297.96ms
step:246/1700 train_loss:4.0534 train_time:70327ms step_avg:298.00ms
step:247/1700 train_loss:4.0050 train_time:70633ms step_avg:298.03ms
step:248/1700 train_loss:4.1098 train_time:70940ms step_avg:298.07ms
step:249/1700 train_loss:3.9223 train_time:71246ms step_avg:298.10ms
step:250/1700 train_loss:3.9685 train_time:71550ms step_avg:298.13ms
step:250/1700 val_loss:4.0002 train_time:71559ms step_avg:298.16ms
step:251/1700 train_loss:4.0673 train_time:71861ms step_avg:298.18ms
step:252/1700 train_loss:4.1571 train_time:72167ms step_avg:298.21ms
step:253/1700 train_loss:3.9258 train_time:72474ms step_avg:298.25ms
step:254/1700 train_loss:3.8678 train_time:72779ms step_avg:298.27ms
step:255/1700 train_loss:4.0695 train_time:73084ms step_avg:298.30ms
step:256/1700 train_loss:3.9808 train_time:73392ms step_avg:298.34ms
step:257/1700 train_loss:3.9902 train_time:73698ms step_avg:298.37ms
step:258/1700 train_loss:3.9788 train_time:74003ms step_avg:298.40ms
step:259/1700 train_loss:4.0207 train_time:74309ms step_avg:298.43ms
step:260/1700 train_loss:4.0674 train_time:74616ms step_avg:298.46ms
step:261/1700 train_loss:4.0206 train_time:74922ms step_avg:298.49ms
step:262/1700 train_loss:3.9986 train_time:75229ms step_avg:298.53ms
step:263/1700 train_loss:3.8958 train_time:75536ms step_avg:298.56ms
step:264/1700 train_loss:3.9889 train_time:75841ms step_avg:298.59ms
step:265/1700 train_loss:3.8699 train_time:76150ms step_avg:298.63ms
step:266/1700 train_loss:3.9125 train_time:76455ms step_avg:298.65ms
step:267/1700 train_loss:3.9277 train_time:76761ms step_avg:298.68ms
step:268/1700 train_loss:3.9565 train_time:77067ms step_avg:298.71ms
step:269/1700 train_loss:3.8472 train_time:77374ms step_avg:298.74ms
step:270/1700 train_loss:4.1085 train_time:77679ms step_avg:298.77ms
step:271/1700 train_loss:3.9697 train_time:77984ms step_avg:298.79ms
step:272/1700 train_loss:3.9171 train_time:78292ms step_avg:298.83ms
step:273/1700 train_loss:3.9420 train_time:78598ms step_avg:298.85ms
step:274/1700 train_loss:4.0300 train_time:78902ms step_avg:298.87ms
step:275/1700 train_loss:4.0608 train_time:79209ms step_avg:298.90ms
step:276/1700 train_loss:4.2178 train_time:79516ms step_avg:298.93ms
step:277/1700 train_loss:4.0299 train_time:79823ms step_avg:298.96ms
step:278/1700 train_loss:4.0802 train_time:80129ms step_avg:298.99ms
step:279/1700 train_loss:3.9969 train_time:80436ms step_avg:299.02ms
step:280/1700 train_loss:4.1869 train_time:80744ms step_avg:299.05ms
step:281/1700 train_loss:3.9707 train_time:81052ms step_avg:299.08ms
step:282/1700 train_loss:3.9629 train_time:81360ms step_avg:299.12ms
step:283/1700 train_loss:3.9050 train_time:81667ms step_avg:299.15ms
step:284/1700 train_loss:4.0430 train_time:81974ms step_avg:299.18ms
step:285/1700 train_loss:4.0596 train_time:82281ms step_avg:299.20ms
step:286/1700 train_loss:4.0824 train_time:82587ms step_avg:299.23ms
step:287/1700 train_loss:3.9058 train_time:82893ms step_avg:299.25ms
step:288/1700 train_loss:4.0154 train_time:83199ms step_avg:299.28ms
step:289/1700 train_loss:3.8718 train_time:83505ms step_avg:299.30ms
step:290/1700 train_loss:3.8538 train_time:83812ms step_avg:299.33ms
step:291/1700 train_loss:3.9260 train_time:84118ms step_avg:299.35ms
step:292/1700 train_loss:3.8611 train_time:84425ms step_avg:299.38ms
step:293/1700 train_loss:3.9042 train_time:84731ms step_avg:299.40ms
step:294/1700 train_loss:3.9421 train_time:85038ms step_avg:299.43ms
step:295/1700 train_loss:3.8323 train_time:85344ms step_avg:299.45ms
step:296/1700 train_loss:3.8662 train_time:85652ms step_avg:299.48ms
step:297/1700 train_loss:3.8667 train_time:85959ms step_avg:299.51ms
step:298/1700 train_loss:3.9782 train_time:86263ms step_avg:299.53ms
step:299/1700 train_loss:3.8223 train_time:86571ms step_avg:299.55ms
step:300/1700 train_loss:3.9617 train_time:86877ms step_avg:299.58ms
step:301/1700 train_loss:3.9674 train_time:87182ms step_avg:299.60ms
step:302/1700 train_loss:3.9250 train_time:87488ms step_avg:299.62ms
step:303/1700 train_loss:3.9662 train_time:87794ms step_avg:299.64ms
step:304/1700 train_loss:3.9599 train_time:88100ms step_avg:299.66ms
step:305/1700 train_loss:4.4502 train_time:88406ms step_avg:299.68ms
step:306/1700 train_loss:3.9419 train_time:88713ms step_avg:299.70ms
step:307/1700 train_loss:3.8370 train_time:89020ms step_avg:299.73ms
step:308/1700 train_loss:3.9829 train_time:89327ms step_avg:299.75ms
step:309/1700 train_loss:3.8646 train_time:89632ms step_avg:299.77ms
step:310/1700 train_loss:4.0853 train_time:89941ms step_avg:299.80ms
step:311/1700 train_loss:3.9244 train_time:90246ms step_avg:299.82ms
step:312/1700 train_loss:3.8556 train_time:90553ms step_avg:299.84ms
step:313/1700 train_loss:3.9264 train_time:90861ms step_avg:299.87ms
step:314/1700 train_loss:4.0601 train_time:91167ms step_avg:299.89ms
step:315/1700 train_loss:3.9367 train_time:91473ms step_avg:299.91ms
step:316/1700 train_loss:3.7894 train_time:91779ms step_avg:299.93ms
step:317/1700 train_loss:3.8705 train_time:92085ms step_avg:299.95ms
step:318/1700 train_loss:3.9173 train_time:92392ms step_avg:299.97ms
step:319/1700 train_loss:3.8852 train_time:92698ms step_avg:299.99ms
step:320/1700 train_loss:4.0112 train_time:93004ms step_avg:300.01ms
step:321/1700 train_loss:3.9560 train_time:93311ms step_avg:300.03ms
step:322/1700 train_loss:3.9294 train_time:93618ms step_avg:300.06ms
step:323/1700 train_loss:4.0004 train_time:93925ms step_avg:300.08ms
step:324/1700 train_loss:3.9392 train_time:94231ms step_avg:300.10ms
step:325/1700 train_loss:4.0136 train_time:94538ms step_avg:300.12ms
step:326/1700 train_loss:3.8876 train_time:94844ms step_avg:300.14ms
step:327/1700 train_loss:4.3902 train_time:95152ms step_avg:300.16ms
step:328/1700 train_loss:4.0709 train_time:95458ms step_avg:300.18ms
step:329/1700 train_loss:3.7917 train_time:95767ms step_avg:300.21ms
step:330/1700 train_loss:3.7353 train_time:96073ms step_avg:300.23ms
step:331/1700 train_loss:3.9717 train_time:96380ms step_avg:300.25ms
step:332/1700 train_loss:3.9023 train_time:96685ms step_avg:300.26ms
step:333/1700 train_loss:3.8798 train_time:96992ms step_avg:300.28ms
step:334/1700 train_loss:3.8402 train_time:97299ms step_avg:300.31ms
step:335/1700 train_loss:3.9979 train_time:97605ms step_avg:300.32ms
step:336/1700 train_loss:3.9504 train_time:97911ms step_avg:300.34ms
step:337/1700 train_loss:4.4107 train_time:98218ms step_avg:300.36ms
step:338/1700 train_loss:3.9352 train_time:98524ms step_avg:300.38ms
step:339/1700 train_loss:3.8515 train_time:98829ms step_avg:300.39ms
step:340/1700 train_loss:3.9276 train_time:99134ms step_avg:300.41ms
step:341/1700 train_loss:3.8500 train_time:99440ms step_avg:300.42ms
step:342/1700 train_loss:3.8114 train_time:99743ms step_avg:300.43ms
step:343/1700 train_loss:3.8366 train_time:100049ms step_avg:300.45ms
step:344/1700 train_loss:3.9850 train_time:100354ms step_avg:300.46ms
step:345/1700 train_loss:3.8105 train_time:100660ms step_avg:300.48ms
step:346/1700 train_loss:3.7558 train_time:100970ms step_avg:300.51ms
step:347/1700 train_loss:3.7948 train_time:101281ms step_avg:300.54ms
step:348/1700 train_loss:3.8536 train_time:101593ms step_avg:300.57ms
step:349/1700 train_loss:3.8266 train_time:101904ms step_avg:300.60ms
step:350/1700 train_loss:3.5638 train_time:102216ms step_avg:300.63ms
step:351/1700 train_loss:3.8221 train_time:102528ms step_avg:300.67ms
step:352/1700 train_loss:4.1912 train_time:102837ms step_avg:300.69ms
step:353/1700 train_loss:3.6533 train_time:103149ms step_avg:300.73ms
step:354/1700 train_loss:3.9325 train_time:103459ms step_avg:300.75ms
step:355/1700 train_loss:3.7870 train_time:103771ms step_avg:300.79ms
step:356/1700 train_loss:3.8719 train_time:104082ms step_avg:300.81ms
step:357/1700 train_loss:3.7670 train_time:104394ms step_avg:300.85ms
step:358/1700 train_loss:3.8586 train_time:104704ms step_avg:300.87ms
step:359/1700 train_loss:3.8163 train_time:105017ms step_avg:300.91ms
step:360/1700 train_loss:3.4285 train_time:105330ms step_avg:300.94ms
step:361/1700 train_loss:4.0209 train_time:105642ms step_avg:300.97ms
step:362/1700 train_loss:3.9118 train_time:105954ms step_avg:301.00ms
step:363/1700 train_loss:3.8361 train_time:106263ms step_avg:301.03ms
step:364/1700 train_loss:3.7348 train_time:106576ms step_avg:301.06ms
step:365/1700 train_loss:3.9081 train_time:106885ms step_avg:301.08ms
step:366/1700 train_loss:3.8608 train_time:107196ms step_avg:301.11ms
step:367/1700 train_loss:3.8515 train_time:107507ms step_avg:301.14ms
step:368/1700 train_loss:3.8479 train_time:107819ms step_avg:301.17ms
step:369/1700 train_loss:3.7399 train_time:108130ms step_avg:301.20ms
step:370/1700 train_loss:3.8871 train_time:108441ms step_avg:301.22ms
step:371/1700 train_loss:3.7368 train_time:108753ms step_avg:301.25ms
step:372/1700 train_loss:3.6896 train_time:109064ms step_avg:301.28ms
step:373/1700 train_loss:3.9092 train_time:109375ms step_avg:301.31ms
step:374/1700 train_loss:3.8228 train_time:109684ms step_avg:301.33ms
step:375/1700 train_loss:3.7968 train_time:109993ms step_avg:301.35ms
step:375/1700 val_loss:3.8232 train_time:110002ms step_avg:301.37ms
step:376/1700 train_loss:3.8686 train_time:110307ms step_avg:301.38ms
step:377/1700 train_loss:3.7885 train_time:110616ms step_avg:301.41ms
step:378/1700 train_loss:3.8398 train_time:110932ms step_avg:301.45ms
step:379/1700 train_loss:3.8662 train_time:111245ms step_avg:301.48ms
step:380/1700 train_loss:3.9404 train_time:111731ms step_avg:301.98ms
step:381/1700 train_loss:3.6906 train_time:112216ms step_avg:302.47ms
step:382/1700 train_loss:3.7542 train_time:112527ms step_avg:302.49ms
step:383/1700 train_loss:3.7748 train_time:112837ms step_avg:302.51ms
step:384/1700 train_loss:3.8716 train_time:113148ms step_avg:302.54ms
step:385/1700 train_loss:3.6654 train_time:113461ms step_avg:302.56ms
step:386/1700 train_loss:3.8623 train_time:113771ms step_avg:302.58ms
step:387/1700 train_loss:3.7867 train_time:114082ms step_avg:302.60ms
step:388/1700 train_loss:3.9607 train_time:114394ms step_avg:302.63ms
step:389/1700 train_loss:3.8013 train_time:114704ms step_avg:302.65ms
step:390/1700 train_loss:3.8667 train_time:115013ms step_avg:302.67ms
step:391/1700 train_loss:3.7050 train_time:115325ms step_avg:302.69ms
step:392/1700 train_loss:3.7596 train_time:115635ms step_avg:302.71ms
step:393/1700 train_loss:3.8142 train_time:115945ms step_avg:302.73ms
step:394/1700 train_loss:3.7972 train_time:116256ms step_avg:302.75ms
step:395/1700 train_loss:3.8090 train_time:116566ms step_avg:302.77ms
step:396/1700 train_loss:3.7158 train_time:116876ms step_avg:302.79ms
step:397/1700 train_loss:3.5756 train_time:117187ms step_avg:302.81ms
step:398/1700 train_loss:3.8203 train_time:117496ms step_avg:302.83ms
step:399/1700 train_loss:3.7799 train_time:117808ms step_avg:302.85ms
step:400/1700 train_loss:3.7043 train_time:118117ms step_avg:302.86ms
step:401/1700 train_loss:3.8051 train_time:118429ms step_avg:302.89ms
step:402/1700 train_loss:3.6933 train_time:118741ms step_avg:302.91ms
step:403/1700 train_loss:3.9745 train_time:119053ms step_avg:302.93ms
step:404/1700 train_loss:3.8785 train_time:119364ms step_avg:302.95ms
step:405/1700 train_loss:3.8516 train_time:119673ms step_avg:302.97ms
step:406/1700 train_loss:3.8576 train_time:119984ms step_avg:302.99ms
step:407/1700 train_loss:3.8334 train_time:120294ms step_avg:303.01ms
step:408/1700 train_loss:3.7411 train_time:120606ms step_avg:303.03ms
step:409/1700 train_loss:3.8058 train_time:120915ms step_avg:303.05ms
step:410/1700 train_loss:3.7454 train_time:121226ms step_avg:303.06ms
step:411/1700 train_loss:3.7572 train_time:121535ms step_avg:303.08ms
step:412/1700 train_loss:3.7650 train_time:121845ms step_avg:303.10ms
step:413/1700 train_loss:3.7584 train_time:122157ms step_avg:303.12ms
step:414/1700 train_loss:3.8719 train_time:122467ms step_avg:303.14ms
step:415/1700 train_loss:3.7094 train_time:122777ms step_avg:303.15ms
step:416/1700 train_loss:3.7918 train_time:123088ms step_avg:303.17ms
step:417/1700 train_loss:3.8925 train_time:123400ms step_avg:303.19ms
step:418/1700 train_loss:3.6683 train_time:123712ms step_avg:303.22ms
step:419/1700 train_loss:3.9198 train_time:124023ms step_avg:303.24ms
step:420/1700 train_loss:3.9999 train_time:124334ms step_avg:303.25ms
step:421/1700 train_loss:3.7534 train_time:124645ms step_avg:303.27ms
step:422/1700 train_loss:3.8853 train_time:124955ms step_avg:303.29ms
step:423/1700 train_loss:3.5814 train_time:125265ms step_avg:303.30ms
step:424/1700 train_loss:3.7683 train_time:125573ms step_avg:303.32ms
step:425/1700 train_loss:3.6527 train_time:125885ms step_avg:303.34ms
step:426/1700 train_loss:3.8586 train_time:126197ms step_avg:303.36ms
step:427/1700 train_loss:3.8298 train_time:126507ms step_avg:303.38ms
step:428/1700 train_loss:3.7663 train_time:126819ms step_avg:303.39ms
step:429/1700 train_loss:3.8721 train_time:127129ms step_avg:303.41ms
step:430/1700 train_loss:3.6920 train_time:127440ms step_avg:303.43ms
step:431/1700 train_loss:3.6270 train_time:127753ms step_avg:303.45ms
step:432/1700 train_loss:3.8363 train_time:128064ms step_avg:303.47ms
step:433/1700 train_loss:3.8575 train_time:128374ms step_avg:303.49ms
step:434/1700 train_loss:3.8389 train_time:128684ms step_avg:303.50ms
step:435/1700 train_loss:3.7459 train_time:128995ms step_avg:303.52ms
step:436/1700 train_loss:3.8275 train_time:129305ms step_avg:303.53ms
step:437/1700 train_loss:3.8168 train_time:129615ms step_avg:303.55ms
step:438/1700 train_loss:3.7959 train_time:129926ms step_avg:303.57ms
step:439/1700 train_loss:3.8567 train_time:130236ms step_avg:303.58ms
step:440/1700 train_loss:3.6908 train_time:130548ms step_avg:303.60ms
step:441/1700 train_loss:3.8022 train_time:130858ms step_avg:303.62ms
step:442/1700 train_loss:3.7107 train_time:131169ms step_avg:303.63ms
step:443/1700 train_loss:3.6166 train_time:131482ms step_avg:303.65ms
step:444/1700 train_loss:3.7585 train_time:131793ms step_avg:303.67ms
step:445/1700 train_loss:4.0099 train_time:132106ms step_avg:303.69ms
step:446/1700 train_loss:3.6351 train_time:132415ms step_avg:303.70ms
step:447/1700 train_loss:3.8239 train_time:132727ms step_avg:303.72ms
step:448/1700 train_loss:3.8777 train_time:133038ms step_avg:303.74ms
step:449/1700 train_loss:3.7015 train_time:133350ms step_avg:303.76ms
step:450/1700 train_loss:3.6618 train_time:133664ms step_avg:303.78ms
step:451/1700 train_loss:3.7180 train_time:133976ms step_avg:303.80ms
step:452/1700 train_loss:4.0382 train_time:134286ms step_avg:303.81ms
step:453/1700 train_loss:3.9408 train_time:134598ms step_avg:303.83ms
step:454/1700 train_loss:3.8073 train_time:134910ms step_avg:303.85ms
step:455/1700 train_loss:3.6994 train_time:135220ms step_avg:303.87ms
step:456/1700 train_loss:3.8120 train_time:135529ms step_avg:303.88ms
step:457/1700 train_loss:3.7479 train_time:135837ms step_avg:303.89ms
step:458/1700 train_loss:3.7572 train_time:136147ms step_avg:303.90ms
step:459/1700 train_loss:3.8575 train_time:136457ms step_avg:303.91ms
step:460/1700 train_loss:3.6607 train_time:136767ms step_avg:303.93ms
step:461/1700 train_loss:3.7815 train_time:137082ms step_avg:303.95ms
step:462/1700 train_loss:3.7395 train_time:137398ms step_avg:303.98ms
step:463/1700 train_loss:3.5861 train_time:137714ms step_avg:304.01ms
step:464/1700 train_loss:3.7340 train_time:138029ms step_avg:304.03ms
step:465/1700 train_loss:3.8207 train_time:138344ms step_avg:304.05ms
step:466/1700 train_loss:3.7146 train_time:138661ms step_avg:304.08ms
step:467/1700 train_loss:3.7234 train_time:138976ms step_avg:304.10ms
step:468/1700 train_loss:3.7057 train_time:139292ms step_avg:304.13ms
step:469/1700 train_loss:3.9107 train_time:139605ms step_avg:304.15ms
step:470/1700 train_loss:3.7361 train_time:139917ms step_avg:304.17ms
step:471/1700 train_loss:3.5980 train_time:140236ms step_avg:304.20ms
step:472/1700 train_loss:3.8125 train_time:140551ms step_avg:304.22ms
step:473/1700 train_loss:3.6771 train_time:140864ms step_avg:304.24ms
step:474/1700 train_loss:3.7960 train_time:141179ms step_avg:304.26ms
step:475/1700 train_loss:3.8661 train_time:141492ms step_avg:304.28ms
step:476/1700 train_loss:4.0303 train_time:141808ms step_avg:304.31ms
step:477/1700 train_loss:3.8157 train_time:142123ms step_avg:304.33ms
step:478/1700 train_loss:3.7780 train_time:142439ms step_avg:304.36ms
step:479/1700 train_loss:3.7341 train_time:142755ms step_avg:304.38ms
step:480/1700 train_loss:3.6793 train_time:143070ms step_avg:304.40ms
step:481/1700 train_loss:3.7156 train_time:143386ms step_avg:304.43ms
step:482/1700 train_loss:3.8318 train_time:143700ms step_avg:304.45ms
step:483/1700 train_loss:3.7534 train_time:144014ms step_avg:304.47ms
step:484/1700 train_loss:3.8289 train_time:144330ms step_avg:304.49ms
step:485/1700 train_loss:3.7611 train_time:144645ms step_avg:304.51ms
step:486/1700 train_loss:3.5961 train_time:144960ms step_avg:304.54ms
step:487/1700 train_loss:3.7377 train_time:145276ms step_avg:304.56ms
step:488/1700 train_loss:3.7459 train_time:145592ms step_avg:304.58ms
step:489/1700 train_loss:3.7411 train_time:145906ms step_avg:304.61ms
step:490/1700 train_loss:3.9529 train_time:146221ms step_avg:304.63ms
step:491/1700 train_loss:3.6912 train_time:146535ms step_avg:304.65ms
step:492/1700 train_loss:3.6479 train_time:146849ms step_avg:304.67ms
step:493/1700 train_loss:3.7798 train_time:147162ms step_avg:304.68ms
step:494/1700 train_loss:3.5520 train_time:147479ms step_avg:304.71ms
step:495/1700 train_loss:3.7021 train_time:147794ms step_avg:304.73ms
step:496/1700 train_loss:3.8844 train_time:148113ms step_avg:304.76ms
step:497/1700 train_loss:3.7397 train_time:148433ms step_avg:304.79ms
step:498/1700 train_loss:3.7178 train_time:148748ms step_avg:304.81ms
step:499/1700 train_loss:3.6926 train_time:149059ms step_avg:304.82ms
step:500/1700 train_loss:3.7988 train_time:149376ms step_avg:304.85ms
step:500/1700 val_loss:3.7088 train_time:149384ms step_avg:304.87ms
step:501/1700 train_loss:3.6953 train_time:149696ms step_avg:304.88ms
step:502/1700 train_loss:3.6427 train_time:150009ms step_avg:304.90ms
step:503/1700 train_loss:3.7496 train_time:150325ms step_avg:304.92ms
step:504/1700 train_loss:3.6059 train_time:150638ms step_avg:304.94ms
step:505/1700 train_loss:4.0258 train_time:150953ms step_avg:304.96ms
step:506/1700 train_loss:3.6715 train_time:151267ms step_avg:304.97ms
step:507/1700 train_loss:3.7631 train_time:151584ms step_avg:305.00ms
step:508/1700 train_loss:3.9363 train_time:151900ms step_avg:305.02ms
step:509/1700 train_loss:3.6415 train_time:152216ms step_avg:305.04ms
step:510/1700 train_loss:3.7395 train_time:152530ms step_avg:305.06ms
step:511/1700 train_loss:3.7412 train_time:152847ms step_avg:305.08ms
step:512/1700 train_loss:3.5647 train_time:153163ms step_avg:305.11ms
step:513/1700 train_loss:3.5603 train_time:153482ms step_avg:305.13ms
step:514/1700 train_loss:3.7868 train_time:153796ms step_avg:305.15ms
step:515/1700 train_loss:3.8978 train_time:154114ms step_avg:305.18ms
step:516/1700 train_loss:3.7843 train_time:154429ms step_avg:305.20ms
step:517/1700 train_loss:3.6225 train_time:154746ms step_avg:305.22ms
step:518/1700 train_loss:3.7832 train_time:155059ms step_avg:305.23ms
step:519/1700 train_loss:3.5312 train_time:155372ms step_avg:305.25ms
step:520/1700 train_loss:3.7933 train_time:155689ms step_avg:305.27ms
step:521/1700 train_loss:3.6651 train_time:156004ms step_avg:305.29ms
step:522/1700 train_loss:3.5513 train_time:156318ms step_avg:305.31ms
step:523/1700 train_loss:3.8056 train_time:156632ms step_avg:305.32ms
step:524/1700 train_loss:3.6006 train_time:156949ms step_avg:305.35ms
step:525/1700 train_loss:3.6667 train_time:157264ms step_avg:305.37ms
step:526/1700 train_loss:3.7055 train_time:157580ms step_avg:305.39ms
step:527/1700 train_loss:3.9576 train_time:157895ms step_avg:305.41ms
step:528/1700 train_loss:3.6863 train_time:158209ms step_avg:305.42ms
step:529/1700 train_loss:3.6676 train_time:158524ms step_avg:305.44ms
step:530/1700 train_loss:3.6664 train_time:158839ms step_avg:305.46ms
step:531/1700 train_loss:3.7783 train_time:159155ms step_avg:305.48ms
step:532/1700 train_loss:3.7407 train_time:159472ms step_avg:305.50ms
step:533/1700 train_loss:3.7400 train_time:159786ms step_avg:305.52ms
step:534/1700 train_loss:3.8045 train_time:160101ms step_avg:305.54ms
step:535/1700 train_loss:3.7585 train_time:160421ms step_avg:305.56ms
step:536/1700 train_loss:3.6258 train_time:160742ms step_avg:305.59ms
step:537/1700 train_loss:3.7008 train_time:161058ms step_avg:305.61ms
step:538/1700 train_loss:3.6337 train_time:161372ms step_avg:305.63ms
step:539/1700 train_loss:3.6329 train_time:161689ms step_avg:305.65ms
step:540/1700 train_loss:3.7033 train_time:162007ms step_avg:305.67ms
step:541/1700 train_loss:3.6227 train_time:162324ms step_avg:305.69ms
step:542/1700 train_loss:3.6557 train_time:162638ms step_avg:305.71ms
step:543/1700 train_loss:3.7183 train_time:162954ms step_avg:305.73ms
step:544/1700 train_loss:3.7017 train_time:163268ms step_avg:305.75ms
step:545/1700 train_loss:3.7347 train_time:163584ms step_avg:305.76ms
step:546/1700 train_loss:3.7640 train_time:163899ms step_avg:305.78ms
step:547/1700 train_loss:3.6104 train_time:164214ms step_avg:305.80ms
step:548/1700 train_loss:3.8467 train_time:164529ms step_avg:305.82ms
step:549/1700 train_loss:3.2736 train_time:164847ms step_avg:305.84ms
step:550/1700 train_loss:3.7422 train_time:165162ms step_avg:305.86ms
step:551/1700 train_loss:3.7433 train_time:165478ms step_avg:305.87ms
step:552/1700 train_loss:3.6707 train_time:165793ms step_avg:305.89ms
step:553/1700 train_loss:3.7648 train_time:166109ms step_avg:305.91ms
step:554/1700 train_loss:3.6820 train_time:166426ms step_avg:305.93ms
step:555/1700 train_loss:3.6824 train_time:166741ms step_avg:305.95ms
step:556/1700 train_loss:3.7934 train_time:167059ms step_avg:305.97ms
step:557/1700 train_loss:3.7039 train_time:167373ms step_avg:305.98ms
step:558/1700 train_loss:3.6220 train_time:167687ms step_avg:306.00ms
step:559/1700 train_loss:3.7198 train_time:168002ms step_avg:306.01ms
step:560/1700 train_loss:3.6254 train_time:168316ms step_avg:306.03ms
step:561/1700 train_loss:3.6701 train_time:168633ms step_avg:306.05ms
step:562/1700 train_loss:3.6748 train_time:168946ms step_avg:306.06ms
step:563/1700 train_loss:3.4853 train_time:169263ms step_avg:306.08ms
step:564/1700 train_loss:3.7369 train_time:169577ms step_avg:306.10ms
step:565/1700 train_loss:3.6076 train_time:169893ms step_avg:306.11ms
step:566/1700 train_loss:3.6493 train_time:170207ms step_avg:306.13ms
step:567/1700 train_loss:3.7244 train_time:170525ms step_avg:306.15ms
step:568/1700 train_loss:3.6612 train_time:170837ms step_avg:306.16ms
step:569/1700 train_loss:3.9929 train_time:171154ms step_avg:306.18ms
step:570/1700 train_loss:3.7037 train_time:171645ms step_avg:306.51ms
step:571/1700 train_loss:3.6597 train_time:172056ms step_avg:306.69ms
step:572/1700 train_loss:3.7605 train_time:172368ms step_avg:306.71ms
step:573/1700 train_loss:3.7265 train_time:172681ms step_avg:306.72ms
step:574/1700 train_loss:3.7351 train_time:172997ms step_avg:306.73ms
step:575/1700 train_loss:3.7857 train_time:173325ms step_avg:306.77ms
step:576/1700 train_loss:3.7367 train_time:173641ms step_avg:306.79ms
step:577/1700 train_loss:3.7654 train_time:173958ms step_avg:306.80ms
step:578/1700 train_loss:3.6770 train_time:174274ms step_avg:306.82ms
step:579/1700 train_loss:3.6785 train_time:174592ms step_avg:306.84ms
step:580/1700 train_loss:3.6755 train_time:174909ms step_avg:306.86ms
step:581/1700 train_loss:3.6021 train_time:175226ms step_avg:306.88ms
step:582/1700 train_loss:3.6389 train_time:175543ms step_avg:306.89ms
step:583/1700 train_loss:3.8568 train_time:175865ms step_avg:306.92ms
step:584/1700 train_loss:3.6320 train_time:176185ms step_avg:306.94ms
step:585/1700 train_loss:3.5899 train_time:176505ms step_avg:306.97ms
step:586/1700 train_loss:3.7914 train_time:176822ms step_avg:306.98ms
step:587/1700 train_loss:3.5175 train_time:177144ms step_avg:307.01ms
step:588/1700 train_loss:3.6725 train_time:177465ms step_avg:307.03ms
step:589/1700 train_loss:3.6473 train_time:177783ms step_avg:307.05ms
step:590/1700 train_loss:4.0004 train_time:178104ms step_avg:307.08ms
step:591/1700 train_loss:3.7824 train_time:178421ms step_avg:307.09ms
step:592/1700 train_loss:3.5168 train_time:178738ms step_avg:307.11ms
step:593/1700 train_loss:3.5419 train_time:179062ms step_avg:307.14ms
step:594/1700 train_loss:3.5041 train_time:179381ms step_avg:307.16ms
step:595/1700 train_loss:3.5542 train_time:179703ms step_avg:307.18ms
step:596/1700 train_loss:3.9327 train_time:180030ms step_avg:307.22ms
step:597/1700 train_loss:3.6477 train_time:180349ms step_avg:307.24ms
step:598/1700 train_loss:3.5850 train_time:180665ms step_avg:307.25ms
step:599/1700 train_loss:3.6651 train_time:180983ms step_avg:307.27ms
step:600/1700 train_loss:3.4805 train_time:181300ms step_avg:307.29ms
step:601/1700 train_loss:3.6069 train_time:181616ms step_avg:307.30ms
step:602/1700 train_loss:3.6535 train_time:181933ms step_avg:307.32ms
step:603/1700 train_loss:3.6736 train_time:182251ms step_avg:307.34ms
step:604/1700 train_loss:3.7860 train_time:182573ms step_avg:307.36ms
step:605/1700 train_loss:3.6159 train_time:182889ms step_avg:307.38ms
step:606/1700 train_loss:3.6139 train_time:183209ms step_avg:307.40ms
step:607/1700 train_loss:3.5853 train_time:183530ms step_avg:307.42ms
step:608/1700 train_loss:3.8449 train_time:183847ms step_avg:307.44ms
step:609/1700 train_loss:3.6468 train_time:184169ms step_avg:307.46ms
step:610/1700 train_loss:3.6178 train_time:184486ms step_avg:307.48ms
step:611/1700 train_loss:3.7130 train_time:184806ms step_avg:307.50ms
step:612/1700 train_loss:3.6102 train_time:185125ms step_avg:307.52ms
step:613/1700 train_loss:3.5836 train_time:185442ms step_avg:307.53ms
step:614/1700 train_loss:3.7685 train_time:185764ms step_avg:307.56ms
step:615/1700 train_loss:3.7058 train_time:186083ms step_avg:307.58ms
step:616/1700 train_loss:3.7025 train_time:186401ms step_avg:307.59ms
step:617/1700 train_loss:3.6377 train_time:186718ms step_avg:307.61ms
step:618/1700 train_loss:3.5625 train_time:187036ms step_avg:307.62ms
step:619/1700 train_loss:3.6925 train_time:187353ms step_avg:307.64ms
step:620/1700 train_loss:3.5617 train_time:187672ms step_avg:307.66ms
step:621/1700 train_loss:3.5872 train_time:187992ms step_avg:307.68ms
step:622/1700 train_loss:3.9334 train_time:188309ms step_avg:307.70ms
step:623/1700 train_loss:3.5742 train_time:188630ms step_avg:307.72ms
step:624/1700 train_loss:3.6068 train_time:188949ms step_avg:307.73ms
step:625/1700 train_loss:3.7029 train_time:189269ms step_avg:307.75ms
step:625/1700 val_loss:3.6274 train_time:189278ms step_avg:307.77ms
step:626/1700 train_loss:3.7138 train_time:189594ms step_avg:307.78ms
step:627/1700 train_loss:3.7478 train_time:189914ms step_avg:307.80ms
step:628/1700 train_loss:3.7292 train_time:190234ms step_avg:307.82ms
step:629/1700 train_loss:3.7705 train_time:190553ms step_avg:307.84ms
step:630/1700 train_loss:3.6007 train_time:190873ms step_avg:307.86ms
step:631/1700 train_loss:3.7296 train_time:191191ms step_avg:307.88ms
step:632/1700 train_loss:3.7528 train_time:191510ms step_avg:307.89ms
step:633/1700 train_loss:3.6544 train_time:191829ms step_avg:307.91ms
step:634/1700 train_loss:3.6123 train_time:192149ms step_avg:307.93ms
step:635/1700 train_loss:3.7011 train_time:192470ms step_avg:307.95ms
step:636/1700 train_loss:3.9527 train_time:192789ms step_avg:307.97ms
step:637/1700 train_loss:3.5520 train_time:193107ms step_avg:307.99ms
step:638/1700 train_loss:3.3634 train_time:193428ms step_avg:308.01ms
step:639/1700 train_loss:3.5942 train_time:193746ms step_avg:308.02ms
step:640/1700 train_loss:3.6389 train_time:194063ms step_avg:308.04ms
step:641/1700 train_loss:3.5830 train_time:194381ms step_avg:308.05ms
step:642/1700 train_loss:3.5842 train_time:194697ms step_avg:308.06ms
step:643/1700 train_loss:3.6370 train_time:195015ms step_avg:308.08ms
step:644/1700 train_loss:3.6117 train_time:195335ms step_avg:308.10ms
step:645/1700 train_loss:3.5652 train_time:195653ms step_avg:308.11ms
step:646/1700 train_loss:3.7882 train_time:195972ms step_avg:308.13ms
step:647/1700 train_loss:3.6850 train_time:196293ms step_avg:308.15ms
step:648/1700 train_loss:3.6688 train_time:196610ms step_avg:308.17ms
step:649/1700 train_loss:3.7108 train_time:196937ms step_avg:308.20ms
step:650/1700 train_loss:3.7708 train_time:197255ms step_avg:308.21ms
step:651/1700 train_loss:3.6245 train_time:197574ms step_avg:308.23ms
step:652/1700 train_loss:3.7713 train_time:197895ms step_avg:308.25ms
step:653/1700 train_loss:3.5946 train_time:198214ms step_avg:308.26ms
step:654/1700 train_loss:3.6684 train_time:198532ms step_avg:308.28ms
step:655/1700 train_loss:3.4359 train_time:198853ms step_avg:308.30ms
step:656/1700 train_loss:3.5850 train_time:199169ms step_avg:308.31ms
step:657/1700 train_loss:3.5819 train_time:199489ms step_avg:308.33ms
step:658/1700 train_loss:3.5049 train_time:199807ms step_avg:308.34ms
step:659/1700 train_loss:3.6950 train_time:200127ms step_avg:308.36ms
step:660/1700 train_loss:3.5889 train_time:200446ms step_avg:308.38ms
step:661/1700 train_loss:3.6863 train_time:200763ms step_avg:308.39ms
step:662/1700 train_loss:3.7550 train_time:201082ms step_avg:308.41ms
step:663/1700 train_loss:3.6800 train_time:201398ms step_avg:308.42ms
step:664/1700 train_loss:3.5619 train_time:201715ms step_avg:308.43ms
step:665/1700 train_loss:3.6163 train_time:202036ms step_avg:308.45ms
step:666/1700 train_loss:3.5010 train_time:202357ms step_avg:308.47ms
step:667/1700 train_loss:3.7906 train_time:202672ms step_avg:308.48ms
step:668/1700 train_loss:3.6227 train_time:202992ms step_avg:308.50ms
step:669/1700 train_loss:3.6505 train_time:203314ms step_avg:308.52ms
step:670/1700 train_loss:3.4881 train_time:203635ms step_avg:308.54ms
step:671/1700 train_loss:3.6021 train_time:203955ms step_avg:308.56ms
step:672/1700 train_loss:3.5657 train_time:204273ms step_avg:308.57ms
step:673/1700 train_loss:3.5781 train_time:204591ms step_avg:308.58ms
step:674/1700 train_loss:3.8591 train_time:204910ms step_avg:308.60ms
step:675/1700 train_loss:3.6362 train_time:205229ms step_avg:308.62ms
step:676/1700 train_loss:3.7213 train_time:205550ms step_avg:308.63ms
step:677/1700 train_loss:3.4993 train_time:205871ms step_avg:308.65ms
step:678/1700 train_loss:3.6063 train_time:206191ms step_avg:308.67ms
step:679/1700 train_loss:3.5605 train_time:206509ms step_avg:308.68ms
step:680/1700 train_loss:3.6884 train_time:206831ms step_avg:308.70ms
step:681/1700 train_loss:3.5909 train_time:207153ms step_avg:308.72ms
step:682/1700 train_loss:3.6210 train_time:207469ms step_avg:308.73ms
step:683/1700 train_loss:3.6701 train_time:207790ms step_avg:308.75ms
step:684/1700 train_loss:3.7373 train_time:208107ms step_avg:308.76ms
step:685/1700 train_loss:3.6481 train_time:208425ms step_avg:308.78ms
step:686/1700 train_loss:3.7044 train_time:208744ms step_avg:308.79ms
step:687/1700 train_loss:3.6422 train_time:209060ms step_avg:308.80ms
step:688/1700 train_loss:3.6686 train_time:209377ms step_avg:308.82ms
step:689/1700 train_loss:3.2006 train_time:209699ms step_avg:308.84ms
step:690/1700 train_loss:3.4070 train_time:210019ms step_avg:308.85ms
step:691/1700 train_loss:3.5471 train_time:210341ms step_avg:308.87ms
step:692/1700 train_loss:3.4249 train_time:210661ms step_avg:308.89ms
step:693/1700 train_loss:3.6290 train_time:210982ms step_avg:308.90ms
step:694/1700 train_loss:3.6540 train_time:211303ms step_avg:308.92ms
step:695/1700 train_loss:3.5612 train_time:211624ms step_avg:308.94ms
step:696/1700 train_loss:3.5409 train_time:211942ms step_avg:308.95ms
step:697/1700 train_loss:3.8746 train_time:212264ms step_avg:308.97ms
step:698/1700 train_loss:3.5986 train_time:212586ms step_avg:308.99ms
step:699/1700 train_loss:3.6506 train_time:212905ms step_avg:309.01ms
step:700/1700 train_loss:3.7800 train_time:213227ms step_avg:309.03ms
step:701/1700 train_loss:3.5750 train_time:213546ms step_avg:309.04ms
step:702/1700 train_loss:3.5547 train_time:213865ms step_avg:309.05ms
step:703/1700 train_loss:3.5253 train_time:214193ms step_avg:309.08ms
step:704/1700 train_loss:3.5016 train_time:214514ms step_avg:309.10ms
step:705/1700 train_loss:3.5770 train_time:214843ms step_avg:309.13ms
step:706/1700 train_loss:3.5710 train_time:215164ms step_avg:309.14ms
step:707/1700 train_loss:3.5908 train_time:215491ms step_avg:309.17ms
step:708/1700 train_loss:3.6578 train_time:215814ms step_avg:309.19ms
step:709/1700 train_loss:3.6112 train_time:216141ms step_avg:309.21ms
step:710/1700 train_loss:3.5919 train_time:216462ms step_avg:309.23ms
step:711/1700 train_loss:3.5551 train_time:216784ms step_avg:309.25ms
step:712/1700 train_loss:3.6013 train_time:217111ms step_avg:309.28ms
step:713/1700 train_loss:3.6561 train_time:217433ms step_avg:309.29ms
step:714/1700 train_loss:3.6610 train_time:217759ms step_avg:309.32ms
step:715/1700 train_loss:3.5684 train_time:218081ms step_avg:309.33ms
step:716/1700 train_loss:3.5846 train_time:218400ms step_avg:309.35ms
step:717/1700 train_loss:3.5980 train_time:218720ms step_avg:309.36ms
step:718/1700 train_loss:3.7129 train_time:219044ms step_avg:309.38ms
step:719/1700 train_loss:3.6049 train_time:219362ms step_avg:309.40ms
step:720/1700 train_loss:3.6857 train_time:219680ms step_avg:309.41ms
step:721/1700 train_loss:3.8527 train_time:220005ms step_avg:309.43ms
step:722/1700 train_loss:3.4740 train_time:220325ms step_avg:309.44ms
step:723/1700 train_loss:3.7399 train_time:220653ms step_avg:309.47ms
step:724/1700 train_loss:3.7782 train_time:220972ms step_avg:309.48ms
step:725/1700 train_loss:3.5696 train_time:221293ms step_avg:309.50ms
step:726/1700 train_loss:3.6615 train_time:221619ms step_avg:309.52ms
step:727/1700 train_loss:3.5434 train_time:221940ms step_avg:309.54ms
step:728/1700 train_loss:3.5828 train_time:222260ms step_avg:309.55ms
step:729/1700 train_loss:3.7390 train_time:222580ms step_avg:309.57ms
step:730/1700 train_loss:3.6698 train_time:222902ms step_avg:309.59ms
step:731/1700 train_loss:3.6767 train_time:223228ms step_avg:309.61ms
step:732/1700 train_loss:3.5695 train_time:223553ms step_avg:309.63ms
step:733/1700 train_loss:3.6068 train_time:223871ms step_avg:309.64ms
step:734/1700 train_loss:3.8406 train_time:224194ms step_avg:309.66ms
step:735/1700 train_loss:3.5740 train_time:224517ms step_avg:309.68ms
step:736/1700 train_loss:3.6183 train_time:224841ms step_avg:309.70ms
step:737/1700 train_loss:3.7451 train_time:225162ms step_avg:309.71ms
step:738/1700 train_loss:3.6891 train_time:225480ms step_avg:309.73ms
step:739/1700 train_loss:3.6104 train_time:225800ms step_avg:309.74ms
step:740/1700 train_loss:3.5133 train_time:226120ms step_avg:309.75ms
step:741/1700 train_loss:4.1181 train_time:226449ms step_avg:309.78ms
step:742/1700 train_loss:3.5011 train_time:226769ms step_avg:309.79ms
step:743/1700 train_loss:3.5631 train_time:227092ms step_avg:309.81ms
step:744/1700 train_loss:3.5934 train_time:227416ms step_avg:309.83ms
step:745/1700 train_loss:3.6596 train_time:227737ms step_avg:309.85ms
step:746/1700 train_loss:3.6005 train_time:228058ms step_avg:309.86ms
step:747/1700 train_loss:3.6074 train_time:228376ms step_avg:309.87ms
step:748/1700 train_loss:3.6568 train_time:228699ms step_avg:309.89ms
step:749/1700 train_loss:3.5781 train_time:229021ms step_avg:309.91ms
step:750/1700 train_loss:3.5674 train_time:229347ms step_avg:309.93ms
step:750/1700 val_loss:3.5755 train_time:229356ms step_avg:309.94ms
step:751/1700 train_loss:3.6100 train_time:229677ms step_avg:309.95ms
step:752/1700 train_loss:3.5786 train_time:229996ms step_avg:309.97ms
step:753/1700 train_loss:3.6241 train_time:230318ms step_avg:309.98ms
step:754/1700 train_loss:3.6246 train_time:230641ms step_avg:310.00ms
step:755/1700 train_loss:3.5996 train_time:230959ms step_avg:310.01ms
step:756/1700 train_loss:3.6847 train_time:231282ms step_avg:310.03ms
step:757/1700 train_loss:3.4755 train_time:231605ms step_avg:310.05ms
step:758/1700 train_loss:3.7322 train_time:231936ms step_avg:310.07ms
step:759/1700 train_loss:3.6606 train_time:232254ms step_avg:310.09ms
step:760/1700 train_loss:3.6012 train_time:232768ms step_avg:310.36ms
step:761/1700 train_loss:3.7142 train_time:233086ms step_avg:310.37ms
step:762/1700 train_loss:3.6194 train_time:233583ms step_avg:310.62ms
step:763/1700 train_loss:3.4532 train_time:233906ms step_avg:310.63ms
step:764/1700 train_loss:3.4429 train_time:234229ms step_avg:310.65ms
step:765/1700 train_loss:3.5521 train_time:234557ms step_avg:310.67ms
step:766/1700 train_loss:3.5534 train_time:234877ms step_avg:310.68ms
step:767/1700 train_loss:4.5992 train_time:235202ms step_avg:310.70ms
step:768/1700 train_loss:3.5574 train_time:235527ms step_avg:310.72ms
step:769/1700 train_loss:3.6003 train_time:235848ms step_avg:310.73ms
step:770/1700 train_loss:3.6865 train_time:236172ms step_avg:310.75ms
step:771/1700 train_loss:4.1887 train_time:236496ms step_avg:310.77ms
step:772/1700 train_loss:3.6037 train_time:236818ms step_avg:310.78ms
step:773/1700 train_loss:3.6097 train_time:237137ms step_avg:310.80ms
step:774/1700 train_loss:3.5709 train_time:237459ms step_avg:310.81ms
step:775/1700 train_loss:3.7011 train_time:237778ms step_avg:310.82ms
step:776/1700 train_loss:3.4946 train_time:238099ms step_avg:310.83ms
step:777/1700 train_loss:3.6329 train_time:238421ms step_avg:310.85ms
step:778/1700 train_loss:3.6195 train_time:238745ms step_avg:310.87ms
step:779/1700 train_loss:3.5869 train_time:239072ms step_avg:310.89ms
step:780/1700 train_loss:3.5907 train_time:239394ms step_avg:310.90ms
step:781/1700 train_loss:3.4833 train_time:239717ms step_avg:310.92ms
step:782/1700 train_loss:3.6428 train_time:240038ms step_avg:310.93ms
step:783/1700 train_loss:3.5822 train_time:240360ms step_avg:310.94ms
step:784/1700 train_loss:3.5453 train_time:240679ms step_avg:310.96ms
step:785/1700 train_loss:3.5560 train_time:241000ms step_avg:310.97ms
step:786/1700 train_loss:3.5765 train_time:241320ms step_avg:310.98ms
step:787/1700 train_loss:3.5304 train_time:241642ms step_avg:310.99ms
step:788/1700 train_loss:3.5921 train_time:241963ms step_avg:311.01ms
step:789/1700 train_loss:3.5538 train_time:242285ms step_avg:311.02ms
step:790/1700 train_loss:3.4769 train_time:242607ms step_avg:311.03ms
step:791/1700 train_loss:3.5395 train_time:242933ms step_avg:311.05ms
step:792/1700 train_loss:3.6062 train_time:243254ms step_avg:311.07ms
step:793/1700 train_loss:3.6186 train_time:243576ms step_avg:311.08ms
step:794/1700 train_loss:3.6437 train_time:243902ms step_avg:311.10ms
step:795/1700 train_loss:3.5781 train_time:244224ms step_avg:311.11ms
step:796/1700 train_loss:3.6893 train_time:244549ms step_avg:311.13ms
step:797/1700 train_loss:3.5936 train_time:244872ms step_avg:311.15ms
step:798/1700 train_loss:3.3998 train_time:245196ms step_avg:311.16ms
step:799/1700 train_loss:3.4843 train_time:245517ms step_avg:311.17ms
step:800/1700 train_loss:4.2438 train_time:245841ms step_avg:311.19ms
step:801/1700 train_loss:3.7097 train_time:246163ms step_avg:311.20ms
step:802/1700 train_loss:3.5565 train_time:246481ms step_avg:311.21ms
step:803/1700 train_loss:3.6100 train_time:246803ms step_avg:311.23ms
step:804/1700 train_loss:3.5928 train_time:247123ms step_avg:311.24ms
step:805/1700 train_loss:3.5350 train_time:247445ms step_avg:311.25ms
step:806/1700 train_loss:3.5380 train_time:247776ms step_avg:311.28ms
step:807/1700 train_loss:3.5686 train_time:248100ms step_avg:311.29ms
step:808/1700 train_loss:3.6312 train_time:248421ms step_avg:311.31ms
step:809/1700 train_loss:3.8462 train_time:248744ms step_avg:311.32ms
step:810/1700 train_loss:3.6929 train_time:249066ms step_avg:311.33ms
step:811/1700 train_loss:3.4951 train_time:249392ms step_avg:311.35ms
step:812/1700 train_loss:3.6131 train_time:249717ms step_avg:311.37ms
step:813/1700 train_loss:3.6295 train_time:250039ms step_avg:311.38ms
step:814/1700 train_loss:3.5671 train_time:250359ms step_avg:311.39ms
step:815/1700 train_loss:3.4252 train_time:250686ms step_avg:311.41ms
step:816/1700 train_loss:3.7671 train_time:251012ms step_avg:311.43ms
step:817/1700 train_loss:3.5804 train_time:251335ms step_avg:311.44ms
step:818/1700 train_loss:3.5444 train_time:251660ms step_avg:311.46ms
step:819/1700 train_loss:3.5577 train_time:251983ms step_avg:311.47ms
step:820/1700 train_loss:3.5407 train_time:252304ms step_avg:311.49ms
step:821/1700 train_loss:3.4342 train_time:252629ms step_avg:311.50ms
step:822/1700 train_loss:3.5592 train_time:252952ms step_avg:311.52ms
step:823/1700 train_loss:3.6528 train_time:253275ms step_avg:311.53ms
step:824/1700 train_loss:3.3918 train_time:253598ms step_avg:311.55ms
step:825/1700 train_loss:3.5953 train_time:253922ms step_avg:311.56ms
step:826/1700 train_loss:3.6922 train_time:254248ms step_avg:311.58ms
step:827/1700 train_loss:3.4491 train_time:254577ms step_avg:311.60ms
step:828/1700 train_loss:3.5124 train_time:254905ms step_avg:311.62ms
step:829/1700 train_loss:3.5212 train_time:255229ms step_avg:311.64ms
step:830/1700 train_loss:3.6161 train_time:255552ms step_avg:311.65ms
step:831/1700 train_loss:3.4620 train_time:255878ms step_avg:311.67ms
step:832/1700 train_loss:3.5871 train_time:256205ms step_avg:311.68ms
step:833/1700 train_loss:3.6058 train_time:256529ms step_avg:311.70ms
step:834/1700 train_loss:3.6188 train_time:256858ms step_avg:311.72ms
step:835/1700 train_loss:3.4682 train_time:257190ms step_avg:311.75ms
step:836/1700 train_loss:3.6956 train_time:257515ms step_avg:311.76ms
step:837/1700 train_loss:3.4710 train_time:257839ms step_avg:311.78ms
step:838/1700 train_loss:3.3918 train_time:258160ms step_avg:311.79ms
step:839/1700 train_loss:3.6340 train_time:258483ms step_avg:311.80ms
step:840/1700 train_loss:3.5469 train_time:258808ms step_avg:311.82ms
step:841/1700 train_loss:3.6327 train_time:259136ms step_avg:311.84ms
step:842/1700 train_loss:3.5229 train_time:259458ms step_avg:311.85ms
step:843/1700 train_loss:3.5674 train_time:259781ms step_avg:311.86ms
step:844/1700 train_loss:3.5356 train_time:260106ms step_avg:311.88ms
step:845/1700 train_loss:3.5493 train_time:260433ms step_avg:311.90ms
step:846/1700 train_loss:3.5633 train_time:260758ms step_avg:311.91ms
step:847/1700 train_loss:3.6025 train_time:261084ms step_avg:311.93ms
step:848/1700 train_loss:3.5481 train_time:261410ms step_avg:311.95ms
step:849/1700 train_loss:3.3832 train_time:261737ms step_avg:311.96ms
step:850/1700 train_loss:3.5936 train_time:262060ms step_avg:311.98ms
step:851/1700 train_loss:3.4845 train_time:262384ms step_avg:311.99ms
step:852/1700 train_loss:3.5907 train_time:262712ms step_avg:312.01ms
step:853/1700 train_loss:3.3709 train_time:263036ms step_avg:312.02ms
step:854/1700 train_loss:3.6674 train_time:263358ms step_avg:312.04ms
step:855/1700 train_loss:3.5952 train_time:263680ms step_avg:312.05ms
step:856/1700 train_loss:3.3765 train_time:264005ms step_avg:312.06ms
step:857/1700 train_loss:3.6713 train_time:264337ms step_avg:312.09ms
step:858/1700 train_loss:3.6675 train_time:264663ms step_avg:312.10ms
step:859/1700 train_loss:3.3830 train_time:264993ms step_avg:312.12ms
step:860/1700 train_loss:3.5661 train_time:265318ms step_avg:312.14ms
step:861/1700 train_loss:3.6238 train_time:265644ms step_avg:312.15ms
step:862/1700 train_loss:3.4400 train_time:265965ms step_avg:312.17ms
step:863/1700 train_loss:3.5104 train_time:266296ms step_avg:312.19ms
step:864/1700 train_loss:3.8077 train_time:266627ms step_avg:312.21ms
step:865/1700 train_loss:3.7578 train_time:266965ms step_avg:312.24ms
step:866/1700 train_loss:3.5701 train_time:267290ms step_avg:312.25ms
step:867/1700 train_loss:3.5247 train_time:267613ms step_avg:312.27ms
step:868/1700 train_loss:3.7165 train_time:267943ms step_avg:312.29ms
step:869/1700 train_loss:3.4560 train_time:268270ms step_avg:312.30ms
step:870/1700 train_loss:3.4054 train_time:268594ms step_avg:312.32ms
step:871/1700 train_loss:3.5804 train_time:268919ms step_avg:312.33ms
step:872/1700 train_loss:3.5273 train_time:269241ms step_avg:312.35ms
step:873/1700 train_loss:3.4762 train_time:269566ms step_avg:312.36ms
step:874/1700 train_loss:3.6208 train_time:269889ms step_avg:312.37ms
step:875/1700 train_loss:3.5235 train_time:270214ms step_avg:312.39ms
step:875/1700 val_loss:3.5320 train_time:270223ms step_avg:312.40ms
step:876/1700 train_loss:3.6351 train_time:270550ms step_avg:312.41ms
step:877/1700 train_loss:3.4271 train_time:270874ms step_avg:312.43ms
step:878/1700 train_loss:3.6358 train_time:271199ms step_avg:312.44ms
step:879/1700 train_loss:3.5087 train_time:271523ms step_avg:312.45ms
step:880/1700 train_loss:3.8602 train_time:271850ms step_avg:312.47ms
step:881/1700 train_loss:3.5686 train_time:272176ms step_avg:312.49ms
step:882/1700 train_loss:3.3962 train_time:272495ms step_avg:312.49ms
step:883/1700 train_loss:3.7156 train_time:272820ms step_avg:312.51ms
step:884/1700 train_loss:3.4279 train_time:273145ms step_avg:312.52ms
step:885/1700 train_loss:3.6746 train_time:273473ms step_avg:312.54ms
step:886/1700 train_loss:3.5443 train_time:273804ms step_avg:312.56ms
step:887/1700 train_loss:3.6070 train_time:274133ms step_avg:312.58ms
step:888/1700 train_loss:3.5844 train_time:274455ms step_avg:312.59ms
step:889/1700 train_loss:3.6176 train_time:274779ms step_avg:312.60ms
step:890/1700 train_loss:3.5822 train_time:275113ms step_avg:312.63ms
step:891/1700 train_loss:3.4247 train_time:275435ms step_avg:312.64ms
step:892/1700 train_loss:3.5984 train_time:275758ms step_avg:312.65ms
step:893/1700 train_loss:3.5194 train_time:276084ms step_avg:312.67ms
step:894/1700 train_loss:3.5680 train_time:276413ms step_avg:312.68ms
step:895/1700 train_loss:3.4009 train_time:276735ms step_avg:312.70ms
step:896/1700 train_loss:3.3323 train_time:277065ms step_avg:312.71ms
step:897/1700 train_loss:3.4775 train_time:277390ms step_avg:312.73ms
step:898/1700 train_loss:3.6587 train_time:277717ms step_avg:312.74ms
step:899/1700 train_loss:3.5250 train_time:278040ms step_avg:312.76ms
step:900/1700 train_loss:3.5853 train_time:278367ms step_avg:312.77ms
step:901/1700 train_loss:3.7384 train_time:278689ms step_avg:312.78ms
step:902/1700 train_loss:3.5530 train_time:279015ms step_avg:312.80ms
step:903/1700 train_loss:3.4553 train_time:279337ms step_avg:312.81ms
step:904/1700 train_loss:3.6843 train_time:279665ms step_avg:312.82ms
step:905/1700 train_loss:3.6312 train_time:279989ms step_avg:312.84ms
step:906/1700 train_loss:3.4807 train_time:280315ms step_avg:312.85ms
step:907/1700 train_loss:3.4833 train_time:280637ms step_avg:312.86ms
step:908/1700 train_loss:3.7741 train_time:280971ms step_avg:312.89ms
step:909/1700 train_loss:3.4949 train_time:281294ms step_avg:312.90ms
step:910/1700 train_loss:3.6796 train_time:281616ms step_avg:312.91ms
step:911/1700 train_loss:3.8713 train_time:281950ms step_avg:312.93ms
step:912/1700 train_loss:3.3384 train_time:282274ms step_avg:312.94ms
step:913/1700 train_loss:3.6487 train_time:282596ms step_avg:312.95ms
step:914/1700 train_loss:3.5010 train_time:282922ms step_avg:312.97ms
step:915/1700 train_loss:3.5796 train_time:283248ms step_avg:312.98ms
step:916/1700 train_loss:3.7304 train_time:283576ms step_avg:313.00ms
step:917/1700 train_loss:3.4954 train_time:283897ms step_avg:313.01ms
step:918/1700 train_loss:3.4749 train_time:284225ms step_avg:313.02ms
step:919/1700 train_loss:3.5682 train_time:284550ms step_avg:313.04ms
step:920/1700 train_loss:3.4602 train_time:284880ms step_avg:313.06ms
step:921/1700 train_loss:3.5105 train_time:285215ms step_avg:313.08ms
step:922/1700 train_loss:3.4712 train_time:285541ms step_avg:313.09ms
step:923/1700 train_loss:3.6278 train_time:285869ms step_avg:313.11ms
step:924/1700 train_loss:3.5012 train_time:286194ms step_avg:313.12ms
step:925/1700 train_loss:3.4883 train_time:286519ms step_avg:313.14ms
step:926/1700 train_loss:3.6078 train_time:286851ms step_avg:313.16ms
step:927/1700 train_loss:3.4746 train_time:287178ms step_avg:313.17ms
step:928/1700 train_loss:3.6640 train_time:287507ms step_avg:313.19ms
step:929/1700 train_loss:3.5376 train_time:287832ms step_avg:313.20ms
step:930/1700 train_loss:3.3877 train_time:288162ms step_avg:313.22ms
step:931/1700 train_loss:3.7222 train_time:288489ms step_avg:313.23ms
step:932/1700 train_loss:3.3985 train_time:288815ms step_avg:313.25ms
step:933/1700 train_loss:3.3663 train_time:289140ms step_avg:313.26ms
step:934/1700 train_loss:3.5918 train_time:289473ms step_avg:313.28ms
step:935/1700 train_loss:3.5446 train_time:289797ms step_avg:313.29ms
step:936/1700 train_loss:3.3972 train_time:290134ms step_avg:313.32ms
step:937/1700 train_loss:3.3535 train_time:290463ms step_avg:313.34ms
step:938/1700 train_loss:3.5144 train_time:290789ms step_avg:313.35ms
step:939/1700 train_loss:3.3177 train_time:291116ms step_avg:313.37ms
step:940/1700 train_loss:3.5951 train_time:291439ms step_avg:313.38ms
step:941/1700 train_loss:3.4407 train_time:291775ms step_avg:313.40ms
step:942/1700 train_loss:3.4408 train_time:292105ms step_avg:313.42ms
step:943/1700 train_loss:3.5668 train_time:292438ms step_avg:313.44ms
step:944/1700 train_loss:3.4495 train_time:292762ms step_avg:313.45ms
step:945/1700 train_loss:3.4630 train_time:293087ms step_avg:313.46ms
step:946/1700 train_loss:3.6277 train_time:293422ms step_avg:313.48ms
step:947/1700 train_loss:3.5351 train_time:293748ms step_avg:313.50ms
step:948/1700 train_loss:3.6010 train_time:294079ms step_avg:313.52ms
step:949/1700 train_loss:3.7678 train_time:294409ms step_avg:313.53ms
step:950/1700 train_loss:3.3961 train_time:294917ms step_avg:313.74ms
step:951/1700 train_loss:3.4651 train_time:295243ms step_avg:313.75ms
step:952/1700 train_loss:3.7093 train_time:295661ms step_avg:313.87ms
step:953/1700 train_loss:3.4204 train_time:295981ms step_avg:313.87ms
step:954/1700 train_loss:3.4838 train_time:296311ms step_avg:313.89ms
step:955/1700 train_loss:3.5844 train_time:296645ms step_avg:313.91ms
step:956/1700 train_loss:3.4580 train_time:296975ms step_avg:313.93ms
step:957/1700 train_loss:3.4868 train_time:297297ms step_avg:313.94ms
step:958/1700 train_loss:3.4547 train_time:297631ms step_avg:313.96ms
step:959/1700 train_loss:3.5179 train_time:297962ms step_avg:313.98ms
step:960/1700 train_loss:3.5185 train_time:298293ms step_avg:313.99ms
step:961/1700 train_loss:3.5292 train_time:298621ms step_avg:314.01ms
step:962/1700 train_loss:3.4165 train_time:298958ms step_avg:314.03ms
step:963/1700 train_loss:3.6641 train_time:299286ms step_avg:314.05ms
step:964/1700 train_loss:3.6154 train_time:299612ms step_avg:314.06ms
step:965/1700 train_loss:3.6288 train_time:299941ms step_avg:314.07ms
step:966/1700 train_loss:3.4445 train_time:300273ms step_avg:314.09ms
step:967/1700 train_loss:3.4977 train_time:300595ms step_avg:314.10ms
step:968/1700 train_loss:3.7302 train_time:300922ms step_avg:314.12ms
step:969/1700 train_loss:3.5412 train_time:301250ms step_avg:314.13ms
step:970/1700 train_loss:3.5424 train_time:301576ms step_avg:314.14ms
step:971/1700 train_loss:3.6026 train_time:301905ms step_avg:314.16ms
step:972/1700 train_loss:3.3886 train_time:302232ms step_avg:314.17ms
step:973/1700 train_loss:3.5492 train_time:302560ms step_avg:314.18ms
step:974/1700 train_loss:3.4913 train_time:302884ms step_avg:314.20ms
step:975/1700 train_loss:3.5623 train_time:303211ms step_avg:314.21ms
step:976/1700 train_loss:3.6126 train_time:303537ms step_avg:314.22ms
step:977/1700 train_loss:3.4914 train_time:303865ms step_avg:314.23ms
step:978/1700 train_loss:3.6919 train_time:304194ms step_avg:314.25ms
step:979/1700 train_loss:3.5918 train_time:304519ms step_avg:314.26ms
step:980/1700 train_loss:3.3737 train_time:304848ms step_avg:314.28ms
step:981/1700 train_loss:3.6396 train_time:305177ms step_avg:314.29ms
step:982/1700 train_loss:3.4394 train_time:305501ms step_avg:314.30ms
step:983/1700 train_loss:3.6018 train_time:305823ms step_avg:314.31ms
step:984/1700 train_loss:3.5668 train_time:306155ms step_avg:314.33ms
step:985/1700 train_loss:3.5335 train_time:306487ms step_avg:314.35ms
step:986/1700 train_loss:3.5137 train_time:306813ms step_avg:314.36ms
step:987/1700 train_loss:3.5988 train_time:307140ms step_avg:314.37ms
step:988/1700 train_loss:3.4427 train_time:307465ms step_avg:314.38ms
step:989/1700 train_loss:3.5134 train_time:307795ms step_avg:314.40ms
step:990/1700 train_loss:3.5230 train_time:308121ms step_avg:314.41ms
step:991/1700 train_loss:3.4403 train_time:308454ms step_avg:314.43ms
step:992/1700 train_loss:3.6819 train_time:308788ms step_avg:314.45ms
step:993/1700 train_loss:3.4929 train_time:309113ms step_avg:314.46ms
step:994/1700 train_loss:3.4650 train_time:309438ms step_avg:314.47ms
step:995/1700 train_loss:3.5320 train_time:309779ms step_avg:314.50ms
step:996/1700 train_loss:3.6161 train_time:310104ms step_avg:314.51ms
step:997/1700 train_loss:3.5561 train_time:310428ms step_avg:314.52ms
step:998/1700 train_loss:3.4884 train_time:310753ms step_avg:314.53ms
step:999/1700 train_loss:3.7898 train_time:311078ms step_avg:314.54ms
step:1000/1700 train_loss:3.4629 train_time:311402ms step_avg:314.55ms
step:1000/1700 val_loss:3.4955 train_time:311411ms step_avg:314.56ms
step:1001/1700 train_loss:3.6162 train_time:311730ms step_avg:314.56ms
step:1002/1700 train_loss:3.4701 train_time:312057ms step_avg:314.57ms
step:1003/1700 train_loss:3.5321 train_time:312380ms step_avg:314.58ms
step:1004/1700 train_loss:3.4046 train_time:312712ms step_avg:314.60ms
step:1005/1700 train_loss:3.5829 train_time:313045ms step_avg:314.62ms
step:1006/1700 train_loss:3.6301 train_time:313377ms step_avg:314.64ms
step:1007/1700 train_loss:3.4196 train_time:313705ms step_avg:314.65ms
step:1008/1700 train_loss:3.4919 train_time:314031ms step_avg:314.66ms
step:1009/1700 train_loss:3.4670 train_time:314358ms step_avg:314.67ms
step:1010/1700 train_loss:3.5888 train_time:314691ms step_avg:314.69ms
step:1011/1700 train_loss:3.6927 train_time:315030ms step_avg:314.71ms
step:1012/1700 train_loss:3.5763 train_time:315356ms step_avg:314.73ms
step:1013/1700 train_loss:3.5625 train_time:315680ms step_avg:314.74ms
step:1014/1700 train_loss:3.4185 train_time:316010ms step_avg:314.75ms
step:1015/1700 train_loss:3.5617 train_time:316331ms step_avg:314.76ms
step:1016/1700 train_loss:3.6564 train_time:316656ms step_avg:314.77ms
step:1017/1700 train_loss:3.3544 train_time:316981ms step_avg:314.78ms
step:1018/1700 train_loss:3.4367 train_time:317313ms step_avg:314.79ms
step:1019/1700 train_loss:3.4234 train_time:317646ms step_avg:314.81ms
step:1020/1700 train_loss:3.4175 train_time:317970ms step_avg:314.82ms
step:1021/1700 train_loss:3.5546 train_time:318297ms step_avg:314.83ms
step:1022/1700 train_loss:3.4286 train_time:318627ms step_avg:314.85ms
step:1023/1700 train_loss:3.3854 train_time:318955ms step_avg:314.86ms
step:1024/1700 train_loss:3.5088 train_time:319284ms step_avg:314.88ms
step:1025/1700 train_loss:3.5340 train_time:319614ms step_avg:314.89ms
step:1026/1700 train_loss:3.5040 train_time:319944ms step_avg:314.91ms
step:1027/1700 train_loss:3.5093 train_time:320273ms step_avg:314.92ms
step:1028/1700 train_loss:3.6553 train_time:320596ms step_avg:314.93ms
step:1029/1700 train_loss:3.3548 train_time:320925ms step_avg:314.94ms
step:1030/1700 train_loss:3.4292 train_time:321263ms step_avg:314.96ms
step:1031/1700 train_loss:3.3481 train_time:321591ms step_avg:314.98ms
step:1032/1700 train_loss:3.5694 train_time:321913ms step_avg:314.98ms
step:1033/1700 train_loss:3.5494 train_time:322238ms step_avg:314.99ms
step:1034/1700 train_loss:3.7323 train_time:322569ms step_avg:315.01ms
step:1035/1700 train_loss:3.5219 train_time:322901ms step_avg:315.03ms
step:1036/1700 train_loss:3.4399 train_time:323238ms step_avg:315.05ms
step:1037/1700 train_loss:3.4776 train_time:323570ms step_avg:315.06ms
step:1038/1700 train_loss:3.5189 train_time:323899ms step_avg:315.08ms
step:1039/1700 train_loss:3.8288 train_time:324229ms step_avg:315.09ms
step:1040/1700 train_loss:3.6508 train_time:324556ms step_avg:315.10ms
step:1041/1700 train_loss:3.5493 train_time:324884ms step_avg:315.12ms
step:1042/1700 train_loss:3.4478 train_time:325213ms step_avg:315.13ms
step:1043/1700 train_loss:3.5199 train_time:325548ms step_avg:315.15ms
step:1044/1700 train_loss:3.5573 train_time:325883ms step_avg:315.17ms
step:1045/1700 train_loss:3.4739 train_time:326208ms step_avg:315.18ms
step:1046/1700 train_loss:3.4855 train_time:326535ms step_avg:315.19ms
step:1047/1700 train_loss:3.5579 train_time:326869ms step_avg:315.21ms
step:1048/1700 train_loss:3.4638 train_time:327199ms step_avg:315.22ms
step:1049/1700 train_loss:3.6778 train_time:327530ms step_avg:315.24ms
step:1050/1700 train_loss:3.5408 train_time:327866ms step_avg:315.26ms
step:1051/1700 train_loss:3.4357 train_time:328195ms step_avg:315.27ms
step:1052/1700 train_loss:3.4314 train_time:328525ms step_avg:315.28ms
step:1053/1700 train_loss:3.5374 train_time:328854ms step_avg:315.30ms
step:1054/1700 train_loss:3.3939 train_time:329188ms step_avg:315.31ms
step:1055/1700 train_loss:3.7371 train_time:329512ms step_avg:315.32ms
step:1056/1700 train_loss:3.5790 train_time:329849ms step_avg:315.34ms
step:1057/1700 train_loss:3.4200 train_time:330179ms step_avg:315.36ms
step:1058/1700 train_loss:3.5431 train_time:330511ms step_avg:315.37ms
step:1059/1700 train_loss:3.6264 train_time:330837ms step_avg:315.38ms
step:1060/1700 train_loss:3.3450 train_time:331171ms step_avg:315.40ms
step:1061/1700 train_loss:3.4012 train_time:331510ms step_avg:315.42ms
step:1062/1700 train_loss:3.4791 train_time:331839ms step_avg:315.44ms
step:1063/1700 train_loss:3.4547 train_time:332168ms step_avg:315.45ms
step:1064/1700 train_loss:3.4204 train_time:332497ms step_avg:315.46ms
step:1065/1700 train_loss:3.5068 train_time:332826ms step_avg:315.47ms
step:1066/1700 train_loss:3.4247 train_time:333151ms step_avg:315.48ms
step:1067/1700 train_loss:3.4010 train_time:333481ms step_avg:315.50ms
step:1068/1700 train_loss:3.4600 train_time:333815ms step_avg:315.52ms
step:1069/1700 train_loss:3.3235 train_time:334151ms step_avg:315.53ms
step:1070/1700 train_loss:3.4772 train_time:334475ms step_avg:315.54ms
step:1071/1700 train_loss:3.3441 train_time:334813ms step_avg:315.56ms
step:1072/1700 train_loss:3.6068 train_time:335140ms step_avg:315.57ms
step:1073/1700 train_loss:3.5535 train_time:335479ms step_avg:315.60ms
step:1074/1700 train_loss:3.4829 train_time:335806ms step_avg:315.61ms
step:1075/1700 train_loss:3.5676 train_time:336130ms step_avg:315.61ms
step:1076/1700 train_loss:3.4766 train_time:336462ms step_avg:315.63ms
step:1077/1700 train_loss:3.4408 train_time:336788ms step_avg:315.64ms
step:1078/1700 train_loss:3.8372 train_time:337115ms step_avg:315.65ms
step:1079/1700 train_loss:3.4758 train_time:337446ms step_avg:315.67ms
step:1080/1700 train_loss:3.1179 train_time:337791ms step_avg:315.69ms
step:1081/1700 train_loss:3.5771 train_time:338118ms step_avg:315.70ms
step:1082/1700 train_loss:3.4765 train_time:338452ms step_avg:315.72ms
step:1083/1700 train_loss:3.5536 train_time:338791ms step_avg:315.74ms
step:1084/1700 train_loss:3.6359 train_time:339121ms step_avg:315.76ms
step:1085/1700 train_loss:3.5515 train_time:339448ms step_avg:315.77ms
step:1086/1700 train_loss:3.5140 train_time:339777ms step_avg:315.78ms
step:1087/1700 train_loss:3.4743 train_time:340105ms step_avg:315.79ms
step:1088/1700 train_loss:3.6825 train_time:340443ms step_avg:315.81ms
step:1089/1700 train_loss:3.5554 train_time:340778ms step_avg:315.83ms
step:1090/1700 train_loss:3.4100 train_time:341105ms step_avg:315.84ms
step:1091/1700 train_loss:3.4268 train_time:341438ms step_avg:315.85ms
step:1092/1700 train_loss:3.5292 train_time:341769ms step_avg:315.87ms
step:1093/1700 train_loss:3.3322 train_time:342097ms step_avg:315.88ms
step:1094/1700 train_loss:3.5392 train_time:342424ms step_avg:315.89ms
step:1095/1700 train_loss:3.6521 train_time:342751ms step_avg:315.90ms
step:1096/1700 train_loss:3.4955 train_time:343085ms step_avg:315.92ms
step:1097/1700 train_loss:3.4584 train_time:343412ms step_avg:315.93ms
step:1098/1700 train_loss:3.4774 train_time:343746ms step_avg:315.94ms
step:1099/1700 train_loss:3.5328 train_time:344072ms step_avg:315.95ms
step:1100/1700 train_loss:3.6026 train_time:344407ms step_avg:315.97ms
step:1101/1700 train_loss:3.5710 train_time:344740ms step_avg:315.99ms
step:1102/1700 train_loss:3.4821 train_time:345073ms step_avg:316.00ms
step:1103/1700 train_loss:3.3378 train_time:345403ms step_avg:316.01ms
step:1104/1700 train_loss:3.3662 train_time:345736ms step_avg:316.03ms
step:1105/1700 train_loss:3.4987 train_time:346070ms step_avg:316.05ms
step:1106/1700 train_loss:3.3620 train_time:346398ms step_avg:316.06ms
step:1107/1700 train_loss:4.1101 train_time:346735ms step_avg:316.08ms
step:1108/1700 train_loss:3.2743 train_time:347066ms step_avg:316.09ms
step:1109/1700 train_loss:3.6139 train_time:347393ms step_avg:316.10ms
step:1110/1700 train_loss:3.3881 train_time:347724ms step_avg:316.11ms
step:1111/1700 train_loss:3.5492 train_time:348048ms step_avg:316.12ms
step:1112/1700 train_loss:3.4748 train_time:348376ms step_avg:316.13ms
step:1113/1700 train_loss:3.5358 train_time:348710ms step_avg:316.15ms
step:1114/1700 train_loss:3.6079 train_time:349038ms step_avg:316.16ms
step:1115/1700 train_loss:3.4830 train_time:349364ms step_avg:316.17ms
step:1116/1700 train_loss:3.4092 train_time:349694ms step_avg:316.18ms
step:1117/1700 train_loss:3.2893 train_time:350040ms step_avg:316.21ms
step:1118/1700 train_loss:3.4738 train_time:350362ms step_avg:316.21ms
step:1119/1700 train_loss:3.6439 train_time:350703ms step_avg:316.23ms
step:1120/1700 train_loss:3.6727 train_time:351028ms step_avg:316.24ms
step:1121/1700 train_loss:3.5304 train_time:351357ms step_avg:316.25ms
step:1122/1700 train_loss:3.5385 train_time:351686ms step_avg:316.26ms
step:1123/1700 train_loss:3.4329 train_time:352014ms step_avg:316.27ms
step:1124/1700 train_loss:3.5068 train_time:352339ms step_avg:316.28ms
step:1125/1700 train_loss:3.6362 train_time:352674ms step_avg:316.30ms
step:1125/1700 val_loss:3.4605 train_time:352684ms step_avg:316.31ms
step:1126/1700 train_loss:3.3988 train_time:353010ms step_avg:316.32ms
step:1127/1700 train_loss:3.2649 train_time:353347ms step_avg:316.34ms
step:1128/1700 train_loss:3.5270 train_time:353683ms step_avg:316.35ms
step:1129/1700 train_loss:3.7293 train_time:354012ms step_avg:316.36ms
step:1130/1700 train_loss:3.2849 train_time:354345ms step_avg:316.38ms
step:1131/1700 train_loss:3.6104 train_time:354674ms step_avg:316.39ms
step:1132/1700 train_loss:3.4327 train_time:355004ms step_avg:316.40ms
step:1133/1700 train_loss:3.4456 train_time:355333ms step_avg:316.41ms
step:1134/1700 train_loss:3.4109 train_time:355660ms step_avg:316.42ms
step:1135/1700 train_loss:3.5380 train_time:355999ms step_avg:316.44ms
step:1136/1700 train_loss:3.5001 train_time:356328ms step_avg:316.45ms
step:1137/1700 train_loss:3.5742 train_time:356656ms step_avg:316.47ms
step:1138/1700 train_loss:3.6043 train_time:356991ms step_avg:316.48ms
step:1139/1700 train_loss:3.5136 train_time:357325ms step_avg:316.50ms
step:1140/1700 train_loss:3.4006 train_time:357814ms step_avg:316.65ms
step:1141/1700 train_loss:3.7049 train_time:358145ms step_avg:316.66ms
step:1142/1700 train_loss:3.5147 train_time:358470ms step_avg:316.67ms
step:1143/1700 train_loss:3.5715 train_time:358961ms step_avg:316.82ms
step:1144/1700 train_loss:3.6175 train_time:359285ms step_avg:316.83ms
step:1145/1700 train_loss:3.2133 train_time:359620ms step_avg:316.85ms
step:1146/1700 train_loss:3.5435 train_time:359950ms step_avg:316.86ms
step:1147/1700 train_loss:3.3974 train_time:360278ms step_avg:316.87ms
step:1148/1700 train_loss:3.4498 train_time:360606ms step_avg:316.88ms
step:1149/1700 train_loss:3.5079 train_time:360937ms step_avg:316.89ms
step:1150/1700 train_loss:3.5903 train_time:361265ms step_avg:316.90ms
step:1151/1700 train_loss:3.5504 train_time:361595ms step_avg:316.91ms
step:1152/1700 train_loss:3.4411 train_time:361933ms step_avg:316.93ms
step:1153/1700 train_loss:3.4091 train_time:362271ms step_avg:316.95ms
step:1154/1700 train_loss:3.6319 train_time:362603ms step_avg:316.96ms
step:1155/1700 train_loss:3.6322 train_time:362939ms step_avg:316.98ms
step:1156/1700 train_loss:3.3443 train_time:363272ms step_avg:316.99ms
step:1157/1700 train_loss:3.3433 train_time:363601ms step_avg:317.00ms
step:1158/1700 train_loss:3.4910 train_time:363942ms step_avg:317.02ms
step:1159/1700 train_loss:3.5103 train_time:364276ms step_avg:317.04ms
step:1160/1700 train_loss:3.2754 train_time:364608ms step_avg:317.05ms
step:1161/1700 train_loss:3.3470 train_time:364938ms step_avg:317.06ms
step:1162/1700 train_loss:3.3366 train_time:365270ms step_avg:317.07ms
step:1163/1700 train_loss:3.5580 train_time:365601ms step_avg:317.09ms
step:1164/1700 train_loss:3.3676 train_time:365936ms step_avg:317.10ms
step:1165/1700 train_loss:3.4800 train_time:366268ms step_avg:317.12ms
step:1166/1700 train_loss:3.4494 train_time:366604ms step_avg:317.13ms
step:1167/1700 train_loss:3.4539 train_time:366935ms step_avg:317.14ms
step:1168/1700 train_loss:3.4374 train_time:367263ms step_avg:317.15ms
step:1169/1700 train_loss:3.4369 train_time:367594ms step_avg:317.16ms
step:1170/1700 train_loss:3.6433 train_time:367927ms step_avg:317.18ms
step:1171/1700 train_loss:3.4149 train_time:368259ms step_avg:317.19ms
step:1172/1700 train_loss:3.5000 train_time:368588ms step_avg:317.20ms
step:1173/1700 train_loss:3.4643 train_time:368922ms step_avg:317.22ms
step:1174/1700 train_loss:3.3710 train_time:369253ms step_avg:317.23ms
step:1175/1700 train_loss:3.4666 train_time:369583ms step_avg:317.24ms
step:1176/1700 train_loss:3.8047 train_time:369943ms step_avg:317.28ms
step:1177/1700 train_loss:3.4311 train_time:370279ms step_avg:317.29ms
step:1178/1700 train_loss:3.4442 train_time:370613ms step_avg:317.31ms
step:1179/1700 train_loss:3.3193 train_time:370958ms step_avg:317.33ms
step:1180/1700 train_loss:3.4676 train_time:371292ms step_avg:317.34ms
step:1181/1700 train_loss:3.4816 train_time:371624ms step_avg:317.36ms
step:1182/1700 train_loss:3.4151 train_time:371956ms step_avg:317.37ms
step:1183/1700 train_loss:3.3328 train_time:372293ms step_avg:317.39ms
step:1184/1700 train_loss:3.4598 train_time:372626ms step_avg:317.40ms
step:1185/1700 train_loss:3.5727 train_time:372959ms step_avg:317.41ms
step:1186/1700 train_loss:3.7457 train_time:373292ms step_avg:317.43ms
step:1187/1700 train_loss:3.5897 train_time:373629ms step_avg:317.44ms
step:1188/1700 train_loss:3.4270 train_time:373963ms step_avg:317.46ms
step:1189/1700 train_loss:3.2845 train_time:374311ms step_avg:317.48ms
step:1190/1700 train_loss:3.4174 train_time:374646ms step_avg:317.50ms
step:1191/1700 train_loss:3.4174 train_time:374975ms step_avg:317.51ms
step:1192/1700 train_loss:3.4612 train_time:375309ms step_avg:317.52ms
step:1193/1700 train_loss:3.3783 train_time:375644ms step_avg:317.54ms
step:1194/1700 train_loss:3.5378 train_time:375978ms step_avg:317.55ms
step:1195/1700 train_loss:3.4017 train_time:376303ms step_avg:317.56ms
step:1196/1700 train_loss:3.3900 train_time:376642ms step_avg:317.57ms
step:1197/1700 train_loss:3.3825 train_time:376980ms step_avg:317.59ms
step:1198/1700 train_loss:3.4631 train_time:377313ms step_avg:317.60ms
step:1199/1700 train_loss:3.4349 train_time:377644ms step_avg:317.61ms
step:1200/1700 train_loss:3.3976 train_time:377983ms step_avg:317.63ms
step:1201/1700 train_loss:3.8149 train_time:378335ms step_avg:317.66ms
step:1202/1700 train_loss:3.3498 train_time:378667ms step_avg:317.67ms
step:1203/1700 train_loss:3.4433 train_time:378993ms step_avg:317.68ms
step:1204/1700 train_loss:3.4363 train_time:379330ms step_avg:317.70ms
step:1205/1700 train_loss:3.5031 train_time:379681ms step_avg:317.72ms
step:1206/1700 train_loss:3.5068 train_time:380014ms step_avg:317.74ms
step:1207/1700 train_loss:3.4661 train_time:380354ms step_avg:317.76ms
step:1208/1700 train_loss:3.3863 train_time:380687ms step_avg:317.77ms
step:1209/1700 train_loss:3.4531 train_time:381015ms step_avg:317.78ms
step:1210/1700 train_loss:3.3922 train_time:381349ms step_avg:317.79ms
step:1211/1700 train_loss:3.4615 train_time:381683ms step_avg:317.80ms
step:1212/1700 train_loss:3.6900 train_time:382028ms step_avg:317.83ms
step:1213/1700 train_loss:3.3694 train_time:382357ms step_avg:317.84ms
step:1214/1700 train_loss:3.6252 train_time:382688ms step_avg:317.85ms
step:1215/1700 train_loss:3.4216 train_time:383019ms step_avg:317.86ms
step:1216/1700 train_loss:3.5096 train_time:383353ms step_avg:317.87ms
step:1217/1700 train_loss:3.4659 train_time:383690ms step_avg:317.89ms
step:1218/1700 train_loss:3.4675 train_time:384015ms step_avg:317.89ms
step:1219/1700 train_loss:3.5012 train_time:384347ms step_avg:317.90ms
step:1220/1700 train_loss:3.4265 train_time:384677ms step_avg:317.92ms
step:1221/1700 train_loss:3.4732 train_time:385006ms step_avg:317.92ms
step:1222/1700 train_loss:3.4886 train_time:385341ms step_avg:317.94ms
step:1223/1700 train_loss:3.4946 train_time:385670ms step_avg:317.95ms
step:1224/1700 train_loss:3.4491 train_time:386012ms step_avg:317.97ms
step:1225/1700 train_loss:3.4396 train_time:386344ms step_avg:317.98ms
step:1226/1700 train_loss:3.3351 train_time:386679ms step_avg:317.99ms
step:1227/1700 train_loss:3.4870 train_time:387015ms step_avg:318.01ms
step:1228/1700 train_loss:3.2880 train_time:387340ms step_avg:318.01ms
step:1229/1700 train_loss:3.5794 train_time:387674ms step_avg:318.03ms
step:1230/1700 train_loss:3.3967 train_time:388007ms step_avg:318.04ms
step:1231/1700 train_loss:3.4027 train_time:388350ms step_avg:318.06ms
step:1232/1700 train_loss:3.5700 train_time:388691ms step_avg:318.08ms
step:1233/1700 train_loss:3.2900 train_time:389029ms step_avg:318.09ms
step:1234/1700 train_loss:3.3684 train_time:389361ms step_avg:318.11ms
step:1235/1700 train_loss:3.4182 train_time:389692ms step_avg:318.12ms
step:1236/1700 train_loss:3.4335 train_time:390024ms step_avg:318.13ms
step:1237/1700 train_loss:3.4263 train_time:390359ms step_avg:318.14ms
step:1238/1700 train_loss:3.3921 train_time:390689ms step_avg:318.15ms
step:1239/1700 train_loss:3.6200 train_time:391020ms step_avg:318.16ms
step:1240/1700 train_loss:3.3781 train_time:391366ms step_avg:318.18ms
step:1241/1700 train_loss:3.2584 train_time:391694ms step_avg:318.19ms
step:1242/1700 train_loss:3.5291 train_time:392036ms step_avg:318.21ms
step:1243/1700 train_loss:3.2508 train_time:392372ms step_avg:318.23ms
step:1244/1700 train_loss:3.4553 train_time:392704ms step_avg:318.24ms
step:1245/1700 train_loss:3.6643 train_time:393036ms step_avg:318.25ms
step:1246/1700 train_loss:3.3528 train_time:393369ms step_avg:318.26ms
step:1247/1700 train_loss:3.4438 train_time:393695ms step_avg:318.27ms
step:1248/1700 train_loss:3.5281 train_time:394023ms step_avg:318.27ms
step:1249/1700 train_loss:3.2452 train_time:394356ms step_avg:318.29ms
step:1250/1700 train_loss:3.3586 train_time:394690ms step_avg:318.30ms
step:1250/1700 val_loss:3.4074 train_time:394699ms step_avg:318.31ms
step:1251/1700 train_loss:3.3596 train_time:395023ms step_avg:318.31ms
step:1252/1700 train_loss:3.2399 train_time:395355ms step_avg:318.32ms
step:1253/1700 train_loss:3.4944 train_time:395688ms step_avg:318.33ms
step:1254/1700 train_loss:3.5974 train_time:396034ms step_avg:318.36ms
step:1255/1700 train_loss:3.3298 train_time:396361ms step_avg:318.36ms
step:1256/1700 train_loss:3.5418 train_time:396689ms step_avg:318.37ms
step:1257/1700 train_loss:3.2873 train_time:397026ms step_avg:318.39ms
step:1258/1700 train_loss:3.4627 train_time:397357ms step_avg:318.40ms
step:1259/1700 train_loss:3.5459 train_time:397692ms step_avg:318.41ms
step:1260/1700 train_loss:3.3952 train_time:398020ms step_avg:318.42ms
step:1261/1700 train_loss:3.4503 train_time:398360ms step_avg:318.43ms
step:1262/1700 train_loss:3.5745 train_time:398693ms step_avg:318.44ms
step:1263/1700 train_loss:3.2342 train_time:399022ms step_avg:318.45ms
step:1264/1700 train_loss:3.4913 train_time:399358ms step_avg:318.47ms
step:1265/1700 train_loss:3.4212 train_time:399694ms step_avg:318.48ms
step:1266/1700 train_loss:3.4003 train_time:400025ms step_avg:318.49ms
step:1267/1700 train_loss:3.3294 train_time:400358ms step_avg:318.50ms
step:1268/1700 train_loss:3.3204 train_time:400701ms step_avg:318.52ms
step:1269/1700 train_loss:3.4381 train_time:401034ms step_avg:318.53ms
step:1270/1700 train_loss:3.3344 train_time:401370ms step_avg:318.55ms
step:1271/1700 train_loss:3.4519 train_time:401700ms step_avg:318.56ms
step:1272/1700 train_loss:3.3983 train_time:402040ms step_avg:318.57ms
step:1273/1700 train_loss:3.4421 train_time:402373ms step_avg:318.59ms
step:1274/1700 train_loss:3.3374 train_time:402707ms step_avg:318.60ms
step:1275/1700 train_loss:3.4708 train_time:403033ms step_avg:318.60ms
step:1276/1700 train_loss:3.4066 train_time:403362ms step_avg:318.61ms
step:1277/1700 train_loss:3.5028 train_time:403693ms step_avg:318.62ms
step:1278/1700 train_loss:3.4368 train_time:404025ms step_avg:318.63ms
step:1279/1700 train_loss:3.3679 train_time:404376ms step_avg:318.66ms
step:1280/1700 train_loss:3.3118 train_time:404711ms step_avg:318.67ms
step:1281/1700 train_loss:3.3951 train_time:405041ms step_avg:318.68ms
step:1282/1700 train_loss:3.3669 train_time:405379ms step_avg:318.69ms
step:1283/1700 train_loss:3.5401 train_time:405712ms step_avg:318.71ms
step:1284/1700 train_loss:3.3579 train_time:406041ms step_avg:318.71ms
step:1285/1700 train_loss:3.5113 train_time:406375ms step_avg:318.73ms
step:1286/1700 train_loss:3.3816 train_time:406719ms step_avg:318.75ms
step:1287/1700 train_loss:3.4579 train_time:407048ms step_avg:318.75ms
step:1288/1700 train_loss:3.4244 train_time:407379ms step_avg:318.76ms
step:1289/1700 train_loss:3.4269 train_time:407717ms step_avg:318.78ms
step:1290/1700 train_loss:3.4434 train_time:408058ms step_avg:318.80ms
step:1291/1700 train_loss:3.5789 train_time:408403ms step_avg:318.82ms
step:1292/1700 train_loss:3.3936 train_time:408746ms step_avg:318.83ms
step:1293/1700 train_loss:3.2018 train_time:409091ms step_avg:318.85ms
step:1294/1700 train_loss:3.4183 train_time:409422ms step_avg:318.86ms
step:1295/1700 train_loss:3.3980 train_time:409773ms step_avg:318.89ms
step:1296/1700 train_loss:3.4492 train_time:410108ms step_avg:318.90ms
step:1297/1700 train_loss:3.4862 train_time:410445ms step_avg:318.92ms
step:1298/1700 train_loss:3.4877 train_time:410775ms step_avg:318.92ms
step:1299/1700 train_loss:3.4512 train_time:411111ms step_avg:318.94ms
step:1300/1700 train_loss:3.4322 train_time:411438ms step_avg:318.94ms
step:1301/1700 train_loss:3.5627 train_time:411772ms step_avg:318.96ms
step:1302/1700 train_loss:3.4041 train_time:412107ms step_avg:318.97ms
step:1303/1700 train_loss:3.4998 train_time:412438ms step_avg:318.98ms
step:1304/1700 train_loss:3.4279 train_time:412770ms step_avg:318.99ms
step:1305/1700 train_loss:3.5067 train_time:413109ms step_avg:319.00ms
step:1306/1700 train_loss:3.5989 train_time:413457ms step_avg:319.03ms
step:1307/1700 train_loss:3.3855 train_time:413799ms step_avg:319.04ms
step:1308/1700 train_loss:3.3390 train_time:414130ms step_avg:319.05ms
step:1309/1700 train_loss:3.3574 train_time:414466ms step_avg:319.07ms
step:1310/1700 train_loss:3.4110 train_time:414797ms step_avg:319.07ms
step:1311/1700 train_loss:3.3394 train_time:415131ms step_avg:319.09ms
step:1312/1700 train_loss:3.5143 train_time:415463ms step_avg:319.10ms
step:1313/1700 train_loss:3.3995 train_time:415797ms step_avg:319.11ms
step:1314/1700 train_loss:3.4608 train_time:416131ms step_avg:319.12ms
step:1315/1700 train_loss:3.4381 train_time:416465ms step_avg:319.13ms
step:1316/1700 train_loss:3.4630 train_time:416795ms step_avg:319.14ms
step:1317/1700 train_loss:3.3087 train_time:417141ms step_avg:319.16ms
step:1318/1700 train_loss:3.5739 train_time:417473ms step_avg:319.17ms
step:1319/1700 train_loss:3.4863 train_time:417807ms step_avg:319.18ms
step:1320/1700 train_loss:3.3684 train_time:418135ms step_avg:319.19ms
step:1321/1700 train_loss:3.4831 train_time:418480ms step_avg:319.21ms
step:1322/1700 train_loss:3.4576 train_time:418814ms step_avg:319.22ms
step:1323/1700 train_loss:3.4359 train_time:419151ms step_avg:319.23ms
step:1324/1700 train_loss:3.6180 train_time:419494ms step_avg:319.25ms
step:1325/1700 train_loss:3.4826 train_time:419838ms step_avg:319.27ms
step:1326/1700 train_loss:3.5282 train_time:420170ms step_avg:319.28ms
step:1327/1700 train_loss:3.5419 train_time:420500ms step_avg:319.29ms
step:1328/1700 train_loss:3.2919 train_time:420838ms step_avg:319.30ms
step:1329/1700 train_loss:3.3680 train_time:421173ms step_avg:319.31ms
step:1330/1700 train_loss:3.4311 train_time:421679ms step_avg:319.45ms
step:1331/1700 train_loss:2.6193 train_time:422030ms step_avg:319.48ms
step:1332/1700 train_loss:3.4286 train_time:422368ms step_avg:319.49ms
step:1333/1700 train_loss:3.4063 train_time:422783ms step_avg:319.56ms
step:1334/1700 train_loss:3.3834 train_time:423112ms step_avg:319.57ms
step:1335/1700 train_loss:3.7951 train_time:423461ms step_avg:319.59ms
step:1336/1700 train_loss:3.5239 train_time:423789ms step_avg:319.60ms
step:1337/1700 train_loss:3.4172 train_time:424120ms step_avg:319.61ms
step:1338/1700 train_loss:3.3519 train_time:424461ms step_avg:319.62ms
step:1339/1700 train_loss:3.3428 train_time:424799ms step_avg:319.64ms
step:1340/1700 train_loss:3.5999 train_time:425136ms step_avg:319.65ms
step:1341/1700 train_loss:3.5719 train_time:425472ms step_avg:319.66ms
step:1342/1700 train_loss:3.3865 train_time:425808ms step_avg:319.68ms
step:1343/1700 train_loss:3.3288 train_time:426135ms step_avg:319.68ms
step:1344/1700 train_loss:3.6478 train_time:426472ms step_avg:319.69ms
step:1345/1700 train_loss:3.4049 train_time:426806ms step_avg:319.71ms
step:1346/1700 train_loss:3.4175 train_time:427139ms step_avg:319.72ms
step:1347/1700 train_loss:3.4665 train_time:427471ms step_avg:319.72ms
step:1348/1700 train_loss:3.4366 train_time:427805ms step_avg:319.73ms
step:1349/1700 train_loss:3.3457 train_time:428133ms step_avg:319.74ms
step:1350/1700 train_loss:3.3183 train_time:428465ms step_avg:319.75ms
step:1351/1700 train_loss:3.3938 train_time:428805ms step_avg:319.77ms
step:1352/1700 train_loss:3.3267 train_time:429142ms step_avg:319.78ms
step:1353/1700 train_loss:3.4392 train_time:429473ms step_avg:319.79ms
step:1354/1700 train_loss:3.2903 train_time:429800ms step_avg:319.79ms
step:1355/1700 train_loss:3.3516 train_time:430131ms step_avg:319.80ms
step:1356/1700 train_loss:3.4587 train_time:430468ms step_avg:319.81ms
step:1357/1700 train_loss:3.3067 train_time:430805ms step_avg:319.83ms
step:1358/1700 train_loss:3.2397 train_time:431135ms step_avg:319.83ms
step:1359/1700 train_loss:3.5672 train_time:431475ms step_avg:319.85ms
step:1360/1700 train_loss:3.4765 train_time:431809ms step_avg:319.86ms
step:1361/1700 train_loss:3.2300 train_time:432143ms step_avg:319.87ms
step:1362/1700 train_loss:3.4929 train_time:432487ms step_avg:319.89ms
step:1363/1700 train_loss:3.4017 train_time:432822ms step_avg:319.90ms
step:1364/1700 train_loss:3.1940 train_time:433165ms step_avg:319.91ms
step:1365/1700 train_loss:3.4404 train_time:433498ms step_avg:319.93ms
step:1366/1700 train_loss:3.3236 train_time:433838ms step_avg:319.94ms
step:1367/1700 train_loss:3.3566 train_time:434169ms step_avg:319.95ms
step:1368/1700 train_loss:3.3635 train_time:434511ms step_avg:319.96ms
step:1369/1700 train_loss:3.4729 train_time:434837ms step_avg:319.97ms
step:1370/1700 train_loss:3.4457 train_time:435170ms step_avg:319.98ms
step:1371/1700 train_loss:3.4017 train_time:435508ms step_avg:319.99ms
step:1372/1700 train_loss:3.3182 train_time:435849ms step_avg:320.01ms
step:1373/1700 train_loss:3.6564 train_time:436183ms step_avg:320.02ms
step:1374/1700 train_loss:3.3665 train_time:436512ms step_avg:320.02ms
step:1375/1700 train_loss:3.4138 train_time:436848ms step_avg:320.03ms
step:1375/1700 val_loss:3.3614 train_time:436856ms step_avg:320.04ms
step:1376/1700 train_loss:3.4146 train_time:437180ms step_avg:320.04ms
step:1377/1700 train_loss:3.2088 train_time:437520ms step_avg:320.06ms
step:1378/1700 train_loss:3.5858 train_time:437852ms step_avg:320.07ms
step:1379/1700 train_loss:3.3932 train_time:438183ms step_avg:320.08ms
step:1380/1700 train_loss:3.5254 train_time:438524ms step_avg:320.09ms
step:1381/1700 train_loss:3.5240 train_time:438855ms step_avg:320.10ms
step:1382/1700 train_loss:3.1695 train_time:439194ms step_avg:320.11ms
step:1383/1700 train_loss:3.3613 train_time:439540ms step_avg:320.13ms
step:1384/1700 train_loss:3.7615 train_time:439881ms step_avg:320.15ms
step:1385/1700 train_loss:3.2623 train_time:440214ms step_avg:320.16ms
step:1386/1700 train_loss:3.4439 train_time:440550ms step_avg:320.17ms
step:1387/1700 train_loss:3.5287 train_time:440887ms step_avg:320.18ms
step:1388/1700 train_loss:3.4530 train_time:441215ms step_avg:320.18ms
step:1389/1700 train_loss:3.3913 train_time:441549ms step_avg:320.20ms
step:1390/1700 train_loss:3.2481 train_time:441887ms step_avg:320.21ms
step:1391/1700 train_loss:3.3941 train_time:442223ms step_avg:320.22ms
step:1392/1700 train_loss:3.3703 train_time:442558ms step_avg:320.23ms
step:1393/1700 train_loss:3.6190 train_time:442891ms step_avg:320.24ms
step:1394/1700 train_loss:3.3438 train_time:443224ms step_avg:320.25ms
step:1395/1700 train_loss:3.3391 train_time:443559ms step_avg:320.26ms
step:1396/1700 train_loss:3.2954 train_time:443892ms step_avg:320.27ms
step:1397/1700 train_loss:3.5587 train_time:444228ms step_avg:320.28ms
step:1398/1700 train_loss:3.4428 train_time:444560ms step_avg:320.29ms
step:1399/1700 train_loss:3.4590 train_time:444895ms step_avg:320.30ms
step:1400/1700 train_loss:3.3552 train_time:445225ms step_avg:320.31ms
step:1401/1700 train_loss:3.3010 train_time:445559ms step_avg:320.32ms
step:1402/1700 train_loss:3.3755 train_time:445894ms step_avg:320.33ms
step:1403/1700 train_loss:3.3681 train_time:446236ms step_avg:320.34ms
step:1404/1700 train_loss:3.3940 train_time:446566ms step_avg:320.35ms
step:1405/1700 train_loss:3.3477 train_time:446902ms step_avg:320.36ms
step:1406/1700 train_loss:3.5445 train_time:447243ms step_avg:320.37ms
step:1407/1700 train_loss:3.3216 train_time:447569ms step_avg:320.38ms
step:1408/1700 train_loss:3.3594 train_time:447912ms step_avg:320.39ms
step:1409/1700 train_loss:3.3582 train_time:448243ms step_avg:320.40ms
step:1410/1700 train_loss:3.2184 train_time:448574ms step_avg:320.41ms
step:1411/1700 train_loss:3.3558 train_time:448901ms step_avg:320.41ms
step:1412/1700 train_loss:3.3457 train_time:449252ms step_avg:320.44ms
step:1413/1700 train_loss:3.3347 train_time:449587ms step_avg:320.45ms
step:1414/1700 train_loss:3.4117 train_time:449921ms step_avg:320.46ms
step:1415/1700 train_loss:3.3758 train_time:450253ms step_avg:320.46ms
step:1416/1700 train_loss:3.4049 train_time:450590ms step_avg:320.48ms
step:1417/1700 train_loss:3.3830 train_time:450923ms step_avg:320.49ms
step:1418/1700 train_loss:3.4552 train_time:451259ms step_avg:320.50ms
step:1419/1700 train_loss:3.2734 train_time:451613ms step_avg:320.52ms
step:1420/1700 train_loss:3.3340 train_time:451956ms step_avg:320.54ms
step:1421/1700 train_loss:3.4369 train_time:452287ms step_avg:320.54ms
step:1422/1700 train_loss:3.3806 train_time:452625ms step_avg:320.56ms
step:1423/1700 train_loss:3.4041 train_time:452964ms step_avg:320.57ms
step:1424/1700 train_loss:3.4246 train_time:453301ms step_avg:320.58ms
step:1425/1700 train_loss:3.3847 train_time:453637ms step_avg:320.59ms
step:1426/1700 train_loss:3.3684 train_time:453969ms step_avg:320.60ms
step:1427/1700 train_loss:3.3771 train_time:454305ms step_avg:320.61ms
step:1428/1700 train_loss:3.2282 train_time:454658ms step_avg:320.63ms
step:1429/1700 train_loss:3.3750 train_time:454987ms step_avg:320.64ms
step:1430/1700 train_loss:3.3220 train_time:455323ms step_avg:320.65ms
step:1431/1700 train_loss:3.4241 train_time:455657ms step_avg:320.66ms
step:1432/1700 train_loss:3.4029 train_time:455987ms step_avg:320.67ms
step:1433/1700 train_loss:3.3077 train_time:456321ms step_avg:320.68ms
step:1434/1700 train_loss:3.3639 train_time:456660ms step_avg:320.69ms
step:1435/1700 train_loss:3.3798 train_time:456997ms step_avg:320.70ms
step:1436/1700 train_loss:3.1774 train_time:457348ms step_avg:320.72ms
step:1437/1700 train_loss:3.3360 train_time:457691ms step_avg:320.74ms
step:1438/1700 train_loss:3.1689 train_time:458022ms step_avg:320.74ms
step:1439/1700 train_loss:3.2742 train_time:458354ms step_avg:320.75ms
step:1440/1700 train_loss:3.4532 train_time:458697ms step_avg:320.77ms
step:1441/1700 train_loss:3.4240 train_time:459032ms step_avg:320.78ms
step:1442/1700 train_loss:3.3598 train_time:459370ms step_avg:320.79ms
step:1443/1700 train_loss:3.2348 train_time:459703ms step_avg:320.80ms
step:1444/1700 train_loss:3.3891 train_time:460039ms step_avg:320.81ms
step:1445/1700 train_loss:3.4312 train_time:460387ms step_avg:320.83ms
step:1446/1700 train_loss:3.5259 train_time:460732ms step_avg:320.84ms
step:1447/1700 train_loss:3.4905 train_time:461062ms step_avg:320.85ms
step:1448/1700 train_loss:3.3785 train_time:461398ms step_avg:320.86ms
step:1449/1700 train_loss:3.2505 train_time:461740ms step_avg:320.88ms
step:1450/1700 train_loss:3.3425 train_time:462075ms step_avg:320.89ms
step:1451/1700 train_loss:3.3432 train_time:462411ms step_avg:320.90ms
step:1452/1700 train_loss:3.4420 train_time:462742ms step_avg:320.90ms
step:1453/1700 train_loss:3.4376 train_time:463074ms step_avg:320.91ms
step:1454/1700 train_loss:3.2525 train_time:463411ms step_avg:320.92ms
step:1455/1700 train_loss:3.3736 train_time:463748ms step_avg:320.93ms
step:1456/1700 train_loss:3.3013 train_time:464085ms step_avg:320.94ms
step:1457/1700 train_loss:3.3284 train_time:464418ms step_avg:320.95ms
step:1458/1700 train_loss:3.3720 train_time:464764ms step_avg:320.97ms
step:1459/1700 train_loss:3.3225 train_time:465096ms step_avg:320.98ms
step:1460/1700 train_loss:3.2014 train_time:465432ms step_avg:320.99ms
step:1461/1700 train_loss:3.4655 train_time:465769ms step_avg:321.00ms
step:1462/1700 train_loss:3.3120 train_time:466101ms step_avg:321.01ms
step:1463/1700 train_loss:3.3609 train_time:466440ms step_avg:321.02ms
step:1464/1700 train_loss:3.4800 train_time:466772ms step_avg:321.03ms
step:1465/1700 train_loss:3.3094 train_time:467106ms step_avg:321.03ms
step:1466/1700 train_loss:3.5078 train_time:467443ms step_avg:321.05ms
step:1467/1700 train_loss:3.4051 train_time:467774ms step_avg:321.05ms
step:1468/1700 train_loss:3.3976 train_time:468105ms step_avg:321.06ms
step:1469/1700 train_loss:3.3288 train_time:468446ms step_avg:321.07ms
step:1470/1700 train_loss:3.4346 train_time:468780ms step_avg:321.08ms
step:1471/1700 train_loss:3.3290 train_time:469117ms step_avg:321.09ms
step:1472/1700 train_loss:3.3129 train_time:469456ms step_avg:321.11ms
step:1473/1700 train_loss:3.3803 train_time:469803ms step_avg:321.12ms
step:1474/1700 train_loss:3.2923 train_time:470140ms step_avg:321.13ms
step:1475/1700 train_loss:3.2783 train_time:470484ms step_avg:321.15ms
step:1476/1700 train_loss:3.4797 train_time:470817ms step_avg:321.16ms
step:1477/1700 train_loss:3.3566 train_time:471150ms step_avg:321.17ms
step:1478/1700 train_loss:3.1891 train_time:471483ms step_avg:321.17ms
step:1479/1700 train_loss:3.3038 train_time:471826ms step_avg:321.19ms
step:1480/1700 train_loss:3.2793 train_time:472172ms step_avg:321.21ms
step:1481/1700 train_loss:3.3526 train_time:472519ms step_avg:321.22ms
step:1482/1700 train_loss:3.4377 train_time:472853ms step_avg:321.23ms
step:1483/1700 train_loss:3.3162 train_time:473184ms step_avg:321.24ms
step:1484/1700 train_loss:3.4931 train_time:473517ms step_avg:321.25ms
step:1485/1700 train_loss:3.4112 train_time:473853ms step_avg:321.26ms
step:1486/1700 train_loss:3.3153 train_time:474203ms step_avg:321.28ms
step:1487/1700 train_loss:3.3051 train_time:474536ms step_avg:321.28ms
step:1488/1700 train_loss:3.3180 train_time:474869ms step_avg:321.29ms
step:1489/1700 train_loss:3.2621 train_time:475214ms step_avg:321.31ms
step:1490/1700 train_loss:3.3746 train_time:475556ms step_avg:321.32ms
step:1491/1700 train_loss:3.2763 train_time:475901ms step_avg:321.34ms
step:1492/1700 train_loss:3.3615 train_time:476236ms step_avg:321.35ms
step:1493/1700 train_loss:3.2909 train_time:476578ms step_avg:321.36ms
step:1494/1700 train_loss:3.2071 train_time:476909ms step_avg:321.37ms
step:1495/1700 train_loss:3.2998 train_time:477249ms step_avg:321.38ms
step:1496/1700 train_loss:3.4788 train_time:477583ms step_avg:321.39ms
step:1497/1700 train_loss:3.3356 train_time:477927ms step_avg:321.40ms
step:1498/1700 train_loss:3.0697 train_time:478266ms step_avg:321.42ms
step:1499/1700 train_loss:3.3956 train_time:478602ms step_avg:321.43ms
step:1500/1700 train_loss:3.3519 train_time:478938ms step_avg:321.44ms
step:1500/1700 val_loss:3.3182 train_time:478948ms step_avg:321.44ms
step:1501/1700 train_loss:3.3811 train_time:479288ms step_avg:321.45ms
step:1502/1700 train_loss:3.3565 train_time:479642ms step_avg:321.48ms
step:1503/1700 train_loss:3.3329 train_time:479982ms step_avg:321.49ms
step:1504/1700 train_loss:3.1222 train_time:480342ms step_avg:321.51ms
step:1505/1700 train_loss:3.3986 train_time:480687ms step_avg:321.53ms
step:1506/1700 train_loss:3.2853 train_time:481042ms step_avg:321.55ms
step:1507/1700 train_loss:3.2839 train_time:481387ms step_avg:321.57ms
step:1508/1700 train_loss:3.2484 train_time:481720ms step_avg:321.58ms
step:1509/1700 train_loss:3.3184 train_time:482051ms step_avg:321.58ms
step:1510/1700 train_loss:3.2123 train_time:482395ms step_avg:321.60ms
step:1511/1700 train_loss:3.5194 train_time:482735ms step_avg:321.61ms
step:1512/1700 train_loss:3.3150 train_time:483064ms step_avg:321.61ms
step:1513/1700 train_loss:3.3139 train_time:483401ms step_avg:321.62ms
step:1514/1700 train_loss:3.4504 train_time:483730ms step_avg:321.63ms
step:1515/1700 train_loss:3.4568 train_time:484079ms step_avg:321.65ms
step:1516/1700 train_loss:3.3017 train_time:484420ms step_avg:321.66ms
step:1517/1700 train_loss:3.1256 train_time:484767ms step_avg:321.68ms
step:1518/1700 train_loss:3.2730 train_time:485103ms step_avg:321.69ms
step:1519/1700 train_loss:3.2882 train_time:485451ms step_avg:321.70ms
step:1520/1700 train_loss:3.3367 train_time:485953ms step_avg:321.82ms
step:1521/1700 train_loss:3.2492 train_time:486290ms step_avg:321.83ms
step:1522/1700 train_loss:3.5391 train_time:486625ms step_avg:321.84ms
step:1523/1700 train_loss:3.1663 train_time:486958ms step_avg:321.85ms
step:1524/1700 train_loss:3.2774 train_time:487479ms step_avg:321.98ms
step:1525/1700 train_loss:3.3389 train_time:487820ms step_avg:321.99ms
step:1526/1700 train_loss:3.3247 train_time:488164ms step_avg:322.01ms
step:1527/1700 train_loss:3.3789 train_time:488503ms step_avg:322.02ms
step:1528/1700 train_loss:3.2837 train_time:488839ms step_avg:322.03ms
step:1529/1700 train_loss:3.2500 train_time:489173ms step_avg:322.04ms
step:1530/1700 train_loss:3.3003 train_time:489505ms step_avg:322.04ms
step:1531/1700 train_loss:3.3141 train_time:489844ms step_avg:322.05ms
step:1532/1700 train_loss:3.3203 train_time:490181ms step_avg:322.06ms
step:1533/1700 train_loss:3.2749 train_time:490555ms step_avg:322.10ms
step:1534/1700 train_loss:3.4430 train_time:490893ms step_avg:322.11ms
step:1535/1700 train_loss:3.2218 train_time:491231ms step_avg:322.12ms
step:1536/1700 train_loss:3.3747 train_time:491564ms step_avg:322.13ms
step:1537/1700 train_loss:3.2870 train_time:491909ms step_avg:322.14ms
step:1538/1700 train_loss:3.1621 train_time:492256ms step_avg:322.16ms
step:1539/1700 train_loss:3.1304 train_time:492600ms step_avg:322.17ms
step:1540/1700 train_loss:3.2267 train_time:492934ms step_avg:322.18ms
step:1541/1700 train_loss:3.2397 train_time:493279ms step_avg:322.19ms
step:1542/1700 train_loss:3.1517 train_time:493618ms step_avg:322.20ms
step:1543/1700 train_loss:3.4274 train_time:493956ms step_avg:322.21ms
step:1544/1700 train_loss:3.2512 train_time:494292ms step_avg:322.22ms
step:1545/1700 train_loss:3.1312 train_time:494658ms step_avg:322.25ms
step:1546/1700 train_loss:3.3163 train_time:494998ms step_avg:322.26ms
step:1547/1700 train_loss:3.3232 train_time:495334ms step_avg:322.27ms
step:1548/1700 train_loss:3.1044 train_time:495672ms step_avg:322.28ms
step:1549/1700 train_loss:3.4655 train_time:496010ms step_avg:322.29ms
step:1550/1700 train_loss:3.4322 train_time:496357ms step_avg:322.31ms
step:1551/1700 train_loss:3.2857 train_time:496711ms step_avg:322.33ms
step:1552/1700 train_loss:3.4468 train_time:497047ms step_avg:322.34ms
step:1553/1700 train_loss:3.3526 train_time:497386ms step_avg:322.35ms
step:1554/1700 train_loss:3.2809 train_time:497728ms step_avg:322.36ms
step:1555/1700 train_loss:3.4374 train_time:498065ms step_avg:322.37ms
step:1556/1700 train_loss:3.3946 train_time:498399ms step_avg:322.38ms
step:1557/1700 train_loss:3.2779 train_time:498731ms step_avg:322.39ms
step:1558/1700 train_loss:3.2267 train_time:499068ms step_avg:322.40ms
step:1559/1700 train_loss:3.2318 train_time:499405ms step_avg:322.40ms
step:1560/1700 train_loss:3.2745 train_time:499741ms step_avg:322.41ms
step:1561/1700 train_loss:3.3035 train_time:500076ms step_avg:322.42ms
step:1562/1700 train_loss:3.3067 train_time:500423ms step_avg:322.44ms
step:1563/1700 train_loss:3.3571 train_time:500767ms step_avg:322.45ms
step:1564/1700 train_loss:3.2669 train_time:501099ms step_avg:322.46ms
step:1565/1700 train_loss:3.3777 train_time:501435ms step_avg:322.47ms
step:1566/1700 train_loss:3.2437 train_time:501774ms step_avg:322.48ms
step:1567/1700 train_loss:3.3955 train_time:502110ms step_avg:322.49ms
step:1568/1700 train_loss:3.2081 train_time:502452ms step_avg:322.50ms
step:1569/1700 train_loss:3.2318 train_time:502791ms step_avg:322.51ms
step:1570/1700 train_loss:3.1585 train_time:503121ms step_avg:322.51ms
step:1571/1700 train_loss:3.1552 train_time:503460ms step_avg:322.52ms
step:1572/1700 train_loss:3.2137 train_time:503797ms step_avg:322.53ms
step:1573/1700 train_loss:3.2858 train_time:504131ms step_avg:322.54ms
step:1574/1700 train_loss:3.0686 train_time:504475ms step_avg:322.55ms
step:1575/1700 train_loss:3.2910 train_time:504809ms step_avg:322.56ms
step:1576/1700 train_loss:3.2943 train_time:505165ms step_avg:322.58ms
step:1577/1700 train_loss:3.2722 train_time:505508ms step_avg:322.60ms
step:1578/1700 train_loss:3.3077 train_time:505861ms step_avg:322.62ms
step:1579/1700 train_loss:3.2930 train_time:506197ms step_avg:322.62ms
step:1580/1700 train_loss:3.2415 train_time:506529ms step_avg:322.63ms
step:1581/1700 train_loss:3.4674 train_time:506869ms step_avg:322.64ms
step:1582/1700 train_loss:3.3327 train_time:507209ms step_avg:322.65ms
step:1583/1700 train_loss:3.3858 train_time:507542ms step_avg:322.66ms
step:1584/1700 train_loss:3.1198 train_time:507892ms step_avg:322.68ms
step:1585/1700 train_loss:3.2605 train_time:508241ms step_avg:322.69ms
step:1586/1700 train_loss:3.1969 train_time:508576ms step_avg:322.70ms
step:1587/1700 train_loss:3.3606 train_time:508907ms step_avg:322.71ms
step:1588/1700 train_loss:3.2649 train_time:509243ms step_avg:322.71ms
step:1589/1700 train_loss:3.2301 train_time:509575ms step_avg:322.72ms
step:1590/1700 train_loss:3.2290 train_time:509923ms step_avg:322.74ms
step:1591/1700 train_loss:3.2667 train_time:510254ms step_avg:322.74ms
step:1592/1700 train_loss:3.0884 train_time:510601ms step_avg:322.76ms
step:1593/1700 train_loss:3.2233 train_time:510936ms step_avg:322.76ms
step:1594/1700 train_loss:3.5606 train_time:511276ms step_avg:322.78ms
step:1595/1700 train_loss:3.1970 train_time:511614ms step_avg:322.78ms
step:1596/1700 train_loss:3.1429 train_time:511970ms step_avg:322.81ms
step:1597/1700 train_loss:3.3319 train_time:512304ms step_avg:322.81ms
step:1598/1700 train_loss:3.2515 train_time:512659ms step_avg:322.83ms
step:1599/1700 train_loss:3.1132 train_time:513005ms step_avg:322.85ms
step:1600/1700 train_loss:3.2723 train_time:513347ms step_avg:322.86ms
step:1601/1700 train_loss:3.2603 train_time:513683ms step_avg:322.87ms
step:1602/1700 train_loss:3.3268 train_time:514017ms step_avg:322.88ms
step:1603/1700 train_loss:3.2297 train_time:514349ms step_avg:322.88ms
step:1604/1700 train_loss:3.1404 train_time:514701ms step_avg:322.90ms
step:1605/1700 train_loss:3.3096 train_time:515044ms step_avg:322.91ms
step:1606/1700 train_loss:3.0956 train_time:515398ms step_avg:322.93ms
step:1607/1700 train_loss:3.3110 train_time:515731ms step_avg:322.94ms
step:1608/1700 train_loss:3.1773 train_time:516077ms step_avg:322.95ms
step:1609/1700 train_loss:3.2694 train_time:516410ms step_avg:322.96ms
step:1610/1700 train_loss:3.2924 train_time:516765ms step_avg:322.98ms
step:1611/1700 train_loss:3.1433 train_time:517101ms step_avg:322.99ms
step:1612/1700 train_loss:3.2826 train_time:517460ms step_avg:323.01ms
step:1613/1700 train_loss:3.2681 train_time:517794ms step_avg:323.02ms
step:1614/1700 train_loss:3.1170 train_time:518171ms step_avg:323.05ms
step:1615/1700 train_loss:3.3580 train_time:518525ms step_avg:323.07ms
step:1616/1700 train_loss:3.3246 train_time:518867ms step_avg:323.08ms
step:1617/1700 train_loss:3.2905 train_time:519203ms step_avg:323.09ms
step:1618/1700 train_loss:3.1974 train_time:519551ms step_avg:323.10ms
step:1619/1700 train_loss:3.3933 train_time:519887ms step_avg:323.11ms
step:1620/1700 train_loss:3.3492 train_time:520219ms step_avg:323.12ms
step:1621/1700 train_loss:3.2735 train_time:520557ms step_avg:323.13ms
step:1622/1700 train_loss:3.5155 train_time:520893ms step_avg:323.13ms
step:1623/1700 train_loss:3.2267 train_time:521236ms step_avg:323.15ms
step:1624/1700 train_loss:3.5097 train_time:521592ms step_avg:323.17ms
step:1625/1700 train_loss:3.2607 train_time:521927ms step_avg:323.17ms
step:1625/1700 val_loss:3.2866 train_time:521936ms step_avg:323.18ms
step:1626/1700 train_loss:3.3439 train_time:522263ms step_avg:323.18ms
step:1627/1700 train_loss:3.3588 train_time:522600ms step_avg:323.19ms
step:1628/1700 train_loss:3.3842 train_time:522932ms step_avg:323.20ms
step:1629/1700 train_loss:3.2344 train_time:523271ms step_avg:323.21ms
step:1630/1700 train_loss:3.2997 train_time:523609ms step_avg:323.22ms
step:1631/1700 train_loss:3.2599 train_time:523944ms step_avg:323.22ms
step:1632/1700 train_loss:3.1517 train_time:524282ms step_avg:323.23ms
step:1633/1700 train_loss:3.2761 train_time:524617ms step_avg:323.24ms
step:1634/1700 train_loss:3.2830 train_time:524954ms step_avg:323.25ms
step:1635/1700 train_loss:3.2655 train_time:525299ms step_avg:323.26ms
step:1636/1700 train_loss:3.3544 train_time:525634ms step_avg:323.27ms
step:1637/1700 train_loss:3.6237 train_time:525977ms step_avg:323.28ms
step:1638/1700 train_loss:3.2453 train_time:526343ms step_avg:323.31ms
step:1639/1700 train_loss:3.2153 train_time:526684ms step_avg:323.32ms
step:1640/1700 train_loss:3.2441 train_time:527023ms step_avg:323.33ms
step:1641/1700 train_loss:3.3517 train_time:527351ms step_avg:323.33ms
step:1642/1700 train_loss:3.3088 train_time:527685ms step_avg:323.34ms
step:1643/1700 train_loss:3.2863 train_time:528034ms step_avg:323.35ms
step:1644/1700 train_loss:3.2040 train_time:528369ms step_avg:323.36ms
step:1645/1700 train_loss:3.1952 train_time:528710ms step_avg:323.37ms
step:1646/1700 train_loss:3.2894 train_time:529050ms step_avg:323.38ms
step:1647/1700 train_loss:3.1509 train_time:529402ms step_avg:323.40ms
step:1648/1700 train_loss:3.3117 train_time:529738ms step_avg:323.41ms
step:1649/1700 train_loss:3.3482 train_time:530074ms step_avg:323.41ms
step:1650/1700 train_loss:3.2354 train_time:530415ms step_avg:323.42ms
step:1651/1700 train_loss:3.3194 train_time:530753ms step_avg:323.43ms
step:1652/1700 train_loss:3.2858 train_time:531095ms step_avg:323.44ms
step:1653/1700 train_loss:3.2146 train_time:531429ms step_avg:323.45ms
step:1654/1700 train_loss:3.3513 train_time:531767ms step_avg:323.46ms
step:1655/1700 train_loss:3.1928 train_time:532102ms step_avg:323.47ms
step:1656/1700 train_loss:2.9691 train_time:532452ms step_avg:323.48ms
step:1657/1700 train_loss:3.2994 train_time:532787ms step_avg:323.49ms
step:1658/1700 train_loss:3.3239 train_time:533123ms step_avg:323.50ms
step:1659/1700 train_loss:3.2675 train_time:533463ms step_avg:323.51ms
step:1660/1700 train_loss:3.5202 train_time:533817ms step_avg:323.53ms
step:1661/1700 train_loss:3.3552 train_time:534156ms step_avg:323.53ms
step:1662/1700 train_loss:3.3322 train_time:534491ms step_avg:323.54ms
step:1663/1700 train_loss:3.3103 train_time:534835ms step_avg:323.55ms
step:1664/1700 train_loss:3.3338 train_time:535171ms step_avg:323.56ms
step:1665/1700 train_loss:3.1537 train_time:535509ms step_avg:323.57ms
step:1666/1700 train_loss:3.3095 train_time:535842ms step_avg:323.58ms
step:1667/1700 train_loss:3.1019 train_time:536182ms step_avg:323.59ms
step:1668/1700 train_loss:3.2666 train_time:536519ms step_avg:323.59ms
step:1669/1700 train_loss:3.2975 train_time:536856ms step_avg:323.60ms
step:1670/1700 train_loss:3.1161 train_time:537195ms step_avg:323.61ms
step:1671/1700 train_loss:3.2695 train_time:537557ms step_avg:323.63ms
step:1672/1700 train_loss:3.1591 train_time:537900ms step_avg:323.65ms
step:1673/1700 train_loss:3.4469 train_time:538239ms step_avg:323.66ms
step:1674/1700 train_loss:3.3134 train_time:538576ms step_avg:323.66ms
step:1675/1700 train_loss:3.2782 train_time:538912ms step_avg:323.67ms
step:1676/1700 train_loss:3.1662 train_time:539251ms step_avg:323.68ms
step:1677/1700 train_loss:3.2267 train_time:539593ms step_avg:323.69ms
step:1678/1700 train_loss:3.5677 train_time:539930ms step_avg:323.70ms
step:1679/1700 train_loss:3.2829 train_time:540267ms step_avg:323.71ms
step:1680/1700 train_loss:3.2691 train_time:540604ms step_avg:323.72ms
step:1681/1700 train_loss:3.4014 train_time:540943ms step_avg:323.72ms
step:1682/1700 train_loss:3.3245 train_time:541283ms step_avg:323.73ms
step:1683/1700 train_loss:3.2353 train_time:541631ms step_avg:323.75ms
step:1684/1700 train_loss:3.3329 train_time:541969ms step_avg:323.76ms
step:1685/1700 train_loss:3.1653 train_time:542304ms step_avg:323.76ms
step:1686/1700 train_loss:3.3285 train_time:542644ms step_avg:323.77ms
step:1687/1700 train_loss:3.2187 train_time:542999ms step_avg:323.79ms
step:1688/1700 train_loss:3.0406 train_time:543337ms step_avg:323.80ms
step:1689/1700 train_loss:3.3612 train_time:543685ms step_avg:323.81ms
step:1690/1700 train_loss:3.3158 train_time:544020ms step_avg:323.82ms
step:1691/1700 train_loss:3.3111 train_time:544362ms step_avg:323.83ms
step:1692/1700 train_loss:3.2111 train_time:544725ms step_avg:323.86ms
step:1693/1700 train_loss:3.2055 train_time:545062ms step_avg:323.86ms
step:1694/1700 train_loss:3.3006 train_time:545400ms step_avg:323.87ms
step:1695/1700 train_loss:3.2301 train_time:545738ms step_avg:323.88ms
step:1696/1700 train_loss:3.3162 train_time:546078ms step_avg:323.89ms
step:1697/1700 train_loss:3.2639 train_time:546420ms step_avg:323.90ms
step:1698/1700 train_loss:3.3789 train_time:546758ms step_avg:323.91ms
step:1699/1700 train_loss:3.2037 train_time:547108ms step_avg:323.92ms
step:1700/1700 train_loss:3.2614 train_time:547447ms step_avg:323.93ms
step:1700/1700 val_loss:3.2764 train_time:547456ms step_avg:323.94ms
