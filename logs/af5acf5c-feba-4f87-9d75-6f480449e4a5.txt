====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(g, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        if self.layer_id != 0:
            self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x: torch.Tensor, v1: torch.Tensor | None = None, v_weighted_skip: torch.Tensor | None = None):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # v needs to be accessed by subsequent blocks
        else:
            v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        if v_weighted_skip is not None:
            v = v + v_weighted_skip.view_as(v)
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1, v

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config, layer_id: int):
        super().__init__()
        self.layer_id = layer_id
        self.attn = CausalSelfAttention(config, layer_id)
        self.mlp = MLP(config)
        if self.layer_id != 0:
            self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, v_weighted_skip=None):
        if self.layer_id != 0:
            x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1, v = self.attn(F.rms_norm(x, (x.size(-1),)), v1, v_weighted_skip)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1, v

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config, layer_id) for layer_id in range(config.n_layer)]),
        ))

        # U-net design by @brendanh0gan
        self.encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.decoder_layers = config.n_layer - self.encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.decoder_layers))
        self.v_skip_weights = nn.Parameter(torch.zeros(self.decoder_layers))

        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target):

        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        v_skip_connections = []

        # Encoder pass - process only the first half of the blocks
        for i in range(self.encoder_layers):
            x, v1, v = self.transformer.h[i](x, v1, x0)
            skip_connections.append(x)  # Store the output for skip connections
            v_skip_connections.append(v)

        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.decoder_layers):
            skip_connection = skip_connections.pop()  # Get the corresponding encoder output
            v_skip_connection = v_skip_connections.pop()
            # Apply learnable weight to skip connection
            weighted_skip = self.skip_weights[i] * skip_connection
            v_weighted_skip = self.v_skip_weights[i] * v_skip_connection
            x, v1, v = self.transformer.h[self.encoder_layers + i](x + weighted_skip, v1, x0, v_weighted_skip)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = 30 * torch.tanh(logits / 30) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss.float()

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 3000 # number of iterations to run
    warmup_iters : int = 0
    warmdown_iters : int = 900 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()

if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.9, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.9, 0.95), fused=True)
params = list(raw_model.transformer.h.parameters())
matrix_params = [p for p in params if p.ndim == 2]
scalar_params = [p for p in params if p.ndim < 2]+[raw_model.skip_weights]
optimizer3 = Muon(matrix_params, lr=0.04, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.9, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/500, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        print(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")
        with open(logfile, "a") as f:
            f.write(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")
    with open(logfile, "a") as f:
        f.write(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.5.1+cu124 compiled for CUDA 12.4
nvidia-smi:
Fri Nov 15 13:30:07 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   32C    P0             69W /  400W |    3263MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A100-SXM4-80GB          On  |   00000000:84:00.0 Off |                    0 |
| N/A   35C    P0             93W /  400W |    3407MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A100-SXM4-80GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   31C    P0             86W /  400W |    3481MiB /  81920MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A100-SXM4-80GB          On  |   00000000:C4:00.0 Off |                    0 |
| N/A   35C    P0             72W /  400W |    3337MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/3000 val_loss:10.8258 train_time:298ms step_avg:nanms
step:1/3000 train_loss:10.8258 train_time:105832ms step_avg:nanms
step:2/3000 train_loss:10.1619 train_time:106468ms step_avg:nanms
step:3/3000 train_loss:8.5165 train_time:107130ms step_avg:nanms
step:4/3000 train_loss:7.5676 train_time:107794ms step_avg:nanms
step:5/3000 train_loss:7.3970 train_time:108459ms step_avg:nanms
step:6/3000 train_loss:7.1179 train_time:109125ms step_avg:nanms
step:7/3000 train_loss:7.2947 train_time:109791ms step_avg:nanms
step:8/3000 train_loss:6.8968 train_time:110457ms step_avg:nanms
step:9/3000 train_loss:6.7426 train_time:111124ms step_avg:nanms
step:10/3000 train_loss:6.6216 train_time:111791ms step_avg:nanms
step:11/3000 train_loss:6.5481 train_time:656ms step_avg:nanms
step:12/3000 train_loss:6.4264 train_time:1325ms step_avg:nanms
step:13/3000 train_loss:6.3749 train_time:1992ms step_avg:664.09ms
step:14/3000 train_loss:6.3325 train_time:2659ms step_avg:664.82ms
step:15/3000 train_loss:6.3251 train_time:3326ms step_avg:665.27ms
step:16/3000 train_loss:6.2644 train_time:3996ms step_avg:666.05ms
step:17/3000 train_loss:6.3572 train_time:4665ms step_avg:666.38ms
step:18/3000 train_loss:6.1703 train_time:5333ms step_avg:666.58ms
step:19/3000 train_loss:6.1831 train_time:6001ms step_avg:666.80ms
step:20/3000 train_loss:5.8643 train_time:6670ms step_avg:667.02ms
step:21/3000 train_loss:6.1732 train_time:7339ms step_avg:667.14ms
step:22/3000 train_loss:6.4261 train_time:8007ms step_avg:667.29ms
step:23/3000 train_loss:6.0847 train_time:8676ms step_avg:667.37ms
step:24/3000 train_loss:6.2443 train_time:9345ms step_avg:667.48ms
step:25/3000 train_loss:5.9109 train_time:10015ms step_avg:667.67ms
step:26/3000 train_loss:5.8394 train_time:10685ms step_avg:667.84ms
step:27/3000 train_loss:6.1037 train_time:11355ms step_avg:667.96ms
step:28/3000 train_loss:5.7020 train_time:12026ms step_avg:668.11ms
step:29/3000 train_loss:5.9299 train_time:12696ms step_avg:668.20ms
step:30/3000 train_loss:5.7581 train_time:13366ms step_avg:668.31ms
step:31/3000 train_loss:5.7456 train_time:14036ms step_avg:668.40ms
step:32/3000 train_loss:5.5931 train_time:14707ms step_avg:668.49ms
step:33/3000 train_loss:5.8792 train_time:15377ms step_avg:668.57ms
step:34/3000 train_loss:5.8032 train_time:16049ms step_avg:668.69ms
step:35/3000 train_loss:5.9103 train_time:16718ms step_avg:668.74ms
step:36/3000 train_loss:5.8723 train_time:17390ms step_avg:668.84ms
step:37/3000 train_loss:5.7608 train_time:18062ms step_avg:668.95ms
step:38/3000 train_loss:5.6149 train_time:18732ms step_avg:668.99ms
step:39/3000 train_loss:5.6161 train_time:19403ms step_avg:669.07ms
step:40/3000 train_loss:5.5581 train_time:20073ms step_avg:669.12ms
step:41/3000 train_loss:5.5292 train_time:20744ms step_avg:669.16ms
step:42/3000 train_loss:5.4528 train_time:21416ms step_avg:669.24ms
step:43/3000 train_loss:5.5493 train_time:22087ms step_avg:669.29ms
step:44/3000 train_loss:5.5072 train_time:22759ms step_avg:669.37ms
step:45/3000 train_loss:5.6484 train_time:23431ms step_avg:669.45ms
step:46/3000 train_loss:5.4601 train_time:24101ms step_avg:669.48ms
step:47/3000 train_loss:5.3230 train_time:24773ms step_avg:669.54ms
step:48/3000 train_loss:5.4954 train_time:25444ms step_avg:669.59ms
step:49/3000 train_loss:5.3997 train_time:26116ms step_avg:669.63ms
step:50/3000 train_loss:5.4992 train_time:26787ms step_avg:669.67ms
step:51/3000 train_loss:5.4019 train_time:27458ms step_avg:669.72ms
step:52/3000 train_loss:5.2667 train_time:28130ms step_avg:669.76ms
step:53/3000 train_loss:5.3978 train_time:28801ms step_avg:669.79ms
step:54/3000 train_loss:5.2561 train_time:29472ms step_avg:669.81ms
step:55/3000 train_loss:5.6626 train_time:30143ms step_avg:669.84ms
step:56/3000 train_loss:5.2858 train_time:30815ms step_avg:669.89ms
step:57/3000 train_loss:5.1355 train_time:31485ms step_avg:669.90ms
step:58/3000 train_loss:5.2695 train_time:32156ms step_avg:669.92ms
step:59/3000 train_loss:5.2538 train_time:32827ms step_avg:669.95ms
step:60/3000 train_loss:5.3764 train_time:33499ms step_avg:669.98ms
step:61/3000 train_loss:5.1147 train_time:34170ms step_avg:670.00ms
step:62/3000 train_loss:5.2137 train_time:34842ms step_avg:670.04ms
step:63/3000 train_loss:5.1885 train_time:35513ms step_avg:670.06ms
step:64/3000 train_loss:5.2649 train_time:36185ms step_avg:670.09ms
step:65/3000 train_loss:5.0283 train_time:36857ms step_avg:670.13ms
step:66/3000 train_loss:5.1710 train_time:37528ms step_avg:670.14ms
step:67/3000 train_loss:5.0397 train_time:38198ms step_avg:670.15ms
step:68/3000 train_loss:5.3462 train_time:38869ms step_avg:670.15ms
step:69/3000 train_loss:4.9448 train_time:39540ms step_avg:670.17ms
step:70/3000 train_loss:5.0170 train_time:40211ms step_avg:670.19ms
step:71/3000 train_loss:5.2066 train_time:40882ms step_avg:670.19ms
step:72/3000 train_loss:5.1083 train_time:41553ms step_avg:670.21ms
step:73/3000 train_loss:4.9916 train_time:42224ms step_avg:670.22ms
step:74/3000 train_loss:5.1275 train_time:42895ms step_avg:670.23ms
step:75/3000 train_loss:5.1219 train_time:43566ms step_avg:670.25ms
step:76/3000 train_loss:5.0432 train_time:44236ms step_avg:670.25ms
step:77/3000 train_loss:5.1541 train_time:44909ms step_avg:670.28ms
step:78/3000 train_loss:5.3033 train_time:45580ms step_avg:670.29ms
step:79/3000 train_loss:5.0316 train_time:46251ms step_avg:670.30ms
step:80/3000 train_loss:5.0871 train_time:46922ms step_avg:670.31ms
step:81/3000 train_loss:4.8750 train_time:47593ms step_avg:670.33ms
step:82/3000 train_loss:5.0537 train_time:48264ms step_avg:670.34ms
step:83/3000 train_loss:5.0057 train_time:48936ms step_avg:670.35ms
step:84/3000 train_loss:5.0105 train_time:49606ms step_avg:670.35ms
step:85/3000 train_loss:4.8595 train_time:50278ms step_avg:670.38ms
step:86/3000 train_loss:5.0587 train_time:50949ms step_avg:670.39ms
step:87/3000 train_loss:4.9661 train_time:51621ms step_avg:670.40ms
step:88/3000 train_loss:5.0113 train_time:52291ms step_avg:670.39ms
step:89/3000 train_loss:4.9680 train_time:52964ms step_avg:670.42ms
step:90/3000 train_loss:4.8868 train_time:53634ms step_avg:670.43ms
step:91/3000 train_loss:4.8906 train_time:54305ms step_avg:670.43ms
step:92/3000 train_loss:5.0398 train_time:54976ms step_avg:670.44ms
step:93/3000 train_loss:4.8484 train_time:55647ms step_avg:670.44ms
step:94/3000 train_loss:4.8563 train_time:56318ms step_avg:670.46ms
step:95/3000 train_loss:4.9136 train_time:56991ms step_avg:670.48ms
step:96/3000 train_loss:4.8066 train_time:57662ms step_avg:670.49ms
step:97/3000 train_loss:4.8813 train_time:58333ms step_avg:670.50ms
step:98/3000 train_loss:4.8224 train_time:59004ms step_avg:670.49ms
step:99/3000 train_loss:4.9194 train_time:59676ms step_avg:670.51ms
step:100/3000 train_loss:4.9037 train_time:60346ms step_avg:670.51ms
step:101/3000 train_loss:4.8413 train_time:61017ms step_avg:670.52ms
step:102/3000 train_loss:4.9226 train_time:61688ms step_avg:670.52ms
step:103/3000 train_loss:4.8376 train_time:62359ms step_avg:670.53ms
step:104/3000 train_loss:4.7460 train_time:63030ms step_avg:670.53ms
step:105/3000 train_loss:4.7917 train_time:63700ms step_avg:670.53ms
step:106/3000 train_loss:4.9087 train_time:64371ms step_avg:670.53ms
step:107/3000 train_loss:4.7543 train_time:65042ms step_avg:670.54ms
step:108/3000 train_loss:4.5623 train_time:65713ms step_avg:670.55ms
step:109/3000 train_loss:4.7239 train_time:66385ms step_avg:670.55ms
step:110/3000 train_loss:4.7082 train_time:67056ms step_avg:670.56ms
step:111/3000 train_loss:4.6504 train_time:67727ms step_avg:670.57ms
step:112/3000 train_loss:4.8038 train_time:68398ms step_avg:670.57ms
step:113/3000 train_loss:4.7037 train_time:69069ms step_avg:670.57ms
step:114/3000 train_loss:4.5552 train_time:69739ms step_avg:670.57ms
step:115/3000 train_loss:4.7093 train_time:70410ms step_avg:670.58ms
step:116/3000 train_loss:4.6424 train_time:71081ms step_avg:670.57ms
step:117/3000 train_loss:4.5668 train_time:71751ms step_avg:670.57ms
step:118/3000 train_loss:4.7703 train_time:72422ms step_avg:670.57ms
step:119/3000 train_loss:4.6678 train_time:73094ms step_avg:670.58ms
step:120/3000 train_loss:4.5593 train_time:73764ms step_avg:670.58ms
step:121/3000 train_loss:4.4938 train_time:74436ms step_avg:670.60ms
step:122/3000 train_loss:4.6381 train_time:75107ms step_avg:670.59ms
step:123/3000 train_loss:4.4660 train_time:75779ms step_avg:670.61ms
step:124/3000 train_loss:4.7933 train_time:76449ms step_avg:670.61ms
step:125/3000 train_loss:4.6354 train_time:77119ms step_avg:670.60ms
step:125/3000 val_loss:4.5709 train_time:77131ms step_avg:670.70ms
step:126/3000 train_loss:4.5821 train_time:77793ms step_avg:670.63ms
step:127/3000 train_loss:4.6250 train_time:78462ms step_avg:670.62ms
step:128/3000 train_loss:4.5337 train_time:79132ms step_avg:670.61ms
step:129/3000 train_loss:4.8375 train_time:79803ms step_avg:670.61ms
step:130/3000 train_loss:4.5289 train_time:80472ms step_avg:670.60ms
step:131/3000 train_loss:4.5620 train_time:81144ms step_avg:670.61ms
step:132/3000 train_loss:4.5104 train_time:81815ms step_avg:670.61ms
step:133/3000 train_loss:4.5959 train_time:82487ms step_avg:670.63ms
step:134/3000 train_loss:4.4170 train_time:83157ms step_avg:670.62ms
step:135/3000 train_loss:4.5894 train_time:83828ms step_avg:670.62ms
step:136/3000 train_loss:4.3436 train_time:84500ms step_avg:670.63ms
step:137/3000 train_loss:4.5085 train_time:85176ms step_avg:670.68ms
step:138/3000 train_loss:4.4267 train_time:85850ms step_avg:670.70ms
step:139/3000 train_loss:4.5123 train_time:86522ms step_avg:670.71ms
step:140/3000 train_loss:4.6034 train_time:87194ms step_avg:670.73ms
step:141/3000 train_loss:4.4570 train_time:87868ms step_avg:670.75ms
step:142/3000 train_loss:4.4402 train_time:88542ms step_avg:670.78ms
step:143/3000 train_loss:4.3781 train_time:89213ms step_avg:670.78ms
step:144/3000 train_loss:4.4842 train_time:89885ms step_avg:670.78ms
step:145/3000 train_loss:4.4308 train_time:90558ms step_avg:670.80ms
step:146/3000 train_loss:4.3052 train_time:91229ms step_avg:670.80ms
step:147/3000 train_loss:4.4461 train_time:91900ms step_avg:670.80ms
step:148/3000 train_loss:4.4832 train_time:92571ms step_avg:670.80ms
step:149/3000 train_loss:4.4304 train_time:93242ms step_avg:670.80ms
step:150/3000 train_loss:4.5416 train_time:93913ms step_avg:670.81ms
step:151/3000 train_loss:4.3852 train_time:94585ms step_avg:670.81ms
step:152/3000 train_loss:4.3901 train_time:95256ms step_avg:670.82ms
step:153/3000 train_loss:4.4775 train_time:95927ms step_avg:670.82ms
step:154/3000 train_loss:4.4601 train_time:96598ms step_avg:670.82ms
step:155/3000 train_loss:4.3828 train_time:97270ms step_avg:670.83ms
step:156/3000 train_loss:4.4539 train_time:97942ms step_avg:670.83ms
step:157/3000 train_loss:4.5180 train_time:98611ms step_avg:670.82ms
step:158/3000 train_loss:4.3391 train_time:99283ms step_avg:670.83ms
step:159/3000 train_loss:4.4275 train_time:99953ms step_avg:670.83ms
step:160/3000 train_loss:4.2250 train_time:100624ms step_avg:670.83ms
step:161/3000 train_loss:4.4455 train_time:101295ms step_avg:670.83ms
step:162/3000 train_loss:4.4505 train_time:101967ms step_avg:670.83ms
step:163/3000 train_loss:4.4350 train_time:102638ms step_avg:670.84ms
step:164/3000 train_loss:4.2919 train_time:103311ms step_avg:670.85ms
step:165/3000 train_loss:4.3843 train_time:103981ms step_avg:670.85ms
step:166/3000 train_loss:4.4502 train_time:104652ms step_avg:670.85ms
step:167/3000 train_loss:4.2875 train_time:105321ms step_avg:670.84ms
step:168/3000 train_loss:4.3525 train_time:105992ms step_avg:670.84ms
step:169/3000 train_loss:4.2465 train_time:106663ms step_avg:670.84ms
step:170/3000 train_loss:4.1322 train_time:107333ms step_avg:670.83ms
step:171/3000 train_loss:4.2881 train_time:108003ms step_avg:670.82ms
step:172/3000 train_loss:4.3059 train_time:108672ms step_avg:670.82ms
step:173/3000 train_loss:4.3459 train_time:109343ms step_avg:670.81ms
step:174/3000 train_loss:4.5146 train_time:110013ms step_avg:670.81ms
step:175/3000 train_loss:4.3364 train_time:110683ms step_avg:670.81ms
step:176/3000 train_loss:4.1882 train_time:111354ms step_avg:670.80ms
step:177/3000 train_loss:4.1553 train_time:112023ms step_avg:670.80ms
step:178/3000 train_loss:4.2791 train_time:112694ms step_avg:670.79ms
step:179/3000 train_loss:4.2304 train_time:113364ms step_avg:670.79ms
step:180/3000 train_loss:4.2012 train_time:114034ms step_avg:670.79ms
step:181/3000 train_loss:4.3870 train_time:114704ms step_avg:670.79ms
step:182/3000 train_loss:4.2494 train_time:115374ms step_avg:670.78ms
step:183/3000 train_loss:4.2265 train_time:116044ms step_avg:670.78ms
step:184/3000 train_loss:4.2293 train_time:116714ms step_avg:670.77ms
step:185/3000 train_loss:4.3062 train_time:117383ms step_avg:670.76ms
step:186/3000 train_loss:4.2682 train_time:118055ms step_avg:670.77ms
step:187/3000 train_loss:4.3431 train_time:118725ms step_avg:670.76ms
step:188/3000 train_loss:4.2714 train_time:119395ms step_avg:670.76ms
step:189/3000 train_loss:4.1977 train_time:120065ms step_avg:670.76ms
step:190/3000 train_loss:4.3043 train_time:120740ms step_avg:670.78ms
step:191/3000 train_loss:4.3745 train_time:121598ms step_avg:671.81ms
step:192/3000 train_loss:4.1683 train_time:122268ms step_avg:671.80ms
step:193/3000 train_loss:4.2427 train_time:122939ms step_avg:671.80ms
step:194/3000 train_loss:4.1954 train_time:123609ms step_avg:671.79ms
step:195/3000 train_loss:4.1601 train_time:124278ms step_avg:671.77ms
step:196/3000 train_loss:4.1554 train_time:124950ms step_avg:671.78ms
step:197/3000 train_loss:4.2111 train_time:125619ms step_avg:671.76ms
step:198/3000 train_loss:4.1035 train_time:126288ms step_avg:671.75ms
step:199/3000 train_loss:4.3362 train_time:126958ms step_avg:671.73ms
step:200/3000 train_loss:4.2565 train_time:127629ms step_avg:671.73ms
step:201/3000 train_loss:5.0023 train_time:128299ms step_avg:671.72ms
step:202/3000 train_loss:4.4599 train_time:128969ms step_avg:671.72ms
step:203/3000 train_loss:4.2378 train_time:129639ms step_avg:671.70ms
step:204/3000 train_loss:4.1626 train_time:130309ms step_avg:671.70ms
step:205/3000 train_loss:4.1990 train_time:130979ms step_avg:671.69ms
step:206/3000 train_loss:4.1720 train_time:131649ms step_avg:671.68ms
step:207/3000 train_loss:4.1705 train_time:132319ms step_avg:671.67ms
step:208/3000 train_loss:4.1114 train_time:132989ms step_avg:671.66ms
step:209/3000 train_loss:4.2798 train_time:133659ms step_avg:671.66ms
step:210/3000 train_loss:4.1026 train_time:134329ms step_avg:671.64ms
step:211/3000 train_loss:4.2159 train_time:134999ms step_avg:671.64ms
step:212/3000 train_loss:4.2043 train_time:135669ms step_avg:671.63ms
step:213/3000 train_loss:4.1470 train_time:136338ms step_avg:671.61ms
step:214/3000 train_loss:4.1048 train_time:137008ms step_avg:671.61ms
step:215/3000 train_loss:4.2380 train_time:137678ms step_avg:671.60ms
step:216/3000 train_loss:4.1625 train_time:138347ms step_avg:671.59ms
step:217/3000 train_loss:4.1550 train_time:139017ms step_avg:671.58ms
step:218/3000 train_loss:4.1718 train_time:139688ms step_avg:671.58ms
step:219/3000 train_loss:4.0903 train_time:140358ms step_avg:671.57ms
step:220/3000 train_loss:4.2161 train_time:141028ms step_avg:671.56ms
step:221/3000 train_loss:4.1141 train_time:141698ms step_avg:671.55ms
step:222/3000 train_loss:4.1977 train_time:142367ms step_avg:671.54ms
step:223/3000 train_loss:4.1243 train_time:143037ms step_avg:671.54ms
step:224/3000 train_loss:4.1313 train_time:143707ms step_avg:671.53ms
step:225/3000 train_loss:4.1155 train_time:144377ms step_avg:671.52ms
step:226/3000 train_loss:4.1321 train_time:145047ms step_avg:671.51ms
step:227/3000 train_loss:4.1253 train_time:145718ms step_avg:671.51ms
step:228/3000 train_loss:4.0472 train_time:146387ms step_avg:671.50ms
step:229/3000 train_loss:4.2700 train_time:147058ms step_avg:671.50ms
step:230/3000 train_loss:4.1431 train_time:147727ms step_avg:671.49ms
step:231/3000 train_loss:4.0238 train_time:148397ms step_avg:671.48ms
step:232/3000 train_loss:4.2410 train_time:149067ms step_avg:671.47ms
step:233/3000 train_loss:4.0668 train_time:149736ms step_avg:671.46ms
step:234/3000 train_loss:4.0958 train_time:150406ms step_avg:671.46ms
step:235/3000 train_loss:4.2580 train_time:151076ms step_avg:671.45ms
step:236/3000 train_loss:4.1553 train_time:151746ms step_avg:671.44ms
step:237/3000 train_loss:4.2505 train_time:152417ms step_avg:671.44ms
step:238/3000 train_loss:4.1911 train_time:153087ms step_avg:671.43ms
step:239/3000 train_loss:4.1786 train_time:153757ms step_avg:671.43ms
step:240/3000 train_loss:4.1505 train_time:154428ms step_avg:671.42ms
step:241/3000 train_loss:4.3601 train_time:155098ms step_avg:671.42ms
step:242/3000 train_loss:4.1759 train_time:155768ms step_avg:671.41ms
step:243/3000 train_loss:4.5119 train_time:156439ms step_avg:671.41ms
step:244/3000 train_loss:4.1320 train_time:157109ms step_avg:671.41ms
step:245/3000 train_loss:4.1447 train_time:157780ms step_avg:671.40ms
step:246/3000 train_loss:4.1710 train_time:158449ms step_avg:671.40ms
step:247/3000 train_loss:4.0819 train_time:159119ms step_avg:671.39ms
step:248/3000 train_loss:4.0933 train_time:159790ms step_avg:671.39ms
step:249/3000 train_loss:4.0178 train_time:160460ms step_avg:671.38ms
step:250/3000 train_loss:4.1864 train_time:161130ms step_avg:671.37ms
step:250/3000 val_loss:4.0914 train_time:161141ms step_avg:671.42ms
step:251/3000 train_loss:4.0742 train_time:161803ms step_avg:671.38ms
step:252/3000 train_loss:4.0076 train_time:162472ms step_avg:671.37ms
step:253/3000 train_loss:4.0261 train_time:163142ms step_avg:671.36ms
step:254/3000 train_loss:4.0880 train_time:163812ms step_avg:671.36ms
step:255/3000 train_loss:4.0332 train_time:164482ms step_avg:671.35ms
step:256/3000 train_loss:3.9703 train_time:165152ms step_avg:671.35ms
step:257/3000 train_loss:4.0308 train_time:165823ms step_avg:671.35ms
step:258/3000 train_loss:4.1755 train_time:166493ms step_avg:671.34ms
step:259/3000 train_loss:3.9783 train_time:167162ms step_avg:671.33ms
step:260/3000 train_loss:4.0862 train_time:167834ms step_avg:671.33ms
step:261/3000 train_loss:4.0434 train_time:168504ms step_avg:671.33ms
step:262/3000 train_loss:4.0673 train_time:169174ms step_avg:671.33ms
step:263/3000 train_loss:4.1287 train_time:169844ms step_avg:671.32ms
step:264/3000 train_loss:4.1819 train_time:170515ms step_avg:671.32ms
step:265/3000 train_loss:4.5237 train_time:171186ms step_avg:671.32ms
step:266/3000 train_loss:4.0729 train_time:171856ms step_avg:671.31ms
step:267/3000 train_loss:4.2580 train_time:172526ms step_avg:671.31ms
step:268/3000 train_loss:4.0024 train_time:173197ms step_avg:671.31ms
step:269/3000 train_loss:4.0230 train_time:173868ms step_avg:671.30ms
step:270/3000 train_loss:4.1325 train_time:174537ms step_avg:671.30ms
step:271/3000 train_loss:4.1901 train_time:175208ms step_avg:671.29ms
step:272/3000 train_loss:4.0672 train_time:175878ms step_avg:671.29ms
step:273/3000 train_loss:4.0149 train_time:176547ms step_avg:671.28ms
step:274/3000 train_loss:4.0206 train_time:177217ms step_avg:671.28ms
step:275/3000 train_loss:3.9950 train_time:177887ms step_avg:671.27ms
step:276/3000 train_loss:4.0952 train_time:178558ms step_avg:671.27ms
step:277/3000 train_loss:4.1259 train_time:179229ms step_avg:671.27ms
step:278/3000 train_loss:3.9211 train_time:179900ms step_avg:671.27ms
step:279/3000 train_loss:4.2299 train_time:180570ms step_avg:671.26ms
step:280/3000 train_loss:4.0708 train_time:181241ms step_avg:671.26ms
step:281/3000 train_loss:4.0815 train_time:181911ms step_avg:671.26ms
step:282/3000 train_loss:4.0323 train_time:182582ms step_avg:671.26ms
step:283/3000 train_loss:4.2449 train_time:183253ms step_avg:671.26ms
step:284/3000 train_loss:3.9849 train_time:183924ms step_avg:671.26ms
step:285/3000 train_loss:3.9565 train_time:184595ms step_avg:671.25ms
step:286/3000 train_loss:4.1379 train_time:185265ms step_avg:671.25ms
step:287/3000 train_loss:4.0658 train_time:185935ms step_avg:671.25ms
step:288/3000 train_loss:4.0932 train_time:186607ms step_avg:671.25ms
step:289/3000 train_loss:4.1378 train_time:187276ms step_avg:671.24ms
step:290/3000 train_loss:4.1222 train_time:187947ms step_avg:671.24ms
step:291/3000 train_loss:3.9689 train_time:188616ms step_avg:671.23ms
step:292/3000 train_loss:3.9483 train_time:189286ms step_avg:671.23ms
step:293/3000 train_loss:4.1452 train_time:189957ms step_avg:671.23ms
step:294/3000 train_loss:3.9403 train_time:190627ms step_avg:671.22ms
step:295/3000 train_loss:4.2278 train_time:191298ms step_avg:671.22ms
step:296/3000 train_loss:4.1220 train_time:191969ms step_avg:671.22ms
step:297/3000 train_loss:3.9486 train_time:192638ms step_avg:671.21ms
step:298/3000 train_loss:4.0544 train_time:193307ms step_avg:671.21ms
step:299/3000 train_loss:3.9712 train_time:193979ms step_avg:671.21ms
step:300/3000 train_loss:4.1538 train_time:194648ms step_avg:671.20ms
step:301/3000 train_loss:3.9416 train_time:195320ms step_avg:671.20ms
step:302/3000 train_loss:4.0535 train_time:195991ms step_avg:671.20ms
step:303/3000 train_loss:3.9955 train_time:196661ms step_avg:671.20ms
step:304/3000 train_loss:3.9875 train_time:197331ms step_avg:671.19ms
step:305/3000 train_loss:3.9945 train_time:198000ms step_avg:671.19ms
step:306/3000 train_loss:4.1020 train_time:198670ms step_avg:671.18ms
step:307/3000 train_loss:4.0417 train_time:199341ms step_avg:671.18ms
step:308/3000 train_loss:3.9458 train_time:200011ms step_avg:671.18ms
step:309/3000 train_loss:4.1219 train_time:200681ms step_avg:671.17ms
step:310/3000 train_loss:3.9721 train_time:201352ms step_avg:671.17ms
step:311/3000 train_loss:4.0540 train_time:202021ms step_avg:671.17ms
step:312/3000 train_loss:4.0370 train_time:202692ms step_avg:671.17ms
step:313/3000 train_loss:3.9344 train_time:203364ms step_avg:671.17ms
step:314/3000 train_loss:4.0212 train_time:204034ms step_avg:671.17ms
step:315/3000 train_loss:4.0183 train_time:204705ms step_avg:671.16ms
step:316/3000 train_loss:3.7539 train_time:205376ms step_avg:671.16ms
step:317/3000 train_loss:4.0065 train_time:206047ms step_avg:671.16ms
step:318/3000 train_loss:3.9905 train_time:206718ms step_avg:671.16ms
step:319/3000 train_loss:3.9158 train_time:207388ms step_avg:671.16ms
step:320/3000 train_loss:3.9484 train_time:208059ms step_avg:671.16ms
step:321/3000 train_loss:3.9915 train_time:208728ms step_avg:671.15ms
step:322/3000 train_loss:4.0130 train_time:209398ms step_avg:671.15ms
step:323/3000 train_loss:3.8763 train_time:210068ms step_avg:671.14ms
step:324/3000 train_loss:3.9589 train_time:210739ms step_avg:671.14ms
step:325/3000 train_loss:4.0101 train_time:211409ms step_avg:671.14ms
step:326/3000 train_loss:4.0195 train_time:212080ms step_avg:671.14ms
step:327/3000 train_loss:3.9918 train_time:212749ms step_avg:671.13ms
step:328/3000 train_loss:4.2552 train_time:213418ms step_avg:671.13ms
step:329/3000 train_loss:4.0157 train_time:214090ms step_avg:671.13ms
step:330/3000 train_loss:4.0741 train_time:214760ms step_avg:671.12ms
step:331/3000 train_loss:3.9291 train_time:215430ms step_avg:671.12ms
step:332/3000 train_loss:4.5309 train_time:216101ms step_avg:671.12ms
step:333/3000 train_loss:3.9311 train_time:216771ms step_avg:671.12ms
step:334/3000 train_loss:3.8340 train_time:217440ms step_avg:671.11ms
step:335/3000 train_loss:3.9989 train_time:218109ms step_avg:671.10ms
step:336/3000 train_loss:4.0229 train_time:218779ms step_avg:671.10ms
step:337/3000 train_loss:4.0136 train_time:219450ms step_avg:671.10ms
step:338/3000 train_loss:3.9961 train_time:220119ms step_avg:671.10ms
step:339/3000 train_loss:4.0423 train_time:220791ms step_avg:671.10ms
step:340/3000 train_loss:3.9125 train_time:221460ms step_avg:671.09ms
step:341/3000 train_loss:3.9075 train_time:222131ms step_avg:671.09ms
step:342/3000 train_loss:3.9197 train_time:222800ms step_avg:671.08ms
step:343/3000 train_loss:3.9571 train_time:223470ms step_avg:671.08ms
step:344/3000 train_loss:4.0476 train_time:224140ms step_avg:671.08ms
step:345/3000 train_loss:3.9693 train_time:224808ms step_avg:671.07ms
step:346/3000 train_loss:3.9419 train_time:225478ms step_avg:671.07ms
step:347/3000 train_loss:3.9986 train_time:226149ms step_avg:671.06ms
step:348/3000 train_loss:3.9395 train_time:226819ms step_avg:671.06ms
step:349/3000 train_loss:3.9218 train_time:227489ms step_avg:671.06ms
step:350/3000 train_loss:3.9897 train_time:228159ms step_avg:671.05ms
step:351/3000 train_loss:4.2162 train_time:228828ms step_avg:671.05ms
step:352/3000 train_loss:3.8673 train_time:229499ms step_avg:671.05ms
step:353/3000 train_loss:3.9184 train_time:230168ms step_avg:671.04ms
step:354/3000 train_loss:3.9100 train_time:230837ms step_avg:671.04ms
step:355/3000 train_loss:3.8229 train_time:231507ms step_avg:671.03ms
step:356/3000 train_loss:3.9623 train_time:232176ms step_avg:671.03ms
step:357/3000 train_loss:3.8791 train_time:232847ms step_avg:671.03ms
step:358/3000 train_loss:4.0062 train_time:233517ms step_avg:671.02ms
step:359/3000 train_loss:3.9867 train_time:234187ms step_avg:671.02ms
step:360/3000 train_loss:3.9031 train_time:234856ms step_avg:671.02ms
step:361/3000 train_loss:4.1640 train_time:235526ms step_avg:671.02ms
step:362/3000 train_loss:4.1510 train_time:236196ms step_avg:671.01ms
step:363/3000 train_loss:3.9641 train_time:236864ms step_avg:671.00ms
step:364/3000 train_loss:3.8547 train_time:237534ms step_avg:671.00ms
step:365/3000 train_loss:4.0337 train_time:238204ms step_avg:671.00ms
step:366/3000 train_loss:3.8764 train_time:238873ms step_avg:670.99ms
step:367/3000 train_loss:3.9667 train_time:239543ms step_avg:670.99ms
step:368/3000 train_loss:3.8989 train_time:240212ms step_avg:670.98ms
step:369/3000 train_loss:3.9824 train_time:240883ms step_avg:670.98ms
step:370/3000 train_loss:3.9585 train_time:241551ms step_avg:670.98ms
step:371/3000 train_loss:3.7653 train_time:242220ms step_avg:670.97ms
step:372/3000 train_loss:3.8846 train_time:242889ms step_avg:670.97ms
step:373/3000 train_loss:3.9482 train_time:243558ms step_avg:670.96ms
step:374/3000 train_loss:3.9029 train_time:244226ms step_avg:670.95ms
step:375/3000 train_loss:3.9904 train_time:244896ms step_avg:670.95ms
step:375/3000 val_loss:3.9262 train_time:244908ms step_avg:670.98ms
step:376/3000 train_loss:3.9004 train_time:245568ms step_avg:670.95ms
step:377/3000 train_loss:3.8058 train_time:246238ms step_avg:670.95ms
step:378/3000 train_loss:3.4098 train_time:246907ms step_avg:670.94ms
step:379/3000 train_loss:3.7783 train_time:247577ms step_avg:670.94ms
step:380/3000 train_loss:3.7895 train_time:248256ms step_avg:670.96ms
step:381/3000 train_loss:3.8952 train_time:248925ms step_avg:670.96ms
step:382/3000 train_loss:3.9433 train_time:249595ms step_avg:670.96ms
step:383/3000 train_loss:3.9110 train_time:250263ms step_avg:670.95ms
step:384/3000 train_loss:3.8929 train_time:250932ms step_avg:670.94ms
step:385/3000 train_loss:3.9789 train_time:251600ms step_avg:670.93ms
step:386/3000 train_loss:3.8992 train_time:252268ms step_avg:670.93ms
step:387/3000 train_loss:3.9984 train_time:252936ms step_avg:670.92ms
step:388/3000 train_loss:4.1790 train_time:253605ms step_avg:670.91ms
step:389/3000 train_loss:3.8987 train_time:254273ms step_avg:670.91ms
step:390/3000 train_loss:3.8874 train_time:254943ms step_avg:670.90ms
step:391/3000 train_loss:3.9914 train_time:255612ms step_avg:670.90ms
step:392/3000 train_loss:3.9095 train_time:256283ms step_avg:670.90ms
step:393/3000 train_loss:4.0261 train_time:256951ms step_avg:670.89ms
step:394/3000 train_loss:3.8621 train_time:257622ms step_avg:670.89ms
step:395/3000 train_loss:3.9925 train_time:258291ms step_avg:670.89ms
step:396/3000 train_loss:3.7326 train_time:258961ms step_avg:670.88ms
step:397/3000 train_loss:3.9423 train_time:259629ms step_avg:670.88ms
step:398/3000 train_loss:3.9785 train_time:260299ms step_avg:670.87ms
step:399/3000 train_loss:3.9930 train_time:260968ms step_avg:670.87ms
step:400/3000 train_loss:3.8754 train_time:261637ms step_avg:670.86ms
step:401/3000 train_loss:3.9410 train_time:262306ms step_avg:670.86ms
step:402/3000 train_loss:4.0201 train_time:262977ms step_avg:670.86ms
step:403/3000 train_loss:3.9417 train_time:263647ms step_avg:670.86ms
step:404/3000 train_loss:4.0663 train_time:264318ms step_avg:670.86ms
step:405/3000 train_loss:3.7952 train_time:264985ms step_avg:670.85ms
step:406/3000 train_loss:3.8965 train_time:265653ms step_avg:670.84ms
step:407/3000 train_loss:4.2035 train_time:266321ms step_avg:670.83ms
step:408/3000 train_loss:3.8911 train_time:266990ms step_avg:670.83ms
step:409/3000 train_loss:3.9274 train_time:267659ms step_avg:670.82ms
step:410/3000 train_loss:3.9698 train_time:268329ms step_avg:670.82ms
step:411/3000 train_loss:3.8582 train_time:268998ms step_avg:670.82ms
step:412/3000 train_loss:3.8729 train_time:269668ms step_avg:670.82ms
step:413/3000 train_loss:4.3094 train_time:270337ms step_avg:670.81ms
step:414/3000 train_loss:3.7379 train_time:271007ms step_avg:670.81ms
step:415/3000 train_loss:4.1161 train_time:271676ms step_avg:670.81ms
step:416/3000 train_loss:3.8597 train_time:272346ms step_avg:670.80ms
step:417/3000 train_loss:3.8760 train_time:273015ms step_avg:670.80ms
step:418/3000 train_loss:4.0557 train_time:273684ms step_avg:670.79ms
step:419/3000 train_loss:3.7923 train_time:274353ms step_avg:670.79ms
step:420/3000 train_loss:3.9187 train_time:275021ms step_avg:670.78ms
step:421/3000 train_loss:3.8210 train_time:275690ms step_avg:670.78ms
step:422/3000 train_loss:3.7513 train_time:276361ms step_avg:670.78ms
step:423/3000 train_loss:3.8885 train_time:277029ms step_avg:670.77ms
step:424/3000 train_loss:3.9838 train_time:277697ms step_avg:670.77ms
step:425/3000 train_loss:3.7351 train_time:278366ms step_avg:670.76ms
step:426/3000 train_loss:3.9198 train_time:279035ms step_avg:670.76ms
step:427/3000 train_loss:3.7810 train_time:279703ms step_avg:670.75ms
step:428/3000 train_loss:4.0087 train_time:280373ms step_avg:670.75ms
step:429/3000 train_loss:3.9283 train_time:281042ms step_avg:670.74ms
step:430/3000 train_loss:3.8684 train_time:281710ms step_avg:670.74ms
step:431/3000 train_loss:3.8337 train_time:282379ms step_avg:670.73ms
step:432/3000 train_loss:3.7429 train_time:283048ms step_avg:670.73ms
step:433/3000 train_loss:3.8813 train_time:283718ms step_avg:670.73ms
step:434/3000 train_loss:3.9329 train_time:284387ms step_avg:670.72ms
step:435/3000 train_loss:3.8822 train_time:285056ms step_avg:670.72ms
step:436/3000 train_loss:3.9243 train_time:285724ms step_avg:670.71ms
step:437/3000 train_loss:3.9394 train_time:286392ms step_avg:670.71ms
step:438/3000 train_loss:3.8150 train_time:287060ms step_avg:670.70ms
step:439/3000 train_loss:3.8333 train_time:287729ms step_avg:670.70ms
step:440/3000 train_loss:3.8142 train_time:288397ms step_avg:670.69ms
step:441/3000 train_loss:3.9949 train_time:289066ms step_avg:670.69ms
step:442/3000 train_loss:3.8752 train_time:289736ms step_avg:670.69ms
step:443/3000 train_loss:3.8553 train_time:290404ms step_avg:670.68ms
step:444/3000 train_loss:3.7531 train_time:291073ms step_avg:670.68ms
step:445/3000 train_loss:4.0342 train_time:291742ms step_avg:670.67ms
step:446/3000 train_loss:3.9500 train_time:292410ms step_avg:670.67ms
step:447/3000 train_loss:3.9459 train_time:293080ms step_avg:670.66ms
step:448/3000 train_loss:3.8612 train_time:293748ms step_avg:670.66ms
step:449/3000 train_loss:3.9704 train_time:294417ms step_avg:670.65ms
step:450/3000 train_loss:3.8039 train_time:295084ms step_avg:670.65ms
step:451/3000 train_loss:3.8374 train_time:295753ms step_avg:670.64ms
step:452/3000 train_loss:3.6957 train_time:296422ms step_avg:670.64ms
step:453/3000 train_loss:3.8129 train_time:297090ms step_avg:670.63ms
step:454/3000 train_loss:3.7926 train_time:297759ms step_avg:670.63ms
step:455/3000 train_loss:3.7461 train_time:298427ms step_avg:670.62ms
step:456/3000 train_loss:3.9636 train_time:299095ms step_avg:670.62ms
step:457/3000 train_loss:3.8462 train_time:299764ms step_avg:670.61ms
step:458/3000 train_loss:3.9165 train_time:300434ms step_avg:670.61ms
step:459/3000 train_loss:3.9487 train_time:301102ms step_avg:670.61ms
step:460/3000 train_loss:3.7520 train_time:301771ms step_avg:670.60ms
step:461/3000 train_loss:3.9148 train_time:302439ms step_avg:670.60ms
step:462/3000 train_loss:3.8190 train_time:303109ms step_avg:670.59ms
step:463/3000 train_loss:3.8449 train_time:303777ms step_avg:670.59ms
step:464/3000 train_loss:3.8976 train_time:304446ms step_avg:670.59ms
step:465/3000 train_loss:3.8347 train_time:305115ms step_avg:670.58ms
step:466/3000 train_loss:3.8360 train_time:305784ms step_avg:670.58ms
step:467/3000 train_loss:3.9314 train_time:306452ms step_avg:670.57ms
step:468/3000 train_loss:3.9539 train_time:307121ms step_avg:670.57ms
step:469/3000 train_loss:3.9269 train_time:307790ms step_avg:670.57ms
step:470/3000 train_loss:3.8136 train_time:308459ms step_avg:670.56ms
step:471/3000 train_loss:3.8980 train_time:309128ms step_avg:670.56ms
step:472/3000 train_loss:3.9443 train_time:309797ms step_avg:670.56ms
step:473/3000 train_loss:3.8971 train_time:310466ms step_avg:670.55ms
step:474/3000 train_loss:3.8448 train_time:311135ms step_avg:670.55ms
step:475/3000 train_loss:3.7011 train_time:311804ms step_avg:670.55ms
step:476/3000 train_loss:4.1431 train_time:312473ms step_avg:670.54ms
step:477/3000 train_loss:3.8919 train_time:313142ms step_avg:670.54ms
step:478/3000 train_loss:3.7033 train_time:313811ms step_avg:670.54ms
step:479/3000 train_loss:3.9358 train_time:314480ms step_avg:670.53ms
step:480/3000 train_loss:3.8941 train_time:315149ms step_avg:670.53ms
step:481/3000 train_loss:4.0391 train_time:315817ms step_avg:670.52ms
step:482/3000 train_loss:3.8455 train_time:316485ms step_avg:670.52ms
step:483/3000 train_loss:3.6503 train_time:317153ms step_avg:670.51ms
step:484/3000 train_loss:3.9278 train_time:317822ms step_avg:670.51ms
step:485/3000 train_loss:3.7889 train_time:318491ms step_avg:670.51ms
step:486/3000 train_loss:3.7980 train_time:319160ms step_avg:670.50ms
step:487/3000 train_loss:3.7183 train_time:319829ms step_avg:670.50ms
step:488/3000 train_loss:3.7934 train_time:320498ms step_avg:670.50ms
step:489/3000 train_loss:3.9993 train_time:321166ms step_avg:670.49ms
step:490/3000 train_loss:3.8371 train_time:321835ms step_avg:670.49ms
step:491/3000 train_loss:3.7210 train_time:322503ms step_avg:670.48ms
step:492/3000 train_loss:3.7425 train_time:323172ms step_avg:670.48ms
step:493/3000 train_loss:3.8581 train_time:323840ms step_avg:670.48ms
step:494/3000 train_loss:3.7008 train_time:324509ms step_avg:670.47ms
step:495/3000 train_loss:3.8339 train_time:325177ms step_avg:670.47ms
step:496/3000 train_loss:3.7790 train_time:325847ms step_avg:670.47ms
step:497/3000 train_loss:3.6495 train_time:326516ms step_avg:670.46ms
step:498/3000 train_loss:3.8542 train_time:327184ms step_avg:670.46ms
step:499/3000 train_loss:3.9248 train_time:327853ms step_avg:670.46ms
step:500/3000 train_loss:3.9631 train_time:328522ms step_avg:670.45ms
step:500/3000 val_loss:3.8356 train_time:328533ms step_avg:670.48ms
step:501/3000 train_loss:3.8770 train_time:329193ms step_avg:670.45ms
step:502/3000 train_loss:3.9300 train_time:329863ms step_avg:670.45ms
step:503/3000 train_loss:3.8699 train_time:330532ms step_avg:670.45ms
step:504/3000 train_loss:3.9050 train_time:331201ms step_avg:670.45ms
step:505/3000 train_loss:3.8561 train_time:331868ms step_avg:670.44ms
step:506/3000 train_loss:3.9481 train_time:332537ms step_avg:670.44ms
step:507/3000 train_loss:3.7825 train_time:333206ms step_avg:670.43ms
step:508/3000 train_loss:3.8873 train_time:333874ms step_avg:670.43ms
step:509/3000 train_loss:3.9520 train_time:334543ms step_avg:670.43ms
step:510/3000 train_loss:3.8997 train_time:335210ms step_avg:670.42ms
step:511/3000 train_loss:3.7124 train_time:335879ms step_avg:670.42ms
step:512/3000 train_loss:3.9053 train_time:336546ms step_avg:670.41ms
step:513/3000 train_loss:3.8430 train_time:337216ms step_avg:670.41ms
step:514/3000 train_loss:3.8109 train_time:337884ms step_avg:670.40ms
step:515/3000 train_loss:3.9000 train_time:338553ms step_avg:670.40ms
step:516/3000 train_loss:3.8714 train_time:339222ms step_avg:670.40ms
step:517/3000 train_loss:4.2284 train_time:339891ms step_avg:670.40ms
step:518/3000 train_loss:3.8152 train_time:340560ms step_avg:670.39ms
step:519/3000 train_loss:3.9198 train_time:341228ms step_avg:670.39ms
step:520/3000 train_loss:3.8078 train_time:341896ms step_avg:670.38ms
step:521/3000 train_loss:3.8188 train_time:342565ms step_avg:670.38ms
step:522/3000 train_loss:3.7825 train_time:343233ms step_avg:670.38ms
step:523/3000 train_loss:3.7916 train_time:343903ms step_avg:670.38ms
step:524/3000 train_loss:4.4122 train_time:344571ms step_avg:670.37ms
step:525/3000 train_loss:3.8752 train_time:345239ms step_avg:670.37ms
step:526/3000 train_loss:3.8090 train_time:345908ms step_avg:670.36ms
step:527/3000 train_loss:3.8200 train_time:346576ms step_avg:670.36ms
step:528/3000 train_loss:3.7814 train_time:347244ms step_avg:670.36ms
step:529/3000 train_loss:3.7579 train_time:347912ms step_avg:670.35ms
step:530/3000 train_loss:3.9799 train_time:348580ms step_avg:670.35ms
step:531/3000 train_loss:3.7786 train_time:349249ms step_avg:670.34ms
step:532/3000 train_loss:4.0486 train_time:349917ms step_avg:670.34ms
step:533/3000 train_loss:3.8604 train_time:350584ms step_avg:670.33ms
step:534/3000 train_loss:3.7887 train_time:351255ms step_avg:670.33ms
step:535/3000 train_loss:3.8169 train_time:351923ms step_avg:670.33ms
step:536/3000 train_loss:3.7502 train_time:352592ms step_avg:670.33ms
step:537/3000 train_loss:3.8823 train_time:353262ms step_avg:670.33ms
step:538/3000 train_loss:3.8666 train_time:353929ms step_avg:670.32ms
step:539/3000 train_loss:3.7646 train_time:354597ms step_avg:670.32ms
step:540/3000 train_loss:4.2564 train_time:355264ms step_avg:670.31ms
step:541/3000 train_loss:3.7984 train_time:355931ms step_avg:670.30ms
step:542/3000 train_loss:3.9134 train_time:356599ms step_avg:670.30ms
step:543/3000 train_loss:3.7406 train_time:357267ms step_avg:670.29ms
step:544/3000 train_loss:3.7159 train_time:357935ms step_avg:670.29ms
step:545/3000 train_loss:3.8002 train_time:358603ms step_avg:670.29ms
step:546/3000 train_loss:3.7232 train_time:359272ms step_avg:670.28ms
step:547/3000 train_loss:3.7757 train_time:359941ms step_avg:670.28ms
step:548/3000 train_loss:3.7822 train_time:360609ms step_avg:670.28ms
step:549/3000 train_loss:3.7551 train_time:361277ms step_avg:670.27ms
step:550/3000 train_loss:3.8528 train_time:361945ms step_avg:670.27ms
step:551/3000 train_loss:3.7403 train_time:362615ms step_avg:670.27ms
step:552/3000 train_loss:3.7631 train_time:363283ms step_avg:670.26ms
step:553/3000 train_loss:4.0815 train_time:363950ms step_avg:670.26ms
step:554/3000 train_loss:3.8861 train_time:364617ms step_avg:670.25ms
step:555/3000 train_loss:3.8412 train_time:365285ms step_avg:670.25ms
step:556/3000 train_loss:3.7873 train_time:365953ms step_avg:670.24ms
step:557/3000 train_loss:3.8277 train_time:366621ms step_avg:670.24ms
step:558/3000 train_loss:3.4788 train_time:367289ms step_avg:670.24ms
step:559/3000 train_loss:3.7479 train_time:367959ms step_avg:670.23ms
step:560/3000 train_loss:3.7872 train_time:368626ms step_avg:670.23ms
step:561/3000 train_loss:3.8385 train_time:369294ms step_avg:670.23ms
step:562/3000 train_loss:3.7431 train_time:369963ms step_avg:670.22ms
step:563/3000 train_loss:3.6825 train_time:370631ms step_avg:670.22ms
step:564/3000 train_loss:3.8887 train_time:371298ms step_avg:670.21ms
step:565/3000 train_loss:3.6993 train_time:371967ms step_avg:670.21ms
step:566/3000 train_loss:3.8172 train_time:372635ms step_avg:670.21ms
step:567/3000 train_loss:3.7629 train_time:373303ms step_avg:670.20ms
step:568/3000 train_loss:3.7335 train_time:373972ms step_avg:670.20ms
step:569/3000 train_loss:3.8175 train_time:374641ms step_avg:670.20ms
step:570/3000 train_loss:3.7838 train_time:375321ms step_avg:670.22ms
step:571/3000 train_loss:3.8115 train_time:375989ms step_avg:670.21ms
step:572/3000 train_loss:3.7196 train_time:376841ms step_avg:670.54ms
step:573/3000 train_loss:3.6609 train_time:377509ms step_avg:670.53ms
step:574/3000 train_loss:3.6603 train_time:378179ms step_avg:670.53ms
step:575/3000 train_loss:4.1499 train_time:378847ms step_avg:670.53ms
step:576/3000 train_loss:3.6855 train_time:379515ms step_avg:670.52ms
step:577/3000 train_loss:3.7654 train_time:380183ms step_avg:670.52ms
step:578/3000 train_loss:3.9114 train_time:380852ms step_avg:670.51ms
step:579/3000 train_loss:3.7063 train_time:381521ms step_avg:670.51ms
step:580/3000 train_loss:3.7562 train_time:382188ms step_avg:670.50ms
step:581/3000 train_loss:3.8565 train_time:382857ms step_avg:670.50ms
step:582/3000 train_loss:3.8718 train_time:383525ms step_avg:670.50ms
step:583/3000 train_loss:3.8099 train_time:384193ms step_avg:670.49ms
step:584/3000 train_loss:3.9856 train_time:384862ms step_avg:670.49ms
step:585/3000 train_loss:3.7762 train_time:385531ms step_avg:670.49ms
step:586/3000 train_loss:3.7909 train_time:386200ms step_avg:670.49ms
step:587/3000 train_loss:3.8245 train_time:386869ms step_avg:670.48ms
step:588/3000 train_loss:4.2117 train_time:387536ms step_avg:670.48ms
step:589/3000 train_loss:3.6478 train_time:388205ms step_avg:670.48ms
step:590/3000 train_loss:4.0053 train_time:388873ms step_avg:670.47ms
step:591/3000 train_loss:3.8234 train_time:389542ms step_avg:670.47ms
step:592/3000 train_loss:3.6697 train_time:390210ms step_avg:670.46ms
step:593/3000 train_loss:3.7619 train_time:390878ms step_avg:670.46ms
step:594/3000 train_loss:3.8607 train_time:391546ms step_avg:670.46ms
step:595/3000 train_loss:3.7438 train_time:392213ms step_avg:670.45ms
step:596/3000 train_loss:3.7803 train_time:392880ms step_avg:670.44ms
step:597/3000 train_loss:3.8069 train_time:393549ms step_avg:670.44ms
step:598/3000 train_loss:3.8203 train_time:394217ms step_avg:670.44ms
step:599/3000 train_loss:3.8149 train_time:394885ms step_avg:670.43ms
step:600/3000 train_loss:3.9287 train_time:395554ms step_avg:670.43ms
step:601/3000 train_loss:3.7830 train_time:396221ms step_avg:670.42ms
step:602/3000 train_loss:3.7441 train_time:396889ms step_avg:670.42ms
step:603/3000 train_loss:3.8873 train_time:397558ms step_avg:670.42ms
step:604/3000 train_loss:3.6999 train_time:398226ms step_avg:670.41ms
step:605/3000 train_loss:3.8032 train_time:398895ms step_avg:670.41ms
step:606/3000 train_loss:4.1108 train_time:399562ms step_avg:670.41ms
step:607/3000 train_loss:3.8316 train_time:400230ms step_avg:670.40ms
step:608/3000 train_loss:3.8393 train_time:400898ms step_avg:670.40ms
step:609/3000 train_loss:3.7895 train_time:401565ms step_avg:670.39ms
step:610/3000 train_loss:3.7104 train_time:402233ms step_avg:670.39ms
step:611/3000 train_loss:3.7864 train_time:402901ms step_avg:670.38ms
step:612/3000 train_loss:3.7329 train_time:403572ms step_avg:670.38ms
step:613/3000 train_loss:3.8633 train_time:404240ms step_avg:670.38ms
step:614/3000 train_loss:3.7034 train_time:404909ms step_avg:670.38ms
step:615/3000 train_loss:3.7504 train_time:405577ms step_avg:670.38ms
step:616/3000 train_loss:3.7066 train_time:406245ms step_avg:670.37ms
step:617/3000 train_loss:3.7749 train_time:406914ms step_avg:670.37ms
step:618/3000 train_loss:3.8369 train_time:407583ms step_avg:670.37ms
step:619/3000 train_loss:3.6119 train_time:408252ms step_avg:670.36ms
step:620/3000 train_loss:3.8918 train_time:408921ms step_avg:670.36ms
step:621/3000 train_loss:3.7974 train_time:409589ms step_avg:670.36ms
step:622/3000 train_loss:3.7934 train_time:410258ms step_avg:670.36ms
step:623/3000 train_loss:3.6208 train_time:410927ms step_avg:670.35ms
step:624/3000 train_loss:3.7296 train_time:411595ms step_avg:670.35ms
step:625/3000 train_loss:3.8456 train_time:412263ms step_avg:670.35ms
step:625/3000 val_loss:3.7599 train_time:412275ms step_avg:670.37ms
step:626/3000 train_loss:3.7877 train_time:412935ms step_avg:670.35ms
step:627/3000 train_loss:3.7580 train_time:413605ms step_avg:670.35ms
step:628/3000 train_loss:3.6666 train_time:414274ms step_avg:670.35ms
step:629/3000 train_loss:3.8000 train_time:414942ms step_avg:670.34ms
step:630/3000 train_loss:3.6479 train_time:415612ms step_avg:670.34ms
step:631/3000 train_loss:3.8348 train_time:416280ms step_avg:670.34ms
