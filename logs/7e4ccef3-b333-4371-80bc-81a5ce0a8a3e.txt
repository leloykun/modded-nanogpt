====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import flex_attention, create_block_mask, BlockMask, _score_mod_signature
from torch._inductor.lowering import make_pointwise, register_lowering
# Some internal torch.compile details
from torch._inductor.virtualized import ops
from functools import partial
flex_attention = torch.compile(flex_attention, dynamic=False)
create_block_mask = torch.compile(create_block_mask, dynamic=False)

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                p.grad = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(p.grad, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# Attention Tanh softcapping

@torch.library.custom_op("approx::tanh", mutates_args=())
def _tanh_approx(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

@_tanh_approx.register_fake
def _(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

def _tanh_approx_lowering(inp):
    fn = partial(ops.inline_asm_elementwise, asm="tanh.approx.f32 $0, $1;")
    return make_pointwise(fn)(inp)

register_lowering(torch.ops.approx.tanh)(_tanh_approx_lowering)

class _TanhApprox(torch.autograd.Function):
    @staticmethod
    def forward(x):
        return torch.ops.approx.tanh(x)

    @staticmethod
    def setup_context(ctx, inputs, output):
        (x,) = inputs
        result = output
        ctx.save_for_backward(result)

    @staticmethod
    def backward(ctx, grad_output):
        (result,) = ctx.saved_tensors
        return grad_output * (1 - result * result)

    @staticmethod
    def vmap(info, in_dims, x):
        return torch.tanh(x), 0

_tanh_approx = _TanhApprox.apply

def generate_tanh_softcap(soft_cap: int, approx: bool=True) -> _score_mod_signature:
    tanh = _tanh_approx if approx else torch.tanh

    def tanh_softcap(score, b, h, q_idx, kv_idx):
        return soft_cap * tanh(score / soft_cap)

    prefix = "tanh_softcap_approx" if approx else "tanh_softcap"
    tanh_softcap.__name__ = f"{prefix}_{soft_cap}"

    return tanh_softcap

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1, block_mask: BlockMask, score_mod: _score_mod_signature):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=score_mod, block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, block_mask: BlockMask, score_mod: _score_mod_signature):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1, block_mask, score_mod)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attention_soft_cap : int = 50
    lm_head_soft_cap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.attention_soft_cap = config.attention_soft_cap
        self.lm_head_soft_cap = config.lm_head_soft_cap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target, attn_blocksize):

        docs = (idx == 50256).cumsum(0)
        def document_causal_mask(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < attn_blocksize
            return causal_mask & document_mask & window_mask

        softcap_mod = generate_tanh_softcap(self.attention_soft_cap, approx=True)  # @leloykun

        S = len(idx)
        block_mask = create_block_mask(document_causal_mask, None, None, S, S, device="cuda", _compile=True)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x, v1 = self.transformer.h[i](x, v1, x0, block_mask, softcap_mod)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x, v1 = self.transformer.h[self.num_encoder_layers + i](x, v1, x0, block_mask, softcap_mod)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = self.lm_head_soft_cap * torch.tanh(logits / self.lm_head_soft_cap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        batch_size = self.B * self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.B*self.T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = buf[:-1] # inputs
        y = buf[1:] # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    device_batch_size : int = 1 # batch size, in sequences, per device
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1700 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 622 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    block_size_warmup_iters : int = 1600
    block_size_warmup_step : int = 8
    block_size_warmup_max : int = 1792
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
gpt_config = GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768)
model = GPT(gpt_config)
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.8, 0.95), fused=True)
param_names = [name for name, _ in raw_model.named_parameters()]
params = list(raw_model.transformer.h.parameters())
qk_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and ("c_q" in n or "c_k" in n)]
matrix_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and "c_q" not in n and "c_k" not in n]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer5 = Muon(qk_params, lr=0.08, momentum=0.95)
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4, optimizer5]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # Set the attention blocksize for the current step, in chunks of 64
    attn_blocksize = torch.tensor(
        args.block_size_warmup_step
        * (
            1 +
            (min(step/args.block_size_warmup_iters, 1) * (args.block_size_warmup_max - args.block_size_warmup_step))
            // args.block_size_warmup_step
        ),
        dtype=torch.int,
        device='cuda',
    )
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, attn_blocksize=attn_blocksize)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # if master_process:
        #     print("============== Weight norms: ==============")
        #     with open(logfile, "a") as f:
        #         f.write("============== Weight norms: ==============\n")
        #         for name, p in model.named_parameters():
        #             if p.ndim != 2:
        #                 continue
        #             if "transformer.wte" in name:
        #                 l1_to_l2_norm = torch.norm(p.data.float(), p=2, dim=1).mean().item()
        #                 l1_to_rms_norm = (1/p.data.size(1))**0.5 * l1_to_l2_norm
        #                 print(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
        #                 f.write(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
        #             elif "attn.c_q" in name or "attn.c_k" in name:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #                 for h in range(gpt_config.n_head):
        #                     head_dim = gpt_config.n_embd // gpt_config.n_head
        #                     frobenius_norm = torch.linalg.norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
        #                     spectral_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
        #                     nuclear_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
        #                     print(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                     f.write(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #             else:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #         f.write("===========================================\n")
        #     print("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y, attn_blocksize=attn_blocksize)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # if master_process and (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
    #     print("============== Gradient norms: ==============")
    #     with open(logfile, "a") as f:
    #         f.write("============== Gradient norms: ==============\n")
    #         for name, p in model.named_parameters():
    #             if p.ndim != 2:
    #                 continue
    #             if p.grad is None:
    #                 continue
    #             if "transformer.wte" in name:
    #                 l1_to_l2_norm = torch.norm(p.grad.float(), p=2, dim=1).mean().item()
    #                 l1_to_rms_norm = (1/p.grad.size(1))**0.5 * l1_to_l2_norm
    #                 print(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
    #                 f.write(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
    #             elif "attn.c_q" in name or "attn.c_k" in name:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #                 for h in range(gpt_config.n_head):
    #                     head_dim = gpt_config.n_embd // gpt_config.n_head
    #                     frobenius_norm = torch.linalg.norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
    #                     spectral_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
    #                     nuclear_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
    #                     print(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                     f.write(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #             else:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #         f.write("===========================================\n")
    #     print("===========================================")
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241126+cu124 compiled for CUDA 12.4
nvidia-smi:
Wed Nov 27 01:10:14 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:34:00.0 Off |                    0 |
| N/A   47C    P0            107W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:48:00.0 Off |                    0 |
| N/A   49C    P0            117W /  700W |      24MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:AE:00.0 Off |                    0 |
| N/A   46C    P0            117W /  700W |      24MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:C2:00.0 Off |                    0 |
| N/A   49C    P0            125W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1100000000 across 11 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1700 train_loss:10.8258 train_time:22807ms step_avg:nanms
step:2/1700 train_loss:10.1253 train_time:23541ms step_avg:nanms
step:3/1700 train_loss:8.4069 train_time:23834ms step_avg:nanms
step:4/1700 train_loss:7.6235 train_time:24127ms step_avg:nanms
step:5/1700 train_loss:7.4050 train_time:24420ms step_avg:nanms
step:6/1700 train_loss:7.0001 train_time:24712ms step_avg:nanms
step:7/1700 train_loss:6.9570 train_time:25007ms step_avg:nanms
step:8/1700 train_loss:6.4774 train_time:25298ms step_avg:nanms
step:9/1700 train_loss:6.7378 train_time:25592ms step_avg:nanms
step:10/1700 train_loss:6.5685 train_time:25884ms step_avg:nanms
step:11/1700 train_loss:6.4269 train_time:284ms step_avg:nanms
step:12/1700 train_loss:6.2531 train_time:579ms step_avg:nanms
step:13/1700 train_loss:6.1940 train_time:871ms step_avg:290.37ms
step:14/1700 train_loss:6.1222 train_time:1163ms step_avg:290.83ms
step:15/1700 train_loss:6.0979 train_time:1458ms step_avg:291.51ms
step:16/1700 train_loss:5.8994 train_time:1751ms step_avg:291.90ms
step:17/1700 train_loss:5.8346 train_time:2045ms step_avg:292.14ms
step:18/1700 train_loss:6.4334 train_time:2339ms step_avg:292.36ms
step:19/1700 train_loss:5.8409 train_time:2631ms step_avg:292.37ms
step:20/1700 train_loss:5.9854 train_time:2923ms step_avg:292.33ms
step:21/1700 train_loss:5.9259 train_time:3220ms step_avg:292.68ms
step:22/1700 train_loss:5.6407 train_time:3515ms step_avg:292.94ms
step:23/1700 train_loss:5.7568 train_time:3808ms step_avg:292.90ms
step:24/1700 train_loss:5.7957 train_time:4103ms step_avg:293.04ms
step:25/1700 train_loss:5.5453 train_time:4396ms step_avg:293.06ms
step:26/1700 train_loss:5.6605 train_time:4689ms step_avg:293.04ms
step:27/1700 train_loss:5.6228 train_time:4983ms step_avg:293.11ms
step:28/1700 train_loss:5.5985 train_time:5280ms step_avg:293.33ms
step:29/1700 train_loss:5.6527 train_time:5573ms step_avg:293.34ms
step:30/1700 train_loss:5.6660 train_time:5866ms step_avg:293.30ms
step:31/1700 train_loss:6.0049 train_time:6161ms step_avg:293.37ms
step:32/1700 train_loss:5.4828 train_time:6456ms step_avg:293.45ms
step:33/1700 train_loss:5.3468 train_time:6750ms step_avg:293.48ms
step:34/1700 train_loss:5.3696 train_time:7043ms step_avg:293.46ms
step:35/1700 train_loss:5.6065 train_time:7340ms step_avg:293.58ms
step:36/1700 train_loss:5.4946 train_time:7634ms step_avg:293.62ms
step:37/1700 train_loss:5.5150 train_time:7927ms step_avg:293.59ms
step:38/1700 train_loss:5.3501 train_time:8222ms step_avg:293.63ms
step:39/1700 train_loss:5.4164 train_time:8519ms step_avg:293.74ms
step:40/1700 train_loss:5.2371 train_time:8812ms step_avg:293.73ms
step:41/1700 train_loss:5.4057 train_time:9105ms step_avg:293.71ms
step:42/1700 train_loss:5.2699 train_time:9400ms step_avg:293.75ms
step:43/1700 train_loss:5.2910 train_time:9694ms step_avg:293.75ms
step:44/1700 train_loss:5.1763 train_time:9987ms step_avg:293.73ms
step:45/1700 train_loss:5.0825 train_time:10283ms step_avg:293.79ms
step:46/1700 train_loss:5.1639 train_time:10579ms step_avg:293.85ms
step:47/1700 train_loss:5.0760 train_time:10873ms step_avg:293.85ms
step:48/1700 train_loss:5.2493 train_time:11166ms step_avg:293.84ms
step:49/1700 train_loss:5.0787 train_time:11461ms step_avg:293.88ms
step:50/1700 train_loss:5.1178 train_time:11756ms step_avg:293.91ms
step:51/1700 train_loss:5.1049 train_time:12051ms step_avg:293.92ms
step:52/1700 train_loss:5.2241 train_time:12345ms step_avg:293.92ms
step:53/1700 train_loss:5.0460 train_time:12640ms step_avg:293.95ms
step:54/1700 train_loss:5.1133 train_time:12934ms step_avg:293.95ms
step:55/1700 train_loss:5.0135 train_time:13227ms step_avg:293.93ms
step:56/1700 train_loss:5.0387 train_time:13520ms step_avg:293.92ms
step:57/1700 train_loss:5.0668 train_time:13816ms step_avg:293.95ms
step:58/1700 train_loss:5.0867 train_time:14108ms step_avg:293.92ms
step:59/1700 train_loss:5.0741 train_time:14402ms step_avg:293.92ms
step:60/1700 train_loss:4.9641 train_time:14698ms step_avg:293.96ms
step:61/1700 train_loss:5.0641 train_time:14993ms step_avg:293.99ms
step:62/1700 train_loss:5.0737 train_time:15287ms step_avg:293.97ms
step:63/1700 train_loss:5.0233 train_time:15582ms step_avg:294.00ms
step:64/1700 train_loss:4.9828 train_time:15878ms step_avg:294.04ms
step:65/1700 train_loss:4.8330 train_time:16175ms step_avg:294.08ms
step:66/1700 train_loss:4.8570 train_time:16468ms step_avg:294.08ms
step:67/1700 train_loss:4.9793 train_time:16762ms step_avg:294.07ms
step:68/1700 train_loss:4.9477 train_time:17058ms step_avg:294.11ms
step:69/1700 train_loss:4.9852 train_time:17353ms step_avg:294.12ms
step:70/1700 train_loss:4.8328 train_time:17646ms step_avg:294.10ms
step:71/1700 train_loss:4.9188 train_time:17941ms step_avg:294.12ms
step:72/1700 train_loss:4.9063 train_time:18238ms step_avg:294.16ms
step:73/1700 train_loss:4.8938 train_time:18533ms step_avg:294.17ms
step:74/1700 train_loss:4.7651 train_time:18826ms step_avg:294.15ms
step:75/1700 train_loss:4.7929 train_time:19120ms step_avg:294.15ms
step:76/1700 train_loss:4.7001 train_time:19416ms step_avg:294.17ms
step:77/1700 train_loss:4.8993 train_time:19711ms step_avg:294.20ms
step:78/1700 train_loss:4.8302 train_time:20005ms step_avg:294.19ms
step:79/1700 train_loss:4.5625 train_time:20301ms step_avg:294.22ms
step:80/1700 train_loss:4.8317 train_time:20596ms step_avg:294.22ms
step:81/1700 train_loss:4.7627 train_time:20890ms step_avg:294.23ms
step:82/1700 train_loss:4.8160 train_time:21184ms step_avg:294.23ms
step:83/1700 train_loss:4.7995 train_time:21478ms step_avg:294.23ms
step:84/1700 train_loss:4.6902 train_time:21772ms step_avg:294.21ms
step:85/1700 train_loss:4.6876 train_time:22063ms step_avg:294.18ms
step:86/1700 train_loss:4.8190 train_time:22358ms step_avg:294.18ms
step:87/1700 train_loss:4.7921 train_time:22650ms step_avg:294.16ms
step:88/1700 train_loss:4.6401 train_time:22943ms step_avg:294.14ms
step:89/1700 train_loss:4.6529 train_time:23238ms step_avg:294.15ms
step:90/1700 train_loss:4.5795 train_time:23530ms step_avg:294.13ms
step:91/1700 train_loss:4.7248 train_time:23823ms step_avg:294.11ms
step:92/1700 train_loss:4.7131 train_time:24119ms step_avg:294.14ms
step:93/1700 train_loss:4.7832 train_time:24414ms step_avg:294.14ms
step:94/1700 train_loss:4.9077 train_time:24705ms step_avg:294.11ms
step:95/1700 train_loss:4.6402 train_time:25000ms step_avg:294.12ms
step:96/1700 train_loss:4.5610 train_time:25293ms step_avg:294.11ms
step:97/1700 train_loss:4.7198 train_time:25585ms step_avg:294.08ms
step:98/1700 train_loss:4.5455 train_time:25880ms step_avg:294.10ms
step:99/1700 train_loss:4.5310 train_time:26171ms step_avg:294.06ms
step:100/1700 train_loss:4.5875 train_time:26464ms step_avg:294.05ms
step:101/1700 train_loss:4.4495 train_time:26759ms step_avg:294.05ms
step:102/1700 train_loss:4.6117 train_time:27052ms step_avg:294.04ms
step:103/1700 train_loss:4.5415 train_time:27345ms step_avg:294.03ms
step:104/1700 train_loss:4.6099 train_time:27638ms step_avg:294.02ms
step:105/1700 train_loss:4.5954 train_time:27931ms step_avg:294.01ms
step:106/1700 train_loss:4.7532 train_time:28223ms step_avg:293.98ms
step:107/1700 train_loss:4.5597 train_time:28518ms step_avg:294.00ms
step:108/1700 train_loss:4.4099 train_time:28812ms step_avg:294.00ms
step:109/1700 train_loss:4.7721 train_time:29104ms step_avg:293.98ms
step:110/1700 train_loss:4.5499 train_time:29399ms step_avg:293.99ms
step:111/1700 train_loss:4.4686 train_time:29691ms step_avg:293.97ms
step:112/1700 train_loss:4.7089 train_time:29984ms step_avg:293.96ms
step:113/1700 train_loss:4.3656 train_time:30280ms step_avg:293.98ms
step:114/1700 train_loss:4.5605 train_time:30572ms step_avg:293.97ms
step:115/1700 train_loss:4.4955 train_time:30866ms step_avg:293.96ms
step:116/1700 train_loss:4.5396 train_time:31168ms step_avg:294.04ms
step:117/1700 train_loss:4.3106 train_time:31468ms step_avg:294.10ms
step:118/1700 train_loss:4.5420 train_time:31769ms step_avg:294.16ms
step:119/1700 train_loss:4.3729 train_time:32070ms step_avg:294.22ms
step:120/1700 train_loss:4.4683 train_time:32370ms step_avg:294.28ms
step:121/1700 train_loss:4.4559 train_time:32671ms step_avg:294.34ms
step:122/1700 train_loss:4.3402 train_time:32972ms step_avg:294.39ms
step:123/1700 train_loss:4.4279 train_time:33271ms step_avg:294.44ms
step:124/1700 train_loss:4.2877 train_time:33572ms step_avg:294.49ms
step:125/1700 train_loss:4.2969 train_time:33872ms step_avg:294.54ms
step:125/1700 val_loss:4.4068 train_time:33880ms step_avg:294.61ms
step:126/1700 train_loss:4.2651 train_time:34179ms step_avg:294.65ms
step:127/1700 train_loss:4.4620 train_time:34480ms step_avg:294.70ms
step:128/1700 train_loss:4.4503 train_time:34780ms step_avg:294.75ms
step:129/1700 train_loss:4.4504 train_time:35082ms step_avg:294.81ms
step:130/1700 train_loss:4.3998 train_time:35385ms step_avg:294.88ms
step:131/1700 train_loss:4.5405 train_time:35687ms step_avg:294.93ms
step:132/1700 train_loss:4.3131 train_time:35989ms step_avg:294.99ms
step:133/1700 train_loss:4.2802 train_time:36291ms step_avg:295.05ms
step:134/1700 train_loss:4.4321 train_time:36592ms step_avg:295.10ms
step:135/1700 train_loss:4.2789 train_time:36893ms step_avg:295.14ms
step:136/1700 train_loss:4.2866 train_time:37193ms step_avg:295.18ms
step:137/1700 train_loss:4.3398 train_time:37493ms step_avg:295.22ms
step:138/1700 train_loss:4.3607 train_time:37793ms step_avg:295.26ms
step:139/1700 train_loss:4.4779 train_time:38093ms step_avg:295.30ms
step:140/1700 train_loss:4.3591 train_time:38393ms step_avg:295.33ms
step:141/1700 train_loss:4.2449 train_time:38694ms step_avg:295.37ms
step:142/1700 train_loss:4.3727 train_time:38994ms step_avg:295.41ms
step:143/1700 train_loss:4.4678 train_time:39294ms step_avg:295.45ms
step:144/1700 train_loss:4.5261 train_time:39595ms step_avg:295.48ms
step:145/1700 train_loss:4.2923 train_time:39895ms step_avg:295.52ms
step:146/1700 train_loss:4.3286 train_time:40195ms step_avg:295.55ms
step:147/1700 train_loss:4.3587 train_time:40495ms step_avg:295.58ms
step:148/1700 train_loss:4.1513 train_time:40795ms step_avg:295.62ms
step:149/1700 train_loss:4.3162 train_time:41096ms step_avg:295.65ms
step:150/1700 train_loss:4.2721 train_time:41395ms step_avg:295.68ms
step:151/1700 train_loss:4.2803 train_time:41695ms step_avg:295.71ms
step:152/1700 train_loss:4.1788 train_time:41995ms step_avg:295.74ms
step:153/1700 train_loss:4.3592 train_time:42295ms step_avg:295.77ms
step:154/1700 train_loss:4.1425 train_time:42596ms step_avg:295.80ms
step:155/1700 train_loss:4.1490 train_time:42895ms step_avg:295.83ms
step:156/1700 train_loss:4.2861 train_time:43196ms step_avg:295.86ms
step:157/1700 train_loss:4.3549 train_time:43496ms step_avg:295.89ms
step:158/1700 train_loss:4.2553 train_time:43797ms step_avg:295.92ms
step:159/1700 train_loss:4.2287 train_time:44097ms step_avg:295.95ms
step:160/1700 train_loss:4.1668 train_time:44399ms step_avg:295.99ms
step:161/1700 train_loss:4.2230 train_time:44699ms step_avg:296.02ms
step:162/1700 train_loss:4.2415 train_time:45001ms step_avg:296.06ms
step:163/1700 train_loss:4.2075 train_time:45302ms step_avg:296.09ms
step:164/1700 train_loss:4.1503 train_time:45603ms step_avg:296.13ms
step:165/1700 train_loss:4.2279 train_time:45905ms step_avg:296.16ms
step:166/1700 train_loss:4.3600 train_time:46208ms step_avg:296.21ms
step:167/1700 train_loss:4.2883 train_time:46510ms step_avg:296.24ms
step:168/1700 train_loss:4.1966 train_time:46811ms step_avg:296.27ms
step:169/1700 train_loss:4.2499 train_time:47112ms step_avg:296.30ms
step:170/1700 train_loss:4.2994 train_time:47414ms step_avg:296.33ms
step:171/1700 train_loss:3.8032 train_time:47714ms step_avg:296.36ms
step:172/1700 train_loss:4.1356 train_time:48014ms step_avg:296.39ms
step:173/1700 train_loss:4.1527 train_time:48316ms step_avg:296.42ms
step:174/1700 train_loss:4.3385 train_time:48616ms step_avg:296.44ms
step:175/1700 train_loss:4.1529 train_time:48918ms step_avg:296.47ms
step:176/1700 train_loss:4.2185 train_time:49218ms step_avg:296.50ms
step:177/1700 train_loss:4.3471 train_time:49518ms step_avg:296.52ms
step:178/1700 train_loss:4.2185 train_time:49821ms step_avg:296.55ms
step:179/1700 train_loss:4.1628 train_time:50123ms step_avg:296.59ms
step:180/1700 train_loss:4.2148 train_time:50424ms step_avg:296.61ms
step:181/1700 train_loss:4.1170 train_time:50726ms step_avg:296.64ms
step:182/1700 train_loss:4.1616 train_time:51029ms step_avg:296.68ms
step:183/1700 train_loss:4.1217 train_time:51332ms step_avg:296.72ms
step:184/1700 train_loss:4.2692 train_time:51632ms step_avg:296.74ms
step:185/1700 train_loss:4.1672 train_time:51932ms step_avg:296.76ms
step:186/1700 train_loss:4.2792 train_time:52234ms step_avg:296.78ms
step:187/1700 train_loss:4.1949 train_time:52534ms step_avg:296.80ms
step:188/1700 train_loss:4.1578 train_time:52835ms step_avg:296.83ms
step:189/1700 train_loss:4.0041 train_time:53136ms step_avg:296.85ms
step:190/1700 train_loss:4.1058 train_time:53610ms step_avg:297.83ms
step:191/1700 train_loss:4.0974 train_time:53911ms step_avg:297.85ms
step:192/1700 train_loss:4.0300 train_time:54212ms step_avg:297.87ms
step:193/1700 train_loss:4.2544 train_time:54514ms step_avg:297.89ms
step:194/1700 train_loss:4.1767 train_time:54814ms step_avg:297.90ms
step:195/1700 train_loss:4.3670 train_time:55115ms step_avg:297.92ms
step:196/1700 train_loss:4.1888 train_time:55415ms step_avg:297.93ms
step:197/1700 train_loss:4.0541 train_time:55717ms step_avg:297.95ms
step:198/1700 train_loss:4.1804 train_time:56017ms step_avg:297.96ms
step:199/1700 train_loss:4.0365 train_time:56319ms step_avg:297.98ms
step:200/1700 train_loss:4.1209 train_time:56619ms step_avg:298.00ms
step:201/1700 train_loss:3.9922 train_time:56923ms step_avg:298.02ms
step:202/1700 train_loss:4.2511 train_time:57225ms step_avg:298.05ms
step:203/1700 train_loss:4.0585 train_time:57527ms step_avg:298.07ms
step:204/1700 train_loss:4.1914 train_time:57829ms step_avg:298.09ms
step:205/1700 train_loss:4.2468 train_time:58132ms step_avg:298.11ms
step:206/1700 train_loss:3.9494 train_time:58434ms step_avg:298.13ms
step:207/1700 train_loss:4.0927 train_time:58735ms step_avg:298.15ms
step:208/1700 train_loss:4.1207 train_time:59035ms step_avg:298.16ms
step:209/1700 train_loss:4.2495 train_time:59336ms step_avg:298.17ms
step:210/1700 train_loss:4.1931 train_time:59637ms step_avg:298.18ms
step:211/1700 train_loss:4.0626 train_time:59938ms step_avg:298.20ms
step:212/1700 train_loss:4.1182 train_time:60239ms step_avg:298.21ms
step:213/1700 train_loss:4.0487 train_time:60540ms step_avg:298.23ms
step:214/1700 train_loss:4.1195 train_time:60843ms step_avg:298.25ms
step:215/1700 train_loss:3.9660 train_time:61146ms step_avg:298.27ms
step:216/1700 train_loss:4.0030 train_time:61449ms step_avg:298.29ms
step:217/1700 train_loss:4.0099 train_time:61750ms step_avg:298.31ms
step:218/1700 train_loss:4.0896 train_time:62050ms step_avg:298.32ms
step:219/1700 train_loss:4.0728 train_time:62352ms step_avg:298.33ms
step:220/1700 train_loss:4.0841 train_time:62652ms step_avg:298.34ms
step:221/1700 train_loss:4.0903 train_time:62953ms step_avg:298.36ms
step:222/1700 train_loss:3.9945 train_time:63254ms step_avg:298.37ms
step:223/1700 train_loss:3.9850 train_time:63555ms step_avg:298.38ms
step:224/1700 train_loss:4.3005 train_time:63854ms step_avg:298.38ms
step:225/1700 train_loss:3.9036 train_time:64153ms step_avg:298.38ms
step:226/1700 train_loss:3.9878 train_time:64452ms step_avg:298.39ms
step:227/1700 train_loss:3.9837 train_time:64751ms step_avg:298.39ms
step:228/1700 train_loss:4.1420 train_time:65051ms step_avg:298.40ms
step:229/1700 train_loss:3.9263 train_time:65351ms step_avg:298.41ms
step:230/1700 train_loss:4.0484 train_time:65650ms step_avg:298.41ms
step:231/1700 train_loss:3.9029 train_time:65958ms step_avg:298.45ms
step:232/1700 train_loss:3.9765 train_time:66266ms step_avg:298.49ms
step:233/1700 train_loss:4.0892 train_time:66573ms step_avg:298.54ms
step:234/1700 train_loss:4.0392 train_time:66880ms step_avg:298.57ms
step:235/1700 train_loss:3.9261 train_time:67189ms step_avg:298.62ms
step:236/1700 train_loss:4.1013 train_time:67494ms step_avg:298.64ms
step:237/1700 train_loss:4.0872 train_time:67800ms step_avg:298.68ms
step:238/1700 train_loss:3.9375 train_time:68108ms step_avg:298.72ms
step:239/1700 train_loss:4.0701 train_time:68415ms step_avg:298.76ms
step:240/1700 train_loss:4.1293 train_time:68722ms step_avg:298.79ms
step:241/1700 train_loss:3.9753 train_time:69030ms step_avg:298.83ms
step:242/1700 train_loss:4.1374 train_time:69337ms step_avg:298.87ms
step:243/1700 train_loss:4.0137 train_time:69644ms step_avg:298.90ms
step:244/1700 train_loss:4.0756 train_time:69951ms step_avg:298.93ms
step:245/1700 train_loss:4.1451 train_time:70257ms step_avg:298.96ms
step:246/1700 train_loss:4.0555 train_time:70564ms step_avg:299.00ms
step:247/1700 train_loss:3.9973 train_time:70871ms step_avg:299.03ms
step:248/1700 train_loss:4.1097 train_time:71177ms step_avg:299.06ms
step:249/1700 train_loss:3.9326 train_time:71484ms step_avg:299.09ms
step:250/1700 train_loss:3.9742 train_time:71790ms step_avg:299.13ms
step:250/1700 val_loss:4.0038 train_time:71799ms step_avg:299.16ms
step:251/1700 train_loss:4.0712 train_time:72100ms step_avg:299.17ms
step:252/1700 train_loss:4.1588 train_time:72407ms step_avg:299.20ms
step:253/1700 train_loss:3.9298 train_time:72715ms step_avg:299.24ms
step:254/1700 train_loss:3.8780 train_time:73021ms step_avg:299.26ms
step:255/1700 train_loss:4.0772 train_time:73327ms step_avg:299.30ms
step:256/1700 train_loss:3.9841 train_time:73633ms step_avg:299.32ms
step:257/1700 train_loss:3.9935 train_time:73939ms step_avg:299.35ms
step:258/1700 train_loss:3.9812 train_time:74246ms step_avg:299.38ms
step:259/1700 train_loss:4.0216 train_time:74552ms step_avg:299.41ms
step:260/1700 train_loss:4.0691 train_time:74861ms step_avg:299.44ms
step:261/1700 train_loss:4.0115 train_time:75167ms step_avg:299.47ms
step:262/1700 train_loss:3.9933 train_time:75474ms step_avg:299.50ms
step:263/1700 train_loss:3.8967 train_time:75781ms step_avg:299.53ms
step:264/1700 train_loss:3.9946 train_time:76087ms step_avg:299.56ms
step:265/1700 train_loss:3.8734 train_time:76395ms step_avg:299.59ms
step:266/1700 train_loss:3.9230 train_time:76703ms step_avg:299.62ms
step:267/1700 train_loss:3.9329 train_time:77010ms step_avg:299.65ms
step:268/1700 train_loss:3.9521 train_time:77317ms step_avg:299.68ms
step:269/1700 train_loss:3.8522 train_time:77623ms step_avg:299.70ms
step:270/1700 train_loss:4.1025 train_time:77930ms step_avg:299.73ms
step:271/1700 train_loss:3.9723 train_time:78237ms step_avg:299.76ms
step:272/1700 train_loss:3.9238 train_time:78544ms step_avg:299.78ms
step:273/1700 train_loss:3.9499 train_time:78849ms step_avg:299.80ms
step:274/1700 train_loss:4.0369 train_time:79158ms step_avg:299.84ms
step:275/1700 train_loss:4.0603 train_time:79465ms step_avg:299.87ms
step:276/1700 train_loss:4.2287 train_time:79771ms step_avg:299.89ms
step:277/1700 train_loss:4.0338 train_time:80079ms step_avg:299.92ms
step:278/1700 train_loss:4.0803 train_time:80386ms step_avg:299.95ms
step:279/1700 train_loss:3.9984 train_time:80692ms step_avg:299.97ms
step:280/1700 train_loss:4.1815 train_time:81002ms step_avg:300.01ms
step:281/1700 train_loss:3.9711 train_time:81309ms step_avg:300.03ms
step:282/1700 train_loss:3.9526 train_time:81617ms step_avg:300.06ms
step:283/1700 train_loss:3.9101 train_time:81924ms step_avg:300.09ms
step:284/1700 train_loss:4.0481 train_time:82232ms step_avg:300.12ms
step:285/1700 train_loss:4.0638 train_time:82539ms step_avg:300.14ms
step:286/1700 train_loss:4.0975 train_time:82845ms step_avg:300.16ms
step:287/1700 train_loss:3.9114 train_time:83151ms step_avg:300.19ms
step:288/1700 train_loss:4.0138 train_time:83458ms step_avg:300.21ms
step:289/1700 train_loss:3.8792 train_time:83766ms step_avg:300.24ms
step:290/1700 train_loss:3.8570 train_time:84074ms step_avg:300.27ms
step:291/1700 train_loss:3.9321 train_time:84382ms step_avg:300.29ms
step:292/1700 train_loss:3.8611 train_time:84688ms step_avg:300.31ms
step:293/1700 train_loss:3.9114 train_time:84995ms step_avg:300.34ms
step:294/1700 train_loss:3.9428 train_time:85303ms step_avg:300.36ms
step:295/1700 train_loss:3.8516 train_time:85610ms step_avg:300.38ms
step:296/1700 train_loss:3.8744 train_time:85918ms step_avg:300.41ms
step:297/1700 train_loss:3.8767 train_time:86225ms step_avg:300.44ms
step:298/1700 train_loss:3.9783 train_time:86530ms step_avg:300.45ms
step:299/1700 train_loss:3.8303 train_time:86840ms step_avg:300.48ms
step:300/1700 train_loss:3.9681 train_time:87147ms step_avg:300.51ms
step:301/1700 train_loss:3.9666 train_time:87454ms step_avg:300.53ms
step:302/1700 train_loss:3.9392 train_time:87759ms step_avg:300.55ms
step:303/1700 train_loss:3.9859 train_time:88066ms step_avg:300.57ms
step:304/1700 train_loss:3.9684 train_time:88372ms step_avg:300.59ms
step:305/1700 train_loss:4.4677 train_time:88680ms step_avg:300.61ms
step:306/1700 train_loss:3.9387 train_time:88986ms step_avg:300.63ms
step:307/1700 train_loss:3.8363 train_time:89293ms step_avg:300.65ms
step:308/1700 train_loss:3.9858 train_time:89601ms step_avg:300.68ms
step:309/1700 train_loss:3.8739 train_time:89908ms step_avg:300.70ms
step:310/1700 train_loss:4.0843 train_time:90215ms step_avg:300.72ms
step:311/1700 train_loss:3.9197 train_time:90522ms step_avg:300.74ms
step:312/1700 train_loss:3.8620 train_time:90830ms step_avg:300.76ms
step:313/1700 train_loss:3.9396 train_time:91137ms step_avg:300.78ms
step:314/1700 train_loss:4.0568 train_time:91444ms step_avg:300.80ms
step:315/1700 train_loss:3.9436 train_time:91751ms step_avg:300.82ms
step:316/1700 train_loss:3.7952 train_time:92060ms step_avg:300.85ms
step:317/1700 train_loss:3.8696 train_time:92370ms step_avg:300.88ms
step:318/1700 train_loss:3.9262 train_time:92676ms step_avg:300.90ms
step:319/1700 train_loss:3.8879 train_time:92983ms step_avg:300.92ms
step:320/1700 train_loss:4.0201 train_time:93290ms step_avg:300.94ms
step:321/1700 train_loss:3.9515 train_time:93596ms step_avg:300.95ms
step:322/1700 train_loss:3.9311 train_time:93905ms step_avg:300.98ms
step:323/1700 train_loss:4.0067 train_time:94210ms step_avg:300.99ms
step:324/1700 train_loss:3.9477 train_time:94517ms step_avg:301.01ms
step:325/1700 train_loss:4.0155 train_time:94823ms step_avg:301.03ms
step:326/1700 train_loss:3.8818 train_time:95130ms step_avg:301.05ms
step:327/1700 train_loss:4.3984 train_time:95438ms step_avg:301.07ms
step:328/1700 train_loss:4.0737 train_time:95746ms step_avg:301.09ms
step:329/1700 train_loss:3.8072 train_time:96056ms step_avg:301.12ms
step:330/1700 train_loss:3.7443 train_time:96363ms step_avg:301.13ms
step:331/1700 train_loss:3.9786 train_time:96670ms step_avg:301.15ms
step:332/1700 train_loss:3.9152 train_time:96978ms step_avg:301.17ms
step:333/1700 train_loss:3.8849 train_time:97286ms step_avg:301.19ms
step:334/1700 train_loss:3.8393 train_time:97592ms step_avg:301.21ms
step:335/1700 train_loss:4.0046 train_time:97900ms step_avg:301.23ms
step:336/1700 train_loss:3.9515 train_time:98207ms step_avg:301.25ms
step:337/1700 train_loss:4.4181 train_time:98514ms step_avg:301.27ms
step:338/1700 train_loss:3.9455 train_time:98821ms step_avg:301.28ms
step:339/1700 train_loss:3.8668 train_time:99128ms step_avg:301.30ms
step:340/1700 train_loss:3.9317 train_time:99433ms step_avg:301.31ms
step:341/1700 train_loss:3.8589 train_time:99739ms step_avg:301.33ms
step:342/1700 train_loss:3.8189 train_time:100044ms step_avg:301.34ms
step:343/1700 train_loss:3.8362 train_time:100350ms step_avg:301.35ms
step:344/1700 train_loss:4.0014 train_time:100655ms step_avg:301.36ms
step:345/1700 train_loss:3.8148 train_time:100963ms step_avg:301.38ms
step:346/1700 train_loss:3.7656 train_time:101273ms step_avg:301.41ms
step:347/1700 train_loss:3.8061 train_time:101586ms step_avg:301.44ms
step:348/1700 train_loss:3.8622 train_time:101898ms step_avg:301.47ms
step:349/1700 train_loss:3.8322 train_time:102210ms step_avg:301.50ms
step:350/1700 train_loss:3.5731 train_time:102521ms step_avg:301.53ms
step:351/1700 train_loss:3.8252 train_time:102835ms step_avg:301.57ms
step:352/1700 train_loss:4.1900 train_time:103146ms step_avg:301.60ms
step:353/1700 train_loss:3.6552 train_time:103458ms step_avg:301.63ms
step:354/1700 train_loss:3.9266 train_time:103767ms step_avg:301.65ms
step:355/1700 train_loss:3.7902 train_time:104080ms step_avg:301.68ms
step:356/1700 train_loss:3.8816 train_time:104391ms step_avg:301.71ms
step:357/1700 train_loss:3.7582 train_time:104703ms step_avg:301.74ms
step:358/1700 train_loss:3.8601 train_time:105014ms step_avg:301.76ms
step:359/1700 train_loss:3.8237 train_time:105326ms step_avg:301.79ms
step:360/1700 train_loss:3.4331 train_time:105642ms step_avg:301.83ms
step:361/1700 train_loss:4.0259 train_time:105955ms step_avg:301.86ms
step:362/1700 train_loss:3.9178 train_time:106265ms step_avg:301.89ms
step:363/1700 train_loss:3.8431 train_time:106576ms step_avg:301.92ms
step:364/1700 train_loss:3.7431 train_time:106888ms step_avg:301.94ms
step:365/1700 train_loss:3.9127 train_time:107200ms step_avg:301.97ms
step:366/1700 train_loss:3.8708 train_time:107514ms step_avg:302.01ms
step:367/1700 train_loss:3.8663 train_time:107824ms step_avg:302.03ms
step:368/1700 train_loss:3.8444 train_time:108135ms step_avg:302.05ms
step:369/1700 train_loss:3.7378 train_time:108446ms step_avg:302.08ms
step:370/1700 train_loss:3.8864 train_time:108757ms step_avg:302.10ms
step:371/1700 train_loss:3.7390 train_time:109067ms step_avg:302.13ms
step:372/1700 train_loss:3.6979 train_time:109380ms step_avg:302.16ms
step:373/1700 train_loss:3.9154 train_time:109691ms step_avg:302.18ms
step:374/1700 train_loss:3.8287 train_time:110002ms step_avg:302.20ms
step:375/1700 train_loss:3.7992 train_time:110312ms step_avg:302.23ms
step:375/1700 val_loss:3.8271 train_time:110321ms step_avg:302.25ms
step:376/1700 train_loss:3.8682 train_time:110628ms step_avg:302.26ms
step:377/1700 train_loss:3.7863 train_time:110941ms step_avg:302.29ms
step:378/1700 train_loss:3.8508 train_time:111255ms step_avg:302.32ms
step:379/1700 train_loss:3.8642 train_time:111567ms step_avg:302.35ms
step:380/1700 train_loss:3.9432 train_time:112057ms step_avg:302.86ms
step:381/1700 train_loss:3.6891 train_time:112548ms step_avg:303.36ms
step:382/1700 train_loss:3.7651 train_time:112861ms step_avg:303.39ms
step:383/1700 train_loss:3.7804 train_time:113170ms step_avg:303.41ms
step:384/1700 train_loss:3.8835 train_time:113482ms step_avg:303.43ms
step:385/1700 train_loss:3.6667 train_time:113794ms step_avg:303.45ms
step:386/1700 train_loss:3.8611 train_time:114105ms step_avg:303.47ms
step:387/1700 train_loss:3.7844 train_time:114416ms step_avg:303.49ms
step:388/1700 train_loss:3.9689 train_time:114727ms step_avg:303.51ms
step:389/1700 train_loss:3.8013 train_time:115037ms step_avg:303.53ms
step:390/1700 train_loss:3.8770 train_time:115351ms step_avg:303.56ms
step:391/1700 train_loss:3.7060 train_time:115663ms step_avg:303.58ms
step:392/1700 train_loss:3.7761 train_time:115972ms step_avg:303.59ms
step:393/1700 train_loss:3.8173 train_time:116283ms step_avg:303.61ms
step:394/1700 train_loss:3.7997 train_time:116595ms step_avg:303.63ms
step:395/1700 train_loss:3.8043 train_time:116905ms step_avg:303.65ms
step:396/1700 train_loss:3.7191 train_time:117219ms step_avg:303.68ms
step:397/1700 train_loss:3.5774 train_time:117531ms step_avg:303.70ms
step:398/1700 train_loss:3.8264 train_time:117842ms step_avg:303.72ms
step:399/1700 train_loss:3.7945 train_time:118152ms step_avg:303.73ms
step:400/1700 train_loss:3.7113 train_time:118463ms step_avg:303.75ms
step:401/1700 train_loss:3.8113 train_time:118775ms step_avg:303.77ms
step:402/1700 train_loss:3.6934 train_time:119086ms step_avg:303.79ms
step:403/1700 train_loss:3.9909 train_time:119399ms step_avg:303.82ms
step:404/1700 train_loss:3.8804 train_time:119714ms step_avg:303.84ms
step:405/1700 train_loss:3.8422 train_time:120025ms step_avg:303.86ms
step:406/1700 train_loss:3.8641 train_time:120337ms step_avg:303.88ms
step:407/1700 train_loss:3.8376 train_time:120648ms step_avg:303.90ms
step:408/1700 train_loss:3.7437 train_time:120959ms step_avg:303.92ms
step:409/1700 train_loss:3.8108 train_time:121268ms step_avg:303.93ms
step:410/1700 train_loss:3.7564 train_time:121579ms step_avg:303.95ms
step:411/1700 train_loss:3.7631 train_time:121891ms step_avg:303.97ms
step:412/1700 train_loss:3.7692 train_time:122202ms step_avg:303.98ms
step:413/1700 train_loss:3.7595 train_time:122514ms step_avg:304.00ms
step:414/1700 train_loss:3.8829 train_time:122825ms step_avg:304.02ms
step:415/1700 train_loss:3.7249 train_time:123136ms step_avg:304.04ms
step:416/1700 train_loss:3.7947 train_time:123446ms step_avg:304.05ms
step:417/1700 train_loss:3.8964 train_time:123759ms step_avg:304.07ms
step:418/1700 train_loss:3.6718 train_time:124069ms step_avg:304.09ms
step:419/1700 train_loss:3.9256 train_time:124381ms step_avg:304.11ms
step:420/1700 train_loss:4.0017 train_time:124694ms step_avg:304.13ms
step:421/1700 train_loss:3.7626 train_time:125004ms step_avg:304.14ms
step:422/1700 train_loss:3.8877 train_time:125315ms step_avg:304.16ms
step:423/1700 train_loss:3.5853 train_time:125626ms step_avg:304.18ms
step:424/1700 train_loss:3.7820 train_time:125936ms step_avg:304.19ms
step:425/1700 train_loss:3.6486 train_time:126248ms step_avg:304.21ms
step:426/1700 train_loss:3.8626 train_time:126561ms step_avg:304.23ms
step:427/1700 train_loss:3.8348 train_time:126872ms step_avg:304.25ms
step:428/1700 train_loss:3.7684 train_time:127184ms step_avg:304.27ms
step:429/1700 train_loss:3.8741 train_time:127495ms step_avg:304.28ms
step:430/1700 train_loss:3.6957 train_time:127805ms step_avg:304.30ms
step:431/1700 train_loss:3.6290 train_time:128119ms step_avg:304.32ms
step:432/1700 train_loss:3.8355 train_time:128430ms step_avg:304.34ms
step:433/1700 train_loss:3.8679 train_time:128741ms step_avg:304.35ms
step:434/1700 train_loss:3.8471 train_time:129050ms step_avg:304.36ms
step:435/1700 train_loss:3.7522 train_time:129364ms step_avg:304.38ms
step:436/1700 train_loss:3.8308 train_time:129674ms step_avg:304.40ms
step:437/1700 train_loss:3.8295 train_time:129984ms step_avg:304.41ms
step:438/1700 train_loss:3.7948 train_time:130296ms step_avg:304.43ms
step:439/1700 train_loss:3.8570 train_time:130606ms step_avg:304.44ms
step:440/1700 train_loss:3.6997 train_time:130918ms step_avg:304.46ms
step:441/1700 train_loss:3.8044 train_time:131229ms step_avg:304.48ms
step:442/1700 train_loss:3.7212 train_time:131539ms step_avg:304.49ms
step:443/1700 train_loss:3.6232 train_time:131852ms step_avg:304.51ms
step:444/1700 train_loss:3.7676 train_time:132163ms step_avg:304.52ms
step:445/1700 train_loss:4.0173 train_time:132474ms step_avg:304.54ms
step:446/1700 train_loss:3.6333 train_time:132784ms step_avg:304.55ms
step:447/1700 train_loss:3.8310 train_time:133096ms step_avg:304.57ms
step:448/1700 train_loss:3.8817 train_time:133407ms step_avg:304.58ms
step:449/1700 train_loss:3.7088 train_time:133720ms step_avg:304.60ms
step:450/1700 train_loss:3.6665 train_time:134032ms step_avg:304.62ms
step:451/1700 train_loss:3.7262 train_time:134346ms step_avg:304.64ms
step:452/1700 train_loss:4.0390 train_time:134657ms step_avg:304.65ms
step:453/1700 train_loss:3.9443 train_time:134968ms step_avg:304.67ms
step:454/1700 train_loss:3.8019 train_time:135280ms step_avg:304.68ms
step:455/1700 train_loss:3.7074 train_time:135591ms step_avg:304.70ms
step:456/1700 train_loss:3.8194 train_time:135900ms step_avg:304.71ms
step:457/1700 train_loss:3.7511 train_time:136209ms step_avg:304.72ms
step:458/1700 train_loss:3.7671 train_time:136519ms step_avg:304.73ms
step:459/1700 train_loss:3.8648 train_time:136828ms step_avg:304.74ms
step:460/1700 train_loss:3.6679 train_time:137141ms step_avg:304.76ms
step:461/1700 train_loss:3.7862 train_time:137456ms step_avg:304.78ms
step:462/1700 train_loss:3.7553 train_time:137772ms step_avg:304.81ms
step:463/1700 train_loss:3.5889 train_time:138085ms step_avg:304.82ms
step:464/1700 train_loss:3.7408 train_time:138401ms step_avg:304.85ms
step:465/1700 train_loss:3.8296 train_time:138715ms step_avg:304.87ms
step:466/1700 train_loss:3.7187 train_time:139030ms step_avg:304.89ms
step:467/1700 train_loss:3.7265 train_time:139344ms step_avg:304.91ms
step:468/1700 train_loss:3.6992 train_time:139662ms step_avg:304.94ms
step:469/1700 train_loss:3.9132 train_time:139975ms step_avg:304.96ms
step:470/1700 train_loss:3.7479 train_time:140288ms step_avg:304.97ms
step:471/1700 train_loss:3.6154 train_time:140605ms step_avg:305.00ms
step:472/1700 train_loss:3.8248 train_time:140921ms step_avg:305.02ms
step:473/1700 train_loss:3.6777 train_time:141234ms step_avg:305.04ms
step:474/1700 train_loss:3.8034 train_time:141550ms step_avg:305.06ms
step:475/1700 train_loss:3.8656 train_time:141863ms step_avg:305.08ms
step:476/1700 train_loss:4.0335 train_time:142179ms step_avg:305.11ms
step:477/1700 train_loss:3.8209 train_time:142494ms step_avg:305.13ms
step:478/1700 train_loss:3.7871 train_time:142811ms step_avg:305.15ms
step:479/1700 train_loss:3.7386 train_time:143127ms step_avg:305.17ms
step:480/1700 train_loss:3.6853 train_time:143443ms step_avg:305.20ms
step:481/1700 train_loss:3.7198 train_time:143757ms step_avg:305.22ms
step:482/1700 train_loss:3.8347 train_time:144072ms step_avg:305.24ms
step:483/1700 train_loss:3.7551 train_time:144385ms step_avg:305.25ms
step:484/1700 train_loss:3.8348 train_time:144701ms step_avg:305.28ms
step:485/1700 train_loss:3.7699 train_time:145018ms step_avg:305.30ms
step:486/1700 train_loss:3.6005 train_time:145335ms step_avg:305.33ms
step:487/1700 train_loss:3.7384 train_time:145652ms step_avg:305.35ms
step:488/1700 train_loss:3.7437 train_time:145967ms step_avg:305.37ms
step:489/1700 train_loss:3.7536 train_time:146280ms step_avg:305.39ms
step:490/1700 train_loss:3.9629 train_time:146598ms step_avg:305.41ms
step:491/1700 train_loss:3.6976 train_time:146913ms step_avg:305.43ms
step:492/1700 train_loss:3.6529 train_time:147226ms step_avg:305.45ms
step:493/1700 train_loss:3.7829 train_time:147540ms step_avg:305.47ms
step:494/1700 train_loss:3.5562 train_time:147856ms step_avg:305.49ms
step:495/1700 train_loss:3.7158 train_time:148173ms step_avg:305.51ms
step:496/1700 train_loss:3.8964 train_time:148492ms step_avg:305.54ms
step:497/1700 train_loss:3.7413 train_time:148808ms step_avg:305.56ms
step:498/1700 train_loss:3.7214 train_time:149125ms step_avg:305.58ms
step:499/1700 train_loss:3.6959 train_time:149436ms step_avg:305.60ms
step:500/1700 train_loss:3.8049 train_time:149752ms step_avg:305.62ms
step:500/1700 val_loss:3.7124 train_time:149761ms step_avg:305.63ms
step:501/1700 train_loss:3.7052 train_time:150076ms step_avg:305.65ms
step:502/1700 train_loss:3.6452 train_time:150390ms step_avg:305.67ms
step:503/1700 train_loss:3.7596 train_time:150706ms step_avg:305.69ms
step:504/1700 train_loss:3.6136 train_time:151022ms step_avg:305.71ms
step:505/1700 train_loss:4.0330 train_time:151336ms step_avg:305.73ms
step:506/1700 train_loss:3.6877 train_time:151650ms step_avg:305.75ms
step:507/1700 train_loss:3.7746 train_time:151968ms step_avg:305.77ms
step:508/1700 train_loss:3.9441 train_time:152283ms step_avg:305.79ms
step:509/1700 train_loss:3.6521 train_time:152599ms step_avg:305.81ms
step:510/1700 train_loss:3.7491 train_time:152913ms step_avg:305.83ms
step:511/1700 train_loss:3.7418 train_time:153231ms step_avg:305.85ms
step:512/1700 train_loss:3.5669 train_time:153549ms step_avg:305.87ms
step:513/1700 train_loss:3.5607 train_time:153867ms step_avg:305.90ms
step:514/1700 train_loss:3.7944 train_time:154180ms step_avg:305.91ms
step:515/1700 train_loss:3.9151 train_time:154499ms step_avg:305.94ms
step:516/1700 train_loss:3.7911 train_time:154814ms step_avg:305.96ms
step:517/1700 train_loss:3.6315 train_time:155132ms step_avg:305.98ms
step:518/1700 train_loss:3.7915 train_time:155446ms step_avg:306.00ms
step:519/1700 train_loss:3.5311 train_time:155761ms step_avg:306.01ms
step:520/1700 train_loss:3.7934 train_time:156078ms step_avg:306.03ms
step:521/1700 train_loss:3.6536 train_time:156395ms step_avg:306.06ms
step:522/1700 train_loss:3.5622 train_time:156710ms step_avg:306.07ms
step:523/1700 train_loss:3.8083 train_time:157026ms step_avg:306.09ms
step:524/1700 train_loss:3.6092 train_time:157342ms step_avg:306.11ms
step:525/1700 train_loss:3.6732 train_time:157656ms step_avg:306.13ms
step:526/1700 train_loss:3.7181 train_time:157972ms step_avg:306.15ms
step:527/1700 train_loss:3.9691 train_time:158286ms step_avg:306.16ms
step:528/1700 train_loss:3.6944 train_time:158600ms step_avg:306.18ms
step:529/1700 train_loss:3.6760 train_time:158915ms step_avg:306.19ms
step:530/1700 train_loss:3.6700 train_time:159231ms step_avg:306.21ms
step:531/1700 train_loss:3.7811 train_time:159548ms step_avg:306.23ms
step:532/1700 train_loss:3.7489 train_time:159864ms step_avg:306.25ms
step:533/1700 train_loss:3.7496 train_time:160179ms step_avg:306.27ms
step:534/1700 train_loss:3.8163 train_time:160495ms step_avg:306.29ms
step:535/1700 train_loss:3.7692 train_time:160816ms step_avg:306.32ms
step:536/1700 train_loss:3.6364 train_time:161132ms step_avg:306.34ms
step:537/1700 train_loss:3.7018 train_time:161449ms step_avg:306.35ms
step:538/1700 train_loss:3.6392 train_time:161764ms step_avg:306.37ms
step:539/1700 train_loss:3.6384 train_time:162081ms step_avg:306.39ms
step:540/1700 train_loss:3.7044 train_time:162398ms step_avg:306.41ms
step:541/1700 train_loss:3.6266 train_time:162716ms step_avg:306.43ms
step:542/1700 train_loss:3.6687 train_time:163031ms step_avg:306.45ms
step:543/1700 train_loss:3.7277 train_time:163347ms step_avg:306.47ms
step:544/1700 train_loss:3.7007 train_time:163662ms step_avg:306.48ms
step:545/1700 train_loss:3.7508 train_time:163977ms step_avg:306.50ms
step:546/1700 train_loss:3.7657 train_time:164292ms step_avg:306.51ms
step:547/1700 train_loss:3.6154 train_time:164606ms step_avg:306.53ms
step:548/1700 train_loss:3.8559 train_time:164922ms step_avg:306.55ms
step:549/1700 train_loss:3.3055 train_time:165238ms step_avg:306.56ms
step:550/1700 train_loss:3.7385 train_time:165554ms step_avg:306.58ms
step:551/1700 train_loss:3.7409 train_time:165872ms step_avg:306.60ms
step:552/1700 train_loss:3.6761 train_time:166186ms step_avg:306.62ms
step:553/1700 train_loss:3.7727 train_time:166501ms step_avg:306.63ms
step:554/1700 train_loss:3.6850 train_time:166818ms step_avg:306.65ms
step:555/1700 train_loss:3.6760 train_time:167133ms step_avg:306.67ms
step:556/1700 train_loss:3.8000 train_time:167449ms step_avg:306.68ms
step:557/1700 train_loss:3.7027 train_time:167762ms step_avg:306.69ms
step:558/1700 train_loss:3.6291 train_time:168078ms step_avg:306.71ms
step:559/1700 train_loss:3.7281 train_time:168393ms step_avg:306.73ms
step:560/1700 train_loss:3.6329 train_time:168708ms step_avg:306.74ms
step:561/1700 train_loss:3.6777 train_time:169022ms step_avg:306.76ms
step:562/1700 train_loss:3.6782 train_time:169335ms step_avg:306.77ms
step:563/1700 train_loss:3.4879 train_time:169651ms step_avg:306.78ms
step:564/1700 train_loss:3.7391 train_time:169965ms step_avg:306.80ms
step:565/1700 train_loss:3.6133 train_time:170281ms step_avg:306.81ms
step:566/1700 train_loss:3.6564 train_time:170596ms step_avg:306.83ms
step:567/1700 train_loss:3.7258 train_time:170914ms step_avg:306.85ms
step:568/1700 train_loss:3.6717 train_time:171229ms step_avg:306.86ms
step:569/1700 train_loss:3.9999 train_time:171542ms step_avg:306.87ms
step:570/1700 train_loss:3.7064 train_time:172033ms step_avg:307.20ms
step:571/1700 train_loss:3.6650 train_time:172452ms step_avg:307.40ms
step:572/1700 train_loss:3.7634 train_time:172764ms step_avg:307.41ms
step:573/1700 train_loss:3.7334 train_time:173077ms step_avg:307.42ms
step:574/1700 train_loss:3.7438 train_time:173393ms step_avg:307.43ms
step:575/1700 train_loss:3.7854 train_time:173719ms step_avg:307.47ms
step:576/1700 train_loss:3.7399 train_time:174038ms step_avg:307.49ms
step:577/1700 train_loss:3.7663 train_time:174356ms step_avg:307.51ms
step:578/1700 train_loss:3.6823 train_time:174675ms step_avg:307.53ms
step:579/1700 train_loss:3.6855 train_time:174994ms step_avg:307.55ms
step:580/1700 train_loss:3.6840 train_time:175313ms step_avg:307.57ms
step:581/1700 train_loss:3.6026 train_time:175632ms step_avg:307.59ms
step:582/1700 train_loss:3.6451 train_time:175951ms step_avg:307.61ms
step:583/1700 train_loss:3.8553 train_time:176273ms step_avg:307.63ms
step:584/1700 train_loss:3.6395 train_time:176591ms step_avg:307.65ms
step:585/1700 train_loss:3.6002 train_time:176910ms step_avg:307.67ms
step:586/1700 train_loss:3.8011 train_time:177226ms step_avg:307.68ms
step:587/1700 train_loss:3.5246 train_time:177545ms step_avg:307.70ms
step:588/1700 train_loss:3.6789 train_time:177863ms step_avg:307.72ms
step:589/1700 train_loss:3.6578 train_time:178180ms step_avg:307.74ms
step:590/1700 train_loss:4.0063 train_time:178500ms step_avg:307.76ms
step:591/1700 train_loss:3.7866 train_time:178820ms step_avg:307.78ms
step:592/1700 train_loss:3.5230 train_time:179136ms step_avg:307.79ms
step:593/1700 train_loss:3.5518 train_time:179459ms step_avg:307.82ms
step:594/1700 train_loss:3.5065 train_time:179780ms step_avg:307.84ms
step:595/1700 train_loss:3.5657 train_time:180101ms step_avg:307.86ms
step:596/1700 train_loss:3.9418 train_time:180423ms step_avg:307.89ms
step:597/1700 train_loss:3.6583 train_time:180742ms step_avg:307.91ms
step:598/1700 train_loss:3.5999 train_time:181058ms step_avg:307.92ms
step:599/1700 train_loss:3.6781 train_time:181377ms step_avg:307.94ms
step:600/1700 train_loss:3.4864 train_time:181695ms step_avg:307.96ms
step:601/1700 train_loss:3.6072 train_time:182014ms step_avg:307.98ms
step:602/1700 train_loss:3.6512 train_time:182333ms step_avg:307.99ms
step:603/1700 train_loss:3.6753 train_time:182654ms step_avg:308.02ms
step:604/1700 train_loss:3.7944 train_time:182978ms step_avg:308.04ms
step:605/1700 train_loss:3.6212 train_time:183296ms step_avg:308.06ms
step:606/1700 train_loss:3.6268 train_time:183614ms step_avg:308.08ms
step:607/1700 train_loss:3.5908 train_time:183937ms step_avg:308.10ms
step:608/1700 train_loss:3.8474 train_time:184256ms step_avg:308.12ms
step:609/1700 train_loss:3.6524 train_time:184577ms step_avg:308.14ms
step:610/1700 train_loss:3.6292 train_time:184896ms step_avg:308.16ms
step:611/1700 train_loss:3.7193 train_time:185214ms step_avg:308.18ms
step:612/1700 train_loss:3.6137 train_time:185532ms step_avg:308.19ms
step:613/1700 train_loss:3.5826 train_time:185852ms step_avg:308.21ms
step:614/1700 train_loss:3.7755 train_time:186176ms step_avg:308.24ms
step:615/1700 train_loss:3.7173 train_time:186494ms step_avg:308.25ms
step:616/1700 train_loss:3.7018 train_time:186811ms step_avg:308.27ms
step:617/1700 train_loss:3.6441 train_time:187130ms step_avg:308.29ms
step:618/1700 train_loss:3.5658 train_time:187449ms step_avg:308.30ms
step:619/1700 train_loss:3.6977 train_time:187767ms step_avg:308.32ms
step:620/1700 train_loss:3.5688 train_time:188086ms step_avg:308.34ms
step:621/1700 train_loss:3.5949 train_time:188405ms step_avg:308.36ms
step:622/1700 train_loss:3.9341 train_time:188723ms step_avg:308.37ms
step:623/1700 train_loss:3.5792 train_time:189043ms step_avg:308.39ms
step:624/1700 train_loss:3.6085 train_time:189362ms step_avg:308.41ms
step:625/1700 train_loss:3.7062 train_time:189681ms step_avg:308.43ms
step:625/1700 val_loss:3.6350 train_time:189690ms step_avg:308.44ms
step:626/1700 train_loss:3.7184 train_time:190002ms step_avg:308.44ms
step:627/1700 train_loss:3.7537 train_time:190322ms step_avg:308.46ms
step:628/1700 train_loss:3.7260 train_time:190642ms step_avg:308.48ms
step:629/1700 train_loss:3.7809 train_time:190958ms step_avg:308.49ms
step:630/1700 train_loss:3.6060 train_time:191276ms step_avg:308.51ms
step:631/1700 train_loss:3.7368 train_time:191596ms step_avg:308.53ms
step:632/1700 train_loss:3.7527 train_time:191917ms step_avg:308.55ms
step:633/1700 train_loss:3.6632 train_time:192236ms step_avg:308.56ms
step:634/1700 train_loss:3.6106 train_time:192554ms step_avg:308.58ms
step:635/1700 train_loss:3.7099 train_time:192876ms step_avg:308.60ms
step:636/1700 train_loss:3.9617 train_time:193194ms step_avg:308.62ms
step:637/1700 train_loss:3.5544 train_time:193513ms step_avg:308.63ms
step:638/1700 train_loss:3.3704 train_time:193834ms step_avg:308.65ms
step:639/1700 train_loss:3.6008 train_time:194154ms step_avg:308.67ms
step:640/1700 train_loss:3.6453 train_time:194471ms step_avg:308.68ms
step:641/1700 train_loss:3.5871 train_time:194792ms step_avg:308.70ms
step:642/1700 train_loss:3.5975 train_time:195110ms step_avg:308.72ms
step:643/1700 train_loss:3.6397 train_time:195430ms step_avg:308.74ms
step:644/1700 train_loss:3.6190 train_time:195752ms step_avg:308.76ms
step:645/1700 train_loss:3.5652 train_time:196067ms step_avg:308.77ms
step:646/1700 train_loss:3.7890 train_time:196388ms step_avg:308.79ms
step:647/1700 train_loss:3.6870 train_time:196710ms step_avg:308.81ms
step:648/1700 train_loss:3.6733 train_time:197026ms step_avg:308.82ms
step:649/1700 train_loss:3.7183 train_time:197355ms step_avg:308.85ms
step:650/1700 train_loss:3.7795 train_time:197674ms step_avg:308.87ms
step:651/1700 train_loss:3.6351 train_time:197996ms step_avg:308.89ms
step:652/1700 train_loss:3.7785 train_time:198316ms step_avg:308.90ms
step:653/1700 train_loss:3.5985 train_time:198637ms step_avg:308.92ms
step:654/1700 train_loss:3.6720 train_time:198954ms step_avg:308.94ms
step:655/1700 train_loss:3.4391 train_time:199275ms step_avg:308.95ms
step:656/1700 train_loss:3.5900 train_time:199593ms step_avg:308.97ms
step:657/1700 train_loss:3.5854 train_time:199913ms step_avg:308.98ms
step:658/1700 train_loss:3.5119 train_time:200233ms step_avg:309.00ms
step:659/1700 train_loss:3.6938 train_time:200553ms step_avg:309.02ms
step:660/1700 train_loss:3.5986 train_time:200873ms step_avg:309.04ms
step:661/1700 train_loss:3.6938 train_time:201191ms step_avg:309.05ms
step:662/1700 train_loss:3.7635 train_time:201514ms step_avg:309.07ms
step:663/1700 train_loss:3.6834 train_time:201832ms step_avg:309.08ms
step:664/1700 train_loss:3.5689 train_time:202147ms step_avg:309.09ms
step:665/1700 train_loss:3.6294 train_time:202469ms step_avg:309.11ms
step:666/1700 train_loss:3.5075 train_time:202792ms step_avg:309.13ms
step:667/1700 train_loss:3.7942 train_time:203108ms step_avg:309.14ms
step:668/1700 train_loss:3.6246 train_time:203429ms step_avg:309.16ms
step:669/1700 train_loss:3.6574 train_time:203751ms step_avg:309.18ms
step:670/1700 train_loss:3.4957 train_time:204072ms step_avg:309.20ms
step:671/1700 train_loss:3.6131 train_time:204394ms step_avg:309.22ms
step:672/1700 train_loss:3.5713 train_time:204713ms step_avg:309.23ms
step:673/1700 train_loss:3.5834 train_time:205030ms step_avg:309.25ms
step:674/1700 train_loss:3.8647 train_time:205351ms step_avg:309.26ms
step:675/1700 train_loss:3.6481 train_time:205671ms step_avg:309.28ms
step:676/1700 train_loss:3.7285 train_time:205992ms step_avg:309.30ms
step:677/1700 train_loss:3.5030 train_time:206315ms step_avg:309.32ms
step:678/1700 train_loss:3.6077 train_time:206634ms step_avg:309.33ms
step:679/1700 train_loss:3.5634 train_time:206951ms step_avg:309.34ms
step:680/1700 train_loss:3.6885 train_time:207272ms step_avg:309.36ms
step:681/1700 train_loss:3.5992 train_time:207594ms step_avg:309.38ms
step:682/1700 train_loss:3.6236 train_time:207910ms step_avg:309.39ms
step:683/1700 train_loss:3.6748 train_time:208228ms step_avg:309.40ms
step:684/1700 train_loss:3.7445 train_time:208547ms step_avg:309.42ms
step:685/1700 train_loss:3.6588 train_time:208868ms step_avg:309.43ms
step:686/1700 train_loss:3.7004 train_time:209186ms step_avg:309.45ms
step:687/1700 train_loss:3.6482 train_time:209501ms step_avg:309.46ms
step:688/1700 train_loss:3.6735 train_time:209820ms step_avg:309.47ms
step:689/1700 train_loss:3.2098 train_time:210143ms step_avg:309.49ms
step:690/1700 train_loss:3.4153 train_time:210462ms step_avg:309.50ms
step:691/1700 train_loss:3.5581 train_time:210786ms step_avg:309.52ms
step:692/1700 train_loss:3.4361 train_time:211106ms step_avg:309.54ms
step:693/1700 train_loss:3.6408 train_time:211428ms step_avg:309.56ms
step:694/1700 train_loss:3.6636 train_time:211750ms step_avg:309.58ms
step:695/1700 train_loss:3.5656 train_time:212072ms step_avg:309.59ms
step:696/1700 train_loss:3.5492 train_time:212396ms step_avg:309.61ms
step:697/1700 train_loss:3.8753 train_time:212718ms step_avg:309.63ms
step:698/1700 train_loss:3.6060 train_time:213039ms step_avg:309.65ms
step:699/1700 train_loss:3.6572 train_time:213360ms step_avg:309.67ms
step:700/1700 train_loss:3.7818 train_time:213682ms step_avg:309.68ms
step:701/1700 train_loss:3.5844 train_time:214001ms step_avg:309.70ms
step:702/1700 train_loss:3.5533 train_time:214320ms step_avg:309.71ms
step:703/1700 train_loss:3.5261 train_time:214642ms step_avg:309.73ms
step:704/1700 train_loss:3.5088 train_time:214963ms step_avg:309.75ms
step:705/1700 train_loss:3.5864 train_time:215293ms step_avg:309.77ms
step:706/1700 train_loss:3.5783 train_time:215617ms step_avg:309.79ms
step:707/1700 train_loss:3.5937 train_time:215943ms step_avg:309.82ms
step:708/1700 train_loss:3.6607 train_time:216265ms step_avg:309.83ms
step:709/1700 train_loss:3.6163 train_time:216589ms step_avg:309.86ms
step:710/1700 train_loss:3.5954 train_time:216911ms step_avg:309.87ms
step:711/1700 train_loss:3.5597 train_time:217234ms step_avg:309.89ms
step:712/1700 train_loss:3.6104 train_time:217559ms step_avg:309.91ms
step:713/1700 train_loss:3.6588 train_time:217879ms step_avg:309.93ms
step:714/1700 train_loss:3.6611 train_time:218203ms step_avg:309.95ms
step:715/1700 train_loss:3.5684 train_time:218525ms step_avg:309.96ms
step:716/1700 train_loss:3.5838 train_time:218845ms step_avg:309.98ms
step:717/1700 train_loss:3.6108 train_time:219167ms step_avg:310.00ms
step:718/1700 train_loss:3.7209 train_time:219489ms step_avg:310.01ms
step:719/1700 train_loss:3.6146 train_time:219810ms step_avg:310.03ms
step:720/1700 train_loss:3.6969 train_time:220132ms step_avg:310.04ms
step:721/1700 train_loss:3.8626 train_time:220460ms step_avg:310.07ms
step:722/1700 train_loss:3.4840 train_time:220779ms step_avg:310.08ms
step:723/1700 train_loss:3.7437 train_time:221102ms step_avg:310.10ms
step:724/1700 train_loss:3.7912 train_time:221421ms step_avg:310.11ms
step:725/1700 train_loss:3.5832 train_time:221742ms step_avg:310.13ms
step:726/1700 train_loss:3.6740 train_time:222072ms step_avg:310.16ms
step:727/1700 train_loss:3.5551 train_time:222394ms step_avg:310.17ms
step:728/1700 train_loss:3.5920 train_time:222717ms step_avg:310.19ms
step:729/1700 train_loss:3.7478 train_time:223038ms step_avg:310.21ms
step:730/1700 train_loss:3.6816 train_time:223360ms step_avg:310.22ms
step:731/1700 train_loss:3.6812 train_time:223683ms step_avg:310.24ms
step:732/1700 train_loss:3.5716 train_time:224003ms step_avg:310.25ms
step:733/1700 train_loss:3.6075 train_time:224321ms step_avg:310.26ms
step:734/1700 train_loss:3.8450 train_time:224642ms step_avg:310.28ms
step:735/1700 train_loss:3.5742 train_time:224962ms step_avg:310.29ms
step:736/1700 train_loss:3.6191 train_time:225286ms step_avg:310.31ms
step:737/1700 train_loss:3.7506 train_time:225607ms step_avg:310.33ms
step:738/1700 train_loss:3.6896 train_time:225928ms step_avg:310.34ms
step:739/1700 train_loss:3.6163 train_time:226248ms step_avg:310.35ms
step:740/1700 train_loss:3.5226 train_time:226570ms step_avg:310.37ms
step:741/1700 train_loss:4.1164 train_time:226902ms step_avg:310.40ms
step:742/1700 train_loss:3.5067 train_time:227222ms step_avg:310.41ms
step:743/1700 train_loss:3.5753 train_time:227544ms step_avg:310.43ms
step:744/1700 train_loss:3.5936 train_time:227872ms step_avg:310.45ms
step:745/1700 train_loss:3.6638 train_time:228195ms step_avg:310.47ms
step:746/1700 train_loss:3.6087 train_time:228519ms step_avg:310.49ms
step:747/1700 train_loss:3.6125 train_time:228838ms step_avg:310.50ms
step:748/1700 train_loss:3.6600 train_time:229160ms step_avg:310.51ms
step:749/1700 train_loss:3.5801 train_time:229484ms step_avg:310.53ms
step:750/1700 train_loss:3.5735 train_time:229812ms step_avg:310.56ms
step:750/1700 val_loss:3.5814 train_time:229821ms step_avg:310.57ms
step:751/1700 train_loss:3.6171 train_time:230137ms step_avg:310.58ms
step:752/1700 train_loss:3.5819 train_time:230458ms step_avg:310.59ms
step:753/1700 train_loss:3.6255 train_time:230779ms step_avg:310.60ms
step:754/1700 train_loss:3.6317 train_time:231101ms step_avg:310.62ms
step:755/1700 train_loss:3.6059 train_time:231418ms step_avg:310.63ms
step:756/1700 train_loss:3.6934 train_time:231742ms step_avg:310.65ms
step:757/1700 train_loss:3.4787 train_time:232064ms step_avg:310.66ms
step:758/1700 train_loss:3.7381 train_time:232394ms step_avg:310.69ms
step:759/1700 train_loss:3.6727 train_time:232716ms step_avg:310.70ms
step:760/1700 train_loss:3.6073 train_time:233213ms step_avg:310.95ms
step:761/1700 train_loss:3.7169 train_time:233531ms step_avg:310.96ms
step:762/1700 train_loss:3.6253 train_time:234033ms step_avg:311.21ms
step:763/1700 train_loss:3.4601 train_time:234357ms step_avg:311.23ms
step:764/1700 train_loss:3.4498 train_time:234679ms step_avg:311.25ms
step:765/1700 train_loss:3.5552 train_time:235001ms step_avg:311.26ms
step:766/1700 train_loss:3.5627 train_time:235322ms step_avg:311.27ms
step:767/1700 train_loss:4.6057 train_time:235648ms step_avg:311.29ms
step:768/1700 train_loss:3.5586 train_time:235971ms step_avg:311.31ms
step:769/1700 train_loss:3.6044 train_time:236292ms step_avg:311.32ms
step:770/1700 train_loss:3.6935 train_time:236614ms step_avg:311.33ms
step:771/1700 train_loss:4.1917 train_time:236938ms step_avg:311.35ms
step:772/1700 train_loss:3.6067 train_time:237261ms step_avg:311.37ms
step:773/1700 train_loss:3.6198 train_time:237580ms step_avg:311.38ms
step:774/1700 train_loss:3.5715 train_time:237901ms step_avg:311.39ms
step:775/1700 train_loss:3.7140 train_time:238220ms step_avg:311.40ms
step:776/1700 train_loss:3.5064 train_time:238542ms step_avg:311.41ms
step:777/1700 train_loss:3.6329 train_time:238863ms step_avg:311.43ms
step:778/1700 train_loss:3.6299 train_time:239188ms step_avg:311.44ms
step:779/1700 train_loss:3.5950 train_time:239514ms step_avg:311.46ms
step:780/1700 train_loss:3.5933 train_time:239835ms step_avg:311.47ms
step:781/1700 train_loss:3.4924 train_time:240160ms step_avg:311.49ms
step:782/1700 train_loss:3.6467 train_time:240481ms step_avg:311.50ms
step:783/1700 train_loss:3.5883 train_time:240802ms step_avg:311.52ms
step:784/1700 train_loss:3.5492 train_time:241122ms step_avg:311.53ms
step:785/1700 train_loss:3.5600 train_time:241442ms step_avg:311.54ms
step:786/1700 train_loss:3.5847 train_time:241765ms step_avg:311.55ms
step:787/1700 train_loss:3.5335 train_time:242085ms step_avg:311.56ms
step:788/1700 train_loss:3.6007 train_time:242407ms step_avg:311.58ms
step:789/1700 train_loss:3.5627 train_time:242736ms step_avg:311.60ms
step:790/1700 train_loss:3.4894 train_time:243056ms step_avg:311.61ms
step:791/1700 train_loss:3.5474 train_time:243380ms step_avg:311.63ms
step:792/1700 train_loss:3.6090 train_time:243703ms step_avg:311.64ms
step:793/1700 train_loss:3.6183 train_time:244023ms step_avg:311.65ms
step:794/1700 train_loss:3.6473 train_time:244350ms step_avg:311.67ms
step:795/1700 train_loss:3.5805 train_time:244674ms step_avg:311.69ms
step:796/1700 train_loss:3.6986 train_time:244999ms step_avg:311.70ms
step:797/1700 train_loss:3.6010 train_time:245321ms step_avg:311.72ms
step:798/1700 train_loss:3.4024 train_time:245643ms step_avg:311.73ms
step:799/1700 train_loss:3.4880 train_time:245964ms step_avg:311.74ms
step:800/1700 train_loss:4.2407 train_time:246291ms step_avg:311.76ms
step:801/1700 train_loss:3.7177 train_time:246613ms step_avg:311.77ms
step:802/1700 train_loss:3.5632 train_time:246932ms step_avg:311.78ms
step:803/1700 train_loss:3.6184 train_time:247255ms step_avg:311.80ms
step:804/1700 train_loss:3.6017 train_time:247577ms step_avg:311.81ms
step:805/1700 train_loss:3.5417 train_time:247900ms step_avg:311.82ms
step:806/1700 train_loss:3.5436 train_time:248228ms step_avg:311.84ms
step:807/1700 train_loss:3.5722 train_time:248555ms step_avg:311.86ms
step:808/1700 train_loss:3.6398 train_time:248877ms step_avg:311.88ms
step:809/1700 train_loss:3.8530 train_time:249200ms step_avg:311.89ms
step:810/1700 train_loss:3.6962 train_time:249522ms step_avg:311.90ms
step:811/1700 train_loss:3.4984 train_time:249848ms step_avg:311.92ms
step:812/1700 train_loss:3.6189 train_time:250175ms step_avg:311.94ms
step:813/1700 train_loss:3.6335 train_time:250497ms step_avg:311.95ms
step:814/1700 train_loss:3.5701 train_time:250819ms step_avg:311.96ms
step:815/1700 train_loss:3.4272 train_time:251145ms step_avg:311.98ms
step:816/1700 train_loss:3.7744 train_time:251467ms step_avg:311.99ms
step:817/1700 train_loss:3.5855 train_time:251790ms step_avg:312.01ms
step:818/1700 train_loss:3.5536 train_time:252117ms step_avg:312.03ms
step:819/1700 train_loss:3.5609 train_time:252441ms step_avg:312.04ms
step:820/1700 train_loss:3.5498 train_time:252761ms step_avg:312.05ms
step:821/1700 train_loss:3.4357 train_time:253085ms step_avg:312.07ms
step:822/1700 train_loss:3.5590 train_time:253411ms step_avg:312.08ms
step:823/1700 train_loss:3.6564 train_time:253731ms step_avg:312.09ms
step:824/1700 train_loss:3.3893 train_time:254057ms step_avg:312.11ms
step:825/1700 train_loss:3.6024 train_time:254381ms step_avg:312.12ms
step:826/1700 train_loss:3.7008 train_time:254706ms step_avg:312.14ms
step:827/1700 train_loss:3.4509 train_time:255035ms step_avg:312.16ms
step:828/1700 train_loss:3.5128 train_time:255361ms step_avg:312.18ms
step:829/1700 train_loss:3.5214 train_time:255684ms step_avg:312.19ms
step:830/1700 train_loss:3.6205 train_time:256005ms step_avg:312.20ms
step:831/1700 train_loss:3.4644 train_time:256335ms step_avg:312.22ms
step:832/1700 train_loss:3.5920 train_time:256662ms step_avg:312.24ms
step:833/1700 train_loss:3.6150 train_time:256988ms step_avg:312.26ms
step:834/1700 train_loss:3.6269 train_time:257315ms step_avg:312.27ms
step:835/1700 train_loss:3.4690 train_time:257645ms step_avg:312.30ms
step:836/1700 train_loss:3.6965 train_time:257968ms step_avg:312.31ms
step:837/1700 train_loss:3.4777 train_time:258292ms step_avg:312.32ms
step:838/1700 train_loss:3.3963 train_time:258616ms step_avg:312.34ms
step:839/1700 train_loss:3.6406 train_time:258939ms step_avg:312.35ms
step:840/1700 train_loss:3.5631 train_time:259262ms step_avg:312.36ms
step:841/1700 train_loss:3.6424 train_time:259586ms step_avg:312.38ms
step:842/1700 train_loss:3.5223 train_time:259908ms step_avg:312.39ms
step:843/1700 train_loss:3.5795 train_time:260236ms step_avg:312.41ms
step:844/1700 train_loss:3.5373 train_time:260559ms step_avg:312.42ms
step:845/1700 train_loss:3.5528 train_time:260883ms step_avg:312.43ms
step:846/1700 train_loss:3.5700 train_time:261208ms step_avg:312.45ms
step:847/1700 train_loss:3.6093 train_time:261534ms step_avg:312.47ms
step:848/1700 train_loss:3.5545 train_time:261860ms step_avg:312.48ms
step:849/1700 train_loss:3.3885 train_time:262184ms step_avg:312.50ms
step:850/1700 train_loss:3.6001 train_time:262508ms step_avg:312.51ms
step:851/1700 train_loss:3.4863 train_time:262836ms step_avg:312.53ms
step:852/1700 train_loss:3.6013 train_time:263163ms step_avg:312.55ms
step:853/1700 train_loss:3.3799 train_time:263484ms step_avg:312.56ms
step:854/1700 train_loss:3.6686 train_time:263809ms step_avg:312.57ms
step:855/1700 train_loss:3.6021 train_time:264134ms step_avg:312.58ms
step:856/1700 train_loss:3.3824 train_time:264458ms step_avg:312.60ms
step:857/1700 train_loss:3.6764 train_time:264786ms step_avg:312.62ms
step:858/1700 train_loss:3.6721 train_time:265111ms step_avg:312.63ms
step:859/1700 train_loss:3.3883 train_time:265440ms step_avg:312.65ms
step:860/1700 train_loss:3.5700 train_time:265764ms step_avg:312.66ms
step:861/1700 train_loss:3.6324 train_time:266091ms step_avg:312.68ms
step:862/1700 train_loss:3.4374 train_time:266412ms step_avg:312.69ms
step:863/1700 train_loss:3.5201 train_time:266740ms step_avg:312.71ms
step:864/1700 train_loss:3.8120 train_time:267069ms step_avg:312.73ms
step:865/1700 train_loss:3.7586 train_time:267406ms step_avg:312.76ms
step:866/1700 train_loss:3.5765 train_time:267732ms step_avg:312.77ms
step:867/1700 train_loss:3.5224 train_time:268055ms step_avg:312.78ms
step:868/1700 train_loss:3.7196 train_time:268382ms step_avg:312.80ms
step:869/1700 train_loss:3.4563 train_time:268708ms step_avg:312.81ms
step:870/1700 train_loss:3.4072 train_time:269035ms step_avg:312.83ms
step:871/1700 train_loss:3.5847 train_time:269359ms step_avg:312.84ms
step:872/1700 train_loss:3.5391 train_time:269681ms step_avg:312.86ms
step:873/1700 train_loss:3.4791 train_time:270005ms step_avg:312.87ms
step:874/1700 train_loss:3.6251 train_time:270329ms step_avg:312.88ms
step:875/1700 train_loss:3.5328 train_time:270653ms step_avg:312.89ms
step:875/1700 val_loss:3.5379 train_time:270662ms step_avg:312.90ms
step:876/1700 train_loss:3.6411 train_time:270986ms step_avg:312.92ms
step:877/1700 train_loss:3.4305 train_time:271307ms step_avg:312.93ms
step:878/1700 train_loss:3.6444 train_time:271632ms step_avg:312.94ms
step:879/1700 train_loss:3.5192 train_time:271957ms step_avg:312.95ms
step:880/1700 train_loss:3.8661 train_time:272281ms step_avg:312.97ms
step:881/1700 train_loss:3.5775 train_time:272606ms step_avg:312.98ms
step:882/1700 train_loss:3.3955 train_time:272925ms step_avg:312.99ms
step:883/1700 train_loss:3.7202 train_time:273251ms step_avg:313.00ms
step:884/1700 train_loss:3.4275 train_time:273575ms step_avg:313.01ms
step:885/1700 train_loss:3.6815 train_time:273898ms step_avg:313.03ms
step:886/1700 train_loss:3.5475 train_time:274227ms step_avg:313.04ms
step:887/1700 train_loss:3.6155 train_time:274556ms step_avg:313.06ms
step:888/1700 train_loss:3.5930 train_time:274876ms step_avg:313.07ms
step:889/1700 train_loss:3.6233 train_time:275203ms step_avg:313.09ms
step:890/1700 train_loss:3.5883 train_time:275536ms step_avg:313.11ms
step:891/1700 train_loss:3.4279 train_time:275858ms step_avg:313.12ms
step:892/1700 train_loss:3.6036 train_time:276182ms step_avg:313.13ms
step:893/1700 train_loss:3.5187 train_time:276507ms step_avg:313.14ms
step:894/1700 train_loss:3.5739 train_time:276830ms step_avg:313.16ms
step:895/1700 train_loss:3.4066 train_time:277152ms step_avg:313.17ms
step:896/1700 train_loss:3.3090 train_time:277487ms step_avg:313.19ms
step:897/1700 train_loss:3.4771 train_time:277812ms step_avg:313.20ms
step:898/1700 train_loss:3.6697 train_time:278137ms step_avg:313.22ms
step:899/1700 train_loss:3.5267 train_time:278463ms step_avg:313.23ms
step:900/1700 train_loss:3.5863 train_time:278788ms step_avg:313.25ms
step:901/1700 train_loss:3.7424 train_time:279108ms step_avg:313.25ms
step:902/1700 train_loss:3.5148 train_time:279431ms step_avg:313.26ms
step:903/1700 train_loss:3.4677 train_time:279757ms step_avg:313.28ms
step:904/1700 train_loss:3.6853 train_time:280085ms step_avg:313.29ms
step:905/1700 train_loss:3.6309 train_time:280406ms step_avg:313.30ms
step:906/1700 train_loss:3.4827 train_time:280731ms step_avg:313.32ms
step:907/1700 train_loss:3.4939 train_time:281052ms step_avg:313.32ms
step:908/1700 train_loss:3.7829 train_time:281388ms step_avg:313.35ms
step:909/1700 train_loss:3.5019 train_time:281713ms step_avg:313.36ms
step:910/1700 train_loss:3.6851 train_time:282035ms step_avg:313.37ms
step:911/1700 train_loss:3.8748 train_time:282370ms step_avg:313.40ms
step:912/1700 train_loss:3.3267 train_time:282691ms step_avg:313.40ms
step:913/1700 train_loss:3.6535 train_time:283014ms step_avg:313.42ms
step:914/1700 train_loss:3.5174 train_time:283344ms step_avg:313.43ms
step:915/1700 train_loss:3.5894 train_time:283669ms step_avg:313.45ms
step:916/1700 train_loss:3.7315 train_time:283995ms step_avg:313.46ms
step:917/1700 train_loss:3.4989 train_time:284319ms step_avg:313.47ms
step:918/1700 train_loss:3.4798 train_time:284647ms step_avg:313.49ms
step:919/1700 train_loss:3.5704 train_time:284970ms step_avg:313.50ms
step:920/1700 train_loss:3.4629 train_time:285301ms step_avg:313.52ms
step:921/1700 train_loss:3.5177 train_time:285632ms step_avg:313.54ms
step:922/1700 train_loss:3.4715 train_time:285957ms step_avg:313.55ms
step:923/1700 train_loss:3.6322 train_time:286285ms step_avg:313.57ms
step:924/1700 train_loss:3.4894 train_time:286606ms step_avg:313.57ms
step:925/1700 train_loss:3.4907 train_time:286930ms step_avg:313.58ms
step:926/1700 train_loss:3.6090 train_time:287261ms step_avg:313.60ms
step:927/1700 train_loss:3.4821 train_time:287588ms step_avg:313.62ms
step:928/1700 train_loss:3.6708 train_time:287917ms step_avg:313.64ms
step:929/1700 train_loss:3.5388 train_time:288242ms step_avg:313.65ms
step:930/1700 train_loss:3.3838 train_time:288571ms step_avg:313.66ms
step:931/1700 train_loss:3.7081 train_time:288894ms step_avg:313.67ms
step:932/1700 train_loss:3.4018 train_time:289224ms step_avg:313.69ms
step:933/1700 train_loss:3.3578 train_time:289548ms step_avg:313.70ms
step:934/1700 train_loss:3.5935 train_time:289879ms step_avg:313.72ms
step:935/1700 train_loss:3.5549 train_time:290204ms step_avg:313.73ms
step:936/1700 train_loss:3.4057 train_time:290538ms step_avg:313.76ms
step:937/1700 train_loss:3.3543 train_time:290869ms step_avg:313.77ms
step:938/1700 train_loss:3.5248 train_time:291194ms step_avg:313.79ms
step:939/1700 train_loss:3.3202 train_time:291521ms step_avg:313.80ms
step:940/1700 train_loss:3.5995 train_time:291844ms step_avg:313.81ms
step:941/1700 train_loss:3.4460 train_time:292175ms step_avg:313.83ms
step:942/1700 train_loss:3.4459 train_time:292505ms step_avg:313.85ms
step:943/1700 train_loss:3.5687 train_time:292838ms step_avg:313.87ms
step:944/1700 train_loss:3.4580 train_time:293164ms step_avg:313.88ms
step:945/1700 train_loss:3.4672 train_time:293491ms step_avg:313.89ms
step:946/1700 train_loss:3.6353 train_time:293826ms step_avg:313.92ms
step:947/1700 train_loss:3.5345 train_time:294152ms step_avg:313.93ms
step:948/1700 train_loss:3.6080 train_time:294484ms step_avg:313.95ms
step:949/1700 train_loss:3.7650 train_time:294812ms step_avg:313.96ms
step:950/1700 train_loss:3.3999 train_time:295310ms step_avg:314.16ms
step:951/1700 train_loss:3.4619 train_time:295639ms step_avg:314.18ms
step:952/1700 train_loss:3.7186 train_time:296076ms step_avg:314.31ms
step:953/1700 train_loss:3.4262 train_time:296401ms step_avg:314.32ms
step:954/1700 train_loss:3.4910 train_time:296729ms step_avg:314.33ms
step:955/1700 train_loss:3.5837 train_time:297064ms step_avg:314.35ms
step:956/1700 train_loss:3.4620 train_time:297390ms step_avg:314.37ms
step:957/1700 train_loss:3.4978 train_time:297709ms step_avg:314.37ms
step:958/1700 train_loss:3.4586 train_time:298045ms step_avg:314.39ms
step:959/1700 train_loss:3.5164 train_time:298376ms step_avg:314.41ms
step:960/1700 train_loss:3.5215 train_time:298705ms step_avg:314.43ms
step:961/1700 train_loss:3.5331 train_time:299031ms step_avg:314.44ms
step:962/1700 train_loss:3.4195 train_time:299368ms step_avg:314.46ms
step:963/1700 train_loss:3.6705 train_time:299696ms step_avg:314.48ms
step:964/1700 train_loss:3.6247 train_time:300021ms step_avg:314.49ms
step:965/1700 train_loss:3.5493 train_time:300350ms step_avg:314.50ms
step:966/1700 train_loss:3.4479 train_time:300680ms step_avg:314.52ms
step:967/1700 train_loss:3.5002 train_time:301001ms step_avg:314.53ms
step:968/1700 train_loss:3.7350 train_time:301327ms step_avg:314.54ms
step:969/1700 train_loss:3.5450 train_time:301651ms step_avg:314.55ms
step:970/1700 train_loss:3.5433 train_time:301975ms step_avg:314.56ms
step:971/1700 train_loss:3.6095 train_time:302304ms step_avg:314.57ms
step:972/1700 train_loss:3.3911 train_time:302629ms step_avg:314.58ms
step:973/1700 train_loss:3.5582 train_time:302955ms step_avg:314.60ms
step:974/1700 train_loss:3.5085 train_time:303279ms step_avg:314.60ms
step:975/1700 train_loss:3.5623 train_time:303605ms step_avg:314.62ms
step:976/1700 train_loss:3.6153 train_time:303929ms step_avg:314.63ms
step:977/1700 train_loss:3.4970 train_time:304257ms step_avg:314.64ms
step:978/1700 train_loss:3.6933 train_time:304585ms step_avg:314.65ms
step:979/1700 train_loss:3.6044 train_time:304908ms step_avg:314.66ms
step:980/1700 train_loss:3.3778 train_time:305234ms step_avg:314.67ms
step:981/1700 train_loss:3.6467 train_time:305562ms step_avg:314.69ms
step:982/1700 train_loss:3.4432 train_time:305886ms step_avg:314.70ms
step:983/1700 train_loss:3.5992 train_time:306207ms step_avg:314.70ms
step:984/1700 train_loss:3.5723 train_time:306534ms step_avg:314.72ms
step:985/1700 train_loss:3.5459 train_time:306868ms step_avg:314.74ms
step:986/1700 train_loss:3.5224 train_time:307194ms step_avg:314.75ms
step:987/1700 train_loss:3.6040 train_time:307522ms step_avg:314.76ms
step:988/1700 train_loss:3.4420 train_time:307848ms step_avg:314.77ms
step:989/1700 train_loss:3.5151 train_time:308173ms step_avg:314.78ms
step:990/1700 train_loss:3.5204 train_time:308498ms step_avg:314.79ms
step:991/1700 train_loss:3.4457 train_time:308828ms step_avg:314.81ms
step:992/1700 train_loss:3.6875 train_time:309164ms step_avg:314.83ms
step:993/1700 train_loss:3.4999 train_time:309487ms step_avg:314.84ms
step:994/1700 train_loss:3.4683 train_time:309812ms step_avg:314.85ms
step:995/1700 train_loss:3.5383 train_time:310151ms step_avg:314.87ms
step:996/1700 train_loss:3.6251 train_time:310477ms step_avg:314.89ms
step:997/1700 train_loss:3.5648 train_time:310798ms step_avg:314.89ms
step:998/1700 train_loss:3.4862 train_time:311123ms step_avg:314.90ms
step:999/1700 train_loss:3.8009 train_time:311445ms step_avg:314.91ms
step:1000/1700 train_loss:3.4741 train_time:311769ms step_avg:314.92ms
step:1000/1700 val_loss:3.5010 train_time:311778ms step_avg:314.93ms
step:1001/1700 train_loss:3.6186 train_time:312097ms step_avg:314.93ms
step:1002/1700 train_loss:3.4746 train_time:312420ms step_avg:314.94ms
step:1003/1700 train_loss:3.5317 train_time:312743ms step_avg:314.95ms
step:1004/1700 train_loss:3.4115 train_time:313073ms step_avg:314.96ms
step:1005/1700 train_loss:3.5860 train_time:313402ms step_avg:314.98ms
step:1006/1700 train_loss:3.6414 train_time:313734ms step_avg:314.99ms
step:1007/1700 train_loss:3.4200 train_time:314060ms step_avg:315.01ms
step:1008/1700 train_loss:3.4939 train_time:314387ms step_avg:315.02ms
step:1009/1700 train_loss:3.4765 train_time:314717ms step_avg:315.03ms
step:1010/1700 train_loss:3.5923 train_time:315046ms step_avg:315.05ms
step:1011/1700 train_loss:3.6978 train_time:315384ms step_avg:315.07ms
step:1012/1700 train_loss:3.5972 train_time:315710ms step_avg:315.08ms
step:1013/1700 train_loss:3.5713 train_time:316036ms step_avg:315.09ms
step:1014/1700 train_loss:3.4254 train_time:316364ms step_avg:315.10ms
step:1015/1700 train_loss:3.5719 train_time:316686ms step_avg:315.11ms
step:1016/1700 train_loss:3.6604 train_time:317010ms step_avg:315.12ms
step:1017/1700 train_loss:3.3623 train_time:317338ms step_avg:315.13ms
step:1018/1700 train_loss:3.4433 train_time:317668ms step_avg:315.15ms
step:1019/1700 train_loss:3.4369 train_time:318000ms step_avg:315.16ms
step:1020/1700 train_loss:3.4264 train_time:318321ms step_avg:315.17ms
step:1021/1700 train_loss:3.5604 train_time:318647ms step_avg:315.18ms
step:1022/1700 train_loss:3.4301 train_time:318974ms step_avg:315.19ms
step:1023/1700 train_loss:3.3888 train_time:319303ms step_avg:315.21ms
step:1024/1700 train_loss:3.5149 train_time:319630ms step_avg:315.22ms
step:1025/1700 train_loss:3.5425 train_time:319960ms step_avg:315.23ms
step:1026/1700 train_loss:3.5133 train_time:320289ms step_avg:315.25ms
step:1027/1700 train_loss:3.5163 train_time:320619ms step_avg:315.26ms
step:1028/1700 train_loss:3.6621 train_time:320941ms step_avg:315.27ms
step:1029/1700 train_loss:3.3589 train_time:321268ms step_avg:315.28ms
step:1030/1700 train_loss:3.4310 train_time:321602ms step_avg:315.30ms
step:1031/1700 train_loss:3.3557 train_time:321930ms step_avg:315.31ms
step:1032/1700 train_loss:3.5714 train_time:322254ms step_avg:315.32ms
step:1033/1700 train_loss:3.5476 train_time:322580ms step_avg:315.33ms
step:1034/1700 train_loss:3.7413 train_time:322907ms step_avg:315.34ms
step:1035/1700 train_loss:3.5305 train_time:323237ms step_avg:315.35ms
step:1036/1700 train_loss:3.4577 train_time:323573ms step_avg:315.37ms
step:1037/1700 train_loss:3.4814 train_time:323904ms step_avg:315.39ms
step:1038/1700 train_loss:3.5273 train_time:324235ms step_avg:315.40ms
step:1039/1700 train_loss:3.8355 train_time:324563ms step_avg:315.42ms
step:1040/1700 train_loss:3.6623 train_time:324891ms step_avg:315.43ms
step:1041/1700 train_loss:3.5522 train_time:325219ms step_avg:315.44ms
step:1042/1700 train_loss:3.4489 train_time:325547ms step_avg:315.45ms
step:1043/1700 train_loss:3.5219 train_time:325882ms step_avg:315.47ms
step:1044/1700 train_loss:3.5665 train_time:326216ms step_avg:315.49ms
step:1045/1700 train_loss:3.4847 train_time:326540ms step_avg:315.50ms
step:1046/1700 train_loss:3.4925 train_time:326868ms step_avg:315.51ms
step:1047/1700 train_loss:3.5628 train_time:327201ms step_avg:315.53ms
step:1048/1700 train_loss:3.4683 train_time:327531ms step_avg:315.54ms
step:1049/1700 train_loss:3.6855 train_time:327862ms step_avg:315.56ms
step:1050/1700 train_loss:3.5386 train_time:328195ms step_avg:315.57ms
step:1051/1700 train_loss:3.4426 train_time:328525ms step_avg:315.59ms
step:1052/1700 train_loss:3.4361 train_time:328856ms step_avg:315.60ms
step:1053/1700 train_loss:3.5431 train_time:329183ms step_avg:315.61ms
step:1054/1700 train_loss:3.3940 train_time:329511ms step_avg:315.62ms
step:1055/1700 train_loss:3.7410 train_time:329838ms step_avg:315.63ms
step:1056/1700 train_loss:3.5832 train_time:330169ms step_avg:315.65ms
step:1057/1700 train_loss:3.4212 train_time:330501ms step_avg:315.67ms
step:1058/1700 train_loss:3.5427 train_time:330831ms step_avg:315.68ms
step:1059/1700 train_loss:3.6360 train_time:331160ms step_avg:315.69ms
step:1060/1700 train_loss:3.3455 train_time:331494ms step_avg:315.71ms
step:1061/1700 train_loss:3.4172 train_time:331831ms step_avg:315.73ms
step:1062/1700 train_loss:3.4883 train_time:332160ms step_avg:315.74ms
step:1063/1700 train_loss:3.4606 train_time:332487ms step_avg:315.75ms
step:1064/1700 train_loss:3.4245 train_time:332818ms step_avg:315.77ms
step:1065/1700 train_loss:3.5072 train_time:333145ms step_avg:315.78ms
step:1066/1700 train_loss:3.4332 train_time:333471ms step_avg:315.79ms
step:1067/1700 train_loss:3.4097 train_time:333800ms step_avg:315.80ms
step:1068/1700 train_loss:3.4577 train_time:334134ms step_avg:315.82ms
step:1069/1700 train_loss:3.3311 train_time:334467ms step_avg:315.83ms
step:1070/1700 train_loss:3.4844 train_time:334793ms step_avg:315.84ms
step:1071/1700 train_loss:3.3507 train_time:335131ms step_avg:315.86ms
step:1072/1700 train_loss:3.6163 train_time:335457ms step_avg:315.87ms
step:1073/1700 train_loss:3.5573 train_time:335795ms step_avg:315.89ms
step:1074/1700 train_loss:3.4806 train_time:336121ms step_avg:315.90ms
step:1075/1700 train_loss:3.5657 train_time:336447ms step_avg:315.91ms
step:1076/1700 train_loss:3.4863 train_time:336779ms step_avg:315.93ms
step:1077/1700 train_loss:3.4468 train_time:337105ms step_avg:315.94ms
step:1078/1700 train_loss:3.8424 train_time:337433ms step_avg:315.95ms
step:1079/1700 train_loss:3.4879 train_time:337761ms step_avg:315.96ms
step:1080/1700 train_loss:3.1294 train_time:338105ms step_avg:315.99ms
step:1081/1700 train_loss:3.5832 train_time:338432ms step_avg:316.00ms
step:1082/1700 train_loss:3.4788 train_time:338765ms step_avg:316.01ms
step:1083/1700 train_loss:3.5652 train_time:339107ms step_avg:316.04ms
step:1084/1700 train_loss:3.6425 train_time:339437ms step_avg:316.05ms
step:1085/1700 train_loss:3.5511 train_time:339762ms step_avg:316.06ms
step:1086/1700 train_loss:3.5207 train_time:340092ms step_avg:316.07ms
step:1087/1700 train_loss:3.4843 train_time:340422ms step_avg:316.08ms
step:1088/1700 train_loss:3.6890 train_time:340761ms step_avg:316.11ms
step:1089/1700 train_loss:3.5641 train_time:341098ms step_avg:316.12ms
step:1090/1700 train_loss:3.4110 train_time:341423ms step_avg:316.13ms
step:1091/1700 train_loss:3.4273 train_time:341757ms step_avg:316.15ms
step:1092/1700 train_loss:3.5347 train_time:342087ms step_avg:316.16ms
step:1093/1700 train_loss:3.3343 train_time:342420ms step_avg:316.18ms
step:1094/1700 train_loss:3.5466 train_time:342745ms step_avg:316.19ms
step:1095/1700 train_loss:3.6588 train_time:343074ms step_avg:316.20ms
step:1096/1700 train_loss:3.4965 train_time:343406ms step_avg:316.21ms
step:1097/1700 train_loss:3.4665 train_time:343740ms step_avg:316.23ms
step:1098/1700 train_loss:3.4825 train_time:344071ms step_avg:316.24ms
step:1099/1700 train_loss:3.5382 train_time:344398ms step_avg:316.25ms
step:1100/1700 train_loss:3.6117 train_time:344735ms step_avg:316.27ms
step:1101/1700 train_loss:3.5774 train_time:345067ms step_avg:316.28ms
step:1102/1700 train_loss:3.4879 train_time:345398ms step_avg:316.30ms
step:1103/1700 train_loss:3.3425 train_time:345728ms step_avg:316.31ms
step:1104/1700 train_loss:3.3661 train_time:346063ms step_avg:316.33ms
step:1105/1700 train_loss:3.4986 train_time:346395ms step_avg:316.34ms
step:1106/1700 train_loss:3.3694 train_time:346722ms step_avg:316.35ms
step:1107/1700 train_loss:4.1301 train_time:347057ms step_avg:316.37ms
step:1108/1700 train_loss:3.2855 train_time:347384ms step_avg:316.38ms
step:1109/1700 train_loss:3.6241 train_time:347713ms step_avg:316.39ms
step:1110/1700 train_loss:3.3940 train_time:348042ms step_avg:316.40ms
step:1111/1700 train_loss:3.5568 train_time:348367ms step_avg:316.41ms
step:1112/1700 train_loss:3.4857 train_time:348694ms step_avg:316.42ms
step:1113/1700 train_loss:3.5344 train_time:349026ms step_avg:316.43ms
step:1114/1700 train_loss:3.6173 train_time:349357ms step_avg:316.45ms
step:1115/1700 train_loss:3.4910 train_time:349681ms step_avg:316.45ms
step:1116/1700 train_loss:3.4190 train_time:350013ms step_avg:316.47ms
step:1117/1700 train_loss:3.2985 train_time:350360ms step_avg:316.49ms
step:1118/1700 train_loss:3.4782 train_time:350682ms step_avg:316.50ms
step:1119/1700 train_loss:3.6416 train_time:351023ms step_avg:316.52ms
step:1120/1700 train_loss:3.6885 train_time:351349ms step_avg:316.53ms
step:1121/1700 train_loss:3.5330 train_time:351678ms step_avg:316.54ms
step:1122/1700 train_loss:3.5429 train_time:352006ms step_avg:316.55ms
step:1123/1700 train_loss:3.4418 train_time:352333ms step_avg:316.56ms
step:1124/1700 train_loss:3.5105 train_time:352658ms step_avg:316.57ms
step:1125/1700 train_loss:3.6436 train_time:352991ms step_avg:316.58ms
step:1125/1700 val_loss:3.4653 train_time:353000ms step_avg:316.59ms
step:1126/1700 train_loss:3.3974 train_time:353327ms step_avg:316.60ms
step:1127/1700 train_loss:3.2727 train_time:353662ms step_avg:316.62ms
step:1128/1700 train_loss:3.5295 train_time:353998ms step_avg:316.63ms
step:1129/1700 train_loss:3.7349 train_time:354327ms step_avg:316.65ms
step:1130/1700 train_loss:3.2858 train_time:354659ms step_avg:316.66ms
step:1131/1700 train_loss:3.6142 train_time:354988ms step_avg:316.67ms
step:1132/1700 train_loss:3.4330 train_time:355317ms step_avg:316.68ms
step:1133/1700 train_loss:3.4504 train_time:355646ms step_avg:316.69ms
step:1134/1700 train_loss:3.4163 train_time:355973ms step_avg:316.70ms
step:1135/1700 train_loss:3.5427 train_time:356310ms step_avg:316.72ms
step:1136/1700 train_loss:3.5022 train_time:356640ms step_avg:316.73ms
step:1137/1700 train_loss:3.5748 train_time:356970ms step_avg:316.74ms
step:1138/1700 train_loss:3.6135 train_time:357304ms step_avg:316.76ms
step:1139/1700 train_loss:3.5111 train_time:357637ms step_avg:316.77ms
step:1140/1700 train_loss:3.4009 train_time:358135ms step_avg:316.93ms
step:1141/1700 train_loss:3.7023 train_time:358463ms step_avg:316.94ms
step:1142/1700 train_loss:3.5209 train_time:358792ms step_avg:316.95ms
step:1143/1700 train_loss:3.5784 train_time:359291ms step_avg:317.11ms
step:1144/1700 train_loss:3.6205 train_time:359616ms step_avg:317.12ms
step:1145/1700 train_loss:3.2214 train_time:359952ms step_avg:317.14ms
step:1146/1700 train_loss:3.5514 train_time:360278ms step_avg:317.15ms
step:1147/1700 train_loss:3.4022 train_time:360606ms step_avg:317.16ms
step:1148/1700 train_loss:3.4528 train_time:360935ms step_avg:317.17ms
step:1149/1700 train_loss:3.5116 train_time:361264ms step_avg:317.18ms
step:1150/1700 train_loss:3.5935 train_time:361594ms step_avg:317.19ms
step:1151/1700 train_loss:3.5545 train_time:361923ms step_avg:317.20ms
step:1152/1700 train_loss:3.4505 train_time:362259ms step_avg:317.21ms
step:1153/1700 train_loss:3.4128 train_time:362598ms step_avg:317.23ms
step:1154/1700 train_loss:3.6436 train_time:362928ms step_avg:317.24ms
step:1155/1700 train_loss:3.6361 train_time:363262ms step_avg:317.26ms
step:1156/1700 train_loss:3.3501 train_time:363596ms step_avg:317.27ms
step:1157/1700 train_loss:3.3469 train_time:363922ms step_avg:317.28ms
step:1158/1700 train_loss:3.4948 train_time:364260ms step_avg:317.30ms
step:1159/1700 train_loss:3.5122 train_time:364598ms step_avg:317.32ms
step:1160/1700 train_loss:3.2805 train_time:364931ms step_avg:317.33ms
step:1161/1700 train_loss:3.3492 train_time:365258ms step_avg:317.34ms
step:1162/1700 train_loss:3.3373 train_time:365590ms step_avg:317.35ms
step:1163/1700 train_loss:3.5597 train_time:365918ms step_avg:317.36ms
step:1164/1700 train_loss:3.3655 train_time:366257ms step_avg:317.38ms
step:1165/1700 train_loss:3.4837 train_time:366590ms step_avg:317.39ms
step:1166/1700 train_loss:3.4547 train_time:366921ms step_avg:317.41ms
step:1167/1700 train_loss:3.4599 train_time:367255ms step_avg:317.42ms
step:1168/1700 train_loss:3.4431 train_time:367584ms step_avg:317.43ms
step:1169/1700 train_loss:3.4373 train_time:367914ms step_avg:317.44ms
step:1170/1700 train_loss:3.6479 train_time:368249ms step_avg:317.46ms
step:1171/1700 train_loss:3.4123 train_time:368581ms step_avg:317.47ms
step:1172/1700 train_loss:3.5108 train_time:368912ms step_avg:317.48ms
step:1173/1700 train_loss:3.4673 train_time:369248ms step_avg:317.50ms
step:1174/1700 train_loss:3.3737 train_time:369576ms step_avg:317.51ms
step:1175/1700 train_loss:3.4691 train_time:369903ms step_avg:317.51ms
step:1176/1700 train_loss:3.8110 train_time:370260ms step_avg:317.55ms
step:1177/1700 train_loss:3.4402 train_time:370595ms step_avg:317.56ms
step:1178/1700 train_loss:3.4454 train_time:370931ms step_avg:317.58ms
step:1179/1700 train_loss:3.3287 train_time:371274ms step_avg:317.60ms
step:1180/1700 train_loss:3.4681 train_time:371607ms step_avg:317.61ms
step:1181/1700 train_loss:3.4875 train_time:371938ms step_avg:317.62ms
step:1182/1700 train_loss:3.4246 train_time:372270ms step_avg:317.64ms
step:1183/1700 train_loss:3.3388 train_time:372606ms step_avg:317.65ms
step:1184/1700 train_loss:3.4718 train_time:372940ms step_avg:317.67ms
step:1185/1700 train_loss:3.5799 train_time:373272ms step_avg:317.68ms
step:1186/1700 train_loss:3.7535 train_time:373606ms step_avg:317.69ms
step:1187/1700 train_loss:3.5920 train_time:373940ms step_avg:317.71ms
step:1188/1700 train_loss:3.4340 train_time:374276ms step_avg:317.72ms
step:1189/1700 train_loss:3.2913 train_time:374623ms step_avg:317.75ms
step:1190/1700 train_loss:3.4139 train_time:374958ms step_avg:317.76ms
step:1191/1700 train_loss:3.4168 train_time:375288ms step_avg:317.77ms
step:1192/1700 train_loss:3.4645 train_time:375621ms step_avg:317.78ms
step:1193/1700 train_loss:3.3812 train_time:375956ms step_avg:317.80ms
step:1194/1700 train_loss:3.5488 train_time:376290ms step_avg:317.81ms
step:1195/1700 train_loss:3.4064 train_time:376616ms step_avg:317.82ms
step:1196/1700 train_loss:3.3894 train_time:376956ms step_avg:317.84ms
step:1197/1700 train_loss:3.3901 train_time:377297ms step_avg:317.86ms
step:1198/1700 train_loss:3.4693 train_time:377627ms step_avg:317.87ms
step:1199/1700 train_loss:3.4399 train_time:377956ms step_avg:317.88ms
step:1200/1700 train_loss:3.3982 train_time:378297ms step_avg:317.90ms
step:1201/1700 train_loss:3.8181 train_time:378647ms step_avg:317.92ms
step:1202/1700 train_loss:3.3472 train_time:378979ms step_avg:317.94ms
step:1203/1700 train_loss:3.4458 train_time:379309ms step_avg:317.95ms
step:1204/1700 train_loss:3.4358 train_time:379643ms step_avg:317.96ms
step:1205/1700 train_loss:3.5112 train_time:379994ms step_avg:317.99ms
step:1206/1700 train_loss:3.5070 train_time:380327ms step_avg:318.00ms
step:1207/1700 train_loss:3.4739 train_time:380665ms step_avg:318.02ms
step:1208/1700 train_loss:3.3920 train_time:380998ms step_avg:318.03ms
step:1209/1700 train_loss:3.4543 train_time:381326ms step_avg:318.04ms
step:1210/1700 train_loss:3.3924 train_time:381660ms step_avg:318.05ms
step:1211/1700 train_loss:3.4652 train_time:381993ms step_avg:318.06ms
step:1212/1700 train_loss:3.7015 train_time:382335ms step_avg:318.08ms
step:1213/1700 train_loss:3.3832 train_time:382664ms step_avg:318.09ms
step:1214/1700 train_loss:3.6299 train_time:382993ms step_avg:318.10ms
step:1215/1700 train_loss:3.4271 train_time:383322ms step_avg:318.11ms
step:1216/1700 train_loss:3.5136 train_time:383657ms step_avg:318.12ms
step:1217/1700 train_loss:3.4633 train_time:383993ms step_avg:318.14ms
step:1218/1700 train_loss:3.4732 train_time:384317ms step_avg:318.14ms
step:1219/1700 train_loss:3.5028 train_time:384652ms step_avg:318.16ms
step:1220/1700 train_loss:3.4257 train_time:384980ms step_avg:318.17ms
step:1221/1700 train_loss:3.4752 train_time:385312ms step_avg:318.18ms
step:1222/1700 train_loss:3.4924 train_time:385648ms step_avg:318.19ms
step:1223/1700 train_loss:3.4945 train_time:385977ms step_avg:318.20ms
step:1224/1700 train_loss:3.4519 train_time:386319ms step_avg:318.22ms
step:1225/1700 train_loss:3.4396 train_time:386653ms step_avg:318.23ms
step:1226/1700 train_loss:3.3418 train_time:386988ms step_avg:318.25ms
step:1227/1700 train_loss:3.4961 train_time:387324ms step_avg:318.26ms
step:1228/1700 train_loss:3.2880 train_time:387651ms step_avg:318.27ms
step:1229/1700 train_loss:3.5849 train_time:387986ms step_avg:318.28ms
step:1230/1700 train_loss:3.4029 train_time:388318ms step_avg:318.29ms
step:1231/1700 train_loss:3.4106 train_time:388662ms step_avg:318.31ms
step:1232/1700 train_loss:3.5655 train_time:389002ms step_avg:318.33ms
step:1233/1700 train_loss:3.3022 train_time:389339ms step_avg:318.35ms
step:1234/1700 train_loss:3.3746 train_time:389671ms step_avg:318.36ms
step:1235/1700 train_loss:3.4265 train_time:390003ms step_avg:318.37ms
step:1236/1700 train_loss:3.4375 train_time:390335ms step_avg:318.38ms
step:1237/1700 train_loss:3.4281 train_time:390671ms step_avg:318.40ms
step:1238/1700 train_loss:3.3941 train_time:391002ms step_avg:318.41ms
step:1239/1700 train_loss:3.6259 train_time:391334ms step_avg:318.42ms
step:1240/1700 train_loss:3.3807 train_time:391679ms step_avg:318.44ms
step:1241/1700 train_loss:3.2625 train_time:392010ms step_avg:318.45ms
step:1242/1700 train_loss:3.5373 train_time:392353ms step_avg:318.47ms
step:1243/1700 train_loss:3.2573 train_time:392687ms step_avg:318.48ms
step:1244/1700 train_loss:3.4290 train_time:393017ms step_avg:318.49ms
step:1245/1700 train_loss:3.6740 train_time:393352ms step_avg:318.50ms
step:1246/1700 train_loss:3.3629 train_time:393682ms step_avg:318.51ms
step:1247/1700 train_loss:3.4423 train_time:394010ms step_avg:318.52ms
step:1248/1700 train_loss:3.5340 train_time:394338ms step_avg:318.53ms
step:1249/1700 train_loss:3.2459 train_time:394672ms step_avg:318.54ms
step:1250/1700 train_loss:3.3613 train_time:395005ms step_avg:318.55ms
step:1250/1700 val_loss:3.4107 train_time:395014ms step_avg:318.56ms
step:1251/1700 train_loss:3.3674 train_time:395341ms step_avg:318.57ms
step:1252/1700 train_loss:3.2445 train_time:395674ms step_avg:318.58ms
step:1253/1700 train_loss:3.5002 train_time:396006ms step_avg:318.59ms
step:1254/1700 train_loss:3.5943 train_time:396351ms step_avg:318.61ms
step:1255/1700 train_loss:3.3382 train_time:396681ms step_avg:318.62ms
step:1256/1700 train_loss:3.5338 train_time:397008ms step_avg:318.63ms
step:1257/1700 train_loss:3.2892 train_time:397343ms step_avg:318.64ms
step:1258/1700 train_loss:3.4614 train_time:397676ms step_avg:318.65ms
step:1259/1700 train_loss:3.5453 train_time:398006ms step_avg:318.66ms
step:1260/1700 train_loss:3.4003 train_time:398334ms step_avg:318.67ms
step:1261/1700 train_loss:3.4490 train_time:398673ms step_avg:318.68ms
step:1262/1700 train_loss:3.5769 train_time:399005ms step_avg:318.69ms
step:1263/1700 train_loss:3.2452 train_time:399335ms step_avg:318.70ms
step:1264/1700 train_loss:3.4938 train_time:399671ms step_avg:318.72ms
step:1265/1700 train_loss:3.4204 train_time:400005ms step_avg:318.73ms
step:1266/1700 train_loss:3.4051 train_time:400338ms step_avg:318.74ms
step:1267/1700 train_loss:3.3345 train_time:400670ms step_avg:318.75ms
step:1268/1700 train_loss:3.3304 train_time:401013ms step_avg:318.77ms
step:1269/1700 train_loss:3.4389 train_time:401345ms step_avg:318.78ms
step:1270/1700 train_loss:3.3374 train_time:401682ms step_avg:318.80ms
step:1271/1700 train_loss:3.4581 train_time:402014ms step_avg:318.81ms
step:1272/1700 train_loss:3.4049 train_time:402354ms step_avg:318.82ms
step:1273/1700 train_loss:3.4428 train_time:402685ms step_avg:318.83ms
step:1274/1700 train_loss:3.3444 train_time:403022ms step_avg:318.85ms
step:1275/1700 train_loss:3.4744 train_time:403347ms step_avg:318.85ms
step:1276/1700 train_loss:3.4062 train_time:403677ms step_avg:318.86ms
step:1277/1700 train_loss:3.5032 train_time:404007ms step_avg:318.87ms
step:1278/1700 train_loss:3.4475 train_time:404343ms step_avg:318.88ms
step:1279/1700 train_loss:3.3692 train_time:404692ms step_avg:318.91ms
step:1280/1700 train_loss:3.3094 train_time:405026ms step_avg:318.92ms
step:1281/1700 train_loss:3.3965 train_time:405356ms step_avg:318.93ms
step:1282/1700 train_loss:3.3694 train_time:405695ms step_avg:318.94ms
step:1283/1700 train_loss:3.5405 train_time:406025ms step_avg:318.95ms
step:1284/1700 train_loss:3.3631 train_time:406354ms step_avg:318.96ms
step:1285/1700 train_loss:3.5142 train_time:406687ms step_avg:318.97ms
step:1286/1700 train_loss:3.3833 train_time:407029ms step_avg:318.99ms
step:1287/1700 train_loss:3.4606 train_time:407358ms step_avg:319.00ms
step:1288/1700 train_loss:3.4293 train_time:407687ms step_avg:319.00ms
step:1289/1700 train_loss:3.4307 train_time:408025ms step_avg:319.02ms
step:1290/1700 train_loss:3.4473 train_time:408367ms step_avg:319.04ms
step:1291/1700 train_loss:3.5772 train_time:408712ms step_avg:319.06ms
step:1292/1700 train_loss:3.3993 train_time:409055ms step_avg:319.08ms
step:1293/1700 train_loss:3.2052 train_time:409399ms step_avg:319.10ms
step:1294/1700 train_loss:3.4241 train_time:409727ms step_avg:319.10ms
step:1295/1700 train_loss:3.4052 train_time:410077ms step_avg:319.13ms
step:1296/1700 train_loss:3.4507 train_time:410411ms step_avg:319.14ms
step:1297/1700 train_loss:3.4892 train_time:410747ms step_avg:319.15ms
step:1298/1700 train_loss:3.4899 train_time:411078ms step_avg:319.16ms
step:1299/1700 train_loss:3.4504 train_time:411413ms step_avg:319.17ms
step:1300/1700 train_loss:3.4284 train_time:411741ms step_avg:319.18ms
step:1301/1700 train_loss:3.5629 train_time:412076ms step_avg:319.19ms
step:1302/1700 train_loss:3.4091 train_time:412409ms step_avg:319.20ms
step:1303/1700 train_loss:3.5050 train_time:412741ms step_avg:319.21ms
step:1304/1700 train_loss:3.4287 train_time:413071ms step_avg:319.22ms
step:1305/1700 train_loss:3.5158 train_time:413411ms step_avg:319.24ms
step:1306/1700 train_loss:3.5955 train_time:413754ms step_avg:319.25ms
step:1307/1700 train_loss:3.3827 train_time:414095ms step_avg:319.27ms
step:1308/1700 train_loss:3.3422 train_time:414427ms step_avg:319.28ms
step:1309/1700 train_loss:3.3588 train_time:414764ms step_avg:319.29ms
step:1310/1700 train_loss:3.4130 train_time:415093ms step_avg:319.30ms
step:1311/1700 train_loss:3.3467 train_time:415426ms step_avg:319.31ms
step:1312/1700 train_loss:3.5177 train_time:415760ms step_avg:319.32ms
step:1313/1700 train_loss:3.4073 train_time:416091ms step_avg:319.33ms
step:1314/1700 train_loss:3.4650 train_time:416425ms step_avg:319.34ms
step:1315/1700 train_loss:3.4446 train_time:416759ms step_avg:319.36ms
step:1316/1700 train_loss:3.4650 train_time:417088ms step_avg:319.36ms
step:1317/1700 train_loss:3.3107 train_time:417433ms step_avg:319.38ms
step:1318/1700 train_loss:3.5827 train_time:417763ms step_avg:319.39ms
step:1319/1700 train_loss:3.4945 train_time:418097ms step_avg:319.40ms
step:1320/1700 train_loss:3.3752 train_time:418425ms step_avg:319.41ms
step:1321/1700 train_loss:3.4879 train_time:418768ms step_avg:319.43ms
step:1322/1700 train_loss:3.4603 train_time:419100ms step_avg:319.44ms
step:1323/1700 train_loss:3.4415 train_time:419433ms step_avg:319.45ms
step:1324/1700 train_loss:3.6241 train_time:419780ms step_avg:319.47ms
step:1325/1700 train_loss:3.4927 train_time:420122ms step_avg:319.48ms
step:1326/1700 train_loss:3.5344 train_time:420455ms step_avg:319.49ms
step:1327/1700 train_loss:3.5492 train_time:420783ms step_avg:319.50ms
step:1328/1700 train_loss:3.2917 train_time:421122ms step_avg:319.52ms
step:1329/1700 train_loss:3.3744 train_time:421455ms step_avg:319.53ms
step:1330/1700 train_loss:3.4306 train_time:421960ms step_avg:319.67ms
step:1331/1700 train_loss:2.6658 train_time:422313ms step_avg:319.69ms
step:1332/1700 train_loss:3.4364 train_time:422650ms step_avg:319.71ms
step:1333/1700 train_loss:3.4094 train_time:423064ms step_avg:319.78ms
step:1334/1700 train_loss:3.3887 train_time:423390ms step_avg:319.78ms
step:1335/1700 train_loss:3.7948 train_time:423743ms step_avg:319.81ms
step:1336/1700 train_loss:3.5309 train_time:424070ms step_avg:319.81ms
step:1337/1700 train_loss:3.4191 train_time:424401ms step_avg:319.82ms
step:1338/1700 train_loss:3.3529 train_time:424742ms step_avg:319.84ms
step:1339/1700 train_loss:3.3451 train_time:425079ms step_avg:319.85ms
step:1340/1700 train_loss:3.6024 train_time:425418ms step_avg:319.86ms
step:1341/1700 train_loss:3.5694 train_time:425752ms step_avg:319.87ms
step:1342/1700 train_loss:3.3893 train_time:426088ms step_avg:319.89ms
step:1343/1700 train_loss:3.3364 train_time:426419ms step_avg:319.89ms
step:1344/1700 train_loss:3.6470 train_time:426755ms step_avg:319.91ms
step:1345/1700 train_loss:3.4112 train_time:427089ms step_avg:319.92ms
step:1346/1700 train_loss:3.4173 train_time:427426ms step_avg:319.93ms
step:1347/1700 train_loss:3.4685 train_time:427759ms step_avg:319.94ms
step:1348/1700 train_loss:3.4347 train_time:428092ms step_avg:319.95ms
step:1349/1700 train_loss:3.3543 train_time:428422ms step_avg:319.96ms
step:1350/1700 train_loss:3.3234 train_time:428755ms step_avg:319.97ms
step:1351/1700 train_loss:3.3962 train_time:429096ms step_avg:319.98ms
step:1352/1700 train_loss:3.3283 train_time:429433ms step_avg:319.99ms
step:1353/1700 train_loss:3.4400 train_time:429765ms step_avg:320.00ms
step:1354/1700 train_loss:3.2911 train_time:430094ms step_avg:320.01ms
step:1355/1700 train_loss:3.3508 train_time:430425ms step_avg:320.02ms
step:1356/1700 train_loss:3.4634 train_time:430763ms step_avg:320.03ms
step:1357/1700 train_loss:3.3085 train_time:431101ms step_avg:320.05ms
step:1358/1700 train_loss:3.2389 train_time:431431ms step_avg:320.05ms
step:1359/1700 train_loss:3.5655 train_time:431769ms step_avg:320.07ms
step:1360/1700 train_loss:3.4792 train_time:432105ms step_avg:320.08ms
step:1361/1700 train_loss:3.2281 train_time:432439ms step_avg:320.09ms
step:1362/1700 train_loss:3.4971 train_time:432782ms step_avg:320.11ms
step:1363/1700 train_loss:3.4063 train_time:433117ms step_avg:320.12ms
step:1364/1700 train_loss:3.1858 train_time:433458ms step_avg:320.13ms
step:1365/1700 train_loss:3.4468 train_time:433792ms step_avg:320.14ms
step:1366/1700 train_loss:3.3260 train_time:434131ms step_avg:320.16ms
step:1367/1700 train_loss:3.3616 train_time:434462ms step_avg:320.16ms
step:1368/1700 train_loss:3.3625 train_time:434806ms step_avg:320.18ms
step:1369/1700 train_loss:3.4786 train_time:435132ms step_avg:320.19ms
step:1370/1700 train_loss:3.4458 train_time:435470ms step_avg:320.20ms
step:1371/1700 train_loss:3.4066 train_time:435808ms step_avg:320.21ms
step:1372/1700 train_loss:3.3225 train_time:436148ms step_avg:320.23ms
step:1373/1700 train_loss:3.6621 train_time:436484ms step_avg:320.24ms
step:1374/1700 train_loss:3.3684 train_time:436812ms step_avg:320.24ms
step:1375/1700 train_loss:3.4204 train_time:437147ms step_avg:320.25ms
step:1375/1700 val_loss:3.3643 train_time:437156ms step_avg:320.26ms
step:1376/1700 train_loss:3.4162 train_time:437482ms step_avg:320.27ms
step:1377/1700 train_loss:3.2050 train_time:437824ms step_avg:320.28ms
step:1378/1700 train_loss:3.5931 train_time:438156ms step_avg:320.29ms
step:1379/1700 train_loss:3.3979 train_time:438487ms step_avg:320.30ms
step:1380/1700 train_loss:3.5294 train_time:438829ms step_avg:320.31ms
step:1381/1700 train_loss:3.5278 train_time:439160ms step_avg:320.32ms
step:1382/1700 train_loss:3.1662 train_time:439498ms step_avg:320.33ms
step:1383/1700 train_loss:3.3678 train_time:439846ms step_avg:320.35ms
step:1384/1700 train_loss:3.7657 train_time:440190ms step_avg:320.37ms
step:1385/1700 train_loss:3.2652 train_time:440526ms step_avg:320.38ms
step:1386/1700 train_loss:3.4444 train_time:440861ms step_avg:320.39ms
step:1387/1700 train_loss:3.5344 train_time:441197ms step_avg:320.40ms
step:1388/1700 train_loss:3.4579 train_time:441527ms step_avg:320.41ms
step:1389/1700 train_loss:3.3940 train_time:441860ms step_avg:320.42ms
step:1390/1700 train_loss:3.2499 train_time:442198ms step_avg:320.43ms
step:1391/1700 train_loss:3.3941 train_time:442534ms step_avg:320.44ms
step:1392/1700 train_loss:3.3741 train_time:442871ms step_avg:320.46ms
step:1393/1700 train_loss:3.6252 train_time:443204ms step_avg:320.47ms
step:1394/1700 train_loss:3.3358 train_time:443541ms step_avg:320.48ms
step:1395/1700 train_loss:3.3455 train_time:443877ms step_avg:320.49ms
step:1396/1700 train_loss:3.2990 train_time:444209ms step_avg:320.50ms
step:1397/1700 train_loss:3.5567 train_time:444544ms step_avg:320.51ms
step:1398/1700 train_loss:3.4463 train_time:444877ms step_avg:320.52ms
step:1399/1700 train_loss:3.4608 train_time:445211ms step_avg:320.53ms
step:1400/1700 train_loss:3.3598 train_time:445541ms step_avg:320.53ms
step:1401/1700 train_loss:3.3060 train_time:445875ms step_avg:320.54ms
step:1402/1700 train_loss:3.3860 train_time:446209ms step_avg:320.55ms
step:1403/1700 train_loss:3.3641 train_time:446550ms step_avg:320.57ms
step:1404/1700 train_loss:3.3960 train_time:446878ms step_avg:320.57ms
step:1405/1700 train_loss:3.3505 train_time:447213ms step_avg:320.58ms
step:1406/1700 train_loss:3.5445 train_time:447556ms step_avg:320.60ms
step:1407/1700 train_loss:3.3324 train_time:447883ms step_avg:320.60ms
step:1408/1700 train_loss:3.3625 train_time:448227ms step_avg:320.62ms
step:1409/1700 train_loss:3.3605 train_time:448560ms step_avg:320.63ms
step:1410/1700 train_loss:3.2218 train_time:448890ms step_avg:320.64ms
step:1411/1700 train_loss:3.3608 train_time:449218ms step_avg:320.64ms
step:1412/1700 train_loss:3.3470 train_time:449569ms step_avg:320.66ms
step:1413/1700 train_loss:3.3331 train_time:449905ms step_avg:320.67ms
step:1414/1700 train_loss:3.4169 train_time:450238ms step_avg:320.68ms
step:1415/1700 train_loss:3.3786 train_time:450570ms step_avg:320.69ms
step:1416/1700 train_loss:3.4076 train_time:450908ms step_avg:320.70ms
step:1417/1700 train_loss:3.3888 train_time:451242ms step_avg:320.71ms
step:1418/1700 train_loss:3.4618 train_time:451578ms step_avg:320.72ms
step:1419/1700 train_loss:3.2732 train_time:451933ms step_avg:320.75ms
step:1420/1700 train_loss:3.3355 train_time:452276ms step_avg:320.76ms
step:1421/1700 train_loss:3.4396 train_time:452606ms step_avg:320.77ms
step:1422/1700 train_loss:3.3912 train_time:452946ms step_avg:320.78ms
step:1423/1700 train_loss:3.4040 train_time:453285ms step_avg:320.80ms
step:1424/1700 train_loss:3.4259 train_time:453621ms step_avg:320.81ms
step:1425/1700 train_loss:3.3898 train_time:453960ms step_avg:320.82ms
step:1426/1700 train_loss:3.3684 train_time:454290ms step_avg:320.83ms
step:1427/1700 train_loss:3.3811 train_time:454626ms step_avg:320.84ms
step:1428/1700 train_loss:3.2333 train_time:454977ms step_avg:320.86ms
step:1429/1700 train_loss:3.3776 train_time:455307ms step_avg:320.86ms
step:1430/1700 train_loss:3.3274 train_time:455645ms step_avg:320.88ms
step:1431/1700 train_loss:3.4305 train_time:455977ms step_avg:320.88ms
step:1432/1700 train_loss:3.4075 train_time:456307ms step_avg:320.89ms
step:1433/1700 train_loss:3.3103 train_time:456645ms step_avg:320.90ms
step:1434/1700 train_loss:3.3663 train_time:456984ms step_avg:320.92ms
step:1435/1700 train_loss:3.3862 train_time:457320ms step_avg:320.93ms
step:1436/1700 train_loss:3.1834 train_time:457669ms step_avg:320.95ms
step:1437/1700 train_loss:3.3431 train_time:458012ms step_avg:320.96ms
step:1438/1700 train_loss:3.1705 train_time:458346ms step_avg:320.97ms
step:1439/1700 train_loss:3.2719 train_time:458681ms step_avg:320.98ms
step:1440/1700 train_loss:3.4608 train_time:459026ms step_avg:321.00ms
step:1441/1700 train_loss:3.4254 train_time:459360ms step_avg:321.01ms
step:1442/1700 train_loss:3.3631 train_time:459699ms step_avg:321.02ms
step:1443/1700 train_loss:3.2377 train_time:460033ms step_avg:321.03ms
step:1444/1700 train_loss:3.3909 train_time:460370ms step_avg:321.04ms
step:1445/1700 train_loss:3.4418 train_time:460717ms step_avg:321.06ms
step:1446/1700 train_loss:3.5255 train_time:461063ms step_avg:321.07ms
step:1447/1700 train_loss:3.4994 train_time:461395ms step_avg:321.08ms
step:1448/1700 train_loss:3.3809 train_time:461731ms step_avg:321.09ms
step:1449/1700 train_loss:3.2547 train_time:462073ms step_avg:321.11ms
step:1450/1700 train_loss:3.3459 train_time:462409ms step_avg:321.12ms
step:1451/1700 train_loss:3.3467 train_time:462744ms step_avg:321.13ms
step:1452/1700 train_loss:3.4472 train_time:463073ms step_avg:321.13ms
step:1453/1700 train_loss:3.4390 train_time:463406ms step_avg:321.14ms
step:1454/1700 train_loss:3.2565 train_time:463743ms step_avg:321.15ms
step:1455/1700 train_loss:3.3772 train_time:464078ms step_avg:321.16ms
step:1456/1700 train_loss:3.3048 train_time:464416ms step_avg:321.17ms
step:1457/1700 train_loss:3.3338 train_time:464752ms step_avg:321.18ms
step:1458/1700 train_loss:3.3778 train_time:465095ms step_avg:321.20ms
step:1459/1700 train_loss:3.3264 train_time:465427ms step_avg:321.21ms
step:1460/1700 train_loss:3.2034 train_time:465763ms step_avg:321.22ms
step:1461/1700 train_loss:3.4703 train_time:466101ms step_avg:321.23ms
step:1462/1700 train_loss:3.3227 train_time:466433ms step_avg:321.23ms
step:1463/1700 train_loss:3.3633 train_time:466773ms step_avg:321.25ms
step:1464/1700 train_loss:3.4788 train_time:467103ms step_avg:321.25ms
step:1465/1700 train_loss:3.3093 train_time:467439ms step_avg:321.26ms
step:1466/1700 train_loss:3.5118 train_time:467776ms step_avg:321.27ms
step:1467/1700 train_loss:3.4048 train_time:468107ms step_avg:321.28ms
step:1468/1700 train_loss:3.3974 train_time:468437ms step_avg:321.29ms
step:1469/1700 train_loss:3.3341 train_time:468779ms step_avg:321.30ms
step:1470/1700 train_loss:3.4343 train_time:469110ms step_avg:321.31ms
step:1471/1700 train_loss:3.3311 train_time:469446ms step_avg:321.32ms
step:1472/1700 train_loss:3.3139 train_time:469782ms step_avg:321.33ms
step:1473/1700 train_loss:3.3776 train_time:470131ms step_avg:321.35ms
step:1474/1700 train_loss:3.2975 train_time:470468ms step_avg:321.36ms
step:1475/1700 train_loss:3.2805 train_time:470815ms step_avg:321.38ms
step:1476/1700 train_loss:3.4828 train_time:471145ms step_avg:321.38ms
step:1477/1700 train_loss:3.3608 train_time:471477ms step_avg:321.39ms
step:1478/1700 train_loss:3.1836 train_time:471811ms step_avg:321.40ms
step:1479/1700 train_loss:3.3054 train_time:472150ms step_avg:321.41ms
step:1480/1700 train_loss:3.2849 train_time:472494ms step_avg:321.42ms
step:1481/1700 train_loss:3.3562 train_time:472839ms step_avg:321.44ms
step:1482/1700 train_loss:3.4403 train_time:473171ms step_avg:321.45ms
step:1483/1700 train_loss:3.3217 train_time:473501ms step_avg:321.45ms
step:1484/1700 train_loss:3.4945 train_time:473834ms step_avg:321.46ms
step:1485/1700 train_loss:3.4121 train_time:474167ms step_avg:321.47ms
step:1486/1700 train_loss:3.3229 train_time:474516ms step_avg:321.49ms
step:1487/1700 train_loss:3.3072 train_time:474850ms step_avg:321.50ms
step:1488/1700 train_loss:3.3228 train_time:475183ms step_avg:321.50ms
step:1489/1700 train_loss:3.2669 train_time:475527ms step_avg:321.52ms
step:1490/1700 train_loss:3.3812 train_time:475867ms step_avg:321.53ms
step:1491/1700 train_loss:3.2784 train_time:476211ms step_avg:321.55ms
step:1492/1700 train_loss:3.3634 train_time:476546ms step_avg:321.56ms
step:1493/1700 train_loss:3.2917 train_time:476886ms step_avg:321.57ms
step:1494/1700 train_loss:3.2069 train_time:477214ms step_avg:321.57ms
step:1495/1700 train_loss:3.3037 train_time:477554ms step_avg:321.59ms
step:1496/1700 train_loss:3.4734 train_time:477889ms step_avg:321.59ms
step:1497/1700 train_loss:3.3410 train_time:478232ms step_avg:321.61ms
step:1498/1700 train_loss:3.0711 train_time:478571ms step_avg:321.62ms
step:1499/1700 train_loss:3.3999 train_time:478910ms step_avg:321.63ms
step:1500/1700 train_loss:3.3563 train_time:479246ms step_avg:321.64ms
step:1500/1700 val_loss:3.3205 train_time:479255ms step_avg:321.65ms
step:1501/1700 train_loss:3.3853 train_time:479597ms step_avg:321.66ms
step:1502/1700 train_loss:3.3533 train_time:479953ms step_avg:321.68ms
step:1503/1700 train_loss:3.3351 train_time:480292ms step_avg:321.70ms
step:1504/1700 train_loss:3.1216 train_time:480650ms step_avg:321.72ms
step:1505/1700 train_loss:3.4041 train_time:480997ms step_avg:321.74ms
step:1506/1700 train_loss:3.2889 train_time:481351ms step_avg:321.76ms
step:1507/1700 train_loss:3.2884 train_time:481696ms step_avg:321.77ms
step:1508/1700 train_loss:3.2531 train_time:482030ms step_avg:321.78ms
step:1509/1700 train_loss:3.3196 train_time:482362ms step_avg:321.79ms
step:1510/1700 train_loss:3.2147 train_time:482703ms step_avg:321.80ms
step:1511/1700 train_loss:3.5217 train_time:483042ms step_avg:321.81ms
step:1512/1700 train_loss:3.3145 train_time:483369ms step_avg:321.82ms
step:1513/1700 train_loss:3.3152 train_time:483709ms step_avg:321.83ms
step:1514/1700 train_loss:3.4499 train_time:484036ms step_avg:321.83ms
step:1515/1700 train_loss:3.4615 train_time:484387ms step_avg:321.85ms
step:1516/1700 train_loss:3.3070 train_time:484728ms step_avg:321.86ms
step:1517/1700 train_loss:3.1291 train_time:485075ms step_avg:321.88ms
step:1518/1700 train_loss:3.2773 train_time:485410ms step_avg:321.89ms
step:1519/1700 train_loss:3.2875 train_time:485756ms step_avg:321.91ms
step:1520/1700 train_loss:3.3413 train_time:486317ms step_avg:322.06ms
step:1521/1700 train_loss:3.2510 train_time:486653ms step_avg:322.07ms
step:1522/1700 train_loss:3.5387 train_time:486990ms step_avg:322.08ms
step:1523/1700 train_loss:3.1692 train_time:487323ms step_avg:322.09ms
step:1524/1700 train_loss:3.2807 train_time:487842ms step_avg:322.22ms
step:1525/1700 train_loss:3.3407 train_time:488184ms step_avg:322.23ms
step:1526/1700 train_loss:3.3282 train_time:488527ms step_avg:322.25ms
step:1527/1700 train_loss:3.3824 train_time:488868ms step_avg:322.26ms
step:1528/1700 train_loss:3.2792 train_time:489202ms step_avg:322.27ms
step:1529/1700 train_loss:3.2470 train_time:489534ms step_avg:322.27ms
step:1530/1700 train_loss:3.3034 train_time:489864ms step_avg:322.28ms
step:1531/1700 train_loss:3.3149 train_time:490200ms step_avg:322.29ms
step:1532/1700 train_loss:3.3200 train_time:490537ms step_avg:322.30ms
step:1533/1700 train_loss:3.2748 train_time:490907ms step_avg:322.33ms
step:1534/1700 train_loss:3.4400 train_time:491244ms step_avg:322.34ms
step:1535/1700 train_loss:3.2208 train_time:491582ms step_avg:322.35ms
step:1536/1700 train_loss:3.3712 train_time:491914ms step_avg:322.36ms
step:1537/1700 train_loss:3.2895 train_time:492258ms step_avg:322.37ms
step:1538/1700 train_loss:3.1729 train_time:492601ms step_avg:322.38ms
step:1539/1700 train_loss:3.1325 train_time:492945ms step_avg:322.40ms
step:1540/1700 train_loss:3.2282 train_time:493278ms step_avg:322.40ms
step:1541/1700 train_loss:3.2439 train_time:493623ms step_avg:322.42ms
step:1542/1700 train_loss:3.1543 train_time:493958ms step_avg:322.43ms
step:1543/1700 train_loss:3.4333 train_time:494295ms step_avg:322.44ms
step:1544/1700 train_loss:3.2511 train_time:494630ms step_avg:322.44ms
step:1545/1700 train_loss:3.1341 train_time:494995ms step_avg:322.47ms
step:1546/1700 train_loss:3.3159 train_time:495334ms step_avg:322.48ms
step:1547/1700 train_loss:3.3240 train_time:495671ms step_avg:322.49ms
step:1548/1700 train_loss:3.1084 train_time:496009ms step_avg:322.50ms
step:1549/1700 train_loss:3.4729 train_time:496344ms step_avg:322.51ms
step:1550/1700 train_loss:3.4363 train_time:496688ms step_avg:322.52ms
step:1551/1700 train_loss:3.2868 train_time:497044ms step_avg:322.55ms
step:1552/1700 train_loss:3.4529 train_time:497379ms step_avg:322.55ms
step:1553/1700 train_loss:3.3518 train_time:497716ms step_avg:322.56ms
step:1554/1700 train_loss:3.2847 train_time:498058ms step_avg:322.58ms
step:1555/1700 train_loss:3.4376 train_time:498396ms step_avg:322.59ms
step:1556/1700 train_loss:3.3993 train_time:498730ms step_avg:322.59ms
step:1557/1700 train_loss:3.2734 train_time:499061ms step_avg:322.60ms
step:1558/1700 train_loss:3.2293 train_time:499398ms step_avg:322.61ms
step:1559/1700 train_loss:3.2377 train_time:499734ms step_avg:322.62ms
step:1560/1700 train_loss:3.2731 train_time:500072ms step_avg:322.63ms
step:1561/1700 train_loss:3.3045 train_time:500407ms step_avg:322.63ms
step:1562/1700 train_loss:3.3053 train_time:500752ms step_avg:322.65ms
step:1563/1700 train_loss:3.3586 train_time:501098ms step_avg:322.66ms
step:1564/1700 train_loss:3.2660 train_time:501429ms step_avg:322.67ms
step:1565/1700 train_loss:3.3793 train_time:501765ms step_avg:322.68ms
step:1566/1700 train_loss:3.2475 train_time:502103ms step_avg:322.69ms
step:1567/1700 train_loss:3.3989 train_time:502439ms step_avg:322.70ms
step:1568/1700 train_loss:3.2097 train_time:502782ms step_avg:322.71ms
step:1569/1700 train_loss:3.2387 train_time:503119ms step_avg:322.72ms
step:1570/1700 train_loss:3.1578 train_time:503449ms step_avg:322.72ms
step:1571/1700 train_loss:3.1647 train_time:503789ms step_avg:322.73ms
step:1572/1700 train_loss:3.2138 train_time:504127ms step_avg:322.74ms
step:1573/1700 train_loss:3.2913 train_time:504460ms step_avg:322.75ms
step:1574/1700 train_loss:3.0690 train_time:504802ms step_avg:322.76ms
step:1575/1700 train_loss:3.2842 train_time:505136ms step_avg:322.77ms
step:1576/1700 train_loss:3.2986 train_time:505492ms step_avg:322.79ms
step:1577/1700 train_loss:3.2755 train_time:505836ms step_avg:322.81ms
step:1578/1700 train_loss:3.3077 train_time:506190ms step_avg:322.83ms
step:1579/1700 train_loss:3.2941 train_time:506525ms step_avg:322.83ms
step:1580/1700 train_loss:3.2447 train_time:506857ms step_avg:322.84ms
step:1581/1700 train_loss:3.4703 train_time:507196ms step_avg:322.85ms
step:1582/1700 train_loss:3.3361 train_time:507533ms step_avg:322.86ms
step:1583/1700 train_loss:3.3874 train_time:507865ms step_avg:322.86ms
step:1584/1700 train_loss:3.1195 train_time:508216ms step_avg:322.88ms
step:1585/1700 train_loss:3.2699 train_time:508565ms step_avg:322.90ms
step:1586/1700 train_loss:3.2008 train_time:508899ms step_avg:322.91ms
step:1587/1700 train_loss:3.3593 train_time:509232ms step_avg:322.91ms
step:1588/1700 train_loss:3.2668 train_time:509568ms step_avg:322.92ms
step:1589/1700 train_loss:3.2335 train_time:509900ms step_avg:322.93ms
step:1590/1700 train_loss:3.2269 train_time:510249ms step_avg:322.94ms
step:1591/1700 train_loss:3.2674 train_time:510580ms step_avg:322.95ms
step:1592/1700 train_loss:3.0963 train_time:510927ms step_avg:322.96ms
step:1593/1700 train_loss:3.2200 train_time:511262ms step_avg:322.97ms
step:1594/1700 train_loss:3.5660 train_time:511601ms step_avg:322.98ms
step:1595/1700 train_loss:3.1962 train_time:511938ms step_avg:322.99ms
step:1596/1700 train_loss:3.1585 train_time:512295ms step_avg:323.01ms
step:1597/1700 train_loss:3.3290 train_time:512629ms step_avg:323.02ms
step:1598/1700 train_loss:3.2553 train_time:512984ms step_avg:323.04ms
step:1599/1700 train_loss:3.1120 train_time:513326ms step_avg:323.05ms
step:1600/1700 train_loss:3.2718 train_time:513671ms step_avg:323.06ms
step:1601/1700 train_loss:3.2616 train_time:514003ms step_avg:323.07ms
step:1602/1700 train_loss:3.3308 train_time:514340ms step_avg:323.08ms
step:1603/1700 train_loss:3.2323 train_time:514673ms step_avg:323.08ms
step:1604/1700 train_loss:3.1397 train_time:515026ms step_avg:323.10ms
step:1605/1700 train_loss:3.3076 train_time:515367ms step_avg:323.11ms
step:1606/1700 train_loss:3.0929 train_time:515720ms step_avg:323.13ms
step:1607/1700 train_loss:3.3134 train_time:516055ms step_avg:323.14ms
step:1608/1700 train_loss:3.1776 train_time:516396ms step_avg:323.15ms
step:1609/1700 train_loss:3.2701 train_time:516732ms step_avg:323.16ms
step:1610/1700 train_loss:3.2933 train_time:517088ms step_avg:323.18ms
step:1611/1700 train_loss:3.1501 train_time:517421ms step_avg:323.19ms
step:1612/1700 train_loss:3.2840 train_time:517782ms step_avg:323.21ms
step:1613/1700 train_loss:3.2744 train_time:518117ms step_avg:323.22ms
step:1614/1700 train_loss:3.1157 train_time:518493ms step_avg:323.25ms
step:1615/1700 train_loss:3.3582 train_time:518849ms step_avg:323.27ms
step:1616/1700 train_loss:3.3280 train_time:519192ms step_avg:323.28ms
step:1617/1700 train_loss:3.2897 train_time:519527ms step_avg:323.29ms
step:1618/1700 train_loss:3.2006 train_time:519875ms step_avg:323.31ms
step:1619/1700 train_loss:3.3968 train_time:520213ms step_avg:323.31ms
step:1620/1700 train_loss:3.3475 train_time:520547ms step_avg:323.32ms
step:1621/1700 train_loss:3.2747 train_time:520882ms step_avg:323.33ms
step:1622/1700 train_loss:3.5140 train_time:521217ms step_avg:323.34ms
step:1623/1700 train_loss:3.2251 train_time:521560ms step_avg:323.35ms
step:1624/1700 train_loss:3.5103 train_time:521917ms step_avg:323.37ms
step:1625/1700 train_loss:3.2668 train_time:522251ms step_avg:323.38ms
step:1625/1700 val_loss:3.2879 train_time:522261ms step_avg:323.38ms
step:1626/1700 train_loss:3.3414 train_time:522591ms step_avg:323.39ms
step:1627/1700 train_loss:3.3600 train_time:522929ms step_avg:323.39ms
step:1628/1700 train_loss:3.3860 train_time:523261ms step_avg:323.40ms
step:1629/1700 train_loss:3.2354 train_time:523601ms step_avg:323.41ms
step:1630/1700 train_loss:3.3022 train_time:523941ms step_avg:323.42ms
step:1631/1700 train_loss:3.2667 train_time:524278ms step_avg:323.43ms
step:1632/1700 train_loss:3.1552 train_time:524616ms step_avg:323.44ms
step:1633/1700 train_loss:3.2760 train_time:524951ms step_avg:323.44ms
step:1634/1700 train_loss:3.2819 train_time:525287ms step_avg:323.45ms
step:1635/1700 train_loss:3.2756 train_time:525633ms step_avg:323.47ms
step:1636/1700 train_loss:3.3527 train_time:525965ms step_avg:323.47ms
step:1637/1700 train_loss:3.6239 train_time:526309ms step_avg:323.48ms
step:1638/1700 train_loss:3.2460 train_time:526675ms step_avg:323.51ms
step:1639/1700 train_loss:3.2190 train_time:527019ms step_avg:323.52ms
step:1640/1700 train_loss:3.2479 train_time:527358ms step_avg:323.53ms
step:1641/1700 train_loss:3.3537 train_time:527688ms step_avg:323.54ms
step:1642/1700 train_loss:3.3073 train_time:528023ms step_avg:323.54ms
step:1643/1700 train_loss:3.2924 train_time:528370ms step_avg:323.56ms
step:1644/1700 train_loss:3.2056 train_time:528704ms step_avg:323.56ms
step:1645/1700 train_loss:3.1918 train_time:529045ms step_avg:323.58ms
step:1646/1700 train_loss:3.2932 train_time:529386ms step_avg:323.59ms
step:1647/1700 train_loss:3.1586 train_time:529738ms step_avg:323.60ms
step:1648/1700 train_loss:3.3116 train_time:530072ms step_avg:323.61ms
step:1649/1700 train_loss:3.3498 train_time:530405ms step_avg:323.62ms
step:1650/1700 train_loss:3.2402 train_time:530740ms step_avg:323.62ms
step:1651/1700 train_loss:3.3191 train_time:531076ms step_avg:323.63ms
step:1652/1700 train_loss:3.2846 train_time:531419ms step_avg:323.64ms
step:1653/1700 train_loss:3.2257 train_time:531754ms step_avg:323.65ms
step:1654/1700 train_loss:3.3498 train_time:532092ms step_avg:323.66ms
step:1655/1700 train_loss:3.1918 train_time:532431ms step_avg:323.67ms
step:1656/1700 train_loss:2.9714 train_time:532777ms step_avg:323.68ms
step:1657/1700 train_loss:3.2970 train_time:533113ms step_avg:323.69ms
step:1658/1700 train_loss:3.3251 train_time:533447ms step_avg:323.69ms
step:1659/1700 train_loss:3.2690 train_time:533785ms step_avg:323.70ms
step:1660/1700 train_loss:3.5178 train_time:534139ms step_avg:323.72ms
step:1661/1700 train_loss:3.3587 train_time:534476ms step_avg:323.73ms
step:1662/1700 train_loss:3.3309 train_time:534815ms step_avg:323.74ms
step:1663/1700 train_loss:3.3041 train_time:535156ms step_avg:323.75ms
step:1664/1700 train_loss:3.3350 train_time:535493ms step_avg:323.76ms
step:1665/1700 train_loss:3.1582 train_time:535832ms step_avg:323.77ms
step:1666/1700 train_loss:3.3141 train_time:536168ms step_avg:323.77ms
step:1667/1700 train_loss:3.1006 train_time:536510ms step_avg:323.78ms
step:1668/1700 train_loss:3.2680 train_time:536844ms step_avg:323.79ms
step:1669/1700 train_loss:3.3035 train_time:537178ms step_avg:323.80ms
step:1670/1700 train_loss:3.1133 train_time:537515ms step_avg:323.80ms
step:1671/1700 train_loss:3.2702 train_time:537883ms step_avg:323.83ms
step:1672/1700 train_loss:3.1629 train_time:538226ms step_avg:323.84ms
step:1673/1700 train_loss:3.4512 train_time:538565ms step_avg:323.85ms
step:1674/1700 train_loss:3.3158 train_time:538899ms step_avg:323.86ms
step:1675/1700 train_loss:3.2834 train_time:539235ms step_avg:323.86ms
step:1676/1700 train_loss:3.1618 train_time:539576ms step_avg:323.88ms
step:1677/1700 train_loss:3.2309 train_time:539919ms step_avg:323.89ms
step:1678/1700 train_loss:3.5667 train_time:540256ms step_avg:323.89ms
step:1679/1700 train_loss:3.2842 train_time:540597ms step_avg:323.90ms
step:1680/1700 train_loss:3.2718 train_time:540936ms step_avg:323.91ms
step:1681/1700 train_loss:3.4003 train_time:541276ms step_avg:323.92ms
step:1682/1700 train_loss:3.3253 train_time:541617ms step_avg:323.93ms
step:1683/1700 train_loss:3.2364 train_time:541966ms step_avg:323.95ms
step:1684/1700 train_loss:3.3331 train_time:542302ms step_avg:323.96ms
step:1685/1700 train_loss:3.1618 train_time:542638ms step_avg:323.96ms
step:1686/1700 train_loss:3.3294 train_time:542979ms step_avg:323.97ms
step:1687/1700 train_loss:3.2217 train_time:543332ms step_avg:323.99ms
step:1688/1700 train_loss:3.0400 train_time:543670ms step_avg:324.00ms
step:1689/1700 train_loss:3.3449 train_time:544018ms step_avg:324.01ms
step:1690/1700 train_loss:3.3203 train_time:544355ms step_avg:324.02ms
step:1691/1700 train_loss:3.3145 train_time:544698ms step_avg:324.03ms
step:1692/1700 train_loss:3.2142 train_time:545058ms step_avg:324.05ms
step:1693/1700 train_loss:3.2114 train_time:545395ms step_avg:324.06ms
step:1694/1700 train_loss:3.3029 train_time:545732ms step_avg:324.07ms
step:1695/1700 train_loss:3.2274 train_time:546068ms step_avg:324.08ms
step:1696/1700 train_loss:3.3177 train_time:546407ms step_avg:324.08ms
step:1697/1700 train_loss:3.2693 train_time:546749ms step_avg:324.10ms
step:1698/1700 train_loss:3.3766 train_time:547085ms step_avg:324.10ms
step:1699/1700 train_loss:3.1964 train_time:547433ms step_avg:324.12ms
step:1700/1700 train_loss:3.2610 train_time:547770ms step_avg:324.12ms
step:1700/1700 val_loss:3.2775 train_time:547779ms step_avg:324.13ms
