import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.1357, -4.2084, 1.0726),
        (4.132, -4.2045, 1.0725),
        (4.077, -4.1489, 1.0719),
        (4.0422, -4.1139, 1.0717),
        (3.9129, -3.9845, 1.0715),
        (3.3337, -3.2386, 0.9049),
        (2.2005, -1.6921, 0.4915),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 06:48:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            108W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            106W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0            109W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.09ms
step:1/1770 train_time:65ms step_avg:65.13ms
step:2/1770 train_time:137ms step_avg:68.72ms
step:3/1770 train_time:230ms step_avg:76.71ms
step:4/1770 train_time:325ms step_avg:81.26ms
step:5/1770 train_time:421ms step_avg:84.21ms
step:6/1770 train_time:517ms step_avg:86.25ms
step:7/1770 train_time:613ms step_avg:87.62ms
step:8/1770 train_time:709ms step_avg:88.63ms
step:9/1770 train_time:805ms step_avg:89.48ms
step:10/1770 train_time:902ms step_avg:90.22ms
step:11/1770 train_time:998ms step_avg:90.69ms
step:12/1770 train_time:1093ms step_avg:91.12ms
step:13/1770 train_time:1189ms step_avg:91.49ms
step:14/1770 train_time:1286ms step_avg:91.83ms
step:15/1770 train_time:1381ms step_avg:92.08ms
step:16/1770 train_time:1477ms step_avg:92.33ms
step:17/1770 train_time:1574ms step_avg:92.57ms
step:18/1770 train_time:1671ms step_avg:92.84ms
step:19/1770 train_time:1767ms step_avg:93.00ms
step:20/1770 train_time:1863ms step_avg:93.15ms
step:21/1770 train_time:1959ms step_avg:93.29ms
step:22/1770 train_time:2055ms step_avg:93.41ms
step:23/1770 train_time:2151ms step_avg:93.53ms
step:24/1770 train_time:2248ms step_avg:93.66ms
step:25/1770 train_time:2344ms step_avg:93.77ms
step:26/1770 train_time:2440ms step_avg:93.84ms
step:27/1770 train_time:2536ms step_avg:93.91ms
step:28/1770 train_time:2632ms step_avg:94.02ms
step:29/1770 train_time:2728ms step_avg:94.07ms
step:30/1770 train_time:2824ms step_avg:94.13ms
step:31/1770 train_time:2920ms step_avg:94.19ms
step:32/1770 train_time:3016ms step_avg:94.26ms
step:33/1770 train_time:3113ms step_avg:94.35ms
step:34/1770 train_time:3209ms step_avg:94.39ms
step:35/1770 train_time:3306ms step_avg:94.44ms
step:36/1770 train_time:3402ms step_avg:94.49ms
step:37/1770 train_time:3497ms step_avg:94.52ms
step:38/1770 train_time:3594ms step_avg:94.58ms
step:39/1770 train_time:3689ms step_avg:94.60ms
step:40/1770 train_time:3785ms step_avg:94.63ms
step:41/1770 train_time:3881ms step_avg:94.66ms
step:42/1770 train_time:3977ms step_avg:94.69ms
step:43/1770 train_time:4074ms step_avg:94.74ms
step:44/1770 train_time:4170ms step_avg:94.77ms
step:45/1770 train_time:4267ms step_avg:94.81ms
step:46/1770 train_time:4363ms step_avg:94.84ms
step:47/1770 train_time:4459ms step_avg:94.87ms
step:48/1770 train_time:4555ms step_avg:94.90ms
step:49/1770 train_time:4651ms step_avg:94.92ms
step:50/1770 train_time:4747ms step_avg:94.94ms
step:51/1770 train_time:4843ms step_avg:94.97ms
step:52/1770 train_time:4939ms step_avg:94.98ms
step:53/1770 train_time:5035ms step_avg:95.01ms
step:54/1770 train_time:5132ms step_avg:95.03ms
step:55/1770 train_time:5227ms step_avg:95.04ms
step:56/1770 train_time:5324ms step_avg:95.06ms
step:57/1770 train_time:5419ms step_avg:95.08ms
step:58/1770 train_time:5516ms step_avg:95.11ms
step:59/1770 train_time:5613ms step_avg:95.13ms
step:60/1770 train_time:5709ms step_avg:95.15ms
step:61/1770 train_time:5804ms step_avg:95.15ms
step:62/1770 train_time:5900ms step_avg:95.17ms
step:63/1770 train_time:5997ms step_avg:95.19ms
step:64/1770 train_time:6093ms step_avg:95.21ms
step:65/1770 train_time:6189ms step_avg:95.22ms
step:66/1770 train_time:6286ms step_avg:95.24ms
step:67/1770 train_time:6381ms step_avg:95.24ms
step:68/1770 train_time:6477ms step_avg:95.25ms
step:69/1770 train_time:6573ms step_avg:95.26ms
step:70/1770 train_time:6669ms step_avg:95.27ms
step:71/1770 train_time:6765ms step_avg:95.28ms
step:72/1770 train_time:6861ms step_avg:95.29ms
step:73/1770 train_time:6957ms step_avg:95.30ms
step:74/1770 train_time:7053ms step_avg:95.31ms
step:75/1770 train_time:7149ms step_avg:95.32ms
step:76/1770 train_time:7244ms step_avg:95.32ms
step:77/1770 train_time:7340ms step_avg:95.33ms
step:78/1770 train_time:7436ms step_avg:95.34ms
step:79/1770 train_time:7533ms step_avg:95.35ms
step:80/1770 train_time:7629ms step_avg:95.36ms
step:81/1770 train_time:7725ms step_avg:95.36ms
step:82/1770 train_time:7820ms step_avg:95.36ms
step:83/1770 train_time:7916ms step_avg:95.37ms
step:84/1770 train_time:8011ms step_avg:95.37ms
step:85/1770 train_time:8108ms step_avg:95.39ms
step:86/1770 train_time:8203ms step_avg:95.38ms
step:87/1770 train_time:8299ms step_avg:95.39ms
step:88/1770 train_time:8394ms step_avg:95.39ms
step:89/1770 train_time:8490ms step_avg:95.40ms
step:90/1770 train_time:8586ms step_avg:95.40ms
step:91/1770 train_time:8682ms step_avg:95.40ms
step:92/1770 train_time:8778ms step_avg:95.42ms
step:93/1770 train_time:8874ms step_avg:95.42ms
step:94/1770 train_time:8970ms step_avg:95.42ms
step:95/1770 train_time:9066ms step_avg:95.43ms
step:96/1770 train_time:9161ms step_avg:95.43ms
step:97/1770 train_time:9257ms step_avg:95.43ms
step:98/1770 train_time:9354ms step_avg:95.44ms
step:99/1770 train_time:9450ms step_avg:95.46ms
step:100/1770 train_time:9545ms step_avg:95.45ms
step:101/1770 train_time:9641ms step_avg:95.45ms
step:102/1770 train_time:9736ms step_avg:95.46ms
step:103/1770 train_time:9833ms step_avg:95.47ms
step:104/1770 train_time:9929ms step_avg:95.47ms
step:105/1770 train_time:10024ms step_avg:95.47ms
step:106/1770 train_time:10121ms step_avg:95.48ms
step:107/1770 train_time:10216ms step_avg:95.48ms
step:108/1770 train_time:10313ms step_avg:95.49ms
step:109/1770 train_time:10409ms step_avg:95.49ms
step:110/1770 train_time:10504ms step_avg:95.49ms
step:111/1770 train_time:10600ms step_avg:95.50ms
step:112/1770 train_time:10696ms step_avg:95.50ms
step:113/1770 train_time:10793ms step_avg:95.51ms
step:114/1770 train_time:10888ms step_avg:95.51ms
step:115/1770 train_time:10983ms step_avg:95.51ms
step:116/1770 train_time:11079ms step_avg:95.51ms
step:117/1770 train_time:11175ms step_avg:95.51ms
step:118/1770 train_time:11272ms step_avg:95.52ms
step:119/1770 train_time:11367ms step_avg:95.52ms
step:120/1770 train_time:11463ms step_avg:95.53ms
step:121/1770 train_time:11559ms step_avg:95.53ms
step:122/1770 train_time:11655ms step_avg:95.53ms
step:123/1770 train_time:11751ms step_avg:95.54ms
step:124/1770 train_time:11847ms step_avg:95.54ms
step:125/1770 train_time:11943ms step_avg:95.54ms
step:125/1770 val_loss:4.6439 train_time:12033ms step_avg:96.26ms
step:126/1770 train_time:12056ms step_avg:95.68ms
step:127/1770 train_time:12140ms step_avg:95.59ms
step:128/1770 train_time:12242ms step_avg:95.64ms
step:129/1770 train_time:12338ms step_avg:95.65ms
step:130/1770 train_time:12435ms step_avg:95.65ms
step:131/1770 train_time:12531ms step_avg:95.65ms
step:132/1770 train_time:12626ms step_avg:95.65ms
step:133/1770 train_time:12722ms step_avg:95.65ms
step:134/1770 train_time:12818ms step_avg:95.66ms
step:135/1770 train_time:12915ms step_avg:95.67ms
step:136/1770 train_time:13012ms step_avg:95.68ms
step:137/1770 train_time:13108ms step_avg:95.68ms
step:138/1770 train_time:13204ms step_avg:95.68ms
step:139/1770 train_time:13301ms step_avg:95.69ms
step:140/1770 train_time:13398ms step_avg:95.70ms
step:141/1770 train_time:13494ms step_avg:95.70ms
step:142/1770 train_time:13591ms step_avg:95.71ms
step:143/1770 train_time:13687ms step_avg:95.71ms
step:144/1770 train_time:13783ms step_avg:95.72ms
step:145/1770 train_time:13879ms step_avg:95.72ms
step:146/1770 train_time:13976ms step_avg:95.72ms
step:147/1770 train_time:14072ms step_avg:95.73ms
step:148/1770 train_time:14169ms step_avg:95.73ms
step:149/1770 train_time:14265ms step_avg:95.74ms
step:150/1770 train_time:14362ms step_avg:95.74ms
step:151/1770 train_time:14458ms step_avg:95.75ms
step:152/1770 train_time:14554ms step_avg:95.75ms
step:153/1770 train_time:14651ms step_avg:95.76ms
step:154/1770 train_time:14747ms step_avg:95.76ms
step:155/1770 train_time:14844ms step_avg:95.77ms
step:156/1770 train_time:14940ms step_avg:95.77ms
step:157/1770 train_time:15037ms step_avg:95.77ms
step:158/1770 train_time:15133ms step_avg:95.78ms
step:159/1770 train_time:15230ms step_avg:95.78ms
step:160/1770 train_time:15326ms step_avg:95.79ms
step:161/1770 train_time:15422ms step_avg:95.79ms
step:162/1770 train_time:15519ms step_avg:95.80ms
step:163/1770 train_time:15616ms step_avg:95.80ms
step:164/1770 train_time:15713ms step_avg:95.81ms
step:165/1770 train_time:15810ms step_avg:95.82ms
step:166/1770 train_time:15906ms step_avg:95.82ms
step:167/1770 train_time:16002ms step_avg:95.82ms
step:168/1770 train_time:16099ms step_avg:95.83ms
step:169/1770 train_time:16195ms step_avg:95.83ms
step:170/1770 train_time:16292ms step_avg:95.84ms
step:171/1770 train_time:16389ms step_avg:95.84ms
step:172/1770 train_time:16487ms step_avg:95.85ms
step:173/1770 train_time:16584ms step_avg:95.86ms
step:174/1770 train_time:16681ms step_avg:95.87ms
step:175/1770 train_time:16778ms step_avg:95.87ms
step:176/1770 train_time:16874ms step_avg:95.87ms
step:177/1770 train_time:16970ms step_avg:95.88ms
step:178/1770 train_time:17066ms step_avg:95.88ms
step:179/1770 train_time:17163ms step_avg:95.88ms
step:180/1770 train_time:17259ms step_avg:95.88ms
step:181/1770 train_time:17356ms step_avg:95.89ms
step:182/1770 train_time:17453ms step_avg:95.89ms
step:183/1770 train_time:17549ms step_avg:95.90ms
step:184/1770 train_time:17646ms step_avg:95.90ms
step:185/1770 train_time:17743ms step_avg:95.91ms
step:186/1770 train_time:17840ms step_avg:95.91ms
step:187/1770 train_time:17936ms step_avg:95.91ms
step:188/1770 train_time:18033ms step_avg:95.92ms
step:189/1770 train_time:18129ms step_avg:95.92ms
step:190/1770 train_time:18226ms step_avg:95.93ms
step:191/1770 train_time:18322ms step_avg:95.93ms
step:192/1770 train_time:18419ms step_avg:95.93ms
step:193/1770 train_time:18516ms step_avg:95.94ms
step:194/1770 train_time:18613ms step_avg:95.94ms
step:195/1770 train_time:18709ms step_avg:95.94ms
step:196/1770 train_time:18807ms step_avg:95.95ms
step:197/1770 train_time:18903ms step_avg:95.95ms
step:198/1770 train_time:19000ms step_avg:95.96ms
step:199/1770 train_time:19096ms step_avg:95.96ms
step:200/1770 train_time:19193ms step_avg:95.96ms
step:201/1770 train_time:19289ms step_avg:95.97ms
step:202/1770 train_time:19386ms step_avg:95.97ms
step:203/1770 train_time:19483ms step_avg:95.98ms
step:204/1770 train_time:19580ms step_avg:95.98ms
step:205/1770 train_time:19676ms step_avg:95.98ms
step:206/1770 train_time:19773ms step_avg:95.98ms
step:207/1770 train_time:19869ms step_avg:95.99ms
step:208/1770 train_time:19966ms step_avg:95.99ms
step:209/1770 train_time:20062ms step_avg:95.99ms
step:210/1770 train_time:20158ms step_avg:95.99ms
step:211/1770 train_time:20255ms step_avg:96.00ms
step:212/1770 train_time:20351ms step_avg:96.00ms
step:213/1770 train_time:20448ms step_avg:96.00ms
step:214/1770 train_time:20544ms step_avg:96.00ms
step:215/1770 train_time:20641ms step_avg:96.00ms
step:216/1770 train_time:20738ms step_avg:96.01ms
step:217/1770 train_time:20834ms step_avg:96.01ms
step:218/1770 train_time:20931ms step_avg:96.01ms
step:219/1770 train_time:21028ms step_avg:96.02ms
step:220/1770 train_time:21125ms step_avg:96.02ms
step:221/1770 train_time:21221ms step_avg:96.02ms
step:222/1770 train_time:21317ms step_avg:96.02ms
step:223/1770 train_time:21414ms step_avg:96.03ms
step:224/1770 train_time:21511ms step_avg:96.03ms
step:225/1770 train_time:21609ms step_avg:96.04ms
step:226/1770 train_time:21706ms step_avg:96.04ms
step:227/1770 train_time:21802ms step_avg:96.05ms
step:228/1770 train_time:21899ms step_avg:96.05ms
step:229/1770 train_time:21995ms step_avg:96.05ms
step:230/1770 train_time:22092ms step_avg:96.05ms
step:231/1770 train_time:22188ms step_avg:96.05ms
step:232/1770 train_time:22285ms step_avg:96.06ms
step:233/1770 train_time:22382ms step_avg:96.06ms
step:234/1770 train_time:22478ms step_avg:96.06ms
step:235/1770 train_time:22574ms step_avg:96.06ms
step:236/1770 train_time:22671ms step_avg:96.06ms
step:237/1770 train_time:22768ms step_avg:96.07ms
step:238/1770 train_time:22865ms step_avg:96.07ms
step:239/1770 train_time:22962ms step_avg:96.08ms
step:240/1770 train_time:23058ms step_avg:96.08ms
step:241/1770 train_time:23156ms step_avg:96.08ms
step:242/1770 train_time:23252ms step_avg:96.08ms
step:243/1770 train_time:23349ms step_avg:96.09ms
step:244/1770 train_time:23446ms step_avg:96.09ms
step:245/1770 train_time:23543ms step_avg:96.09ms
step:246/1770 train_time:23639ms step_avg:96.09ms
step:247/1770 train_time:23735ms step_avg:96.09ms
step:248/1770 train_time:23832ms step_avg:96.10ms
step:249/1770 train_time:23928ms step_avg:96.10ms
step:250/1770 train_time:24025ms step_avg:96.10ms
step:250/1770 val_loss:4.1092 train_time:24115ms step_avg:96.46ms
step:251/1770 train_time:24137ms step_avg:96.16ms
step:252/1770 train_time:24223ms step_avg:96.12ms
step:253/1770 train_time:24320ms step_avg:96.13ms
step:254/1770 train_time:24419ms step_avg:96.14ms
step:255/1770 train_time:24515ms step_avg:96.14ms
step:256/1770 train_time:24611ms step_avg:96.14ms
step:257/1770 train_time:24707ms step_avg:96.14ms
step:258/1770 train_time:24803ms step_avg:96.14ms
step:259/1770 train_time:24900ms step_avg:96.14ms
step:260/1770 train_time:24996ms step_avg:96.14ms
step:261/1770 train_time:25093ms step_avg:96.14ms
step:262/1770 train_time:25191ms step_avg:96.15ms
step:263/1770 train_time:25288ms step_avg:96.15ms
step:264/1770 train_time:25385ms step_avg:96.15ms
step:265/1770 train_time:25482ms step_avg:96.16ms
step:266/1770 train_time:25579ms step_avg:96.16ms
step:267/1770 train_time:25676ms step_avg:96.16ms
step:268/1770 train_time:25773ms step_avg:96.17ms
step:269/1770 train_time:25871ms step_avg:96.17ms
step:270/1770 train_time:25967ms step_avg:96.17ms
step:271/1770 train_time:26064ms step_avg:96.18ms
step:272/1770 train_time:26161ms step_avg:96.18ms
step:273/1770 train_time:26258ms step_avg:96.18ms
step:274/1770 train_time:26355ms step_avg:96.19ms
step:275/1770 train_time:26454ms step_avg:96.19ms
step:276/1770 train_time:26551ms step_avg:96.20ms
step:277/1770 train_time:26648ms step_avg:96.20ms
step:278/1770 train_time:26744ms step_avg:96.20ms
step:279/1770 train_time:26841ms step_avg:96.21ms
step:280/1770 train_time:26939ms step_avg:96.21ms
step:281/1770 train_time:27037ms step_avg:96.22ms
step:282/1770 train_time:27134ms step_avg:96.22ms
step:283/1770 train_time:27232ms step_avg:96.23ms
step:284/1770 train_time:27328ms step_avg:96.23ms
step:285/1770 train_time:27425ms step_avg:96.23ms
step:286/1770 train_time:27522ms step_avg:96.23ms
step:287/1770 train_time:27619ms step_avg:96.23ms
step:288/1770 train_time:27716ms step_avg:96.24ms
step:289/1770 train_time:27813ms step_avg:96.24ms
step:290/1770 train_time:27911ms step_avg:96.25ms
step:291/1770 train_time:28009ms step_avg:96.25ms
step:292/1770 train_time:28105ms step_avg:96.25ms
step:293/1770 train_time:28203ms step_avg:96.26ms
step:294/1770 train_time:28299ms step_avg:96.26ms
step:295/1770 train_time:28396ms step_avg:96.26ms
step:296/1770 train_time:28493ms step_avg:96.26ms
step:297/1770 train_time:28590ms step_avg:96.26ms
step:298/1770 train_time:28687ms step_avg:96.27ms
step:299/1770 train_time:28784ms step_avg:96.27ms
step:300/1770 train_time:28881ms step_avg:96.27ms
step:301/1770 train_time:28979ms step_avg:96.27ms
step:302/1770 train_time:29076ms step_avg:96.28ms
step:303/1770 train_time:29173ms step_avg:96.28ms
step:304/1770 train_time:29270ms step_avg:96.28ms
step:305/1770 train_time:29368ms step_avg:96.29ms
step:306/1770 train_time:29466ms step_avg:96.29ms
step:307/1770 train_time:29563ms step_avg:96.29ms
step:308/1770 train_time:29660ms step_avg:96.30ms
step:309/1770 train_time:29756ms step_avg:96.30ms
step:310/1770 train_time:29853ms step_avg:96.30ms
step:311/1770 train_time:29951ms step_avg:96.31ms
step:312/1770 train_time:30048ms step_avg:96.31ms
step:313/1770 train_time:30145ms step_avg:96.31ms
step:314/1770 train_time:30242ms step_avg:96.31ms
step:315/1770 train_time:30339ms step_avg:96.32ms
step:316/1770 train_time:30437ms step_avg:96.32ms
step:317/1770 train_time:30535ms step_avg:96.33ms
step:318/1770 train_time:30633ms step_avg:96.33ms
step:319/1770 train_time:30731ms step_avg:96.33ms
step:320/1770 train_time:30828ms step_avg:96.34ms
step:321/1770 train_time:30924ms step_avg:96.34ms
step:322/1770 train_time:31021ms step_avg:96.34ms
step:323/1770 train_time:31119ms step_avg:96.34ms
step:324/1770 train_time:31215ms step_avg:96.34ms
step:325/1770 train_time:31312ms step_avg:96.35ms
step:326/1770 train_time:31410ms step_avg:96.35ms
step:327/1770 train_time:31507ms step_avg:96.35ms
step:328/1770 train_time:31604ms step_avg:96.35ms
step:329/1770 train_time:31701ms step_avg:96.36ms
step:330/1770 train_time:31798ms step_avg:96.36ms
step:331/1770 train_time:31896ms step_avg:96.36ms
step:332/1770 train_time:31992ms step_avg:96.36ms
step:333/1770 train_time:32090ms step_avg:96.37ms
step:334/1770 train_time:32187ms step_avg:96.37ms
step:335/1770 train_time:32284ms step_avg:96.37ms
step:336/1770 train_time:32382ms step_avg:96.37ms
step:337/1770 train_time:32479ms step_avg:96.38ms
step:338/1770 train_time:32576ms step_avg:96.38ms
step:339/1770 train_time:32674ms step_avg:96.38ms
step:340/1770 train_time:32770ms step_avg:96.38ms
step:341/1770 train_time:32867ms step_avg:96.38ms
step:342/1770 train_time:32964ms step_avg:96.39ms
step:343/1770 train_time:33061ms step_avg:96.39ms
step:344/1770 train_time:33159ms step_avg:96.39ms
step:345/1770 train_time:33255ms step_avg:96.39ms
step:346/1770 train_time:33352ms step_avg:96.39ms
step:347/1770 train_time:33450ms step_avg:96.40ms
step:348/1770 train_time:33547ms step_avg:96.40ms
step:349/1770 train_time:33644ms step_avg:96.40ms
step:350/1770 train_time:33741ms step_avg:96.40ms
step:351/1770 train_time:33838ms step_avg:96.41ms
step:352/1770 train_time:33936ms step_avg:96.41ms
step:353/1770 train_time:34034ms step_avg:96.41ms
step:354/1770 train_time:34131ms step_avg:96.41ms
step:355/1770 train_time:34228ms step_avg:96.42ms
step:356/1770 train_time:34325ms step_avg:96.42ms
step:357/1770 train_time:34422ms step_avg:96.42ms
step:358/1770 train_time:34519ms step_avg:96.42ms
step:359/1770 train_time:34616ms step_avg:96.42ms
step:360/1770 train_time:34713ms step_avg:96.42ms
step:361/1770 train_time:34809ms step_avg:96.42ms
step:362/1770 train_time:34906ms step_avg:96.43ms
step:363/1770 train_time:35003ms step_avg:96.43ms
step:364/1770 train_time:35100ms step_avg:96.43ms
step:365/1770 train_time:35197ms step_avg:96.43ms
step:366/1770 train_time:35294ms step_avg:96.43ms
step:367/1770 train_time:35391ms step_avg:96.43ms
step:368/1770 train_time:35488ms step_avg:96.43ms
step:369/1770 train_time:35585ms step_avg:96.44ms
step:370/1770 train_time:35682ms step_avg:96.44ms
step:371/1770 train_time:35779ms step_avg:96.44ms
step:372/1770 train_time:35876ms step_avg:96.44ms
step:373/1770 train_time:35973ms step_avg:96.44ms
step:374/1770 train_time:36071ms step_avg:96.45ms
step:375/1770 train_time:36168ms step_avg:96.45ms
step:375/1770 val_loss:3.9115 train_time:36259ms step_avg:96.69ms
step:376/1770 train_time:36281ms step_avg:96.49ms
step:377/1770 train_time:36369ms step_avg:96.47ms
step:378/1770 train_time:36470ms step_avg:96.48ms
step:379/1770 train_time:36567ms step_avg:96.48ms
step:380/1770 train_time:36665ms step_avg:96.49ms
step:381/1770 train_time:36761ms step_avg:96.49ms
step:382/1770 train_time:36858ms step_avg:96.49ms
step:383/1770 train_time:36955ms step_avg:96.49ms
step:384/1770 train_time:37051ms step_avg:96.49ms
step:385/1770 train_time:37148ms step_avg:96.49ms
step:386/1770 train_time:37245ms step_avg:96.49ms
step:387/1770 train_time:37342ms step_avg:96.49ms
step:388/1770 train_time:37441ms step_avg:96.50ms
step:389/1770 train_time:37539ms step_avg:96.50ms
step:390/1770 train_time:37636ms step_avg:96.50ms
step:391/1770 train_time:37733ms step_avg:96.50ms
step:392/1770 train_time:37830ms step_avg:96.50ms
step:393/1770 train_time:37927ms step_avg:96.51ms
step:394/1770 train_time:38024ms step_avg:96.51ms
step:395/1770 train_time:38121ms step_avg:96.51ms
step:396/1770 train_time:38220ms step_avg:96.51ms
step:397/1770 train_time:38318ms step_avg:96.52ms
step:398/1770 train_time:38417ms step_avg:96.53ms
step:399/1770 train_time:38516ms step_avg:96.53ms
step:400/1770 train_time:38615ms step_avg:96.54ms
step:401/1770 train_time:38714ms step_avg:96.54ms
step:402/1770 train_time:38813ms step_avg:96.55ms
step:403/1770 train_time:38913ms step_avg:96.56ms
step:404/1770 train_time:39012ms step_avg:96.56ms
step:405/1770 train_time:39113ms step_avg:96.57ms
step:406/1770 train_time:39210ms step_avg:96.58ms
step:407/1770 train_time:39309ms step_avg:96.58ms
step:408/1770 train_time:39408ms step_avg:96.59ms
step:409/1770 train_time:39508ms step_avg:96.60ms
step:410/1770 train_time:39607ms step_avg:96.60ms
step:411/1770 train_time:39705ms step_avg:96.61ms
step:412/1770 train_time:39804ms step_avg:96.61ms
step:413/1770 train_time:39904ms step_avg:96.62ms
step:414/1770 train_time:40003ms step_avg:96.63ms
step:415/1770 train_time:40102ms step_avg:96.63ms
step:416/1770 train_time:40201ms step_avg:96.64ms
step:417/1770 train_time:40300ms step_avg:96.64ms
step:418/1770 train_time:40401ms step_avg:96.65ms
step:419/1770 train_time:40501ms step_avg:96.66ms
step:420/1770 train_time:40601ms step_avg:96.67ms
step:421/1770 train_time:40700ms step_avg:96.67ms
step:422/1770 train_time:40800ms step_avg:96.68ms
step:423/1770 train_time:40898ms step_avg:96.69ms
step:424/1770 train_time:40998ms step_avg:96.69ms
step:425/1770 train_time:41097ms step_avg:96.70ms
step:426/1770 train_time:41197ms step_avg:96.71ms
step:427/1770 train_time:41296ms step_avg:96.71ms
step:428/1770 train_time:41395ms step_avg:96.72ms
step:429/1770 train_time:41494ms step_avg:96.72ms
step:430/1770 train_time:41594ms step_avg:96.73ms
step:431/1770 train_time:41692ms step_avg:96.73ms
step:432/1770 train_time:41791ms step_avg:96.74ms
step:433/1770 train_time:41890ms step_avg:96.74ms
step:434/1770 train_time:41989ms step_avg:96.75ms
step:435/1770 train_time:42088ms step_avg:96.75ms
step:436/1770 train_time:42187ms step_avg:96.76ms
step:437/1770 train_time:42287ms step_avg:96.77ms
step:438/1770 train_time:42385ms step_avg:96.77ms
step:439/1770 train_time:42484ms step_avg:96.78ms
step:440/1770 train_time:42584ms step_avg:96.78ms
step:441/1770 train_time:42685ms step_avg:96.79ms
step:442/1770 train_time:42785ms step_avg:96.80ms
step:443/1770 train_time:42883ms step_avg:96.80ms
step:444/1770 train_time:42983ms step_avg:96.81ms
step:445/1770 train_time:43083ms step_avg:96.81ms
step:446/1770 train_time:43184ms step_avg:96.82ms
step:447/1770 train_time:43284ms step_avg:96.83ms
step:448/1770 train_time:43383ms step_avg:96.84ms
step:449/1770 train_time:43483ms step_avg:96.84ms
step:450/1770 train_time:43582ms step_avg:96.85ms
step:451/1770 train_time:43681ms step_avg:96.85ms
step:452/1770 train_time:43779ms step_avg:96.86ms
step:453/1770 train_time:43878ms step_avg:96.86ms
step:454/1770 train_time:43978ms step_avg:96.87ms
step:455/1770 train_time:44077ms step_avg:96.87ms
step:456/1770 train_time:44176ms step_avg:96.88ms
step:457/1770 train_time:44275ms step_avg:96.88ms
step:458/1770 train_time:44375ms step_avg:96.89ms
step:459/1770 train_time:44474ms step_avg:96.89ms
step:460/1770 train_time:44573ms step_avg:96.90ms
step:461/1770 train_time:44672ms step_avg:96.90ms
step:462/1770 train_time:44771ms step_avg:96.91ms
step:463/1770 train_time:44870ms step_avg:96.91ms
step:464/1770 train_time:44969ms step_avg:96.92ms
step:465/1770 train_time:45069ms step_avg:96.92ms
step:466/1770 train_time:45168ms step_avg:96.93ms
step:467/1770 train_time:45267ms step_avg:96.93ms
step:468/1770 train_time:45367ms step_avg:96.94ms
step:469/1770 train_time:45466ms step_avg:96.94ms
step:470/1770 train_time:45566ms step_avg:96.95ms
step:471/1770 train_time:45666ms step_avg:96.96ms
step:472/1770 train_time:45766ms step_avg:96.96ms
step:473/1770 train_time:45866ms step_avg:96.97ms
step:474/1770 train_time:45965ms step_avg:96.97ms
step:475/1770 train_time:46065ms step_avg:96.98ms
step:476/1770 train_time:46164ms step_avg:96.98ms
step:477/1770 train_time:46263ms step_avg:96.99ms
step:478/1770 train_time:46362ms step_avg:96.99ms
step:479/1770 train_time:46462ms step_avg:97.00ms
step:480/1770 train_time:46561ms step_avg:97.00ms
step:481/1770 train_time:46661ms step_avg:97.01ms
step:482/1770 train_time:46761ms step_avg:97.02ms
step:483/1770 train_time:46862ms step_avg:97.02ms
step:484/1770 train_time:46962ms step_avg:97.03ms
step:485/1770 train_time:47062ms step_avg:97.03ms
step:486/1770 train_time:47162ms step_avg:97.04ms
step:487/1770 train_time:47261ms step_avg:97.04ms
step:488/1770 train_time:47360ms step_avg:97.05ms
step:489/1770 train_time:47458ms step_avg:97.05ms
step:490/1770 train_time:47557ms step_avg:97.06ms
step:491/1770 train_time:47656ms step_avg:97.06ms
step:492/1770 train_time:47755ms step_avg:97.06ms
step:493/1770 train_time:47854ms step_avg:97.07ms
step:494/1770 train_time:47952ms step_avg:97.07ms
step:495/1770 train_time:48051ms step_avg:97.07ms
step:496/1770 train_time:48150ms step_avg:97.08ms
step:497/1770 train_time:48249ms step_avg:97.08ms
step:498/1770 train_time:48349ms step_avg:97.09ms
step:499/1770 train_time:48447ms step_avg:97.09ms
step:500/1770 train_time:48547ms step_avg:97.09ms
step:500/1770 val_loss:3.7565 train_time:48640ms step_avg:97.28ms
step:501/1770 train_time:48663ms step_avg:97.13ms
step:502/1770 train_time:48750ms step_avg:97.11ms
step:503/1770 train_time:48849ms step_avg:97.12ms
step:504/1770 train_time:48949ms step_avg:97.12ms
step:505/1770 train_time:49048ms step_avg:97.12ms
step:506/1770 train_time:49146ms step_avg:97.13ms
step:507/1770 train_time:49245ms step_avg:97.13ms
step:508/1770 train_time:49344ms step_avg:97.13ms
step:509/1770 train_time:49443ms step_avg:97.14ms
step:510/1770 train_time:49541ms step_avg:97.14ms
step:511/1770 train_time:49640ms step_avg:97.14ms
step:512/1770 train_time:49739ms step_avg:97.15ms
step:513/1770 train_time:49840ms step_avg:97.15ms
step:514/1770 train_time:49939ms step_avg:97.16ms
step:515/1770 train_time:50038ms step_avg:97.16ms
step:516/1770 train_time:50137ms step_avg:97.16ms
step:517/1770 train_time:50235ms step_avg:97.17ms
step:518/1770 train_time:50334ms step_avg:97.17ms
step:519/1770 train_time:50433ms step_avg:97.17ms
step:520/1770 train_time:50532ms step_avg:97.18ms
step:521/1770 train_time:50631ms step_avg:97.18ms
step:522/1770 train_time:50730ms step_avg:97.18ms
step:523/1770 train_time:50830ms step_avg:97.19ms
step:524/1770 train_time:50929ms step_avg:97.19ms
step:525/1770 train_time:51028ms step_avg:97.20ms
step:526/1770 train_time:51127ms step_avg:97.20ms
step:527/1770 train_time:51227ms step_avg:97.21ms
step:528/1770 train_time:51327ms step_avg:97.21ms
step:529/1770 train_time:51426ms step_avg:97.21ms
step:530/1770 train_time:51526ms step_avg:97.22ms
step:531/1770 train_time:51625ms step_avg:97.22ms
step:532/1770 train_time:51726ms step_avg:97.23ms
step:533/1770 train_time:51827ms step_avg:97.24ms
step:534/1770 train_time:51927ms step_avg:97.24ms
step:535/1770 train_time:52026ms step_avg:97.25ms
step:536/1770 train_time:52126ms step_avg:97.25ms
step:537/1770 train_time:52226ms step_avg:97.25ms
step:538/1770 train_time:52326ms step_avg:97.26ms
step:539/1770 train_time:52425ms step_avg:97.26ms
step:540/1770 train_time:52524ms step_avg:97.27ms
step:541/1770 train_time:52623ms step_avg:97.27ms
step:542/1770 train_time:52722ms step_avg:97.27ms
step:543/1770 train_time:52821ms step_avg:97.28ms
step:544/1770 train_time:52920ms step_avg:97.28ms
step:545/1770 train_time:53019ms step_avg:97.28ms
step:546/1770 train_time:53119ms step_avg:97.29ms
step:547/1770 train_time:53218ms step_avg:97.29ms
step:548/1770 train_time:53317ms step_avg:97.29ms
step:549/1770 train_time:53417ms step_avg:97.30ms
step:550/1770 train_time:53516ms step_avg:97.30ms
step:551/1770 train_time:53615ms step_avg:97.31ms
step:552/1770 train_time:53715ms step_avg:97.31ms
step:553/1770 train_time:53815ms step_avg:97.31ms
step:554/1770 train_time:53915ms step_avg:97.32ms
step:555/1770 train_time:54015ms step_avg:97.33ms
step:556/1770 train_time:54117ms step_avg:97.33ms
step:557/1770 train_time:54216ms step_avg:97.34ms
step:558/1770 train_time:54315ms step_avg:97.34ms
step:559/1770 train_time:54415ms step_avg:97.34ms
step:560/1770 train_time:54514ms step_avg:97.35ms
step:561/1770 train_time:54614ms step_avg:97.35ms
step:562/1770 train_time:54712ms step_avg:97.35ms
step:563/1770 train_time:54812ms step_avg:97.36ms
step:564/1770 train_time:54911ms step_avg:97.36ms
step:565/1770 train_time:55010ms step_avg:97.36ms
step:566/1770 train_time:55110ms step_avg:97.37ms
step:567/1770 train_time:55210ms step_avg:97.37ms
step:568/1770 train_time:55310ms step_avg:97.38ms
step:569/1770 train_time:55410ms step_avg:97.38ms
step:570/1770 train_time:55509ms step_avg:97.38ms
step:571/1770 train_time:55610ms step_avg:97.39ms
step:572/1770 train_time:55710ms step_avg:97.40ms
step:573/1770 train_time:55810ms step_avg:97.40ms
step:574/1770 train_time:55910ms step_avg:97.40ms
step:575/1770 train_time:56010ms step_avg:97.41ms
step:576/1770 train_time:56110ms step_avg:97.41ms
step:577/1770 train_time:56210ms step_avg:97.42ms
step:578/1770 train_time:56310ms step_avg:97.42ms
step:579/1770 train_time:56410ms step_avg:97.43ms
step:580/1770 train_time:56510ms step_avg:97.43ms
step:581/1770 train_time:56610ms step_avg:97.44ms
step:582/1770 train_time:56710ms step_avg:97.44ms
step:583/1770 train_time:56810ms step_avg:97.44ms
step:584/1770 train_time:56909ms step_avg:97.45ms
step:585/1770 train_time:57009ms step_avg:97.45ms
step:586/1770 train_time:57109ms step_avg:97.46ms
step:587/1770 train_time:57208ms step_avg:97.46ms
step:588/1770 train_time:57307ms step_avg:97.46ms
step:589/1770 train_time:57407ms step_avg:97.46ms
step:590/1770 train_time:57507ms step_avg:97.47ms
step:591/1770 train_time:57608ms step_avg:97.47ms
step:592/1770 train_time:57708ms step_avg:97.48ms
step:593/1770 train_time:57809ms step_avg:97.49ms
step:594/1770 train_time:57909ms step_avg:97.49ms
step:595/1770 train_time:58008ms step_avg:97.49ms
step:596/1770 train_time:58107ms step_avg:97.49ms
step:597/1770 train_time:58207ms step_avg:97.50ms
step:598/1770 train_time:58307ms step_avg:97.50ms
step:599/1770 train_time:58407ms step_avg:97.51ms
step:600/1770 train_time:58506ms step_avg:97.51ms
step:601/1770 train_time:58606ms step_avg:97.51ms
step:602/1770 train_time:58706ms step_avg:97.52ms
step:603/1770 train_time:58807ms step_avg:97.52ms
step:604/1770 train_time:58907ms step_avg:97.53ms
step:605/1770 train_time:59008ms step_avg:97.53ms
step:606/1770 train_time:59107ms step_avg:97.54ms
step:607/1770 train_time:59207ms step_avg:97.54ms
step:608/1770 train_time:59306ms step_avg:97.54ms
step:609/1770 train_time:59405ms step_avg:97.55ms
step:610/1770 train_time:59504ms step_avg:97.55ms
step:611/1770 train_time:59603ms step_avg:97.55ms
step:612/1770 train_time:59702ms step_avg:97.55ms
step:613/1770 train_time:59802ms step_avg:97.56ms
step:614/1770 train_time:59901ms step_avg:97.56ms
step:615/1770 train_time:60000ms step_avg:97.56ms
step:616/1770 train_time:60100ms step_avg:97.56ms
step:617/1770 train_time:60199ms step_avg:97.57ms
step:618/1770 train_time:60299ms step_avg:97.57ms
step:619/1770 train_time:60398ms step_avg:97.57ms
step:620/1770 train_time:60497ms step_avg:97.58ms
step:621/1770 train_time:60596ms step_avg:97.58ms
step:622/1770 train_time:60695ms step_avg:97.58ms
step:623/1770 train_time:60795ms step_avg:97.58ms
step:624/1770 train_time:60894ms step_avg:97.59ms
step:625/1770 train_time:60995ms step_avg:97.59ms
step:625/1770 val_loss:3.6690 train_time:61089ms step_avg:97.74ms
step:626/1770 train_time:61109ms step_avg:97.62ms
step:627/1770 train_time:61200ms step_avg:97.61ms
step:628/1770 train_time:61302ms step_avg:97.61ms
step:629/1770 train_time:61403ms step_avg:97.62ms
step:630/1770 train_time:61502ms step_avg:97.62ms
step:631/1770 train_time:61601ms step_avg:97.63ms
step:632/1770 train_time:61701ms step_avg:97.63ms
step:633/1770 train_time:61800ms step_avg:97.63ms
step:634/1770 train_time:61900ms step_avg:97.63ms
step:635/1770 train_time:61999ms step_avg:97.64ms
step:636/1770 train_time:62099ms step_avg:97.64ms
step:637/1770 train_time:62199ms step_avg:97.64ms
step:638/1770 train_time:62299ms step_avg:97.65ms
step:639/1770 train_time:62399ms step_avg:97.65ms
step:640/1770 train_time:62500ms step_avg:97.66ms
step:641/1770 train_time:62600ms step_avg:97.66ms
step:642/1770 train_time:62700ms step_avg:97.66ms
step:643/1770 train_time:62800ms step_avg:97.67ms
step:644/1770 train_time:62901ms step_avg:97.67ms
step:645/1770 train_time:63001ms step_avg:97.68ms
step:646/1770 train_time:63101ms step_avg:97.68ms
step:647/1770 train_time:63201ms step_avg:97.68ms
step:648/1770 train_time:63301ms step_avg:97.69ms
step:649/1770 train_time:63401ms step_avg:97.69ms
step:650/1770 train_time:63502ms step_avg:97.70ms
step:651/1770 train_time:63602ms step_avg:97.70ms
step:652/1770 train_time:63702ms step_avg:97.70ms
step:653/1770 train_time:63802ms step_avg:97.71ms
step:654/1770 train_time:63902ms step_avg:97.71ms
step:655/1770 train_time:64002ms step_avg:97.71ms
step:656/1770 train_time:64102ms step_avg:97.72ms
step:657/1770 train_time:64202ms step_avg:97.72ms
step:658/1770 train_time:64303ms step_avg:97.72ms
step:659/1770 train_time:64404ms step_avg:97.73ms
step:660/1770 train_time:64504ms step_avg:97.73ms
step:661/1770 train_time:64605ms step_avg:97.74ms
step:662/1770 train_time:64706ms step_avg:97.74ms
step:663/1770 train_time:64807ms step_avg:97.75ms
step:664/1770 train_time:64907ms step_avg:97.75ms
step:665/1770 train_time:65009ms step_avg:97.76ms
step:666/1770 train_time:65111ms step_avg:97.76ms
step:667/1770 train_time:65212ms step_avg:97.77ms
step:668/1770 train_time:65314ms step_avg:97.78ms
step:669/1770 train_time:65415ms step_avg:97.78ms
step:670/1770 train_time:65516ms step_avg:97.79ms
step:671/1770 train_time:65618ms step_avg:97.79ms
step:672/1770 train_time:65719ms step_avg:97.80ms
step:673/1770 train_time:65821ms step_avg:97.80ms
step:674/1770 train_time:65922ms step_avg:97.81ms
step:675/1770 train_time:66023ms step_avg:97.81ms
step:676/1770 train_time:66125ms step_avg:97.82ms
step:677/1770 train_time:66225ms step_avg:97.82ms
step:678/1770 train_time:66326ms step_avg:97.83ms
step:679/1770 train_time:66427ms step_avg:97.83ms
step:680/1770 train_time:66528ms step_avg:97.84ms
step:681/1770 train_time:66630ms step_avg:97.84ms
step:682/1770 train_time:66732ms step_avg:97.85ms
step:683/1770 train_time:66833ms step_avg:97.85ms
step:684/1770 train_time:66934ms step_avg:97.86ms
step:685/1770 train_time:67036ms step_avg:97.86ms
step:686/1770 train_time:67137ms step_avg:97.87ms
step:687/1770 train_time:67238ms step_avg:97.87ms
step:688/1770 train_time:67339ms step_avg:97.88ms
step:689/1770 train_time:67440ms step_avg:97.88ms
step:690/1770 train_time:67542ms step_avg:97.89ms
step:691/1770 train_time:67643ms step_avg:97.89ms
step:692/1770 train_time:67744ms step_avg:97.90ms
step:693/1770 train_time:67845ms step_avg:97.90ms
step:694/1770 train_time:67946ms step_avg:97.90ms
step:695/1770 train_time:68046ms step_avg:97.91ms
step:696/1770 train_time:68147ms step_avg:97.91ms
step:697/1770 train_time:68248ms step_avg:97.92ms
step:698/1770 train_time:68349ms step_avg:97.92ms
step:699/1770 train_time:68451ms step_avg:97.93ms
step:700/1770 train_time:68553ms step_avg:97.93ms
step:701/1770 train_time:68654ms step_avg:97.94ms
step:702/1770 train_time:68755ms step_avg:97.94ms
step:703/1770 train_time:68858ms step_avg:97.95ms
step:704/1770 train_time:68959ms step_avg:97.95ms
step:705/1770 train_time:69060ms step_avg:97.96ms
step:706/1770 train_time:69162ms step_avg:97.96ms
step:707/1770 train_time:69262ms step_avg:97.97ms
step:708/1770 train_time:69363ms step_avg:97.97ms
step:709/1770 train_time:69465ms step_avg:97.98ms
step:710/1770 train_time:69566ms step_avg:97.98ms
step:711/1770 train_time:69666ms step_avg:97.98ms
step:712/1770 train_time:69767ms step_avg:97.99ms
step:713/1770 train_time:69868ms step_avg:97.99ms
step:714/1770 train_time:69969ms step_avg:98.00ms
step:715/1770 train_time:70072ms step_avg:98.00ms
step:716/1770 train_time:70174ms step_avg:98.01ms
step:717/1770 train_time:70276ms step_avg:98.01ms
step:718/1770 train_time:70376ms step_avg:98.02ms
step:719/1770 train_time:70478ms step_avg:98.02ms
step:720/1770 train_time:70579ms step_avg:98.03ms
step:721/1770 train_time:70680ms step_avg:98.03ms
step:722/1770 train_time:70782ms step_avg:98.04ms
step:723/1770 train_time:70883ms step_avg:98.04ms
step:724/1770 train_time:70984ms step_avg:98.04ms
step:725/1770 train_time:71085ms step_avg:98.05ms
step:726/1770 train_time:71186ms step_avg:98.05ms
step:727/1770 train_time:71286ms step_avg:98.06ms
step:728/1770 train_time:71387ms step_avg:98.06ms
step:729/1770 train_time:71488ms step_avg:98.06ms
step:730/1770 train_time:71589ms step_avg:98.07ms
step:731/1770 train_time:71691ms step_avg:98.07ms
step:732/1770 train_time:71793ms step_avg:98.08ms
step:733/1770 train_time:71894ms step_avg:98.08ms
step:734/1770 train_time:71996ms step_avg:98.09ms
step:735/1770 train_time:72098ms step_avg:98.09ms
step:736/1770 train_time:72199ms step_avg:98.10ms
step:737/1770 train_time:72301ms step_avg:98.10ms
step:738/1770 train_time:72402ms step_avg:98.11ms
step:739/1770 train_time:72503ms step_avg:98.11ms
step:740/1770 train_time:72604ms step_avg:98.11ms
step:741/1770 train_time:72706ms step_avg:98.12ms
step:742/1770 train_time:72807ms step_avg:98.12ms
step:743/1770 train_time:72908ms step_avg:98.13ms
step:744/1770 train_time:73008ms step_avg:98.13ms
step:745/1770 train_time:73110ms step_avg:98.13ms
step:746/1770 train_time:73211ms step_avg:98.14ms
step:747/1770 train_time:73312ms step_avg:98.14ms
step:748/1770 train_time:73414ms step_avg:98.15ms
step:749/1770 train_time:73515ms step_avg:98.15ms
step:750/1770 train_time:73617ms step_avg:98.16ms
step:750/1770 val_loss:3.6043 train_time:73712ms step_avg:98.28ms
step:751/1770 train_time:73732ms step_avg:98.18ms
step:752/1770 train_time:73823ms step_avg:98.17ms
step:753/1770 train_time:73926ms step_avg:98.17ms
step:754/1770 train_time:74027ms step_avg:98.18ms
step:755/1770 train_time:74128ms step_avg:98.18ms
step:756/1770 train_time:74229ms step_avg:98.19ms
step:757/1770 train_time:74330ms step_avg:98.19ms
step:758/1770 train_time:74431ms step_avg:98.19ms
step:759/1770 train_time:74532ms step_avg:98.20ms
step:760/1770 train_time:74633ms step_avg:98.20ms
step:761/1770 train_time:74734ms step_avg:98.20ms
step:762/1770 train_time:74836ms step_avg:98.21ms
step:763/1770 train_time:74938ms step_avg:98.22ms
step:764/1770 train_time:75039ms step_avg:98.22ms
step:765/1770 train_time:75140ms step_avg:98.22ms
step:766/1770 train_time:75240ms step_avg:98.22ms
step:767/1770 train_time:75342ms step_avg:98.23ms
step:768/1770 train_time:75442ms step_avg:98.23ms
step:769/1770 train_time:75543ms step_avg:98.24ms
step:770/1770 train_time:75644ms step_avg:98.24ms
step:771/1770 train_time:75746ms step_avg:98.24ms
step:772/1770 train_time:75848ms step_avg:98.25ms
step:773/1770 train_time:75950ms step_avg:98.25ms
step:774/1770 train_time:76051ms step_avg:98.26ms
step:775/1770 train_time:76152ms step_avg:98.26ms
step:776/1770 train_time:76253ms step_avg:98.26ms
step:777/1770 train_time:76354ms step_avg:98.27ms
step:778/1770 train_time:76456ms step_avg:98.27ms
step:779/1770 train_time:76557ms step_avg:98.28ms
step:780/1770 train_time:76658ms step_avg:98.28ms
step:781/1770 train_time:76759ms step_avg:98.28ms
step:782/1770 train_time:76860ms step_avg:98.29ms
step:783/1770 train_time:76960ms step_avg:98.29ms
step:784/1770 train_time:77061ms step_avg:98.29ms
step:785/1770 train_time:77163ms step_avg:98.30ms
step:786/1770 train_time:77264ms step_avg:98.30ms
step:787/1770 train_time:77366ms step_avg:98.30ms
step:788/1770 train_time:77468ms step_avg:98.31ms
step:789/1770 train_time:77569ms step_avg:98.31ms
step:790/1770 train_time:77671ms step_avg:98.32ms
step:791/1770 train_time:77772ms step_avg:98.32ms
step:792/1770 train_time:77874ms step_avg:98.33ms
step:793/1770 train_time:77976ms step_avg:98.33ms
step:794/1770 train_time:78077ms step_avg:98.33ms
step:795/1770 train_time:78179ms step_avg:98.34ms
step:796/1770 train_time:78280ms step_avg:98.34ms
step:797/1770 train_time:78382ms step_avg:98.35ms
step:798/1770 train_time:78483ms step_avg:98.35ms
step:799/1770 train_time:78585ms step_avg:98.35ms
step:800/1770 train_time:78688ms step_avg:98.36ms
step:801/1770 train_time:78789ms step_avg:98.36ms
step:802/1770 train_time:78891ms step_avg:98.37ms
step:803/1770 train_time:78991ms step_avg:98.37ms
step:804/1770 train_time:79093ms step_avg:98.37ms
step:805/1770 train_time:79195ms step_avg:98.38ms
step:806/1770 train_time:79296ms step_avg:98.38ms
step:807/1770 train_time:79398ms step_avg:98.39ms
step:808/1770 train_time:79500ms step_avg:98.39ms
step:809/1770 train_time:79601ms step_avg:98.39ms
step:810/1770 train_time:79702ms step_avg:98.40ms
step:811/1770 train_time:79804ms step_avg:98.40ms
step:812/1770 train_time:79905ms step_avg:98.40ms
step:813/1770 train_time:80007ms step_avg:98.41ms
step:814/1770 train_time:80109ms step_avg:98.41ms
step:815/1770 train_time:80211ms step_avg:98.42ms
step:816/1770 train_time:80312ms step_avg:98.42ms
step:817/1770 train_time:80413ms step_avg:98.43ms
step:818/1770 train_time:80514ms step_avg:98.43ms
step:819/1770 train_time:80616ms step_avg:98.43ms
step:820/1770 train_time:80718ms step_avg:98.44ms
step:821/1770 train_time:80819ms step_avg:98.44ms
step:822/1770 train_time:80920ms step_avg:98.44ms
step:823/1770 train_time:81021ms step_avg:98.45ms
step:824/1770 train_time:81121ms step_avg:98.45ms
step:825/1770 train_time:81223ms step_avg:98.45ms
step:826/1770 train_time:81324ms step_avg:98.46ms
step:827/1770 train_time:81427ms step_avg:98.46ms
step:828/1770 train_time:81529ms step_avg:98.47ms
step:829/1770 train_time:81631ms step_avg:98.47ms
step:830/1770 train_time:81732ms step_avg:98.47ms
step:831/1770 train_time:81833ms step_avg:98.48ms
step:832/1770 train_time:81934ms step_avg:98.48ms
step:833/1770 train_time:82036ms step_avg:98.48ms
step:834/1770 train_time:82137ms step_avg:98.49ms
step:835/1770 train_time:82239ms step_avg:98.49ms
step:836/1770 train_time:82340ms step_avg:98.49ms
step:837/1770 train_time:82442ms step_avg:98.50ms
step:838/1770 train_time:82542ms step_avg:98.50ms
step:839/1770 train_time:82643ms step_avg:98.50ms
step:840/1770 train_time:82744ms step_avg:98.50ms
step:841/1770 train_time:82846ms step_avg:98.51ms
step:842/1770 train_time:82947ms step_avg:98.51ms
step:843/1770 train_time:83050ms step_avg:98.52ms
step:844/1770 train_time:83151ms step_avg:98.52ms
step:845/1770 train_time:83253ms step_avg:98.52ms
step:846/1770 train_time:83355ms step_avg:98.53ms
step:847/1770 train_time:83456ms step_avg:98.53ms
step:848/1770 train_time:83557ms step_avg:98.53ms
step:849/1770 train_time:83659ms step_avg:98.54ms
step:850/1770 train_time:83761ms step_avg:98.54ms
step:851/1770 train_time:83862ms step_avg:98.55ms
step:852/1770 train_time:83963ms step_avg:98.55ms
step:853/1770 train_time:84064ms step_avg:98.55ms
step:854/1770 train_time:84166ms step_avg:98.55ms
step:855/1770 train_time:84267ms step_avg:98.56ms
step:856/1770 train_time:84369ms step_avg:98.56ms
step:857/1770 train_time:84471ms step_avg:98.57ms
step:858/1770 train_time:84573ms step_avg:98.57ms
step:859/1770 train_time:84675ms step_avg:98.57ms
step:860/1770 train_time:84776ms step_avg:98.58ms
step:861/1770 train_time:84878ms step_avg:98.58ms
step:862/1770 train_time:84979ms step_avg:98.58ms
step:863/1770 train_time:85080ms step_avg:98.59ms
step:864/1770 train_time:85181ms step_avg:98.59ms
step:865/1770 train_time:85281ms step_avg:98.59ms
step:866/1770 train_time:85383ms step_avg:98.59ms
step:867/1770 train_time:85485ms step_avg:98.60ms
step:868/1770 train_time:85587ms step_avg:98.60ms
step:869/1770 train_time:85688ms step_avg:98.61ms
step:870/1770 train_time:85790ms step_avg:98.61ms
step:871/1770 train_time:85891ms step_avg:98.61ms
step:872/1770 train_time:85993ms step_avg:98.62ms
step:873/1770 train_time:86095ms step_avg:98.62ms
step:874/1770 train_time:86196ms step_avg:98.62ms
step:875/1770 train_time:86298ms step_avg:98.63ms
step:875/1770 val_loss:3.5550 train_time:86393ms step_avg:98.73ms
step:876/1770 train_time:86414ms step_avg:98.65ms
step:877/1770 train_time:86506ms step_avg:98.64ms
step:878/1770 train_time:86608ms step_avg:98.64ms
step:879/1770 train_time:86709ms step_avg:98.64ms
step:880/1770 train_time:86809ms step_avg:98.65ms
step:881/1770 train_time:86910ms step_avg:98.65ms
step:882/1770 train_time:87011ms step_avg:98.65ms
step:883/1770 train_time:87112ms step_avg:98.65ms
step:884/1770 train_time:87213ms step_avg:98.66ms
step:885/1770 train_time:87314ms step_avg:98.66ms
step:886/1770 train_time:87416ms step_avg:98.66ms
step:887/1770 train_time:87519ms step_avg:98.67ms
step:888/1770 train_time:87620ms step_avg:98.67ms
step:889/1770 train_time:87721ms step_avg:98.67ms
step:890/1770 train_time:87822ms step_avg:98.68ms
step:891/1770 train_time:87923ms step_avg:98.68ms
step:892/1770 train_time:88024ms step_avg:98.68ms
step:893/1770 train_time:88125ms step_avg:98.68ms
step:894/1770 train_time:88227ms step_avg:98.69ms
step:895/1770 train_time:88329ms step_avg:98.69ms
step:896/1770 train_time:88431ms step_avg:98.70ms
step:897/1770 train_time:88533ms step_avg:98.70ms
step:898/1770 train_time:88634ms step_avg:98.70ms
step:899/1770 train_time:88736ms step_avg:98.71ms
step:900/1770 train_time:88838ms step_avg:98.71ms
step:901/1770 train_time:88939ms step_avg:98.71ms
step:902/1770 train_time:89041ms step_avg:98.71ms
step:903/1770 train_time:89142ms step_avg:98.72ms
step:904/1770 train_time:89243ms step_avg:98.72ms
step:905/1770 train_time:89344ms step_avg:98.72ms
step:906/1770 train_time:89446ms step_avg:98.73ms
step:907/1770 train_time:89548ms step_avg:98.73ms
step:908/1770 train_time:89649ms step_avg:98.73ms
step:909/1770 train_time:89750ms step_avg:98.74ms
step:910/1770 train_time:89853ms step_avg:98.74ms
step:911/1770 train_time:89954ms step_avg:98.74ms
step:912/1770 train_time:90056ms step_avg:98.75ms
step:913/1770 train_time:90158ms step_avg:98.75ms
step:914/1770 train_time:90260ms step_avg:98.75ms
step:915/1770 train_time:90361ms step_avg:98.76ms
step:916/1770 train_time:90462ms step_avg:98.76ms
step:917/1770 train_time:90563ms step_avg:98.76ms
step:918/1770 train_time:90665ms step_avg:98.76ms
step:919/1770 train_time:90767ms step_avg:98.77ms
step:920/1770 train_time:90870ms step_avg:98.77ms
step:921/1770 train_time:90973ms step_avg:98.78ms
step:922/1770 train_time:91076ms step_avg:98.78ms
step:923/1770 train_time:91178ms step_avg:98.78ms
step:924/1770 train_time:91280ms step_avg:98.79ms
step:925/1770 train_time:91382ms step_avg:98.79ms
step:926/1770 train_time:91484ms step_avg:98.80ms
step:927/1770 train_time:91587ms step_avg:98.80ms
step:928/1770 train_time:91689ms step_avg:98.80ms
step:929/1770 train_time:91792ms step_avg:98.81ms
step:930/1770 train_time:91896ms step_avg:98.81ms
step:931/1770 train_time:91998ms step_avg:98.82ms
step:932/1770 train_time:92100ms step_avg:98.82ms
step:933/1770 train_time:92203ms step_avg:98.82ms
step:934/1770 train_time:92306ms step_avg:98.83ms
step:935/1770 train_time:92409ms step_avg:98.83ms
step:936/1770 train_time:92512ms step_avg:98.84ms
step:937/1770 train_time:92614ms step_avg:98.84ms
step:938/1770 train_time:92717ms step_avg:98.85ms
step:939/1770 train_time:92820ms step_avg:98.85ms
step:940/1770 train_time:92923ms step_avg:98.85ms
step:941/1770 train_time:93025ms step_avg:98.86ms
step:942/1770 train_time:93128ms step_avg:98.86ms
step:943/1770 train_time:93231ms step_avg:98.87ms
step:944/1770 train_time:93334ms step_avg:98.87ms
step:945/1770 train_time:93437ms step_avg:98.87ms
step:946/1770 train_time:93540ms step_avg:98.88ms
step:947/1770 train_time:93642ms step_avg:98.88ms
step:948/1770 train_time:93744ms step_avg:98.89ms
step:949/1770 train_time:93846ms step_avg:98.89ms
step:950/1770 train_time:93949ms step_avg:98.89ms
step:951/1770 train_time:94052ms step_avg:98.90ms
step:952/1770 train_time:94154ms step_avg:98.90ms
step:953/1770 train_time:94257ms step_avg:98.91ms
step:954/1770 train_time:94360ms step_avg:98.91ms
step:955/1770 train_time:94462ms step_avg:98.91ms
step:956/1770 train_time:94564ms step_avg:98.92ms
step:957/1770 train_time:94668ms step_avg:98.92ms
step:958/1770 train_time:94771ms step_avg:98.93ms
step:959/1770 train_time:94873ms step_avg:98.93ms
step:960/1770 train_time:94976ms step_avg:98.93ms
step:961/1770 train_time:95078ms step_avg:98.94ms
step:962/1770 train_time:95181ms step_avg:98.94ms
step:963/1770 train_time:95283ms step_avg:98.94ms
step:964/1770 train_time:95385ms step_avg:98.95ms
step:965/1770 train_time:95488ms step_avg:98.95ms
step:966/1770 train_time:95591ms step_avg:98.96ms
step:967/1770 train_time:95694ms step_avg:98.96ms
step:968/1770 train_time:95797ms step_avg:98.96ms
step:969/1770 train_time:95899ms step_avg:98.97ms
step:970/1770 train_time:96001ms step_avg:98.97ms
step:971/1770 train_time:96103ms step_avg:98.97ms
step:972/1770 train_time:96205ms step_avg:98.98ms
step:973/1770 train_time:96307ms step_avg:98.98ms
step:974/1770 train_time:96410ms step_avg:98.98ms
step:975/1770 train_time:96514ms step_avg:98.99ms
step:976/1770 train_time:96616ms step_avg:98.99ms
step:977/1770 train_time:96719ms step_avg:99.00ms
step:978/1770 train_time:96821ms step_avg:99.00ms
step:979/1770 train_time:96924ms step_avg:99.00ms
step:980/1770 train_time:97027ms step_avg:99.01ms
step:981/1770 train_time:97129ms step_avg:99.01ms
step:982/1770 train_time:97232ms step_avg:99.01ms
step:983/1770 train_time:97334ms step_avg:99.02ms
step:984/1770 train_time:97437ms step_avg:99.02ms
step:985/1770 train_time:97540ms step_avg:99.03ms
step:986/1770 train_time:97643ms step_avg:99.03ms
step:987/1770 train_time:97745ms step_avg:99.03ms
step:988/1770 train_time:97847ms step_avg:99.04ms
step:989/1770 train_time:97951ms step_avg:99.04ms
step:990/1770 train_time:98054ms step_avg:99.04ms
step:991/1770 train_time:98156ms step_avg:99.05ms
step:992/1770 train_time:98260ms step_avg:99.05ms
step:993/1770 train_time:98362ms step_avg:99.06ms
step:994/1770 train_time:98465ms step_avg:99.06ms
step:995/1770 train_time:98567ms step_avg:99.06ms
step:996/1770 train_time:98670ms step_avg:99.07ms
step:997/1770 train_time:98772ms step_avg:99.07ms
step:998/1770 train_time:98874ms step_avg:99.07ms
step:999/1770 train_time:98977ms step_avg:99.08ms
step:1000/1770 train_time:99080ms step_avg:99.08ms
step:1000/1770 val_loss:3.5172 train_time:99176ms step_avg:99.18ms
step:1001/1770 train_time:99196ms step_avg:99.10ms
step:1002/1770 train_time:99292ms step_avg:99.09ms
step:1003/1770 train_time:99397ms step_avg:99.10ms
step:1004/1770 train_time:99500ms step_avg:99.10ms
step:1005/1770 train_time:99603ms step_avg:99.11ms
step:1006/1770 train_time:99705ms step_avg:99.11ms
step:1007/1770 train_time:99807ms step_avg:99.11ms
step:1008/1770 train_time:99909ms step_avg:99.12ms
step:1009/1770 train_time:100012ms step_avg:99.12ms
step:1010/1770 train_time:100114ms step_avg:99.12ms
step:1011/1770 train_time:100218ms step_avg:99.13ms
step:1012/1770 train_time:100322ms step_avg:99.13ms
step:1013/1770 train_time:100424ms step_avg:99.14ms
step:1014/1770 train_time:100526ms step_avg:99.14ms
step:1015/1770 train_time:100630ms step_avg:99.14ms
step:1016/1770 train_time:100733ms step_avg:99.15ms
step:1017/1770 train_time:100836ms step_avg:99.15ms
step:1018/1770 train_time:100938ms step_avg:99.15ms
step:1019/1770 train_time:101041ms step_avg:99.16ms
step:1020/1770 train_time:101143ms step_avg:99.16ms
step:1021/1770 train_time:101246ms step_avg:99.16ms
step:1022/1770 train_time:101349ms step_avg:99.17ms
step:1023/1770 train_time:101452ms step_avg:99.17ms
step:1024/1770 train_time:101554ms step_avg:99.17ms
step:1025/1770 train_time:101657ms step_avg:99.18ms
step:1026/1770 train_time:101760ms step_avg:99.18ms
step:1027/1770 train_time:101862ms step_avg:99.18ms
step:1028/1770 train_time:101964ms step_avg:99.19ms
step:1029/1770 train_time:102067ms step_avg:99.19ms
step:1030/1770 train_time:102170ms step_avg:99.19ms
step:1031/1770 train_time:102272ms step_avg:99.20ms
step:1032/1770 train_time:102375ms step_avg:99.20ms
step:1033/1770 train_time:102479ms step_avg:99.20ms
step:1034/1770 train_time:102581ms step_avg:99.21ms
step:1035/1770 train_time:102683ms step_avg:99.21ms
step:1036/1770 train_time:102785ms step_avg:99.21ms
step:1037/1770 train_time:102888ms step_avg:99.22ms
step:1038/1770 train_time:102991ms step_avg:99.22ms
step:1039/1770 train_time:103093ms step_avg:99.22ms
step:1040/1770 train_time:103195ms step_avg:99.23ms
step:1041/1770 train_time:103298ms step_avg:99.23ms
step:1042/1770 train_time:103400ms step_avg:99.23ms
step:1043/1770 train_time:103503ms step_avg:99.24ms
step:1044/1770 train_time:103606ms step_avg:99.24ms
step:1045/1770 train_time:103709ms step_avg:99.24ms
step:1046/1770 train_time:103811ms step_avg:99.25ms
step:1047/1770 train_time:103913ms step_avg:99.25ms
step:1048/1770 train_time:104015ms step_avg:99.25ms
step:1049/1770 train_time:104118ms step_avg:99.25ms
step:1050/1770 train_time:104221ms step_avg:99.26ms
step:1051/1770 train_time:104324ms step_avg:99.26ms
step:1052/1770 train_time:104426ms step_avg:99.26ms
step:1053/1770 train_time:104530ms step_avg:99.27ms
step:1054/1770 train_time:104632ms step_avg:99.27ms
step:1055/1770 train_time:104735ms step_avg:99.27ms
step:1056/1770 train_time:104837ms step_avg:99.28ms
step:1057/1770 train_time:104940ms step_avg:99.28ms
step:1058/1770 train_time:105043ms step_avg:99.28ms
step:1059/1770 train_time:105146ms step_avg:99.29ms
step:1060/1770 train_time:105250ms step_avg:99.29ms
step:1061/1770 train_time:105353ms step_avg:99.30ms
step:1062/1770 train_time:105456ms step_avg:99.30ms
step:1063/1770 train_time:105562ms step_avg:99.31ms
step:1064/1770 train_time:105666ms step_avg:99.31ms
step:1065/1770 train_time:105768ms step_avg:99.31ms
step:1066/1770 train_time:105872ms step_avg:99.32ms
step:1067/1770 train_time:105974ms step_avg:99.32ms
step:1068/1770 train_time:106077ms step_avg:99.32ms
step:1069/1770 train_time:106180ms step_avg:99.33ms
step:1070/1770 train_time:106282ms step_avg:99.33ms
step:1071/1770 train_time:106385ms step_avg:99.33ms
step:1072/1770 train_time:106487ms step_avg:99.34ms
step:1073/1770 train_time:106590ms step_avg:99.34ms
step:1074/1770 train_time:106693ms step_avg:99.34ms
step:1075/1770 train_time:106795ms step_avg:99.34ms
step:1076/1770 train_time:106898ms step_avg:99.35ms
step:1077/1770 train_time:107002ms step_avg:99.35ms
step:1078/1770 train_time:107104ms step_avg:99.35ms
step:1079/1770 train_time:107207ms step_avg:99.36ms
step:1080/1770 train_time:107309ms step_avg:99.36ms
step:1081/1770 train_time:107412ms step_avg:99.36ms
step:1082/1770 train_time:107515ms step_avg:99.37ms
step:1083/1770 train_time:107617ms step_avg:99.37ms
step:1084/1770 train_time:107719ms step_avg:99.37ms
step:1085/1770 train_time:107822ms step_avg:99.38ms
step:1086/1770 train_time:107924ms step_avg:99.38ms
step:1087/1770 train_time:108027ms step_avg:99.38ms
step:1088/1770 train_time:108129ms step_avg:99.38ms
step:1089/1770 train_time:108233ms step_avg:99.39ms
step:1090/1770 train_time:108335ms step_avg:99.39ms
step:1091/1770 train_time:108438ms step_avg:99.39ms
step:1092/1770 train_time:108541ms step_avg:99.40ms
step:1093/1770 train_time:108644ms step_avg:99.40ms
step:1094/1770 train_time:108747ms step_avg:99.40ms
step:1095/1770 train_time:108851ms step_avg:99.41ms
step:1096/1770 train_time:108953ms step_avg:99.41ms
step:1097/1770 train_time:109056ms step_avg:99.41ms
step:1098/1770 train_time:109159ms step_avg:99.42ms
step:1099/1770 train_time:109262ms step_avg:99.42ms
step:1100/1770 train_time:109364ms step_avg:99.42ms
step:1101/1770 train_time:109468ms step_avg:99.43ms
step:1102/1770 train_time:109570ms step_avg:99.43ms
step:1103/1770 train_time:109673ms step_avg:99.43ms
step:1104/1770 train_time:109777ms step_avg:99.44ms
step:1105/1770 train_time:109880ms step_avg:99.44ms
step:1106/1770 train_time:109983ms step_avg:99.44ms
step:1107/1770 train_time:110086ms step_avg:99.45ms
step:1108/1770 train_time:110189ms step_avg:99.45ms
step:1109/1770 train_time:110291ms step_avg:99.45ms
step:1110/1770 train_time:110395ms step_avg:99.45ms
step:1111/1770 train_time:110498ms step_avg:99.46ms
step:1112/1770 train_time:110602ms step_avg:99.46ms
step:1113/1770 train_time:110705ms step_avg:99.47ms
step:1114/1770 train_time:110808ms step_avg:99.47ms
step:1115/1770 train_time:110911ms step_avg:99.47ms
step:1116/1770 train_time:111013ms step_avg:99.47ms
step:1117/1770 train_time:111116ms step_avg:99.48ms
step:1118/1770 train_time:111219ms step_avg:99.48ms
step:1119/1770 train_time:111323ms step_avg:99.48ms
step:1120/1770 train_time:111425ms step_avg:99.49ms
step:1121/1770 train_time:111528ms step_avg:99.49ms
step:1122/1770 train_time:111631ms step_avg:99.49ms
step:1123/1770 train_time:111733ms step_avg:99.50ms
step:1124/1770 train_time:111836ms step_avg:99.50ms
step:1125/1770 train_time:111938ms step_avg:99.50ms
step:1125/1770 val_loss:3.4755 train_time:112036ms step_avg:99.59ms
step:1126/1770 train_time:112057ms step_avg:99.52ms
step:1127/1770 train_time:112149ms step_avg:99.51ms
step:1128/1770 train_time:112252ms step_avg:99.51ms
step:1129/1770 train_time:112354ms step_avg:99.52ms
step:1130/1770 train_time:112457ms step_avg:99.52ms
step:1131/1770 train_time:112560ms step_avg:99.52ms
step:1132/1770 train_time:112663ms step_avg:99.53ms
step:1133/1770 train_time:112765ms step_avg:99.53ms
step:1134/1770 train_time:112869ms step_avg:99.53ms
step:1135/1770 train_time:112971ms step_avg:99.53ms
step:1136/1770 train_time:113074ms step_avg:99.54ms
step:1137/1770 train_time:113178ms step_avg:99.54ms
step:1138/1770 train_time:113282ms step_avg:99.54ms
step:1139/1770 train_time:113385ms step_avg:99.55ms
step:1140/1770 train_time:113488ms step_avg:99.55ms
step:1141/1770 train_time:113590ms step_avg:99.55ms
step:1142/1770 train_time:113693ms step_avg:99.56ms
step:1143/1770 train_time:113795ms step_avg:99.56ms
step:1144/1770 train_time:113898ms step_avg:99.56ms
step:1145/1770 train_time:114001ms step_avg:99.56ms
step:1146/1770 train_time:114104ms step_avg:99.57ms
step:1147/1770 train_time:114208ms step_avg:99.57ms
step:1148/1770 train_time:114311ms step_avg:99.57ms
step:1149/1770 train_time:114414ms step_avg:99.58ms
step:1150/1770 train_time:114517ms step_avg:99.58ms
step:1151/1770 train_time:114620ms step_avg:99.58ms
step:1152/1770 train_time:114723ms step_avg:99.59ms
step:1153/1770 train_time:114826ms step_avg:99.59ms
step:1154/1770 train_time:114929ms step_avg:99.59ms
step:1155/1770 train_time:115032ms step_avg:99.59ms
step:1156/1770 train_time:115134ms step_avg:99.60ms
step:1157/1770 train_time:115238ms step_avg:99.60ms
step:1158/1770 train_time:115340ms step_avg:99.60ms
step:1159/1770 train_time:115442ms step_avg:99.61ms
step:1160/1770 train_time:115545ms step_avg:99.61ms
step:1161/1770 train_time:115648ms step_avg:99.61ms
step:1162/1770 train_time:115752ms step_avg:99.61ms
step:1163/1770 train_time:115855ms step_avg:99.62ms
step:1164/1770 train_time:115958ms step_avg:99.62ms
step:1165/1770 train_time:116062ms step_avg:99.62ms
step:1166/1770 train_time:116165ms step_avg:99.63ms
step:1167/1770 train_time:116268ms step_avg:99.63ms
step:1168/1770 train_time:116371ms step_avg:99.63ms
step:1169/1770 train_time:116473ms step_avg:99.63ms
step:1170/1770 train_time:116576ms step_avg:99.64ms
step:1171/1770 train_time:116679ms step_avg:99.64ms
step:1172/1770 train_time:116782ms step_avg:99.64ms
step:1173/1770 train_time:116884ms step_avg:99.65ms
step:1174/1770 train_time:116987ms step_avg:99.65ms
step:1175/1770 train_time:117090ms step_avg:99.65ms
step:1176/1770 train_time:117192ms step_avg:99.65ms
step:1177/1770 train_time:117295ms step_avg:99.66ms
step:1178/1770 train_time:117399ms step_avg:99.66ms
step:1179/1770 train_time:117501ms step_avg:99.66ms
step:1180/1770 train_time:117604ms step_avg:99.66ms
step:1181/1770 train_time:117707ms step_avg:99.67ms
step:1182/1770 train_time:117811ms step_avg:99.67ms
step:1183/1770 train_time:117915ms step_avg:99.67ms
step:1184/1770 train_time:118019ms step_avg:99.68ms
step:1185/1770 train_time:118122ms step_avg:99.68ms
step:1186/1770 train_time:118227ms step_avg:99.69ms
step:1187/1770 train_time:118333ms step_avg:99.69ms
step:1188/1770 train_time:118436ms step_avg:99.69ms
step:1189/1770 train_time:118539ms step_avg:99.70ms
step:1190/1770 train_time:118643ms step_avg:99.70ms
step:1191/1770 train_time:118749ms step_avg:99.70ms
step:1192/1770 train_time:118852ms step_avg:99.71ms
step:1193/1770 train_time:118955ms step_avg:99.71ms
step:1194/1770 train_time:119059ms step_avg:99.71ms
step:1195/1770 train_time:119164ms step_avg:99.72ms
step:1196/1770 train_time:119269ms step_avg:99.72ms
step:1197/1770 train_time:119372ms step_avg:99.73ms
step:1198/1770 train_time:119476ms step_avg:99.73ms
step:1199/1770 train_time:119579ms step_avg:99.73ms
step:1200/1770 train_time:119683ms step_avg:99.74ms
step:1201/1770 train_time:119788ms step_avg:99.74ms
step:1202/1770 train_time:119892ms step_avg:99.74ms
step:1203/1770 train_time:119996ms step_avg:99.75ms
step:1204/1770 train_time:120101ms step_avg:99.75ms
step:1205/1770 train_time:120204ms step_avg:99.75ms
step:1206/1770 train_time:120309ms step_avg:99.76ms
step:1207/1770 train_time:120413ms step_avg:99.76ms
step:1208/1770 train_time:120517ms step_avg:99.77ms
step:1209/1770 train_time:120621ms step_avg:99.77ms
step:1210/1770 train_time:120725ms step_avg:99.77ms
step:1211/1770 train_time:120829ms step_avg:99.78ms
step:1212/1770 train_time:120935ms step_avg:99.78ms
step:1213/1770 train_time:121038ms step_avg:99.78ms
step:1214/1770 train_time:121141ms step_avg:99.79ms
step:1215/1770 train_time:121245ms step_avg:99.79ms
step:1216/1770 train_time:121351ms step_avg:99.80ms
step:1217/1770 train_time:121454ms step_avg:99.80ms
step:1218/1770 train_time:121558ms step_avg:99.80ms
step:1219/1770 train_time:121661ms step_avg:99.80ms
step:1220/1770 train_time:121765ms step_avg:99.81ms
step:1221/1770 train_time:121869ms step_avg:99.81ms
step:1222/1770 train_time:121975ms step_avg:99.82ms
step:1223/1770 train_time:122078ms step_avg:99.82ms
step:1224/1770 train_time:122183ms step_avg:99.82ms
step:1225/1770 train_time:122287ms step_avg:99.83ms
step:1226/1770 train_time:122391ms step_avg:99.83ms
step:1227/1770 train_time:122497ms step_avg:99.83ms
step:1228/1770 train_time:122602ms step_avg:99.84ms
step:1229/1770 train_time:122706ms step_avg:99.84ms
step:1230/1770 train_time:122811ms step_avg:99.85ms
step:1231/1770 train_time:122915ms step_avg:99.85ms
step:1232/1770 train_time:123018ms step_avg:99.85ms
step:1233/1770 train_time:123121ms step_avg:99.85ms
step:1234/1770 train_time:123224ms step_avg:99.86ms
step:1235/1770 train_time:123329ms step_avg:99.86ms
step:1236/1770 train_time:123434ms step_avg:99.87ms
step:1237/1770 train_time:123537ms step_avg:99.87ms
step:1238/1770 train_time:123641ms step_avg:99.87ms
step:1239/1770 train_time:123746ms step_avg:99.88ms
step:1240/1770 train_time:123851ms step_avg:99.88ms
step:1241/1770 train_time:123954ms step_avg:99.88ms
step:1242/1770 train_time:124059ms step_avg:99.89ms
step:1243/1770 train_time:124163ms step_avg:99.89ms
step:1244/1770 train_time:124267ms step_avg:99.89ms
step:1245/1770 train_time:124371ms step_avg:99.90ms
step:1246/1770 train_time:124475ms step_avg:99.90ms
step:1247/1770 train_time:124580ms step_avg:99.90ms
step:1248/1770 train_time:124684ms step_avg:99.91ms
step:1249/1770 train_time:124788ms step_avg:99.91ms
step:1250/1770 train_time:124892ms step_avg:99.91ms
step:1250/1770 val_loss:3.4272 train_time:124992ms step_avg:99.99ms
step:1251/1770 train_time:125013ms step_avg:99.93ms
step:1252/1770 train_time:125107ms step_avg:99.93ms
step:1253/1770 train_time:125212ms step_avg:99.93ms
step:1254/1770 train_time:125317ms step_avg:99.93ms
step:1255/1770 train_time:125422ms step_avg:99.94ms
step:1256/1770 train_time:125525ms step_avg:99.94ms
step:1257/1770 train_time:125628ms step_avg:99.94ms
step:1258/1770 train_time:125733ms step_avg:99.95ms
step:1259/1770 train_time:125837ms step_avg:99.95ms
step:1260/1770 train_time:125940ms step_avg:99.95ms
step:1261/1770 train_time:126045ms step_avg:99.96ms
step:1262/1770 train_time:126149ms step_avg:99.96ms
step:1263/1770 train_time:126253ms step_avg:99.96ms
step:1264/1770 train_time:126359ms step_avg:99.97ms
step:1265/1770 train_time:126462ms step_avg:99.97ms
step:1266/1770 train_time:126566ms step_avg:99.97ms
step:1267/1770 train_time:126670ms step_avg:99.98ms
step:1268/1770 train_time:126774ms step_avg:99.98ms
step:1269/1770 train_time:126878ms step_avg:99.98ms
step:1270/1770 train_time:126982ms step_avg:99.99ms
step:1271/1770 train_time:127087ms step_avg:99.99ms
step:1272/1770 train_time:127190ms step_avg:99.99ms
step:1273/1770 train_time:127294ms step_avg:100.00ms
step:1274/1770 train_time:127398ms step_avg:100.00ms
step:1275/1770 train_time:127502ms step_avg:100.00ms
step:1276/1770 train_time:127606ms step_avg:100.00ms
step:1277/1770 train_time:127710ms step_avg:100.01ms
step:1278/1770 train_time:127814ms step_avg:100.01ms
step:1279/1770 train_time:127918ms step_avg:100.01ms
step:1280/1770 train_time:128023ms step_avg:100.02ms
step:1281/1770 train_time:128126ms step_avg:100.02ms
step:1282/1770 train_time:128232ms step_avg:100.02ms
step:1283/1770 train_time:128336ms step_avg:100.03ms
step:1284/1770 train_time:128439ms step_avg:100.03ms
step:1285/1770 train_time:128544ms step_avg:100.03ms
step:1286/1770 train_time:128650ms step_avg:100.04ms
step:1287/1770 train_time:128755ms step_avg:100.04ms
step:1288/1770 train_time:128859ms step_avg:100.05ms
step:1289/1770 train_time:128963ms step_avg:100.05ms
step:1290/1770 train_time:129067ms step_avg:100.05ms
step:1291/1770 train_time:129171ms step_avg:100.05ms
step:1292/1770 train_time:129275ms step_avg:100.06ms
step:1293/1770 train_time:129380ms step_avg:100.06ms
step:1294/1770 train_time:129483ms step_avg:100.06ms
step:1295/1770 train_time:129587ms step_avg:100.07ms
step:1296/1770 train_time:129691ms step_avg:100.07ms
step:1297/1770 train_time:129796ms step_avg:100.07ms
step:1298/1770 train_time:129899ms step_avg:100.08ms
step:1299/1770 train_time:130003ms step_avg:100.08ms
step:1300/1770 train_time:130107ms step_avg:100.08ms
step:1301/1770 train_time:130212ms step_avg:100.09ms
step:1302/1770 train_time:130316ms step_avg:100.09ms
step:1303/1770 train_time:130419ms step_avg:100.09ms
step:1304/1770 train_time:130523ms step_avg:100.09ms
step:1305/1770 train_time:130627ms step_avg:100.10ms
step:1306/1770 train_time:130731ms step_avg:100.10ms
step:1307/1770 train_time:130834ms step_avg:100.10ms
step:1308/1770 train_time:130938ms step_avg:100.11ms
step:1309/1770 train_time:131041ms step_avg:100.11ms
step:1310/1770 train_time:131146ms step_avg:100.11ms
step:1311/1770 train_time:131249ms step_avg:100.11ms
step:1312/1770 train_time:131353ms step_avg:100.12ms
step:1313/1770 train_time:131457ms step_avg:100.12ms
step:1314/1770 train_time:131560ms step_avg:100.12ms
step:1315/1770 train_time:131663ms step_avg:100.12ms
step:1316/1770 train_time:131767ms step_avg:100.13ms
step:1317/1770 train_time:131871ms step_avg:100.13ms
step:1318/1770 train_time:131979ms step_avg:100.14ms
step:1319/1770 train_time:132083ms step_avg:100.14ms
step:1320/1770 train_time:132187ms step_avg:100.14ms
step:1321/1770 train_time:132290ms step_avg:100.14ms
step:1322/1770 train_time:132395ms step_avg:100.15ms
step:1323/1770 train_time:132500ms step_avg:100.15ms
step:1324/1770 train_time:132604ms step_avg:100.15ms
step:1325/1770 train_time:132710ms step_avg:100.16ms
step:1326/1770 train_time:132813ms step_avg:100.16ms
step:1327/1770 train_time:132920ms step_avg:100.17ms
step:1328/1770 train_time:133023ms step_avg:100.17ms
step:1329/1770 train_time:133127ms step_avg:100.17ms
step:1330/1770 train_time:133230ms step_avg:100.17ms
step:1331/1770 train_time:133334ms step_avg:100.18ms
step:1332/1770 train_time:133437ms step_avg:100.18ms
step:1333/1770 train_time:133541ms step_avg:100.18ms
step:1334/1770 train_time:133645ms step_avg:100.18ms
step:1335/1770 train_time:133750ms step_avg:100.19ms
step:1336/1770 train_time:133853ms step_avg:100.19ms
step:1337/1770 train_time:133957ms step_avg:100.19ms
step:1338/1770 train_time:134061ms step_avg:100.20ms
step:1339/1770 train_time:134165ms step_avg:100.20ms
step:1340/1770 train_time:134271ms step_avg:100.20ms
step:1341/1770 train_time:134374ms step_avg:100.20ms
step:1342/1770 train_time:134479ms step_avg:100.21ms
step:1343/1770 train_time:134583ms step_avg:100.21ms
step:1344/1770 train_time:134687ms step_avg:100.21ms
step:1345/1770 train_time:134792ms step_avg:100.22ms
step:1346/1770 train_time:134896ms step_avg:100.22ms
step:1347/1770 train_time:135000ms step_avg:100.22ms
step:1348/1770 train_time:135106ms step_avg:100.23ms
step:1349/1770 train_time:135209ms step_avg:100.23ms
step:1350/1770 train_time:135313ms step_avg:100.23ms
step:1351/1770 train_time:135418ms step_avg:100.24ms
step:1352/1770 train_time:135521ms step_avg:100.24ms
step:1353/1770 train_time:135627ms step_avg:100.24ms
step:1354/1770 train_time:135731ms step_avg:100.24ms
step:1355/1770 train_time:135835ms step_avg:100.25ms
step:1356/1770 train_time:135938ms step_avg:100.25ms
step:1357/1770 train_time:136042ms step_avg:100.25ms
step:1358/1770 train_time:136146ms step_avg:100.26ms
step:1359/1770 train_time:136251ms step_avg:100.26ms
step:1360/1770 train_time:136356ms step_avg:100.26ms
step:1361/1770 train_time:136461ms step_avg:100.27ms
step:1362/1770 train_time:136565ms step_avg:100.27ms
step:1363/1770 train_time:136670ms step_avg:100.27ms
step:1364/1770 train_time:136775ms step_avg:100.27ms
step:1365/1770 train_time:136878ms step_avg:100.28ms
step:1366/1770 train_time:136981ms step_avg:100.28ms
step:1367/1770 train_time:137086ms step_avg:100.28ms
step:1368/1770 train_time:137189ms step_avg:100.28ms
step:1369/1770 train_time:137294ms step_avg:100.29ms
step:1370/1770 train_time:137398ms step_avg:100.29ms
step:1371/1770 train_time:137503ms step_avg:100.29ms
step:1372/1770 train_time:137607ms step_avg:100.30ms
step:1373/1770 train_time:137712ms step_avg:100.30ms
step:1374/1770 train_time:137817ms step_avg:100.30ms
step:1375/1770 train_time:137921ms step_avg:100.31ms
step:1375/1770 val_loss:3.3841 train_time:138020ms step_avg:100.38ms
step:1376/1770 train_time:138040ms step_avg:100.32ms
step:1377/1770 train_time:138136ms step_avg:100.32ms
step:1378/1770 train_time:138239ms step_avg:100.32ms
step:1379/1770 train_time:138343ms step_avg:100.32ms
step:1380/1770 train_time:138447ms step_avg:100.32ms
step:1381/1770 train_time:138552ms step_avg:100.33ms
step:1382/1770 train_time:138656ms step_avg:100.33ms
step:1383/1770 train_time:138760ms step_avg:100.33ms
step:1384/1770 train_time:138864ms step_avg:100.34ms
step:1385/1770 train_time:138969ms step_avg:100.34ms
step:1386/1770 train_time:139074ms step_avg:100.34ms
step:1387/1770 train_time:139178ms step_avg:100.34ms
step:1388/1770 train_time:139282ms step_avg:100.35ms
step:1389/1770 train_time:139387ms step_avg:100.35ms
step:1390/1770 train_time:139491ms step_avg:100.35ms
step:1391/1770 train_time:139595ms step_avg:100.36ms
step:1392/1770 train_time:139699ms step_avg:100.36ms
step:1393/1770 train_time:139803ms step_avg:100.36ms
step:1394/1770 train_time:139908ms step_avg:100.36ms
step:1395/1770 train_time:140012ms step_avg:100.37ms
step:1396/1770 train_time:140117ms step_avg:100.37ms
step:1397/1770 train_time:140222ms step_avg:100.37ms
step:1398/1770 train_time:140326ms step_avg:100.38ms
step:1399/1770 train_time:140429ms step_avg:100.38ms
step:1400/1770 train_time:140535ms step_avg:100.38ms
step:1401/1770 train_time:140639ms step_avg:100.38ms
step:1402/1770 train_time:140743ms step_avg:100.39ms
step:1403/1770 train_time:140847ms step_avg:100.39ms
step:1404/1770 train_time:140952ms step_avg:100.39ms
step:1405/1770 train_time:141056ms step_avg:100.40ms
step:1406/1770 train_time:141161ms step_avg:100.40ms
step:1407/1770 train_time:141264ms step_avg:100.40ms
step:1408/1770 train_time:141368ms step_avg:100.40ms
step:1409/1770 train_time:141472ms step_avg:100.41ms
step:1410/1770 train_time:141577ms step_avg:100.41ms
step:1411/1770 train_time:141681ms step_avg:100.41ms
step:1412/1770 train_time:141785ms step_avg:100.41ms
step:1413/1770 train_time:141888ms step_avg:100.42ms
step:1414/1770 train_time:141993ms step_avg:100.42ms
step:1415/1770 train_time:142098ms step_avg:100.42ms
step:1416/1770 train_time:142202ms step_avg:100.43ms
step:1417/1770 train_time:142306ms step_avg:100.43ms
step:1418/1770 train_time:142410ms step_avg:100.43ms
step:1419/1770 train_time:142516ms step_avg:100.43ms
step:1420/1770 train_time:142620ms step_avg:100.44ms
step:1421/1770 train_time:142723ms step_avg:100.44ms
step:1422/1770 train_time:142827ms step_avg:100.44ms
step:1423/1770 train_time:142932ms step_avg:100.44ms
step:1424/1770 train_time:143036ms step_avg:100.45ms
step:1425/1770 train_time:143140ms step_avg:100.45ms
step:1426/1770 train_time:143244ms step_avg:100.45ms
step:1427/1770 train_time:143348ms step_avg:100.45ms
step:1428/1770 train_time:143453ms step_avg:100.46ms
step:1429/1770 train_time:143558ms step_avg:100.46ms
step:1430/1770 train_time:143662ms step_avg:100.46ms
step:1431/1770 train_time:143766ms step_avg:100.47ms
step:1432/1770 train_time:143870ms step_avg:100.47ms
step:1433/1770 train_time:143974ms step_avg:100.47ms
step:1434/1770 train_time:144077ms step_avg:100.47ms
step:1435/1770 train_time:144181ms step_avg:100.47ms
step:1436/1770 train_time:144287ms step_avg:100.48ms
step:1437/1770 train_time:144391ms step_avg:100.48ms
step:1438/1770 train_time:144496ms step_avg:100.48ms
step:1439/1770 train_time:144599ms step_avg:100.49ms
step:1440/1770 train_time:144702ms step_avg:100.49ms
step:1441/1770 train_time:144809ms step_avg:100.49ms
step:1442/1770 train_time:144913ms step_avg:100.49ms
step:1443/1770 train_time:145017ms step_avg:100.50ms
step:1444/1770 train_time:145123ms step_avg:100.50ms
step:1445/1770 train_time:145227ms step_avg:100.50ms
step:1446/1770 train_time:145332ms step_avg:100.51ms
step:1447/1770 train_time:145437ms step_avg:100.51ms
step:1448/1770 train_time:145542ms step_avg:100.51ms
step:1449/1770 train_time:145648ms step_avg:100.52ms
step:1450/1770 train_time:145754ms step_avg:100.52ms
step:1451/1770 train_time:145860ms step_avg:100.52ms
step:1452/1770 train_time:145965ms step_avg:100.53ms
step:1453/1770 train_time:146070ms step_avg:100.53ms
step:1454/1770 train_time:146176ms step_avg:100.53ms
step:1455/1770 train_time:146282ms step_avg:100.54ms
step:1456/1770 train_time:146389ms step_avg:100.54ms
step:1457/1770 train_time:146495ms step_avg:100.55ms
step:1458/1770 train_time:146599ms step_avg:100.55ms
step:1459/1770 train_time:146705ms step_avg:100.55ms
step:1460/1770 train_time:146811ms step_avg:100.56ms
step:1461/1770 train_time:146916ms step_avg:100.56ms
step:1462/1770 train_time:147022ms step_avg:100.56ms
step:1463/1770 train_time:147127ms step_avg:100.57ms
step:1464/1770 train_time:147235ms step_avg:100.57ms
step:1465/1770 train_time:147340ms step_avg:100.57ms
step:1466/1770 train_time:147447ms step_avg:100.58ms
step:1467/1770 train_time:147554ms step_avg:100.58ms
step:1468/1770 train_time:147660ms step_avg:100.59ms
step:1469/1770 train_time:147765ms step_avg:100.59ms
step:1470/1770 train_time:147870ms step_avg:100.59ms
step:1471/1770 train_time:147975ms step_avg:100.60ms
step:1472/1770 train_time:148080ms step_avg:100.60ms
step:1473/1770 train_time:148187ms step_avg:100.60ms
step:1474/1770 train_time:148293ms step_avg:100.61ms
step:1475/1770 train_time:148398ms step_avg:100.61ms
step:1476/1770 train_time:148503ms step_avg:100.61ms
step:1477/1770 train_time:148609ms step_avg:100.62ms
step:1478/1770 train_time:148715ms step_avg:100.62ms
step:1479/1770 train_time:148821ms step_avg:100.62ms
step:1480/1770 train_time:148926ms step_avg:100.63ms
step:1481/1770 train_time:149034ms step_avg:100.63ms
step:1482/1770 train_time:149139ms step_avg:100.63ms
step:1483/1770 train_time:149243ms step_avg:100.64ms
step:1484/1770 train_time:149348ms step_avg:100.64ms
step:1485/1770 train_time:149453ms step_avg:100.64ms
step:1486/1770 train_time:149559ms step_avg:100.65ms
step:1487/1770 train_time:149663ms step_avg:100.65ms
step:1488/1770 train_time:149769ms step_avg:100.65ms
step:1489/1770 train_time:149876ms step_avg:100.66ms
step:1490/1770 train_time:149982ms step_avg:100.66ms
step:1491/1770 train_time:150087ms step_avg:100.66ms
step:1492/1770 train_time:150193ms step_avg:100.67ms
step:1493/1770 train_time:150301ms step_avg:100.67ms
step:1494/1770 train_time:150410ms step_avg:100.68ms
step:1495/1770 train_time:150515ms step_avg:100.68ms
step:1496/1770 train_time:150620ms step_avg:100.68ms
step:1497/1770 train_time:150725ms step_avg:100.68ms
step:1498/1770 train_time:150830ms step_avg:100.69ms
step:1499/1770 train_time:150934ms step_avg:100.69ms
step:1500/1770 train_time:151039ms step_avg:100.69ms
step:1500/1770 val_loss:3.3460 train_time:151139ms step_avg:100.76ms
step:1501/1770 train_time:151161ms step_avg:100.71ms
step:1502/1770 train_time:151256ms step_avg:100.70ms
step:1503/1770 train_time:151362ms step_avg:100.71ms
step:1504/1770 train_time:151468ms step_avg:100.71ms
step:1505/1770 train_time:151574ms step_avg:100.71ms
step:1506/1770 train_time:151679ms step_avg:100.72ms
step:1507/1770 train_time:151785ms step_avg:100.72ms
step:1508/1770 train_time:151892ms step_avg:100.72ms
step:1509/1770 train_time:151996ms step_avg:100.73ms
step:1510/1770 train_time:152101ms step_avg:100.73ms
step:1511/1770 train_time:152207ms step_avg:100.73ms
step:1512/1770 train_time:152312ms step_avg:100.74ms
step:1513/1770 train_time:152418ms step_avg:100.74ms
step:1514/1770 train_time:152524ms step_avg:100.74ms
step:1515/1770 train_time:152630ms step_avg:100.75ms
step:1516/1770 train_time:152735ms step_avg:100.75ms
step:1517/1770 train_time:152840ms step_avg:100.75ms
step:1518/1770 train_time:152947ms step_avg:100.76ms
step:1519/1770 train_time:153051ms step_avg:100.76ms
step:1520/1770 train_time:153158ms step_avg:100.76ms
step:1521/1770 train_time:153262ms step_avg:100.76ms
step:1522/1770 train_time:153368ms step_avg:100.77ms
step:1523/1770 train_time:153475ms step_avg:100.77ms
step:1524/1770 train_time:153580ms step_avg:100.77ms
step:1525/1770 train_time:153686ms step_avg:100.78ms
step:1526/1770 train_time:153791ms step_avg:100.78ms
step:1527/1770 train_time:153895ms step_avg:100.78ms
step:1528/1770 train_time:154001ms step_avg:100.79ms
step:1529/1770 train_time:154106ms step_avg:100.79ms
step:1530/1770 train_time:154211ms step_avg:100.79ms
step:1531/1770 train_time:154317ms step_avg:100.80ms
step:1532/1770 train_time:154422ms step_avg:100.80ms
step:1533/1770 train_time:154527ms step_avg:100.80ms
step:1534/1770 train_time:154633ms step_avg:100.80ms
step:1535/1770 train_time:154737ms step_avg:100.81ms
step:1536/1770 train_time:154842ms step_avg:100.81ms
step:1537/1770 train_time:154949ms step_avg:100.81ms
step:1538/1770 train_time:155055ms step_avg:100.82ms
step:1539/1770 train_time:155160ms step_avg:100.82ms
step:1540/1770 train_time:155269ms step_avg:100.82ms
step:1541/1770 train_time:155376ms step_avg:100.83ms
step:1542/1770 train_time:155481ms step_avg:100.83ms
step:1543/1770 train_time:155586ms step_avg:100.83ms
step:1544/1770 train_time:155694ms step_avg:100.84ms
step:1545/1770 train_time:155799ms step_avg:100.84ms
step:1546/1770 train_time:155905ms step_avg:100.84ms
step:1547/1770 train_time:156010ms step_avg:100.85ms
step:1548/1770 train_time:156114ms step_avg:100.85ms
step:1549/1770 train_time:156219ms step_avg:100.85ms
step:1550/1770 train_time:156325ms step_avg:100.86ms
step:1551/1770 train_time:156429ms step_avg:100.86ms
step:1552/1770 train_time:156537ms step_avg:100.86ms
step:1553/1770 train_time:156641ms step_avg:100.86ms
step:1554/1770 train_time:156746ms step_avg:100.87ms
step:1555/1770 train_time:156853ms step_avg:100.87ms
step:1556/1770 train_time:156957ms step_avg:100.87ms
step:1557/1770 train_time:157062ms step_avg:100.87ms
step:1558/1770 train_time:157168ms step_avg:100.88ms
step:1559/1770 train_time:157272ms step_avg:100.88ms
step:1560/1770 train_time:157377ms step_avg:100.88ms
step:1561/1770 train_time:157484ms step_avg:100.89ms
step:1562/1770 train_time:157590ms step_avg:100.89ms
step:1563/1770 train_time:157695ms step_avg:100.89ms
step:1564/1770 train_time:157799ms step_avg:100.89ms
step:1565/1770 train_time:157904ms step_avg:100.90ms
step:1566/1770 train_time:158010ms step_avg:100.90ms
step:1567/1770 train_time:158115ms step_avg:100.90ms
step:1568/1770 train_time:158220ms step_avg:100.91ms
step:1569/1770 train_time:158328ms step_avg:100.91ms
step:1570/1770 train_time:158433ms step_avg:100.91ms
step:1571/1770 train_time:158539ms step_avg:100.92ms
step:1572/1770 train_time:158645ms step_avg:100.92ms
step:1573/1770 train_time:158753ms step_avg:100.92ms
step:1574/1770 train_time:158859ms step_avg:100.93ms
step:1575/1770 train_time:158963ms step_avg:100.93ms
step:1576/1770 train_time:159069ms step_avg:100.93ms
step:1577/1770 train_time:159175ms step_avg:100.94ms
step:1578/1770 train_time:159281ms step_avg:100.94ms
step:1579/1770 train_time:159386ms step_avg:100.94ms
step:1580/1770 train_time:159491ms step_avg:100.94ms
step:1581/1770 train_time:159598ms step_avg:100.95ms
step:1582/1770 train_time:159705ms step_avg:100.95ms
step:1583/1770 train_time:159811ms step_avg:100.95ms
step:1584/1770 train_time:159916ms step_avg:100.96ms
step:1585/1770 train_time:160021ms step_avg:100.96ms
step:1586/1770 train_time:160130ms step_avg:100.96ms
step:1587/1770 train_time:160236ms step_avg:100.97ms
step:1588/1770 train_time:160342ms step_avg:100.97ms
step:1589/1770 train_time:160448ms step_avg:100.97ms
step:1590/1770 train_time:160553ms step_avg:100.98ms
step:1591/1770 train_time:160657ms step_avg:100.98ms
step:1592/1770 train_time:160764ms step_avg:100.98ms
step:1593/1770 train_time:160869ms step_avg:100.98ms
step:1594/1770 train_time:160974ms step_avg:100.99ms
step:1595/1770 train_time:161079ms step_avg:100.99ms
step:1596/1770 train_time:161185ms step_avg:100.99ms
step:1597/1770 train_time:161290ms step_avg:101.00ms
step:1598/1770 train_time:161396ms step_avg:101.00ms
step:1599/1770 train_time:161503ms step_avg:101.00ms
step:1600/1770 train_time:161610ms step_avg:101.01ms
step:1601/1770 train_time:161717ms step_avg:101.01ms
step:1602/1770 train_time:161823ms step_avg:101.01ms
step:1603/1770 train_time:161928ms step_avg:101.02ms
step:1604/1770 train_time:162032ms step_avg:101.02ms
step:1605/1770 train_time:162138ms step_avg:101.02ms
step:1606/1770 train_time:162244ms step_avg:101.02ms
step:1607/1770 train_time:162353ms step_avg:101.03ms
step:1608/1770 train_time:162459ms step_avg:101.03ms
step:1609/1770 train_time:162565ms step_avg:101.03ms
step:1610/1770 train_time:162670ms step_avg:101.04ms
step:1611/1770 train_time:162776ms step_avg:101.04ms
step:1612/1770 train_time:162882ms step_avg:101.04ms
step:1613/1770 train_time:162987ms step_avg:101.05ms
step:1614/1770 train_time:163093ms step_avg:101.05ms
step:1615/1770 train_time:163199ms step_avg:101.05ms
step:1616/1770 train_time:163305ms step_avg:101.06ms
step:1617/1770 train_time:163413ms step_avg:101.06ms
step:1618/1770 train_time:163520ms step_avg:101.06ms
step:1619/1770 train_time:163626ms step_avg:101.07ms
step:1620/1770 train_time:163732ms step_avg:101.07ms
step:1621/1770 train_time:163838ms step_avg:101.07ms
step:1622/1770 train_time:163944ms step_avg:101.08ms
step:1623/1770 train_time:164052ms step_avg:101.08ms
step:1624/1770 train_time:164157ms step_avg:101.08ms
step:1625/1770 train_time:164261ms step_avg:101.08ms
step:1625/1770 val_loss:3.3121 train_time:164362ms step_avg:101.15ms
step:1626/1770 train_time:164382ms step_avg:101.10ms
step:1627/1770 train_time:164476ms step_avg:101.09ms
step:1628/1770 train_time:164581ms step_avg:101.09ms
step:1629/1770 train_time:164685ms step_avg:101.10ms
step:1630/1770 train_time:164790ms step_avg:101.10ms
step:1631/1770 train_time:164895ms step_avg:101.10ms
step:1632/1770 train_time:165000ms step_avg:101.10ms
step:1633/1770 train_time:165107ms step_avg:101.11ms
step:1634/1770 train_time:165212ms step_avg:101.11ms
step:1635/1770 train_time:165318ms step_avg:101.11ms
step:1636/1770 train_time:165424ms step_avg:101.11ms
step:1637/1770 train_time:165529ms step_avg:101.12ms
step:1638/1770 train_time:165635ms step_avg:101.12ms
step:1639/1770 train_time:165740ms step_avg:101.12ms
step:1640/1770 train_time:165846ms step_avg:101.13ms
step:1641/1770 train_time:165951ms step_avg:101.13ms
step:1642/1770 train_time:166056ms step_avg:101.13ms
step:1643/1770 train_time:166162ms step_avg:101.13ms
step:1644/1770 train_time:166268ms step_avg:101.14ms
step:1645/1770 train_time:166373ms step_avg:101.14ms
step:1646/1770 train_time:166480ms step_avg:101.14ms
step:1647/1770 train_time:166586ms step_avg:101.14ms
step:1648/1770 train_time:166690ms step_avg:101.15ms
step:1649/1770 train_time:166796ms step_avg:101.15ms
step:1650/1770 train_time:166901ms step_avg:101.15ms
step:1651/1770 train_time:167006ms step_avg:101.15ms
step:1652/1770 train_time:167112ms step_avg:101.16ms
step:1653/1770 train_time:167216ms step_avg:101.16ms
step:1654/1770 train_time:167326ms step_avg:101.16ms
step:1655/1770 train_time:167433ms step_avg:101.17ms
step:1656/1770 train_time:167538ms step_avg:101.17ms
step:1657/1770 train_time:167646ms step_avg:101.17ms
step:1658/1770 train_time:167751ms step_avg:101.18ms
step:1659/1770 train_time:167858ms step_avg:101.18ms
step:1660/1770 train_time:167963ms step_avg:101.18ms
step:1661/1770 train_time:168069ms step_avg:101.19ms
step:1662/1770 train_time:168174ms step_avg:101.19ms
step:1663/1770 train_time:168279ms step_avg:101.19ms
step:1664/1770 train_time:168385ms step_avg:101.19ms
step:1665/1770 train_time:168489ms step_avg:101.19ms
step:1666/1770 train_time:168595ms step_avg:101.20ms
step:1667/1770 train_time:168700ms step_avg:101.20ms
step:1668/1770 train_time:168806ms step_avg:101.20ms
step:1669/1770 train_time:168909ms step_avg:101.20ms
step:1670/1770 train_time:169014ms step_avg:101.21ms
step:1671/1770 train_time:169120ms step_avg:101.21ms
step:1672/1770 train_time:169226ms step_avg:101.21ms
step:1673/1770 train_time:169332ms step_avg:101.21ms
step:1674/1770 train_time:169437ms step_avg:101.22ms
step:1675/1770 train_time:169542ms step_avg:101.22ms
step:1676/1770 train_time:169648ms step_avg:101.22ms
step:1677/1770 train_time:169756ms step_avg:101.23ms
step:1678/1770 train_time:169861ms step_avg:101.23ms
step:1679/1770 train_time:169966ms step_avg:101.23ms
step:1680/1770 train_time:170071ms step_avg:101.23ms
step:1681/1770 train_time:170176ms step_avg:101.24ms
step:1682/1770 train_time:170284ms step_avg:101.24ms
step:1683/1770 train_time:170388ms step_avg:101.24ms
step:1684/1770 train_time:170493ms step_avg:101.24ms
step:1685/1770 train_time:170598ms step_avg:101.25ms
step:1686/1770 train_time:170704ms step_avg:101.25ms
step:1687/1770 train_time:170812ms step_avg:101.25ms
step:1688/1770 train_time:170919ms step_avg:101.26ms
step:1689/1770 train_time:171025ms step_avg:101.26ms
step:1690/1770 train_time:171130ms step_avg:101.26ms
step:1691/1770 train_time:171235ms step_avg:101.26ms
step:1692/1770 train_time:171341ms step_avg:101.27ms
step:1693/1770 train_time:171447ms step_avg:101.27ms
step:1694/1770 train_time:171551ms step_avg:101.27ms
step:1695/1770 train_time:171658ms step_avg:101.27ms
step:1696/1770 train_time:171765ms step_avg:101.28ms
step:1697/1770 train_time:171873ms step_avg:101.28ms
step:1698/1770 train_time:171980ms step_avg:101.28ms
step:1699/1770 train_time:172085ms step_avg:101.29ms
step:1700/1770 train_time:172189ms step_avg:101.29ms
step:1701/1770 train_time:172294ms step_avg:101.29ms
step:1702/1770 train_time:172400ms step_avg:101.29ms
step:1703/1770 train_time:172505ms step_avg:101.29ms
step:1704/1770 train_time:172611ms step_avg:101.30ms
step:1705/1770 train_time:172716ms step_avg:101.30ms
step:1706/1770 train_time:172821ms step_avg:101.30ms
step:1707/1770 train_time:172928ms step_avg:101.30ms
step:1708/1770 train_time:173033ms step_avg:101.31ms
step:1709/1770 train_time:173141ms step_avg:101.31ms
step:1710/1770 train_time:173250ms step_avg:101.32ms
step:1711/1770 train_time:173359ms step_avg:101.32ms
step:1712/1770 train_time:173465ms step_avg:101.32ms
step:1713/1770 train_time:173570ms step_avg:101.33ms
step:1714/1770 train_time:173676ms step_avg:101.33ms
step:1715/1770 train_time:173781ms step_avg:101.33ms
step:1716/1770 train_time:173888ms step_avg:101.33ms
step:1717/1770 train_time:173994ms step_avg:101.34ms
step:1718/1770 train_time:174101ms step_avg:101.34ms
step:1719/1770 train_time:174207ms step_avg:101.34ms
step:1720/1770 train_time:174316ms step_avg:101.35ms
step:1721/1770 train_time:174422ms step_avg:101.35ms
step:1722/1770 train_time:174531ms step_avg:101.35ms
step:1723/1770 train_time:174639ms step_avg:101.36ms
step:1724/1770 train_time:174748ms step_avg:101.36ms
step:1725/1770 train_time:174857ms step_avg:101.37ms
step:1726/1770 train_time:174964ms step_avg:101.37ms
step:1727/1770 train_time:175071ms step_avg:101.37ms
step:1728/1770 train_time:175179ms step_avg:101.38ms
step:1729/1770 train_time:175286ms step_avg:101.38ms
step:1730/1770 train_time:175393ms step_avg:101.38ms
step:1731/1770 train_time:175502ms step_avg:101.39ms
step:1732/1770 train_time:175607ms step_avg:101.39ms
step:1733/1770 train_time:175715ms step_avg:101.39ms
step:1734/1770 train_time:175821ms step_avg:101.40ms
step:1735/1770 train_time:175928ms step_avg:101.40ms
step:1736/1770 train_time:176033ms step_avg:101.40ms
step:1737/1770 train_time:176139ms step_avg:101.40ms
step:1738/1770 train_time:176245ms step_avg:101.41ms
step:1739/1770 train_time:176351ms step_avg:101.41ms
step:1740/1770 train_time:176456ms step_avg:101.41ms
step:1741/1770 train_time:176565ms step_avg:101.42ms
step:1742/1770 train_time:176675ms step_avg:101.42ms
step:1743/1770 train_time:176783ms step_avg:101.42ms
step:1744/1770 train_time:176888ms step_avg:101.43ms
step:1745/1770 train_time:176994ms step_avg:101.43ms
step:1746/1770 train_time:177104ms step_avg:101.43ms
step:1747/1770 train_time:177209ms step_avg:101.44ms
step:1748/1770 train_time:177317ms step_avg:101.44ms
step:1749/1770 train_time:177424ms step_avg:101.44ms
step:1750/1770 train_time:177529ms step_avg:101.45ms
step:1750/1770 val_loss:3.2847 train_time:177630ms step_avg:101.50ms
step:1751/1770 train_time:177650ms step_avg:101.46ms
step:1752/1770 train_time:177747ms step_avg:101.45ms
step:1753/1770 train_time:177853ms step_avg:101.46ms
step:1754/1770 train_time:177960ms step_avg:101.46ms
step:1755/1770 train_time:178065ms step_avg:101.46ms
step:1756/1770 train_time:178173ms step_avg:101.47ms
step:1757/1770 train_time:178279ms step_avg:101.47ms
step:1758/1770 train_time:178385ms step_avg:101.47ms
step:1759/1770 train_time:178492ms step_avg:101.47ms
step:1760/1770 train_time:178597ms step_avg:101.48ms
step:1761/1770 train_time:178706ms step_avg:101.48ms
step:1762/1770 train_time:178815ms step_avg:101.48ms
step:1763/1770 train_time:178920ms step_avg:101.49ms
step:1764/1770 train_time:179027ms step_avg:101.49ms
step:1765/1770 train_time:179134ms step_avg:101.49ms
step:1766/1770 train_time:179244ms step_avg:101.50ms
step:1767/1770 train_time:179349ms step_avg:101.50ms
step:1768/1770 train_time:179457ms step_avg:101.50ms
step:1769/1770 train_time:179561ms step_avg:101.50ms
step:1770/1770 train_time:179667ms step_avg:101.51ms
step:1770/1770 val_loss:3.2817 train_time:179768ms step_avg:101.56ms
peak memory allocated: 30724 MiB reserved: 45392 MiB
