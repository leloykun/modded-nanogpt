import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.1357, -4.2084, 1.0726),
        (4.132, -4.2045, 1.0725),
        (4.077, -4.1489, 1.0719),
        (4.0422, -4.1139, 1.0717),
        (3.9129, -3.9845, 1.0715),
        (3.3337, -3.2386, 0.9049),
        (2.2005, -1.6921, 0.4915),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 07:00:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            114W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            115W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            115W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            108W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:57ms step_avg:57.24ms
step:2/1770 train_time:135ms step_avg:67.70ms
step:3/1770 train_time:228ms step_avg:75.95ms
step:4/1770 train_time:323ms step_avg:80.87ms
step:5/1770 train_time:419ms step_avg:83.88ms
step:6/1770 train_time:515ms step_avg:85.90ms
step:7/1770 train_time:611ms step_avg:87.32ms
step:8/1770 train_time:707ms step_avg:88.42ms
step:9/1770 train_time:804ms step_avg:89.36ms
step:10/1770 train_time:901ms step_avg:90.07ms
step:11/1770 train_time:996ms step_avg:90.59ms
step:12/1770 train_time:1092ms step_avg:91.03ms
step:13/1770 train_time:1188ms step_avg:91.40ms
step:14/1770 train_time:1284ms step_avg:91.75ms
step:15/1770 train_time:1381ms step_avg:92.07ms
step:16/1770 train_time:1477ms step_avg:92.33ms
step:17/1770 train_time:1575ms step_avg:92.62ms
step:18/1770 train_time:1670ms step_avg:92.78ms
step:19/1770 train_time:1766ms step_avg:92.95ms
step:20/1770 train_time:1863ms step_avg:93.13ms
step:21/1770 train_time:1959ms step_avg:93.26ms
step:22/1770 train_time:2055ms step_avg:93.40ms
step:23/1770 train_time:2151ms step_avg:93.52ms
step:24/1770 train_time:2247ms step_avg:93.63ms
step:25/1770 train_time:2344ms step_avg:93.74ms
step:26/1770 train_time:2440ms step_avg:93.84ms
step:27/1770 train_time:2536ms step_avg:93.93ms
step:28/1770 train_time:2632ms step_avg:94.01ms
step:29/1770 train_time:2728ms step_avg:94.08ms
step:30/1770 train_time:2824ms step_avg:94.15ms
step:31/1770 train_time:2920ms step_avg:94.20ms
step:32/1770 train_time:3016ms step_avg:94.26ms
step:33/1770 train_time:3113ms step_avg:94.32ms
step:34/1770 train_time:3209ms step_avg:94.37ms
step:35/1770 train_time:3305ms step_avg:94.43ms
step:36/1770 train_time:3402ms step_avg:94.49ms
step:37/1770 train_time:3498ms step_avg:94.54ms
step:38/1770 train_time:3594ms step_avg:94.58ms
step:39/1770 train_time:3691ms step_avg:94.63ms
step:40/1770 train_time:3787ms step_avg:94.68ms
step:41/1770 train_time:3884ms step_avg:94.72ms
step:42/1770 train_time:3980ms step_avg:94.77ms
step:43/1770 train_time:4076ms step_avg:94.80ms
step:44/1770 train_time:4173ms step_avg:94.84ms
step:45/1770 train_time:4269ms step_avg:94.87ms
step:46/1770 train_time:4366ms step_avg:94.91ms
step:47/1770 train_time:4462ms step_avg:94.94ms
step:48/1770 train_time:4558ms step_avg:94.96ms
step:49/1770 train_time:4655ms step_avg:94.99ms
step:50/1770 train_time:4751ms step_avg:95.01ms
step:51/1770 train_time:4847ms step_avg:95.04ms
step:52/1770 train_time:4944ms step_avg:95.07ms
step:53/1770 train_time:5040ms step_avg:95.09ms
step:54/1770 train_time:5136ms step_avg:95.11ms
step:55/1770 train_time:5232ms step_avg:95.13ms
step:56/1770 train_time:5328ms step_avg:95.14ms
step:57/1770 train_time:5425ms step_avg:95.18ms
step:58/1770 train_time:5521ms step_avg:95.19ms
step:59/1770 train_time:5617ms step_avg:95.20ms
step:60/1770 train_time:5713ms step_avg:95.22ms
step:61/1770 train_time:5809ms step_avg:95.23ms
step:62/1770 train_time:5905ms step_avg:95.24ms
step:63/1770 train_time:6001ms step_avg:95.26ms
step:64/1770 train_time:6098ms step_avg:95.28ms
step:65/1770 train_time:6194ms step_avg:95.29ms
step:66/1770 train_time:6289ms step_avg:95.29ms
step:67/1770 train_time:6386ms step_avg:95.31ms
step:68/1770 train_time:6482ms step_avg:95.32ms
step:69/1770 train_time:6579ms step_avg:95.34ms
step:70/1770 train_time:6675ms step_avg:95.36ms
step:71/1770 train_time:6771ms step_avg:95.37ms
step:72/1770 train_time:6867ms step_avg:95.38ms
step:73/1770 train_time:6964ms step_avg:95.39ms
step:74/1770 train_time:7060ms step_avg:95.40ms
step:75/1770 train_time:7156ms step_avg:95.41ms
step:76/1770 train_time:7252ms step_avg:95.42ms
step:77/1770 train_time:7348ms step_avg:95.42ms
step:78/1770 train_time:7444ms step_avg:95.44ms
step:79/1770 train_time:7540ms step_avg:95.44ms
step:80/1770 train_time:7636ms step_avg:95.45ms
step:81/1770 train_time:7732ms step_avg:95.46ms
step:82/1770 train_time:7828ms step_avg:95.46ms
step:83/1770 train_time:7925ms step_avg:95.48ms
step:84/1770 train_time:8021ms step_avg:95.49ms
step:85/1770 train_time:8118ms step_avg:95.51ms
step:86/1770 train_time:8214ms step_avg:95.51ms
step:87/1770 train_time:8310ms step_avg:95.51ms
step:88/1770 train_time:8406ms step_avg:95.52ms
step:89/1770 train_time:8502ms step_avg:95.53ms
step:90/1770 train_time:8598ms step_avg:95.53ms
step:91/1770 train_time:8694ms step_avg:95.53ms
step:92/1770 train_time:8790ms step_avg:95.54ms
step:93/1770 train_time:8886ms step_avg:95.55ms
step:94/1770 train_time:8982ms step_avg:95.55ms
step:95/1770 train_time:9079ms step_avg:95.57ms
step:96/1770 train_time:9175ms step_avg:95.57ms
step:97/1770 train_time:9272ms step_avg:95.58ms
step:98/1770 train_time:9368ms step_avg:95.59ms
step:99/1770 train_time:9464ms step_avg:95.60ms
step:100/1770 train_time:9560ms step_avg:95.60ms
step:101/1770 train_time:9656ms step_avg:95.60ms
step:102/1770 train_time:9752ms step_avg:95.61ms
step:103/1770 train_time:9848ms step_avg:95.61ms
step:104/1770 train_time:9945ms step_avg:95.63ms
step:105/1770 train_time:10042ms step_avg:95.64ms
step:106/1770 train_time:10138ms step_avg:95.64ms
step:107/1770 train_time:10233ms step_avg:95.64ms
step:108/1770 train_time:10330ms step_avg:95.65ms
step:109/1770 train_time:10426ms step_avg:95.65ms
step:110/1770 train_time:10522ms step_avg:95.66ms
step:111/1770 train_time:10619ms step_avg:95.67ms
step:112/1770 train_time:10714ms step_avg:95.66ms
step:113/1770 train_time:10810ms step_avg:95.66ms
step:114/1770 train_time:10906ms step_avg:95.67ms
step:115/1770 train_time:11002ms step_avg:95.67ms
step:116/1770 train_time:11099ms step_avg:95.68ms
step:117/1770 train_time:11195ms step_avg:95.68ms
step:118/1770 train_time:11291ms step_avg:95.68ms
step:119/1770 train_time:11387ms step_avg:95.69ms
step:120/1770 train_time:11484ms step_avg:95.70ms
step:121/1770 train_time:11580ms step_avg:95.70ms
step:122/1770 train_time:11676ms step_avg:95.70ms
step:123/1770 train_time:11772ms step_avg:95.71ms
step:124/1770 train_time:11868ms step_avg:95.71ms
step:125/1770 train_time:11963ms step_avg:95.71ms
step:125/1770 val_loss:4.6420 train_time:12053ms step_avg:96.43ms
step:126/1770 train_time:12076ms step_avg:95.84ms
step:127/1770 train_time:12162ms step_avg:95.76ms
step:128/1770 train_time:12265ms step_avg:95.82ms
step:129/1770 train_time:12363ms step_avg:95.84ms
step:130/1770 train_time:12459ms step_avg:95.84ms
step:131/1770 train_time:12555ms step_avg:95.84ms
step:132/1770 train_time:12651ms step_avg:95.84ms
step:133/1770 train_time:12747ms step_avg:95.84ms
step:134/1770 train_time:12843ms step_avg:95.84ms
step:135/1770 train_time:12940ms step_avg:95.85ms
step:136/1770 train_time:13036ms step_avg:95.86ms
step:137/1770 train_time:13133ms step_avg:95.86ms
step:138/1770 train_time:13230ms step_avg:95.87ms
step:139/1770 train_time:13327ms step_avg:95.88ms
step:140/1770 train_time:13425ms step_avg:95.89ms
step:141/1770 train_time:13521ms step_avg:95.89ms
step:142/1770 train_time:13618ms step_avg:95.90ms
step:143/1770 train_time:13714ms step_avg:95.90ms
step:144/1770 train_time:13811ms step_avg:95.91ms
step:145/1770 train_time:13908ms step_avg:95.92ms
step:146/1770 train_time:14005ms step_avg:95.93ms
step:147/1770 train_time:14102ms step_avg:95.93ms
step:148/1770 train_time:14198ms step_avg:95.94ms
step:149/1770 train_time:14295ms step_avg:95.94ms
step:150/1770 train_time:14392ms step_avg:95.95ms
step:151/1770 train_time:14489ms step_avg:95.95ms
step:152/1770 train_time:14586ms step_avg:95.96ms
step:153/1770 train_time:14683ms step_avg:95.97ms
step:154/1770 train_time:14780ms step_avg:95.97ms
step:155/1770 train_time:14877ms step_avg:95.98ms
step:156/1770 train_time:14973ms step_avg:95.98ms
step:157/1770 train_time:15071ms step_avg:95.99ms
step:158/1770 train_time:15168ms step_avg:96.00ms
step:159/1770 train_time:15265ms step_avg:96.01ms
step:160/1770 train_time:15363ms step_avg:96.02ms
step:161/1770 train_time:15460ms step_avg:96.03ms
step:162/1770 train_time:15557ms step_avg:96.03ms
step:163/1770 train_time:15654ms step_avg:96.03ms
step:164/1770 train_time:15750ms step_avg:96.04ms
step:165/1770 train_time:15847ms step_avg:96.04ms
step:166/1770 train_time:15944ms step_avg:96.05ms
step:167/1770 train_time:16040ms step_avg:96.05ms
step:168/1770 train_time:16136ms step_avg:96.05ms
step:169/1770 train_time:16233ms step_avg:96.05ms
step:170/1770 train_time:16330ms step_avg:96.06ms
step:171/1770 train_time:16427ms step_avg:96.06ms
step:172/1770 train_time:16524ms step_avg:96.07ms
step:173/1770 train_time:16621ms step_avg:96.07ms
step:174/1770 train_time:16718ms step_avg:96.08ms
step:175/1770 train_time:16814ms step_avg:96.08ms
step:176/1770 train_time:16911ms step_avg:96.09ms
step:177/1770 train_time:17009ms step_avg:96.09ms
step:178/1770 train_time:17107ms step_avg:96.11ms
step:179/1770 train_time:17205ms step_avg:96.11ms
step:180/1770 train_time:17301ms step_avg:96.12ms
step:181/1770 train_time:17397ms step_avg:96.12ms
step:182/1770 train_time:17494ms step_avg:96.12ms
step:183/1770 train_time:17591ms step_avg:96.13ms
step:184/1770 train_time:17688ms step_avg:96.13ms
step:185/1770 train_time:17785ms step_avg:96.14ms
step:186/1770 train_time:17882ms step_avg:96.14ms
step:187/1770 train_time:17978ms step_avg:96.14ms
step:188/1770 train_time:18076ms step_avg:96.15ms
step:189/1770 train_time:18172ms step_avg:96.15ms
step:190/1770 train_time:18269ms step_avg:96.15ms
step:191/1770 train_time:18366ms step_avg:96.16ms
step:192/1770 train_time:18464ms step_avg:96.17ms
step:193/1770 train_time:18561ms step_avg:96.17ms
step:194/1770 train_time:18657ms step_avg:96.17ms
step:195/1770 train_time:18754ms step_avg:96.17ms
step:196/1770 train_time:18850ms step_avg:96.17ms
step:197/1770 train_time:18947ms step_avg:96.18ms
step:198/1770 train_time:19043ms step_avg:96.18ms
step:199/1770 train_time:19140ms step_avg:96.18ms
step:200/1770 train_time:19236ms step_avg:96.18ms
step:201/1770 train_time:19333ms step_avg:96.18ms
step:202/1770 train_time:19429ms step_avg:96.19ms
step:203/1770 train_time:19526ms step_avg:96.19ms
step:204/1770 train_time:19623ms step_avg:96.19ms
step:205/1770 train_time:19720ms step_avg:96.19ms
step:206/1770 train_time:19817ms step_avg:96.20ms
step:207/1770 train_time:19914ms step_avg:96.20ms
step:208/1770 train_time:20011ms step_avg:96.20ms
step:209/1770 train_time:20108ms step_avg:96.21ms
step:210/1770 train_time:20205ms step_avg:96.21ms
step:211/1770 train_time:20301ms step_avg:96.22ms
step:212/1770 train_time:20398ms step_avg:96.22ms
step:213/1770 train_time:20495ms step_avg:96.22ms
step:214/1770 train_time:20592ms step_avg:96.22ms
step:215/1770 train_time:20688ms step_avg:96.22ms
step:216/1770 train_time:20785ms step_avg:96.23ms
step:217/1770 train_time:20881ms step_avg:96.22ms
step:218/1770 train_time:20977ms step_avg:96.23ms
step:219/1770 train_time:21074ms step_avg:96.23ms
step:220/1770 train_time:21171ms step_avg:96.23ms
step:221/1770 train_time:21268ms step_avg:96.23ms
step:222/1770 train_time:21364ms step_avg:96.24ms
step:223/1770 train_time:21461ms step_avg:96.24ms
step:224/1770 train_time:21558ms step_avg:96.24ms
step:225/1770 train_time:21655ms step_avg:96.24ms
step:226/1770 train_time:21752ms step_avg:96.25ms
step:227/1770 train_time:21849ms step_avg:96.25ms
step:228/1770 train_time:21946ms step_avg:96.26ms
step:229/1770 train_time:22043ms step_avg:96.26ms
step:230/1770 train_time:22140ms step_avg:96.26ms
step:231/1770 train_time:22236ms step_avg:96.26ms
step:232/1770 train_time:22333ms step_avg:96.26ms
step:233/1770 train_time:22429ms step_avg:96.26ms
step:234/1770 train_time:22527ms step_avg:96.27ms
step:235/1770 train_time:22624ms step_avg:96.27ms
step:236/1770 train_time:22720ms step_avg:96.27ms
step:237/1770 train_time:22816ms step_avg:96.27ms
step:238/1770 train_time:22913ms step_avg:96.27ms
step:239/1770 train_time:23010ms step_avg:96.28ms
step:240/1770 train_time:23108ms step_avg:96.28ms
step:241/1770 train_time:23205ms step_avg:96.29ms
step:242/1770 train_time:23620ms step_avg:97.60ms
step:243/1770 train_time:23718ms step_avg:97.60ms
step:244/1770 train_time:23814ms step_avg:97.60ms
step:245/1770 train_time:23911ms step_avg:97.59ms
step:246/1770 train_time:24007ms step_avg:97.59ms
step:247/1770 train_time:24104ms step_avg:97.59ms
step:248/1770 train_time:24200ms step_avg:97.58ms
step:249/1770 train_time:24296ms step_avg:97.57ms
step:250/1770 train_time:24393ms step_avg:97.57ms
step:250/1770 val_loss:4.1015 train_time:24484ms step_avg:97.94ms
step:251/1770 train_time:24505ms step_avg:97.63ms
step:252/1770 train_time:24593ms step_avg:97.59ms
step:253/1770 train_time:24692ms step_avg:97.60ms
step:254/1770 train_time:24788ms step_avg:97.59ms
step:255/1770 train_time:24886ms step_avg:97.59ms
step:256/1770 train_time:24982ms step_avg:97.59ms
step:257/1770 train_time:25079ms step_avg:97.58ms
step:258/1770 train_time:25175ms step_avg:97.58ms
step:259/1770 train_time:25272ms step_avg:97.57ms
step:260/1770 train_time:25368ms step_avg:97.57ms
step:261/1770 train_time:25464ms step_avg:97.56ms
step:262/1770 train_time:25561ms step_avg:97.56ms
step:263/1770 train_time:25658ms step_avg:97.56ms
step:264/1770 train_time:25756ms step_avg:97.56ms
step:265/1770 train_time:25853ms step_avg:97.56ms
step:266/1770 train_time:25949ms step_avg:97.55ms
step:267/1770 train_time:26046ms step_avg:97.55ms
step:268/1770 train_time:26144ms step_avg:97.55ms
step:269/1770 train_time:26241ms step_avg:97.55ms
step:270/1770 train_time:26338ms step_avg:97.55ms
step:271/1770 train_time:26436ms step_avg:97.55ms
step:272/1770 train_time:26533ms step_avg:97.55ms
step:273/1770 train_time:26630ms step_avg:97.55ms
step:274/1770 train_time:26727ms step_avg:97.54ms
step:275/1770 train_time:26825ms step_avg:97.54ms
step:276/1770 train_time:26922ms step_avg:97.54ms
step:277/1770 train_time:27019ms step_avg:97.54ms
step:278/1770 train_time:27117ms step_avg:97.54ms
step:279/1770 train_time:27214ms step_avg:97.54ms
step:280/1770 train_time:27311ms step_avg:97.54ms
step:281/1770 train_time:27408ms step_avg:97.54ms
step:282/1770 train_time:27506ms step_avg:97.54ms
step:283/1770 train_time:27603ms step_avg:97.54ms
step:284/1770 train_time:27701ms step_avg:97.54ms
step:285/1770 train_time:27798ms step_avg:97.54ms
step:286/1770 train_time:27895ms step_avg:97.53ms
step:287/1770 train_time:27992ms step_avg:97.53ms
step:288/1770 train_time:28090ms step_avg:97.53ms
step:289/1770 train_time:28187ms step_avg:97.53ms
step:290/1770 train_time:28284ms step_avg:97.53ms
step:291/1770 train_time:28381ms step_avg:97.53ms
step:292/1770 train_time:28478ms step_avg:97.53ms
step:293/1770 train_time:28575ms step_avg:97.53ms
step:294/1770 train_time:28672ms step_avg:97.52ms
step:295/1770 train_time:28769ms step_avg:97.52ms
step:296/1770 train_time:28866ms step_avg:97.52ms
step:297/1770 train_time:28963ms step_avg:97.52ms
step:298/1770 train_time:29061ms step_avg:97.52ms
step:299/1770 train_time:29158ms step_avg:97.52ms
step:300/1770 train_time:29255ms step_avg:97.52ms
step:301/1770 train_time:29353ms step_avg:97.52ms
step:302/1770 train_time:29450ms step_avg:97.52ms
step:303/1770 train_time:29547ms step_avg:97.51ms
step:304/1770 train_time:29644ms step_avg:97.51ms
step:305/1770 train_time:29742ms step_avg:97.51ms
step:306/1770 train_time:29839ms step_avg:97.51ms
step:307/1770 train_time:29937ms step_avg:97.51ms
step:308/1770 train_time:30034ms step_avg:97.51ms
step:309/1770 train_time:30131ms step_avg:97.51ms
step:310/1770 train_time:30228ms step_avg:97.51ms
step:311/1770 train_time:30325ms step_avg:97.51ms
step:312/1770 train_time:30423ms step_avg:97.51ms
step:313/1770 train_time:30520ms step_avg:97.51ms
step:314/1770 train_time:30617ms step_avg:97.51ms
step:315/1770 train_time:30714ms step_avg:97.50ms
step:316/1770 train_time:30811ms step_avg:97.50ms
step:317/1770 train_time:30907ms step_avg:97.50ms
step:318/1770 train_time:31004ms step_avg:97.50ms
step:319/1770 train_time:31102ms step_avg:97.50ms
step:320/1770 train_time:31199ms step_avg:97.50ms
step:321/1770 train_time:31297ms step_avg:97.50ms
step:322/1770 train_time:31394ms step_avg:97.50ms
step:323/1770 train_time:31491ms step_avg:97.49ms
step:324/1770 train_time:31588ms step_avg:97.49ms
step:325/1770 train_time:31685ms step_avg:97.49ms
step:326/1770 train_time:31783ms step_avg:97.49ms
step:327/1770 train_time:31879ms step_avg:97.49ms
step:328/1770 train_time:31976ms step_avg:97.49ms
step:329/1770 train_time:32074ms step_avg:97.49ms
step:330/1770 train_time:32171ms step_avg:97.49ms
step:331/1770 train_time:32267ms step_avg:97.48ms
step:332/1770 train_time:32365ms step_avg:97.49ms
step:333/1770 train_time:32464ms step_avg:97.49ms
step:334/1770 train_time:32561ms step_avg:97.49ms
step:335/1770 train_time:32658ms step_avg:97.49ms
step:336/1770 train_time:32755ms step_avg:97.49ms
step:337/1770 train_time:32852ms step_avg:97.48ms
step:338/1770 train_time:32949ms step_avg:97.48ms
step:339/1770 train_time:33046ms step_avg:97.48ms
step:340/1770 train_time:33144ms step_avg:97.48ms
step:341/1770 train_time:33241ms step_avg:97.48ms
step:342/1770 train_time:33338ms step_avg:97.48ms
step:343/1770 train_time:33436ms step_avg:97.48ms
step:344/1770 train_time:33533ms step_avg:97.48ms
step:345/1770 train_time:33630ms step_avg:97.48ms
step:346/1770 train_time:33726ms step_avg:97.48ms
step:347/1770 train_time:33824ms step_avg:97.48ms
step:348/1770 train_time:33921ms step_avg:97.48ms
step:349/1770 train_time:34019ms step_avg:97.47ms
step:350/1770 train_time:34116ms step_avg:97.48ms
step:351/1770 train_time:34213ms step_avg:97.47ms
step:352/1770 train_time:34310ms step_avg:97.47ms
step:353/1770 train_time:34408ms step_avg:97.47ms
step:354/1770 train_time:34506ms step_avg:97.48ms
step:355/1770 train_time:34603ms step_avg:97.47ms
step:356/1770 train_time:34700ms step_avg:97.47ms
step:357/1770 train_time:34797ms step_avg:97.47ms
step:358/1770 train_time:34894ms step_avg:97.47ms
step:359/1770 train_time:34991ms step_avg:97.47ms
step:360/1770 train_time:35088ms step_avg:97.47ms
step:361/1770 train_time:35185ms step_avg:97.47ms
step:362/1770 train_time:35283ms step_avg:97.47ms
step:363/1770 train_time:35380ms step_avg:97.47ms
step:364/1770 train_time:35478ms step_avg:97.47ms
step:365/1770 train_time:35575ms step_avg:97.47ms
step:366/1770 train_time:35672ms step_avg:97.46ms
step:367/1770 train_time:35769ms step_avg:97.46ms
step:368/1770 train_time:35866ms step_avg:97.46ms
step:369/1770 train_time:35964ms step_avg:97.46ms
step:370/1770 train_time:36061ms step_avg:97.46ms
step:371/1770 train_time:36158ms step_avg:97.46ms
step:372/1770 train_time:36255ms step_avg:97.46ms
step:373/1770 train_time:36352ms step_avg:97.46ms
step:374/1770 train_time:36449ms step_avg:97.46ms
step:375/1770 train_time:36546ms step_avg:97.46ms
step:375/1770 val_loss:3.8998 train_time:36638ms step_avg:97.70ms
step:376/1770 train_time:36660ms step_avg:97.50ms
step:377/1770 train_time:36747ms step_avg:97.47ms
step:378/1770 train_time:36845ms step_avg:97.47ms
step:379/1770 train_time:36944ms step_avg:97.48ms
step:380/1770 train_time:37041ms step_avg:97.48ms
step:381/1770 train_time:37138ms step_avg:97.48ms
step:382/1770 train_time:37235ms step_avg:97.47ms
step:383/1770 train_time:37332ms step_avg:97.47ms
step:384/1770 train_time:37430ms step_avg:97.47ms
step:385/1770 train_time:37528ms step_avg:97.47ms
step:386/1770 train_time:37625ms step_avg:97.47ms
step:387/1770 train_time:37722ms step_avg:97.47ms
step:388/1770 train_time:37819ms step_avg:97.47ms
step:389/1770 train_time:37917ms step_avg:97.47ms
step:390/1770 train_time:38013ms step_avg:97.47ms
step:391/1770 train_time:38111ms step_avg:97.47ms
step:392/1770 train_time:38208ms step_avg:97.47ms
step:393/1770 train_time:38305ms step_avg:97.47ms
step:394/1770 train_time:38402ms step_avg:97.47ms
step:395/1770 train_time:38500ms step_avg:97.47ms
step:396/1770 train_time:38599ms step_avg:97.47ms
step:397/1770 train_time:38698ms step_avg:97.48ms
step:398/1770 train_time:38796ms step_avg:97.48ms
step:399/1770 train_time:38895ms step_avg:97.48ms
step:400/1770 train_time:38994ms step_avg:97.48ms
step:401/1770 train_time:39092ms step_avg:97.49ms
step:402/1770 train_time:39191ms step_avg:97.49ms
step:403/1770 train_time:39291ms step_avg:97.50ms
step:404/1770 train_time:39390ms step_avg:97.50ms
step:405/1770 train_time:39489ms step_avg:97.50ms
step:406/1770 train_time:39588ms step_avg:97.51ms
step:407/1770 train_time:39690ms step_avg:97.52ms
step:408/1770 train_time:39793ms step_avg:97.53ms
step:409/1770 train_time:39890ms step_avg:97.53ms
step:410/1770 train_time:39990ms step_avg:97.54ms
step:411/1770 train_time:40089ms step_avg:97.54ms
step:412/1770 train_time:40189ms step_avg:97.55ms
step:413/1770 train_time:40288ms step_avg:97.55ms
step:414/1770 train_time:40386ms step_avg:97.55ms
step:415/1770 train_time:40486ms step_avg:97.56ms
step:416/1770 train_time:40585ms step_avg:97.56ms
step:417/1770 train_time:40684ms step_avg:97.56ms
step:418/1770 train_time:40784ms step_avg:97.57ms
step:419/1770 train_time:40883ms step_avg:97.57ms
step:420/1770 train_time:40984ms step_avg:97.58ms
step:421/1770 train_time:41083ms step_avg:97.58ms
step:422/1770 train_time:41183ms step_avg:97.59ms
step:423/1770 train_time:41282ms step_avg:97.59ms
step:424/1770 train_time:41381ms step_avg:97.60ms
step:425/1770 train_time:41480ms step_avg:97.60ms
step:426/1770 train_time:41579ms step_avg:97.60ms
step:427/1770 train_time:41678ms step_avg:97.61ms
step:428/1770 train_time:41778ms step_avg:97.61ms
step:429/1770 train_time:41877ms step_avg:97.62ms
step:430/1770 train_time:41976ms step_avg:97.62ms
step:431/1770 train_time:42076ms step_avg:97.62ms
step:432/1770 train_time:42175ms step_avg:97.63ms
step:433/1770 train_time:42274ms step_avg:97.63ms
step:434/1770 train_time:42373ms step_avg:97.63ms
step:435/1770 train_time:42472ms step_avg:97.64ms
step:436/1770 train_time:42571ms step_avg:97.64ms
step:437/1770 train_time:42671ms step_avg:97.64ms
step:438/1770 train_time:42771ms step_avg:97.65ms
step:439/1770 train_time:42870ms step_avg:97.65ms
step:440/1770 train_time:42971ms step_avg:97.66ms
step:441/1770 train_time:43071ms step_avg:97.67ms
step:442/1770 train_time:43172ms step_avg:97.67ms
step:443/1770 train_time:43273ms step_avg:97.68ms
step:444/1770 train_time:43373ms step_avg:97.69ms
step:445/1770 train_time:43472ms step_avg:97.69ms
step:446/1770 train_time:43572ms step_avg:97.69ms
step:447/1770 train_time:43673ms step_avg:97.70ms
step:448/1770 train_time:43771ms step_avg:97.70ms
step:449/1770 train_time:43870ms step_avg:97.71ms
step:450/1770 train_time:43970ms step_avg:97.71ms
step:451/1770 train_time:44070ms step_avg:97.72ms
step:452/1770 train_time:44171ms step_avg:97.72ms
step:453/1770 train_time:44271ms step_avg:97.73ms
step:454/1770 train_time:44371ms step_avg:97.73ms
step:455/1770 train_time:44471ms step_avg:97.74ms
step:456/1770 train_time:44571ms step_avg:97.74ms
step:457/1770 train_time:44671ms step_avg:97.75ms
step:458/1770 train_time:44772ms step_avg:97.75ms
step:459/1770 train_time:44872ms step_avg:97.76ms
step:460/1770 train_time:44970ms step_avg:97.76ms
step:461/1770 train_time:45070ms step_avg:97.77ms
step:462/1770 train_time:45170ms step_avg:97.77ms
step:463/1770 train_time:45270ms step_avg:97.77ms
step:464/1770 train_time:45369ms step_avg:97.78ms
step:465/1770 train_time:45469ms step_avg:97.78ms
step:466/1770 train_time:45568ms step_avg:97.79ms
step:467/1770 train_time:45668ms step_avg:97.79ms
step:468/1770 train_time:45767ms step_avg:97.79ms
step:469/1770 train_time:45866ms step_avg:97.79ms
step:470/1770 train_time:45965ms step_avg:97.80ms
step:471/1770 train_time:46065ms step_avg:97.80ms
step:472/1770 train_time:46165ms step_avg:97.81ms
step:473/1770 train_time:46264ms step_avg:97.81ms
step:474/1770 train_time:46363ms step_avg:97.81ms
step:475/1770 train_time:46463ms step_avg:97.82ms
step:476/1770 train_time:46562ms step_avg:97.82ms
step:477/1770 train_time:46661ms step_avg:97.82ms
step:478/1770 train_time:46759ms step_avg:97.82ms
step:479/1770 train_time:46860ms step_avg:97.83ms
step:480/1770 train_time:46958ms step_avg:97.83ms
step:481/1770 train_time:47058ms step_avg:97.83ms
step:482/1770 train_time:47158ms step_avg:97.84ms
step:483/1770 train_time:47258ms step_avg:97.84ms
step:484/1770 train_time:47358ms step_avg:97.85ms
step:485/1770 train_time:47458ms step_avg:97.85ms
step:486/1770 train_time:47558ms step_avg:97.86ms
step:487/1770 train_time:47657ms step_avg:97.86ms
step:488/1770 train_time:47755ms step_avg:97.86ms
step:489/1770 train_time:47855ms step_avg:97.86ms
step:490/1770 train_time:47953ms step_avg:97.86ms
step:491/1770 train_time:48052ms step_avg:97.87ms
step:492/1770 train_time:48152ms step_avg:97.87ms
step:493/1770 train_time:48251ms step_avg:97.87ms
step:494/1770 train_time:48351ms step_avg:97.88ms
step:495/1770 train_time:48450ms step_avg:97.88ms
step:496/1770 train_time:48550ms step_avg:97.88ms
step:497/1770 train_time:48650ms step_avg:97.89ms
step:498/1770 train_time:48751ms step_avg:97.89ms
step:499/1770 train_time:48850ms step_avg:97.90ms
step:500/1770 train_time:48950ms step_avg:97.90ms
step:500/1770 val_loss:3.7488 train_time:49044ms step_avg:98.09ms
step:501/1770 train_time:49064ms step_avg:97.93ms
step:502/1770 train_time:49157ms step_avg:97.92ms
step:503/1770 train_time:49257ms step_avg:97.93ms
step:504/1770 train_time:49357ms step_avg:97.93ms
step:505/1770 train_time:49456ms step_avg:97.93ms
step:506/1770 train_time:49555ms step_avg:97.93ms
step:507/1770 train_time:49654ms step_avg:97.94ms
step:508/1770 train_time:49754ms step_avg:97.94ms
step:509/1770 train_time:49853ms step_avg:97.94ms
step:510/1770 train_time:49951ms step_avg:97.94ms
step:511/1770 train_time:50050ms step_avg:97.94ms
step:512/1770 train_time:50149ms step_avg:97.95ms
step:513/1770 train_time:50248ms step_avg:97.95ms
step:514/1770 train_time:50348ms step_avg:97.95ms
step:515/1770 train_time:50447ms step_avg:97.96ms
step:516/1770 train_time:50545ms step_avg:97.96ms
step:517/1770 train_time:50645ms step_avg:97.96ms
step:518/1770 train_time:50744ms step_avg:97.96ms
step:519/1770 train_time:50844ms step_avg:97.97ms
step:520/1770 train_time:50944ms step_avg:97.97ms
step:521/1770 train_time:51043ms step_avg:97.97ms
step:522/1770 train_time:51144ms step_avg:97.98ms
step:523/1770 train_time:51245ms step_avg:97.98ms
step:524/1770 train_time:51345ms step_avg:97.99ms
step:525/1770 train_time:51445ms step_avg:97.99ms
step:526/1770 train_time:51545ms step_avg:97.99ms
step:527/1770 train_time:51646ms step_avg:98.00ms
step:528/1770 train_time:51745ms step_avg:98.00ms
step:529/1770 train_time:51845ms step_avg:98.01ms
step:530/1770 train_time:51945ms step_avg:98.01ms
step:531/1770 train_time:52045ms step_avg:98.01ms
step:532/1770 train_time:52147ms step_avg:98.02ms
step:533/1770 train_time:52246ms step_avg:98.02ms
step:534/1770 train_time:52346ms step_avg:98.03ms
step:535/1770 train_time:52446ms step_avg:98.03ms
step:536/1770 train_time:52546ms step_avg:98.03ms
step:537/1770 train_time:52646ms step_avg:98.04ms
step:538/1770 train_time:52746ms step_avg:98.04ms
step:539/1770 train_time:52846ms step_avg:98.04ms
step:540/1770 train_time:52946ms step_avg:98.05ms
step:541/1770 train_time:53045ms step_avg:98.05ms
step:542/1770 train_time:53145ms step_avg:98.05ms
step:543/1770 train_time:53245ms step_avg:98.06ms
step:544/1770 train_time:53345ms step_avg:98.06ms
step:545/1770 train_time:53445ms step_avg:98.07ms
step:546/1770 train_time:53545ms step_avg:98.07ms
step:547/1770 train_time:53646ms step_avg:98.07ms
step:548/1770 train_time:53745ms step_avg:98.08ms
step:549/1770 train_time:53845ms step_avg:98.08ms
step:550/1770 train_time:53946ms step_avg:98.08ms
step:551/1770 train_time:54045ms step_avg:98.09ms
step:552/1770 train_time:54145ms step_avg:98.09ms
step:553/1770 train_time:54245ms step_avg:98.09ms
step:554/1770 train_time:54345ms step_avg:98.10ms
step:555/1770 train_time:54445ms step_avg:98.10ms
step:556/1770 train_time:54544ms step_avg:98.10ms
step:557/1770 train_time:54644ms step_avg:98.10ms
step:558/1770 train_time:54745ms step_avg:98.11ms
step:559/1770 train_time:54845ms step_avg:98.11ms
step:560/1770 train_time:54945ms step_avg:98.12ms
step:561/1770 train_time:55045ms step_avg:98.12ms
step:562/1770 train_time:55145ms step_avg:98.12ms
step:563/1770 train_time:55246ms step_avg:98.13ms
step:564/1770 train_time:55346ms step_avg:98.13ms
step:565/1770 train_time:55446ms step_avg:98.13ms
step:566/1770 train_time:55545ms step_avg:98.14ms
step:567/1770 train_time:55645ms step_avg:98.14ms
step:568/1770 train_time:55746ms step_avg:98.14ms
step:569/1770 train_time:55845ms step_avg:98.15ms
step:570/1770 train_time:55945ms step_avg:98.15ms
step:571/1770 train_time:56045ms step_avg:98.15ms
step:572/1770 train_time:56146ms step_avg:98.16ms
step:573/1770 train_time:56245ms step_avg:98.16ms
step:574/1770 train_time:56345ms step_avg:98.16ms
step:575/1770 train_time:56446ms step_avg:98.17ms
step:576/1770 train_time:56546ms step_avg:98.17ms
step:577/1770 train_time:56645ms step_avg:98.17ms
step:578/1770 train_time:56746ms step_avg:98.18ms
step:579/1770 train_time:56846ms step_avg:98.18ms
step:580/1770 train_time:56947ms step_avg:98.18ms
step:581/1770 train_time:57046ms step_avg:98.19ms
step:582/1770 train_time:57146ms step_avg:98.19ms
step:583/1770 train_time:57246ms step_avg:98.19ms
step:584/1770 train_time:57346ms step_avg:98.19ms
step:585/1770 train_time:57446ms step_avg:98.20ms
step:586/1770 train_time:57546ms step_avg:98.20ms
step:587/1770 train_time:57646ms step_avg:98.20ms
step:588/1770 train_time:57747ms step_avg:98.21ms
step:589/1770 train_time:57847ms step_avg:98.21ms
step:590/1770 train_time:57946ms step_avg:98.21ms
step:591/1770 train_time:58046ms step_avg:98.22ms
step:592/1770 train_time:58147ms step_avg:98.22ms
step:593/1770 train_time:58247ms step_avg:98.22ms
step:594/1770 train_time:58346ms step_avg:98.23ms
step:595/1770 train_time:58445ms step_avg:98.23ms
step:596/1770 train_time:58546ms step_avg:98.23ms
step:597/1770 train_time:58646ms step_avg:98.23ms
step:598/1770 train_time:58745ms step_avg:98.24ms
step:599/1770 train_time:58845ms step_avg:98.24ms
step:600/1770 train_time:58945ms step_avg:98.24ms
step:601/1770 train_time:59045ms step_avg:98.25ms
step:602/1770 train_time:59145ms step_avg:98.25ms
step:603/1770 train_time:59245ms step_avg:98.25ms
step:604/1770 train_time:59346ms step_avg:98.25ms
step:605/1770 train_time:59445ms step_avg:98.26ms
step:606/1770 train_time:59545ms step_avg:98.26ms
step:607/1770 train_time:59645ms step_avg:98.26ms
step:608/1770 train_time:59745ms step_avg:98.27ms
step:609/1770 train_time:59846ms step_avg:98.27ms
step:610/1770 train_time:59945ms step_avg:98.27ms
step:611/1770 train_time:60045ms step_avg:98.27ms
step:612/1770 train_time:60146ms step_avg:98.28ms
step:613/1770 train_time:60246ms step_avg:98.28ms
step:614/1770 train_time:60346ms step_avg:98.28ms
step:615/1770 train_time:60445ms step_avg:98.29ms
step:616/1770 train_time:60545ms step_avg:98.29ms
step:617/1770 train_time:60645ms step_avg:98.29ms
step:618/1770 train_time:60745ms step_avg:98.29ms
step:619/1770 train_time:60846ms step_avg:98.30ms
step:620/1770 train_time:60946ms step_avg:98.30ms
step:621/1770 train_time:61046ms step_avg:98.30ms
step:622/1770 train_time:61146ms step_avg:98.31ms
step:623/1770 train_time:61246ms step_avg:98.31ms
step:624/1770 train_time:61346ms step_avg:98.31ms
step:625/1770 train_time:61446ms step_avg:98.31ms
step:625/1770 val_loss:3.6620 train_time:61540ms step_avg:98.46ms
step:626/1770 train_time:61560ms step_avg:98.34ms
step:627/1770 train_time:61652ms step_avg:98.33ms
step:628/1770 train_time:61753ms step_avg:98.33ms
step:629/1770 train_time:61853ms step_avg:98.34ms
step:630/1770 train_time:61953ms step_avg:98.34ms
step:631/1770 train_time:62053ms step_avg:98.34ms
step:632/1770 train_time:62152ms step_avg:98.34ms
step:633/1770 train_time:62252ms step_avg:98.34ms
step:634/1770 train_time:62352ms step_avg:98.35ms
step:635/1770 train_time:62451ms step_avg:98.35ms
step:636/1770 train_time:62551ms step_avg:98.35ms
step:637/1770 train_time:62651ms step_avg:98.35ms
step:638/1770 train_time:62751ms step_avg:98.36ms
step:639/1770 train_time:62851ms step_avg:98.36ms
step:640/1770 train_time:62951ms step_avg:98.36ms
step:641/1770 train_time:63051ms step_avg:98.36ms
step:642/1770 train_time:63152ms step_avg:98.37ms
step:643/1770 train_time:63252ms step_avg:98.37ms
step:644/1770 train_time:63353ms step_avg:98.37ms
step:645/1770 train_time:63453ms step_avg:98.38ms
step:646/1770 train_time:63553ms step_avg:98.38ms
step:647/1770 train_time:63653ms step_avg:98.38ms
step:648/1770 train_time:63752ms step_avg:98.38ms
step:649/1770 train_time:63852ms step_avg:98.39ms
step:650/1770 train_time:63952ms step_avg:98.39ms
step:651/1770 train_time:64052ms step_avg:98.39ms
step:652/1770 train_time:64152ms step_avg:98.39ms
step:653/1770 train_time:64251ms step_avg:98.39ms
step:654/1770 train_time:64351ms step_avg:98.40ms
step:655/1770 train_time:64452ms step_avg:98.40ms
step:656/1770 train_time:64551ms step_avg:98.40ms
step:657/1770 train_time:64652ms step_avg:98.40ms
step:658/1770 train_time:64753ms step_avg:98.41ms
step:659/1770 train_time:64854ms step_avg:98.41ms
step:660/1770 train_time:64955ms step_avg:98.42ms
step:661/1770 train_time:65056ms step_avg:98.42ms
step:662/1770 train_time:65156ms step_avg:98.42ms
step:663/1770 train_time:65257ms step_avg:98.43ms
step:664/1770 train_time:65357ms step_avg:98.43ms
step:665/1770 train_time:65458ms step_avg:98.43ms
step:666/1770 train_time:65560ms step_avg:98.44ms
step:667/1770 train_time:65661ms step_avg:98.44ms
step:668/1770 train_time:65762ms step_avg:98.45ms
step:669/1770 train_time:65865ms step_avg:98.45ms
step:670/1770 train_time:65967ms step_avg:98.46ms
step:671/1770 train_time:66070ms step_avg:98.46ms
step:672/1770 train_time:66171ms step_avg:98.47ms
step:673/1770 train_time:66272ms step_avg:98.47ms
step:674/1770 train_time:66374ms step_avg:98.48ms
step:675/1770 train_time:66475ms step_avg:98.48ms
step:676/1770 train_time:66576ms step_avg:98.48ms
step:677/1770 train_time:66676ms step_avg:98.49ms
step:678/1770 train_time:66777ms step_avg:98.49ms
step:679/1770 train_time:66878ms step_avg:98.49ms
step:680/1770 train_time:66979ms step_avg:98.50ms
step:681/1770 train_time:67080ms step_avg:98.50ms
step:682/1770 train_time:67182ms step_avg:98.51ms
step:683/1770 train_time:67282ms step_avg:98.51ms
step:684/1770 train_time:67384ms step_avg:98.51ms
step:685/1770 train_time:67485ms step_avg:98.52ms
step:686/1770 train_time:67586ms step_avg:98.52ms
step:687/1770 train_time:67688ms step_avg:98.53ms
step:688/1770 train_time:67789ms step_avg:98.53ms
step:689/1770 train_time:67891ms step_avg:98.54ms
step:690/1770 train_time:67993ms step_avg:98.54ms
step:691/1770 train_time:68094ms step_avg:98.54ms
step:692/1770 train_time:68196ms step_avg:98.55ms
step:693/1770 train_time:68297ms step_avg:98.55ms
step:694/1770 train_time:68398ms step_avg:98.56ms
step:695/1770 train_time:68499ms step_avg:98.56ms
step:696/1770 train_time:68600ms step_avg:98.56ms
step:697/1770 train_time:68701ms step_avg:98.57ms
step:698/1770 train_time:68802ms step_avg:98.57ms
step:699/1770 train_time:68903ms step_avg:98.57ms
step:700/1770 train_time:69004ms step_avg:98.58ms
step:701/1770 train_time:69107ms step_avg:98.58ms
step:702/1770 train_time:69208ms step_avg:98.59ms
step:703/1770 train_time:69311ms step_avg:98.59ms
step:704/1770 train_time:69413ms step_avg:98.60ms
step:705/1770 train_time:69515ms step_avg:98.60ms
step:706/1770 train_time:69616ms step_avg:98.61ms
step:707/1770 train_time:69717ms step_avg:98.61ms
step:708/1770 train_time:69818ms step_avg:98.61ms
step:709/1770 train_time:69919ms step_avg:98.62ms
step:710/1770 train_time:70021ms step_avg:98.62ms
step:711/1770 train_time:70121ms step_avg:98.62ms
step:712/1770 train_time:70222ms step_avg:98.63ms
step:713/1770 train_time:70323ms step_avg:98.63ms
step:714/1770 train_time:70426ms step_avg:98.64ms
step:715/1770 train_time:70528ms step_avg:98.64ms
step:716/1770 train_time:70629ms step_avg:98.64ms
step:717/1770 train_time:70731ms step_avg:98.65ms
step:718/1770 train_time:70832ms step_avg:98.65ms
step:719/1770 train_time:70934ms step_avg:98.66ms
step:720/1770 train_time:71035ms step_avg:98.66ms
step:721/1770 train_time:71136ms step_avg:98.66ms
step:722/1770 train_time:71237ms step_avg:98.67ms
step:723/1770 train_time:71339ms step_avg:98.67ms
step:724/1770 train_time:71438ms step_avg:98.67ms
step:725/1770 train_time:71539ms step_avg:98.67ms
step:726/1770 train_time:71639ms step_avg:98.68ms
step:727/1770 train_time:71740ms step_avg:98.68ms
step:728/1770 train_time:71842ms step_avg:98.68ms
step:729/1770 train_time:71943ms step_avg:98.69ms
step:730/1770 train_time:72044ms step_avg:98.69ms
step:731/1770 train_time:72147ms step_avg:98.70ms
step:732/1770 train_time:72249ms step_avg:98.70ms
step:733/1770 train_time:72350ms step_avg:98.70ms
step:734/1770 train_time:72452ms step_avg:98.71ms
step:735/1770 train_time:72553ms step_avg:98.71ms
step:736/1770 train_time:72654ms step_avg:98.71ms
step:737/1770 train_time:72755ms step_avg:98.72ms
step:738/1770 train_time:72856ms step_avg:98.72ms
step:739/1770 train_time:72957ms step_avg:98.72ms
step:740/1770 train_time:73059ms step_avg:98.73ms
step:741/1770 train_time:73161ms step_avg:98.73ms
step:742/1770 train_time:73262ms step_avg:98.74ms
step:743/1770 train_time:73363ms step_avg:98.74ms
step:744/1770 train_time:73464ms step_avg:98.74ms
step:745/1770 train_time:73564ms step_avg:98.74ms
step:746/1770 train_time:73666ms step_avg:98.75ms
step:747/1770 train_time:73768ms step_avg:98.75ms
step:748/1770 train_time:73870ms step_avg:98.76ms
step:749/1770 train_time:73971ms step_avg:98.76ms
step:750/1770 train_time:74073ms step_avg:98.76ms
step:750/1770 val_loss:3.5979 train_time:74168ms step_avg:98.89ms
step:751/1770 train_time:74188ms step_avg:98.79ms
step:752/1770 train_time:74280ms step_avg:98.78ms
step:753/1770 train_time:74381ms step_avg:98.78ms
step:754/1770 train_time:74483ms step_avg:98.78ms
step:755/1770 train_time:74584ms step_avg:98.79ms
step:756/1770 train_time:74685ms step_avg:98.79ms
step:757/1770 train_time:74786ms step_avg:98.79ms
step:758/1770 train_time:74886ms step_avg:98.79ms
step:759/1770 train_time:74986ms step_avg:98.80ms
step:760/1770 train_time:75087ms step_avg:98.80ms
step:761/1770 train_time:75187ms step_avg:98.80ms
step:762/1770 train_time:75289ms step_avg:98.80ms
step:763/1770 train_time:75392ms step_avg:98.81ms
step:764/1770 train_time:75493ms step_avg:98.81ms
step:765/1770 train_time:75593ms step_avg:98.82ms
step:766/1770 train_time:75695ms step_avg:98.82ms
step:767/1770 train_time:75797ms step_avg:98.82ms
step:768/1770 train_time:75899ms step_avg:98.83ms
step:769/1770 train_time:76000ms step_avg:98.83ms
step:770/1770 train_time:76101ms step_avg:98.83ms
step:771/1770 train_time:76203ms step_avg:98.84ms
step:772/1770 train_time:76304ms step_avg:98.84ms
step:773/1770 train_time:76406ms step_avg:98.84ms
step:774/1770 train_time:76507ms step_avg:98.85ms
step:775/1770 train_time:76608ms step_avg:98.85ms
step:776/1770 train_time:76709ms step_avg:98.85ms
step:777/1770 train_time:76810ms step_avg:98.85ms
step:778/1770 train_time:76911ms step_avg:98.86ms
step:779/1770 train_time:77013ms step_avg:98.86ms
step:780/1770 train_time:77113ms step_avg:98.86ms
step:781/1770 train_time:77215ms step_avg:98.87ms
step:782/1770 train_time:77317ms step_avg:98.87ms
step:783/1770 train_time:77419ms step_avg:98.87ms
step:784/1770 train_time:77521ms step_avg:98.88ms
step:785/1770 train_time:77622ms step_avg:98.88ms
step:786/1770 train_time:77723ms step_avg:98.88ms
step:787/1770 train_time:77825ms step_avg:98.89ms
step:788/1770 train_time:77926ms step_avg:98.89ms
step:789/1770 train_time:78028ms step_avg:98.89ms
step:790/1770 train_time:78129ms step_avg:98.90ms
step:791/1770 train_time:78231ms step_avg:98.90ms
step:792/1770 train_time:78332ms step_avg:98.90ms
step:793/1770 train_time:78433ms step_avg:98.91ms
step:794/1770 train_time:78534ms step_avg:98.91ms
step:795/1770 train_time:78637ms step_avg:98.91ms
step:796/1770 train_time:78739ms step_avg:98.92ms
step:797/1770 train_time:78840ms step_avg:98.92ms
step:798/1770 train_time:78942ms step_avg:98.93ms
step:799/1770 train_time:79044ms step_avg:98.93ms
step:800/1770 train_time:79146ms step_avg:98.93ms
step:801/1770 train_time:79247ms step_avg:98.94ms
step:802/1770 train_time:79349ms step_avg:98.94ms
step:803/1770 train_time:79450ms step_avg:98.94ms
step:804/1770 train_time:79551ms step_avg:98.94ms
step:805/1770 train_time:79652ms step_avg:98.95ms
step:806/1770 train_time:79753ms step_avg:98.95ms
step:807/1770 train_time:79855ms step_avg:98.95ms
step:808/1770 train_time:79958ms step_avg:98.96ms
step:809/1770 train_time:80059ms step_avg:98.96ms
step:810/1770 train_time:80161ms step_avg:98.96ms
step:811/1770 train_time:80262ms step_avg:98.97ms
step:812/1770 train_time:80364ms step_avg:98.97ms
step:813/1770 train_time:80466ms step_avg:98.97ms
step:814/1770 train_time:80567ms step_avg:98.98ms
step:815/1770 train_time:80668ms step_avg:98.98ms
step:816/1770 train_time:80769ms step_avg:98.98ms
step:817/1770 train_time:80870ms step_avg:98.98ms
step:818/1770 train_time:80972ms step_avg:98.99ms
step:819/1770 train_time:81074ms step_avg:98.99ms
step:820/1770 train_time:81177ms step_avg:99.00ms
step:821/1770 train_time:81278ms step_avg:99.00ms
step:822/1770 train_time:81380ms step_avg:99.00ms
step:823/1770 train_time:81481ms step_avg:99.00ms
step:824/1770 train_time:81583ms step_avg:99.01ms
step:825/1770 train_time:81685ms step_avg:99.01ms
step:826/1770 train_time:81786ms step_avg:99.01ms
step:827/1770 train_time:81888ms step_avg:99.02ms
step:828/1770 train_time:81989ms step_avg:99.02ms
step:829/1770 train_time:82090ms step_avg:99.02ms
step:830/1770 train_time:82192ms step_avg:99.03ms
step:831/1770 train_time:82293ms step_avg:99.03ms
step:832/1770 train_time:82395ms step_avg:99.03ms
step:833/1770 train_time:82497ms step_avg:99.04ms
step:834/1770 train_time:82599ms step_avg:99.04ms
step:835/1770 train_time:82701ms step_avg:99.04ms
step:836/1770 train_time:82803ms step_avg:99.05ms
step:837/1770 train_time:82905ms step_avg:99.05ms
step:838/1770 train_time:83006ms step_avg:99.05ms
step:839/1770 train_time:83107ms step_avg:99.05ms
step:840/1770 train_time:83209ms step_avg:99.06ms
step:841/1770 train_time:83310ms step_avg:99.06ms
step:842/1770 train_time:83411ms step_avg:99.06ms
step:843/1770 train_time:83513ms step_avg:99.07ms
step:844/1770 train_time:83614ms step_avg:99.07ms
step:845/1770 train_time:83716ms step_avg:99.07ms
step:846/1770 train_time:83817ms step_avg:99.07ms
step:847/1770 train_time:83919ms step_avg:99.08ms
step:848/1770 train_time:84021ms step_avg:99.08ms
step:849/1770 train_time:84122ms step_avg:99.08ms
step:850/1770 train_time:84224ms step_avg:99.09ms
step:851/1770 train_time:84326ms step_avg:99.09ms
step:852/1770 train_time:84427ms step_avg:99.09ms
step:853/1770 train_time:84529ms step_avg:99.10ms
step:854/1770 train_time:84630ms step_avg:99.10ms
step:855/1770 train_time:84731ms step_avg:99.10ms
step:856/1770 train_time:84833ms step_avg:99.10ms
step:857/1770 train_time:84934ms step_avg:99.11ms
step:858/1770 train_time:85036ms step_avg:99.11ms
step:859/1770 train_time:85138ms step_avg:99.11ms
step:860/1770 train_time:85240ms step_avg:99.12ms
step:861/1770 train_time:85342ms step_avg:99.12ms
step:862/1770 train_time:85444ms step_avg:99.12ms
step:863/1770 train_time:85545ms step_avg:99.13ms
step:864/1770 train_time:85646ms step_avg:99.13ms
step:865/1770 train_time:85747ms step_avg:99.13ms
step:866/1770 train_time:85849ms step_avg:99.13ms
step:867/1770 train_time:85951ms step_avg:99.14ms
step:868/1770 train_time:86052ms step_avg:99.14ms
step:869/1770 train_time:86153ms step_avg:99.14ms
step:870/1770 train_time:86256ms step_avg:99.14ms
step:871/1770 train_time:86357ms step_avg:99.15ms
step:872/1770 train_time:86459ms step_avg:99.15ms
step:873/1770 train_time:86562ms step_avg:99.15ms
step:874/1770 train_time:86663ms step_avg:99.16ms
step:875/1770 train_time:86765ms step_avg:99.16ms
step:875/1770 val_loss:3.5489 train_time:86860ms step_avg:99.27ms
step:876/1770 train_time:86881ms step_avg:99.18ms
step:877/1770 train_time:86976ms step_avg:99.17ms
step:878/1770 train_time:87077ms step_avg:99.18ms
step:879/1770 train_time:87179ms step_avg:99.18ms
step:880/1770 train_time:87280ms step_avg:99.18ms
step:881/1770 train_time:87380ms step_avg:99.18ms
step:882/1770 train_time:87481ms step_avg:99.18ms
step:883/1770 train_time:87581ms step_avg:99.19ms
step:884/1770 train_time:87682ms step_avg:99.19ms
step:885/1770 train_time:87783ms step_avg:99.19ms
step:886/1770 train_time:87884ms step_avg:99.19ms
step:887/1770 train_time:87987ms step_avg:99.20ms
step:888/1770 train_time:88090ms step_avg:99.20ms
step:889/1770 train_time:88192ms step_avg:99.20ms
step:890/1770 train_time:88293ms step_avg:99.21ms
step:891/1770 train_time:88395ms step_avg:99.21ms
step:892/1770 train_time:88496ms step_avg:99.21ms
step:893/1770 train_time:88597ms step_avg:99.21ms
step:894/1770 train_time:88698ms step_avg:99.22ms
step:895/1770 train_time:88799ms step_avg:99.22ms
step:896/1770 train_time:88901ms step_avg:99.22ms
step:897/1770 train_time:89003ms step_avg:99.22ms
step:898/1770 train_time:89105ms step_avg:99.23ms
step:899/1770 train_time:89206ms step_avg:99.23ms
step:900/1770 train_time:89307ms step_avg:99.23ms
step:901/1770 train_time:89409ms step_avg:99.23ms
step:902/1770 train_time:89511ms step_avg:99.24ms
step:903/1770 train_time:89613ms step_avg:99.24ms
step:904/1770 train_time:89714ms step_avg:99.24ms
step:905/1770 train_time:89815ms step_avg:99.24ms
step:906/1770 train_time:89916ms step_avg:99.24ms
step:907/1770 train_time:90017ms step_avg:99.25ms
step:908/1770 train_time:90118ms step_avg:99.25ms
step:909/1770 train_time:90219ms step_avg:99.25ms
step:910/1770 train_time:90321ms step_avg:99.25ms
step:911/1770 train_time:90422ms step_avg:99.26ms
step:912/1770 train_time:90524ms step_avg:99.26ms
step:913/1770 train_time:90626ms step_avg:99.26ms
step:914/1770 train_time:90727ms step_avg:99.26ms
step:915/1770 train_time:90829ms step_avg:99.27ms
step:916/1770 train_time:90931ms step_avg:99.27ms
step:917/1770 train_time:91032ms step_avg:99.27ms
step:918/1770 train_time:91134ms step_avg:99.27ms
step:919/1770 train_time:91234ms step_avg:99.27ms
step:920/1770 train_time:91337ms step_avg:99.28ms
step:921/1770 train_time:91440ms step_avg:99.28ms
step:922/1770 train_time:91543ms step_avg:99.29ms
step:923/1770 train_time:91645ms step_avg:99.29ms
step:924/1770 train_time:91747ms step_avg:99.29ms
step:925/1770 train_time:91850ms step_avg:99.30ms
step:926/1770 train_time:91953ms step_avg:99.30ms
step:927/1770 train_time:92055ms step_avg:99.30ms
step:928/1770 train_time:92157ms step_avg:99.31ms
step:929/1770 train_time:92260ms step_avg:99.31ms
step:930/1770 train_time:92363ms step_avg:99.31ms
step:931/1770 train_time:92466ms step_avg:99.32ms
step:932/1770 train_time:92568ms step_avg:99.32ms
step:933/1770 train_time:92671ms step_avg:99.33ms
step:934/1770 train_time:92773ms step_avg:99.33ms
step:935/1770 train_time:92875ms step_avg:99.33ms
step:936/1770 train_time:92978ms step_avg:99.34ms
step:937/1770 train_time:93080ms step_avg:99.34ms
step:938/1770 train_time:93182ms step_avg:99.34ms
step:939/1770 train_time:93285ms step_avg:99.34ms
step:940/1770 train_time:93387ms step_avg:99.35ms
step:941/1770 train_time:93490ms step_avg:99.35ms
step:942/1770 train_time:93594ms step_avg:99.36ms
step:943/1770 train_time:93697ms step_avg:99.36ms
step:944/1770 train_time:93799ms step_avg:99.36ms
step:945/1770 train_time:93901ms step_avg:99.37ms
step:946/1770 train_time:94004ms step_avg:99.37ms
step:947/1770 train_time:94106ms step_avg:99.37ms
step:948/1770 train_time:94210ms step_avg:99.38ms
step:949/1770 train_time:94312ms step_avg:99.38ms
step:950/1770 train_time:94415ms step_avg:99.38ms
step:951/1770 train_time:94518ms step_avg:99.39ms
step:952/1770 train_time:94620ms step_avg:99.39ms
step:953/1770 train_time:94724ms step_avg:99.40ms
step:954/1770 train_time:94826ms step_avg:99.40ms
step:955/1770 train_time:94929ms step_avg:99.40ms
step:956/1770 train_time:95031ms step_avg:99.41ms
step:957/1770 train_time:95134ms step_avg:99.41ms
step:958/1770 train_time:95236ms step_avg:99.41ms
step:959/1770 train_time:95339ms step_avg:99.42ms
step:960/1770 train_time:95441ms step_avg:99.42ms
step:961/1770 train_time:95544ms step_avg:99.42ms
step:962/1770 train_time:95647ms step_avg:99.42ms
step:963/1770 train_time:95749ms step_avg:99.43ms
step:964/1770 train_time:95852ms step_avg:99.43ms
step:965/1770 train_time:95955ms step_avg:99.43ms
step:966/1770 train_time:96057ms step_avg:99.44ms
step:967/1770 train_time:96159ms step_avg:99.44ms
step:968/1770 train_time:96263ms step_avg:99.45ms
step:969/1770 train_time:96366ms step_avg:99.45ms
step:970/1770 train_time:96469ms step_avg:99.45ms
step:971/1770 train_time:96572ms step_avg:99.46ms
step:972/1770 train_time:96674ms step_avg:99.46ms
step:973/1770 train_time:96776ms step_avg:99.46ms
step:974/1770 train_time:96879ms step_avg:99.47ms
step:975/1770 train_time:96983ms step_avg:99.47ms
step:976/1770 train_time:97086ms step_avg:99.47ms
step:977/1770 train_time:97189ms step_avg:99.48ms
step:978/1770 train_time:97292ms step_avg:99.48ms
step:979/1770 train_time:97395ms step_avg:99.48ms
step:980/1770 train_time:97498ms step_avg:99.49ms
step:981/1770 train_time:97601ms step_avg:99.49ms
step:982/1770 train_time:97703ms step_avg:99.49ms
step:983/1770 train_time:97805ms step_avg:99.50ms
step:984/1770 train_time:97908ms step_avg:99.50ms
step:985/1770 train_time:98012ms step_avg:99.50ms
step:986/1770 train_time:98116ms step_avg:99.51ms
step:987/1770 train_time:98218ms step_avg:99.51ms
step:988/1770 train_time:98320ms step_avg:99.51ms
step:989/1770 train_time:98424ms step_avg:99.52ms
step:990/1770 train_time:98527ms step_avg:99.52ms
step:991/1770 train_time:98630ms step_avg:99.53ms
step:992/1770 train_time:98735ms step_avg:99.53ms
step:993/1770 train_time:98837ms step_avg:99.53ms
step:994/1770 train_time:98940ms step_avg:99.54ms
step:995/1770 train_time:99043ms step_avg:99.54ms
step:996/1770 train_time:99146ms step_avg:99.54ms
step:997/1770 train_time:99248ms step_avg:99.55ms
step:998/1770 train_time:99352ms step_avg:99.55ms
step:999/1770 train_time:99454ms step_avg:99.55ms
step:1000/1770 train_time:99557ms step_avg:99.56ms
step:1000/1770 val_loss:3.5108 train_time:99654ms step_avg:99.65ms
step:1001/1770 train_time:99674ms step_avg:99.57ms
step:1002/1770 train_time:99769ms step_avg:99.57ms
step:1003/1770 train_time:99873ms step_avg:99.57ms
step:1004/1770 train_time:99976ms step_avg:99.58ms
step:1005/1770 train_time:100078ms step_avg:99.58ms
step:1006/1770 train_time:100181ms step_avg:99.58ms
step:1007/1770 train_time:100283ms step_avg:99.59ms
step:1008/1770 train_time:100386ms step_avg:99.59ms
step:1009/1770 train_time:100488ms step_avg:99.59ms
step:1010/1770 train_time:100590ms step_avg:99.59ms
step:1011/1770 train_time:100694ms step_avg:99.60ms
step:1012/1770 train_time:100798ms step_avg:99.60ms
step:1013/1770 train_time:100900ms step_avg:99.61ms
step:1014/1770 train_time:101004ms step_avg:99.61ms
step:1015/1770 train_time:101106ms step_avg:99.61ms
step:1016/1770 train_time:101209ms step_avg:99.62ms
step:1017/1770 train_time:101312ms step_avg:99.62ms
step:1018/1770 train_time:101414ms step_avg:99.62ms
step:1019/1770 train_time:101517ms step_avg:99.62ms
step:1020/1770 train_time:101619ms step_avg:99.63ms
step:1021/1770 train_time:101722ms step_avg:99.63ms
step:1022/1770 train_time:101825ms step_avg:99.63ms
step:1023/1770 train_time:101928ms step_avg:99.64ms
step:1024/1770 train_time:102031ms step_avg:99.64ms
step:1025/1770 train_time:102134ms step_avg:99.64ms
step:1026/1770 train_time:102236ms step_avg:99.65ms
step:1027/1770 train_time:102339ms step_avg:99.65ms
step:1028/1770 train_time:102442ms step_avg:99.65ms
step:1029/1770 train_time:102545ms step_avg:99.65ms
step:1030/1770 train_time:102647ms step_avg:99.66ms
step:1031/1770 train_time:102749ms step_avg:99.66ms
step:1032/1770 train_time:102852ms step_avg:99.66ms
step:1033/1770 train_time:102955ms step_avg:99.67ms
step:1034/1770 train_time:103058ms step_avg:99.67ms
step:1035/1770 train_time:103161ms step_avg:99.67ms
step:1036/1770 train_time:103263ms step_avg:99.67ms
step:1037/1770 train_time:103367ms step_avg:99.68ms
step:1038/1770 train_time:103469ms step_avg:99.68ms
step:1039/1770 train_time:103571ms step_avg:99.68ms
step:1040/1770 train_time:103673ms step_avg:99.69ms
step:1041/1770 train_time:103775ms step_avg:99.69ms
step:1042/1770 train_time:103879ms step_avg:99.69ms
step:1043/1770 train_time:103982ms step_avg:99.70ms
step:1044/1770 train_time:104085ms step_avg:99.70ms
step:1045/1770 train_time:104188ms step_avg:99.70ms
step:1046/1770 train_time:104290ms step_avg:99.70ms
step:1047/1770 train_time:104393ms step_avg:99.71ms
step:1048/1770 train_time:104496ms step_avg:99.71ms
step:1049/1770 train_time:104599ms step_avg:99.71ms
step:1050/1770 train_time:104701ms step_avg:99.72ms
step:1051/1770 train_time:104805ms step_avg:99.72ms
step:1052/1770 train_time:104908ms step_avg:99.72ms
step:1053/1770 train_time:105011ms step_avg:99.73ms
step:1054/1770 train_time:105113ms step_avg:99.73ms
step:1055/1770 train_time:105217ms step_avg:99.73ms
step:1056/1770 train_time:105321ms step_avg:99.74ms
step:1057/1770 train_time:105423ms step_avg:99.74ms
step:1058/1770 train_time:105527ms step_avg:99.74ms
step:1059/1770 train_time:105630ms step_avg:99.74ms
step:1060/1770 train_time:105733ms step_avg:99.75ms
step:1061/1770 train_time:105836ms step_avg:99.75ms
step:1062/1770 train_time:105942ms step_avg:99.76ms
step:1063/1770 train_time:106046ms step_avg:99.76ms
step:1064/1770 train_time:106149ms step_avg:99.76ms
step:1065/1770 train_time:106252ms step_avg:99.77ms
step:1066/1770 train_time:106355ms step_avg:99.77ms
step:1067/1770 train_time:106458ms step_avg:99.77ms
step:1068/1770 train_time:106562ms step_avg:99.78ms
step:1069/1770 train_time:106664ms step_avg:99.78ms
step:1070/1770 train_time:106767ms step_avg:99.78ms
step:1071/1770 train_time:106870ms step_avg:99.79ms
step:1072/1770 train_time:106973ms step_avg:99.79ms
step:1073/1770 train_time:107075ms step_avg:99.79ms
step:1074/1770 train_time:107179ms step_avg:99.79ms
step:1075/1770 train_time:107282ms step_avg:99.80ms
step:1076/1770 train_time:107385ms step_avg:99.80ms
step:1077/1770 train_time:107488ms step_avg:99.80ms
step:1078/1770 train_time:107591ms step_avg:99.81ms
step:1079/1770 train_time:107694ms step_avg:99.81ms
step:1080/1770 train_time:107797ms step_avg:99.81ms
step:1081/1770 train_time:107900ms step_avg:99.81ms
step:1082/1770 train_time:108004ms step_avg:99.82ms
step:1083/1770 train_time:108107ms step_avg:99.82ms
step:1084/1770 train_time:108210ms step_avg:99.82ms
step:1085/1770 train_time:108312ms step_avg:99.83ms
step:1086/1770 train_time:108416ms step_avg:99.83ms
step:1087/1770 train_time:108518ms step_avg:99.83ms
step:1088/1770 train_time:108621ms step_avg:99.84ms
step:1089/1770 train_time:108725ms step_avg:99.84ms
step:1090/1770 train_time:108829ms step_avg:99.84ms
step:1091/1770 train_time:108931ms step_avg:99.85ms
step:1092/1770 train_time:109033ms step_avg:99.85ms
step:1093/1770 train_time:109137ms step_avg:99.85ms
step:1094/1770 train_time:109241ms step_avg:99.85ms
step:1095/1770 train_time:109344ms step_avg:99.86ms
step:1096/1770 train_time:109447ms step_avg:99.86ms
step:1097/1770 train_time:109550ms step_avg:99.86ms
step:1098/1770 train_time:109652ms step_avg:99.86ms
step:1099/1770 train_time:109754ms step_avg:99.87ms
step:1100/1770 train_time:109857ms step_avg:99.87ms
step:1101/1770 train_time:109960ms step_avg:99.87ms
step:1102/1770 train_time:110063ms step_avg:99.88ms
step:1103/1770 train_time:110166ms step_avg:99.88ms
step:1104/1770 train_time:110270ms step_avg:99.88ms
step:1105/1770 train_time:110372ms step_avg:99.88ms
step:1106/1770 train_time:110475ms step_avg:99.89ms
step:1107/1770 train_time:110579ms step_avg:99.89ms
step:1108/1770 train_time:110682ms step_avg:99.89ms
step:1109/1770 train_time:110785ms step_avg:99.90ms
step:1110/1770 train_time:110889ms step_avg:99.90ms
step:1111/1770 train_time:110991ms step_avg:99.90ms
step:1112/1770 train_time:111095ms step_avg:99.91ms
step:1113/1770 train_time:111197ms step_avg:99.91ms
step:1114/1770 train_time:111301ms step_avg:99.91ms
step:1115/1770 train_time:111405ms step_avg:99.91ms
step:1116/1770 train_time:111508ms step_avg:99.92ms
step:1117/1770 train_time:111611ms step_avg:99.92ms
step:1118/1770 train_time:111714ms step_avg:99.92ms
step:1119/1770 train_time:111818ms step_avg:99.93ms
step:1120/1770 train_time:111921ms step_avg:99.93ms
step:1121/1770 train_time:112025ms step_avg:99.93ms
step:1122/1770 train_time:112127ms step_avg:99.93ms
step:1123/1770 train_time:112229ms step_avg:99.94ms
step:1124/1770 train_time:112332ms step_avg:99.94ms
step:1125/1770 train_time:112435ms step_avg:99.94ms
step:1125/1770 val_loss:3.4686 train_time:112532ms step_avg:100.03ms
step:1126/1770 train_time:112554ms step_avg:99.96ms
step:1127/1770 train_time:112645ms step_avg:99.95ms
step:1128/1770 train_time:112748ms step_avg:99.95ms
step:1129/1770 train_time:112851ms step_avg:99.96ms
step:1130/1770 train_time:112954ms step_avg:99.96ms
step:1131/1770 train_time:113057ms step_avg:99.96ms
step:1132/1770 train_time:113160ms step_avg:99.97ms
step:1133/1770 train_time:113264ms step_avg:99.97ms
step:1134/1770 train_time:113367ms step_avg:99.97ms
step:1135/1770 train_time:113469ms step_avg:99.97ms
step:1136/1770 train_time:113572ms step_avg:99.98ms
step:1137/1770 train_time:113676ms step_avg:99.98ms
step:1138/1770 train_time:113780ms step_avg:99.98ms
step:1139/1770 train_time:113883ms step_avg:99.99ms
step:1140/1770 train_time:113985ms step_avg:99.99ms
step:1141/1770 train_time:114088ms step_avg:99.99ms
step:1142/1770 train_time:114191ms step_avg:99.99ms
step:1143/1770 train_time:114293ms step_avg:99.99ms
step:1144/1770 train_time:114396ms step_avg:100.00ms
step:1145/1770 train_time:114499ms step_avg:100.00ms
step:1146/1770 train_time:114602ms step_avg:100.00ms
step:1147/1770 train_time:114706ms step_avg:100.01ms
step:1148/1770 train_time:114809ms step_avg:100.01ms
step:1149/1770 train_time:114911ms step_avg:100.01ms
step:1150/1770 train_time:115014ms step_avg:100.01ms
step:1151/1770 train_time:115117ms step_avg:100.02ms
step:1152/1770 train_time:115221ms step_avg:100.02ms
step:1153/1770 train_time:115324ms step_avg:100.02ms
step:1154/1770 train_time:115426ms step_avg:100.02ms
step:1155/1770 train_time:115529ms step_avg:100.03ms
step:1156/1770 train_time:115633ms step_avg:100.03ms
step:1157/1770 train_time:115739ms step_avg:100.03ms
step:1158/1770 train_time:115842ms step_avg:100.04ms
step:1159/1770 train_time:115944ms step_avg:100.04ms
step:1160/1770 train_time:116046ms step_avg:100.04ms
step:1161/1770 train_time:116149ms step_avg:100.04ms
step:1162/1770 train_time:116252ms step_avg:100.04ms
step:1163/1770 train_time:116356ms step_avg:100.05ms
step:1164/1770 train_time:116459ms step_avg:100.05ms
step:1165/1770 train_time:116561ms step_avg:100.05ms
step:1166/1770 train_time:116667ms step_avg:100.06ms
step:1167/1770 train_time:116768ms step_avg:100.06ms
step:1168/1770 train_time:116871ms step_avg:100.06ms
step:1169/1770 train_time:116973ms step_avg:100.06ms
step:1170/1770 train_time:117077ms step_avg:100.07ms
step:1171/1770 train_time:117180ms step_avg:100.07ms
step:1172/1770 train_time:117284ms step_avg:100.07ms
step:1173/1770 train_time:117386ms step_avg:100.07ms
step:1174/1770 train_time:117489ms step_avg:100.08ms
step:1175/1770 train_time:117592ms step_avg:100.08ms
step:1176/1770 train_time:117695ms step_avg:100.08ms
step:1177/1770 train_time:117799ms step_avg:100.08ms
step:1178/1770 train_time:117902ms step_avg:100.09ms
step:1179/1770 train_time:118005ms step_avg:100.09ms
step:1180/1770 train_time:118107ms step_avg:100.09ms
step:1181/1770 train_time:118210ms step_avg:100.09ms
step:1182/1770 train_time:118313ms step_avg:100.10ms
step:1183/1770 train_time:118418ms step_avg:100.10ms
step:1184/1770 train_time:118522ms step_avg:100.10ms
step:1185/1770 train_time:118626ms step_avg:100.11ms
step:1186/1770 train_time:118730ms step_avg:100.11ms
step:1187/1770 train_time:118836ms step_avg:100.11ms
step:1188/1770 train_time:118939ms step_avg:100.12ms
step:1189/1770 train_time:119043ms step_avg:100.12ms
step:1190/1770 train_time:119147ms step_avg:100.12ms
step:1191/1770 train_time:119251ms step_avg:100.13ms
step:1192/1770 train_time:119354ms step_avg:100.13ms
step:1193/1770 train_time:119458ms step_avg:100.13ms
step:1194/1770 train_time:119562ms step_avg:100.14ms
step:1195/1770 train_time:119667ms step_avg:100.14ms
step:1196/1770 train_time:119771ms step_avg:100.14ms
step:1197/1770 train_time:119876ms step_avg:100.15ms
step:1198/1770 train_time:119981ms step_avg:100.15ms
step:1199/1770 train_time:120085ms step_avg:100.15ms
step:1200/1770 train_time:120189ms step_avg:100.16ms
step:1201/1770 train_time:120294ms step_avg:100.16ms
step:1202/1770 train_time:120397ms step_avg:100.16ms
step:1203/1770 train_time:120501ms step_avg:100.17ms
step:1204/1770 train_time:120606ms step_avg:100.17ms
step:1205/1770 train_time:120709ms step_avg:100.17ms
step:1206/1770 train_time:120814ms step_avg:100.18ms
step:1207/1770 train_time:120919ms step_avg:100.18ms
step:1208/1770 train_time:121023ms step_avg:100.18ms
step:1209/1770 train_time:121126ms step_avg:100.19ms
step:1210/1770 train_time:121230ms step_avg:100.19ms
step:1211/1770 train_time:121334ms step_avg:100.19ms
step:1212/1770 train_time:121441ms step_avg:100.20ms
step:1213/1770 train_time:121545ms step_avg:100.20ms
step:1214/1770 train_time:121648ms step_avg:100.20ms
step:1215/1770 train_time:121752ms step_avg:100.21ms
step:1216/1770 train_time:121858ms step_avg:100.21ms
step:1217/1770 train_time:121962ms step_avg:100.22ms
step:1218/1770 train_time:122066ms step_avg:100.22ms
step:1219/1770 train_time:122170ms step_avg:100.22ms
step:1220/1770 train_time:122273ms step_avg:100.22ms
step:1221/1770 train_time:122378ms step_avg:100.23ms
step:1222/1770 train_time:122485ms step_avg:100.23ms
step:1223/1770 train_time:122587ms step_avg:100.23ms
step:1224/1770 train_time:122693ms step_avg:100.24ms
step:1225/1770 train_time:122796ms step_avg:100.24ms
step:1226/1770 train_time:122901ms step_avg:100.25ms
step:1227/1770 train_time:123008ms step_avg:100.25ms
step:1228/1770 train_time:123113ms step_avg:100.25ms
step:1229/1770 train_time:123217ms step_avg:100.26ms
step:1230/1770 train_time:123322ms step_avg:100.26ms
step:1231/1770 train_time:123427ms step_avg:100.27ms
step:1232/1770 train_time:123530ms step_avg:100.27ms
step:1233/1770 train_time:123633ms step_avg:100.27ms
step:1234/1770 train_time:123736ms step_avg:100.27ms
step:1235/1770 train_time:123841ms step_avg:100.28ms
step:1236/1770 train_time:123946ms step_avg:100.28ms
step:1237/1770 train_time:124050ms step_avg:100.28ms
step:1238/1770 train_time:124154ms step_avg:100.29ms
step:1239/1770 train_time:124259ms step_avg:100.29ms
step:1240/1770 train_time:124363ms step_avg:100.29ms
step:1241/1770 train_time:124468ms step_avg:100.30ms
step:1242/1770 train_time:124572ms step_avg:100.30ms
step:1243/1770 train_time:124676ms step_avg:100.30ms
step:1244/1770 train_time:124781ms step_avg:100.31ms
step:1245/1770 train_time:124884ms step_avg:100.31ms
step:1246/1770 train_time:124989ms step_avg:100.31ms
step:1247/1770 train_time:125093ms step_avg:100.32ms
step:1248/1770 train_time:125197ms step_avg:100.32ms
step:1249/1770 train_time:125301ms step_avg:100.32ms
step:1250/1770 train_time:125406ms step_avg:100.32ms
step:1250/1770 val_loss:3.4218 train_time:125506ms step_avg:100.40ms
step:1251/1770 train_time:125526ms step_avg:100.34ms
step:1252/1770 train_time:125621ms step_avg:100.34ms
step:1253/1770 train_time:125725ms step_avg:100.34ms
step:1254/1770 train_time:125829ms step_avg:100.34ms
step:1255/1770 train_time:125937ms step_avg:100.35ms
step:1256/1770 train_time:126041ms step_avg:100.35ms
step:1257/1770 train_time:126144ms step_avg:100.35ms
step:1258/1770 train_time:126249ms step_avg:100.36ms
step:1259/1770 train_time:126354ms step_avg:100.36ms
step:1260/1770 train_time:126457ms step_avg:100.36ms
step:1261/1770 train_time:126562ms step_avg:100.37ms
step:1262/1770 train_time:126666ms step_avg:100.37ms
step:1263/1770 train_time:126771ms step_avg:100.37ms
step:1264/1770 train_time:126878ms step_avg:100.38ms
step:1265/1770 train_time:126982ms step_avg:100.38ms
step:1266/1770 train_time:127086ms step_avg:100.38ms
step:1267/1770 train_time:127190ms step_avg:100.39ms
step:1268/1770 train_time:127295ms step_avg:100.39ms
step:1269/1770 train_time:127399ms step_avg:100.39ms
step:1270/1770 train_time:127503ms step_avg:100.40ms
step:1271/1770 train_time:127608ms step_avg:100.40ms
step:1272/1770 train_time:127711ms step_avg:100.40ms
step:1273/1770 train_time:127816ms step_avg:100.41ms
step:1274/1770 train_time:127919ms step_avg:100.41ms
step:1275/1770 train_time:128023ms step_avg:100.41ms
step:1276/1770 train_time:128127ms step_avg:100.41ms
step:1277/1770 train_time:128231ms step_avg:100.42ms
step:1278/1770 train_time:128336ms step_avg:100.42ms
step:1279/1770 train_time:128441ms step_avg:100.42ms
step:1280/1770 train_time:128546ms step_avg:100.43ms
step:1281/1770 train_time:128650ms step_avg:100.43ms
step:1282/1770 train_time:128755ms step_avg:100.43ms
step:1283/1770 train_time:128859ms step_avg:100.44ms
step:1284/1770 train_time:128963ms step_avg:100.44ms
step:1285/1770 train_time:129068ms step_avg:100.44ms
step:1286/1770 train_time:129173ms step_avg:100.45ms
step:1287/1770 train_time:129279ms step_avg:100.45ms
step:1288/1770 train_time:129383ms step_avg:100.45ms
step:1289/1770 train_time:129488ms step_avg:100.46ms
step:1290/1770 train_time:129592ms step_avg:100.46ms
step:1291/1770 train_time:129696ms step_avg:100.46ms
step:1292/1770 train_time:129800ms step_avg:100.46ms
step:1293/1770 train_time:129904ms step_avg:100.47ms
step:1294/1770 train_time:130007ms step_avg:100.47ms
step:1295/1770 train_time:130112ms step_avg:100.47ms
step:1296/1770 train_time:130216ms step_avg:100.48ms
step:1297/1770 train_time:130319ms step_avg:100.48ms
step:1298/1770 train_time:130423ms step_avg:100.48ms
step:1299/1770 train_time:130529ms step_avg:100.48ms
step:1300/1770 train_time:130632ms step_avg:100.49ms
step:1301/1770 train_time:130737ms step_avg:100.49ms
step:1302/1770 train_time:130841ms step_avg:100.49ms
step:1303/1770 train_time:130945ms step_avg:100.49ms
step:1304/1770 train_time:131049ms step_avg:100.50ms
step:1305/1770 train_time:131153ms step_avg:100.50ms
step:1306/1770 train_time:131257ms step_avg:100.50ms
step:1307/1770 train_time:131360ms step_avg:100.51ms
step:1308/1770 train_time:131464ms step_avg:100.51ms
step:1309/1770 train_time:131569ms step_avg:100.51ms
step:1310/1770 train_time:131674ms step_avg:100.51ms
step:1311/1770 train_time:131778ms step_avg:100.52ms
step:1312/1770 train_time:131882ms step_avg:100.52ms
step:1313/1770 train_time:131985ms step_avg:100.52ms
step:1314/1770 train_time:132089ms step_avg:100.52ms
step:1315/1770 train_time:132195ms step_avg:100.53ms
step:1316/1770 train_time:132298ms step_avg:100.53ms
step:1317/1770 train_time:132402ms step_avg:100.53ms
step:1318/1770 train_time:132508ms step_avg:100.54ms
step:1319/1770 train_time:132613ms step_avg:100.54ms
step:1320/1770 train_time:132717ms step_avg:100.54ms
step:1321/1770 train_time:132821ms step_avg:100.55ms
step:1322/1770 train_time:132926ms step_avg:100.55ms
step:1323/1770 train_time:133031ms step_avg:100.55ms
step:1324/1770 train_time:133135ms step_avg:100.56ms
step:1325/1770 train_time:133241ms step_avg:100.56ms
step:1326/1770 train_time:133344ms step_avg:100.56ms
step:1327/1770 train_time:133452ms step_avg:100.57ms
step:1328/1770 train_time:133555ms step_avg:100.57ms
step:1329/1770 train_time:133659ms step_avg:100.57ms
step:1330/1770 train_time:133763ms step_avg:100.57ms
step:1331/1770 train_time:133867ms step_avg:100.58ms
step:1332/1770 train_time:133971ms step_avg:100.58ms
step:1333/1770 train_time:134075ms step_avg:100.58ms
step:1334/1770 train_time:134179ms step_avg:100.58ms
step:1335/1770 train_time:134283ms step_avg:100.59ms
step:1336/1770 train_time:134387ms step_avg:100.59ms
step:1337/1770 train_time:134492ms step_avg:100.59ms
step:1338/1770 train_time:134596ms step_avg:100.59ms
step:1339/1770 train_time:134700ms step_avg:100.60ms
step:1340/1770 train_time:134805ms step_avg:100.60ms
step:1341/1770 train_time:134908ms step_avg:100.60ms
step:1342/1770 train_time:135014ms step_avg:100.61ms
step:1343/1770 train_time:135119ms step_avg:100.61ms
step:1344/1770 train_time:135224ms step_avg:100.61ms
step:1345/1770 train_time:135328ms step_avg:100.62ms
step:1346/1770 train_time:135432ms step_avg:100.62ms
step:1347/1770 train_time:135536ms step_avg:100.62ms
step:1348/1770 train_time:135643ms step_avg:100.63ms
step:1349/1770 train_time:135747ms step_avg:100.63ms
step:1350/1770 train_time:135851ms step_avg:100.63ms
step:1351/1770 train_time:135956ms step_avg:100.63ms
step:1352/1770 train_time:136060ms step_avg:100.64ms
step:1353/1770 train_time:136165ms step_avg:100.64ms
step:1354/1770 train_time:136269ms step_avg:100.64ms
step:1355/1770 train_time:136373ms step_avg:100.64ms
step:1356/1770 train_time:136478ms step_avg:100.65ms
step:1357/1770 train_time:136582ms step_avg:100.65ms
step:1358/1770 train_time:136686ms step_avg:100.65ms
step:1359/1770 train_time:136791ms step_avg:100.66ms
step:1360/1770 train_time:136896ms step_avg:100.66ms
step:1361/1770 train_time:137001ms step_avg:100.66ms
step:1362/1770 train_time:137105ms step_avg:100.66ms
step:1363/1770 train_time:137208ms step_avg:100.67ms
step:1364/1770 train_time:137312ms step_avg:100.67ms
step:1365/1770 train_time:137416ms step_avg:100.67ms
step:1366/1770 train_time:137520ms step_avg:100.67ms
step:1367/1770 train_time:137625ms step_avg:100.68ms
step:1368/1770 train_time:137728ms step_avg:100.68ms
step:1369/1770 train_time:137833ms step_avg:100.68ms
step:1370/1770 train_time:137938ms step_avg:100.68ms
step:1371/1770 train_time:138042ms step_avg:100.69ms
step:1372/1770 train_time:138146ms step_avg:100.69ms
step:1373/1770 train_time:138250ms step_avg:100.69ms
step:1374/1770 train_time:138355ms step_avg:100.70ms
step:1375/1770 train_time:138459ms step_avg:100.70ms
step:1375/1770 val_loss:3.3787 train_time:138557ms step_avg:100.77ms
step:1376/1770 train_time:138579ms step_avg:100.71ms
step:1377/1770 train_time:138672ms step_avg:100.71ms
step:1378/1770 train_time:138777ms step_avg:100.71ms
step:1379/1770 train_time:138881ms step_avg:100.71ms
step:1380/1770 train_time:138984ms step_avg:100.71ms
step:1381/1770 train_time:139089ms step_avg:100.72ms
step:1382/1770 train_time:139193ms step_avg:100.72ms
step:1383/1770 train_time:139298ms step_avg:100.72ms
step:1384/1770 train_time:139401ms step_avg:100.72ms
step:1385/1770 train_time:139506ms step_avg:100.73ms
step:1386/1770 train_time:139611ms step_avg:100.73ms
step:1387/1770 train_time:139716ms step_avg:100.73ms
step:1388/1770 train_time:139820ms step_avg:100.73ms
step:1389/1770 train_time:139923ms step_avg:100.74ms
step:1390/1770 train_time:140027ms step_avg:100.74ms
step:1391/1770 train_time:140132ms step_avg:100.74ms
step:1392/1770 train_time:140237ms step_avg:100.74ms
step:1393/1770 train_time:140341ms step_avg:100.75ms
step:1394/1770 train_time:140445ms step_avg:100.75ms
step:1395/1770 train_time:140549ms step_avg:100.75ms
step:1396/1770 train_time:140654ms step_avg:100.76ms
step:1397/1770 train_time:140758ms step_avg:100.76ms
step:1398/1770 train_time:140864ms step_avg:100.76ms
step:1399/1770 train_time:140967ms step_avg:100.76ms
step:1400/1770 train_time:141072ms step_avg:100.77ms
step:1401/1770 train_time:141176ms step_avg:100.77ms
step:1402/1770 train_time:141281ms step_avg:100.77ms
step:1403/1770 train_time:141385ms step_avg:100.77ms
step:1404/1770 train_time:141491ms step_avg:100.78ms
step:1405/1770 train_time:141594ms step_avg:100.78ms
step:1406/1770 train_time:141699ms step_avg:100.78ms
step:1407/1770 train_time:141802ms step_avg:100.78ms
step:1408/1770 train_time:141906ms step_avg:100.79ms
step:1409/1770 train_time:142010ms step_avg:100.79ms
step:1410/1770 train_time:142115ms step_avg:100.79ms
step:1411/1770 train_time:142220ms step_avg:100.79ms
step:1412/1770 train_time:142324ms step_avg:100.80ms
step:1413/1770 train_time:142428ms step_avg:100.80ms
step:1414/1770 train_time:142533ms step_avg:100.80ms
step:1415/1770 train_time:142638ms step_avg:100.80ms
step:1416/1770 train_time:142743ms step_avg:100.81ms
step:1417/1770 train_time:142847ms step_avg:100.81ms
step:1418/1770 train_time:142951ms step_avg:100.81ms
step:1419/1770 train_time:143055ms step_avg:100.81ms
step:1420/1770 train_time:143160ms step_avg:100.82ms
step:1421/1770 train_time:143264ms step_avg:100.82ms
step:1422/1770 train_time:143368ms step_avg:100.82ms
step:1423/1770 train_time:143473ms step_avg:100.82ms
step:1424/1770 train_time:143579ms step_avg:100.83ms
step:1425/1770 train_time:143683ms step_avg:100.83ms
step:1426/1770 train_time:143788ms step_avg:100.83ms
step:1427/1770 train_time:143892ms step_avg:100.84ms
step:1428/1770 train_time:143997ms step_avg:100.84ms
step:1429/1770 train_time:144101ms step_avg:100.84ms
step:1430/1770 train_time:144206ms step_avg:100.84ms
step:1431/1770 train_time:144311ms step_avg:100.85ms
step:1432/1770 train_time:144414ms step_avg:100.85ms
step:1433/1770 train_time:144519ms step_avg:100.85ms
step:1434/1770 train_time:144623ms step_avg:100.85ms
step:1435/1770 train_time:144727ms step_avg:100.85ms
step:1436/1770 train_time:144833ms step_avg:100.86ms
step:1437/1770 train_time:144938ms step_avg:100.86ms
step:1438/1770 train_time:145042ms step_avg:100.86ms
step:1439/1770 train_time:145146ms step_avg:100.87ms
step:1440/1770 train_time:145249ms step_avg:100.87ms
step:1441/1770 train_time:145356ms step_avg:100.87ms
step:1442/1770 train_time:145459ms step_avg:100.87ms
step:1443/1770 train_time:145564ms step_avg:100.88ms
step:1444/1770 train_time:145669ms step_avg:100.88ms
step:1445/1770 train_time:145773ms step_avg:100.88ms
step:1446/1770 train_time:145879ms step_avg:100.88ms
step:1447/1770 train_time:145984ms step_avg:100.89ms
step:1448/1770 train_time:146090ms step_avg:100.89ms
step:1449/1770 train_time:146197ms step_avg:100.89ms
step:1450/1770 train_time:146302ms step_avg:100.90ms
step:1451/1770 train_time:146407ms step_avg:100.90ms
step:1452/1770 train_time:146513ms step_avg:100.90ms
step:1453/1770 train_time:146618ms step_avg:100.91ms
step:1454/1770 train_time:146723ms step_avg:100.91ms
step:1455/1770 train_time:146830ms step_avg:100.91ms
step:1456/1770 train_time:146936ms step_avg:100.92ms
step:1457/1770 train_time:147041ms step_avg:100.92ms
step:1458/1770 train_time:147146ms step_avg:100.92ms
step:1459/1770 train_time:147253ms step_avg:100.93ms
step:1460/1770 train_time:147358ms step_avg:100.93ms
step:1461/1770 train_time:147464ms step_avg:100.93ms
step:1462/1770 train_time:147570ms step_avg:100.94ms
step:1463/1770 train_time:147675ms step_avg:100.94ms
step:1464/1770 train_time:147782ms step_avg:100.94ms
step:1465/1770 train_time:147887ms step_avg:100.95ms
step:1466/1770 train_time:147993ms step_avg:100.95ms
step:1467/1770 train_time:148099ms step_avg:100.95ms
step:1468/1770 train_time:148205ms step_avg:100.96ms
step:1469/1770 train_time:148310ms step_avg:100.96ms
step:1470/1770 train_time:148417ms step_avg:100.96ms
step:1471/1770 train_time:148521ms step_avg:100.97ms
step:1472/1770 train_time:148626ms step_avg:100.97ms
step:1473/1770 train_time:148733ms step_avg:100.97ms
step:1474/1770 train_time:148839ms step_avg:100.98ms
step:1475/1770 train_time:148944ms step_avg:100.98ms
step:1476/1770 train_time:149049ms step_avg:100.98ms
step:1477/1770 train_time:149156ms step_avg:100.99ms
step:1478/1770 train_time:149261ms step_avg:100.99ms
step:1479/1770 train_time:149367ms step_avg:100.99ms
step:1480/1770 train_time:149473ms step_avg:101.00ms
step:1481/1770 train_time:149581ms step_avg:101.00ms
step:1482/1770 train_time:149687ms step_avg:101.00ms
step:1483/1770 train_time:149792ms step_avg:101.01ms
step:1484/1770 train_time:149898ms step_avg:101.01ms
step:1485/1770 train_time:150003ms step_avg:101.01ms
step:1486/1770 train_time:150109ms step_avg:101.02ms
step:1487/1770 train_time:150215ms step_avg:101.02ms
step:1488/1770 train_time:150321ms step_avg:101.02ms
step:1489/1770 train_time:150427ms step_avg:101.03ms
step:1490/1770 train_time:150532ms step_avg:101.03ms
step:1491/1770 train_time:150638ms step_avg:101.03ms
step:1492/1770 train_time:150743ms step_avg:101.03ms
step:1493/1770 train_time:150851ms step_avg:101.04ms
step:1494/1770 train_time:150960ms step_avg:101.04ms
step:1495/1770 train_time:151065ms step_avg:101.05ms
step:1496/1770 train_time:151169ms step_avg:101.05ms
step:1497/1770 train_time:151275ms step_avg:101.05ms
step:1498/1770 train_time:151380ms step_avg:101.05ms
step:1499/1770 train_time:151485ms step_avg:101.06ms
step:1500/1770 train_time:151589ms step_avg:101.06ms
step:1500/1770 val_loss:3.3409 train_time:151689ms step_avg:101.13ms
step:1501/1770 train_time:151709ms step_avg:101.07ms
step:1502/1770 train_time:151805ms step_avg:101.07ms
step:1503/1770 train_time:151909ms step_avg:101.07ms
step:1504/1770 train_time:152015ms step_avg:101.07ms
step:1505/1770 train_time:152122ms step_avg:101.08ms
step:1506/1770 train_time:152226ms step_avg:101.08ms
step:1507/1770 train_time:152332ms step_avg:101.08ms
step:1508/1770 train_time:152439ms step_avg:101.09ms
step:1509/1770 train_time:152544ms step_avg:101.09ms
step:1510/1770 train_time:152649ms step_avg:101.09ms
step:1511/1770 train_time:152755ms step_avg:101.10ms
step:1512/1770 train_time:152861ms step_avg:101.10ms
step:1513/1770 train_time:152965ms step_avg:101.10ms
step:1514/1770 train_time:153071ms step_avg:101.10ms
step:1515/1770 train_time:153176ms step_avg:101.11ms
step:1516/1770 train_time:153281ms step_avg:101.11ms
step:1517/1770 train_time:153387ms step_avg:101.11ms
step:1518/1770 train_time:153495ms step_avg:101.12ms
step:1519/1770 train_time:153599ms step_avg:101.12ms
step:1520/1770 train_time:153704ms step_avg:101.12ms
step:1521/1770 train_time:153809ms step_avg:101.12ms
step:1522/1770 train_time:153915ms step_avg:101.13ms
step:1523/1770 train_time:154022ms step_avg:101.13ms
step:1524/1770 train_time:154127ms step_avg:101.13ms
step:1525/1770 train_time:154232ms step_avg:101.14ms
step:1526/1770 train_time:154337ms step_avg:101.14ms
step:1527/1770 train_time:154442ms step_avg:101.14ms
step:1528/1770 train_time:154548ms step_avg:101.14ms
step:1529/1770 train_time:154654ms step_avg:101.15ms
step:1530/1770 train_time:154760ms step_avg:101.15ms
step:1531/1770 train_time:154866ms step_avg:101.15ms
step:1532/1770 train_time:154971ms step_avg:101.16ms
step:1533/1770 train_time:155077ms step_avg:101.16ms
step:1534/1770 train_time:155183ms step_avg:101.16ms
step:1535/1770 train_time:155288ms step_avg:101.16ms
step:1536/1770 train_time:155393ms step_avg:101.17ms
step:1537/1770 train_time:155499ms step_avg:101.17ms
step:1538/1770 train_time:155606ms step_avg:101.17ms
step:1539/1770 train_time:155711ms step_avg:101.18ms
step:1540/1770 train_time:155821ms step_avg:101.18ms
step:1541/1770 train_time:155928ms step_avg:101.19ms
step:1542/1770 train_time:156033ms step_avg:101.19ms
step:1543/1770 train_time:156138ms step_avg:101.19ms
step:1544/1770 train_time:156245ms step_avg:101.19ms
step:1545/1770 train_time:156350ms step_avg:101.20ms
step:1546/1770 train_time:156455ms step_avg:101.20ms
step:1547/1770 train_time:156560ms step_avg:101.20ms
step:1548/1770 train_time:156666ms step_avg:101.21ms
step:1549/1770 train_time:156770ms step_avg:101.21ms
step:1550/1770 train_time:156877ms step_avg:101.21ms
step:1551/1770 train_time:156981ms step_avg:101.21ms
step:1552/1770 train_time:157089ms step_avg:101.22ms
step:1553/1770 train_time:157194ms step_avg:101.22ms
step:1554/1770 train_time:157299ms step_avg:101.22ms
step:1555/1770 train_time:157405ms step_avg:101.23ms
step:1556/1770 train_time:157510ms step_avg:101.23ms
step:1557/1770 train_time:157616ms step_avg:101.23ms
step:1558/1770 train_time:157721ms step_avg:101.23ms
step:1559/1770 train_time:157826ms step_avg:101.24ms
step:1560/1770 train_time:157930ms step_avg:101.24ms
step:1561/1770 train_time:158037ms step_avg:101.24ms
step:1562/1770 train_time:158143ms step_avg:101.24ms
step:1563/1770 train_time:158249ms step_avg:101.25ms
step:1564/1770 train_time:158354ms step_avg:101.25ms
step:1565/1770 train_time:158460ms step_avg:101.25ms
step:1566/1770 train_time:158565ms step_avg:101.25ms
step:1567/1770 train_time:158670ms step_avg:101.26ms
step:1568/1770 train_time:158776ms step_avg:101.26ms
step:1569/1770 train_time:158884ms step_avg:101.26ms
step:1570/1770 train_time:158990ms step_avg:101.27ms
step:1571/1770 train_time:159096ms step_avg:101.27ms
step:1572/1770 train_time:159202ms step_avg:101.27ms
step:1573/1770 train_time:159311ms step_avg:101.28ms
step:1574/1770 train_time:159415ms step_avg:101.28ms
step:1575/1770 train_time:159520ms step_avg:101.28ms
step:1576/1770 train_time:159624ms step_avg:101.28ms
step:1577/1770 train_time:159730ms step_avg:101.29ms
step:1578/1770 train_time:159837ms step_avg:101.29ms
step:1579/1770 train_time:159941ms step_avg:101.29ms
step:1580/1770 train_time:160046ms step_avg:101.30ms
step:1581/1770 train_time:160155ms step_avg:101.30ms
step:1582/1770 train_time:160263ms step_avg:101.30ms
step:1583/1770 train_time:160368ms step_avg:101.31ms
step:1584/1770 train_time:160474ms step_avg:101.31ms
step:1585/1770 train_time:160579ms step_avg:101.31ms
step:1586/1770 train_time:160688ms step_avg:101.32ms
step:1587/1770 train_time:160794ms step_avg:101.32ms
step:1588/1770 train_time:160900ms step_avg:101.32ms
step:1589/1770 train_time:161007ms step_avg:101.33ms
step:1590/1770 train_time:161112ms step_avg:101.33ms
step:1591/1770 train_time:161217ms step_avg:101.33ms
step:1592/1770 train_time:161323ms step_avg:101.33ms
step:1593/1770 train_time:161428ms step_avg:101.34ms
step:1594/1770 train_time:161534ms step_avg:101.34ms
step:1595/1770 train_time:161640ms step_avg:101.34ms
step:1596/1770 train_time:161747ms step_avg:101.34ms
step:1597/1770 train_time:161852ms step_avg:101.35ms
step:1598/1770 train_time:161957ms step_avg:101.35ms
step:1599/1770 train_time:162064ms step_avg:101.35ms
step:1600/1770 train_time:162171ms step_avg:101.36ms
step:1601/1770 train_time:162277ms step_avg:101.36ms
step:1602/1770 train_time:162383ms step_avg:101.36ms
step:1603/1770 train_time:162488ms step_avg:101.37ms
step:1604/1770 train_time:162593ms step_avg:101.37ms
step:1605/1770 train_time:162698ms step_avg:101.37ms
step:1606/1770 train_time:162803ms step_avg:101.37ms
step:1607/1770 train_time:162912ms step_avg:101.38ms
step:1608/1770 train_time:163018ms step_avg:101.38ms
step:1609/1770 train_time:163123ms step_avg:101.38ms
step:1610/1770 train_time:163229ms step_avg:101.38ms
step:1611/1770 train_time:163335ms step_avg:101.39ms
step:1612/1770 train_time:163441ms step_avg:101.39ms
step:1613/1770 train_time:163546ms step_avg:101.39ms
step:1614/1770 train_time:163652ms step_avg:101.40ms
step:1615/1770 train_time:163758ms step_avg:101.40ms
step:1616/1770 train_time:163863ms step_avg:101.40ms
step:1617/1770 train_time:163971ms step_avg:101.40ms
step:1618/1770 train_time:164078ms step_avg:101.41ms
step:1619/1770 train_time:164184ms step_avg:101.41ms
step:1620/1770 train_time:164290ms step_avg:101.41ms
step:1621/1770 train_time:164396ms step_avg:101.42ms
step:1622/1770 train_time:164502ms step_avg:101.42ms
step:1623/1770 train_time:164610ms step_avg:101.42ms
step:1624/1770 train_time:164715ms step_avg:101.43ms
step:1625/1770 train_time:164820ms step_avg:101.43ms
step:1625/1770 val_loss:3.3069 train_time:164920ms step_avg:101.49ms
step:1626/1770 train_time:164940ms step_avg:101.44ms
step:1627/1770 train_time:165039ms step_avg:101.44ms
step:1628/1770 train_time:165143ms step_avg:101.44ms
step:1629/1770 train_time:165247ms step_avg:101.44ms
step:1630/1770 train_time:165352ms step_avg:101.44ms
step:1631/1770 train_time:165457ms step_avg:101.45ms
step:1632/1770 train_time:165562ms step_avg:101.45ms
step:1633/1770 train_time:165668ms step_avg:101.45ms
step:1634/1770 train_time:165773ms step_avg:101.45ms
step:1635/1770 train_time:165879ms step_avg:101.45ms
step:1636/1770 train_time:165985ms step_avg:101.46ms
step:1637/1770 train_time:166092ms step_avg:101.46ms
step:1638/1770 train_time:166198ms step_avg:101.46ms
step:1639/1770 train_time:166304ms step_avg:101.47ms
step:1640/1770 train_time:166410ms step_avg:101.47ms
step:1641/1770 train_time:166516ms step_avg:101.47ms
step:1642/1770 train_time:166621ms step_avg:101.47ms
step:1643/1770 train_time:166726ms step_avg:101.48ms
step:1644/1770 train_time:166833ms step_avg:101.48ms
step:1645/1770 train_time:166938ms step_avg:101.48ms
step:1646/1770 train_time:167045ms step_avg:101.49ms
step:1647/1770 train_time:167151ms step_avg:101.49ms
step:1648/1770 train_time:167256ms step_avg:101.49ms
step:1649/1770 train_time:167361ms step_avg:101.49ms
step:1650/1770 train_time:167466ms step_avg:101.49ms
step:1651/1770 train_time:167571ms step_avg:101.50ms
step:1652/1770 train_time:167677ms step_avg:101.50ms
step:1653/1770 train_time:167783ms step_avg:101.50ms
step:1654/1770 train_time:167890ms step_avg:101.51ms
step:1655/1770 train_time:167998ms step_avg:101.51ms
step:1656/1770 train_time:168103ms step_avg:101.51ms
step:1657/1770 train_time:168210ms step_avg:101.51ms
step:1658/1770 train_time:168316ms step_avg:101.52ms
step:1659/1770 train_time:168422ms step_avg:101.52ms
step:1660/1770 train_time:168527ms step_avg:101.52ms
step:1661/1770 train_time:168633ms step_avg:101.53ms
step:1662/1770 train_time:168738ms step_avg:101.53ms
step:1663/1770 train_time:168843ms step_avg:101.53ms
step:1664/1770 train_time:168948ms step_avg:101.53ms
step:1665/1770 train_time:169053ms step_avg:101.53ms
step:1666/1770 train_time:169159ms step_avg:101.54ms
step:1667/1770 train_time:169265ms step_avg:101.54ms
step:1668/1770 train_time:169371ms step_avg:101.54ms
step:1669/1770 train_time:169475ms step_avg:101.54ms
step:1670/1770 train_time:169579ms step_avg:101.54ms
step:1671/1770 train_time:169684ms step_avg:101.55ms
step:1672/1770 train_time:169791ms step_avg:101.55ms
step:1673/1770 train_time:169897ms step_avg:101.55ms
step:1674/1770 train_time:170003ms step_avg:101.55ms
step:1675/1770 train_time:170108ms step_avg:101.56ms
step:1676/1770 train_time:170214ms step_avg:101.56ms
step:1677/1770 train_time:170323ms step_avg:101.56ms
step:1678/1770 train_time:170428ms step_avg:101.57ms
step:1679/1770 train_time:170533ms step_avg:101.57ms
step:1680/1770 train_time:170638ms step_avg:101.57ms
step:1681/1770 train_time:170744ms step_avg:101.57ms
step:1682/1770 train_time:170852ms step_avg:101.58ms
step:1683/1770 train_time:170957ms step_avg:101.58ms
step:1684/1770 train_time:171062ms step_avg:101.58ms
step:1685/1770 train_time:171167ms step_avg:101.58ms
step:1686/1770 train_time:171274ms step_avg:101.59ms
step:1687/1770 train_time:171382ms step_avg:101.59ms
step:1688/1770 train_time:171487ms step_avg:101.59ms
step:1689/1770 train_time:171593ms step_avg:101.59ms
step:1690/1770 train_time:171698ms step_avg:101.60ms
step:1691/1770 train_time:171803ms step_avg:101.60ms
step:1692/1770 train_time:171910ms step_avg:101.60ms
step:1693/1770 train_time:172016ms step_avg:101.60ms
step:1694/1770 train_time:172121ms step_avg:101.61ms
step:1695/1770 train_time:172227ms step_avg:101.61ms
step:1696/1770 train_time:172334ms step_avg:101.61ms
step:1697/1770 train_time:172442ms step_avg:101.62ms
step:1698/1770 train_time:172548ms step_avg:101.62ms
step:1699/1770 train_time:172653ms step_avg:101.62ms
step:1700/1770 train_time:172758ms step_avg:101.62ms
step:1701/1770 train_time:172863ms step_avg:101.62ms
step:1702/1770 train_time:172970ms step_avg:101.63ms
step:1703/1770 train_time:173075ms step_avg:101.63ms
step:1704/1770 train_time:173182ms step_avg:101.63ms
step:1705/1770 train_time:173287ms step_avg:101.63ms
step:1706/1770 train_time:173392ms step_avg:101.64ms
step:1707/1770 train_time:173498ms step_avg:101.64ms
step:1708/1770 train_time:173604ms step_avg:101.64ms
step:1709/1770 train_time:173712ms step_avg:101.65ms
step:1710/1770 train_time:173821ms step_avg:101.65ms
step:1711/1770 train_time:173929ms step_avg:101.65ms
step:1712/1770 train_time:174036ms step_avg:101.66ms
step:1713/1770 train_time:174142ms step_avg:101.66ms
step:1714/1770 train_time:174248ms step_avg:101.66ms
step:1715/1770 train_time:174354ms step_avg:101.66ms
step:1716/1770 train_time:174460ms step_avg:101.67ms
step:1717/1770 train_time:174565ms step_avg:101.67ms
step:1718/1770 train_time:174672ms step_avg:101.67ms
step:1719/1770 train_time:174780ms step_avg:101.68ms
step:1720/1770 train_time:174887ms step_avg:101.68ms
step:1721/1770 train_time:174994ms step_avg:101.68ms
step:1722/1770 train_time:175104ms step_avg:101.69ms
step:1723/1770 train_time:175212ms step_avg:101.69ms
step:1724/1770 train_time:175320ms step_avg:101.69ms
step:1725/1770 train_time:175429ms step_avg:101.70ms
step:1726/1770 train_time:175537ms step_avg:101.70ms
step:1727/1770 train_time:175644ms step_avg:101.70ms
step:1728/1770 train_time:175753ms step_avg:101.71ms
step:1729/1770 train_time:175859ms step_avg:101.71ms
step:1730/1770 train_time:175966ms step_avg:101.71ms
step:1731/1770 train_time:176075ms step_avg:101.72ms
step:1732/1770 train_time:176180ms step_avg:101.72ms
step:1733/1770 train_time:176289ms step_avg:101.72ms
step:1734/1770 train_time:176395ms step_avg:101.73ms
step:1735/1770 train_time:176502ms step_avg:101.73ms
step:1736/1770 train_time:176608ms step_avg:101.73ms
step:1737/1770 train_time:176714ms step_avg:101.74ms
step:1738/1770 train_time:176821ms step_avg:101.74ms
step:1739/1770 train_time:176926ms step_avg:101.74ms
step:1740/1770 train_time:177033ms step_avg:101.74ms
step:1741/1770 train_time:177141ms step_avg:101.75ms
step:1742/1770 train_time:177251ms step_avg:101.75ms
step:1743/1770 train_time:177358ms step_avg:101.75ms
step:1744/1770 train_time:177464ms step_avg:101.76ms
step:1745/1770 train_time:177569ms step_avg:101.76ms
step:1746/1770 train_time:177679ms step_avg:101.76ms
step:1747/1770 train_time:177785ms step_avg:101.77ms
step:1748/1770 train_time:177893ms step_avg:101.77ms
step:1749/1770 train_time:178000ms step_avg:101.77ms
step:1750/1770 train_time:178106ms step_avg:101.77ms
step:1750/1770 val_loss:3.2801 train_time:178207ms step_avg:101.83ms
step:1751/1770 train_time:178227ms step_avg:101.79ms
step:1752/1770 train_time:178322ms step_avg:101.78ms
step:1753/1770 train_time:178428ms step_avg:101.78ms
step:1754/1770 train_time:178534ms step_avg:101.79ms
step:1755/1770 train_time:178640ms step_avg:101.79ms
step:1756/1770 train_time:178747ms step_avg:101.79ms
step:1757/1770 train_time:178853ms step_avg:101.79ms
step:1758/1770 train_time:178959ms step_avg:101.80ms
step:1759/1770 train_time:179065ms step_avg:101.80ms
step:1760/1770 train_time:179172ms step_avg:101.80ms
step:1761/1770 train_time:179280ms step_avg:101.81ms
step:1762/1770 train_time:179389ms step_avg:101.81ms
step:1763/1770 train_time:179495ms step_avg:101.81ms
step:1764/1770 train_time:179602ms step_avg:101.82ms
step:1765/1770 train_time:179709ms step_avg:101.82ms
step:1766/1770 train_time:179819ms step_avg:101.82ms
step:1767/1770 train_time:179924ms step_avg:101.82ms
step:1768/1770 train_time:180030ms step_avg:101.83ms
step:1769/1770 train_time:180135ms step_avg:101.83ms
step:1770/1770 train_time:180241ms step_avg:101.83ms
step:1770/1770 val_loss:3.2770 train_time:180342ms step_avg:101.89ms
peak memory allocated: 30724 MiB reserved: 45392 MiB
