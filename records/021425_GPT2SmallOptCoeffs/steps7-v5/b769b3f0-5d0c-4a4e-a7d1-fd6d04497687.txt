import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.1357, -4.2084, 1.0726),
        (4.132, -4.2045, 1.0725),
        (4.077, -4.1489, 1.0719),
        (4.0422, -4.1139, 1.0717),
        (3.9129, -3.9845, 1.0715),
        (3.3337, -3.2386, 0.9049),
        (2.2005, -1.6921, 0.4915),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 07:15:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            116W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            116W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            107W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:67ms step_avg:66.86ms
step:2/1770 train_time:142ms step_avg:71.00ms
step:3/1770 train_time:235ms step_avg:78.24ms
step:4/1770 train_time:330ms step_avg:82.47ms
step:5/1770 train_time:426ms step_avg:85.11ms
step:6/1770 train_time:522ms step_avg:86.92ms
step:7/1770 train_time:618ms step_avg:88.22ms
step:8/1770 train_time:714ms step_avg:89.21ms
step:9/1770 train_time:810ms step_avg:90.03ms
step:10/1770 train_time:907ms step_avg:90.66ms
step:11/1770 train_time:1003ms step_avg:91.20ms
step:12/1770 train_time:1100ms step_avg:91.70ms
step:13/1770 train_time:1196ms step_avg:92.01ms
step:14/1770 train_time:1292ms step_avg:92.30ms
step:15/1770 train_time:1388ms step_avg:92.53ms
step:16/1770 train_time:1484ms step_avg:92.78ms
step:17/1770 train_time:1581ms step_avg:92.98ms
step:18/1770 train_time:1677ms step_avg:93.16ms
step:19/1770 train_time:1773ms step_avg:93.32ms
step:20/1770 train_time:1869ms step_avg:93.45ms
step:21/1770 train_time:1965ms step_avg:93.59ms
step:22/1770 train_time:2063ms step_avg:93.75ms
step:23/1770 train_time:2158ms step_avg:93.84ms
step:24/1770 train_time:2255ms step_avg:93.95ms
step:25/1770 train_time:2351ms step_avg:94.02ms
step:26/1770 train_time:2447ms step_avg:94.10ms
step:27/1770 train_time:2543ms step_avg:94.20ms
step:28/1770 train_time:2640ms step_avg:94.29ms
step:29/1770 train_time:2736ms step_avg:94.36ms
step:30/1770 train_time:2832ms step_avg:94.41ms
step:31/1770 train_time:2928ms step_avg:94.47ms
step:32/1770 train_time:3025ms step_avg:94.53ms
step:33/1770 train_time:3121ms step_avg:94.57ms
step:34/1770 train_time:3217ms step_avg:94.61ms
step:35/1770 train_time:3313ms step_avg:94.66ms
step:36/1770 train_time:3409ms step_avg:94.70ms
step:37/1770 train_time:3505ms step_avg:94.74ms
step:38/1770 train_time:3602ms step_avg:94.78ms
step:39/1770 train_time:3697ms step_avg:94.81ms
step:40/1770 train_time:3793ms step_avg:94.84ms
step:41/1770 train_time:3889ms step_avg:94.86ms
step:42/1770 train_time:3986ms step_avg:94.90ms
step:43/1770 train_time:4082ms step_avg:94.93ms
step:44/1770 train_time:4178ms step_avg:94.96ms
step:45/1770 train_time:4274ms step_avg:94.98ms
step:46/1770 train_time:4370ms step_avg:95.00ms
step:47/1770 train_time:4466ms step_avg:95.03ms
step:48/1770 train_time:4563ms step_avg:95.07ms
step:49/1770 train_time:4660ms step_avg:95.09ms
step:50/1770 train_time:4756ms step_avg:95.12ms
step:51/1770 train_time:4852ms step_avg:95.14ms
step:52/1770 train_time:4948ms step_avg:95.16ms
step:53/1770 train_time:5045ms step_avg:95.19ms
step:54/1770 train_time:5141ms step_avg:95.20ms
step:55/1770 train_time:5237ms step_avg:95.22ms
step:56/1770 train_time:5333ms step_avg:95.23ms
step:57/1770 train_time:5429ms step_avg:95.24ms
step:58/1770 train_time:5525ms step_avg:95.25ms
step:59/1770 train_time:5621ms step_avg:95.27ms
step:60/1770 train_time:5717ms step_avg:95.28ms
step:61/1770 train_time:5812ms step_avg:95.28ms
step:62/1770 train_time:5908ms step_avg:95.30ms
step:63/1770 train_time:6005ms step_avg:95.31ms
step:64/1770 train_time:6102ms step_avg:95.34ms
step:65/1770 train_time:6197ms step_avg:95.34ms
step:66/1770 train_time:6293ms step_avg:95.34ms
step:67/1770 train_time:6388ms step_avg:95.35ms
step:68/1770 train_time:6485ms step_avg:95.36ms
step:69/1770 train_time:6581ms step_avg:95.38ms
step:70/1770 train_time:6677ms step_avg:95.39ms
step:71/1770 train_time:6773ms step_avg:95.40ms
step:72/1770 train_time:6870ms step_avg:95.41ms
step:73/1770 train_time:6966ms step_avg:95.42ms
step:74/1770 train_time:7062ms step_avg:95.43ms
step:75/1770 train_time:7158ms step_avg:95.44ms
step:76/1770 train_time:7254ms step_avg:95.44ms
step:77/1770 train_time:7350ms step_avg:95.45ms
step:78/1770 train_time:7446ms step_avg:95.46ms
step:79/1770 train_time:7542ms step_avg:95.46ms
step:80/1770 train_time:7637ms step_avg:95.47ms
step:81/1770 train_time:7733ms step_avg:95.47ms
step:82/1770 train_time:7829ms step_avg:95.48ms
step:83/1770 train_time:7926ms step_avg:95.49ms
step:84/1770 train_time:8022ms step_avg:95.50ms
step:85/1770 train_time:8118ms step_avg:95.51ms
step:86/1770 train_time:8214ms step_avg:95.51ms
step:87/1770 train_time:8310ms step_avg:95.52ms
step:88/1770 train_time:8406ms step_avg:95.52ms
step:89/1770 train_time:8503ms step_avg:95.54ms
step:90/1770 train_time:8599ms step_avg:95.55ms
step:91/1770 train_time:8695ms step_avg:95.55ms
step:92/1770 train_time:8791ms step_avg:95.55ms
step:93/1770 train_time:8887ms step_avg:95.56ms
step:94/1770 train_time:8983ms step_avg:95.56ms
step:95/1770 train_time:9079ms step_avg:95.57ms
step:96/1770 train_time:9175ms step_avg:95.57ms
step:97/1770 train_time:9270ms step_avg:95.57ms
step:98/1770 train_time:9367ms step_avg:95.58ms
step:99/1770 train_time:9463ms step_avg:95.59ms
step:100/1770 train_time:9560ms step_avg:95.60ms
step:101/1770 train_time:9656ms step_avg:95.60ms
step:102/1770 train_time:9752ms step_avg:95.61ms
step:103/1770 train_time:9848ms step_avg:95.61ms
step:104/1770 train_time:9944ms step_avg:95.61ms
step:105/1770 train_time:10041ms step_avg:95.62ms
step:106/1770 train_time:10137ms step_avg:95.63ms
step:107/1770 train_time:10233ms step_avg:95.64ms
step:108/1770 train_time:10329ms step_avg:95.64ms
step:109/1770 train_time:10425ms step_avg:95.64ms
step:110/1770 train_time:10521ms step_avg:95.65ms
step:111/1770 train_time:10617ms step_avg:95.64ms
step:112/1770 train_time:10713ms step_avg:95.65ms
step:113/1770 train_time:10809ms step_avg:95.65ms
step:114/1770 train_time:10905ms step_avg:95.66ms
step:115/1770 train_time:11001ms step_avg:95.66ms
step:116/1770 train_time:11098ms step_avg:95.67ms
step:117/1770 train_time:11194ms step_avg:95.67ms
step:118/1770 train_time:11290ms step_avg:95.67ms
step:119/1770 train_time:11385ms step_avg:95.68ms
step:120/1770 train_time:11482ms step_avg:95.68ms
step:121/1770 train_time:11578ms step_avg:95.68ms
step:122/1770 train_time:11674ms step_avg:95.69ms
step:123/1770 train_time:11770ms step_avg:95.69ms
step:124/1770 train_time:11866ms step_avg:95.69ms
step:125/1770 train_time:11962ms step_avg:95.70ms
step:125/1770 val_loss:4.6398 train_time:12052ms step_avg:96.42ms
step:126/1770 train_time:12075ms step_avg:95.83ms
step:127/1770 train_time:12161ms step_avg:95.75ms
step:128/1770 train_time:12262ms step_avg:95.80ms
step:129/1770 train_time:12361ms step_avg:95.82ms
step:130/1770 train_time:12457ms step_avg:95.82ms
step:131/1770 train_time:12553ms step_avg:95.82ms
step:132/1770 train_time:12649ms step_avg:95.82ms
step:133/1770 train_time:12745ms step_avg:95.83ms
step:134/1770 train_time:12842ms step_avg:95.84ms
step:135/1770 train_time:12938ms step_avg:95.84ms
step:136/1770 train_time:13035ms step_avg:95.84ms
step:137/1770 train_time:13131ms step_avg:95.85ms
step:138/1770 train_time:13227ms step_avg:95.85ms
step:139/1770 train_time:13324ms step_avg:95.86ms
step:140/1770 train_time:13422ms step_avg:95.87ms
step:141/1770 train_time:13518ms step_avg:95.88ms
step:142/1770 train_time:13614ms step_avg:95.88ms
step:143/1770 train_time:13711ms step_avg:95.88ms
step:144/1770 train_time:13808ms step_avg:95.89ms
step:145/1770 train_time:13904ms step_avg:95.89ms
step:146/1770 train_time:14001ms step_avg:95.90ms
step:147/1770 train_time:14098ms step_avg:95.90ms
step:148/1770 train_time:14194ms step_avg:95.91ms
step:149/1770 train_time:14290ms step_avg:95.91ms
step:150/1770 train_time:14387ms step_avg:95.91ms
step:151/1770 train_time:14483ms step_avg:95.92ms
step:152/1770 train_time:14580ms step_avg:95.92ms
step:153/1770 train_time:14677ms step_avg:95.93ms
step:154/1770 train_time:14773ms step_avg:95.93ms
step:155/1770 train_time:14870ms step_avg:95.93ms
step:156/1770 train_time:14966ms step_avg:95.94ms
step:157/1770 train_time:15063ms step_avg:95.94ms
step:158/1770 train_time:15161ms step_avg:95.96ms
step:159/1770 train_time:15259ms step_avg:95.97ms
step:160/1770 train_time:15356ms step_avg:95.97ms
step:161/1770 train_time:15452ms step_avg:95.98ms
step:162/1770 train_time:15548ms step_avg:95.98ms
step:163/1770 train_time:15645ms step_avg:95.98ms
step:164/1770 train_time:15742ms step_avg:95.99ms
step:165/1770 train_time:15838ms step_avg:95.99ms
step:166/1770 train_time:15935ms step_avg:95.99ms
step:167/1770 train_time:16031ms step_avg:96.00ms
step:168/1770 train_time:16128ms step_avg:96.00ms
step:169/1770 train_time:16225ms step_avg:96.00ms
step:170/1770 train_time:16321ms step_avg:96.01ms
step:171/1770 train_time:16419ms step_avg:96.02ms
step:172/1770 train_time:16515ms step_avg:96.02ms
step:173/1770 train_time:16612ms step_avg:96.02ms
step:174/1770 train_time:16708ms step_avg:96.03ms
step:175/1770 train_time:16805ms step_avg:96.03ms
step:176/1770 train_time:16902ms step_avg:96.03ms
step:177/1770 train_time:16999ms step_avg:96.04ms
step:178/1770 train_time:17095ms step_avg:96.04ms
step:179/1770 train_time:17192ms step_avg:96.04ms
step:180/1770 train_time:17288ms step_avg:96.05ms
step:181/1770 train_time:17385ms step_avg:96.05ms
step:182/1770 train_time:17481ms step_avg:96.05ms
step:183/1770 train_time:17578ms step_avg:96.06ms
step:184/1770 train_time:17675ms step_avg:96.06ms
step:185/1770 train_time:17771ms step_avg:96.06ms
step:186/1770 train_time:17868ms step_avg:96.06ms
step:187/1770 train_time:17964ms step_avg:96.07ms
step:188/1770 train_time:18062ms step_avg:96.07ms
step:189/1770 train_time:18159ms step_avg:96.08ms
step:190/1770 train_time:18255ms step_avg:96.08ms
step:191/1770 train_time:18353ms step_avg:96.09ms
step:192/1770 train_time:18449ms step_avg:96.09ms
step:193/1770 train_time:18546ms step_avg:96.09ms
step:194/1770 train_time:18642ms step_avg:96.10ms
step:195/1770 train_time:18739ms step_avg:96.10ms
step:196/1770 train_time:18836ms step_avg:96.10ms
step:197/1770 train_time:18932ms step_avg:96.10ms
step:198/1770 train_time:19028ms step_avg:96.10ms
step:199/1770 train_time:19125ms step_avg:96.11ms
step:200/1770 train_time:19222ms step_avg:96.11ms
step:201/1770 train_time:19320ms step_avg:96.12ms
step:202/1770 train_time:19417ms step_avg:96.12ms
step:203/1770 train_time:19513ms step_avg:96.12ms
step:204/1770 train_time:19611ms step_avg:96.13ms
step:205/1770 train_time:19707ms step_avg:96.13ms
step:206/1770 train_time:19803ms step_avg:96.13ms
step:207/1770 train_time:19901ms step_avg:96.14ms
step:208/1770 train_time:19997ms step_avg:96.14ms
step:209/1770 train_time:20093ms step_avg:96.14ms
step:210/1770 train_time:20190ms step_avg:96.14ms
step:211/1770 train_time:20286ms step_avg:96.14ms
step:212/1770 train_time:20383ms step_avg:96.14ms
step:213/1770 train_time:20480ms step_avg:96.15ms
step:214/1770 train_time:20577ms step_avg:96.15ms
step:215/1770 train_time:20673ms step_avg:96.15ms
step:216/1770 train_time:20770ms step_avg:96.16ms
step:217/1770 train_time:20867ms step_avg:96.16ms
step:218/1770 train_time:20963ms step_avg:96.16ms
step:219/1770 train_time:21060ms step_avg:96.17ms
step:220/1770 train_time:21158ms step_avg:96.17ms
step:221/1770 train_time:21254ms step_avg:96.17ms
step:222/1770 train_time:21350ms step_avg:96.17ms
step:223/1770 train_time:21447ms step_avg:96.17ms
step:224/1770 train_time:21544ms step_avg:96.18ms
step:225/1770 train_time:21641ms step_avg:96.18ms
step:226/1770 train_time:21737ms step_avg:96.18ms
step:227/1770 train_time:21833ms step_avg:96.18ms
step:228/1770 train_time:21929ms step_avg:96.18ms
step:229/1770 train_time:22025ms step_avg:96.18ms
step:230/1770 train_time:22122ms step_avg:96.18ms
step:231/1770 train_time:22219ms step_avg:96.19ms
step:232/1770 train_time:22316ms step_avg:96.19ms
step:233/1770 train_time:22413ms step_avg:96.19ms
step:234/1770 train_time:22508ms step_avg:96.19ms
step:235/1770 train_time:22604ms step_avg:96.19ms
step:236/1770 train_time:22701ms step_avg:96.19ms
step:237/1770 train_time:22798ms step_avg:96.19ms
step:238/1770 train_time:22894ms step_avg:96.19ms
step:239/1770 train_time:22991ms step_avg:96.20ms
step:240/1770 train_time:23087ms step_avg:96.20ms
step:241/1770 train_time:23184ms step_avg:96.20ms
step:242/1770 train_time:23617ms step_avg:97.59ms
step:243/1770 train_time:23675ms step_avg:97.43ms
step:244/1770 train_time:23771ms step_avg:97.42ms
step:245/1770 train_time:23867ms step_avg:97.42ms
step:246/1770 train_time:23964ms step_avg:97.41ms
step:247/1770 train_time:24060ms step_avg:97.41ms
step:248/1770 train_time:24156ms step_avg:97.40ms
step:249/1770 train_time:24253ms step_avg:97.40ms
step:250/1770 train_time:24349ms step_avg:97.40ms
step:250/1770 val_loss:4.1046 train_time:24440ms step_avg:97.76ms
step:251/1770 train_time:24463ms step_avg:97.46ms
step:252/1770 train_time:24550ms step_avg:97.42ms
step:253/1770 train_time:24648ms step_avg:97.42ms
step:254/1770 train_time:24747ms step_avg:97.43ms
step:255/1770 train_time:24844ms step_avg:97.43ms
step:256/1770 train_time:24939ms step_avg:97.42ms
step:257/1770 train_time:25035ms step_avg:97.41ms
step:258/1770 train_time:25132ms step_avg:97.41ms
step:259/1770 train_time:25228ms step_avg:97.40ms
step:260/1770 train_time:25324ms step_avg:97.40ms
step:261/1770 train_time:25421ms step_avg:97.40ms
step:262/1770 train_time:25517ms step_avg:97.39ms
step:263/1770 train_time:25614ms step_avg:97.39ms
step:264/1770 train_time:25710ms step_avg:97.39ms
step:265/1770 train_time:25808ms step_avg:97.39ms
step:266/1770 train_time:25905ms step_avg:97.39ms
step:267/1770 train_time:26002ms step_avg:97.39ms
step:268/1770 train_time:26099ms step_avg:97.39ms
step:269/1770 train_time:26197ms step_avg:97.38ms
step:270/1770 train_time:26295ms step_avg:97.39ms
step:271/1770 train_time:26391ms step_avg:97.39ms
step:272/1770 train_time:26488ms step_avg:97.38ms
step:273/1770 train_time:26585ms step_avg:97.38ms
step:274/1770 train_time:26683ms step_avg:97.38ms
step:275/1770 train_time:26780ms step_avg:97.38ms
step:276/1770 train_time:26877ms step_avg:97.38ms
step:277/1770 train_time:26974ms step_avg:97.38ms
step:278/1770 train_time:27071ms step_avg:97.38ms
step:279/1770 train_time:27168ms step_avg:97.38ms
step:280/1770 train_time:27265ms step_avg:97.38ms
step:281/1770 train_time:27363ms step_avg:97.38ms
step:282/1770 train_time:27460ms step_avg:97.38ms
step:283/1770 train_time:27557ms step_avg:97.37ms
step:284/1770 train_time:27654ms step_avg:97.37ms
step:285/1770 train_time:27750ms step_avg:97.37ms
step:286/1770 train_time:27848ms step_avg:97.37ms
step:287/1770 train_time:27945ms step_avg:97.37ms
step:288/1770 train_time:28043ms step_avg:97.37ms
step:289/1770 train_time:28139ms step_avg:97.37ms
step:290/1770 train_time:28237ms step_avg:97.37ms
step:291/1770 train_time:28333ms step_avg:97.36ms
step:292/1770 train_time:28430ms step_avg:97.36ms
step:293/1770 train_time:28527ms step_avg:97.36ms
step:294/1770 train_time:28624ms step_avg:97.36ms
step:295/1770 train_time:28721ms step_avg:97.36ms
step:296/1770 train_time:28819ms step_avg:97.36ms
step:297/1770 train_time:28915ms step_avg:97.36ms
step:298/1770 train_time:29013ms step_avg:97.36ms
step:299/1770 train_time:29110ms step_avg:97.36ms
step:300/1770 train_time:29208ms step_avg:97.36ms
step:301/1770 train_time:29305ms step_avg:97.36ms
step:302/1770 train_time:29402ms step_avg:97.36ms
step:303/1770 train_time:29499ms step_avg:97.36ms
step:304/1770 train_time:29596ms step_avg:97.35ms
step:305/1770 train_time:29693ms step_avg:97.35ms
step:306/1770 train_time:29790ms step_avg:97.35ms
step:307/1770 train_time:29887ms step_avg:97.35ms
step:308/1770 train_time:29985ms step_avg:97.35ms
step:309/1770 train_time:30082ms step_avg:97.35ms
step:310/1770 train_time:30179ms step_avg:97.35ms
step:311/1770 train_time:30276ms step_avg:97.35ms
step:312/1770 train_time:30373ms step_avg:97.35ms
step:313/1770 train_time:30470ms step_avg:97.35ms
step:314/1770 train_time:30567ms step_avg:97.35ms
step:315/1770 train_time:30664ms step_avg:97.34ms
step:316/1770 train_time:30761ms step_avg:97.35ms
step:317/1770 train_time:30858ms step_avg:97.34ms
step:318/1770 train_time:30955ms step_avg:97.34ms
step:319/1770 train_time:31052ms step_avg:97.34ms
step:320/1770 train_time:31149ms step_avg:97.34ms
step:321/1770 train_time:31246ms step_avg:97.34ms
step:322/1770 train_time:31344ms step_avg:97.34ms
step:323/1770 train_time:31443ms step_avg:97.35ms
step:324/1770 train_time:31539ms step_avg:97.34ms
step:325/1770 train_time:31636ms step_avg:97.34ms
step:326/1770 train_time:31733ms step_avg:97.34ms
step:327/1770 train_time:31830ms step_avg:97.34ms
step:328/1770 train_time:31927ms step_avg:97.34ms
step:329/1770 train_time:32024ms step_avg:97.34ms
step:330/1770 train_time:32122ms step_avg:97.34ms
step:331/1770 train_time:32219ms step_avg:97.34ms
step:332/1770 train_time:32316ms step_avg:97.34ms
step:333/1770 train_time:32414ms step_avg:97.34ms
step:334/1770 train_time:32512ms step_avg:97.34ms
step:335/1770 train_time:32609ms step_avg:97.34ms
step:336/1770 train_time:32706ms step_avg:97.34ms
step:337/1770 train_time:32804ms step_avg:97.34ms
step:338/1770 train_time:32902ms step_avg:97.34ms
step:339/1770 train_time:32998ms step_avg:97.34ms
step:340/1770 train_time:33095ms step_avg:97.34ms
step:341/1770 train_time:33192ms step_avg:97.34ms
step:342/1770 train_time:33289ms step_avg:97.34ms
step:343/1770 train_time:33387ms step_avg:97.34ms
step:344/1770 train_time:33484ms step_avg:97.34ms
step:345/1770 train_time:33582ms step_avg:97.34ms
step:346/1770 train_time:33679ms step_avg:97.34ms
step:347/1770 train_time:33776ms step_avg:97.34ms
step:348/1770 train_time:33873ms step_avg:97.34ms
step:349/1770 train_time:33971ms step_avg:97.34ms
step:350/1770 train_time:34067ms step_avg:97.33ms
step:351/1770 train_time:34164ms step_avg:97.33ms
step:352/1770 train_time:34262ms step_avg:97.33ms
step:353/1770 train_time:34358ms step_avg:97.33ms
step:354/1770 train_time:34455ms step_avg:97.33ms
step:355/1770 train_time:34551ms step_avg:97.33ms
step:356/1770 train_time:34648ms step_avg:97.33ms
step:357/1770 train_time:34746ms step_avg:97.33ms
step:358/1770 train_time:34843ms step_avg:97.33ms
step:359/1770 train_time:34941ms step_avg:97.33ms
step:360/1770 train_time:35037ms step_avg:97.33ms
step:361/1770 train_time:35134ms step_avg:97.32ms
step:362/1770 train_time:35231ms step_avg:97.32ms
step:363/1770 train_time:35327ms step_avg:97.32ms
step:364/1770 train_time:35424ms step_avg:97.32ms
step:365/1770 train_time:35521ms step_avg:97.32ms
step:366/1770 train_time:35618ms step_avg:97.32ms
step:367/1770 train_time:35715ms step_avg:97.32ms
step:368/1770 train_time:35812ms step_avg:97.32ms
step:369/1770 train_time:35910ms step_avg:97.32ms
step:370/1770 train_time:36007ms step_avg:97.32ms
step:371/1770 train_time:36104ms step_avg:97.32ms
step:372/1770 train_time:36201ms step_avg:97.31ms
step:373/1770 train_time:36298ms step_avg:97.31ms
step:374/1770 train_time:36395ms step_avg:97.31ms
step:375/1770 train_time:36491ms step_avg:97.31ms
step:375/1770 val_loss:3.8995 train_time:36583ms step_avg:97.55ms
step:376/1770 train_time:36604ms step_avg:97.35ms
step:377/1770 train_time:36693ms step_avg:97.33ms
step:378/1770 train_time:36792ms step_avg:97.33ms
step:379/1770 train_time:36889ms step_avg:97.33ms
step:380/1770 train_time:36986ms step_avg:97.33ms
step:381/1770 train_time:37083ms step_avg:97.33ms
step:382/1770 train_time:37180ms step_avg:97.33ms
step:383/1770 train_time:37277ms step_avg:97.33ms
step:384/1770 train_time:37374ms step_avg:97.33ms
step:385/1770 train_time:37470ms step_avg:97.33ms
step:386/1770 train_time:37567ms step_avg:97.32ms
step:387/1770 train_time:37664ms step_avg:97.32ms
step:388/1770 train_time:37762ms step_avg:97.32ms
step:389/1770 train_time:37860ms step_avg:97.33ms
step:390/1770 train_time:37957ms step_avg:97.33ms
step:391/1770 train_time:38053ms step_avg:97.32ms
step:392/1770 train_time:38150ms step_avg:97.32ms
step:393/1770 train_time:38247ms step_avg:97.32ms
step:394/1770 train_time:38344ms step_avg:97.32ms
step:395/1770 train_time:38442ms step_avg:97.32ms
step:396/1770 train_time:38541ms step_avg:97.33ms
step:397/1770 train_time:38641ms step_avg:97.33ms
step:398/1770 train_time:38740ms step_avg:97.34ms
step:399/1770 train_time:38839ms step_avg:97.34ms
step:400/1770 train_time:38938ms step_avg:97.34ms
step:401/1770 train_time:39037ms step_avg:97.35ms
step:402/1770 train_time:39136ms step_avg:97.35ms
step:403/1770 train_time:39235ms step_avg:97.36ms
step:404/1770 train_time:39334ms step_avg:97.36ms
step:405/1770 train_time:39433ms step_avg:97.36ms
step:406/1770 train_time:39532ms step_avg:97.37ms
step:407/1770 train_time:39630ms step_avg:97.37ms
step:408/1770 train_time:39730ms step_avg:97.38ms
step:409/1770 train_time:39830ms step_avg:97.38ms
step:410/1770 train_time:39929ms step_avg:97.39ms
step:411/1770 train_time:40028ms step_avg:97.39ms
step:412/1770 train_time:40127ms step_avg:97.39ms
step:413/1770 train_time:40226ms step_avg:97.40ms
step:414/1770 train_time:40325ms step_avg:97.40ms
step:415/1770 train_time:40425ms step_avg:97.41ms
step:416/1770 train_time:40525ms step_avg:97.42ms
step:417/1770 train_time:40625ms step_avg:97.42ms
step:418/1770 train_time:40724ms step_avg:97.43ms
step:419/1770 train_time:40824ms step_avg:97.43ms
step:420/1770 train_time:40924ms step_avg:97.44ms
step:421/1770 train_time:41025ms step_avg:97.45ms
step:422/1770 train_time:41124ms step_avg:97.45ms
step:423/1770 train_time:41225ms step_avg:97.46ms
step:424/1770 train_time:41324ms step_avg:97.46ms
step:425/1770 train_time:41424ms step_avg:97.47ms
step:426/1770 train_time:41524ms step_avg:97.47ms
step:427/1770 train_time:41624ms step_avg:97.48ms
step:428/1770 train_time:41723ms step_avg:97.48ms
step:429/1770 train_time:41823ms step_avg:97.49ms
step:430/1770 train_time:41923ms step_avg:97.49ms
step:431/1770 train_time:42023ms step_avg:97.50ms
step:432/1770 train_time:42122ms step_avg:97.51ms
step:433/1770 train_time:42222ms step_avg:97.51ms
step:434/1770 train_time:42322ms step_avg:97.52ms
step:435/1770 train_time:42422ms step_avg:97.52ms
step:436/1770 train_time:42521ms step_avg:97.53ms
step:437/1770 train_time:42621ms step_avg:97.53ms
step:438/1770 train_time:42719ms step_avg:97.53ms
step:439/1770 train_time:42819ms step_avg:97.54ms
step:440/1770 train_time:42918ms step_avg:97.54ms
step:441/1770 train_time:43016ms step_avg:97.54ms
step:442/1770 train_time:43115ms step_avg:97.55ms
step:443/1770 train_time:43214ms step_avg:97.55ms
step:444/1770 train_time:43312ms step_avg:97.55ms
step:445/1770 train_time:43411ms step_avg:97.55ms
step:446/1770 train_time:43510ms step_avg:97.56ms
step:447/1770 train_time:43609ms step_avg:97.56ms
step:448/1770 train_time:43708ms step_avg:97.56ms
step:449/1770 train_time:43807ms step_avg:97.57ms
step:450/1770 train_time:43906ms step_avg:97.57ms
step:451/1770 train_time:44006ms step_avg:97.57ms
step:452/1770 train_time:44105ms step_avg:97.58ms
step:453/1770 train_time:44205ms step_avg:97.58ms
step:454/1770 train_time:44306ms step_avg:97.59ms
step:455/1770 train_time:44406ms step_avg:97.59ms
step:456/1770 train_time:44506ms step_avg:97.60ms
step:457/1770 train_time:44606ms step_avg:97.61ms
step:458/1770 train_time:44707ms step_avg:97.61ms
step:459/1770 train_time:44808ms step_avg:97.62ms
step:460/1770 train_time:44907ms step_avg:97.62ms
step:461/1770 train_time:45006ms step_avg:97.63ms
step:462/1770 train_time:45105ms step_avg:97.63ms
step:463/1770 train_time:45204ms step_avg:97.63ms
step:464/1770 train_time:45304ms step_avg:97.64ms
step:465/1770 train_time:45403ms step_avg:97.64ms
step:466/1770 train_time:45503ms step_avg:97.64ms
step:467/1770 train_time:45602ms step_avg:97.65ms
step:468/1770 train_time:45703ms step_avg:97.66ms
step:469/1770 train_time:45803ms step_avg:97.66ms
step:470/1770 train_time:45903ms step_avg:97.67ms
step:471/1770 train_time:46003ms step_avg:97.67ms
step:472/1770 train_time:46103ms step_avg:97.68ms
step:473/1770 train_time:46204ms step_avg:97.68ms
step:474/1770 train_time:46303ms step_avg:97.69ms
step:475/1770 train_time:46403ms step_avg:97.69ms
step:476/1770 train_time:46502ms step_avg:97.69ms
step:477/1770 train_time:46601ms step_avg:97.70ms
step:478/1770 train_time:46701ms step_avg:97.70ms
step:479/1770 train_time:46801ms step_avg:97.71ms
step:480/1770 train_time:46901ms step_avg:97.71ms
step:481/1770 train_time:47001ms step_avg:97.72ms
step:482/1770 train_time:47100ms step_avg:97.72ms
step:483/1770 train_time:47199ms step_avg:97.72ms
step:484/1770 train_time:47297ms step_avg:97.72ms
step:485/1770 train_time:47397ms step_avg:97.73ms
step:486/1770 train_time:47496ms step_avg:97.73ms
step:487/1770 train_time:47594ms step_avg:97.73ms
step:488/1770 train_time:47693ms step_avg:97.73ms
step:489/1770 train_time:47793ms step_avg:97.74ms
step:490/1770 train_time:47891ms step_avg:97.74ms
step:491/1770 train_time:47990ms step_avg:97.74ms
step:492/1770 train_time:48089ms step_avg:97.74ms
step:493/1770 train_time:48188ms step_avg:97.74ms
step:494/1770 train_time:48287ms step_avg:97.75ms
step:495/1770 train_time:48385ms step_avg:97.75ms
step:496/1770 train_time:48485ms step_avg:97.75ms
step:497/1770 train_time:48584ms step_avg:97.75ms
step:498/1770 train_time:48684ms step_avg:97.76ms
step:499/1770 train_time:48784ms step_avg:97.76ms
step:500/1770 train_time:48885ms step_avg:97.77ms
step:500/1770 val_loss:3.7493 train_time:48980ms step_avg:97.96ms
step:501/1770 train_time:49002ms step_avg:97.81ms
step:502/1770 train_time:49093ms step_avg:97.79ms
step:503/1770 train_time:49193ms step_avg:97.80ms
step:504/1770 train_time:49293ms step_avg:97.80ms
step:505/1770 train_time:49391ms step_avg:97.80ms
step:506/1770 train_time:49490ms step_avg:97.81ms
step:507/1770 train_time:49589ms step_avg:97.81ms
step:508/1770 train_time:49688ms step_avg:97.81ms
step:509/1770 train_time:49787ms step_avg:97.81ms
step:510/1770 train_time:49886ms step_avg:97.82ms
step:511/1770 train_time:49985ms step_avg:97.82ms
step:512/1770 train_time:50086ms step_avg:97.82ms
step:513/1770 train_time:50187ms step_avg:97.83ms
step:514/1770 train_time:50288ms step_avg:97.84ms
step:515/1770 train_time:50389ms step_avg:97.84ms
step:516/1770 train_time:50489ms step_avg:97.85ms
step:517/1770 train_time:50589ms step_avg:97.85ms
step:518/1770 train_time:50688ms step_avg:97.85ms
step:519/1770 train_time:50787ms step_avg:97.86ms
step:520/1770 train_time:50886ms step_avg:97.86ms
step:521/1770 train_time:50986ms step_avg:97.86ms
step:522/1770 train_time:51085ms step_avg:97.86ms
step:523/1770 train_time:51185ms step_avg:97.87ms
step:524/1770 train_time:51285ms step_avg:97.87ms
step:525/1770 train_time:51386ms step_avg:97.88ms
step:526/1770 train_time:51487ms step_avg:97.88ms
step:527/1770 train_time:51587ms step_avg:97.89ms
step:528/1770 train_time:51687ms step_avg:97.89ms
step:529/1770 train_time:51787ms step_avg:97.90ms
step:530/1770 train_time:51887ms step_avg:97.90ms
step:531/1770 train_time:51987ms step_avg:97.90ms
step:532/1770 train_time:52087ms step_avg:97.91ms
step:533/1770 train_time:52188ms step_avg:97.91ms
step:534/1770 train_time:52290ms step_avg:97.92ms
step:535/1770 train_time:52390ms step_avg:97.93ms
step:536/1770 train_time:52490ms step_avg:97.93ms
step:537/1770 train_time:52590ms step_avg:97.93ms
step:538/1770 train_time:52690ms step_avg:97.94ms
step:539/1770 train_time:52790ms step_avg:97.94ms
step:540/1770 train_time:52889ms step_avg:97.94ms
step:541/1770 train_time:52989ms step_avg:97.95ms
step:542/1770 train_time:53089ms step_avg:97.95ms
step:543/1770 train_time:53188ms step_avg:97.95ms
step:544/1770 train_time:53289ms step_avg:97.96ms
step:545/1770 train_time:53389ms step_avg:97.96ms
step:546/1770 train_time:53489ms step_avg:97.97ms
step:547/1770 train_time:53589ms step_avg:97.97ms
step:548/1770 train_time:53689ms step_avg:97.97ms
step:549/1770 train_time:53789ms step_avg:97.98ms
step:550/1770 train_time:53889ms step_avg:97.98ms
step:551/1770 train_time:53989ms step_avg:97.98ms
step:552/1770 train_time:54089ms step_avg:97.99ms
step:553/1770 train_time:54189ms step_avg:97.99ms
step:554/1770 train_time:54289ms step_avg:97.99ms
step:555/1770 train_time:54389ms step_avg:98.00ms
step:556/1770 train_time:54489ms step_avg:98.00ms
step:557/1770 train_time:54589ms step_avg:98.01ms
step:558/1770 train_time:54689ms step_avg:98.01ms
step:559/1770 train_time:54789ms step_avg:98.01ms
step:560/1770 train_time:54890ms step_avg:98.02ms
step:561/1770 train_time:54990ms step_avg:98.02ms
step:562/1770 train_time:55089ms step_avg:98.02ms
step:563/1770 train_time:55189ms step_avg:98.03ms
step:564/1770 train_time:55288ms step_avg:98.03ms
step:565/1770 train_time:55388ms step_avg:98.03ms
step:566/1770 train_time:55487ms step_avg:98.03ms
step:567/1770 train_time:55587ms step_avg:98.04ms
step:568/1770 train_time:55687ms step_avg:98.04ms
step:569/1770 train_time:55788ms step_avg:98.05ms
step:570/1770 train_time:55888ms step_avg:98.05ms
step:571/1770 train_time:55989ms step_avg:98.05ms
step:572/1770 train_time:56090ms step_avg:98.06ms
step:573/1770 train_time:56190ms step_avg:98.06ms
step:574/1770 train_time:56290ms step_avg:98.07ms
step:575/1770 train_time:56390ms step_avg:98.07ms
step:576/1770 train_time:56490ms step_avg:98.07ms
step:577/1770 train_time:56589ms step_avg:98.07ms
step:578/1770 train_time:56689ms step_avg:98.08ms
step:579/1770 train_time:56789ms step_avg:98.08ms
step:580/1770 train_time:56889ms step_avg:98.08ms
step:581/1770 train_time:56989ms step_avg:98.09ms
step:582/1770 train_time:57089ms step_avg:98.09ms
step:583/1770 train_time:57189ms step_avg:98.10ms
step:584/1770 train_time:57290ms step_avg:98.10ms
step:585/1770 train_time:57389ms step_avg:98.10ms
step:586/1770 train_time:57490ms step_avg:98.10ms
step:587/1770 train_time:57589ms step_avg:98.11ms
step:588/1770 train_time:57689ms step_avg:98.11ms
step:589/1770 train_time:57789ms step_avg:98.11ms
step:590/1770 train_time:57888ms step_avg:98.12ms
step:591/1770 train_time:57989ms step_avg:98.12ms
step:592/1770 train_time:58089ms step_avg:98.12ms
step:593/1770 train_time:58189ms step_avg:98.13ms
step:594/1770 train_time:58289ms step_avg:98.13ms
step:595/1770 train_time:58389ms step_avg:98.13ms
step:596/1770 train_time:58490ms step_avg:98.14ms
step:597/1770 train_time:58589ms step_avg:98.14ms
step:598/1770 train_time:58689ms step_avg:98.14ms
step:599/1770 train_time:58789ms step_avg:98.14ms
step:600/1770 train_time:58889ms step_avg:98.15ms
step:601/1770 train_time:58989ms step_avg:98.15ms
step:602/1770 train_time:59090ms step_avg:98.16ms
step:603/1770 train_time:59190ms step_avg:98.16ms
step:604/1770 train_time:59290ms step_avg:98.16ms
step:605/1770 train_time:59390ms step_avg:98.17ms
step:606/1770 train_time:59490ms step_avg:98.17ms
step:607/1770 train_time:59589ms step_avg:98.17ms
step:608/1770 train_time:59689ms step_avg:98.17ms
step:609/1770 train_time:59790ms step_avg:98.18ms
step:610/1770 train_time:59890ms step_avg:98.18ms
step:611/1770 train_time:59989ms step_avg:98.18ms
step:612/1770 train_time:60090ms step_avg:98.19ms
step:613/1770 train_time:60189ms step_avg:98.19ms
step:614/1770 train_time:60289ms step_avg:98.19ms
step:615/1770 train_time:60390ms step_avg:98.19ms
step:616/1770 train_time:60490ms step_avg:98.20ms
step:617/1770 train_time:60589ms step_avg:98.20ms
step:618/1770 train_time:60690ms step_avg:98.20ms
step:619/1770 train_time:60789ms step_avg:98.21ms
step:620/1770 train_time:60889ms step_avg:98.21ms
step:621/1770 train_time:60988ms step_avg:98.21ms
step:622/1770 train_time:61088ms step_avg:98.21ms
step:623/1770 train_time:61189ms step_avg:98.22ms
step:624/1770 train_time:61289ms step_avg:98.22ms
step:625/1770 train_time:61389ms step_avg:98.22ms
step:625/1770 val_loss:3.6646 train_time:61483ms step_avg:98.37ms
step:626/1770 train_time:61504ms step_avg:98.25ms
step:627/1770 train_time:61595ms step_avg:98.24ms
step:628/1770 train_time:61695ms step_avg:98.24ms
step:629/1770 train_time:61795ms step_avg:98.24ms
step:630/1770 train_time:61894ms step_avg:98.24ms
step:631/1770 train_time:61994ms step_avg:98.25ms
step:632/1770 train_time:62093ms step_avg:98.25ms
step:633/1770 train_time:62192ms step_avg:98.25ms
step:634/1770 train_time:62291ms step_avg:98.25ms
step:635/1770 train_time:62390ms step_avg:98.25ms
step:636/1770 train_time:62489ms step_avg:98.25ms
step:637/1770 train_time:62589ms step_avg:98.26ms
step:638/1770 train_time:62688ms step_avg:98.26ms
step:639/1770 train_time:62788ms step_avg:98.26ms
step:640/1770 train_time:62888ms step_avg:98.26ms
step:641/1770 train_time:62988ms step_avg:98.27ms
step:642/1770 train_time:63089ms step_avg:98.27ms
step:643/1770 train_time:63189ms step_avg:98.27ms
step:644/1770 train_time:63290ms step_avg:98.28ms
step:645/1770 train_time:63389ms step_avg:98.28ms
step:646/1770 train_time:63489ms step_avg:98.28ms
step:647/1770 train_time:63588ms step_avg:98.28ms
step:648/1770 train_time:63688ms step_avg:98.28ms
step:649/1770 train_time:63787ms step_avg:98.29ms
step:650/1770 train_time:63887ms step_avg:98.29ms
step:651/1770 train_time:63987ms step_avg:98.29ms
step:652/1770 train_time:64087ms step_avg:98.29ms
step:653/1770 train_time:64187ms step_avg:98.30ms
step:654/1770 train_time:64288ms step_avg:98.30ms
step:655/1770 train_time:64388ms step_avg:98.30ms
step:656/1770 train_time:64488ms step_avg:98.31ms
step:657/1770 train_time:64588ms step_avg:98.31ms
step:658/1770 train_time:64690ms step_avg:98.31ms
step:659/1770 train_time:64791ms step_avg:98.32ms
step:660/1770 train_time:64892ms step_avg:98.32ms
step:661/1770 train_time:64993ms step_avg:98.33ms
step:662/1770 train_time:65094ms step_avg:98.33ms
step:663/1770 train_time:65195ms step_avg:98.33ms
step:664/1770 train_time:65297ms step_avg:98.34ms
step:665/1770 train_time:65398ms step_avg:98.34ms
step:666/1770 train_time:65499ms step_avg:98.35ms
step:667/1770 train_time:65600ms step_avg:98.35ms
step:668/1770 train_time:65701ms step_avg:98.35ms
step:669/1770 train_time:65802ms step_avg:98.36ms
step:670/1770 train_time:65903ms step_avg:98.36ms
step:671/1770 train_time:66004ms step_avg:98.37ms
step:672/1770 train_time:66108ms step_avg:98.37ms
step:673/1770 train_time:66209ms step_avg:98.38ms
step:674/1770 train_time:66310ms step_avg:98.38ms
step:675/1770 train_time:66412ms step_avg:98.39ms
step:676/1770 train_time:66512ms step_avg:98.39ms
step:677/1770 train_time:66613ms step_avg:98.39ms
step:678/1770 train_time:66713ms step_avg:98.40ms
step:679/1770 train_time:66814ms step_avg:98.40ms
step:680/1770 train_time:66915ms step_avg:98.40ms
step:681/1770 train_time:67016ms step_avg:98.41ms
step:682/1770 train_time:67118ms step_avg:98.41ms
step:683/1770 train_time:67218ms step_avg:98.42ms
step:684/1770 train_time:67320ms step_avg:98.42ms
step:685/1770 train_time:67421ms step_avg:98.42ms
step:686/1770 train_time:67521ms step_avg:98.43ms
step:687/1770 train_time:67622ms step_avg:98.43ms
step:688/1770 train_time:67724ms step_avg:98.44ms
step:689/1770 train_time:67827ms step_avg:98.44ms
step:690/1770 train_time:67929ms step_avg:98.45ms
step:691/1770 train_time:68030ms step_avg:98.45ms
step:692/1770 train_time:68131ms step_avg:98.45ms
step:693/1770 train_time:68232ms step_avg:98.46ms
step:694/1770 train_time:68333ms step_avg:98.46ms
step:695/1770 train_time:68434ms step_avg:98.47ms
step:696/1770 train_time:68535ms step_avg:98.47ms
step:697/1770 train_time:68636ms step_avg:98.47ms
step:698/1770 train_time:68738ms step_avg:98.48ms
step:699/1770 train_time:68838ms step_avg:98.48ms
step:700/1770 train_time:68939ms step_avg:98.48ms
step:701/1770 train_time:69040ms step_avg:98.49ms
step:702/1770 train_time:69140ms step_avg:98.49ms
step:703/1770 train_time:69241ms step_avg:98.49ms
step:704/1770 train_time:69342ms step_avg:98.50ms
step:705/1770 train_time:69443ms step_avg:98.50ms
step:706/1770 train_time:69545ms step_avg:98.51ms
step:707/1770 train_time:69648ms step_avg:98.51ms
step:708/1770 train_time:69749ms step_avg:98.52ms
step:709/1770 train_time:69851ms step_avg:98.52ms
step:710/1770 train_time:69952ms step_avg:98.52ms
step:711/1770 train_time:70053ms step_avg:98.53ms
step:712/1770 train_time:70154ms step_avg:98.53ms
step:713/1770 train_time:70255ms step_avg:98.53ms
step:714/1770 train_time:70355ms step_avg:98.54ms
step:715/1770 train_time:70456ms step_avg:98.54ms
step:716/1770 train_time:70557ms step_avg:98.54ms
step:717/1770 train_time:70658ms step_avg:98.55ms
step:718/1770 train_time:70759ms step_avg:98.55ms
step:719/1770 train_time:70860ms step_avg:98.55ms
step:720/1770 train_time:70961ms step_avg:98.56ms
step:721/1770 train_time:71061ms step_avg:98.56ms
step:722/1770 train_time:71163ms step_avg:98.56ms
step:723/1770 train_time:71266ms step_avg:98.57ms
step:724/1770 train_time:71368ms step_avg:98.57ms
step:725/1770 train_time:71469ms step_avg:98.58ms
step:726/1770 train_time:71570ms step_avg:98.58ms
step:727/1770 train_time:71672ms step_avg:98.59ms
step:728/1770 train_time:71774ms step_avg:98.59ms
step:729/1770 train_time:71874ms step_avg:98.59ms
step:730/1770 train_time:71975ms step_avg:98.60ms
step:731/1770 train_time:72076ms step_avg:98.60ms
step:732/1770 train_time:72177ms step_avg:98.60ms
step:733/1770 train_time:72278ms step_avg:98.61ms
step:734/1770 train_time:72379ms step_avg:98.61ms
step:735/1770 train_time:72480ms step_avg:98.61ms
step:736/1770 train_time:72580ms step_avg:98.61ms
step:737/1770 train_time:72681ms step_avg:98.62ms
step:738/1770 train_time:72782ms step_avg:98.62ms
step:739/1770 train_time:72883ms step_avg:98.62ms
step:740/1770 train_time:72985ms step_avg:98.63ms
step:741/1770 train_time:73087ms step_avg:98.63ms
step:742/1770 train_time:73189ms step_avg:98.64ms
step:743/1770 train_time:73290ms step_avg:98.64ms
step:744/1770 train_time:73392ms step_avg:98.64ms
step:745/1770 train_time:73493ms step_avg:98.65ms
step:746/1770 train_time:73594ms step_avg:98.65ms
step:747/1770 train_time:73695ms step_avg:98.65ms
step:748/1770 train_time:73797ms step_avg:98.66ms
step:749/1770 train_time:73898ms step_avg:98.66ms
step:750/1770 train_time:73999ms step_avg:98.67ms
step:750/1770 val_loss:3.5973 train_time:74094ms step_avg:98.79ms
step:751/1770 train_time:74114ms step_avg:98.69ms
step:752/1770 train_time:74207ms step_avg:98.68ms
step:753/1770 train_time:74310ms step_avg:98.69ms
step:754/1770 train_time:74411ms step_avg:98.69ms
step:755/1770 train_time:74512ms step_avg:98.69ms
step:756/1770 train_time:74613ms step_avg:98.69ms
step:757/1770 train_time:74713ms step_avg:98.70ms
step:758/1770 train_time:74814ms step_avg:98.70ms
step:759/1770 train_time:74914ms step_avg:98.70ms
step:760/1770 train_time:75015ms step_avg:98.70ms
step:761/1770 train_time:75118ms step_avg:98.71ms
step:762/1770 train_time:75221ms step_avg:98.72ms
step:763/1770 train_time:75324ms step_avg:98.72ms
step:764/1770 train_time:75425ms step_avg:98.72ms
step:765/1770 train_time:75526ms step_avg:98.73ms
step:766/1770 train_time:75626ms step_avg:98.73ms
step:767/1770 train_time:75727ms step_avg:98.73ms
step:768/1770 train_time:75828ms step_avg:98.73ms
step:769/1770 train_time:75928ms step_avg:98.74ms
step:770/1770 train_time:76029ms step_avg:98.74ms
step:771/1770 train_time:76130ms step_avg:98.74ms
step:772/1770 train_time:76231ms step_avg:98.74ms
step:773/1770 train_time:76332ms step_avg:98.75ms
step:774/1770 train_time:76433ms step_avg:98.75ms
step:775/1770 train_time:76536ms step_avg:98.76ms
step:776/1770 train_time:76636ms step_avg:98.76ms
step:777/1770 train_time:76739ms step_avg:98.76ms
step:778/1770 train_time:76841ms step_avg:98.77ms
step:779/1770 train_time:76942ms step_avg:98.77ms
step:780/1770 train_time:77043ms step_avg:98.77ms
step:781/1770 train_time:77144ms step_avg:98.78ms
step:782/1770 train_time:77245ms step_avg:98.78ms
step:783/1770 train_time:77346ms step_avg:98.78ms
step:784/1770 train_time:77446ms step_avg:98.78ms
step:785/1770 train_time:77548ms step_avg:98.79ms
step:786/1770 train_time:77649ms step_avg:98.79ms
step:787/1770 train_time:77750ms step_avg:98.79ms
step:788/1770 train_time:77851ms step_avg:98.80ms
step:789/1770 train_time:77952ms step_avg:98.80ms
step:790/1770 train_time:78054ms step_avg:98.80ms
step:791/1770 train_time:78155ms step_avg:98.81ms
step:792/1770 train_time:78257ms step_avg:98.81ms
step:793/1770 train_time:78359ms step_avg:98.81ms
step:794/1770 train_time:78462ms step_avg:98.82ms
step:795/1770 train_time:78564ms step_avg:98.82ms
step:796/1770 train_time:78666ms step_avg:98.83ms
step:797/1770 train_time:78767ms step_avg:98.83ms
step:798/1770 train_time:78868ms step_avg:98.83ms
step:799/1770 train_time:78969ms step_avg:98.84ms
step:800/1770 train_time:79071ms step_avg:98.84ms
step:801/1770 train_time:79171ms step_avg:98.84ms
step:802/1770 train_time:79273ms step_avg:98.84ms
step:803/1770 train_time:79374ms step_avg:98.85ms
step:804/1770 train_time:79476ms step_avg:98.85ms
step:805/1770 train_time:79578ms step_avg:98.85ms
step:806/1770 train_time:79679ms step_avg:98.86ms
step:807/1770 train_time:79781ms step_avg:98.86ms
step:808/1770 train_time:79883ms step_avg:98.86ms
step:809/1770 train_time:79985ms step_avg:98.87ms
step:810/1770 train_time:80086ms step_avg:98.87ms
step:811/1770 train_time:80187ms step_avg:98.87ms
step:812/1770 train_time:80289ms step_avg:98.88ms
step:813/1770 train_time:80392ms step_avg:98.88ms
step:814/1770 train_time:80493ms step_avg:98.89ms
step:815/1770 train_time:80594ms step_avg:98.89ms
step:816/1770 train_time:80695ms step_avg:98.89ms
step:817/1770 train_time:80796ms step_avg:98.89ms
step:818/1770 train_time:80898ms step_avg:98.90ms
step:819/1770 train_time:81000ms step_avg:98.90ms
step:820/1770 train_time:81101ms step_avg:98.90ms
step:821/1770 train_time:81202ms step_avg:98.91ms
step:822/1770 train_time:81304ms step_avg:98.91ms
step:823/1770 train_time:81405ms step_avg:98.91ms
step:824/1770 train_time:81506ms step_avg:98.92ms
step:825/1770 train_time:81608ms step_avg:98.92ms
step:826/1770 train_time:81708ms step_avg:98.92ms
step:827/1770 train_time:81810ms step_avg:98.92ms
step:828/1770 train_time:81911ms step_avg:98.93ms
step:829/1770 train_time:82012ms step_avg:98.93ms
step:830/1770 train_time:82113ms step_avg:98.93ms
step:831/1770 train_time:82214ms step_avg:98.93ms
step:832/1770 train_time:82316ms step_avg:98.94ms
step:833/1770 train_time:82419ms step_avg:98.94ms
step:834/1770 train_time:82521ms step_avg:98.95ms
step:835/1770 train_time:82623ms step_avg:98.95ms
step:836/1770 train_time:82725ms step_avg:98.95ms
step:837/1770 train_time:82826ms step_avg:98.96ms
step:838/1770 train_time:82927ms step_avg:98.96ms
step:839/1770 train_time:83029ms step_avg:98.96ms
step:840/1770 train_time:83130ms step_avg:98.96ms
step:841/1770 train_time:83231ms step_avg:98.97ms
step:842/1770 train_time:83332ms step_avg:98.97ms
step:843/1770 train_time:83434ms step_avg:98.97ms
step:844/1770 train_time:83534ms step_avg:98.97ms
step:845/1770 train_time:83636ms step_avg:98.98ms
step:846/1770 train_time:83738ms step_avg:98.98ms
step:847/1770 train_time:83841ms step_avg:98.99ms
step:848/1770 train_time:83943ms step_avg:98.99ms
step:849/1770 train_time:84045ms step_avg:98.99ms
step:850/1770 train_time:84145ms step_avg:98.99ms
step:851/1770 train_time:84246ms step_avg:99.00ms
step:852/1770 train_time:84347ms step_avg:99.00ms
step:853/1770 train_time:84448ms step_avg:99.00ms
step:854/1770 train_time:84549ms step_avg:99.00ms
step:855/1770 train_time:84650ms step_avg:99.01ms
step:856/1770 train_time:84751ms step_avg:99.01ms
step:857/1770 train_time:84852ms step_avg:99.01ms
step:858/1770 train_time:84953ms step_avg:99.01ms
step:859/1770 train_time:85053ms step_avg:99.01ms
step:860/1770 train_time:85154ms step_avg:99.02ms
step:861/1770 train_time:85256ms step_avg:99.02ms
step:862/1770 train_time:85358ms step_avg:99.02ms
step:863/1770 train_time:85460ms step_avg:99.03ms
step:864/1770 train_time:85562ms step_avg:99.03ms
step:865/1770 train_time:85664ms step_avg:99.03ms
step:866/1770 train_time:85766ms step_avg:99.04ms
step:867/1770 train_time:85867ms step_avg:99.04ms
step:868/1770 train_time:85969ms step_avg:99.04ms
step:869/1770 train_time:86070ms step_avg:99.05ms
step:870/1770 train_time:86171ms step_avg:99.05ms
step:871/1770 train_time:86272ms step_avg:99.05ms
step:872/1770 train_time:86373ms step_avg:99.05ms
step:873/1770 train_time:86473ms step_avg:99.05ms
step:874/1770 train_time:86574ms step_avg:99.05ms
step:875/1770 train_time:86675ms step_avg:99.06ms
step:875/1770 val_loss:3.5494 train_time:86770ms step_avg:99.17ms
step:876/1770 train_time:86791ms step_avg:99.08ms
step:877/1770 train_time:86883ms step_avg:99.07ms
step:878/1770 train_time:86986ms step_avg:99.07ms
step:879/1770 train_time:87088ms step_avg:99.08ms
step:880/1770 train_time:87190ms step_avg:99.08ms
step:881/1770 train_time:87290ms step_avg:99.08ms
step:882/1770 train_time:87391ms step_avg:99.08ms
step:883/1770 train_time:87492ms step_avg:99.09ms
step:884/1770 train_time:87593ms step_avg:99.09ms
step:885/1770 train_time:87693ms step_avg:99.09ms
step:886/1770 train_time:87797ms step_avg:99.09ms
step:887/1770 train_time:87901ms step_avg:99.10ms
step:888/1770 train_time:88002ms step_avg:99.10ms
step:889/1770 train_time:88104ms step_avg:99.10ms
step:890/1770 train_time:88205ms step_avg:99.11ms
step:891/1770 train_time:88306ms step_avg:99.11ms
step:892/1770 train_time:88407ms step_avg:99.11ms
step:893/1770 train_time:88508ms step_avg:99.11ms
step:894/1770 train_time:88609ms step_avg:99.12ms
step:895/1770 train_time:88710ms step_avg:99.12ms
step:896/1770 train_time:88811ms step_avg:99.12ms
step:897/1770 train_time:88912ms step_avg:99.12ms
step:898/1770 train_time:89014ms step_avg:99.12ms
step:899/1770 train_time:89116ms step_avg:99.13ms
step:900/1770 train_time:89219ms step_avg:99.13ms
step:901/1770 train_time:89321ms step_avg:99.14ms
step:902/1770 train_time:89423ms step_avg:99.14ms
step:903/1770 train_time:89524ms step_avg:99.14ms
step:904/1770 train_time:89625ms step_avg:99.14ms
step:905/1770 train_time:89726ms step_avg:99.14ms
step:906/1770 train_time:89826ms step_avg:99.15ms
step:907/1770 train_time:89928ms step_avg:99.15ms
step:908/1770 train_time:90029ms step_avg:99.15ms
step:909/1770 train_time:90130ms step_avg:99.15ms
step:910/1770 train_time:90231ms step_avg:99.15ms
step:911/1770 train_time:90332ms step_avg:99.16ms
step:912/1770 train_time:90433ms step_avg:99.16ms
step:913/1770 train_time:90534ms step_avg:99.16ms
step:914/1770 train_time:90635ms step_avg:99.16ms
step:915/1770 train_time:90737ms step_avg:99.17ms
step:916/1770 train_time:90839ms step_avg:99.17ms
step:917/1770 train_time:90941ms step_avg:99.17ms
step:918/1770 train_time:91043ms step_avg:99.18ms
step:919/1770 train_time:91144ms step_avg:99.18ms
step:920/1770 train_time:91248ms step_avg:99.18ms
step:921/1770 train_time:91350ms step_avg:99.19ms
step:922/1770 train_time:91453ms step_avg:99.19ms
step:923/1770 train_time:91555ms step_avg:99.19ms
step:924/1770 train_time:91658ms step_avg:99.20ms
step:925/1770 train_time:91760ms step_avg:99.20ms
step:926/1770 train_time:91862ms step_avg:99.20ms
step:927/1770 train_time:91965ms step_avg:99.21ms
step:928/1770 train_time:92068ms step_avg:99.21ms
step:929/1770 train_time:92170ms step_avg:99.21ms
step:930/1770 train_time:92272ms step_avg:99.22ms
step:931/1770 train_time:92374ms step_avg:99.22ms
step:932/1770 train_time:92477ms step_avg:99.22ms
step:933/1770 train_time:92580ms step_avg:99.23ms
step:934/1770 train_time:92683ms step_avg:99.23ms
step:935/1770 train_time:92785ms step_avg:99.24ms
step:936/1770 train_time:92888ms step_avg:99.24ms
step:937/1770 train_time:92990ms step_avg:99.24ms
step:938/1770 train_time:93092ms step_avg:99.25ms
step:939/1770 train_time:93195ms step_avg:99.25ms
step:940/1770 train_time:93297ms step_avg:99.25ms
step:941/1770 train_time:93402ms step_avg:99.26ms
step:942/1770 train_time:93504ms step_avg:99.26ms
step:943/1770 train_time:93608ms step_avg:99.27ms
step:944/1770 train_time:93709ms step_avg:99.27ms
step:945/1770 train_time:93812ms step_avg:99.27ms
step:946/1770 train_time:93915ms step_avg:99.28ms
step:947/1770 train_time:94018ms step_avg:99.28ms
step:948/1770 train_time:94122ms step_avg:99.28ms
step:949/1770 train_time:94224ms step_avg:99.29ms
step:950/1770 train_time:94327ms step_avg:99.29ms
step:951/1770 train_time:94429ms step_avg:99.29ms
step:952/1770 train_time:94532ms step_avg:99.30ms
step:953/1770 train_time:94634ms step_avg:99.30ms
step:954/1770 train_time:94736ms step_avg:99.30ms
step:955/1770 train_time:94840ms step_avg:99.31ms
step:956/1770 train_time:94942ms step_avg:99.31ms
step:957/1770 train_time:95045ms step_avg:99.32ms
step:958/1770 train_time:95147ms step_avg:99.32ms
step:959/1770 train_time:95250ms step_avg:99.32ms
step:960/1770 train_time:95352ms step_avg:99.32ms
step:961/1770 train_time:95454ms step_avg:99.33ms
step:962/1770 train_time:95557ms step_avg:99.33ms
step:963/1770 train_time:95660ms step_avg:99.34ms
step:964/1770 train_time:95763ms step_avg:99.34ms
step:965/1770 train_time:95865ms step_avg:99.34ms
step:966/1770 train_time:95967ms step_avg:99.35ms
step:967/1770 train_time:96070ms step_avg:99.35ms
step:968/1770 train_time:96173ms step_avg:99.35ms
step:969/1770 train_time:96276ms step_avg:99.36ms
step:970/1770 train_time:96379ms step_avg:99.36ms
step:971/1770 train_time:96482ms step_avg:99.36ms
step:972/1770 train_time:96584ms step_avg:99.37ms
step:973/1770 train_time:96686ms step_avg:99.37ms
step:974/1770 train_time:96788ms step_avg:99.37ms
step:975/1770 train_time:96891ms step_avg:99.38ms
step:976/1770 train_time:96994ms step_avg:99.38ms
step:977/1770 train_time:97096ms step_avg:99.38ms
step:978/1770 train_time:97199ms step_avg:99.39ms
step:979/1770 train_time:97303ms step_avg:99.39ms
step:980/1770 train_time:97406ms step_avg:99.39ms
step:981/1770 train_time:97509ms step_avg:99.40ms
step:982/1770 train_time:97611ms step_avg:99.40ms
step:983/1770 train_time:97713ms step_avg:99.40ms
step:984/1770 train_time:97816ms step_avg:99.41ms
step:985/1770 train_time:97920ms step_avg:99.41ms
step:986/1770 train_time:98023ms step_avg:99.41ms
step:987/1770 train_time:98125ms step_avg:99.42ms
step:988/1770 train_time:98228ms step_avg:99.42ms
step:989/1770 train_time:98331ms step_avg:99.42ms
step:990/1770 train_time:98433ms step_avg:99.43ms
step:991/1770 train_time:98536ms step_avg:99.43ms
step:992/1770 train_time:98639ms step_avg:99.43ms
step:993/1770 train_time:98742ms step_avg:99.44ms
step:994/1770 train_time:98845ms step_avg:99.44ms
step:995/1770 train_time:98948ms step_avg:99.44ms
step:996/1770 train_time:99050ms step_avg:99.45ms
step:997/1770 train_time:99152ms step_avg:99.45ms
step:998/1770 train_time:99254ms step_avg:99.45ms
step:999/1770 train_time:99357ms step_avg:99.46ms
step:1000/1770 train_time:99460ms step_avg:99.46ms
step:1000/1770 val_loss:3.5103 train_time:99557ms step_avg:99.56ms
step:1001/1770 train_time:99577ms step_avg:99.48ms
step:1002/1770 train_time:99673ms step_avg:99.47ms
step:1003/1770 train_time:99777ms step_avg:99.48ms
step:1004/1770 train_time:99880ms step_avg:99.48ms
step:1005/1770 train_time:99982ms step_avg:99.48ms
step:1006/1770 train_time:100084ms step_avg:99.49ms
step:1007/1770 train_time:100186ms step_avg:99.49ms
step:1008/1770 train_time:100288ms step_avg:99.49ms
step:1009/1770 train_time:100390ms step_avg:99.50ms
step:1010/1770 train_time:100493ms step_avg:99.50ms
step:1011/1770 train_time:100598ms step_avg:99.50ms
step:1012/1770 train_time:100702ms step_avg:99.51ms
step:1013/1770 train_time:100805ms step_avg:99.51ms
step:1014/1770 train_time:100909ms step_avg:99.52ms
step:1015/1770 train_time:101012ms step_avg:99.52ms
step:1016/1770 train_time:101115ms step_avg:99.52ms
step:1017/1770 train_time:101218ms step_avg:99.53ms
step:1018/1770 train_time:101320ms step_avg:99.53ms
step:1019/1770 train_time:101422ms step_avg:99.53ms
step:1020/1770 train_time:101524ms step_avg:99.53ms
step:1021/1770 train_time:101628ms step_avg:99.54ms
step:1022/1770 train_time:101730ms step_avg:99.54ms
step:1023/1770 train_time:101834ms step_avg:99.54ms
step:1024/1770 train_time:101937ms step_avg:99.55ms
step:1025/1770 train_time:102040ms step_avg:99.55ms
step:1026/1770 train_time:102143ms step_avg:99.55ms
step:1027/1770 train_time:102246ms step_avg:99.56ms
step:1028/1770 train_time:102349ms step_avg:99.56ms
step:1029/1770 train_time:102452ms step_avg:99.57ms
step:1030/1770 train_time:102555ms step_avg:99.57ms
step:1031/1770 train_time:102658ms step_avg:99.57ms
step:1032/1770 train_time:102760ms step_avg:99.57ms
step:1033/1770 train_time:102863ms step_avg:99.58ms
step:1034/1770 train_time:102965ms step_avg:99.58ms
step:1035/1770 train_time:103068ms step_avg:99.58ms
step:1036/1770 train_time:103171ms step_avg:99.59ms
step:1037/1770 train_time:103273ms step_avg:99.59ms
step:1038/1770 train_time:103376ms step_avg:99.59ms
step:1039/1770 train_time:103479ms step_avg:99.59ms
step:1040/1770 train_time:103581ms step_avg:99.60ms
step:1041/1770 train_time:103684ms step_avg:99.60ms
step:1042/1770 train_time:103786ms step_avg:99.60ms
step:1043/1770 train_time:103888ms step_avg:99.61ms
step:1044/1770 train_time:103991ms step_avg:99.61ms
step:1045/1770 train_time:104094ms step_avg:99.61ms
step:1046/1770 train_time:104196ms step_avg:99.61ms
step:1047/1770 train_time:104299ms step_avg:99.62ms
step:1048/1770 train_time:104401ms step_avg:99.62ms
step:1049/1770 train_time:104504ms step_avg:99.62ms
step:1050/1770 train_time:104606ms step_avg:99.62ms
step:1051/1770 train_time:104709ms step_avg:99.63ms
step:1052/1770 train_time:104812ms step_avg:99.63ms
step:1053/1770 train_time:104915ms step_avg:99.63ms
step:1054/1770 train_time:105017ms step_avg:99.64ms
step:1055/1770 train_time:105120ms step_avg:99.64ms
step:1056/1770 train_time:105224ms step_avg:99.64ms
step:1057/1770 train_time:105326ms step_avg:99.65ms
step:1058/1770 train_time:105429ms step_avg:99.65ms
step:1059/1770 train_time:105532ms step_avg:99.65ms
step:1060/1770 train_time:105635ms step_avg:99.66ms
step:1061/1770 train_time:105738ms step_avg:99.66ms
step:1062/1770 train_time:105841ms step_avg:99.66ms
step:1063/1770 train_time:105947ms step_avg:99.67ms
step:1064/1770 train_time:106050ms step_avg:99.67ms
step:1065/1770 train_time:106154ms step_avg:99.67ms
step:1066/1770 train_time:106257ms step_avg:99.68ms
step:1067/1770 train_time:106360ms step_avg:99.68ms
step:1068/1770 train_time:106463ms step_avg:99.68ms
step:1069/1770 train_time:106566ms step_avg:99.69ms
step:1070/1770 train_time:106668ms step_avg:99.69ms
step:1071/1770 train_time:106773ms step_avg:99.69ms
step:1072/1770 train_time:106876ms step_avg:99.70ms
step:1073/1770 train_time:106978ms step_avg:99.70ms
step:1074/1770 train_time:107080ms step_avg:99.70ms
step:1075/1770 train_time:107183ms step_avg:99.71ms
step:1076/1770 train_time:107286ms step_avg:99.71ms
step:1077/1770 train_time:107390ms step_avg:99.71ms
step:1078/1770 train_time:107493ms step_avg:99.71ms
step:1079/1770 train_time:107595ms step_avg:99.72ms
step:1080/1770 train_time:107698ms step_avg:99.72ms
step:1081/1770 train_time:107801ms step_avg:99.72ms
step:1082/1770 train_time:107904ms step_avg:99.73ms
step:1083/1770 train_time:108006ms step_avg:99.73ms
step:1084/1770 train_time:108109ms step_avg:99.73ms
step:1085/1770 train_time:108212ms step_avg:99.73ms
step:1086/1770 train_time:108315ms step_avg:99.74ms
step:1087/1770 train_time:108418ms step_avg:99.74ms
step:1088/1770 train_time:108520ms step_avg:99.74ms
step:1089/1770 train_time:108623ms step_avg:99.75ms
step:1090/1770 train_time:108727ms step_avg:99.75ms
step:1091/1770 train_time:108830ms step_avg:99.75ms
step:1092/1770 train_time:108933ms step_avg:99.76ms
step:1093/1770 train_time:109036ms step_avg:99.76ms
step:1094/1770 train_time:109140ms step_avg:99.76ms
step:1095/1770 train_time:109243ms step_avg:99.76ms
step:1096/1770 train_time:109346ms step_avg:99.77ms
step:1097/1770 train_time:109448ms step_avg:99.77ms
step:1098/1770 train_time:109551ms step_avg:99.77ms
step:1099/1770 train_time:109655ms step_avg:99.78ms
step:1100/1770 train_time:109758ms step_avg:99.78ms
step:1101/1770 train_time:109861ms step_avg:99.78ms
step:1102/1770 train_time:109963ms step_avg:99.79ms
step:1103/1770 train_time:110066ms step_avg:99.79ms
step:1104/1770 train_time:110169ms step_avg:99.79ms
step:1105/1770 train_time:110273ms step_avg:99.79ms
step:1106/1770 train_time:110376ms step_avg:99.80ms
step:1107/1770 train_time:110478ms step_avg:99.80ms
step:1108/1770 train_time:110582ms step_avg:99.80ms
step:1109/1770 train_time:110684ms step_avg:99.81ms
step:1110/1770 train_time:110787ms step_avg:99.81ms
step:1111/1770 train_time:110890ms step_avg:99.81ms
step:1112/1770 train_time:110994ms step_avg:99.81ms
step:1113/1770 train_time:111097ms step_avg:99.82ms
step:1114/1770 train_time:111199ms step_avg:99.82ms
step:1115/1770 train_time:111302ms step_avg:99.82ms
step:1116/1770 train_time:111404ms step_avg:99.82ms
step:1117/1770 train_time:111507ms step_avg:99.83ms
step:1118/1770 train_time:111610ms step_avg:99.83ms
step:1119/1770 train_time:111714ms step_avg:99.83ms
step:1120/1770 train_time:111817ms step_avg:99.84ms
step:1121/1770 train_time:111919ms step_avg:99.84ms
step:1122/1770 train_time:112021ms step_avg:99.84ms
step:1123/1770 train_time:112124ms step_avg:99.84ms
step:1124/1770 train_time:112228ms step_avg:99.85ms
step:1125/1770 train_time:112331ms step_avg:99.85ms
step:1125/1770 val_loss:3.4700 train_time:112428ms step_avg:99.94ms
step:1126/1770 train_time:112450ms step_avg:99.87ms
step:1127/1770 train_time:112543ms step_avg:99.86ms
step:1128/1770 train_time:112647ms step_avg:99.86ms
step:1129/1770 train_time:112750ms step_avg:99.87ms
step:1130/1770 train_time:112852ms step_avg:99.87ms
step:1131/1770 train_time:112956ms step_avg:99.87ms
step:1132/1770 train_time:113059ms step_avg:99.88ms
step:1133/1770 train_time:113162ms step_avg:99.88ms
step:1134/1770 train_time:113265ms step_avg:99.88ms
step:1135/1770 train_time:113367ms step_avg:99.88ms
step:1136/1770 train_time:113470ms step_avg:99.89ms
step:1137/1770 train_time:113573ms step_avg:99.89ms
step:1138/1770 train_time:113676ms step_avg:99.89ms
step:1139/1770 train_time:113780ms step_avg:99.89ms
step:1140/1770 train_time:113883ms step_avg:99.90ms
step:1141/1770 train_time:113986ms step_avg:99.90ms
step:1142/1770 train_time:114089ms step_avg:99.90ms
step:1143/1770 train_time:114191ms step_avg:99.91ms
step:1144/1770 train_time:114294ms step_avg:99.91ms
step:1145/1770 train_time:114397ms step_avg:99.91ms
step:1146/1770 train_time:114501ms step_avg:99.91ms
step:1147/1770 train_time:114604ms step_avg:99.92ms
step:1148/1770 train_time:114707ms step_avg:99.92ms
step:1149/1770 train_time:114810ms step_avg:99.92ms
step:1150/1770 train_time:114913ms step_avg:99.92ms
step:1151/1770 train_time:115017ms step_avg:99.93ms
step:1152/1770 train_time:115120ms step_avg:99.93ms
step:1153/1770 train_time:115223ms step_avg:99.93ms
step:1154/1770 train_time:115326ms step_avg:99.94ms
step:1155/1770 train_time:115429ms step_avg:99.94ms
step:1156/1770 train_time:115532ms step_avg:99.94ms
step:1157/1770 train_time:115636ms step_avg:99.94ms
step:1158/1770 train_time:115739ms step_avg:99.95ms
step:1159/1770 train_time:115842ms step_avg:99.95ms
step:1160/1770 train_time:115945ms step_avg:99.95ms
step:1161/1770 train_time:116047ms step_avg:99.95ms
step:1162/1770 train_time:116151ms step_avg:99.96ms
step:1163/1770 train_time:116255ms step_avg:99.96ms
step:1164/1770 train_time:116358ms step_avg:99.96ms
step:1165/1770 train_time:116461ms step_avg:99.97ms
step:1166/1770 train_time:116565ms step_avg:99.97ms
step:1167/1770 train_time:116667ms step_avg:99.97ms
step:1168/1770 train_time:116769ms step_avg:99.97ms
step:1169/1770 train_time:116872ms step_avg:99.98ms
step:1170/1770 train_time:116974ms step_avg:99.98ms
step:1171/1770 train_time:117078ms step_avg:99.98ms
step:1172/1770 train_time:117182ms step_avg:99.98ms
step:1173/1770 train_time:117285ms step_avg:99.99ms
step:1174/1770 train_time:117388ms step_avg:99.99ms
step:1175/1770 train_time:117491ms step_avg:99.99ms
step:1176/1770 train_time:117593ms step_avg:99.99ms
step:1177/1770 train_time:117696ms step_avg:100.00ms
step:1178/1770 train_time:117799ms step_avg:100.00ms
step:1179/1770 train_time:117903ms step_avg:100.00ms
step:1180/1770 train_time:118005ms step_avg:100.00ms
step:1181/1770 train_time:118108ms step_avg:100.01ms
step:1182/1770 train_time:118211ms step_avg:100.01ms
step:1183/1770 train_time:118315ms step_avg:100.01ms
step:1184/1770 train_time:118420ms step_avg:100.02ms
step:1185/1770 train_time:118524ms step_avg:100.02ms
step:1186/1770 train_time:118629ms step_avg:100.02ms
step:1187/1770 train_time:118735ms step_avg:100.03ms
step:1188/1770 train_time:118839ms step_avg:100.03ms
step:1189/1770 train_time:118943ms step_avg:100.04ms
step:1190/1770 train_time:119047ms step_avg:100.04ms
step:1191/1770 train_time:119152ms step_avg:100.04ms
step:1192/1770 train_time:119255ms step_avg:100.05ms
step:1193/1770 train_time:119359ms step_avg:100.05ms
step:1194/1770 train_time:119464ms step_avg:100.05ms
step:1195/1770 train_time:119569ms step_avg:100.06ms
step:1196/1770 train_time:119673ms step_avg:100.06ms
step:1197/1770 train_time:119777ms step_avg:100.06ms
step:1198/1770 train_time:119881ms step_avg:100.07ms
step:1199/1770 train_time:119985ms step_avg:100.07ms
step:1200/1770 train_time:120090ms step_avg:100.07ms
step:1201/1770 train_time:120195ms step_avg:100.08ms
step:1202/1770 train_time:120298ms step_avg:100.08ms
step:1203/1770 train_time:120402ms step_avg:100.08ms
step:1204/1770 train_time:120506ms step_avg:100.09ms
step:1205/1770 train_time:120609ms step_avg:100.09ms
step:1206/1770 train_time:120716ms step_avg:100.10ms
step:1207/1770 train_time:120820ms step_avg:100.10ms
step:1208/1770 train_time:120925ms step_avg:100.10ms
step:1209/1770 train_time:121028ms step_avg:100.11ms
step:1210/1770 train_time:121132ms step_avg:100.11ms
step:1211/1770 train_time:121236ms step_avg:100.11ms
step:1212/1770 train_time:121343ms step_avg:100.12ms
step:1213/1770 train_time:121447ms step_avg:100.12ms
step:1214/1770 train_time:121551ms step_avg:100.12ms
step:1215/1770 train_time:121655ms step_avg:100.13ms
step:1216/1770 train_time:121760ms step_avg:100.13ms
step:1217/1770 train_time:121864ms step_avg:100.14ms
step:1218/1770 train_time:121968ms step_avg:100.14ms
step:1219/1770 train_time:122072ms step_avg:100.14ms
step:1220/1770 train_time:122175ms step_avg:100.14ms
step:1221/1770 train_time:122280ms step_avg:100.15ms
step:1222/1770 train_time:122387ms step_avg:100.15ms
step:1223/1770 train_time:122490ms step_avg:100.16ms
step:1224/1770 train_time:122595ms step_avg:100.16ms
step:1225/1770 train_time:122699ms step_avg:100.16ms
step:1226/1770 train_time:122804ms step_avg:100.17ms
step:1227/1770 train_time:122910ms step_avg:100.17ms
step:1228/1770 train_time:123016ms step_avg:100.18ms
step:1229/1770 train_time:123120ms step_avg:100.18ms
step:1230/1770 train_time:123225ms step_avg:100.18ms
step:1231/1770 train_time:123329ms step_avg:100.19ms
step:1232/1770 train_time:123433ms step_avg:100.19ms
step:1233/1770 train_time:123536ms step_avg:100.19ms
step:1234/1770 train_time:123639ms step_avg:100.19ms
step:1235/1770 train_time:123744ms step_avg:100.20ms
step:1236/1770 train_time:123849ms step_avg:100.20ms
step:1237/1770 train_time:123953ms step_avg:100.20ms
step:1238/1770 train_time:124057ms step_avg:100.21ms
step:1239/1770 train_time:124162ms step_avg:100.21ms
step:1240/1770 train_time:124266ms step_avg:100.21ms
step:1241/1770 train_time:124371ms step_avg:100.22ms
step:1242/1770 train_time:124475ms step_avg:100.22ms
step:1243/1770 train_time:124579ms step_avg:100.22ms
step:1244/1770 train_time:124683ms step_avg:100.23ms
step:1245/1770 train_time:124786ms step_avg:100.23ms
step:1246/1770 train_time:124891ms step_avg:100.23ms
step:1247/1770 train_time:124995ms step_avg:100.24ms
step:1248/1770 train_time:125100ms step_avg:100.24ms
step:1249/1770 train_time:125204ms step_avg:100.24ms
step:1250/1770 train_time:125308ms step_avg:100.25ms
step:1250/1770 val_loss:3.4216 train_time:125408ms step_avg:100.33ms
step:1251/1770 train_time:125430ms step_avg:100.26ms
step:1252/1770 train_time:125526ms step_avg:100.26ms
step:1253/1770 train_time:125630ms step_avg:100.26ms
step:1254/1770 train_time:125734ms step_avg:100.27ms
step:1255/1770 train_time:125840ms step_avg:100.27ms
step:1256/1770 train_time:125943ms step_avg:100.27ms
step:1257/1770 train_time:126047ms step_avg:100.28ms
step:1258/1770 train_time:126151ms step_avg:100.28ms
step:1259/1770 train_time:126256ms step_avg:100.28ms
step:1260/1770 train_time:126360ms step_avg:100.29ms
step:1261/1770 train_time:126465ms step_avg:100.29ms
step:1262/1770 train_time:126569ms step_avg:100.29ms
step:1263/1770 train_time:126672ms step_avg:100.29ms
step:1264/1770 train_time:126778ms step_avg:100.30ms
step:1265/1770 train_time:126882ms step_avg:100.30ms
step:1266/1770 train_time:126987ms step_avg:100.31ms
step:1267/1770 train_time:127091ms step_avg:100.31ms
step:1268/1770 train_time:127195ms step_avg:100.31ms
step:1269/1770 train_time:127299ms step_avg:100.31ms
step:1270/1770 train_time:127404ms step_avg:100.32ms
step:1271/1770 train_time:127508ms step_avg:100.32ms
step:1272/1770 train_time:127611ms step_avg:100.32ms
step:1273/1770 train_time:127716ms step_avg:100.33ms
step:1274/1770 train_time:127820ms step_avg:100.33ms
step:1275/1770 train_time:127924ms step_avg:100.33ms
step:1276/1770 train_time:128028ms step_avg:100.34ms
step:1277/1770 train_time:128130ms step_avg:100.34ms
step:1278/1770 train_time:128235ms step_avg:100.34ms
step:1279/1770 train_time:128339ms step_avg:100.34ms
step:1280/1770 train_time:128445ms step_avg:100.35ms
step:1281/1770 train_time:128549ms step_avg:100.35ms
step:1282/1770 train_time:128654ms step_avg:100.35ms
step:1283/1770 train_time:128759ms step_avg:100.36ms
step:1284/1770 train_time:128863ms step_avg:100.36ms
step:1285/1770 train_time:128968ms step_avg:100.36ms
step:1286/1770 train_time:129073ms step_avg:100.37ms
step:1287/1770 train_time:129178ms step_avg:100.37ms
step:1288/1770 train_time:129282ms step_avg:100.37ms
step:1289/1770 train_time:129386ms step_avg:100.38ms
step:1290/1770 train_time:129489ms step_avg:100.38ms
step:1291/1770 train_time:129594ms step_avg:100.38ms
step:1292/1770 train_time:129698ms step_avg:100.39ms
step:1293/1770 train_time:129803ms step_avg:100.39ms
step:1294/1770 train_time:129907ms step_avg:100.39ms
step:1295/1770 train_time:130012ms step_avg:100.40ms
step:1296/1770 train_time:130115ms step_avg:100.40ms
step:1297/1770 train_time:130220ms step_avg:100.40ms
step:1298/1770 train_time:130323ms step_avg:100.40ms
step:1299/1770 train_time:130427ms step_avg:100.41ms
step:1300/1770 train_time:130531ms step_avg:100.41ms
step:1301/1770 train_time:130635ms step_avg:100.41ms
step:1302/1770 train_time:130740ms step_avg:100.41ms
step:1303/1770 train_time:130844ms step_avg:100.42ms
step:1304/1770 train_time:130947ms step_avg:100.42ms
step:1305/1770 train_time:131051ms step_avg:100.42ms
step:1306/1770 train_time:131155ms step_avg:100.42ms
step:1307/1770 train_time:131260ms step_avg:100.43ms
step:1308/1770 train_time:131364ms step_avg:100.43ms
step:1309/1770 train_time:131468ms step_avg:100.43ms
step:1310/1770 train_time:131572ms step_avg:100.44ms
step:1311/1770 train_time:131676ms step_avg:100.44ms
step:1312/1770 train_time:131780ms step_avg:100.44ms
step:1313/1770 train_time:131884ms step_avg:100.45ms
step:1314/1770 train_time:131988ms step_avg:100.45ms
step:1315/1770 train_time:132093ms step_avg:100.45ms
step:1316/1770 train_time:132197ms step_avg:100.45ms
step:1317/1770 train_time:132302ms step_avg:100.46ms
step:1318/1770 train_time:132408ms step_avg:100.46ms
step:1319/1770 train_time:132512ms step_avg:100.46ms
step:1320/1770 train_time:132617ms step_avg:100.47ms
step:1321/1770 train_time:132721ms step_avg:100.47ms
step:1322/1770 train_time:132826ms step_avg:100.47ms
step:1323/1770 train_time:132930ms step_avg:100.48ms
step:1324/1770 train_time:133034ms step_avg:100.48ms
step:1325/1770 train_time:133141ms step_avg:100.48ms
step:1326/1770 train_time:133245ms step_avg:100.49ms
step:1327/1770 train_time:133352ms step_avg:100.49ms
step:1328/1770 train_time:133455ms step_avg:100.49ms
step:1329/1770 train_time:133559ms step_avg:100.50ms
step:1330/1770 train_time:133663ms step_avg:100.50ms
step:1331/1770 train_time:133766ms step_avg:100.50ms
step:1332/1770 train_time:133870ms step_avg:100.50ms
step:1333/1770 train_time:133973ms step_avg:100.51ms
step:1334/1770 train_time:134077ms step_avg:100.51ms
step:1335/1770 train_time:134182ms step_avg:100.51ms
step:1336/1770 train_time:134286ms step_avg:100.51ms
step:1337/1770 train_time:134390ms step_avg:100.52ms
step:1338/1770 train_time:134495ms step_avg:100.52ms
step:1339/1770 train_time:134599ms step_avg:100.52ms
step:1340/1770 train_time:134704ms step_avg:100.53ms
step:1341/1770 train_time:134808ms step_avg:100.53ms
step:1342/1770 train_time:134913ms step_avg:100.53ms
step:1343/1770 train_time:135017ms step_avg:100.53ms
step:1344/1770 train_time:135121ms step_avg:100.54ms
step:1345/1770 train_time:135225ms step_avg:100.54ms
step:1346/1770 train_time:135329ms step_avg:100.54ms
step:1347/1770 train_time:135433ms step_avg:100.54ms
step:1348/1770 train_time:135538ms step_avg:100.55ms
step:1349/1770 train_time:135643ms step_avg:100.55ms
step:1350/1770 train_time:135748ms step_avg:100.55ms
step:1351/1770 train_time:135852ms step_avg:100.56ms
step:1352/1770 train_time:135956ms step_avg:100.56ms
step:1353/1770 train_time:136062ms step_avg:100.56ms
step:1354/1770 train_time:136165ms step_avg:100.57ms
step:1355/1770 train_time:136269ms step_avg:100.57ms
step:1356/1770 train_time:136373ms step_avg:100.57ms
step:1357/1770 train_time:136477ms step_avg:100.57ms
step:1358/1770 train_time:136583ms step_avg:100.58ms
step:1359/1770 train_time:136687ms step_avg:100.58ms
step:1360/1770 train_time:136792ms step_avg:100.58ms
step:1361/1770 train_time:136896ms step_avg:100.58ms
step:1362/1770 train_time:137000ms step_avg:100.59ms
step:1363/1770 train_time:137105ms step_avg:100.59ms
step:1364/1770 train_time:137209ms step_avg:100.59ms
step:1365/1770 train_time:137313ms step_avg:100.60ms
step:1366/1770 train_time:137416ms step_avg:100.60ms
step:1367/1770 train_time:137522ms step_avg:100.60ms
step:1368/1770 train_time:137626ms step_avg:100.60ms
step:1369/1770 train_time:137730ms step_avg:100.61ms
step:1370/1770 train_time:137834ms step_avg:100.61ms
step:1371/1770 train_time:137940ms step_avg:100.61ms
step:1372/1770 train_time:138044ms step_avg:100.61ms
step:1373/1770 train_time:138148ms step_avg:100.62ms
step:1374/1770 train_time:138253ms step_avg:100.62ms
step:1375/1770 train_time:138357ms step_avg:100.62ms
step:1375/1770 val_loss:3.3801 train_time:138456ms step_avg:100.70ms
step:1376/1770 train_time:138478ms step_avg:100.64ms
step:1377/1770 train_time:138571ms step_avg:100.63ms
step:1378/1770 train_time:138675ms step_avg:100.63ms
step:1379/1770 train_time:138779ms step_avg:100.64ms
step:1380/1770 train_time:138883ms step_avg:100.64ms
step:1381/1770 train_time:138987ms step_avg:100.64ms
step:1382/1770 train_time:139091ms step_avg:100.64ms
step:1383/1770 train_time:139195ms step_avg:100.65ms
step:1384/1770 train_time:139299ms step_avg:100.65ms
step:1385/1770 train_time:139404ms step_avg:100.65ms
step:1386/1770 train_time:139508ms step_avg:100.66ms
step:1387/1770 train_time:139613ms step_avg:100.66ms
step:1388/1770 train_time:139717ms step_avg:100.66ms
step:1389/1770 train_time:139823ms step_avg:100.66ms
step:1390/1770 train_time:139927ms step_avg:100.67ms
step:1391/1770 train_time:140031ms step_avg:100.67ms
step:1392/1770 train_time:140135ms step_avg:100.67ms
step:1393/1770 train_time:140239ms step_avg:100.67ms
step:1394/1770 train_time:140343ms step_avg:100.68ms
step:1395/1770 train_time:140448ms step_avg:100.68ms
step:1396/1770 train_time:140553ms step_avg:100.68ms
step:1397/1770 train_time:140658ms step_avg:100.69ms
step:1398/1770 train_time:140763ms step_avg:100.69ms
step:1399/1770 train_time:140866ms step_avg:100.69ms
step:1400/1770 train_time:140972ms step_avg:100.69ms
step:1401/1770 train_time:141075ms step_avg:100.70ms
step:1402/1770 train_time:141179ms step_avg:100.70ms
step:1403/1770 train_time:141284ms step_avg:100.70ms
step:1404/1770 train_time:141389ms step_avg:100.70ms
step:1405/1770 train_time:141492ms step_avg:100.71ms
step:1406/1770 train_time:141597ms step_avg:100.71ms
step:1407/1770 train_time:141701ms step_avg:100.71ms
step:1408/1770 train_time:141805ms step_avg:100.71ms
step:1409/1770 train_time:141909ms step_avg:100.72ms
step:1410/1770 train_time:142014ms step_avg:100.72ms
step:1411/1770 train_time:142117ms step_avg:100.72ms
step:1412/1770 train_time:142222ms step_avg:100.72ms
step:1413/1770 train_time:142326ms step_avg:100.73ms
step:1414/1770 train_time:142431ms step_avg:100.73ms
step:1415/1770 train_time:142536ms step_avg:100.73ms
step:1416/1770 train_time:142640ms step_avg:100.73ms
step:1417/1770 train_time:142744ms step_avg:100.74ms
step:1418/1770 train_time:142848ms step_avg:100.74ms
step:1419/1770 train_time:142952ms step_avg:100.74ms
step:1420/1770 train_time:143057ms step_avg:100.74ms
step:1421/1770 train_time:143161ms step_avg:100.75ms
step:1422/1770 train_time:143264ms step_avg:100.75ms
step:1423/1770 train_time:143370ms step_avg:100.75ms
step:1424/1770 train_time:143475ms step_avg:100.75ms
step:1425/1770 train_time:143578ms step_avg:100.76ms
step:1426/1770 train_time:143684ms step_avg:100.76ms
step:1427/1770 train_time:143787ms step_avg:100.76ms
step:1428/1770 train_time:143893ms step_avg:100.77ms
step:1429/1770 train_time:143997ms step_avg:100.77ms
step:1430/1770 train_time:144101ms step_avg:100.77ms
step:1431/1770 train_time:144207ms step_avg:100.77ms
step:1432/1770 train_time:144311ms step_avg:100.78ms
step:1433/1770 train_time:144415ms step_avg:100.78ms
step:1434/1770 train_time:144518ms step_avg:100.78ms
step:1435/1770 train_time:144624ms step_avg:100.78ms
step:1436/1770 train_time:144730ms step_avg:100.79ms
step:1437/1770 train_time:144834ms step_avg:100.79ms
step:1438/1770 train_time:144939ms step_avg:100.79ms
step:1439/1770 train_time:145042ms step_avg:100.79ms
step:1440/1770 train_time:145146ms step_avg:100.80ms
step:1441/1770 train_time:145253ms step_avg:100.80ms
step:1442/1770 train_time:145356ms step_avg:100.80ms
step:1443/1770 train_time:145461ms step_avg:100.80ms
step:1444/1770 train_time:145566ms step_avg:100.81ms
step:1445/1770 train_time:145670ms step_avg:100.81ms
step:1446/1770 train_time:145774ms step_avg:100.81ms
step:1447/1770 train_time:145880ms step_avg:100.82ms
step:1448/1770 train_time:145986ms step_avg:100.82ms
step:1449/1770 train_time:146092ms step_avg:100.82ms
step:1450/1770 train_time:146197ms step_avg:100.83ms
step:1451/1770 train_time:146303ms step_avg:100.83ms
step:1452/1770 train_time:146409ms step_avg:100.83ms
step:1453/1770 train_time:146514ms step_avg:100.84ms
step:1454/1770 train_time:146620ms step_avg:100.84ms
step:1455/1770 train_time:146726ms step_avg:100.84ms
step:1456/1770 train_time:146832ms step_avg:100.85ms
step:1457/1770 train_time:146939ms step_avg:100.85ms
step:1458/1770 train_time:147045ms step_avg:100.85ms
step:1459/1770 train_time:147151ms step_avg:100.86ms
step:1460/1770 train_time:147255ms step_avg:100.86ms
step:1461/1770 train_time:147361ms step_avg:100.86ms
step:1462/1770 train_time:147467ms step_avg:100.87ms
step:1463/1770 train_time:147572ms step_avg:100.87ms
step:1464/1770 train_time:147679ms step_avg:100.87ms
step:1465/1770 train_time:147785ms step_avg:100.88ms
step:1466/1770 train_time:147891ms step_avg:100.88ms
step:1467/1770 train_time:147998ms step_avg:100.88ms
step:1468/1770 train_time:148104ms step_avg:100.89ms
step:1469/1770 train_time:148209ms step_avg:100.89ms
step:1470/1770 train_time:148315ms step_avg:100.89ms
step:1471/1770 train_time:148420ms step_avg:100.90ms
step:1472/1770 train_time:148525ms step_avg:100.90ms
step:1473/1770 train_time:148631ms step_avg:100.90ms
step:1474/1770 train_time:148738ms step_avg:100.91ms
step:1475/1770 train_time:148844ms step_avg:100.91ms
step:1476/1770 train_time:148948ms step_avg:100.91ms
step:1477/1770 train_time:149055ms step_avg:100.92ms
step:1478/1770 train_time:149160ms step_avg:100.92ms
step:1479/1770 train_time:149266ms step_avg:100.92ms
step:1480/1770 train_time:149370ms step_avg:100.93ms
step:1481/1770 train_time:149479ms step_avg:100.93ms
step:1482/1770 train_time:149583ms step_avg:100.93ms
step:1483/1770 train_time:149688ms step_avg:100.94ms
step:1484/1770 train_time:149793ms step_avg:100.94ms
step:1485/1770 train_time:149899ms step_avg:100.94ms
step:1486/1770 train_time:150004ms step_avg:100.95ms
step:1487/1770 train_time:150110ms step_avg:100.95ms
step:1488/1770 train_time:150216ms step_avg:100.95ms
step:1489/1770 train_time:150322ms step_avg:100.96ms
step:1490/1770 train_time:150427ms step_avg:100.96ms
step:1491/1770 train_time:150533ms step_avg:100.96ms
step:1492/1770 train_time:150639ms step_avg:100.96ms
step:1493/1770 train_time:150747ms step_avg:100.97ms
step:1494/1770 train_time:150855ms step_avg:100.97ms
step:1495/1770 train_time:150961ms step_avg:100.98ms
step:1496/1770 train_time:151065ms step_avg:100.98ms
step:1497/1770 train_time:151170ms step_avg:100.98ms
step:1498/1770 train_time:151275ms step_avg:100.98ms
step:1499/1770 train_time:151380ms step_avg:100.99ms
step:1500/1770 train_time:151484ms step_avg:100.99ms
step:1500/1770 val_loss:3.3418 train_time:151584ms step_avg:101.06ms
step:1501/1770 train_time:151605ms step_avg:101.00ms
step:1502/1770 train_time:151702ms step_avg:101.00ms
step:1503/1770 train_time:151807ms step_avg:101.00ms
step:1504/1770 train_time:151912ms step_avg:101.01ms
step:1505/1770 train_time:152019ms step_avg:101.01ms
step:1506/1770 train_time:152123ms step_avg:101.01ms
step:1507/1770 train_time:152228ms step_avg:101.01ms
step:1508/1770 train_time:152336ms step_avg:101.02ms
step:1509/1770 train_time:152441ms step_avg:101.02ms
step:1510/1770 train_time:152545ms step_avg:101.02ms
step:1511/1770 train_time:152652ms step_avg:101.03ms
step:1512/1770 train_time:152758ms step_avg:101.03ms
step:1513/1770 train_time:152863ms step_avg:101.03ms
step:1514/1770 train_time:152969ms step_avg:101.04ms
step:1515/1770 train_time:153074ms step_avg:101.04ms
step:1516/1770 train_time:153180ms step_avg:101.04ms
step:1517/1770 train_time:153285ms step_avg:101.04ms
step:1518/1770 train_time:153392ms step_avg:101.05ms
step:1519/1770 train_time:153496ms step_avg:101.05ms
step:1520/1770 train_time:153602ms step_avg:101.05ms
step:1521/1770 train_time:153707ms step_avg:101.06ms
step:1522/1770 train_time:153813ms step_avg:101.06ms
step:1523/1770 train_time:153920ms step_avg:101.06ms
step:1524/1770 train_time:154025ms step_avg:101.07ms
step:1525/1770 train_time:154131ms step_avg:101.07ms
step:1526/1770 train_time:154235ms step_avg:101.07ms
step:1527/1770 train_time:154339ms step_avg:101.07ms
step:1528/1770 train_time:154446ms step_avg:101.08ms
step:1529/1770 train_time:154551ms step_avg:101.08ms
step:1530/1770 train_time:154658ms step_avg:101.08ms
step:1531/1770 train_time:154763ms step_avg:101.09ms
step:1532/1770 train_time:154869ms step_avg:101.09ms
step:1533/1770 train_time:154975ms step_avg:101.09ms
step:1534/1770 train_time:155080ms step_avg:101.10ms
step:1535/1770 train_time:155185ms step_avg:101.10ms
step:1536/1770 train_time:155291ms step_avg:101.10ms
step:1537/1770 train_time:155398ms step_avg:101.10ms
step:1538/1770 train_time:155504ms step_avg:101.11ms
step:1539/1770 train_time:155610ms step_avg:101.11ms
step:1540/1770 train_time:155719ms step_avg:101.12ms
step:1541/1770 train_time:155825ms step_avg:101.12ms
step:1542/1770 train_time:155931ms step_avg:101.12ms
step:1543/1770 train_time:156037ms step_avg:101.13ms
step:1544/1770 train_time:156144ms step_avg:101.13ms
step:1545/1770 train_time:156249ms step_avg:101.13ms
step:1546/1770 train_time:156355ms step_avg:101.14ms
step:1547/1770 train_time:156460ms step_avg:101.14ms
step:1548/1770 train_time:156565ms step_avg:101.14ms
step:1549/1770 train_time:156670ms step_avg:101.14ms
step:1550/1770 train_time:156776ms step_avg:101.15ms
step:1551/1770 train_time:156881ms step_avg:101.15ms
step:1552/1770 train_time:156988ms step_avg:101.15ms
step:1553/1770 train_time:157093ms step_avg:101.15ms
step:1554/1770 train_time:157198ms step_avg:101.16ms
step:1555/1770 train_time:157303ms step_avg:101.16ms
step:1556/1770 train_time:157408ms step_avg:101.16ms
step:1557/1770 train_time:157513ms step_avg:101.16ms
step:1558/1770 train_time:157619ms step_avg:101.17ms
step:1559/1770 train_time:157724ms step_avg:101.17ms
step:1560/1770 train_time:157828ms step_avg:101.17ms
step:1561/1770 train_time:157936ms step_avg:101.18ms
step:1562/1770 train_time:158041ms step_avg:101.18ms
step:1563/1770 train_time:158146ms step_avg:101.18ms
step:1564/1770 train_time:158251ms step_avg:101.18ms
step:1565/1770 train_time:158357ms step_avg:101.19ms
step:1566/1770 train_time:158462ms step_avg:101.19ms
step:1567/1770 train_time:158567ms step_avg:101.19ms
step:1568/1770 train_time:158673ms step_avg:101.19ms
step:1569/1770 train_time:158783ms step_avg:101.20ms
step:1570/1770 train_time:158889ms step_avg:101.20ms
step:1571/1770 train_time:158994ms step_avg:101.21ms
step:1572/1770 train_time:159100ms step_avg:101.21ms
step:1573/1770 train_time:159207ms step_avg:101.21ms
step:1574/1770 train_time:159312ms step_avg:101.21ms
step:1575/1770 train_time:159417ms step_avg:101.22ms
step:1576/1770 train_time:159522ms step_avg:101.22ms
step:1577/1770 train_time:159628ms step_avg:101.22ms
step:1578/1770 train_time:159734ms step_avg:101.23ms
step:1579/1770 train_time:159839ms step_avg:101.23ms
step:1580/1770 train_time:159944ms step_avg:101.23ms
step:1581/1770 train_time:160053ms step_avg:101.24ms
step:1582/1770 train_time:160161ms step_avg:101.24ms
step:1583/1770 train_time:160266ms step_avg:101.24ms
step:1584/1770 train_time:160373ms step_avg:101.25ms
step:1585/1770 train_time:160478ms step_avg:101.25ms
step:1586/1770 train_time:160587ms step_avg:101.25ms
step:1587/1770 train_time:160694ms step_avg:101.26ms
step:1588/1770 train_time:160799ms step_avg:101.26ms
step:1589/1770 train_time:160905ms step_avg:101.26ms
step:1590/1770 train_time:161010ms step_avg:101.26ms
step:1591/1770 train_time:161115ms step_avg:101.27ms
step:1592/1770 train_time:161221ms step_avg:101.27ms
step:1593/1770 train_time:161326ms step_avg:101.27ms
step:1594/1770 train_time:161433ms step_avg:101.28ms
step:1595/1770 train_time:161538ms step_avg:101.28ms
step:1596/1770 train_time:161644ms step_avg:101.28ms
step:1597/1770 train_time:161750ms step_avg:101.28ms
step:1598/1770 train_time:161856ms step_avg:101.29ms
step:1599/1770 train_time:161963ms step_avg:101.29ms
step:1600/1770 train_time:162071ms step_avg:101.29ms
step:1601/1770 train_time:162176ms step_avg:101.30ms
step:1602/1770 train_time:162282ms step_avg:101.30ms
step:1603/1770 train_time:162388ms step_avg:101.30ms
step:1604/1770 train_time:162492ms step_avg:101.30ms
step:1605/1770 train_time:162598ms step_avg:101.31ms
step:1606/1770 train_time:162703ms step_avg:101.31ms
step:1607/1770 train_time:162812ms step_avg:101.31ms
step:1608/1770 train_time:162918ms step_avg:101.32ms
step:1609/1770 train_time:163024ms step_avg:101.32ms
step:1610/1770 train_time:163130ms step_avg:101.32ms
step:1611/1770 train_time:163236ms step_avg:101.33ms
step:1612/1770 train_time:163342ms step_avg:101.33ms
step:1613/1770 train_time:163447ms step_avg:101.33ms
step:1614/1770 train_time:163555ms step_avg:101.33ms
step:1615/1770 train_time:163660ms step_avg:101.34ms
step:1616/1770 train_time:163765ms step_avg:101.34ms
step:1617/1770 train_time:163873ms step_avg:101.34ms
step:1618/1770 train_time:163980ms step_avg:101.35ms
step:1619/1770 train_time:164086ms step_avg:101.35ms
step:1620/1770 train_time:164193ms step_avg:101.35ms
step:1621/1770 train_time:164298ms step_avg:101.36ms
step:1622/1770 train_time:164405ms step_avg:101.36ms
step:1623/1770 train_time:164513ms step_avg:101.36ms
step:1624/1770 train_time:164617ms step_avg:101.37ms
step:1625/1770 train_time:164721ms step_avg:101.37ms
step:1625/1770 val_loss:3.3075 train_time:164822ms step_avg:101.43ms
step:1626/1770 train_time:164843ms step_avg:101.38ms
step:1627/1770 train_time:164939ms step_avg:101.38ms
step:1628/1770 train_time:165045ms step_avg:101.38ms
step:1629/1770 train_time:165149ms step_avg:101.38ms
step:1630/1770 train_time:165255ms step_avg:101.38ms
step:1631/1770 train_time:165359ms step_avg:101.39ms
step:1632/1770 train_time:165465ms step_avg:101.39ms
step:1633/1770 train_time:165571ms step_avg:101.39ms
step:1634/1770 train_time:165675ms step_avg:101.39ms
step:1635/1770 train_time:165780ms step_avg:101.39ms
step:1636/1770 train_time:165887ms step_avg:101.40ms
step:1637/1770 train_time:165993ms step_avg:101.40ms
step:1638/1770 train_time:166097ms step_avg:101.40ms
step:1639/1770 train_time:166204ms step_avg:101.41ms
step:1640/1770 train_time:166309ms step_avg:101.41ms
step:1641/1770 train_time:166415ms step_avg:101.41ms
step:1642/1770 train_time:166520ms step_avg:101.41ms
step:1643/1770 train_time:166625ms step_avg:101.42ms
step:1644/1770 train_time:166732ms step_avg:101.42ms
step:1645/1770 train_time:166837ms step_avg:101.42ms
step:1646/1770 train_time:166944ms step_avg:101.42ms
step:1647/1770 train_time:167050ms step_avg:101.43ms
step:1648/1770 train_time:167155ms step_avg:101.43ms
step:1649/1770 train_time:167261ms step_avg:101.43ms
step:1650/1770 train_time:167366ms step_avg:101.43ms
step:1651/1770 train_time:167471ms step_avg:101.44ms
step:1652/1770 train_time:167576ms step_avg:101.44ms
step:1653/1770 train_time:167680ms step_avg:101.44ms
step:1654/1770 train_time:167788ms step_avg:101.44ms
step:1655/1770 train_time:167895ms step_avg:101.45ms
step:1656/1770 train_time:168000ms step_avg:101.45ms
step:1657/1770 train_time:168108ms step_avg:101.45ms
step:1658/1770 train_time:168213ms step_avg:101.46ms
step:1659/1770 train_time:168320ms step_avg:101.46ms
step:1660/1770 train_time:168426ms step_avg:101.46ms
step:1661/1770 train_time:168531ms step_avg:101.46ms
step:1662/1770 train_time:168637ms step_avg:101.47ms
step:1663/1770 train_time:168742ms step_avg:101.47ms
step:1664/1770 train_time:168848ms step_avg:101.47ms
step:1665/1770 train_time:168952ms step_avg:101.47ms
step:1666/1770 train_time:169058ms step_avg:101.48ms
step:1667/1770 train_time:169163ms step_avg:101.48ms
step:1668/1770 train_time:169268ms step_avg:101.48ms
step:1669/1770 train_time:169372ms step_avg:101.48ms
step:1670/1770 train_time:169477ms step_avg:101.48ms
step:1671/1770 train_time:169584ms step_avg:101.49ms
step:1672/1770 train_time:169690ms step_avg:101.49ms
step:1673/1770 train_time:169796ms step_avg:101.49ms
step:1674/1770 train_time:169901ms step_avg:101.49ms
step:1675/1770 train_time:170007ms step_avg:101.50ms
step:1676/1770 train_time:170114ms step_avg:101.50ms
step:1677/1770 train_time:170223ms step_avg:101.50ms
step:1678/1770 train_time:170328ms step_avg:101.51ms
step:1679/1770 train_time:170434ms step_avg:101.51ms
step:1680/1770 train_time:170539ms step_avg:101.51ms
step:1681/1770 train_time:170644ms step_avg:101.51ms
step:1682/1770 train_time:170752ms step_avg:101.52ms
step:1683/1770 train_time:170857ms step_avg:101.52ms
step:1684/1770 train_time:170962ms step_avg:101.52ms
step:1685/1770 train_time:171067ms step_avg:101.52ms
step:1686/1770 train_time:171174ms step_avg:101.53ms
step:1687/1770 train_time:171282ms step_avg:101.53ms
step:1688/1770 train_time:171387ms step_avg:101.53ms
step:1689/1770 train_time:171493ms step_avg:101.54ms
step:1690/1770 train_time:171598ms step_avg:101.54ms
step:1691/1770 train_time:171703ms step_avg:101.54ms
step:1692/1770 train_time:171809ms step_avg:101.54ms
step:1693/1770 train_time:171915ms step_avg:101.54ms
step:1694/1770 train_time:172020ms step_avg:101.55ms
step:1695/1770 train_time:172126ms step_avg:101.55ms
step:1696/1770 train_time:172233ms step_avg:101.55ms
step:1697/1770 train_time:172341ms step_avg:101.56ms
step:1698/1770 train_time:172448ms step_avg:101.56ms
step:1699/1770 train_time:172552ms step_avg:101.56ms
step:1700/1770 train_time:172657ms step_avg:101.56ms
step:1701/1770 train_time:172762ms step_avg:101.57ms
step:1702/1770 train_time:172869ms step_avg:101.57ms
step:1703/1770 train_time:172974ms step_avg:101.57ms
step:1704/1770 train_time:173080ms step_avg:101.57ms
step:1705/1770 train_time:173185ms step_avg:101.57ms
step:1706/1770 train_time:173290ms step_avg:101.58ms
step:1707/1770 train_time:173397ms step_avg:101.58ms
step:1708/1770 train_time:173503ms step_avg:101.58ms
step:1709/1770 train_time:173610ms step_avg:101.59ms
step:1710/1770 train_time:173720ms step_avg:101.59ms
step:1711/1770 train_time:173828ms step_avg:101.59ms
step:1712/1770 train_time:173934ms step_avg:101.60ms
step:1713/1770 train_time:174039ms step_avg:101.60ms
step:1714/1770 train_time:174145ms step_avg:101.60ms
step:1715/1770 train_time:174251ms step_avg:101.60ms
step:1716/1770 train_time:174357ms step_avg:101.61ms
step:1717/1770 train_time:174463ms step_avg:101.61ms
step:1718/1770 train_time:174570ms step_avg:101.61ms
step:1719/1770 train_time:174677ms step_avg:101.62ms
step:1720/1770 train_time:174785ms step_avg:101.62ms
step:1721/1770 train_time:174890ms step_avg:101.62ms
step:1722/1770 train_time:174999ms step_avg:101.63ms
step:1723/1770 train_time:175108ms step_avg:101.63ms
step:1724/1770 train_time:175217ms step_avg:101.63ms
step:1725/1770 train_time:175326ms step_avg:101.64ms
step:1726/1770 train_time:175434ms step_avg:101.64ms
step:1727/1770 train_time:175540ms step_avg:101.64ms
step:1728/1770 train_time:175649ms step_avg:101.65ms
step:1729/1770 train_time:175755ms step_avg:101.65ms
step:1730/1770 train_time:175861ms step_avg:101.65ms
step:1731/1770 train_time:175970ms step_avg:101.66ms
step:1732/1770 train_time:176075ms step_avg:101.66ms
step:1733/1770 train_time:176184ms step_avg:101.66ms
step:1734/1770 train_time:176289ms step_avg:101.67ms
step:1735/1770 train_time:176397ms step_avg:101.67ms
step:1736/1770 train_time:176503ms step_avg:101.67ms
step:1737/1770 train_time:176610ms step_avg:101.68ms
step:1738/1770 train_time:176717ms step_avg:101.68ms
step:1739/1770 train_time:176822ms step_avg:101.68ms
step:1740/1770 train_time:176928ms step_avg:101.68ms
step:1741/1770 train_time:177036ms step_avg:101.69ms
step:1742/1770 train_time:177146ms step_avg:101.69ms
step:1743/1770 train_time:177253ms step_avg:101.69ms
step:1744/1770 train_time:177359ms step_avg:101.70ms
step:1745/1770 train_time:177465ms step_avg:101.70ms
step:1746/1770 train_time:177575ms step_avg:101.70ms
step:1747/1770 train_time:177680ms step_avg:101.71ms
step:1748/1770 train_time:177790ms step_avg:101.71ms
step:1749/1770 train_time:177897ms step_avg:101.71ms
step:1750/1770 train_time:178003ms step_avg:101.72ms
step:1750/1770 val_loss:3.2803 train_time:178103ms step_avg:101.77ms
step:1751/1770 train_time:178123ms step_avg:101.73ms
step:1752/1770 train_time:178217ms step_avg:101.72ms
step:1753/1770 train_time:178323ms step_avg:101.72ms
step:1754/1770 train_time:178430ms step_avg:101.73ms
step:1755/1770 train_time:178536ms step_avg:101.73ms
step:1756/1770 train_time:178643ms step_avg:101.73ms
step:1757/1770 train_time:178750ms step_avg:101.74ms
step:1758/1770 train_time:178856ms step_avg:101.74ms
step:1759/1770 train_time:178963ms step_avg:101.74ms
step:1760/1770 train_time:179069ms step_avg:101.74ms
step:1761/1770 train_time:179177ms step_avg:101.75ms
step:1762/1770 train_time:179287ms step_avg:101.75ms
step:1763/1770 train_time:179393ms step_avg:101.75ms
step:1764/1770 train_time:179499ms step_avg:101.76ms
step:1765/1770 train_time:179606ms step_avg:101.76ms
step:1766/1770 train_time:179717ms step_avg:101.77ms
step:1767/1770 train_time:179822ms step_avg:101.77ms
step:1768/1770 train_time:179929ms step_avg:101.77ms
step:1769/1770 train_time:180034ms step_avg:101.77ms
step:1770/1770 train_time:180140ms step_avg:101.77ms
step:1770/1770 val_loss:3.2775 train_time:180242ms step_avg:101.83ms
peak memory allocated: 30724 MiB reserved: 45392 MiB
