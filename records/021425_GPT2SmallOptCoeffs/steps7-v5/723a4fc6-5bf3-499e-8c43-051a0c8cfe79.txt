import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.1357, -4.2084, 1.0726),
        (4.132, -4.2045, 1.0725),
        (4.077, -4.1489, 1.0719),
        (4.0422, -4.1139, 1.0717),
        (3.9129, -3.9845, 1.0715),
        (3.3337, -3.2386, 0.9049),
        (2.2005, -1.6921, 0.4915),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 07:10:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            116W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            109W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            116W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            115W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            107W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1770 train_time:57ms step_avg:56.98ms
step:2/1770 train_time:140ms step_avg:69.96ms
step:3/1770 train_time:233ms step_avg:77.51ms
step:4/1770 train_time:328ms step_avg:81.92ms
step:5/1770 train_time:424ms step_avg:84.82ms
step:6/1770 train_time:520ms step_avg:86.66ms
step:7/1770 train_time:616ms step_avg:88.02ms
step:8/1770 train_time:712ms step_avg:89.06ms
step:9/1770 train_time:809ms step_avg:89.91ms
step:10/1770 train_time:905ms step_avg:90.55ms
step:11/1770 train_time:1001ms step_avg:91.04ms
step:12/1770 train_time:1098ms step_avg:91.47ms
step:13/1770 train_time:1194ms step_avg:91.86ms
step:14/1770 train_time:1291ms step_avg:92.23ms
step:15/1770 train_time:1388ms step_avg:92.52ms
step:16/1770 train_time:1484ms step_avg:92.74ms
step:17/1770 train_time:1581ms step_avg:93.00ms
step:18/1770 train_time:1677ms step_avg:93.18ms
step:19/1770 train_time:1774ms step_avg:93.35ms
step:20/1770 train_time:1870ms step_avg:93.52ms
step:21/1770 train_time:1967ms step_avg:93.66ms
step:22/1770 train_time:2063ms step_avg:93.79ms
step:23/1770 train_time:2159ms step_avg:93.86ms
step:24/1770 train_time:2255ms step_avg:93.94ms
step:25/1770 train_time:2351ms step_avg:94.05ms
step:26/1770 train_time:2448ms step_avg:94.14ms
step:27/1770 train_time:2544ms step_avg:94.20ms
step:28/1770 train_time:2640ms step_avg:94.28ms
step:29/1770 train_time:2741ms step_avg:94.51ms
step:30/1770 train_time:2833ms step_avg:94.42ms
step:31/1770 train_time:2930ms step_avg:94.51ms
step:32/1770 train_time:3025ms step_avg:94.52ms
step:33/1770 train_time:3121ms step_avg:94.57ms
step:34/1770 train_time:3217ms step_avg:94.62ms
step:35/1770 train_time:3313ms step_avg:94.67ms
step:36/1770 train_time:3410ms step_avg:94.73ms
step:37/1770 train_time:3506ms step_avg:94.76ms
step:38/1770 train_time:3603ms step_avg:94.80ms
step:39/1770 train_time:3699ms step_avg:94.84ms
step:40/1770 train_time:3795ms step_avg:94.88ms
step:41/1770 train_time:3892ms step_avg:94.93ms
step:42/1770 train_time:3988ms step_avg:94.96ms
step:43/1770 train_time:4085ms step_avg:95.00ms
step:44/1770 train_time:4181ms step_avg:95.03ms
step:45/1770 train_time:4277ms step_avg:95.04ms
step:46/1770 train_time:4373ms step_avg:95.06ms
step:47/1770 train_time:4469ms step_avg:95.09ms
step:48/1770 train_time:4567ms step_avg:95.14ms
step:49/1770 train_time:4664ms step_avg:95.19ms
step:50/1770 train_time:4760ms step_avg:95.21ms
step:51/1770 train_time:4856ms step_avg:95.22ms
step:52/1770 train_time:4953ms step_avg:95.24ms
step:53/1770 train_time:5050ms step_avg:95.28ms
step:54/1770 train_time:5146ms step_avg:95.30ms
step:55/1770 train_time:5244ms step_avg:95.34ms
step:56/1770 train_time:5339ms step_avg:95.34ms
step:57/1770 train_time:5435ms step_avg:95.36ms
step:58/1770 train_time:5532ms step_avg:95.38ms
step:59/1770 train_time:5629ms step_avg:95.40ms
step:60/1770 train_time:5725ms step_avg:95.41ms
step:61/1770 train_time:5820ms step_avg:95.42ms
step:62/1770 train_time:5916ms step_avg:95.42ms
step:63/1770 train_time:6012ms step_avg:95.43ms
step:64/1770 train_time:6109ms step_avg:95.45ms
step:65/1770 train_time:6206ms step_avg:95.48ms
step:66/1770 train_time:6303ms step_avg:95.50ms
step:67/1770 train_time:6399ms step_avg:95.51ms
step:68/1770 train_time:6495ms step_avg:95.52ms
step:69/1770 train_time:6591ms step_avg:95.53ms
step:70/1770 train_time:6689ms step_avg:95.55ms
step:71/1770 train_time:6784ms step_avg:95.55ms
step:72/1770 train_time:6880ms step_avg:95.55ms
step:73/1770 train_time:6976ms step_avg:95.56ms
step:74/1770 train_time:7072ms step_avg:95.56ms
step:75/1770 train_time:7168ms step_avg:95.57ms
step:76/1770 train_time:7265ms step_avg:95.59ms
step:77/1770 train_time:7360ms step_avg:95.58ms
step:78/1770 train_time:7456ms step_avg:95.59ms
step:79/1770 train_time:7552ms step_avg:95.59ms
step:80/1770 train_time:7648ms step_avg:95.60ms
step:81/1770 train_time:7744ms step_avg:95.60ms
step:82/1770 train_time:7840ms step_avg:95.61ms
step:83/1770 train_time:7936ms step_avg:95.61ms
step:84/1770 train_time:8032ms step_avg:95.61ms
step:85/1770 train_time:8128ms step_avg:95.63ms
step:86/1770 train_time:8225ms step_avg:95.64ms
step:87/1770 train_time:8321ms step_avg:95.65ms
step:88/1770 train_time:8417ms step_avg:95.65ms
step:89/1770 train_time:8513ms step_avg:95.65ms
step:90/1770 train_time:8610ms step_avg:95.67ms
step:91/1770 train_time:8707ms step_avg:95.68ms
step:92/1770 train_time:8803ms step_avg:95.68ms
step:93/1770 train_time:8900ms step_avg:95.70ms
step:94/1770 train_time:8995ms step_avg:95.69ms
step:95/1770 train_time:9091ms step_avg:95.70ms
step:96/1770 train_time:9188ms step_avg:95.71ms
step:97/1770 train_time:9284ms step_avg:95.71ms
step:98/1770 train_time:9380ms step_avg:95.72ms
step:99/1770 train_time:9476ms step_avg:95.71ms
step:100/1770 train_time:9572ms step_avg:95.72ms
step:101/1770 train_time:9668ms step_avg:95.73ms
step:102/1770 train_time:9765ms step_avg:95.73ms
step:103/1770 train_time:9860ms step_avg:95.73ms
step:104/1770 train_time:9957ms step_avg:95.74ms
step:105/1770 train_time:10052ms step_avg:95.74ms
step:106/1770 train_time:10148ms step_avg:95.74ms
step:107/1770 train_time:10244ms step_avg:95.74ms
step:108/1770 train_time:10340ms step_avg:95.74ms
step:109/1770 train_time:10436ms step_avg:95.74ms
step:110/1770 train_time:10537ms step_avg:95.79ms
step:111/1770 train_time:10628ms step_avg:95.75ms
step:112/1770 train_time:10724ms step_avg:95.75ms
step:113/1770 train_time:10819ms step_avg:95.74ms
step:114/1770 train_time:10915ms step_avg:95.75ms
step:115/1770 train_time:11011ms step_avg:95.75ms
step:116/1770 train_time:11108ms step_avg:95.76ms
step:117/1770 train_time:11204ms step_avg:95.76ms
step:118/1770 train_time:11300ms step_avg:95.76ms
step:119/1770 train_time:11396ms step_avg:95.76ms
step:120/1770 train_time:11492ms step_avg:95.77ms
step:121/1770 train_time:11588ms step_avg:95.77ms
step:122/1770 train_time:11685ms step_avg:95.78ms
step:123/1770 train_time:11781ms step_avg:95.78ms
step:124/1770 train_time:11877ms step_avg:95.78ms
step:125/1770 train_time:11973ms step_avg:95.78ms
step:125/1770 val_loss:4.6360 train_time:12063ms step_avg:96.51ms
step:126/1770 train_time:12082ms step_avg:95.89ms
step:127/1770 train_time:12171ms step_avg:95.83ms
step:128/1770 train_time:12273ms step_avg:95.89ms
step:129/1770 train_time:12373ms step_avg:95.92ms
step:130/1770 train_time:12471ms step_avg:95.93ms
step:131/1770 train_time:12567ms step_avg:95.93ms
step:132/1770 train_time:12670ms step_avg:95.99ms
step:133/1770 train_time:12759ms step_avg:95.93ms
step:134/1770 train_time:12855ms step_avg:95.93ms
step:135/1770 train_time:12952ms step_avg:95.94ms
step:136/1770 train_time:13048ms step_avg:95.94ms
step:137/1770 train_time:13144ms step_avg:95.94ms
step:138/1770 train_time:13241ms step_avg:95.95ms
step:139/1770 train_time:13337ms step_avg:95.95ms
step:140/1770 train_time:13435ms step_avg:95.96ms
step:141/1770 train_time:13531ms step_avg:95.97ms
step:142/1770 train_time:13629ms step_avg:95.98ms
step:143/1770 train_time:13727ms step_avg:95.99ms
step:144/1770 train_time:13823ms step_avg:95.99ms
step:145/1770 train_time:13919ms step_avg:95.99ms
step:146/1770 train_time:14017ms step_avg:96.00ms
step:147/1770 train_time:14113ms step_avg:96.01ms
step:148/1770 train_time:14210ms step_avg:96.01ms
step:149/1770 train_time:14306ms step_avg:96.02ms
step:150/1770 train_time:14403ms step_avg:96.02ms
step:151/1770 train_time:14499ms step_avg:96.02ms
step:152/1770 train_time:14596ms step_avg:96.03ms
step:153/1770 train_time:14694ms step_avg:96.04ms
step:154/1770 train_time:14790ms step_avg:96.04ms
step:155/1770 train_time:14886ms step_avg:96.04ms
step:156/1770 train_time:14982ms step_avg:96.04ms
step:157/1770 train_time:15079ms step_avg:96.04ms
step:158/1770 train_time:15175ms step_avg:96.05ms
step:159/1770 train_time:15273ms step_avg:96.05ms
step:160/1770 train_time:15370ms step_avg:96.06ms
step:161/1770 train_time:15466ms step_avg:96.06ms
step:162/1770 train_time:15562ms step_avg:96.06ms
step:163/1770 train_time:15659ms step_avg:96.07ms
step:164/1770 train_time:15756ms step_avg:96.07ms
step:165/1770 train_time:15853ms step_avg:96.08ms
step:166/1770 train_time:15951ms step_avg:96.09ms
step:167/1770 train_time:16046ms step_avg:96.09ms
step:168/1770 train_time:16143ms step_avg:96.09ms
step:169/1770 train_time:16240ms step_avg:96.09ms
step:170/1770 train_time:16336ms step_avg:96.10ms
step:171/1770 train_time:16434ms step_avg:96.10ms
step:172/1770 train_time:16531ms step_avg:96.11ms
step:173/1770 train_time:16629ms step_avg:96.12ms
step:174/1770 train_time:16725ms step_avg:96.12ms
step:175/1770 train_time:16821ms step_avg:96.12ms
step:176/1770 train_time:16918ms step_avg:96.13ms
step:177/1770 train_time:17015ms step_avg:96.13ms
step:178/1770 train_time:17112ms step_avg:96.13ms
step:179/1770 train_time:17209ms step_avg:96.14ms
step:180/1770 train_time:17305ms step_avg:96.14ms
step:181/1770 train_time:17402ms step_avg:96.14ms
step:182/1770 train_time:17498ms step_avg:96.14ms
step:183/1770 train_time:17594ms step_avg:96.14ms
step:184/1770 train_time:17691ms step_avg:96.15ms
step:185/1770 train_time:17788ms step_avg:96.15ms
step:186/1770 train_time:17884ms step_avg:96.15ms
step:187/1770 train_time:17980ms step_avg:96.15ms
step:188/1770 train_time:18076ms step_avg:96.15ms
step:189/1770 train_time:18174ms step_avg:96.16ms
step:190/1770 train_time:18270ms step_avg:96.16ms
step:191/1770 train_time:18373ms step_avg:96.19ms
step:192/1770 train_time:18464ms step_avg:96.17ms
step:193/1770 train_time:18561ms step_avg:96.17ms
step:194/1770 train_time:18657ms step_avg:96.17ms
step:195/1770 train_time:18754ms step_avg:96.18ms
step:196/1770 train_time:18851ms step_avg:96.18ms
step:197/1770 train_time:18949ms step_avg:96.19ms
step:198/1770 train_time:19045ms step_avg:96.19ms
step:199/1770 train_time:19142ms step_avg:96.19ms
step:200/1770 train_time:19238ms step_avg:96.19ms
step:201/1770 train_time:19336ms step_avg:96.20ms
step:202/1770 train_time:19433ms step_avg:96.20ms
step:203/1770 train_time:19531ms step_avg:96.21ms
step:204/1770 train_time:19628ms step_avg:96.22ms
step:205/1770 train_time:19725ms step_avg:96.22ms
step:206/1770 train_time:19821ms step_avg:96.22ms
step:207/1770 train_time:19918ms step_avg:96.22ms
step:208/1770 train_time:20014ms step_avg:96.22ms
step:209/1770 train_time:20112ms step_avg:96.23ms
step:210/1770 train_time:20209ms step_avg:96.24ms
step:211/1770 train_time:20307ms step_avg:96.24ms
step:212/1770 train_time:20403ms step_avg:96.24ms
step:213/1770 train_time:20499ms step_avg:96.24ms
step:214/1770 train_time:20596ms step_avg:96.24ms
step:215/1770 train_time:20693ms step_avg:96.25ms
step:216/1770 train_time:20790ms step_avg:96.25ms
step:217/1770 train_time:20887ms step_avg:96.25ms
step:218/1770 train_time:20983ms step_avg:96.25ms
step:219/1770 train_time:21080ms step_avg:96.25ms
step:220/1770 train_time:21176ms step_avg:96.26ms
step:221/1770 train_time:21273ms step_avg:96.26ms
step:222/1770 train_time:21371ms step_avg:96.27ms
step:223/1770 train_time:21474ms step_avg:96.30ms
step:224/1770 train_time:21564ms step_avg:96.27ms
step:225/1770 train_time:21661ms step_avg:96.27ms
step:226/1770 train_time:21757ms step_avg:96.27ms
step:227/1770 train_time:21855ms step_avg:96.28ms
step:228/1770 train_time:21951ms step_avg:96.28ms
step:229/1770 train_time:22049ms step_avg:96.28ms
step:230/1770 train_time:22145ms step_avg:96.28ms
step:231/1770 train_time:22242ms step_avg:96.29ms
step:232/1770 train_time:22338ms step_avg:96.29ms
step:233/1770 train_time:22435ms step_avg:96.29ms
step:234/1770 train_time:22533ms step_avg:96.29ms
step:235/1770 train_time:22631ms step_avg:96.30ms
step:236/1770 train_time:22728ms step_avg:96.31ms
step:237/1770 train_time:22825ms step_avg:96.31ms
step:238/1770 train_time:22921ms step_avg:96.31ms
step:239/1770 train_time:23018ms step_avg:96.31ms
step:240/1770 train_time:23115ms step_avg:96.31ms
step:241/1770 train_time:23213ms step_avg:96.32ms
step:242/1770 train_time:23635ms step_avg:97.67ms
step:243/1770 train_time:23690ms step_avg:97.49ms
step:244/1770 train_time:23787ms step_avg:97.49ms
step:245/1770 train_time:23883ms step_avg:97.48ms
step:246/1770 train_time:23979ms step_avg:97.47ms
step:247/1770 train_time:24076ms step_avg:97.47ms
step:248/1770 train_time:24172ms step_avg:97.47ms
step:249/1770 train_time:24269ms step_avg:97.47ms
step:250/1770 train_time:24365ms step_avg:97.46ms
step:250/1770 val_loss:4.0970 train_time:24457ms step_avg:97.83ms
step:251/1770 train_time:24476ms step_avg:97.51ms
step:252/1770 train_time:24565ms step_avg:97.48ms
step:253/1770 train_time:24664ms step_avg:97.49ms
step:254/1770 train_time:24761ms step_avg:97.48ms
step:255/1770 train_time:24857ms step_avg:97.48ms
step:256/1770 train_time:24954ms step_avg:97.48ms
step:257/1770 train_time:25050ms step_avg:97.47ms
step:258/1770 train_time:25147ms step_avg:97.47ms
step:259/1770 train_time:25243ms step_avg:97.46ms
step:260/1770 train_time:25340ms step_avg:97.46ms
step:261/1770 train_time:25436ms step_avg:97.46ms
step:262/1770 train_time:25533ms step_avg:97.45ms
step:263/1770 train_time:25629ms step_avg:97.45ms
step:264/1770 train_time:25726ms step_avg:97.45ms
step:265/1770 train_time:25823ms step_avg:97.45ms
step:266/1770 train_time:25920ms step_avg:97.44ms
step:267/1770 train_time:26017ms step_avg:97.44ms
step:268/1770 train_time:26113ms step_avg:97.44ms
step:269/1770 train_time:26211ms step_avg:97.44ms
step:270/1770 train_time:26308ms step_avg:97.44ms
step:271/1770 train_time:26405ms step_avg:97.44ms
step:272/1770 train_time:26502ms step_avg:97.43ms
step:273/1770 train_time:26599ms step_avg:97.43ms
step:274/1770 train_time:26696ms step_avg:97.43ms
step:275/1770 train_time:26793ms step_avg:97.43ms
step:276/1770 train_time:26890ms step_avg:97.43ms
step:277/1770 train_time:26988ms step_avg:97.43ms
step:278/1770 train_time:27085ms step_avg:97.43ms
step:279/1770 train_time:27182ms step_avg:97.43ms
step:280/1770 train_time:27279ms step_avg:97.42ms
step:281/1770 train_time:27376ms step_avg:97.42ms
step:282/1770 train_time:27473ms step_avg:97.42ms
step:283/1770 train_time:27570ms step_avg:97.42ms
step:284/1770 train_time:27668ms step_avg:97.42ms
step:285/1770 train_time:27764ms step_avg:97.42ms
step:286/1770 train_time:27861ms step_avg:97.42ms
step:287/1770 train_time:27958ms step_avg:97.41ms
step:288/1770 train_time:28055ms step_avg:97.41ms
step:289/1770 train_time:28152ms step_avg:97.41ms
step:290/1770 train_time:28249ms step_avg:97.41ms
step:291/1770 train_time:28347ms step_avg:97.41ms
step:292/1770 train_time:28444ms step_avg:97.41ms
step:293/1770 train_time:28541ms step_avg:97.41ms
step:294/1770 train_time:28638ms step_avg:97.41ms
step:295/1770 train_time:28736ms step_avg:97.41ms
step:296/1770 train_time:28833ms step_avg:97.41ms
step:297/1770 train_time:28930ms step_avg:97.41ms
step:298/1770 train_time:29027ms step_avg:97.41ms
step:299/1770 train_time:29124ms step_avg:97.40ms
step:300/1770 train_time:29221ms step_avg:97.40ms
step:301/1770 train_time:29318ms step_avg:97.40ms
step:302/1770 train_time:29415ms step_avg:97.40ms
step:303/1770 train_time:29512ms step_avg:97.40ms
step:304/1770 train_time:29609ms step_avg:97.40ms
step:305/1770 train_time:29708ms step_avg:97.40ms
step:306/1770 train_time:29805ms step_avg:97.40ms
step:307/1770 train_time:29903ms step_avg:97.40ms
step:308/1770 train_time:29999ms step_avg:97.40ms
step:309/1770 train_time:30096ms step_avg:97.40ms
step:310/1770 train_time:30193ms step_avg:97.40ms
step:311/1770 train_time:30290ms step_avg:97.40ms
step:312/1770 train_time:30388ms step_avg:97.40ms
step:313/1770 train_time:30485ms step_avg:97.40ms
step:314/1770 train_time:30582ms step_avg:97.39ms
step:315/1770 train_time:30679ms step_avg:97.39ms
step:316/1770 train_time:30777ms step_avg:97.40ms
step:317/1770 train_time:30872ms step_avg:97.39ms
step:318/1770 train_time:30970ms step_avg:97.39ms
step:319/1770 train_time:31067ms step_avg:97.39ms
step:320/1770 train_time:31164ms step_avg:97.39ms
step:321/1770 train_time:31261ms step_avg:97.39ms
step:322/1770 train_time:31357ms step_avg:97.38ms
step:323/1770 train_time:31455ms step_avg:97.38ms
step:324/1770 train_time:31552ms step_avg:97.38ms
step:325/1770 train_time:31649ms step_avg:97.38ms
step:326/1770 train_time:31748ms step_avg:97.39ms
step:327/1770 train_time:31845ms step_avg:97.38ms
step:328/1770 train_time:31942ms step_avg:97.38ms
step:329/1770 train_time:32039ms step_avg:97.38ms
step:330/1770 train_time:32136ms step_avg:97.38ms
step:331/1770 train_time:32233ms step_avg:97.38ms
step:332/1770 train_time:32331ms step_avg:97.38ms
step:333/1770 train_time:32428ms step_avg:97.38ms
step:334/1770 train_time:32525ms step_avg:97.38ms
step:335/1770 train_time:32623ms step_avg:97.38ms
step:336/1770 train_time:32719ms step_avg:97.38ms
step:337/1770 train_time:32816ms step_avg:97.38ms
step:338/1770 train_time:32913ms step_avg:97.38ms
step:339/1770 train_time:33010ms step_avg:97.38ms
step:340/1770 train_time:33108ms step_avg:97.38ms
step:341/1770 train_time:33206ms step_avg:97.38ms
step:342/1770 train_time:33303ms step_avg:97.38ms
step:343/1770 train_time:33399ms step_avg:97.37ms
step:344/1770 train_time:33496ms step_avg:97.37ms
step:345/1770 train_time:33594ms step_avg:97.37ms
step:346/1770 train_time:33690ms step_avg:97.37ms
step:347/1770 train_time:33787ms step_avg:97.37ms
step:348/1770 train_time:33885ms step_avg:97.37ms
step:349/1770 train_time:33982ms step_avg:97.37ms
step:350/1770 train_time:34079ms step_avg:97.37ms
step:351/1770 train_time:34176ms step_avg:97.37ms
step:352/1770 train_time:34274ms step_avg:97.37ms
step:353/1770 train_time:34371ms step_avg:97.37ms
step:354/1770 train_time:34469ms step_avg:97.37ms
step:355/1770 train_time:34566ms step_avg:97.37ms
step:356/1770 train_time:34663ms step_avg:97.37ms
step:357/1770 train_time:34761ms step_avg:97.37ms
step:358/1770 train_time:34858ms step_avg:97.37ms
step:359/1770 train_time:34955ms step_avg:97.37ms
step:360/1770 train_time:35052ms step_avg:97.37ms
step:361/1770 train_time:35150ms step_avg:97.37ms
step:362/1770 train_time:35248ms step_avg:97.37ms
step:363/1770 train_time:35346ms step_avg:97.37ms
step:364/1770 train_time:35443ms step_avg:97.37ms
step:365/1770 train_time:35541ms step_avg:97.37ms
step:366/1770 train_time:35637ms step_avg:97.37ms
step:367/1770 train_time:35734ms step_avg:97.37ms
step:368/1770 train_time:35831ms step_avg:97.37ms
step:369/1770 train_time:35928ms step_avg:97.36ms
step:370/1770 train_time:36025ms step_avg:97.36ms
step:371/1770 train_time:36122ms step_avg:97.36ms
step:372/1770 train_time:36219ms step_avg:97.36ms
step:373/1770 train_time:36315ms step_avg:97.36ms
step:374/1770 train_time:36412ms step_avg:97.36ms
step:375/1770 train_time:36510ms step_avg:97.36ms
step:375/1770 val_loss:3.8955 train_time:36602ms step_avg:97.60ms
step:376/1770 train_time:36621ms step_avg:97.40ms
step:377/1770 train_time:36709ms step_avg:97.37ms
step:378/1770 train_time:36808ms step_avg:97.38ms
step:379/1770 train_time:36906ms step_avg:97.38ms
step:380/1770 train_time:37003ms step_avg:97.38ms
step:381/1770 train_time:37100ms step_avg:97.37ms
step:382/1770 train_time:37197ms step_avg:97.37ms
step:383/1770 train_time:37295ms step_avg:97.38ms
step:384/1770 train_time:37392ms step_avg:97.37ms
step:385/1770 train_time:37489ms step_avg:97.37ms
step:386/1770 train_time:37586ms step_avg:97.37ms
step:387/1770 train_time:37683ms step_avg:97.37ms
step:388/1770 train_time:37780ms step_avg:97.37ms
step:389/1770 train_time:37877ms step_avg:97.37ms
step:390/1770 train_time:37975ms step_avg:97.37ms
step:391/1770 train_time:38073ms step_avg:97.37ms
step:392/1770 train_time:38171ms step_avg:97.37ms
step:393/1770 train_time:38268ms step_avg:97.37ms
step:394/1770 train_time:38365ms step_avg:97.37ms
step:395/1770 train_time:38462ms step_avg:97.37ms
step:396/1770 train_time:38561ms step_avg:97.38ms
step:397/1770 train_time:38661ms step_avg:97.38ms
step:398/1770 train_time:38759ms step_avg:97.38ms
step:399/1770 train_time:38858ms step_avg:97.39ms
step:400/1770 train_time:38957ms step_avg:97.39ms
step:401/1770 train_time:39057ms step_avg:97.40ms
step:402/1770 train_time:39156ms step_avg:97.40ms
step:403/1770 train_time:39257ms step_avg:97.41ms
step:404/1770 train_time:39356ms step_avg:97.42ms
step:405/1770 train_time:39456ms step_avg:97.42ms
step:406/1770 train_time:39557ms step_avg:97.43ms
step:407/1770 train_time:39658ms step_avg:97.44ms
step:408/1770 train_time:39757ms step_avg:97.44ms
step:409/1770 train_time:39857ms step_avg:97.45ms
step:410/1770 train_time:39956ms step_avg:97.45ms
step:411/1770 train_time:40056ms step_avg:97.46ms
step:412/1770 train_time:40155ms step_avg:97.46ms
step:413/1770 train_time:40254ms step_avg:97.47ms
step:414/1770 train_time:40353ms step_avg:97.47ms
step:415/1770 train_time:40452ms step_avg:97.48ms
step:416/1770 train_time:40551ms step_avg:97.48ms
step:417/1770 train_time:40652ms step_avg:97.49ms
step:418/1770 train_time:40753ms step_avg:97.49ms
step:419/1770 train_time:40852ms step_avg:97.50ms
step:420/1770 train_time:40952ms step_avg:97.51ms
step:421/1770 train_time:41052ms step_avg:97.51ms
step:422/1770 train_time:41152ms step_avg:97.52ms
step:423/1770 train_time:41252ms step_avg:97.52ms
step:424/1770 train_time:41352ms step_avg:97.53ms
step:425/1770 train_time:41451ms step_avg:97.53ms
step:426/1770 train_time:41550ms step_avg:97.53ms
step:427/1770 train_time:41649ms step_avg:97.54ms
step:428/1770 train_time:41748ms step_avg:97.54ms
step:429/1770 train_time:41848ms step_avg:97.55ms
step:430/1770 train_time:41946ms step_avg:97.55ms
step:431/1770 train_time:42045ms step_avg:97.55ms
step:432/1770 train_time:42144ms step_avg:97.56ms
step:433/1770 train_time:42243ms step_avg:97.56ms
step:434/1770 train_time:42342ms step_avg:97.56ms
step:435/1770 train_time:42441ms step_avg:97.57ms
step:436/1770 train_time:42539ms step_avg:97.57ms
step:437/1770 train_time:42639ms step_avg:97.57ms
step:438/1770 train_time:42739ms step_avg:97.58ms
step:439/1770 train_time:42837ms step_avg:97.58ms
step:440/1770 train_time:42936ms step_avg:97.58ms
step:441/1770 train_time:43035ms step_avg:97.59ms
step:442/1770 train_time:43135ms step_avg:97.59ms
step:443/1770 train_time:43234ms step_avg:97.59ms
step:444/1770 train_time:43334ms step_avg:97.60ms
step:445/1770 train_time:43435ms step_avg:97.61ms
step:446/1770 train_time:43536ms step_avg:97.61ms
step:447/1770 train_time:43637ms step_avg:97.62ms
step:448/1770 train_time:43737ms step_avg:97.63ms
step:449/1770 train_time:43836ms step_avg:97.63ms
step:450/1770 train_time:43936ms step_avg:97.64ms
step:451/1770 train_time:44037ms step_avg:97.64ms
step:452/1770 train_time:44135ms step_avg:97.64ms
step:453/1770 train_time:44235ms step_avg:97.65ms
step:454/1770 train_time:44336ms step_avg:97.66ms
step:455/1770 train_time:44435ms step_avg:97.66ms
step:456/1770 train_time:44536ms step_avg:97.67ms
step:457/1770 train_time:44635ms step_avg:97.67ms
step:458/1770 train_time:44736ms step_avg:97.68ms
step:459/1770 train_time:44836ms step_avg:97.68ms
step:460/1770 train_time:44935ms step_avg:97.69ms
step:461/1770 train_time:45035ms step_avg:97.69ms
step:462/1770 train_time:45136ms step_avg:97.70ms
step:463/1770 train_time:45235ms step_avg:97.70ms
step:464/1770 train_time:45336ms step_avg:97.71ms
step:465/1770 train_time:45436ms step_avg:97.71ms
step:466/1770 train_time:45535ms step_avg:97.72ms
step:467/1770 train_time:45636ms step_avg:97.72ms
step:468/1770 train_time:45735ms step_avg:97.73ms
step:469/1770 train_time:45835ms step_avg:97.73ms
step:470/1770 train_time:45935ms step_avg:97.73ms
step:471/1770 train_time:46035ms step_avg:97.74ms
step:472/1770 train_time:46134ms step_avg:97.74ms
step:473/1770 train_time:46233ms step_avg:97.75ms
step:474/1770 train_time:46337ms step_avg:97.76ms
step:475/1770 train_time:46433ms step_avg:97.75ms
step:476/1770 train_time:46533ms step_avg:97.76ms
step:477/1770 train_time:46633ms step_avg:97.76ms
step:478/1770 train_time:46733ms step_avg:97.77ms
step:479/1770 train_time:46832ms step_avg:97.77ms
step:480/1770 train_time:46933ms step_avg:97.78ms
step:481/1770 train_time:47033ms step_avg:97.78ms
step:482/1770 train_time:47134ms step_avg:97.79ms
step:483/1770 train_time:47233ms step_avg:97.79ms
step:484/1770 train_time:47334ms step_avg:97.80ms
step:485/1770 train_time:47434ms step_avg:97.80ms
step:486/1770 train_time:47535ms step_avg:97.81ms
step:487/1770 train_time:47634ms step_avg:97.81ms
step:488/1770 train_time:47733ms step_avg:97.81ms
step:489/1770 train_time:47832ms step_avg:97.82ms
step:490/1770 train_time:47931ms step_avg:97.82ms
step:491/1770 train_time:48033ms step_avg:97.83ms
step:492/1770 train_time:48130ms step_avg:97.82ms
step:493/1770 train_time:48229ms step_avg:97.83ms
step:494/1770 train_time:48328ms step_avg:97.83ms
step:495/1770 train_time:48427ms step_avg:97.83ms
step:496/1770 train_time:48525ms step_avg:97.83ms
step:497/1770 train_time:48624ms step_avg:97.84ms
step:498/1770 train_time:48723ms step_avg:97.84ms
step:499/1770 train_time:48822ms step_avg:97.84ms
step:500/1770 train_time:48920ms step_avg:97.84ms
step:500/1770 val_loss:3.7454 train_time:49014ms step_avg:98.03ms
step:501/1770 train_time:49033ms step_avg:97.87ms
step:502/1770 train_time:49124ms step_avg:97.86ms
step:503/1770 train_time:49225ms step_avg:97.86ms
step:504/1770 train_time:49324ms step_avg:97.87ms
step:505/1770 train_time:49424ms step_avg:97.87ms
step:506/1770 train_time:49523ms step_avg:97.87ms
step:507/1770 train_time:49622ms step_avg:97.87ms
step:508/1770 train_time:49722ms step_avg:97.88ms
step:509/1770 train_time:49821ms step_avg:97.88ms
step:510/1770 train_time:49920ms step_avg:97.88ms
step:511/1770 train_time:50020ms step_avg:97.89ms
step:512/1770 train_time:50120ms step_avg:97.89ms
step:513/1770 train_time:50222ms step_avg:97.90ms
step:514/1770 train_time:50322ms step_avg:97.90ms
step:515/1770 train_time:50422ms step_avg:97.91ms
step:516/1770 train_time:50522ms step_avg:97.91ms
step:517/1770 train_time:50622ms step_avg:97.92ms
step:518/1770 train_time:50722ms step_avg:97.92ms
step:519/1770 train_time:50822ms step_avg:97.92ms
step:520/1770 train_time:50921ms step_avg:97.93ms
step:521/1770 train_time:51021ms step_avg:97.93ms
step:522/1770 train_time:51121ms step_avg:97.93ms
step:523/1770 train_time:51222ms step_avg:97.94ms
step:524/1770 train_time:51323ms step_avg:97.94ms
step:525/1770 train_time:51422ms step_avg:97.95ms
step:526/1770 train_time:51522ms step_avg:97.95ms
step:527/1770 train_time:51621ms step_avg:97.95ms
step:528/1770 train_time:51726ms step_avg:97.97ms
step:529/1770 train_time:51820ms step_avg:97.96ms
step:530/1770 train_time:51920ms step_avg:97.96ms
step:531/1770 train_time:52019ms step_avg:97.96ms
step:532/1770 train_time:52119ms step_avg:97.97ms
step:533/1770 train_time:52219ms step_avg:97.97ms
step:534/1770 train_time:52320ms step_avg:97.98ms
step:535/1770 train_time:52420ms step_avg:97.98ms
step:536/1770 train_time:52520ms step_avg:97.99ms
step:537/1770 train_time:52621ms step_avg:97.99ms
step:538/1770 train_time:52720ms step_avg:97.99ms
step:539/1770 train_time:52820ms step_avg:98.00ms
step:540/1770 train_time:52920ms step_avg:98.00ms
step:541/1770 train_time:53023ms step_avg:98.01ms
step:542/1770 train_time:53119ms step_avg:98.01ms
step:543/1770 train_time:53219ms step_avg:98.01ms
step:544/1770 train_time:53320ms step_avg:98.01ms
step:545/1770 train_time:53420ms step_avg:98.02ms
step:546/1770 train_time:53521ms step_avg:98.02ms
step:547/1770 train_time:53621ms step_avg:98.03ms
step:548/1770 train_time:53721ms step_avg:98.03ms
step:549/1770 train_time:53821ms step_avg:98.03ms
step:550/1770 train_time:53920ms step_avg:98.04ms
step:551/1770 train_time:54020ms step_avg:98.04ms
step:552/1770 train_time:54119ms step_avg:98.04ms
step:553/1770 train_time:54218ms step_avg:98.04ms
step:554/1770 train_time:54318ms step_avg:98.05ms
step:555/1770 train_time:54418ms step_avg:98.05ms
step:556/1770 train_time:54518ms step_avg:98.05ms
step:557/1770 train_time:54618ms step_avg:98.06ms
step:558/1770 train_time:54718ms step_avg:98.06ms
step:559/1770 train_time:54818ms step_avg:98.06ms
step:560/1770 train_time:54917ms step_avg:98.07ms
step:561/1770 train_time:55016ms step_avg:98.07ms
step:562/1770 train_time:55116ms step_avg:98.07ms
step:563/1770 train_time:55215ms step_avg:98.07ms
step:564/1770 train_time:55314ms step_avg:98.08ms
step:565/1770 train_time:55413ms step_avg:98.08ms
step:566/1770 train_time:55513ms step_avg:98.08ms
step:567/1770 train_time:55612ms step_avg:98.08ms
step:568/1770 train_time:55712ms step_avg:98.08ms
step:569/1770 train_time:55812ms step_avg:98.09ms
step:570/1770 train_time:55912ms step_avg:98.09ms
step:571/1770 train_time:56012ms step_avg:98.09ms
step:572/1770 train_time:56113ms step_avg:98.10ms
step:573/1770 train_time:56212ms step_avg:98.10ms
step:574/1770 train_time:56311ms step_avg:98.10ms
step:575/1770 train_time:56410ms step_avg:98.10ms
step:576/1770 train_time:56509ms step_avg:98.11ms
step:577/1770 train_time:56609ms step_avg:98.11ms
step:578/1770 train_time:56708ms step_avg:98.11ms
step:579/1770 train_time:56807ms step_avg:98.11ms
step:580/1770 train_time:56907ms step_avg:98.11ms
step:581/1770 train_time:57007ms step_avg:98.12ms
step:582/1770 train_time:57106ms step_avg:98.12ms
step:583/1770 train_time:57206ms step_avg:98.12ms
step:584/1770 train_time:57306ms step_avg:98.13ms
step:585/1770 train_time:57406ms step_avg:98.13ms
step:586/1770 train_time:57506ms step_avg:98.13ms
step:587/1770 train_time:57606ms step_avg:98.14ms
step:588/1770 train_time:57706ms step_avg:98.14ms
step:589/1770 train_time:57805ms step_avg:98.14ms
step:590/1770 train_time:57905ms step_avg:98.14ms
step:591/1770 train_time:58005ms step_avg:98.15ms
step:592/1770 train_time:58105ms step_avg:98.15ms
step:593/1770 train_time:58205ms step_avg:98.15ms
step:594/1770 train_time:58306ms step_avg:98.16ms
step:595/1770 train_time:58406ms step_avg:98.16ms
step:596/1770 train_time:58505ms step_avg:98.16ms
step:597/1770 train_time:58606ms step_avg:98.17ms
step:598/1770 train_time:58705ms step_avg:98.17ms
step:599/1770 train_time:58805ms step_avg:98.17ms
step:600/1770 train_time:58905ms step_avg:98.17ms
step:601/1770 train_time:59004ms step_avg:98.18ms
step:602/1770 train_time:59104ms step_avg:98.18ms
step:603/1770 train_time:59204ms step_avg:98.18ms
step:604/1770 train_time:59304ms step_avg:98.19ms
step:605/1770 train_time:59405ms step_avg:98.19ms
step:606/1770 train_time:59505ms step_avg:98.19ms
step:607/1770 train_time:59605ms step_avg:98.20ms
step:608/1770 train_time:59706ms step_avg:98.20ms
step:609/1770 train_time:59806ms step_avg:98.20ms
step:610/1770 train_time:59906ms step_avg:98.21ms
step:611/1770 train_time:60006ms step_avg:98.21ms
step:612/1770 train_time:60106ms step_avg:98.21ms
step:613/1770 train_time:60206ms step_avg:98.21ms
step:614/1770 train_time:60306ms step_avg:98.22ms
step:615/1770 train_time:60406ms step_avg:98.22ms
step:616/1770 train_time:60506ms step_avg:98.22ms
step:617/1770 train_time:60606ms step_avg:98.23ms
step:618/1770 train_time:60706ms step_avg:98.23ms
step:619/1770 train_time:60806ms step_avg:98.23ms
step:620/1770 train_time:60905ms step_avg:98.23ms
step:621/1770 train_time:61006ms step_avg:98.24ms
step:622/1770 train_time:61106ms step_avg:98.24ms
step:623/1770 train_time:61205ms step_avg:98.24ms
step:624/1770 train_time:61306ms step_avg:98.25ms
step:625/1770 train_time:61406ms step_avg:98.25ms
step:625/1770 val_loss:3.6627 train_time:61501ms step_avg:98.40ms
step:626/1770 train_time:61520ms step_avg:98.27ms
step:627/1770 train_time:61613ms step_avg:98.27ms
step:628/1770 train_time:61716ms step_avg:98.27ms
step:629/1770 train_time:61815ms step_avg:98.28ms
step:630/1770 train_time:61914ms step_avg:98.28ms
step:631/1770 train_time:62013ms step_avg:98.28ms
step:632/1770 train_time:62112ms step_avg:98.28ms
step:633/1770 train_time:62211ms step_avg:98.28ms
step:634/1770 train_time:62310ms step_avg:98.28ms
step:635/1770 train_time:62410ms step_avg:98.28ms
step:636/1770 train_time:62509ms step_avg:98.28ms
step:637/1770 train_time:62608ms step_avg:98.29ms
step:638/1770 train_time:62708ms step_avg:98.29ms
step:639/1770 train_time:62808ms step_avg:98.29ms
step:640/1770 train_time:62907ms step_avg:98.29ms
step:641/1770 train_time:63007ms step_avg:98.29ms
step:642/1770 train_time:63106ms step_avg:98.30ms
step:643/1770 train_time:63206ms step_avg:98.30ms
step:644/1770 train_time:63305ms step_avg:98.30ms
step:645/1770 train_time:63404ms step_avg:98.30ms
step:646/1770 train_time:63503ms step_avg:98.30ms
step:647/1770 train_time:63603ms step_avg:98.30ms
step:648/1770 train_time:63703ms step_avg:98.31ms
step:649/1770 train_time:63803ms step_avg:98.31ms
step:650/1770 train_time:63903ms step_avg:98.31ms
step:651/1770 train_time:64004ms step_avg:98.32ms
step:652/1770 train_time:64104ms step_avg:98.32ms
step:653/1770 train_time:64204ms step_avg:98.32ms
step:654/1770 train_time:64303ms step_avg:98.32ms
step:655/1770 train_time:64403ms step_avg:98.33ms
step:656/1770 train_time:64503ms step_avg:98.33ms
step:657/1770 train_time:64603ms step_avg:98.33ms
step:658/1770 train_time:64704ms step_avg:98.33ms
step:659/1770 train_time:64805ms step_avg:98.34ms
step:660/1770 train_time:64906ms step_avg:98.34ms
step:661/1770 train_time:65007ms step_avg:98.35ms
step:662/1770 train_time:65108ms step_avg:98.35ms
step:663/1770 train_time:65209ms step_avg:98.35ms
step:664/1770 train_time:65309ms step_avg:98.36ms
step:665/1770 train_time:65411ms step_avg:98.36ms
step:666/1770 train_time:65512ms step_avg:98.37ms
step:667/1770 train_time:65613ms step_avg:98.37ms
step:668/1770 train_time:65714ms step_avg:98.37ms
step:669/1770 train_time:65815ms step_avg:98.38ms
step:670/1770 train_time:65916ms step_avg:98.38ms
step:671/1770 train_time:66016ms step_avg:98.38ms
step:672/1770 train_time:66117ms step_avg:98.39ms
step:673/1770 train_time:66219ms step_avg:98.39ms
step:674/1770 train_time:66320ms step_avg:98.40ms
step:675/1770 train_time:66423ms step_avg:98.40ms
step:676/1770 train_time:66524ms step_avg:98.41ms
step:677/1770 train_time:66625ms step_avg:98.41ms
step:678/1770 train_time:66726ms step_avg:98.42ms
step:679/1770 train_time:66827ms step_avg:98.42ms
step:680/1770 train_time:66927ms step_avg:98.42ms
step:681/1770 train_time:67028ms step_avg:98.43ms
step:682/1770 train_time:67129ms step_avg:98.43ms
step:683/1770 train_time:67230ms step_avg:98.43ms
step:684/1770 train_time:67332ms step_avg:98.44ms
step:685/1770 train_time:67433ms step_avg:98.44ms
step:686/1770 train_time:67534ms step_avg:98.45ms
step:687/1770 train_time:67635ms step_avg:98.45ms
step:688/1770 train_time:67736ms step_avg:98.45ms
step:689/1770 train_time:67836ms step_avg:98.46ms
step:690/1770 train_time:67938ms step_avg:98.46ms
step:691/1770 train_time:68041ms step_avg:98.47ms
step:692/1770 train_time:68142ms step_avg:98.47ms
step:693/1770 train_time:68244ms step_avg:98.48ms
step:694/1770 train_time:68345ms step_avg:98.48ms
step:695/1770 train_time:68449ms step_avg:98.49ms
step:696/1770 train_time:68548ms step_avg:98.49ms
step:697/1770 train_time:68649ms step_avg:98.49ms
step:698/1770 train_time:68749ms step_avg:98.49ms
step:699/1770 train_time:68851ms step_avg:98.50ms
step:700/1770 train_time:68951ms step_avg:98.50ms
step:701/1770 train_time:69053ms step_avg:98.51ms
step:702/1770 train_time:69155ms step_avg:98.51ms
step:703/1770 train_time:69255ms step_avg:98.51ms
step:704/1770 train_time:69356ms step_avg:98.52ms
step:705/1770 train_time:69458ms step_avg:98.52ms
step:706/1770 train_time:69559ms step_avg:98.53ms
step:707/1770 train_time:69661ms step_avg:98.53ms
step:708/1770 train_time:69764ms step_avg:98.54ms
step:709/1770 train_time:69865ms step_avg:98.54ms
step:710/1770 train_time:69966ms step_avg:98.54ms
step:711/1770 train_time:70067ms step_avg:98.55ms
step:712/1770 train_time:70169ms step_avg:98.55ms
step:713/1770 train_time:70269ms step_avg:98.55ms
step:714/1770 train_time:70370ms step_avg:98.56ms
step:715/1770 train_time:70472ms step_avg:98.56ms
step:716/1770 train_time:70573ms step_avg:98.57ms
step:717/1770 train_time:70675ms step_avg:98.57ms
step:718/1770 train_time:70776ms step_avg:98.57ms
step:719/1770 train_time:70877ms step_avg:98.58ms
step:720/1770 train_time:70978ms step_avg:98.58ms
step:721/1770 train_time:71079ms step_avg:98.58ms
step:722/1770 train_time:71182ms step_avg:98.59ms
step:723/1770 train_time:71283ms step_avg:98.59ms
step:724/1770 train_time:71384ms step_avg:98.60ms
step:725/1770 train_time:71486ms step_avg:98.60ms
step:726/1770 train_time:71587ms step_avg:98.60ms
step:727/1770 train_time:71688ms step_avg:98.61ms
step:728/1770 train_time:71789ms step_avg:98.61ms
step:729/1770 train_time:71890ms step_avg:98.61ms
step:730/1770 train_time:71991ms step_avg:98.62ms
step:731/1770 train_time:72092ms step_avg:98.62ms
step:732/1770 train_time:72193ms step_avg:98.62ms
step:733/1770 train_time:72294ms step_avg:98.63ms
step:734/1770 train_time:72395ms step_avg:98.63ms
step:735/1770 train_time:72495ms step_avg:98.63ms
step:736/1770 train_time:72597ms step_avg:98.64ms
step:737/1770 train_time:72698ms step_avg:98.64ms
step:738/1770 train_time:72801ms step_avg:98.65ms
step:739/1770 train_time:72903ms step_avg:98.65ms
step:740/1770 train_time:73005ms step_avg:98.66ms
step:741/1770 train_time:73106ms step_avg:98.66ms
step:742/1770 train_time:73207ms step_avg:98.66ms
step:743/1770 train_time:73308ms step_avg:98.66ms
step:744/1770 train_time:73409ms step_avg:98.67ms
step:745/1770 train_time:73509ms step_avg:98.67ms
step:746/1770 train_time:73611ms step_avg:98.67ms
step:747/1770 train_time:73712ms step_avg:98.68ms
step:748/1770 train_time:73813ms step_avg:98.68ms
step:749/1770 train_time:73914ms step_avg:98.68ms
step:750/1770 train_time:74015ms step_avg:98.69ms
step:750/1770 val_loss:3.5994 train_time:74110ms step_avg:98.81ms
step:751/1770 train_time:74129ms step_avg:98.71ms
step:752/1770 train_time:74223ms step_avg:98.70ms
step:753/1770 train_time:74325ms step_avg:98.70ms
step:754/1770 train_time:74425ms step_avg:98.71ms
step:755/1770 train_time:74526ms step_avg:98.71ms
step:756/1770 train_time:74627ms step_avg:98.71ms
step:757/1770 train_time:74727ms step_avg:98.72ms
step:758/1770 train_time:74829ms step_avg:98.72ms
step:759/1770 train_time:74929ms step_avg:98.72ms
step:760/1770 train_time:75030ms step_avg:98.72ms
step:761/1770 train_time:75132ms step_avg:98.73ms
step:762/1770 train_time:75233ms step_avg:98.73ms
step:763/1770 train_time:75335ms step_avg:98.74ms
step:764/1770 train_time:75438ms step_avg:98.74ms
step:765/1770 train_time:75540ms step_avg:98.74ms
step:766/1770 train_time:75641ms step_avg:98.75ms
step:767/1770 train_time:75743ms step_avg:98.75ms
step:768/1770 train_time:75844ms step_avg:98.75ms
step:769/1770 train_time:75946ms step_avg:98.76ms
step:770/1770 train_time:76047ms step_avg:98.76ms
step:771/1770 train_time:76148ms step_avg:98.77ms
step:772/1770 train_time:76249ms step_avg:98.77ms
step:773/1770 train_time:76350ms step_avg:98.77ms
step:774/1770 train_time:76450ms step_avg:98.77ms
step:775/1770 train_time:76552ms step_avg:98.78ms
step:776/1770 train_time:76653ms step_avg:98.78ms
step:777/1770 train_time:76754ms step_avg:98.78ms
step:778/1770 train_time:76855ms step_avg:98.78ms
step:779/1770 train_time:76956ms step_avg:98.79ms
step:780/1770 train_time:77058ms step_avg:98.79ms
step:781/1770 train_time:77160ms step_avg:98.80ms
step:782/1770 train_time:77262ms step_avg:98.80ms
step:783/1770 train_time:77364ms step_avg:98.80ms
step:784/1770 train_time:77465ms step_avg:98.81ms
step:785/1770 train_time:77566ms step_avg:98.81ms
step:786/1770 train_time:77667ms step_avg:98.81ms
step:787/1770 train_time:77767ms step_avg:98.82ms
step:788/1770 train_time:77868ms step_avg:98.82ms
step:789/1770 train_time:77970ms step_avg:98.82ms
step:790/1770 train_time:78071ms step_avg:98.82ms
step:791/1770 train_time:78172ms step_avg:98.83ms
step:792/1770 train_time:78274ms step_avg:98.83ms
step:793/1770 train_time:78375ms step_avg:98.83ms
step:794/1770 train_time:78479ms step_avg:98.84ms
step:795/1770 train_time:78581ms step_avg:98.84ms
step:796/1770 train_time:78683ms step_avg:98.85ms
step:797/1770 train_time:78785ms step_avg:98.85ms
step:798/1770 train_time:78886ms step_avg:98.85ms
step:799/1770 train_time:78987ms step_avg:98.86ms
step:800/1770 train_time:79088ms step_avg:98.86ms
step:801/1770 train_time:79189ms step_avg:98.86ms
step:802/1770 train_time:79291ms step_avg:98.87ms
step:803/1770 train_time:79392ms step_avg:98.87ms
step:804/1770 train_time:79492ms step_avg:98.87ms
step:805/1770 train_time:79593ms step_avg:98.87ms
step:806/1770 train_time:79695ms step_avg:98.88ms
step:807/1770 train_time:79796ms step_avg:98.88ms
step:808/1770 train_time:79898ms step_avg:98.88ms
step:809/1770 train_time:80000ms step_avg:98.89ms
step:810/1770 train_time:80103ms step_avg:98.89ms
step:811/1770 train_time:80204ms step_avg:98.90ms
step:812/1770 train_time:80306ms step_avg:98.90ms
step:813/1770 train_time:80407ms step_avg:98.90ms
step:814/1770 train_time:80508ms step_avg:98.90ms
step:815/1770 train_time:80609ms step_avg:98.91ms
step:816/1770 train_time:80710ms step_avg:98.91ms
step:817/1770 train_time:80812ms step_avg:98.91ms
step:818/1770 train_time:80913ms step_avg:98.92ms
step:819/1770 train_time:81020ms step_avg:98.93ms
step:820/1770 train_time:81116ms step_avg:98.92ms
step:821/1770 train_time:81218ms step_avg:98.93ms
step:822/1770 train_time:81320ms step_avg:98.93ms
step:823/1770 train_time:81423ms step_avg:98.93ms
step:824/1770 train_time:81524ms step_avg:98.94ms
step:825/1770 train_time:81625ms step_avg:98.94ms
step:826/1770 train_time:81726ms step_avg:98.94ms
step:827/1770 train_time:81828ms step_avg:98.95ms
step:828/1770 train_time:81929ms step_avg:98.95ms
step:829/1770 train_time:82030ms step_avg:98.95ms
step:830/1770 train_time:82131ms step_avg:98.95ms
step:831/1770 train_time:82232ms step_avg:98.96ms
step:832/1770 train_time:82333ms step_avg:98.96ms
step:833/1770 train_time:82434ms step_avg:98.96ms
step:834/1770 train_time:82535ms step_avg:98.96ms
step:835/1770 train_time:82636ms step_avg:98.97ms
step:836/1770 train_time:82739ms step_avg:98.97ms
step:837/1770 train_time:82841ms step_avg:98.97ms
step:838/1770 train_time:82944ms step_avg:98.98ms
step:839/1770 train_time:83046ms step_avg:98.98ms
step:840/1770 train_time:83148ms step_avg:98.99ms
step:841/1770 train_time:83251ms step_avg:98.99ms
step:842/1770 train_time:83352ms step_avg:98.99ms
step:843/1770 train_time:83453ms step_avg:98.99ms
step:844/1770 train_time:83553ms step_avg:99.00ms
step:845/1770 train_time:83654ms step_avg:99.00ms
step:846/1770 train_time:83755ms step_avg:99.00ms
step:847/1770 train_time:83858ms step_avg:99.01ms
step:848/1770 train_time:83960ms step_avg:99.01ms
step:849/1770 train_time:84062ms step_avg:99.01ms
step:850/1770 train_time:84164ms step_avg:99.02ms
step:851/1770 train_time:84265ms step_avg:99.02ms
step:852/1770 train_time:84367ms step_avg:99.02ms
step:853/1770 train_time:84468ms step_avg:99.02ms
step:854/1770 train_time:84569ms step_avg:99.03ms
step:855/1770 train_time:84670ms step_avg:99.03ms
step:856/1770 train_time:84771ms step_avg:99.03ms
step:857/1770 train_time:84872ms step_avg:99.03ms
step:858/1770 train_time:84973ms step_avg:99.04ms
step:859/1770 train_time:85074ms step_avg:99.04ms
step:860/1770 train_time:85176ms step_avg:99.04ms
step:861/1770 train_time:85279ms step_avg:99.05ms
step:862/1770 train_time:85381ms step_avg:99.05ms
step:863/1770 train_time:85482ms step_avg:99.05ms
step:864/1770 train_time:85583ms step_avg:99.05ms
step:865/1770 train_time:85684ms step_avg:99.06ms
step:866/1770 train_time:85786ms step_avg:99.06ms
step:867/1770 train_time:85888ms step_avg:99.06ms
step:868/1770 train_time:85989ms step_avg:99.07ms
step:869/1770 train_time:86089ms step_avg:99.07ms
step:870/1770 train_time:86190ms step_avg:99.07ms
step:871/1770 train_time:86291ms step_avg:99.07ms
step:872/1770 train_time:86393ms step_avg:99.07ms
step:873/1770 train_time:86494ms step_avg:99.08ms
step:874/1770 train_time:86595ms step_avg:99.08ms
step:875/1770 train_time:86696ms step_avg:99.08ms
step:875/1770 val_loss:3.5508 train_time:86793ms step_avg:99.19ms
step:876/1770 train_time:86813ms step_avg:99.10ms
step:877/1770 train_time:86909ms step_avg:99.10ms
step:878/1770 train_time:87011ms step_avg:99.10ms
step:879/1770 train_time:87112ms step_avg:99.10ms
step:880/1770 train_time:87213ms step_avg:99.11ms
step:881/1770 train_time:87314ms step_avg:99.11ms
step:882/1770 train_time:87415ms step_avg:99.11ms
step:883/1770 train_time:87516ms step_avg:99.11ms
step:884/1770 train_time:87617ms step_avg:99.11ms
step:885/1770 train_time:87718ms step_avg:99.12ms
step:886/1770 train_time:87820ms step_avg:99.12ms
step:887/1770 train_time:87921ms step_avg:99.12ms
step:888/1770 train_time:88023ms step_avg:99.12ms
step:889/1770 train_time:88124ms step_avg:99.13ms
step:890/1770 train_time:88224ms step_avg:99.13ms
step:891/1770 train_time:88328ms step_avg:99.13ms
step:892/1770 train_time:88430ms step_avg:99.14ms
step:893/1770 train_time:88533ms step_avg:99.14ms
step:894/1770 train_time:88634ms step_avg:99.14ms
step:895/1770 train_time:88735ms step_avg:99.15ms
step:896/1770 train_time:88837ms step_avg:99.15ms
step:897/1770 train_time:88937ms step_avg:99.15ms
step:898/1770 train_time:89039ms step_avg:99.15ms
step:899/1770 train_time:89141ms step_avg:99.16ms
step:900/1770 train_time:89242ms step_avg:99.16ms
step:901/1770 train_time:89343ms step_avg:99.16ms
step:902/1770 train_time:89444ms step_avg:99.16ms
step:903/1770 train_time:89546ms step_avg:99.16ms
step:904/1770 train_time:89649ms step_avg:99.17ms
step:905/1770 train_time:89751ms step_avg:99.17ms
step:906/1770 train_time:89853ms step_avg:99.18ms
step:907/1770 train_time:89955ms step_avg:99.18ms
step:908/1770 train_time:90056ms step_avg:99.18ms
step:909/1770 train_time:90157ms step_avg:99.18ms
step:910/1770 train_time:90258ms step_avg:99.18ms
step:911/1770 train_time:90360ms step_avg:99.19ms
step:912/1770 train_time:90461ms step_avg:99.19ms
step:913/1770 train_time:90562ms step_avg:99.19ms
step:914/1770 train_time:90664ms step_avg:99.19ms
step:915/1770 train_time:90765ms step_avg:99.20ms
step:916/1770 train_time:90867ms step_avg:99.20ms
step:917/1770 train_time:90969ms step_avg:99.20ms
step:918/1770 train_time:91071ms step_avg:99.21ms
step:919/1770 train_time:91173ms step_avg:99.21ms
step:920/1770 train_time:91276ms step_avg:99.21ms
step:921/1770 train_time:91379ms step_avg:99.22ms
step:922/1770 train_time:91481ms step_avg:99.22ms
step:923/1770 train_time:91584ms step_avg:99.22ms
step:924/1770 train_time:91686ms step_avg:99.23ms
step:925/1770 train_time:91789ms step_avg:99.23ms
step:926/1770 train_time:91892ms step_avg:99.24ms
step:927/1770 train_time:91995ms step_avg:99.24ms
step:928/1770 train_time:92096ms step_avg:99.24ms
step:929/1770 train_time:92199ms step_avg:99.24ms
step:930/1770 train_time:92301ms step_avg:99.25ms
step:931/1770 train_time:92404ms step_avg:99.25ms
step:932/1770 train_time:92506ms step_avg:99.26ms
step:933/1770 train_time:92609ms step_avg:99.26ms
step:934/1770 train_time:92712ms step_avg:99.26ms
step:935/1770 train_time:92815ms step_avg:99.27ms
step:936/1770 train_time:92917ms step_avg:99.27ms
step:937/1770 train_time:93020ms step_avg:99.27ms
step:938/1770 train_time:93124ms step_avg:99.28ms
step:939/1770 train_time:93224ms step_avg:99.28ms
step:940/1770 train_time:93327ms step_avg:99.28ms
step:941/1770 train_time:93431ms step_avg:99.29ms
step:942/1770 train_time:93534ms step_avg:99.29ms
step:943/1770 train_time:93637ms step_avg:99.30ms
step:944/1770 train_time:93739ms step_avg:99.30ms
step:945/1770 train_time:93842ms step_avg:99.30ms
step:946/1770 train_time:93945ms step_avg:99.31ms
step:947/1770 train_time:94048ms step_avg:99.31ms
step:948/1770 train_time:94150ms step_avg:99.31ms
step:949/1770 train_time:94254ms step_avg:99.32ms
step:950/1770 train_time:94357ms step_avg:99.32ms
step:951/1770 train_time:94459ms step_avg:99.33ms
step:952/1770 train_time:94562ms step_avg:99.33ms
step:953/1770 train_time:94664ms step_avg:99.33ms
step:954/1770 train_time:94768ms step_avg:99.34ms
step:955/1770 train_time:94872ms step_avg:99.34ms
step:956/1770 train_time:94974ms step_avg:99.34ms
step:957/1770 train_time:95076ms step_avg:99.35ms
step:958/1770 train_time:95179ms step_avg:99.35ms
step:959/1770 train_time:95281ms step_avg:99.35ms
step:960/1770 train_time:95383ms step_avg:99.36ms
step:961/1770 train_time:95486ms step_avg:99.36ms
step:962/1770 train_time:95590ms step_avg:99.37ms
step:963/1770 train_time:95692ms step_avg:99.37ms
step:964/1770 train_time:95795ms step_avg:99.37ms
step:965/1770 train_time:95897ms step_avg:99.38ms
step:966/1770 train_time:96000ms step_avg:99.38ms
step:967/1770 train_time:96102ms step_avg:99.38ms
step:968/1770 train_time:96205ms step_avg:99.39ms
step:969/1770 train_time:96315ms step_avg:99.40ms
step:970/1770 train_time:96410ms step_avg:99.39ms
step:971/1770 train_time:96514ms step_avg:99.40ms
step:972/1770 train_time:96616ms step_avg:99.40ms
step:973/1770 train_time:96718ms step_avg:99.40ms
step:974/1770 train_time:96821ms step_avg:99.41ms
step:975/1770 train_time:96924ms step_avg:99.41ms
step:976/1770 train_time:97027ms step_avg:99.41ms
step:977/1770 train_time:97131ms step_avg:99.42ms
step:978/1770 train_time:97234ms step_avg:99.42ms
step:979/1770 train_time:97337ms step_avg:99.42ms
step:980/1770 train_time:97439ms step_avg:99.43ms
step:981/1770 train_time:97541ms step_avg:99.43ms
step:982/1770 train_time:97644ms step_avg:99.43ms
step:983/1770 train_time:97748ms step_avg:99.44ms
step:984/1770 train_time:97851ms step_avg:99.44ms
step:985/1770 train_time:97955ms step_avg:99.45ms
step:986/1770 train_time:98057ms step_avg:99.45ms
step:987/1770 train_time:98160ms step_avg:99.45ms
step:988/1770 train_time:98262ms step_avg:99.46ms
step:989/1770 train_time:98366ms step_avg:99.46ms
step:990/1770 train_time:98468ms step_avg:99.46ms
step:991/1770 train_time:98571ms step_avg:99.47ms
step:992/1770 train_time:98674ms step_avg:99.47ms
step:993/1770 train_time:98777ms step_avg:99.47ms
step:994/1770 train_time:98880ms step_avg:99.48ms
step:995/1770 train_time:98983ms step_avg:99.48ms
step:996/1770 train_time:99085ms step_avg:99.48ms
step:997/1770 train_time:99189ms step_avg:99.49ms
step:998/1770 train_time:99292ms step_avg:99.49ms
step:999/1770 train_time:99395ms step_avg:99.49ms
step:1000/1770 train_time:99497ms step_avg:99.50ms
step:1000/1770 val_loss:3.5120 train_time:99594ms step_avg:99.59ms
step:1001/1770 train_time:99613ms step_avg:99.51ms
step:1002/1770 train_time:99709ms step_avg:99.51ms
step:1003/1770 train_time:99812ms step_avg:99.51ms
step:1004/1770 train_time:99915ms step_avg:99.52ms
step:1005/1770 train_time:100017ms step_avg:99.52ms
step:1006/1770 train_time:100119ms step_avg:99.52ms
step:1007/1770 train_time:100222ms step_avg:99.52ms
step:1008/1770 train_time:100325ms step_avg:99.53ms
step:1009/1770 train_time:100427ms step_avg:99.53ms
step:1010/1770 train_time:100529ms step_avg:99.53ms
step:1011/1770 train_time:100633ms step_avg:99.54ms
step:1012/1770 train_time:100737ms step_avg:99.54ms
step:1013/1770 train_time:100840ms step_avg:99.55ms
step:1014/1770 train_time:100944ms step_avg:99.55ms
step:1015/1770 train_time:101047ms step_avg:99.55ms
step:1016/1770 train_time:101150ms step_avg:99.56ms
step:1017/1770 train_time:101253ms step_avg:99.56ms
step:1018/1770 train_time:101354ms step_avg:99.56ms
step:1019/1770 train_time:101456ms step_avg:99.56ms
step:1020/1770 train_time:101558ms step_avg:99.57ms
step:1021/1770 train_time:101661ms step_avg:99.57ms
step:1022/1770 train_time:101764ms step_avg:99.57ms
step:1023/1770 train_time:101867ms step_avg:99.58ms
step:1024/1770 train_time:101970ms step_avg:99.58ms
step:1025/1770 train_time:102073ms step_avg:99.58ms
step:1026/1770 train_time:102175ms step_avg:99.59ms
step:1027/1770 train_time:102278ms step_avg:99.59ms
step:1028/1770 train_time:102381ms step_avg:99.59ms
step:1029/1770 train_time:102484ms step_avg:99.60ms
step:1030/1770 train_time:102588ms step_avg:99.60ms
step:1031/1770 train_time:102690ms step_avg:99.60ms
step:1032/1770 train_time:102792ms step_avg:99.61ms
step:1033/1770 train_time:102895ms step_avg:99.61ms
step:1034/1770 train_time:102997ms step_avg:99.61ms
step:1035/1770 train_time:103099ms step_avg:99.61ms
step:1036/1770 train_time:103202ms step_avg:99.62ms
step:1037/1770 train_time:103305ms step_avg:99.62ms
step:1038/1770 train_time:103408ms step_avg:99.62ms
step:1039/1770 train_time:103510ms step_avg:99.63ms
step:1040/1770 train_time:103613ms step_avg:99.63ms
step:1041/1770 train_time:103715ms step_avg:99.63ms
step:1042/1770 train_time:103817ms step_avg:99.63ms
step:1043/1770 train_time:103920ms step_avg:99.64ms
step:1044/1770 train_time:104023ms step_avg:99.64ms
step:1045/1770 train_time:104127ms step_avg:99.64ms
step:1046/1770 train_time:104229ms step_avg:99.65ms
step:1047/1770 train_time:104331ms step_avg:99.65ms
step:1048/1770 train_time:104434ms step_avg:99.65ms
step:1049/1770 train_time:104537ms step_avg:99.65ms
step:1050/1770 train_time:104640ms step_avg:99.66ms
step:1051/1770 train_time:104743ms step_avg:99.66ms
step:1052/1770 train_time:104846ms step_avg:99.66ms
step:1053/1770 train_time:104949ms step_avg:99.67ms
step:1054/1770 train_time:105051ms step_avg:99.67ms
step:1055/1770 train_time:105154ms step_avg:99.67ms
step:1056/1770 train_time:105256ms step_avg:99.67ms
step:1057/1770 train_time:105358ms step_avg:99.68ms
step:1058/1770 train_time:105461ms step_avg:99.68ms
step:1059/1770 train_time:105565ms step_avg:99.68ms
step:1060/1770 train_time:105668ms step_avg:99.69ms
step:1061/1770 train_time:105770ms step_avg:99.69ms
step:1062/1770 train_time:105874ms step_avg:99.69ms
step:1063/1770 train_time:105979ms step_avg:99.70ms
step:1064/1770 train_time:106083ms step_avg:99.70ms
step:1065/1770 train_time:106186ms step_avg:99.71ms
step:1066/1770 train_time:106289ms step_avg:99.71ms
step:1067/1770 train_time:106391ms step_avg:99.71ms
step:1068/1770 train_time:106495ms step_avg:99.71ms
step:1069/1770 train_time:106598ms step_avg:99.72ms
step:1070/1770 train_time:106700ms step_avg:99.72ms
step:1071/1770 train_time:106806ms step_avg:99.73ms
step:1072/1770 train_time:106908ms step_avg:99.73ms
step:1073/1770 train_time:107010ms step_avg:99.73ms
step:1074/1770 train_time:107114ms step_avg:99.73ms
step:1075/1770 train_time:107216ms step_avg:99.74ms
step:1076/1770 train_time:107320ms step_avg:99.74ms
step:1077/1770 train_time:107423ms step_avg:99.74ms
step:1078/1770 train_time:107526ms step_avg:99.75ms
step:1079/1770 train_time:107629ms step_avg:99.75ms
step:1080/1770 train_time:107732ms step_avg:99.75ms
step:1081/1770 train_time:107834ms step_avg:99.75ms
step:1082/1770 train_time:107937ms step_avg:99.76ms
step:1083/1770 train_time:108039ms step_avg:99.76ms
step:1084/1770 train_time:108143ms step_avg:99.76ms
step:1085/1770 train_time:108246ms step_avg:99.77ms
step:1086/1770 train_time:108349ms step_avg:99.77ms
step:1087/1770 train_time:108451ms step_avg:99.77ms
step:1088/1770 train_time:108555ms step_avg:99.77ms
step:1089/1770 train_time:108658ms step_avg:99.78ms
step:1090/1770 train_time:108761ms step_avg:99.78ms
step:1091/1770 train_time:108864ms step_avg:99.78ms
step:1092/1770 train_time:108967ms step_avg:99.79ms
step:1093/1770 train_time:109070ms step_avg:99.79ms
step:1094/1770 train_time:109173ms step_avg:99.79ms
step:1095/1770 train_time:109276ms step_avg:99.80ms
step:1096/1770 train_time:109379ms step_avg:99.80ms
step:1097/1770 train_time:109482ms step_avg:99.80ms
step:1098/1770 train_time:109586ms step_avg:99.80ms
step:1099/1770 train_time:109689ms step_avg:99.81ms
step:1100/1770 train_time:109791ms step_avg:99.81ms
step:1101/1770 train_time:109894ms step_avg:99.81ms
step:1102/1770 train_time:109999ms step_avg:99.82ms
step:1103/1770 train_time:110100ms step_avg:99.82ms
step:1104/1770 train_time:110203ms step_avg:99.82ms
step:1105/1770 train_time:110306ms step_avg:99.82ms
step:1106/1770 train_time:110410ms step_avg:99.83ms
step:1107/1770 train_time:110512ms step_avg:99.83ms
step:1108/1770 train_time:110616ms step_avg:99.83ms
step:1109/1770 train_time:110719ms step_avg:99.84ms
step:1110/1770 train_time:110822ms step_avg:99.84ms
step:1111/1770 train_time:110925ms step_avg:99.84ms
step:1112/1770 train_time:111030ms step_avg:99.85ms
step:1113/1770 train_time:111133ms step_avg:99.85ms
step:1114/1770 train_time:111236ms step_avg:99.85ms
step:1115/1770 train_time:111339ms step_avg:99.86ms
step:1116/1770 train_time:111442ms step_avg:99.86ms
step:1117/1770 train_time:111546ms step_avg:99.86ms
step:1118/1770 train_time:111648ms step_avg:99.86ms
step:1119/1770 train_time:111751ms step_avg:99.87ms
step:1120/1770 train_time:111853ms step_avg:99.87ms
step:1121/1770 train_time:111955ms step_avg:99.87ms
step:1122/1770 train_time:112059ms step_avg:99.87ms
step:1123/1770 train_time:112160ms step_avg:99.88ms
step:1124/1770 train_time:112264ms step_avg:99.88ms
step:1125/1770 train_time:112367ms step_avg:99.88ms
step:1125/1770 val_loss:3.4716 train_time:112464ms step_avg:99.97ms
step:1126/1770 train_time:112484ms step_avg:99.90ms
step:1127/1770 train_time:112578ms step_avg:99.89ms
step:1128/1770 train_time:112683ms step_avg:99.90ms
step:1129/1770 train_time:112786ms step_avg:99.90ms
step:1130/1770 train_time:112890ms step_avg:99.90ms
step:1131/1770 train_time:112993ms step_avg:99.91ms
step:1132/1770 train_time:113096ms step_avg:99.91ms
step:1133/1770 train_time:113198ms step_avg:99.91ms
step:1134/1770 train_time:113300ms step_avg:99.91ms
step:1135/1770 train_time:113403ms step_avg:99.91ms
step:1136/1770 train_time:113506ms step_avg:99.92ms
step:1137/1770 train_time:113611ms step_avg:99.92ms
step:1138/1770 train_time:113713ms step_avg:99.92ms
step:1139/1770 train_time:113817ms step_avg:99.93ms
step:1140/1770 train_time:113919ms step_avg:99.93ms
step:1141/1770 train_time:114022ms step_avg:99.93ms
step:1142/1770 train_time:114125ms step_avg:99.93ms
step:1143/1770 train_time:114227ms step_avg:99.94ms
step:1144/1770 train_time:114330ms step_avg:99.94ms
step:1145/1770 train_time:114433ms step_avg:99.94ms
step:1146/1770 train_time:114537ms step_avg:99.94ms
step:1147/1770 train_time:114640ms step_avg:99.95ms
step:1148/1770 train_time:114743ms step_avg:99.95ms
step:1149/1770 train_time:114845ms step_avg:99.95ms
step:1150/1770 train_time:114949ms step_avg:99.96ms
step:1151/1770 train_time:115054ms step_avg:99.96ms
step:1152/1770 train_time:115157ms step_avg:99.96ms
step:1153/1770 train_time:115260ms step_avg:99.96ms
step:1154/1770 train_time:115362ms step_avg:99.97ms
step:1155/1770 train_time:115465ms step_avg:99.97ms
step:1156/1770 train_time:115568ms step_avg:99.97ms
step:1157/1770 train_time:115674ms step_avg:99.98ms
step:1158/1770 train_time:115776ms step_avg:99.98ms
step:1159/1770 train_time:115879ms step_avg:99.98ms
step:1160/1770 train_time:115983ms step_avg:99.98ms
step:1161/1770 train_time:116085ms step_avg:99.99ms
step:1162/1770 train_time:116191ms step_avg:99.99ms
step:1163/1770 train_time:116293ms step_avg:99.99ms
step:1164/1770 train_time:116396ms step_avg:100.00ms
step:1165/1770 train_time:116499ms step_avg:100.00ms
step:1166/1770 train_time:116602ms step_avg:100.00ms
step:1167/1770 train_time:116704ms step_avg:100.00ms
step:1168/1770 train_time:116807ms step_avg:100.01ms
step:1169/1770 train_time:116910ms step_avg:100.01ms
step:1170/1770 train_time:117013ms step_avg:100.01ms
step:1171/1770 train_time:117116ms step_avg:100.01ms
step:1172/1770 train_time:117218ms step_avg:100.02ms
step:1173/1770 train_time:117322ms step_avg:100.02ms
step:1174/1770 train_time:117424ms step_avg:100.02ms
step:1175/1770 train_time:117528ms step_avg:100.02ms
step:1176/1770 train_time:117631ms step_avg:100.03ms
step:1177/1770 train_time:117733ms step_avg:100.03ms
step:1178/1770 train_time:117836ms step_avg:100.03ms
step:1179/1770 train_time:117939ms step_avg:100.03ms
step:1180/1770 train_time:118042ms step_avg:100.04ms
step:1181/1770 train_time:118145ms step_avg:100.04ms
step:1182/1770 train_time:118248ms step_avg:100.04ms
step:1183/1770 train_time:118353ms step_avg:100.04ms
step:1184/1770 train_time:118458ms step_avg:100.05ms
step:1185/1770 train_time:118561ms step_avg:100.05ms
step:1186/1770 train_time:118666ms step_avg:100.06ms
step:1187/1770 train_time:118772ms step_avg:100.06ms
step:1188/1770 train_time:118875ms step_avg:100.06ms
step:1189/1770 train_time:118979ms step_avg:100.07ms
step:1190/1770 train_time:119083ms step_avg:100.07ms
step:1191/1770 train_time:119186ms step_avg:100.07ms
step:1192/1770 train_time:119291ms step_avg:100.08ms
step:1193/1770 train_time:119395ms step_avg:100.08ms
step:1194/1770 train_time:119499ms step_avg:100.08ms
step:1195/1770 train_time:119603ms step_avg:100.09ms
step:1196/1770 train_time:119707ms step_avg:100.09ms
step:1197/1770 train_time:119811ms step_avg:100.09ms
step:1198/1770 train_time:119915ms step_avg:100.10ms
step:1199/1770 train_time:120019ms step_avg:100.10ms
step:1200/1770 train_time:120123ms step_avg:100.10ms
step:1201/1770 train_time:120229ms step_avg:100.11ms
step:1202/1770 train_time:120332ms step_avg:100.11ms
step:1203/1770 train_time:120436ms step_avg:100.11ms
step:1204/1770 train_time:120540ms step_avg:100.12ms
step:1205/1770 train_time:120643ms step_avg:100.12ms
step:1206/1770 train_time:120748ms step_avg:100.12ms
step:1207/1770 train_time:120853ms step_avg:100.13ms
step:1208/1770 train_time:120957ms step_avg:100.13ms
step:1209/1770 train_time:121061ms step_avg:100.13ms
step:1210/1770 train_time:121165ms step_avg:100.14ms
step:1211/1770 train_time:121272ms step_avg:100.14ms
step:1212/1770 train_time:121376ms step_avg:100.15ms
step:1213/1770 train_time:121480ms step_avg:100.15ms
step:1214/1770 train_time:121583ms step_avg:100.15ms
step:1215/1770 train_time:121687ms step_avg:100.15ms
step:1216/1770 train_time:121792ms step_avg:100.16ms
step:1217/1770 train_time:121896ms step_avg:100.16ms
step:1218/1770 train_time:122000ms step_avg:100.16ms
step:1219/1770 train_time:122105ms step_avg:100.17ms
step:1220/1770 train_time:122210ms step_avg:100.17ms
step:1221/1770 train_time:122314ms step_avg:100.17ms
step:1222/1770 train_time:122421ms step_avg:100.18ms
step:1223/1770 train_time:122524ms step_avg:100.18ms
step:1224/1770 train_time:122628ms step_avg:100.19ms
step:1225/1770 train_time:122733ms step_avg:100.19ms
step:1226/1770 train_time:122837ms step_avg:100.19ms
step:1227/1770 train_time:122942ms step_avg:100.20ms
step:1228/1770 train_time:123047ms step_avg:100.20ms
step:1229/1770 train_time:123152ms step_avg:100.20ms
step:1230/1770 train_time:123257ms step_avg:100.21ms
step:1231/1770 train_time:123361ms step_avg:100.21ms
step:1232/1770 train_time:123465ms step_avg:100.21ms
step:1233/1770 train_time:123568ms step_avg:100.22ms
step:1234/1770 train_time:123672ms step_avg:100.22ms
step:1235/1770 train_time:123777ms step_avg:100.22ms
step:1236/1770 train_time:123882ms step_avg:100.23ms
step:1237/1770 train_time:123986ms step_avg:100.23ms
step:1238/1770 train_time:124091ms step_avg:100.24ms
step:1239/1770 train_time:124196ms step_avg:100.24ms
step:1240/1770 train_time:124300ms step_avg:100.24ms
step:1241/1770 train_time:124404ms step_avg:100.24ms
step:1242/1770 train_time:124508ms step_avg:100.25ms
step:1243/1770 train_time:124613ms step_avg:100.25ms
step:1244/1770 train_time:124717ms step_avg:100.26ms
step:1245/1770 train_time:124820ms step_avg:100.26ms
step:1246/1770 train_time:124925ms step_avg:100.26ms
step:1247/1770 train_time:125029ms step_avg:100.26ms
step:1248/1770 train_time:125133ms step_avg:100.27ms
step:1249/1770 train_time:125237ms step_avg:100.27ms
step:1250/1770 train_time:125341ms step_avg:100.27ms
step:1250/1770 val_loss:3.4245 train_time:125441ms step_avg:100.35ms
step:1251/1770 train_time:125460ms step_avg:100.29ms
step:1252/1770 train_time:125558ms step_avg:100.29ms
step:1253/1770 train_time:125662ms step_avg:100.29ms
step:1254/1770 train_time:125765ms step_avg:100.29ms
step:1255/1770 train_time:125871ms step_avg:100.30ms
step:1256/1770 train_time:125975ms step_avg:100.30ms
step:1257/1770 train_time:126079ms step_avg:100.30ms
step:1258/1770 train_time:126184ms step_avg:100.31ms
step:1259/1770 train_time:126289ms step_avg:100.31ms
step:1260/1770 train_time:126393ms step_avg:100.31ms
step:1261/1770 train_time:126498ms step_avg:100.32ms
step:1262/1770 train_time:126602ms step_avg:100.32ms
step:1263/1770 train_time:126706ms step_avg:100.32ms
step:1264/1770 train_time:126812ms step_avg:100.33ms
step:1265/1770 train_time:126915ms step_avg:100.33ms
step:1266/1770 train_time:127020ms step_avg:100.33ms
step:1267/1770 train_time:127124ms step_avg:100.33ms
step:1268/1770 train_time:127234ms step_avg:100.34ms
step:1269/1770 train_time:127333ms step_avg:100.34ms
step:1270/1770 train_time:127437ms step_avg:100.34ms
step:1271/1770 train_time:127541ms step_avg:100.35ms
step:1272/1770 train_time:127644ms step_avg:100.35ms
step:1273/1770 train_time:127750ms step_avg:100.35ms
step:1274/1770 train_time:127854ms step_avg:100.36ms
step:1275/1770 train_time:127958ms step_avg:100.36ms
step:1276/1770 train_time:128062ms step_avg:100.36ms
step:1277/1770 train_time:128165ms step_avg:100.36ms
step:1278/1770 train_time:128270ms step_avg:100.37ms
step:1279/1770 train_time:128374ms step_avg:100.37ms
step:1280/1770 train_time:128479ms step_avg:100.37ms
step:1281/1770 train_time:128583ms step_avg:100.38ms
step:1282/1770 train_time:128688ms step_avg:100.38ms
step:1283/1770 train_time:128792ms step_avg:100.38ms
step:1284/1770 train_time:128896ms step_avg:100.39ms
step:1285/1770 train_time:129000ms step_avg:100.39ms
step:1286/1770 train_time:129104ms step_avg:100.39ms
step:1287/1770 train_time:129210ms step_avg:100.40ms
step:1288/1770 train_time:129314ms step_avg:100.40ms
step:1289/1770 train_time:129418ms step_avg:100.40ms
step:1290/1770 train_time:129521ms step_avg:100.40ms
step:1291/1770 train_time:129625ms step_avg:100.41ms
step:1292/1770 train_time:129728ms step_avg:100.41ms
step:1293/1770 train_time:129833ms step_avg:100.41ms
step:1294/1770 train_time:129937ms step_avg:100.42ms
step:1295/1770 train_time:130042ms step_avg:100.42ms
step:1296/1770 train_time:130146ms step_avg:100.42ms
step:1297/1770 train_time:130250ms step_avg:100.42ms
step:1298/1770 train_time:130354ms step_avg:100.43ms
step:1299/1770 train_time:130458ms step_avg:100.43ms
step:1300/1770 train_time:130562ms step_avg:100.43ms
step:1301/1770 train_time:130666ms step_avg:100.44ms
step:1302/1770 train_time:130771ms step_avg:100.44ms
step:1303/1770 train_time:130875ms step_avg:100.44ms
step:1304/1770 train_time:130979ms step_avg:100.44ms
step:1305/1770 train_time:131083ms step_avg:100.45ms
step:1306/1770 train_time:131186ms step_avg:100.45ms
step:1307/1770 train_time:131291ms step_avg:100.45ms
step:1308/1770 train_time:131395ms step_avg:100.46ms
step:1309/1770 train_time:131499ms step_avg:100.46ms
step:1310/1770 train_time:131602ms step_avg:100.46ms
step:1311/1770 train_time:131706ms step_avg:100.46ms
step:1312/1770 train_time:131811ms step_avg:100.47ms
step:1313/1770 train_time:131915ms step_avg:100.47ms
step:1314/1770 train_time:132018ms step_avg:100.47ms
step:1315/1770 train_time:132122ms step_avg:100.47ms
step:1316/1770 train_time:132226ms step_avg:100.48ms
step:1317/1770 train_time:132330ms step_avg:100.48ms
step:1318/1770 train_time:132441ms step_avg:100.49ms
step:1319/1770 train_time:132541ms step_avg:100.49ms
step:1320/1770 train_time:132645ms step_avg:100.49ms
step:1321/1770 train_time:132750ms step_avg:100.49ms
step:1322/1770 train_time:132855ms step_avg:100.50ms
step:1323/1770 train_time:132960ms step_avg:100.50ms
step:1324/1770 train_time:133063ms step_avg:100.50ms
step:1325/1770 train_time:133169ms step_avg:100.51ms
step:1326/1770 train_time:133273ms step_avg:100.51ms
step:1327/1770 train_time:133380ms step_avg:100.51ms
step:1328/1770 train_time:133483ms step_avg:100.51ms
step:1329/1770 train_time:133586ms step_avg:100.52ms
step:1330/1770 train_time:133690ms step_avg:100.52ms
step:1331/1770 train_time:133795ms step_avg:100.52ms
step:1332/1770 train_time:133898ms step_avg:100.52ms
step:1333/1770 train_time:134001ms step_avg:100.53ms
step:1334/1770 train_time:134105ms step_avg:100.53ms
step:1335/1770 train_time:134210ms step_avg:100.53ms
step:1336/1770 train_time:134313ms step_avg:100.53ms
step:1337/1770 train_time:134417ms step_avg:100.54ms
step:1338/1770 train_time:134522ms step_avg:100.54ms
step:1339/1770 train_time:134627ms step_avg:100.54ms
step:1340/1770 train_time:134732ms step_avg:100.55ms
step:1341/1770 train_time:134835ms step_avg:100.55ms
step:1342/1770 train_time:134941ms step_avg:100.55ms
step:1343/1770 train_time:135045ms step_avg:100.55ms
step:1344/1770 train_time:135150ms step_avg:100.56ms
step:1345/1770 train_time:135253ms step_avg:100.56ms
step:1346/1770 train_time:135358ms step_avg:100.56ms
step:1347/1770 train_time:135462ms step_avg:100.57ms
step:1348/1770 train_time:135567ms step_avg:100.57ms
step:1349/1770 train_time:135672ms step_avg:100.57ms
step:1350/1770 train_time:135776ms step_avg:100.57ms
step:1351/1770 train_time:135880ms step_avg:100.58ms
step:1352/1770 train_time:135984ms step_avg:100.58ms
step:1353/1770 train_time:136089ms step_avg:100.58ms
step:1354/1770 train_time:136193ms step_avg:100.59ms
step:1355/1770 train_time:136297ms step_avg:100.59ms
step:1356/1770 train_time:136400ms step_avg:100.59ms
step:1357/1770 train_time:136504ms step_avg:100.59ms
step:1358/1770 train_time:136609ms step_avg:100.60ms
step:1359/1770 train_time:136713ms step_avg:100.60ms
step:1360/1770 train_time:136819ms step_avg:100.60ms
step:1361/1770 train_time:136924ms step_avg:100.61ms
step:1362/1770 train_time:137028ms step_avg:100.61ms
step:1363/1770 train_time:137132ms step_avg:100.61ms
step:1364/1770 train_time:137236ms step_avg:100.61ms
step:1365/1770 train_time:137339ms step_avg:100.61ms
step:1366/1770 train_time:137443ms step_avg:100.62ms
step:1367/1770 train_time:137548ms step_avg:100.62ms
step:1368/1770 train_time:137652ms step_avg:100.62ms
step:1369/1770 train_time:137756ms step_avg:100.63ms
step:1370/1770 train_time:137861ms step_avg:100.63ms
step:1371/1770 train_time:137965ms step_avg:100.63ms
step:1372/1770 train_time:138070ms step_avg:100.63ms
step:1373/1770 train_time:138175ms step_avg:100.64ms
step:1374/1770 train_time:138280ms step_avg:100.64ms
step:1375/1770 train_time:138385ms step_avg:100.64ms
step:1375/1770 val_loss:3.3813 train_time:138485ms step_avg:100.72ms
step:1376/1770 train_time:138505ms step_avg:100.66ms
step:1377/1770 train_time:138599ms step_avg:100.65ms
step:1378/1770 train_time:138703ms step_avg:100.66ms
step:1379/1770 train_time:138806ms step_avg:100.66ms
step:1380/1770 train_time:138911ms step_avg:100.66ms
step:1381/1770 train_time:139015ms step_avg:100.66ms
step:1382/1770 train_time:139118ms step_avg:100.66ms
step:1383/1770 train_time:139223ms step_avg:100.67ms
step:1384/1770 train_time:139327ms step_avg:100.67ms
step:1385/1770 train_time:139431ms step_avg:100.67ms
step:1386/1770 train_time:139536ms step_avg:100.68ms
step:1387/1770 train_time:139641ms step_avg:100.68ms
step:1388/1770 train_time:139744ms step_avg:100.68ms
step:1389/1770 train_time:139849ms step_avg:100.68ms
step:1390/1770 train_time:139953ms step_avg:100.69ms
step:1391/1770 train_time:140057ms step_avg:100.69ms
step:1392/1770 train_time:140161ms step_avg:100.69ms
step:1393/1770 train_time:140264ms step_avg:100.69ms
step:1394/1770 train_time:140368ms step_avg:100.69ms
step:1395/1770 train_time:140473ms step_avg:100.70ms
step:1396/1770 train_time:140578ms step_avg:100.70ms
step:1397/1770 train_time:140682ms step_avg:100.70ms
step:1398/1770 train_time:140792ms step_avg:100.71ms
step:1399/1770 train_time:140891ms step_avg:100.71ms
step:1400/1770 train_time:140995ms step_avg:100.71ms
step:1401/1770 train_time:141099ms step_avg:100.71ms
step:1402/1770 train_time:141203ms step_avg:100.72ms
step:1403/1770 train_time:141306ms step_avg:100.72ms
step:1404/1770 train_time:141412ms step_avg:100.72ms
step:1405/1770 train_time:141516ms step_avg:100.72ms
step:1406/1770 train_time:141620ms step_avg:100.73ms
step:1407/1770 train_time:141724ms step_avg:100.73ms
step:1408/1770 train_time:141828ms step_avg:100.73ms
step:1409/1770 train_time:141933ms step_avg:100.73ms
step:1410/1770 train_time:142038ms step_avg:100.74ms
step:1411/1770 train_time:142141ms step_avg:100.74ms
step:1412/1770 train_time:142245ms step_avg:100.74ms
step:1413/1770 train_time:142349ms step_avg:100.74ms
step:1414/1770 train_time:142454ms step_avg:100.75ms
step:1415/1770 train_time:142558ms step_avg:100.75ms
step:1416/1770 train_time:142663ms step_avg:100.75ms
step:1417/1770 train_time:142767ms step_avg:100.75ms
step:1418/1770 train_time:142871ms step_avg:100.76ms
step:1419/1770 train_time:142976ms step_avg:100.76ms
step:1420/1770 train_time:143080ms step_avg:100.76ms
step:1421/1770 train_time:143184ms step_avg:100.76ms
step:1422/1770 train_time:143289ms step_avg:100.77ms
step:1423/1770 train_time:143394ms step_avg:100.77ms
step:1424/1770 train_time:143499ms step_avg:100.77ms
step:1425/1770 train_time:143602ms step_avg:100.77ms
step:1426/1770 train_time:143707ms step_avg:100.78ms
step:1427/1770 train_time:143811ms step_avg:100.78ms
step:1428/1770 train_time:143916ms step_avg:100.78ms
step:1429/1770 train_time:144020ms step_avg:100.78ms
step:1430/1770 train_time:144124ms step_avg:100.79ms
step:1431/1770 train_time:144229ms step_avg:100.79ms
step:1432/1770 train_time:144333ms step_avg:100.79ms
step:1433/1770 train_time:144437ms step_avg:100.79ms
step:1434/1770 train_time:144541ms step_avg:100.80ms
step:1435/1770 train_time:144645ms step_avg:100.80ms
step:1436/1770 train_time:144751ms step_avg:100.80ms
step:1437/1770 train_time:144855ms step_avg:100.80ms
step:1438/1770 train_time:144959ms step_avg:100.81ms
step:1439/1770 train_time:145063ms step_avg:100.81ms
step:1440/1770 train_time:145168ms step_avg:100.81ms
step:1441/1770 train_time:145274ms step_avg:100.82ms
step:1442/1770 train_time:145378ms step_avg:100.82ms
step:1443/1770 train_time:145489ms step_avg:100.82ms
step:1444/1770 train_time:145586ms step_avg:100.82ms
step:1445/1770 train_time:145691ms step_avg:100.82ms
step:1446/1770 train_time:145796ms step_avg:100.83ms
step:1447/1770 train_time:145902ms step_avg:100.83ms
step:1448/1770 train_time:146007ms step_avg:100.83ms
step:1449/1770 train_time:146114ms step_avg:100.84ms
step:1450/1770 train_time:146219ms step_avg:100.84ms
step:1451/1770 train_time:146324ms step_avg:100.84ms
step:1452/1770 train_time:146431ms step_avg:100.85ms
step:1453/1770 train_time:146536ms step_avg:100.85ms
step:1454/1770 train_time:146640ms step_avg:100.85ms
step:1455/1770 train_time:146747ms step_avg:100.86ms
step:1456/1770 train_time:146853ms step_avg:100.86ms
step:1457/1770 train_time:146959ms step_avg:100.86ms
step:1458/1770 train_time:147064ms step_avg:100.87ms
step:1459/1770 train_time:147171ms step_avg:100.87ms
step:1460/1770 train_time:147276ms step_avg:100.87ms
step:1461/1770 train_time:147382ms step_avg:100.88ms
step:1462/1770 train_time:147487ms step_avg:100.88ms
step:1463/1770 train_time:147593ms step_avg:100.88ms
step:1464/1770 train_time:147700ms step_avg:100.89ms
step:1465/1770 train_time:147805ms step_avg:100.89ms
step:1466/1770 train_time:147912ms step_avg:100.89ms
step:1467/1770 train_time:148017ms step_avg:100.90ms
step:1468/1770 train_time:148123ms step_avg:100.90ms
step:1469/1770 train_time:148229ms step_avg:100.90ms
step:1470/1770 train_time:148334ms step_avg:100.91ms
step:1471/1770 train_time:148439ms step_avg:100.91ms
step:1472/1770 train_time:148545ms step_avg:100.91ms
step:1473/1770 train_time:148651ms step_avg:100.92ms
step:1474/1770 train_time:148757ms step_avg:100.92ms
step:1475/1770 train_time:148863ms step_avg:100.92ms
step:1476/1770 train_time:148967ms step_avg:100.93ms
step:1477/1770 train_time:149075ms step_avg:100.93ms
step:1478/1770 train_time:149180ms step_avg:100.93ms
step:1479/1770 train_time:149286ms step_avg:100.94ms
step:1480/1770 train_time:149391ms step_avg:100.94ms
step:1481/1770 train_time:149499ms step_avg:100.94ms
step:1482/1770 train_time:149604ms step_avg:100.95ms
step:1483/1770 train_time:149709ms step_avg:100.95ms
step:1484/1770 train_time:149813ms step_avg:100.95ms
step:1485/1770 train_time:149918ms step_avg:100.96ms
step:1486/1770 train_time:150024ms step_avg:100.96ms
step:1487/1770 train_time:150129ms step_avg:100.96ms
step:1488/1770 train_time:150235ms step_avg:100.96ms
step:1489/1770 train_time:150342ms step_avg:100.97ms
step:1490/1770 train_time:150449ms step_avg:100.97ms
step:1491/1770 train_time:150554ms step_avg:100.98ms
step:1492/1770 train_time:150660ms step_avg:100.98ms
step:1493/1770 train_time:150769ms step_avg:100.98ms
step:1494/1770 train_time:150877ms step_avg:100.99ms
step:1495/1770 train_time:150983ms step_avg:100.99ms
step:1496/1770 train_time:151087ms step_avg:100.99ms
step:1497/1770 train_time:151193ms step_avg:101.00ms
step:1498/1770 train_time:151297ms step_avg:101.00ms
step:1499/1770 train_time:151402ms step_avg:101.00ms
step:1500/1770 train_time:151506ms step_avg:101.00ms
step:1500/1770 val_loss:3.3445 train_time:151606ms step_avg:101.07ms
step:1501/1770 train_time:151626ms step_avg:101.02ms
step:1502/1770 train_time:151723ms step_avg:101.01ms
step:1503/1770 train_time:151828ms step_avg:101.02ms
step:1504/1770 train_time:151933ms step_avg:101.02ms
step:1505/1770 train_time:152040ms step_avg:101.02ms
step:1506/1770 train_time:152145ms step_avg:101.03ms
step:1507/1770 train_time:152250ms step_avg:101.03ms
step:1508/1770 train_time:152357ms step_avg:101.03ms
step:1509/1770 train_time:152462ms step_avg:101.04ms
step:1510/1770 train_time:152568ms step_avg:101.04ms
step:1511/1770 train_time:152672ms step_avg:101.04ms
step:1512/1770 train_time:152778ms step_avg:101.04ms
step:1513/1770 train_time:152883ms step_avg:101.05ms
step:1514/1770 train_time:152989ms step_avg:101.05ms
step:1515/1770 train_time:153094ms step_avg:101.05ms
step:1516/1770 train_time:153199ms step_avg:101.05ms
step:1517/1770 train_time:153305ms step_avg:101.06ms
step:1518/1770 train_time:153413ms step_avg:101.06ms
step:1519/1770 train_time:153517ms step_avg:101.06ms
step:1520/1770 train_time:153623ms step_avg:101.07ms
step:1521/1770 train_time:153728ms step_avg:101.07ms
step:1522/1770 train_time:153834ms step_avg:101.07ms
step:1523/1770 train_time:153940ms step_avg:101.08ms
step:1524/1770 train_time:154053ms step_avg:101.08ms
step:1525/1770 train_time:154152ms step_avg:101.08ms
step:1526/1770 train_time:154256ms step_avg:101.09ms
step:1527/1770 train_time:154361ms step_avg:101.09ms
step:1528/1770 train_time:154468ms step_avg:101.09ms
step:1529/1770 train_time:154574ms step_avg:101.09ms
step:1530/1770 train_time:154680ms step_avg:101.10ms
step:1531/1770 train_time:154786ms step_avg:101.10ms
step:1532/1770 train_time:154892ms step_avg:101.10ms
step:1533/1770 train_time:154997ms step_avg:101.11ms
step:1534/1770 train_time:155103ms step_avg:101.11ms
step:1535/1770 train_time:155207ms step_avg:101.11ms
step:1536/1770 train_time:155313ms step_avg:101.12ms
step:1537/1770 train_time:155419ms step_avg:101.12ms
step:1538/1770 train_time:155527ms step_avg:101.12ms
step:1539/1770 train_time:155631ms step_avg:101.12ms
step:1540/1770 train_time:155740ms step_avg:101.13ms
step:1541/1770 train_time:155848ms step_avg:101.13ms
step:1542/1770 train_time:155954ms step_avg:101.14ms
step:1543/1770 train_time:156058ms step_avg:101.14ms
step:1544/1770 train_time:156166ms step_avg:101.14ms
step:1545/1770 train_time:156271ms step_avg:101.15ms
step:1546/1770 train_time:156377ms step_avg:101.15ms
step:1547/1770 train_time:156482ms step_avg:101.15ms
step:1548/1770 train_time:156588ms step_avg:101.15ms
step:1549/1770 train_time:156693ms step_avg:101.16ms
step:1550/1770 train_time:156799ms step_avg:101.16ms
step:1551/1770 train_time:156904ms step_avg:101.16ms
step:1552/1770 train_time:157012ms step_avg:101.17ms
step:1553/1770 train_time:157117ms step_avg:101.17ms
step:1554/1770 train_time:157222ms step_avg:101.17ms
step:1555/1770 train_time:157327ms step_avg:101.18ms
step:1556/1770 train_time:157432ms step_avg:101.18ms
step:1557/1770 train_time:157537ms step_avg:101.18ms
step:1558/1770 train_time:157644ms step_avg:101.18ms
step:1559/1770 train_time:157755ms step_avg:101.19ms
step:1560/1770 train_time:157855ms step_avg:101.19ms
step:1561/1770 train_time:157962ms step_avg:101.19ms
step:1562/1770 train_time:158068ms step_avg:101.20ms
step:1563/1770 train_time:158172ms step_avg:101.20ms
step:1564/1770 train_time:158277ms step_avg:101.20ms
step:1565/1770 train_time:158382ms step_avg:101.20ms
step:1566/1770 train_time:158487ms step_avg:101.21ms
step:1567/1770 train_time:158592ms step_avg:101.21ms
step:1568/1770 train_time:158697ms step_avg:101.21ms
step:1569/1770 train_time:158806ms step_avg:101.22ms
step:1570/1770 train_time:158912ms step_avg:101.22ms
step:1571/1770 train_time:159017ms step_avg:101.22ms
step:1572/1770 train_time:159123ms step_avg:101.22ms
step:1573/1770 train_time:159231ms step_avg:101.23ms
step:1574/1770 train_time:159337ms step_avg:101.23ms
step:1575/1770 train_time:159441ms step_avg:101.23ms
step:1576/1770 train_time:159546ms step_avg:101.23ms
step:1577/1770 train_time:159652ms step_avg:101.24ms
step:1578/1770 train_time:159759ms step_avg:101.24ms
step:1579/1770 train_time:159864ms step_avg:101.24ms
step:1580/1770 train_time:159970ms step_avg:101.25ms
step:1581/1770 train_time:160077ms step_avg:101.25ms
step:1582/1770 train_time:160185ms step_avg:101.25ms
step:1583/1770 train_time:160291ms step_avg:101.26ms
step:1584/1770 train_time:160396ms step_avg:101.26ms
step:1585/1770 train_time:160501ms step_avg:101.26ms
step:1586/1770 train_time:160611ms step_avg:101.27ms
step:1587/1770 train_time:160717ms step_avg:101.27ms
step:1588/1770 train_time:160823ms step_avg:101.27ms
step:1589/1770 train_time:160930ms step_avg:101.28ms
step:1590/1770 train_time:161035ms step_avg:101.28ms
step:1591/1770 train_time:161139ms step_avg:101.28ms
step:1592/1770 train_time:161247ms step_avg:101.29ms
step:1593/1770 train_time:161352ms step_avg:101.29ms
step:1594/1770 train_time:161458ms step_avg:101.29ms
step:1595/1770 train_time:161563ms step_avg:101.29ms
step:1596/1770 train_time:161670ms step_avg:101.30ms
step:1597/1770 train_time:161775ms step_avg:101.30ms
step:1598/1770 train_time:161880ms step_avg:101.30ms
step:1599/1770 train_time:161989ms step_avg:101.31ms
step:1600/1770 train_time:162098ms step_avg:101.31ms
step:1601/1770 train_time:162204ms step_avg:101.31ms
step:1602/1770 train_time:162309ms step_avg:101.32ms
step:1603/1770 train_time:162414ms step_avg:101.32ms
step:1604/1770 train_time:162519ms step_avg:101.32ms
step:1605/1770 train_time:162624ms step_avg:101.32ms
step:1606/1770 train_time:162729ms step_avg:101.33ms
step:1607/1770 train_time:162839ms step_avg:101.33ms
step:1608/1770 train_time:162945ms step_avg:101.33ms
step:1609/1770 train_time:163051ms step_avg:101.34ms
step:1610/1770 train_time:163156ms step_avg:101.34ms
step:1611/1770 train_time:163262ms step_avg:101.34ms
step:1612/1770 train_time:163370ms step_avg:101.35ms
step:1613/1770 train_time:163474ms step_avg:101.35ms
step:1614/1770 train_time:163579ms step_avg:101.35ms
step:1615/1770 train_time:163685ms step_avg:101.35ms
step:1616/1770 train_time:163791ms step_avg:101.36ms
step:1617/1770 train_time:163899ms step_avg:101.36ms
step:1618/1770 train_time:164005ms step_avg:101.36ms
step:1619/1770 train_time:164112ms step_avg:101.37ms
step:1620/1770 train_time:164218ms step_avg:101.37ms
step:1621/1770 train_time:164325ms step_avg:101.37ms
step:1622/1770 train_time:164431ms step_avg:101.38ms
step:1623/1770 train_time:164539ms step_avg:101.38ms
step:1624/1770 train_time:164645ms step_avg:101.38ms
step:1625/1770 train_time:164750ms step_avg:101.38ms
step:1625/1770 val_loss:3.3098 train_time:164851ms step_avg:101.45ms
step:1626/1770 train_time:164871ms step_avg:101.40ms
step:1627/1770 train_time:164968ms step_avg:101.39ms
step:1628/1770 train_time:165074ms step_avg:101.40ms
step:1629/1770 train_time:165179ms step_avg:101.40ms
step:1630/1770 train_time:165284ms step_avg:101.40ms
step:1631/1770 train_time:165388ms step_avg:101.40ms
step:1632/1770 train_time:165493ms step_avg:101.41ms
step:1633/1770 train_time:165600ms step_avg:101.41ms
step:1634/1770 train_time:165705ms step_avg:101.41ms
step:1635/1770 train_time:165810ms step_avg:101.41ms
step:1636/1770 train_time:165916ms step_avg:101.42ms
step:1637/1770 train_time:166022ms step_avg:101.42ms
step:1638/1770 train_time:166126ms step_avg:101.42ms
step:1639/1770 train_time:166233ms step_avg:101.42ms
step:1640/1770 train_time:166339ms step_avg:101.43ms
step:1641/1770 train_time:166444ms step_avg:101.43ms
step:1642/1770 train_time:166550ms step_avg:101.43ms
step:1643/1770 train_time:166656ms step_avg:101.43ms
step:1644/1770 train_time:166763ms step_avg:101.44ms
step:1645/1770 train_time:166867ms step_avg:101.44ms
step:1646/1770 train_time:166976ms step_avg:101.44ms
step:1647/1770 train_time:167083ms step_avg:101.45ms
step:1648/1770 train_time:167187ms step_avg:101.45ms
step:1649/1770 train_time:167292ms step_avg:101.45ms
step:1650/1770 train_time:167399ms step_avg:101.45ms
step:1651/1770 train_time:167504ms step_avg:101.46ms
step:1652/1770 train_time:167609ms step_avg:101.46ms
step:1653/1770 train_time:167715ms step_avg:101.46ms
step:1654/1770 train_time:167823ms step_avg:101.47ms
step:1655/1770 train_time:167930ms step_avg:101.47ms
step:1656/1770 train_time:168035ms step_avg:101.47ms
step:1657/1770 train_time:168142ms step_avg:101.47ms
step:1658/1770 train_time:168247ms step_avg:101.48ms
step:1659/1770 train_time:168354ms step_avg:101.48ms
step:1660/1770 train_time:168459ms step_avg:101.48ms
step:1661/1770 train_time:168565ms step_avg:101.48ms
step:1662/1770 train_time:168670ms step_avg:101.49ms
step:1663/1770 train_time:168776ms step_avg:101.49ms
step:1664/1770 train_time:168881ms step_avg:101.49ms
step:1665/1770 train_time:168986ms step_avg:101.49ms
step:1666/1770 train_time:169092ms step_avg:101.50ms
step:1667/1770 train_time:169197ms step_avg:101.50ms
step:1668/1770 train_time:169301ms step_avg:101.50ms
step:1669/1770 train_time:169406ms step_avg:101.50ms
step:1670/1770 train_time:169510ms step_avg:101.50ms
step:1671/1770 train_time:169617ms step_avg:101.51ms
step:1672/1770 train_time:169723ms step_avg:101.51ms
step:1673/1770 train_time:169829ms step_avg:101.51ms
step:1674/1770 train_time:169936ms step_avg:101.51ms
step:1675/1770 train_time:170040ms step_avg:101.52ms
step:1676/1770 train_time:170147ms step_avg:101.52ms
step:1677/1770 train_time:170255ms step_avg:101.52ms
step:1678/1770 train_time:170360ms step_avg:101.53ms
step:1679/1770 train_time:170465ms step_avg:101.53ms
step:1680/1770 train_time:170571ms step_avg:101.53ms
step:1681/1770 train_time:170676ms step_avg:101.53ms
step:1682/1770 train_time:170784ms step_avg:101.54ms
step:1683/1770 train_time:170889ms step_avg:101.54ms
step:1684/1770 train_time:170996ms step_avg:101.54ms
step:1685/1770 train_time:171101ms step_avg:101.54ms
step:1686/1770 train_time:171207ms step_avg:101.55ms
step:1687/1770 train_time:171314ms step_avg:101.55ms
step:1688/1770 train_time:171419ms step_avg:101.55ms
step:1689/1770 train_time:171526ms step_avg:101.55ms
step:1690/1770 train_time:171631ms step_avg:101.56ms
step:1691/1770 train_time:171737ms step_avg:101.56ms
step:1692/1770 train_time:171843ms step_avg:101.56ms
step:1693/1770 train_time:171949ms step_avg:101.56ms
step:1694/1770 train_time:172055ms step_avg:101.57ms
step:1695/1770 train_time:172160ms step_avg:101.57ms
step:1696/1770 train_time:172267ms step_avg:101.57ms
step:1697/1770 train_time:172375ms step_avg:101.58ms
step:1698/1770 train_time:172481ms step_avg:101.58ms
step:1699/1770 train_time:172586ms step_avg:101.58ms
step:1700/1770 train_time:172691ms step_avg:101.58ms
step:1701/1770 train_time:172797ms step_avg:101.59ms
step:1702/1770 train_time:172902ms step_avg:101.59ms
step:1703/1770 train_time:173008ms step_avg:101.59ms
step:1704/1770 train_time:173114ms step_avg:101.59ms
step:1705/1770 train_time:173219ms step_avg:101.59ms
step:1706/1770 train_time:173324ms step_avg:101.60ms
step:1707/1770 train_time:173430ms step_avg:101.60ms
step:1708/1770 train_time:173536ms step_avg:101.60ms
step:1709/1770 train_time:173643ms step_avg:101.61ms
step:1710/1770 train_time:173753ms step_avg:101.61ms
step:1711/1770 train_time:173861ms step_avg:101.61ms
step:1712/1770 train_time:173967ms step_avg:101.62ms
step:1713/1770 train_time:174071ms step_avg:101.62ms
step:1714/1770 train_time:174178ms step_avg:101.62ms
step:1715/1770 train_time:174283ms step_avg:101.62ms
step:1716/1770 train_time:174390ms step_avg:101.63ms
step:1717/1770 train_time:174496ms step_avg:101.63ms
step:1718/1770 train_time:174603ms step_avg:101.63ms
step:1719/1770 train_time:174709ms step_avg:101.63ms
step:1720/1770 train_time:174817ms step_avg:101.64ms
step:1721/1770 train_time:174924ms step_avg:101.64ms
step:1722/1770 train_time:175033ms step_avg:101.65ms
step:1723/1770 train_time:175141ms step_avg:101.65ms
step:1724/1770 train_time:175249ms step_avg:101.65ms
step:1725/1770 train_time:175359ms step_avg:101.66ms
step:1726/1770 train_time:175466ms step_avg:101.66ms
step:1727/1770 train_time:175573ms step_avg:101.66ms
step:1728/1770 train_time:175681ms step_avg:101.67ms
step:1729/1770 train_time:175788ms step_avg:101.67ms
step:1730/1770 train_time:175895ms step_avg:101.67ms
step:1731/1770 train_time:176003ms step_avg:101.68ms
step:1732/1770 train_time:176108ms step_avg:101.68ms
step:1733/1770 train_time:176217ms step_avg:101.68ms
step:1734/1770 train_time:176322ms step_avg:101.68ms
step:1735/1770 train_time:176428ms step_avg:101.69ms
step:1736/1770 train_time:176534ms step_avg:101.69ms
step:1737/1770 train_time:176641ms step_avg:101.69ms
step:1738/1770 train_time:176747ms step_avg:101.70ms
step:1739/1770 train_time:176853ms step_avg:101.70ms
step:1740/1770 train_time:176959ms step_avg:101.70ms
step:1741/1770 train_time:177068ms step_avg:101.70ms
step:1742/1770 train_time:177177ms step_avg:101.71ms
step:1743/1770 train_time:177284ms step_avg:101.71ms
step:1744/1770 train_time:177390ms step_avg:101.71ms
step:1745/1770 train_time:177496ms step_avg:101.72ms
step:1746/1770 train_time:177605ms step_avg:101.72ms
step:1747/1770 train_time:177710ms step_avg:101.72ms
step:1748/1770 train_time:177818ms step_avg:101.73ms
step:1749/1770 train_time:177926ms step_avg:101.73ms
step:1750/1770 train_time:178032ms step_avg:101.73ms
step:1750/1770 val_loss:3.2827 train_time:178133ms step_avg:101.79ms
step:1751/1770 train_time:178153ms step_avg:101.74ms
step:1752/1770 train_time:178248ms step_avg:101.74ms
step:1753/1770 train_time:178354ms step_avg:101.74ms
step:1754/1770 train_time:178461ms step_avg:101.75ms
step:1755/1770 train_time:178568ms step_avg:101.75ms
step:1756/1770 train_time:178675ms step_avg:101.75ms
step:1757/1770 train_time:178781ms step_avg:101.75ms
step:1758/1770 train_time:178888ms step_avg:101.76ms
step:1759/1770 train_time:178994ms step_avg:101.76ms
step:1760/1770 train_time:179100ms step_avg:101.76ms
step:1761/1770 train_time:179209ms step_avg:101.77ms
step:1762/1770 train_time:179318ms step_avg:101.77ms
step:1763/1770 train_time:179423ms step_avg:101.77ms
step:1764/1770 train_time:179530ms step_avg:101.77ms
step:1765/1770 train_time:179636ms step_avg:101.78ms
step:1766/1770 train_time:179746ms step_avg:101.78ms
step:1767/1770 train_time:179851ms step_avg:101.78ms
step:1768/1770 train_time:179958ms step_avg:101.79ms
step:1769/1770 train_time:180064ms step_avg:101.79ms
step:1770/1770 train_time:180169ms step_avg:101.79ms
step:1770/1770 val_loss:3.2798 train_time:180271ms step_avg:101.85ms
peak memory allocated: 30724 MiB reserved: 45392 MiB
