import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.1357, -4.2084, 1.0726),
        (4.132, -4.2045, 1.0725),
        (4.077, -4.1489, 1.0719),
        (4.0422, -4.1139, 1.0717),
        (3.9129, -3.9845, 1.0715),
        (3.3337, -3.2386, 0.9049),
        (2.2005, -1.6921, 0.4915),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 07:06:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   31C    P0            112W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            108W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            108W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            106W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:0.15ms
step:1/1770 train_time:69ms step_avg:69.37ms
step:2/1770 train_time:146ms step_avg:72.97ms
step:3/1770 train_time:239ms step_avg:79.54ms
step:4/1770 train_time:334ms step_avg:83.56ms
step:5/1770 train_time:430ms step_avg:85.98ms
step:6/1770 train_time:526ms step_avg:87.68ms
step:7/1770 train_time:622ms step_avg:88.85ms
step:8/1770 train_time:719ms step_avg:89.91ms
step:9/1770 train_time:815ms step_avg:90.52ms
step:10/1770 train_time:910ms step_avg:91.05ms
step:11/1770 train_time:1006ms step_avg:91.45ms
step:12/1770 train_time:1102ms step_avg:91.82ms
step:13/1770 train_time:1198ms step_avg:92.16ms
step:14/1770 train_time:1295ms step_avg:92.47ms
step:15/1770 train_time:1391ms step_avg:92.71ms
step:16/1770 train_time:1487ms step_avg:92.92ms
step:17/1770 train_time:1583ms step_avg:93.10ms
step:18/1770 train_time:1679ms step_avg:93.28ms
step:19/1770 train_time:1775ms step_avg:93.45ms
step:20/1770 train_time:1872ms step_avg:93.59ms
step:21/1770 train_time:1968ms step_avg:93.70ms
step:22/1770 train_time:2064ms step_avg:93.81ms
step:23/1770 train_time:2160ms step_avg:93.90ms
step:24/1770 train_time:2256ms step_avg:94.01ms
step:25/1770 train_time:2352ms step_avg:94.09ms
step:26/1770 train_time:2449ms step_avg:94.18ms
step:27/1770 train_time:2544ms step_avg:94.23ms
step:28/1770 train_time:2641ms step_avg:94.31ms
step:29/1770 train_time:2737ms step_avg:94.38ms
step:30/1770 train_time:2834ms step_avg:94.46ms
step:31/1770 train_time:2930ms step_avg:94.51ms
step:32/1770 train_time:3026ms step_avg:94.56ms
step:33/1770 train_time:3122ms step_avg:94.61ms
step:34/1770 train_time:3219ms step_avg:94.68ms
step:35/1770 train_time:3314ms step_avg:94.70ms
step:36/1770 train_time:3410ms step_avg:94.73ms
step:37/1770 train_time:3506ms step_avg:94.76ms
step:38/1770 train_time:3602ms step_avg:94.78ms
step:39/1770 train_time:3698ms step_avg:94.81ms
step:40/1770 train_time:3795ms step_avg:94.87ms
step:41/1770 train_time:3890ms step_avg:94.88ms
step:42/1770 train_time:3986ms step_avg:94.90ms
step:43/1770 train_time:4081ms step_avg:94.92ms
step:44/1770 train_time:4178ms step_avg:94.95ms
step:45/1770 train_time:4274ms step_avg:94.98ms
step:46/1770 train_time:4371ms step_avg:95.01ms
step:47/1770 train_time:4466ms step_avg:95.02ms
step:48/1770 train_time:4562ms step_avg:95.04ms
step:49/1770 train_time:4658ms step_avg:95.06ms
step:50/1770 train_time:4755ms step_avg:95.10ms
step:51/1770 train_time:4851ms step_avg:95.12ms
step:52/1770 train_time:4947ms step_avg:95.14ms
step:53/1770 train_time:5043ms step_avg:95.15ms
step:54/1770 train_time:5139ms step_avg:95.16ms
step:55/1770 train_time:5235ms step_avg:95.18ms
step:56/1770 train_time:5332ms step_avg:95.21ms
step:57/1770 train_time:5428ms step_avg:95.23ms
step:58/1770 train_time:5524ms step_avg:95.25ms
step:59/1770 train_time:5620ms step_avg:95.26ms
step:60/1770 train_time:5716ms step_avg:95.27ms
step:61/1770 train_time:5812ms step_avg:95.27ms
step:62/1770 train_time:5907ms step_avg:95.28ms
step:63/1770 train_time:6003ms step_avg:95.28ms
step:64/1770 train_time:6099ms step_avg:95.30ms
step:65/1770 train_time:6195ms step_avg:95.31ms
step:66/1770 train_time:6292ms step_avg:95.33ms
step:67/1770 train_time:6387ms step_avg:95.33ms
step:68/1770 train_time:6483ms step_avg:95.34ms
step:69/1770 train_time:6579ms step_avg:95.34ms
step:70/1770 train_time:6676ms step_avg:95.37ms
step:71/1770 train_time:6772ms step_avg:95.38ms
step:72/1770 train_time:6868ms step_avg:95.39ms
step:73/1770 train_time:6964ms step_avg:95.39ms
step:74/1770 train_time:7060ms step_avg:95.40ms
step:75/1770 train_time:7156ms step_avg:95.41ms
step:76/1770 train_time:7252ms step_avg:95.42ms
step:77/1770 train_time:7348ms step_avg:95.43ms
step:78/1770 train_time:7444ms step_avg:95.44ms
step:79/1770 train_time:7540ms step_avg:95.44ms
step:80/1770 train_time:7636ms step_avg:95.45ms
step:81/1770 train_time:7732ms step_avg:95.45ms
step:82/1770 train_time:7829ms step_avg:95.47ms
step:83/1770 train_time:7925ms step_avg:95.48ms
step:84/1770 train_time:8021ms step_avg:95.48ms
step:85/1770 train_time:8117ms step_avg:95.50ms
step:86/1770 train_time:8213ms step_avg:95.50ms
step:87/1770 train_time:8308ms step_avg:95.50ms
step:88/1770 train_time:8404ms step_avg:95.50ms
step:89/1770 train_time:8500ms step_avg:95.50ms
step:90/1770 train_time:8597ms step_avg:95.52ms
step:91/1770 train_time:8692ms step_avg:95.52ms
step:92/1770 train_time:8788ms step_avg:95.52ms
step:93/1770 train_time:8884ms step_avg:95.52ms
step:94/1770 train_time:8980ms step_avg:95.53ms
step:95/1770 train_time:9076ms step_avg:95.53ms
step:96/1770 train_time:9172ms step_avg:95.54ms
step:97/1770 train_time:9267ms step_avg:95.53ms
step:98/1770 train_time:9363ms step_avg:95.54ms
step:99/1770 train_time:9459ms step_avg:95.55ms
step:100/1770 train_time:9555ms step_avg:95.55ms
step:101/1770 train_time:9652ms step_avg:95.56ms
step:102/1770 train_time:9748ms step_avg:95.57ms
step:103/1770 train_time:9844ms step_avg:95.57ms
step:104/1770 train_time:9940ms step_avg:95.58ms
step:105/1770 train_time:10036ms step_avg:95.58ms
step:106/1770 train_time:10132ms step_avg:95.59ms
step:107/1770 train_time:10228ms step_avg:95.59ms
step:108/1770 train_time:10325ms step_avg:95.60ms
step:109/1770 train_time:10420ms step_avg:95.60ms
step:110/1770 train_time:10517ms step_avg:95.61ms
step:111/1770 train_time:10612ms step_avg:95.61ms
step:112/1770 train_time:10708ms step_avg:95.61ms
step:113/1770 train_time:10804ms step_avg:95.61ms
step:114/1770 train_time:10900ms step_avg:95.62ms
step:115/1770 train_time:10996ms step_avg:95.62ms
step:116/1770 train_time:11092ms step_avg:95.62ms
step:117/1770 train_time:11187ms step_avg:95.62ms
step:118/1770 train_time:11283ms step_avg:95.62ms
step:119/1770 train_time:11379ms step_avg:95.62ms
step:120/1770 train_time:11475ms step_avg:95.63ms
step:121/1770 train_time:11571ms step_avg:95.63ms
step:122/1770 train_time:11667ms step_avg:95.63ms
step:123/1770 train_time:11762ms step_avg:95.63ms
step:124/1770 train_time:11859ms step_avg:95.64ms
step:125/1770 train_time:11955ms step_avg:95.64ms
step:125/1770 val_loss:4.6292 train_time:12046ms step_avg:96.37ms
step:126/1770 train_time:12067ms step_avg:95.77ms
step:127/1770 train_time:12158ms step_avg:95.73ms
step:128/1770 train_time:12260ms step_avg:95.78ms
step:129/1770 train_time:12356ms step_avg:95.79ms
step:130/1770 train_time:12452ms step_avg:95.79ms
step:131/1770 train_time:12549ms step_avg:95.79ms
step:132/1770 train_time:12645ms step_avg:95.79ms
step:133/1770 train_time:12741ms step_avg:95.80ms
step:134/1770 train_time:12837ms step_avg:95.80ms
step:135/1770 train_time:12933ms step_avg:95.80ms
step:136/1770 train_time:13031ms step_avg:95.81ms
step:137/1770 train_time:13127ms step_avg:95.82ms
step:138/1770 train_time:13223ms step_avg:95.82ms
step:139/1770 train_time:13319ms step_avg:95.82ms
step:140/1770 train_time:13415ms step_avg:95.82ms
step:141/1770 train_time:13512ms step_avg:95.83ms
step:142/1770 train_time:13608ms step_avg:95.83ms
step:143/1770 train_time:13706ms step_avg:95.85ms
step:144/1770 train_time:13802ms step_avg:95.85ms
step:145/1770 train_time:13899ms step_avg:95.85ms
step:146/1770 train_time:13995ms step_avg:95.86ms
step:147/1770 train_time:14092ms step_avg:95.86ms
step:148/1770 train_time:14188ms step_avg:95.86ms
step:149/1770 train_time:14284ms step_avg:95.87ms
step:150/1770 train_time:14380ms step_avg:95.87ms
step:151/1770 train_time:14477ms step_avg:95.87ms
step:152/1770 train_time:14573ms step_avg:95.88ms
step:153/1770 train_time:14671ms step_avg:95.89ms
step:154/1770 train_time:14767ms step_avg:95.89ms
step:155/1770 train_time:14864ms step_avg:95.90ms
step:156/1770 train_time:14960ms step_avg:95.90ms
step:157/1770 train_time:15057ms step_avg:95.90ms
step:158/1770 train_time:15153ms step_avg:95.90ms
step:159/1770 train_time:15250ms step_avg:95.91ms
step:160/1770 train_time:15346ms step_avg:95.92ms
step:161/1770 train_time:15443ms step_avg:95.92ms
step:162/1770 train_time:15540ms step_avg:95.92ms
step:163/1770 train_time:15636ms step_avg:95.93ms
step:164/1770 train_time:15733ms step_avg:95.93ms
step:165/1770 train_time:15829ms step_avg:95.93ms
step:166/1770 train_time:15926ms step_avg:95.94ms
step:167/1770 train_time:16023ms step_avg:95.95ms
step:168/1770 train_time:16119ms step_avg:95.95ms
step:169/1770 train_time:16216ms step_avg:95.95ms
step:170/1770 train_time:16312ms step_avg:95.96ms
step:171/1770 train_time:16410ms step_avg:95.97ms
step:172/1770 train_time:16507ms step_avg:95.97ms
step:173/1770 train_time:16604ms step_avg:95.98ms
step:174/1770 train_time:16700ms step_avg:95.98ms
step:175/1770 train_time:16797ms step_avg:95.98ms
step:176/1770 train_time:16893ms step_avg:95.98ms
step:177/1770 train_time:16990ms step_avg:95.99ms
step:178/1770 train_time:17086ms step_avg:95.99ms
step:179/1770 train_time:17183ms step_avg:96.00ms
step:180/1770 train_time:17279ms step_avg:96.00ms
step:181/1770 train_time:17376ms step_avg:96.00ms
step:182/1770 train_time:17473ms step_avg:96.00ms
step:183/1770 train_time:17570ms step_avg:96.01ms
step:184/1770 train_time:17667ms step_avg:96.02ms
step:185/1770 train_time:17765ms step_avg:96.03ms
step:186/1770 train_time:17862ms step_avg:96.03ms
step:187/1770 train_time:17958ms step_avg:96.03ms
step:188/1770 train_time:18055ms step_avg:96.04ms
step:189/1770 train_time:18152ms step_avg:96.04ms
step:190/1770 train_time:18249ms step_avg:96.05ms
step:191/1770 train_time:18346ms step_avg:96.05ms
step:192/1770 train_time:18443ms step_avg:96.06ms
step:193/1770 train_time:18540ms step_avg:96.06ms
step:194/1770 train_time:18636ms step_avg:96.06ms
step:195/1770 train_time:18733ms step_avg:96.07ms
step:196/1770 train_time:18830ms step_avg:96.07ms
step:197/1770 train_time:18927ms step_avg:96.08ms
step:198/1770 train_time:19024ms step_avg:96.08ms
step:199/1770 train_time:19120ms step_avg:96.08ms
step:200/1770 train_time:19216ms step_avg:96.08ms
step:201/1770 train_time:19313ms step_avg:96.09ms
step:202/1770 train_time:19410ms step_avg:96.09ms
step:203/1770 train_time:19507ms step_avg:96.10ms
step:204/1770 train_time:19604ms step_avg:96.10ms
step:205/1770 train_time:19701ms step_avg:96.10ms
step:206/1770 train_time:19798ms step_avg:96.11ms
step:207/1770 train_time:19895ms step_avg:96.11ms
step:208/1770 train_time:19991ms step_avg:96.11ms
step:209/1770 train_time:20089ms step_avg:96.12ms
step:210/1770 train_time:20186ms step_avg:96.12ms
step:211/1770 train_time:20282ms step_avg:96.12ms
step:212/1770 train_time:20378ms step_avg:96.12ms
step:213/1770 train_time:20475ms step_avg:96.13ms
step:214/1770 train_time:20571ms step_avg:96.13ms
step:215/1770 train_time:20669ms step_avg:96.14ms
step:216/1770 train_time:20767ms step_avg:96.14ms
step:217/1770 train_time:20864ms step_avg:96.15ms
step:218/1770 train_time:20961ms step_avg:96.15ms
step:219/1770 train_time:21057ms step_avg:96.15ms
step:220/1770 train_time:21154ms step_avg:96.15ms
step:221/1770 train_time:21251ms step_avg:96.16ms
step:222/1770 train_time:21348ms step_avg:96.16ms
step:223/1770 train_time:21444ms step_avg:96.16ms
step:224/1770 train_time:21540ms step_avg:96.16ms
step:225/1770 train_time:21636ms step_avg:96.16ms
step:226/1770 train_time:21733ms step_avg:96.16ms
step:227/1770 train_time:21831ms step_avg:96.17ms
step:228/1770 train_time:21929ms step_avg:96.18ms
step:229/1770 train_time:22027ms step_avg:96.19ms
step:230/1770 train_time:22124ms step_avg:96.19ms
step:231/1770 train_time:22220ms step_avg:96.19ms
step:232/1770 train_time:22316ms step_avg:96.19ms
step:233/1770 train_time:22413ms step_avg:96.19ms
step:234/1770 train_time:22511ms step_avg:96.20ms
step:235/1770 train_time:22607ms step_avg:96.20ms
step:236/1770 train_time:22703ms step_avg:96.20ms
step:237/1770 train_time:22800ms step_avg:96.20ms
step:238/1770 train_time:22896ms step_avg:96.20ms
step:239/1770 train_time:22993ms step_avg:96.21ms
step:240/1770 train_time:23090ms step_avg:96.21ms
step:241/1770 train_time:23187ms step_avg:96.21ms
step:242/1770 train_time:23617ms step_avg:97.59ms
step:243/1770 train_time:23673ms step_avg:97.42ms
step:244/1770 train_time:23769ms step_avg:97.41ms
step:245/1770 train_time:23866ms step_avg:97.41ms
step:246/1770 train_time:23962ms step_avg:97.41ms
step:247/1770 train_time:24059ms step_avg:97.40ms
step:248/1770 train_time:24155ms step_avg:97.40ms
step:249/1770 train_time:24252ms step_avg:97.40ms
step:250/1770 train_time:24349ms step_avg:97.39ms
step:250/1770 val_loss:4.1029 train_time:24441ms step_avg:97.76ms
step:251/1770 train_time:24461ms step_avg:97.45ms
step:252/1770 train_time:24549ms step_avg:97.42ms
step:253/1770 train_time:24651ms step_avg:97.43ms
step:254/1770 train_time:24750ms step_avg:97.44ms
step:255/1770 train_time:24847ms step_avg:97.44ms
step:256/1770 train_time:24943ms step_avg:97.43ms
step:257/1770 train_time:25039ms step_avg:97.43ms
step:258/1770 train_time:25135ms step_avg:97.42ms
step:259/1770 train_time:25232ms step_avg:97.42ms
step:260/1770 train_time:25329ms step_avg:97.42ms
step:261/1770 train_time:25425ms step_avg:97.42ms
step:262/1770 train_time:25522ms step_avg:97.41ms
step:263/1770 train_time:25618ms step_avg:97.41ms
step:264/1770 train_time:25715ms step_avg:97.41ms
step:265/1770 train_time:25812ms step_avg:97.41ms
step:266/1770 train_time:25910ms step_avg:97.41ms
step:267/1770 train_time:26008ms step_avg:97.41ms
step:268/1770 train_time:26106ms step_avg:97.41ms
step:269/1770 train_time:26204ms step_avg:97.41ms
step:270/1770 train_time:26301ms step_avg:97.41ms
step:271/1770 train_time:26397ms step_avg:97.41ms
step:272/1770 train_time:26495ms step_avg:97.41ms
step:273/1770 train_time:26592ms step_avg:97.41ms
step:274/1770 train_time:26690ms step_avg:97.41ms
step:275/1770 train_time:26787ms step_avg:97.41ms
step:276/1770 train_time:26884ms step_avg:97.41ms
step:277/1770 train_time:26981ms step_avg:97.40ms
step:278/1770 train_time:27078ms step_avg:97.40ms
step:279/1770 train_time:27176ms step_avg:97.41ms
step:280/1770 train_time:27272ms step_avg:97.40ms
step:281/1770 train_time:27369ms step_avg:97.40ms
step:282/1770 train_time:27467ms step_avg:97.40ms
step:283/1770 train_time:27565ms step_avg:97.40ms
step:284/1770 train_time:27662ms step_avg:97.40ms
step:285/1770 train_time:27759ms step_avg:97.40ms
step:286/1770 train_time:27856ms step_avg:97.40ms
step:287/1770 train_time:27953ms step_avg:97.40ms
step:288/1770 train_time:28051ms step_avg:97.40ms
step:289/1770 train_time:28148ms step_avg:97.40ms
step:290/1770 train_time:28247ms step_avg:97.40ms
step:291/1770 train_time:28345ms step_avg:97.41ms
step:292/1770 train_time:28442ms step_avg:97.40ms
step:293/1770 train_time:28539ms step_avg:97.40ms
step:294/1770 train_time:28635ms step_avg:97.40ms
step:295/1770 train_time:28733ms step_avg:97.40ms
step:296/1770 train_time:28830ms step_avg:97.40ms
step:297/1770 train_time:28927ms step_avg:97.40ms
step:298/1770 train_time:29025ms step_avg:97.40ms
step:299/1770 train_time:29122ms step_avg:97.40ms
step:300/1770 train_time:29219ms step_avg:97.40ms
step:301/1770 train_time:29316ms step_avg:97.39ms
step:302/1770 train_time:29413ms step_avg:97.39ms
step:303/1770 train_time:29510ms step_avg:97.39ms
step:304/1770 train_time:29608ms step_avg:97.39ms
step:305/1770 train_time:29705ms step_avg:97.39ms
step:306/1770 train_time:29803ms step_avg:97.40ms
step:307/1770 train_time:29900ms step_avg:97.39ms
step:308/1770 train_time:29997ms step_avg:97.39ms
step:309/1770 train_time:30094ms step_avg:97.39ms
step:310/1770 train_time:30192ms step_avg:97.39ms
step:311/1770 train_time:30289ms step_avg:97.39ms
step:312/1770 train_time:30387ms step_avg:97.39ms
step:313/1770 train_time:30484ms step_avg:97.39ms
step:314/1770 train_time:30581ms step_avg:97.39ms
step:315/1770 train_time:30678ms step_avg:97.39ms
step:316/1770 train_time:30775ms step_avg:97.39ms
step:317/1770 train_time:30872ms step_avg:97.39ms
step:318/1770 train_time:30969ms step_avg:97.39ms
step:319/1770 train_time:31067ms step_avg:97.39ms
step:320/1770 train_time:31165ms step_avg:97.39ms
step:321/1770 train_time:31262ms step_avg:97.39ms
step:322/1770 train_time:31360ms step_avg:97.39ms
step:323/1770 train_time:31456ms step_avg:97.39ms
step:324/1770 train_time:31553ms step_avg:97.39ms
step:325/1770 train_time:31650ms step_avg:97.39ms
step:326/1770 train_time:31749ms step_avg:97.39ms
step:327/1770 train_time:31847ms step_avg:97.39ms
step:328/1770 train_time:31944ms step_avg:97.39ms
step:329/1770 train_time:32041ms step_avg:97.39ms
step:330/1770 train_time:32138ms step_avg:97.39ms
step:331/1770 train_time:32235ms step_avg:97.39ms
step:332/1770 train_time:32332ms step_avg:97.39ms
step:333/1770 train_time:32429ms step_avg:97.38ms
step:334/1770 train_time:32527ms step_avg:97.39ms
step:335/1770 train_time:32625ms step_avg:97.39ms
step:336/1770 train_time:32722ms step_avg:97.39ms
step:337/1770 train_time:32819ms step_avg:97.38ms
step:338/1770 train_time:32916ms step_avg:97.38ms
step:339/1770 train_time:33013ms step_avg:97.38ms
step:340/1770 train_time:33110ms step_avg:97.38ms
step:341/1770 train_time:33208ms step_avg:97.38ms
step:342/1770 train_time:33305ms step_avg:97.38ms
step:343/1770 train_time:33402ms step_avg:97.38ms
step:344/1770 train_time:33499ms step_avg:97.38ms
step:345/1770 train_time:33597ms step_avg:97.38ms
step:346/1770 train_time:33694ms step_avg:97.38ms
step:347/1770 train_time:33792ms step_avg:97.38ms
step:348/1770 train_time:33889ms step_avg:97.38ms
step:349/1770 train_time:33987ms step_avg:97.39ms
step:350/1770 train_time:34085ms step_avg:97.39ms
step:351/1770 train_time:34182ms step_avg:97.38ms
step:352/1770 train_time:34279ms step_avg:97.38ms
step:353/1770 train_time:34376ms step_avg:97.38ms
step:354/1770 train_time:34473ms step_avg:97.38ms
step:355/1770 train_time:34570ms step_avg:97.38ms
step:356/1770 train_time:34667ms step_avg:97.38ms
step:357/1770 train_time:34764ms step_avg:97.38ms
step:358/1770 train_time:34862ms step_avg:97.38ms
step:359/1770 train_time:34959ms step_avg:97.38ms
step:360/1770 train_time:35056ms step_avg:97.38ms
step:361/1770 train_time:35153ms step_avg:97.38ms
step:362/1770 train_time:35250ms step_avg:97.38ms
step:363/1770 train_time:35347ms step_avg:97.38ms
step:364/1770 train_time:35445ms step_avg:97.38ms
step:365/1770 train_time:35541ms step_avg:97.37ms
step:366/1770 train_time:35639ms step_avg:97.37ms
step:367/1770 train_time:35735ms step_avg:97.37ms
step:368/1770 train_time:35833ms step_avg:97.37ms
step:369/1770 train_time:35931ms step_avg:97.38ms
step:370/1770 train_time:36028ms step_avg:97.37ms
step:371/1770 train_time:36125ms step_avg:97.37ms
step:372/1770 train_time:36223ms step_avg:97.37ms
step:373/1770 train_time:36319ms step_avg:97.37ms
step:374/1770 train_time:36417ms step_avg:97.37ms
step:375/1770 train_time:36514ms step_avg:97.37ms
step:375/1770 val_loss:3.8945 train_time:36606ms step_avg:97.62ms
step:376/1770 train_time:36626ms step_avg:97.41ms
step:377/1770 train_time:36716ms step_avg:97.39ms
step:378/1770 train_time:36816ms step_avg:97.40ms
step:379/1770 train_time:36914ms step_avg:97.40ms
step:380/1770 train_time:37011ms step_avg:97.40ms
step:381/1770 train_time:37109ms step_avg:97.40ms
step:382/1770 train_time:37206ms step_avg:97.40ms
step:383/1770 train_time:37303ms step_avg:97.40ms
step:384/1770 train_time:37399ms step_avg:97.39ms
step:385/1770 train_time:37497ms step_avg:97.39ms
step:386/1770 train_time:37593ms step_avg:97.39ms
step:387/1770 train_time:37691ms step_avg:97.39ms
step:388/1770 train_time:37790ms step_avg:97.40ms
step:389/1770 train_time:37887ms step_avg:97.40ms
step:390/1770 train_time:37984ms step_avg:97.39ms
step:391/1770 train_time:38081ms step_avg:97.39ms
step:392/1770 train_time:38178ms step_avg:97.39ms
step:393/1770 train_time:38275ms step_avg:97.39ms
step:394/1770 train_time:38372ms step_avg:97.39ms
step:395/1770 train_time:38469ms step_avg:97.39ms
step:396/1770 train_time:38568ms step_avg:97.39ms
step:397/1770 train_time:38667ms step_avg:97.40ms
step:398/1770 train_time:38767ms step_avg:97.40ms
step:399/1770 train_time:38866ms step_avg:97.41ms
step:400/1770 train_time:38965ms step_avg:97.41ms
step:401/1770 train_time:39065ms step_avg:97.42ms
step:402/1770 train_time:39165ms step_avg:97.43ms
step:403/1770 train_time:39265ms step_avg:97.43ms
step:404/1770 train_time:39365ms step_avg:97.44ms
step:405/1770 train_time:39465ms step_avg:97.44ms
step:406/1770 train_time:39564ms step_avg:97.45ms
step:407/1770 train_time:39663ms step_avg:97.45ms
step:408/1770 train_time:39761ms step_avg:97.45ms
step:409/1770 train_time:39860ms step_avg:97.46ms
step:410/1770 train_time:39958ms step_avg:97.46ms
step:411/1770 train_time:40056ms step_avg:97.46ms
step:412/1770 train_time:40155ms step_avg:97.46ms
step:413/1770 train_time:40254ms step_avg:97.47ms
step:414/1770 train_time:40353ms step_avg:97.47ms
step:415/1770 train_time:40453ms step_avg:97.48ms
step:416/1770 train_time:40552ms step_avg:97.48ms
step:417/1770 train_time:40652ms step_avg:97.49ms
step:418/1770 train_time:40752ms step_avg:97.49ms
step:419/1770 train_time:40852ms step_avg:97.50ms
step:420/1770 train_time:40953ms step_avg:97.51ms
step:421/1770 train_time:41053ms step_avg:97.51ms
step:422/1770 train_time:41153ms step_avg:97.52ms
step:423/1770 train_time:41253ms step_avg:97.52ms
step:424/1770 train_time:41352ms step_avg:97.53ms
step:425/1770 train_time:41453ms step_avg:97.54ms
step:426/1770 train_time:41552ms step_avg:97.54ms
step:427/1770 train_time:41651ms step_avg:97.54ms
step:428/1770 train_time:41751ms step_avg:97.55ms
step:429/1770 train_time:41851ms step_avg:97.55ms
step:430/1770 train_time:41950ms step_avg:97.56ms
step:431/1770 train_time:42050ms step_avg:97.56ms
step:432/1770 train_time:42150ms step_avg:97.57ms
step:433/1770 train_time:42251ms step_avg:97.58ms
step:434/1770 train_time:42351ms step_avg:97.58ms
step:435/1770 train_time:42451ms step_avg:97.59ms
step:436/1770 train_time:42551ms step_avg:97.59ms
step:437/1770 train_time:42650ms step_avg:97.60ms
step:438/1770 train_time:42750ms step_avg:97.60ms
step:439/1770 train_time:42849ms step_avg:97.61ms
step:440/1770 train_time:42948ms step_avg:97.61ms
step:441/1770 train_time:43048ms step_avg:97.61ms
step:442/1770 train_time:43147ms step_avg:97.62ms
step:443/1770 train_time:43246ms step_avg:97.62ms
step:444/1770 train_time:43346ms step_avg:97.63ms
step:445/1770 train_time:43445ms step_avg:97.63ms
step:446/1770 train_time:43544ms step_avg:97.63ms
step:447/1770 train_time:43643ms step_avg:97.64ms
step:448/1770 train_time:43743ms step_avg:97.64ms
step:449/1770 train_time:43841ms step_avg:97.64ms
step:450/1770 train_time:43940ms step_avg:97.64ms
step:451/1770 train_time:44038ms step_avg:97.65ms
step:452/1770 train_time:44137ms step_avg:97.65ms
step:453/1770 train_time:44236ms step_avg:97.65ms
step:454/1770 train_time:44335ms step_avg:97.65ms
step:455/1770 train_time:44434ms step_avg:97.66ms
step:456/1770 train_time:44533ms step_avg:97.66ms
step:457/1770 train_time:44633ms step_avg:97.66ms
step:458/1770 train_time:44732ms step_avg:97.67ms
step:459/1770 train_time:44832ms step_avg:97.67ms
step:460/1770 train_time:44931ms step_avg:97.68ms
step:461/1770 train_time:45031ms step_avg:97.68ms
step:462/1770 train_time:45131ms step_avg:97.69ms
step:463/1770 train_time:45231ms step_avg:97.69ms
step:464/1770 train_time:45332ms step_avg:97.70ms
step:465/1770 train_time:45432ms step_avg:97.70ms
step:466/1770 train_time:45531ms step_avg:97.71ms
step:467/1770 train_time:45631ms step_avg:97.71ms
step:468/1770 train_time:45730ms step_avg:97.71ms
step:469/1770 train_time:45830ms step_avg:97.72ms
step:470/1770 train_time:45930ms step_avg:97.72ms
step:471/1770 train_time:46030ms step_avg:97.73ms
step:472/1770 train_time:46130ms step_avg:97.73ms
step:473/1770 train_time:46230ms step_avg:97.74ms
step:474/1770 train_time:46330ms step_avg:97.74ms
step:475/1770 train_time:46431ms step_avg:97.75ms
step:476/1770 train_time:46531ms step_avg:97.75ms
step:477/1770 train_time:46631ms step_avg:97.76ms
step:478/1770 train_time:46730ms step_avg:97.76ms
step:479/1770 train_time:46830ms step_avg:97.77ms
step:480/1770 train_time:46930ms step_avg:97.77ms
step:481/1770 train_time:47029ms step_avg:97.77ms
step:482/1770 train_time:47128ms step_avg:97.78ms
step:483/1770 train_time:47228ms step_avg:97.78ms
step:484/1770 train_time:47327ms step_avg:97.78ms
step:485/1770 train_time:47427ms step_avg:97.79ms
step:486/1770 train_time:47526ms step_avg:97.79ms
step:487/1770 train_time:47626ms step_avg:97.79ms
step:488/1770 train_time:47725ms step_avg:97.80ms
step:489/1770 train_time:47824ms step_avg:97.80ms
step:490/1770 train_time:47923ms step_avg:97.80ms
step:491/1770 train_time:48022ms step_avg:97.80ms
step:492/1770 train_time:48121ms step_avg:97.81ms
step:493/1770 train_time:48220ms step_avg:97.81ms
step:494/1770 train_time:48318ms step_avg:97.81ms
step:495/1770 train_time:48417ms step_avg:97.81ms
step:496/1770 train_time:48516ms step_avg:97.81ms
step:497/1770 train_time:48615ms step_avg:97.82ms
step:498/1770 train_time:48714ms step_avg:97.82ms
step:499/1770 train_time:48813ms step_avg:97.82ms
step:500/1770 train_time:48912ms step_avg:97.82ms
step:500/1770 val_loss:3.7493 train_time:49007ms step_avg:98.01ms
step:501/1770 train_time:49029ms step_avg:97.86ms
step:502/1770 train_time:49121ms step_avg:97.85ms
step:503/1770 train_time:49222ms step_avg:97.86ms
step:504/1770 train_time:49321ms step_avg:97.86ms
step:505/1770 train_time:49420ms step_avg:97.86ms
step:506/1770 train_time:49518ms step_avg:97.86ms
step:507/1770 train_time:49617ms step_avg:97.86ms
step:508/1770 train_time:49717ms step_avg:97.87ms
step:509/1770 train_time:49816ms step_avg:97.87ms
step:510/1770 train_time:49915ms step_avg:97.87ms
step:511/1770 train_time:50014ms step_avg:97.88ms
step:512/1770 train_time:50114ms step_avg:97.88ms
step:513/1770 train_time:50214ms step_avg:97.88ms
step:514/1770 train_time:50314ms step_avg:97.89ms
step:515/1770 train_time:50415ms step_avg:97.89ms
step:516/1770 train_time:50516ms step_avg:97.90ms
step:517/1770 train_time:50616ms step_avg:97.90ms
step:518/1770 train_time:50716ms step_avg:97.91ms
step:519/1770 train_time:50815ms step_avg:97.91ms
step:520/1770 train_time:50916ms step_avg:97.91ms
step:521/1770 train_time:51015ms step_avg:97.92ms
step:522/1770 train_time:51114ms step_avg:97.92ms
step:523/1770 train_time:51214ms step_avg:97.92ms
step:524/1770 train_time:51314ms step_avg:97.93ms
step:525/1770 train_time:51414ms step_avg:97.93ms
step:526/1770 train_time:51514ms step_avg:97.93ms
step:527/1770 train_time:51614ms step_avg:97.94ms
step:528/1770 train_time:51714ms step_avg:97.94ms
step:529/1770 train_time:51815ms step_avg:97.95ms
step:530/1770 train_time:51915ms step_avg:97.95ms
step:531/1770 train_time:52016ms step_avg:97.96ms
step:532/1770 train_time:52116ms step_avg:97.96ms
step:533/1770 train_time:52216ms step_avg:97.97ms
step:534/1770 train_time:52316ms step_avg:97.97ms
step:535/1770 train_time:52416ms step_avg:97.97ms
step:536/1770 train_time:52516ms step_avg:97.98ms
step:537/1770 train_time:52616ms step_avg:97.98ms
step:538/1770 train_time:52716ms step_avg:97.98ms
step:539/1770 train_time:52815ms step_avg:97.99ms
step:540/1770 train_time:52916ms step_avg:97.99ms
step:541/1770 train_time:53016ms step_avg:98.00ms
step:542/1770 train_time:53116ms step_avg:98.00ms
step:543/1770 train_time:53216ms step_avg:98.00ms
step:544/1770 train_time:53316ms step_avg:98.01ms
step:545/1770 train_time:53416ms step_avg:98.01ms
step:546/1770 train_time:53516ms step_avg:98.02ms
step:547/1770 train_time:53616ms step_avg:98.02ms
step:548/1770 train_time:53716ms step_avg:98.02ms
step:549/1770 train_time:53816ms step_avg:98.03ms
step:550/1770 train_time:53916ms step_avg:98.03ms
step:551/1770 train_time:54016ms step_avg:98.03ms
step:552/1770 train_time:54117ms step_avg:98.04ms
step:553/1770 train_time:54217ms step_avg:98.04ms
step:554/1770 train_time:54317ms step_avg:98.05ms
step:555/1770 train_time:54417ms step_avg:98.05ms
step:556/1770 train_time:54517ms step_avg:98.05ms
step:557/1770 train_time:54617ms step_avg:98.06ms
step:558/1770 train_time:54717ms step_avg:98.06ms
step:559/1770 train_time:54816ms step_avg:98.06ms
step:560/1770 train_time:54917ms step_avg:98.07ms
step:561/1770 train_time:55017ms step_avg:98.07ms
step:562/1770 train_time:55117ms step_avg:98.07ms
step:563/1770 train_time:55216ms step_avg:98.08ms
step:564/1770 train_time:55316ms step_avg:98.08ms
step:565/1770 train_time:55416ms step_avg:98.08ms
step:566/1770 train_time:55516ms step_avg:98.08ms
step:567/1770 train_time:55616ms step_avg:98.09ms
step:568/1770 train_time:55716ms step_avg:98.09ms
step:569/1770 train_time:55815ms step_avg:98.09ms
step:570/1770 train_time:55916ms step_avg:98.10ms
step:571/1770 train_time:56016ms step_avg:98.10ms
step:572/1770 train_time:56116ms step_avg:98.11ms
step:573/1770 train_time:56216ms step_avg:98.11ms
step:574/1770 train_time:56316ms step_avg:98.11ms
step:575/1770 train_time:56417ms step_avg:98.12ms
step:576/1770 train_time:56517ms step_avg:98.12ms
step:577/1770 train_time:56616ms step_avg:98.12ms
step:578/1770 train_time:56716ms step_avg:98.13ms
step:579/1770 train_time:56816ms step_avg:98.13ms
step:580/1770 train_time:56916ms step_avg:98.13ms
step:581/1770 train_time:57016ms step_avg:98.13ms
step:582/1770 train_time:57116ms step_avg:98.14ms
step:583/1770 train_time:57216ms step_avg:98.14ms
step:584/1770 train_time:57317ms step_avg:98.14ms
step:585/1770 train_time:57417ms step_avg:98.15ms
step:586/1770 train_time:57517ms step_avg:98.15ms
step:587/1770 train_time:57617ms step_avg:98.15ms
step:588/1770 train_time:57717ms step_avg:98.16ms
step:589/1770 train_time:57817ms step_avg:98.16ms
step:590/1770 train_time:57917ms step_avg:98.16ms
step:591/1770 train_time:58017ms step_avg:98.17ms
step:592/1770 train_time:58117ms step_avg:98.17ms
step:593/1770 train_time:58217ms step_avg:98.17ms
step:594/1770 train_time:58317ms step_avg:98.18ms
step:595/1770 train_time:58417ms step_avg:98.18ms
step:596/1770 train_time:58516ms step_avg:98.18ms
step:597/1770 train_time:58615ms step_avg:98.18ms
step:598/1770 train_time:58715ms step_avg:98.19ms
step:599/1770 train_time:58815ms step_avg:98.19ms
step:600/1770 train_time:58916ms step_avg:98.19ms
step:601/1770 train_time:59016ms step_avg:98.20ms
step:602/1770 train_time:59116ms step_avg:98.20ms
step:603/1770 train_time:59216ms step_avg:98.20ms
step:604/1770 train_time:59317ms step_avg:98.21ms
step:605/1770 train_time:59416ms step_avg:98.21ms
step:606/1770 train_time:59516ms step_avg:98.21ms
step:607/1770 train_time:59616ms step_avg:98.21ms
step:608/1770 train_time:59716ms step_avg:98.22ms
step:609/1770 train_time:59816ms step_avg:98.22ms
step:610/1770 train_time:59916ms step_avg:98.22ms
step:611/1770 train_time:60016ms step_avg:98.23ms
step:612/1770 train_time:60116ms step_avg:98.23ms
step:613/1770 train_time:60217ms step_avg:98.23ms
step:614/1770 train_time:60317ms step_avg:98.24ms
step:615/1770 train_time:60417ms step_avg:98.24ms
step:616/1770 train_time:60517ms step_avg:98.24ms
step:617/1770 train_time:60616ms step_avg:98.24ms
step:618/1770 train_time:60716ms step_avg:98.25ms
step:619/1770 train_time:60816ms step_avg:98.25ms
step:620/1770 train_time:60916ms step_avg:98.25ms
step:621/1770 train_time:61016ms step_avg:98.25ms
step:622/1770 train_time:61116ms step_avg:98.26ms
step:623/1770 train_time:61215ms step_avg:98.26ms
step:624/1770 train_time:61315ms step_avg:98.26ms
step:625/1770 train_time:61415ms step_avg:98.26ms
step:625/1770 val_loss:3.6588 train_time:61510ms step_avg:98.42ms
step:626/1770 train_time:61533ms step_avg:98.30ms
step:627/1770 train_time:61622ms step_avg:98.28ms
step:628/1770 train_time:61724ms step_avg:98.29ms
step:629/1770 train_time:61826ms step_avg:98.29ms
step:630/1770 train_time:61925ms step_avg:98.29ms
step:631/1770 train_time:62025ms step_avg:98.30ms
step:632/1770 train_time:62123ms step_avg:98.30ms
step:633/1770 train_time:62222ms step_avg:98.30ms
step:634/1770 train_time:62321ms step_avg:98.30ms
step:635/1770 train_time:62421ms step_avg:98.30ms
step:636/1770 train_time:62520ms step_avg:98.30ms
step:637/1770 train_time:62619ms step_avg:98.30ms
step:638/1770 train_time:62719ms step_avg:98.31ms
step:639/1770 train_time:62820ms step_avg:98.31ms
step:640/1770 train_time:62919ms step_avg:98.31ms
step:641/1770 train_time:63019ms step_avg:98.31ms
step:642/1770 train_time:63118ms step_avg:98.31ms
step:643/1770 train_time:63217ms step_avg:98.32ms
step:644/1770 train_time:63316ms step_avg:98.32ms
step:645/1770 train_time:63416ms step_avg:98.32ms
step:646/1770 train_time:63516ms step_avg:98.32ms
step:647/1770 train_time:63617ms step_avg:98.33ms
step:648/1770 train_time:63716ms step_avg:98.33ms
step:649/1770 train_time:63816ms step_avg:98.33ms
step:650/1770 train_time:63916ms step_avg:98.33ms
step:651/1770 train_time:64016ms step_avg:98.34ms
step:652/1770 train_time:64116ms step_avg:98.34ms
step:653/1770 train_time:64216ms step_avg:98.34ms
step:654/1770 train_time:64316ms step_avg:98.34ms
step:655/1770 train_time:64416ms step_avg:98.34ms
step:656/1770 train_time:64515ms step_avg:98.35ms
step:657/1770 train_time:64615ms step_avg:98.35ms
step:658/1770 train_time:64716ms step_avg:98.35ms
step:659/1770 train_time:64818ms step_avg:98.36ms
step:660/1770 train_time:64919ms step_avg:98.36ms
step:661/1770 train_time:65019ms step_avg:98.36ms
step:662/1770 train_time:65120ms step_avg:98.37ms
step:663/1770 train_time:65221ms step_avg:98.37ms
step:664/1770 train_time:65323ms step_avg:98.38ms
step:665/1770 train_time:65424ms step_avg:98.38ms
step:666/1770 train_time:65525ms step_avg:98.39ms
step:667/1770 train_time:65626ms step_avg:98.39ms
step:668/1770 train_time:65727ms step_avg:98.39ms
step:669/1770 train_time:65829ms step_avg:98.40ms
step:670/1770 train_time:65931ms step_avg:98.40ms
step:671/1770 train_time:66033ms step_avg:98.41ms
step:672/1770 train_time:66135ms step_avg:98.41ms
step:673/1770 train_time:66236ms step_avg:98.42ms
step:674/1770 train_time:66337ms step_avg:98.42ms
step:675/1770 train_time:66439ms step_avg:98.43ms
step:676/1770 train_time:66540ms step_avg:98.43ms
step:677/1770 train_time:66641ms step_avg:98.44ms
step:678/1770 train_time:66742ms step_avg:98.44ms
step:679/1770 train_time:66843ms step_avg:98.44ms
step:680/1770 train_time:66944ms step_avg:98.45ms
step:681/1770 train_time:67045ms step_avg:98.45ms
step:682/1770 train_time:67146ms step_avg:98.45ms
step:683/1770 train_time:67247ms step_avg:98.46ms
step:684/1770 train_time:67348ms step_avg:98.46ms
step:685/1770 train_time:67449ms step_avg:98.47ms
step:686/1770 train_time:67550ms step_avg:98.47ms
step:687/1770 train_time:67652ms step_avg:98.47ms
step:688/1770 train_time:67754ms step_avg:98.48ms
step:689/1770 train_time:67856ms step_avg:98.49ms
step:690/1770 train_time:67957ms step_avg:98.49ms
step:691/1770 train_time:68059ms step_avg:98.49ms
step:692/1770 train_time:68159ms step_avg:98.50ms
step:693/1770 train_time:68260ms step_avg:98.50ms
step:694/1770 train_time:68361ms step_avg:98.50ms
step:695/1770 train_time:68461ms step_avg:98.51ms
step:696/1770 train_time:68562ms step_avg:98.51ms
step:697/1770 train_time:68664ms step_avg:98.51ms
step:698/1770 train_time:68766ms step_avg:98.52ms
step:699/1770 train_time:68867ms step_avg:98.52ms
step:700/1770 train_time:68968ms step_avg:98.53ms
step:701/1770 train_time:69070ms step_avg:98.53ms
step:702/1770 train_time:69171ms step_avg:98.53ms
step:703/1770 train_time:69273ms step_avg:98.54ms
step:704/1770 train_time:69374ms step_avg:98.54ms
step:705/1770 train_time:69476ms step_avg:98.55ms
step:706/1770 train_time:69578ms step_avg:98.55ms
step:707/1770 train_time:69679ms step_avg:98.56ms
step:708/1770 train_time:69781ms step_avg:98.56ms
step:709/1770 train_time:69882ms step_avg:98.56ms
step:710/1770 train_time:69983ms step_avg:98.57ms
step:711/1770 train_time:70084ms step_avg:98.57ms
step:712/1770 train_time:70185ms step_avg:98.57ms
step:713/1770 train_time:70287ms step_avg:98.58ms
step:714/1770 train_time:70387ms step_avg:98.58ms
step:715/1770 train_time:70488ms step_avg:98.58ms
step:716/1770 train_time:70589ms step_avg:98.59ms
step:717/1770 train_time:70691ms step_avg:98.59ms
step:718/1770 train_time:70793ms step_avg:98.60ms
step:719/1770 train_time:70894ms step_avg:98.60ms
step:720/1770 train_time:70996ms step_avg:98.61ms
step:721/1770 train_time:71098ms step_avg:98.61ms
step:722/1770 train_time:71199ms step_avg:98.61ms
step:723/1770 train_time:71301ms step_avg:98.62ms
step:724/1770 train_time:71402ms step_avg:98.62ms
step:725/1770 train_time:71503ms step_avg:98.62ms
step:726/1770 train_time:71604ms step_avg:98.63ms
step:727/1770 train_time:71705ms step_avg:98.63ms
step:728/1770 train_time:71806ms step_avg:98.63ms
step:729/1770 train_time:71907ms step_avg:98.64ms
step:730/1770 train_time:72008ms step_avg:98.64ms
step:731/1770 train_time:72110ms step_avg:98.65ms
step:732/1770 train_time:72213ms step_avg:98.65ms
step:733/1770 train_time:72315ms step_avg:98.66ms
step:734/1770 train_time:72416ms step_avg:98.66ms
step:735/1770 train_time:72517ms step_avg:98.66ms
step:736/1770 train_time:72617ms step_avg:98.66ms
step:737/1770 train_time:72719ms step_avg:98.67ms
step:738/1770 train_time:72820ms step_avg:98.67ms
step:739/1770 train_time:72921ms step_avg:98.67ms
step:740/1770 train_time:73022ms step_avg:98.68ms
step:741/1770 train_time:73123ms step_avg:98.68ms
step:742/1770 train_time:73224ms step_avg:98.69ms
step:743/1770 train_time:73325ms step_avg:98.69ms
step:744/1770 train_time:73425ms step_avg:98.69ms
step:745/1770 train_time:73526ms step_avg:98.69ms
step:746/1770 train_time:73626ms step_avg:98.69ms
step:747/1770 train_time:73728ms step_avg:98.70ms
step:748/1770 train_time:73830ms step_avg:98.70ms
step:749/1770 train_time:73932ms step_avg:98.71ms
step:750/1770 train_time:74034ms step_avg:98.71ms
step:750/1770 val_loss:3.5940 train_time:74129ms step_avg:98.84ms
step:751/1770 train_time:74150ms step_avg:98.74ms
step:752/1770 train_time:74244ms step_avg:98.73ms
step:753/1770 train_time:74348ms step_avg:98.74ms
step:754/1770 train_time:74449ms step_avg:98.74ms
step:755/1770 train_time:74550ms step_avg:98.74ms
step:756/1770 train_time:74651ms step_avg:98.74ms
step:757/1770 train_time:74751ms step_avg:98.75ms
step:758/1770 train_time:74852ms step_avg:98.75ms
step:759/1770 train_time:74953ms step_avg:98.75ms
step:760/1770 train_time:75054ms step_avg:98.75ms
step:761/1770 train_time:75154ms step_avg:98.76ms
step:762/1770 train_time:75255ms step_avg:98.76ms
step:763/1770 train_time:75357ms step_avg:98.76ms
step:764/1770 train_time:75459ms step_avg:98.77ms
step:765/1770 train_time:75561ms step_avg:98.77ms
step:766/1770 train_time:75663ms step_avg:98.78ms
step:767/1770 train_time:75765ms step_avg:98.78ms
step:768/1770 train_time:75868ms step_avg:98.79ms
step:769/1770 train_time:75968ms step_avg:98.79ms
step:770/1770 train_time:76069ms step_avg:98.79ms
step:771/1770 train_time:76170ms step_avg:98.79ms
step:772/1770 train_time:76271ms step_avg:98.80ms
step:773/1770 train_time:76372ms step_avg:98.80ms
step:774/1770 train_time:76473ms step_avg:98.80ms
step:775/1770 train_time:76574ms step_avg:98.81ms
step:776/1770 train_time:76676ms step_avg:98.81ms
step:777/1770 train_time:76778ms step_avg:98.81ms
step:778/1770 train_time:76879ms step_avg:98.82ms
step:779/1770 train_time:76983ms step_avg:98.82ms
step:780/1770 train_time:77082ms step_avg:98.82ms
step:781/1770 train_time:77183ms step_avg:98.83ms
step:782/1770 train_time:77285ms step_avg:98.83ms
step:783/1770 train_time:77386ms step_avg:98.83ms
step:784/1770 train_time:77487ms step_avg:98.84ms
step:785/1770 train_time:77588ms step_avg:98.84ms
step:786/1770 train_time:77689ms step_avg:98.84ms
step:787/1770 train_time:77790ms step_avg:98.84ms
step:788/1770 train_time:77892ms step_avg:98.85ms
step:789/1770 train_time:77993ms step_avg:98.85ms
step:790/1770 train_time:78094ms step_avg:98.85ms
step:791/1770 train_time:78195ms step_avg:98.86ms
step:792/1770 train_time:78296ms step_avg:98.86ms
step:793/1770 train_time:78398ms step_avg:98.86ms
step:794/1770 train_time:78500ms step_avg:98.87ms
step:795/1770 train_time:78602ms step_avg:98.87ms
step:796/1770 train_time:78704ms step_avg:98.87ms
step:797/1770 train_time:78805ms step_avg:98.88ms
step:798/1770 train_time:78907ms step_avg:98.88ms
step:799/1770 train_time:79008ms step_avg:98.88ms
step:800/1770 train_time:79110ms step_avg:98.89ms
step:801/1770 train_time:79211ms step_avg:98.89ms
step:802/1770 train_time:79312ms step_avg:98.89ms
step:803/1770 train_time:79413ms step_avg:98.90ms
step:804/1770 train_time:79514ms step_avg:98.90ms
step:805/1770 train_time:79615ms step_avg:98.90ms
step:806/1770 train_time:79717ms step_avg:98.90ms
step:807/1770 train_time:79818ms step_avg:98.91ms
step:808/1770 train_time:79921ms step_avg:98.91ms
step:809/1770 train_time:80023ms step_avg:98.92ms
step:810/1770 train_time:80126ms step_avg:98.92ms
step:811/1770 train_time:80227ms step_avg:98.92ms
step:812/1770 train_time:80328ms step_avg:98.93ms
step:813/1770 train_time:80429ms step_avg:98.93ms
step:814/1770 train_time:80530ms step_avg:98.93ms
step:815/1770 train_time:80631ms step_avg:98.93ms
step:816/1770 train_time:80732ms step_avg:98.94ms
step:817/1770 train_time:80833ms step_avg:98.94ms
step:818/1770 train_time:80934ms step_avg:98.94ms
step:819/1770 train_time:81035ms step_avg:98.94ms
step:820/1770 train_time:81137ms step_avg:98.95ms
step:821/1770 train_time:81238ms step_avg:98.95ms
step:822/1770 train_time:81340ms step_avg:98.95ms
step:823/1770 train_time:81442ms step_avg:98.96ms
step:824/1770 train_time:81544ms step_avg:98.96ms
step:825/1770 train_time:81645ms step_avg:98.96ms
step:826/1770 train_time:81747ms step_avg:98.97ms
step:827/1770 train_time:81848ms step_avg:98.97ms
step:828/1770 train_time:81948ms step_avg:98.97ms
step:829/1770 train_time:82049ms step_avg:98.97ms
step:830/1770 train_time:82151ms step_avg:98.98ms
step:831/1770 train_time:82253ms step_avg:98.98ms
step:832/1770 train_time:82355ms step_avg:98.98ms
step:833/1770 train_time:82455ms step_avg:98.99ms
step:834/1770 train_time:82557ms step_avg:98.99ms
step:835/1770 train_time:82658ms step_avg:98.99ms
step:836/1770 train_time:82761ms step_avg:99.00ms
step:837/1770 train_time:82863ms step_avg:99.00ms
step:838/1770 train_time:82965ms step_avg:99.00ms
step:839/1770 train_time:83065ms step_avg:99.01ms
step:840/1770 train_time:83166ms step_avg:99.01ms
step:841/1770 train_time:83268ms step_avg:99.01ms
step:842/1770 train_time:83369ms step_avg:99.01ms
step:843/1770 train_time:83470ms step_avg:99.02ms
step:844/1770 train_time:83571ms step_avg:99.02ms
step:845/1770 train_time:83672ms step_avg:99.02ms
step:846/1770 train_time:83773ms step_avg:99.02ms
step:847/1770 train_time:83874ms step_avg:99.03ms
step:848/1770 train_time:83976ms step_avg:99.03ms
step:849/1770 train_time:84077ms step_avg:99.03ms
step:850/1770 train_time:84180ms step_avg:99.04ms
step:851/1770 train_time:84282ms step_avg:99.04ms
step:852/1770 train_time:84384ms step_avg:99.04ms
step:853/1770 train_time:84485ms step_avg:99.04ms
step:854/1770 train_time:84586ms step_avg:99.05ms
step:855/1770 train_time:84688ms step_avg:99.05ms
step:856/1770 train_time:84789ms step_avg:99.05ms
step:857/1770 train_time:84890ms step_avg:99.05ms
step:858/1770 train_time:84992ms step_avg:99.06ms
step:859/1770 train_time:85092ms step_avg:99.06ms
step:860/1770 train_time:85193ms step_avg:99.06ms
step:861/1770 train_time:85294ms step_avg:99.06ms
step:862/1770 train_time:85395ms step_avg:99.07ms
step:863/1770 train_time:85496ms step_avg:99.07ms
step:864/1770 train_time:85598ms step_avg:99.07ms
step:865/1770 train_time:85701ms step_avg:99.08ms
step:866/1770 train_time:85803ms step_avg:99.08ms
step:867/1770 train_time:85904ms step_avg:99.08ms
step:868/1770 train_time:86006ms step_avg:99.09ms
step:869/1770 train_time:86107ms step_avg:99.09ms
step:870/1770 train_time:86208ms step_avg:99.09ms
step:871/1770 train_time:86309ms step_avg:99.09ms
step:872/1770 train_time:86410ms step_avg:99.09ms
step:873/1770 train_time:86511ms step_avg:99.10ms
step:874/1770 train_time:86612ms step_avg:99.10ms
step:875/1770 train_time:86714ms step_avg:99.10ms
step:875/1770 val_loss:3.5474 train_time:86810ms step_avg:99.21ms
step:876/1770 train_time:86830ms step_avg:99.12ms
step:877/1770 train_time:86926ms step_avg:99.12ms
step:878/1770 train_time:87027ms step_avg:99.12ms
step:879/1770 train_time:87128ms step_avg:99.12ms
step:880/1770 train_time:87229ms step_avg:99.12ms
step:881/1770 train_time:87330ms step_avg:99.13ms
step:882/1770 train_time:87430ms step_avg:99.13ms
step:883/1770 train_time:87532ms step_avg:99.13ms
step:884/1770 train_time:87632ms step_avg:99.13ms
step:885/1770 train_time:87734ms step_avg:99.13ms
step:886/1770 train_time:87835ms step_avg:99.14ms
step:887/1770 train_time:87938ms step_avg:99.14ms
step:888/1770 train_time:88040ms step_avg:99.14ms
step:889/1770 train_time:88142ms step_avg:99.15ms
step:890/1770 train_time:88243ms step_avg:99.15ms
step:891/1770 train_time:88344ms step_avg:99.15ms
step:892/1770 train_time:88445ms step_avg:99.15ms
step:893/1770 train_time:88547ms step_avg:99.16ms
step:894/1770 train_time:88648ms step_avg:99.16ms
step:895/1770 train_time:88749ms step_avg:99.16ms
step:896/1770 train_time:88851ms step_avg:99.16ms
step:897/1770 train_time:88952ms step_avg:99.17ms
step:898/1770 train_time:89053ms step_avg:99.17ms
step:899/1770 train_time:89155ms step_avg:99.17ms
step:900/1770 train_time:89257ms step_avg:99.17ms
step:901/1770 train_time:89360ms step_avg:99.18ms
step:902/1770 train_time:89462ms step_avg:99.18ms
step:903/1770 train_time:89563ms step_avg:99.18ms
step:904/1770 train_time:89664ms step_avg:99.19ms
step:905/1770 train_time:89765ms step_avg:99.19ms
step:906/1770 train_time:89865ms step_avg:99.19ms
step:907/1770 train_time:89966ms step_avg:99.19ms
step:908/1770 train_time:90067ms step_avg:99.19ms
step:909/1770 train_time:90169ms step_avg:99.20ms
step:910/1770 train_time:90270ms step_avg:99.20ms
step:911/1770 train_time:90372ms step_avg:99.20ms
step:912/1770 train_time:90473ms step_avg:99.20ms
step:913/1770 train_time:90574ms step_avg:99.21ms
step:914/1770 train_time:90676ms step_avg:99.21ms
step:915/1770 train_time:90778ms step_avg:99.21ms
step:916/1770 train_time:90881ms step_avg:99.21ms
step:917/1770 train_time:90984ms step_avg:99.22ms
step:918/1770 train_time:91086ms step_avg:99.22ms
step:919/1770 train_time:91187ms step_avg:99.22ms
step:920/1770 train_time:91289ms step_avg:99.23ms
step:921/1770 train_time:91392ms step_avg:99.23ms
step:922/1770 train_time:91495ms step_avg:99.24ms
step:923/1770 train_time:91598ms step_avg:99.24ms
step:924/1770 train_time:91700ms step_avg:99.24ms
step:925/1770 train_time:91803ms step_avg:99.25ms
step:926/1770 train_time:91906ms step_avg:99.25ms
step:927/1770 train_time:92008ms step_avg:99.25ms
step:928/1770 train_time:92111ms step_avg:99.26ms
step:929/1770 train_time:92213ms step_avg:99.26ms
step:930/1770 train_time:92315ms step_avg:99.26ms
step:931/1770 train_time:92418ms step_avg:99.27ms
step:932/1770 train_time:92520ms step_avg:99.27ms
step:933/1770 train_time:92623ms step_avg:99.27ms
step:934/1770 train_time:92726ms step_avg:99.28ms
step:935/1770 train_time:92828ms step_avg:99.28ms
step:936/1770 train_time:92931ms step_avg:99.28ms
step:937/1770 train_time:93033ms step_avg:99.29ms
step:938/1770 train_time:93136ms step_avg:99.29ms
step:939/1770 train_time:93238ms step_avg:99.30ms
step:940/1770 train_time:93341ms step_avg:99.30ms
step:941/1770 train_time:93445ms step_avg:99.30ms
step:942/1770 train_time:93547ms step_avg:99.31ms
step:943/1770 train_time:93650ms step_avg:99.31ms
step:944/1770 train_time:93752ms step_avg:99.31ms
step:945/1770 train_time:93855ms step_avg:99.32ms
step:946/1770 train_time:93958ms step_avg:99.32ms
step:947/1770 train_time:94062ms step_avg:99.33ms
step:948/1770 train_time:94165ms step_avg:99.33ms
step:949/1770 train_time:94268ms step_avg:99.33ms
step:950/1770 train_time:94371ms step_avg:99.34ms
step:951/1770 train_time:94474ms step_avg:99.34ms
step:952/1770 train_time:94578ms step_avg:99.35ms
step:953/1770 train_time:94682ms step_avg:99.35ms
step:954/1770 train_time:94784ms step_avg:99.35ms
step:955/1770 train_time:94886ms step_avg:99.36ms
step:956/1770 train_time:94988ms step_avg:99.36ms
step:957/1770 train_time:95091ms step_avg:99.36ms
step:958/1770 train_time:95194ms step_avg:99.37ms
step:959/1770 train_time:95297ms step_avg:99.37ms
step:960/1770 train_time:95399ms step_avg:99.37ms
step:961/1770 train_time:95502ms step_avg:99.38ms
step:962/1770 train_time:95605ms step_avg:99.38ms
step:963/1770 train_time:95707ms step_avg:99.38ms
step:964/1770 train_time:95810ms step_avg:99.39ms
step:965/1770 train_time:95913ms step_avg:99.39ms
step:966/1770 train_time:96015ms step_avg:99.39ms
step:967/1770 train_time:96119ms step_avg:99.40ms
step:968/1770 train_time:96222ms step_avg:99.40ms
step:969/1770 train_time:96324ms step_avg:99.41ms
step:970/1770 train_time:96426ms step_avg:99.41ms
step:971/1770 train_time:96529ms step_avg:99.41ms
step:972/1770 train_time:96632ms step_avg:99.42ms
step:973/1770 train_time:96735ms step_avg:99.42ms
step:974/1770 train_time:96838ms step_avg:99.42ms
step:975/1770 train_time:96942ms step_avg:99.43ms
step:976/1770 train_time:97044ms step_avg:99.43ms
step:977/1770 train_time:97146ms step_avg:99.43ms
step:978/1770 train_time:97248ms step_avg:99.44ms
step:979/1770 train_time:97352ms step_avg:99.44ms
step:980/1770 train_time:97454ms step_avg:99.44ms
step:981/1770 train_time:97557ms step_avg:99.45ms
step:982/1770 train_time:97659ms step_avg:99.45ms
step:983/1770 train_time:97763ms step_avg:99.45ms
step:984/1770 train_time:97866ms step_avg:99.46ms
step:985/1770 train_time:97969ms step_avg:99.46ms
step:986/1770 train_time:98072ms step_avg:99.46ms
step:987/1770 train_time:98174ms step_avg:99.47ms
step:988/1770 train_time:98277ms step_avg:99.47ms
step:989/1770 train_time:98382ms step_avg:99.48ms
step:990/1770 train_time:98484ms step_avg:99.48ms
step:991/1770 train_time:98587ms step_avg:99.48ms
step:992/1770 train_time:98689ms step_avg:99.49ms
step:993/1770 train_time:98792ms step_avg:99.49ms
step:994/1770 train_time:98894ms step_avg:99.49ms
step:995/1770 train_time:98998ms step_avg:99.50ms
step:996/1770 train_time:99101ms step_avg:99.50ms
step:997/1770 train_time:99203ms step_avg:99.50ms
step:998/1770 train_time:99305ms step_avg:99.50ms
step:999/1770 train_time:99408ms step_avg:99.51ms
step:1000/1770 train_time:99511ms step_avg:99.51ms
step:1000/1770 val_loss:3.5095 train_time:99607ms step_avg:99.61ms
step:1001/1770 train_time:99630ms step_avg:99.53ms
step:1002/1770 train_time:99725ms step_avg:99.53ms
step:1003/1770 train_time:99831ms step_avg:99.53ms
step:1004/1770 train_time:99934ms step_avg:99.54ms
step:1005/1770 train_time:100036ms step_avg:99.54ms
step:1006/1770 train_time:100138ms step_avg:99.54ms
step:1007/1770 train_time:100240ms step_avg:99.54ms
step:1008/1770 train_time:100343ms step_avg:99.55ms
step:1009/1770 train_time:100444ms step_avg:99.55ms
step:1010/1770 train_time:100547ms step_avg:99.55ms
step:1011/1770 train_time:100651ms step_avg:99.56ms
step:1012/1770 train_time:100755ms step_avg:99.56ms
step:1013/1770 train_time:100857ms step_avg:99.56ms
step:1014/1770 train_time:100959ms step_avg:99.57ms
step:1015/1770 train_time:101061ms step_avg:99.57ms
step:1016/1770 train_time:101164ms step_avg:99.57ms
step:1017/1770 train_time:101268ms step_avg:99.58ms
step:1018/1770 train_time:101371ms step_avg:99.58ms
step:1019/1770 train_time:101474ms step_avg:99.58ms
step:1020/1770 train_time:101576ms step_avg:99.58ms
step:1021/1770 train_time:101678ms step_avg:99.59ms
step:1022/1770 train_time:101781ms step_avg:99.59ms
step:1023/1770 train_time:101883ms step_avg:99.59ms
step:1024/1770 train_time:101986ms step_avg:99.60ms
step:1025/1770 train_time:102089ms step_avg:99.60ms
step:1026/1770 train_time:102194ms step_avg:99.60ms
step:1027/1770 train_time:102296ms step_avg:99.61ms
step:1028/1770 train_time:102399ms step_avg:99.61ms
step:1029/1770 train_time:102502ms step_avg:99.61ms
step:1030/1770 train_time:102605ms step_avg:99.62ms
step:1031/1770 train_time:102708ms step_avg:99.62ms
step:1032/1770 train_time:102812ms step_avg:99.62ms
step:1033/1770 train_time:102915ms step_avg:99.63ms
step:1034/1770 train_time:103018ms step_avg:99.63ms
step:1035/1770 train_time:103119ms step_avg:99.63ms
step:1036/1770 train_time:103222ms step_avg:99.63ms
step:1037/1770 train_time:103324ms step_avg:99.64ms
step:1038/1770 train_time:103427ms step_avg:99.64ms
step:1039/1770 train_time:103530ms step_avg:99.64ms
step:1040/1770 train_time:103633ms step_avg:99.65ms
step:1041/1770 train_time:103735ms step_avg:99.65ms
step:1042/1770 train_time:103838ms step_avg:99.65ms
step:1043/1770 train_time:103940ms step_avg:99.65ms
step:1044/1770 train_time:104043ms step_avg:99.66ms
step:1045/1770 train_time:104146ms step_avg:99.66ms
step:1046/1770 train_time:104249ms step_avg:99.66ms
step:1047/1770 train_time:104352ms step_avg:99.67ms
step:1048/1770 train_time:104455ms step_avg:99.67ms
step:1049/1770 train_time:104557ms step_avg:99.67ms
step:1050/1770 train_time:104660ms step_avg:99.68ms
step:1051/1770 train_time:104762ms step_avg:99.68ms
step:1052/1770 train_time:104865ms step_avg:99.68ms
step:1053/1770 train_time:104968ms step_avg:99.68ms
step:1054/1770 train_time:105072ms step_avg:99.69ms
step:1055/1770 train_time:105175ms step_avg:99.69ms
step:1056/1770 train_time:105277ms step_avg:99.69ms
step:1057/1770 train_time:105380ms step_avg:99.70ms
step:1058/1770 train_time:105483ms step_avg:99.70ms
step:1059/1770 train_time:105586ms step_avg:99.70ms
step:1060/1770 train_time:105689ms step_avg:99.71ms
step:1061/1770 train_time:105794ms step_avg:99.71ms
step:1062/1770 train_time:105897ms step_avg:99.71ms
step:1063/1770 train_time:106002ms step_avg:99.72ms
step:1064/1770 train_time:106105ms step_avg:99.72ms
step:1065/1770 train_time:106209ms step_avg:99.73ms
step:1066/1770 train_time:106313ms step_avg:99.73ms
step:1067/1770 train_time:106416ms step_avg:99.73ms
step:1068/1770 train_time:106520ms step_avg:99.74ms
step:1069/1770 train_time:106622ms step_avg:99.74ms
step:1070/1770 train_time:106725ms step_avg:99.74ms
step:1071/1770 train_time:106828ms step_avg:99.75ms
step:1072/1770 train_time:106931ms step_avg:99.75ms
step:1073/1770 train_time:107033ms step_avg:99.75ms
step:1074/1770 train_time:107136ms step_avg:99.75ms
step:1075/1770 train_time:107240ms step_avg:99.76ms
step:1076/1770 train_time:107343ms step_avg:99.76ms
step:1077/1770 train_time:107447ms step_avg:99.76ms
step:1078/1770 train_time:107550ms step_avg:99.77ms
step:1079/1770 train_time:107653ms step_avg:99.77ms
step:1080/1770 train_time:107756ms step_avg:99.77ms
step:1081/1770 train_time:107858ms step_avg:99.78ms
step:1082/1770 train_time:107961ms step_avg:99.78ms
step:1083/1770 train_time:108063ms step_avg:99.78ms
step:1084/1770 train_time:108166ms step_avg:99.78ms
step:1085/1770 train_time:108269ms step_avg:99.79ms
step:1086/1770 train_time:108372ms step_avg:99.79ms
step:1087/1770 train_time:108475ms step_avg:99.79ms
step:1088/1770 train_time:108578ms step_avg:99.80ms
step:1089/1770 train_time:108681ms step_avg:99.80ms
step:1090/1770 train_time:108785ms step_avg:99.80ms
step:1091/1770 train_time:108887ms step_avg:99.80ms
step:1092/1770 train_time:108990ms step_avg:99.81ms
step:1093/1770 train_time:109093ms step_avg:99.81ms
step:1094/1770 train_time:109197ms step_avg:99.81ms
step:1095/1770 train_time:109300ms step_avg:99.82ms
step:1096/1770 train_time:109403ms step_avg:99.82ms
step:1097/1770 train_time:109505ms step_avg:99.82ms
step:1098/1770 train_time:109609ms step_avg:99.83ms
step:1099/1770 train_time:109712ms step_avg:99.83ms
step:1100/1770 train_time:109815ms step_avg:99.83ms
step:1101/1770 train_time:109917ms step_avg:99.83ms
step:1102/1770 train_time:110020ms step_avg:99.84ms
step:1103/1770 train_time:110122ms step_avg:99.84ms
step:1104/1770 train_time:110225ms step_avg:99.84ms
step:1105/1770 train_time:110328ms step_avg:99.84ms
step:1106/1770 train_time:110431ms step_avg:99.85ms
step:1107/1770 train_time:110534ms step_avg:99.85ms
step:1108/1770 train_time:110637ms step_avg:99.85ms
step:1109/1770 train_time:110740ms step_avg:99.86ms
step:1110/1770 train_time:110843ms step_avg:99.86ms
step:1111/1770 train_time:110946ms step_avg:99.86ms
step:1112/1770 train_time:111050ms step_avg:99.86ms
step:1113/1770 train_time:111153ms step_avg:99.87ms
step:1114/1770 train_time:111257ms step_avg:99.87ms
step:1115/1770 train_time:111359ms step_avg:99.87ms
step:1116/1770 train_time:111462ms step_avg:99.88ms
step:1117/1770 train_time:111565ms step_avg:99.88ms
step:1118/1770 train_time:111669ms step_avg:99.88ms
step:1119/1770 train_time:111772ms step_avg:99.89ms
step:1120/1770 train_time:111874ms step_avg:99.89ms
step:1121/1770 train_time:111976ms step_avg:99.89ms
step:1122/1770 train_time:112078ms step_avg:99.89ms
step:1123/1770 train_time:112180ms step_avg:99.89ms
step:1124/1770 train_time:112283ms step_avg:99.90ms
step:1125/1770 train_time:112386ms step_avg:99.90ms
step:1125/1770 val_loss:3.4686 train_time:112483ms step_avg:99.99ms
step:1126/1770 train_time:112505ms step_avg:99.92ms
step:1127/1770 train_time:112598ms step_avg:99.91ms
step:1128/1770 train_time:112702ms step_avg:99.91ms
step:1129/1770 train_time:112805ms step_avg:99.92ms
step:1130/1770 train_time:112907ms step_avg:99.92ms
step:1131/1770 train_time:113010ms step_avg:99.92ms
step:1132/1770 train_time:113113ms step_avg:99.92ms
step:1133/1770 train_time:113216ms step_avg:99.93ms
step:1134/1770 train_time:113319ms step_avg:99.93ms
step:1135/1770 train_time:113422ms step_avg:99.93ms
step:1136/1770 train_time:113524ms step_avg:99.93ms
step:1137/1770 train_time:113628ms step_avg:99.94ms
step:1138/1770 train_time:113731ms step_avg:99.94ms
step:1139/1770 train_time:113836ms step_avg:99.94ms
step:1140/1770 train_time:113939ms step_avg:99.95ms
step:1141/1770 train_time:114042ms step_avg:99.95ms
step:1142/1770 train_time:114145ms step_avg:99.95ms
step:1143/1770 train_time:114248ms step_avg:99.95ms
step:1144/1770 train_time:114350ms step_avg:99.96ms
step:1145/1770 train_time:114453ms step_avg:99.96ms
step:1146/1770 train_time:114557ms step_avg:99.96ms
step:1147/1770 train_time:114662ms step_avg:99.97ms
step:1148/1770 train_time:114764ms step_avg:99.97ms
step:1149/1770 train_time:114868ms step_avg:99.97ms
step:1150/1770 train_time:114971ms step_avg:99.97ms
step:1151/1770 train_time:115074ms step_avg:99.98ms
step:1152/1770 train_time:115178ms step_avg:99.98ms
step:1153/1770 train_time:115281ms step_avg:99.98ms
step:1154/1770 train_time:115383ms step_avg:99.99ms
step:1155/1770 train_time:115486ms step_avg:99.99ms
step:1156/1770 train_time:115589ms step_avg:99.99ms
step:1157/1770 train_time:115693ms step_avg:99.99ms
step:1158/1770 train_time:115798ms step_avg:100.00ms
step:1159/1770 train_time:115900ms step_avg:100.00ms
step:1160/1770 train_time:116003ms step_avg:100.00ms
step:1161/1770 train_time:116105ms step_avg:100.00ms
step:1162/1770 train_time:116208ms step_avg:100.01ms
step:1163/1770 train_time:116310ms step_avg:100.01ms
step:1164/1770 train_time:116413ms step_avg:100.01ms
step:1165/1770 train_time:116517ms step_avg:100.01ms
step:1166/1770 train_time:116621ms step_avg:100.02ms
step:1167/1770 train_time:116723ms step_avg:100.02ms
step:1168/1770 train_time:116826ms step_avg:100.02ms
step:1169/1770 train_time:116930ms step_avg:100.03ms
step:1170/1770 train_time:117033ms step_avg:100.03ms
step:1171/1770 train_time:117136ms step_avg:100.03ms
step:1172/1770 train_time:117239ms step_avg:100.03ms
step:1173/1770 train_time:117341ms step_avg:100.04ms
step:1174/1770 train_time:117444ms step_avg:100.04ms
step:1175/1770 train_time:117547ms step_avg:100.04ms
step:1176/1770 train_time:117650ms step_avg:100.04ms
step:1177/1770 train_time:117753ms step_avg:100.05ms
step:1178/1770 train_time:117857ms step_avg:100.05ms
step:1179/1770 train_time:117960ms step_avg:100.05ms
step:1180/1770 train_time:118063ms step_avg:100.05ms
step:1181/1770 train_time:118165ms step_avg:100.05ms
step:1182/1770 train_time:118268ms step_avg:100.06ms
step:1183/1770 train_time:118372ms step_avg:100.06ms
step:1184/1770 train_time:118477ms step_avg:100.06ms
step:1185/1770 train_time:118581ms step_avg:100.07ms
step:1186/1770 train_time:118685ms step_avg:100.07ms
step:1187/1770 train_time:118791ms step_avg:100.08ms
step:1188/1770 train_time:118894ms step_avg:100.08ms
step:1189/1770 train_time:118999ms step_avg:100.08ms
step:1190/1770 train_time:119102ms step_avg:100.09ms
step:1191/1770 train_time:119207ms step_avg:100.09ms
step:1192/1770 train_time:119311ms step_avg:100.09ms
step:1193/1770 train_time:119415ms step_avg:100.10ms
step:1194/1770 train_time:119521ms step_avg:100.10ms
step:1195/1770 train_time:119625ms step_avg:100.10ms
step:1196/1770 train_time:119730ms step_avg:100.11ms
step:1197/1770 train_time:119834ms step_avg:100.11ms
step:1198/1770 train_time:119938ms step_avg:100.12ms
step:1199/1770 train_time:120042ms step_avg:100.12ms
step:1200/1770 train_time:120146ms step_avg:100.12ms
step:1201/1770 train_time:120251ms step_avg:100.13ms
step:1202/1770 train_time:120354ms step_avg:100.13ms
step:1203/1770 train_time:120459ms step_avg:100.13ms
step:1204/1770 train_time:120563ms step_avg:100.14ms
step:1205/1770 train_time:120667ms step_avg:100.14ms
step:1206/1770 train_time:120772ms step_avg:100.14ms
step:1207/1770 train_time:120877ms step_avg:100.15ms
step:1208/1770 train_time:120981ms step_avg:100.15ms
step:1209/1770 train_time:121085ms step_avg:100.15ms
step:1210/1770 train_time:121189ms step_avg:100.16ms
step:1211/1770 train_time:121293ms step_avg:100.16ms
step:1212/1770 train_time:121400ms step_avg:100.17ms
step:1213/1770 train_time:121504ms step_avg:100.17ms
step:1214/1770 train_time:121607ms step_avg:100.17ms
step:1215/1770 train_time:121711ms step_avg:100.17ms
step:1216/1770 train_time:121816ms step_avg:100.18ms
step:1217/1770 train_time:121920ms step_avg:100.18ms
step:1218/1770 train_time:122025ms step_avg:100.18ms
step:1219/1770 train_time:122129ms step_avg:100.19ms
step:1220/1770 train_time:122233ms step_avg:100.19ms
step:1221/1770 train_time:122338ms step_avg:100.19ms
step:1222/1770 train_time:122444ms step_avg:100.20ms
step:1223/1770 train_time:122548ms step_avg:100.20ms
step:1224/1770 train_time:122653ms step_avg:100.21ms
step:1225/1770 train_time:122758ms step_avg:100.21ms
step:1226/1770 train_time:122862ms step_avg:100.21ms
step:1227/1770 train_time:122967ms step_avg:100.22ms
step:1228/1770 train_time:123073ms step_avg:100.22ms
step:1229/1770 train_time:123177ms step_avg:100.23ms
step:1230/1770 train_time:123281ms step_avg:100.23ms
step:1231/1770 train_time:123385ms step_avg:100.23ms
step:1232/1770 train_time:123489ms step_avg:100.23ms
step:1233/1770 train_time:123592ms step_avg:100.24ms
step:1234/1770 train_time:123696ms step_avg:100.24ms
step:1235/1770 train_time:123801ms step_avg:100.24ms
step:1236/1770 train_time:123906ms step_avg:100.25ms
step:1237/1770 train_time:124010ms step_avg:100.25ms
step:1238/1770 train_time:124114ms step_avg:100.25ms
step:1239/1770 train_time:124220ms step_avg:100.26ms
step:1240/1770 train_time:124324ms step_avg:100.26ms
step:1241/1770 train_time:124429ms step_avg:100.27ms
step:1242/1770 train_time:124534ms step_avg:100.27ms
step:1243/1770 train_time:124638ms step_avg:100.27ms
step:1244/1770 train_time:124742ms step_avg:100.27ms
step:1245/1770 train_time:124846ms step_avg:100.28ms
step:1246/1770 train_time:124950ms step_avg:100.28ms
step:1247/1770 train_time:125054ms step_avg:100.28ms
step:1248/1770 train_time:125159ms step_avg:100.29ms
step:1249/1770 train_time:125262ms step_avg:100.29ms
step:1250/1770 train_time:125366ms step_avg:100.29ms
step:1250/1770 val_loss:3.4204 train_time:125466ms step_avg:100.37ms
step:1251/1770 train_time:125487ms step_avg:100.31ms
step:1252/1770 train_time:125586ms step_avg:100.31ms
step:1253/1770 train_time:125691ms step_avg:100.31ms
step:1254/1770 train_time:125795ms step_avg:100.31ms
step:1255/1770 train_time:125900ms step_avg:100.32ms
step:1256/1770 train_time:126004ms step_avg:100.32ms
step:1257/1770 train_time:126108ms step_avg:100.32ms
step:1258/1770 train_time:126212ms step_avg:100.33ms
step:1259/1770 train_time:126316ms step_avg:100.33ms
step:1260/1770 train_time:126421ms step_avg:100.33ms
step:1261/1770 train_time:126526ms step_avg:100.34ms
step:1262/1770 train_time:126630ms step_avg:100.34ms
step:1263/1770 train_time:126733ms step_avg:100.34ms
step:1264/1770 train_time:126840ms step_avg:100.35ms
step:1265/1770 train_time:126943ms step_avg:100.35ms
step:1266/1770 train_time:127048ms step_avg:100.35ms
step:1267/1770 train_time:127152ms step_avg:100.36ms
step:1268/1770 train_time:127256ms step_avg:100.36ms
step:1269/1770 train_time:127361ms step_avg:100.36ms
step:1270/1770 train_time:127465ms step_avg:100.37ms
step:1271/1770 train_time:127569ms step_avg:100.37ms
step:1272/1770 train_time:127672ms step_avg:100.37ms
step:1273/1770 train_time:127777ms step_avg:100.37ms
step:1274/1770 train_time:127881ms step_avg:100.38ms
step:1275/1770 train_time:127985ms step_avg:100.38ms
step:1276/1770 train_time:128089ms step_avg:100.38ms
step:1277/1770 train_time:128192ms step_avg:100.39ms
step:1278/1770 train_time:128297ms step_avg:100.39ms
step:1279/1770 train_time:128401ms step_avg:100.39ms
step:1280/1770 train_time:128507ms step_avg:100.40ms
step:1281/1770 train_time:128611ms step_avg:100.40ms
step:1282/1770 train_time:128716ms step_avg:100.40ms
step:1283/1770 train_time:128820ms step_avg:100.41ms
step:1284/1770 train_time:128924ms step_avg:100.41ms
step:1285/1770 train_time:129028ms step_avg:100.41ms
step:1286/1770 train_time:129133ms step_avg:100.41ms
step:1287/1770 train_time:129238ms step_avg:100.42ms
step:1288/1770 train_time:129343ms step_avg:100.42ms
step:1289/1770 train_time:129447ms step_avg:100.42ms
step:1290/1770 train_time:129550ms step_avg:100.43ms
step:1291/1770 train_time:129654ms step_avg:100.43ms
step:1292/1770 train_time:129758ms step_avg:100.43ms
step:1293/1770 train_time:129863ms step_avg:100.44ms
step:1294/1770 train_time:129966ms step_avg:100.44ms
step:1295/1770 train_time:130070ms step_avg:100.44ms
step:1296/1770 train_time:130174ms step_avg:100.44ms
step:1297/1770 train_time:130278ms step_avg:100.45ms
step:1298/1770 train_time:130383ms step_avg:100.45ms
step:1299/1770 train_time:130486ms step_avg:100.45ms
step:1300/1770 train_time:130590ms step_avg:100.45ms
step:1301/1770 train_time:130695ms step_avg:100.46ms
step:1302/1770 train_time:130801ms step_avg:100.46ms
step:1303/1770 train_time:130904ms step_avg:100.46ms
step:1304/1770 train_time:131008ms step_avg:100.47ms
step:1305/1770 train_time:131111ms step_avg:100.47ms
step:1306/1770 train_time:131215ms step_avg:100.47ms
step:1307/1770 train_time:131320ms step_avg:100.47ms
step:1308/1770 train_time:131424ms step_avg:100.48ms
step:1309/1770 train_time:131527ms step_avg:100.48ms
step:1310/1770 train_time:131631ms step_avg:100.48ms
step:1311/1770 train_time:131735ms step_avg:100.48ms
step:1312/1770 train_time:131840ms step_avg:100.49ms
step:1313/1770 train_time:131943ms step_avg:100.49ms
step:1314/1770 train_time:132047ms step_avg:100.49ms
step:1315/1770 train_time:132150ms step_avg:100.49ms
step:1316/1770 train_time:132254ms step_avg:100.50ms
step:1317/1770 train_time:132359ms step_avg:100.50ms
step:1318/1770 train_time:132467ms step_avg:100.51ms
step:1319/1770 train_time:132570ms step_avg:100.51ms
step:1320/1770 train_time:132675ms step_avg:100.51ms
step:1321/1770 train_time:132779ms step_avg:100.51ms
step:1322/1770 train_time:132884ms step_avg:100.52ms
step:1323/1770 train_time:132989ms step_avg:100.52ms
step:1324/1770 train_time:133093ms step_avg:100.52ms
step:1325/1770 train_time:133199ms step_avg:100.53ms
step:1326/1770 train_time:133303ms step_avg:100.53ms
step:1327/1770 train_time:133410ms step_avg:100.54ms
step:1328/1770 train_time:133514ms step_avg:100.54ms
step:1329/1770 train_time:133617ms step_avg:100.54ms
step:1330/1770 train_time:133722ms step_avg:100.54ms
step:1331/1770 train_time:133826ms step_avg:100.55ms
step:1332/1770 train_time:133929ms step_avg:100.55ms
step:1333/1770 train_time:134032ms step_avg:100.55ms
step:1334/1770 train_time:134136ms step_avg:100.55ms
step:1335/1770 train_time:134241ms step_avg:100.55ms
step:1336/1770 train_time:134345ms step_avg:100.56ms
step:1337/1770 train_time:134450ms step_avg:100.56ms
step:1338/1770 train_time:134554ms step_avg:100.56ms
step:1339/1770 train_time:134658ms step_avg:100.57ms
step:1340/1770 train_time:134763ms step_avg:100.57ms
step:1341/1770 train_time:134867ms step_avg:100.57ms
step:1342/1770 train_time:134972ms step_avg:100.58ms
step:1343/1770 train_time:135076ms step_avg:100.58ms
step:1344/1770 train_time:135181ms step_avg:100.58ms
step:1345/1770 train_time:135285ms step_avg:100.58ms
step:1346/1770 train_time:135389ms step_avg:100.59ms
step:1347/1770 train_time:135493ms step_avg:100.59ms
step:1348/1770 train_time:135599ms step_avg:100.59ms
step:1349/1770 train_time:135703ms step_avg:100.60ms
step:1350/1770 train_time:135807ms step_avg:100.60ms
step:1351/1770 train_time:135911ms step_avg:100.60ms
step:1352/1770 train_time:136015ms step_avg:100.60ms
step:1353/1770 train_time:136121ms step_avg:100.61ms
step:1354/1770 train_time:136225ms step_avg:100.61ms
step:1355/1770 train_time:136328ms step_avg:100.61ms
step:1356/1770 train_time:136432ms step_avg:100.61ms
step:1357/1770 train_time:136536ms step_avg:100.62ms
step:1358/1770 train_time:136641ms step_avg:100.62ms
step:1359/1770 train_time:136745ms step_avg:100.62ms
step:1360/1770 train_time:136851ms step_avg:100.63ms
step:1361/1770 train_time:136954ms step_avg:100.63ms
step:1362/1770 train_time:137059ms step_avg:100.63ms
step:1363/1770 train_time:137163ms step_avg:100.63ms
step:1364/1770 train_time:137267ms step_avg:100.64ms
step:1365/1770 train_time:137371ms step_avg:100.64ms
step:1366/1770 train_time:137474ms step_avg:100.64ms
step:1367/1770 train_time:137581ms step_avg:100.64ms
step:1368/1770 train_time:137684ms step_avg:100.65ms
step:1369/1770 train_time:137788ms step_avg:100.65ms
step:1370/1770 train_time:137892ms step_avg:100.65ms
step:1371/1770 train_time:137997ms step_avg:100.65ms
step:1372/1770 train_time:138101ms step_avg:100.66ms
step:1373/1770 train_time:138204ms step_avg:100.66ms
step:1374/1770 train_time:138311ms step_avg:100.66ms
step:1375/1770 train_time:138415ms step_avg:100.67ms
step:1375/1770 val_loss:3.3779 train_time:138515ms step_avg:100.74ms
step:1376/1770 train_time:138535ms step_avg:100.68ms
step:1377/1770 train_time:138632ms step_avg:100.68ms
step:1378/1770 train_time:138736ms step_avg:100.68ms
step:1379/1770 train_time:138841ms step_avg:100.68ms
step:1380/1770 train_time:138945ms step_avg:100.68ms
step:1381/1770 train_time:139049ms step_avg:100.69ms
step:1382/1770 train_time:139153ms step_avg:100.69ms
step:1383/1770 train_time:139258ms step_avg:100.69ms
step:1384/1770 train_time:139361ms step_avg:100.69ms
step:1385/1770 train_time:139465ms step_avg:100.70ms
step:1386/1770 train_time:139570ms step_avg:100.70ms
step:1387/1770 train_time:139674ms step_avg:100.70ms
step:1388/1770 train_time:139778ms step_avg:100.70ms
step:1389/1770 train_time:139883ms step_avg:100.71ms
step:1390/1770 train_time:139987ms step_avg:100.71ms
step:1391/1770 train_time:140090ms step_avg:100.71ms
step:1392/1770 train_time:140194ms step_avg:100.71ms
step:1393/1770 train_time:140299ms step_avg:100.72ms
step:1394/1770 train_time:140403ms step_avg:100.72ms
step:1395/1770 train_time:140509ms step_avg:100.72ms
step:1396/1770 train_time:140614ms step_avg:100.73ms
step:1397/1770 train_time:140718ms step_avg:100.73ms
step:1398/1770 train_time:140823ms step_avg:100.73ms
step:1399/1770 train_time:140927ms step_avg:100.73ms
step:1400/1770 train_time:141032ms step_avg:100.74ms
step:1401/1770 train_time:141136ms step_avg:100.74ms
step:1402/1770 train_time:141240ms step_avg:100.74ms
step:1403/1770 train_time:141345ms step_avg:100.74ms
step:1404/1770 train_time:141450ms step_avg:100.75ms
step:1405/1770 train_time:141555ms step_avg:100.75ms
step:1406/1770 train_time:141660ms step_avg:100.75ms
step:1407/1770 train_time:141764ms step_avg:100.76ms
step:1408/1770 train_time:141868ms step_avg:100.76ms
step:1409/1770 train_time:141972ms step_avg:100.76ms
step:1410/1770 train_time:142077ms step_avg:100.76ms
step:1411/1770 train_time:142181ms step_avg:100.77ms
step:1412/1770 train_time:142286ms step_avg:100.77ms
step:1413/1770 train_time:142390ms step_avg:100.77ms
step:1414/1770 train_time:142494ms step_avg:100.77ms
step:1415/1770 train_time:142600ms step_avg:100.78ms
step:1416/1770 train_time:142704ms step_avg:100.78ms
step:1417/1770 train_time:142808ms step_avg:100.78ms
step:1418/1770 train_time:142912ms step_avg:100.78ms
step:1419/1770 train_time:143017ms step_avg:100.79ms
step:1420/1770 train_time:143122ms step_avg:100.79ms
step:1421/1770 train_time:143225ms step_avg:100.79ms
step:1422/1770 train_time:143329ms step_avg:100.79ms
step:1423/1770 train_time:143434ms step_avg:100.80ms
step:1424/1770 train_time:143539ms step_avg:100.80ms
step:1425/1770 train_time:143643ms step_avg:100.80ms
step:1426/1770 train_time:143748ms step_avg:100.81ms
step:1427/1770 train_time:143852ms step_avg:100.81ms
step:1428/1770 train_time:143956ms step_avg:100.81ms
step:1429/1770 train_time:144061ms step_avg:100.81ms
step:1430/1770 train_time:144165ms step_avg:100.81ms
step:1431/1770 train_time:144270ms step_avg:100.82ms
step:1432/1770 train_time:144373ms step_avg:100.82ms
step:1433/1770 train_time:144478ms step_avg:100.82ms
step:1434/1770 train_time:144581ms step_avg:100.82ms
step:1435/1770 train_time:144685ms step_avg:100.83ms
step:1436/1770 train_time:144792ms step_avg:100.83ms
step:1437/1770 train_time:144897ms step_avg:100.83ms
step:1438/1770 train_time:145001ms step_avg:100.83ms
step:1439/1770 train_time:145104ms step_avg:100.84ms
step:1440/1770 train_time:145208ms step_avg:100.84ms
step:1441/1770 train_time:145315ms step_avg:100.84ms
step:1442/1770 train_time:145418ms step_avg:100.84ms
step:1443/1770 train_time:145523ms step_avg:100.85ms
step:1444/1770 train_time:145629ms step_avg:100.85ms
step:1445/1770 train_time:145734ms step_avg:100.85ms
step:1446/1770 train_time:145839ms step_avg:100.86ms
step:1447/1770 train_time:145945ms step_avg:100.86ms
step:1448/1770 train_time:146049ms step_avg:100.86ms
step:1449/1770 train_time:146156ms step_avg:100.87ms
step:1450/1770 train_time:146262ms step_avg:100.87ms
step:1451/1770 train_time:146367ms step_avg:100.87ms
step:1452/1770 train_time:146473ms step_avg:100.88ms
step:1453/1770 train_time:146578ms step_avg:100.88ms
step:1454/1770 train_time:146683ms step_avg:100.88ms
step:1455/1770 train_time:146790ms step_avg:100.89ms
step:1456/1770 train_time:146896ms step_avg:100.89ms
step:1457/1770 train_time:147002ms step_avg:100.89ms
step:1458/1770 train_time:147106ms step_avg:100.90ms
step:1459/1770 train_time:147213ms step_avg:100.90ms
step:1460/1770 train_time:147318ms step_avg:100.90ms
step:1461/1770 train_time:147424ms step_avg:100.91ms
step:1462/1770 train_time:147530ms step_avg:100.91ms
step:1463/1770 train_time:147635ms step_avg:100.91ms
step:1464/1770 train_time:147742ms step_avg:100.92ms
step:1465/1770 train_time:147846ms step_avg:100.92ms
step:1466/1770 train_time:147952ms step_avg:100.92ms
step:1467/1770 train_time:148059ms step_avg:100.93ms
step:1468/1770 train_time:148165ms step_avg:100.93ms
step:1469/1770 train_time:148270ms step_avg:100.93ms
step:1470/1770 train_time:148375ms step_avg:100.94ms
step:1471/1770 train_time:148480ms step_avg:100.94ms
step:1472/1770 train_time:148585ms step_avg:100.94ms
step:1473/1770 train_time:148691ms step_avg:100.94ms
step:1474/1770 train_time:148798ms step_avg:100.95ms
step:1475/1770 train_time:148904ms step_avg:100.95ms
step:1476/1770 train_time:149008ms step_avg:100.95ms
step:1477/1770 train_time:149115ms step_avg:100.96ms
step:1478/1770 train_time:149221ms step_avg:100.96ms
step:1479/1770 train_time:149328ms step_avg:100.97ms
step:1480/1770 train_time:149432ms step_avg:100.97ms
step:1481/1770 train_time:149541ms step_avg:100.97ms
step:1482/1770 train_time:149645ms step_avg:100.98ms
step:1483/1770 train_time:149750ms step_avg:100.98ms
step:1484/1770 train_time:149855ms step_avg:100.98ms
step:1485/1770 train_time:149961ms step_avg:100.98ms
step:1486/1770 train_time:150066ms step_avg:100.99ms
step:1487/1770 train_time:150171ms step_avg:100.99ms
step:1488/1770 train_time:150277ms step_avg:100.99ms
step:1489/1770 train_time:150384ms step_avg:101.00ms
step:1490/1770 train_time:150490ms step_avg:101.00ms
step:1491/1770 train_time:150596ms step_avg:101.00ms
step:1492/1770 train_time:150701ms step_avg:101.01ms
step:1493/1770 train_time:150810ms step_avg:101.01ms
step:1494/1770 train_time:150920ms step_avg:101.02ms
step:1495/1770 train_time:151025ms step_avg:101.02ms
step:1496/1770 train_time:151129ms step_avg:101.02ms
step:1497/1770 train_time:151235ms step_avg:101.03ms
step:1498/1770 train_time:151339ms step_avg:101.03ms
step:1499/1770 train_time:151444ms step_avg:101.03ms
step:1500/1770 train_time:151549ms step_avg:101.03ms
step:1500/1770 val_loss:3.3406 train_time:151648ms step_avg:101.10ms
step:1501/1770 train_time:151668ms step_avg:101.04ms
step:1502/1770 train_time:151769ms step_avg:101.04ms
step:1503/1770 train_time:151874ms step_avg:101.05ms
step:1504/1770 train_time:151979ms step_avg:101.05ms
step:1505/1770 train_time:152086ms step_avg:101.05ms
step:1506/1770 train_time:152190ms step_avg:101.06ms
step:1507/1770 train_time:152296ms step_avg:101.06ms
step:1508/1770 train_time:152403ms step_avg:101.06ms
step:1509/1770 train_time:152509ms step_avg:101.07ms
step:1510/1770 train_time:152613ms step_avg:101.07ms
step:1511/1770 train_time:152719ms step_avg:101.07ms
step:1512/1770 train_time:152824ms step_avg:101.07ms
step:1513/1770 train_time:152930ms step_avg:101.08ms
step:1514/1770 train_time:153035ms step_avg:101.08ms
step:1515/1770 train_time:153141ms step_avg:101.08ms
step:1516/1770 train_time:153247ms step_avg:101.09ms
step:1517/1770 train_time:153352ms step_avg:101.09ms
step:1518/1770 train_time:153460ms step_avg:101.09ms
step:1519/1770 train_time:153564ms step_avg:101.10ms
step:1520/1770 train_time:153669ms step_avg:101.10ms
step:1521/1770 train_time:153775ms step_avg:101.10ms
step:1522/1770 train_time:153881ms step_avg:101.10ms
step:1523/1770 train_time:153987ms step_avg:101.11ms
step:1524/1770 train_time:154093ms step_avg:101.11ms
step:1525/1770 train_time:154197ms step_avg:101.11ms
step:1526/1770 train_time:154301ms step_avg:101.11ms
step:1527/1770 train_time:154406ms step_avg:101.12ms
step:1528/1770 train_time:154513ms step_avg:101.12ms
step:1529/1770 train_time:154619ms step_avg:101.12ms
step:1530/1770 train_time:154725ms step_avg:101.13ms
step:1531/1770 train_time:154830ms step_avg:101.13ms
step:1532/1770 train_time:154936ms step_avg:101.13ms
step:1533/1770 train_time:155041ms step_avg:101.14ms
step:1534/1770 train_time:155147ms step_avg:101.14ms
step:1535/1770 train_time:155251ms step_avg:101.14ms
step:1536/1770 train_time:155357ms step_avg:101.14ms
step:1537/1770 train_time:155463ms step_avg:101.15ms
step:1538/1770 train_time:155571ms step_avg:101.15ms
step:1539/1770 train_time:155676ms step_avg:101.15ms
step:1540/1770 train_time:155784ms step_avg:101.16ms
step:1541/1770 train_time:155892ms step_avg:101.16ms
step:1542/1770 train_time:155997ms step_avg:101.17ms
step:1543/1770 train_time:156102ms step_avg:101.17ms
step:1544/1770 train_time:156209ms step_avg:101.17ms
step:1545/1770 train_time:156314ms step_avg:101.17ms
step:1546/1770 train_time:156420ms step_avg:101.18ms
step:1547/1770 train_time:156524ms step_avg:101.18ms
step:1548/1770 train_time:156629ms step_avg:101.18ms
step:1549/1770 train_time:156734ms step_avg:101.18ms
step:1550/1770 train_time:156841ms step_avg:101.19ms
step:1551/1770 train_time:156946ms step_avg:101.19ms
step:1552/1770 train_time:157055ms step_avg:101.20ms
step:1553/1770 train_time:157160ms step_avg:101.20ms
step:1554/1770 train_time:157264ms step_avg:101.20ms
step:1555/1770 train_time:157371ms step_avg:101.20ms
step:1556/1770 train_time:157475ms step_avg:101.21ms
step:1557/1770 train_time:157580ms step_avg:101.21ms
step:1558/1770 train_time:157686ms step_avg:101.21ms
step:1559/1770 train_time:157792ms step_avg:101.21ms
step:1560/1770 train_time:157897ms step_avg:101.22ms
step:1561/1770 train_time:158004ms step_avg:101.22ms
step:1562/1770 train_time:158110ms step_avg:101.22ms
step:1563/1770 train_time:158215ms step_avg:101.23ms
step:1564/1770 train_time:158320ms step_avg:101.23ms
step:1565/1770 train_time:158426ms step_avg:101.23ms
step:1566/1770 train_time:158531ms step_avg:101.23ms
step:1567/1770 train_time:158636ms step_avg:101.24ms
step:1568/1770 train_time:158741ms step_avg:101.24ms
step:1569/1770 train_time:158851ms step_avg:101.24ms
step:1570/1770 train_time:158957ms step_avg:101.25ms
step:1571/1770 train_time:159062ms step_avg:101.25ms
step:1572/1770 train_time:159168ms step_avg:101.25ms
step:1573/1770 train_time:159276ms step_avg:101.26ms
step:1574/1770 train_time:159382ms step_avg:101.26ms
step:1575/1770 train_time:159486ms step_avg:101.26ms
step:1576/1770 train_time:159592ms step_avg:101.26ms
step:1577/1770 train_time:159699ms step_avg:101.27ms
step:1578/1770 train_time:159805ms step_avg:101.27ms
step:1579/1770 train_time:159910ms step_avg:101.27ms
step:1580/1770 train_time:160015ms step_avg:101.28ms
step:1581/1770 train_time:160124ms step_avg:101.28ms
step:1582/1770 train_time:160231ms step_avg:101.28ms
step:1583/1770 train_time:160336ms step_avg:101.29ms
step:1584/1770 train_time:160442ms step_avg:101.29ms
step:1585/1770 train_time:160547ms step_avg:101.29ms
step:1586/1770 train_time:160657ms step_avg:101.30ms
step:1587/1770 train_time:160763ms step_avg:101.30ms
step:1588/1770 train_time:160869ms step_avg:101.30ms
step:1589/1770 train_time:160976ms step_avg:101.31ms
step:1590/1770 train_time:161081ms step_avg:101.31ms
step:1591/1770 train_time:161185ms step_avg:101.31ms
step:1592/1770 train_time:161292ms step_avg:101.31ms
step:1593/1770 train_time:161397ms step_avg:101.32ms
step:1594/1770 train_time:161502ms step_avg:101.32ms
step:1595/1770 train_time:161607ms step_avg:101.32ms
step:1596/1770 train_time:161713ms step_avg:101.32ms
step:1597/1770 train_time:161818ms step_avg:101.33ms
step:1598/1770 train_time:161924ms step_avg:101.33ms
step:1599/1770 train_time:162031ms step_avg:101.33ms
step:1600/1770 train_time:162138ms step_avg:101.34ms
step:1601/1770 train_time:162245ms step_avg:101.34ms
step:1602/1770 train_time:162351ms step_avg:101.34ms
step:1603/1770 train_time:162456ms step_avg:101.34ms
step:1604/1770 train_time:162560ms step_avg:101.35ms
step:1605/1770 train_time:162665ms step_avg:101.35ms
step:1606/1770 train_time:162772ms step_avg:101.35ms
step:1607/1770 train_time:162881ms step_avg:101.36ms
step:1608/1770 train_time:162987ms step_avg:101.36ms
step:1609/1770 train_time:163093ms step_avg:101.36ms
step:1610/1770 train_time:163199ms step_avg:101.37ms
step:1611/1770 train_time:163305ms step_avg:101.37ms
step:1612/1770 train_time:163412ms step_avg:101.37ms
step:1613/1770 train_time:163517ms step_avg:101.37ms
step:1614/1770 train_time:163622ms step_avg:101.38ms
step:1615/1770 train_time:163729ms step_avg:101.38ms
step:1616/1770 train_time:163835ms step_avg:101.38ms
step:1617/1770 train_time:163942ms step_avg:101.39ms
step:1618/1770 train_time:164049ms step_avg:101.39ms
step:1619/1770 train_time:164156ms step_avg:101.39ms
step:1620/1770 train_time:164263ms step_avg:101.40ms
step:1621/1770 train_time:164370ms step_avg:101.40ms
step:1622/1770 train_time:164477ms step_avg:101.40ms
step:1623/1770 train_time:164584ms step_avg:101.41ms
step:1624/1770 train_time:164689ms step_avg:101.41ms
step:1625/1770 train_time:164794ms step_avg:101.41ms
step:1625/1770 val_loss:3.3065 train_time:164895ms step_avg:101.47ms
step:1626/1770 train_time:164917ms step_avg:101.42ms
step:1627/1770 train_time:165014ms step_avg:101.42ms
step:1628/1770 train_time:165119ms step_avg:101.42ms
step:1629/1770 train_time:165224ms step_avg:101.43ms
step:1630/1770 train_time:165329ms step_avg:101.43ms
step:1631/1770 train_time:165434ms step_avg:101.43ms
step:1632/1770 train_time:165539ms step_avg:101.43ms
step:1633/1770 train_time:165645ms step_avg:101.44ms
step:1634/1770 train_time:165749ms step_avg:101.44ms
step:1635/1770 train_time:165855ms step_avg:101.44ms
step:1636/1770 train_time:165961ms step_avg:101.44ms
step:1637/1770 train_time:166068ms step_avg:101.45ms
step:1638/1770 train_time:166173ms step_avg:101.45ms
step:1639/1770 train_time:166279ms step_avg:101.45ms
step:1640/1770 train_time:166384ms step_avg:101.45ms
step:1641/1770 train_time:166490ms step_avg:101.46ms
step:1642/1770 train_time:166594ms step_avg:101.46ms
step:1643/1770 train_time:166700ms step_avg:101.46ms
step:1644/1770 train_time:166807ms step_avg:101.46ms
step:1645/1770 train_time:166912ms step_avg:101.47ms
step:1646/1770 train_time:167019ms step_avg:101.47ms
step:1647/1770 train_time:167125ms step_avg:101.47ms
step:1648/1770 train_time:167229ms step_avg:101.47ms
step:1649/1770 train_time:167335ms step_avg:101.48ms
step:1650/1770 train_time:167440ms step_avg:101.48ms
step:1651/1770 train_time:167545ms step_avg:101.48ms
step:1652/1770 train_time:167650ms step_avg:101.48ms
step:1653/1770 train_time:167755ms step_avg:101.49ms
step:1654/1770 train_time:167863ms step_avg:101.49ms
step:1655/1770 train_time:167971ms step_avg:101.49ms
step:1656/1770 train_time:168076ms step_avg:101.49ms
step:1657/1770 train_time:168184ms step_avg:101.50ms
step:1658/1770 train_time:168289ms step_avg:101.50ms
step:1659/1770 train_time:168395ms step_avg:101.50ms
step:1660/1770 train_time:168501ms step_avg:101.51ms
step:1661/1770 train_time:168606ms step_avg:101.51ms
step:1662/1770 train_time:168711ms step_avg:101.51ms
step:1663/1770 train_time:168816ms step_avg:101.51ms
step:1664/1770 train_time:168922ms step_avg:101.52ms
step:1665/1770 train_time:169027ms step_avg:101.52ms
step:1666/1770 train_time:169133ms step_avg:101.52ms
step:1667/1770 train_time:169238ms step_avg:101.52ms
step:1668/1770 train_time:169343ms step_avg:101.52ms
step:1669/1770 train_time:169448ms step_avg:101.53ms
step:1670/1770 train_time:169552ms step_avg:101.53ms
step:1671/1770 train_time:169659ms step_avg:101.53ms
step:1672/1770 train_time:169764ms step_avg:101.53ms
step:1673/1770 train_time:169870ms step_avg:101.54ms
step:1674/1770 train_time:169976ms step_avg:101.54ms
step:1675/1770 train_time:170081ms step_avg:101.54ms
step:1676/1770 train_time:170188ms step_avg:101.54ms
step:1677/1770 train_time:170296ms step_avg:101.55ms
step:1678/1770 train_time:170401ms step_avg:101.55ms
step:1679/1770 train_time:170506ms step_avg:101.55ms
step:1680/1770 train_time:170612ms step_avg:101.55ms
step:1681/1770 train_time:170717ms step_avg:101.56ms
step:1682/1770 train_time:170825ms step_avg:101.56ms
step:1683/1770 train_time:170930ms step_avg:101.56ms
step:1684/1770 train_time:171035ms step_avg:101.56ms
step:1685/1770 train_time:171141ms step_avg:101.57ms
step:1686/1770 train_time:171247ms step_avg:101.57ms
step:1687/1770 train_time:171355ms step_avg:101.57ms
step:1688/1770 train_time:171460ms step_avg:101.58ms
step:1689/1770 train_time:171566ms step_avg:101.58ms
step:1690/1770 train_time:171671ms step_avg:101.58ms
step:1691/1770 train_time:171776ms step_avg:101.58ms
step:1692/1770 train_time:171882ms step_avg:101.58ms
step:1693/1770 train_time:171987ms step_avg:101.59ms
step:1694/1770 train_time:172092ms step_avg:101.59ms
step:1695/1770 train_time:172199ms step_avg:101.59ms
step:1696/1770 train_time:172307ms step_avg:101.60ms
step:1697/1770 train_time:172415ms step_avg:101.60ms
step:1698/1770 train_time:172520ms step_avg:101.60ms
step:1699/1770 train_time:172626ms step_avg:101.60ms
step:1700/1770 train_time:172730ms step_avg:101.61ms
step:1701/1770 train_time:172836ms step_avg:101.61ms
step:1702/1770 train_time:172942ms step_avg:101.61ms
step:1703/1770 train_time:173047ms step_avg:101.61ms
step:1704/1770 train_time:173153ms step_avg:101.62ms
step:1705/1770 train_time:173259ms step_avg:101.62ms
step:1706/1770 train_time:173364ms step_avg:101.62ms
step:1707/1770 train_time:173470ms step_avg:101.62ms
step:1708/1770 train_time:173576ms step_avg:101.63ms
step:1709/1770 train_time:173683ms step_avg:101.63ms
step:1710/1770 train_time:173793ms step_avg:101.63ms
step:1711/1770 train_time:173901ms step_avg:101.64ms
step:1712/1770 train_time:174008ms step_avg:101.64ms
step:1713/1770 train_time:174113ms step_avg:101.64ms
step:1714/1770 train_time:174219ms step_avg:101.64ms
step:1715/1770 train_time:174325ms step_avg:101.65ms
step:1716/1770 train_time:174432ms step_avg:101.65ms
step:1717/1770 train_time:174538ms step_avg:101.65ms
step:1718/1770 train_time:174645ms step_avg:101.66ms
step:1719/1770 train_time:174751ms step_avg:101.66ms
step:1720/1770 train_time:174859ms step_avg:101.66ms
step:1721/1770 train_time:174965ms step_avg:101.66ms
step:1722/1770 train_time:175075ms step_avg:101.67ms
step:1723/1770 train_time:175183ms step_avg:101.67ms
step:1724/1770 train_time:175291ms step_avg:101.68ms
step:1725/1770 train_time:175401ms step_avg:101.68ms
step:1726/1770 train_time:175508ms step_avg:101.68ms
step:1727/1770 train_time:175615ms step_avg:101.69ms
step:1728/1770 train_time:175723ms step_avg:101.69ms
step:1729/1770 train_time:175830ms step_avg:101.69ms
step:1730/1770 train_time:175936ms step_avg:101.70ms
step:1731/1770 train_time:176045ms step_avg:101.70ms
step:1732/1770 train_time:176150ms step_avg:101.70ms
step:1733/1770 train_time:176259ms step_avg:101.71ms
step:1734/1770 train_time:176364ms step_avg:101.71ms
step:1735/1770 train_time:176471ms step_avg:101.71ms
step:1736/1770 train_time:176577ms step_avg:101.72ms
step:1737/1770 train_time:176683ms step_avg:101.72ms
step:1738/1770 train_time:176790ms step_avg:101.72ms
step:1739/1770 train_time:176896ms step_avg:101.72ms
step:1740/1770 train_time:177002ms step_avg:101.73ms
step:1741/1770 train_time:177111ms step_avg:101.73ms
step:1742/1770 train_time:177221ms step_avg:101.73ms
step:1743/1770 train_time:177328ms step_avg:101.74ms
step:1744/1770 train_time:177433ms step_avg:101.74ms
step:1745/1770 train_time:177540ms step_avg:101.74ms
step:1746/1770 train_time:177649ms step_avg:101.75ms
step:1747/1770 train_time:177754ms step_avg:101.75ms
step:1748/1770 train_time:177862ms step_avg:101.75ms
step:1749/1770 train_time:177970ms step_avg:101.76ms
step:1750/1770 train_time:178075ms step_avg:101.76ms
step:1750/1770 val_loss:3.2798 train_time:178177ms step_avg:101.82ms
step:1751/1770 train_time:178199ms step_avg:101.77ms
step:1752/1770 train_time:178295ms step_avg:101.77ms
step:1753/1770 train_time:178402ms step_avg:101.77ms
step:1754/1770 train_time:178508ms step_avg:101.77ms
step:1755/1770 train_time:178614ms step_avg:101.77ms
step:1756/1770 train_time:178722ms step_avg:101.78ms
step:1757/1770 train_time:178828ms step_avg:101.78ms
step:1758/1770 train_time:178934ms step_avg:101.78ms
step:1759/1770 train_time:179041ms step_avg:101.79ms
step:1760/1770 train_time:179148ms step_avg:101.79ms
step:1761/1770 train_time:179256ms step_avg:101.79ms
step:1762/1770 train_time:179366ms step_avg:101.80ms
step:1763/1770 train_time:179472ms step_avg:101.80ms
step:1764/1770 train_time:179579ms step_avg:101.80ms
step:1765/1770 train_time:179686ms step_avg:101.80ms
step:1766/1770 train_time:179796ms step_avg:101.81ms
step:1767/1770 train_time:179901ms step_avg:101.81ms
step:1768/1770 train_time:180009ms step_avg:101.82ms
step:1769/1770 train_time:180114ms step_avg:101.82ms
step:1770/1770 train_time:180220ms step_avg:101.82ms
step:1770/1770 val_loss:3.2766 train_time:180322ms step_avg:101.88ms
peak memory allocated: 30724 MiB reserved: 45392 MiB
