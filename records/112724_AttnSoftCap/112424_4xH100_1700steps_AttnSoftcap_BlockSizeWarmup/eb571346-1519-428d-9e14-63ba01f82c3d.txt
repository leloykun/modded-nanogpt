====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
from dataclasses import dataclass

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP
# Use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import flex_attention, create_block_mask, BlockMask, _score_mod_signature
from torch._inductor.lowering import make_pointwise, register_lowering
# Some internal torch.compile details
from torch._inductor.virtualized import ops
from functools import partial
flex_attention = torch.compile(flex_attention, dynamic=False)
create_block_mask = torch.compile(create_block_mask, dynamic=False)

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True,
                 backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]

            # generate weight updates in distributed fashion
            total_params = sum(p.numel() for p in group['params'])
            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)
            curr_idx = 0
            for i, p in enumerate(group['params']):
                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs
                if i % int(os.environ['WORLD_SIZE']) == int(os.environ['RANK']):
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.mul_(momentum).add_(g)
                    if group['nesterov']:
                        g = g.add(buf, alpha=momentum)
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    g *= max(1, g.size(0)/g.size(1))**0.5
                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()
                curr_idx += p.numel()

            # sync updates across devices. we are not memory-constrained so can do this simple deserialization
            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)

            # deserialize and apply updates
            curr_idx = 0
            for p in group['params']:
                p.grad = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)
                p.data.add_(p.grad, alpha=-lr)
                curr_idx += p.numel()

# -----------------------------------------------------------------------------
# Attention Tanh softcapping

@torch.library.custom_op("approx::tanh", mutates_args=())
def _tanh_approx(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

@_tanh_approx.register_fake
def _(inp: torch.Tensor) -> torch.Tensor:
    return torch.tanh(inp)

def _tanh_approx_lowering(inp):
    fn = partial(ops.inline_asm_elementwise, asm="tanh.approx.f32 $0, $1;")
    return make_pointwise(fn)(inp)

register_lowering(torch.ops.approx.tanh)(_tanh_approx_lowering)

class _TanhApprox(torch.autograd.Function):
    @staticmethod
    def forward(x):
        return torch.ops.approx.tanh(x)

    @staticmethod
    def setup_context(ctx, inputs, output):
        (x,) = inputs
        result = output
        ctx.save_for_backward(result)

    @staticmethod
    def backward(ctx, grad_output):
        (result,) = ctx.saved_tensors
        return grad_output * (1 - result * result)

    @staticmethod
    def vmap(info, in_dims, x):
        return torch.tanh(x), 0

_tanh_approx = _TanhApprox.apply

def generate_tanh_softcap(soft_cap: int, approx: bool=True) -> _score_mod_signature:
    tanh = _tanh_approx if approx else torch.tanh

    def tanh_softcap(score, b, h, q_idx, kv_idx):
        return soft_cap * tanh(score / soft_cap)

    prefix = "tanh_softcap_approx" if approx else "tanh_softcap"
    tanh_softcap.__name__ = f"{prefix}_{soft_cap}"

    return tanh_softcap

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        self.inv_freq = None
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CastedLinear(nn.Linear):
    def forward(self, x):
        return F.linear(x, self.weight.to(x.dtype))

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_k = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_v = CastedLinear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = CastedLinear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)
        self.lamb = nn.Parameter(torch.tensor(0.5)) # @Grad62304977

    def forward(self, x, v1, block_mask: BlockMask, score_mod: _score_mod_signature):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        if v1 is None:
            v1 = v # This happens if we are in the first block. v needs to be accessed by subsequent blocks
        v = (1 - self.lamb) * v + self.lamb * v1.view_as(v) # @Grad62304977
        cos, sin = self.rotary(q)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), score_mod=score_mod, block_mask=block_mask)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y, v1

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = CastedLinear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = CastedLinear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, v1, x0, block_mask: BlockMask, score_mod: _score_mod_signature):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        x1, v1 = self.attn(F.rms_norm(x, (x.size(-1),)), v1, block_mask, score_mod)
        x = x + x1
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x, v1

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768
    attention_soft_cap : int = 50
    lm_head_soft_cap : int = 30

class GPT(nn.Module):

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.attention_soft_cap = config.attention_soft_cap
        self.lm_head_soft_cap = config.lm_head_soft_cap

        # U-net design by @brendanh0gan
        self.num_encoder_layers = config.n_layer // 2 # Half of the layers for encoder
        self.num_decoder_layers = config.n_layer - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = CastedLinear(config.n_embd, config.vocab_size, bias=False)
        self.lm_head.weight.data.zero_() # @Grad62304977

    def forward(self, idx, target, attn_blocksize):

        docs = (idx == 50256).cumsum(0)
        def document_causal_mask(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            window_mask = q_idx - kv_idx < attn_blocksize
            return causal_mask & document_mask & window_mask

        softcap_mod = generate_tanh_softcap(self.attention_soft_cap, approx=True)  # @leloykun

        S = len(idx)
        block_mask = create_block_mask(document_causal_mask, None, None, S, S, device="cuda", _compile=True)

        # forward the GPT model itself
        x = self.transformer.wte(idx[None]) # token embeddings of shape (b, t, n_embd)
        x = F.rms_norm(x, (x.size(-1),)) # @Grad62304977
        x0 = x
        v1 = None

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x, v1 = self.transformer.h[i](x, v1, x0, block_mask, softcap_mod)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x, v1 = self.transformer.h[self.num_encoder_layers + i](x, v1, x0, block_mask, softcap_mod)

        x = F.rms_norm(x, (x.size(-1),))
        logits = self.lm_head(x)
        logits = self.lm_head_soft_cap * torch.tanh(logits / self.lm_head_soft_cap) # @Grad62304977
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target.view(-1))
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        batch_size = self.B * self.T * self.num_processes
        buf = self.tokens[self.current_position:self.current_position+self.B*self.T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = buf[:-1] # inputs
        y = buf[1:] # targets
        # advance current position and load next shard if necessary
        self.current_position += batch_size
        if self.current_position + batch_size >= len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8 # batch size, in sequences, across all devices
    device_batch_size : int = 1 # batch size, in sequences, per device
    sequence_length : int = 64*1024 # sequence length, in tokens
    num_iterations : int = 1700 # number of iterations to run
    warmup_iters : int = 0
    cooldown_iters : int = 622 # number of iterations of linear warmup/cooldown for triangular or trapezoidal schedule
    block_size_warmup_iters : int = 1600
    block_size_warmup_step : int = 8
    block_size_warmup_max : int = 1792
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
args = Hyperparameters()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
def print0(s, logonly=False):
    if master_process:
        with open(logfile, "a") as f:
            if not logonly:
                print(s)
            f.write(s+'\n')
# log information about the hardware/software environment this is running on
# and print the full `nvidia-smi` to file
print0(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:")
import subprocess
result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
print0(f'{result.stdout}', logonly=True)
print0('='*100, logonly=True)

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size)
print0(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
print0(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
print0('='*100, logonly=True)
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
gpt_config = GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768)
model = GPT(gpt_config)
model = model.cuda().bfloat16()
for m in model.modules():
    if isinstance(m, CastedLinear):
        m.float()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model

# CUDNN attention is ~4ms faster than Flash, but doesn't get selected by default in PyTorch 2.5.1
from torch.backends.cuda import enable_cudnn_sdp, enable_flash_sdp, enable_math_sdp, enable_mem_efficient_sdp
enable_cudnn_sdp(True)
enable_flash_sdp(False)
enable_mem_efficient_sdp(False)
enable_math_sdp(False)

# init the optimizer(s)
optimizer1 = torch.optim.Adam([raw_model.transformer.wte.weight], lr=0.6,   betas=(0.8, 0.95), fused=True)
optimizer2 = torch.optim.Adam([raw_model.lm_head.weight],         lr=0.008, betas=(0.8, 0.95), fused=True)
param_names = [name for name, _ in raw_model.named_parameters()]
params = list(raw_model.transformer.h.parameters())
qk_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and ("c_q" in n or "c_k" in n)]
matrix_params = [p for n, p in zip(param_names, params) if p.ndim == 2 and "c_q" not in n and "c_k" not in n]
scalar_params = [p for p in params if p.ndim < 2] + [raw_model.skip_weights]
optimizer5 = Muon(qk_params, lr=0.08, momentum=0.95)
optimizer3 = Muon(matrix_params, lr=0.05, momentum=0.95)
optimizer4 = torch.optim.Adam(scalar_params, lr=0.04, betas=(0.8, 0.95), fused=True) # note that this learning rate is neither sensitive nor tuned
optimizers = [optimizer1, optimizer2, optimizer3, optimizer4, optimizer5]
# learning rate decay scheduler (linear warmup and cooldown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.cooldown_iters:
        return 1.0
    # 3) linear cooldown
    else:
        decay_ratio = (args.num_iterations - it) / args.cooldown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # Set the attention blocksize for the current step, in chunks of 64
    attn_blocksize = torch.tensor(
        args.block_size_warmup_step
        * (
            1 +
            (min(step/args.block_size_warmup_iters, 1) * (args.block_size_warmup_max - args.block_size_warmup_step))
            // args.block_size_warmup_step
        ),
        dtype=torch.int,
        device='cuda',
    )
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            with torch.no_grad():
                x_val, y_val = val_loader.next_batch()
                val_loss += model(x_val, y_val, attn_blocksize=attn_blocksize)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # log val loss to console and to logfile
        print0(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
        # if master_process:
        #     print("============== Weight norms: ==============")
        #     with open(logfile, "a") as f:
        #         f.write("============== Weight norms: ==============\n")
        #         for name, p in model.named_parameters():
        #             if p.ndim != 2:
        #                 continue
        #             if "transformer.wte" in name:
        #                 l1_to_l2_norm = torch.norm(p.data.float(), p=2, dim=1).mean().item()
        #                 l1_to_rms_norm = (1/p.data.size(1))**0.5 * l1_to_l2_norm
        #                 print(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
        #                 f.write(f"W {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
        #             elif "attn.c_q" in name or "attn.c_k" in name:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #                 for h in range(gpt_config.n_head):
        #                     head_dim = gpt_config.n_embd // gpt_config.n_head
        #                     frobenius_norm = torch.linalg.norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
        #                     spectral_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
        #                     nuclear_norm = torch.linalg.matrix_norm(p.data.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
        #                     print(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                     f.write(f"W {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #             else:
        #                 frobenius_norm = torch.linalg.norm(p.data.float(), ord='fro').item()
        #                 spectral_norm = torch.linalg.matrix_norm(p.data.float(), ord=2).item()
        #                 nuclear_norm = torch.linalg.matrix_norm(p.data.float(), ord="nuc").item()
        #                 print(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
        #                 f.write(f"W {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
        #         f.write("===========================================\n")
        #     print("===========================================")
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        loss = model(x, y, attn_blocksize=attn_blocksize)
        train_loss = loss.detach()
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # momentum warmup for Muon
    frac = min(step/300, 1)
    optimizer3.param_groups[0]['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # if master_process and (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
    #     print("============== Gradient norms: ==============")
    #     with open(logfile, "a") as f:
    #         f.write("============== Gradient norms: ==============\n")
    #         for name, p in model.named_parameters():
    #             if p.ndim != 2:
    #                 continue
    #             if p.grad is None:
    #                 continue
    #             if "transformer.wte" in name:
    #                 l1_to_l2_norm = torch.norm(p.grad.float(), p=2, dim=1).mean().item()
    #                 l1_to_rms_norm = (1/p.grad.size(1))**0.5 * l1_to_l2_norm
    #                 print(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}")
    #                 f.write(f"G {name = } | {l1_to_l2_norm = :.5f} | {l1_to_rms_norm = :.5f}\n")
    #             elif "attn.c_q" in name or "attn.c_k" in name:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #                 for h in range(gpt_config.n_head):
    #                     head_dim = gpt_config.n_embd // gpt_config.n_head
    #                     frobenius_norm = torch.linalg.norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord='fro').item()
    #                     spectral_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord=2).item()
    #                     nuclear_norm = torch.linalg.matrix_norm(p.grad.float()[h*head_dim:(h+1)*head_dim,], ord="nuc").item()
    #                     print(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                     f.write(f"G {name = } | {h = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #             else:
    #                 frobenius_norm = torch.linalg.norm(p.grad.float(), ord='fro').item()
    #                 spectral_norm = torch.linalg.matrix_norm(p.grad.float(), ord=2).item()
    #                 nuclear_norm = torch.linalg.matrix_norm(p.grad.float(), ord="nuc").item()
    #                 print(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}")
    #                 f.write(f"G {name = } | {frobenius_norm = :.5f} | {spectral_norm = :.5f} | {nuclear_norm = :.5f}\n")
    #         f.write("===========================================\n")
    #     print("===========================================")
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    approx_time = training_time_ms + 1000 * (time.time() - t0)
    print0(f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")

# -------------------------------------------------------------------------
# clean up nice
dist.destroy_process_group()
====================================================================================================
Running pytorch 2.6.0.dev20241126+cu124 compiled for CUDA 12.4
nvidia-smi:
Wed Nov 27 08:20:44 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:0A:00.0 Off |                    0 |
| N/A   29C    P0             69W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:0B:00.0 Off |                    0 |
| N/A   27C    P0             86W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:0C:00.0 Off |                    0 |
| N/A   26C    P0             79W /  700W |      22MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:0D:00.0 Off |                    0 |
| N/A   30C    P0             92W /  700W |      23MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
Training DataLoader: total number of tokens: 1100000000 across 11 files
Validation DataLoader: total number of tokens: 100000000 across 1 files
====================================================================================================
step:0/1700 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1700 train_loss:10.8258 train_time:151032ms step_avg:nanms
step:2/1700 train_loss:10.1165 train_time:162438ms step_avg:nanms
step:3/1700 train_loss:8.3503 train_time:162733ms step_avg:nanms
step:4/1700 train_loss:7.6032 train_time:163023ms step_avg:nanms
step:5/1700 train_loss:7.4727 train_time:163317ms step_avg:nanms
step:6/1700 train_loss:7.0191 train_time:163613ms step_avg:nanms
step:7/1700 train_loss:6.9925 train_time:163909ms step_avg:nanms
step:8/1700 train_loss:6.4464 train_time:164201ms step_avg:nanms
step:9/1700 train_loss:6.7544 train_time:164496ms step_avg:nanms
step:10/1700 train_loss:6.5916 train_time:164794ms step_avg:nanms
step:11/1700 train_loss:6.4008 train_time:287ms step_avg:nanms
step:12/1700 train_loss:6.2219 train_time:579ms step_avg:nanms
step:13/1700 train_loss:6.1849 train_time:874ms step_avg:291.34ms
step:14/1700 train_loss:6.1323 train_time:1171ms step_avg:292.69ms
step:15/1700 train_loss:6.1207 train_time:1464ms step_avg:292.76ms
step:16/1700 train_loss:5.9042 train_time:1758ms step_avg:292.95ms
step:17/1700 train_loss:5.8317 train_time:2052ms step_avg:293.09ms
step:18/1700 train_loss:6.4131 train_time:2348ms step_avg:293.48ms
step:19/1700 train_loss:5.8146 train_time:2642ms step_avg:293.58ms
step:20/1700 train_loss:5.9932 train_time:2936ms step_avg:293.59ms
step:21/1700 train_loss:5.9224 train_time:3232ms step_avg:293.78ms
step:22/1700 train_loss:5.6572 train_time:3528ms step_avg:293.97ms
step:23/1700 train_loss:5.7393 train_time:3820ms step_avg:293.83ms
step:24/1700 train_loss:5.7611 train_time:4114ms step_avg:293.85ms
step:25/1700 train_loss:5.5514 train_time:4410ms step_avg:294.00ms
step:26/1700 train_loss:5.6785 train_time:4704ms step_avg:294.01ms
step:27/1700 train_loss:5.6077 train_time:4997ms step_avg:293.95ms
step:28/1700 train_loss:5.5856 train_time:5291ms step_avg:293.96ms
step:29/1700 train_loss:5.6591 train_time:5586ms step_avg:293.98ms
step:30/1700 train_loss:5.6738 train_time:5879ms step_avg:293.95ms
step:31/1700 train_loss:5.9966 train_time:6174ms step_avg:293.99ms
step:32/1700 train_loss:5.4936 train_time:6469ms step_avg:294.05ms
step:33/1700 train_loss:5.3779 train_time:6762ms step_avg:293.99ms
step:34/1700 train_loss:5.3950 train_time:7055ms step_avg:293.95ms
step:35/1700 train_loss:5.5939 train_time:7351ms step_avg:294.06ms
step:36/1700 train_loss:5.5092 train_time:7649ms step_avg:294.18ms
step:37/1700 train_loss:5.5273 train_time:7941ms step_avg:294.13ms
step:38/1700 train_loss:5.3374 train_time:8236ms step_avg:294.14ms
step:39/1700 train_loss:5.4308 train_time:8534ms step_avg:294.26ms
step:40/1700 train_loss:5.2270 train_time:8829ms step_avg:294.31ms
step:41/1700 train_loss:5.3942 train_time:9124ms step_avg:294.33ms
step:42/1700 train_loss:5.2734 train_time:9418ms step_avg:294.30ms
step:43/1700 train_loss:5.2991 train_time:9713ms step_avg:294.34ms
step:44/1700 train_loss:5.1939 train_time:10010ms step_avg:294.41ms
step:45/1700 train_loss:5.0831 train_time:10305ms step_avg:294.42ms
step:46/1700 train_loss:5.1785 train_time:10598ms step_avg:294.39ms
step:47/1700 train_loss:5.1041 train_time:10895ms step_avg:294.45ms
step:48/1700 train_loss:5.2174 train_time:11190ms step_avg:294.47ms
step:49/1700 train_loss:5.0609 train_time:11486ms step_avg:294.51ms
step:50/1700 train_loss:5.1060 train_time:11779ms step_avg:294.48ms
step:51/1700 train_loss:5.1063 train_time:12075ms step_avg:294.51ms
step:52/1700 train_loss:5.2269 train_time:12371ms step_avg:294.56ms
step:53/1700 train_loss:5.0548 train_time:12667ms step_avg:294.59ms
step:54/1700 train_loss:5.1139 train_time:12961ms step_avg:294.57ms
step:55/1700 train_loss:5.0092 train_time:13257ms step_avg:294.60ms
step:56/1700 train_loss:5.0181 train_time:13551ms step_avg:294.59ms
step:57/1700 train_loss:5.0452 train_time:13849ms step_avg:294.66ms
step:58/1700 train_loss:5.0978 train_time:14144ms step_avg:294.67ms
step:59/1700 train_loss:5.1090 train_time:14438ms step_avg:294.65ms
step:60/1700 train_loss:4.9740 train_time:14735ms step_avg:294.71ms
step:61/1700 train_loss:5.1124 train_time:15027ms step_avg:294.65ms
step:62/1700 train_loss:5.0805 train_time:15322ms step_avg:294.65ms
step:63/1700 train_loss:5.0265 train_time:15617ms step_avg:294.66ms
step:64/1700 train_loss:4.9836 train_time:15913ms step_avg:294.69ms
step:65/1700 train_loss:4.8458 train_time:16211ms step_avg:294.74ms
step:66/1700 train_loss:4.8616 train_time:16505ms step_avg:294.73ms
step:67/1700 train_loss:4.9841 train_time:16799ms step_avg:294.73ms
step:68/1700 train_loss:4.9682 train_time:17093ms step_avg:294.71ms
step:69/1700 train_loss:5.0109 train_time:17390ms step_avg:294.75ms
step:70/1700 train_loss:4.8506 train_time:17685ms step_avg:294.75ms
step:71/1700 train_loss:4.9191 train_time:17981ms step_avg:294.76ms
step:72/1700 train_loss:4.9150 train_time:18275ms step_avg:294.76ms
step:73/1700 train_loss:4.8978 train_time:18572ms step_avg:294.80ms
step:74/1700 train_loss:4.7434 train_time:18869ms step_avg:294.82ms
step:75/1700 train_loss:4.7775 train_time:19163ms step_avg:294.81ms
step:76/1700 train_loss:4.6714 train_time:19456ms step_avg:294.79ms
step:77/1700 train_loss:4.8957 train_time:19753ms step_avg:294.82ms
step:78/1700 train_loss:4.8204 train_time:20050ms step_avg:294.85ms
step:79/1700 train_loss:4.5834 train_time:20344ms step_avg:294.85ms
step:80/1700 train_loss:4.8238 train_time:20639ms step_avg:294.85ms
step:81/1700 train_loss:4.7734 train_time:20935ms step_avg:294.85ms
step:82/1700 train_loss:4.8160 train_time:21231ms step_avg:294.87ms
step:83/1700 train_loss:4.7889 train_time:21525ms step_avg:294.86ms
step:84/1700 train_loss:4.6682 train_time:21818ms step_avg:294.84ms
step:85/1700 train_loss:4.6974 train_time:22113ms step_avg:294.85ms
step:86/1700 train_loss:4.7897 train_time:22411ms step_avg:294.88ms
step:87/1700 train_loss:4.7904 train_time:22705ms step_avg:294.87ms
step:88/1700 train_loss:4.6327 train_time:22999ms step_avg:294.87ms
step:89/1700 train_loss:4.6494 train_time:23294ms step_avg:294.86ms
step:90/1700 train_loss:4.5801 train_time:23592ms step_avg:294.90ms
step:91/1700 train_loss:4.7241 train_time:23889ms step_avg:294.93ms
step:92/1700 train_loss:4.6974 train_time:24184ms step_avg:294.93ms
step:93/1700 train_loss:4.8013 train_time:24479ms step_avg:294.93ms
step:94/1700 train_loss:4.9345 train_time:24774ms step_avg:294.93ms
step:95/1700 train_loss:4.6396 train_time:25070ms step_avg:294.94ms
step:96/1700 train_loss:4.5425 train_time:25364ms step_avg:294.93ms
step:97/1700 train_loss:4.7089 train_time:25657ms step_avg:294.91ms
step:98/1700 train_loss:4.5603 train_time:25952ms step_avg:294.91ms
step:99/1700 train_loss:4.5475 train_time:26252ms step_avg:294.96ms
step:100/1700 train_loss:4.5927 train_time:26550ms step_avg:295.00ms
step:101/1700 train_loss:4.4573 train_time:26844ms step_avg:294.99ms
step:102/1700 train_loss:4.6195 train_time:27137ms step_avg:294.97ms
step:103/1700 train_loss:4.5353 train_time:27434ms step_avg:294.99ms
step:104/1700 train_loss:4.6009 train_time:27732ms step_avg:295.02ms
step:105/1700 train_loss:4.6020 train_time:28027ms step_avg:295.02ms
step:106/1700 train_loss:4.7546 train_time:28322ms step_avg:295.02ms
step:107/1700 train_loss:4.5323 train_time:28618ms step_avg:295.03ms
step:108/1700 train_loss:4.4061 train_time:28913ms step_avg:295.03ms
step:109/1700 train_loss:4.7820 train_time:29210ms step_avg:295.05ms
step:110/1700 train_loss:4.5524 train_time:29505ms step_avg:295.05ms
step:111/1700 train_loss:4.4439 train_time:29799ms step_avg:295.04ms
step:112/1700 train_loss:4.6977 train_time:30095ms step_avg:295.05ms
step:113/1700 train_loss:4.3814 train_time:30392ms step_avg:295.07ms
step:114/1700 train_loss:4.5718 train_time:30689ms step_avg:295.09ms
step:115/1700 train_loss:4.5015 train_time:30989ms step_avg:295.14ms
step:116/1700 train_loss:4.5295 train_time:31291ms step_avg:295.20ms
step:117/1700 train_loss:4.3248 train_time:31594ms step_avg:295.27ms
step:118/1700 train_loss:4.5511 train_time:31900ms step_avg:295.37ms
step:119/1700 train_loss:4.3581 train_time:32201ms step_avg:295.42ms
step:120/1700 train_loss:4.4603 train_time:32503ms step_avg:295.48ms
step:121/1700 train_loss:4.4407 train_time:32805ms step_avg:295.54ms
step:122/1700 train_loss:4.3379 train_time:33106ms step_avg:295.59ms
step:123/1700 train_loss:4.4262 train_time:33408ms step_avg:295.65ms
step:124/1700 train_loss:4.2851 train_time:33711ms step_avg:295.71ms
step:125/1700 train_loss:4.2936 train_time:34014ms step_avg:295.78ms
step:125/1700 val_loss:4.4035 train_time:34024ms step_avg:295.86ms
step:126/1700 train_loss:4.2578 train_time:34320ms step_avg:295.86ms
step:127/1700 train_loss:4.4584 train_time:34622ms step_avg:295.91ms
step:128/1700 train_loss:4.4470 train_time:34924ms step_avg:295.97ms
step:129/1700 train_loss:4.4474 train_time:35227ms step_avg:296.02ms
step:130/1700 train_loss:4.3887 train_time:35530ms step_avg:296.08ms
step:131/1700 train_loss:4.5411 train_time:35834ms step_avg:296.15ms
step:132/1700 train_loss:4.3083 train_time:36134ms step_avg:296.18ms
step:133/1700 train_loss:4.2913 train_time:36435ms step_avg:296.22ms
step:134/1700 train_loss:4.4368 train_time:36738ms step_avg:296.27ms
step:135/1700 train_loss:4.2743 train_time:37040ms step_avg:296.32ms
step:136/1700 train_loss:4.2663 train_time:37343ms step_avg:296.37ms
step:137/1700 train_loss:4.3369 train_time:37646ms step_avg:296.43ms
step:138/1700 train_loss:4.3542 train_time:37948ms step_avg:296.47ms
step:139/1700 train_loss:4.4679 train_time:38251ms step_avg:296.52ms
step:140/1700 train_loss:4.3588 train_time:38553ms step_avg:296.56ms
step:141/1700 train_loss:4.2510 train_time:38855ms step_avg:296.60ms
step:142/1700 train_loss:4.3598 train_time:39158ms step_avg:296.65ms
step:143/1700 train_loss:4.4701 train_time:39461ms step_avg:296.70ms
step:144/1700 train_loss:4.5530 train_time:39765ms step_avg:296.75ms
step:145/1700 train_loss:4.3024 train_time:40069ms step_avg:296.80ms
step:146/1700 train_loss:4.3165 train_time:40374ms step_avg:296.87ms
step:147/1700 train_loss:4.3495 train_time:40675ms step_avg:296.90ms
step:148/1700 train_loss:4.1470 train_time:40978ms step_avg:296.94ms
step:149/1700 train_loss:4.3159 train_time:41280ms step_avg:296.98ms
step:150/1700 train_loss:4.2748 train_time:41584ms step_avg:297.03ms
step:151/1700 train_loss:4.2599 train_time:41887ms step_avg:297.07ms
step:152/1700 train_loss:4.1443 train_time:42191ms step_avg:297.12ms
step:153/1700 train_loss:4.3453 train_time:42495ms step_avg:297.17ms
step:154/1700 train_loss:4.1549 train_time:42796ms step_avg:297.19ms
step:155/1700 train_loss:4.1518 train_time:43098ms step_avg:297.23ms
step:156/1700 train_loss:4.2745 train_time:43399ms step_avg:297.25ms
step:157/1700 train_loss:4.3467 train_time:43701ms step_avg:297.29ms
step:158/1700 train_loss:4.2471 train_time:44004ms step_avg:297.32ms
step:159/1700 train_loss:4.2070 train_time:44307ms step_avg:297.36ms
step:160/1700 train_loss:4.1500 train_time:44610ms step_avg:297.40ms
step:161/1700 train_loss:4.2133 train_time:44914ms step_avg:297.44ms
step:162/1700 train_loss:4.2476 train_time:45216ms step_avg:297.47ms
step:163/1700 train_loss:4.2027 train_time:45517ms step_avg:297.49ms
step:164/1700 train_loss:4.1463 train_time:45818ms step_avg:297.52ms
step:165/1700 train_loss:4.2289 train_time:46119ms step_avg:297.54ms
step:166/1700 train_loss:4.3462 train_time:46420ms step_avg:297.56ms
step:167/1700 train_loss:4.2682 train_time:46721ms step_avg:297.59ms
step:168/1700 train_loss:4.1961 train_time:47024ms step_avg:297.62ms
step:169/1700 train_loss:4.2436 train_time:47328ms step_avg:297.66ms
step:170/1700 train_loss:4.2907 train_time:47632ms step_avg:297.70ms
step:171/1700 train_loss:3.7903 train_time:47935ms step_avg:297.73ms
step:172/1700 train_loss:4.1357 train_time:48237ms step_avg:297.76ms
step:173/1700 train_loss:4.1430 train_time:48542ms step_avg:297.80ms
step:174/1700 train_loss:4.3327 train_time:48844ms step_avg:297.83ms
step:175/1700 train_loss:4.1640 train_time:49146ms step_avg:297.85ms
step:176/1700 train_loss:4.2174 train_time:49449ms step_avg:297.88ms
step:177/1700 train_loss:4.3536 train_time:49752ms step_avg:297.91ms
step:178/1700 train_loss:4.2177 train_time:50053ms step_avg:297.94ms
step:179/1700 train_loss:4.1684 train_time:50355ms step_avg:297.96ms
step:180/1700 train_loss:4.2133 train_time:50656ms step_avg:297.98ms
step:181/1700 train_loss:4.1030 train_time:50958ms step_avg:298.00ms
step:182/1700 train_loss:4.1576 train_time:51261ms step_avg:298.03ms
step:183/1700 train_loss:4.1247 train_time:51563ms step_avg:298.05ms
step:184/1700 train_loss:4.2730 train_time:51867ms step_avg:298.09ms
step:185/1700 train_loss:4.1738 train_time:52171ms step_avg:298.12ms
step:186/1700 train_loss:4.2713 train_time:52474ms step_avg:298.15ms
step:187/1700 train_loss:4.1787 train_time:52776ms step_avg:298.17ms
step:188/1700 train_loss:4.1490 train_time:53078ms step_avg:298.19ms
step:189/1700 train_loss:4.0010 train_time:53381ms step_avg:298.22ms
step:190/1700 train_loss:4.1067 train_time:53870ms step_avg:299.28ms
step:191/1700 train_loss:4.0840 train_time:54172ms step_avg:299.29ms
step:192/1700 train_loss:4.0309 train_time:54473ms step_avg:299.30ms
step:193/1700 train_loss:4.2445 train_time:54776ms step_avg:299.32ms
step:194/1700 train_loss:4.1773 train_time:55076ms step_avg:299.33ms
step:195/1700 train_loss:4.3617 train_time:55377ms step_avg:299.34ms
step:196/1700 train_loss:4.1831 train_time:55678ms step_avg:299.35ms
step:197/1700 train_loss:4.0484 train_time:55979ms step_avg:299.35ms
step:198/1700 train_loss:4.1816 train_time:56280ms step_avg:299.36ms
step:199/1700 train_loss:4.0315 train_time:56581ms step_avg:299.37ms
step:200/1700 train_loss:4.1232 train_time:56882ms step_avg:299.38ms
step:201/1700 train_loss:3.9928 train_time:57185ms step_avg:299.40ms
step:202/1700 train_loss:4.2510 train_time:57488ms step_avg:299.42ms
step:203/1700 train_loss:4.0566 train_time:57791ms step_avg:299.44ms
step:204/1700 train_loss:4.1947 train_time:58094ms step_avg:299.45ms
step:205/1700 train_loss:4.2597 train_time:58398ms step_avg:299.48ms
step:206/1700 train_loss:3.9453 train_time:58699ms step_avg:299.48ms
step:207/1700 train_loss:4.0881 train_time:59001ms step_avg:299.50ms
step:208/1700 train_loss:4.0904 train_time:59306ms step_avg:299.52ms
step:209/1700 train_loss:4.2262 train_time:59610ms step_avg:299.55ms
step:210/1700 train_loss:4.1693 train_time:59912ms step_avg:299.56ms
step:211/1700 train_loss:4.0490 train_time:60216ms step_avg:299.58ms
step:212/1700 train_loss:4.1182 train_time:60518ms step_avg:299.59ms
step:213/1700 train_loss:4.0359 train_time:60819ms step_avg:299.60ms
step:214/1700 train_loss:4.1119 train_time:61120ms step_avg:299.61ms
step:215/1700 train_loss:3.9544 train_time:61422ms step_avg:299.62ms
step:216/1700 train_loss:4.0062 train_time:61726ms step_avg:299.64ms
step:217/1700 train_loss:4.0028 train_time:62029ms step_avg:299.66ms
step:218/1700 train_loss:4.0821 train_time:62332ms step_avg:299.67ms
step:219/1700 train_loss:4.0688 train_time:62634ms step_avg:299.69ms
step:220/1700 train_loss:4.0791 train_time:62938ms step_avg:299.70ms
step:221/1700 train_loss:4.0869 train_time:63237ms step_avg:299.70ms
step:222/1700 train_loss:3.9891 train_time:63538ms step_avg:299.71ms
step:223/1700 train_loss:3.9772 train_time:63840ms step_avg:299.72ms
step:224/1700 train_loss:4.2901 train_time:64139ms step_avg:299.71ms
step:225/1700 train_loss:3.9073 train_time:64439ms step_avg:299.71ms
step:226/1700 train_loss:3.9787 train_time:64736ms step_avg:299.71ms
step:227/1700 train_loss:3.9808 train_time:65035ms step_avg:299.70ms
step:228/1700 train_loss:4.1334 train_time:65334ms step_avg:299.70ms
step:229/1700 train_loss:3.9241 train_time:65634ms step_avg:299.70ms
step:230/1700 train_loss:4.0581 train_time:65934ms step_avg:299.70ms
step:231/1700 train_loss:3.9091 train_time:66240ms step_avg:299.73ms
step:232/1700 train_loss:3.9713 train_time:66545ms step_avg:299.75ms
step:233/1700 train_loss:4.0945 train_time:66852ms step_avg:299.79ms
step:234/1700 train_loss:4.0338 train_time:67159ms step_avg:299.82ms
step:235/1700 train_loss:3.9164 train_time:67468ms step_avg:299.86ms
step:236/1700 train_loss:4.0945 train_time:67775ms step_avg:299.89ms
step:237/1700 train_loss:4.0864 train_time:68081ms step_avg:299.92ms
step:238/1700 train_loss:3.9504 train_time:68389ms step_avg:299.95ms
step:239/1700 train_loss:4.0775 train_time:68695ms step_avg:299.98ms
step:240/1700 train_loss:4.1160 train_time:69002ms step_avg:300.01ms
step:241/1700 train_loss:3.9604 train_time:69309ms step_avg:300.04ms
step:242/1700 train_loss:4.1389 train_time:69616ms step_avg:300.07ms
step:243/1700 train_loss:4.0145 train_time:69922ms step_avg:300.10ms
step:244/1700 train_loss:4.0806 train_time:70230ms step_avg:300.13ms
step:245/1700 train_loss:4.1437 train_time:70539ms step_avg:300.17ms
step:246/1700 train_loss:4.0571 train_time:70847ms step_avg:300.20ms
step:247/1700 train_loss:3.9954 train_time:71156ms step_avg:300.23ms
step:248/1700 train_loss:4.1048 train_time:71463ms step_avg:300.26ms
step:249/1700 train_loss:3.9167 train_time:71772ms step_avg:300.30ms
step:250/1700 train_loss:3.9786 train_time:72077ms step_avg:300.32ms
step:250/1700 val_loss:4.0048 train_time:72086ms step_avg:300.36ms
step:251/1700 train_loss:4.0757 train_time:72389ms step_avg:300.37ms
step:252/1700 train_loss:4.1711 train_time:72696ms step_avg:300.40ms
step:253/1700 train_loss:3.9228 train_time:73002ms step_avg:300.42ms
step:254/1700 train_loss:3.8759 train_time:73309ms step_avg:300.45ms
step:255/1700 train_loss:4.0700 train_time:73617ms step_avg:300.48ms
step:256/1700 train_loss:3.9786 train_time:73922ms step_avg:300.50ms
step:257/1700 train_loss:3.9825 train_time:74229ms step_avg:300.52ms
step:258/1700 train_loss:3.9758 train_time:74536ms step_avg:300.55ms
step:259/1700 train_loss:4.0211 train_time:74843ms step_avg:300.57ms
step:260/1700 train_loss:4.0650 train_time:75148ms step_avg:300.59ms
step:261/1700 train_loss:4.0199 train_time:75460ms step_avg:300.64ms
step:262/1700 train_loss:3.9972 train_time:75766ms step_avg:300.66ms
step:263/1700 train_loss:3.8943 train_time:76072ms step_avg:300.68ms
step:264/1700 train_loss:3.9905 train_time:76379ms step_avg:300.70ms
step:265/1700 train_loss:3.8724 train_time:76687ms step_avg:300.74ms
step:266/1700 train_loss:3.9132 train_time:76996ms step_avg:300.76ms
step:267/1700 train_loss:3.9292 train_time:77304ms step_avg:300.79ms
step:268/1700 train_loss:3.9582 train_time:77613ms step_avg:300.82ms
step:269/1700 train_loss:3.8487 train_time:77919ms step_avg:300.84ms
step:270/1700 train_loss:4.0934 train_time:78226ms step_avg:300.87ms
step:271/1700 train_loss:3.9662 train_time:78532ms step_avg:300.89ms
step:272/1700 train_loss:3.9236 train_time:78839ms step_avg:300.91ms
step:273/1700 train_loss:3.9472 train_time:79145ms step_avg:300.93ms
step:274/1700 train_loss:4.0326 train_time:79452ms step_avg:300.96ms
step:275/1700 train_loss:4.0495 train_time:79761ms step_avg:300.98ms
step:276/1700 train_loss:4.2155 train_time:80069ms step_avg:301.01ms
step:277/1700 train_loss:4.0350 train_time:80377ms step_avg:301.04ms
step:278/1700 train_loss:4.0885 train_time:80684ms step_avg:301.06ms
step:279/1700 train_loss:4.0025 train_time:80989ms step_avg:301.07ms
step:280/1700 train_loss:4.1960 train_time:81297ms step_avg:301.10ms
step:281/1700 train_loss:3.9714 train_time:81605ms step_avg:301.13ms
step:282/1700 train_loss:3.9564 train_time:81911ms step_avg:301.14ms
step:283/1700 train_loss:3.9115 train_time:82218ms step_avg:301.16ms
step:284/1700 train_loss:4.0377 train_time:82525ms step_avg:301.19ms
step:285/1700 train_loss:4.0553 train_time:82832ms step_avg:301.21ms
step:286/1700 train_loss:4.0841 train_time:83139ms step_avg:301.23ms
step:287/1700 train_loss:3.9119 train_time:83445ms step_avg:301.25ms
step:288/1700 train_loss:4.0204 train_time:83753ms step_avg:301.27ms
step:289/1700 train_loss:3.8769 train_time:84061ms step_avg:301.29ms
step:290/1700 train_loss:3.8499 train_time:84368ms step_avg:301.31ms
step:291/1700 train_loss:3.9248 train_time:84674ms step_avg:301.33ms
step:292/1700 train_loss:3.8631 train_time:84981ms step_avg:301.35ms
step:293/1700 train_loss:3.9029 train_time:85288ms step_avg:301.37ms
step:294/1700 train_loss:3.9370 train_time:85596ms step_avg:301.40ms
step:295/1700 train_loss:3.8380 train_time:85904ms step_avg:301.42ms
step:296/1700 train_loss:3.8654 train_time:86211ms step_avg:301.44ms
step:297/1700 train_loss:3.8674 train_time:86519ms step_avg:301.46ms
step:298/1700 train_loss:3.9783 train_time:86826ms step_avg:301.48ms
step:299/1700 train_loss:3.8211 train_time:87133ms step_avg:301.50ms
step:300/1700 train_loss:3.9658 train_time:87441ms step_avg:301.52ms
step:301/1700 train_loss:3.9615 train_time:87747ms step_avg:301.54ms
step:302/1700 train_loss:3.9322 train_time:88055ms step_avg:301.56ms
step:303/1700 train_loss:3.9786 train_time:88361ms step_avg:301.57ms
step:304/1700 train_loss:3.9696 train_time:88667ms step_avg:301.59ms
step:305/1700 train_loss:4.4616 train_time:88974ms step_avg:301.61ms
step:306/1700 train_loss:3.9316 train_time:89283ms step_avg:301.63ms
step:307/1700 train_loss:3.8333 train_time:89589ms step_avg:301.65ms
step:308/1700 train_loss:3.9820 train_time:89897ms step_avg:301.67ms
step:309/1700 train_loss:3.8597 train_time:90202ms step_avg:301.68ms
step:310/1700 train_loss:4.0831 train_time:90508ms step_avg:301.69ms
step:311/1700 train_loss:3.9202 train_time:90816ms step_avg:301.71ms
step:312/1700 train_loss:3.8540 train_time:91123ms step_avg:301.73ms
step:313/1700 train_loss:3.9318 train_time:91430ms step_avg:301.75ms
step:314/1700 train_loss:4.0548 train_time:91738ms step_avg:301.77ms
step:315/1700 train_loss:3.9321 train_time:92045ms step_avg:301.79ms
step:316/1700 train_loss:3.7909 train_time:92351ms step_avg:301.80ms
step:317/1700 train_loss:3.8709 train_time:92659ms step_avg:301.82ms
step:318/1700 train_loss:3.9214 train_time:92968ms step_avg:301.84ms
step:319/1700 train_loss:3.8832 train_time:93275ms step_avg:301.86ms
step:320/1700 train_loss:4.0173 train_time:93583ms step_avg:301.88ms
step:321/1700 train_loss:3.9586 train_time:93888ms step_avg:301.89ms
step:322/1700 train_loss:3.9350 train_time:94196ms step_avg:301.91ms
step:323/1700 train_loss:4.0084 train_time:94502ms step_avg:301.92ms
step:324/1700 train_loss:3.9409 train_time:94809ms step_avg:301.94ms
step:325/1700 train_loss:4.0071 train_time:95116ms step_avg:301.96ms
step:326/1700 train_loss:3.8830 train_time:95424ms step_avg:301.97ms
step:327/1700 train_loss:4.3913 train_time:95733ms step_avg:302.00ms
step:328/1700 train_loss:4.0753 train_time:96044ms step_avg:302.02ms
step:329/1700 train_loss:3.7996 train_time:96351ms step_avg:302.04ms
step:330/1700 train_loss:3.7455 train_time:96658ms step_avg:302.06ms
step:331/1700 train_loss:3.9697 train_time:96965ms step_avg:302.07ms
step:332/1700 train_loss:3.9103 train_time:97273ms step_avg:302.09ms
step:333/1700 train_loss:3.8763 train_time:97581ms step_avg:302.11ms
step:334/1700 train_loss:3.8419 train_time:97888ms step_avg:302.12ms
step:335/1700 train_loss:4.0019 train_time:98196ms step_avg:302.14ms
step:336/1700 train_loss:3.9555 train_time:98503ms step_avg:302.16ms
step:337/1700 train_loss:4.4196 train_time:98812ms step_avg:302.18ms
step:338/1700 train_loss:3.9460 train_time:99120ms step_avg:302.19ms
step:339/1700 train_loss:3.8576 train_time:99424ms step_avg:302.20ms
step:340/1700 train_loss:3.9287 train_time:99731ms step_avg:302.22ms
step:341/1700 train_loss:3.8571 train_time:100037ms step_avg:302.23ms
step:342/1700 train_loss:3.8152 train_time:100341ms step_avg:302.23ms
step:343/1700 train_loss:3.8375 train_time:100646ms step_avg:302.24ms
step:344/1700 train_loss:3.9910 train_time:100951ms step_avg:302.25ms
step:345/1700 train_loss:3.8164 train_time:101256ms step_avg:302.26ms
step:346/1700 train_loss:3.7629 train_time:101567ms step_avg:302.28ms
step:347/1700 train_loss:3.8039 train_time:101879ms step_avg:302.31ms
step:348/1700 train_loss:3.8610 train_time:102190ms step_avg:302.34ms
step:349/1700 train_loss:3.8286 train_time:102502ms step_avg:302.37ms
step:350/1700 train_loss:3.5710 train_time:102814ms step_avg:302.39ms
step:351/1700 train_loss:3.8252 train_time:103124ms step_avg:302.42ms
step:352/1700 train_loss:4.1883 train_time:103437ms step_avg:302.45ms
step:353/1700 train_loss:3.6552 train_time:103749ms step_avg:302.47ms
step:354/1700 train_loss:3.9286 train_time:104059ms step_avg:302.50ms
step:355/1700 train_loss:3.7890 train_time:104371ms step_avg:302.52ms
step:356/1700 train_loss:3.8779 train_time:104681ms step_avg:302.55ms
step:357/1700 train_loss:3.7559 train_time:104992ms step_avg:302.57ms
step:358/1700 train_loss:3.8579 train_time:105303ms step_avg:302.59ms
step:359/1700 train_loss:3.8197 train_time:105613ms step_avg:302.62ms
step:360/1700 train_loss:3.4346 train_time:105926ms step_avg:302.65ms
step:361/1700 train_loss:4.0293 train_time:106237ms step_avg:302.67ms
step:362/1700 train_loss:3.9154 train_time:106550ms step_avg:302.70ms
step:363/1700 train_loss:3.8439 train_time:106857ms step_avg:302.71ms
step:364/1700 train_loss:3.7423 train_time:107172ms step_avg:302.74ms
step:365/1700 train_loss:3.9147 train_time:107484ms step_avg:302.77ms
step:366/1700 train_loss:3.8690 train_time:107795ms step_avg:302.80ms
step:367/1700 train_loss:3.8554 train_time:108107ms step_avg:302.82ms
step:368/1700 train_loss:3.8524 train_time:108418ms step_avg:302.84ms
step:369/1700 train_loss:3.7339 train_time:108732ms step_avg:302.88ms
step:370/1700 train_loss:3.8865 train_time:109043ms step_avg:302.90ms
step:371/1700 train_loss:3.7387 train_time:109352ms step_avg:302.91ms
step:372/1700 train_loss:3.6945 train_time:109662ms step_avg:302.93ms
step:373/1700 train_loss:3.9134 train_time:109971ms step_avg:302.95ms
step:374/1700 train_loss:3.8302 train_time:110284ms step_avg:302.98ms
step:375/1700 train_loss:3.7941 train_time:110596ms step_avg:303.00ms
step:375/1700 val_loss:3.8235 train_time:110605ms step_avg:303.03ms
step:376/1700 train_loss:3.8692 train_time:110911ms step_avg:303.04ms
step:377/1700 train_loss:3.7868 train_time:111222ms step_avg:303.06ms
step:378/1700 train_loss:3.8413 train_time:111534ms step_avg:303.08ms
step:379/1700 train_loss:3.8561 train_time:111844ms step_avg:303.10ms
step:380/1700 train_loss:3.9426 train_time:112344ms step_avg:303.63ms
step:381/1700 train_loss:3.6892 train_time:112900ms step_avg:304.31ms
step:382/1700 train_loss:3.7654 train_time:113212ms step_avg:304.33ms
step:383/1700 train_loss:3.7823 train_time:113523ms step_avg:304.35ms
step:384/1700 train_loss:3.8794 train_time:113834ms step_avg:304.37ms
step:385/1700 train_loss:3.6738 train_time:114147ms step_avg:304.39ms
step:386/1700 train_loss:3.8538 train_time:114458ms step_avg:304.41ms
step:387/1700 train_loss:3.7852 train_time:114768ms step_avg:304.42ms
step:388/1700 train_loss:3.9665 train_time:115079ms step_avg:304.44ms
step:389/1700 train_loss:3.7968 train_time:115388ms step_avg:304.45ms
step:390/1700 train_loss:3.8711 train_time:115698ms step_avg:304.47ms
step:391/1700 train_loss:3.7068 train_time:116009ms step_avg:304.49ms
step:392/1700 train_loss:3.7671 train_time:116319ms step_avg:304.50ms
step:393/1700 train_loss:3.8116 train_time:116629ms step_avg:304.51ms
step:394/1700 train_loss:3.7930 train_time:116941ms step_avg:304.53ms
step:395/1700 train_loss:3.8042 train_time:117251ms step_avg:304.55ms
step:396/1700 train_loss:3.7161 train_time:117562ms step_avg:304.57ms
step:397/1700 train_loss:3.5747 train_time:117873ms step_avg:304.58ms
step:398/1700 train_loss:3.8229 train_time:118182ms step_avg:304.59ms
step:399/1700 train_loss:3.7844 train_time:118493ms step_avg:304.61ms
step:400/1700 train_loss:3.7070 train_time:118805ms step_avg:304.63ms
step:401/1700 train_loss:3.8044 train_time:119115ms step_avg:304.64ms
step:402/1700 train_loss:3.6976 train_time:119426ms step_avg:304.66ms
step:403/1700 train_loss:3.9810 train_time:119738ms step_avg:304.68ms
step:404/1700 train_loss:3.8723 train_time:120048ms step_avg:304.69ms
step:405/1700 train_loss:3.8349 train_time:120360ms step_avg:304.71ms
step:406/1700 train_loss:3.8581 train_time:120670ms step_avg:304.72ms
step:407/1700 train_loss:3.8395 train_time:120982ms step_avg:304.74ms
step:408/1700 train_loss:3.7460 train_time:121293ms step_avg:304.76ms
step:409/1700 train_loss:3.8042 train_time:121601ms step_avg:304.76ms
step:410/1700 train_loss:3.7535 train_time:121914ms step_avg:304.78ms
step:411/1700 train_loss:3.7611 train_time:122224ms step_avg:304.80ms
step:412/1700 train_loss:3.7729 train_time:122534ms step_avg:304.81ms
step:413/1700 train_loss:3.7728 train_time:122845ms step_avg:304.83ms
step:414/1700 train_loss:3.8838 train_time:123157ms step_avg:304.84ms
step:415/1700 train_loss:3.7145 train_time:123467ms step_avg:304.86ms
step:416/1700 train_loss:3.7952 train_time:123779ms step_avg:304.87ms
step:417/1700 train_loss:3.8926 train_time:124089ms step_avg:304.89ms
step:418/1700 train_loss:3.6734 train_time:124400ms step_avg:304.90ms
step:419/1700 train_loss:3.9158 train_time:124711ms step_avg:304.92ms
step:420/1700 train_loss:3.9944 train_time:125021ms step_avg:304.93ms
step:421/1700 train_loss:3.7551 train_time:125331ms step_avg:304.94ms
step:422/1700 train_loss:3.8927 train_time:125641ms step_avg:304.95ms
step:423/1700 train_loss:3.5820 train_time:125951ms step_avg:304.97ms
step:424/1700 train_loss:3.7777 train_time:126261ms step_avg:304.98ms
step:425/1700 train_loss:3.6539 train_time:126571ms step_avg:304.99ms
step:426/1700 train_loss:3.8579 train_time:126881ms step_avg:305.00ms
step:427/1700 train_loss:3.8273 train_time:127192ms step_avg:305.02ms
step:428/1700 train_loss:3.7610 train_time:127505ms step_avg:305.04ms
step:429/1700 train_loss:3.8763 train_time:127818ms step_avg:305.05ms
step:430/1700 train_loss:3.6862 train_time:128130ms step_avg:305.07ms
step:431/1700 train_loss:3.6235 train_time:128444ms step_avg:305.09ms
step:432/1700 train_loss:3.8352 train_time:128756ms step_avg:305.11ms
step:433/1700 train_loss:3.8682 train_time:129069ms step_avg:305.13ms
step:434/1700 train_loss:3.8483 train_time:129381ms step_avg:305.14ms
step:435/1700 train_loss:3.7473 train_time:129692ms step_avg:305.16ms
step:436/1700 train_loss:3.8326 train_time:130001ms step_avg:305.17ms
step:437/1700 train_loss:3.8238 train_time:130312ms step_avg:305.18ms
step:438/1700 train_loss:3.7972 train_time:130622ms step_avg:305.19ms
step:439/1700 train_loss:3.8574 train_time:130932ms step_avg:305.20ms
step:440/1700 train_loss:3.6974 train_time:131243ms step_avg:305.22ms
step:441/1700 train_loss:3.8061 train_time:131556ms step_avg:305.23ms
step:442/1700 train_loss:3.7147 train_time:131866ms step_avg:305.25ms
step:443/1700 train_loss:3.6166 train_time:132176ms step_avg:305.26ms
step:444/1700 train_loss:3.7622 train_time:132487ms step_avg:305.27ms
step:445/1700 train_loss:4.0230 train_time:132796ms step_avg:305.28ms
step:446/1700 train_loss:3.6418 train_time:133107ms step_avg:305.29ms
step:447/1700 train_loss:3.8285 train_time:133419ms step_avg:305.31ms
step:448/1700 train_loss:3.8864 train_time:133730ms step_avg:305.32ms
step:449/1700 train_loss:3.7035 train_time:134041ms step_avg:305.33ms
step:450/1700 train_loss:3.6644 train_time:134350ms step_avg:305.34ms
step:451/1700 train_loss:3.7165 train_time:134668ms step_avg:305.37ms
step:452/1700 train_loss:4.0453 train_time:134980ms step_avg:305.38ms
step:453/1700 train_loss:3.9467 train_time:135292ms step_avg:305.40ms
step:454/1700 train_loss:3.8080 train_time:135601ms step_avg:305.41ms
step:455/1700 train_loss:3.7050 train_time:135910ms step_avg:305.42ms
step:456/1700 train_loss:3.8112 train_time:136219ms step_avg:305.42ms
step:457/1700 train_loss:3.7466 train_time:136528ms step_avg:305.43ms
step:458/1700 train_loss:3.7573 train_time:136836ms step_avg:305.44ms
step:459/1700 train_loss:3.8636 train_time:137145ms step_avg:305.44ms
step:460/1700 train_loss:3.6715 train_time:137456ms step_avg:305.46ms
step:461/1700 train_loss:3.7873 train_time:137769ms step_avg:305.47ms
step:462/1700 train_loss:3.7438 train_time:138084ms step_avg:305.50ms
step:463/1700 train_loss:3.5954 train_time:138399ms step_avg:305.52ms
step:464/1700 train_loss:3.7445 train_time:138713ms step_avg:305.54ms
step:465/1700 train_loss:3.8189 train_time:139027ms step_avg:305.55ms
step:466/1700 train_loss:3.7187 train_time:139343ms step_avg:305.58ms
step:467/1700 train_loss:3.7239 train_time:139657ms step_avg:305.60ms
step:468/1700 train_loss:3.7093 train_time:139972ms step_avg:305.62ms
step:469/1700 train_loss:3.9155 train_time:140286ms step_avg:305.63ms
step:470/1700 train_loss:3.7450 train_time:140599ms step_avg:305.65ms
step:471/1700 train_loss:3.6031 train_time:140919ms step_avg:305.68ms
step:472/1700 train_loss:3.8221 train_time:141232ms step_avg:305.70ms
step:473/1700 train_loss:3.6783 train_time:141546ms step_avg:305.71ms
step:474/1700 train_loss:3.8050 train_time:141863ms step_avg:305.74ms
step:475/1700 train_loss:3.8637 train_time:142178ms step_avg:305.76ms
step:476/1700 train_loss:4.0398 train_time:142492ms step_avg:305.78ms
step:477/1700 train_loss:3.8150 train_time:142807ms step_avg:305.80ms
step:478/1700 train_loss:3.7822 train_time:143120ms step_avg:305.81ms
step:479/1700 train_loss:3.7385 train_time:143433ms step_avg:305.83ms
step:480/1700 train_loss:3.6922 train_time:143749ms step_avg:305.85ms
step:481/1700 train_loss:3.7194 train_time:144065ms step_avg:305.87ms
step:482/1700 train_loss:3.8393 train_time:144381ms step_avg:305.89ms
step:483/1700 train_loss:3.7574 train_time:144696ms step_avg:305.91ms
step:484/1700 train_loss:3.8304 train_time:145010ms step_avg:305.93ms
step:485/1700 train_loss:3.7642 train_time:145324ms step_avg:305.95ms
step:486/1700 train_loss:3.6011 train_time:145637ms step_avg:305.96ms
step:487/1700 train_loss:3.7298 train_time:145950ms step_avg:305.98ms
step:488/1700 train_loss:3.7407 train_time:146264ms step_avg:305.99ms
step:489/1700 train_loss:3.7475 train_time:146578ms step_avg:306.01ms
step:490/1700 train_loss:3.9584 train_time:146895ms step_avg:306.03ms
step:491/1700 train_loss:3.6935 train_time:147209ms step_avg:306.05ms
step:492/1700 train_loss:3.6485 train_time:147520ms step_avg:306.06ms
step:493/1700 train_loss:3.7786 train_time:147834ms step_avg:306.07ms
step:494/1700 train_loss:3.5529 train_time:148149ms step_avg:306.09ms
step:495/1700 train_loss:3.7113 train_time:148464ms step_avg:306.11ms
step:496/1700 train_loss:3.8906 train_time:148778ms step_avg:306.13ms
step:497/1700 train_loss:3.7406 train_time:149095ms step_avg:306.15ms
step:498/1700 train_loss:3.7239 train_time:149409ms step_avg:306.17ms
step:499/1700 train_loss:3.6933 train_time:149720ms step_avg:306.18ms
step:500/1700 train_loss:3.8007 train_time:150035ms step_avg:306.19ms
step:500/1700 val_loss:3.7083 train_time:150044ms step_avg:306.21ms
step:501/1700 train_loss:3.6967 train_time:150359ms step_avg:306.23ms
step:502/1700 train_loss:3.6405 train_time:150675ms step_avg:306.25ms
step:503/1700 train_loss:3.7456 train_time:150987ms step_avg:306.26ms
step:504/1700 train_loss:3.6111 train_time:151301ms step_avg:306.28ms
step:505/1700 train_loss:4.0258 train_time:151615ms step_avg:306.29ms
step:506/1700 train_loss:3.6828 train_time:151927ms step_avg:306.31ms
step:507/1700 train_loss:3.7638 train_time:152241ms step_avg:306.32ms
step:508/1700 train_loss:3.9476 train_time:152555ms step_avg:306.33ms
step:509/1700 train_loss:3.6487 train_time:152868ms step_avg:306.35ms
step:510/1700 train_loss:3.7433 train_time:153183ms step_avg:306.37ms
step:511/1700 train_loss:3.7434 train_time:153500ms step_avg:306.39ms
step:512/1700 train_loss:3.5697 train_time:153837ms step_avg:306.45ms
step:513/1700 train_loss:3.5630 train_time:154155ms step_avg:306.47ms
step:514/1700 train_loss:3.7895 train_time:154469ms step_avg:306.49ms
step:515/1700 train_loss:3.9105 train_time:154786ms step_avg:306.51ms
step:516/1700 train_loss:3.7928 train_time:155100ms step_avg:306.52ms
step:517/1700 train_loss:3.6280 train_time:155416ms step_avg:306.54ms
step:518/1700 train_loss:3.7868 train_time:155732ms step_avg:306.56ms
step:519/1700 train_loss:3.5329 train_time:156046ms step_avg:306.57ms
step:520/1700 train_loss:3.7934 train_time:156360ms step_avg:306.59ms
step:521/1700 train_loss:3.6583 train_time:156675ms step_avg:306.60ms
step:522/1700 train_loss:3.5656 train_time:156989ms step_avg:306.62ms
step:523/1700 train_loss:3.8135 train_time:157305ms step_avg:306.64ms
step:524/1700 train_loss:3.6077 train_time:157619ms step_avg:306.65ms
step:525/1700 train_loss:3.6644 train_time:157935ms step_avg:306.67ms
step:526/1700 train_loss:3.7067 train_time:158250ms step_avg:306.69ms
step:527/1700 train_loss:3.9690 train_time:158565ms step_avg:306.70ms
step:528/1700 train_loss:3.6879 train_time:158878ms step_avg:306.71ms
step:529/1700 train_loss:3.6694 train_time:159193ms step_avg:306.73ms
step:530/1700 train_loss:3.6734 train_time:159507ms step_avg:306.74ms
step:531/1700 train_loss:3.7831 train_time:159822ms step_avg:306.76ms
step:532/1700 train_loss:3.7432 train_time:160138ms step_avg:306.78ms
step:533/1700 train_loss:3.7446 train_time:160454ms step_avg:306.80ms
step:534/1700 train_loss:3.8105 train_time:160769ms step_avg:306.81ms
step:535/1700 train_loss:3.7647 train_time:161090ms step_avg:306.84ms
step:536/1700 train_loss:3.6303 train_time:161405ms step_avg:306.85ms
step:537/1700 train_loss:3.6992 train_time:161721ms step_avg:306.87ms
step:538/1700 train_loss:3.6401 train_time:162036ms step_avg:306.89ms
step:539/1700 train_loss:3.6373 train_time:162353ms step_avg:306.91ms
step:540/1700 train_loss:3.7045 train_time:162670ms step_avg:306.92ms
step:541/1700 train_loss:3.6251 train_time:162986ms step_avg:306.94ms
step:542/1700 train_loss:3.6693 train_time:163300ms step_avg:306.95ms
step:543/1700 train_loss:3.7233 train_time:163613ms step_avg:306.97ms
step:544/1700 train_loss:3.6991 train_time:163930ms step_avg:306.98ms
step:545/1700 train_loss:3.7408 train_time:164244ms step_avg:307.00ms
step:546/1700 train_loss:3.7646 train_time:164559ms step_avg:307.01ms
step:547/1700 train_loss:3.6074 train_time:164875ms step_avg:307.03ms
step:548/1700 train_loss:3.8557 train_time:165188ms step_avg:307.04ms
step:549/1700 train_loss:3.2946 train_time:165502ms step_avg:307.05ms
step:550/1700 train_loss:3.7406 train_time:165816ms step_avg:307.07ms
step:551/1700 train_loss:3.7396 train_time:166131ms step_avg:307.08ms
step:552/1700 train_loss:3.6732 train_time:166443ms step_avg:307.09ms
step:553/1700 train_loss:3.7639 train_time:166756ms step_avg:307.10ms
step:554/1700 train_loss:3.6812 train_time:167069ms step_avg:307.11ms
step:555/1700 train_loss:3.6838 train_time:167385ms step_avg:307.13ms
step:556/1700 train_loss:3.7954 train_time:167700ms step_avg:307.14ms
step:557/1700 train_loss:3.6982 train_time:168015ms step_avg:307.16ms
step:558/1700 train_loss:3.6228 train_time:168328ms step_avg:307.17ms
step:559/1700 train_loss:3.7207 train_time:168642ms step_avg:307.18ms
step:560/1700 train_loss:3.6320 train_time:168954ms step_avg:307.19ms
step:561/1700 train_loss:3.6701 train_time:169270ms step_avg:307.20ms
step:562/1700 train_loss:3.6755 train_time:169584ms step_avg:307.22ms
step:563/1700 train_loss:3.4953 train_time:169898ms step_avg:307.23ms
step:564/1700 train_loss:3.7443 train_time:170211ms step_avg:307.24ms
step:565/1700 train_loss:3.6129 train_time:170527ms step_avg:307.26ms
step:566/1700 train_loss:3.6594 train_time:170844ms step_avg:307.27ms
step:567/1700 train_loss:3.7294 train_time:171158ms step_avg:307.28ms
step:568/1700 train_loss:3.6665 train_time:171470ms step_avg:307.29ms
step:569/1700 train_loss:3.9999 train_time:171782ms step_avg:307.30ms
step:570/1700 train_loss:3.7015 train_time:172285ms step_avg:307.65ms
step:571/1700 train_loss:3.6599 train_time:172717ms step_avg:307.87ms
step:572/1700 train_loss:3.7622 train_time:173027ms step_avg:307.88ms
step:573/1700 train_loss:3.7276 train_time:173340ms step_avg:307.89ms
step:574/1700 train_loss:3.7414 train_time:173654ms step_avg:307.90ms
step:575/1700 train_loss:3.7813 train_time:173976ms step_avg:307.92ms
step:576/1700 train_loss:3.7391 train_time:174294ms step_avg:307.94ms
step:577/1700 train_loss:3.7624 train_time:174611ms step_avg:307.96ms
step:578/1700 train_loss:3.6772 train_time:174927ms step_avg:307.97ms
step:579/1700 train_loss:3.6842 train_time:175245ms step_avg:307.99ms
step:580/1700 train_loss:3.6805 train_time:175564ms step_avg:308.01ms
step:581/1700 train_loss:3.6031 train_time:175879ms step_avg:308.02ms
step:582/1700 train_loss:3.6444 train_time:176197ms step_avg:308.04ms
step:583/1700 train_loss:3.8555 train_time:176517ms step_avg:308.06ms
step:584/1700 train_loss:3.6341 train_time:176835ms step_avg:308.07ms
step:585/1700 train_loss:3.5947 train_time:177153ms step_avg:308.09ms
step:586/1700 train_loss:3.7971 train_time:177471ms step_avg:308.11ms
step:587/1700 train_loss:3.5218 train_time:177790ms step_avg:308.13ms
step:588/1700 train_loss:3.6800 train_time:178107ms step_avg:308.14ms
step:589/1700 train_loss:3.6586 train_time:178429ms step_avg:308.17ms
step:590/1700 train_loss:4.0086 train_time:178748ms step_avg:308.19ms
step:591/1700 train_loss:3.7920 train_time:179066ms step_avg:308.20ms
step:592/1700 train_loss:3.5194 train_time:179383ms step_avg:308.22ms
step:593/1700 train_loss:3.5407 train_time:179703ms step_avg:308.24ms
step:594/1700 train_loss:3.5088 train_time:180023ms step_avg:308.26ms
step:595/1700 train_loss:3.5645 train_time:180343ms step_avg:308.28ms
step:596/1700 train_loss:3.9308 train_time:180662ms step_avg:308.30ms
step:597/1700 train_loss:3.6589 train_time:180980ms step_avg:308.31ms
step:598/1700 train_loss:3.5914 train_time:181296ms step_avg:308.33ms
step:599/1700 train_loss:3.6704 train_time:181615ms step_avg:308.34ms
step:600/1700 train_loss:3.4836 train_time:181934ms step_avg:308.36ms
step:601/1700 train_loss:3.6091 train_time:182252ms step_avg:308.38ms
step:602/1700 train_loss:3.6502 train_time:182569ms step_avg:308.39ms
step:603/1700 train_loss:3.6738 train_time:182888ms step_avg:308.41ms
step:604/1700 train_loss:3.7944 train_time:183208ms step_avg:308.43ms
step:605/1700 train_loss:3.6176 train_time:183526ms step_avg:308.45ms
step:606/1700 train_loss:3.6189 train_time:183844ms step_avg:308.46ms
step:607/1700 train_loss:3.5862 train_time:184166ms step_avg:308.49ms
step:608/1700 train_loss:3.8450 train_time:184482ms step_avg:308.50ms
step:609/1700 train_loss:3.6485 train_time:184797ms step_avg:308.51ms
step:610/1700 train_loss:3.6129 train_time:185113ms step_avg:308.52ms
step:611/1700 train_loss:3.7153 train_time:185432ms step_avg:308.54ms
step:612/1700 train_loss:3.6113 train_time:185749ms step_avg:308.55ms
step:613/1700 train_loss:3.5838 train_time:186065ms step_avg:308.57ms
step:614/1700 train_loss:3.7726 train_time:186384ms step_avg:308.58ms
step:615/1700 train_loss:3.7237 train_time:186699ms step_avg:308.59ms
step:616/1700 train_loss:3.7076 train_time:187018ms step_avg:308.61ms
step:617/1700 train_loss:3.6403 train_time:187332ms step_avg:308.62ms
step:618/1700 train_loss:3.5602 train_time:187652ms step_avg:308.64ms
step:619/1700 train_loss:3.6985 train_time:187971ms step_avg:308.66ms
step:620/1700 train_loss:3.5677 train_time:188293ms step_avg:308.68ms
step:621/1700 train_loss:3.5967 train_time:188612ms step_avg:308.69ms
step:622/1700 train_loss:3.9325 train_time:188929ms step_avg:308.71ms
step:623/1700 train_loss:3.5756 train_time:189247ms step_avg:308.72ms
step:624/1700 train_loss:3.6037 train_time:189565ms step_avg:308.74ms
step:625/1700 train_loss:3.7058 train_time:189884ms step_avg:308.75ms
step:625/1700 val_loss:3.6304 train_time:189893ms step_avg:308.77ms
step:626/1700 train_loss:3.7118 train_time:190206ms step_avg:308.78ms
step:627/1700 train_loss:3.7467 train_time:190521ms step_avg:308.79ms
step:628/1700 train_loss:3.7253 train_time:190842ms step_avg:308.81ms
step:629/1700 train_loss:3.7793 train_time:191157ms step_avg:308.82ms
step:630/1700 train_loss:3.6046 train_time:191474ms step_avg:308.83ms
step:631/1700 train_loss:3.7328 train_time:191789ms step_avg:308.84ms
step:632/1700 train_loss:3.7517 train_time:192106ms step_avg:308.85ms
step:633/1700 train_loss:3.6547 train_time:192424ms step_avg:308.87ms
step:634/1700 train_loss:3.6135 train_time:192744ms step_avg:308.88ms
step:635/1700 train_loss:3.7028 train_time:193064ms step_avg:308.90ms
step:636/1700 train_loss:3.9557 train_time:193381ms step_avg:308.92ms
step:637/1700 train_loss:3.5488 train_time:193698ms step_avg:308.93ms
step:638/1700 train_loss:3.3696 train_time:194016ms step_avg:308.94ms
step:639/1700 train_loss:3.6038 train_time:194333ms step_avg:308.96ms
step:640/1700 train_loss:3.6416 train_time:194651ms step_avg:308.97ms
step:641/1700 train_loss:3.5839 train_time:194965ms step_avg:308.98ms
step:642/1700 train_loss:3.5863 train_time:195283ms step_avg:308.99ms
step:643/1700 train_loss:3.6424 train_time:195601ms step_avg:309.01ms
step:644/1700 train_loss:3.6154 train_time:195920ms step_avg:309.02ms
step:645/1700 train_loss:3.5651 train_time:196236ms step_avg:309.03ms
step:646/1700 train_loss:3.7852 train_time:196551ms step_avg:309.04ms
step:647/1700 train_loss:3.6784 train_time:196870ms step_avg:309.06ms
step:648/1700 train_loss:3.6708 train_time:197186ms step_avg:309.07ms
step:649/1700 train_loss:3.7175 train_time:197506ms step_avg:309.09ms
step:650/1700 train_loss:3.7841 train_time:197824ms step_avg:309.10ms
step:651/1700 train_loss:3.6298 train_time:198144ms step_avg:309.12ms
step:652/1700 train_loss:3.7778 train_time:198466ms step_avg:309.14ms
step:653/1700 train_loss:3.5918 train_time:198784ms step_avg:309.15ms
step:654/1700 train_loss:3.6733 train_time:199103ms step_avg:309.17ms
step:655/1700 train_loss:3.4346 train_time:199421ms step_avg:309.18ms
step:656/1700 train_loss:3.5888 train_time:199735ms step_avg:309.19ms
step:657/1700 train_loss:3.5846 train_time:200056ms step_avg:309.21ms
step:658/1700 train_loss:3.5112 train_time:200371ms step_avg:309.21ms
step:659/1700 train_loss:3.6988 train_time:200689ms step_avg:309.23ms
step:660/1700 train_loss:3.5942 train_time:201006ms step_avg:309.24ms
step:661/1700 train_loss:3.6863 train_time:201324ms step_avg:309.25ms
step:662/1700 train_loss:3.7588 train_time:201642ms step_avg:309.27ms
step:663/1700 train_loss:3.6782 train_time:201962ms step_avg:309.28ms
step:664/1700 train_loss:3.5611 train_time:202277ms step_avg:309.29ms
step:665/1700 train_loss:3.6245 train_time:202597ms step_avg:309.31ms
step:666/1700 train_loss:3.5023 train_time:202917ms step_avg:309.32ms
step:667/1700 train_loss:3.7915 train_time:203235ms step_avg:309.34ms
step:668/1700 train_loss:3.6204 train_time:203554ms step_avg:309.35ms
step:669/1700 train_loss:3.6522 train_time:203872ms step_avg:309.37ms
step:670/1700 train_loss:3.4950 train_time:204191ms step_avg:309.38ms
step:671/1700 train_loss:3.6083 train_time:204511ms step_avg:309.40ms
step:672/1700 train_loss:3.5704 train_time:204826ms step_avg:309.41ms
step:673/1700 train_loss:3.5731 train_time:205145ms step_avg:309.42ms
step:674/1700 train_loss:3.8581 train_time:205466ms step_avg:309.44ms
step:675/1700 train_loss:3.6369 train_time:205785ms step_avg:309.45ms
step:676/1700 train_loss:3.7217 train_time:206103ms step_avg:309.46ms
step:677/1700 train_loss:3.4953 train_time:206420ms step_avg:309.47ms
step:678/1700 train_loss:3.6100 train_time:206739ms step_avg:309.49ms
step:679/1700 train_loss:3.5560 train_time:207060ms step_avg:309.51ms
step:680/1700 train_loss:3.6899 train_time:207383ms step_avg:309.53ms
step:681/1700 train_loss:3.5969 train_time:207702ms step_avg:309.54ms
step:682/1700 train_loss:3.6204 train_time:208020ms step_avg:309.55ms
step:683/1700 train_loss:3.6694 train_time:208340ms step_avg:309.57ms
step:684/1700 train_loss:3.7420 train_time:208661ms step_avg:309.59ms
step:685/1700 train_loss:3.6499 train_time:208979ms step_avg:309.60ms
step:686/1700 train_loss:3.7000 train_time:209296ms step_avg:309.61ms
step:687/1700 train_loss:3.6456 train_time:209613ms step_avg:309.62ms
step:688/1700 train_loss:3.6712 train_time:209932ms step_avg:309.63ms
step:689/1700 train_loss:3.2011 train_time:210253ms step_avg:309.65ms
step:690/1700 train_loss:3.4148 train_time:210569ms step_avg:309.66ms
step:691/1700 train_loss:3.5562 train_time:210887ms step_avg:309.67ms
step:692/1700 train_loss:3.4288 train_time:211203ms step_avg:309.68ms
step:693/1700 train_loss:3.6304 train_time:211525ms step_avg:309.70ms
step:694/1700 train_loss:3.6599 train_time:211844ms step_avg:309.71ms
step:695/1700 train_loss:3.5636 train_time:212162ms step_avg:309.72ms
step:696/1700 train_loss:3.5479 train_time:212482ms step_avg:309.74ms
step:697/1700 train_loss:3.8822 train_time:212804ms step_avg:309.76ms
step:698/1700 train_loss:3.6021 train_time:213125ms step_avg:309.77ms
step:699/1700 train_loss:3.6521 train_time:213448ms step_avg:309.79ms
step:700/1700 train_loss:3.7678 train_time:213768ms step_avg:309.81ms
step:701/1700 train_loss:3.5788 train_time:214085ms step_avg:309.82ms
step:702/1700 train_loss:3.5544 train_time:214404ms step_avg:309.83ms
step:703/1700 train_loss:3.5261 train_time:214727ms step_avg:309.85ms
step:704/1700 train_loss:3.5058 train_time:215048ms step_avg:309.87ms
step:705/1700 train_loss:3.5831 train_time:215375ms step_avg:309.89ms
step:706/1700 train_loss:3.5717 train_time:215693ms step_avg:309.90ms
step:707/1700 train_loss:3.5921 train_time:216019ms step_avg:309.93ms
step:708/1700 train_loss:3.6608 train_time:216348ms step_avg:309.95ms
step:709/1700 train_loss:3.6114 train_time:216673ms step_avg:309.98ms
step:710/1700 train_loss:3.5908 train_time:216993ms step_avg:309.99ms
step:711/1700 train_loss:3.5559 train_time:217316ms step_avg:310.01ms
step:712/1700 train_loss:3.6008 train_time:217642ms step_avg:310.03ms
step:713/1700 train_loss:3.6631 train_time:217965ms step_avg:310.05ms
step:714/1700 train_loss:3.6621 train_time:218288ms step_avg:310.07ms
step:715/1700 train_loss:3.5678 train_time:218610ms step_avg:310.08ms
step:716/1700 train_loss:3.5811 train_time:218930ms step_avg:310.10ms
step:717/1700 train_loss:3.5933 train_time:219250ms step_avg:310.11ms
step:718/1700 train_loss:3.7124 train_time:219567ms step_avg:310.12ms
step:719/1700 train_loss:3.6084 train_time:219887ms step_avg:310.14ms
step:720/1700 train_loss:3.6941 train_time:220208ms step_avg:310.15ms
step:721/1700 train_loss:3.8619 train_time:220527ms step_avg:310.16ms
step:722/1700 train_loss:3.4725 train_time:220845ms step_avg:310.18ms
step:723/1700 train_loss:3.7452 train_time:221167ms step_avg:310.19ms
step:724/1700 train_loss:3.7860 train_time:221484ms step_avg:310.20ms
step:725/1700 train_loss:3.5752 train_time:221806ms step_avg:310.22ms
step:726/1700 train_loss:3.6653 train_time:222131ms step_avg:310.24ms
step:727/1700 train_loss:3.5459 train_time:222452ms step_avg:310.25ms
step:728/1700 train_loss:3.5881 train_time:222771ms step_avg:310.27ms
step:729/1700 train_loss:3.7451 train_time:223090ms step_avg:310.28ms
step:730/1700 train_loss:3.6800 train_time:223410ms step_avg:310.29ms
step:731/1700 train_loss:3.6805 train_time:223733ms step_avg:310.31ms
step:732/1700 train_loss:3.5644 train_time:224053ms step_avg:310.32ms
step:733/1700 train_loss:3.6060 train_time:224369ms step_avg:310.33ms
step:734/1700 train_loss:3.8465 train_time:224693ms step_avg:310.35ms
step:735/1700 train_loss:3.5736 train_time:225009ms step_avg:310.36ms
step:736/1700 train_loss:3.6220 train_time:225334ms step_avg:310.38ms
step:737/1700 train_loss:3.7502 train_time:225654ms step_avg:310.39ms
step:738/1700 train_loss:3.6855 train_time:225973ms step_avg:310.40ms
step:739/1700 train_loss:3.6111 train_time:226294ms step_avg:310.42ms
step:740/1700 train_loss:3.5161 train_time:226615ms step_avg:310.43ms
step:741/1700 train_loss:4.1203 train_time:226945ms step_avg:310.46ms
step:742/1700 train_loss:3.5055 train_time:227265ms step_avg:310.47ms
step:743/1700 train_loss:3.5656 train_time:227586ms step_avg:310.49ms
step:744/1700 train_loss:3.5953 train_time:227912ms step_avg:310.51ms
step:745/1700 train_loss:3.6608 train_time:228231ms step_avg:310.52ms
step:746/1700 train_loss:3.6005 train_time:228550ms step_avg:310.53ms
step:747/1700 train_loss:3.6067 train_time:228868ms step_avg:310.54ms
step:748/1700 train_loss:3.6616 train_time:229188ms step_avg:310.55ms
step:749/1700 train_loss:3.5798 train_time:229510ms step_avg:310.57ms
step:750/1700 train_loss:3.5767 train_time:229837ms step_avg:310.59ms
step:750/1700 val_loss:3.5789 train_time:229846ms step_avg:310.60ms
step:751/1700 train_loss:3.6177 train_time:230161ms step_avg:310.61ms
step:752/1700 train_loss:3.5774 train_time:230481ms step_avg:310.62ms
step:753/1700 train_loss:3.6249 train_time:230804ms step_avg:310.64ms
step:754/1700 train_loss:3.6248 train_time:231127ms step_avg:310.65ms
step:755/1700 train_loss:3.6012 train_time:231446ms step_avg:310.67ms
step:756/1700 train_loss:3.6847 train_time:231766ms step_avg:310.68ms
step:757/1700 train_loss:3.4749 train_time:232091ms step_avg:310.70ms
step:758/1700 train_loss:3.7372 train_time:232416ms step_avg:310.72ms
step:759/1700 train_loss:3.6633 train_time:232732ms step_avg:310.72ms
step:760/1700 train_loss:3.6041 train_time:233240ms step_avg:310.99ms
step:761/1700 train_loss:3.7148 train_time:233557ms step_avg:310.99ms
step:762/1700 train_loss:3.6185 train_time:234061ms step_avg:311.25ms
step:763/1700 train_loss:3.4613 train_time:234384ms step_avg:311.27ms
step:764/1700 train_loss:3.4442 train_time:234706ms step_avg:311.28ms
step:765/1700 train_loss:3.5538 train_time:235028ms step_avg:311.30ms
step:766/1700 train_loss:3.5583 train_time:235349ms step_avg:311.31ms
step:767/1700 train_loss:4.5849 train_time:235675ms step_avg:311.33ms
step:768/1700 train_loss:3.5583 train_time:235999ms step_avg:311.34ms
step:769/1700 train_loss:3.6013 train_time:236318ms step_avg:311.35ms
step:770/1700 train_loss:3.6867 train_time:236640ms step_avg:311.37ms
step:771/1700 train_loss:4.1942 train_time:236961ms step_avg:311.38ms
step:772/1700 train_loss:3.6033 train_time:237286ms step_avg:311.40ms
step:773/1700 train_loss:3.6143 train_time:237606ms step_avg:311.41ms
step:774/1700 train_loss:3.5710 train_time:237928ms step_avg:311.42ms
step:775/1700 train_loss:3.7024 train_time:238250ms step_avg:311.44ms
step:776/1700 train_loss:3.5006 train_time:238571ms step_avg:311.45ms
step:777/1700 train_loss:3.6355 train_time:238889ms step_avg:311.46ms
step:778/1700 train_loss:3.6258 train_time:239210ms step_avg:311.47ms
step:779/1700 train_loss:3.5878 train_time:239533ms step_avg:311.49ms
step:780/1700 train_loss:3.5951 train_time:239855ms step_avg:311.50ms
step:781/1700 train_loss:3.4884 train_time:240177ms step_avg:311.51ms
step:782/1700 train_loss:3.6415 train_time:240496ms step_avg:311.52ms
step:783/1700 train_loss:3.5887 train_time:240817ms step_avg:311.54ms
step:784/1700 train_loss:3.5463 train_time:241137ms step_avg:311.55ms
step:785/1700 train_loss:3.5567 train_time:241460ms step_avg:311.56ms
step:786/1700 train_loss:3.5774 train_time:241780ms step_avg:311.57ms
step:787/1700 train_loss:3.5348 train_time:242101ms step_avg:311.58ms
step:788/1700 train_loss:3.5965 train_time:242419ms step_avg:311.59ms
step:789/1700 train_loss:3.5561 train_time:242739ms step_avg:311.60ms
step:790/1700 train_loss:3.4802 train_time:243059ms step_avg:311.61ms
step:791/1700 train_loss:3.5400 train_time:243384ms step_avg:311.63ms
step:792/1700 train_loss:3.6101 train_time:243705ms step_avg:311.64ms
step:793/1700 train_loss:3.6203 train_time:244029ms step_avg:311.66ms
step:794/1700 train_loss:3.6482 train_time:244356ms step_avg:311.68ms
step:795/1700 train_loss:3.5789 train_time:244678ms step_avg:311.69ms
step:796/1700 train_loss:3.7015 train_time:245001ms step_avg:311.71ms
step:797/1700 train_loss:3.5968 train_time:245320ms step_avg:311.72ms
step:798/1700 train_loss:3.4013 train_time:245640ms step_avg:311.73ms
step:799/1700 train_loss:3.4835 train_time:245963ms step_avg:311.74ms
step:800/1700 train_loss:4.2190 train_time:246286ms step_avg:311.75ms
step:801/1700 train_loss:3.7123 train_time:246604ms step_avg:311.76ms
step:802/1700 train_loss:3.5645 train_time:246921ms step_avg:311.77ms
step:803/1700 train_loss:3.6153 train_time:247242ms step_avg:311.78ms
step:804/1700 train_loss:3.5966 train_time:247563ms step_avg:311.79ms
step:805/1700 train_loss:3.5426 train_time:247891ms step_avg:311.81ms
step:806/1700 train_loss:3.5415 train_time:248217ms step_avg:311.83ms
step:807/1700 train_loss:3.5767 train_time:248541ms step_avg:311.85ms
step:808/1700 train_loss:3.6394 train_time:248862ms step_avg:311.86ms
step:809/1700 train_loss:3.8455 train_time:249190ms step_avg:311.88ms
step:810/1700 train_loss:3.6925 train_time:249510ms step_avg:311.89ms
step:811/1700 train_loss:3.5002 train_time:249836ms step_avg:311.91ms
step:812/1700 train_loss:3.6160 train_time:250157ms step_avg:311.92ms
step:813/1700 train_loss:3.6286 train_time:250479ms step_avg:311.93ms
step:814/1700 train_loss:3.5762 train_time:250800ms step_avg:311.94ms
step:815/1700 train_loss:3.4240 train_time:251125ms step_avg:311.96ms
step:816/1700 train_loss:3.7697 train_time:251446ms step_avg:311.97ms
step:817/1700 train_loss:3.5841 train_time:251769ms step_avg:311.98ms
step:818/1700 train_loss:3.5497 train_time:252097ms step_avg:312.00ms
step:819/1700 train_loss:3.5578 train_time:252419ms step_avg:312.01ms
step:820/1700 train_loss:3.5453 train_time:252740ms step_avg:312.02ms
step:821/1700 train_loss:3.4357 train_time:253063ms step_avg:312.04ms
step:822/1700 train_loss:3.5581 train_time:253385ms step_avg:312.05ms
step:823/1700 train_loss:3.6535 train_time:253706ms step_avg:312.06ms
step:824/1700 train_loss:3.3890 train_time:254029ms step_avg:312.07ms
step:825/1700 train_loss:3.5979 train_time:254355ms step_avg:312.09ms
step:826/1700 train_loss:3.6918 train_time:254675ms step_avg:312.10ms
step:827/1700 train_loss:3.4476 train_time:255003ms step_avg:312.12ms
step:828/1700 train_loss:3.5149 train_time:255328ms step_avg:312.14ms
step:829/1700 train_loss:3.5211 train_time:255650ms step_avg:312.15ms
step:830/1700 train_loss:3.6205 train_time:255971ms step_avg:312.16ms
step:831/1700 train_loss:3.4610 train_time:256296ms step_avg:312.18ms
step:832/1700 train_loss:3.5813 train_time:256619ms step_avg:312.19ms
step:833/1700 train_loss:3.6071 train_time:256944ms step_avg:312.20ms
step:834/1700 train_loss:3.6242 train_time:257267ms step_avg:312.22ms
step:835/1700 train_loss:3.4699 train_time:257595ms step_avg:312.24ms
step:836/1700 train_loss:3.6952 train_time:257918ms step_avg:312.25ms
step:837/1700 train_loss:3.4778 train_time:258239ms step_avg:312.26ms
step:838/1700 train_loss:3.3962 train_time:258560ms step_avg:312.27ms
step:839/1700 train_loss:3.6347 train_time:258883ms step_avg:312.28ms
step:840/1700 train_loss:3.5565 train_time:259203ms step_avg:312.29ms
step:841/1700 train_loss:3.6358 train_time:259528ms step_avg:312.31ms
step:842/1700 train_loss:3.5230 train_time:259852ms step_avg:312.32ms
step:843/1700 train_loss:3.5726 train_time:260178ms step_avg:312.34ms
step:844/1700 train_loss:3.5338 train_time:260501ms step_avg:312.35ms
step:845/1700 train_loss:3.5561 train_time:260826ms step_avg:312.37ms
step:846/1700 train_loss:3.5720 train_time:261150ms step_avg:312.38ms
step:847/1700 train_loss:3.6028 train_time:261474ms step_avg:312.39ms
step:848/1700 train_loss:3.5489 train_time:261802ms step_avg:312.41ms
step:849/1700 train_loss:3.3815 train_time:262128ms step_avg:312.43ms
step:850/1700 train_loss:3.5997 train_time:262450ms step_avg:312.44ms
step:851/1700 train_loss:3.4894 train_time:262772ms step_avg:312.45ms
step:852/1700 train_loss:3.5975 train_time:263098ms step_avg:312.47ms
step:853/1700 train_loss:3.3724 train_time:263416ms step_avg:312.47ms
step:854/1700 train_loss:3.6667 train_time:263736ms step_avg:312.48ms
step:855/1700 train_loss:3.6018 train_time:264055ms step_avg:312.49ms
step:856/1700 train_loss:3.3781 train_time:264376ms step_avg:312.50ms
step:857/1700 train_loss:3.6748 train_time:264705ms step_avg:312.52ms
step:858/1700 train_loss:3.6670 train_time:265032ms step_avg:312.54ms
step:859/1700 train_loss:3.3853 train_time:265357ms step_avg:312.55ms
step:860/1700 train_loss:3.5625 train_time:265681ms step_avg:312.57ms
step:861/1700 train_loss:3.6208 train_time:266005ms step_avg:312.58ms
step:862/1700 train_loss:3.4359 train_time:266328ms step_avg:312.59ms
step:863/1700 train_loss:3.5148 train_time:266654ms step_avg:312.61ms
step:864/1700 train_loss:3.8079 train_time:266978ms step_avg:312.62ms
step:865/1700 train_loss:3.7499 train_time:267311ms step_avg:312.64ms
step:866/1700 train_loss:3.5715 train_time:267633ms step_avg:312.66ms
step:867/1700 train_loss:3.5202 train_time:267951ms step_avg:312.66ms
step:868/1700 train_loss:3.7129 train_time:268276ms step_avg:312.68ms
step:869/1700 train_loss:3.4525 train_time:268603ms step_avg:312.69ms
step:870/1700 train_loss:3.4047 train_time:268930ms step_avg:312.71ms
step:871/1700 train_loss:3.5855 train_time:269251ms step_avg:312.72ms
step:872/1700 train_loss:3.5265 train_time:269576ms step_avg:312.73ms
step:873/1700 train_loss:3.4764 train_time:269896ms step_avg:312.74ms
step:874/1700 train_loss:3.6229 train_time:270216ms step_avg:312.75ms
step:875/1700 train_loss:3.5289 train_time:270541ms step_avg:312.76ms
step:875/1700 val_loss:3.5337 train_time:270550ms step_avg:312.78ms
step:876/1700 train_loss:3.6318 train_time:270871ms step_avg:312.78ms
step:877/1700 train_loss:3.4320 train_time:271193ms step_avg:312.80ms
step:878/1700 train_loss:3.6365 train_time:271516ms step_avg:312.81ms
step:879/1700 train_loss:3.5064 train_time:271838ms step_avg:312.82ms
step:880/1700 train_loss:3.8607 train_time:272162ms step_avg:312.83ms
step:881/1700 train_loss:3.5790 train_time:272485ms step_avg:312.84ms
step:882/1700 train_loss:3.3945 train_time:272809ms step_avg:312.85ms
step:883/1700 train_loss:3.7203 train_time:273132ms step_avg:312.87ms
step:884/1700 train_loss:3.4280 train_time:273452ms step_avg:312.87ms
step:885/1700 train_loss:3.6804 train_time:273773ms step_avg:312.88ms
step:886/1700 train_loss:3.5471 train_time:274101ms step_avg:312.90ms
step:887/1700 train_loss:3.6106 train_time:274428ms step_avg:312.92ms
step:888/1700 train_loss:3.5909 train_time:274750ms step_avg:312.93ms
step:889/1700 train_loss:3.6190 train_time:275072ms step_avg:312.94ms
step:890/1700 train_loss:3.5889 train_time:275403ms step_avg:312.96ms
step:891/1700 train_loss:3.4259 train_time:275726ms step_avg:312.97ms
step:892/1700 train_loss:3.5989 train_time:276048ms step_avg:312.98ms
step:893/1700 train_loss:3.5180 train_time:276370ms step_avg:312.99ms
step:894/1700 train_loss:3.5722 train_time:276690ms step_avg:313.00ms
step:895/1700 train_loss:3.4033 train_time:277011ms step_avg:313.01ms
step:896/1700 train_loss:3.3201 train_time:277341ms step_avg:313.03ms
step:897/1700 train_loss:3.4812 train_time:277670ms step_avg:313.04ms
step:898/1700 train_loss:3.6587 train_time:277997ms step_avg:313.06ms
step:899/1700 train_loss:3.5292 train_time:278320ms step_avg:313.07ms
step:900/1700 train_loss:3.5839 train_time:278645ms step_avg:313.08ms
step:901/1700 train_loss:3.7411 train_time:278966ms step_avg:313.09ms
step:902/1700 train_loss:3.5190 train_time:279288ms step_avg:313.10ms
step:903/1700 train_loss:3.4595 train_time:279611ms step_avg:313.11ms
step:904/1700 train_loss:3.6809 train_time:279941ms step_avg:313.13ms
step:905/1700 train_loss:3.6290 train_time:280261ms step_avg:313.14ms
step:906/1700 train_loss:3.4791 train_time:280583ms step_avg:313.15ms
step:907/1700 train_loss:3.4828 train_time:280906ms step_avg:313.16ms
step:908/1700 train_loss:3.7735 train_time:281234ms step_avg:313.18ms
step:909/1700 train_loss:3.4973 train_time:281556ms step_avg:313.19ms
step:910/1700 train_loss:3.6782 train_time:281880ms step_avg:313.20ms
step:911/1700 train_loss:3.8714 train_time:282210ms step_avg:313.22ms
step:912/1700 train_loss:3.3111 train_time:282528ms step_avg:313.22ms
step:913/1700 train_loss:3.6506 train_time:282847ms step_avg:313.23ms
step:914/1700 train_loss:3.5064 train_time:283172ms step_avg:313.24ms
step:915/1700 train_loss:3.5810 train_time:283502ms step_avg:313.26ms
step:916/1700 train_loss:3.7267 train_time:283831ms step_avg:313.28ms
step:917/1700 train_loss:3.4934 train_time:284150ms step_avg:313.29ms
step:918/1700 train_loss:3.4822 train_time:284480ms step_avg:313.30ms
step:919/1700 train_loss:3.5625 train_time:284809ms step_avg:313.32ms
step:920/1700 train_loss:3.4544 train_time:285138ms step_avg:313.34ms
step:921/1700 train_loss:3.5120 train_time:285467ms step_avg:313.36ms
step:922/1700 train_loss:3.4747 train_time:285788ms step_avg:313.36ms
step:923/1700 train_loss:3.6327 train_time:286115ms step_avg:313.38ms
step:924/1700 train_loss:3.4856 train_time:286437ms step_avg:313.39ms
step:925/1700 train_loss:3.4886 train_time:286761ms step_avg:313.40ms
step:926/1700 train_loss:3.6193 train_time:287111ms step_avg:313.44ms
step:927/1700 train_loss:3.4706 train_time:287437ms step_avg:313.45ms
step:928/1700 train_loss:3.6687 train_time:287766ms step_avg:313.47ms
step:929/1700 train_loss:3.5389 train_time:288089ms step_avg:313.48ms
step:930/1700 train_loss:3.3897 train_time:288417ms step_avg:313.50ms
step:931/1700 train_loss:3.6987 train_time:288740ms step_avg:313.51ms
step:932/1700 train_loss:3.3942 train_time:289063ms step_avg:313.52ms
step:933/1700 train_loss:3.3526 train_time:289391ms step_avg:313.53ms
step:934/1700 train_loss:3.5973 train_time:289719ms step_avg:313.55ms
step:935/1700 train_loss:3.5534 train_time:290047ms step_avg:313.56ms
step:936/1700 train_loss:3.3958 train_time:290388ms step_avg:313.59ms
step:937/1700 train_loss:3.3539 train_time:290715ms step_avg:313.61ms
step:938/1700 train_loss:3.5200 train_time:291039ms step_avg:313.62ms
step:939/1700 train_loss:3.3100 train_time:291368ms step_avg:313.64ms
step:940/1700 train_loss:3.5933 train_time:291692ms step_avg:313.65ms
step:941/1700 train_loss:3.4351 train_time:292024ms step_avg:313.67ms
step:942/1700 train_loss:3.4366 train_time:292353ms step_avg:313.68ms
step:943/1700 train_loss:3.5671 train_time:292688ms step_avg:313.71ms
step:944/1700 train_loss:3.4533 train_time:293014ms step_avg:313.72ms
step:945/1700 train_loss:3.4606 train_time:293342ms step_avg:313.73ms
step:946/1700 train_loss:3.6255 train_time:293670ms step_avg:313.75ms
step:947/1700 train_loss:3.5361 train_time:293996ms step_avg:313.76ms
step:948/1700 train_loss:3.6086 train_time:294327ms step_avg:313.78ms
step:949/1700 train_loss:3.7655 train_time:294653ms step_avg:313.79ms
step:950/1700 train_loss:3.3982 train_time:295170ms step_avg:314.01ms
step:951/1700 train_loss:3.4655 train_time:295495ms step_avg:314.02ms
step:952/1700 train_loss:3.7193 train_time:295936ms step_avg:314.16ms
step:953/1700 train_loss:3.4254 train_time:296291ms step_avg:314.20ms
step:954/1700 train_loss:3.4866 train_time:296619ms step_avg:314.22ms
step:955/1700 train_loss:3.5820 train_time:296954ms step_avg:314.24ms
step:956/1700 train_loss:3.4619 train_time:297282ms step_avg:314.25ms
step:957/1700 train_loss:3.4931 train_time:297607ms step_avg:314.26ms
step:958/1700 train_loss:3.4602 train_time:297943ms step_avg:314.29ms
step:959/1700 train_loss:3.5130 train_time:298273ms step_avg:314.30ms
step:960/1700 train_loss:3.5194 train_time:298600ms step_avg:314.32ms
step:961/1700 train_loss:3.5314 train_time:298926ms step_avg:314.33ms
step:962/1700 train_loss:3.4170 train_time:299260ms step_avg:314.35ms
step:963/1700 train_loss:3.6651 train_time:299587ms step_avg:314.36ms
step:964/1700 train_loss:3.6257 train_time:299911ms step_avg:314.37ms
step:965/1700 train_loss:3.6101 train_time:300235ms step_avg:314.38ms
step:966/1700 train_loss:3.4433 train_time:300565ms step_avg:314.40ms
step:967/1700 train_loss:3.4986 train_time:300888ms step_avg:314.41ms
step:968/1700 train_loss:3.7316 train_time:301215ms step_avg:314.42ms
step:969/1700 train_loss:3.5398 train_time:301541ms step_avg:314.43ms
step:970/1700 train_loss:3.5422 train_time:301866ms step_avg:314.44ms
step:971/1700 train_loss:3.6057 train_time:302194ms step_avg:314.46ms
step:972/1700 train_loss:3.3922 train_time:302521ms step_avg:314.47ms
step:973/1700 train_loss:3.5514 train_time:302845ms step_avg:314.48ms
step:974/1700 train_loss:3.4995 train_time:303171ms step_avg:314.49ms
step:975/1700 train_loss:3.5638 train_time:303496ms step_avg:314.50ms
step:976/1700 train_loss:3.6051 train_time:303820ms step_avg:314.51ms
step:977/1700 train_loss:3.4897 train_time:304146ms step_avg:314.53ms
step:978/1700 train_loss:3.6935 train_time:304470ms step_avg:314.54ms
step:979/1700 train_loss:3.5930 train_time:304794ms step_avg:314.54ms
step:980/1700 train_loss:3.3812 train_time:305121ms step_avg:314.56ms
step:981/1700 train_loss:3.6513 train_time:305445ms step_avg:314.57ms
step:982/1700 train_loss:3.4381 train_time:305769ms step_avg:314.58ms
step:983/1700 train_loss:3.6007 train_time:306092ms step_avg:314.59ms
step:984/1700 train_loss:3.5666 train_time:306420ms step_avg:314.60ms
step:985/1700 train_loss:3.5459 train_time:306757ms step_avg:314.62ms
step:986/1700 train_loss:3.5217 train_time:307083ms step_avg:314.63ms
step:987/1700 train_loss:3.6027 train_time:307412ms step_avg:314.65ms
step:988/1700 train_loss:3.4434 train_time:307736ms step_avg:314.66ms
step:989/1700 train_loss:3.5141 train_time:308063ms step_avg:314.67ms
step:990/1700 train_loss:3.5053 train_time:308386ms step_avg:314.68ms
step:991/1700 train_loss:3.4390 train_time:308718ms step_avg:314.70ms
step:992/1700 train_loss:3.6885 train_time:309050ms step_avg:314.71ms
step:993/1700 train_loss:3.4954 train_time:309373ms step_avg:314.72ms
step:994/1700 train_loss:3.4728 train_time:309697ms step_avg:314.73ms
step:995/1700 train_loss:3.5329 train_time:310032ms step_avg:314.75ms
step:996/1700 train_loss:3.6205 train_time:310354ms step_avg:314.76ms
step:997/1700 train_loss:3.5565 train_time:310675ms step_avg:314.77ms
step:998/1700 train_loss:3.4843 train_time:310999ms step_avg:314.78ms
step:999/1700 train_loss:3.7967 train_time:311324ms step_avg:314.79ms
step:1000/1700 train_loss:3.4695 train_time:311652ms step_avg:314.80ms
step:1000/1700 val_loss:3.4955 train_time:311661ms step_avg:314.81ms
step:1001/1700 train_loss:3.6154 train_time:311980ms step_avg:314.81ms
step:1002/1700 train_loss:3.4672 train_time:312302ms step_avg:314.82ms
step:1003/1700 train_loss:3.5317 train_time:312626ms step_avg:314.83ms
step:1004/1700 train_loss:3.4035 train_time:312957ms step_avg:314.85ms
step:1005/1700 train_loss:3.5884 train_time:313286ms step_avg:314.86ms
step:1006/1700 train_loss:3.6324 train_time:313616ms step_avg:314.88ms
step:1007/1700 train_loss:3.4242 train_time:313942ms step_avg:314.89ms
step:1008/1700 train_loss:3.4950 train_time:314269ms step_avg:314.90ms
step:1009/1700 train_loss:3.4703 train_time:314596ms step_avg:314.91ms
step:1010/1700 train_loss:3.5922 train_time:314927ms step_avg:314.93ms
step:1011/1700 train_loss:3.6896 train_time:315261ms step_avg:314.95ms
step:1012/1700 train_loss:3.5870 train_time:315589ms step_avg:314.96ms
step:1013/1700 train_loss:3.5648 train_time:315913ms step_avg:314.97ms
step:1014/1700 train_loss:3.4272 train_time:316243ms step_avg:314.98ms
step:1015/1700 train_loss:3.5686 train_time:316562ms step_avg:314.99ms
step:1016/1700 train_loss:3.6637 train_time:316885ms step_avg:314.99ms
step:1017/1700 train_loss:3.3542 train_time:317215ms step_avg:315.01ms
step:1018/1700 train_loss:3.4445 train_time:317547ms step_avg:315.03ms
step:1019/1700 train_loss:3.4304 train_time:317877ms step_avg:315.04ms
step:1020/1700 train_loss:3.4240 train_time:318201ms step_avg:315.05ms
step:1021/1700 train_loss:3.5601 train_time:318525ms step_avg:315.06ms
step:1022/1700 train_loss:3.4221 train_time:318854ms step_avg:315.07ms
step:1023/1700 train_loss:3.3824 train_time:319180ms step_avg:315.08ms
step:1024/1700 train_loss:3.5118 train_time:319506ms step_avg:315.09ms
step:1025/1700 train_loss:3.5350 train_time:319840ms step_avg:315.11ms
step:1026/1700 train_loss:3.5066 train_time:320170ms step_avg:315.13ms
step:1027/1700 train_loss:3.5151 train_time:320498ms step_avg:315.14ms
step:1028/1700 train_loss:3.6587 train_time:320822ms step_avg:315.15ms
step:1029/1700 train_loss:3.3513 train_time:321153ms step_avg:315.16ms
step:1030/1700 train_loss:3.4280 train_time:321484ms step_avg:315.18ms
step:1031/1700 train_loss:3.3496 train_time:321812ms step_avg:315.19ms
step:1032/1700 train_loss:3.5632 train_time:322134ms step_avg:315.20ms
step:1033/1700 train_loss:3.5483 train_time:322460ms step_avg:315.21ms
step:1034/1700 train_loss:3.7367 train_time:322789ms step_avg:315.22ms
step:1035/1700 train_loss:3.5291 train_time:323116ms step_avg:315.24ms
step:1036/1700 train_loss:3.4405 train_time:323452ms step_avg:315.26ms
step:1037/1700 train_loss:3.4775 train_time:323782ms step_avg:315.27ms
step:1038/1700 train_loss:3.5215 train_time:324111ms step_avg:315.28ms
step:1039/1700 train_loss:3.8296 train_time:324437ms step_avg:315.29ms
step:1040/1700 train_loss:3.6580 train_time:324764ms step_avg:315.30ms
step:1041/1700 train_loss:3.5480 train_time:325099ms step_avg:315.32ms
step:1042/1700 train_loss:3.4465 train_time:325427ms step_avg:315.34ms
step:1043/1700 train_loss:3.5226 train_time:325762ms step_avg:315.36ms
step:1044/1700 train_loss:3.5625 train_time:326097ms step_avg:315.37ms
step:1045/1700 train_loss:3.4781 train_time:326421ms step_avg:315.38ms
step:1046/1700 train_loss:3.4985 train_time:326745ms step_avg:315.39ms
step:1047/1700 train_loss:3.5563 train_time:327078ms step_avg:315.41ms
step:1048/1700 train_loss:3.4601 train_time:327405ms step_avg:315.42ms
step:1049/1700 train_loss:3.6778 train_time:327733ms step_avg:315.43ms
step:1050/1700 train_loss:3.5440 train_time:328066ms step_avg:315.45ms
step:1051/1700 train_loss:3.4410 train_time:328396ms step_avg:315.46ms
step:1052/1700 train_loss:3.4338 train_time:328723ms step_avg:315.47ms
step:1053/1700 train_loss:3.5380 train_time:329052ms step_avg:315.49ms
step:1054/1700 train_loss:3.3953 train_time:329377ms step_avg:315.50ms
step:1055/1700 train_loss:3.7441 train_time:329699ms step_avg:315.50ms
step:1056/1700 train_loss:3.5806 train_time:330030ms step_avg:315.52ms
step:1057/1700 train_loss:3.4208 train_time:330359ms step_avg:315.53ms
step:1058/1700 train_loss:3.5437 train_time:330690ms step_avg:315.54ms
step:1059/1700 train_loss:3.6258 train_time:331016ms step_avg:315.55ms
step:1060/1700 train_loss:3.3484 train_time:331347ms step_avg:315.57ms
step:1061/1700 train_loss:3.4031 train_time:331683ms step_avg:315.59ms
step:1062/1700 train_loss:3.4804 train_time:332007ms step_avg:315.60ms
step:1063/1700 train_loss:3.4569 train_time:332338ms step_avg:315.61ms
step:1064/1700 train_loss:3.4256 train_time:332665ms step_avg:315.62ms
step:1065/1700 train_loss:3.5087 train_time:332992ms step_avg:315.63ms
step:1066/1700 train_loss:3.4269 train_time:333318ms step_avg:315.64ms
step:1067/1700 train_loss:3.4023 train_time:333646ms step_avg:315.65ms
step:1068/1700 train_loss:3.4547 train_time:333974ms step_avg:315.67ms
step:1069/1700 train_loss:3.3261 train_time:334304ms step_avg:315.68ms
step:1070/1700 train_loss:3.4811 train_time:334629ms step_avg:315.69ms
step:1071/1700 train_loss:3.3581 train_time:334962ms step_avg:315.70ms
step:1072/1700 train_loss:3.6104 train_time:335285ms step_avg:315.71ms
step:1073/1700 train_loss:3.5553 train_time:335621ms step_avg:315.73ms
step:1074/1700 train_loss:3.4838 train_time:335946ms step_avg:315.74ms
step:1075/1700 train_loss:3.5662 train_time:336273ms step_avg:315.75ms
step:1076/1700 train_loss:3.4797 train_time:336604ms step_avg:315.76ms
step:1077/1700 train_loss:3.4423 train_time:336929ms step_avg:315.77ms
step:1078/1700 train_loss:3.8417 train_time:337253ms step_avg:315.78ms
step:1079/1700 train_loss:3.4803 train_time:337581ms step_avg:315.79ms
step:1080/1700 train_loss:3.1180 train_time:337922ms step_avg:315.81ms
step:1081/1700 train_loss:3.5810 train_time:338245ms step_avg:315.82ms
step:1082/1700 train_loss:3.4756 train_time:338576ms step_avg:315.84ms
step:1083/1700 train_loss:3.5523 train_time:338912ms step_avg:315.85ms
step:1084/1700 train_loss:3.6421 train_time:339240ms step_avg:315.87ms
step:1085/1700 train_loss:3.5458 train_time:339566ms step_avg:315.88ms
step:1086/1700 train_loss:3.5202 train_time:339896ms step_avg:315.89ms
step:1087/1700 train_loss:3.4784 train_time:340223ms step_avg:315.90ms
step:1088/1700 train_loss:3.6726 train_time:340557ms step_avg:315.92ms
step:1089/1700 train_loss:3.5596 train_time:340891ms step_avg:315.93ms
step:1090/1700 train_loss:3.4097 train_time:341216ms step_avg:315.94ms
step:1091/1700 train_loss:3.4229 train_time:341547ms step_avg:315.95ms
step:1092/1700 train_loss:3.5330 train_time:341877ms step_avg:315.97ms
step:1093/1700 train_loss:3.3281 train_time:342203ms step_avg:315.98ms
step:1094/1700 train_loss:3.5418 train_time:342530ms step_avg:315.99ms
step:1095/1700 train_loss:3.6553 train_time:342857ms step_avg:316.00ms
step:1096/1700 train_loss:3.4913 train_time:343190ms step_avg:316.01ms
step:1097/1700 train_loss:3.4609 train_time:343514ms step_avg:316.02ms
step:1098/1700 train_loss:3.4771 train_time:343844ms step_avg:316.03ms
step:1099/1700 train_loss:3.5365 train_time:344168ms step_avg:316.04ms
step:1100/1700 train_loss:3.6090 train_time:344501ms step_avg:316.06ms
step:1101/1700 train_loss:3.5745 train_time:344831ms step_avg:316.07ms
step:1102/1700 train_loss:3.4880 train_time:345165ms step_avg:316.08ms
step:1103/1700 train_loss:3.3379 train_time:345493ms step_avg:316.10ms
step:1104/1700 train_loss:3.3600 train_time:345826ms step_avg:316.11ms
step:1105/1700 train_loss:3.4974 train_time:346159ms step_avg:316.13ms
step:1106/1700 train_loss:3.3681 train_time:346485ms step_avg:316.14ms
step:1107/1700 train_loss:4.1208 train_time:346825ms step_avg:316.16ms
step:1108/1700 train_loss:3.2785 train_time:347156ms step_avg:316.17ms
step:1109/1700 train_loss:3.6175 train_time:347484ms step_avg:316.18ms
step:1110/1700 train_loss:3.3876 train_time:347811ms step_avg:316.19ms
step:1111/1700 train_loss:3.5490 train_time:348135ms step_avg:316.20ms
step:1112/1700 train_loss:3.4834 train_time:348461ms step_avg:316.21ms
step:1113/1700 train_loss:3.5361 train_time:348796ms step_avg:316.22ms
step:1114/1700 train_loss:3.6122 train_time:349125ms step_avg:316.24ms
step:1115/1700 train_loss:3.4888 train_time:349450ms step_avg:316.24ms
step:1116/1700 train_loss:3.4055 train_time:349780ms step_avg:316.26ms
step:1117/1700 train_loss:3.2957 train_time:350126ms step_avg:316.28ms
step:1118/1700 train_loss:3.4733 train_time:350448ms step_avg:316.29ms
step:1119/1700 train_loss:3.6490 train_time:350784ms step_avg:316.31ms
step:1120/1700 train_loss:3.6724 train_time:351113ms step_avg:316.32ms
step:1121/1700 train_loss:3.5335 train_time:351437ms step_avg:316.33ms
step:1122/1700 train_loss:3.5384 train_time:351765ms step_avg:316.34ms
step:1123/1700 train_loss:3.4373 train_time:352092ms step_avg:316.35ms
step:1124/1700 train_loss:3.5052 train_time:352413ms step_avg:316.35ms
step:1125/1700 train_loss:3.6402 train_time:352748ms step_avg:316.37ms
step:1125/1700 val_loss:3.4633 train_time:352757ms step_avg:316.37ms
step:1126/1700 train_loss:3.3989 train_time:353079ms step_avg:316.38ms
step:1127/1700 train_loss:3.2707 train_time:353414ms step_avg:316.40ms
step:1128/1700 train_loss:3.5296 train_time:353750ms step_avg:316.41ms
step:1129/1700 train_loss:3.7323 train_time:354080ms step_avg:316.43ms
step:1130/1700 train_loss:3.2856 train_time:354413ms step_avg:316.44ms
step:1131/1700 train_loss:3.6122 train_time:354743ms step_avg:316.45ms
step:1132/1700 train_loss:3.4326 train_time:355069ms step_avg:316.46ms
step:1133/1700 train_loss:3.4487 train_time:355397ms step_avg:316.47ms
step:1134/1700 train_loss:3.4144 train_time:355722ms step_avg:316.48ms
step:1135/1700 train_loss:3.5394 train_time:356062ms step_avg:316.50ms
step:1136/1700 train_loss:3.4988 train_time:356389ms step_avg:316.51ms
step:1137/1700 train_loss:3.5748 train_time:356720ms step_avg:316.52ms
step:1138/1700 train_loss:3.6109 train_time:357052ms step_avg:316.54ms
step:1139/1700 train_loss:3.5088 train_time:357385ms step_avg:316.55ms
step:1140/1700 train_loss:3.4048 train_time:357892ms step_avg:316.72ms
step:1141/1700 train_loss:3.7022 train_time:358223ms step_avg:316.73ms
step:1142/1700 train_loss:3.5144 train_time:358549ms step_avg:316.74ms
step:1143/1700 train_loss:3.5737 train_time:359055ms step_avg:316.91ms
step:1144/1700 train_loss:3.6186 train_time:359381ms step_avg:316.91ms
step:1145/1700 train_loss:3.2158 train_time:359712ms step_avg:316.93ms
step:1146/1700 train_loss:3.5475 train_time:360043ms step_avg:316.94ms
step:1147/1700 train_loss:3.3995 train_time:360372ms step_avg:316.95ms
step:1148/1700 train_loss:3.4531 train_time:360695ms step_avg:316.96ms
step:1149/1700 train_loss:3.5084 train_time:361023ms step_avg:316.96ms
step:1150/1700 train_loss:3.5903 train_time:361351ms step_avg:316.97ms
step:1151/1700 train_loss:3.5497 train_time:361680ms step_avg:316.98ms
step:1152/1700 train_loss:3.4487 train_time:362016ms step_avg:317.00ms
step:1153/1700 train_loss:3.4065 train_time:362354ms step_avg:317.02ms
step:1154/1700 train_loss:3.6335 train_time:362683ms step_avg:317.03ms
step:1155/1700 train_loss:3.6372 train_time:363018ms step_avg:317.05ms
step:1156/1700 train_loss:3.3542 train_time:363347ms step_avg:317.06ms
step:1157/1700 train_loss:3.3418 train_time:363673ms step_avg:317.06ms
step:1158/1700 train_loss:3.4927 train_time:364014ms step_avg:317.09ms
step:1159/1700 train_loss:3.5155 train_time:364344ms step_avg:317.10ms
step:1160/1700 train_loss:3.2780 train_time:364672ms step_avg:317.11ms
step:1161/1700 train_loss:3.3453 train_time:364999ms step_avg:317.11ms
step:1162/1700 train_loss:3.3424 train_time:365327ms step_avg:317.12ms
step:1163/1700 train_loss:3.5580 train_time:365658ms step_avg:317.14ms
step:1164/1700 train_loss:3.3668 train_time:365992ms step_avg:317.15ms
step:1165/1700 train_loss:3.4815 train_time:366322ms step_avg:317.16ms
step:1166/1700 train_loss:3.4506 train_time:366651ms step_avg:317.17ms
step:1167/1700 train_loss:3.4575 train_time:366986ms step_avg:317.19ms
step:1168/1700 train_loss:3.4377 train_time:367313ms step_avg:317.20ms
step:1169/1700 train_loss:3.4406 train_time:367641ms step_avg:317.21ms
step:1170/1700 train_loss:3.6454 train_time:367971ms step_avg:317.22ms
step:1171/1700 train_loss:3.4142 train_time:368304ms step_avg:317.23ms
step:1172/1700 train_loss:3.5026 train_time:368632ms step_avg:317.24ms
step:1173/1700 train_loss:3.4625 train_time:368967ms step_avg:317.25ms
step:1174/1700 train_loss:3.3733 train_time:369294ms step_avg:317.26ms
step:1175/1700 train_loss:3.4654 train_time:369620ms step_avg:317.27ms
step:1176/1700 train_loss:3.8140 train_time:369978ms step_avg:317.31ms
step:1177/1700 train_loss:3.4361 train_time:370309ms step_avg:317.32ms
step:1178/1700 train_loss:3.4458 train_time:370642ms step_avg:317.33ms
step:1179/1700 train_loss:3.3272 train_time:370984ms step_avg:317.35ms
step:1180/1700 train_loss:3.4686 train_time:371315ms step_avg:317.36ms
step:1181/1700 train_loss:3.4742 train_time:371641ms step_avg:317.37ms
step:1182/1700 train_loss:3.4170 train_time:371971ms step_avg:317.38ms
step:1183/1700 train_loss:3.3339 train_time:372305ms step_avg:317.40ms
step:1184/1700 train_loss:3.4672 train_time:372638ms step_avg:317.41ms
step:1185/1700 train_loss:3.5748 train_time:372968ms step_avg:317.42ms
step:1186/1700 train_loss:3.7455 train_time:373300ms step_avg:317.43ms
step:1187/1700 train_loss:3.5932 train_time:373632ms step_avg:317.44ms
step:1188/1700 train_loss:3.4280 train_time:373968ms step_avg:317.46ms
step:1189/1700 train_loss:3.2887 train_time:374313ms step_avg:317.48ms
step:1190/1700 train_loss:3.4192 train_time:374645ms step_avg:317.50ms
step:1191/1700 train_loss:3.4191 train_time:374973ms step_avg:317.50ms
step:1192/1700 train_loss:3.4644 train_time:375303ms step_avg:317.52ms
step:1193/1700 train_loss:3.3813 train_time:375632ms step_avg:317.52ms
step:1194/1700 train_loss:3.5417 train_time:375961ms step_avg:317.53ms
step:1195/1700 train_loss:3.4087 train_time:376287ms step_avg:317.54ms
step:1196/1700 train_loss:3.3939 train_time:376624ms step_avg:317.56ms
step:1197/1700 train_loss:3.3828 train_time:376958ms step_avg:317.57ms
step:1198/1700 train_loss:3.4641 train_time:377287ms step_avg:317.58ms
step:1199/1700 train_loss:3.4387 train_time:377613ms step_avg:317.59ms
step:1200/1700 train_loss:3.4022 train_time:377950ms step_avg:317.61ms
step:1201/1700 train_loss:3.8162 train_time:378298ms step_avg:317.63ms
step:1202/1700 train_loss:3.3477 train_time:378632ms step_avg:317.64ms
step:1203/1700 train_loss:3.4403 train_time:378961ms step_avg:317.65ms
step:1204/1700 train_loss:3.4361 train_time:379297ms step_avg:317.67ms
step:1205/1700 train_loss:3.5032 train_time:379647ms step_avg:317.70ms
step:1206/1700 train_loss:3.5067 train_time:379981ms step_avg:317.71ms
step:1207/1700 train_loss:3.4643 train_time:380317ms step_avg:317.73ms
step:1208/1700 train_loss:3.3904 train_time:380650ms step_avg:317.74ms
step:1209/1700 train_loss:3.4505 train_time:380981ms step_avg:317.75ms
step:1210/1700 train_loss:3.3886 train_time:381315ms step_avg:317.76ms
step:1211/1700 train_loss:3.4573 train_time:381646ms step_avg:317.77ms
step:1212/1700 train_loss:3.6990 train_time:381984ms step_avg:317.79ms
step:1213/1700 train_loss:3.3793 train_time:382312ms step_avg:317.80ms
step:1214/1700 train_loss:3.6224 train_time:382641ms step_avg:317.81ms
step:1215/1700 train_loss:3.4228 train_time:382967ms step_avg:317.82ms
step:1216/1700 train_loss:3.5121 train_time:383297ms step_avg:317.83ms
step:1217/1700 train_loss:3.4615 train_time:383630ms step_avg:317.84ms
step:1218/1700 train_loss:3.4740 train_time:383957ms step_avg:317.85ms
step:1219/1700 train_loss:3.4997 train_time:384291ms step_avg:317.86ms
step:1220/1700 train_loss:3.4291 train_time:384619ms step_avg:317.87ms
step:1221/1700 train_loss:3.4809 train_time:384950ms step_avg:317.88ms
step:1222/1700 train_loss:3.4853 train_time:385281ms step_avg:317.89ms
step:1223/1700 train_loss:3.4929 train_time:385610ms step_avg:317.90ms
step:1224/1700 train_loss:3.4466 train_time:385955ms step_avg:317.92ms
step:1225/1700 train_loss:3.4388 train_time:386285ms step_avg:317.93ms
step:1226/1700 train_loss:3.3333 train_time:386616ms step_avg:317.94ms
step:1227/1700 train_loss:3.4908 train_time:386950ms step_avg:317.95ms
step:1228/1700 train_loss:3.2836 train_time:387278ms step_avg:317.96ms
step:1229/1700 train_loss:3.5825 train_time:387610ms step_avg:317.97ms
step:1230/1700 train_loss:3.3961 train_time:387943ms step_avg:317.99ms
step:1231/1700 train_loss:3.4070 train_time:388284ms step_avg:318.01ms
step:1232/1700 train_loss:3.5664 train_time:388626ms step_avg:318.02ms
step:1233/1700 train_loss:3.2924 train_time:388959ms step_avg:318.04ms
step:1234/1700 train_loss:3.3712 train_time:389289ms step_avg:318.05ms
step:1235/1700 train_loss:3.4212 train_time:389621ms step_avg:318.06ms
step:1236/1700 train_loss:3.4338 train_time:389951ms step_avg:318.07ms
step:1237/1700 train_loss:3.4291 train_time:390290ms step_avg:318.08ms
step:1238/1700 train_loss:3.3962 train_time:390622ms step_avg:318.10ms
step:1239/1700 train_loss:3.6244 train_time:390948ms step_avg:318.10ms
step:1240/1700 train_loss:3.3763 train_time:391293ms step_avg:318.12ms
step:1241/1700 train_loss:3.2588 train_time:391621ms step_avg:318.13ms
step:1242/1700 train_loss:3.5358 train_time:391960ms step_avg:318.15ms
step:1243/1700 train_loss:3.2525 train_time:392295ms step_avg:318.16ms
step:1244/1700 train_loss:3.4437 train_time:392621ms step_avg:318.17ms
step:1245/1700 train_loss:3.6673 train_time:392949ms step_avg:318.18ms
step:1246/1700 train_loss:3.3582 train_time:393281ms step_avg:318.19ms
step:1247/1700 train_loss:3.4401 train_time:393606ms step_avg:318.19ms
step:1248/1700 train_loss:3.5329 train_time:393930ms step_avg:318.20ms
step:1249/1700 train_loss:3.2393 train_time:394260ms step_avg:318.21ms
step:1250/1700 train_loss:3.3608 train_time:394591ms step_avg:318.22ms
step:1250/1700 val_loss:3.4079 train_time:394600ms step_avg:318.23ms
step:1251/1700 train_loss:3.3650 train_time:394933ms step_avg:318.24ms
step:1252/1700 train_loss:3.2399 train_time:395264ms step_avg:318.25ms
step:1253/1700 train_loss:3.4925 train_time:395590ms step_avg:318.25ms
step:1254/1700 train_loss:3.5968 train_time:395939ms step_avg:318.28ms
step:1255/1700 train_loss:3.3362 train_time:396269ms step_avg:318.29ms
step:1256/1700 train_loss:3.5379 train_time:396593ms step_avg:318.29ms
step:1257/1700 train_loss:3.2894 train_time:396927ms step_avg:318.31ms
step:1258/1700 train_loss:3.4613 train_time:397260ms step_avg:318.32ms
step:1259/1700 train_loss:3.5492 train_time:397590ms step_avg:318.33ms
step:1260/1700 train_loss:3.3982 train_time:397917ms step_avg:318.33ms
step:1261/1700 train_loss:3.4553 train_time:398254ms step_avg:318.35ms
step:1262/1700 train_loss:3.5741 train_time:398584ms step_avg:318.36ms
step:1263/1700 train_loss:3.2412 train_time:398915ms step_avg:318.37ms
step:1264/1700 train_loss:3.4923 train_time:399251ms step_avg:318.38ms
step:1265/1700 train_loss:3.4168 train_time:399589ms step_avg:318.40ms
step:1266/1700 train_loss:3.4038 train_time:399923ms step_avg:318.41ms
step:1267/1700 train_loss:3.3312 train_time:400256ms step_avg:318.42ms
step:1268/1700 train_loss:3.3306 train_time:400596ms step_avg:318.44ms
step:1269/1700 train_loss:3.4384 train_time:400928ms step_avg:318.45ms
step:1270/1700 train_loss:3.3339 train_time:401265ms step_avg:318.46ms
step:1271/1700 train_loss:3.4549 train_time:401595ms step_avg:318.47ms
step:1272/1700 train_loss:3.4015 train_time:401930ms step_avg:318.49ms
step:1273/1700 train_loss:3.4357 train_time:402258ms step_avg:318.49ms
step:1274/1700 train_loss:3.3395 train_time:402597ms step_avg:318.51ms
step:1275/1700 train_loss:3.4689 train_time:402925ms step_avg:318.52ms
step:1276/1700 train_loss:3.4070 train_time:403254ms step_avg:318.53ms
step:1277/1700 train_loss:3.4975 train_time:403583ms step_avg:318.53ms
step:1278/1700 train_loss:3.4384 train_time:403915ms step_avg:318.55ms
step:1279/1700 train_loss:3.3637 train_time:404268ms step_avg:318.57ms
step:1280/1700 train_loss:3.3095 train_time:404601ms step_avg:318.58ms
step:1281/1700 train_loss:3.3969 train_time:404929ms step_avg:318.59ms
step:1282/1700 train_loss:3.3644 train_time:405265ms step_avg:318.60ms
step:1283/1700 train_loss:3.5391 train_time:405591ms step_avg:318.61ms
step:1284/1700 train_loss:3.3612 train_time:405923ms step_avg:318.62ms
step:1285/1700 train_loss:3.5153 train_time:406258ms step_avg:318.63ms
step:1286/1700 train_loss:3.3840 train_time:406592ms step_avg:318.65ms
step:1287/1700 train_loss:3.4604 train_time:406918ms step_avg:318.65ms
step:1288/1700 train_loss:3.4276 train_time:407246ms step_avg:318.66ms
step:1289/1700 train_loss:3.4307 train_time:407585ms step_avg:318.67ms
step:1290/1700 train_loss:3.4431 train_time:407926ms step_avg:318.69ms
step:1291/1700 train_loss:3.5822 train_time:408269ms step_avg:318.71ms
step:1292/1700 train_loss:3.3912 train_time:408607ms step_avg:318.73ms
step:1293/1700 train_loss:3.1982 train_time:408946ms step_avg:318.74ms
step:1294/1700 train_loss:3.4169 train_time:409273ms step_avg:318.75ms
step:1295/1700 train_loss:3.4037 train_time:409622ms step_avg:318.77ms
step:1296/1700 train_loss:3.4495 train_time:409956ms step_avg:318.78ms
step:1297/1700 train_loss:3.4870 train_time:410290ms step_avg:318.80ms
step:1298/1700 train_loss:3.4878 train_time:410616ms step_avg:318.80ms
step:1299/1700 train_loss:3.4511 train_time:410951ms step_avg:318.81ms
step:1300/1700 train_loss:3.4340 train_time:411277ms step_avg:318.82ms
step:1301/1700 train_loss:3.5632 train_time:411607ms step_avg:318.83ms
step:1302/1700 train_loss:3.4032 train_time:411935ms step_avg:318.84ms
step:1303/1700 train_loss:3.5001 train_time:412267ms step_avg:318.85ms
step:1304/1700 train_loss:3.4296 train_time:412593ms step_avg:318.85ms
step:1305/1700 train_loss:3.5144 train_time:412935ms step_avg:318.87ms
step:1306/1700 train_loss:3.5999 train_time:413274ms step_avg:318.88ms
step:1307/1700 train_loss:3.3831 train_time:413612ms step_avg:318.90ms
step:1308/1700 train_loss:3.3392 train_time:413943ms step_avg:318.91ms
step:1309/1700 train_loss:3.3581 train_time:414277ms step_avg:318.92ms
step:1310/1700 train_loss:3.4112 train_time:414609ms step_avg:318.93ms
step:1311/1700 train_loss:3.3414 train_time:414939ms step_avg:318.94ms
step:1312/1700 train_loss:3.5121 train_time:415271ms step_avg:318.95ms
step:1313/1700 train_loss:3.4062 train_time:415601ms step_avg:318.96ms
step:1314/1700 train_loss:3.4617 train_time:415931ms step_avg:318.97ms
step:1315/1700 train_loss:3.4371 train_time:416263ms step_avg:318.98ms
step:1316/1700 train_loss:3.4655 train_time:416595ms step_avg:318.99ms
step:1317/1700 train_loss:3.3097 train_time:416936ms step_avg:319.00ms
step:1318/1700 train_loss:3.5831 train_time:417262ms step_avg:319.01ms
step:1319/1700 train_loss:3.4896 train_time:417595ms step_avg:319.02ms
step:1320/1700 train_loss:3.3733 train_time:417922ms step_avg:319.02ms
step:1321/1700 train_loss:3.4834 train_time:418259ms step_avg:319.04ms
step:1322/1700 train_loss:3.4561 train_time:418586ms step_avg:319.04ms
step:1323/1700 train_loss:3.4386 train_time:418916ms step_avg:319.05ms
step:1324/1700 train_loss:3.6231 train_time:419258ms step_avg:319.07ms
step:1325/1700 train_loss:3.4835 train_time:419599ms step_avg:319.09ms
step:1326/1700 train_loss:3.5265 train_time:419929ms step_avg:319.09ms
step:1327/1700 train_loss:3.5467 train_time:420257ms step_avg:319.10ms
step:1328/1700 train_loss:3.2933 train_time:420591ms step_avg:319.11ms
step:1329/1700 train_loss:3.3719 train_time:420924ms step_avg:319.12ms
step:1330/1700 train_loss:3.4309 train_time:421443ms step_avg:319.28ms
step:1331/1700 train_loss:2.6647 train_time:421790ms step_avg:319.30ms
step:1332/1700 train_loss:3.4359 train_time:422125ms step_avg:319.31ms
step:1333/1700 train_loss:3.4064 train_time:422560ms step_avg:319.40ms
step:1334/1700 train_loss:3.3839 train_time:422885ms step_avg:319.40ms
step:1335/1700 train_loss:3.7926 train_time:423235ms step_avg:319.42ms
step:1336/1700 train_loss:3.5289 train_time:423563ms step_avg:319.43ms
step:1337/1700 train_loss:3.4193 train_time:423896ms step_avg:319.44ms
step:1338/1700 train_loss:3.3483 train_time:424233ms step_avg:319.45ms
step:1339/1700 train_loss:3.3473 train_time:424571ms step_avg:319.47ms
step:1340/1700 train_loss:3.6004 train_time:424905ms step_avg:319.48ms
step:1341/1700 train_loss:3.5715 train_time:425240ms step_avg:319.49ms
step:1342/1700 train_loss:3.3908 train_time:425577ms step_avg:319.50ms
step:1343/1700 train_loss:3.3328 train_time:425907ms step_avg:319.51ms
step:1344/1700 train_loss:3.6401 train_time:426245ms step_avg:319.52ms
step:1345/1700 train_loss:3.4056 train_time:426588ms step_avg:319.54ms
step:1346/1700 train_loss:3.4171 train_time:426924ms step_avg:319.55ms
step:1347/1700 train_loss:3.4662 train_time:427255ms step_avg:319.56ms
step:1348/1700 train_loss:3.4293 train_time:427587ms step_avg:319.57ms
step:1349/1700 train_loss:3.3522 train_time:427911ms step_avg:319.57ms
step:1350/1700 train_loss:3.3237 train_time:428242ms step_avg:319.58ms
step:1351/1700 train_loss:3.3912 train_time:428583ms step_avg:319.60ms
step:1352/1700 train_loss:3.3318 train_time:428919ms step_avg:319.61ms
step:1353/1700 train_loss:3.4418 train_time:429249ms step_avg:319.62ms
step:1354/1700 train_loss:3.2923 train_time:429604ms step_avg:319.65ms
step:1355/1700 train_loss:3.3547 train_time:429930ms step_avg:319.65ms
step:1356/1700 train_loss:3.4629 train_time:430266ms step_avg:319.66ms
step:1357/1700 train_loss:3.3028 train_time:430605ms step_avg:319.68ms
step:1358/1700 train_loss:3.2409 train_time:430934ms step_avg:319.68ms
step:1359/1700 train_loss:3.5620 train_time:431270ms step_avg:319.70ms
step:1360/1700 train_loss:3.4757 train_time:431607ms step_avg:319.71ms
step:1361/1700 train_loss:3.2305 train_time:431939ms step_avg:319.72ms
step:1362/1700 train_loss:3.4941 train_time:432283ms step_avg:319.74ms
step:1363/1700 train_loss:3.3999 train_time:432620ms step_avg:319.75ms
step:1364/1700 train_loss:3.1902 train_time:432959ms step_avg:319.76ms
step:1365/1700 train_loss:3.4424 train_time:433293ms step_avg:319.77ms
step:1366/1700 train_loss:3.3266 train_time:433631ms step_avg:319.79ms
step:1367/1700 train_loss:3.3605 train_time:433958ms step_avg:319.79ms
step:1368/1700 train_loss:3.3607 train_time:434302ms step_avg:319.81ms
step:1369/1700 train_loss:3.4749 train_time:434630ms step_avg:319.82ms
step:1370/1700 train_loss:3.4479 train_time:434963ms step_avg:319.83ms
step:1371/1700 train_loss:3.3987 train_time:435302ms step_avg:319.84ms
step:1372/1700 train_loss:3.3167 train_time:435640ms step_avg:319.85ms
step:1373/1700 train_loss:3.6578 train_time:435977ms step_avg:319.87ms
step:1374/1700 train_loss:3.3617 train_time:436304ms step_avg:319.87ms
step:1375/1700 train_loss:3.4135 train_time:436639ms step_avg:319.88ms
step:1375/1700 val_loss:3.3630 train_time:436648ms step_avg:319.89ms
step:1376/1700 train_loss:3.4116 train_time:436982ms step_avg:319.90ms
step:1377/1700 train_loss:3.2053 train_time:437323ms step_avg:319.91ms
step:1378/1700 train_loss:3.5930 train_time:437652ms step_avg:319.92ms
step:1379/1700 train_loss:3.3906 train_time:437979ms step_avg:319.93ms
step:1380/1700 train_loss:3.5272 train_time:438325ms step_avg:319.95ms
step:1381/1700 train_loss:3.5264 train_time:438658ms step_avg:319.95ms
step:1382/1700 train_loss:3.1698 train_time:438996ms step_avg:319.97ms
step:1383/1700 train_loss:3.3661 train_time:439344ms step_avg:319.99ms
step:1384/1700 train_loss:3.7663 train_time:439681ms step_avg:320.00ms
step:1385/1700 train_loss:3.2649 train_time:440014ms step_avg:320.01ms
step:1386/1700 train_loss:3.4428 train_time:440345ms step_avg:320.02ms
step:1387/1700 train_loss:3.5280 train_time:440678ms step_avg:320.03ms
step:1388/1700 train_loss:3.4579 train_time:441004ms step_avg:320.03ms
step:1389/1700 train_loss:3.3896 train_time:441338ms step_avg:320.04ms
step:1390/1700 train_loss:3.2512 train_time:441674ms step_avg:320.05ms
step:1391/1700 train_loss:3.3910 train_time:442010ms step_avg:320.06ms
step:1392/1700 train_loss:3.3660 train_time:442341ms step_avg:320.07ms
step:1393/1700 train_loss:3.6253 train_time:442669ms step_avg:320.08ms
step:1394/1700 train_loss:3.3386 train_time:443003ms step_avg:320.09ms
step:1395/1700 train_loss:3.3400 train_time:443343ms step_avg:320.10ms
step:1396/1700 train_loss:3.2961 train_time:443672ms step_avg:320.11ms
step:1397/1700 train_loss:3.5603 train_time:444005ms step_avg:320.12ms
step:1398/1700 train_loss:3.4467 train_time:444340ms step_avg:320.13ms
step:1399/1700 train_loss:3.4584 train_time:444675ms step_avg:320.14ms
step:1400/1700 train_loss:3.3535 train_time:445012ms step_avg:320.15ms
step:1401/1700 train_loss:3.2974 train_time:445347ms step_avg:320.16ms
step:1402/1700 train_loss:3.3786 train_time:445684ms step_avg:320.18ms
step:1403/1700 train_loss:3.3652 train_time:446019ms step_avg:320.19ms
step:1404/1700 train_loss:3.3941 train_time:446347ms step_avg:320.19ms
step:1405/1700 train_loss:3.3488 train_time:446682ms step_avg:320.20ms
step:1406/1700 train_loss:3.5500 train_time:447021ms step_avg:320.22ms
step:1407/1700 train_loss:3.3259 train_time:447347ms step_avg:320.22ms
step:1408/1700 train_loss:3.3588 train_time:447684ms step_avg:320.23ms
step:1409/1700 train_loss:3.3587 train_time:448017ms step_avg:320.24ms
step:1410/1700 train_loss:3.2229 train_time:448349ms step_avg:320.25ms
step:1411/1700 train_loss:3.3481 train_time:448679ms step_avg:320.26ms
step:1412/1700 train_loss:3.3435 train_time:449030ms step_avg:320.28ms
step:1413/1700 train_loss:3.3348 train_time:449360ms step_avg:320.29ms
step:1414/1700 train_loss:3.4126 train_time:449693ms step_avg:320.29ms
step:1415/1700 train_loss:3.3722 train_time:450026ms step_avg:320.30ms
step:1416/1700 train_loss:3.4061 train_time:450360ms step_avg:320.31ms
step:1417/1700 train_loss:3.3841 train_time:450692ms step_avg:320.32ms
step:1418/1700 train_loss:3.4572 train_time:451024ms step_avg:320.33ms
step:1419/1700 train_loss:3.2743 train_time:451375ms step_avg:320.35ms
step:1420/1700 train_loss:3.3353 train_time:451717ms step_avg:320.37ms
step:1421/1700 train_loss:3.4389 train_time:452046ms step_avg:320.37ms
step:1422/1700 train_loss:3.3877 train_time:452382ms step_avg:320.38ms
step:1423/1700 train_loss:3.4053 train_time:452726ms step_avg:320.40ms
step:1424/1700 train_loss:3.4216 train_time:453066ms step_avg:320.41ms
step:1425/1700 train_loss:3.3890 train_time:453399ms step_avg:320.42ms
step:1426/1700 train_loss:3.3685 train_time:453730ms step_avg:320.43ms
step:1427/1700 train_loss:3.3756 train_time:454067ms step_avg:320.44ms
step:1428/1700 train_loss:3.2299 train_time:454411ms step_avg:320.46ms
step:1429/1700 train_loss:3.3744 train_time:454742ms step_avg:320.47ms
step:1430/1700 train_loss:3.3234 train_time:455079ms step_avg:320.48ms
step:1431/1700 train_loss:3.4246 train_time:455413ms step_avg:320.49ms
step:1432/1700 train_loss:3.4024 train_time:455741ms step_avg:320.49ms
step:1433/1700 train_loss:3.3077 train_time:456077ms step_avg:320.50ms
step:1434/1700 train_loss:3.3646 train_time:456412ms step_avg:320.51ms
step:1435/1700 train_loss:3.3864 train_time:456744ms step_avg:320.52ms
step:1436/1700 train_loss:3.1859 train_time:457086ms step_avg:320.54ms
step:1437/1700 train_loss:3.3366 train_time:457424ms step_avg:320.55ms
step:1438/1700 train_loss:3.1678 train_time:457755ms step_avg:320.56ms
step:1439/1700 train_loss:3.2734 train_time:458084ms step_avg:320.56ms
step:1440/1700 train_loss:3.4587 train_time:458425ms step_avg:320.58ms
step:1441/1700 train_loss:3.4255 train_time:458760ms step_avg:320.59ms
step:1442/1700 train_loss:3.3658 train_time:459102ms step_avg:320.60ms
step:1443/1700 train_loss:3.2332 train_time:459437ms step_avg:320.61ms
step:1444/1700 train_loss:3.3898 train_time:459770ms step_avg:320.62ms
step:1445/1700 train_loss:3.4370 train_time:460114ms step_avg:320.64ms
step:1446/1700 train_loss:3.5261 train_time:460457ms step_avg:320.65ms
step:1447/1700 train_loss:3.4954 train_time:460791ms step_avg:320.66ms
step:1448/1700 train_loss:3.3805 train_time:461124ms step_avg:320.67ms
step:1449/1700 train_loss:3.2478 train_time:461462ms step_avg:320.68ms
step:1450/1700 train_loss:3.3417 train_time:461800ms step_avg:320.69ms
step:1451/1700 train_loss:3.3431 train_time:462135ms step_avg:320.70ms
step:1452/1700 train_loss:3.4441 train_time:462466ms step_avg:320.71ms
step:1453/1700 train_loss:3.4371 train_time:462797ms step_avg:320.72ms
step:1454/1700 train_loss:3.2522 train_time:463134ms step_avg:320.73ms
step:1455/1700 train_loss:3.3773 train_time:463468ms step_avg:320.74ms
step:1456/1700 train_loss:3.3018 train_time:463806ms step_avg:320.75ms
step:1457/1700 train_loss:3.3377 train_time:464140ms step_avg:320.76ms
step:1458/1700 train_loss:3.3784 train_time:464483ms step_avg:320.78ms
step:1459/1700 train_loss:3.3227 train_time:464817ms step_avg:320.78ms
step:1460/1700 train_loss:3.2058 train_time:465153ms step_avg:320.80ms
step:1461/1700 train_loss:3.4637 train_time:465489ms step_avg:320.81ms
step:1462/1700 train_loss:3.3134 train_time:465821ms step_avg:320.81ms
step:1463/1700 train_loss:3.3613 train_time:466159ms step_avg:320.82ms
step:1464/1700 train_loss:3.4760 train_time:466486ms step_avg:320.83ms
step:1465/1700 train_loss:3.3079 train_time:466821ms step_avg:320.84ms
step:1466/1700 train_loss:3.5139 train_time:467158ms step_avg:320.85ms
step:1467/1700 train_loss:3.4051 train_time:467488ms step_avg:320.86ms
step:1468/1700 train_loss:3.4001 train_time:467818ms step_avg:320.86ms
step:1469/1700 train_loss:3.3343 train_time:468161ms step_avg:320.88ms
step:1470/1700 train_loss:3.4375 train_time:468495ms step_avg:320.89ms
step:1471/1700 train_loss:3.3296 train_time:468830ms step_avg:320.90ms
step:1472/1700 train_loss:3.3137 train_time:469168ms step_avg:320.91ms
step:1473/1700 train_loss:3.3823 train_time:469517ms step_avg:320.93ms
step:1474/1700 train_loss:3.2959 train_time:469854ms step_avg:320.94ms
step:1475/1700 train_loss:3.2793 train_time:470196ms step_avg:320.95ms
step:1476/1700 train_loss:3.4786 train_time:470522ms step_avg:320.96ms
step:1477/1700 train_loss:3.3528 train_time:470856ms step_avg:320.97ms
step:1478/1700 train_loss:3.1895 train_time:471191ms step_avg:320.97ms
step:1479/1700 train_loss:3.3082 train_time:471533ms step_avg:320.99ms
step:1480/1700 train_loss:3.2887 train_time:471878ms step_avg:321.01ms
step:1481/1700 train_loss:3.3534 train_time:472222ms step_avg:321.02ms
step:1482/1700 train_loss:3.4359 train_time:472558ms step_avg:321.03ms
step:1483/1700 train_loss:3.3189 train_time:472887ms step_avg:321.04ms
step:1484/1700 train_loss:3.4931 train_time:473222ms step_avg:321.05ms
step:1485/1700 train_loss:3.4125 train_time:473554ms step_avg:321.05ms
step:1486/1700 train_loss:3.3230 train_time:473903ms step_avg:321.07ms
step:1487/1700 train_loss:3.3002 train_time:474239ms step_avg:321.08ms
step:1488/1700 train_loss:3.3163 train_time:474571ms step_avg:321.09ms
step:1489/1700 train_loss:3.2622 train_time:474917ms step_avg:321.11ms
step:1490/1700 train_loss:3.3818 train_time:475260ms step_avg:321.12ms
step:1491/1700 train_loss:3.2735 train_time:475598ms step_avg:321.13ms
step:1492/1700 train_loss:3.3606 train_time:475929ms step_avg:321.14ms
step:1493/1700 train_loss:3.2938 train_time:476267ms step_avg:321.15ms
step:1494/1700 train_loss:3.2043 train_time:476596ms step_avg:321.16ms
step:1495/1700 train_loss:3.3047 train_time:476934ms step_avg:321.17ms
step:1496/1700 train_loss:3.4738 train_time:477268ms step_avg:321.18ms
step:1497/1700 train_loss:3.3408 train_time:477609ms step_avg:321.19ms
step:1498/1700 train_loss:3.0739 train_time:477947ms step_avg:321.20ms
step:1499/1700 train_loss:3.3998 train_time:478284ms step_avg:321.21ms
step:1500/1700 train_loss:3.3533 train_time:478622ms step_avg:321.22ms
step:1500/1700 val_loss:3.3190 train_time:478631ms step_avg:321.23ms
step:1501/1700 train_loss:3.3856 train_time:478975ms step_avg:321.24ms
step:1502/1700 train_loss:3.3538 train_time:479331ms step_avg:321.27ms
step:1503/1700 train_loss:3.3348 train_time:479675ms step_avg:321.28ms
step:1504/1700 train_loss:3.1204 train_time:480033ms step_avg:321.31ms
step:1505/1700 train_loss:3.4010 train_time:480376ms step_avg:321.32ms
step:1506/1700 train_loss:3.2812 train_time:480731ms step_avg:321.34ms
step:1507/1700 train_loss:3.2861 train_time:481076ms step_avg:321.36ms
step:1508/1700 train_loss:3.2489 train_time:481410ms step_avg:321.37ms
step:1509/1700 train_loss:3.3187 train_time:481744ms step_avg:321.38ms
step:1510/1700 train_loss:3.2092 train_time:482079ms step_avg:321.39ms
step:1511/1700 train_loss:3.5236 train_time:482415ms step_avg:321.40ms
step:1512/1700 train_loss:3.3143 train_time:482743ms step_avg:321.40ms
step:1513/1700 train_loss:3.3085 train_time:483084ms step_avg:321.41ms
step:1514/1700 train_loss:3.4499 train_time:483409ms step_avg:321.42ms
step:1515/1700 train_loss:3.4584 train_time:483754ms step_avg:321.43ms
step:1516/1700 train_loss:3.3039 train_time:484092ms step_avg:321.44ms
step:1517/1700 train_loss:3.1224 train_time:484440ms step_avg:321.46ms
step:1518/1700 train_loss:3.2729 train_time:484770ms step_avg:321.47ms
step:1519/1700 train_loss:3.2897 train_time:485121ms step_avg:321.48ms
step:1520/1700 train_loss:3.3372 train_time:485634ms step_avg:321.61ms
step:1521/1700 train_loss:3.2440 train_time:485969ms step_avg:321.62ms
step:1522/1700 train_loss:3.5468 train_time:486306ms step_avg:321.63ms
step:1523/1700 train_loss:3.1650 train_time:486639ms step_avg:321.64ms
step:1524/1700 train_loss:3.2799 train_time:487172ms step_avg:321.78ms
step:1525/1700 train_loss:3.3366 train_time:487515ms step_avg:321.79ms
step:1526/1700 train_loss:3.3231 train_time:487857ms step_avg:321.81ms
step:1527/1700 train_loss:3.3815 train_time:488195ms step_avg:321.82ms
step:1528/1700 train_loss:3.2816 train_time:488529ms step_avg:321.82ms
step:1529/1700 train_loss:3.2450 train_time:488862ms step_avg:321.83ms
step:1530/1700 train_loss:3.2999 train_time:489193ms step_avg:321.84ms
step:1531/1700 train_loss:3.3142 train_time:489531ms step_avg:321.85ms
step:1532/1700 train_loss:3.3223 train_time:489869ms step_avg:321.86ms
step:1533/1700 train_loss:3.2754 train_time:490238ms step_avg:321.89ms
step:1534/1700 train_loss:3.4386 train_time:490573ms step_avg:321.90ms
step:1535/1700 train_loss:3.2196 train_time:490914ms step_avg:321.91ms
step:1536/1700 train_loss:3.3680 train_time:491246ms step_avg:321.92ms
step:1537/1700 train_loss:3.2838 train_time:491584ms step_avg:321.93ms
step:1538/1700 train_loss:3.1667 train_time:491932ms step_avg:321.94ms
step:1539/1700 train_loss:3.1321 train_time:492275ms step_avg:321.96ms
step:1540/1700 train_loss:3.2217 train_time:492605ms step_avg:321.96ms
step:1541/1700 train_loss:3.2392 train_time:492951ms step_avg:321.98ms
step:1542/1700 train_loss:3.1492 train_time:493285ms step_avg:321.99ms
step:1543/1700 train_loss:3.4304 train_time:493620ms step_avg:322.00ms
step:1544/1700 train_loss:3.2497 train_time:493953ms step_avg:322.00ms
step:1545/1700 train_loss:3.1288 train_time:494318ms step_avg:322.03ms
step:1546/1700 train_loss:3.3131 train_time:494659ms step_avg:322.04ms
step:1547/1700 train_loss:3.3237 train_time:495001ms step_avg:322.06ms
step:1548/1700 train_loss:3.1039 train_time:495336ms step_avg:322.07ms
step:1549/1700 train_loss:3.4663 train_time:495671ms step_avg:322.07ms
step:1550/1700 train_loss:3.4316 train_time:496015ms step_avg:322.09ms
step:1551/1700 train_loss:3.2845 train_time:496365ms step_avg:322.11ms
step:1552/1700 train_loss:3.4498 train_time:496702ms step_avg:322.12ms
step:1553/1700 train_loss:3.3525 train_time:497036ms step_avg:322.12ms
step:1554/1700 train_loss:3.2811 train_time:497379ms step_avg:322.14ms
step:1555/1700 train_loss:3.4365 train_time:497718ms step_avg:322.15ms
step:1556/1700 train_loss:3.3982 train_time:498051ms step_avg:322.15ms
step:1557/1700 train_loss:3.2776 train_time:498382ms step_avg:322.16ms
step:1558/1700 train_loss:3.2238 train_time:498719ms step_avg:322.17ms
step:1559/1700 train_loss:3.2357 train_time:499056ms step_avg:322.18ms
step:1560/1700 train_loss:3.2678 train_time:499393ms step_avg:322.19ms
step:1561/1700 train_loss:3.2989 train_time:499732ms step_avg:322.20ms
step:1562/1700 train_loss:3.3059 train_time:500078ms step_avg:322.22ms
step:1563/1700 train_loss:3.3558 train_time:500418ms step_avg:322.23ms
step:1564/1700 train_loss:3.2638 train_time:500747ms step_avg:322.23ms
step:1565/1700 train_loss:3.3772 train_time:501087ms step_avg:322.24ms
step:1566/1700 train_loss:3.2453 train_time:501431ms step_avg:322.26ms
step:1567/1700 train_loss:3.3931 train_time:501768ms step_avg:322.27ms
step:1568/1700 train_loss:3.2140 train_time:502106ms step_avg:322.28ms
step:1569/1700 train_loss:3.2285 train_time:502442ms step_avg:322.28ms
step:1570/1700 train_loss:3.1563 train_time:502774ms step_avg:322.29ms
step:1571/1700 train_loss:3.1628 train_time:503120ms step_avg:322.31ms
step:1572/1700 train_loss:3.2145 train_time:503454ms step_avg:322.31ms
step:1573/1700 train_loss:3.2887 train_time:503789ms step_avg:322.32ms
step:1574/1700 train_loss:3.0639 train_time:504130ms step_avg:322.33ms
step:1575/1700 train_loss:3.2869 train_time:504464ms step_avg:322.34ms
step:1576/1700 train_loss:3.2986 train_time:504820ms step_avg:322.36ms
step:1577/1700 train_loss:3.2723 train_time:505161ms step_avg:322.37ms
step:1578/1700 train_loss:3.3076 train_time:505511ms step_avg:322.39ms
step:1579/1700 train_loss:3.2963 train_time:505849ms step_avg:322.40ms
step:1580/1700 train_loss:3.2472 train_time:506181ms step_avg:322.41ms
step:1581/1700 train_loss:3.4656 train_time:506517ms step_avg:322.42ms
step:1582/1700 train_loss:3.3301 train_time:506859ms step_avg:322.43ms
step:1583/1700 train_loss:3.3867 train_time:507193ms step_avg:322.44ms
step:1584/1700 train_loss:3.1182 train_time:507546ms step_avg:322.46ms
step:1585/1700 train_loss:3.2675 train_time:507893ms step_avg:322.47ms
step:1586/1700 train_loss:3.2007 train_time:508228ms step_avg:322.48ms
step:1587/1700 train_loss:3.3619 train_time:508559ms step_avg:322.49ms
step:1588/1700 train_loss:3.2648 train_time:508892ms step_avg:322.49ms
step:1589/1700 train_loss:3.2261 train_time:509226ms step_avg:322.50ms
step:1590/1700 train_loss:3.2263 train_time:509573ms step_avg:322.51ms
step:1591/1700 train_loss:3.2639 train_time:509904ms step_avg:322.52ms
step:1592/1700 train_loss:3.0943 train_time:510246ms step_avg:322.53ms
step:1593/1700 train_loss:3.2172 train_time:510578ms step_avg:322.54ms
step:1594/1700 train_loss:3.5617 train_time:510920ms step_avg:322.55ms
step:1595/1700 train_loss:3.1963 train_time:511257ms step_avg:322.56ms
step:1596/1700 train_loss:3.1455 train_time:511613ms step_avg:322.58ms
step:1597/1700 train_loss:3.3299 train_time:511947ms step_avg:322.59ms
step:1598/1700 train_loss:3.2546 train_time:512296ms step_avg:322.60ms
step:1599/1700 train_loss:3.1100 train_time:512640ms step_avg:322.62ms
step:1600/1700 train_loss:3.2720 train_time:512981ms step_avg:322.63ms
step:1601/1700 train_loss:3.2617 train_time:513311ms step_avg:322.63ms
step:1602/1700 train_loss:3.3297 train_time:513649ms step_avg:322.64ms
step:1603/1700 train_loss:3.2277 train_time:513980ms step_avg:322.65ms
step:1604/1700 train_loss:3.1431 train_time:514329ms step_avg:322.67ms
step:1605/1700 train_loss:3.3069 train_time:514672ms step_avg:322.68ms
step:1606/1700 train_loss:3.0957 train_time:515023ms step_avg:322.70ms
step:1607/1700 train_loss:3.3083 train_time:515360ms step_avg:322.70ms
step:1608/1700 train_loss:3.1784 train_time:515700ms step_avg:322.72ms
step:1609/1700 train_loss:3.2682 train_time:516035ms step_avg:322.72ms
step:1610/1700 train_loss:3.2910 train_time:516387ms step_avg:322.74ms
step:1611/1700 train_loss:3.1455 train_time:516722ms step_avg:322.75ms
step:1612/1700 train_loss:3.2791 train_time:517085ms step_avg:322.77ms
step:1613/1700 train_loss:3.2703 train_time:517420ms step_avg:322.78ms
step:1614/1700 train_loss:3.1155 train_time:517791ms step_avg:322.81ms
step:1615/1700 train_loss:3.3634 train_time:518147ms step_avg:322.83ms
step:1616/1700 train_loss:3.3272 train_time:518486ms step_avg:322.84ms
step:1617/1700 train_loss:3.2897 train_time:518820ms step_avg:322.85ms
step:1618/1700 train_loss:3.1987 train_time:519162ms step_avg:322.86ms
step:1619/1700 train_loss:3.3969 train_time:519496ms step_avg:322.87ms
step:1620/1700 train_loss:3.3501 train_time:519827ms step_avg:322.87ms
step:1621/1700 train_loss:3.2705 train_time:520164ms step_avg:322.88ms
step:1622/1700 train_loss:3.5164 train_time:520499ms step_avg:322.89ms
step:1623/1700 train_loss:3.2187 train_time:520836ms step_avg:322.90ms
step:1624/1700 train_loss:3.5118 train_time:521193ms step_avg:322.92ms
step:1625/1700 train_loss:3.2632 train_time:521528ms step_avg:322.93ms
step:1625/1700 val_loss:3.2868 train_time:521537ms step_avg:322.93ms
step:1626/1700 train_loss:3.3412 train_time:521859ms step_avg:322.93ms
step:1627/1700 train_loss:3.3541 train_time:522194ms step_avg:322.94ms
step:1628/1700 train_loss:3.3851 train_time:522529ms step_avg:322.95ms
step:1629/1700 train_loss:3.2347 train_time:522867ms step_avg:322.96ms
step:1630/1700 train_loss:3.3016 train_time:523203ms step_avg:322.96ms
step:1631/1700 train_loss:3.2624 train_time:523539ms step_avg:322.97ms
step:1632/1700 train_loss:3.1492 train_time:523877ms step_avg:322.98ms
step:1633/1700 train_loss:3.2801 train_time:524215ms step_avg:322.99ms
step:1634/1700 train_loss:3.2782 train_time:524551ms step_avg:323.00ms
step:1635/1700 train_loss:3.2723 train_time:524892ms step_avg:323.01ms
step:1636/1700 train_loss:3.3517 train_time:525226ms step_avg:323.02ms
step:1637/1700 train_loss:3.6215 train_time:525569ms step_avg:323.03ms
step:1638/1700 train_loss:3.2455 train_time:525934ms step_avg:323.06ms
step:1639/1700 train_loss:3.2164 train_time:526275ms step_avg:323.07ms
step:1640/1700 train_loss:3.2430 train_time:526613ms step_avg:323.08ms
step:1641/1700 train_loss:3.3522 train_time:526940ms step_avg:323.08ms
step:1642/1700 train_loss:3.3038 train_time:527271ms step_avg:323.08ms
step:1643/1700 train_loss:3.2931 train_time:527619ms step_avg:323.10ms
step:1644/1700 train_loss:3.2045 train_time:527949ms step_avg:323.10ms
step:1645/1700 train_loss:3.1925 train_time:528287ms step_avg:323.11ms
step:1646/1700 train_loss:3.2887 train_time:528625ms step_avg:323.12ms
step:1647/1700 train_loss:3.1535 train_time:528972ms step_avg:323.14ms
step:1648/1700 train_loss:3.3137 train_time:529308ms step_avg:323.14ms
step:1649/1700 train_loss:3.3475 train_time:529638ms step_avg:323.15ms
step:1650/1700 train_loss:3.2418 train_time:529974ms step_avg:323.15ms
step:1651/1700 train_loss:3.3175 train_time:530312ms step_avg:323.16ms
step:1652/1700 train_loss:3.2848 train_time:530651ms step_avg:323.17ms
step:1653/1700 train_loss:3.2194 train_time:530980ms step_avg:323.18ms
step:1654/1700 train_loss:3.3506 train_time:531319ms step_avg:323.19ms
step:1655/1700 train_loss:3.1909 train_time:531653ms step_avg:323.19ms
step:1656/1700 train_loss:2.9716 train_time:531999ms step_avg:323.21ms
step:1657/1700 train_loss:3.2967 train_time:532332ms step_avg:323.21ms
step:1658/1700 train_loss:3.3214 train_time:532664ms step_avg:323.22ms
step:1659/1700 train_loss:3.2665 train_time:533001ms step_avg:323.23ms
step:1660/1700 train_loss:3.5216 train_time:533352ms step_avg:323.24ms
step:1661/1700 train_loss:3.3547 train_time:533687ms step_avg:323.25ms
step:1662/1700 train_loss:3.3229 train_time:534022ms step_avg:323.26ms
step:1663/1700 train_loss:3.3023 train_time:534364ms step_avg:323.27ms
step:1664/1700 train_loss:3.3365 train_time:534695ms step_avg:323.27ms
step:1665/1700 train_loss:3.1520 train_time:535034ms step_avg:323.28ms
step:1666/1700 train_loss:3.3102 train_time:535367ms step_avg:323.29ms
step:1667/1700 train_loss:3.1007 train_time:535706ms step_avg:323.30ms
step:1668/1700 train_loss:3.2661 train_time:536041ms step_avg:323.31ms
step:1669/1700 train_loss:3.2963 train_time:536379ms step_avg:323.31ms
step:1670/1700 train_loss:3.1125 train_time:536710ms step_avg:323.32ms
step:1671/1700 train_loss:3.2697 train_time:537074ms step_avg:323.34ms
step:1672/1700 train_loss:3.1634 train_time:537419ms step_avg:323.36ms
step:1673/1700 train_loss:3.4514 train_time:537753ms step_avg:323.36ms
step:1674/1700 train_loss:3.3147 train_time:538088ms step_avg:323.37ms
step:1675/1700 train_loss:3.2771 train_time:538419ms step_avg:323.37ms
step:1676/1700 train_loss:3.1631 train_time:538761ms step_avg:323.39ms
step:1677/1700 train_loss:3.2300 train_time:539107ms step_avg:323.40ms
step:1678/1700 train_loss:3.5685 train_time:539444ms step_avg:323.41ms
step:1679/1700 train_loss:3.2846 train_time:539784ms step_avg:323.42ms
step:1680/1700 train_loss:3.2691 train_time:540121ms step_avg:323.43ms
step:1681/1700 train_loss:3.3999 train_time:540458ms step_avg:323.43ms
step:1682/1700 train_loss:3.3197 train_time:540796ms step_avg:323.44ms
step:1683/1700 train_loss:3.2351 train_time:541147ms step_avg:323.46ms
step:1684/1700 train_loss:3.3343 train_time:541483ms step_avg:323.47ms
step:1685/1700 train_loss:3.1614 train_time:541820ms step_avg:323.47ms
step:1686/1700 train_loss:3.3256 train_time:542157ms step_avg:323.48ms
step:1687/1700 train_loss:3.2184 train_time:542509ms step_avg:323.50ms
step:1688/1700 train_loss:3.0344 train_time:542848ms step_avg:323.51ms
step:1689/1700 train_loss:3.3772 train_time:543195ms step_avg:323.52ms
step:1690/1700 train_loss:3.3125 train_time:543531ms step_avg:323.53ms
step:1691/1700 train_loss:3.3110 train_time:543870ms step_avg:323.54ms
step:1692/1700 train_loss:3.2145 train_time:544228ms step_avg:323.56ms
step:1693/1700 train_loss:3.2069 train_time:544566ms step_avg:323.57ms
step:1694/1700 train_loss:3.3002 train_time:544902ms step_avg:323.58ms
step:1695/1700 train_loss:3.2264 train_time:545238ms step_avg:323.58ms
step:1696/1700 train_loss:3.3150 train_time:545579ms step_avg:323.59ms
step:1697/1700 train_loss:3.2637 train_time:545923ms step_avg:323.61ms
step:1698/1700 train_loss:3.3752 train_time:546253ms step_avg:323.61ms
step:1699/1700 train_loss:3.1997 train_time:546597ms step_avg:323.62ms
step:1700/1700 train_loss:3.2608 train_time:546936ms step_avg:323.63ms
step:1700/1700 val_loss:3.2764 train_time:546944ms step_avg:323.64ms
