import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.13 + 0.01 * min(layer_idx, 11 - layer_idx)  # unet pattern attention scale by @leloykun

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04), dict(params=attn_scale_params, lr=0.01),]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 17:33:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:23990ms step_avg:nanms
step:2/1395 train_time:24441ms step_avg:nanms
step:3/1395 train_time:24560ms step_avg:nanms
step:4/1395 train_time:24682ms step_avg:nanms
step:5/1395 train_time:24804ms step_avg:nanms
step:6/1395 train_time:24926ms step_avg:nanms
step:7/1395 train_time:25048ms step_avg:nanms
step:8/1395 train_time:25171ms step_avg:nanms
step:9/1395 train_time:25294ms step_avg:nanms
step:10/1395 train_time:25417ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:247ms step_avg:nanms
step:13/1395 train_time:371ms step_avg:123.73ms
step:14/1395 train_time:495ms step_avg:123.81ms
step:15/1395 train_time:618ms step_avg:123.60ms
step:16/1395 train_time:742ms step_avg:123.62ms
step:17/1395 train_time:866ms step_avg:123.71ms
step:18/1395 train_time:989ms step_avg:123.61ms
step:19/1395 train_time:1112ms step_avg:123.51ms
step:20/1395 train_time:1235ms step_avg:123.49ms
step:21/1395 train_time:1358ms step_avg:123.47ms
step:22/1395 train_time:1482ms step_avg:123.48ms
step:23/1395 train_time:1604ms step_avg:123.42ms
step:24/1395 train_time:1728ms step_avg:123.45ms
step:25/1395 train_time:1851ms step_avg:123.40ms
step:26/1395 train_time:1976ms step_avg:123.51ms
step:27/1395 train_time:2099ms step_avg:123.48ms
step:28/1395 train_time:2223ms step_avg:123.49ms
step:29/1395 train_time:2348ms step_avg:123.60ms
step:30/1395 train_time:2471ms step_avg:123.57ms
step:31/1395 train_time:2595ms step_avg:123.56ms
step:32/1395 train_time:2718ms step_avg:123.54ms
step:33/1395 train_time:2842ms step_avg:123.55ms
step:34/1395 train_time:2965ms step_avg:123.55ms
step:35/1395 train_time:3087ms step_avg:123.50ms
step:36/1395 train_time:3211ms step_avg:123.50ms
step:37/1395 train_time:3334ms step_avg:123.49ms
step:38/1395 train_time:3458ms step_avg:123.50ms
step:39/1395 train_time:3582ms step_avg:123.51ms
step:40/1395 train_time:3705ms step_avg:123.49ms
step:41/1395 train_time:3829ms step_avg:123.53ms
step:42/1395 train_time:3953ms step_avg:123.52ms
step:43/1395 train_time:4076ms step_avg:123.51ms
step:44/1395 train_time:4199ms step_avg:123.49ms
step:45/1395 train_time:4322ms step_avg:123.48ms
step:46/1395 train_time:4445ms step_avg:123.47ms
step:47/1395 train_time:4569ms step_avg:123.48ms
step:48/1395 train_time:4691ms step_avg:123.45ms
step:49/1395 train_time:4814ms step_avg:123.44ms
step:50/1395 train_time:4939ms step_avg:123.47ms
step:51/1395 train_time:5062ms step_avg:123.47ms
step:52/1395 train_time:5187ms step_avg:123.49ms
step:53/1395 train_time:5310ms step_avg:123.48ms
step:54/1395 train_time:5432ms step_avg:123.47ms
step:55/1395 train_time:5556ms step_avg:123.47ms
step:56/1395 train_time:5679ms step_avg:123.45ms
step:57/1395 train_time:5802ms step_avg:123.45ms
step:58/1395 train_time:5927ms step_avg:123.47ms
step:59/1395 train_time:6050ms step_avg:123.47ms
step:60/1395 train_time:6173ms step_avg:123.46ms
step:61/1395 train_time:6297ms step_avg:123.46ms
step:62/1395 train_time:6421ms step_avg:123.48ms
step:63/1395 train_time:6544ms step_avg:123.48ms
step:64/1395 train_time:6668ms step_avg:123.48ms
step:65/1395 train_time:6791ms step_avg:123.47ms
step:66/1395 train_time:6914ms step_avg:123.46ms
step:67/1395 train_time:7037ms step_avg:123.46ms
step:68/1395 train_time:7160ms step_avg:123.45ms
step:69/1395 train_time:7283ms step_avg:123.44ms
step:70/1395 train_time:7406ms step_avg:123.44ms
step:71/1395 train_time:7530ms step_avg:123.44ms
step:72/1395 train_time:7654ms step_avg:123.45ms
step:73/1395 train_time:7778ms step_avg:123.46ms
step:74/1395 train_time:7902ms step_avg:123.47ms
step:75/1395 train_time:8026ms step_avg:123.47ms
step:76/1395 train_time:8149ms step_avg:123.47ms
step:77/1395 train_time:8271ms step_avg:123.45ms
step:78/1395 train_time:8394ms step_avg:123.45ms
step:79/1395 train_time:8518ms step_avg:123.45ms
step:80/1395 train_time:8641ms step_avg:123.45ms
step:81/1395 train_time:8765ms step_avg:123.45ms
step:82/1395 train_time:8888ms step_avg:123.45ms
step:83/1395 train_time:9013ms step_avg:123.47ms
step:84/1395 train_time:9136ms step_avg:123.46ms
step:85/1395 train_time:9259ms step_avg:123.45ms
step:86/1395 train_time:9382ms step_avg:123.45ms
step:87/1395 train_time:9506ms step_avg:123.45ms
step:88/1395 train_time:9629ms step_avg:123.45ms
step:89/1395 train_time:9751ms step_avg:123.44ms
step:90/1395 train_time:9875ms step_avg:123.44ms
step:91/1395 train_time:9998ms step_avg:123.43ms
step:92/1395 train_time:10121ms step_avg:123.43ms
step:93/1395 train_time:10244ms step_avg:123.42ms
step:94/1395 train_time:10368ms step_avg:123.42ms
step:95/1395 train_time:10491ms step_avg:123.43ms
step:96/1395 train_time:10615ms step_avg:123.43ms
step:97/1395 train_time:10738ms step_avg:123.43ms
step:98/1395 train_time:10862ms step_avg:123.43ms
step:99/1395 train_time:10985ms step_avg:123.42ms
step:100/1395 train_time:11107ms step_avg:123.42ms
step:101/1395 train_time:11233ms step_avg:123.44ms
step:102/1395 train_time:11358ms step_avg:123.46ms
step:103/1395 train_time:11482ms step_avg:123.46ms
step:104/1395 train_time:11606ms step_avg:123.46ms
step:105/1395 train_time:11730ms step_avg:123.47ms
step:106/1395 train_time:11856ms step_avg:123.50ms
step:107/1395 train_time:11982ms step_avg:123.53ms
step:108/1395 train_time:12108ms step_avg:123.55ms
step:109/1395 train_time:12233ms step_avg:123.57ms
step:110/1395 train_time:12359ms step_avg:123.59ms
step:111/1395 train_time:12485ms step_avg:123.62ms
step:112/1395 train_time:12612ms step_avg:123.65ms
step:113/1395 train_time:12738ms step_avg:123.67ms
step:114/1395 train_time:12864ms step_avg:123.69ms
step:115/1395 train_time:12990ms step_avg:123.71ms
step:116/1395 train_time:13116ms step_avg:123.74ms
step:117/1395 train_time:13242ms step_avg:123.75ms
step:118/1395 train_time:13367ms step_avg:123.77ms
step:119/1395 train_time:13494ms step_avg:123.79ms
step:120/1395 train_time:13620ms step_avg:123.82ms
step:121/1395 train_time:13746ms step_avg:123.84ms
step:122/1395 train_time:13872ms step_avg:123.86ms
step:123/1395 train_time:13998ms step_avg:123.88ms
step:124/1395 train_time:14124ms step_avg:123.90ms
step:125/1395 train_time:14250ms step_avg:123.91ms
step:125/1395 val_loss:4.3518 train_time:14351ms step_avg:124.79ms
step:126/1395 train_time:14379ms step_avg:123.96ms
step:127/1395 train_time:14517ms step_avg:124.08ms
step:128/1395 train_time:14647ms step_avg:124.13ms
step:129/1395 train_time:14772ms step_avg:124.14ms
step:130/1395 train_time:14898ms step_avg:124.15ms
step:131/1395 train_time:15024ms step_avg:124.17ms
step:132/1395 train_time:15150ms step_avg:124.18ms
step:133/1395 train_time:15275ms step_avg:124.19ms
step:134/1395 train_time:15402ms step_avg:124.21ms
step:135/1395 train_time:15529ms step_avg:124.23ms
step:136/1395 train_time:15655ms step_avg:124.24ms
step:137/1395 train_time:15781ms step_avg:124.26ms
step:138/1395 train_time:15908ms step_avg:124.28ms
step:139/1395 train_time:16034ms step_avg:124.29ms
step:140/1395 train_time:16159ms step_avg:124.30ms
step:141/1395 train_time:16286ms step_avg:124.32ms
step:142/1395 train_time:16411ms step_avg:124.33ms
step:143/1395 train_time:16538ms step_avg:124.34ms
step:144/1395 train_time:16664ms step_avg:124.36ms
step:145/1395 train_time:16790ms step_avg:124.37ms
step:146/1395 train_time:16917ms step_avg:124.39ms
step:147/1395 train_time:17043ms step_avg:124.40ms
step:148/1395 train_time:17170ms step_avg:124.42ms
step:149/1395 train_time:17296ms step_avg:124.43ms
step:150/1395 train_time:17422ms step_avg:124.44ms
step:151/1395 train_time:17548ms step_avg:124.46ms
step:152/1395 train_time:17673ms step_avg:124.46ms
step:153/1395 train_time:17798ms step_avg:124.46ms
step:154/1395 train_time:17924ms step_avg:124.47ms
step:155/1395 train_time:18050ms step_avg:124.48ms
step:156/1395 train_time:18175ms step_avg:124.49ms
step:157/1395 train_time:18302ms step_avg:124.50ms
step:158/1395 train_time:18428ms step_avg:124.51ms
step:159/1395 train_time:18554ms step_avg:124.52ms
step:160/1395 train_time:18681ms step_avg:124.54ms
step:161/1395 train_time:18807ms step_avg:124.55ms
step:162/1395 train_time:18932ms step_avg:124.55ms
step:163/1395 train_time:19057ms step_avg:124.56ms
step:164/1395 train_time:19183ms step_avg:124.57ms
step:165/1395 train_time:19309ms step_avg:124.57ms
step:166/1395 train_time:19434ms step_avg:124.58ms
step:167/1395 train_time:19560ms step_avg:124.59ms
step:168/1395 train_time:19686ms step_avg:124.60ms
step:169/1395 train_time:19812ms step_avg:124.61ms
step:170/1395 train_time:19938ms step_avg:124.61ms
step:171/1395 train_time:20064ms step_avg:124.62ms
step:172/1395 train_time:20190ms step_avg:124.63ms
step:173/1395 train_time:20317ms step_avg:124.64ms
step:174/1395 train_time:20443ms step_avg:124.65ms
step:175/1395 train_time:20569ms step_avg:124.66ms
step:176/1395 train_time:20695ms step_avg:124.67ms
step:177/1395 train_time:20822ms step_avg:124.68ms
step:178/1395 train_time:20949ms step_avg:124.69ms
step:179/1395 train_time:21074ms step_avg:124.70ms
step:180/1395 train_time:21201ms step_avg:124.71ms
step:181/1395 train_time:21328ms step_avg:124.72ms
step:182/1395 train_time:21454ms step_avg:124.73ms
step:183/1395 train_time:21580ms step_avg:124.74ms
step:184/1395 train_time:21706ms step_avg:124.75ms
step:185/1395 train_time:21831ms step_avg:124.75ms
step:186/1395 train_time:21957ms step_avg:124.76ms
step:187/1395 train_time:22084ms step_avg:124.77ms
step:188/1395 train_time:22210ms step_avg:124.78ms
step:189/1395 train_time:22335ms step_avg:124.78ms
step:190/1395 train_time:22461ms step_avg:124.78ms
step:191/1395 train_time:22587ms step_avg:124.79ms
step:192/1395 train_time:22713ms step_avg:124.80ms
step:193/1395 train_time:22839ms step_avg:124.80ms
step:194/1395 train_time:22965ms step_avg:124.81ms
step:195/1395 train_time:23091ms step_avg:124.82ms
step:196/1395 train_time:23217ms step_avg:124.82ms
step:197/1395 train_time:23343ms step_avg:124.83ms
step:198/1395 train_time:23469ms step_avg:124.84ms
step:199/1395 train_time:23595ms step_avg:124.84ms
step:200/1395 train_time:23720ms step_avg:124.84ms
step:201/1395 train_time:23847ms step_avg:124.85ms
step:202/1395 train_time:23972ms step_avg:124.85ms
step:203/1395 train_time:24098ms step_avg:124.86ms
step:204/1395 train_time:24225ms step_avg:124.87ms
step:205/1395 train_time:24351ms step_avg:124.88ms
step:206/1395 train_time:24477ms step_avg:124.88ms
step:207/1395 train_time:24602ms step_avg:124.88ms
step:208/1395 train_time:24729ms step_avg:124.89ms
step:209/1395 train_time:24857ms step_avg:124.91ms
step:210/1395 train_time:24985ms step_avg:124.93ms
step:211/1395 train_time:25114ms step_avg:124.94ms
step:212/1395 train_time:25242ms step_avg:124.96ms
step:213/1395 train_time:25371ms step_avg:124.98ms
step:214/1395 train_time:25499ms step_avg:125.00ms
step:215/1395 train_time:25628ms step_avg:125.01ms
step:216/1395 train_time:25756ms step_avg:125.03ms
step:217/1395 train_time:25885ms step_avg:125.05ms
step:218/1395 train_time:26013ms step_avg:125.06ms
step:219/1395 train_time:26141ms step_avg:125.08ms
step:220/1395 train_time:26270ms step_avg:125.09ms
step:221/1395 train_time:26398ms step_avg:125.11ms
step:222/1395 train_time:26528ms step_avg:125.13ms
step:223/1395 train_time:26656ms step_avg:125.14ms
step:224/1395 train_time:26784ms step_avg:125.16ms
step:225/1395 train_time:26912ms step_avg:125.17ms
step:226/1395 train_time:27041ms step_avg:125.19ms
step:227/1395 train_time:27169ms step_avg:125.20ms
step:228/1395 train_time:27297ms step_avg:125.21ms
step:229/1395 train_time:27425ms step_avg:125.23ms
step:230/1395 train_time:27553ms step_avg:125.24ms
step:231/1395 train_time:27680ms step_avg:125.25ms
step:232/1395 train_time:27809ms step_avg:125.27ms
step:233/1395 train_time:27937ms step_avg:125.28ms
step:234/1395 train_time:28067ms step_avg:125.30ms
step:235/1395 train_time:28195ms step_avg:125.31ms
step:236/1395 train_time:28323ms step_avg:125.32ms
step:237/1395 train_time:28451ms step_avg:125.33ms
step:238/1395 train_time:28579ms step_avg:125.35ms
step:239/1395 train_time:28708ms step_avg:125.36ms
step:240/1395 train_time:28836ms step_avg:125.37ms
step:241/1395 train_time:28964ms step_avg:125.38ms
step:242/1395 train_time:29093ms step_avg:125.40ms
step:243/1395 train_time:29221ms step_avg:125.41ms
step:244/1395 train_time:29349ms step_avg:125.42ms
step:245/1395 train_time:29478ms step_avg:125.44ms
step:246/1395 train_time:29606ms step_avg:125.45ms
step:247/1395 train_time:29734ms step_avg:125.46ms
step:248/1395 train_time:29863ms step_avg:125.47ms
step:249/1395 train_time:29992ms step_avg:125.49ms
step:250/1395 train_time:30120ms step_avg:125.50ms
step:250/1395 val_loss:3.9528 train_time:30224ms step_avg:125.93ms
step:251/1395 train_time:30253ms step_avg:125.53ms
step:252/1395 train_time:30389ms step_avg:125.58ms
step:253/1395 train_time:30519ms step_avg:125.59ms
step:254/1395 train_time:30647ms step_avg:125.60ms
step:255/1395 train_time:30775ms step_avg:125.61ms
step:256/1395 train_time:30902ms step_avg:125.62ms
step:257/1395 train_time:31031ms step_avg:125.63ms
step:258/1395 train_time:31158ms step_avg:125.64ms
step:259/1395 train_time:31287ms step_avg:125.65ms
step:260/1395 train_time:31416ms step_avg:125.66ms
step:261/1395 train_time:31544ms step_avg:125.68ms
step:262/1395 train_time:31674ms step_avg:125.69ms
step:263/1395 train_time:31803ms step_avg:125.70ms
step:264/1395 train_time:31931ms step_avg:125.71ms
step:265/1395 train_time:32059ms step_avg:125.72ms
step:266/1395 train_time:32186ms step_avg:125.73ms
step:267/1395 train_time:32315ms step_avg:125.74ms
step:268/1395 train_time:32443ms step_avg:125.75ms
step:269/1395 train_time:32572ms step_avg:125.76ms
step:270/1395 train_time:32699ms step_avg:125.77ms
step:271/1395 train_time:32827ms step_avg:125.77ms
step:272/1395 train_time:32956ms step_avg:125.78ms
step:273/1395 train_time:33084ms step_avg:125.79ms
step:274/1395 train_time:33213ms step_avg:125.81ms
step:275/1395 train_time:33341ms step_avg:125.82ms
step:276/1395 train_time:33471ms step_avg:125.83ms
step:277/1395 train_time:33599ms step_avg:125.84ms
step:278/1395 train_time:33728ms step_avg:125.85ms
step:279/1395 train_time:33856ms step_avg:125.86ms
step:280/1395 train_time:33985ms step_avg:125.87ms
step:281/1395 train_time:34113ms step_avg:125.88ms
step:282/1395 train_time:34241ms step_avg:125.89ms
step:283/1395 train_time:34371ms step_avg:125.90ms
step:284/1395 train_time:34499ms step_avg:125.91ms
step:285/1395 train_time:34627ms step_avg:125.92ms
step:286/1395 train_time:34755ms step_avg:125.93ms
step:287/1395 train_time:34883ms step_avg:125.93ms
step:288/1395 train_time:35012ms step_avg:125.94ms
step:289/1395 train_time:35140ms step_avg:125.95ms
step:290/1395 train_time:35268ms step_avg:125.96ms
step:291/1395 train_time:35397ms step_avg:125.97ms
step:292/1395 train_time:35526ms step_avg:125.98ms
step:293/1395 train_time:35654ms step_avg:125.99ms
step:294/1395 train_time:35782ms step_avg:125.99ms
step:295/1395 train_time:35912ms step_avg:126.01ms
step:296/1395 train_time:36040ms step_avg:126.01ms
step:297/1395 train_time:36169ms step_avg:126.02ms
step:298/1395 train_time:36297ms step_avg:126.03ms
step:299/1395 train_time:36425ms step_avg:126.04ms
step:300/1395 train_time:36553ms step_avg:126.05ms
step:301/1395 train_time:36682ms step_avg:126.05ms
step:302/1395 train_time:36812ms step_avg:126.07ms
step:303/1395 train_time:36939ms step_avg:126.07ms
step:304/1395 train_time:37068ms step_avg:126.08ms
step:305/1395 train_time:37196ms step_avg:126.09ms
step:306/1395 train_time:37325ms step_avg:126.10ms
step:307/1395 train_time:37453ms step_avg:126.11ms
step:308/1395 train_time:37582ms step_avg:126.11ms
step:309/1395 train_time:37710ms step_avg:126.12ms
step:310/1395 train_time:37838ms step_avg:126.13ms
step:311/1395 train_time:37968ms step_avg:126.14ms
step:312/1395 train_time:38096ms step_avg:126.15ms
step:313/1395 train_time:38227ms step_avg:126.16ms
step:314/1395 train_time:38358ms step_avg:126.18ms
step:315/1395 train_time:38487ms step_avg:126.19ms
step:316/1395 train_time:38618ms step_avg:126.20ms
step:317/1395 train_time:38749ms step_avg:126.22ms
step:318/1395 train_time:38880ms step_avg:126.23ms
step:319/1395 train_time:39012ms step_avg:126.25ms
step:320/1395 train_time:39142ms step_avg:126.26ms
step:321/1395 train_time:39272ms step_avg:126.28ms
step:322/1395 train_time:39403ms step_avg:126.29ms
step:323/1395 train_time:39533ms step_avg:126.30ms
step:324/1395 train_time:39663ms step_avg:126.32ms
step:325/1395 train_time:39794ms step_avg:126.33ms
step:326/1395 train_time:39925ms step_avg:126.34ms
step:327/1395 train_time:40056ms step_avg:126.36ms
step:328/1395 train_time:40186ms step_avg:126.37ms
step:329/1395 train_time:40317ms step_avg:126.39ms
step:330/1395 train_time:40447ms step_avg:126.40ms
step:331/1395 train_time:40577ms step_avg:126.41ms
step:332/1395 train_time:40708ms step_avg:126.42ms
step:333/1395 train_time:40839ms step_avg:126.44ms
step:334/1395 train_time:40970ms step_avg:126.45ms
step:335/1395 train_time:41100ms step_avg:126.46ms
step:336/1395 train_time:41230ms step_avg:126.47ms
step:337/1395 train_time:41361ms step_avg:126.49ms
step:338/1395 train_time:41490ms step_avg:126.50ms
step:339/1395 train_time:41623ms step_avg:126.51ms
step:340/1395 train_time:41753ms step_avg:126.53ms
step:341/1395 train_time:41884ms step_avg:126.54ms
step:342/1395 train_time:42014ms step_avg:126.55ms
step:343/1395 train_time:42145ms step_avg:126.56ms
step:344/1395 train_time:42274ms step_avg:126.57ms
step:345/1395 train_time:42405ms step_avg:126.58ms
step:346/1395 train_time:42536ms step_avg:126.60ms
step:347/1395 train_time:42668ms step_avg:126.61ms
step:348/1395 train_time:42799ms step_avg:126.62ms
step:349/1395 train_time:42929ms step_avg:126.63ms
step:350/1395 train_time:43059ms step_avg:126.65ms
step:351/1395 train_time:43190ms step_avg:126.66ms
step:352/1395 train_time:43321ms step_avg:126.67ms
step:353/1395 train_time:43451ms step_avg:126.68ms
step:354/1395 train_time:43581ms step_avg:126.69ms
step:355/1395 train_time:43711ms step_avg:126.70ms
step:356/1395 train_time:43842ms step_avg:126.71ms
step:357/1395 train_time:43974ms step_avg:126.73ms
step:358/1395 train_time:44104ms step_avg:126.73ms
step:359/1395 train_time:44235ms step_avg:126.75ms
step:360/1395 train_time:44365ms step_avg:126.76ms
step:361/1395 train_time:44496ms step_avg:126.77ms
step:362/1395 train_time:44627ms step_avg:126.78ms
step:363/1395 train_time:44758ms step_avg:126.79ms
step:364/1395 train_time:44888ms step_avg:126.80ms
step:365/1395 train_time:45019ms step_avg:126.81ms
step:366/1395 train_time:45150ms step_avg:126.83ms
step:367/1395 train_time:45281ms step_avg:126.84ms
step:368/1395 train_time:45411ms step_avg:126.85ms
step:369/1395 train_time:45541ms step_avg:126.85ms
step:370/1395 train_time:45671ms step_avg:126.86ms
step:371/1395 train_time:45802ms step_avg:126.87ms
step:372/1395 train_time:45933ms step_avg:126.89ms
step:373/1395 train_time:46063ms step_avg:126.90ms
step:374/1395 train_time:46194ms step_avg:126.91ms
step:375/1395 train_time:46324ms step_avg:126.92ms
step:375/1395 val_loss:3.7749 train_time:46429ms step_avg:127.20ms
step:376/1395 train_time:46461ms step_avg:126.94ms
step:377/1395 train_time:46599ms step_avg:126.97ms
step:378/1395 train_time:46730ms step_avg:126.98ms
step:379/1395 train_time:46859ms step_avg:126.99ms
step:380/1395 train_time:46990ms step_avg:127.00ms
step:381/1395 train_time:47119ms step_avg:127.01ms
step:382/1395 train_time:47249ms step_avg:127.01ms
step:383/1395 train_time:47379ms step_avg:127.02ms
step:384/1395 train_time:47509ms step_avg:127.03ms
step:385/1395 train_time:47640ms step_avg:127.04ms
step:386/1395 train_time:47772ms step_avg:127.05ms
step:387/1395 train_time:47903ms step_avg:127.06ms
step:388/1395 train_time:48033ms step_avg:127.07ms
step:389/1395 train_time:48164ms step_avg:127.08ms
step:390/1395 train_time:48294ms step_avg:127.09ms
step:391/1395 train_time:48425ms step_avg:127.10ms
step:392/1395 train_time:48556ms step_avg:127.11ms
step:393/1395 train_time:48686ms step_avg:127.12ms
step:394/1395 train_time:48816ms step_avg:127.13ms
step:395/1395 train_time:48946ms step_avg:127.13ms
step:396/1395 train_time:49076ms step_avg:127.14ms
step:397/1395 train_time:49207ms step_avg:127.15ms
step:398/1395 train_time:49337ms step_avg:127.16ms
step:399/1395 train_time:49467ms step_avg:127.16ms
step:400/1395 train_time:49596ms step_avg:127.17ms
step:401/1395 train_time:49728ms step_avg:127.18ms
step:402/1395 train_time:49858ms step_avg:127.19ms
step:403/1395 train_time:49989ms step_avg:127.20ms
step:404/1395 train_time:50118ms step_avg:127.20ms
step:405/1395 train_time:50248ms step_avg:127.21ms
step:406/1395 train_time:50379ms step_avg:127.22ms
step:407/1395 train_time:50509ms step_avg:127.23ms
step:408/1395 train_time:50639ms step_avg:127.23ms
step:409/1395 train_time:50770ms step_avg:127.24ms
step:410/1395 train_time:50900ms step_avg:127.25ms
step:411/1395 train_time:51030ms step_avg:127.26ms
step:412/1395 train_time:51160ms step_avg:127.26ms
step:413/1395 train_time:51289ms step_avg:127.27ms
step:414/1395 train_time:51419ms step_avg:127.28ms
step:415/1395 train_time:51550ms step_avg:127.28ms
step:416/1395 train_time:51682ms step_avg:127.30ms
step:417/1395 train_time:51815ms step_avg:127.31ms
step:418/1395 train_time:51948ms step_avg:127.32ms
step:419/1395 train_time:52080ms step_avg:127.34ms
step:420/1395 train_time:52213ms step_avg:127.35ms
step:421/1395 train_time:52346ms step_avg:127.36ms
step:422/1395 train_time:52478ms step_avg:127.37ms
step:423/1395 train_time:52612ms step_avg:127.39ms
step:424/1395 train_time:52745ms step_avg:127.40ms
step:425/1395 train_time:52878ms step_avg:127.42ms
step:426/1395 train_time:53010ms step_avg:127.43ms
step:427/1395 train_time:53143ms step_avg:127.44ms
step:428/1395 train_time:53275ms step_avg:127.45ms
step:429/1395 train_time:53408ms step_avg:127.46ms
step:430/1395 train_time:53540ms step_avg:127.48ms
step:431/1395 train_time:53672ms step_avg:127.49ms
step:432/1395 train_time:53806ms step_avg:127.50ms
step:433/1395 train_time:53938ms step_avg:127.51ms
step:434/1395 train_time:54070ms step_avg:127.52ms
step:435/1395 train_time:54202ms step_avg:127.54ms
step:436/1395 train_time:54334ms step_avg:127.55ms
step:437/1395 train_time:54467ms step_avg:127.56ms
step:438/1395 train_time:54599ms step_avg:127.57ms
step:439/1395 train_time:54731ms step_avg:127.58ms
step:440/1395 train_time:54865ms step_avg:127.59ms
step:441/1395 train_time:54998ms step_avg:127.60ms
step:442/1395 train_time:55129ms step_avg:127.61ms
step:443/1395 train_time:55262ms step_avg:127.63ms
step:444/1395 train_time:55394ms step_avg:127.64ms
step:445/1395 train_time:55527ms step_avg:127.65ms
step:446/1395 train_time:55659ms step_avg:127.66ms
step:447/1395 train_time:55792ms step_avg:127.67ms
step:448/1395 train_time:55925ms step_avg:127.68ms
step:449/1395 train_time:56057ms step_avg:127.69ms
step:450/1395 train_time:56189ms step_avg:127.70ms
step:451/1395 train_time:56321ms step_avg:127.71ms
step:452/1395 train_time:56452ms step_avg:127.72ms
step:453/1395 train_time:56585ms step_avg:127.73ms
step:454/1395 train_time:56716ms step_avg:127.74ms
step:455/1395 train_time:56849ms step_avg:127.75ms
step:456/1395 train_time:56982ms step_avg:127.76ms
step:457/1395 train_time:57114ms step_avg:127.77ms
step:458/1395 train_time:57246ms step_avg:127.78ms
step:459/1395 train_time:57379ms step_avg:127.79ms
step:460/1395 train_time:57510ms step_avg:127.80ms
step:461/1395 train_time:57642ms step_avg:127.81ms
step:462/1395 train_time:57775ms step_avg:127.82ms
step:463/1395 train_time:57908ms step_avg:127.83ms
step:464/1395 train_time:58041ms step_avg:127.84ms
step:465/1395 train_time:58172ms step_avg:127.85ms
step:466/1395 train_time:58305ms step_avg:127.86ms
step:467/1395 train_time:58437ms step_avg:127.87ms
step:468/1395 train_time:58568ms step_avg:127.88ms
step:469/1395 train_time:58701ms step_avg:127.89ms
step:470/1395 train_time:58833ms step_avg:127.90ms
step:471/1395 train_time:58967ms step_avg:127.91ms
step:472/1395 train_time:59099ms step_avg:127.92ms
step:473/1395 train_time:59231ms step_avg:127.93ms
step:474/1395 train_time:59363ms step_avg:127.94ms
step:475/1395 train_time:59495ms step_avg:127.95ms
step:476/1395 train_time:59627ms step_avg:127.96ms
step:477/1395 train_time:59759ms step_avg:127.96ms
step:478/1395 train_time:59891ms step_avg:127.97ms
step:479/1395 train_time:60023ms step_avg:127.98ms
step:480/1395 train_time:60155ms step_avg:127.99ms
step:481/1395 train_time:60288ms step_avg:128.00ms
step:482/1395 train_time:60420ms step_avg:128.01ms
step:483/1395 train_time:60552ms step_avg:128.02ms
step:484/1395 train_time:60685ms step_avg:128.03ms
step:485/1395 train_time:60817ms step_avg:128.03ms
step:486/1395 train_time:60949ms step_avg:128.04ms
step:487/1395 train_time:61083ms step_avg:128.06ms
step:488/1395 train_time:61214ms step_avg:128.06ms
step:489/1395 train_time:61346ms step_avg:128.07ms
step:490/1395 train_time:61479ms step_avg:128.08ms
step:491/1395 train_time:61610ms step_avg:128.09ms
step:492/1395 train_time:61743ms step_avg:128.10ms
step:493/1395 train_time:61875ms step_avg:128.11ms
step:494/1395 train_time:62007ms step_avg:128.11ms
step:495/1395 train_time:62140ms step_avg:128.12ms
step:496/1395 train_time:62273ms step_avg:128.13ms
step:497/1395 train_time:62404ms step_avg:128.14ms
step:498/1395 train_time:62537ms step_avg:128.15ms
step:499/1395 train_time:62669ms step_avg:128.16ms
step:500/1395 train_time:62801ms step_avg:128.17ms
step:500/1395 val_loss:3.6596 train_time:62906ms step_avg:128.38ms
step:501/1395 train_time:62937ms step_avg:128.18ms
step:502/1395 train_time:63073ms step_avg:128.20ms
step:503/1395 train_time:63207ms step_avg:128.21ms
step:504/1395 train_time:63337ms step_avg:128.21ms
step:505/1395 train_time:63469ms step_avg:128.22ms
step:506/1395 train_time:63601ms step_avg:128.23ms
step:507/1395 train_time:63733ms step_avg:128.24ms
step:508/1395 train_time:63866ms step_avg:128.24ms
step:509/1395 train_time:64000ms step_avg:128.26ms
step:510/1395 train_time:64132ms step_avg:128.26ms
step:511/1395 train_time:64265ms step_avg:128.27ms
step:512/1395 train_time:64397ms step_avg:128.28ms
step:513/1395 train_time:64530ms step_avg:128.29ms
step:514/1395 train_time:64664ms step_avg:128.30ms
step:515/1395 train_time:64795ms step_avg:128.31ms
step:516/1395 train_time:64928ms step_avg:128.32ms
step:517/1395 train_time:65061ms step_avg:128.33ms
step:518/1395 train_time:65193ms step_avg:128.33ms
step:519/1395 train_time:65327ms step_avg:128.34ms
step:520/1395 train_time:65461ms step_avg:128.35ms
step:521/1395 train_time:65594ms step_avg:128.36ms
step:522/1395 train_time:65727ms step_avg:128.37ms
step:523/1395 train_time:65863ms step_avg:128.39ms
step:524/1395 train_time:65996ms step_avg:128.40ms
step:525/1395 train_time:66129ms step_avg:128.41ms
step:526/1395 train_time:66263ms step_avg:128.42ms
step:527/1395 train_time:66396ms step_avg:128.43ms
step:528/1395 train_time:66530ms step_avg:128.44ms
step:529/1395 train_time:66665ms step_avg:128.45ms
step:530/1395 train_time:66798ms step_avg:128.46ms
step:531/1395 train_time:66932ms step_avg:128.47ms
step:532/1395 train_time:67066ms step_avg:128.48ms
step:533/1395 train_time:67202ms step_avg:128.49ms
step:534/1395 train_time:67335ms step_avg:128.50ms
step:535/1395 train_time:67469ms step_avg:128.51ms
step:536/1395 train_time:67604ms step_avg:128.52ms
step:537/1395 train_time:67736ms step_avg:128.53ms
step:538/1395 train_time:67869ms step_avg:128.54ms
step:539/1395 train_time:68004ms step_avg:128.55ms
step:540/1395 train_time:68138ms step_avg:128.56ms
step:541/1395 train_time:68271ms step_avg:128.57ms
step:542/1395 train_time:68404ms step_avg:128.58ms
step:543/1395 train_time:68537ms step_avg:128.59ms
step:544/1395 train_time:68670ms step_avg:128.60ms
step:545/1395 train_time:68804ms step_avg:128.61ms
step:546/1395 train_time:68938ms step_avg:128.62ms
step:547/1395 train_time:69071ms step_avg:128.62ms
step:548/1395 train_time:69207ms step_avg:128.64ms
step:549/1395 train_time:69340ms step_avg:128.65ms
step:550/1395 train_time:69475ms step_avg:128.66ms
step:551/1395 train_time:69609ms step_avg:128.67ms
step:552/1395 train_time:69742ms step_avg:128.68ms
step:553/1395 train_time:69876ms step_avg:128.69ms
step:554/1395 train_time:70010ms step_avg:128.69ms
step:555/1395 train_time:70146ms step_avg:128.71ms
step:556/1395 train_time:70278ms step_avg:128.71ms
step:557/1395 train_time:70413ms step_avg:128.73ms
step:558/1395 train_time:70547ms step_avg:128.74ms
step:559/1395 train_time:70680ms step_avg:128.74ms
step:560/1395 train_time:70813ms step_avg:128.75ms
step:561/1395 train_time:70946ms step_avg:128.76ms
step:562/1395 train_time:71079ms step_avg:128.77ms
step:563/1395 train_time:71214ms step_avg:128.78ms
step:564/1395 train_time:71348ms step_avg:128.79ms
step:565/1395 train_time:71482ms step_avg:128.80ms
step:566/1395 train_time:71616ms step_avg:128.81ms
step:567/1395 train_time:71749ms step_avg:128.81ms
step:568/1395 train_time:71883ms step_avg:128.82ms
step:569/1395 train_time:72017ms step_avg:128.83ms
step:570/1395 train_time:72150ms step_avg:128.84ms
step:571/1395 train_time:72284ms step_avg:128.85ms
step:572/1395 train_time:72418ms step_avg:128.86ms
step:573/1395 train_time:72552ms step_avg:128.87ms
step:574/1395 train_time:72688ms step_avg:128.88ms
step:575/1395 train_time:72821ms step_avg:128.89ms
step:576/1395 train_time:72954ms step_avg:128.89ms
step:577/1395 train_time:73088ms step_avg:128.90ms
step:578/1395 train_time:73222ms step_avg:128.91ms
step:579/1395 train_time:73356ms step_avg:128.92ms
step:580/1395 train_time:73490ms step_avg:128.93ms
step:581/1395 train_time:73626ms step_avg:128.94ms
step:582/1395 train_time:73759ms step_avg:128.95ms
step:583/1395 train_time:73893ms step_avg:128.96ms
step:584/1395 train_time:74027ms step_avg:128.97ms
step:585/1395 train_time:74159ms step_avg:128.97ms
step:586/1395 train_time:74293ms step_avg:128.98ms
step:587/1395 train_time:74427ms step_avg:128.99ms
step:588/1395 train_time:74562ms step_avg:129.00ms
step:589/1395 train_time:74695ms step_avg:129.01ms
step:590/1395 train_time:74829ms step_avg:129.02ms
step:591/1395 train_time:74962ms step_avg:129.02ms
step:592/1395 train_time:75097ms step_avg:129.03ms
step:593/1395 train_time:75231ms step_avg:129.04ms
step:594/1395 train_time:75365ms step_avg:129.05ms
step:595/1395 train_time:75501ms step_avg:129.06ms
step:596/1395 train_time:75635ms step_avg:129.07ms
step:597/1395 train_time:75768ms step_avg:129.08ms
step:598/1395 train_time:75901ms step_avg:129.08ms
step:599/1395 train_time:76035ms step_avg:129.09ms
step:600/1395 train_time:76168ms step_avg:129.10ms
step:601/1395 train_time:76302ms step_avg:129.11ms
step:602/1395 train_time:76435ms step_avg:129.11ms
step:603/1395 train_time:76569ms step_avg:129.12ms
step:604/1395 train_time:76703ms step_avg:129.13ms
step:605/1395 train_time:76838ms step_avg:129.14ms
step:606/1395 train_time:76972ms step_avg:129.15ms
step:607/1395 train_time:77106ms step_avg:129.15ms
step:608/1395 train_time:77240ms step_avg:129.16ms
step:609/1395 train_time:77374ms step_avg:129.17ms
step:610/1395 train_time:77506ms step_avg:129.18ms
step:611/1395 train_time:77639ms step_avg:129.18ms
step:612/1395 train_time:77772ms step_avg:129.19ms
step:613/1395 train_time:77907ms step_avg:129.20ms
step:614/1395 train_time:78041ms step_avg:129.21ms
step:615/1395 train_time:78174ms step_avg:129.21ms
step:616/1395 train_time:78307ms step_avg:129.22ms
step:617/1395 train_time:78441ms step_avg:129.23ms
step:618/1395 train_time:78575ms step_avg:129.24ms
step:619/1395 train_time:78708ms step_avg:129.24ms
step:620/1395 train_time:78842ms step_avg:129.25ms
step:621/1395 train_time:78976ms step_avg:129.26ms
step:622/1395 train_time:79110ms step_avg:129.26ms
step:623/1395 train_time:79246ms step_avg:129.28ms
step:624/1395 train_time:79381ms step_avg:129.29ms
step:625/1395 train_time:79516ms step_avg:129.29ms
step:625/1395 val_loss:3.5793 train_time:79626ms step_avg:129.47ms
step:626/1395 train_time:79656ms step_avg:129.31ms
step:627/1395 train_time:79796ms step_avg:129.33ms
step:628/1395 train_time:79930ms step_avg:129.34ms
step:629/1395 train_time:80066ms step_avg:129.35ms
step:630/1395 train_time:80200ms step_avg:129.35ms
step:631/1395 train_time:80334ms step_avg:129.36ms
step:632/1395 train_time:80469ms step_avg:129.37ms
step:633/1395 train_time:80603ms step_avg:129.38ms
step:634/1395 train_time:80739ms step_avg:129.39ms
step:635/1395 train_time:80874ms step_avg:129.40ms
step:636/1395 train_time:81010ms step_avg:129.41ms
step:637/1395 train_time:81144ms step_avg:129.42ms
step:638/1395 train_time:81280ms step_avg:129.43ms
step:639/1395 train_time:81414ms step_avg:129.43ms
step:640/1395 train_time:81548ms step_avg:129.44ms
step:641/1395 train_time:81683ms step_avg:129.45ms
step:642/1395 train_time:81818ms step_avg:129.46ms
step:643/1395 train_time:81954ms step_avg:129.47ms
step:644/1395 train_time:82090ms step_avg:129.48ms
step:645/1395 train_time:82225ms step_avg:129.49ms
step:646/1395 train_time:82359ms step_avg:129.50ms
step:647/1395 train_time:82493ms step_avg:129.50ms
step:648/1395 train_time:82631ms step_avg:129.52ms
step:649/1395 train_time:82766ms step_avg:129.52ms
step:650/1395 train_time:82902ms step_avg:129.53ms
step:651/1395 train_time:83037ms step_avg:129.54ms
step:652/1395 train_time:83172ms step_avg:129.55ms
step:653/1395 train_time:83308ms step_avg:129.56ms
step:654/1395 train_time:83444ms step_avg:129.57ms
step:655/1395 train_time:83578ms step_avg:129.58ms
step:656/1395 train_time:83713ms step_avg:129.59ms
step:657/1395 train_time:83848ms step_avg:129.60ms
step:658/1395 train_time:83984ms step_avg:129.60ms
step:659/1395 train_time:84118ms step_avg:129.61ms
step:660/1395 train_time:84253ms step_avg:129.62ms
step:661/1395 train_time:84390ms step_avg:129.63ms
step:662/1395 train_time:84525ms step_avg:129.64ms
step:663/1395 train_time:84659ms step_avg:129.65ms
step:664/1395 train_time:84795ms step_avg:129.66ms
step:665/1395 train_time:84931ms step_avg:129.67ms
step:666/1395 train_time:85066ms step_avg:129.67ms
step:667/1395 train_time:85202ms step_avg:129.68ms
step:668/1395 train_time:85338ms step_avg:129.69ms
step:669/1395 train_time:85473ms step_avg:129.70ms
step:670/1395 train_time:85608ms step_avg:129.71ms
step:671/1395 train_time:85744ms step_avg:129.72ms
step:672/1395 train_time:85880ms step_avg:129.73ms
step:673/1395 train_time:86015ms step_avg:129.74ms
step:674/1395 train_time:86150ms step_avg:129.74ms
step:675/1395 train_time:86288ms step_avg:129.76ms
step:676/1395 train_time:86423ms step_avg:129.76ms
step:677/1395 train_time:86558ms step_avg:129.77ms
step:678/1395 train_time:86692ms step_avg:129.78ms
step:679/1395 train_time:86827ms step_avg:129.79ms
step:680/1395 train_time:86964ms step_avg:129.80ms
step:681/1395 train_time:87100ms step_avg:129.81ms
step:682/1395 train_time:87234ms step_avg:129.81ms
step:683/1395 train_time:87369ms step_avg:129.82ms
step:684/1395 train_time:87505ms step_avg:129.83ms
step:685/1395 train_time:87640ms step_avg:129.84ms
step:686/1395 train_time:87775ms step_avg:129.84ms
step:687/1395 train_time:87909ms step_avg:129.85ms
step:688/1395 train_time:88045ms step_avg:129.86ms
step:689/1395 train_time:88183ms step_avg:129.87ms
step:690/1395 train_time:88320ms step_avg:129.88ms
step:691/1395 train_time:88454ms step_avg:129.89ms
step:692/1395 train_time:88588ms step_avg:129.89ms
step:693/1395 train_time:88723ms step_avg:129.90ms
step:694/1395 train_time:88858ms step_avg:129.91ms
step:695/1395 train_time:88992ms step_avg:129.91ms
step:696/1395 train_time:89126ms step_avg:129.92ms
step:697/1395 train_time:89261ms step_avg:129.93ms
step:698/1395 train_time:89396ms step_avg:129.94ms
step:699/1395 train_time:89531ms step_avg:129.94ms
step:700/1395 train_time:89667ms step_avg:129.95ms
step:701/1395 train_time:89801ms step_avg:129.96ms
step:702/1395 train_time:89938ms step_avg:129.97ms
step:703/1395 train_time:90072ms step_avg:129.97ms
step:704/1395 train_time:90206ms step_avg:129.98ms
step:705/1395 train_time:90343ms step_avg:129.99ms
step:706/1395 train_time:90481ms step_avg:130.00ms
step:707/1395 train_time:90616ms step_avg:130.01ms
step:708/1395 train_time:90752ms step_avg:130.02ms
step:709/1395 train_time:90888ms step_avg:130.03ms
step:710/1395 train_time:91024ms step_avg:130.03ms
step:711/1395 train_time:91160ms step_avg:130.04ms
step:712/1395 train_time:91295ms step_avg:130.05ms
step:713/1395 train_time:91431ms step_avg:130.06ms
step:714/1395 train_time:91565ms step_avg:130.06ms
step:715/1395 train_time:91701ms step_avg:130.07ms
step:716/1395 train_time:91836ms step_avg:130.08ms
step:717/1395 train_time:91971ms step_avg:130.09ms
step:718/1395 train_time:92106ms step_avg:130.09ms
step:719/1395 train_time:92240ms step_avg:130.10ms
step:720/1395 train_time:92376ms step_avg:130.11ms
step:721/1395 train_time:92510ms step_avg:130.11ms
step:722/1395 train_time:92645ms step_avg:130.12ms
step:723/1395 train_time:92780ms step_avg:130.13ms
step:724/1395 train_time:92915ms step_avg:130.13ms
step:725/1395 train_time:93050ms step_avg:130.14ms
step:726/1395 train_time:93188ms step_avg:130.15ms
step:727/1395 train_time:93327ms step_avg:130.16ms
step:728/1395 train_time:93463ms step_avg:130.17ms
step:729/1395 train_time:93599ms step_avg:130.18ms
step:730/1395 train_time:93737ms step_avg:130.19ms
step:731/1395 train_time:93872ms step_avg:130.20ms
step:732/1395 train_time:94010ms step_avg:130.21ms
step:733/1395 train_time:94146ms step_avg:130.22ms
step:734/1395 train_time:94282ms step_avg:130.22ms
step:735/1395 train_time:94420ms step_avg:130.23ms
step:736/1395 train_time:94556ms step_avg:130.24ms
step:737/1395 train_time:94693ms step_avg:130.25ms
step:738/1395 train_time:94829ms step_avg:130.26ms
step:739/1395 train_time:94965ms step_avg:130.27ms
step:740/1395 train_time:95101ms step_avg:130.28ms
step:741/1395 train_time:95240ms step_avg:130.29ms
step:742/1395 train_time:95377ms step_avg:130.30ms
step:743/1395 train_time:95512ms step_avg:130.30ms
step:744/1395 train_time:95649ms step_avg:130.31ms
step:745/1395 train_time:95787ms step_avg:130.32ms
step:746/1395 train_time:95922ms step_avg:130.33ms
step:747/1395 train_time:96058ms step_avg:130.34ms
step:748/1395 train_time:96195ms step_avg:130.34ms
step:749/1395 train_time:96333ms step_avg:130.36ms
step:750/1395 train_time:96472ms step_avg:130.37ms
step:750/1395 val_loss:3.5260 train_time:96584ms step_avg:130.52ms
step:751/1395 train_time:96615ms step_avg:130.39ms
step:752/1395 train_time:96756ms step_avg:130.40ms
step:753/1395 train_time:96891ms step_avg:130.41ms
step:754/1395 train_time:97026ms step_avg:130.41ms
step:755/1395 train_time:97162ms step_avg:130.42ms
step:756/1395 train_time:97298ms step_avg:130.43ms
step:757/1395 train_time:97437ms step_avg:130.44ms
step:758/1395 train_time:97575ms step_avg:130.45ms
step:759/1395 train_time:97712ms step_avg:130.46ms
step:760/1395 train_time:97849ms step_avg:130.46ms
step:761/1395 train_time:97986ms step_avg:130.47ms
step:762/1395 train_time:98121ms step_avg:130.48ms
step:763/1395 train_time:98258ms step_avg:130.49ms
step:764/1395 train_time:98396ms step_avg:130.50ms
step:765/1395 train_time:98532ms step_avg:130.51ms
step:766/1395 train_time:98671ms step_avg:130.52ms
step:767/1395 train_time:98807ms step_avg:130.52ms
step:768/1395 train_time:98943ms step_avg:130.53ms
step:769/1395 train_time:99080ms step_avg:130.54ms
step:770/1395 train_time:99217ms step_avg:130.55ms
step:771/1395 train_time:99354ms step_avg:130.56ms
step:772/1395 train_time:99490ms step_avg:130.56ms
step:773/1395 train_time:99627ms step_avg:130.57ms
step:774/1395 train_time:99763ms step_avg:130.58ms
step:775/1395 train_time:99899ms step_avg:130.59ms
step:776/1395 train_time:100036ms step_avg:130.60ms
step:777/1395 train_time:100174ms step_avg:130.60ms
step:778/1395 train_time:100310ms step_avg:130.61ms
step:779/1395 train_time:100446ms step_avg:130.62ms
step:780/1395 train_time:100584ms step_avg:130.63ms
step:781/1395 train_time:100720ms step_avg:130.64ms
step:782/1395 train_time:100856ms step_avg:130.64ms
step:783/1395 train_time:100992ms step_avg:130.65ms
step:784/1395 train_time:101129ms step_avg:130.66ms
step:785/1395 train_time:101264ms step_avg:130.66ms
step:786/1395 train_time:101400ms step_avg:130.67ms
step:787/1395 train_time:101537ms step_avg:130.68ms
step:788/1395 train_time:101674ms step_avg:130.69ms
step:789/1395 train_time:101809ms step_avg:130.69ms
step:790/1395 train_time:101945ms step_avg:130.70ms
step:791/1395 train_time:102082ms step_avg:130.71ms
step:792/1395 train_time:102219ms step_avg:130.72ms
step:793/1395 train_time:102355ms step_avg:130.72ms
step:794/1395 train_time:102492ms step_avg:130.73ms
step:795/1395 train_time:102632ms step_avg:130.74ms
step:796/1395 train_time:102769ms step_avg:130.75ms
step:797/1395 train_time:102904ms step_avg:130.76ms
step:798/1395 train_time:103041ms step_avg:130.76ms
step:799/1395 train_time:103180ms step_avg:130.77ms
step:800/1395 train_time:103316ms step_avg:130.78ms
step:801/1395 train_time:103452ms step_avg:130.79ms
step:802/1395 train_time:103589ms step_avg:130.79ms
step:803/1395 train_time:103723ms step_avg:130.80ms
step:804/1395 train_time:103859ms step_avg:130.80ms
step:805/1395 train_time:103997ms step_avg:130.81ms
step:806/1395 train_time:104132ms step_avg:130.82ms
step:807/1395 train_time:104267ms step_avg:130.82ms
step:808/1395 train_time:104404ms step_avg:130.83ms
step:809/1395 train_time:104540ms step_avg:130.84ms
step:810/1395 train_time:104677ms step_avg:130.85ms
step:811/1395 train_time:104813ms step_avg:130.85ms
step:812/1395 train_time:104949ms step_avg:130.86ms
step:813/1395 train_time:105084ms step_avg:130.86ms
step:814/1395 train_time:105221ms step_avg:130.87ms
step:815/1395 train_time:105357ms step_avg:130.88ms
step:816/1395 train_time:105494ms step_avg:130.89ms
step:817/1395 train_time:105630ms step_avg:130.89ms
step:818/1395 train_time:105766ms step_avg:130.90ms
step:819/1395 train_time:105903ms step_avg:130.91ms
step:820/1395 train_time:106039ms step_avg:130.91ms
step:821/1395 train_time:106176ms step_avg:130.92ms
step:822/1395 train_time:106312ms step_avg:130.93ms
step:823/1395 train_time:106447ms step_avg:130.93ms
step:824/1395 train_time:106583ms step_avg:130.94ms
step:825/1395 train_time:106720ms step_avg:130.95ms
step:826/1395 train_time:106858ms step_avg:130.95ms
step:827/1395 train_time:106996ms step_avg:130.96ms
step:828/1395 train_time:107133ms step_avg:130.97ms
step:829/1395 train_time:107270ms step_avg:130.98ms
step:830/1395 train_time:107408ms step_avg:130.99ms
step:831/1395 train_time:107546ms step_avg:130.99ms
step:832/1395 train_time:107684ms step_avg:131.00ms
step:833/1395 train_time:107821ms step_avg:131.01ms
step:834/1395 train_time:107960ms step_avg:131.02ms
step:835/1395 train_time:108097ms step_avg:131.03ms
step:836/1395 train_time:108237ms step_avg:131.04ms
step:837/1395 train_time:108375ms step_avg:131.05ms
step:838/1395 train_time:108512ms step_avg:131.05ms
step:839/1395 train_time:108649ms step_avg:131.06ms
step:840/1395 train_time:108785ms step_avg:131.07ms
step:841/1395 train_time:108922ms step_avg:131.07ms
step:842/1395 train_time:109060ms step_avg:131.08ms
step:843/1395 train_time:109198ms step_avg:131.09ms
step:844/1395 train_time:109336ms step_avg:131.10ms
step:845/1395 train_time:109472ms step_avg:131.10ms
step:846/1395 train_time:109612ms step_avg:131.11ms
step:847/1395 train_time:109749ms step_avg:131.12ms
step:848/1395 train_time:109887ms step_avg:131.13ms
step:849/1395 train_time:110024ms step_avg:131.14ms
step:850/1395 train_time:110163ms step_avg:131.15ms
step:851/1395 train_time:110303ms step_avg:131.16ms
step:852/1395 train_time:110441ms step_avg:131.17ms
step:853/1395 train_time:110578ms step_avg:131.17ms
step:854/1395 train_time:110713ms step_avg:131.18ms
step:855/1395 train_time:110850ms step_avg:131.18ms
step:856/1395 train_time:110986ms step_avg:131.19ms
step:857/1395 train_time:111125ms step_avg:131.20ms
step:858/1395 train_time:111265ms step_avg:131.21ms
step:859/1395 train_time:111403ms step_avg:131.22ms
step:860/1395 train_time:111540ms step_avg:131.22ms
step:861/1395 train_time:111679ms step_avg:131.23ms
step:862/1395 train_time:111818ms step_avg:131.24ms
step:863/1395 train_time:111958ms step_avg:131.25ms
step:864/1395 train_time:112096ms step_avg:131.26ms
step:865/1395 train_time:112233ms step_avg:131.27ms
step:866/1395 train_time:112378ms step_avg:131.28ms
step:867/1395 train_time:112518ms step_avg:131.29ms
step:868/1395 train_time:112654ms step_avg:131.30ms
step:869/1395 train_time:112791ms step_avg:131.31ms
step:870/1395 train_time:112931ms step_avg:131.32ms
step:871/1395 train_time:113068ms step_avg:131.32ms
step:872/1395 train_time:113204ms step_avg:131.33ms
step:873/1395 train_time:113341ms step_avg:131.33ms
step:874/1395 train_time:113479ms step_avg:131.34ms
step:875/1395 train_time:113618ms step_avg:131.35ms
step:875/1395 val_loss:3.4779 train_time:113729ms step_avg:131.48ms
step:876/1395 train_time:113759ms step_avg:131.36ms
step:877/1395 train_time:113899ms step_avg:131.37ms
step:878/1395 train_time:114037ms step_avg:131.38ms
step:879/1395 train_time:114174ms step_avg:131.39ms
step:880/1395 train_time:114312ms step_avg:131.39ms
step:881/1395 train_time:114448ms step_avg:131.40ms
step:882/1395 train_time:114587ms step_avg:131.41ms
step:883/1395 train_time:114725ms step_avg:131.41ms
step:884/1395 train_time:114863ms step_avg:131.42ms
step:885/1395 train_time:115001ms step_avg:131.43ms
step:886/1395 train_time:115141ms step_avg:131.44ms
step:887/1395 train_time:115278ms step_avg:131.45ms
step:888/1395 train_time:115419ms step_avg:131.46ms
step:889/1395 train_time:115561ms step_avg:131.47ms
step:890/1395 train_time:115697ms step_avg:131.47ms
step:891/1395 train_time:115834ms step_avg:131.48ms
step:892/1395 train_time:115971ms step_avg:131.49ms
step:893/1395 train_time:116108ms step_avg:131.49ms
step:894/1395 train_time:116245ms step_avg:131.50ms
step:895/1395 train_time:116384ms step_avg:131.51ms
step:896/1395 train_time:116521ms step_avg:131.51ms
step:897/1395 train_time:116660ms step_avg:131.52ms
step:898/1395 train_time:116797ms step_avg:131.53ms
step:899/1395 train_time:116937ms step_avg:131.54ms
step:900/1395 train_time:117074ms step_avg:131.54ms
step:901/1395 train_time:117213ms step_avg:131.55ms
step:902/1395 train_time:117349ms step_avg:131.56ms
step:903/1395 train_time:117490ms step_avg:131.57ms
step:904/1395 train_time:117629ms step_avg:131.58ms
step:905/1395 train_time:117766ms step_avg:131.58ms
step:906/1395 train_time:117904ms step_avg:131.59ms
step:907/1395 train_time:118044ms step_avg:131.60ms
step:908/1395 train_time:118181ms step_avg:131.61ms
step:909/1395 train_time:118318ms step_avg:131.61ms
step:910/1395 train_time:118460ms step_avg:131.62ms
step:911/1395 train_time:118598ms step_avg:131.63ms
step:912/1395 train_time:118733ms step_avg:131.63ms
step:913/1395 train_time:118872ms step_avg:131.64ms
step:914/1395 train_time:119010ms step_avg:131.65ms
step:915/1395 train_time:119148ms step_avg:131.66ms
step:916/1395 train_time:119287ms step_avg:131.66ms
step:917/1395 train_time:119426ms step_avg:131.67ms
step:918/1395 train_time:119564ms step_avg:131.68ms
step:919/1395 train_time:119707ms step_avg:131.69ms
step:920/1395 train_time:119845ms step_avg:131.70ms
step:921/1395 train_time:119983ms step_avg:131.70ms
step:922/1395 train_time:120123ms step_avg:131.71ms
step:923/1395 train_time:120259ms step_avg:131.72ms
step:924/1395 train_time:120397ms step_avg:131.73ms
step:925/1395 train_time:120536ms step_avg:131.73ms
step:926/1395 train_time:120673ms step_avg:131.74ms
step:927/1395 train_time:120810ms step_avg:131.74ms
step:928/1395 train_time:120947ms step_avg:131.75ms
step:929/1395 train_time:121086ms step_avg:131.76ms
step:930/1395 train_time:121223ms step_avg:131.76ms
step:931/1395 train_time:121360ms step_avg:131.77ms
step:932/1395 train_time:121499ms step_avg:131.78ms
step:933/1395 train_time:121639ms step_avg:131.79ms
step:934/1395 train_time:121776ms step_avg:131.79ms
step:935/1395 train_time:121917ms step_avg:131.80ms
step:936/1395 train_time:122056ms step_avg:131.81ms
step:937/1395 train_time:122200ms step_avg:131.82ms
step:938/1395 train_time:122341ms step_avg:131.83ms
step:939/1395 train_time:122480ms step_avg:131.84ms
step:940/1395 train_time:122622ms step_avg:131.85ms
step:941/1395 train_time:122760ms step_avg:131.86ms
step:942/1395 train_time:122897ms step_avg:131.86ms
step:943/1395 train_time:123039ms step_avg:131.87ms
step:944/1395 train_time:123183ms step_avg:131.89ms
step:945/1395 train_time:123321ms step_avg:131.89ms
step:946/1395 train_time:123463ms step_avg:131.90ms
step:947/1395 train_time:123604ms step_avg:131.91ms
step:948/1395 train_time:123743ms step_avg:131.92ms
step:949/1395 train_time:123883ms step_avg:131.93ms
step:950/1395 train_time:124020ms step_avg:131.94ms
step:951/1395 train_time:124160ms step_avg:131.95ms
step:952/1395 train_time:124298ms step_avg:131.95ms
step:953/1395 train_time:124437ms step_avg:131.96ms
step:954/1395 train_time:124576ms step_avg:131.97ms
step:955/1395 train_time:124713ms step_avg:131.97ms
step:956/1395 train_time:124855ms step_avg:131.98ms
step:957/1395 train_time:124993ms step_avg:131.99ms
step:958/1395 train_time:125134ms step_avg:132.00ms
step:959/1395 train_time:125277ms step_avg:132.01ms
step:960/1395 train_time:125417ms step_avg:132.02ms
step:961/1395 train_time:125556ms step_avg:132.03ms
step:962/1395 train_time:125695ms step_avg:132.03ms
step:963/1395 train_time:125840ms step_avg:132.05ms
step:964/1395 train_time:125980ms step_avg:132.05ms
step:965/1395 train_time:126119ms step_avg:132.06ms
step:966/1395 train_time:126259ms step_avg:132.07ms
step:967/1395 train_time:126400ms step_avg:132.08ms
step:968/1395 train_time:126537ms step_avg:132.08ms
step:969/1395 train_time:126677ms step_avg:132.09ms
step:970/1395 train_time:126815ms step_avg:132.10ms
step:971/1395 train_time:126955ms step_avg:132.11ms
step:972/1395 train_time:127093ms step_avg:132.11ms
step:973/1395 train_time:127231ms step_avg:132.12ms
step:974/1395 train_time:127371ms step_avg:132.13ms
step:975/1395 train_time:127509ms step_avg:132.13ms
step:976/1395 train_time:127647ms step_avg:132.14ms
step:977/1395 train_time:127784ms step_avg:132.15ms
step:978/1395 train_time:127923ms step_avg:132.15ms
step:979/1395 train_time:128064ms step_avg:132.16ms
step:980/1395 train_time:128203ms step_avg:132.17ms
step:981/1395 train_time:128341ms step_avg:132.17ms
step:982/1395 train_time:128479ms step_avg:132.18ms
step:983/1395 train_time:128617ms step_avg:132.19ms
step:984/1395 train_time:128756ms step_avg:132.19ms
step:985/1395 train_time:128897ms step_avg:132.20ms
step:986/1395 train_time:129042ms step_avg:132.22ms
step:987/1395 train_time:129181ms step_avg:132.22ms
step:988/1395 train_time:129320ms step_avg:132.23ms
step:989/1395 train_time:129459ms step_avg:132.24ms
step:990/1395 train_time:129601ms step_avg:132.25ms
step:991/1395 train_time:129740ms step_avg:132.25ms
step:992/1395 train_time:129883ms step_avg:132.26ms
step:993/1395 train_time:130030ms step_avg:132.28ms
step:994/1395 train_time:130166ms step_avg:132.28ms
step:995/1395 train_time:130305ms step_avg:132.29ms
step:996/1395 train_time:130443ms step_avg:132.29ms
step:997/1395 train_time:130582ms step_avg:132.30ms
step:998/1395 train_time:130722ms step_avg:132.31ms
step:999/1395 train_time:130861ms step_avg:132.32ms
step:1000/1395 train_time:131000ms step_avg:132.32ms
step:1000/1395 val_loss:3.4153 train_time:131110ms step_avg:132.43ms
step:1001/1395 train_time:131141ms step_avg:132.33ms
step:1002/1395 train_time:131283ms step_avg:132.34ms
step:1003/1395 train_time:131425ms step_avg:132.35ms
step:1004/1395 train_time:131565ms step_avg:132.36ms
step:1005/1395 train_time:131704ms step_avg:132.37ms
step:1006/1395 train_time:131842ms step_avg:132.37ms
step:1007/1395 train_time:131980ms step_avg:132.38ms
step:1008/1395 train_time:132119ms step_avg:132.38ms
step:1009/1395 train_time:132264ms step_avg:132.40ms
step:1010/1395 train_time:132406ms step_avg:132.41ms
step:1011/1395 train_time:132547ms step_avg:132.41ms
step:1012/1395 train_time:132685ms step_avg:132.42ms
step:1013/1395 train_time:132826ms step_avg:132.43ms
step:1014/1395 train_time:132964ms step_avg:132.43ms
step:1015/1395 train_time:133104ms step_avg:132.44ms
step:1016/1395 train_time:133242ms step_avg:132.45ms
step:1017/1395 train_time:133382ms step_avg:132.45ms
step:1018/1395 train_time:133522ms step_avg:132.46ms
step:1019/1395 train_time:133662ms step_avg:132.47ms
step:1020/1395 train_time:133805ms step_avg:132.48ms
step:1021/1395 train_time:133944ms step_avg:132.49ms
step:1022/1395 train_time:134082ms step_avg:132.49ms
step:1023/1395 train_time:134222ms step_avg:132.50ms
step:1024/1395 train_time:134362ms step_avg:132.51ms
step:1025/1395 train_time:134503ms step_avg:132.51ms
step:1026/1395 train_time:134642ms step_avg:132.52ms
step:1027/1395 train_time:134781ms step_avg:132.53ms
step:1028/1395 train_time:134922ms step_avg:132.54ms
step:1029/1395 train_time:135065ms step_avg:132.55ms
step:1030/1395 train_time:135204ms step_avg:132.55ms
step:1031/1395 train_time:135342ms step_avg:132.56ms
step:1032/1395 train_time:135481ms step_avg:132.56ms
step:1033/1395 train_time:135619ms step_avg:132.57ms
step:1034/1395 train_time:135757ms step_avg:132.58ms
step:1035/1395 train_time:135897ms step_avg:132.58ms
step:1036/1395 train_time:136037ms step_avg:132.59ms
step:1037/1395 train_time:136182ms step_avg:132.60ms
step:1038/1395 train_time:136322ms step_avg:132.61ms
step:1039/1395 train_time:136461ms step_avg:132.61ms
step:1040/1395 train_time:136599ms step_avg:132.62ms
step:1041/1395 train_time:136739ms step_avg:132.63ms
step:1042/1395 train_time:136878ms step_avg:132.63ms
step:1043/1395 train_time:137018ms step_avg:132.64ms
step:1044/1395 train_time:137164ms step_avg:132.65ms
step:1045/1395 train_time:137306ms step_avg:132.66ms
step:1046/1395 train_time:137446ms step_avg:132.67ms
step:1047/1395 train_time:137584ms step_avg:132.68ms
step:1048/1395 train_time:137725ms step_avg:132.68ms
step:1049/1395 train_time:137865ms step_avg:132.69ms
step:1050/1395 train_time:138008ms step_avg:132.70ms
step:1051/1395 train_time:138153ms step_avg:132.71ms
step:1052/1395 train_time:138292ms step_avg:132.72ms
step:1053/1395 train_time:138430ms step_avg:132.72ms
step:1054/1395 train_time:138570ms step_avg:132.73ms
step:1055/1395 train_time:138709ms step_avg:132.74ms
step:1056/1395 train_time:138847ms step_avg:132.74ms
step:1057/1395 train_time:138987ms step_avg:132.75ms
step:1058/1395 train_time:139129ms step_avg:132.76ms
step:1059/1395 train_time:139271ms step_avg:132.77ms
step:1060/1395 train_time:139413ms step_avg:132.77ms
step:1061/1395 train_time:139550ms step_avg:132.78ms
step:1062/1395 train_time:139690ms step_avg:132.79ms
step:1063/1395 train_time:139829ms step_avg:132.79ms
step:1064/1395 train_time:139967ms step_avg:132.80ms
step:1065/1395 train_time:140109ms step_avg:132.80ms
step:1066/1395 train_time:140251ms step_avg:132.81ms
step:1067/1395 train_time:140392ms step_avg:132.82ms
step:1068/1395 train_time:140531ms step_avg:132.83ms
step:1069/1395 train_time:140676ms step_avg:132.84ms
step:1070/1395 train_time:140814ms step_avg:132.84ms
step:1071/1395 train_time:140959ms step_avg:132.85ms
step:1072/1395 train_time:141096ms step_avg:132.86ms
step:1073/1395 train_time:141233ms step_avg:132.86ms
step:1074/1395 train_time:141371ms step_avg:132.87ms
step:1075/1395 train_time:141513ms step_avg:132.88ms
step:1076/1395 train_time:141651ms step_avg:132.88ms
step:1077/1395 train_time:141789ms step_avg:132.89ms
step:1078/1395 train_time:141931ms step_avg:132.89ms
step:1079/1395 train_time:142076ms step_avg:132.91ms
step:1080/1395 train_time:142216ms step_avg:132.91ms
step:1081/1395 train_time:142354ms step_avg:132.92ms
step:1082/1395 train_time:142493ms step_avg:132.92ms
step:1083/1395 train_time:142632ms step_avg:132.93ms
step:1084/1395 train_time:142777ms step_avg:132.94ms
step:1085/1395 train_time:142915ms step_avg:132.94ms
step:1086/1395 train_time:143056ms step_avg:132.95ms
step:1087/1395 train_time:143197ms step_avg:132.96ms
step:1088/1395 train_time:143336ms step_avg:132.96ms
step:1089/1395 train_time:143480ms step_avg:132.97ms
step:1090/1395 train_time:143623ms step_avg:132.98ms
step:1091/1395 train_time:143762ms step_avg:132.99ms
step:1092/1395 train_time:143901ms step_avg:133.00ms
step:1093/1395 train_time:144042ms step_avg:133.00ms
step:1094/1395 train_time:144180ms step_avg:133.01ms
step:1095/1395 train_time:144323ms step_avg:133.02ms
step:1096/1395 train_time:144462ms step_avg:133.02ms
step:1097/1395 train_time:144604ms step_avg:133.03ms
step:1098/1395 train_time:144746ms step_avg:133.04ms
step:1099/1395 train_time:144886ms step_avg:133.04ms
step:1100/1395 train_time:145025ms step_avg:133.05ms
step:1101/1395 train_time:145165ms step_avg:133.06ms
step:1102/1395 train_time:145307ms step_avg:133.07ms
step:1103/1395 train_time:145449ms step_avg:133.07ms
step:1104/1395 train_time:145588ms step_avg:133.08ms
step:1105/1395 train_time:145732ms step_avg:133.09ms
step:1106/1395 train_time:145873ms step_avg:133.10ms
step:1107/1395 train_time:146012ms step_avg:133.10ms
step:1108/1395 train_time:146157ms step_avg:133.11ms
step:1109/1395 train_time:146296ms step_avg:133.12ms
step:1110/1395 train_time:146435ms step_avg:133.12ms
step:1111/1395 train_time:146576ms step_avg:133.13ms
step:1112/1395 train_time:146716ms step_avg:133.14ms
step:1113/1395 train_time:146854ms step_avg:133.14ms
step:1114/1395 train_time:146996ms step_avg:133.15ms
step:1115/1395 train_time:147137ms step_avg:133.16ms
step:1116/1395 train_time:147275ms step_avg:133.16ms
step:1117/1395 train_time:147418ms step_avg:133.17ms
step:1118/1395 train_time:147563ms step_avg:133.18ms
step:1119/1395 train_time:147703ms step_avg:133.19ms
step:1120/1395 train_time:147842ms step_avg:133.19ms
step:1121/1395 train_time:147981ms step_avg:133.20ms
step:1122/1395 train_time:148119ms step_avg:133.20ms
step:1123/1395 train_time:148258ms step_avg:133.21ms
step:1124/1395 train_time:148400ms step_avg:133.21ms
step:1125/1395 train_time:148539ms step_avg:133.22ms
step:1125/1395 val_loss:3.3644 train_time:148653ms step_avg:133.32ms
step:1126/1395 train_time:148684ms step_avg:133.23ms
step:1127/1395 train_time:148825ms step_avg:133.24ms
step:1128/1395 train_time:148965ms step_avg:133.24ms
step:1129/1395 train_time:149108ms step_avg:133.25ms
step:1130/1395 train_time:149246ms step_avg:133.26ms
step:1131/1395 train_time:149387ms step_avg:133.26ms
step:1132/1395 train_time:149525ms step_avg:133.27ms
step:1133/1395 train_time:149665ms step_avg:133.27ms
step:1134/1395 train_time:149805ms step_avg:133.28ms
step:1135/1395 train_time:149944ms step_avg:133.28ms
step:1136/1395 train_time:150090ms step_avg:133.29ms
step:1137/1395 train_time:150227ms step_avg:133.30ms
step:1138/1395 train_time:150370ms step_avg:133.31ms
step:1139/1395 train_time:150512ms step_avg:133.31ms
step:1140/1395 train_time:150652ms step_avg:133.32ms
step:1141/1395 train_time:150793ms step_avg:133.33ms
step:1142/1395 train_time:150933ms step_avg:133.33ms
step:1143/1395 train_time:151076ms step_avg:133.34ms
step:1144/1395 train_time:151216ms step_avg:133.35ms
step:1145/1395 train_time:151356ms step_avg:133.35ms
step:1146/1395 train_time:151497ms step_avg:133.36ms
step:1147/1395 train_time:151641ms step_avg:133.37ms
step:1148/1395 train_time:151782ms step_avg:133.38ms
step:1149/1395 train_time:151922ms step_avg:133.38ms
step:1150/1395 train_time:152062ms step_avg:133.39ms
step:1151/1395 train_time:152206ms step_avg:133.40ms
step:1152/1395 train_time:152346ms step_avg:133.40ms
step:1153/1395 train_time:152490ms step_avg:133.41ms
step:1154/1395 train_time:152629ms step_avg:133.42ms
step:1155/1395 train_time:152771ms step_avg:133.42ms
step:1156/1395 train_time:152918ms step_avg:133.44ms
step:1157/1395 train_time:153061ms step_avg:133.44ms
step:1158/1395 train_time:153200ms step_avg:133.45ms
step:1159/1395 train_time:153341ms step_avg:133.46ms
step:1160/1395 train_time:153480ms step_avg:133.46ms
step:1161/1395 train_time:153623ms step_avg:133.47ms
step:1162/1395 train_time:153764ms step_avg:133.48ms
step:1163/1395 train_time:153905ms step_avg:133.48ms
step:1164/1395 train_time:154045ms step_avg:133.49ms
step:1165/1395 train_time:154183ms step_avg:133.49ms
step:1166/1395 train_time:154325ms step_avg:133.50ms
step:1167/1395 train_time:154465ms step_avg:133.50ms
step:1168/1395 train_time:154606ms step_avg:133.51ms
step:1169/1395 train_time:154747ms step_avg:133.52ms
step:1170/1395 train_time:154887ms step_avg:133.52ms
step:1171/1395 train_time:155029ms step_avg:133.53ms
step:1172/1395 train_time:155170ms step_avg:133.54ms
step:1173/1395 train_time:155310ms step_avg:133.54ms
step:1174/1395 train_time:155461ms step_avg:133.56ms
step:1175/1395 train_time:155604ms step_avg:133.57ms
step:1176/1395 train_time:155746ms step_avg:133.57ms
step:1177/1395 train_time:155894ms step_avg:133.59ms
step:1178/1395 train_time:156034ms step_avg:133.59ms
step:1179/1395 train_time:156173ms step_avg:133.60ms
step:1180/1395 train_time:156321ms step_avg:133.61ms
step:1181/1395 train_time:156464ms step_avg:133.62ms
step:1182/1395 train_time:156603ms step_avg:133.62ms
step:1183/1395 train_time:156745ms step_avg:133.63ms
step:1184/1395 train_time:156885ms step_avg:133.63ms
step:1185/1395 train_time:157029ms step_avg:133.64ms
step:1186/1395 train_time:157169ms step_avg:133.65ms
step:1187/1395 train_time:157322ms step_avg:133.66ms
step:1188/1395 train_time:157460ms step_avg:133.67ms
step:1189/1395 train_time:157603ms step_avg:133.68ms
step:1190/1395 train_time:157745ms step_avg:133.68ms
step:1191/1395 train_time:157887ms step_avg:133.69ms
step:1192/1395 train_time:158026ms step_avg:133.69ms
step:1193/1395 train_time:158166ms step_avg:133.70ms
step:1194/1395 train_time:158306ms step_avg:133.70ms
step:1195/1395 train_time:158447ms step_avg:133.71ms
step:1196/1395 train_time:158588ms step_avg:133.72ms
step:1197/1395 train_time:158730ms step_avg:133.72ms
step:1198/1395 train_time:158879ms step_avg:133.74ms
step:1199/1395 train_time:159021ms step_avg:133.74ms
step:1200/1395 train_time:159161ms step_avg:133.75ms
step:1201/1395 train_time:159301ms step_avg:133.75ms
step:1202/1395 train_time:159455ms step_avg:133.77ms
step:1203/1395 train_time:159602ms step_avg:133.78ms
step:1204/1395 train_time:159745ms step_avg:133.79ms
step:1205/1395 train_time:159888ms step_avg:133.80ms
step:1206/1395 train_time:160030ms step_avg:133.80ms
step:1207/1395 train_time:160172ms step_avg:133.81ms
step:1208/1395 train_time:160316ms step_avg:133.82ms
step:1209/1395 train_time:160457ms step_avg:133.83ms
step:1210/1395 train_time:160602ms step_avg:133.84ms
step:1211/1395 train_time:160744ms step_avg:133.84ms
step:1212/1395 train_time:160885ms step_avg:133.85ms
step:1213/1395 train_time:161026ms step_avg:133.85ms
step:1214/1395 train_time:161168ms step_avg:133.86ms
step:1215/1395 train_time:161313ms step_avg:133.87ms
step:1216/1395 train_time:161450ms step_avg:133.87ms
step:1217/1395 train_time:161593ms step_avg:133.88ms
step:1218/1395 train_time:161731ms step_avg:133.88ms
step:1219/1395 train_time:161869ms step_avg:133.89ms
step:1220/1395 train_time:162010ms step_avg:133.89ms
step:1221/1395 train_time:162149ms step_avg:133.90ms
step:1222/1395 train_time:162288ms step_avg:133.90ms
step:1223/1395 train_time:162429ms step_avg:133.91ms
step:1224/1395 train_time:162571ms step_avg:133.91ms
step:1225/1395 train_time:162714ms step_avg:133.92ms
step:1226/1395 train_time:162855ms step_avg:133.93ms
step:1227/1395 train_time:162997ms step_avg:133.93ms
step:1228/1395 train_time:163138ms step_avg:133.94ms
step:1229/1395 train_time:163278ms step_avg:133.94ms
step:1230/1395 train_time:163424ms step_avg:133.95ms
step:1231/1395 train_time:163566ms step_avg:133.96ms
step:1232/1395 train_time:163711ms step_avg:133.97ms
step:1233/1395 train_time:163851ms step_avg:133.97ms
step:1234/1395 train_time:163991ms step_avg:133.98ms
step:1235/1395 train_time:164131ms step_avg:133.98ms
step:1236/1395 train_time:164272ms step_avg:133.99ms
step:1237/1395 train_time:164411ms step_avg:133.99ms
step:1238/1395 train_time:164561ms step_avg:134.01ms
step:1239/1395 train_time:164702ms step_avg:134.01ms
step:1240/1395 train_time:164844ms step_avg:134.02ms
step:1241/1395 train_time:164988ms step_avg:134.03ms
step:1242/1395 train_time:165127ms step_avg:134.03ms
step:1243/1395 train_time:165270ms step_avg:134.04ms
step:1244/1395 train_time:165410ms step_avg:134.04ms
step:1245/1395 train_time:165551ms step_avg:134.05ms
step:1246/1395 train_time:165691ms step_avg:134.05ms
step:1247/1395 train_time:165834ms step_avg:134.06ms
step:1248/1395 train_time:165974ms step_avg:134.07ms
step:1249/1395 train_time:166113ms step_avg:134.07ms
step:1250/1395 train_time:166254ms step_avg:134.08ms
step:1250/1395 val_loss:3.3189 train_time:166368ms step_avg:134.17ms
step:1251/1395 train_time:166401ms step_avg:134.09ms
step:1252/1395 train_time:166545ms step_avg:134.09ms
step:1253/1395 train_time:166685ms step_avg:134.10ms
step:1254/1395 train_time:166824ms step_avg:134.10ms
step:1255/1395 train_time:166977ms step_avg:134.12ms
step:1256/1395 train_time:167119ms step_avg:134.12ms
step:1257/1395 train_time:167260ms step_avg:134.13ms
step:1258/1395 train_time:167402ms step_avg:134.14ms
step:1259/1395 train_time:167547ms step_avg:134.14ms
step:1260/1395 train_time:167686ms step_avg:134.15ms
step:1261/1395 train_time:167827ms step_avg:134.15ms
step:1262/1395 train_time:167972ms step_avg:134.16ms
step:1263/1395 train_time:168115ms step_avg:134.17ms
step:1264/1395 train_time:168255ms step_avg:134.17ms
step:1265/1395 train_time:168395ms step_avg:134.18ms
step:1266/1395 train_time:168537ms step_avg:134.19ms
step:1267/1395 train_time:168679ms step_avg:134.19ms
step:1268/1395 train_time:168821ms step_avg:134.20ms
step:1269/1395 train_time:168971ms step_avg:134.21ms
step:1270/1395 train_time:169112ms step_avg:134.22ms
step:1271/1395 train_time:169256ms step_avg:134.22ms
step:1272/1395 train_time:169396ms step_avg:134.23ms
step:1273/1395 train_time:169535ms step_avg:134.23ms
step:1274/1395 train_time:169674ms step_avg:134.24ms
step:1275/1395 train_time:169818ms step_avg:134.24ms
step:1276/1395 train_time:169958ms step_avg:134.25ms
step:1277/1395 train_time:170099ms step_avg:134.25ms
step:1278/1395 train_time:170238ms step_avg:134.26ms
step:1279/1395 train_time:170380ms step_avg:134.26ms
step:1280/1395 train_time:170529ms step_avg:134.27ms
step:1281/1395 train_time:170671ms step_avg:134.28ms
step:1282/1395 train_time:170810ms step_avg:134.28ms
step:1283/1395 train_time:170952ms step_avg:134.29ms
step:1284/1395 train_time:171095ms step_avg:134.30ms
step:1285/1395 train_time:171235ms step_avg:134.30ms
step:1286/1395 train_time:171377ms step_avg:134.31ms
step:1287/1395 train_time:171519ms step_avg:134.31ms
step:1288/1395 train_time:171659ms step_avg:134.32ms
step:1289/1395 train_time:171809ms step_avg:134.33ms
step:1290/1395 train_time:171955ms step_avg:134.34ms
step:1291/1395 train_time:172101ms step_avg:134.35ms
step:1292/1395 train_time:172244ms step_avg:134.36ms
step:1293/1395 train_time:172392ms step_avg:134.37ms
step:1294/1395 train_time:172533ms step_avg:134.37ms
step:1295/1395 train_time:172674ms step_avg:134.38ms
step:1296/1395 train_time:172817ms step_avg:134.38ms
step:1297/1395 train_time:172960ms step_avg:134.39ms
step:1298/1395 train_time:173100ms step_avg:134.39ms
step:1299/1395 train_time:173241ms step_avg:134.40ms
step:1300/1395 train_time:173381ms step_avg:134.40ms
step:1301/1395 train_time:173521ms step_avg:134.41ms
step:1302/1395 train_time:173664ms step_avg:134.41ms
step:1303/1395 train_time:173809ms step_avg:134.42ms
step:1304/1395 train_time:173954ms step_avg:134.43ms
step:1305/1395 train_time:174095ms step_avg:134.44ms
step:1306/1395 train_time:174238ms step_avg:134.44ms
step:1307/1395 train_time:174378ms step_avg:134.45ms
step:1308/1395 train_time:174523ms step_avg:134.46ms
step:1309/1395 train_time:174667ms step_avg:134.46ms
step:1310/1395 train_time:174809ms step_avg:134.47ms
step:1311/1395 train_time:174949ms step_avg:134.47ms
step:1312/1395 train_time:175088ms step_avg:134.48ms
step:1313/1395 train_time:175230ms step_avg:134.48ms
step:1314/1395 train_time:175371ms step_avg:134.49ms
step:1315/1395 train_time:175513ms step_avg:134.49ms
step:1316/1395 train_time:175653ms step_avg:134.50ms
step:1317/1395 train_time:175794ms step_avg:134.50ms
step:1318/1395 train_time:175940ms step_avg:134.51ms
step:1319/1395 train_time:176084ms step_avg:134.52ms
step:1320/1395 train_time:176226ms step_avg:134.52ms
step:1321/1395 train_time:176366ms step_avg:134.53ms
step:1322/1395 train_time:176517ms step_avg:134.54ms
step:1323/1395 train_time:176660ms step_avg:134.55ms
step:1324/1395 train_time:176801ms step_avg:134.55ms
step:1325/1395 train_time:176946ms step_avg:134.56ms
step:1326/1395 train_time:177092ms step_avg:134.57ms
step:1327/1395 train_time:177234ms step_avg:134.57ms
step:1328/1395 train_time:177373ms step_avg:134.58ms
step:1329/1395 train_time:177529ms step_avg:134.59ms
step:1330/1395 train_time:177675ms step_avg:134.60ms
step:1331/1395 train_time:177820ms step_avg:134.61ms
step:1332/1395 train_time:177970ms step_avg:134.62ms
step:1333/1395 train_time:178114ms step_avg:134.63ms
step:1334/1395 train_time:178255ms step_avg:134.63ms
step:1335/1395 train_time:178393ms step_avg:134.64ms
step:1336/1395 train_time:178541ms step_avg:134.65ms
step:1337/1395 train_time:178686ms step_avg:134.65ms
step:1338/1395 train_time:178828ms step_avg:134.66ms
step:1339/1395 train_time:178972ms step_avg:134.67ms
step:1340/1395 train_time:179117ms step_avg:134.67ms
step:1341/1395 train_time:179257ms step_avg:134.68ms
step:1342/1395 train_time:179399ms step_avg:134.68ms
step:1343/1395 train_time:179539ms step_avg:134.69ms
step:1344/1395 train_time:179679ms step_avg:134.69ms
step:1345/1395 train_time:179820ms step_avg:134.70ms
step:1346/1395 train_time:179962ms step_avg:134.70ms
step:1347/1395 train_time:180108ms step_avg:134.71ms
step:1348/1395 train_time:180250ms step_avg:134.72ms
step:1349/1395 train_time:180391ms step_avg:134.72ms
step:1350/1395 train_time:180531ms step_avg:134.72ms
step:1351/1395 train_time:180672ms step_avg:134.73ms
step:1352/1395 train_time:180822ms step_avg:134.74ms
step:1353/1395 train_time:180969ms step_avg:134.75ms
step:1354/1395 train_time:181112ms step_avg:134.76ms
step:1355/1395 train_time:181254ms step_avg:134.76ms
step:1356/1395 train_time:181394ms step_avg:134.77ms
step:1357/1395 train_time:181538ms step_avg:134.77ms
step:1358/1395 train_time:181682ms step_avg:134.78ms
step:1359/1395 train_time:181824ms step_avg:134.78ms
step:1360/1395 train_time:181970ms step_avg:134.79ms
step:1361/1395 train_time:182114ms step_avg:134.80ms
step:1362/1395 train_time:182259ms step_avg:134.81ms
step:1363/1395 train_time:182407ms step_avg:134.82ms
step:1364/1395 train_time:182551ms step_avg:134.82ms
step:1365/1395 train_time:182689ms step_avg:134.83ms
step:1366/1395 train_time:182831ms step_avg:134.83ms
step:1367/1395 train_time:182974ms step_avg:134.84ms
step:1368/1395 train_time:183117ms step_avg:134.84ms
step:1369/1395 train_time:183266ms step_avg:134.85ms
step:1370/1395 train_time:183414ms step_avg:134.86ms
step:1371/1395 train_time:183558ms step_avg:134.87ms
step:1372/1395 train_time:183706ms step_avg:134.88ms
step:1373/1395 train_time:183848ms step_avg:134.88ms
step:1374/1395 train_time:183994ms step_avg:134.89ms
step:1375/1395 train_time:184135ms step_avg:134.90ms
step:1375/1395 val_loss:3.2849 train_time:184247ms step_avg:134.98ms
step:1376/1395 train_time:184278ms step_avg:134.90ms
step:1377/1395 train_time:184421ms step_avg:134.91ms
step:1378/1395 train_time:184564ms step_avg:134.92ms
step:1379/1395 train_time:184707ms step_avg:134.92ms
step:1380/1395 train_time:184851ms step_avg:134.93ms
step:1381/1395 train_time:184997ms step_avg:134.94ms
step:1382/1395 train_time:185140ms step_avg:134.94ms
step:1383/1395 train_time:185282ms step_avg:134.95ms
step:1384/1395 train_time:185429ms step_avg:134.96ms
step:1385/1395 train_time:185568ms step_avg:134.96ms
step:1386/1395 train_time:185710ms step_avg:134.96ms
step:1387/1395 train_time:185856ms step_avg:134.97ms
step:1388/1395 train_time:185997ms step_avg:134.98ms
step:1389/1395 train_time:186141ms step_avg:134.98ms
step:1390/1395 train_time:186283ms step_avg:134.99ms
step:1391/1395 train_time:186424ms step_avg:134.99ms
step:1392/1395 train_time:186570ms step_avg:135.00ms
step:1393/1395 train_time:186711ms step_avg:135.00ms
step:1394/1395 train_time:186853ms step_avg:135.01ms
step:1395/1395 train_time:186995ms step_avg:135.01ms
step:1395/1395 val_loss:3.2804 train_time:187110ms step_avg:135.10ms
peak memory allocated: 37620 MiB reserved: 39114 MiB
