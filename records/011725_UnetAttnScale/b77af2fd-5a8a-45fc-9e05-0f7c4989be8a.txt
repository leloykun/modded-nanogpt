import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.13 + 0.01 * min(layer_idx, 11 - layer_idx)  # unet pattern attention scale by @leloykun

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04), dict(params=attn_scale_params, lr=0.01),]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 17:10:05 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   31C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:65811ms step_avg:nanms
step:2/1395 train_time:65864ms step_avg:nanms
step:3/1395 train_time:65988ms step_avg:nanms
step:4/1395 train_time:66110ms step_avg:nanms
step:5/1395 train_time:66234ms step_avg:nanms
step:6/1395 train_time:66356ms step_avg:nanms
step:7/1395 train_time:66478ms step_avg:nanms
step:8/1395 train_time:66602ms step_avg:nanms
step:9/1395 train_time:66725ms step_avg:nanms
step:10/1395 train_time:66847ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:246ms step_avg:nanms
step:13/1395 train_time:370ms step_avg:123.19ms
step:14/1395 train_time:492ms step_avg:123.09ms
step:15/1395 train_time:615ms step_avg:123.00ms
step:16/1395 train_time:739ms step_avg:123.08ms
step:17/1395 train_time:862ms step_avg:123.21ms
step:18/1395 train_time:986ms step_avg:123.25ms
step:19/1395 train_time:1109ms step_avg:123.17ms
step:20/1395 train_time:1232ms step_avg:123.18ms
step:21/1395 train_time:1355ms step_avg:123.17ms
step:22/1395 train_time:1479ms step_avg:123.24ms
step:23/1395 train_time:1602ms step_avg:123.26ms
step:24/1395 train_time:1726ms step_avg:123.25ms
step:25/1395 train_time:1848ms step_avg:123.23ms
step:26/1395 train_time:1973ms step_avg:123.32ms
step:27/1395 train_time:2096ms step_avg:123.28ms
step:28/1395 train_time:2220ms step_avg:123.31ms
step:29/1395 train_time:2343ms step_avg:123.30ms
step:30/1395 train_time:2466ms step_avg:123.30ms
step:31/1395 train_time:2589ms step_avg:123.30ms
step:32/1395 train_time:2712ms step_avg:123.28ms
step:33/1395 train_time:2836ms step_avg:123.28ms
step:34/1395 train_time:2959ms step_avg:123.28ms
step:35/1395 train_time:3082ms step_avg:123.29ms
step:36/1395 train_time:3206ms step_avg:123.31ms
step:37/1395 train_time:3329ms step_avg:123.30ms
step:38/1395 train_time:3454ms step_avg:123.35ms
step:39/1395 train_time:3577ms step_avg:123.36ms
step:40/1395 train_time:3700ms step_avg:123.35ms
step:41/1395 train_time:3823ms step_avg:123.32ms
step:42/1395 train_time:3947ms step_avg:123.34ms
step:43/1395 train_time:4070ms step_avg:123.35ms
step:44/1395 train_time:4193ms step_avg:123.32ms
step:45/1395 train_time:4317ms step_avg:123.34ms
step:46/1395 train_time:4440ms step_avg:123.33ms
step:47/1395 train_time:4564ms step_avg:123.34ms
step:48/1395 train_time:4687ms step_avg:123.35ms
step:49/1395 train_time:4811ms step_avg:123.35ms
step:50/1395 train_time:4934ms step_avg:123.36ms
step:51/1395 train_time:5057ms step_avg:123.35ms
step:52/1395 train_time:5182ms step_avg:123.37ms
step:53/1395 train_time:5304ms step_avg:123.36ms
step:54/1395 train_time:5428ms step_avg:123.36ms
step:55/1395 train_time:5551ms step_avg:123.36ms
step:56/1395 train_time:5674ms step_avg:123.36ms
step:57/1395 train_time:5798ms step_avg:123.37ms
step:58/1395 train_time:5922ms step_avg:123.37ms
step:59/1395 train_time:6046ms step_avg:123.38ms
step:60/1395 train_time:6171ms step_avg:123.41ms
step:61/1395 train_time:6294ms step_avg:123.40ms
step:62/1395 train_time:6417ms step_avg:123.40ms
step:63/1395 train_time:6541ms step_avg:123.41ms
step:64/1395 train_time:6664ms step_avg:123.41ms
step:65/1395 train_time:6788ms step_avg:123.41ms
step:66/1395 train_time:6911ms step_avg:123.40ms
step:67/1395 train_time:7034ms step_avg:123.40ms
step:68/1395 train_time:7157ms step_avg:123.39ms
step:69/1395 train_time:7279ms step_avg:123.37ms
step:70/1395 train_time:7405ms step_avg:123.41ms
step:71/1395 train_time:7528ms step_avg:123.40ms
step:72/1395 train_time:7651ms step_avg:123.40ms
step:73/1395 train_time:7776ms step_avg:123.42ms
step:74/1395 train_time:7900ms step_avg:123.43ms
step:75/1395 train_time:8024ms step_avg:123.45ms
step:76/1395 train_time:8149ms step_avg:123.47ms
step:77/1395 train_time:8272ms step_avg:123.47ms
step:78/1395 train_time:8395ms step_avg:123.46ms
step:79/1395 train_time:8518ms step_avg:123.46ms
step:80/1395 train_time:8641ms step_avg:123.44ms
step:81/1395 train_time:8765ms step_avg:123.45ms
step:82/1395 train_time:8888ms step_avg:123.45ms
step:83/1395 train_time:9011ms step_avg:123.44ms
step:84/1395 train_time:9135ms step_avg:123.44ms
step:85/1395 train_time:9258ms step_avg:123.43ms
step:86/1395 train_time:9382ms step_avg:123.45ms
step:87/1395 train_time:9505ms step_avg:123.44ms
step:88/1395 train_time:9628ms step_avg:123.44ms
step:89/1395 train_time:9753ms step_avg:123.46ms
step:90/1395 train_time:9878ms step_avg:123.47ms
step:91/1395 train_time:10003ms step_avg:123.49ms
step:92/1395 train_time:10126ms step_avg:123.48ms
step:93/1395 train_time:10249ms step_avg:123.48ms
step:94/1395 train_time:10373ms step_avg:123.49ms
step:95/1395 train_time:10496ms step_avg:123.49ms
step:96/1395 train_time:10620ms step_avg:123.48ms
step:97/1395 train_time:10743ms step_avg:123.48ms
step:98/1395 train_time:10866ms step_avg:123.48ms
step:99/1395 train_time:10990ms step_avg:123.48ms
step:100/1395 train_time:11112ms step_avg:123.47ms
step:101/1395 train_time:11235ms step_avg:123.47ms
step:102/1395 train_time:11359ms step_avg:123.47ms
step:103/1395 train_time:11482ms step_avg:123.46ms
step:104/1395 train_time:11605ms step_avg:123.46ms
step:105/1395 train_time:11729ms step_avg:123.46ms
step:106/1395 train_time:11854ms step_avg:123.48ms
step:107/1395 train_time:11980ms step_avg:123.50ms
step:108/1395 train_time:12106ms step_avg:123.53ms
step:109/1395 train_time:12232ms step_avg:123.55ms
step:110/1395 train_time:12359ms step_avg:123.59ms
step:111/1395 train_time:12485ms step_avg:123.62ms
step:112/1395 train_time:12612ms step_avg:123.64ms
step:113/1395 train_time:12737ms step_avg:123.66ms
step:114/1395 train_time:12863ms step_avg:123.68ms
step:115/1395 train_time:12990ms step_avg:123.71ms
step:116/1395 train_time:13116ms step_avg:123.73ms
step:117/1395 train_time:13242ms step_avg:123.76ms
step:118/1395 train_time:13369ms step_avg:123.78ms
step:119/1395 train_time:13494ms step_avg:123.80ms
step:120/1395 train_time:13620ms step_avg:123.82ms
step:121/1395 train_time:13746ms step_avg:123.84ms
step:122/1395 train_time:13872ms step_avg:123.85ms
step:123/1395 train_time:13997ms step_avg:123.86ms
step:124/1395 train_time:14123ms step_avg:123.88ms
step:125/1395 train_time:14248ms step_avg:123.90ms
step:125/1395 val_loss:4.3548 train_time:14349ms step_avg:124.77ms
step:126/1395 train_time:14377ms step_avg:123.94ms
step:127/1395 train_time:14516ms step_avg:124.07ms
step:128/1395 train_time:14646ms step_avg:124.12ms
step:129/1395 train_time:14772ms step_avg:124.13ms
step:130/1395 train_time:14898ms step_avg:124.15ms
step:131/1395 train_time:15023ms step_avg:124.16ms
step:132/1395 train_time:15149ms step_avg:124.18ms
step:133/1395 train_time:15275ms step_avg:124.19ms
step:134/1395 train_time:15401ms step_avg:124.20ms
step:135/1395 train_time:15527ms step_avg:124.22ms
step:136/1395 train_time:15653ms step_avg:124.23ms
step:137/1395 train_time:15779ms step_avg:124.24ms
step:138/1395 train_time:15905ms step_avg:124.26ms
step:139/1395 train_time:16031ms step_avg:124.28ms
step:140/1395 train_time:16157ms step_avg:124.28ms
step:141/1395 train_time:16284ms step_avg:124.31ms
step:142/1395 train_time:16411ms step_avg:124.32ms
step:143/1395 train_time:16537ms step_avg:124.34ms
step:144/1395 train_time:16663ms step_avg:124.35ms
step:145/1395 train_time:16789ms step_avg:124.36ms
step:146/1395 train_time:16915ms step_avg:124.37ms
step:147/1395 train_time:17041ms step_avg:124.39ms
step:148/1395 train_time:17168ms step_avg:124.40ms
step:149/1395 train_time:17293ms step_avg:124.41ms
step:150/1395 train_time:17420ms step_avg:124.43ms
step:151/1395 train_time:17547ms step_avg:124.45ms
step:152/1395 train_time:17673ms step_avg:124.46ms
step:153/1395 train_time:17800ms step_avg:124.47ms
step:154/1395 train_time:17926ms step_avg:124.48ms
step:155/1395 train_time:18052ms step_avg:124.49ms
step:156/1395 train_time:18178ms step_avg:124.50ms
step:157/1395 train_time:18304ms step_avg:124.52ms
step:158/1395 train_time:18430ms step_avg:124.53ms
step:159/1395 train_time:18556ms step_avg:124.54ms
step:160/1395 train_time:18683ms step_avg:124.55ms
step:161/1395 train_time:18809ms step_avg:124.56ms
step:162/1395 train_time:18936ms step_avg:124.58ms
step:163/1395 train_time:19062ms step_avg:124.59ms
step:164/1395 train_time:19189ms step_avg:124.60ms
step:165/1395 train_time:19314ms step_avg:124.60ms
step:166/1395 train_time:19440ms step_avg:124.61ms
step:167/1395 train_time:19566ms step_avg:124.63ms
step:168/1395 train_time:19692ms step_avg:124.64ms
step:169/1395 train_time:19818ms step_avg:124.64ms
step:170/1395 train_time:19945ms step_avg:124.65ms
step:171/1395 train_time:20070ms step_avg:124.66ms
step:172/1395 train_time:20196ms step_avg:124.67ms
step:173/1395 train_time:20323ms step_avg:124.68ms
step:174/1395 train_time:20449ms step_avg:124.69ms
step:175/1395 train_time:20576ms step_avg:124.70ms
step:176/1395 train_time:20702ms step_avg:124.71ms
step:177/1395 train_time:20828ms step_avg:124.72ms
step:178/1395 train_time:20954ms step_avg:124.73ms
step:179/1395 train_time:21080ms step_avg:124.74ms
step:180/1395 train_time:21207ms step_avg:124.74ms
step:181/1395 train_time:21333ms step_avg:124.76ms
step:182/1395 train_time:21459ms step_avg:124.76ms
step:183/1395 train_time:21585ms step_avg:124.77ms
step:184/1395 train_time:21711ms step_avg:124.78ms
step:185/1395 train_time:21837ms step_avg:124.78ms
step:186/1395 train_time:21963ms step_avg:124.79ms
step:187/1395 train_time:22089ms step_avg:124.80ms
step:188/1395 train_time:22216ms step_avg:124.81ms
step:189/1395 train_time:22343ms step_avg:124.82ms
step:190/1395 train_time:22469ms step_avg:124.83ms
step:191/1395 train_time:22595ms step_avg:124.83ms
step:192/1395 train_time:22721ms step_avg:124.84ms
step:193/1395 train_time:22846ms step_avg:124.84ms
step:194/1395 train_time:22972ms step_avg:124.85ms
step:195/1395 train_time:23098ms step_avg:124.85ms
step:196/1395 train_time:23224ms step_avg:124.86ms
step:197/1395 train_time:23349ms step_avg:124.86ms
step:198/1395 train_time:23476ms step_avg:124.87ms
step:199/1395 train_time:23602ms step_avg:124.88ms
step:200/1395 train_time:23729ms step_avg:124.89ms
step:201/1395 train_time:23855ms step_avg:124.90ms
step:202/1395 train_time:23982ms step_avg:124.91ms
step:203/1395 train_time:24107ms step_avg:124.91ms
step:204/1395 train_time:24232ms step_avg:124.91ms
step:205/1395 train_time:24358ms step_avg:124.91ms
step:206/1395 train_time:24485ms step_avg:124.92ms
step:207/1395 train_time:24610ms step_avg:124.92ms
step:208/1395 train_time:24736ms step_avg:124.93ms
step:209/1395 train_time:24865ms step_avg:124.95ms
step:210/1395 train_time:24993ms step_avg:124.97ms
step:211/1395 train_time:25122ms step_avg:124.98ms
step:212/1395 train_time:25250ms step_avg:125.00ms
step:213/1395 train_time:25378ms step_avg:125.02ms
step:214/1395 train_time:25507ms step_avg:125.04ms
step:215/1395 train_time:25635ms step_avg:125.05ms
step:216/1395 train_time:25764ms step_avg:125.07ms
step:217/1395 train_time:25892ms step_avg:125.08ms
step:218/1395 train_time:26021ms step_avg:125.10ms
step:219/1395 train_time:26149ms step_avg:125.12ms
step:220/1395 train_time:26276ms step_avg:125.13ms
step:221/1395 train_time:26405ms step_avg:125.14ms
step:222/1395 train_time:26534ms step_avg:125.16ms
step:223/1395 train_time:26662ms step_avg:125.18ms
step:224/1395 train_time:26791ms step_avg:125.19ms
step:225/1395 train_time:26920ms step_avg:125.21ms
step:226/1395 train_time:27048ms step_avg:125.22ms
step:227/1395 train_time:27176ms step_avg:125.23ms
step:228/1395 train_time:27305ms step_avg:125.25ms
step:229/1395 train_time:27433ms step_avg:125.26ms
step:230/1395 train_time:27560ms step_avg:125.27ms
step:231/1395 train_time:27688ms step_avg:125.29ms
step:232/1395 train_time:27815ms step_avg:125.29ms
step:233/1395 train_time:27944ms step_avg:125.31ms
step:234/1395 train_time:28072ms step_avg:125.32ms
step:235/1395 train_time:28200ms step_avg:125.33ms
step:236/1395 train_time:28328ms step_avg:125.35ms
step:237/1395 train_time:28456ms step_avg:125.36ms
step:238/1395 train_time:28586ms step_avg:125.38ms
step:239/1395 train_time:28715ms step_avg:125.39ms
step:240/1395 train_time:28844ms step_avg:125.41ms
step:241/1395 train_time:28972ms step_avg:125.42ms
step:242/1395 train_time:29100ms step_avg:125.43ms
step:243/1395 train_time:29229ms step_avg:125.45ms
step:244/1395 train_time:29357ms step_avg:125.46ms
step:245/1395 train_time:29486ms step_avg:125.47ms
step:246/1395 train_time:29614ms step_avg:125.48ms
step:247/1395 train_time:29743ms step_avg:125.50ms
step:248/1395 train_time:29871ms step_avg:125.51ms
step:249/1395 train_time:29999ms step_avg:125.52ms
step:250/1395 train_time:30127ms step_avg:125.53ms
step:250/1395 val_loss:3.9517 train_time:30230ms step_avg:125.96ms
step:251/1395 train_time:30261ms step_avg:125.56ms
step:252/1395 train_time:30400ms step_avg:125.62ms
step:253/1395 train_time:30530ms step_avg:125.64ms
step:254/1395 train_time:30659ms step_avg:125.65ms
step:255/1395 train_time:30786ms step_avg:125.66ms
step:256/1395 train_time:30914ms step_avg:125.67ms
step:257/1395 train_time:31042ms step_avg:125.68ms
step:258/1395 train_time:31170ms step_avg:125.69ms
step:259/1395 train_time:31298ms step_avg:125.70ms
step:260/1395 train_time:31428ms step_avg:125.71ms
step:261/1395 train_time:31556ms step_avg:125.72ms
step:262/1395 train_time:31684ms step_avg:125.73ms
step:263/1395 train_time:31812ms step_avg:125.74ms
step:264/1395 train_time:31941ms step_avg:125.75ms
step:265/1395 train_time:32069ms step_avg:125.76ms
step:266/1395 train_time:32198ms step_avg:125.77ms
step:267/1395 train_time:32326ms step_avg:125.78ms
step:268/1395 train_time:32455ms step_avg:125.80ms
step:269/1395 train_time:32585ms step_avg:125.81ms
step:270/1395 train_time:32713ms step_avg:125.82ms
step:271/1395 train_time:32841ms step_avg:125.83ms
step:272/1395 train_time:32969ms step_avg:125.84ms
step:273/1395 train_time:33097ms step_avg:125.85ms
step:274/1395 train_time:33225ms step_avg:125.85ms
step:275/1395 train_time:33354ms step_avg:125.86ms
step:276/1395 train_time:33483ms step_avg:125.87ms
step:277/1395 train_time:33611ms step_avg:125.88ms
step:278/1395 train_time:33739ms step_avg:125.89ms
step:279/1395 train_time:33868ms step_avg:125.90ms
step:280/1395 train_time:33997ms step_avg:125.91ms
step:281/1395 train_time:34125ms step_avg:125.92ms
step:282/1395 train_time:34254ms step_avg:125.94ms
step:283/1395 train_time:34383ms step_avg:125.94ms
step:284/1395 train_time:34510ms step_avg:125.95ms
step:285/1395 train_time:34639ms step_avg:125.96ms
step:286/1395 train_time:34767ms step_avg:125.97ms
step:287/1395 train_time:34896ms step_avg:125.98ms
step:288/1395 train_time:35024ms step_avg:125.99ms
step:289/1395 train_time:35151ms step_avg:125.99ms
step:290/1395 train_time:35281ms step_avg:126.00ms
step:291/1395 train_time:35410ms step_avg:126.01ms
step:292/1395 train_time:35538ms step_avg:126.02ms
step:293/1395 train_time:35666ms step_avg:126.03ms
step:294/1395 train_time:35795ms step_avg:126.04ms
step:295/1395 train_time:35922ms step_avg:126.04ms
step:296/1395 train_time:36050ms step_avg:126.05ms
step:297/1395 train_time:36179ms step_avg:126.06ms
step:298/1395 train_time:36307ms step_avg:126.07ms
step:299/1395 train_time:36435ms step_avg:126.07ms
step:300/1395 train_time:36564ms step_avg:126.08ms
step:301/1395 train_time:36691ms step_avg:126.09ms
step:302/1395 train_time:36821ms step_avg:126.10ms
step:303/1395 train_time:36949ms step_avg:126.11ms
step:304/1395 train_time:37079ms step_avg:126.12ms
step:305/1395 train_time:37207ms step_avg:126.13ms
step:306/1395 train_time:37336ms step_avg:126.13ms
step:307/1395 train_time:37464ms step_avg:126.14ms
step:308/1395 train_time:37592ms step_avg:126.15ms
step:309/1395 train_time:37720ms step_avg:126.15ms
step:310/1395 train_time:37849ms step_avg:126.16ms
step:311/1395 train_time:37978ms step_avg:126.17ms
step:312/1395 train_time:38107ms step_avg:126.18ms
step:313/1395 train_time:38239ms step_avg:126.20ms
step:314/1395 train_time:38369ms step_avg:126.21ms
step:315/1395 train_time:38500ms step_avg:126.23ms
step:316/1395 train_time:38629ms step_avg:126.24ms
step:317/1395 train_time:38761ms step_avg:126.26ms
step:318/1395 train_time:38891ms step_avg:126.27ms
step:319/1395 train_time:39022ms step_avg:126.29ms
step:320/1395 train_time:39153ms step_avg:126.30ms
step:321/1395 train_time:39284ms step_avg:126.32ms
step:322/1395 train_time:39414ms step_avg:126.33ms
step:323/1395 train_time:39545ms step_avg:126.34ms
step:324/1395 train_time:39675ms step_avg:126.35ms
step:325/1395 train_time:39805ms step_avg:126.36ms
step:326/1395 train_time:39934ms step_avg:126.37ms
step:327/1395 train_time:40066ms step_avg:126.39ms
step:328/1395 train_time:40197ms step_avg:126.41ms
step:329/1395 train_time:40328ms step_avg:126.42ms
step:330/1395 train_time:40459ms step_avg:126.43ms
step:331/1395 train_time:40589ms step_avg:126.45ms
step:332/1395 train_time:40721ms step_avg:126.46ms
step:333/1395 train_time:40850ms step_avg:126.47ms
step:334/1395 train_time:40981ms step_avg:126.48ms
step:335/1395 train_time:41111ms step_avg:126.50ms
step:336/1395 train_time:41242ms step_avg:126.51ms
step:337/1395 train_time:41371ms step_avg:126.52ms
step:338/1395 train_time:41502ms step_avg:126.53ms
step:339/1395 train_time:41632ms step_avg:126.54ms
step:340/1395 train_time:41763ms step_avg:126.56ms
step:341/1395 train_time:41893ms step_avg:126.57ms
step:342/1395 train_time:42024ms step_avg:126.58ms
step:343/1395 train_time:42155ms step_avg:126.59ms
step:344/1395 train_time:42286ms step_avg:126.61ms
step:345/1395 train_time:42417ms step_avg:126.62ms
step:346/1395 train_time:42547ms step_avg:126.63ms
step:347/1395 train_time:42679ms step_avg:126.64ms
step:348/1395 train_time:42809ms step_avg:126.65ms
step:349/1395 train_time:42940ms step_avg:126.67ms
step:350/1395 train_time:43070ms step_avg:126.68ms
step:351/1395 train_time:43201ms step_avg:126.69ms
step:352/1395 train_time:43331ms step_avg:126.70ms
step:353/1395 train_time:43461ms step_avg:126.71ms
step:354/1395 train_time:43590ms step_avg:126.72ms
step:355/1395 train_time:43721ms step_avg:126.73ms
step:356/1395 train_time:43852ms step_avg:126.74ms
step:357/1395 train_time:43982ms step_avg:126.75ms
step:358/1395 train_time:44112ms step_avg:126.76ms
step:359/1395 train_time:44242ms step_avg:126.77ms
step:360/1395 train_time:44372ms step_avg:126.78ms
step:361/1395 train_time:44504ms step_avg:126.79ms
step:362/1395 train_time:44634ms step_avg:126.80ms
step:363/1395 train_time:44765ms step_avg:126.81ms
step:364/1395 train_time:44895ms step_avg:126.82ms
step:365/1395 train_time:45026ms step_avg:126.83ms
step:366/1395 train_time:45158ms step_avg:126.85ms
step:367/1395 train_time:45288ms step_avg:126.86ms
step:368/1395 train_time:45418ms step_avg:126.87ms
step:369/1395 train_time:45549ms step_avg:126.88ms
step:370/1395 train_time:45679ms step_avg:126.89ms
step:371/1395 train_time:45810ms step_avg:126.90ms
step:372/1395 train_time:45940ms step_avg:126.91ms
step:373/1395 train_time:46071ms step_avg:126.92ms
step:374/1395 train_time:46201ms step_avg:126.93ms
step:375/1395 train_time:46331ms step_avg:126.93ms
step:375/1395 val_loss:3.7775 train_time:46436ms step_avg:127.22ms
step:376/1395 train_time:46467ms step_avg:126.96ms
step:377/1395 train_time:46606ms step_avg:126.99ms
step:378/1395 train_time:46738ms step_avg:127.00ms
step:379/1395 train_time:46867ms step_avg:127.01ms
step:380/1395 train_time:46998ms step_avg:127.02ms
step:381/1395 train_time:47127ms step_avg:127.03ms
step:382/1395 train_time:47258ms step_avg:127.04ms
step:383/1395 train_time:47387ms step_avg:127.04ms
step:384/1395 train_time:47519ms step_avg:127.06ms
step:385/1395 train_time:47650ms step_avg:127.07ms
step:386/1395 train_time:47781ms step_avg:127.08ms
step:387/1395 train_time:47911ms step_avg:127.08ms
step:388/1395 train_time:48042ms step_avg:127.09ms
step:389/1395 train_time:48170ms step_avg:127.10ms
step:390/1395 train_time:48301ms step_avg:127.11ms
step:391/1395 train_time:48430ms step_avg:127.11ms
step:392/1395 train_time:48561ms step_avg:127.12ms
step:393/1395 train_time:48691ms step_avg:127.13ms
step:394/1395 train_time:48823ms step_avg:127.14ms
step:395/1395 train_time:48955ms step_avg:127.15ms
step:396/1395 train_time:49085ms step_avg:127.16ms
step:397/1395 train_time:49216ms step_avg:127.17ms
step:398/1395 train_time:49346ms step_avg:127.18ms
step:399/1395 train_time:49477ms step_avg:127.19ms
step:400/1395 train_time:49607ms step_avg:127.20ms
step:401/1395 train_time:49738ms step_avg:127.21ms
step:402/1395 train_time:49868ms step_avg:127.21ms
step:403/1395 train_time:49999ms step_avg:127.22ms
step:404/1395 train_time:50129ms step_avg:127.23ms
step:405/1395 train_time:50259ms step_avg:127.24ms
step:406/1395 train_time:50389ms step_avg:127.24ms
step:407/1395 train_time:50520ms step_avg:127.25ms
step:408/1395 train_time:50650ms step_avg:127.26ms
step:409/1395 train_time:50781ms step_avg:127.27ms
step:410/1395 train_time:50910ms step_avg:127.28ms
step:411/1395 train_time:51041ms step_avg:127.29ms
step:412/1395 train_time:51172ms step_avg:127.29ms
step:413/1395 train_time:51302ms step_avg:127.30ms
step:414/1395 train_time:51432ms step_avg:127.31ms
step:415/1395 train_time:51564ms step_avg:127.32ms
step:416/1395 train_time:51696ms step_avg:127.33ms
step:417/1395 train_time:51829ms step_avg:127.34ms
step:418/1395 train_time:51961ms step_avg:127.36ms
step:419/1395 train_time:52092ms step_avg:127.37ms
step:420/1395 train_time:52225ms step_avg:127.38ms
step:421/1395 train_time:52358ms step_avg:127.39ms
step:422/1395 train_time:52490ms step_avg:127.40ms
step:423/1395 train_time:52623ms step_avg:127.42ms
step:424/1395 train_time:52755ms step_avg:127.43ms
step:425/1395 train_time:52890ms step_avg:127.44ms
step:426/1395 train_time:53021ms step_avg:127.45ms
step:427/1395 train_time:53153ms step_avg:127.46ms
step:428/1395 train_time:53285ms step_avg:127.48ms
step:429/1395 train_time:53417ms step_avg:127.49ms
step:430/1395 train_time:53549ms step_avg:127.50ms
step:431/1395 train_time:53682ms step_avg:127.51ms
step:432/1395 train_time:53816ms step_avg:127.53ms
step:433/1395 train_time:53949ms step_avg:127.54ms
step:434/1395 train_time:54081ms step_avg:127.55ms
step:435/1395 train_time:54213ms step_avg:127.56ms
step:436/1395 train_time:54345ms step_avg:127.57ms
step:437/1395 train_time:54477ms step_avg:127.58ms
step:438/1395 train_time:54610ms step_avg:127.59ms
step:439/1395 train_time:54742ms step_avg:127.60ms
step:440/1395 train_time:54873ms step_avg:127.61ms
step:441/1395 train_time:55006ms step_avg:127.62ms
step:442/1395 train_time:55138ms step_avg:127.63ms
step:443/1395 train_time:55269ms step_avg:127.64ms
step:444/1395 train_time:55402ms step_avg:127.65ms
step:445/1395 train_time:55534ms step_avg:127.66ms
step:446/1395 train_time:55667ms step_avg:127.68ms
step:447/1395 train_time:55799ms step_avg:127.69ms
step:448/1395 train_time:55930ms step_avg:127.69ms
step:449/1395 train_time:56063ms step_avg:127.71ms
step:450/1395 train_time:56197ms step_avg:127.72ms
step:451/1395 train_time:56329ms step_avg:127.73ms
step:452/1395 train_time:56461ms step_avg:127.74ms
step:453/1395 train_time:56595ms step_avg:127.75ms
step:454/1395 train_time:56727ms step_avg:127.76ms
step:455/1395 train_time:56859ms step_avg:127.77ms
step:456/1395 train_time:56991ms step_avg:127.78ms
step:457/1395 train_time:57123ms step_avg:127.79ms
step:458/1395 train_time:57256ms step_avg:127.80ms
step:459/1395 train_time:57388ms step_avg:127.81ms
step:460/1395 train_time:57520ms step_avg:127.82ms
step:461/1395 train_time:57652ms step_avg:127.83ms
step:462/1395 train_time:57785ms step_avg:127.84ms
step:463/1395 train_time:57918ms step_avg:127.85ms
step:464/1395 train_time:58050ms step_avg:127.86ms
step:465/1395 train_time:58182ms step_avg:127.87ms
step:466/1395 train_time:58314ms step_avg:127.88ms
step:467/1395 train_time:58446ms step_avg:127.89ms
step:468/1395 train_time:58579ms step_avg:127.90ms
step:469/1395 train_time:58710ms step_avg:127.91ms
step:470/1395 train_time:58843ms step_avg:127.92ms
step:471/1395 train_time:58975ms step_avg:127.93ms
step:472/1395 train_time:59108ms step_avg:127.94ms
step:473/1395 train_time:59240ms step_avg:127.95ms
step:474/1395 train_time:59372ms step_avg:127.96ms
step:475/1395 train_time:59504ms step_avg:127.96ms
step:476/1395 train_time:59636ms step_avg:127.97ms
step:477/1395 train_time:59768ms step_avg:127.98ms
step:478/1395 train_time:59901ms step_avg:127.99ms
step:479/1395 train_time:60033ms step_avg:128.00ms
step:480/1395 train_time:60166ms step_avg:128.01ms
step:481/1395 train_time:60297ms step_avg:128.02ms
step:482/1395 train_time:60430ms step_avg:128.03ms
step:483/1395 train_time:60563ms step_avg:128.04ms
step:484/1395 train_time:60695ms step_avg:128.05ms
step:485/1395 train_time:60827ms step_avg:128.06ms
step:486/1395 train_time:60960ms step_avg:128.07ms
step:487/1395 train_time:61092ms step_avg:128.08ms
step:488/1395 train_time:61224ms step_avg:128.08ms
step:489/1395 train_time:61356ms step_avg:128.09ms
step:490/1395 train_time:61488ms step_avg:128.10ms
step:491/1395 train_time:61622ms step_avg:128.11ms
step:492/1395 train_time:61753ms step_avg:128.12ms
step:493/1395 train_time:61886ms step_avg:128.13ms
step:494/1395 train_time:62020ms step_avg:128.14ms
step:495/1395 train_time:62152ms step_avg:128.15ms
step:496/1395 train_time:62284ms step_avg:128.16ms
step:497/1395 train_time:62416ms step_avg:128.16ms
step:498/1395 train_time:62548ms step_avg:128.17ms
step:499/1395 train_time:62681ms step_avg:128.18ms
step:500/1395 train_time:62812ms step_avg:128.19ms
step:500/1395 val_loss:3.6601 train_time:62918ms step_avg:128.40ms
step:501/1395 train_time:62948ms step_avg:128.20ms
step:502/1395 train_time:63084ms step_avg:128.22ms
step:503/1395 train_time:63217ms step_avg:128.23ms
step:504/1395 train_time:63349ms step_avg:128.24ms
step:505/1395 train_time:63480ms step_avg:128.24ms
step:506/1395 train_time:63613ms step_avg:128.25ms
step:507/1395 train_time:63744ms step_avg:128.26ms
step:508/1395 train_time:63877ms step_avg:128.27ms
step:509/1395 train_time:64009ms step_avg:128.28ms
step:510/1395 train_time:64143ms step_avg:128.29ms
step:511/1395 train_time:64274ms step_avg:128.29ms
step:512/1395 train_time:64406ms step_avg:128.30ms
step:513/1395 train_time:64538ms step_avg:128.31ms
step:514/1395 train_time:64670ms step_avg:128.31ms
step:515/1395 train_time:64802ms step_avg:128.32ms
step:516/1395 train_time:64935ms step_avg:128.33ms
step:517/1395 train_time:65067ms step_avg:128.34ms
step:518/1395 train_time:65199ms step_avg:128.34ms
step:519/1395 train_time:65334ms step_avg:128.36ms
step:520/1395 train_time:65468ms step_avg:128.37ms
step:521/1395 train_time:65601ms step_avg:128.38ms
step:522/1395 train_time:65734ms step_avg:128.39ms
step:523/1395 train_time:65869ms step_avg:128.40ms
step:524/1395 train_time:66002ms step_avg:128.41ms
step:525/1395 train_time:66136ms step_avg:128.42ms
step:526/1395 train_time:66270ms step_avg:128.43ms
step:527/1395 train_time:66403ms step_avg:128.44ms
step:528/1395 train_time:66536ms step_avg:128.45ms
step:529/1395 train_time:66669ms step_avg:128.46ms
step:530/1395 train_time:66802ms step_avg:128.47ms
step:531/1395 train_time:66936ms step_avg:128.48ms
step:532/1395 train_time:67070ms step_avg:128.49ms
step:533/1395 train_time:67205ms step_avg:128.50ms
step:534/1395 train_time:67339ms step_avg:128.51ms
step:535/1395 train_time:67473ms step_avg:128.52ms
step:536/1395 train_time:67607ms step_avg:128.53ms
step:537/1395 train_time:67740ms step_avg:128.54ms
step:538/1395 train_time:67875ms step_avg:128.55ms
step:539/1395 train_time:68009ms step_avg:128.56ms
step:540/1395 train_time:68143ms step_avg:128.57ms
step:541/1395 train_time:68278ms step_avg:128.58ms
step:542/1395 train_time:68411ms step_avg:128.59ms
step:543/1395 train_time:68544ms step_avg:128.60ms
step:544/1395 train_time:68677ms step_avg:128.61ms
step:545/1395 train_time:68812ms step_avg:128.62ms
step:546/1395 train_time:68946ms step_avg:128.63ms
step:547/1395 train_time:69079ms step_avg:128.64ms
step:548/1395 train_time:69214ms step_avg:128.65ms
step:549/1395 train_time:69348ms step_avg:128.66ms
step:550/1395 train_time:69483ms step_avg:128.67ms
step:551/1395 train_time:69616ms step_avg:128.68ms
step:552/1395 train_time:69750ms step_avg:128.69ms
step:553/1395 train_time:69883ms step_avg:128.70ms
step:554/1395 train_time:70017ms step_avg:128.71ms
step:555/1395 train_time:70150ms step_avg:128.72ms
step:556/1395 train_time:70282ms step_avg:128.72ms
step:557/1395 train_time:70417ms step_avg:128.73ms
step:558/1395 train_time:70551ms step_avg:128.74ms
step:559/1395 train_time:70683ms step_avg:128.75ms
step:560/1395 train_time:70817ms step_avg:128.76ms
step:561/1395 train_time:70951ms step_avg:128.77ms
step:562/1395 train_time:71085ms step_avg:128.78ms
step:563/1395 train_time:71219ms step_avg:128.79ms
step:564/1395 train_time:71353ms step_avg:128.80ms
step:565/1395 train_time:71487ms step_avg:128.81ms
step:566/1395 train_time:71621ms step_avg:128.81ms
step:567/1395 train_time:71754ms step_avg:128.82ms
step:568/1395 train_time:71888ms step_avg:128.83ms
step:569/1395 train_time:72023ms step_avg:128.84ms
step:570/1395 train_time:72157ms step_avg:128.85ms
step:571/1395 train_time:72290ms step_avg:128.86ms
step:572/1395 train_time:72424ms step_avg:128.87ms
step:573/1395 train_time:72557ms step_avg:128.88ms
step:574/1395 train_time:72693ms step_avg:128.89ms
step:575/1395 train_time:72827ms step_avg:128.90ms
step:576/1395 train_time:72961ms step_avg:128.91ms
step:577/1395 train_time:73094ms step_avg:128.91ms
step:578/1395 train_time:73227ms step_avg:128.92ms
step:579/1395 train_time:73360ms step_avg:128.93ms
step:580/1395 train_time:73495ms step_avg:128.94ms
step:581/1395 train_time:73631ms step_avg:128.95ms
step:582/1395 train_time:73764ms step_avg:128.96ms
step:583/1395 train_time:73898ms step_avg:128.97ms
step:584/1395 train_time:74033ms step_avg:128.98ms
step:585/1395 train_time:74166ms step_avg:128.98ms
step:586/1395 train_time:74299ms step_avg:128.99ms
step:587/1395 train_time:74433ms step_avg:129.00ms
step:588/1395 train_time:74567ms step_avg:129.01ms
step:589/1395 train_time:74700ms step_avg:129.02ms
step:590/1395 train_time:74835ms step_avg:129.03ms
step:591/1395 train_time:74969ms step_avg:129.03ms
step:592/1395 train_time:75102ms step_avg:129.04ms
step:593/1395 train_time:75235ms step_avg:129.05ms
step:594/1395 train_time:75369ms step_avg:129.06ms
step:595/1395 train_time:75503ms step_avg:129.07ms
step:596/1395 train_time:75638ms step_avg:129.07ms
step:597/1395 train_time:75772ms step_avg:129.08ms
step:598/1395 train_time:75905ms step_avg:129.09ms
step:599/1395 train_time:76038ms step_avg:129.10ms
step:600/1395 train_time:76172ms step_avg:129.10ms
step:601/1395 train_time:76305ms step_avg:129.11ms
step:602/1395 train_time:76438ms step_avg:129.12ms
step:603/1395 train_time:76574ms step_avg:129.13ms
step:604/1395 train_time:76707ms step_avg:129.14ms
step:605/1395 train_time:76841ms step_avg:129.15ms
step:606/1395 train_time:76976ms step_avg:129.15ms
step:607/1395 train_time:77109ms step_avg:129.16ms
step:608/1395 train_time:77243ms step_avg:129.17ms
step:609/1395 train_time:77376ms step_avg:129.18ms
step:610/1395 train_time:77510ms step_avg:129.18ms
step:611/1395 train_time:77643ms step_avg:129.19ms
step:612/1395 train_time:77776ms step_avg:129.20ms
step:613/1395 train_time:77910ms step_avg:129.20ms
step:614/1395 train_time:78043ms step_avg:129.21ms
step:615/1395 train_time:78177ms step_avg:129.22ms
step:616/1395 train_time:78310ms step_avg:129.22ms
step:617/1395 train_time:78444ms step_avg:129.23ms
step:618/1395 train_time:78578ms step_avg:129.24ms
step:619/1395 train_time:78711ms step_avg:129.25ms
step:620/1395 train_time:78844ms step_avg:129.25ms
step:621/1395 train_time:78978ms step_avg:129.26ms
step:622/1395 train_time:79113ms step_avg:129.27ms
step:623/1395 train_time:79248ms step_avg:129.28ms
step:624/1395 train_time:79383ms step_avg:129.29ms
step:625/1395 train_time:79518ms step_avg:129.30ms
step:625/1395 val_loss:3.5787 train_time:79628ms step_avg:129.48ms
step:626/1395 train_time:79659ms step_avg:129.32ms
step:627/1395 train_time:79800ms step_avg:129.34ms
step:628/1395 train_time:79935ms step_avg:129.34ms
step:629/1395 train_time:80069ms step_avg:129.35ms
step:630/1395 train_time:80203ms step_avg:129.36ms
step:631/1395 train_time:80338ms step_avg:129.37ms
step:632/1395 train_time:80471ms step_avg:129.37ms
step:633/1395 train_time:80606ms step_avg:129.38ms
step:634/1395 train_time:80741ms step_avg:129.39ms
step:635/1395 train_time:80878ms step_avg:129.40ms
step:636/1395 train_time:81012ms step_avg:129.41ms
step:637/1395 train_time:81147ms step_avg:129.42ms
step:638/1395 train_time:81281ms step_avg:129.43ms
step:639/1395 train_time:81416ms step_avg:129.44ms
step:640/1395 train_time:81551ms step_avg:129.45ms
step:641/1395 train_time:81685ms step_avg:129.45ms
step:642/1395 train_time:81821ms step_avg:129.46ms
step:643/1395 train_time:81956ms step_avg:129.47ms
step:644/1395 train_time:82091ms step_avg:129.48ms
step:645/1395 train_time:82226ms step_avg:129.49ms
step:646/1395 train_time:82361ms step_avg:129.50ms
step:647/1395 train_time:82496ms step_avg:129.51ms
step:648/1395 train_time:82632ms step_avg:129.52ms
step:649/1395 train_time:82767ms step_avg:129.53ms
step:650/1395 train_time:82903ms step_avg:129.54ms
step:651/1395 train_time:83038ms step_avg:129.54ms
step:652/1395 train_time:83173ms step_avg:129.55ms
step:653/1395 train_time:83308ms step_avg:129.56ms
step:654/1395 train_time:83444ms step_avg:129.57ms
step:655/1395 train_time:83579ms step_avg:129.58ms
step:656/1395 train_time:83713ms step_avg:129.59ms
step:657/1395 train_time:83848ms step_avg:129.60ms
step:658/1395 train_time:83983ms step_avg:129.60ms
step:659/1395 train_time:84118ms step_avg:129.61ms
step:660/1395 train_time:84253ms step_avg:129.62ms
step:661/1395 train_time:84388ms step_avg:129.63ms
step:662/1395 train_time:84522ms step_avg:129.64ms
step:663/1395 train_time:84656ms step_avg:129.64ms
step:664/1395 train_time:84791ms step_avg:129.65ms
step:665/1395 train_time:84927ms step_avg:129.66ms
step:666/1395 train_time:85061ms step_avg:129.67ms
step:667/1395 train_time:85196ms step_avg:129.67ms
step:668/1395 train_time:85331ms step_avg:129.68ms
step:669/1395 train_time:85467ms step_avg:129.69ms
step:670/1395 train_time:85603ms step_avg:129.70ms
step:671/1395 train_time:85739ms step_avg:129.71ms
step:672/1395 train_time:85874ms step_avg:129.72ms
step:673/1395 train_time:86008ms step_avg:129.73ms
step:674/1395 train_time:86143ms step_avg:129.73ms
step:675/1395 train_time:86280ms step_avg:129.74ms
step:676/1395 train_time:86415ms step_avg:129.75ms
step:677/1395 train_time:86550ms step_avg:129.76ms
step:678/1395 train_time:86684ms step_avg:129.77ms
step:679/1395 train_time:86821ms step_avg:129.78ms
step:680/1395 train_time:86956ms step_avg:129.79ms
step:681/1395 train_time:87092ms step_avg:129.79ms
step:682/1395 train_time:87226ms step_avg:129.80ms
step:683/1395 train_time:87361ms step_avg:129.81ms
step:684/1395 train_time:87497ms step_avg:129.82ms
step:685/1395 train_time:87633ms step_avg:129.83ms
step:686/1395 train_time:87768ms step_avg:129.83ms
step:687/1395 train_time:87902ms step_avg:129.84ms
step:688/1395 train_time:88038ms step_avg:129.85ms
step:689/1395 train_time:88174ms step_avg:129.86ms
step:690/1395 train_time:88310ms step_avg:129.87ms
step:691/1395 train_time:88445ms step_avg:129.87ms
step:692/1395 train_time:88580ms step_avg:129.88ms
step:693/1395 train_time:88715ms step_avg:129.89ms
step:694/1395 train_time:88849ms step_avg:129.90ms
step:695/1395 train_time:88983ms step_avg:129.90ms
step:696/1395 train_time:89117ms step_avg:129.91ms
step:697/1395 train_time:89253ms step_avg:129.92ms
step:698/1395 train_time:89388ms step_avg:129.92ms
step:699/1395 train_time:89523ms step_avg:129.93ms
step:700/1395 train_time:89659ms step_avg:129.94ms
step:701/1395 train_time:89793ms step_avg:129.95ms
step:702/1395 train_time:89929ms step_avg:129.95ms
step:703/1395 train_time:90063ms step_avg:129.96ms
step:704/1395 train_time:90198ms step_avg:129.97ms
step:705/1395 train_time:90334ms step_avg:129.98ms
step:706/1395 train_time:90471ms step_avg:129.99ms
step:707/1395 train_time:90606ms step_avg:129.99ms
step:708/1395 train_time:90743ms step_avg:130.00ms
step:709/1395 train_time:90878ms step_avg:130.01ms
step:710/1395 train_time:91014ms step_avg:130.02ms
step:711/1395 train_time:91150ms step_avg:130.03ms
step:712/1395 train_time:91286ms step_avg:130.04ms
step:713/1395 train_time:91421ms step_avg:130.04ms
step:714/1395 train_time:91557ms step_avg:130.05ms
step:715/1395 train_time:91692ms step_avg:130.06ms
step:716/1395 train_time:91826ms step_avg:130.07ms
step:717/1395 train_time:91961ms step_avg:130.07ms
step:718/1395 train_time:92097ms step_avg:130.08ms
step:719/1395 train_time:92230ms step_avg:130.09ms
step:720/1395 train_time:92367ms step_avg:130.09ms
step:721/1395 train_time:92501ms step_avg:130.10ms
step:722/1395 train_time:92637ms step_avg:130.11ms
step:723/1395 train_time:92772ms step_avg:130.12ms
step:724/1395 train_time:92907ms step_avg:130.12ms
step:725/1395 train_time:93042ms step_avg:130.13ms
step:726/1395 train_time:93179ms step_avg:130.14ms
step:727/1395 train_time:93318ms step_avg:130.15ms
step:728/1395 train_time:93454ms step_avg:130.16ms
step:729/1395 train_time:93589ms step_avg:130.17ms
step:730/1395 train_time:93727ms step_avg:130.18ms
step:731/1395 train_time:93863ms step_avg:130.18ms
step:732/1395 train_time:93999ms step_avg:130.19ms
step:733/1395 train_time:94137ms step_avg:130.20ms
step:734/1395 train_time:94273ms step_avg:130.21ms
step:735/1395 train_time:94411ms step_avg:130.22ms
step:736/1395 train_time:94547ms step_avg:130.23ms
step:737/1395 train_time:94683ms step_avg:130.24ms
step:738/1395 train_time:94820ms step_avg:130.25ms
step:739/1395 train_time:94956ms step_avg:130.26ms
step:740/1395 train_time:95093ms step_avg:130.26ms
step:741/1395 train_time:95232ms step_avg:130.28ms
step:742/1395 train_time:95369ms step_avg:130.28ms
step:743/1395 train_time:95504ms step_avg:130.29ms
step:744/1395 train_time:95640ms step_avg:130.30ms
step:745/1395 train_time:95780ms step_avg:130.31ms
step:746/1395 train_time:95916ms step_avg:130.32ms
step:747/1395 train_time:96052ms step_avg:130.33ms
step:748/1395 train_time:96188ms step_avg:130.34ms
step:749/1395 train_time:96325ms step_avg:130.34ms
step:750/1395 train_time:96462ms step_avg:130.35ms
step:750/1395 val_loss:3.5266 train_time:96575ms step_avg:130.51ms
step:751/1395 train_time:96606ms step_avg:130.37ms
step:752/1395 train_time:96747ms step_avg:130.39ms
step:753/1395 train_time:96883ms step_avg:130.39ms
step:754/1395 train_time:97020ms step_avg:130.40ms
step:755/1395 train_time:97155ms step_avg:130.41ms
step:756/1395 train_time:97291ms step_avg:130.42ms
step:757/1395 train_time:97431ms step_avg:130.43ms
step:758/1395 train_time:97569ms step_avg:130.44ms
step:759/1395 train_time:97706ms step_avg:130.45ms
step:760/1395 train_time:97842ms step_avg:130.46ms
step:761/1395 train_time:97978ms step_avg:130.46ms
step:762/1395 train_time:98113ms step_avg:130.47ms
step:763/1395 train_time:98250ms step_avg:130.48ms
step:764/1395 train_time:98387ms step_avg:130.49ms
step:765/1395 train_time:98523ms step_avg:130.49ms
step:766/1395 train_time:98662ms step_avg:130.51ms
step:767/1395 train_time:98799ms step_avg:130.51ms
step:768/1395 train_time:98935ms step_avg:130.52ms
step:769/1395 train_time:99071ms step_avg:130.53ms
step:770/1395 train_time:99208ms step_avg:130.54ms
step:771/1395 train_time:99344ms step_avg:130.54ms
step:772/1395 train_time:99481ms step_avg:130.55ms
step:773/1395 train_time:99618ms step_avg:130.56ms
step:774/1395 train_time:99754ms step_avg:130.57ms
step:775/1395 train_time:99890ms step_avg:130.58ms
step:776/1395 train_time:100027ms step_avg:130.58ms
step:777/1395 train_time:100167ms step_avg:130.60ms
step:778/1395 train_time:100304ms step_avg:130.60ms
step:779/1395 train_time:100440ms step_avg:130.61ms
step:780/1395 train_time:100576ms step_avg:130.62ms
step:781/1395 train_time:100712ms step_avg:130.62ms
step:782/1395 train_time:100848ms step_avg:130.63ms
step:783/1395 train_time:100985ms step_avg:130.64ms
step:784/1395 train_time:101122ms step_avg:130.65ms
step:785/1395 train_time:101259ms step_avg:130.66ms
step:786/1395 train_time:101395ms step_avg:130.66ms
step:787/1395 train_time:101531ms step_avg:130.67ms
step:788/1395 train_time:101667ms step_avg:130.68ms
step:789/1395 train_time:101802ms step_avg:130.68ms
step:790/1395 train_time:101939ms step_avg:130.69ms
step:791/1395 train_time:102075ms step_avg:130.70ms
step:792/1395 train_time:102212ms step_avg:130.71ms
step:793/1395 train_time:102347ms step_avg:130.71ms
step:794/1395 train_time:102485ms step_avg:130.72ms
step:795/1395 train_time:102628ms step_avg:130.74ms
step:796/1395 train_time:102767ms step_avg:130.75ms
step:797/1395 train_time:102904ms step_avg:130.75ms
step:798/1395 train_time:103041ms step_avg:130.76ms
step:799/1395 train_time:103181ms step_avg:130.77ms
step:800/1395 train_time:103316ms step_avg:130.78ms
step:801/1395 train_time:103452ms step_avg:130.79ms
step:802/1395 train_time:103591ms step_avg:130.80ms
step:803/1395 train_time:103725ms step_avg:130.80ms
step:804/1395 train_time:103861ms step_avg:130.81ms
step:805/1395 train_time:103999ms step_avg:130.82ms
step:806/1395 train_time:104134ms step_avg:130.82ms
step:807/1395 train_time:104269ms step_avg:130.83ms
step:808/1395 train_time:104407ms step_avg:130.84ms
step:809/1395 train_time:104544ms step_avg:130.84ms
step:810/1395 train_time:104680ms step_avg:130.85ms
step:811/1395 train_time:104816ms step_avg:130.86ms
step:812/1395 train_time:104952ms step_avg:130.86ms
step:813/1395 train_time:105087ms step_avg:130.87ms
step:814/1395 train_time:105224ms step_avg:130.88ms
step:815/1395 train_time:105361ms step_avg:130.88ms
step:816/1395 train_time:105500ms step_avg:130.89ms
step:817/1395 train_time:105637ms step_avg:130.90ms
step:818/1395 train_time:105771ms step_avg:130.91ms
step:819/1395 train_time:105908ms step_avg:130.91ms
step:820/1395 train_time:106045ms step_avg:130.92ms
step:821/1395 train_time:106181ms step_avg:130.93ms
step:822/1395 train_time:106318ms step_avg:130.93ms
step:823/1395 train_time:106453ms step_avg:130.94ms
step:824/1395 train_time:106589ms step_avg:130.95ms
step:825/1395 train_time:106727ms step_avg:130.95ms
step:826/1395 train_time:106865ms step_avg:130.96ms
step:827/1395 train_time:107003ms step_avg:130.97ms
step:828/1395 train_time:107140ms step_avg:130.98ms
step:829/1395 train_time:107278ms step_avg:130.99ms
step:830/1395 train_time:107416ms step_avg:131.00ms
step:831/1395 train_time:107553ms step_avg:131.00ms
step:832/1395 train_time:107691ms step_avg:131.01ms
step:833/1395 train_time:107827ms step_avg:131.02ms
step:834/1395 train_time:107966ms step_avg:131.03ms
step:835/1395 train_time:108104ms step_avg:131.04ms
step:836/1395 train_time:108245ms step_avg:131.05ms
step:837/1395 train_time:108383ms step_avg:131.06ms
step:838/1395 train_time:108519ms step_avg:131.06ms
step:839/1395 train_time:108656ms step_avg:131.07ms
step:840/1395 train_time:108793ms step_avg:131.08ms
step:841/1395 train_time:108930ms step_avg:131.08ms
step:842/1395 train_time:109068ms step_avg:131.09ms
step:843/1395 train_time:109206ms step_avg:131.10ms
step:844/1395 train_time:109343ms step_avg:131.11ms
step:845/1395 train_time:109480ms step_avg:131.11ms
step:846/1395 train_time:109619ms step_avg:131.12ms
step:847/1395 train_time:109758ms step_avg:131.13ms
step:848/1395 train_time:109894ms step_avg:131.14ms
step:849/1395 train_time:110032ms step_avg:131.15ms
step:850/1395 train_time:110171ms step_avg:131.16ms
step:851/1395 train_time:110310ms step_avg:131.17ms
step:852/1395 train_time:110449ms step_avg:131.17ms
step:853/1395 train_time:110585ms step_avg:131.18ms
step:854/1395 train_time:110721ms step_avg:131.19ms
step:855/1395 train_time:110857ms step_avg:131.19ms
step:856/1395 train_time:110993ms step_avg:131.20ms
step:857/1395 train_time:111131ms step_avg:131.21ms
step:858/1395 train_time:111272ms step_avg:131.22ms
step:859/1395 train_time:111410ms step_avg:131.22ms
step:860/1395 train_time:111546ms step_avg:131.23ms
step:861/1395 train_time:111685ms step_avg:131.24ms
step:862/1395 train_time:111823ms step_avg:131.25ms
step:863/1395 train_time:111962ms step_avg:131.26ms
step:864/1395 train_time:112101ms step_avg:131.27ms
step:865/1395 train_time:112237ms step_avg:131.27ms
step:866/1395 train_time:112384ms step_avg:131.29ms
step:867/1395 train_time:112522ms step_avg:131.30ms
step:868/1395 train_time:112657ms step_avg:131.30ms
step:869/1395 train_time:112794ms step_avg:131.31ms
step:870/1395 train_time:112933ms step_avg:131.32ms
step:871/1395 train_time:113072ms step_avg:131.33ms
step:872/1395 train_time:113208ms step_avg:131.33ms
step:873/1395 train_time:113345ms step_avg:131.34ms
step:874/1395 train_time:113484ms step_avg:131.35ms
step:875/1395 train_time:113624ms step_avg:131.36ms
step:875/1395 val_loss:3.4768 train_time:113737ms step_avg:131.49ms
step:876/1395 train_time:113766ms step_avg:131.37ms
step:877/1395 train_time:113910ms step_avg:131.38ms
step:878/1395 train_time:114049ms step_avg:131.39ms
step:879/1395 train_time:114188ms step_avg:131.40ms
step:880/1395 train_time:114325ms step_avg:131.41ms
step:881/1395 train_time:114463ms step_avg:131.42ms
step:882/1395 train_time:114600ms step_avg:131.42ms
step:883/1395 train_time:114737ms step_avg:131.43ms
step:884/1395 train_time:114874ms step_avg:131.43ms
step:885/1395 train_time:115012ms step_avg:131.44ms
step:886/1395 train_time:115152ms step_avg:131.45ms
step:887/1395 train_time:115290ms step_avg:131.46ms
step:888/1395 train_time:115430ms step_avg:131.47ms
step:889/1395 train_time:115570ms step_avg:131.48ms
step:890/1395 train_time:115707ms step_avg:131.49ms
step:891/1395 train_time:115843ms step_avg:131.49ms
step:892/1395 train_time:115981ms step_avg:131.50ms
step:893/1395 train_time:116118ms step_avg:131.50ms
step:894/1395 train_time:116256ms step_avg:131.51ms
step:895/1395 train_time:116396ms step_avg:131.52ms
step:896/1395 train_time:116532ms step_avg:131.53ms
step:897/1395 train_time:116671ms step_avg:131.53ms
step:898/1395 train_time:116809ms step_avg:131.54ms
step:899/1395 train_time:116949ms step_avg:131.55ms
step:900/1395 train_time:117086ms step_avg:131.56ms
step:901/1395 train_time:117225ms step_avg:131.57ms
step:902/1395 train_time:117360ms step_avg:131.57ms
step:903/1395 train_time:117501ms step_avg:131.58ms
step:904/1395 train_time:117641ms step_avg:131.59ms
step:905/1395 train_time:117777ms step_avg:131.59ms
step:906/1395 train_time:117914ms step_avg:131.60ms
step:907/1395 train_time:118055ms step_avg:131.61ms
step:908/1395 train_time:118192ms step_avg:131.62ms
step:909/1395 train_time:118329ms step_avg:131.62ms
step:910/1395 train_time:118472ms step_avg:131.64ms
step:911/1395 train_time:118609ms step_avg:131.64ms
step:912/1395 train_time:118746ms step_avg:131.65ms
step:913/1395 train_time:118883ms step_avg:131.65ms
step:914/1395 train_time:119021ms step_avg:131.66ms
step:915/1395 train_time:119160ms step_avg:131.67ms
step:916/1395 train_time:119298ms step_avg:131.68ms
step:917/1395 train_time:119437ms step_avg:131.68ms
step:918/1395 train_time:119575ms step_avg:131.69ms
step:919/1395 train_time:119716ms step_avg:131.70ms
step:920/1395 train_time:119853ms step_avg:131.71ms
step:921/1395 train_time:119990ms step_avg:131.71ms
step:922/1395 train_time:120131ms step_avg:131.72ms
step:923/1395 train_time:120267ms step_avg:131.73ms
step:924/1395 train_time:120405ms step_avg:131.73ms
step:925/1395 train_time:120543ms step_avg:131.74ms
step:926/1395 train_time:120682ms step_avg:131.75ms
step:927/1395 train_time:120819ms step_avg:131.75ms
step:928/1395 train_time:120956ms step_avg:131.76ms
step:929/1395 train_time:121094ms step_avg:131.77ms
step:930/1395 train_time:121232ms step_avg:131.77ms
step:931/1395 train_time:121369ms step_avg:131.78ms
step:932/1395 train_time:121508ms step_avg:131.79ms
step:933/1395 train_time:121647ms step_avg:131.80ms
step:934/1395 train_time:121786ms step_avg:131.80ms
step:935/1395 train_time:121928ms step_avg:131.81ms
step:936/1395 train_time:122067ms step_avg:131.82ms
step:937/1395 train_time:122209ms step_avg:131.83ms
step:938/1395 train_time:122351ms step_avg:131.84ms
step:939/1395 train_time:122491ms step_avg:131.85ms
step:940/1395 train_time:122632ms step_avg:131.86ms
step:941/1395 train_time:122770ms step_avg:131.87ms
step:942/1395 train_time:122907ms step_avg:131.87ms
step:943/1395 train_time:123048ms step_avg:131.88ms
step:944/1395 train_time:123192ms step_avg:131.90ms
step:945/1395 train_time:123332ms step_avg:131.91ms
step:946/1395 train_time:123472ms step_avg:131.91ms
step:947/1395 train_time:123614ms step_avg:131.92ms
step:948/1395 train_time:123752ms step_avg:131.93ms
step:949/1395 train_time:123892ms step_avg:131.94ms
step:950/1395 train_time:124030ms step_avg:131.95ms
step:951/1395 train_time:124173ms step_avg:131.96ms
step:952/1395 train_time:124311ms step_avg:131.96ms
step:953/1395 train_time:124450ms step_avg:131.97ms
step:954/1395 train_time:124589ms step_avg:131.98ms
step:955/1395 train_time:124727ms step_avg:131.99ms
step:956/1395 train_time:124871ms step_avg:132.00ms
step:957/1395 train_time:125010ms step_avg:132.01ms
step:958/1395 train_time:125151ms step_avg:132.02ms
step:959/1395 train_time:125294ms step_avg:132.03ms
step:960/1395 train_time:125433ms step_avg:132.04ms
step:961/1395 train_time:125572ms step_avg:132.04ms
step:962/1395 train_time:125711ms step_avg:132.05ms
step:963/1395 train_time:125857ms step_avg:132.06ms
step:964/1395 train_time:125995ms step_avg:132.07ms
step:965/1395 train_time:126133ms step_avg:132.08ms
step:966/1395 train_time:126272ms step_avg:132.08ms
step:967/1395 train_time:126412ms step_avg:132.09ms
step:968/1395 train_time:126549ms step_avg:132.10ms
step:969/1395 train_time:126689ms step_avg:132.11ms
step:970/1395 train_time:126827ms step_avg:132.11ms
step:971/1395 train_time:126966ms step_avg:132.12ms
step:972/1395 train_time:127105ms step_avg:132.13ms
step:973/1395 train_time:127244ms step_avg:132.13ms
step:974/1395 train_time:127383ms step_avg:132.14ms
step:975/1395 train_time:127522ms step_avg:132.15ms
step:976/1395 train_time:127661ms step_avg:132.15ms
step:977/1395 train_time:127800ms step_avg:132.16ms
step:978/1395 train_time:127938ms step_avg:132.17ms
step:979/1395 train_time:128076ms step_avg:132.17ms
step:980/1395 train_time:128215ms step_avg:132.18ms
step:981/1395 train_time:128350ms step_avg:132.18ms
step:982/1395 train_time:128487ms step_avg:132.19ms
step:983/1395 train_time:128625ms step_avg:132.19ms
step:984/1395 train_time:128763ms step_avg:132.20ms
step:985/1395 train_time:128904ms step_avg:132.21ms
step:986/1395 train_time:129048ms step_avg:132.22ms
step:987/1395 train_time:129186ms step_avg:132.23ms
step:988/1395 train_time:129325ms step_avg:132.23ms
step:989/1395 train_time:129464ms step_avg:132.24ms
step:990/1395 train_time:129606ms step_avg:132.25ms
step:991/1395 train_time:129745ms step_avg:132.26ms
step:992/1395 train_time:129888ms step_avg:132.27ms
step:993/1395 train_time:130035ms step_avg:132.28ms
step:994/1395 train_time:130171ms step_avg:132.29ms
step:995/1395 train_time:130309ms step_avg:132.29ms
step:996/1395 train_time:130446ms step_avg:132.30ms
step:997/1395 train_time:130585ms step_avg:132.30ms
step:998/1395 train_time:130721ms step_avg:132.31ms
step:999/1395 train_time:130860ms step_avg:132.32ms
step:1000/1395 train_time:130997ms step_avg:132.32ms
step:1000/1395 val_loss:3.4150 train_time:131108ms step_avg:132.43ms
step:1001/1395 train_time:131139ms step_avg:132.33ms
step:1002/1395 train_time:131282ms step_avg:132.34ms
step:1003/1395 train_time:131423ms step_avg:132.35ms
step:1004/1395 train_time:131564ms step_avg:132.36ms
step:1005/1395 train_time:131703ms step_avg:132.37ms
step:1006/1395 train_time:131841ms step_avg:132.37ms
step:1007/1395 train_time:131980ms step_avg:132.38ms
step:1008/1395 train_time:132118ms step_avg:132.38ms
step:1009/1395 train_time:132263ms step_avg:132.40ms
step:1010/1395 train_time:132401ms step_avg:132.40ms
step:1011/1395 train_time:132541ms step_avg:132.41ms
step:1012/1395 train_time:132680ms step_avg:132.42ms
step:1013/1395 train_time:132820ms step_avg:132.42ms
step:1014/1395 train_time:132958ms step_avg:132.43ms
step:1015/1395 train_time:133097ms step_avg:132.43ms
step:1016/1395 train_time:133234ms step_avg:132.44ms
step:1017/1395 train_time:133373ms step_avg:132.45ms
step:1018/1395 train_time:133511ms step_avg:132.45ms
step:1019/1395 train_time:133652ms step_avg:132.46ms
step:1020/1395 train_time:133794ms step_avg:132.47ms
step:1021/1395 train_time:133933ms step_avg:132.48ms
step:1022/1395 train_time:134071ms step_avg:132.48ms
step:1023/1395 train_time:134212ms step_avg:132.49ms
step:1024/1395 train_time:134352ms step_avg:132.50ms
step:1025/1395 train_time:134491ms step_avg:132.50ms
step:1026/1395 train_time:134630ms step_avg:132.51ms
step:1027/1395 train_time:134768ms step_avg:132.52ms
step:1028/1395 train_time:134909ms step_avg:132.52ms
step:1029/1395 train_time:135052ms step_avg:132.53ms
step:1030/1395 train_time:135192ms step_avg:132.54ms
step:1031/1395 train_time:135329ms step_avg:132.55ms
step:1032/1395 train_time:135465ms step_avg:132.55ms
step:1033/1395 train_time:135603ms step_avg:132.55ms
step:1034/1395 train_time:135742ms step_avg:132.56ms
step:1035/1395 train_time:135882ms step_avg:132.57ms
step:1036/1395 train_time:136023ms step_avg:132.58ms
step:1037/1395 train_time:136166ms step_avg:132.59ms
step:1038/1395 train_time:136305ms step_avg:132.59ms
step:1039/1395 train_time:136443ms step_avg:132.60ms
step:1040/1395 train_time:136582ms step_avg:132.60ms
step:1041/1395 train_time:136722ms step_avg:132.61ms
step:1042/1395 train_time:136860ms step_avg:132.62ms
step:1043/1395 train_time:137000ms step_avg:132.62ms
step:1044/1395 train_time:137144ms step_avg:132.63ms
step:1045/1395 train_time:137285ms step_avg:132.64ms
step:1046/1395 train_time:137425ms step_avg:132.65ms
step:1047/1395 train_time:137563ms step_avg:132.65ms
step:1048/1395 train_time:137703ms step_avg:132.66ms
step:1049/1395 train_time:137843ms step_avg:132.67ms
step:1050/1395 train_time:137985ms step_avg:132.68ms
step:1051/1395 train_time:138127ms step_avg:132.69ms
step:1052/1395 train_time:138265ms step_avg:132.69ms
step:1053/1395 train_time:138403ms step_avg:132.70ms
step:1054/1395 train_time:138544ms step_avg:132.70ms
step:1055/1395 train_time:138683ms step_avg:132.71ms
step:1056/1395 train_time:138823ms step_avg:132.72ms
step:1057/1395 train_time:138963ms step_avg:132.72ms
step:1058/1395 train_time:139105ms step_avg:132.73ms
step:1059/1395 train_time:139247ms step_avg:132.74ms
step:1060/1395 train_time:139387ms step_avg:132.75ms
step:1061/1395 train_time:139524ms step_avg:132.75ms
step:1062/1395 train_time:139665ms step_avg:132.76ms
step:1063/1395 train_time:139804ms step_avg:132.77ms
step:1064/1395 train_time:139942ms step_avg:132.77ms
step:1065/1395 train_time:140081ms step_avg:132.78ms
step:1066/1395 train_time:140224ms step_avg:132.79ms
step:1067/1395 train_time:140366ms step_avg:132.80ms
step:1068/1395 train_time:140504ms step_avg:132.80ms
step:1069/1395 train_time:140649ms step_avg:132.81ms
step:1070/1395 train_time:140787ms step_avg:132.82ms
step:1071/1395 train_time:140933ms step_avg:132.83ms
step:1072/1395 train_time:141071ms step_avg:132.84ms
step:1073/1395 train_time:141209ms step_avg:132.84ms
step:1074/1395 train_time:141348ms step_avg:132.85ms
step:1075/1395 train_time:141490ms step_avg:132.85ms
step:1076/1395 train_time:141629ms step_avg:132.86ms
step:1077/1395 train_time:141767ms step_avg:132.86ms
step:1078/1395 train_time:141909ms step_avg:132.87ms
step:1079/1395 train_time:142055ms step_avg:132.89ms
step:1080/1395 train_time:142195ms step_avg:132.89ms
step:1081/1395 train_time:142334ms step_avg:132.90ms
step:1082/1395 train_time:142474ms step_avg:132.90ms
step:1083/1395 train_time:142612ms step_avg:132.91ms
step:1084/1395 train_time:142758ms step_avg:132.92ms
step:1085/1395 train_time:142896ms step_avg:132.93ms
step:1086/1395 train_time:143036ms step_avg:132.93ms
step:1087/1395 train_time:143177ms step_avg:132.94ms
step:1088/1395 train_time:143317ms step_avg:132.95ms
step:1089/1395 train_time:143462ms step_avg:132.96ms
step:1090/1395 train_time:143604ms step_avg:132.97ms
step:1091/1395 train_time:143744ms step_avg:132.97ms
step:1092/1395 train_time:143882ms step_avg:132.98ms
step:1093/1395 train_time:144021ms step_avg:132.98ms
step:1094/1395 train_time:144159ms step_avg:132.99ms
step:1095/1395 train_time:144297ms step_avg:132.99ms
step:1096/1395 train_time:144441ms step_avg:133.00ms
step:1097/1395 train_time:144583ms step_avg:133.01ms
step:1098/1395 train_time:144725ms step_avg:133.02ms
step:1099/1395 train_time:144865ms step_avg:133.03ms
step:1100/1395 train_time:145003ms step_avg:133.03ms
step:1101/1395 train_time:145142ms step_avg:133.04ms
step:1102/1395 train_time:145283ms step_avg:133.04ms
step:1103/1395 train_time:145423ms step_avg:133.05ms
step:1104/1395 train_time:145562ms step_avg:133.06ms
step:1105/1395 train_time:145705ms step_avg:133.06ms
step:1106/1395 train_time:145845ms step_avg:133.07ms
step:1107/1395 train_time:145985ms step_avg:133.08ms
step:1108/1395 train_time:146129ms step_avg:133.09ms
step:1109/1395 train_time:146268ms step_avg:133.09ms
step:1110/1395 train_time:146407ms step_avg:133.10ms
step:1111/1395 train_time:146547ms step_avg:133.10ms
step:1112/1395 train_time:146685ms step_avg:133.11ms
step:1113/1395 train_time:146823ms step_avg:133.11ms
step:1114/1395 train_time:146965ms step_avg:133.12ms
step:1115/1395 train_time:147106ms step_avg:133.13ms
step:1116/1395 train_time:147245ms step_avg:133.13ms
step:1117/1395 train_time:147387ms step_avg:133.14ms
step:1118/1395 train_time:147533ms step_avg:133.15ms
step:1119/1395 train_time:147673ms step_avg:133.16ms
step:1120/1395 train_time:147812ms step_avg:133.16ms
step:1121/1395 train_time:147951ms step_avg:133.17ms
step:1122/1395 train_time:148091ms step_avg:133.17ms
step:1123/1395 train_time:148229ms step_avg:133.18ms
step:1124/1395 train_time:148371ms step_avg:133.19ms
step:1125/1395 train_time:148509ms step_avg:133.19ms
step:1125/1395 val_loss:3.3645 train_time:148623ms step_avg:133.29ms
step:1126/1395 train_time:148654ms step_avg:133.20ms
step:1127/1395 train_time:148799ms step_avg:133.21ms
step:1128/1395 train_time:148938ms step_avg:133.22ms
step:1129/1395 train_time:149081ms step_avg:133.23ms
step:1130/1395 train_time:149219ms step_avg:133.23ms
step:1131/1395 train_time:149361ms step_avg:133.24ms
step:1132/1395 train_time:149500ms step_avg:133.24ms
step:1133/1395 train_time:149639ms step_avg:133.25ms
step:1134/1395 train_time:149781ms step_avg:133.26ms
step:1135/1395 train_time:149921ms step_avg:133.26ms
step:1136/1395 train_time:150069ms step_avg:133.28ms
step:1137/1395 train_time:150208ms step_avg:133.28ms
step:1138/1395 train_time:150351ms step_avg:133.29ms
step:1139/1395 train_time:150491ms step_avg:133.30ms
step:1140/1395 train_time:150631ms step_avg:133.30ms
step:1141/1395 train_time:150772ms step_avg:133.31ms
step:1142/1395 train_time:150912ms step_avg:133.31ms
step:1143/1395 train_time:151055ms step_avg:133.32ms
step:1144/1395 train_time:151196ms step_avg:133.33ms
step:1145/1395 train_time:151335ms step_avg:133.33ms
step:1146/1395 train_time:151477ms step_avg:133.34ms
step:1147/1395 train_time:151621ms step_avg:133.35ms
step:1148/1395 train_time:151761ms step_avg:133.36ms
step:1149/1395 train_time:151901ms step_avg:133.36ms
step:1150/1395 train_time:152041ms step_avg:133.37ms
step:1151/1395 train_time:152185ms step_avg:133.38ms
step:1152/1395 train_time:152325ms step_avg:133.38ms
step:1153/1395 train_time:152469ms step_avg:133.39ms
step:1154/1395 train_time:152609ms step_avg:133.40ms
step:1155/1395 train_time:152750ms step_avg:133.41ms
step:1156/1395 train_time:152899ms step_avg:133.42ms
step:1157/1395 train_time:153041ms step_avg:133.43ms
step:1158/1395 train_time:153182ms step_avg:133.43ms
step:1159/1395 train_time:153323ms step_avg:133.44ms
step:1160/1395 train_time:153462ms step_avg:133.45ms
step:1161/1395 train_time:153605ms step_avg:133.45ms
step:1162/1395 train_time:153746ms step_avg:133.46ms
step:1163/1395 train_time:153886ms step_avg:133.47ms
step:1164/1395 train_time:154027ms step_avg:133.47ms
step:1165/1395 train_time:154165ms step_avg:133.48ms
step:1166/1395 train_time:154307ms step_avg:133.48ms
step:1167/1395 train_time:154446ms step_avg:133.49ms
step:1168/1395 train_time:154587ms step_avg:133.49ms
step:1169/1395 train_time:154727ms step_avg:133.50ms
step:1170/1395 train_time:154867ms step_avg:133.51ms
step:1171/1395 train_time:155009ms step_avg:133.51ms
step:1172/1395 train_time:155150ms step_avg:133.52ms
step:1173/1395 train_time:155290ms step_avg:133.53ms
step:1174/1395 train_time:155443ms step_avg:133.54ms
step:1175/1395 train_time:155584ms step_avg:133.55ms
step:1176/1395 train_time:155727ms step_avg:133.56ms
step:1177/1395 train_time:155875ms step_avg:133.57ms
step:1178/1395 train_time:156017ms step_avg:133.58ms
step:1179/1395 train_time:156156ms step_avg:133.58ms
step:1180/1395 train_time:156303ms step_avg:133.59ms
step:1181/1395 train_time:156447ms step_avg:133.60ms
step:1182/1395 train_time:156586ms step_avg:133.61ms
step:1183/1395 train_time:156728ms step_avg:133.61ms
step:1184/1395 train_time:156869ms step_avg:133.62ms
step:1185/1395 train_time:157012ms step_avg:133.63ms
step:1186/1395 train_time:157152ms step_avg:133.63ms
step:1187/1395 train_time:157305ms step_avg:133.65ms
step:1188/1395 train_time:157444ms step_avg:133.65ms
step:1189/1395 train_time:157588ms step_avg:133.66ms
step:1190/1395 train_time:157730ms step_avg:133.67ms
step:1191/1395 train_time:157872ms step_avg:133.68ms
step:1192/1395 train_time:158012ms step_avg:133.68ms
step:1193/1395 train_time:158151ms step_avg:133.69ms
step:1194/1395 train_time:158292ms step_avg:133.69ms
step:1195/1395 train_time:158435ms step_avg:133.70ms
step:1196/1395 train_time:158576ms step_avg:133.71ms
step:1197/1395 train_time:158718ms step_avg:133.71ms
step:1198/1395 train_time:158867ms step_avg:133.73ms
step:1199/1395 train_time:159008ms step_avg:133.73ms
step:1200/1395 train_time:159148ms step_avg:133.74ms
step:1201/1395 train_time:159287ms step_avg:133.74ms
step:1202/1395 train_time:159441ms step_avg:133.76ms
step:1203/1395 train_time:159589ms step_avg:133.77ms
step:1204/1395 train_time:159732ms step_avg:133.78ms
step:1205/1395 train_time:159875ms step_avg:133.79ms
step:1206/1395 train_time:160017ms step_avg:133.79ms
step:1207/1395 train_time:160156ms step_avg:133.80ms
step:1208/1395 train_time:160301ms step_avg:133.81ms
step:1209/1395 train_time:160441ms step_avg:133.81ms
step:1210/1395 train_time:160588ms step_avg:133.82ms
step:1211/1395 train_time:160730ms step_avg:133.83ms
step:1212/1395 train_time:160871ms step_avg:133.84ms
step:1213/1395 train_time:161012ms step_avg:133.84ms
step:1214/1395 train_time:161155ms step_avg:133.85ms
step:1215/1395 train_time:161300ms step_avg:133.86ms
step:1216/1395 train_time:161438ms step_avg:133.86ms
step:1217/1395 train_time:161580ms step_avg:133.87ms
step:1218/1395 train_time:161719ms step_avg:133.87ms
step:1219/1395 train_time:161858ms step_avg:133.88ms
step:1220/1395 train_time:161999ms step_avg:133.88ms
step:1221/1395 train_time:162141ms step_avg:133.89ms
step:1222/1395 train_time:162284ms step_avg:133.90ms
step:1223/1395 train_time:162425ms step_avg:133.90ms
step:1224/1395 train_time:162568ms step_avg:133.91ms
step:1225/1395 train_time:162711ms step_avg:133.92ms
step:1226/1395 train_time:162851ms step_avg:133.92ms
step:1227/1395 train_time:162993ms step_avg:133.93ms
step:1228/1395 train_time:163135ms step_avg:133.94ms
step:1229/1395 train_time:163275ms step_avg:133.94ms
step:1230/1395 train_time:163421ms step_avg:133.95ms
step:1231/1395 train_time:163566ms step_avg:133.96ms
step:1232/1395 train_time:163710ms step_avg:133.97ms
step:1233/1395 train_time:163851ms step_avg:133.97ms
step:1234/1395 train_time:163991ms step_avg:133.98ms
step:1235/1395 train_time:164131ms step_avg:133.98ms
step:1236/1395 train_time:164272ms step_avg:133.99ms
step:1237/1395 train_time:164410ms step_avg:133.99ms
step:1238/1395 train_time:164562ms step_avg:134.01ms
step:1239/1395 train_time:164702ms step_avg:134.01ms
step:1240/1395 train_time:164844ms step_avg:134.02ms
step:1241/1395 train_time:164990ms step_avg:134.03ms
step:1242/1395 train_time:165129ms step_avg:134.03ms
step:1243/1395 train_time:165273ms step_avg:134.04ms
step:1244/1395 train_time:165415ms step_avg:134.05ms
step:1245/1395 train_time:165555ms step_avg:134.05ms
step:1246/1395 train_time:165694ms step_avg:134.06ms
step:1247/1395 train_time:165836ms step_avg:134.06ms
step:1248/1395 train_time:165976ms step_avg:134.07ms
step:1249/1395 train_time:166116ms step_avg:134.07ms
step:1250/1395 train_time:166257ms step_avg:134.08ms
step:1250/1395 val_loss:3.3180 train_time:166373ms step_avg:134.17ms
step:1251/1395 train_time:166406ms step_avg:134.09ms
step:1252/1395 train_time:166550ms step_avg:134.10ms
step:1253/1395 train_time:166690ms step_avg:134.10ms
step:1254/1395 train_time:166829ms step_avg:134.11ms
step:1255/1395 train_time:166982ms step_avg:134.12ms
step:1256/1395 train_time:167122ms step_avg:134.13ms
step:1257/1395 train_time:167264ms step_avg:134.13ms
step:1258/1395 train_time:167406ms step_avg:134.14ms
step:1259/1395 train_time:167549ms step_avg:134.15ms
step:1260/1395 train_time:167688ms step_avg:134.15ms
step:1261/1395 train_time:167828ms step_avg:134.16ms
step:1262/1395 train_time:167973ms step_avg:134.16ms
step:1263/1395 train_time:168115ms step_avg:134.17ms
step:1264/1395 train_time:168256ms step_avg:134.18ms
step:1265/1395 train_time:168396ms step_avg:134.18ms
step:1266/1395 train_time:168539ms step_avg:134.19ms
step:1267/1395 train_time:168682ms step_avg:134.19ms
step:1268/1395 train_time:168824ms step_avg:134.20ms
step:1269/1395 train_time:168971ms step_avg:134.21ms
step:1270/1395 train_time:169112ms step_avg:134.22ms
step:1271/1395 train_time:169256ms step_avg:134.22ms
step:1272/1395 train_time:169396ms step_avg:134.23ms
step:1273/1395 train_time:169536ms step_avg:134.23ms
step:1274/1395 train_time:169677ms step_avg:134.24ms
step:1275/1395 train_time:169822ms step_avg:134.25ms
step:1276/1395 train_time:169961ms step_avg:134.25ms
step:1277/1395 train_time:170101ms step_avg:134.26ms
step:1278/1395 train_time:170242ms step_avg:134.26ms
step:1279/1395 train_time:170383ms step_avg:134.27ms
step:1280/1395 train_time:170532ms step_avg:134.28ms
step:1281/1395 train_time:170675ms step_avg:134.28ms
step:1282/1395 train_time:170814ms step_avg:134.29ms
step:1283/1395 train_time:170957ms step_avg:134.29ms
step:1284/1395 train_time:171100ms step_avg:134.30ms
step:1285/1395 train_time:171241ms step_avg:134.31ms
step:1286/1395 train_time:171382ms step_avg:134.31ms
step:1287/1395 train_time:171525ms step_avg:134.32ms
step:1288/1395 train_time:171666ms step_avg:134.32ms
step:1289/1395 train_time:171814ms step_avg:134.33ms
step:1290/1395 train_time:171961ms step_avg:134.34ms
step:1291/1395 train_time:172106ms step_avg:134.35ms
step:1292/1395 train_time:172247ms step_avg:134.36ms
step:1293/1395 train_time:172395ms step_avg:134.37ms
step:1294/1395 train_time:172538ms step_avg:134.38ms
step:1295/1395 train_time:172680ms step_avg:134.38ms
step:1296/1395 train_time:172823ms step_avg:134.39ms
step:1297/1395 train_time:172967ms step_avg:134.40ms
step:1298/1395 train_time:173106ms step_avg:134.40ms
step:1299/1395 train_time:173247ms step_avg:134.40ms
step:1300/1395 train_time:173387ms step_avg:134.41ms
step:1301/1395 train_time:173528ms step_avg:134.41ms
step:1302/1395 train_time:173671ms step_avg:134.42ms
step:1303/1395 train_time:173816ms step_avg:134.43ms
step:1304/1395 train_time:173961ms step_avg:134.44ms
step:1305/1395 train_time:174101ms step_avg:134.44ms
step:1306/1395 train_time:174244ms step_avg:134.45ms
step:1307/1395 train_time:174385ms step_avg:134.45ms
step:1308/1395 train_time:174529ms step_avg:134.46ms
step:1309/1395 train_time:174673ms step_avg:134.47ms
step:1310/1395 train_time:174815ms step_avg:134.47ms
step:1311/1395 train_time:174955ms step_avg:134.48ms
step:1312/1395 train_time:175095ms step_avg:134.48ms
step:1313/1395 train_time:175236ms step_avg:134.49ms
step:1314/1395 train_time:175379ms step_avg:134.49ms
step:1315/1395 train_time:175521ms step_avg:134.50ms
step:1316/1395 train_time:175660ms step_avg:134.50ms
step:1317/1395 train_time:175802ms step_avg:134.51ms
step:1318/1395 train_time:175947ms step_avg:134.52ms
step:1319/1395 train_time:176091ms step_avg:134.52ms
step:1320/1395 train_time:176233ms step_avg:134.53ms
step:1321/1395 train_time:176374ms step_avg:134.53ms
step:1322/1395 train_time:176523ms step_avg:134.55ms
step:1323/1395 train_time:176665ms step_avg:134.55ms
step:1324/1395 train_time:176808ms step_avg:134.56ms
step:1325/1395 train_time:176952ms step_avg:134.56ms
step:1326/1395 train_time:177098ms step_avg:134.57ms
step:1327/1395 train_time:177239ms step_avg:134.58ms
step:1328/1395 train_time:177378ms step_avg:134.58ms
step:1329/1395 train_time:177535ms step_avg:134.60ms
step:1330/1395 train_time:177681ms step_avg:134.61ms
step:1331/1395 train_time:177827ms step_avg:134.62ms
step:1332/1395 train_time:177977ms step_avg:134.63ms
step:1333/1395 train_time:178120ms step_avg:134.63ms
step:1334/1395 train_time:178261ms step_avg:134.64ms
step:1335/1395 train_time:178399ms step_avg:134.64ms
step:1336/1395 train_time:178548ms step_avg:134.65ms
step:1337/1395 train_time:178692ms step_avg:134.66ms
step:1338/1395 train_time:178833ms step_avg:134.66ms
step:1339/1395 train_time:178976ms step_avg:134.67ms
step:1340/1395 train_time:179123ms step_avg:134.68ms
step:1341/1395 train_time:179263ms step_avg:134.68ms
step:1342/1395 train_time:179405ms step_avg:134.69ms
step:1343/1395 train_time:179545ms step_avg:134.69ms
step:1344/1395 train_time:179684ms step_avg:134.70ms
step:1345/1395 train_time:179825ms step_avg:134.70ms
step:1346/1395 train_time:179966ms step_avg:134.70ms
step:1347/1395 train_time:180110ms step_avg:134.71ms
step:1348/1395 train_time:180252ms step_avg:134.72ms
step:1349/1395 train_time:180394ms step_avg:134.72ms
step:1350/1395 train_time:180532ms step_avg:134.73ms
step:1351/1395 train_time:180673ms step_avg:134.73ms
step:1352/1395 train_time:180824ms step_avg:134.74ms
step:1353/1395 train_time:180970ms step_avg:134.75ms
step:1354/1395 train_time:181113ms step_avg:134.76ms
step:1355/1395 train_time:181254ms step_avg:134.76ms
step:1356/1395 train_time:181394ms step_avg:134.77ms
step:1357/1395 train_time:181538ms step_avg:134.77ms
step:1358/1395 train_time:181683ms step_avg:134.78ms
step:1359/1395 train_time:181824ms step_avg:134.78ms
step:1360/1395 train_time:181969ms step_avg:134.79ms
step:1361/1395 train_time:182114ms step_avg:134.80ms
step:1362/1395 train_time:182261ms step_avg:134.81ms
step:1363/1395 train_time:182411ms step_avg:134.82ms
step:1364/1395 train_time:182555ms step_avg:134.83ms
step:1365/1395 train_time:182694ms step_avg:134.83ms
step:1366/1395 train_time:182836ms step_avg:134.83ms
step:1367/1395 train_time:182980ms step_avg:134.84ms
step:1368/1395 train_time:183123ms step_avg:134.85ms
step:1369/1395 train_time:183271ms step_avg:134.86ms
step:1370/1395 train_time:183419ms step_avg:134.87ms
step:1371/1395 train_time:183563ms step_avg:134.87ms
step:1372/1395 train_time:183711ms step_avg:134.88ms
step:1373/1395 train_time:183854ms step_avg:134.89ms
step:1374/1395 train_time:184000ms step_avg:134.90ms
step:1375/1395 train_time:184141ms step_avg:134.90ms
step:1375/1395 val_loss:3.2840 train_time:184252ms step_avg:134.98ms
step:1376/1395 train_time:184282ms step_avg:134.91ms
step:1377/1395 train_time:184426ms step_avg:134.91ms
step:1378/1395 train_time:184567ms step_avg:134.92ms
step:1379/1395 train_time:184710ms step_avg:134.92ms
step:1380/1395 train_time:184852ms step_avg:134.93ms
step:1381/1395 train_time:184999ms step_avg:134.94ms
step:1382/1395 train_time:185143ms step_avg:134.94ms
step:1383/1395 train_time:185284ms step_avg:134.95ms
step:1384/1395 train_time:185432ms step_avg:134.96ms
step:1385/1395 train_time:185571ms step_avg:134.96ms
step:1386/1395 train_time:185712ms step_avg:134.97ms
step:1387/1395 train_time:185858ms step_avg:134.97ms
step:1388/1395 train_time:185998ms step_avg:134.98ms
step:1389/1395 train_time:186141ms step_avg:134.98ms
step:1390/1395 train_time:186285ms step_avg:134.99ms
step:1391/1395 train_time:186426ms step_avg:134.99ms
step:1392/1395 train_time:186570ms step_avg:135.00ms
step:1393/1395 train_time:186710ms step_avg:135.00ms
step:1394/1395 train_time:186852ms step_avg:135.01ms
step:1395/1395 train_time:186992ms step_avg:135.01ms
step:1395/1395 val_loss:3.2797 train_time:187108ms step_avg:135.10ms
peak memory allocated: 37620 MiB reserved: 39114 MiB
