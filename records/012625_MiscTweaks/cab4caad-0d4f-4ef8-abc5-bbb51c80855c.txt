import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Jan 26 00:06:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24463ms step_avg:nanms
step:2/1770 train_time:24881ms step_avg:nanms
step:3/1770 train_time:24977ms step_avg:nanms
step:4/1770 train_time:25070ms step_avg:nanms
step:5/1770 train_time:25163ms step_avg:nanms
step:6/1770 train_time:25257ms step_avg:nanms
step:7/1770 train_time:25351ms step_avg:nanms
step:8/1770 train_time:25445ms step_avg:nanms
step:9/1770 train_time:25539ms step_avg:nanms
step:10/1770 train_time:25632ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.11ms
step:14/1770 train_time:378ms step_avg:94.48ms
step:15/1770 train_time:473ms step_avg:94.56ms
step:16/1770 train_time:567ms step_avg:94.43ms
step:17/1770 train_time:660ms step_avg:94.33ms
step:18/1770 train_time:754ms step_avg:94.29ms
step:19/1770 train_time:848ms step_avg:94.22ms
step:20/1770 train_time:942ms step_avg:94.22ms
step:21/1770 train_time:1036ms step_avg:94.17ms
step:22/1770 train_time:1130ms step_avg:94.17ms
step:23/1770 train_time:1224ms step_avg:94.17ms
step:24/1770 train_time:1318ms step_avg:94.18ms
step:25/1770 train_time:1414ms step_avg:94.24ms
step:26/1770 train_time:1508ms step_avg:94.24ms
step:27/1770 train_time:1602ms step_avg:94.23ms
step:28/1770 train_time:1696ms step_avg:94.23ms
step:29/1770 train_time:1790ms step_avg:94.20ms
step:30/1770 train_time:1884ms step_avg:94.21ms
step:31/1770 train_time:1978ms step_avg:94.20ms
step:32/1770 train_time:2072ms step_avg:94.20ms
step:33/1770 train_time:2167ms step_avg:94.20ms
step:34/1770 train_time:2260ms step_avg:94.18ms
step:35/1770 train_time:2355ms step_avg:94.18ms
step:36/1770 train_time:2449ms step_avg:94.21ms
step:37/1770 train_time:2543ms step_avg:94.20ms
step:38/1770 train_time:2637ms step_avg:94.20ms
step:39/1770 train_time:2732ms step_avg:94.20ms
step:40/1770 train_time:2825ms step_avg:94.18ms
step:41/1770 train_time:2920ms step_avg:94.19ms
step:42/1770 train_time:3013ms step_avg:94.16ms
step:43/1770 train_time:3107ms step_avg:94.15ms
step:44/1770 train_time:3201ms step_avg:94.14ms
step:45/1770 train_time:3295ms step_avg:94.14ms
step:46/1770 train_time:3389ms step_avg:94.13ms
step:47/1770 train_time:3483ms step_avg:94.13ms
step:48/1770 train_time:3576ms step_avg:94.11ms
step:49/1770 train_time:3670ms step_avg:94.10ms
step:50/1770 train_time:3764ms step_avg:94.10ms
step:51/1770 train_time:3858ms step_avg:94.09ms
step:52/1770 train_time:3952ms step_avg:94.10ms
step:53/1770 train_time:4046ms step_avg:94.10ms
step:54/1770 train_time:4141ms step_avg:94.11ms
step:55/1770 train_time:4235ms step_avg:94.10ms
step:56/1770 train_time:4329ms step_avg:94.10ms
step:57/1770 train_time:4422ms step_avg:94.09ms
step:58/1770 train_time:4517ms step_avg:94.09ms
step:59/1770 train_time:4610ms step_avg:94.09ms
step:60/1770 train_time:4705ms step_avg:94.10ms
step:61/1770 train_time:4799ms step_avg:94.10ms
step:62/1770 train_time:4893ms step_avg:94.10ms
step:63/1770 train_time:4987ms step_avg:94.09ms
step:64/1770 train_time:5081ms step_avg:94.09ms
step:65/1770 train_time:5175ms step_avg:94.09ms
step:66/1770 train_time:5268ms step_avg:94.08ms
step:67/1770 train_time:5362ms step_avg:94.08ms
step:68/1770 train_time:5456ms step_avg:94.07ms
step:69/1770 train_time:5550ms step_avg:94.07ms
step:70/1770 train_time:5644ms step_avg:94.07ms
step:71/1770 train_time:5738ms step_avg:94.07ms
step:72/1770 train_time:5832ms step_avg:94.06ms
step:73/1770 train_time:5926ms step_avg:94.06ms
step:74/1770 train_time:6019ms step_avg:94.05ms
step:75/1770 train_time:6113ms step_avg:94.05ms
step:76/1770 train_time:6207ms step_avg:94.04ms
step:77/1770 train_time:6301ms step_avg:94.04ms
step:78/1770 train_time:6395ms step_avg:94.04ms
step:79/1770 train_time:6489ms step_avg:94.04ms
step:80/1770 train_time:6583ms step_avg:94.04ms
step:81/1770 train_time:6677ms step_avg:94.04ms
step:82/1770 train_time:6771ms step_avg:94.04ms
step:83/1770 train_time:6865ms step_avg:94.04ms
step:84/1770 train_time:6959ms step_avg:94.04ms
step:85/1770 train_time:7053ms step_avg:94.04ms
step:86/1770 train_time:7147ms step_avg:94.04ms
step:87/1770 train_time:7241ms step_avg:94.04ms
step:88/1770 train_time:7335ms step_avg:94.04ms
step:89/1770 train_time:7429ms step_avg:94.04ms
step:90/1770 train_time:7523ms step_avg:94.04ms
step:91/1770 train_time:7617ms step_avg:94.03ms
step:92/1770 train_time:7711ms step_avg:94.03ms
step:93/1770 train_time:7805ms step_avg:94.03ms
step:94/1770 train_time:7898ms step_avg:94.03ms
step:95/1770 train_time:7992ms step_avg:94.02ms
step:96/1770 train_time:8086ms step_avg:94.02ms
step:97/1770 train_time:8179ms step_avg:94.01ms
step:98/1770 train_time:8273ms step_avg:94.02ms
step:99/1770 train_time:8368ms step_avg:94.02ms
step:100/1770 train_time:8462ms step_avg:94.02ms
step:101/1770 train_time:8556ms step_avg:94.02ms
step:102/1770 train_time:8650ms step_avg:94.02ms
step:103/1770 train_time:8744ms step_avg:94.02ms
step:104/1770 train_time:8838ms step_avg:94.02ms
step:105/1770 train_time:8932ms step_avg:94.02ms
step:106/1770 train_time:9026ms step_avg:94.02ms
step:107/1770 train_time:9120ms step_avg:94.02ms
step:108/1770 train_time:9214ms step_avg:94.02ms
step:109/1770 train_time:9307ms step_avg:94.01ms
step:110/1770 train_time:9401ms step_avg:94.01ms
step:111/1770 train_time:9495ms step_avg:94.01ms
step:112/1770 train_time:9589ms step_avg:94.01ms
step:113/1770 train_time:9682ms step_avg:94.00ms
step:114/1770 train_time:9777ms step_avg:94.01ms
step:115/1770 train_time:9870ms step_avg:94.00ms
step:116/1770 train_time:9964ms step_avg:94.00ms
step:117/1770 train_time:10059ms step_avg:94.01ms
step:118/1770 train_time:10152ms step_avg:94.00ms
step:119/1770 train_time:10247ms step_avg:94.01ms
step:120/1770 train_time:10341ms step_avg:94.01ms
step:121/1770 train_time:10435ms step_avg:94.01ms
step:122/1770 train_time:10529ms step_avg:94.01ms
step:123/1770 train_time:10622ms step_avg:94.00ms
step:124/1770 train_time:10716ms step_avg:94.00ms
step:125/1770 train_time:10810ms step_avg:94.00ms
step:125/1770 val_loss:4.6525 train_time:10902ms step_avg:94.80ms
step:126/1770 train_time:10925ms step_avg:94.18ms
step:127/1770 train_time:11001ms step_avg:94.03ms
step:128/1770 train_time:11099ms step_avg:94.06ms
step:129/1770 train_time:11198ms step_avg:94.10ms
step:130/1770 train_time:11294ms step_avg:94.11ms
step:131/1770 train_time:11387ms step_avg:94.11ms
step:132/1770 train_time:11481ms step_avg:94.10ms
step:133/1770 train_time:11574ms step_avg:94.10ms
step:134/1770 train_time:11668ms step_avg:94.10ms
step:135/1770 train_time:11762ms step_avg:94.10ms
step:136/1770 train_time:11857ms step_avg:94.10ms
step:137/1770 train_time:11951ms step_avg:94.10ms
step:138/1770 train_time:12046ms step_avg:94.11ms
step:139/1770 train_time:12142ms step_avg:94.12ms
step:140/1770 train_time:12237ms step_avg:94.13ms
step:141/1770 train_time:12331ms step_avg:94.13ms
step:142/1770 train_time:12426ms step_avg:94.13ms
step:143/1770 train_time:12520ms step_avg:94.14ms
step:144/1770 train_time:12615ms step_avg:94.14ms
step:145/1770 train_time:12709ms step_avg:94.14ms
step:146/1770 train_time:12804ms step_avg:94.14ms
step:147/1770 train_time:12898ms step_avg:94.15ms
step:148/1770 train_time:12993ms step_avg:94.15ms
step:149/1770 train_time:13088ms step_avg:94.16ms
step:150/1770 train_time:13183ms step_avg:94.16ms
step:151/1770 train_time:13278ms step_avg:94.17ms
step:152/1770 train_time:13372ms step_avg:94.17ms
step:153/1770 train_time:13467ms step_avg:94.17ms
step:154/1770 train_time:13561ms step_avg:94.17ms
step:155/1770 train_time:13655ms step_avg:94.18ms
step:156/1770 train_time:13750ms step_avg:94.18ms
step:157/1770 train_time:13844ms step_avg:94.18ms
step:158/1770 train_time:13938ms step_avg:94.18ms
step:159/1770 train_time:14033ms step_avg:94.18ms
step:160/1770 train_time:14128ms step_avg:94.19ms
step:161/1770 train_time:14222ms step_avg:94.19ms
step:162/1770 train_time:14317ms step_avg:94.19ms
step:163/1770 train_time:14411ms step_avg:94.19ms
step:164/1770 train_time:14507ms step_avg:94.20ms
step:165/1770 train_time:14601ms step_avg:94.20ms
step:166/1770 train_time:14696ms step_avg:94.21ms
step:167/1770 train_time:14791ms step_avg:94.21ms
step:168/1770 train_time:14885ms step_avg:94.21ms
step:169/1770 train_time:14980ms step_avg:94.21ms
step:170/1770 train_time:15075ms step_avg:94.22ms
step:171/1770 train_time:15170ms step_avg:94.22ms
step:172/1770 train_time:15264ms step_avg:94.22ms
step:173/1770 train_time:15358ms step_avg:94.22ms
step:174/1770 train_time:15453ms step_avg:94.22ms
step:175/1770 train_time:15548ms step_avg:94.23ms
step:176/1770 train_time:15642ms step_avg:94.23ms
step:177/1770 train_time:15736ms step_avg:94.23ms
step:178/1770 train_time:15830ms step_avg:94.23ms
step:179/1770 train_time:15925ms step_avg:94.23ms
step:180/1770 train_time:16020ms step_avg:94.23ms
step:181/1770 train_time:16115ms step_avg:94.24ms
step:182/1770 train_time:16210ms step_avg:94.24ms
step:183/1770 train_time:16304ms step_avg:94.24ms
step:184/1770 train_time:16398ms step_avg:94.24ms
step:185/1770 train_time:16493ms step_avg:94.25ms
step:186/1770 train_time:16588ms step_avg:94.25ms
step:187/1770 train_time:16682ms step_avg:94.25ms
step:188/1770 train_time:16777ms step_avg:94.25ms
step:189/1770 train_time:16871ms step_avg:94.25ms
step:190/1770 train_time:16966ms step_avg:94.25ms
step:191/1770 train_time:17060ms step_avg:94.26ms
step:192/1770 train_time:17155ms step_avg:94.26ms
step:193/1770 train_time:17250ms step_avg:94.26ms
step:194/1770 train_time:17345ms step_avg:94.26ms
step:195/1770 train_time:17439ms step_avg:94.27ms
step:196/1770 train_time:17535ms step_avg:94.27ms
step:197/1770 train_time:17629ms step_avg:94.27ms
step:198/1770 train_time:17724ms step_avg:94.28ms
step:199/1770 train_time:17819ms step_avg:94.28ms
step:200/1770 train_time:17913ms step_avg:94.28ms
step:201/1770 train_time:18008ms step_avg:94.28ms
step:202/1770 train_time:18102ms step_avg:94.28ms
step:203/1770 train_time:18197ms step_avg:94.29ms
step:204/1770 train_time:18292ms step_avg:94.29ms
step:205/1770 train_time:18387ms step_avg:94.29ms
step:206/1770 train_time:18481ms step_avg:94.29ms
step:207/1770 train_time:18576ms step_avg:94.30ms
step:208/1770 train_time:18671ms step_avg:94.30ms
step:209/1770 train_time:18765ms step_avg:94.30ms
step:210/1770 train_time:18860ms step_avg:94.30ms
step:211/1770 train_time:18955ms step_avg:94.30ms
step:212/1770 train_time:19049ms step_avg:94.30ms
step:213/1770 train_time:19143ms step_avg:94.30ms
step:214/1770 train_time:19238ms step_avg:94.30ms
step:215/1770 train_time:19333ms step_avg:94.31ms
step:216/1770 train_time:19428ms step_avg:94.31ms
step:217/1770 train_time:19522ms step_avg:94.31ms
step:218/1770 train_time:19617ms step_avg:94.31ms
step:219/1770 train_time:19712ms step_avg:94.32ms
step:220/1770 train_time:19807ms step_avg:94.32ms
step:221/1770 train_time:19902ms step_avg:94.32ms
step:222/1770 train_time:19997ms step_avg:94.33ms
step:223/1770 train_time:20092ms step_avg:94.33ms
step:224/1770 train_time:20185ms step_avg:94.32ms
step:225/1770 train_time:20280ms step_avg:94.33ms
step:226/1770 train_time:20375ms step_avg:94.33ms
step:227/1770 train_time:20469ms step_avg:94.33ms
step:228/1770 train_time:20563ms step_avg:94.32ms
step:229/1770 train_time:20657ms step_avg:94.33ms
step:230/1770 train_time:20753ms step_avg:94.33ms
step:231/1770 train_time:20848ms step_avg:94.33ms
step:232/1770 train_time:20942ms step_avg:94.34ms
step:233/1770 train_time:21037ms step_avg:94.34ms
step:234/1770 train_time:21132ms step_avg:94.34ms
step:235/1770 train_time:21226ms step_avg:94.34ms
step:236/1770 train_time:21320ms step_avg:94.34ms
step:237/1770 train_time:21415ms step_avg:94.34ms
step:238/1770 train_time:21509ms step_avg:94.34ms
step:239/1770 train_time:21604ms step_avg:94.34ms
step:240/1770 train_time:21698ms step_avg:94.34ms
step:241/1770 train_time:21793ms step_avg:94.34ms
step:242/1770 train_time:21888ms step_avg:94.35ms
step:243/1770 train_time:21983ms step_avg:94.35ms
step:244/1770 train_time:22077ms step_avg:94.35ms
step:245/1770 train_time:22172ms step_avg:94.35ms
step:246/1770 train_time:22266ms step_avg:94.35ms
step:247/1770 train_time:22360ms step_avg:94.35ms
step:248/1770 train_time:22455ms step_avg:94.35ms
step:249/1770 train_time:22550ms step_avg:94.35ms
step:250/1770 train_time:22644ms step_avg:94.35ms
step:250/1770 val_loss:4.1120 train_time:22738ms step_avg:94.74ms
step:251/1770 train_time:22759ms step_avg:94.44ms
step:252/1770 train_time:22845ms step_avg:94.40ms
step:253/1770 train_time:22948ms step_avg:94.44ms
step:254/1770 train_time:23044ms step_avg:94.44ms
step:255/1770 train_time:23139ms step_avg:94.44ms
step:256/1770 train_time:23233ms step_avg:94.44ms
step:257/1770 train_time:23327ms step_avg:94.44ms
step:258/1770 train_time:23421ms step_avg:94.44ms
step:259/1770 train_time:23515ms step_avg:94.44ms
step:260/1770 train_time:23609ms step_avg:94.44ms
step:261/1770 train_time:23704ms step_avg:94.44ms
step:262/1770 train_time:23798ms step_avg:94.44ms
step:263/1770 train_time:23894ms step_avg:94.44ms
step:264/1770 train_time:23990ms step_avg:94.45ms
step:265/1770 train_time:24086ms step_avg:94.45ms
step:266/1770 train_time:24180ms step_avg:94.45ms
step:267/1770 train_time:24275ms step_avg:94.46ms
step:268/1770 train_time:24370ms step_avg:94.46ms
step:269/1770 train_time:24465ms step_avg:94.46ms
step:270/1770 train_time:24560ms step_avg:94.46ms
step:271/1770 train_time:24655ms step_avg:94.46ms
step:272/1770 train_time:24749ms step_avg:94.46ms
step:273/1770 train_time:24844ms step_avg:94.46ms
step:274/1770 train_time:24940ms step_avg:94.47ms
step:275/1770 train_time:25036ms step_avg:94.48ms
step:276/1770 train_time:25131ms step_avg:94.48ms
step:277/1770 train_time:25226ms step_avg:94.48ms
step:278/1770 train_time:25321ms step_avg:94.48ms
step:279/1770 train_time:25416ms step_avg:94.48ms
step:280/1770 train_time:25511ms step_avg:94.49ms
step:281/1770 train_time:25606ms step_avg:94.49ms
step:282/1770 train_time:25701ms step_avg:94.49ms
step:283/1770 train_time:25796ms step_avg:94.49ms
step:284/1770 train_time:25892ms step_avg:94.50ms
step:285/1770 train_time:25986ms step_avg:94.50ms
step:286/1770 train_time:26082ms step_avg:94.50ms
step:287/1770 train_time:26178ms step_avg:94.50ms
step:288/1770 train_time:26273ms step_avg:94.51ms
step:289/1770 train_time:26368ms step_avg:94.51ms
step:290/1770 train_time:26463ms step_avg:94.51ms
step:291/1770 train_time:26558ms step_avg:94.51ms
step:292/1770 train_time:26653ms step_avg:94.52ms
step:293/1770 train_time:26749ms step_avg:94.52ms
step:294/1770 train_time:26844ms step_avg:94.52ms
step:295/1770 train_time:26939ms step_avg:94.52ms
step:296/1770 train_time:27034ms step_avg:94.52ms
step:297/1770 train_time:27129ms step_avg:94.53ms
step:298/1770 train_time:27224ms step_avg:94.53ms
step:299/1770 train_time:27319ms step_avg:94.53ms
step:300/1770 train_time:27415ms step_avg:94.53ms
step:301/1770 train_time:27509ms step_avg:94.53ms
step:302/1770 train_time:27605ms step_avg:94.54ms
step:303/1770 train_time:27701ms step_avg:94.54ms
step:304/1770 train_time:27795ms step_avg:94.54ms
step:305/1770 train_time:27890ms step_avg:94.54ms
step:306/1770 train_time:27986ms step_avg:94.55ms
step:307/1770 train_time:28081ms step_avg:94.55ms
step:308/1770 train_time:28176ms step_avg:94.55ms
step:309/1770 train_time:28272ms step_avg:94.55ms
step:310/1770 train_time:28367ms step_avg:94.56ms
step:311/1770 train_time:28462ms step_avg:94.56ms
step:312/1770 train_time:28558ms step_avg:94.56ms
step:313/1770 train_time:28653ms step_avg:94.56ms
step:314/1770 train_time:28748ms step_avg:94.57ms
step:315/1770 train_time:28843ms step_avg:94.57ms
step:316/1770 train_time:28939ms step_avg:94.57ms
step:317/1770 train_time:29033ms step_avg:94.57ms
step:318/1770 train_time:29128ms step_avg:94.57ms
step:319/1770 train_time:29223ms step_avg:94.57ms
step:320/1770 train_time:29319ms step_avg:94.58ms
step:321/1770 train_time:29414ms step_avg:94.58ms
step:322/1770 train_time:29509ms step_avg:94.58ms
step:323/1770 train_time:29604ms step_avg:94.58ms
step:324/1770 train_time:29700ms step_avg:94.59ms
step:325/1770 train_time:29795ms step_avg:94.59ms
step:326/1770 train_time:29891ms step_avg:94.59ms
step:327/1770 train_time:29986ms step_avg:94.59ms
step:328/1770 train_time:30081ms step_avg:94.60ms
step:329/1770 train_time:30176ms step_avg:94.60ms
step:330/1770 train_time:30272ms step_avg:94.60ms
step:331/1770 train_time:30367ms step_avg:94.60ms
step:332/1770 train_time:30462ms step_avg:94.60ms
step:333/1770 train_time:30557ms step_avg:94.61ms
step:334/1770 train_time:30653ms step_avg:94.61ms
step:335/1770 train_time:30748ms step_avg:94.61ms
step:336/1770 train_time:30843ms step_avg:94.61ms
step:337/1770 train_time:30939ms step_avg:94.61ms
step:338/1770 train_time:31034ms step_avg:94.61ms
step:339/1770 train_time:31128ms step_avg:94.62ms
step:340/1770 train_time:31224ms step_avg:94.62ms
step:341/1770 train_time:31320ms step_avg:94.62ms
step:342/1770 train_time:31414ms step_avg:94.62ms
step:343/1770 train_time:31509ms step_avg:94.62ms
step:344/1770 train_time:31604ms step_avg:94.62ms
step:345/1770 train_time:31699ms step_avg:94.63ms
step:346/1770 train_time:31795ms step_avg:94.63ms
step:347/1770 train_time:31891ms step_avg:94.63ms
step:348/1770 train_time:31986ms step_avg:94.63ms
step:349/1770 train_time:32081ms step_avg:94.63ms
step:350/1770 train_time:32177ms step_avg:94.64ms
step:351/1770 train_time:32272ms step_avg:94.64ms
step:352/1770 train_time:32367ms step_avg:94.64ms
step:353/1770 train_time:32463ms step_avg:94.64ms
step:354/1770 train_time:32558ms step_avg:94.64ms
step:355/1770 train_time:32652ms step_avg:94.64ms
step:356/1770 train_time:32747ms step_avg:94.65ms
step:357/1770 train_time:32842ms step_avg:94.65ms
step:358/1770 train_time:32938ms step_avg:94.65ms
step:359/1770 train_time:33034ms step_avg:94.65ms
step:360/1770 train_time:33129ms step_avg:94.65ms
step:361/1770 train_time:33224ms step_avg:94.66ms
step:362/1770 train_time:33320ms step_avg:94.66ms
step:363/1770 train_time:33415ms step_avg:94.66ms
step:364/1770 train_time:33510ms step_avg:94.66ms
step:365/1770 train_time:33605ms step_avg:94.66ms
step:366/1770 train_time:33700ms step_avg:94.66ms
step:367/1770 train_time:33795ms step_avg:94.66ms
step:368/1770 train_time:33890ms step_avg:94.66ms
step:369/1770 train_time:33985ms step_avg:94.67ms
step:370/1770 train_time:34081ms step_avg:94.67ms
step:371/1770 train_time:34177ms step_avg:94.67ms
step:372/1770 train_time:34272ms step_avg:94.67ms
step:373/1770 train_time:34367ms step_avg:94.67ms
step:374/1770 train_time:34462ms step_avg:94.68ms
step:375/1770 train_time:34558ms step_avg:94.68ms
step:375/1770 val_loss:3.9094 train_time:34651ms step_avg:94.94ms
step:376/1770 train_time:34674ms step_avg:94.74ms
step:377/1770 train_time:34760ms step_avg:94.71ms
step:378/1770 train_time:34859ms step_avg:94.73ms
step:379/1770 train_time:34955ms step_avg:94.73ms
step:380/1770 train_time:35050ms step_avg:94.73ms
step:381/1770 train_time:35144ms step_avg:94.73ms
step:382/1770 train_time:35239ms step_avg:94.73ms
step:383/1770 train_time:35333ms step_avg:94.73ms
step:384/1770 train_time:35428ms step_avg:94.73ms
step:385/1770 train_time:35523ms step_avg:94.73ms
step:386/1770 train_time:35617ms step_avg:94.73ms
step:387/1770 train_time:35713ms step_avg:94.73ms
step:388/1770 train_time:35808ms step_avg:94.73ms
step:389/1770 train_time:35905ms step_avg:94.74ms
step:390/1770 train_time:36001ms step_avg:94.74ms
step:391/1770 train_time:36096ms step_avg:94.74ms
step:392/1770 train_time:36191ms step_avg:94.74ms
step:393/1770 train_time:36286ms step_avg:94.74ms
step:394/1770 train_time:36381ms step_avg:94.74ms
step:395/1770 train_time:36475ms step_avg:94.74ms
step:396/1770 train_time:36572ms step_avg:94.75ms
step:397/1770 train_time:36669ms step_avg:94.75ms
step:398/1770 train_time:36766ms step_avg:94.76ms
step:399/1770 train_time:36863ms step_avg:94.76ms
step:400/1770 train_time:36960ms step_avg:94.77ms
step:401/1770 train_time:37057ms step_avg:94.78ms
step:402/1770 train_time:37154ms step_avg:94.78ms
step:403/1770 train_time:37251ms step_avg:94.79ms
step:404/1770 train_time:37348ms step_avg:94.79ms
step:405/1770 train_time:37445ms step_avg:94.80ms
step:406/1770 train_time:37542ms step_avg:94.80ms
step:407/1770 train_time:37640ms step_avg:94.81ms
step:408/1770 train_time:37737ms step_avg:94.82ms
step:409/1770 train_time:37834ms step_avg:94.82ms
step:410/1770 train_time:37932ms step_avg:94.83ms
step:411/1770 train_time:38029ms step_avg:94.84ms
step:412/1770 train_time:38126ms step_avg:94.84ms
step:413/1770 train_time:38224ms step_avg:94.85ms
step:414/1770 train_time:38320ms step_avg:94.85ms
step:415/1770 train_time:38417ms step_avg:94.86ms
step:416/1770 train_time:38514ms step_avg:94.86ms
step:417/1770 train_time:38611ms step_avg:94.87ms
step:418/1770 train_time:38707ms step_avg:94.87ms
step:419/1770 train_time:38804ms step_avg:94.87ms
step:420/1770 train_time:38901ms step_avg:94.88ms
step:421/1770 train_time:38998ms step_avg:94.89ms
step:422/1770 train_time:39095ms step_avg:94.89ms
step:423/1770 train_time:39192ms step_avg:94.90ms
step:424/1770 train_time:39288ms step_avg:94.90ms
step:425/1770 train_time:39385ms step_avg:94.90ms
step:426/1770 train_time:39482ms step_avg:94.91ms
step:427/1770 train_time:39580ms step_avg:94.92ms
step:428/1770 train_time:39676ms step_avg:94.92ms
step:429/1770 train_time:39773ms step_avg:94.92ms
step:430/1770 train_time:39870ms step_avg:94.93ms
step:431/1770 train_time:39966ms step_avg:94.93ms
step:432/1770 train_time:40063ms step_avg:94.94ms
step:433/1770 train_time:40161ms step_avg:94.94ms
step:434/1770 train_time:40258ms step_avg:94.95ms
step:435/1770 train_time:40355ms step_avg:94.95ms
step:436/1770 train_time:40451ms step_avg:94.96ms
step:437/1770 train_time:40549ms step_avg:94.96ms
step:438/1770 train_time:40646ms step_avg:94.97ms
step:439/1770 train_time:40744ms step_avg:94.97ms
step:440/1770 train_time:40841ms step_avg:94.98ms
step:441/1770 train_time:40939ms step_avg:94.99ms
step:442/1770 train_time:41036ms step_avg:94.99ms
step:443/1770 train_time:41133ms step_avg:95.00ms
step:444/1770 train_time:41230ms step_avg:95.00ms
step:445/1770 train_time:41327ms step_avg:95.00ms
step:446/1770 train_time:41424ms step_avg:95.01ms
step:447/1770 train_time:41521ms step_avg:95.01ms
step:448/1770 train_time:41618ms step_avg:95.02ms
step:449/1770 train_time:41716ms step_avg:95.03ms
step:450/1770 train_time:41813ms step_avg:95.03ms
step:451/1770 train_time:41910ms step_avg:95.03ms
step:452/1770 train_time:42006ms step_avg:95.04ms
step:453/1770 train_time:42104ms step_avg:95.04ms
step:454/1770 train_time:42201ms step_avg:95.05ms
step:455/1770 train_time:42298ms step_avg:95.05ms
step:456/1770 train_time:42395ms step_avg:95.06ms
step:457/1770 train_time:42492ms step_avg:95.06ms
step:458/1770 train_time:42589ms step_avg:95.06ms
step:459/1770 train_time:42686ms step_avg:95.07ms
step:460/1770 train_time:42783ms step_avg:95.07ms
step:461/1770 train_time:42880ms step_avg:95.08ms
step:462/1770 train_time:42977ms step_avg:95.08ms
step:463/1770 train_time:43074ms step_avg:95.09ms
step:464/1770 train_time:43171ms step_avg:95.09ms
step:465/1770 train_time:43268ms step_avg:95.10ms
step:466/1770 train_time:43366ms step_avg:95.10ms
step:467/1770 train_time:43463ms step_avg:95.11ms
step:468/1770 train_time:43561ms step_avg:95.11ms
step:469/1770 train_time:43658ms step_avg:95.11ms
step:470/1770 train_time:43755ms step_avg:95.12ms
step:471/1770 train_time:43852ms step_avg:95.12ms
step:472/1770 train_time:43949ms step_avg:95.13ms
step:473/1770 train_time:44045ms step_avg:95.13ms
step:474/1770 train_time:44143ms step_avg:95.14ms
step:475/1770 train_time:44240ms step_avg:95.14ms
step:476/1770 train_time:44337ms step_avg:95.14ms
step:477/1770 train_time:44434ms step_avg:95.15ms
step:478/1770 train_time:44531ms step_avg:95.15ms
step:479/1770 train_time:44628ms step_avg:95.16ms
step:480/1770 train_time:44725ms step_avg:95.16ms
step:481/1770 train_time:44823ms step_avg:95.17ms
step:482/1770 train_time:44920ms step_avg:95.17ms
step:483/1770 train_time:45017ms step_avg:95.17ms
step:484/1770 train_time:45114ms step_avg:95.18ms
step:485/1770 train_time:45211ms step_avg:95.18ms
step:486/1770 train_time:45308ms step_avg:95.19ms
step:487/1770 train_time:45405ms step_avg:95.19ms
step:488/1770 train_time:45503ms step_avg:95.19ms
step:489/1770 train_time:45600ms step_avg:95.20ms
step:490/1770 train_time:45698ms step_avg:95.20ms
step:491/1770 train_time:45795ms step_avg:95.21ms
step:492/1770 train_time:45892ms step_avg:95.21ms
step:493/1770 train_time:45988ms step_avg:95.21ms
step:494/1770 train_time:46085ms step_avg:95.22ms
step:495/1770 train_time:46182ms step_avg:95.22ms
step:496/1770 train_time:46279ms step_avg:95.22ms
step:497/1770 train_time:46376ms step_avg:95.23ms
step:498/1770 train_time:46473ms step_avg:95.23ms
step:499/1770 train_time:46570ms step_avg:95.23ms
step:500/1770 train_time:46666ms step_avg:95.24ms
step:500/1770 val_loss:3.7544 train_time:46762ms step_avg:95.43ms
step:501/1770 train_time:46783ms step_avg:95.28ms
step:502/1770 train_time:46874ms step_avg:95.27ms
step:503/1770 train_time:46973ms step_avg:95.28ms
step:504/1770 train_time:47072ms step_avg:95.29ms
step:505/1770 train_time:47169ms step_avg:95.29ms
step:506/1770 train_time:47265ms step_avg:95.29ms
step:507/1770 train_time:47362ms step_avg:95.29ms
step:508/1770 train_time:47458ms step_avg:95.30ms
step:509/1770 train_time:47555ms step_avg:95.30ms
step:510/1770 train_time:47651ms step_avg:95.30ms
step:511/1770 train_time:47748ms step_avg:95.31ms
step:512/1770 train_time:47845ms step_avg:95.31ms
step:513/1770 train_time:47942ms step_avg:95.31ms
step:514/1770 train_time:48040ms step_avg:95.32ms
step:515/1770 train_time:48138ms step_avg:95.32ms
step:516/1770 train_time:48235ms step_avg:95.33ms
step:517/1770 train_time:48332ms step_avg:95.33ms
step:518/1770 train_time:48429ms step_avg:95.33ms
step:519/1770 train_time:48525ms step_avg:95.33ms
step:520/1770 train_time:48623ms step_avg:95.34ms
step:521/1770 train_time:48720ms step_avg:95.34ms
step:522/1770 train_time:48817ms step_avg:95.35ms
step:523/1770 train_time:48914ms step_avg:95.35ms
step:524/1770 train_time:49011ms step_avg:95.35ms
step:525/1770 train_time:49109ms step_avg:95.36ms
step:526/1770 train_time:49206ms step_avg:95.36ms
step:527/1770 train_time:49303ms step_avg:95.36ms
step:528/1770 train_time:49401ms step_avg:95.37ms
step:529/1770 train_time:49498ms step_avg:95.37ms
step:530/1770 train_time:49595ms step_avg:95.37ms
step:531/1770 train_time:49692ms step_avg:95.38ms
step:532/1770 train_time:49789ms step_avg:95.38ms
step:533/1770 train_time:49886ms step_avg:95.38ms
step:534/1770 train_time:49983ms step_avg:95.39ms
step:535/1770 train_time:50081ms step_avg:95.39ms
step:536/1770 train_time:50178ms step_avg:95.40ms
step:537/1770 train_time:50277ms step_avg:95.40ms
step:538/1770 train_time:50374ms step_avg:95.41ms
step:539/1770 train_time:50471ms step_avg:95.41ms
step:540/1770 train_time:50568ms step_avg:95.41ms
step:541/1770 train_time:50665ms step_avg:95.41ms
step:542/1770 train_time:50763ms step_avg:95.42ms
step:543/1770 train_time:50860ms step_avg:95.42ms
step:544/1770 train_time:50958ms step_avg:95.43ms
step:545/1770 train_time:51056ms step_avg:95.43ms
step:546/1770 train_time:51153ms step_avg:95.43ms
step:547/1770 train_time:51251ms step_avg:95.44ms
step:548/1770 train_time:51349ms step_avg:95.44ms
step:549/1770 train_time:51446ms step_avg:95.45ms
step:550/1770 train_time:51543ms step_avg:95.45ms
step:551/1770 train_time:51640ms step_avg:95.45ms
step:552/1770 train_time:51737ms step_avg:95.46ms
step:553/1770 train_time:51834ms step_avg:95.46ms
step:554/1770 train_time:51932ms step_avg:95.46ms
step:555/1770 train_time:52030ms step_avg:95.47ms
step:556/1770 train_time:52127ms step_avg:95.47ms
step:557/1770 train_time:52225ms step_avg:95.47ms
step:558/1770 train_time:52323ms step_avg:95.48ms
step:559/1770 train_time:52421ms step_avg:95.48ms
step:560/1770 train_time:52518ms step_avg:95.49ms
step:561/1770 train_time:52615ms step_avg:95.49ms
step:562/1770 train_time:52712ms step_avg:95.49ms
step:563/1770 train_time:52809ms step_avg:95.50ms
step:564/1770 train_time:52906ms step_avg:95.50ms
step:565/1770 train_time:53003ms step_avg:95.50ms
step:566/1770 train_time:53100ms step_avg:95.50ms
step:567/1770 train_time:53198ms step_avg:95.51ms
step:568/1770 train_time:53296ms step_avg:95.51ms
step:569/1770 train_time:53394ms step_avg:95.52ms
step:570/1770 train_time:53492ms step_avg:95.52ms
step:571/1770 train_time:53589ms step_avg:95.52ms
step:572/1770 train_time:53686ms step_avg:95.53ms
step:573/1770 train_time:53783ms step_avg:95.53ms
step:574/1770 train_time:53881ms step_avg:95.53ms
step:575/1770 train_time:53978ms step_avg:95.54ms
step:576/1770 train_time:54075ms step_avg:95.54ms
step:577/1770 train_time:54171ms step_avg:95.54ms
step:578/1770 train_time:54268ms step_avg:95.54ms
step:579/1770 train_time:54366ms step_avg:95.55ms
step:580/1770 train_time:54464ms step_avg:95.55ms
step:581/1770 train_time:54562ms step_avg:95.55ms
step:582/1770 train_time:54659ms step_avg:95.56ms
step:583/1770 train_time:54756ms step_avg:95.56ms
step:584/1770 train_time:54853ms step_avg:95.56ms
step:585/1770 train_time:54951ms step_avg:95.57ms
step:586/1770 train_time:55048ms step_avg:95.57ms
step:587/1770 train_time:55146ms step_avg:95.57ms
step:588/1770 train_time:55244ms step_avg:95.58ms
step:589/1770 train_time:55341ms step_avg:95.58ms
step:590/1770 train_time:55439ms step_avg:95.58ms
step:591/1770 train_time:55536ms step_avg:95.59ms
step:592/1770 train_time:55633ms step_avg:95.59ms
step:593/1770 train_time:55730ms step_avg:95.59ms
step:594/1770 train_time:55827ms step_avg:95.59ms
step:595/1770 train_time:55925ms step_avg:95.60ms
step:596/1770 train_time:56023ms step_avg:95.60ms
step:597/1770 train_time:56120ms step_avg:95.61ms
step:598/1770 train_time:56218ms step_avg:95.61ms
step:599/1770 train_time:56316ms step_avg:95.61ms
step:600/1770 train_time:56414ms step_avg:95.62ms
step:601/1770 train_time:56510ms step_avg:95.62ms
step:602/1770 train_time:56607ms step_avg:95.62ms
step:603/1770 train_time:56705ms step_avg:95.62ms
step:604/1770 train_time:56802ms step_avg:95.63ms
step:605/1770 train_time:56899ms step_avg:95.63ms
step:606/1770 train_time:56997ms step_avg:95.63ms
step:607/1770 train_time:57094ms step_avg:95.64ms
step:608/1770 train_time:57191ms step_avg:95.64ms
step:609/1770 train_time:57288ms step_avg:95.64ms
step:610/1770 train_time:57385ms step_avg:95.64ms
step:611/1770 train_time:57483ms step_avg:95.65ms
step:612/1770 train_time:57580ms step_avg:95.65ms
step:613/1770 train_time:57678ms step_avg:95.65ms
step:614/1770 train_time:57776ms step_avg:95.66ms
step:615/1770 train_time:57873ms step_avg:95.66ms
step:616/1770 train_time:57971ms step_avg:95.66ms
step:617/1770 train_time:58068ms step_avg:95.66ms
step:618/1770 train_time:58165ms step_avg:95.67ms
step:619/1770 train_time:58262ms step_avg:95.67ms
step:620/1770 train_time:58359ms step_avg:95.67ms
step:621/1770 train_time:58457ms step_avg:95.67ms
step:622/1770 train_time:58555ms step_avg:95.68ms
step:623/1770 train_time:58652ms step_avg:95.68ms
step:624/1770 train_time:58750ms step_avg:95.68ms
step:625/1770 train_time:58847ms step_avg:95.69ms
step:625/1770 val_loss:3.6715 train_time:58943ms step_avg:95.84ms
step:626/1770 train_time:58965ms step_avg:95.72ms
step:627/1770 train_time:59053ms step_avg:95.71ms
step:628/1770 train_time:59152ms step_avg:95.71ms
step:629/1770 train_time:59250ms step_avg:95.72ms
step:630/1770 train_time:59347ms step_avg:95.72ms
step:631/1770 train_time:59443ms step_avg:95.72ms
step:632/1770 train_time:59540ms step_avg:95.72ms
step:633/1770 train_time:59637ms step_avg:95.73ms
step:634/1770 train_time:59734ms step_avg:95.73ms
step:635/1770 train_time:59830ms step_avg:95.73ms
step:636/1770 train_time:59927ms step_avg:95.73ms
step:637/1770 train_time:60026ms step_avg:95.73ms
step:638/1770 train_time:60125ms step_avg:95.74ms
step:639/1770 train_time:60223ms step_avg:95.74ms
step:640/1770 train_time:60321ms step_avg:95.75ms
step:641/1770 train_time:60419ms step_avg:95.75ms
step:642/1770 train_time:60516ms step_avg:95.75ms
step:643/1770 train_time:60613ms step_avg:95.75ms
step:644/1770 train_time:60710ms step_avg:95.76ms
step:645/1770 train_time:60807ms step_avg:95.76ms
step:646/1770 train_time:60904ms step_avg:95.76ms
step:647/1770 train_time:61001ms step_avg:95.76ms
step:648/1770 train_time:61099ms step_avg:95.77ms
step:649/1770 train_time:61197ms step_avg:95.77ms
step:650/1770 train_time:61295ms step_avg:95.77ms
step:651/1770 train_time:61392ms step_avg:95.78ms
step:652/1770 train_time:61489ms step_avg:95.78ms
step:653/1770 train_time:61586ms step_avg:95.78ms
step:654/1770 train_time:61684ms step_avg:95.78ms
step:655/1770 train_time:61781ms step_avg:95.78ms
step:656/1770 train_time:61878ms step_avg:95.79ms
step:657/1770 train_time:61976ms step_avg:95.79ms
step:658/1770 train_time:62076ms step_avg:95.80ms
step:659/1770 train_time:62176ms step_avg:95.80ms
step:660/1770 train_time:62276ms step_avg:95.81ms
step:661/1770 train_time:62376ms step_avg:95.82ms
step:662/1770 train_time:62476ms step_avg:95.82ms
step:663/1770 train_time:62575ms step_avg:95.83ms
step:664/1770 train_time:62675ms step_avg:95.83ms
step:665/1770 train_time:62774ms step_avg:95.84ms
step:666/1770 train_time:62874ms step_avg:95.84ms
step:667/1770 train_time:62973ms step_avg:95.85ms
step:668/1770 train_time:63072ms step_avg:95.85ms
step:669/1770 train_time:63170ms step_avg:95.86ms
step:670/1770 train_time:63269ms step_avg:95.86ms
step:671/1770 train_time:63368ms step_avg:95.87ms
step:672/1770 train_time:63467ms step_avg:95.87ms
step:673/1770 train_time:63566ms step_avg:95.88ms
step:674/1770 train_time:63665ms step_avg:95.88ms
step:675/1770 train_time:63764ms step_avg:95.89ms
step:676/1770 train_time:63864ms step_avg:95.89ms
step:677/1770 train_time:63964ms step_avg:95.90ms
step:678/1770 train_time:64063ms step_avg:95.90ms
step:679/1770 train_time:64163ms step_avg:95.91ms
step:680/1770 train_time:64262ms step_avg:95.91ms
step:681/1770 train_time:64361ms step_avg:95.92ms
step:682/1770 train_time:64461ms step_avg:95.92ms
step:683/1770 train_time:64560ms step_avg:95.93ms
step:684/1770 train_time:64659ms step_avg:95.93ms
step:685/1770 train_time:64758ms step_avg:95.94ms
step:686/1770 train_time:64856ms step_avg:95.94ms
step:687/1770 train_time:64956ms step_avg:95.95ms
step:688/1770 train_time:65057ms step_avg:95.95ms
step:689/1770 train_time:65157ms step_avg:95.96ms
step:690/1770 train_time:65256ms step_avg:95.97ms
step:691/1770 train_time:65356ms step_avg:95.97ms
step:692/1770 train_time:65456ms step_avg:95.98ms
step:693/1770 train_time:65556ms step_avg:95.98ms
step:694/1770 train_time:65655ms step_avg:95.99ms
step:695/1770 train_time:65756ms step_avg:95.99ms
step:696/1770 train_time:65855ms step_avg:96.00ms
step:697/1770 train_time:65954ms step_avg:96.00ms
step:698/1770 train_time:66053ms step_avg:96.01ms
step:699/1770 train_time:66152ms step_avg:96.01ms
step:700/1770 train_time:66252ms step_avg:96.02ms
step:701/1770 train_time:66351ms step_avg:96.02ms
step:702/1770 train_time:66450ms step_avg:96.03ms
step:703/1770 train_time:66549ms step_avg:96.03ms
step:704/1770 train_time:66648ms step_avg:96.03ms
step:705/1770 train_time:66747ms step_avg:96.04ms
step:706/1770 train_time:66847ms step_avg:96.04ms
step:707/1770 train_time:66946ms step_avg:96.05ms
step:708/1770 train_time:67045ms step_avg:96.05ms
step:709/1770 train_time:67144ms step_avg:96.06ms
step:710/1770 train_time:67244ms step_avg:96.06ms
step:711/1770 train_time:67344ms step_avg:96.07ms
step:712/1770 train_time:67444ms step_avg:96.07ms
step:713/1770 train_time:67544ms step_avg:96.08ms
step:714/1770 train_time:67643ms step_avg:96.08ms
step:715/1770 train_time:67744ms step_avg:96.09ms
step:716/1770 train_time:67843ms step_avg:96.09ms
step:717/1770 train_time:67942ms step_avg:96.10ms
step:718/1770 train_time:68041ms step_avg:96.10ms
step:719/1770 train_time:68140ms step_avg:96.11ms
step:720/1770 train_time:68239ms step_avg:96.11ms
step:721/1770 train_time:68339ms step_avg:96.12ms
step:722/1770 train_time:68438ms step_avg:96.12ms
step:723/1770 train_time:68538ms step_avg:96.13ms
step:724/1770 train_time:68638ms step_avg:96.13ms
step:725/1770 train_time:68738ms step_avg:96.14ms
step:726/1770 train_time:68838ms step_avg:96.14ms
step:727/1770 train_time:68937ms step_avg:96.15ms
step:728/1770 train_time:69037ms step_avg:96.15ms
step:729/1770 train_time:69136ms step_avg:96.16ms
step:730/1770 train_time:69235ms step_avg:96.16ms
step:731/1770 train_time:69334ms step_avg:96.16ms
step:732/1770 train_time:69434ms step_avg:96.17ms
step:733/1770 train_time:69533ms step_avg:96.17ms
step:734/1770 train_time:69633ms step_avg:96.18ms
step:735/1770 train_time:69732ms step_avg:96.18ms
step:736/1770 train_time:69831ms step_avg:96.19ms
step:737/1770 train_time:69929ms step_avg:96.19ms
step:738/1770 train_time:70028ms step_avg:96.19ms
step:739/1770 train_time:70127ms step_avg:96.20ms
step:740/1770 train_time:70225ms step_avg:96.20ms
step:741/1770 train_time:70325ms step_avg:96.20ms
step:742/1770 train_time:70424ms step_avg:96.21ms
step:743/1770 train_time:70524ms step_avg:96.21ms
step:744/1770 train_time:70623ms step_avg:96.22ms
step:745/1770 train_time:70723ms step_avg:96.22ms
step:746/1770 train_time:70823ms step_avg:96.23ms
step:747/1770 train_time:70923ms step_avg:96.23ms
step:748/1770 train_time:71022ms step_avg:96.24ms
step:749/1770 train_time:71121ms step_avg:96.24ms
step:750/1770 train_time:71220ms step_avg:96.24ms
step:750/1770 val_loss:3.6041 train_time:71318ms step_avg:96.38ms
step:751/1770 train_time:71339ms step_avg:96.27ms
step:752/1770 train_time:71432ms step_avg:96.27ms
step:753/1770 train_time:71532ms step_avg:96.27ms
step:754/1770 train_time:71632ms step_avg:96.28ms
step:755/1770 train_time:71730ms step_avg:96.28ms
step:756/1770 train_time:71829ms step_avg:96.29ms
step:757/1770 train_time:71927ms step_avg:96.29ms
step:758/1770 train_time:72025ms step_avg:96.29ms
step:759/1770 train_time:72124ms step_avg:96.29ms
step:760/1770 train_time:72222ms step_avg:96.30ms
step:761/1770 train_time:72321ms step_avg:96.30ms
step:762/1770 train_time:72421ms step_avg:96.30ms
step:763/1770 train_time:72521ms step_avg:96.31ms
step:764/1770 train_time:72620ms step_avg:96.31ms
step:765/1770 train_time:72721ms step_avg:96.32ms
step:766/1770 train_time:72820ms step_avg:96.32ms
step:767/1770 train_time:72920ms step_avg:96.33ms
step:768/1770 train_time:73020ms step_avg:96.33ms
step:769/1770 train_time:73119ms step_avg:96.34ms
step:770/1770 train_time:73218ms step_avg:96.34ms
step:771/1770 train_time:73316ms step_avg:96.34ms
step:772/1770 train_time:73415ms step_avg:96.35ms
step:773/1770 train_time:73513ms step_avg:96.35ms
step:774/1770 train_time:73612ms step_avg:96.35ms
step:775/1770 train_time:73711ms step_avg:96.35ms
step:776/1770 train_time:73810ms step_avg:96.36ms
step:777/1770 train_time:73910ms step_avg:96.36ms
step:778/1770 train_time:74010ms step_avg:96.37ms
step:779/1770 train_time:74109ms step_avg:96.37ms
step:780/1770 train_time:74207ms step_avg:96.37ms
step:781/1770 train_time:74307ms step_avg:96.38ms
step:782/1770 train_time:74405ms step_avg:96.38ms
step:783/1770 train_time:74505ms step_avg:96.38ms
step:784/1770 train_time:74605ms step_avg:96.39ms
step:785/1770 train_time:74705ms step_avg:96.39ms
step:786/1770 train_time:74805ms step_avg:96.40ms
step:787/1770 train_time:74905ms step_avg:96.40ms
step:788/1770 train_time:75004ms step_avg:96.41ms
step:789/1770 train_time:75104ms step_avg:96.41ms
step:790/1770 train_time:75203ms step_avg:96.41ms
step:791/1770 train_time:75302ms step_avg:96.42ms
step:792/1770 train_time:75402ms step_avg:96.42ms
step:793/1770 train_time:75501ms step_avg:96.43ms
step:794/1770 train_time:75600ms step_avg:96.43ms
step:795/1770 train_time:75700ms step_avg:96.43ms
step:796/1770 train_time:75800ms step_avg:96.44ms
step:797/1770 train_time:75899ms step_avg:96.44ms
step:798/1770 train_time:75999ms step_avg:96.45ms
step:799/1770 train_time:76098ms step_avg:96.45ms
step:800/1770 train_time:76198ms step_avg:96.45ms
step:801/1770 train_time:76297ms step_avg:96.46ms
step:802/1770 train_time:76396ms step_avg:96.46ms
step:803/1770 train_time:76496ms step_avg:96.46ms
step:804/1770 train_time:76594ms step_avg:96.47ms
step:805/1770 train_time:76693ms step_avg:96.47ms
step:806/1770 train_time:76793ms step_avg:96.47ms
step:807/1770 train_time:76892ms step_avg:96.48ms
step:808/1770 train_time:76991ms step_avg:96.48ms
step:809/1770 train_time:77090ms step_avg:96.48ms
step:810/1770 train_time:77191ms step_avg:96.49ms
step:811/1770 train_time:77289ms step_avg:96.49ms
step:812/1770 train_time:77389ms step_avg:96.49ms
step:813/1770 train_time:77489ms step_avg:96.50ms
step:814/1770 train_time:77588ms step_avg:96.50ms
step:815/1770 train_time:77686ms step_avg:96.50ms
step:816/1770 train_time:77786ms step_avg:96.51ms
step:817/1770 train_time:77885ms step_avg:96.51ms
step:818/1770 train_time:77984ms step_avg:96.52ms
step:819/1770 train_time:78083ms step_avg:96.52ms
step:820/1770 train_time:78184ms step_avg:96.52ms
step:821/1770 train_time:78283ms step_avg:96.53ms
step:822/1770 train_time:78382ms step_avg:96.53ms
step:823/1770 train_time:78481ms step_avg:96.53ms
step:824/1770 train_time:78581ms step_avg:96.54ms
step:825/1770 train_time:78681ms step_avg:96.54ms
step:826/1770 train_time:78781ms step_avg:96.55ms
step:827/1770 train_time:78881ms step_avg:96.55ms
step:828/1770 train_time:78980ms step_avg:96.55ms
step:829/1770 train_time:79079ms step_avg:96.56ms
step:830/1770 train_time:79178ms step_avg:96.56ms
step:831/1770 train_time:79277ms step_avg:96.56ms
step:832/1770 train_time:79376ms step_avg:96.56ms
step:833/1770 train_time:79476ms step_avg:96.57ms
step:834/1770 train_time:79575ms step_avg:96.57ms
step:835/1770 train_time:79674ms step_avg:96.57ms
step:836/1770 train_time:79774ms step_avg:96.58ms
step:837/1770 train_time:79874ms step_avg:96.58ms
step:838/1770 train_time:79973ms step_avg:96.59ms
step:839/1770 train_time:80072ms step_avg:96.59ms
step:840/1770 train_time:80171ms step_avg:96.59ms
step:841/1770 train_time:80271ms step_avg:96.60ms
step:842/1770 train_time:80370ms step_avg:96.60ms
step:843/1770 train_time:80470ms step_avg:96.60ms
step:844/1770 train_time:80570ms step_avg:96.61ms
step:845/1770 train_time:80670ms step_avg:96.61ms
step:846/1770 train_time:80768ms step_avg:96.61ms
step:847/1770 train_time:80867ms step_avg:96.62ms
step:848/1770 train_time:80966ms step_avg:96.62ms
step:849/1770 train_time:81066ms step_avg:96.62ms
step:850/1770 train_time:81165ms step_avg:96.62ms
step:851/1770 train_time:81264ms step_avg:96.63ms
step:852/1770 train_time:81363ms step_avg:96.63ms
step:853/1770 train_time:81463ms step_avg:96.63ms
step:854/1770 train_time:81563ms step_avg:96.64ms
step:855/1770 train_time:81664ms step_avg:96.64ms
step:856/1770 train_time:81764ms step_avg:96.65ms
step:857/1770 train_time:81863ms step_avg:96.65ms
step:858/1770 train_time:81963ms step_avg:96.65ms
step:859/1770 train_time:82063ms step_avg:96.66ms
step:860/1770 train_time:82163ms step_avg:96.66ms
step:861/1770 train_time:82263ms step_avg:96.67ms
step:862/1770 train_time:82363ms step_avg:96.67ms
step:863/1770 train_time:82462ms step_avg:96.67ms
step:864/1770 train_time:82562ms step_avg:96.68ms
step:865/1770 train_time:82663ms step_avg:96.68ms
step:866/1770 train_time:82763ms step_avg:96.69ms
step:867/1770 train_time:82863ms step_avg:96.69ms
step:868/1770 train_time:82963ms step_avg:96.69ms
step:869/1770 train_time:83063ms step_avg:96.70ms
step:870/1770 train_time:83163ms step_avg:96.70ms
step:871/1770 train_time:83262ms step_avg:96.70ms
step:872/1770 train_time:83361ms step_avg:96.71ms
step:873/1770 train_time:83460ms step_avg:96.71ms
step:874/1770 train_time:83559ms step_avg:96.71ms
step:875/1770 train_time:83658ms step_avg:96.71ms
step:875/1770 val_loss:3.5549 train_time:83756ms step_avg:96.83ms
step:876/1770 train_time:83778ms step_avg:96.74ms
step:877/1770 train_time:83871ms step_avg:96.74ms
step:878/1770 train_time:83971ms step_avg:96.74ms
step:879/1770 train_time:84071ms step_avg:96.75ms
step:880/1770 train_time:84171ms step_avg:96.75ms
step:881/1770 train_time:84269ms step_avg:96.75ms
step:882/1770 train_time:84368ms step_avg:96.75ms
step:883/1770 train_time:84466ms step_avg:96.75ms
step:884/1770 train_time:84565ms step_avg:96.76ms
step:885/1770 train_time:84664ms step_avg:96.76ms
step:886/1770 train_time:84764ms step_avg:96.76ms
step:887/1770 train_time:84865ms step_avg:96.77ms
step:888/1770 train_time:84966ms step_avg:96.77ms
step:889/1770 train_time:85067ms step_avg:96.78ms
step:890/1770 train_time:85166ms step_avg:96.78ms
step:891/1770 train_time:85266ms step_avg:96.78ms
step:892/1770 train_time:85365ms step_avg:96.79ms
step:893/1770 train_time:85464ms step_avg:96.79ms
step:894/1770 train_time:85563ms step_avg:96.79ms
step:895/1770 train_time:85662ms step_avg:96.79ms
step:896/1770 train_time:85761ms step_avg:96.80ms
step:897/1770 train_time:85860ms step_avg:96.80ms
step:898/1770 train_time:85960ms step_avg:96.80ms
step:899/1770 train_time:86059ms step_avg:96.80ms
step:900/1770 train_time:86158ms step_avg:96.81ms
step:901/1770 train_time:86257ms step_avg:96.81ms
step:902/1770 train_time:86356ms step_avg:96.81ms
step:903/1770 train_time:86456ms step_avg:96.81ms
step:904/1770 train_time:86555ms step_avg:96.82ms
step:905/1770 train_time:86655ms step_avg:96.82ms
step:906/1770 train_time:86754ms step_avg:96.82ms
step:907/1770 train_time:86854ms step_avg:96.83ms
step:908/1770 train_time:86954ms step_avg:96.83ms
step:909/1770 train_time:87053ms step_avg:96.83ms
step:910/1770 train_time:87153ms step_avg:96.84ms
step:911/1770 train_time:87253ms step_avg:96.84ms
step:912/1770 train_time:87353ms step_avg:96.84ms
step:913/1770 train_time:87453ms step_avg:96.85ms
step:914/1770 train_time:87552ms step_avg:96.85ms
step:915/1770 train_time:87652ms step_avg:96.85ms
step:916/1770 train_time:87752ms step_avg:96.86ms
step:917/1770 train_time:87851ms step_avg:96.86ms
step:918/1770 train_time:87949ms step_avg:96.86ms
step:919/1770 train_time:88049ms step_avg:96.86ms
step:920/1770 train_time:88151ms step_avg:96.87ms
step:921/1770 train_time:88252ms step_avg:96.87ms
step:922/1770 train_time:88353ms step_avg:96.88ms
step:923/1770 train_time:88453ms step_avg:96.88ms
step:924/1770 train_time:88554ms step_avg:96.89ms
step:925/1770 train_time:88656ms step_avg:96.89ms
step:926/1770 train_time:88757ms step_avg:96.90ms
step:927/1770 train_time:88857ms step_avg:96.90ms
step:928/1770 train_time:88957ms step_avg:96.90ms
step:929/1770 train_time:89058ms step_avg:96.91ms
step:930/1770 train_time:89159ms step_avg:96.91ms
step:931/1770 train_time:89260ms step_avg:96.92ms
step:932/1770 train_time:89360ms step_avg:96.92ms
step:933/1770 train_time:89462ms step_avg:96.92ms
step:934/1770 train_time:89562ms step_avg:96.93ms
step:935/1770 train_time:89662ms step_avg:96.93ms
step:936/1770 train_time:89762ms step_avg:96.94ms
step:937/1770 train_time:89863ms step_avg:96.94ms
step:938/1770 train_time:89965ms step_avg:96.95ms
step:939/1770 train_time:90066ms step_avg:96.95ms
step:940/1770 train_time:90166ms step_avg:96.95ms
step:941/1770 train_time:90267ms step_avg:96.96ms
step:942/1770 train_time:90369ms step_avg:96.96ms
step:943/1770 train_time:90471ms step_avg:96.97ms
step:944/1770 train_time:90571ms step_avg:96.97ms
step:945/1770 train_time:90671ms step_avg:96.97ms
step:946/1770 train_time:90772ms step_avg:96.98ms
step:947/1770 train_time:90874ms step_avg:96.98ms
step:948/1770 train_time:90975ms step_avg:96.99ms
step:949/1770 train_time:91077ms step_avg:96.99ms
step:950/1770 train_time:91179ms step_avg:97.00ms
step:951/1770 train_time:91280ms step_avg:97.00ms
step:952/1770 train_time:91380ms step_avg:97.01ms
step:953/1770 train_time:91481ms step_avg:97.01ms
step:954/1770 train_time:91580ms step_avg:97.01ms
step:955/1770 train_time:91681ms step_avg:97.02ms
step:956/1770 train_time:91781ms step_avg:97.02ms
step:957/1770 train_time:91881ms step_avg:97.02ms
step:958/1770 train_time:91982ms step_avg:97.03ms
step:959/1770 train_time:92083ms step_avg:97.03ms
step:960/1770 train_time:92184ms step_avg:97.04ms
step:961/1770 train_time:92284ms step_avg:97.04ms
step:962/1770 train_time:92386ms step_avg:97.04ms
step:963/1770 train_time:92487ms step_avg:97.05ms
step:964/1770 train_time:92589ms step_avg:97.05ms
step:965/1770 train_time:92690ms step_avg:97.06ms
step:966/1770 train_time:92791ms step_avg:97.06ms
step:967/1770 train_time:92892ms step_avg:97.07ms
step:968/1770 train_time:92993ms step_avg:97.07ms
step:969/1770 train_time:93094ms step_avg:97.07ms
step:970/1770 train_time:93195ms step_avg:97.08ms
step:971/1770 train_time:93296ms step_avg:97.08ms
step:972/1770 train_time:93398ms step_avg:97.09ms
step:973/1770 train_time:93499ms step_avg:97.09ms
step:974/1770 train_time:93600ms step_avg:97.10ms
step:975/1770 train_time:93701ms step_avg:97.10ms
step:976/1770 train_time:93801ms step_avg:97.10ms
step:977/1770 train_time:93901ms step_avg:97.11ms
step:978/1770 train_time:94001ms step_avg:97.11ms
step:979/1770 train_time:94101ms step_avg:97.11ms
step:980/1770 train_time:94201ms step_avg:97.11ms
step:981/1770 train_time:94302ms step_avg:97.12ms
step:982/1770 train_time:94403ms step_avg:97.12ms
step:983/1770 train_time:94503ms step_avg:97.13ms
step:984/1770 train_time:94605ms step_avg:97.13ms
step:985/1770 train_time:94706ms step_avg:97.13ms
step:986/1770 train_time:94806ms step_avg:97.14ms
step:987/1770 train_time:94907ms step_avg:97.14ms
step:988/1770 train_time:95008ms step_avg:97.15ms
step:989/1770 train_time:95110ms step_avg:97.15ms
step:990/1770 train_time:95210ms step_avg:97.15ms
step:991/1770 train_time:95312ms step_avg:97.16ms
step:992/1770 train_time:95412ms step_avg:97.16ms
step:993/1770 train_time:95513ms step_avg:97.16ms
step:994/1770 train_time:95614ms step_avg:97.17ms
step:995/1770 train_time:95715ms step_avg:97.17ms
step:996/1770 train_time:95816ms step_avg:97.18ms
step:997/1770 train_time:95917ms step_avg:97.18ms
step:998/1770 train_time:96018ms step_avg:97.18ms
step:999/1770 train_time:96119ms step_avg:97.19ms
step:1000/1770 train_time:96220ms step_avg:97.19ms
step:1000/1770 val_loss:3.5172 train_time:96319ms step_avg:97.29ms
step:1001/1770 train_time:96340ms step_avg:97.22ms
step:1002/1770 train_time:96433ms step_avg:97.21ms
step:1003/1770 train_time:96536ms step_avg:97.22ms
step:1004/1770 train_time:96637ms step_avg:97.22ms
step:1005/1770 train_time:96736ms step_avg:97.22ms
step:1006/1770 train_time:96836ms step_avg:97.23ms
step:1007/1770 train_time:96937ms step_avg:97.23ms
step:1008/1770 train_time:97037ms step_avg:97.23ms
step:1009/1770 train_time:97137ms step_avg:97.23ms
step:1010/1770 train_time:97237ms step_avg:97.24ms
step:1011/1770 train_time:97342ms step_avg:97.24ms
step:1012/1770 train_time:97446ms step_avg:97.25ms
step:1013/1770 train_time:97549ms step_avg:97.26ms
step:1014/1770 train_time:97648ms step_avg:97.26ms
step:1015/1770 train_time:97749ms step_avg:97.26ms
step:1016/1770 train_time:97849ms step_avg:97.27ms
step:1017/1770 train_time:97950ms step_avg:97.27ms
step:1018/1770 train_time:98051ms step_avg:97.27ms
step:1019/1770 train_time:98153ms step_avg:97.28ms
step:1020/1770 train_time:98253ms step_avg:97.28ms
step:1021/1770 train_time:98354ms step_avg:97.28ms
step:1022/1770 train_time:98454ms step_avg:97.29ms
step:1023/1770 train_time:98554ms step_avg:97.29ms
step:1024/1770 train_time:98654ms step_avg:97.29ms
step:1025/1770 train_time:98755ms step_avg:97.30ms
step:1026/1770 train_time:98856ms step_avg:97.30ms
step:1027/1770 train_time:98958ms step_avg:97.30ms
step:1028/1770 train_time:99060ms step_avg:97.31ms
step:1029/1770 train_time:99162ms step_avg:97.31ms
step:1030/1770 train_time:99262ms step_avg:97.32ms
step:1031/1770 train_time:99363ms step_avg:97.32ms
step:1032/1770 train_time:99464ms step_avg:97.32ms
step:1033/1770 train_time:99565ms step_avg:97.33ms
step:1034/1770 train_time:99666ms step_avg:97.33ms
step:1035/1770 train_time:99768ms step_avg:97.33ms
step:1036/1770 train_time:99868ms step_avg:97.34ms
step:1037/1770 train_time:99970ms step_avg:97.34ms
step:1038/1770 train_time:100071ms step_avg:97.35ms
step:1039/1770 train_time:100172ms step_avg:97.35ms
step:1040/1770 train_time:100273ms step_avg:97.35ms
step:1041/1770 train_time:100373ms step_avg:97.36ms
step:1042/1770 train_time:100474ms step_avg:97.36ms
step:1043/1770 train_time:100574ms step_avg:97.36ms
step:1044/1770 train_time:100674ms step_avg:97.36ms
step:1045/1770 train_time:100776ms step_avg:97.37ms
step:1046/1770 train_time:100877ms step_avg:97.37ms
step:1047/1770 train_time:100978ms step_avg:97.37ms
step:1048/1770 train_time:101079ms step_avg:97.38ms
step:1049/1770 train_time:101181ms step_avg:97.38ms
step:1050/1770 train_time:101282ms step_avg:97.39ms
step:1051/1770 train_time:101383ms step_avg:97.39ms
step:1052/1770 train_time:101484ms step_avg:97.39ms
step:1053/1770 train_time:101584ms step_avg:97.40ms
step:1054/1770 train_time:101685ms step_avg:97.40ms
step:1055/1770 train_time:101785ms step_avg:97.40ms
step:1056/1770 train_time:101887ms step_avg:97.41ms
step:1057/1770 train_time:101988ms step_avg:97.41ms
step:1058/1770 train_time:102092ms step_avg:97.42ms
step:1059/1770 train_time:102193ms step_avg:97.42ms
step:1060/1770 train_time:102294ms step_avg:97.42ms
step:1061/1770 train_time:102394ms step_avg:97.43ms
step:1062/1770 train_time:102496ms step_avg:97.43ms
step:1063/1770 train_time:102598ms step_avg:97.43ms
step:1064/1770 train_time:102699ms step_avg:97.44ms
step:1065/1770 train_time:102800ms step_avg:97.44ms
step:1066/1770 train_time:102902ms step_avg:97.44ms
step:1067/1770 train_time:103003ms step_avg:97.45ms
step:1068/1770 train_time:103108ms step_avg:97.46ms
step:1069/1770 train_time:103209ms step_avg:97.46ms
step:1070/1770 train_time:103311ms step_avg:97.46ms
step:1071/1770 train_time:103412ms step_avg:97.47ms
step:1072/1770 train_time:103512ms step_avg:97.47ms
step:1073/1770 train_time:103613ms step_avg:97.47ms
step:1074/1770 train_time:103713ms step_avg:97.47ms
step:1075/1770 train_time:103814ms step_avg:97.48ms
step:1076/1770 train_time:103915ms step_avg:97.48ms
step:1077/1770 train_time:104016ms step_avg:97.48ms
step:1078/1770 train_time:104118ms step_avg:97.49ms
step:1079/1770 train_time:104220ms step_avg:97.49ms
step:1080/1770 train_time:104322ms step_avg:97.50ms
step:1081/1770 train_time:104423ms step_avg:97.50ms
step:1082/1770 train_time:104524ms step_avg:97.50ms
step:1083/1770 train_time:104624ms step_avg:97.51ms
step:1084/1770 train_time:104726ms step_avg:97.51ms
step:1085/1770 train_time:104827ms step_avg:97.51ms
step:1086/1770 train_time:104929ms step_avg:97.52ms
step:1087/1770 train_time:105031ms step_avg:97.52ms
step:1088/1770 train_time:105132ms step_avg:97.52ms
step:1089/1770 train_time:105233ms step_avg:97.53ms
step:1090/1770 train_time:105334ms step_avg:97.53ms
step:1091/1770 train_time:105434ms step_avg:97.53ms
step:1092/1770 train_time:105534ms step_avg:97.54ms
step:1093/1770 train_time:105634ms step_avg:97.54ms
step:1094/1770 train_time:105737ms step_avg:97.54ms
step:1095/1770 train_time:105838ms step_avg:97.55ms
step:1096/1770 train_time:105940ms step_avg:97.55ms
step:1097/1770 train_time:106041ms step_avg:97.55ms
step:1098/1770 train_time:106142ms step_avg:97.56ms
step:1099/1770 train_time:106243ms step_avg:97.56ms
step:1100/1770 train_time:106343ms step_avg:97.56ms
step:1101/1770 train_time:106444ms step_avg:97.57ms
step:1102/1770 train_time:106546ms step_avg:97.57ms
step:1103/1770 train_time:106647ms step_avg:97.57ms
step:1104/1770 train_time:106750ms step_avg:97.58ms
step:1105/1770 train_time:106852ms step_avg:97.58ms
step:1106/1770 train_time:106953ms step_avg:97.59ms
step:1107/1770 train_time:107053ms step_avg:97.59ms
step:1108/1770 train_time:107154ms step_avg:97.59ms
step:1109/1770 train_time:107254ms step_avg:97.59ms
step:1110/1770 train_time:107355ms step_avg:97.60ms
step:1111/1770 train_time:107457ms step_avg:97.60ms
step:1112/1770 train_time:107558ms step_avg:97.60ms
step:1113/1770 train_time:107659ms step_avg:97.61ms
step:1114/1770 train_time:107760ms step_avg:97.61ms
step:1115/1770 train_time:107862ms step_avg:97.61ms
step:1116/1770 train_time:107965ms step_avg:97.62ms
step:1117/1770 train_time:108066ms step_avg:97.62ms
step:1118/1770 train_time:108167ms step_avg:97.62ms
step:1119/1770 train_time:108268ms step_avg:97.63ms
step:1120/1770 train_time:108370ms step_avg:97.63ms
step:1121/1770 train_time:108471ms step_avg:97.63ms
step:1122/1770 train_time:108571ms step_avg:97.64ms
step:1123/1770 train_time:108672ms step_avg:97.64ms
step:1124/1770 train_time:108773ms step_avg:97.64ms
step:1125/1770 train_time:108874ms step_avg:97.65ms
step:1125/1770 val_loss:3.4765 train_time:108972ms step_avg:97.73ms
step:1126/1770 train_time:108995ms step_avg:97.67ms
step:1127/1770 train_time:109086ms step_avg:97.66ms
step:1128/1770 train_time:109187ms step_avg:97.66ms
step:1129/1770 train_time:109287ms step_avg:97.67ms
step:1130/1770 train_time:109388ms step_avg:97.67ms
step:1131/1770 train_time:109488ms step_avg:97.67ms
step:1132/1770 train_time:109589ms step_avg:97.67ms
step:1133/1770 train_time:109689ms step_avg:97.68ms
step:1134/1770 train_time:109790ms step_avg:97.68ms
step:1135/1770 train_time:109890ms step_avg:97.68ms
step:1136/1770 train_time:109992ms step_avg:97.68ms
step:1137/1770 train_time:110095ms step_avg:97.69ms
step:1138/1770 train_time:110197ms step_avg:97.69ms
step:1139/1770 train_time:110298ms step_avg:97.70ms
step:1140/1770 train_time:110398ms step_avg:97.70ms
step:1141/1770 train_time:110499ms step_avg:97.70ms
step:1142/1770 train_time:110600ms step_avg:97.70ms
step:1143/1770 train_time:110701ms step_avg:97.71ms
step:1144/1770 train_time:110801ms step_avg:97.71ms
step:1145/1770 train_time:110902ms step_avg:97.71ms
step:1146/1770 train_time:111004ms step_avg:97.71ms
step:1147/1770 train_time:111104ms step_avg:97.72ms
step:1148/1770 train_time:111205ms step_avg:97.72ms
step:1149/1770 train_time:111306ms step_avg:97.72ms
step:1150/1770 train_time:111408ms step_avg:97.73ms
step:1151/1770 train_time:111509ms step_avg:97.73ms
step:1152/1770 train_time:111611ms step_avg:97.73ms
step:1153/1770 train_time:111712ms step_avg:97.74ms
step:1154/1770 train_time:111814ms step_avg:97.74ms
step:1155/1770 train_time:111916ms step_avg:97.74ms
step:1156/1770 train_time:112017ms step_avg:97.75ms
step:1157/1770 train_time:112121ms step_avg:97.75ms
step:1158/1770 train_time:112221ms step_avg:97.75ms
step:1159/1770 train_time:112322ms step_avg:97.76ms
step:1160/1770 train_time:112422ms step_avg:97.76ms
step:1161/1770 train_time:112523ms step_avg:97.76ms
step:1162/1770 train_time:112624ms step_avg:97.76ms
step:1163/1770 train_time:112724ms step_avg:97.77ms
step:1164/1770 train_time:112825ms step_avg:97.77ms
step:1165/1770 train_time:112926ms step_avg:97.77ms
step:1166/1770 train_time:113028ms step_avg:97.77ms
step:1167/1770 train_time:113129ms step_avg:97.78ms
step:1168/1770 train_time:113231ms step_avg:97.78ms
step:1169/1770 train_time:113332ms step_avg:97.78ms
step:1170/1770 train_time:113434ms step_avg:97.79ms
step:1171/1770 train_time:113536ms step_avg:97.79ms
step:1172/1770 train_time:113637ms step_avg:97.79ms
step:1173/1770 train_time:113738ms step_avg:97.80ms
step:1174/1770 train_time:113840ms step_avg:97.80ms
step:1175/1770 train_time:113941ms step_avg:97.80ms
step:1176/1770 train_time:114042ms step_avg:97.81ms
step:1177/1770 train_time:114143ms step_avg:97.81ms
step:1178/1770 train_time:114244ms step_avg:97.81ms
step:1179/1770 train_time:114345ms step_avg:97.81ms
step:1180/1770 train_time:114445ms step_avg:97.82ms
step:1181/1770 train_time:114547ms step_avg:97.82ms
step:1182/1770 train_time:114647ms step_avg:97.82ms
step:1183/1770 train_time:114751ms step_avg:97.83ms
step:1184/1770 train_time:114856ms step_avg:97.83ms
step:1185/1770 train_time:114958ms step_avg:97.84ms
step:1186/1770 train_time:115060ms step_avg:97.84ms
step:1187/1770 train_time:115164ms step_avg:97.85ms
step:1188/1770 train_time:115265ms step_avg:97.85ms
step:1189/1770 train_time:115366ms step_avg:97.85ms
step:1190/1770 train_time:115467ms step_avg:97.85ms
step:1191/1770 train_time:115569ms step_avg:97.86ms
step:1192/1770 train_time:115672ms step_avg:97.86ms
step:1193/1770 train_time:115775ms step_avg:97.87ms
step:1194/1770 train_time:115878ms step_avg:97.87ms
step:1195/1770 train_time:115980ms step_avg:97.87ms
step:1196/1770 train_time:116083ms step_avg:97.88ms
step:1197/1770 train_time:116185ms step_avg:97.88ms
step:1198/1770 train_time:116286ms step_avg:97.88ms
step:1199/1770 train_time:116389ms step_avg:97.89ms
step:1200/1770 train_time:116491ms step_avg:97.89ms
step:1201/1770 train_time:116594ms step_avg:97.90ms
step:1202/1770 train_time:116695ms step_avg:97.90ms
step:1203/1770 train_time:116796ms step_avg:97.90ms
step:1204/1770 train_time:116900ms step_avg:97.91ms
step:1205/1770 train_time:117000ms step_avg:97.91ms
step:1206/1770 train_time:117103ms step_avg:97.91ms
step:1207/1770 train_time:117205ms step_avg:97.92ms
step:1208/1770 train_time:117306ms step_avg:97.92ms
step:1209/1770 train_time:117408ms step_avg:97.92ms
step:1210/1770 train_time:117510ms step_avg:97.92ms
step:1211/1770 train_time:117612ms step_avg:97.93ms
step:1212/1770 train_time:117716ms step_avg:97.93ms
step:1213/1770 train_time:117818ms step_avg:97.94ms
step:1214/1770 train_time:117920ms step_avg:97.94ms
step:1215/1770 train_time:118021ms step_avg:97.94ms
step:1216/1770 train_time:118127ms step_avg:97.95ms
step:1217/1770 train_time:118229ms step_avg:97.95ms
step:1218/1770 train_time:118330ms step_avg:97.96ms
step:1219/1770 train_time:118433ms step_avg:97.96ms
step:1220/1770 train_time:118535ms step_avg:97.96ms
step:1221/1770 train_time:118637ms step_avg:97.97ms
step:1222/1770 train_time:118741ms step_avg:97.97ms
step:1223/1770 train_time:118842ms step_avg:97.97ms
step:1224/1770 train_time:118945ms step_avg:97.98ms
step:1225/1770 train_time:119048ms step_avg:97.98ms
step:1226/1770 train_time:119149ms step_avg:97.98ms
step:1227/1770 train_time:119253ms step_avg:97.99ms
step:1228/1770 train_time:119357ms step_avg:97.99ms
step:1229/1770 train_time:119459ms step_avg:98.00ms
step:1230/1770 train_time:119562ms step_avg:98.00ms
step:1231/1770 train_time:119664ms step_avg:98.01ms
step:1232/1770 train_time:119766ms step_avg:98.01ms
step:1233/1770 train_time:119867ms step_avg:98.01ms
step:1234/1770 train_time:119969ms step_avg:98.01ms
step:1235/1770 train_time:120071ms step_avg:98.02ms
step:1236/1770 train_time:120174ms step_avg:98.02ms
step:1237/1770 train_time:120277ms step_avg:98.03ms
step:1238/1770 train_time:120380ms step_avg:98.03ms
step:1239/1770 train_time:120481ms step_avg:98.03ms
step:1240/1770 train_time:120584ms step_avg:98.04ms
step:1241/1770 train_time:120685ms step_avg:98.04ms
step:1242/1770 train_time:120787ms step_avg:98.04ms
step:1243/1770 train_time:120889ms step_avg:98.04ms
step:1244/1770 train_time:120991ms step_avg:98.05ms
step:1245/1770 train_time:121092ms step_avg:98.05ms
step:1246/1770 train_time:121196ms step_avg:98.05ms
step:1247/1770 train_time:121297ms step_avg:98.06ms
step:1248/1770 train_time:121399ms step_avg:98.06ms
step:1249/1770 train_time:121501ms step_avg:98.06ms
step:1250/1770 train_time:121603ms step_avg:98.07ms
step:1250/1770 val_loss:3.4269 train_time:121704ms step_avg:98.15ms
step:1251/1770 train_time:121727ms step_avg:98.09ms
step:1252/1770 train_time:121819ms step_avg:98.08ms
step:1253/1770 train_time:121923ms step_avg:98.09ms
step:1254/1770 train_time:122025ms step_avg:98.09ms
step:1255/1770 train_time:122129ms step_avg:98.10ms
step:1256/1770 train_time:122230ms step_avg:98.10ms
step:1257/1770 train_time:122332ms step_avg:98.10ms
step:1258/1770 train_time:122434ms step_avg:98.10ms
step:1259/1770 train_time:122536ms step_avg:98.11ms
step:1260/1770 train_time:122638ms step_avg:98.11ms
step:1261/1770 train_time:122741ms step_avg:98.11ms
step:1262/1770 train_time:122845ms step_avg:98.12ms
step:1263/1770 train_time:122946ms step_avg:98.12ms
step:1264/1770 train_time:123050ms step_avg:98.13ms
step:1265/1770 train_time:123152ms step_avg:98.13ms
step:1266/1770 train_time:123255ms step_avg:98.13ms
step:1267/1770 train_time:123357ms step_avg:98.14ms
step:1268/1770 train_time:123459ms step_avg:98.14ms
step:1269/1770 train_time:123561ms step_avg:98.14ms
step:1270/1770 train_time:123663ms step_avg:98.15ms
step:1271/1770 train_time:123765ms step_avg:98.15ms
step:1272/1770 train_time:123866ms step_avg:98.15ms
step:1273/1770 train_time:123970ms step_avg:98.15ms
step:1274/1770 train_time:124072ms step_avg:98.16ms
step:1275/1770 train_time:124174ms step_avg:98.16ms
step:1276/1770 train_time:124276ms step_avg:98.16ms
step:1277/1770 train_time:124378ms step_avg:98.17ms
step:1278/1770 train_time:124482ms step_avg:98.17ms
step:1279/1770 train_time:124584ms step_avg:98.17ms
step:1280/1770 train_time:124687ms step_avg:98.18ms
step:1281/1770 train_time:124788ms step_avg:98.18ms
step:1282/1770 train_time:124892ms step_avg:98.19ms
step:1283/1770 train_time:124995ms step_avg:98.19ms
step:1284/1770 train_time:125098ms step_avg:98.19ms
step:1285/1770 train_time:125200ms step_avg:98.20ms
step:1286/1770 train_time:125304ms step_avg:98.20ms
step:1287/1770 train_time:125408ms step_avg:98.21ms
step:1288/1770 train_time:125511ms step_avg:98.21ms
step:1289/1770 train_time:125613ms step_avg:98.21ms
step:1290/1770 train_time:125714ms step_avg:98.21ms
step:1291/1770 train_time:125817ms step_avg:98.22ms
step:1292/1770 train_time:125920ms step_avg:98.22ms
step:1293/1770 train_time:126023ms step_avg:98.22ms
step:1294/1770 train_time:126124ms step_avg:98.23ms
step:1295/1770 train_time:126226ms step_avg:98.23ms
step:1296/1770 train_time:126330ms step_avg:98.23ms
step:1297/1770 train_time:126432ms step_avg:98.24ms
step:1298/1770 train_time:126534ms step_avg:98.24ms
step:1299/1770 train_time:126636ms step_avg:98.24ms
step:1300/1770 train_time:126737ms step_avg:98.25ms
step:1301/1770 train_time:126840ms step_avg:98.25ms
step:1302/1770 train_time:126943ms step_avg:98.25ms
step:1303/1770 train_time:127045ms step_avg:98.26ms
step:1304/1770 train_time:127147ms step_avg:98.26ms
step:1305/1770 train_time:127249ms step_avg:98.26ms
step:1306/1770 train_time:127351ms step_avg:98.26ms
step:1307/1770 train_time:127453ms step_avg:98.27ms
step:1308/1770 train_time:127556ms step_avg:98.27ms
step:1309/1770 train_time:127659ms step_avg:98.27ms
step:1310/1770 train_time:127760ms step_avg:98.28ms
step:1311/1770 train_time:127862ms step_avg:98.28ms
step:1312/1770 train_time:127963ms step_avg:98.28ms
step:1313/1770 train_time:128064ms step_avg:98.28ms
step:1314/1770 train_time:128166ms step_avg:98.29ms
step:1315/1770 train_time:128269ms step_avg:98.29ms
step:1316/1770 train_time:128371ms step_avg:98.29ms
step:1317/1770 train_time:128474ms step_avg:98.30ms
step:1318/1770 train_time:128579ms step_avg:98.30ms
step:1319/1770 train_time:128682ms step_avg:98.31ms
step:1320/1770 train_time:128783ms step_avg:98.31ms
step:1321/1770 train_time:128885ms step_avg:98.31ms
step:1322/1770 train_time:128987ms step_avg:98.31ms
step:1323/1770 train_time:129091ms step_avg:98.32ms
step:1324/1770 train_time:129193ms step_avg:98.32ms
step:1325/1770 train_time:129297ms step_avg:98.32ms
step:1326/1770 train_time:129399ms step_avg:98.33ms
step:1327/1770 train_time:129503ms step_avg:98.33ms
step:1328/1770 train_time:129605ms step_avg:98.33ms
step:1329/1770 train_time:129708ms step_avg:98.34ms
step:1330/1770 train_time:129810ms step_avg:98.34ms
step:1331/1770 train_time:129911ms step_avg:98.34ms
step:1332/1770 train_time:130013ms step_avg:98.35ms
step:1333/1770 train_time:130115ms step_avg:98.35ms
step:1334/1770 train_time:130217ms step_avg:98.35ms
step:1335/1770 train_time:130319ms step_avg:98.35ms
step:1336/1770 train_time:130421ms step_avg:98.36ms
step:1337/1770 train_time:130524ms step_avg:98.36ms
step:1338/1770 train_time:130625ms step_avg:98.36ms
step:1339/1770 train_time:130727ms step_avg:98.36ms
step:1340/1770 train_time:130830ms step_avg:98.37ms
step:1341/1770 train_time:130932ms step_avg:98.37ms
step:1342/1770 train_time:131035ms step_avg:98.37ms
step:1343/1770 train_time:131138ms step_avg:98.38ms
step:1344/1770 train_time:131240ms step_avg:98.38ms
step:1345/1770 train_time:131342ms step_avg:98.38ms
step:1346/1770 train_time:131444ms step_avg:98.39ms
step:1347/1770 train_time:131547ms step_avg:98.39ms
step:1348/1770 train_time:131651ms step_avg:98.39ms
step:1349/1770 train_time:131754ms step_avg:98.40ms
step:1350/1770 train_time:131856ms step_avg:98.40ms
step:1351/1770 train_time:131958ms step_avg:98.40ms
step:1352/1770 train_time:132060ms step_avg:98.41ms
step:1353/1770 train_time:132164ms step_avg:98.41ms
step:1354/1770 train_time:132265ms step_avg:98.41ms
step:1355/1770 train_time:132367ms step_avg:98.41ms
step:1356/1770 train_time:132469ms step_avg:98.42ms
step:1357/1770 train_time:132572ms step_avg:98.42ms
step:1358/1770 train_time:132674ms step_avg:98.42ms
step:1359/1770 train_time:132777ms step_avg:98.43ms
step:1360/1770 train_time:132880ms step_avg:98.43ms
step:1361/1770 train_time:132982ms step_avg:98.43ms
step:1362/1770 train_time:133084ms step_avg:98.44ms
step:1363/1770 train_time:133187ms step_avg:98.44ms
step:1364/1770 train_time:133289ms step_avg:98.44ms
step:1365/1770 train_time:133391ms step_avg:98.44ms
step:1366/1770 train_time:133493ms step_avg:98.45ms
step:1367/1770 train_time:133597ms step_avg:98.45ms
step:1368/1770 train_time:133699ms step_avg:98.45ms
step:1369/1770 train_time:133802ms step_avg:98.46ms
step:1370/1770 train_time:133905ms step_avg:98.46ms
step:1371/1770 train_time:134007ms step_avg:98.46ms
step:1372/1770 train_time:134109ms step_avg:98.46ms
step:1373/1770 train_time:134211ms step_avg:98.47ms
step:1374/1770 train_time:134314ms step_avg:98.47ms
step:1375/1770 train_time:134416ms step_avg:98.47ms
step:1375/1770 val_loss:3.3848 train_time:134517ms step_avg:98.55ms
step:1376/1770 train_time:134538ms step_avg:98.49ms
step:1377/1770 train_time:134633ms step_avg:98.49ms
step:1378/1770 train_time:134735ms step_avg:98.49ms
step:1379/1770 train_time:134837ms step_avg:98.49ms
step:1380/1770 train_time:134938ms step_avg:98.50ms
step:1381/1770 train_time:135041ms step_avg:98.50ms
step:1382/1770 train_time:135142ms step_avg:98.50ms
step:1383/1770 train_time:135245ms step_avg:98.50ms
step:1384/1770 train_time:135347ms step_avg:98.51ms
step:1385/1770 train_time:135448ms step_avg:98.51ms
step:1386/1770 train_time:135552ms step_avg:98.51ms
step:1387/1770 train_time:135656ms step_avg:98.52ms
step:1388/1770 train_time:135758ms step_avg:98.52ms
step:1389/1770 train_time:135860ms step_avg:98.52ms
step:1390/1770 train_time:135962ms step_avg:98.52ms
step:1391/1770 train_time:136065ms step_avg:98.53ms
step:1392/1770 train_time:136166ms step_avg:98.53ms
step:1393/1770 train_time:136269ms step_avg:98.53ms
step:1394/1770 train_time:136370ms step_avg:98.53ms
step:1395/1770 train_time:136473ms step_avg:98.54ms
step:1396/1770 train_time:136577ms step_avg:98.54ms
step:1397/1770 train_time:136679ms step_avg:98.54ms
step:1398/1770 train_time:136782ms step_avg:98.55ms
step:1399/1770 train_time:136884ms step_avg:98.55ms
step:1400/1770 train_time:136986ms step_avg:98.55ms
step:1401/1770 train_time:137088ms step_avg:98.55ms
step:1402/1770 train_time:137191ms step_avg:98.56ms
step:1403/1770 train_time:137293ms step_avg:98.56ms
step:1404/1770 train_time:137396ms step_avg:98.56ms
step:1405/1770 train_time:137498ms step_avg:98.56ms
step:1406/1770 train_time:137600ms step_avg:98.57ms
step:1407/1770 train_time:137703ms step_avg:98.57ms
step:1408/1770 train_time:137805ms step_avg:98.57ms
step:1409/1770 train_time:137907ms step_avg:98.58ms
step:1410/1770 train_time:138009ms step_avg:98.58ms
step:1411/1770 train_time:138111ms step_avg:98.58ms
step:1412/1770 train_time:138213ms step_avg:98.58ms
step:1413/1770 train_time:138316ms step_avg:98.59ms
step:1414/1770 train_time:138419ms step_avg:98.59ms
step:1415/1770 train_time:138522ms step_avg:98.59ms
step:1416/1770 train_time:138625ms step_avg:98.60ms
step:1417/1770 train_time:138728ms step_avg:98.60ms
step:1418/1770 train_time:138830ms step_avg:98.60ms
step:1419/1770 train_time:138933ms step_avg:98.60ms
step:1420/1770 train_time:139035ms step_avg:98.61ms
step:1421/1770 train_time:139137ms step_avg:98.61ms
step:1422/1770 train_time:139238ms step_avg:98.61ms
step:1423/1770 train_time:139342ms step_avg:98.61ms
step:1424/1770 train_time:139445ms step_avg:98.62ms
step:1425/1770 train_time:139547ms step_avg:98.62ms
step:1426/1770 train_time:139650ms step_avg:98.62ms
step:1427/1770 train_time:139751ms step_avg:98.62ms
step:1428/1770 train_time:139855ms step_avg:98.63ms
step:1429/1770 train_time:139958ms step_avg:98.63ms
step:1430/1770 train_time:140059ms step_avg:98.63ms
step:1431/1770 train_time:140163ms step_avg:98.64ms
step:1432/1770 train_time:140264ms step_avg:98.64ms
step:1433/1770 train_time:140366ms step_avg:98.64ms
step:1434/1770 train_time:140468ms step_avg:98.64ms
step:1435/1770 train_time:140571ms step_avg:98.65ms
step:1436/1770 train_time:140675ms step_avg:98.65ms
step:1437/1770 train_time:140777ms step_avg:98.65ms
step:1438/1770 train_time:140878ms step_avg:98.65ms
step:1439/1770 train_time:140981ms step_avg:98.66ms
step:1440/1770 train_time:141082ms step_avg:98.66ms
step:1441/1770 train_time:141187ms step_avg:98.66ms
step:1442/1770 train_time:141289ms step_avg:98.67ms
step:1443/1770 train_time:141392ms step_avg:98.67ms
step:1444/1770 train_time:141495ms step_avg:98.67ms
step:1445/1770 train_time:141598ms step_avg:98.67ms
step:1446/1770 train_time:141701ms step_avg:98.68ms
step:1447/1770 train_time:141804ms step_avg:98.68ms
step:1448/1770 train_time:141907ms step_avg:98.68ms
step:1449/1770 train_time:142012ms step_avg:98.69ms
step:1450/1770 train_time:142114ms step_avg:98.69ms
step:1451/1770 train_time:142218ms step_avg:98.69ms
step:1452/1770 train_time:142321ms step_avg:98.70ms
step:1453/1770 train_time:142423ms step_avg:98.70ms
step:1454/1770 train_time:142527ms step_avg:98.70ms
step:1455/1770 train_time:142632ms step_avg:98.71ms
step:1456/1770 train_time:142736ms step_avg:98.71ms
step:1457/1770 train_time:142841ms step_avg:98.72ms
step:1458/1770 train_time:142945ms step_avg:98.72ms
step:1459/1770 train_time:143049ms step_avg:98.72ms
step:1460/1770 train_time:143152ms step_avg:98.73ms
step:1461/1770 train_time:143255ms step_avg:98.73ms
step:1462/1770 train_time:143358ms step_avg:98.73ms
step:1463/1770 train_time:143462ms step_avg:98.73ms
step:1464/1770 train_time:143567ms step_avg:98.74ms
step:1465/1770 train_time:143671ms step_avg:98.74ms
step:1466/1770 train_time:143775ms step_avg:98.75ms
step:1467/1770 train_time:143879ms step_avg:98.75ms
step:1468/1770 train_time:143982ms step_avg:98.75ms
step:1469/1770 train_time:144085ms step_avg:98.76ms
step:1470/1770 train_time:144188ms step_avg:98.76ms
step:1471/1770 train_time:144291ms step_avg:98.76ms
step:1472/1770 train_time:144394ms step_avg:98.76ms
step:1473/1770 train_time:144499ms step_avg:98.77ms
step:1474/1770 train_time:144604ms step_avg:98.77ms
step:1475/1770 train_time:144707ms step_avg:98.78ms
step:1476/1770 train_time:144810ms step_avg:98.78ms
step:1477/1770 train_time:144917ms step_avg:98.78ms
step:1478/1770 train_time:145021ms step_avg:98.79ms
step:1479/1770 train_time:145123ms step_avg:98.79ms
step:1480/1770 train_time:145226ms step_avg:98.79ms
step:1481/1770 train_time:145333ms step_avg:98.80ms
step:1482/1770 train_time:145436ms step_avg:98.80ms
step:1483/1770 train_time:145539ms step_avg:98.80ms
step:1484/1770 train_time:145642ms step_avg:98.81ms
step:1485/1770 train_time:145744ms step_avg:98.81ms
step:1486/1770 train_time:145847ms step_avg:98.81ms
step:1487/1770 train_time:145950ms step_avg:98.82ms
step:1488/1770 train_time:146054ms step_avg:98.82ms
step:1489/1770 train_time:146159ms step_avg:98.82ms
step:1490/1770 train_time:146264ms step_avg:98.83ms
step:1491/1770 train_time:146366ms step_avg:98.83ms
step:1492/1770 train_time:146471ms step_avg:98.83ms
step:1493/1770 train_time:146577ms step_avg:98.84ms
step:1494/1770 train_time:146684ms step_avg:98.84ms
step:1495/1770 train_time:146786ms step_avg:98.85ms
step:1496/1770 train_time:146889ms step_avg:98.85ms
step:1497/1770 train_time:146993ms step_avg:98.85ms
step:1498/1770 train_time:147096ms step_avg:98.86ms
step:1499/1770 train_time:147200ms step_avg:98.86ms
step:1500/1770 train_time:147302ms step_avg:98.86ms
step:1500/1770 val_loss:3.3455 train_time:147403ms step_avg:98.93ms
step:1501/1770 train_time:147425ms step_avg:98.88ms
step:1502/1770 train_time:147519ms step_avg:98.87ms
step:1503/1770 train_time:147622ms step_avg:98.88ms
step:1504/1770 train_time:147726ms step_avg:98.88ms
step:1505/1770 train_time:147831ms step_avg:98.88ms
step:1506/1770 train_time:147934ms step_avg:98.89ms
step:1507/1770 train_time:148037ms step_avg:98.89ms
step:1508/1770 train_time:148142ms step_avg:98.89ms
step:1509/1770 train_time:148245ms step_avg:98.90ms
step:1510/1770 train_time:148347ms step_avg:98.90ms
step:1511/1770 train_time:148452ms step_avg:98.90ms
step:1512/1770 train_time:148556ms step_avg:98.91ms
step:1513/1770 train_time:148660ms step_avg:98.91ms
step:1514/1770 train_time:148764ms step_avg:98.91ms
step:1515/1770 train_time:148867ms step_avg:98.91ms
step:1516/1770 train_time:148971ms step_avg:98.92ms
step:1517/1770 train_time:149074ms step_avg:98.92ms
step:1518/1770 train_time:149179ms step_avg:98.92ms
step:1519/1770 train_time:149281ms step_avg:98.93ms
step:1520/1770 train_time:149385ms step_avg:98.93ms
step:1521/1770 train_time:149487ms step_avg:98.93ms
step:1522/1770 train_time:149591ms step_avg:98.94ms
step:1523/1770 train_time:149695ms step_avg:98.94ms
step:1524/1770 train_time:149799ms step_avg:98.94ms
step:1525/1770 train_time:149902ms step_avg:98.95ms
step:1526/1770 train_time:150005ms step_avg:98.95ms
step:1527/1770 train_time:150108ms step_avg:98.95ms
step:1528/1770 train_time:150213ms step_avg:98.95ms
step:1529/1770 train_time:150315ms step_avg:98.96ms
step:1530/1770 train_time:150419ms step_avg:98.96ms
step:1531/1770 train_time:150522ms step_avg:98.96ms
step:1532/1770 train_time:150626ms step_avg:98.97ms
step:1533/1770 train_time:150729ms step_avg:98.97ms
step:1534/1770 train_time:150832ms step_avg:98.97ms
step:1535/1770 train_time:150935ms step_avg:98.97ms
step:1536/1770 train_time:151039ms step_avg:98.98ms
step:1537/1770 train_time:151143ms step_avg:98.98ms
step:1538/1770 train_time:151248ms step_avg:98.98ms
step:1539/1770 train_time:151352ms step_avg:98.99ms
step:1540/1770 train_time:151457ms step_avg:98.99ms
step:1541/1770 train_time:151561ms step_avg:98.99ms
step:1542/1770 train_time:151665ms step_avg:99.00ms
step:1543/1770 train_time:151767ms step_avg:99.00ms
step:1544/1770 train_time:151873ms step_avg:99.00ms
step:1545/1770 train_time:151976ms step_avg:99.01ms
step:1546/1770 train_time:152079ms step_avg:99.01ms
step:1547/1770 train_time:152182ms step_avg:99.01ms
step:1548/1770 train_time:152286ms step_avg:99.02ms
step:1549/1770 train_time:152390ms step_avg:99.02ms
step:1550/1770 train_time:152493ms step_avg:99.02ms
step:1551/1770 train_time:152597ms step_avg:99.02ms
step:1552/1770 train_time:152702ms step_avg:99.03ms
step:1553/1770 train_time:152805ms step_avg:99.03ms
step:1554/1770 train_time:152908ms step_avg:99.03ms
step:1555/1770 train_time:153012ms step_avg:99.04ms
step:1556/1770 train_time:153114ms step_avg:99.04ms
step:1557/1770 train_time:153217ms step_avg:99.04ms
step:1558/1770 train_time:153321ms step_avg:99.04ms
step:1559/1770 train_time:153424ms step_avg:99.05ms
step:1560/1770 train_time:153526ms step_avg:99.05ms
step:1561/1770 train_time:153631ms step_avg:99.05ms
step:1562/1770 train_time:153734ms step_avg:99.06ms
step:1563/1770 train_time:153838ms step_avg:99.06ms
step:1564/1770 train_time:153940ms step_avg:99.06ms
step:1565/1770 train_time:154044ms step_avg:99.06ms
step:1566/1770 train_time:154148ms step_avg:99.07ms
step:1567/1770 train_time:154251ms step_avg:99.07ms
step:1568/1770 train_time:154354ms step_avg:99.07ms
step:1569/1770 train_time:154462ms step_avg:99.08ms
step:1570/1770 train_time:154565ms step_avg:99.08ms
step:1571/1770 train_time:154667ms step_avg:99.08ms
step:1572/1770 train_time:154772ms step_avg:99.09ms
step:1573/1770 train_time:154877ms step_avg:99.09ms
step:1574/1770 train_time:154980ms step_avg:99.09ms
step:1575/1770 train_time:155082ms step_avg:99.09ms
step:1576/1770 train_time:155185ms step_avg:99.10ms
step:1577/1770 train_time:155290ms step_avg:99.10ms
step:1578/1770 train_time:155395ms step_avg:99.10ms
step:1579/1770 train_time:155499ms step_avg:99.11ms
step:1580/1770 train_time:155602ms step_avg:99.11ms
step:1581/1770 train_time:155709ms step_avg:99.11ms
step:1582/1770 train_time:155814ms step_avg:99.12ms
step:1583/1770 train_time:155917ms step_avg:99.12ms
step:1584/1770 train_time:156021ms step_avg:99.12ms
step:1585/1770 train_time:156125ms step_avg:99.13ms
step:1586/1770 train_time:156233ms step_avg:99.13ms
step:1587/1770 train_time:156337ms step_avg:99.14ms
step:1588/1770 train_time:156440ms step_avg:99.14ms
step:1589/1770 train_time:156546ms step_avg:99.14ms
step:1590/1770 train_time:156649ms step_avg:99.14ms
step:1591/1770 train_time:156752ms step_avg:99.15ms
step:1592/1770 train_time:156856ms step_avg:99.15ms
step:1593/1770 train_time:156960ms step_avg:99.15ms
step:1594/1770 train_time:157063ms step_avg:99.16ms
step:1595/1770 train_time:157166ms step_avg:99.16ms
step:1596/1770 train_time:157270ms step_avg:99.16ms
step:1597/1770 train_time:157372ms step_avg:99.16ms
step:1598/1770 train_time:157475ms step_avg:99.17ms
step:1599/1770 train_time:157579ms step_avg:99.17ms
step:1600/1770 train_time:157685ms step_avg:99.17ms
step:1601/1770 train_time:157789ms step_avg:99.18ms
step:1602/1770 train_time:157894ms step_avg:99.18ms
step:1603/1770 train_time:157997ms step_avg:99.18ms
step:1604/1770 train_time:158099ms step_avg:99.18ms
step:1605/1770 train_time:158203ms step_avg:99.19ms
step:1606/1770 train_time:158306ms step_avg:99.19ms
step:1607/1770 train_time:158413ms step_avg:99.19ms
step:1608/1770 train_time:158516ms step_avg:99.20ms
step:1609/1770 train_time:158619ms step_avg:99.20ms
step:1610/1770 train_time:158726ms step_avg:99.20ms
step:1611/1770 train_time:158831ms step_avg:99.21ms
step:1612/1770 train_time:158936ms step_avg:99.21ms
step:1613/1770 train_time:159039ms step_avg:99.21ms
step:1614/1770 train_time:159142ms step_avg:99.22ms
step:1615/1770 train_time:159246ms step_avg:99.22ms
step:1616/1770 train_time:159350ms step_avg:99.22ms
step:1617/1770 train_time:159455ms step_avg:99.23ms
step:1618/1770 train_time:159560ms step_avg:99.23ms
step:1619/1770 train_time:159664ms step_avg:99.23ms
step:1620/1770 train_time:159768ms step_avg:99.23ms
step:1621/1770 train_time:159872ms step_avg:99.24ms
step:1622/1770 train_time:159976ms step_avg:99.24ms
step:1623/1770 train_time:160082ms step_avg:99.25ms
step:1624/1770 train_time:160185ms step_avg:99.25ms
step:1625/1770 train_time:160287ms step_avg:99.25ms
step:1625/1770 val_loss:3.3113 train_time:160389ms step_avg:99.31ms
step:1626/1770 train_time:160411ms step_avg:99.26ms
step:1627/1770 train_time:160500ms step_avg:99.26ms
step:1628/1770 train_time:160602ms step_avg:99.26ms
step:1629/1770 train_time:160706ms step_avg:99.26ms
step:1630/1770 train_time:160809ms step_avg:99.26ms
step:1631/1770 train_time:160912ms step_avg:99.27ms
step:1632/1770 train_time:161014ms step_avg:99.27ms
step:1633/1770 train_time:161118ms step_avg:99.27ms
step:1634/1770 train_time:161220ms step_avg:99.27ms
step:1635/1770 train_time:161323ms step_avg:99.28ms
step:1636/1770 train_time:161428ms step_avg:99.28ms
step:1637/1770 train_time:161533ms step_avg:99.28ms
step:1638/1770 train_time:161636ms step_avg:99.29ms
step:1639/1770 train_time:161739ms step_avg:99.29ms
step:1640/1770 train_time:161844ms step_avg:99.29ms
step:1641/1770 train_time:161948ms step_avg:99.29ms
step:1642/1770 train_time:162050ms step_avg:99.30ms
step:1643/1770 train_time:162153ms step_avg:99.30ms
step:1644/1770 train_time:162257ms step_avg:99.30ms
step:1645/1770 train_time:162360ms step_avg:99.30ms
step:1646/1770 train_time:162465ms step_avg:99.31ms
step:1647/1770 train_time:162570ms step_avg:99.31ms
step:1648/1770 train_time:162674ms step_avg:99.31ms
step:1649/1770 train_time:162777ms step_avg:99.31ms
step:1650/1770 train_time:162880ms step_avg:99.32ms
step:1651/1770 train_time:162983ms step_avg:99.32ms
step:1652/1770 train_time:163087ms step_avg:99.32ms
step:1653/1770 train_time:163191ms step_avg:99.33ms
step:1654/1770 train_time:163298ms step_avg:99.33ms
step:1655/1770 train_time:163404ms step_avg:99.33ms
step:1656/1770 train_time:163508ms step_avg:99.34ms
step:1657/1770 train_time:163614ms step_avg:99.34ms
step:1658/1770 train_time:163717ms step_avg:99.34ms
step:1659/1770 train_time:163822ms step_avg:99.35ms
step:1660/1770 train_time:163925ms step_avg:99.35ms
step:1661/1770 train_time:164030ms step_avg:99.35ms
step:1662/1770 train_time:164134ms step_avg:99.35ms
step:1663/1770 train_time:164237ms step_avg:99.36ms
step:1664/1770 train_time:164339ms step_avg:99.36ms
step:1665/1770 train_time:164442ms step_avg:99.36ms
step:1666/1770 train_time:164547ms step_avg:99.36ms
step:1667/1770 train_time:164650ms step_avg:99.37ms
step:1668/1770 train_time:164753ms step_avg:99.37ms
step:1669/1770 train_time:164855ms step_avg:99.37ms
step:1670/1770 train_time:164958ms step_avg:99.37ms
step:1671/1770 train_time:165062ms step_avg:99.38ms
step:1672/1770 train_time:165166ms step_avg:99.38ms
step:1673/1770 train_time:165271ms step_avg:99.38ms
step:1674/1770 train_time:165375ms step_avg:99.38ms
step:1675/1770 train_time:165477ms step_avg:99.39ms
step:1676/1770 train_time:165582ms step_avg:99.39ms
step:1677/1770 train_time:165689ms step_avg:99.39ms
step:1678/1770 train_time:165792ms step_avg:99.40ms
step:1679/1770 train_time:165895ms step_avg:99.40ms
step:1680/1770 train_time:165998ms step_avg:99.40ms
step:1681/1770 train_time:166102ms step_avg:99.40ms
step:1682/1770 train_time:166208ms step_avg:99.41ms
step:1683/1770 train_time:166311ms step_avg:99.41ms
step:1684/1770 train_time:166414ms step_avg:99.41ms
step:1685/1770 train_time:166517ms step_avg:99.41ms
step:1686/1770 train_time:166621ms step_avg:99.42ms
step:1687/1770 train_time:166727ms step_avg:99.42ms
step:1688/1770 train_time:166830ms step_avg:99.42ms
step:1689/1770 train_time:166933ms step_avg:99.42ms
step:1690/1770 train_time:167036ms step_avg:99.43ms
step:1691/1770 train_time:167139ms step_avg:99.43ms
step:1692/1770 train_time:167242ms step_avg:99.43ms
step:1693/1770 train_time:167348ms step_avg:99.43ms
step:1694/1770 train_time:167451ms step_avg:99.44ms
step:1695/1770 train_time:167554ms step_avg:99.44ms
step:1696/1770 train_time:167660ms step_avg:99.44ms
step:1697/1770 train_time:167765ms step_avg:99.45ms
step:1698/1770 train_time:167869ms step_avg:99.45ms
step:1699/1770 train_time:167973ms step_avg:99.45ms
step:1700/1770 train_time:168076ms step_avg:99.45ms
step:1701/1770 train_time:168179ms step_avg:99.46ms
step:1702/1770 train_time:168282ms step_avg:99.46ms
step:1703/1770 train_time:168385ms step_avg:99.46ms
step:1704/1770 train_time:168489ms step_avg:99.46ms
step:1705/1770 train_time:168592ms step_avg:99.46ms
step:1706/1770 train_time:168694ms step_avg:99.47ms
step:1707/1770 train_time:168798ms step_avg:99.47ms
step:1708/1770 train_time:168903ms step_avg:99.47ms
step:1709/1770 train_time:169007ms step_avg:99.47ms
step:1710/1770 train_time:169114ms step_avg:99.48ms
step:1711/1770 train_time:169220ms step_avg:99.48ms
step:1712/1770 train_time:169324ms step_avg:99.49ms
step:1713/1770 train_time:169427ms step_avg:99.49ms
step:1714/1770 train_time:169531ms step_avg:99.49ms
step:1715/1770 train_time:169634ms step_avg:99.49ms
step:1716/1770 train_time:169739ms step_avg:99.50ms
step:1717/1770 train_time:169842ms step_avg:99.50ms
step:1718/1770 train_time:169948ms step_avg:99.50ms
step:1719/1770 train_time:170053ms step_avg:99.50ms
step:1720/1770 train_time:170158ms step_avg:99.51ms
step:1721/1770 train_time:170262ms step_avg:99.51ms
step:1722/1770 train_time:170369ms step_avg:99.51ms
step:1723/1770 train_time:170476ms step_avg:99.52ms
step:1724/1770 train_time:170582ms step_avg:99.52ms
step:1725/1770 train_time:170688ms step_avg:99.53ms
step:1726/1770 train_time:170794ms step_avg:99.53ms
step:1727/1770 train_time:170898ms step_avg:99.53ms
step:1728/1770 train_time:171004ms step_avg:99.54ms
step:1729/1770 train_time:171108ms step_avg:99.54ms
step:1730/1770 train_time:171213ms step_avg:99.54ms
step:1731/1770 train_time:171319ms step_avg:99.55ms
step:1732/1770 train_time:171422ms step_avg:99.55ms
step:1733/1770 train_time:171528ms step_avg:99.55ms
step:1734/1770 train_time:171632ms step_avg:99.55ms
step:1735/1770 train_time:171737ms step_avg:99.56ms
step:1736/1770 train_time:171840ms step_avg:99.56ms
step:1737/1770 train_time:171945ms step_avg:99.56ms
step:1738/1770 train_time:172049ms step_avg:99.57ms
step:1739/1770 train_time:172153ms step_avg:99.57ms
step:1740/1770 train_time:172257ms step_avg:99.57ms
step:1741/1770 train_time:172365ms step_avg:99.58ms
step:1742/1770 train_time:172471ms step_avg:99.58ms
step:1743/1770 train_time:172577ms step_avg:99.58ms
step:1744/1770 train_time:172681ms step_avg:99.59ms
step:1745/1770 train_time:172785ms step_avg:99.59ms
step:1746/1770 train_time:172892ms step_avg:99.59ms
step:1747/1770 train_time:172995ms step_avg:99.59ms
step:1748/1770 train_time:173102ms step_avg:99.60ms
step:1749/1770 train_time:173207ms step_avg:99.60ms
step:1750/1770 train_time:173311ms step_avg:99.60ms
step:1750/1770 val_loss:3.2844 train_time:173414ms step_avg:99.66ms
step:1751/1770 train_time:173436ms step_avg:99.62ms
step:1752/1770 train_time:173528ms step_avg:99.61ms
step:1753/1770 train_time:173632ms step_avg:99.62ms
step:1754/1770 train_time:173737ms step_avg:99.62ms
step:1755/1770 train_time:173841ms step_avg:99.62ms
step:1756/1770 train_time:173946ms step_avg:99.63ms
step:1757/1770 train_time:174050ms step_avg:99.63ms
step:1758/1770 train_time:174154ms step_avg:99.63ms
step:1759/1770 train_time:174259ms step_avg:99.63ms
step:1760/1770 train_time:174363ms step_avg:99.64ms
step:1761/1770 train_time:174470ms step_avg:99.64ms
step:1762/1770 train_time:174578ms step_avg:99.64ms
step:1763/1770 train_time:174681ms step_avg:99.65ms
step:1764/1770 train_time:174786ms step_avg:99.65ms
step:1765/1770 train_time:174890ms step_avg:99.65ms
step:1766/1770 train_time:174998ms step_avg:99.66ms
step:1767/1770 train_time:175101ms step_avg:99.66ms
step:1768/1770 train_time:175205ms step_avg:99.66ms
step:1769/1770 train_time:175308ms step_avg:99.66ms
step:1770/1770 train_time:175412ms step_avg:99.67ms
step:1770/1770 val_loss:3.2814 train_time:175517ms step_avg:99.73ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
