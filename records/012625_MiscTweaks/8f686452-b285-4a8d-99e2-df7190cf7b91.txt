import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:50:37 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23238ms step_avg:nanms
step:2/1770 train_time:23666ms step_avg:nanms
step:3/1770 train_time:23763ms step_avg:nanms
step:4/1770 train_time:23855ms step_avg:nanms
step:5/1770 train_time:23948ms step_avg:nanms
step:6/1770 train_time:24042ms step_avg:nanms
step:7/1770 train_time:24136ms step_avg:nanms
step:8/1770 train_time:24230ms step_avg:nanms
step:9/1770 train_time:24323ms step_avg:nanms
step:10/1770 train_time:24417ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.15ms
step:14/1770 train_time:378ms step_avg:94.50ms
step:15/1770 train_time:472ms step_avg:94.41ms
step:16/1770 train_time:566ms step_avg:94.29ms
step:17/1770 train_time:660ms step_avg:94.31ms
step:18/1770 train_time:754ms step_avg:94.29ms
step:19/1770 train_time:848ms step_avg:94.22ms
step:20/1770 train_time:942ms step_avg:94.18ms
step:21/1770 train_time:1036ms step_avg:94.20ms
step:22/1770 train_time:1130ms step_avg:94.19ms
step:23/1770 train_time:1225ms step_avg:94.20ms
step:24/1770 train_time:1319ms step_avg:94.20ms
step:25/1770 train_time:1413ms step_avg:94.20ms
step:26/1770 train_time:1507ms step_avg:94.19ms
step:27/1770 train_time:1601ms step_avg:94.20ms
step:28/1770 train_time:1696ms step_avg:94.21ms
step:29/1770 train_time:1790ms step_avg:94.19ms
step:30/1770 train_time:1884ms step_avg:94.21ms
step:31/1770 train_time:1978ms step_avg:94.17ms
step:32/1770 train_time:2072ms step_avg:94.17ms
step:33/1770 train_time:2165ms step_avg:94.15ms
step:34/1770 train_time:2260ms step_avg:94.15ms
step:35/1770 train_time:2353ms step_avg:94.14ms
step:36/1770 train_time:2447ms step_avg:94.13ms
step:37/1770 train_time:2542ms step_avg:94.14ms
step:38/1770 train_time:2636ms step_avg:94.15ms
step:39/1770 train_time:2730ms step_avg:94.15ms
step:40/1770 train_time:2824ms step_avg:94.14ms
step:41/1770 train_time:2919ms step_avg:94.15ms
step:42/1770 train_time:3012ms step_avg:94.14ms
step:43/1770 train_time:3106ms step_avg:94.12ms
step:44/1770 train_time:3200ms step_avg:94.13ms
step:45/1770 train_time:3295ms step_avg:94.13ms
step:46/1770 train_time:3389ms step_avg:94.14ms
step:47/1770 train_time:3483ms step_avg:94.12ms
step:48/1770 train_time:3577ms step_avg:94.12ms
step:49/1770 train_time:3671ms step_avg:94.12ms
step:50/1770 train_time:3765ms step_avg:94.13ms
step:51/1770 train_time:3860ms step_avg:94.15ms
step:52/1770 train_time:3954ms step_avg:94.14ms
step:53/1770 train_time:4048ms step_avg:94.14ms
step:54/1770 train_time:4142ms step_avg:94.14ms
step:55/1770 train_time:4237ms step_avg:94.14ms
step:56/1770 train_time:4330ms step_avg:94.14ms
step:57/1770 train_time:4424ms step_avg:94.14ms
step:58/1770 train_time:4518ms step_avg:94.13ms
step:59/1770 train_time:4613ms step_avg:94.14ms
step:60/1770 train_time:4706ms step_avg:94.13ms
step:61/1770 train_time:4801ms step_avg:94.14ms
step:62/1770 train_time:4895ms step_avg:94.14ms
step:63/1770 train_time:4989ms step_avg:94.13ms
step:64/1770 train_time:5083ms step_avg:94.13ms
step:65/1770 train_time:5177ms step_avg:94.13ms
step:66/1770 train_time:5272ms step_avg:94.14ms
step:67/1770 train_time:5366ms step_avg:94.13ms
step:68/1770 train_time:5459ms step_avg:94.12ms
step:69/1770 train_time:5553ms step_avg:94.13ms
step:70/1770 train_time:5647ms step_avg:94.12ms
step:71/1770 train_time:5742ms step_avg:94.12ms
step:72/1770 train_time:5836ms step_avg:94.13ms
step:73/1770 train_time:5930ms step_avg:94.13ms
step:74/1770 train_time:6024ms step_avg:94.13ms
step:75/1770 train_time:6119ms step_avg:94.13ms
step:76/1770 train_time:6213ms step_avg:94.13ms
step:77/1770 train_time:6307ms step_avg:94.13ms
step:78/1770 train_time:6401ms step_avg:94.14ms
step:79/1770 train_time:6495ms step_avg:94.13ms
step:80/1770 train_time:6589ms step_avg:94.12ms
step:81/1770 train_time:6682ms step_avg:94.12ms
step:82/1770 train_time:6776ms step_avg:94.12ms
step:83/1770 train_time:6871ms step_avg:94.12ms
step:84/1770 train_time:6965ms step_avg:94.12ms
step:85/1770 train_time:7059ms step_avg:94.12ms
step:86/1770 train_time:7153ms step_avg:94.12ms
step:87/1770 train_time:7247ms step_avg:94.12ms
step:88/1770 train_time:7342ms step_avg:94.12ms
step:89/1770 train_time:7436ms step_avg:94.13ms
step:90/1770 train_time:7530ms step_avg:94.12ms
step:91/1770 train_time:7624ms step_avg:94.12ms
step:92/1770 train_time:7718ms step_avg:94.13ms
step:93/1770 train_time:7812ms step_avg:94.12ms
step:94/1770 train_time:7907ms step_avg:94.13ms
step:95/1770 train_time:8001ms step_avg:94.13ms
step:96/1770 train_time:8096ms step_avg:94.13ms
step:97/1770 train_time:8189ms step_avg:94.13ms
step:98/1770 train_time:8283ms step_avg:94.13ms
step:99/1770 train_time:8378ms step_avg:94.13ms
step:100/1770 train_time:8472ms step_avg:94.13ms
step:101/1770 train_time:8566ms step_avg:94.13ms
step:102/1770 train_time:8660ms step_avg:94.13ms
step:103/1770 train_time:8754ms step_avg:94.13ms
step:104/1770 train_time:8848ms step_avg:94.13ms
step:105/1770 train_time:8942ms step_avg:94.13ms
step:106/1770 train_time:9037ms step_avg:94.13ms
step:107/1770 train_time:9131ms step_avg:94.13ms
step:108/1770 train_time:9225ms step_avg:94.13ms
step:109/1770 train_time:9319ms step_avg:94.13ms
step:110/1770 train_time:9413ms step_avg:94.13ms
step:111/1770 train_time:9507ms step_avg:94.13ms
step:112/1770 train_time:9602ms step_avg:94.13ms
step:113/1770 train_time:9695ms step_avg:94.13ms
step:114/1770 train_time:9789ms step_avg:94.12ms
step:115/1770 train_time:9883ms step_avg:94.12ms
step:116/1770 train_time:9977ms step_avg:94.13ms
step:117/1770 train_time:10071ms step_avg:94.13ms
step:118/1770 train_time:10166ms step_avg:94.13ms
step:119/1770 train_time:10260ms step_avg:94.13ms
step:120/1770 train_time:10354ms step_avg:94.12ms
step:121/1770 train_time:10447ms step_avg:94.12ms
step:122/1770 train_time:10542ms step_avg:94.13ms
step:123/1770 train_time:10637ms step_avg:94.13ms
step:124/1770 train_time:10731ms step_avg:94.13ms
step:125/1770 train_time:10825ms step_avg:94.13ms
step:125/1770 val_loss:4.6468 train_time:10917ms step_avg:94.93ms
step:126/1770 train_time:10943ms step_avg:94.34ms
step:127/1770 train_time:11015ms step_avg:94.15ms
step:128/1770 train_time:11112ms step_avg:94.17ms
step:129/1770 train_time:11211ms step_avg:94.21ms
step:130/1770 train_time:11308ms step_avg:94.23ms
step:131/1770 train_time:11402ms step_avg:94.23ms
step:132/1770 train_time:11495ms step_avg:94.23ms
step:133/1770 train_time:11589ms step_avg:94.22ms
step:134/1770 train_time:11683ms step_avg:94.22ms
step:135/1770 train_time:11778ms step_avg:94.22ms
step:136/1770 train_time:11873ms step_avg:94.23ms
step:137/1770 train_time:11967ms step_avg:94.23ms
step:138/1770 train_time:12061ms step_avg:94.23ms
step:139/1770 train_time:12156ms step_avg:94.23ms
step:140/1770 train_time:12252ms step_avg:94.24ms
step:141/1770 train_time:12347ms step_avg:94.25ms
step:142/1770 train_time:12443ms step_avg:94.26ms
step:143/1770 train_time:12536ms step_avg:94.25ms
step:144/1770 train_time:12631ms step_avg:94.26ms
step:145/1770 train_time:12726ms step_avg:94.26ms
step:146/1770 train_time:12820ms step_avg:94.26ms
step:147/1770 train_time:12914ms step_avg:94.26ms
step:148/1770 train_time:13008ms step_avg:94.26ms
step:149/1770 train_time:13102ms step_avg:94.26ms
step:150/1770 train_time:13197ms step_avg:94.26ms
step:151/1770 train_time:13291ms step_avg:94.27ms
step:152/1770 train_time:13386ms step_avg:94.27ms
step:153/1770 train_time:13481ms step_avg:94.28ms
step:154/1770 train_time:13576ms step_avg:94.28ms
step:155/1770 train_time:13671ms step_avg:94.28ms
step:156/1770 train_time:13765ms step_avg:94.28ms
step:157/1770 train_time:13859ms step_avg:94.28ms
step:158/1770 train_time:13954ms step_avg:94.28ms
step:159/1770 train_time:14049ms step_avg:94.29ms
step:160/1770 train_time:14143ms step_avg:94.29ms
step:161/1770 train_time:14238ms step_avg:94.29ms
step:162/1770 train_time:14333ms step_avg:94.30ms
step:163/1770 train_time:14428ms step_avg:94.30ms
step:164/1770 train_time:14523ms step_avg:94.31ms
step:165/1770 train_time:14618ms step_avg:94.31ms
step:166/1770 train_time:14713ms step_avg:94.32ms
step:167/1770 train_time:14809ms step_avg:94.32ms
step:168/1770 train_time:14903ms step_avg:94.32ms
step:169/1770 train_time:14998ms step_avg:94.33ms
step:170/1770 train_time:15093ms step_avg:94.33ms
step:171/1770 train_time:15188ms step_avg:94.34ms
step:172/1770 train_time:15283ms step_avg:94.34ms
step:173/1770 train_time:15378ms step_avg:94.34ms
step:174/1770 train_time:15473ms step_avg:94.35ms
step:175/1770 train_time:15567ms step_avg:94.35ms
step:176/1770 train_time:15662ms step_avg:94.35ms
step:177/1770 train_time:15757ms step_avg:94.36ms
step:178/1770 train_time:15852ms step_avg:94.36ms
step:179/1770 train_time:15947ms step_avg:94.36ms
step:180/1770 train_time:16042ms step_avg:94.36ms
step:181/1770 train_time:16137ms step_avg:94.37ms
step:182/1770 train_time:16232ms step_avg:94.37ms
step:183/1770 train_time:16326ms step_avg:94.37ms
step:184/1770 train_time:16421ms step_avg:94.37ms
step:185/1770 train_time:16516ms step_avg:94.37ms
step:186/1770 train_time:16610ms step_avg:94.37ms
step:187/1770 train_time:16705ms step_avg:94.38ms
step:188/1770 train_time:16800ms step_avg:94.38ms
step:189/1770 train_time:16894ms step_avg:94.38ms
step:190/1770 train_time:16989ms step_avg:94.38ms
step:191/1770 train_time:17083ms step_avg:94.38ms
step:192/1770 train_time:17177ms step_avg:94.38ms
step:193/1770 train_time:17272ms step_avg:94.38ms
step:194/1770 train_time:17367ms step_avg:94.39ms
step:195/1770 train_time:17462ms step_avg:94.39ms
step:196/1770 train_time:17556ms step_avg:94.39ms
step:197/1770 train_time:17650ms step_avg:94.39ms
step:198/1770 train_time:17745ms step_avg:94.39ms
step:199/1770 train_time:17840ms step_avg:94.39ms
step:200/1770 train_time:17934ms step_avg:94.39ms
step:201/1770 train_time:18029ms step_avg:94.39ms
step:202/1770 train_time:18123ms step_avg:94.39ms
step:203/1770 train_time:18217ms step_avg:94.39ms
step:204/1770 train_time:18312ms step_avg:94.39ms
step:205/1770 train_time:18407ms step_avg:94.39ms
step:206/1770 train_time:18501ms step_avg:94.39ms
step:207/1770 train_time:18596ms step_avg:94.40ms
step:208/1770 train_time:18691ms step_avg:94.40ms
step:209/1770 train_time:18786ms step_avg:94.40ms
step:210/1770 train_time:18881ms step_avg:94.40ms
step:211/1770 train_time:18976ms step_avg:94.41ms
step:212/1770 train_time:19071ms step_avg:94.41ms
step:213/1770 train_time:19165ms step_avg:94.41ms
step:214/1770 train_time:19260ms step_avg:94.41ms
step:215/1770 train_time:19355ms step_avg:94.41ms
step:216/1770 train_time:19450ms step_avg:94.42ms
step:217/1770 train_time:19544ms step_avg:94.42ms
step:218/1770 train_time:19639ms step_avg:94.42ms
step:219/1770 train_time:19734ms step_avg:94.42ms
step:220/1770 train_time:19829ms step_avg:94.43ms
step:221/1770 train_time:19924ms step_avg:94.43ms
step:222/1770 train_time:20018ms step_avg:94.43ms
step:223/1770 train_time:20113ms step_avg:94.43ms
step:224/1770 train_time:20208ms step_avg:94.43ms
step:225/1770 train_time:20303ms step_avg:94.43ms
step:226/1770 train_time:20397ms step_avg:94.43ms
step:227/1770 train_time:20492ms step_avg:94.43ms
step:228/1770 train_time:20586ms step_avg:94.43ms
step:229/1770 train_time:20681ms step_avg:94.43ms
step:230/1770 train_time:20776ms step_avg:94.43ms
step:231/1770 train_time:20870ms step_avg:94.44ms
step:232/1770 train_time:20965ms step_avg:94.44ms
step:233/1770 train_time:21059ms step_avg:94.44ms
step:234/1770 train_time:21154ms step_avg:94.44ms
step:235/1770 train_time:21249ms step_avg:94.44ms
step:236/1770 train_time:21344ms step_avg:94.44ms
step:237/1770 train_time:21438ms step_avg:94.44ms
step:238/1770 train_time:21533ms step_avg:94.44ms
step:239/1770 train_time:21628ms step_avg:94.45ms
step:240/1770 train_time:21722ms step_avg:94.45ms
step:241/1770 train_time:21818ms step_avg:94.45ms
step:242/1770 train_time:21913ms step_avg:94.45ms
step:243/1770 train_time:22008ms step_avg:94.45ms
step:244/1770 train_time:22102ms step_avg:94.45ms
step:245/1770 train_time:22196ms step_avg:94.45ms
step:246/1770 train_time:22291ms step_avg:94.45ms
step:247/1770 train_time:22386ms step_avg:94.45ms
step:248/1770 train_time:22480ms step_avg:94.45ms
step:249/1770 train_time:22575ms step_avg:94.45ms
step:250/1770 train_time:22670ms step_avg:94.46ms
step:250/1770 val_loss:4.1102 train_time:22763ms step_avg:94.84ms
step:251/1770 train_time:22787ms step_avg:94.55ms
step:252/1770 train_time:22866ms step_avg:94.49ms
step:253/1770 train_time:22964ms step_avg:94.50ms
step:254/1770 train_time:23058ms step_avg:94.50ms
step:255/1770 train_time:23153ms step_avg:94.50ms
step:256/1770 train_time:23247ms step_avg:94.50ms
step:257/1770 train_time:23342ms step_avg:94.50ms
step:258/1770 train_time:23435ms step_avg:94.50ms
step:259/1770 train_time:23529ms step_avg:94.50ms
step:260/1770 train_time:23624ms step_avg:94.49ms
step:261/1770 train_time:23718ms step_avg:94.49ms
step:262/1770 train_time:23814ms step_avg:94.50ms
step:263/1770 train_time:23909ms step_avg:94.50ms
step:264/1770 train_time:24004ms step_avg:94.51ms
step:265/1770 train_time:24099ms step_avg:94.51ms
step:266/1770 train_time:24195ms step_avg:94.51ms
step:267/1770 train_time:24289ms step_avg:94.51ms
step:268/1770 train_time:24384ms step_avg:94.51ms
step:269/1770 train_time:24479ms step_avg:94.51ms
step:270/1770 train_time:24574ms step_avg:94.52ms
step:271/1770 train_time:24669ms step_avg:94.52ms
step:272/1770 train_time:24765ms step_avg:94.52ms
step:273/1770 train_time:24859ms step_avg:94.52ms
step:274/1770 train_time:24955ms step_avg:94.53ms
step:275/1770 train_time:25050ms step_avg:94.53ms
step:276/1770 train_time:25145ms step_avg:94.53ms
step:277/1770 train_time:25240ms step_avg:94.53ms
step:278/1770 train_time:25335ms step_avg:94.53ms
step:279/1770 train_time:25430ms step_avg:94.53ms
step:280/1770 train_time:25524ms step_avg:94.53ms
step:281/1770 train_time:25619ms step_avg:94.54ms
step:282/1770 train_time:25714ms step_avg:94.54ms
step:283/1770 train_time:25809ms step_avg:94.54ms
step:284/1770 train_time:25904ms step_avg:94.54ms
step:285/1770 train_time:26000ms step_avg:94.55ms
step:286/1770 train_time:26095ms step_avg:94.55ms
step:287/1770 train_time:26190ms step_avg:94.55ms
step:288/1770 train_time:26285ms step_avg:94.55ms
step:289/1770 train_time:26380ms step_avg:94.55ms
step:290/1770 train_time:26475ms step_avg:94.55ms
step:291/1770 train_time:26570ms step_avg:94.56ms
step:292/1770 train_time:26665ms step_avg:94.56ms
step:293/1770 train_time:26760ms step_avg:94.56ms
step:294/1770 train_time:26855ms step_avg:94.56ms
step:295/1770 train_time:26950ms step_avg:94.56ms
step:296/1770 train_time:27045ms step_avg:94.56ms
step:297/1770 train_time:27140ms step_avg:94.57ms
step:298/1770 train_time:27236ms step_avg:94.57ms
step:299/1770 train_time:27332ms step_avg:94.57ms
step:300/1770 train_time:27426ms step_avg:94.57ms
step:301/1770 train_time:27522ms step_avg:94.58ms
step:302/1770 train_time:27616ms step_avg:94.58ms
step:303/1770 train_time:27712ms step_avg:94.58ms
step:304/1770 train_time:27807ms step_avg:94.58ms
step:305/1770 train_time:27902ms step_avg:94.58ms
step:306/1770 train_time:27997ms step_avg:94.58ms
step:307/1770 train_time:28092ms step_avg:94.59ms
step:308/1770 train_time:28187ms step_avg:94.59ms
step:309/1770 train_time:28283ms step_avg:94.59ms
step:310/1770 train_time:28378ms step_avg:94.59ms
step:311/1770 train_time:28473ms step_avg:94.59ms
step:312/1770 train_time:28568ms step_avg:94.60ms
step:313/1770 train_time:28663ms step_avg:94.60ms
step:314/1770 train_time:28758ms step_avg:94.60ms
step:315/1770 train_time:28853ms step_avg:94.60ms
step:316/1770 train_time:28948ms step_avg:94.60ms
step:317/1770 train_time:29043ms step_avg:94.60ms
step:318/1770 train_time:29138ms step_avg:94.60ms
step:319/1770 train_time:29233ms step_avg:94.60ms
step:320/1770 train_time:29328ms step_avg:94.61ms
step:321/1770 train_time:29423ms step_avg:94.61ms
step:322/1770 train_time:29518ms step_avg:94.61ms
step:323/1770 train_time:29613ms step_avg:94.61ms
step:324/1770 train_time:29708ms step_avg:94.61ms
step:325/1770 train_time:29802ms step_avg:94.61ms
step:326/1770 train_time:29897ms step_avg:94.61ms
step:327/1770 train_time:29992ms step_avg:94.61ms
step:328/1770 train_time:30088ms step_avg:94.62ms
step:329/1770 train_time:30183ms step_avg:94.62ms
step:330/1770 train_time:30277ms step_avg:94.62ms
step:331/1770 train_time:30372ms step_avg:94.62ms
step:332/1770 train_time:30468ms step_avg:94.62ms
step:333/1770 train_time:30563ms step_avg:94.62ms
step:334/1770 train_time:30658ms step_avg:94.62ms
step:335/1770 train_time:30753ms step_avg:94.62ms
step:336/1770 train_time:30848ms step_avg:94.63ms
step:337/1770 train_time:30942ms step_avg:94.62ms
step:338/1770 train_time:31037ms step_avg:94.62ms
step:339/1770 train_time:31132ms step_avg:94.63ms
step:340/1770 train_time:31228ms step_avg:94.63ms
step:341/1770 train_time:31323ms step_avg:94.63ms
step:342/1770 train_time:31418ms step_avg:94.63ms
step:343/1770 train_time:31513ms step_avg:94.63ms
step:344/1770 train_time:31608ms step_avg:94.63ms
step:345/1770 train_time:31703ms step_avg:94.64ms
step:346/1770 train_time:31798ms step_avg:94.64ms
step:347/1770 train_time:31893ms step_avg:94.64ms
step:348/1770 train_time:31988ms step_avg:94.64ms
step:349/1770 train_time:32082ms step_avg:94.64ms
step:350/1770 train_time:32177ms step_avg:94.64ms
step:351/1770 train_time:32272ms step_avg:94.64ms
step:352/1770 train_time:32368ms step_avg:94.64ms
step:353/1770 train_time:32463ms step_avg:94.64ms
step:354/1770 train_time:32557ms step_avg:94.64ms
step:355/1770 train_time:32653ms step_avg:94.65ms
step:356/1770 train_time:32748ms step_avg:94.65ms
step:357/1770 train_time:32843ms step_avg:94.65ms
step:358/1770 train_time:32938ms step_avg:94.65ms
step:359/1770 train_time:33032ms step_avg:94.65ms
step:360/1770 train_time:33128ms step_avg:94.65ms
step:361/1770 train_time:33222ms step_avg:94.65ms
step:362/1770 train_time:33317ms step_avg:94.65ms
step:363/1770 train_time:33412ms step_avg:94.65ms
step:364/1770 train_time:33508ms step_avg:94.65ms
step:365/1770 train_time:33603ms step_avg:94.66ms
step:366/1770 train_time:33698ms step_avg:94.66ms
step:367/1770 train_time:33793ms step_avg:94.66ms
step:368/1770 train_time:33889ms step_avg:94.66ms
step:369/1770 train_time:33984ms step_avg:94.66ms
step:370/1770 train_time:34079ms step_avg:94.66ms
step:371/1770 train_time:34174ms step_avg:94.67ms
step:372/1770 train_time:34269ms step_avg:94.67ms
step:373/1770 train_time:34364ms step_avg:94.67ms
step:374/1770 train_time:34459ms step_avg:94.67ms
step:375/1770 train_time:34555ms step_avg:94.67ms
step:375/1770 val_loss:3.9053 train_time:34648ms step_avg:94.93ms
step:376/1770 train_time:34671ms step_avg:94.73ms
step:377/1770 train_time:34751ms step_avg:94.69ms
step:378/1770 train_time:34849ms step_avg:94.70ms
step:379/1770 train_time:34945ms step_avg:94.70ms
step:380/1770 train_time:35039ms step_avg:94.70ms
step:381/1770 train_time:35134ms step_avg:94.70ms
step:382/1770 train_time:35228ms step_avg:94.70ms
step:383/1770 train_time:35324ms step_avg:94.70ms
step:384/1770 train_time:35419ms step_avg:94.70ms
step:385/1770 train_time:35514ms step_avg:94.70ms
step:386/1770 train_time:35609ms step_avg:94.70ms
step:387/1770 train_time:35705ms step_avg:94.71ms
step:388/1770 train_time:35802ms step_avg:94.71ms
step:389/1770 train_time:35898ms step_avg:94.72ms
step:390/1770 train_time:35994ms step_avg:94.72ms
step:391/1770 train_time:36089ms step_avg:94.72ms
step:392/1770 train_time:36184ms step_avg:94.72ms
step:393/1770 train_time:36278ms step_avg:94.72ms
step:394/1770 train_time:36373ms step_avg:94.72ms
step:395/1770 train_time:36467ms step_avg:94.72ms
step:396/1770 train_time:36564ms step_avg:94.72ms
step:397/1770 train_time:36661ms step_avg:94.73ms
step:398/1770 train_time:36758ms step_avg:94.74ms
step:399/1770 train_time:36855ms step_avg:94.74ms
step:400/1770 train_time:36952ms step_avg:94.75ms
step:401/1770 train_time:37049ms step_avg:94.75ms
step:402/1770 train_time:37146ms step_avg:94.76ms
step:403/1770 train_time:37243ms step_avg:94.76ms
step:404/1770 train_time:37339ms step_avg:94.77ms
step:405/1770 train_time:37436ms step_avg:94.77ms
step:406/1770 train_time:37532ms step_avg:94.78ms
step:407/1770 train_time:37629ms step_avg:94.78ms
step:408/1770 train_time:37726ms step_avg:94.79ms
step:409/1770 train_time:37823ms step_avg:94.79ms
step:410/1770 train_time:37920ms step_avg:94.80ms
step:411/1770 train_time:38017ms step_avg:94.81ms
step:412/1770 train_time:38115ms step_avg:94.81ms
step:413/1770 train_time:38212ms step_avg:94.82ms
step:414/1770 train_time:38308ms step_avg:94.82ms
step:415/1770 train_time:38405ms step_avg:94.83ms
step:416/1770 train_time:38502ms step_avg:94.83ms
step:417/1770 train_time:38599ms step_avg:94.84ms
step:418/1770 train_time:38697ms step_avg:94.85ms
step:419/1770 train_time:38794ms step_avg:94.85ms
step:420/1770 train_time:38891ms step_avg:94.86ms
step:421/1770 train_time:38988ms step_avg:94.86ms
step:422/1770 train_time:39084ms step_avg:94.86ms
step:423/1770 train_time:39180ms step_avg:94.87ms
step:424/1770 train_time:39278ms step_avg:94.87ms
step:425/1770 train_time:39375ms step_avg:94.88ms
step:426/1770 train_time:39472ms step_avg:94.88ms
step:427/1770 train_time:39568ms step_avg:94.89ms
step:428/1770 train_time:39665ms step_avg:94.89ms
step:429/1770 train_time:39763ms step_avg:94.90ms
step:430/1770 train_time:39860ms step_avg:94.90ms
step:431/1770 train_time:39957ms step_avg:94.91ms
step:432/1770 train_time:40055ms step_avg:94.92ms
step:433/1770 train_time:40152ms step_avg:94.92ms
step:434/1770 train_time:40249ms step_avg:94.93ms
step:435/1770 train_time:40345ms step_avg:94.93ms
step:436/1770 train_time:40443ms step_avg:94.94ms
step:437/1770 train_time:40540ms step_avg:94.94ms
step:438/1770 train_time:40637ms step_avg:94.95ms
step:439/1770 train_time:40735ms step_avg:94.95ms
step:440/1770 train_time:40831ms step_avg:94.96ms
step:441/1770 train_time:40928ms step_avg:94.96ms
step:442/1770 train_time:41025ms step_avg:94.97ms
step:443/1770 train_time:41122ms step_avg:94.97ms
step:444/1770 train_time:41219ms step_avg:94.98ms
step:445/1770 train_time:41317ms step_avg:94.98ms
step:446/1770 train_time:41414ms step_avg:94.99ms
step:447/1770 train_time:41511ms step_avg:94.99ms
step:448/1770 train_time:41608ms step_avg:95.00ms
step:449/1770 train_time:41704ms step_avg:95.00ms
step:450/1770 train_time:41801ms step_avg:95.00ms
step:451/1770 train_time:41898ms step_avg:95.01ms
step:452/1770 train_time:41996ms step_avg:95.01ms
step:453/1770 train_time:42092ms step_avg:95.02ms
step:454/1770 train_time:42189ms step_avg:95.02ms
step:455/1770 train_time:42286ms step_avg:95.02ms
step:456/1770 train_time:42382ms step_avg:95.03ms
step:457/1770 train_time:42479ms step_avg:95.03ms
step:458/1770 train_time:42578ms step_avg:95.04ms
step:459/1770 train_time:42675ms step_avg:95.04ms
step:460/1770 train_time:42772ms step_avg:95.05ms
step:461/1770 train_time:42868ms step_avg:95.05ms
step:462/1770 train_time:42965ms step_avg:95.06ms
step:463/1770 train_time:43062ms step_avg:95.06ms
step:464/1770 train_time:43160ms step_avg:95.07ms
step:465/1770 train_time:43258ms step_avg:95.07ms
step:466/1770 train_time:43355ms step_avg:95.08ms
step:467/1770 train_time:43452ms step_avg:95.08ms
step:468/1770 train_time:43549ms step_avg:95.09ms
step:469/1770 train_time:43646ms step_avg:95.09ms
step:470/1770 train_time:43742ms step_avg:95.09ms
step:471/1770 train_time:43839ms step_avg:95.10ms
step:472/1770 train_time:43937ms step_avg:95.10ms
step:473/1770 train_time:44033ms step_avg:95.10ms
step:474/1770 train_time:44130ms step_avg:95.11ms
step:475/1770 train_time:44227ms step_avg:95.11ms
step:476/1770 train_time:44324ms step_avg:95.12ms
step:477/1770 train_time:44421ms step_avg:95.12ms
step:478/1770 train_time:44519ms step_avg:95.13ms
step:479/1770 train_time:44616ms step_avg:95.13ms
step:480/1770 train_time:44713ms step_avg:95.14ms
step:481/1770 train_time:44811ms step_avg:95.14ms
step:482/1770 train_time:44907ms step_avg:95.14ms
step:483/1770 train_time:45004ms step_avg:95.15ms
step:484/1770 train_time:45101ms step_avg:95.15ms
step:485/1770 train_time:45198ms step_avg:95.15ms
step:486/1770 train_time:45296ms step_avg:95.16ms
step:487/1770 train_time:45393ms step_avg:95.16ms
step:488/1770 train_time:45490ms step_avg:95.17ms
step:489/1770 train_time:45587ms step_avg:95.17ms
step:490/1770 train_time:45684ms step_avg:95.18ms
step:491/1770 train_time:45781ms step_avg:95.18ms
step:492/1770 train_time:45879ms step_avg:95.18ms
step:493/1770 train_time:45976ms step_avg:95.19ms
step:494/1770 train_time:46072ms step_avg:95.19ms
step:495/1770 train_time:46169ms step_avg:95.19ms
step:496/1770 train_time:46266ms step_avg:95.20ms
step:497/1770 train_time:46363ms step_avg:95.20ms
step:498/1770 train_time:46460ms step_avg:95.20ms
step:499/1770 train_time:46558ms step_avg:95.21ms
step:500/1770 train_time:46655ms step_avg:95.21ms
step:500/1770 val_loss:3.7559 train_time:46750ms step_avg:95.41ms
step:501/1770 train_time:46772ms step_avg:95.26ms
step:502/1770 train_time:46853ms step_avg:95.23ms
step:503/1770 train_time:46952ms step_avg:95.24ms
step:504/1770 train_time:47049ms step_avg:95.24ms
step:505/1770 train_time:47146ms step_avg:95.24ms
step:506/1770 train_time:47242ms step_avg:95.25ms
step:507/1770 train_time:47339ms step_avg:95.25ms
step:508/1770 train_time:47435ms step_avg:95.25ms
step:509/1770 train_time:47532ms step_avg:95.26ms
step:510/1770 train_time:47630ms step_avg:95.26ms
step:511/1770 train_time:47728ms step_avg:95.27ms
step:512/1770 train_time:47825ms step_avg:95.27ms
step:513/1770 train_time:47922ms step_avg:95.27ms
step:514/1770 train_time:48020ms step_avg:95.28ms
step:515/1770 train_time:48117ms step_avg:95.28ms
step:516/1770 train_time:48214ms step_avg:95.28ms
step:517/1770 train_time:48311ms step_avg:95.29ms
step:518/1770 train_time:48408ms step_avg:95.29ms
step:519/1770 train_time:48505ms step_avg:95.29ms
step:520/1770 train_time:48602ms step_avg:95.30ms
step:521/1770 train_time:48699ms step_avg:95.30ms
step:522/1770 train_time:48796ms step_avg:95.31ms
step:523/1770 train_time:48894ms step_avg:95.31ms
step:524/1770 train_time:48992ms step_avg:95.32ms
step:525/1770 train_time:49090ms step_avg:95.32ms
step:526/1770 train_time:49187ms step_avg:95.32ms
step:527/1770 train_time:49284ms step_avg:95.33ms
step:528/1770 train_time:49382ms step_avg:95.33ms
step:529/1770 train_time:49479ms step_avg:95.34ms
step:530/1770 train_time:49577ms step_avg:95.34ms
step:531/1770 train_time:49674ms step_avg:95.34ms
step:532/1770 train_time:49772ms step_avg:95.35ms
step:533/1770 train_time:49870ms step_avg:95.35ms
step:534/1770 train_time:49968ms step_avg:95.36ms
step:535/1770 train_time:50065ms step_avg:95.36ms
step:536/1770 train_time:50161ms step_avg:95.36ms
step:537/1770 train_time:50259ms step_avg:95.37ms
step:538/1770 train_time:50357ms step_avg:95.37ms
step:539/1770 train_time:50454ms step_avg:95.38ms
step:540/1770 train_time:50552ms step_avg:95.38ms
step:541/1770 train_time:50649ms step_avg:95.38ms
step:542/1770 train_time:50746ms step_avg:95.39ms
step:543/1770 train_time:50844ms step_avg:95.39ms
step:544/1770 train_time:50941ms step_avg:95.40ms
step:545/1770 train_time:51040ms step_avg:95.40ms
step:546/1770 train_time:51138ms step_avg:95.41ms
step:547/1770 train_time:51235ms step_avg:95.41ms
step:548/1770 train_time:51333ms step_avg:95.41ms
step:549/1770 train_time:51431ms step_avg:95.42ms
step:550/1770 train_time:51529ms step_avg:95.42ms
step:551/1770 train_time:51626ms step_avg:95.43ms
step:552/1770 train_time:51723ms step_avg:95.43ms
step:553/1770 train_time:51820ms step_avg:95.43ms
step:554/1770 train_time:51918ms step_avg:95.44ms
step:555/1770 train_time:52016ms step_avg:95.44ms
step:556/1770 train_time:52114ms step_avg:95.45ms
step:557/1770 train_time:52212ms step_avg:95.45ms
step:558/1770 train_time:52310ms step_avg:95.46ms
step:559/1770 train_time:52407ms step_avg:95.46ms
step:560/1770 train_time:52504ms step_avg:95.46ms
step:561/1770 train_time:52601ms step_avg:95.46ms
step:562/1770 train_time:52698ms step_avg:95.47ms
step:563/1770 train_time:52796ms step_avg:95.47ms
step:564/1770 train_time:52893ms step_avg:95.48ms
step:565/1770 train_time:52991ms step_avg:95.48ms
step:566/1770 train_time:53089ms step_avg:95.48ms
step:567/1770 train_time:53187ms step_avg:95.49ms
step:568/1770 train_time:53284ms step_avg:95.49ms
step:569/1770 train_time:53381ms step_avg:95.49ms
step:570/1770 train_time:53479ms step_avg:95.50ms
step:571/1770 train_time:53578ms step_avg:95.50ms
step:572/1770 train_time:53676ms step_avg:95.51ms
step:573/1770 train_time:53774ms step_avg:95.51ms
step:574/1770 train_time:53871ms step_avg:95.52ms
step:575/1770 train_time:53969ms step_avg:95.52ms
step:576/1770 train_time:54066ms step_avg:95.52ms
step:577/1770 train_time:54163ms step_avg:95.53ms
step:578/1770 train_time:54261ms step_avg:95.53ms
step:579/1770 train_time:54358ms step_avg:95.53ms
step:580/1770 train_time:54455ms step_avg:95.54ms
step:581/1770 train_time:54553ms step_avg:95.54ms
step:582/1770 train_time:54651ms step_avg:95.54ms
step:583/1770 train_time:54748ms step_avg:95.55ms
step:584/1770 train_time:54846ms step_avg:95.55ms
step:585/1770 train_time:54943ms step_avg:95.55ms
step:586/1770 train_time:55040ms step_avg:95.56ms
step:587/1770 train_time:55138ms step_avg:95.56ms
step:588/1770 train_time:55236ms step_avg:95.56ms
step:589/1770 train_time:55333ms step_avg:95.57ms
step:590/1770 train_time:55431ms step_avg:95.57ms
step:591/1770 train_time:55529ms step_avg:95.57ms
step:592/1770 train_time:55626ms step_avg:95.58ms
step:593/1770 train_time:55724ms step_avg:95.58ms
step:594/1770 train_time:55821ms step_avg:95.58ms
step:595/1770 train_time:55918ms step_avg:95.59ms
step:596/1770 train_time:56016ms step_avg:95.59ms
step:597/1770 train_time:56114ms step_avg:95.59ms
step:598/1770 train_time:56211ms step_avg:95.60ms
step:599/1770 train_time:56308ms step_avg:95.60ms
step:600/1770 train_time:56405ms step_avg:95.60ms
step:601/1770 train_time:56502ms step_avg:95.60ms
step:602/1770 train_time:56600ms step_avg:95.61ms
step:603/1770 train_time:56698ms step_avg:95.61ms
step:604/1770 train_time:56795ms step_avg:95.61ms
step:605/1770 train_time:56893ms step_avg:95.62ms
step:606/1770 train_time:56991ms step_avg:95.62ms
step:607/1770 train_time:57089ms step_avg:95.63ms
step:608/1770 train_time:57186ms step_avg:95.63ms
step:609/1770 train_time:57283ms step_avg:95.63ms
step:610/1770 train_time:57381ms step_avg:95.63ms
step:611/1770 train_time:57478ms step_avg:95.64ms
step:612/1770 train_time:57576ms step_avg:95.64ms
step:613/1770 train_time:57673ms step_avg:95.64ms
step:614/1770 train_time:57771ms step_avg:95.65ms
step:615/1770 train_time:57869ms step_avg:95.65ms
step:616/1770 train_time:57965ms step_avg:95.65ms
step:617/1770 train_time:58063ms step_avg:95.66ms
step:618/1770 train_time:58160ms step_avg:95.66ms
step:619/1770 train_time:58257ms step_avg:95.66ms
step:620/1770 train_time:58355ms step_avg:95.66ms
step:621/1770 train_time:58453ms step_avg:95.67ms
step:622/1770 train_time:58551ms step_avg:95.67ms
step:623/1770 train_time:58649ms step_avg:95.68ms
step:624/1770 train_time:58746ms step_avg:95.68ms
step:625/1770 train_time:58843ms step_avg:95.68ms
step:625/1770 val_loss:3.6671 train_time:58938ms step_avg:95.83ms
step:626/1770 train_time:58961ms step_avg:95.72ms
step:627/1770 train_time:59045ms step_avg:95.70ms
step:628/1770 train_time:59142ms step_avg:95.70ms
step:629/1770 train_time:59239ms step_avg:95.70ms
step:630/1770 train_time:59336ms step_avg:95.70ms
step:631/1770 train_time:59434ms step_avg:95.71ms
step:632/1770 train_time:59530ms step_avg:95.71ms
step:633/1770 train_time:59627ms step_avg:95.71ms
step:634/1770 train_time:59725ms step_avg:95.71ms
step:635/1770 train_time:59822ms step_avg:95.72ms
step:636/1770 train_time:59920ms step_avg:95.72ms
step:637/1770 train_time:60019ms step_avg:95.72ms
step:638/1770 train_time:60116ms step_avg:95.73ms
step:639/1770 train_time:60215ms step_avg:95.73ms
step:640/1770 train_time:60311ms step_avg:95.73ms
step:641/1770 train_time:60408ms step_avg:95.73ms
step:642/1770 train_time:60506ms step_avg:95.74ms
step:643/1770 train_time:60603ms step_avg:95.74ms
step:644/1770 train_time:60700ms step_avg:95.74ms
step:645/1770 train_time:60797ms step_avg:95.74ms
step:646/1770 train_time:60894ms step_avg:95.74ms
step:647/1770 train_time:60991ms step_avg:95.75ms
step:648/1770 train_time:61089ms step_avg:95.75ms
step:649/1770 train_time:61187ms step_avg:95.75ms
step:650/1770 train_time:61285ms step_avg:95.76ms
step:651/1770 train_time:61383ms step_avg:95.76ms
step:652/1770 train_time:61480ms step_avg:95.76ms
step:653/1770 train_time:61577ms step_avg:95.76ms
step:654/1770 train_time:61674ms step_avg:95.77ms
step:655/1770 train_time:61771ms step_avg:95.77ms
step:656/1770 train_time:61868ms step_avg:95.77ms
step:657/1770 train_time:61967ms step_avg:95.78ms
step:658/1770 train_time:62067ms step_avg:95.78ms
step:659/1770 train_time:62167ms step_avg:95.79ms
step:660/1770 train_time:62266ms step_avg:95.79ms
step:661/1770 train_time:62365ms step_avg:95.80ms
step:662/1770 train_time:62465ms step_avg:95.81ms
step:663/1770 train_time:62565ms step_avg:95.81ms
step:664/1770 train_time:62665ms step_avg:95.82ms
step:665/1770 train_time:62765ms step_avg:95.82ms
step:666/1770 train_time:62864ms step_avg:95.83ms
step:667/1770 train_time:62964ms step_avg:95.84ms
step:668/1770 train_time:63064ms step_avg:95.84ms
step:669/1770 train_time:63164ms step_avg:95.85ms
step:670/1770 train_time:63263ms step_avg:95.85ms
step:671/1770 train_time:63362ms step_avg:95.86ms
step:672/1770 train_time:63461ms step_avg:95.86ms
step:673/1770 train_time:63561ms step_avg:95.87ms
step:674/1770 train_time:63660ms step_avg:95.87ms
step:675/1770 train_time:63759ms step_avg:95.88ms
step:676/1770 train_time:63858ms step_avg:95.88ms
step:677/1770 train_time:63957ms step_avg:95.89ms
step:678/1770 train_time:64056ms step_avg:95.89ms
step:679/1770 train_time:64154ms step_avg:95.90ms
step:680/1770 train_time:64253ms step_avg:95.90ms
step:681/1770 train_time:64352ms step_avg:95.90ms
step:682/1770 train_time:64451ms step_avg:95.91ms
step:683/1770 train_time:64550ms step_avg:95.91ms
step:684/1770 train_time:64649ms step_avg:95.92ms
step:685/1770 train_time:64749ms step_avg:95.92ms
step:686/1770 train_time:64849ms step_avg:95.93ms
step:687/1770 train_time:64948ms step_avg:95.94ms
step:688/1770 train_time:65048ms step_avg:95.94ms
step:689/1770 train_time:65148ms step_avg:95.95ms
step:690/1770 train_time:65248ms step_avg:95.95ms
step:691/1770 train_time:65348ms step_avg:95.96ms
step:692/1770 train_time:65447ms step_avg:95.96ms
step:693/1770 train_time:65547ms step_avg:95.97ms
step:694/1770 train_time:65646ms step_avg:95.97ms
step:695/1770 train_time:65746ms step_avg:95.98ms
step:696/1770 train_time:65845ms step_avg:95.98ms
step:697/1770 train_time:65945ms step_avg:95.99ms
step:698/1770 train_time:66044ms step_avg:95.99ms
step:699/1770 train_time:66144ms step_avg:96.00ms
step:700/1770 train_time:66244ms step_avg:96.01ms
step:701/1770 train_time:66344ms step_avg:96.01ms
step:702/1770 train_time:66444ms step_avg:96.02ms
step:703/1770 train_time:66544ms step_avg:96.02ms
step:704/1770 train_time:66643ms step_avg:96.03ms
step:705/1770 train_time:66742ms step_avg:96.03ms
step:706/1770 train_time:66842ms step_avg:96.04ms
step:707/1770 train_time:66941ms step_avg:96.04ms
step:708/1770 train_time:67039ms step_avg:96.04ms
step:709/1770 train_time:67138ms step_avg:96.05ms
step:710/1770 train_time:67237ms step_avg:96.05ms
step:711/1770 train_time:67335ms step_avg:96.06ms
step:712/1770 train_time:67435ms step_avg:96.06ms
step:713/1770 train_time:67534ms step_avg:96.07ms
step:714/1770 train_time:67633ms step_avg:96.07ms
step:715/1770 train_time:67732ms step_avg:96.07ms
step:716/1770 train_time:67831ms step_avg:96.08ms
step:717/1770 train_time:67931ms step_avg:96.08ms
step:718/1770 train_time:68030ms step_avg:96.09ms
step:719/1770 train_time:68129ms step_avg:96.09ms
step:720/1770 train_time:68228ms step_avg:96.10ms
step:721/1770 train_time:68328ms step_avg:96.10ms
step:722/1770 train_time:68428ms step_avg:96.11ms
step:723/1770 train_time:68527ms step_avg:96.11ms
step:724/1770 train_time:68628ms step_avg:96.12ms
step:725/1770 train_time:68728ms step_avg:96.12ms
step:726/1770 train_time:68828ms step_avg:96.13ms
step:727/1770 train_time:68928ms step_avg:96.13ms
step:728/1770 train_time:69028ms step_avg:96.14ms
step:729/1770 train_time:69128ms step_avg:96.14ms
step:730/1770 train_time:69228ms step_avg:96.15ms
step:731/1770 train_time:69327ms step_avg:96.15ms
step:732/1770 train_time:69427ms step_avg:96.16ms
step:733/1770 train_time:69527ms step_avg:96.16ms
step:734/1770 train_time:69626ms step_avg:96.17ms
step:735/1770 train_time:69726ms step_avg:96.17ms
step:736/1770 train_time:69826ms step_avg:96.18ms
step:737/1770 train_time:69925ms step_avg:96.18ms
step:738/1770 train_time:70024ms step_avg:96.19ms
step:739/1770 train_time:70124ms step_avg:96.19ms
step:740/1770 train_time:70222ms step_avg:96.20ms
step:741/1770 train_time:70322ms step_avg:96.20ms
step:742/1770 train_time:70421ms step_avg:96.20ms
step:743/1770 train_time:70520ms step_avg:96.21ms
step:744/1770 train_time:70620ms step_avg:96.21ms
step:745/1770 train_time:70718ms step_avg:96.22ms
step:746/1770 train_time:70818ms step_avg:96.22ms
step:747/1770 train_time:70918ms step_avg:96.22ms
step:748/1770 train_time:71016ms step_avg:96.23ms
step:749/1770 train_time:71115ms step_avg:96.23ms
step:750/1770 train_time:71214ms step_avg:96.24ms
step:750/1770 val_loss:3.6010 train_time:71311ms step_avg:96.37ms
step:751/1770 train_time:71335ms step_avg:96.27ms
step:752/1770 train_time:71417ms step_avg:96.25ms
step:753/1770 train_time:71517ms step_avg:96.25ms
step:754/1770 train_time:71616ms step_avg:96.26ms
step:755/1770 train_time:71714ms step_avg:96.26ms
step:756/1770 train_time:71813ms step_avg:96.26ms
step:757/1770 train_time:71912ms step_avg:96.27ms
step:758/1770 train_time:72010ms step_avg:96.27ms
step:759/1770 train_time:72108ms step_avg:96.27ms
step:760/1770 train_time:72206ms step_avg:96.28ms
step:761/1770 train_time:72305ms step_avg:96.28ms
step:762/1770 train_time:72405ms step_avg:96.28ms
step:763/1770 train_time:72505ms step_avg:96.29ms
step:764/1770 train_time:72605ms step_avg:96.29ms
step:765/1770 train_time:72705ms step_avg:96.30ms
step:766/1770 train_time:72805ms step_avg:96.30ms
step:767/1770 train_time:72905ms step_avg:96.31ms
step:768/1770 train_time:73005ms step_avg:96.31ms
step:769/1770 train_time:73104ms step_avg:96.32ms
step:770/1770 train_time:73203ms step_avg:96.32ms
step:771/1770 train_time:73302ms step_avg:96.32ms
step:772/1770 train_time:73401ms step_avg:96.33ms
step:773/1770 train_time:73502ms step_avg:96.33ms
step:774/1770 train_time:73601ms step_avg:96.34ms
step:775/1770 train_time:73701ms step_avg:96.34ms
step:776/1770 train_time:73802ms step_avg:96.35ms
step:777/1770 train_time:73901ms step_avg:96.35ms
step:778/1770 train_time:74001ms step_avg:96.36ms
step:779/1770 train_time:74100ms step_avg:96.36ms
step:780/1770 train_time:74200ms step_avg:96.36ms
step:781/1770 train_time:74299ms step_avg:96.37ms
step:782/1770 train_time:74398ms step_avg:96.37ms
step:783/1770 train_time:74497ms step_avg:96.37ms
step:784/1770 train_time:74595ms step_avg:96.38ms
step:785/1770 train_time:74694ms step_avg:96.38ms
step:786/1770 train_time:74792ms step_avg:96.38ms
step:787/1770 train_time:74892ms step_avg:96.39ms
step:788/1770 train_time:74990ms step_avg:96.39ms
step:789/1770 train_time:75089ms step_avg:96.39ms
step:790/1770 train_time:75188ms step_avg:96.39ms
step:791/1770 train_time:75288ms step_avg:96.40ms
step:792/1770 train_time:75387ms step_avg:96.40ms
step:793/1770 train_time:75487ms step_avg:96.41ms
step:794/1770 train_time:75587ms step_avg:96.41ms
step:795/1770 train_time:75687ms step_avg:96.42ms
step:796/1770 train_time:75788ms step_avg:96.42ms
step:797/1770 train_time:75887ms step_avg:96.43ms
step:798/1770 train_time:75986ms step_avg:96.43ms
step:799/1770 train_time:76086ms step_avg:96.43ms
step:800/1770 train_time:76185ms step_avg:96.44ms
step:801/1770 train_time:76284ms step_avg:96.44ms
step:802/1770 train_time:76383ms step_avg:96.44ms
step:803/1770 train_time:76482ms step_avg:96.45ms
step:804/1770 train_time:76582ms step_avg:96.45ms
step:805/1770 train_time:76683ms step_avg:96.46ms
step:806/1770 train_time:76783ms step_avg:96.46ms
step:807/1770 train_time:76882ms step_avg:96.46ms
step:808/1770 train_time:76982ms step_avg:96.47ms
step:809/1770 train_time:77082ms step_avg:96.47ms
step:810/1770 train_time:77182ms step_avg:96.48ms
step:811/1770 train_time:77282ms step_avg:96.48ms
step:812/1770 train_time:77381ms step_avg:96.49ms
step:813/1770 train_time:77482ms step_avg:96.49ms
step:814/1770 train_time:77582ms step_avg:96.49ms
step:815/1770 train_time:77682ms step_avg:96.50ms
step:816/1770 train_time:77781ms step_avg:96.50ms
step:817/1770 train_time:77881ms step_avg:96.51ms
step:818/1770 train_time:77981ms step_avg:96.51ms
step:819/1770 train_time:78081ms step_avg:96.52ms
step:820/1770 train_time:78181ms step_avg:96.52ms
step:821/1770 train_time:78281ms step_avg:96.52ms
step:822/1770 train_time:78381ms step_avg:96.53ms
step:823/1770 train_time:78481ms step_avg:96.53ms
step:824/1770 train_time:78581ms step_avg:96.54ms
step:825/1770 train_time:78681ms step_avg:96.54ms
step:826/1770 train_time:78781ms step_avg:96.55ms
step:827/1770 train_time:78882ms step_avg:96.55ms
step:828/1770 train_time:78981ms step_avg:96.55ms
step:829/1770 train_time:79082ms step_avg:96.56ms
step:830/1770 train_time:79182ms step_avg:96.56ms
step:831/1770 train_time:79280ms step_avg:96.57ms
step:832/1770 train_time:79380ms step_avg:96.57ms
step:833/1770 train_time:79480ms step_avg:96.57ms
step:834/1770 train_time:79580ms step_avg:96.58ms
step:835/1770 train_time:79679ms step_avg:96.58ms
step:836/1770 train_time:79779ms step_avg:96.59ms
step:837/1770 train_time:79879ms step_avg:96.59ms
step:838/1770 train_time:79979ms step_avg:96.59ms
step:839/1770 train_time:80078ms step_avg:96.60ms
step:840/1770 train_time:80179ms step_avg:96.60ms
step:841/1770 train_time:80278ms step_avg:96.60ms
step:842/1770 train_time:80377ms step_avg:96.61ms
step:843/1770 train_time:80476ms step_avg:96.61ms
step:844/1770 train_time:80575ms step_avg:96.61ms
step:845/1770 train_time:80674ms step_avg:96.62ms
step:846/1770 train_time:80773ms step_avg:96.62ms
step:847/1770 train_time:80872ms step_avg:96.62ms
step:848/1770 train_time:80971ms step_avg:96.62ms
step:849/1770 train_time:81070ms step_avg:96.63ms
step:850/1770 train_time:81170ms step_avg:96.63ms
step:851/1770 train_time:81270ms step_avg:96.63ms
step:852/1770 train_time:81369ms step_avg:96.64ms
step:853/1770 train_time:81469ms step_avg:96.64ms
step:854/1770 train_time:81569ms step_avg:96.65ms
step:855/1770 train_time:81670ms step_avg:96.65ms
step:856/1770 train_time:81770ms step_avg:96.65ms
step:857/1770 train_time:81869ms step_avg:96.66ms
step:858/1770 train_time:81968ms step_avg:96.66ms
step:859/1770 train_time:82067ms step_avg:96.66ms
step:860/1770 train_time:82167ms step_avg:96.67ms
step:861/1770 train_time:82266ms step_avg:96.67ms
step:862/1770 train_time:82365ms step_avg:96.67ms
step:863/1770 train_time:82465ms step_avg:96.68ms
step:864/1770 train_time:82565ms step_avg:96.68ms
step:865/1770 train_time:82664ms step_avg:96.68ms
step:866/1770 train_time:82766ms step_avg:96.69ms
step:867/1770 train_time:82865ms step_avg:96.69ms
step:868/1770 train_time:82965ms step_avg:96.70ms
step:869/1770 train_time:83066ms step_avg:96.70ms
step:870/1770 train_time:83166ms step_avg:96.70ms
step:871/1770 train_time:83265ms step_avg:96.71ms
step:872/1770 train_time:83365ms step_avg:96.71ms
step:873/1770 train_time:83466ms step_avg:96.72ms
step:874/1770 train_time:83566ms step_avg:96.72ms
step:875/1770 train_time:83666ms step_avg:96.72ms
step:875/1770 val_loss:3.5527 train_time:83764ms step_avg:96.84ms
step:876/1770 train_time:83786ms step_avg:96.75ms
step:877/1770 train_time:83874ms step_avg:96.74ms
step:878/1770 train_time:83974ms step_avg:96.74ms
step:879/1770 train_time:84072ms step_avg:96.75ms
step:880/1770 train_time:84171ms step_avg:96.75ms
step:881/1770 train_time:84270ms step_avg:96.75ms
step:882/1770 train_time:84368ms step_avg:96.75ms
step:883/1770 train_time:84467ms step_avg:96.75ms
step:884/1770 train_time:84566ms step_avg:96.76ms
step:885/1770 train_time:84664ms step_avg:96.76ms
step:886/1770 train_time:84763ms step_avg:96.76ms
step:887/1770 train_time:84864ms step_avg:96.77ms
step:888/1770 train_time:84966ms step_avg:96.77ms
step:889/1770 train_time:85065ms step_avg:96.77ms
step:890/1770 train_time:85165ms step_avg:96.78ms
step:891/1770 train_time:85265ms step_avg:96.78ms
step:892/1770 train_time:85364ms step_avg:96.78ms
step:893/1770 train_time:85464ms step_avg:96.79ms
step:894/1770 train_time:85563ms step_avg:96.79ms
step:895/1770 train_time:85662ms step_avg:96.79ms
step:896/1770 train_time:85761ms step_avg:96.80ms
step:897/1770 train_time:85861ms step_avg:96.80ms
step:898/1770 train_time:85961ms step_avg:96.80ms
step:899/1770 train_time:86060ms step_avg:96.81ms
step:900/1770 train_time:86161ms step_avg:96.81ms
step:901/1770 train_time:86260ms step_avg:96.81ms
step:902/1770 train_time:86361ms step_avg:96.82ms
step:903/1770 train_time:86461ms step_avg:96.82ms
step:904/1770 train_time:86561ms step_avg:96.82ms
step:905/1770 train_time:86660ms step_avg:96.83ms
step:906/1770 train_time:86759ms step_avg:96.83ms
step:907/1770 train_time:86859ms step_avg:96.83ms
step:908/1770 train_time:86958ms step_avg:96.84ms
step:909/1770 train_time:87057ms step_avg:96.84ms
step:910/1770 train_time:87157ms step_avg:96.84ms
step:911/1770 train_time:87257ms step_avg:96.84ms
step:912/1770 train_time:87357ms step_avg:96.85ms
step:913/1770 train_time:87457ms step_avg:96.85ms
step:914/1770 train_time:87557ms step_avg:96.86ms
step:915/1770 train_time:87657ms step_avg:96.86ms
step:916/1770 train_time:87757ms step_avg:96.86ms
step:917/1770 train_time:87857ms step_avg:96.87ms
step:918/1770 train_time:87957ms step_avg:96.87ms
step:919/1770 train_time:88057ms step_avg:96.87ms
step:920/1770 train_time:88158ms step_avg:96.88ms
step:921/1770 train_time:88260ms step_avg:96.88ms
step:922/1770 train_time:88361ms step_avg:96.89ms
step:923/1770 train_time:88462ms step_avg:96.89ms
step:924/1770 train_time:88564ms step_avg:96.90ms
step:925/1770 train_time:88665ms step_avg:96.90ms
step:926/1770 train_time:88766ms step_avg:96.91ms
step:927/1770 train_time:88867ms step_avg:96.91ms
step:928/1770 train_time:88968ms step_avg:96.92ms
step:929/1770 train_time:89068ms step_avg:96.92ms
step:930/1770 train_time:89169ms step_avg:96.92ms
step:931/1770 train_time:89271ms step_avg:96.93ms
step:932/1770 train_time:89371ms step_avg:96.93ms
step:933/1770 train_time:89472ms step_avg:96.94ms
step:934/1770 train_time:89573ms step_avg:96.94ms
step:935/1770 train_time:89673ms step_avg:96.94ms
step:936/1770 train_time:89774ms step_avg:96.95ms
step:937/1770 train_time:89875ms step_avg:96.95ms
step:938/1770 train_time:89976ms step_avg:96.96ms
step:939/1770 train_time:90078ms step_avg:96.96ms
step:940/1770 train_time:90180ms step_avg:96.97ms
step:941/1770 train_time:90282ms step_avg:96.97ms
step:942/1770 train_time:90383ms step_avg:96.98ms
step:943/1770 train_time:90484ms step_avg:96.98ms
step:944/1770 train_time:90585ms step_avg:96.99ms
step:945/1770 train_time:90685ms step_avg:96.99ms
step:946/1770 train_time:90787ms step_avg:96.99ms
step:947/1770 train_time:90888ms step_avg:97.00ms
step:948/1770 train_time:90989ms step_avg:97.00ms
step:949/1770 train_time:91090ms step_avg:97.01ms
step:950/1770 train_time:91192ms step_avg:97.01ms
step:951/1770 train_time:91292ms step_avg:97.02ms
step:952/1770 train_time:91393ms step_avg:97.02ms
step:953/1770 train_time:91494ms step_avg:97.02ms
step:954/1770 train_time:91595ms step_avg:97.03ms
step:955/1770 train_time:91696ms step_avg:97.03ms
step:956/1770 train_time:91797ms step_avg:97.04ms
step:957/1770 train_time:91898ms step_avg:97.04ms
step:958/1770 train_time:92000ms step_avg:97.05ms
step:959/1770 train_time:92102ms step_avg:97.05ms
step:960/1770 train_time:92202ms step_avg:97.05ms
step:961/1770 train_time:92303ms step_avg:97.06ms
step:962/1770 train_time:92404ms step_avg:97.06ms
step:963/1770 train_time:92505ms step_avg:97.07ms
step:964/1770 train_time:92606ms step_avg:97.07ms
step:965/1770 train_time:92707ms step_avg:97.08ms
step:966/1770 train_time:92807ms step_avg:97.08ms
step:967/1770 train_time:92909ms step_avg:97.08ms
step:968/1770 train_time:93010ms step_avg:97.09ms
step:969/1770 train_time:93111ms step_avg:97.09ms
step:970/1770 train_time:93211ms step_avg:97.10ms
step:971/1770 train_time:93312ms step_avg:97.10ms
step:972/1770 train_time:93414ms step_avg:97.10ms
step:973/1770 train_time:93515ms step_avg:97.11ms
step:974/1770 train_time:93616ms step_avg:97.11ms
step:975/1770 train_time:93718ms step_avg:97.12ms
step:976/1770 train_time:93819ms step_avg:97.12ms
step:977/1770 train_time:93920ms step_avg:97.13ms
step:978/1770 train_time:94021ms step_avg:97.13ms
step:979/1770 train_time:94122ms step_avg:97.13ms
step:980/1770 train_time:94224ms step_avg:97.14ms
step:981/1770 train_time:94325ms step_avg:97.14ms
step:982/1770 train_time:94427ms step_avg:97.15ms
step:983/1770 train_time:94528ms step_avg:97.15ms
step:984/1770 train_time:94629ms step_avg:97.15ms
step:985/1770 train_time:94729ms step_avg:97.16ms
step:986/1770 train_time:94831ms step_avg:97.16ms
step:987/1770 train_time:94931ms step_avg:97.17ms
step:988/1770 train_time:95032ms step_avg:97.17ms
step:989/1770 train_time:95134ms step_avg:97.17ms
step:990/1770 train_time:95235ms step_avg:97.18ms
step:991/1770 train_time:95335ms step_avg:97.18ms
step:992/1770 train_time:95438ms step_avg:97.19ms
step:993/1770 train_time:95540ms step_avg:97.19ms
step:994/1770 train_time:95641ms step_avg:97.20ms
step:995/1770 train_time:95742ms step_avg:97.20ms
step:996/1770 train_time:95843ms step_avg:97.20ms
step:997/1770 train_time:95944ms step_avg:97.21ms
step:998/1770 train_time:96045ms step_avg:97.21ms
step:999/1770 train_time:96147ms step_avg:97.22ms
step:1000/1770 train_time:96248ms step_avg:97.22ms
step:1000/1770 val_loss:3.5138 train_time:96347ms step_avg:97.32ms
step:1001/1770 train_time:96371ms step_avg:97.25ms
step:1002/1770 train_time:96457ms step_avg:97.24ms
step:1003/1770 train_time:96560ms step_avg:97.24ms
step:1004/1770 train_time:96661ms step_avg:97.24ms
step:1005/1770 train_time:96761ms step_avg:97.25ms
step:1006/1770 train_time:96861ms step_avg:97.25ms
step:1007/1770 train_time:96961ms step_avg:97.25ms
step:1008/1770 train_time:97062ms step_avg:97.26ms
step:1009/1770 train_time:97162ms step_avg:97.26ms
step:1010/1770 train_time:97263ms step_avg:97.26ms
step:1011/1770 train_time:97368ms step_avg:97.27ms
step:1012/1770 train_time:97470ms step_avg:97.28ms
step:1013/1770 train_time:97572ms step_avg:97.28ms
step:1014/1770 train_time:97672ms step_avg:97.28ms
step:1015/1770 train_time:97772ms step_avg:97.29ms
step:1016/1770 train_time:97872ms step_avg:97.29ms
step:1017/1770 train_time:97972ms step_avg:97.29ms
step:1018/1770 train_time:98072ms step_avg:97.29ms
step:1019/1770 train_time:98172ms step_avg:97.30ms
step:1020/1770 train_time:98273ms step_avg:97.30ms
step:1021/1770 train_time:98375ms step_avg:97.30ms
step:1022/1770 train_time:98476ms step_avg:97.31ms
step:1023/1770 train_time:98577ms step_avg:97.31ms
step:1024/1770 train_time:98677ms step_avg:97.31ms
step:1025/1770 train_time:98778ms step_avg:97.32ms
step:1026/1770 train_time:98879ms step_avg:97.32ms
step:1027/1770 train_time:98980ms step_avg:97.33ms
step:1028/1770 train_time:99080ms step_avg:97.33ms
step:1029/1770 train_time:99182ms step_avg:97.33ms
step:1030/1770 train_time:99284ms step_avg:97.34ms
step:1031/1770 train_time:99386ms step_avg:97.34ms
step:1032/1770 train_time:99487ms step_avg:97.35ms
step:1033/1770 train_time:99589ms step_avg:97.35ms
step:1034/1770 train_time:99690ms step_avg:97.35ms
step:1035/1770 train_time:99790ms step_avg:97.36ms
step:1036/1770 train_time:99890ms step_avg:97.36ms
step:1037/1770 train_time:99991ms step_avg:97.36ms
step:1038/1770 train_time:100092ms step_avg:97.37ms
step:1039/1770 train_time:100192ms step_avg:97.37ms
step:1040/1770 train_time:100294ms step_avg:97.37ms
step:1041/1770 train_time:100395ms step_avg:97.38ms
step:1042/1770 train_time:100497ms step_avg:97.38ms
step:1043/1770 train_time:100597ms step_avg:97.38ms
step:1044/1770 train_time:100697ms step_avg:97.39ms
step:1045/1770 train_time:100798ms step_avg:97.39ms
step:1046/1770 train_time:100898ms step_avg:97.39ms
step:1047/1770 train_time:100999ms step_avg:97.40ms
step:1048/1770 train_time:101101ms step_avg:97.40ms
step:1049/1770 train_time:101202ms step_avg:97.40ms
step:1050/1770 train_time:101304ms step_avg:97.41ms
step:1051/1770 train_time:101406ms step_avg:97.41ms
step:1052/1770 train_time:101508ms step_avg:97.42ms
step:1053/1770 train_time:101609ms step_avg:97.42ms
step:1054/1770 train_time:101709ms step_avg:97.42ms
step:1055/1770 train_time:101809ms step_avg:97.43ms
step:1056/1770 train_time:101911ms step_avg:97.43ms
step:1057/1770 train_time:102012ms step_avg:97.43ms
step:1058/1770 train_time:102112ms step_avg:97.44ms
step:1059/1770 train_time:102213ms step_avg:97.44ms
step:1060/1770 train_time:102314ms step_avg:97.44ms
step:1061/1770 train_time:102416ms step_avg:97.45ms
step:1062/1770 train_time:102519ms step_avg:97.45ms
step:1063/1770 train_time:102620ms step_avg:97.46ms
step:1064/1770 train_time:102722ms step_avg:97.46ms
step:1065/1770 train_time:102823ms step_avg:97.46ms
step:1066/1770 train_time:102924ms step_avg:97.47ms
step:1067/1770 train_time:103026ms step_avg:97.47ms
step:1068/1770 train_time:103129ms step_avg:97.48ms
step:1069/1770 train_time:103230ms step_avg:97.48ms
step:1070/1770 train_time:103331ms step_avg:97.48ms
step:1071/1770 train_time:103432ms step_avg:97.49ms
step:1072/1770 train_time:103532ms step_avg:97.49ms
step:1073/1770 train_time:103632ms step_avg:97.49ms
step:1074/1770 train_time:103733ms step_avg:97.49ms
step:1075/1770 train_time:103835ms step_avg:97.50ms
step:1076/1770 train_time:103936ms step_avg:97.50ms
step:1077/1770 train_time:104037ms step_avg:97.50ms
step:1078/1770 train_time:104137ms step_avg:97.51ms
step:1079/1770 train_time:104239ms step_avg:97.51ms
step:1080/1770 train_time:104340ms step_avg:97.51ms
step:1081/1770 train_time:104440ms step_avg:97.52ms
step:1082/1770 train_time:104543ms step_avg:97.52ms
step:1083/1770 train_time:104645ms step_avg:97.53ms
step:1084/1770 train_time:104748ms step_avg:97.53ms
step:1085/1770 train_time:104849ms step_avg:97.53ms
step:1086/1770 train_time:104950ms step_avg:97.54ms
step:1087/1770 train_time:105050ms step_avg:97.54ms
step:1088/1770 train_time:105152ms step_avg:97.54ms
step:1089/1770 train_time:105252ms step_avg:97.55ms
step:1090/1770 train_time:105353ms step_avg:97.55ms
step:1091/1770 train_time:105454ms step_avg:97.55ms
step:1092/1770 train_time:105556ms step_avg:97.56ms
step:1093/1770 train_time:105656ms step_avg:97.56ms
step:1094/1770 train_time:105757ms step_avg:97.56ms
step:1095/1770 train_time:105859ms step_avg:97.57ms
step:1096/1770 train_time:105959ms step_avg:97.57ms
step:1097/1770 train_time:106061ms step_avg:97.57ms
step:1098/1770 train_time:106162ms step_avg:97.58ms
step:1099/1770 train_time:106264ms step_avg:97.58ms
step:1100/1770 train_time:106367ms step_avg:97.58ms
step:1101/1770 train_time:106468ms step_avg:97.59ms
step:1102/1770 train_time:106569ms step_avg:97.59ms
step:1103/1770 train_time:106669ms step_avg:97.59ms
step:1104/1770 train_time:106770ms step_avg:97.60ms
step:1105/1770 train_time:106871ms step_avg:97.60ms
step:1106/1770 train_time:106972ms step_avg:97.60ms
step:1107/1770 train_time:107073ms step_avg:97.61ms
step:1108/1770 train_time:107174ms step_avg:97.61ms
step:1109/1770 train_time:107275ms step_avg:97.61ms
step:1110/1770 train_time:107377ms step_avg:97.62ms
step:1111/1770 train_time:107478ms step_avg:97.62ms
step:1112/1770 train_time:107579ms step_avg:97.62ms
step:1113/1770 train_time:107679ms step_avg:97.62ms
step:1114/1770 train_time:107780ms step_avg:97.63ms
step:1115/1770 train_time:107882ms step_avg:97.63ms
step:1116/1770 train_time:107984ms step_avg:97.63ms
step:1117/1770 train_time:108086ms step_avg:97.64ms
step:1118/1770 train_time:108187ms step_avg:97.64ms
step:1119/1770 train_time:108288ms step_avg:97.65ms
step:1120/1770 train_time:108390ms step_avg:97.65ms
step:1121/1770 train_time:108491ms step_avg:97.65ms
step:1122/1770 train_time:108592ms step_avg:97.65ms
step:1123/1770 train_time:108692ms step_avg:97.66ms
step:1124/1770 train_time:108793ms step_avg:97.66ms
step:1125/1770 train_time:108894ms step_avg:97.66ms
step:1125/1770 val_loss:3.4719 train_time:108994ms step_avg:97.75ms
step:1126/1770 train_time:109016ms step_avg:97.68ms
step:1127/1770 train_time:109103ms step_avg:97.67ms
step:1128/1770 train_time:109204ms step_avg:97.68ms
step:1129/1770 train_time:109304ms step_avg:97.68ms
step:1130/1770 train_time:109406ms step_avg:97.68ms
step:1131/1770 train_time:109507ms step_avg:97.69ms
step:1132/1770 train_time:109608ms step_avg:97.69ms
step:1133/1770 train_time:109709ms step_avg:97.69ms
step:1134/1770 train_time:109810ms step_avg:97.70ms
step:1135/1770 train_time:109911ms step_avg:97.70ms
step:1136/1770 train_time:110015ms step_avg:97.70ms
step:1137/1770 train_time:110117ms step_avg:97.71ms
step:1138/1770 train_time:110217ms step_avg:97.71ms
step:1139/1770 train_time:110318ms step_avg:97.71ms
step:1140/1770 train_time:110418ms step_avg:97.72ms
step:1141/1770 train_time:110519ms step_avg:97.72ms
step:1142/1770 train_time:110619ms step_avg:97.72ms
step:1143/1770 train_time:110720ms step_avg:97.72ms
step:1144/1770 train_time:110820ms step_avg:97.73ms
step:1145/1770 train_time:110922ms step_avg:97.73ms
step:1146/1770 train_time:111024ms step_avg:97.73ms
step:1147/1770 train_time:111127ms step_avg:97.74ms
step:1148/1770 train_time:111228ms step_avg:97.74ms
step:1149/1770 train_time:111329ms step_avg:97.74ms
step:1150/1770 train_time:111430ms step_avg:97.75ms
step:1151/1770 train_time:111532ms step_avg:97.75ms
step:1152/1770 train_time:111633ms step_avg:97.75ms
step:1153/1770 train_time:111734ms step_avg:97.75ms
step:1154/1770 train_time:111835ms step_avg:97.76ms
step:1155/1770 train_time:111936ms step_avg:97.76ms
step:1156/1770 train_time:112037ms step_avg:97.76ms
step:1157/1770 train_time:112140ms step_avg:97.77ms
step:1158/1770 train_time:112241ms step_avg:97.77ms
step:1159/1770 train_time:112341ms step_avg:97.77ms
step:1160/1770 train_time:112442ms step_avg:97.78ms
step:1161/1770 train_time:112543ms step_avg:97.78ms
step:1162/1770 train_time:112644ms step_avg:97.78ms
step:1163/1770 train_time:112745ms step_avg:97.78ms
step:1164/1770 train_time:112847ms step_avg:97.79ms
step:1165/1770 train_time:112949ms step_avg:97.79ms
step:1166/1770 train_time:113051ms step_avg:97.79ms
step:1167/1770 train_time:113153ms step_avg:97.80ms
step:1168/1770 train_time:113254ms step_avg:97.80ms
step:1169/1770 train_time:113354ms step_avg:97.80ms
step:1170/1770 train_time:113455ms step_avg:97.81ms
step:1171/1770 train_time:113556ms step_avg:97.81ms
step:1172/1770 train_time:113657ms step_avg:97.81ms
step:1173/1770 train_time:113758ms step_avg:97.81ms
step:1174/1770 train_time:113860ms step_avg:97.82ms
step:1175/1770 train_time:113962ms step_avg:97.82ms
step:1176/1770 train_time:114063ms step_avg:97.82ms
step:1177/1770 train_time:114164ms step_avg:97.83ms
step:1178/1770 train_time:114265ms step_avg:97.83ms
step:1179/1770 train_time:114367ms step_avg:97.83ms
step:1180/1770 train_time:114469ms step_avg:97.84ms
step:1181/1770 train_time:114570ms step_avg:97.84ms
step:1182/1770 train_time:114672ms step_avg:97.84ms
step:1183/1770 train_time:114775ms step_avg:97.85ms
step:1184/1770 train_time:114879ms step_avg:97.85ms
step:1185/1770 train_time:114980ms step_avg:97.86ms
step:1186/1770 train_time:115083ms step_avg:97.86ms
step:1187/1770 train_time:115187ms step_avg:97.87ms
step:1188/1770 train_time:115289ms step_avg:97.87ms
step:1189/1770 train_time:115391ms step_avg:97.87ms
step:1190/1770 train_time:115493ms step_avg:97.88ms
step:1191/1770 train_time:115595ms step_avg:97.88ms
step:1192/1770 train_time:115697ms step_avg:97.88ms
step:1193/1770 train_time:115799ms step_avg:97.89ms
step:1194/1770 train_time:115900ms step_avg:97.89ms
step:1195/1770 train_time:116002ms step_avg:97.89ms
step:1196/1770 train_time:116106ms step_avg:97.90ms
step:1197/1770 train_time:116207ms step_avg:97.90ms
step:1198/1770 train_time:116309ms step_avg:97.90ms
step:1199/1770 train_time:116411ms step_avg:97.91ms
step:1200/1770 train_time:116513ms step_avg:97.91ms
step:1201/1770 train_time:116615ms step_avg:97.91ms
step:1202/1770 train_time:116716ms step_avg:97.92ms
step:1203/1770 train_time:116818ms step_avg:97.92ms
step:1204/1770 train_time:116920ms step_avg:97.92ms
step:1205/1770 train_time:117023ms step_avg:97.93ms
step:1206/1770 train_time:117125ms step_avg:97.93ms
step:1207/1770 train_time:117227ms step_avg:97.93ms
step:1208/1770 train_time:117328ms step_avg:97.94ms
step:1209/1770 train_time:117431ms step_avg:97.94ms
step:1210/1770 train_time:117532ms step_avg:97.94ms
step:1211/1770 train_time:117635ms step_avg:97.95ms
step:1212/1770 train_time:117739ms step_avg:97.95ms
step:1213/1770 train_time:117840ms step_avg:97.96ms
step:1214/1770 train_time:117942ms step_avg:97.96ms
step:1215/1770 train_time:118044ms step_avg:97.96ms
step:1216/1770 train_time:118149ms step_avg:97.97ms
step:1217/1770 train_time:118251ms step_avg:97.97ms
step:1218/1770 train_time:118352ms step_avg:97.97ms
step:1219/1770 train_time:118454ms step_avg:97.98ms
step:1220/1770 train_time:118557ms step_avg:97.98ms
step:1221/1770 train_time:118659ms step_avg:97.98ms
step:1222/1770 train_time:118764ms step_avg:97.99ms
step:1223/1770 train_time:118866ms step_avg:97.99ms
step:1224/1770 train_time:118969ms step_avg:98.00ms
step:1225/1770 train_time:119071ms step_avg:98.00ms
step:1226/1770 train_time:119173ms step_avg:98.00ms
step:1227/1770 train_time:119277ms step_avg:98.01ms
step:1228/1770 train_time:119380ms step_avg:98.01ms
step:1229/1770 train_time:119482ms step_avg:98.02ms
step:1230/1770 train_time:119585ms step_avg:98.02ms
step:1231/1770 train_time:119687ms step_avg:98.02ms
step:1232/1770 train_time:119789ms step_avg:98.03ms
step:1233/1770 train_time:119891ms step_avg:98.03ms
step:1234/1770 train_time:119993ms step_avg:98.03ms
step:1235/1770 train_time:120095ms step_avg:98.04ms
step:1236/1770 train_time:120198ms step_avg:98.04ms
step:1237/1770 train_time:120300ms step_avg:98.04ms
step:1238/1770 train_time:120402ms step_avg:98.05ms
step:1239/1770 train_time:120504ms step_avg:98.05ms
step:1240/1770 train_time:120606ms step_avg:98.05ms
step:1241/1770 train_time:120709ms step_avg:98.06ms
step:1242/1770 train_time:120811ms step_avg:98.06ms
step:1243/1770 train_time:120913ms step_avg:98.06ms
step:1244/1770 train_time:121014ms step_avg:98.07ms
step:1245/1770 train_time:121117ms step_avg:98.07ms
step:1246/1770 train_time:121219ms step_avg:98.07ms
step:1247/1770 train_time:121321ms step_avg:98.08ms
step:1248/1770 train_time:121425ms step_avg:98.08ms
step:1249/1770 train_time:121526ms step_avg:98.08ms
step:1250/1770 train_time:121628ms step_avg:98.09ms
step:1250/1770 val_loss:3.4250 train_time:121729ms step_avg:98.17ms
step:1251/1770 train_time:121751ms step_avg:98.11ms
step:1252/1770 train_time:121838ms step_avg:98.10ms
step:1253/1770 train_time:121940ms step_avg:98.10ms
step:1254/1770 train_time:122043ms step_avg:98.11ms
step:1255/1770 train_time:122147ms step_avg:98.11ms
step:1256/1770 train_time:122249ms step_avg:98.11ms
step:1257/1770 train_time:122350ms step_avg:98.12ms
step:1258/1770 train_time:122453ms step_avg:98.12ms
step:1259/1770 train_time:122554ms step_avg:98.12ms
step:1260/1770 train_time:122656ms step_avg:98.13ms
step:1261/1770 train_time:122760ms step_avg:98.13ms
step:1262/1770 train_time:122864ms step_avg:98.13ms
step:1263/1770 train_time:122965ms step_avg:98.14ms
step:1264/1770 train_time:123068ms step_avg:98.14ms
step:1265/1770 train_time:123169ms step_avg:98.14ms
step:1266/1770 train_time:123272ms step_avg:98.15ms
step:1267/1770 train_time:123375ms step_avg:98.15ms
step:1268/1770 train_time:123477ms step_avg:98.15ms
step:1269/1770 train_time:123579ms step_avg:98.16ms
step:1270/1770 train_time:123681ms step_avg:98.16ms
step:1271/1770 train_time:123784ms step_avg:98.16ms
step:1272/1770 train_time:123887ms step_avg:98.17ms
step:1273/1770 train_time:123989ms step_avg:98.17ms
step:1274/1770 train_time:124091ms step_avg:98.17ms
step:1275/1770 train_time:124193ms step_avg:98.18ms
step:1276/1770 train_time:124295ms step_avg:98.18ms
step:1277/1770 train_time:124397ms step_avg:98.18ms
step:1278/1770 train_time:124501ms step_avg:98.19ms
step:1279/1770 train_time:124604ms step_avg:98.19ms
step:1280/1770 train_time:124707ms step_avg:98.19ms
step:1281/1770 train_time:124809ms step_avg:98.20ms
step:1282/1770 train_time:124912ms step_avg:98.20ms
step:1283/1770 train_time:125014ms step_avg:98.20ms
step:1284/1770 train_time:125116ms step_avg:98.21ms
step:1285/1770 train_time:125218ms step_avg:98.21ms
step:1286/1770 train_time:125321ms step_avg:98.21ms
step:1287/1770 train_time:125425ms step_avg:98.22ms
step:1288/1770 train_time:125528ms step_avg:98.22ms
step:1289/1770 train_time:125630ms step_avg:98.22ms
step:1290/1770 train_time:125731ms step_avg:98.23ms
step:1291/1770 train_time:125833ms step_avg:98.23ms
step:1292/1770 train_time:125935ms step_avg:98.23ms
step:1293/1770 train_time:126038ms step_avg:98.24ms
step:1294/1770 train_time:126140ms step_avg:98.24ms
step:1295/1770 train_time:126242ms step_avg:98.24ms
step:1296/1770 train_time:126344ms step_avg:98.25ms
step:1297/1770 train_time:126446ms step_avg:98.25ms
step:1298/1770 train_time:126550ms step_avg:98.25ms
step:1299/1770 train_time:126651ms step_avg:98.26ms
step:1300/1770 train_time:126753ms step_avg:98.26ms
step:1301/1770 train_time:126856ms step_avg:98.26ms
step:1302/1770 train_time:126958ms step_avg:98.26ms
step:1303/1770 train_time:127060ms step_avg:98.27ms
step:1304/1770 train_time:127162ms step_avg:98.27ms
step:1305/1770 train_time:127265ms step_avg:98.27ms
step:1306/1770 train_time:127367ms step_avg:98.28ms
step:1307/1770 train_time:127469ms step_avg:98.28ms
step:1308/1770 train_time:127571ms step_avg:98.28ms
step:1309/1770 train_time:127673ms step_avg:98.29ms
step:1310/1770 train_time:127775ms step_avg:98.29ms
step:1311/1770 train_time:127876ms step_avg:98.29ms
step:1312/1770 train_time:127978ms step_avg:98.29ms
step:1313/1770 train_time:128079ms step_avg:98.30ms
step:1314/1770 train_time:128182ms step_avg:98.30ms
step:1315/1770 train_time:128285ms step_avg:98.30ms
step:1316/1770 train_time:128387ms step_avg:98.31ms
step:1317/1770 train_time:128490ms step_avg:98.31ms
step:1318/1770 train_time:128595ms step_avg:98.31ms
step:1319/1770 train_time:128698ms step_avg:98.32ms
step:1320/1770 train_time:128800ms step_avg:98.32ms
step:1321/1770 train_time:128902ms step_avg:98.32ms
step:1322/1770 train_time:129004ms step_avg:98.33ms
step:1323/1770 train_time:129107ms step_avg:98.33ms
step:1324/1770 train_time:129210ms step_avg:98.33ms
step:1325/1770 train_time:129315ms step_avg:98.34ms
step:1326/1770 train_time:129417ms step_avg:98.34ms
step:1327/1770 train_time:129522ms step_avg:98.35ms
step:1328/1770 train_time:129624ms step_avg:98.35ms
step:1329/1770 train_time:129727ms step_avg:98.35ms
step:1330/1770 train_time:129829ms step_avg:98.36ms
step:1331/1770 train_time:129931ms step_avg:98.36ms
step:1332/1770 train_time:130034ms step_avg:98.36ms
step:1333/1770 train_time:130136ms step_avg:98.36ms
step:1334/1770 train_time:130237ms step_avg:98.37ms
step:1335/1770 train_time:130340ms step_avg:98.37ms
step:1336/1770 train_time:130441ms step_avg:98.37ms
step:1337/1770 train_time:130543ms step_avg:98.37ms
step:1338/1770 train_time:130645ms step_avg:98.38ms
step:1339/1770 train_time:130748ms step_avg:98.38ms
step:1340/1770 train_time:130851ms step_avg:98.38ms
step:1341/1770 train_time:130953ms step_avg:98.39ms
step:1342/1770 train_time:131056ms step_avg:98.39ms
step:1343/1770 train_time:131158ms step_avg:98.39ms
step:1344/1770 train_time:131262ms step_avg:98.40ms
step:1345/1770 train_time:131364ms step_avg:98.40ms
step:1346/1770 train_time:131466ms step_avg:98.40ms
step:1347/1770 train_time:131568ms step_avg:98.41ms
step:1348/1770 train_time:131673ms step_avg:98.41ms
step:1349/1770 train_time:131776ms step_avg:98.41ms
step:1350/1770 train_time:131878ms step_avg:98.42ms
step:1351/1770 train_time:131980ms step_avg:98.42ms
step:1352/1770 train_time:132083ms step_avg:98.42ms
step:1353/1770 train_time:132186ms step_avg:98.43ms
step:1354/1770 train_time:132288ms step_avg:98.43ms
step:1355/1770 train_time:132389ms step_avg:98.43ms
step:1356/1770 train_time:132491ms step_avg:98.43ms
step:1357/1770 train_time:132593ms step_avg:98.44ms
step:1358/1770 train_time:132696ms step_avg:98.44ms
step:1359/1770 train_time:132799ms step_avg:98.44ms
step:1360/1770 train_time:132902ms step_avg:98.45ms
step:1361/1770 train_time:133004ms step_avg:98.45ms
step:1362/1770 train_time:133107ms step_avg:98.45ms
step:1363/1770 train_time:133210ms step_avg:98.46ms
step:1364/1770 train_time:133313ms step_avg:98.46ms
step:1365/1770 train_time:133415ms step_avg:98.46ms
step:1366/1770 train_time:133516ms step_avg:98.46ms
step:1367/1770 train_time:133619ms step_avg:98.47ms
step:1368/1770 train_time:133722ms step_avg:98.47ms
step:1369/1770 train_time:133825ms step_avg:98.47ms
step:1370/1770 train_time:133929ms step_avg:98.48ms
step:1371/1770 train_time:134031ms step_avg:98.48ms
step:1372/1770 train_time:134132ms step_avg:98.48ms
step:1373/1770 train_time:134235ms step_avg:98.49ms
step:1374/1770 train_time:134338ms step_avg:98.49ms
step:1375/1770 train_time:134441ms step_avg:98.49ms
step:1375/1770 val_loss:3.3806 train_time:134542ms step_avg:98.57ms
step:1376/1770 train_time:134564ms step_avg:98.51ms
step:1377/1770 train_time:134651ms step_avg:98.50ms
step:1378/1770 train_time:134753ms step_avg:98.50ms
step:1379/1770 train_time:134856ms step_avg:98.51ms
step:1380/1770 train_time:134958ms step_avg:98.51ms
step:1381/1770 train_time:135060ms step_avg:98.51ms
step:1382/1770 train_time:135161ms step_avg:98.51ms
step:1383/1770 train_time:135264ms step_avg:98.52ms
step:1384/1770 train_time:135366ms step_avg:98.52ms
step:1385/1770 train_time:135469ms step_avg:98.52ms
step:1386/1770 train_time:135572ms step_avg:98.53ms
step:1387/1770 train_time:135675ms step_avg:98.53ms
step:1388/1770 train_time:135777ms step_avg:98.53ms
step:1389/1770 train_time:135880ms step_avg:98.53ms
step:1390/1770 train_time:135981ms step_avg:98.54ms
step:1391/1770 train_time:136083ms step_avg:98.54ms
step:1392/1770 train_time:136186ms step_avg:98.54ms
step:1393/1770 train_time:136288ms step_avg:98.55ms
step:1394/1770 train_time:136390ms step_avg:98.55ms
step:1395/1770 train_time:136494ms step_avg:98.55ms
step:1396/1770 train_time:136598ms step_avg:98.56ms
step:1397/1770 train_time:136700ms step_avg:98.56ms
step:1398/1770 train_time:136802ms step_avg:98.56ms
step:1399/1770 train_time:136905ms step_avg:98.56ms
step:1400/1770 train_time:137007ms step_avg:98.57ms
step:1401/1770 train_time:137110ms step_avg:98.57ms
step:1402/1770 train_time:137213ms step_avg:98.57ms
step:1403/1770 train_time:137315ms step_avg:98.58ms
step:1404/1770 train_time:137417ms step_avg:98.58ms
step:1405/1770 train_time:137519ms step_avg:98.58ms
step:1406/1770 train_time:137622ms step_avg:98.58ms
step:1407/1770 train_time:137723ms step_avg:98.59ms
step:1408/1770 train_time:137826ms step_avg:98.59ms
step:1409/1770 train_time:137928ms step_avg:98.59ms
step:1410/1770 train_time:138030ms step_avg:98.59ms
step:1411/1770 train_time:138132ms step_avg:98.60ms
step:1412/1770 train_time:138234ms step_avg:98.60ms
step:1413/1770 train_time:138337ms step_avg:98.60ms
step:1414/1770 train_time:138439ms step_avg:98.60ms
step:1415/1770 train_time:138541ms step_avg:98.61ms
step:1416/1770 train_time:138644ms step_avg:98.61ms
step:1417/1770 train_time:138747ms step_avg:98.61ms
step:1418/1770 train_time:138849ms step_avg:98.61ms
step:1419/1770 train_time:138951ms step_avg:98.62ms
step:1420/1770 train_time:139053ms step_avg:98.62ms
step:1421/1770 train_time:139156ms step_avg:98.62ms
step:1422/1770 train_time:139258ms step_avg:98.62ms
step:1423/1770 train_time:139360ms step_avg:98.63ms
step:1424/1770 train_time:139463ms step_avg:98.63ms
step:1425/1770 train_time:139565ms step_avg:98.63ms
step:1426/1770 train_time:139668ms step_avg:98.64ms
step:1427/1770 train_time:139769ms step_avg:98.64ms
step:1428/1770 train_time:139874ms step_avg:98.64ms
step:1429/1770 train_time:139977ms step_avg:98.64ms
step:1430/1770 train_time:140078ms step_avg:98.65ms
step:1431/1770 train_time:140182ms step_avg:98.65ms
step:1432/1770 train_time:140284ms step_avg:98.65ms
step:1433/1770 train_time:140386ms step_avg:98.65ms
step:1434/1770 train_time:140487ms step_avg:98.66ms
step:1435/1770 train_time:140589ms step_avg:98.66ms
step:1436/1770 train_time:140693ms step_avg:98.66ms
step:1437/1770 train_time:140796ms step_avg:98.67ms
step:1438/1770 train_time:140898ms step_avg:98.67ms
step:1439/1770 train_time:141000ms step_avg:98.67ms
step:1440/1770 train_time:141102ms step_avg:98.67ms
step:1441/1770 train_time:141207ms step_avg:98.68ms
step:1442/1770 train_time:141308ms step_avg:98.68ms
step:1443/1770 train_time:141412ms step_avg:98.68ms
step:1444/1770 train_time:141514ms step_avg:98.69ms
step:1445/1770 train_time:141617ms step_avg:98.69ms
step:1446/1770 train_time:141720ms step_avg:98.69ms
step:1447/1770 train_time:141823ms step_avg:98.69ms
step:1448/1770 train_time:141927ms step_avg:98.70ms
step:1449/1770 train_time:142030ms step_avg:98.70ms
step:1450/1770 train_time:142133ms step_avg:98.70ms
step:1451/1770 train_time:142237ms step_avg:98.71ms
step:1452/1770 train_time:142342ms step_avg:98.71ms
step:1453/1770 train_time:142445ms step_avg:98.71ms
step:1454/1770 train_time:142548ms step_avg:98.72ms
step:1455/1770 train_time:142653ms step_avg:98.72ms
step:1456/1770 train_time:142757ms step_avg:98.73ms
step:1457/1770 train_time:142860ms step_avg:98.73ms
step:1458/1770 train_time:142963ms step_avg:98.73ms
step:1459/1770 train_time:143069ms step_avg:98.74ms
step:1460/1770 train_time:143172ms step_avg:98.74ms
step:1461/1770 train_time:143276ms step_avg:98.74ms
step:1462/1770 train_time:143379ms step_avg:98.75ms
step:1463/1770 train_time:143483ms step_avg:98.75ms
step:1464/1770 train_time:143588ms step_avg:98.75ms
step:1465/1770 train_time:143691ms step_avg:98.76ms
step:1466/1770 train_time:143795ms step_avg:98.76ms
step:1467/1770 train_time:143899ms step_avg:98.76ms
step:1468/1770 train_time:144003ms step_avg:98.77ms
step:1469/1770 train_time:144105ms step_avg:98.77ms
step:1470/1770 train_time:144208ms step_avg:98.77ms
step:1471/1770 train_time:144311ms step_avg:98.78ms
step:1472/1770 train_time:144415ms step_avg:98.78ms
step:1473/1770 train_time:144519ms step_avg:98.78ms
step:1474/1770 train_time:144623ms step_avg:98.79ms
step:1475/1770 train_time:144725ms step_avg:98.79ms
step:1476/1770 train_time:144828ms step_avg:98.79ms
step:1477/1770 train_time:144934ms step_avg:98.80ms
step:1478/1770 train_time:145037ms step_avg:98.80ms
step:1479/1770 train_time:145140ms step_avg:98.80ms
step:1480/1770 train_time:145244ms step_avg:98.81ms
step:1481/1770 train_time:145350ms step_avg:98.81ms
step:1482/1770 train_time:145454ms step_avg:98.81ms
step:1483/1770 train_time:145558ms step_avg:98.82ms
step:1484/1770 train_time:145662ms step_avg:98.82ms
step:1485/1770 train_time:145764ms step_avg:98.82ms
step:1486/1770 train_time:145866ms step_avg:98.83ms
step:1487/1770 train_time:145970ms step_avg:98.83ms
step:1488/1770 train_time:146074ms step_avg:98.83ms
step:1489/1770 train_time:146178ms step_avg:98.84ms
step:1490/1770 train_time:146281ms step_avg:98.84ms
step:1491/1770 train_time:146385ms step_avg:98.84ms
step:1492/1770 train_time:146489ms step_avg:98.85ms
step:1493/1770 train_time:146595ms step_avg:98.85ms
step:1494/1770 train_time:146702ms step_avg:98.86ms
step:1495/1770 train_time:146804ms step_avg:98.86ms
step:1496/1770 train_time:146907ms step_avg:98.86ms
step:1497/1770 train_time:147011ms step_avg:98.86ms
step:1498/1770 train_time:147113ms step_avg:98.87ms
step:1499/1770 train_time:147216ms step_avg:98.87ms
step:1500/1770 train_time:147319ms step_avg:98.87ms
step:1500/1770 val_loss:3.3430 train_time:147420ms step_avg:98.94ms
step:1501/1770 train_time:147442ms step_avg:98.89ms
step:1502/1770 train_time:147529ms step_avg:98.88ms
step:1503/1770 train_time:147632ms step_avg:98.88ms
step:1504/1770 train_time:147735ms step_avg:98.89ms
step:1505/1770 train_time:147840ms step_avg:98.89ms
step:1506/1770 train_time:147943ms step_avg:98.89ms
step:1507/1770 train_time:148047ms step_avg:98.90ms
step:1508/1770 train_time:148151ms step_avg:98.90ms
step:1509/1770 train_time:148254ms step_avg:98.90ms
step:1510/1770 train_time:148356ms step_avg:98.90ms
step:1511/1770 train_time:148462ms step_avg:98.91ms
step:1512/1770 train_time:148566ms step_avg:98.91ms
step:1513/1770 train_time:148670ms step_avg:98.92ms
step:1514/1770 train_time:148773ms step_avg:98.92ms
step:1515/1770 train_time:148876ms step_avg:98.92ms
step:1516/1770 train_time:148980ms step_avg:98.92ms
step:1517/1770 train_time:149083ms step_avg:98.93ms
step:1518/1770 train_time:149188ms step_avg:98.93ms
step:1519/1770 train_time:149290ms step_avg:98.93ms
step:1520/1770 train_time:149395ms step_avg:98.94ms
step:1521/1770 train_time:149498ms step_avg:98.94ms
step:1522/1770 train_time:149601ms step_avg:98.94ms
step:1523/1770 train_time:149705ms step_avg:98.95ms
step:1524/1770 train_time:149809ms step_avg:98.95ms
step:1525/1770 train_time:149912ms step_avg:98.95ms
step:1526/1770 train_time:150014ms step_avg:98.95ms
step:1527/1770 train_time:150118ms step_avg:98.96ms
step:1528/1770 train_time:150223ms step_avg:98.96ms
step:1529/1770 train_time:150326ms step_avg:98.96ms
step:1530/1770 train_time:150430ms step_avg:98.97ms
step:1531/1770 train_time:150532ms step_avg:98.97ms
step:1532/1770 train_time:150636ms step_avg:98.97ms
step:1533/1770 train_time:150740ms step_avg:98.98ms
step:1534/1770 train_time:150845ms step_avg:98.98ms
step:1535/1770 train_time:150948ms step_avg:98.98ms
step:1536/1770 train_time:151051ms step_avg:98.98ms
step:1537/1770 train_time:151155ms step_avg:98.99ms
step:1538/1770 train_time:151260ms step_avg:98.99ms
step:1539/1770 train_time:151363ms step_avg:98.99ms
step:1540/1770 train_time:151469ms step_avg:99.00ms
step:1541/1770 train_time:151573ms step_avg:99.00ms
step:1542/1770 train_time:151677ms step_avg:99.01ms
step:1543/1770 train_time:151780ms step_avg:99.01ms
step:1544/1770 train_time:151885ms step_avg:99.01ms
step:1545/1770 train_time:151989ms step_avg:99.02ms
step:1546/1770 train_time:152093ms step_avg:99.02ms
step:1547/1770 train_time:152196ms step_avg:99.02ms
step:1548/1770 train_time:152299ms step_avg:99.02ms
step:1549/1770 train_time:152402ms step_avg:99.03ms
step:1550/1770 train_time:152506ms step_avg:99.03ms
step:1551/1770 train_time:152610ms step_avg:99.03ms
step:1552/1770 train_time:152715ms step_avg:99.04ms
step:1553/1770 train_time:152819ms step_avg:99.04ms
step:1554/1770 train_time:152922ms step_avg:99.04ms
step:1555/1770 train_time:153025ms step_avg:99.05ms
step:1556/1770 train_time:153129ms step_avg:99.05ms
step:1557/1770 train_time:153232ms step_avg:99.05ms
step:1558/1770 train_time:153335ms step_avg:99.05ms
step:1559/1770 train_time:153439ms step_avg:99.06ms
step:1560/1770 train_time:153543ms step_avg:99.06ms
step:1561/1770 train_time:153648ms step_avg:99.06ms
step:1562/1770 train_time:153752ms step_avg:99.07ms
step:1563/1770 train_time:153855ms step_avg:99.07ms
step:1564/1770 train_time:153958ms step_avg:99.07ms
step:1565/1770 train_time:154061ms step_avg:99.07ms
step:1566/1770 train_time:154165ms step_avg:99.08ms
step:1567/1770 train_time:154269ms step_avg:99.08ms
step:1568/1770 train_time:154372ms step_avg:99.08ms
step:1569/1770 train_time:154479ms step_avg:99.09ms
step:1570/1770 train_time:154582ms step_avg:99.09ms
step:1571/1770 train_time:154685ms step_avg:99.09ms
step:1572/1770 train_time:154790ms step_avg:99.10ms
step:1573/1770 train_time:154895ms step_avg:99.10ms
step:1574/1770 train_time:154998ms step_avg:99.10ms
step:1575/1770 train_time:155101ms step_avg:99.11ms
step:1576/1770 train_time:155204ms step_avg:99.11ms
step:1577/1770 train_time:155309ms step_avg:99.11ms
step:1578/1770 train_time:155414ms step_avg:99.12ms
step:1579/1770 train_time:155517ms step_avg:99.12ms
step:1580/1770 train_time:155620ms step_avg:99.12ms
step:1581/1770 train_time:155726ms step_avg:99.13ms
step:1582/1770 train_time:155831ms step_avg:99.13ms
step:1583/1770 train_time:155934ms step_avg:99.13ms
step:1584/1770 train_time:156038ms step_avg:99.13ms
step:1585/1770 train_time:156141ms step_avg:99.14ms
step:1586/1770 train_time:156248ms step_avg:99.14ms
step:1587/1770 train_time:156352ms step_avg:99.15ms
step:1588/1770 train_time:156456ms step_avg:99.15ms
step:1589/1770 train_time:156561ms step_avg:99.15ms
step:1590/1770 train_time:156664ms step_avg:99.15ms
step:1591/1770 train_time:156768ms step_avg:99.16ms
step:1592/1770 train_time:156872ms step_avg:99.16ms
step:1593/1770 train_time:156975ms step_avg:99.16ms
step:1594/1770 train_time:157078ms step_avg:99.17ms
step:1595/1770 train_time:157181ms step_avg:99.17ms
step:1596/1770 train_time:157285ms step_avg:99.17ms
step:1597/1770 train_time:157389ms step_avg:99.17ms
step:1598/1770 train_time:157491ms step_avg:99.18ms
step:1599/1770 train_time:157596ms step_avg:99.18ms
step:1600/1770 train_time:157702ms step_avg:99.18ms
step:1601/1770 train_time:157806ms step_avg:99.19ms
step:1602/1770 train_time:157911ms step_avg:99.19ms
step:1603/1770 train_time:158014ms step_avg:99.19ms
step:1604/1770 train_time:158117ms step_avg:99.19ms
step:1605/1770 train_time:158220ms step_avg:99.20ms
step:1606/1770 train_time:158323ms step_avg:99.20ms
step:1607/1770 train_time:158430ms step_avg:99.20ms
step:1608/1770 train_time:158534ms step_avg:99.21ms
step:1609/1770 train_time:158637ms step_avg:99.21ms
step:1610/1770 train_time:158741ms step_avg:99.21ms
step:1611/1770 train_time:158847ms step_avg:99.22ms
step:1612/1770 train_time:158952ms step_avg:99.22ms
step:1613/1770 train_time:159055ms step_avg:99.22ms
step:1614/1770 train_time:159158ms step_avg:99.23ms
step:1615/1770 train_time:159262ms step_avg:99.23ms
step:1616/1770 train_time:159365ms step_avg:99.23ms
step:1617/1770 train_time:159470ms step_avg:99.23ms
step:1618/1770 train_time:159574ms step_avg:99.24ms
step:1619/1770 train_time:159677ms step_avg:99.24ms
step:1620/1770 train_time:159782ms step_avg:99.24ms
step:1621/1770 train_time:159887ms step_avg:99.25ms
step:1622/1770 train_time:159991ms step_avg:99.25ms
step:1623/1770 train_time:160096ms step_avg:99.25ms
step:1624/1770 train_time:160199ms step_avg:99.26ms
step:1625/1770 train_time:160303ms step_avg:99.26ms
step:1625/1770 val_loss:3.3082 train_time:160405ms step_avg:99.32ms
step:1626/1770 train_time:160427ms step_avg:99.27ms
step:1627/1770 train_time:160514ms step_avg:99.27ms
step:1628/1770 train_time:160616ms step_avg:99.27ms
step:1629/1770 train_time:160719ms step_avg:99.27ms
step:1630/1770 train_time:160822ms step_avg:99.27ms
step:1631/1770 train_time:160925ms step_avg:99.27ms
step:1632/1770 train_time:161028ms step_avg:99.28ms
step:1633/1770 train_time:161131ms step_avg:99.28ms
step:1634/1770 train_time:161234ms step_avg:99.28ms
step:1635/1770 train_time:161337ms step_avg:99.28ms
step:1636/1770 train_time:161442ms step_avg:99.29ms
step:1637/1770 train_time:161547ms step_avg:99.29ms
step:1638/1770 train_time:161650ms step_avg:99.29ms
step:1639/1770 train_time:161754ms step_avg:99.30ms
step:1640/1770 train_time:161858ms step_avg:99.30ms
step:1641/1770 train_time:161961ms step_avg:99.30ms
step:1642/1770 train_time:162063ms step_avg:99.30ms
step:1643/1770 train_time:162168ms step_avg:99.31ms
step:1644/1770 train_time:162273ms step_avg:99.31ms
step:1645/1770 train_time:162377ms step_avg:99.31ms
step:1646/1770 train_time:162482ms step_avg:99.32ms
step:1647/1770 train_time:162587ms step_avg:99.32ms
step:1648/1770 train_time:162691ms step_avg:99.32ms
step:1649/1770 train_time:162795ms step_avg:99.33ms
step:1650/1770 train_time:162898ms step_avg:99.33ms
step:1651/1770 train_time:163000ms step_avg:99.33ms
step:1652/1770 train_time:163104ms step_avg:99.33ms
step:1653/1770 train_time:163207ms step_avg:99.33ms
step:1654/1770 train_time:163313ms step_avg:99.34ms
step:1655/1770 train_time:163419ms step_avg:99.34ms
step:1656/1770 train_time:163522ms step_avg:99.35ms
step:1657/1770 train_time:163628ms step_avg:99.35ms
step:1658/1770 train_time:163732ms step_avg:99.35ms
step:1659/1770 train_time:163838ms step_avg:99.36ms
step:1660/1770 train_time:163941ms step_avg:99.36ms
step:1661/1770 train_time:164045ms step_avg:99.36ms
step:1662/1770 train_time:164150ms step_avg:99.36ms
step:1663/1770 train_time:164252ms step_avg:99.37ms
step:1664/1770 train_time:164355ms step_avg:99.37ms
step:1665/1770 train_time:164458ms step_avg:99.37ms
step:1666/1770 train_time:164563ms step_avg:99.37ms
step:1667/1770 train_time:164666ms step_avg:99.38ms
step:1668/1770 train_time:164771ms step_avg:99.38ms
step:1669/1770 train_time:164873ms step_avg:99.38ms
step:1670/1770 train_time:164976ms step_avg:99.38ms
step:1671/1770 train_time:165079ms step_avg:99.39ms
step:1672/1770 train_time:165183ms step_avg:99.39ms
step:1673/1770 train_time:165289ms step_avg:99.39ms
step:1674/1770 train_time:165392ms step_avg:99.39ms
step:1675/1770 train_time:165494ms step_avg:99.40ms
step:1676/1770 train_time:165598ms step_avg:99.40ms
step:1677/1770 train_time:165705ms step_avg:99.40ms
step:1678/1770 train_time:165807ms step_avg:99.40ms
step:1679/1770 train_time:165911ms step_avg:99.41ms
step:1680/1770 train_time:166015ms step_avg:99.41ms
step:1681/1770 train_time:166118ms step_avg:99.41ms
step:1682/1770 train_time:166223ms step_avg:99.42ms
step:1683/1770 train_time:166326ms step_avg:99.42ms
step:1684/1770 train_time:166430ms step_avg:99.42ms
step:1685/1770 train_time:166533ms step_avg:99.42ms
step:1686/1770 train_time:166637ms step_avg:99.43ms
step:1687/1770 train_time:166742ms step_avg:99.43ms
step:1688/1770 train_time:166847ms step_avg:99.43ms
step:1689/1770 train_time:166950ms step_avg:99.43ms
step:1690/1770 train_time:167052ms step_avg:99.44ms
step:1691/1770 train_time:167156ms step_avg:99.44ms
step:1692/1770 train_time:167259ms step_avg:99.44ms
step:1693/1770 train_time:167363ms step_avg:99.44ms
step:1694/1770 train_time:167468ms step_avg:99.45ms
step:1695/1770 train_time:167572ms step_avg:99.45ms
step:1696/1770 train_time:167678ms step_avg:99.45ms
step:1697/1770 train_time:167783ms step_avg:99.46ms
step:1698/1770 train_time:167888ms step_avg:99.46ms
step:1699/1770 train_time:167991ms step_avg:99.46ms
step:1700/1770 train_time:168094ms step_avg:99.46ms
step:1701/1770 train_time:168197ms step_avg:99.47ms
step:1702/1770 train_time:168301ms step_avg:99.47ms
step:1703/1770 train_time:168404ms step_avg:99.47ms
step:1704/1770 train_time:168508ms step_avg:99.47ms
step:1705/1770 train_time:168612ms step_avg:99.48ms
step:1706/1770 train_time:168715ms step_avg:99.48ms
step:1707/1770 train_time:168819ms step_avg:99.48ms
step:1708/1770 train_time:168923ms step_avg:99.48ms
step:1709/1770 train_time:169029ms step_avg:99.49ms
step:1710/1770 train_time:169136ms step_avg:99.49ms
step:1711/1770 train_time:169241ms step_avg:99.50ms
step:1712/1770 train_time:169346ms step_avg:99.50ms
step:1713/1770 train_time:169449ms step_avg:99.50ms
step:1714/1770 train_time:169553ms step_avg:99.50ms
step:1715/1770 train_time:169656ms step_avg:99.50ms
step:1716/1770 train_time:169760ms step_avg:99.51ms
step:1717/1770 train_time:169864ms step_avg:99.51ms
step:1718/1770 train_time:169969ms step_avg:99.51ms
step:1719/1770 train_time:170075ms step_avg:99.52ms
step:1720/1770 train_time:170180ms step_avg:99.52ms
step:1721/1770 train_time:170283ms step_avg:99.52ms
step:1722/1770 train_time:170390ms step_avg:99.53ms
step:1723/1770 train_time:170495ms step_avg:99.53ms
step:1724/1770 train_time:170602ms step_avg:99.53ms
step:1725/1770 train_time:170708ms step_avg:99.54ms
step:1726/1770 train_time:170814ms step_avg:99.54ms
step:1727/1770 train_time:170917ms step_avg:99.54ms
step:1728/1770 train_time:171023ms step_avg:99.55ms
step:1729/1770 train_time:171127ms step_avg:99.55ms
step:1730/1770 train_time:171233ms step_avg:99.55ms
step:1731/1770 train_time:171338ms step_avg:99.56ms
step:1732/1770 train_time:171441ms step_avg:99.56ms
step:1733/1770 train_time:171547ms step_avg:99.56ms
step:1734/1770 train_time:171651ms step_avg:99.57ms
step:1735/1770 train_time:171756ms step_avg:99.57ms
step:1736/1770 train_time:171860ms step_avg:99.57ms
step:1737/1770 train_time:171965ms step_avg:99.57ms
step:1738/1770 train_time:172069ms step_avg:99.58ms
step:1739/1770 train_time:172174ms step_avg:99.58ms
step:1740/1770 train_time:172278ms step_avg:99.58ms
step:1741/1770 train_time:172385ms step_avg:99.59ms
step:1742/1770 train_time:172491ms step_avg:99.59ms
step:1743/1770 train_time:172596ms step_avg:99.59ms
step:1744/1770 train_time:172700ms step_avg:99.60ms
step:1745/1770 train_time:172803ms step_avg:99.60ms
step:1746/1770 train_time:172910ms step_avg:99.60ms
step:1747/1770 train_time:173013ms step_avg:99.60ms
step:1748/1770 train_time:173120ms step_avg:99.61ms
step:1749/1770 train_time:173226ms step_avg:99.61ms
step:1750/1770 train_time:173329ms step_avg:99.61ms
step:1750/1770 val_loss:3.2812 train_time:173432ms step_avg:99.67ms
step:1751/1770 train_time:173456ms step_avg:99.63ms
step:1752/1770 train_time:173541ms step_avg:99.62ms
step:1753/1770 train_time:173645ms step_avg:99.62ms
step:1754/1770 train_time:173749ms step_avg:99.63ms
step:1755/1770 train_time:173853ms step_avg:99.63ms
step:1756/1770 train_time:173958ms step_avg:99.63ms
step:1757/1770 train_time:174062ms step_avg:99.63ms
step:1758/1770 train_time:174166ms step_avg:99.64ms
step:1759/1770 train_time:174271ms step_avg:99.64ms
step:1760/1770 train_time:174375ms step_avg:99.64ms
step:1761/1770 train_time:174481ms step_avg:99.65ms
step:1762/1770 train_time:174589ms step_avg:99.65ms
step:1763/1770 train_time:174692ms step_avg:99.65ms
step:1764/1770 train_time:174797ms step_avg:99.66ms
step:1765/1770 train_time:174903ms step_avg:99.66ms
step:1766/1770 train_time:175011ms step_avg:99.66ms
step:1767/1770 train_time:175113ms step_avg:99.67ms
step:1768/1770 train_time:175218ms step_avg:99.67ms
step:1769/1770 train_time:175322ms step_avg:99.67ms
step:1770/1770 train_time:175425ms step_avg:99.67ms
step:1770/1770 val_loss:3.2782 train_time:175530ms step_avg:99.73ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
