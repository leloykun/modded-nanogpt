import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 forward & backward by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# Custom operators : FP8 forward & bfloat16 backward

@torch.library.custom_op("nanogpt::mm_mixed", mutates_args=())
def mm_mixed_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_mixed_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_mixed_backward", mutates_args=())
def mm_mixed_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        x_bfloat16 = x_f8.to(torch.bfloat16)
        w_bfloat16 = w_f8.to(torch.bfloat16)
        grad_bfloat16 = grad.mul(grad_s).to(torch.bfloat16)
        grad_x = torch._scaled_mm(
            grad_bfloat16,
            w_bfloat16.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        grad_w = torch._scaled_mm(
            x_bfloat16.t().contiguous(),
            grad_bfloat16.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_mixed_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_mixed_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_mixed_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_mixed_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_mixed_op.register_autograd(mm_mixed_backward, setup_context=mm_mixed_setup_context)

def linear_mixed(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm_mixed(_x, w, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 22:35:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24247ms step_avg:nanms
step:2/1770 train_time:24712ms step_avg:nanms
step:3/1770 train_time:24806ms step_avg:nanms
step:4/1770 train_time:24899ms step_avg:nanms
step:5/1770 train_time:24993ms step_avg:nanms
step:6/1770 train_time:25088ms step_avg:nanms
step:7/1770 train_time:25182ms step_avg:nanms
step:8/1770 train_time:25276ms step_avg:nanms
step:9/1770 train_time:25371ms step_avg:nanms
step:10/1770 train_time:25465ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.63ms
step:14/1770 train_time:379ms step_avg:94.71ms
step:15/1770 train_time:473ms step_avg:94.65ms
step:16/1770 train_time:568ms step_avg:94.71ms
step:17/1770 train_time:663ms step_avg:94.77ms
step:18/1770 train_time:758ms step_avg:94.79ms
step:19/1770 train_time:853ms step_avg:94.72ms
step:20/1770 train_time:947ms step_avg:94.73ms
step:21/1770 train_time:1042ms step_avg:94.75ms
step:22/1770 train_time:1137ms step_avg:94.71ms
step:23/1770 train_time:1231ms step_avg:94.69ms
step:24/1770 train_time:1326ms step_avg:94.72ms
step:25/1770 train_time:1421ms step_avg:94.72ms
step:26/1770 train_time:1515ms step_avg:94.71ms
step:27/1770 train_time:1610ms step_avg:94.68ms
step:28/1770 train_time:1705ms step_avg:94.70ms
step:29/1770 train_time:1799ms step_avg:94.70ms
step:30/1770 train_time:1893ms step_avg:94.67ms
step:31/1770 train_time:1989ms step_avg:94.69ms
step:32/1770 train_time:2084ms step_avg:94.71ms
step:33/1770 train_time:2178ms step_avg:94.72ms
step:34/1770 train_time:2273ms step_avg:94.70ms
step:35/1770 train_time:2367ms step_avg:94.70ms
step:36/1770 train_time:2463ms step_avg:94.72ms
step:37/1770 train_time:2557ms step_avg:94.70ms
step:38/1770 train_time:2652ms step_avg:94.70ms
step:39/1770 train_time:2747ms step_avg:94.72ms
step:40/1770 train_time:2842ms step_avg:94.73ms
step:41/1770 train_time:2936ms step_avg:94.72ms
step:42/1770 train_time:3031ms step_avg:94.71ms
step:43/1770 train_time:3126ms step_avg:94.73ms
step:44/1770 train_time:3221ms step_avg:94.73ms
step:45/1770 train_time:3315ms step_avg:94.72ms
step:46/1770 train_time:3410ms step_avg:94.72ms
step:47/1770 train_time:3505ms step_avg:94.73ms
step:48/1770 train_time:3600ms step_avg:94.73ms
step:49/1770 train_time:3694ms step_avg:94.72ms
step:50/1770 train_time:3789ms step_avg:94.72ms
step:51/1770 train_time:3884ms step_avg:94.73ms
step:52/1770 train_time:3979ms step_avg:94.74ms
step:53/1770 train_time:4073ms step_avg:94.73ms
step:54/1770 train_time:4168ms step_avg:94.73ms
step:55/1770 train_time:4263ms step_avg:94.73ms
step:56/1770 train_time:4357ms step_avg:94.72ms
step:57/1770 train_time:4452ms step_avg:94.73ms
step:58/1770 train_time:4547ms step_avg:94.74ms
step:59/1770 train_time:4642ms step_avg:94.74ms
step:60/1770 train_time:4736ms step_avg:94.73ms
step:61/1770 train_time:4831ms step_avg:94.72ms
step:62/1770 train_time:4926ms step_avg:94.73ms
step:63/1770 train_time:5021ms step_avg:94.73ms
step:64/1770 train_time:5115ms step_avg:94.72ms
step:65/1770 train_time:5209ms step_avg:94.71ms
step:66/1770 train_time:5304ms step_avg:94.72ms
step:67/1770 train_time:5398ms step_avg:94.71ms
step:68/1770 train_time:5493ms step_avg:94.70ms
step:69/1770 train_time:5588ms step_avg:94.71ms
step:70/1770 train_time:5683ms step_avg:94.71ms
step:71/1770 train_time:5777ms step_avg:94.70ms
step:72/1770 train_time:5871ms step_avg:94.70ms
step:73/1770 train_time:5966ms step_avg:94.70ms
step:74/1770 train_time:6062ms step_avg:94.71ms
step:75/1770 train_time:6156ms step_avg:94.70ms
step:76/1770 train_time:6250ms step_avg:94.70ms
step:77/1770 train_time:6345ms step_avg:94.71ms
step:78/1770 train_time:6440ms step_avg:94.70ms
step:79/1770 train_time:6534ms step_avg:94.69ms
step:80/1770 train_time:6628ms step_avg:94.69ms
step:81/1770 train_time:6724ms step_avg:94.70ms
step:82/1770 train_time:6819ms step_avg:94.71ms
step:83/1770 train_time:6913ms step_avg:94.70ms
step:84/1770 train_time:7008ms step_avg:94.70ms
step:85/1770 train_time:7103ms step_avg:94.70ms
step:86/1770 train_time:7197ms step_avg:94.70ms
step:87/1770 train_time:7291ms step_avg:94.69ms
step:88/1770 train_time:7386ms step_avg:94.69ms
step:89/1770 train_time:7481ms step_avg:94.70ms
step:90/1770 train_time:7575ms step_avg:94.69ms
step:91/1770 train_time:7670ms step_avg:94.69ms
step:92/1770 train_time:7765ms step_avg:94.69ms
step:93/1770 train_time:7859ms step_avg:94.69ms
step:94/1770 train_time:7954ms step_avg:94.69ms
step:95/1770 train_time:8048ms step_avg:94.69ms
step:96/1770 train_time:8143ms step_avg:94.69ms
step:97/1770 train_time:8238ms step_avg:94.69ms
step:98/1770 train_time:8333ms step_avg:94.69ms
step:99/1770 train_time:8427ms step_avg:94.69ms
step:100/1770 train_time:8523ms step_avg:94.69ms
step:101/1770 train_time:8616ms step_avg:94.69ms
step:102/1770 train_time:8711ms step_avg:94.69ms
step:103/1770 train_time:8806ms step_avg:94.69ms
step:104/1770 train_time:8901ms step_avg:94.69ms
step:105/1770 train_time:8995ms step_avg:94.68ms
step:106/1770 train_time:9089ms step_avg:94.68ms
step:107/1770 train_time:9184ms step_avg:94.68ms
step:108/1770 train_time:9279ms step_avg:94.68ms
step:109/1770 train_time:9373ms step_avg:94.68ms
step:110/1770 train_time:9468ms step_avg:94.68ms
step:111/1770 train_time:9563ms step_avg:94.68ms
step:112/1770 train_time:9657ms step_avg:94.68ms
step:113/1770 train_time:9752ms step_avg:94.68ms
step:114/1770 train_time:9846ms step_avg:94.68ms
step:115/1770 train_time:9941ms step_avg:94.68ms
step:116/1770 train_time:10035ms step_avg:94.67ms
step:117/1770 train_time:10130ms step_avg:94.67ms
step:118/1770 train_time:10225ms step_avg:94.67ms
step:119/1770 train_time:10319ms step_avg:94.67ms
step:120/1770 train_time:10413ms step_avg:94.67ms
step:121/1770 train_time:10508ms step_avg:94.66ms
step:122/1770 train_time:10602ms step_avg:94.66ms
step:123/1770 train_time:10696ms step_avg:94.66ms
step:124/1770 train_time:10791ms step_avg:94.66ms
step:125/1770 train_time:10886ms step_avg:94.66ms
step:125/1770 val_loss:4.6561 train_time:10978ms step_avg:95.46ms
step:126/1770 train_time:11001ms step_avg:94.83ms
step:127/1770 train_time:11077ms step_avg:94.67ms
step:128/1770 train_time:11172ms step_avg:94.68ms
step:129/1770 train_time:11267ms step_avg:94.68ms
step:130/1770 train_time:11361ms step_avg:94.68ms
step:131/1770 train_time:11456ms step_avg:94.68ms
step:132/1770 train_time:11550ms step_avg:94.68ms
step:133/1770 train_time:11645ms step_avg:94.67ms
step:134/1770 train_time:11740ms step_avg:94.68ms
step:135/1770 train_time:11835ms step_avg:94.68ms
step:136/1770 train_time:11931ms step_avg:94.69ms
step:137/1770 train_time:12026ms step_avg:94.69ms
step:138/1770 train_time:12121ms step_avg:94.70ms
step:139/1770 train_time:12216ms step_avg:94.70ms
step:140/1770 train_time:12311ms step_avg:94.70ms
step:141/1770 train_time:12406ms step_avg:94.70ms
step:142/1770 train_time:12501ms step_avg:94.71ms
step:143/1770 train_time:12597ms step_avg:94.71ms
step:144/1770 train_time:12692ms step_avg:94.71ms
step:145/1770 train_time:12787ms step_avg:94.72ms
step:146/1770 train_time:12882ms step_avg:94.72ms
step:147/1770 train_time:12978ms step_avg:94.73ms
step:148/1770 train_time:13073ms step_avg:94.73ms
step:149/1770 train_time:13168ms step_avg:94.73ms
step:150/1770 train_time:13263ms step_avg:94.74ms
step:151/1770 train_time:13359ms step_avg:94.74ms
step:152/1770 train_time:13454ms step_avg:94.75ms
step:153/1770 train_time:13549ms step_avg:94.75ms
step:154/1770 train_time:13644ms step_avg:94.75ms
step:155/1770 train_time:13740ms step_avg:94.76ms
step:156/1770 train_time:13835ms step_avg:94.76ms
step:157/1770 train_time:13930ms step_avg:94.76ms
step:158/1770 train_time:14025ms step_avg:94.76ms
step:159/1770 train_time:14120ms step_avg:94.77ms
step:160/1770 train_time:14216ms step_avg:94.77ms
step:161/1770 train_time:14311ms step_avg:94.78ms
step:162/1770 train_time:14406ms step_avg:94.78ms
step:163/1770 train_time:14502ms step_avg:94.78ms
step:164/1770 train_time:14597ms step_avg:94.79ms
step:165/1770 train_time:14692ms step_avg:94.79ms
step:166/1770 train_time:14787ms step_avg:94.79ms
step:167/1770 train_time:14883ms step_avg:94.79ms
step:168/1770 train_time:14978ms step_avg:94.80ms
step:169/1770 train_time:15074ms step_avg:94.80ms
step:170/1770 train_time:15168ms step_avg:94.80ms
step:171/1770 train_time:15264ms step_avg:94.80ms
step:172/1770 train_time:15359ms step_avg:94.81ms
step:173/1770 train_time:15454ms step_avg:94.81ms
step:174/1770 train_time:15549ms step_avg:94.81ms
step:175/1770 train_time:15644ms step_avg:94.81ms
step:176/1770 train_time:15740ms step_avg:94.82ms
step:177/1770 train_time:15835ms step_avg:94.82ms
step:178/1770 train_time:15930ms step_avg:94.82ms
step:179/1770 train_time:16025ms step_avg:94.82ms
step:180/1770 train_time:16120ms step_avg:94.83ms
step:181/1770 train_time:16217ms step_avg:94.83ms
step:182/1770 train_time:16312ms step_avg:94.84ms
step:183/1770 train_time:16406ms step_avg:94.83ms
step:184/1770 train_time:16502ms step_avg:94.84ms
step:185/1770 train_time:16597ms step_avg:94.84ms
step:186/1770 train_time:16692ms step_avg:94.84ms
step:187/1770 train_time:16787ms step_avg:94.84ms
step:188/1770 train_time:16882ms step_avg:94.84ms
step:189/1770 train_time:16978ms step_avg:94.85ms
step:190/1770 train_time:17073ms step_avg:94.85ms
step:191/1770 train_time:17168ms step_avg:94.85ms
step:192/1770 train_time:17262ms step_avg:94.85ms
step:193/1770 train_time:17358ms step_avg:94.85ms
step:194/1770 train_time:17453ms step_avg:94.85ms
step:195/1770 train_time:17548ms step_avg:94.85ms
step:196/1770 train_time:17643ms step_avg:94.86ms
step:197/1770 train_time:17739ms step_avg:94.86ms
step:198/1770 train_time:17834ms step_avg:94.86ms
step:199/1770 train_time:17929ms step_avg:94.86ms
step:200/1770 train_time:18025ms step_avg:94.87ms
step:201/1770 train_time:18120ms step_avg:94.87ms
step:202/1770 train_time:18215ms step_avg:94.87ms
step:203/1770 train_time:18310ms step_avg:94.87ms
step:204/1770 train_time:18405ms step_avg:94.87ms
step:205/1770 train_time:18501ms step_avg:94.87ms
step:206/1770 train_time:18596ms step_avg:94.88ms
step:207/1770 train_time:18691ms step_avg:94.88ms
step:208/1770 train_time:18786ms step_avg:94.88ms
step:209/1770 train_time:18881ms step_avg:94.88ms
step:210/1770 train_time:18977ms step_avg:94.88ms
step:211/1770 train_time:19072ms step_avg:94.89ms
step:212/1770 train_time:19167ms step_avg:94.89ms
step:213/1770 train_time:19262ms step_avg:94.89ms
step:214/1770 train_time:19358ms step_avg:94.89ms
step:215/1770 train_time:19452ms step_avg:94.89ms
step:216/1770 train_time:19547ms step_avg:94.89ms
step:217/1770 train_time:19642ms step_avg:94.89ms
step:218/1770 train_time:19737ms step_avg:94.89ms
step:219/1770 train_time:19833ms step_avg:94.89ms
step:220/1770 train_time:19927ms step_avg:94.89ms
step:221/1770 train_time:20022ms step_avg:94.89ms
step:222/1770 train_time:20118ms step_avg:94.89ms
step:223/1770 train_time:20213ms step_avg:94.90ms
step:224/1770 train_time:20308ms step_avg:94.90ms
step:225/1770 train_time:20403ms step_avg:94.90ms
step:226/1770 train_time:20499ms step_avg:94.90ms
step:227/1770 train_time:20594ms step_avg:94.90ms
step:228/1770 train_time:20689ms step_avg:94.90ms
step:229/1770 train_time:20784ms step_avg:94.90ms
step:230/1770 train_time:20879ms step_avg:94.91ms
step:231/1770 train_time:20975ms step_avg:94.91ms
step:232/1770 train_time:21069ms step_avg:94.91ms
step:233/1770 train_time:21165ms step_avg:94.91ms
step:234/1770 train_time:21260ms step_avg:94.91ms
step:235/1770 train_time:21355ms step_avg:94.91ms
step:236/1770 train_time:21450ms step_avg:94.91ms
step:237/1770 train_time:21545ms step_avg:94.91ms
step:238/1770 train_time:21640ms step_avg:94.91ms
step:239/1770 train_time:21736ms step_avg:94.92ms
step:240/1770 train_time:21830ms step_avg:94.91ms
step:241/1770 train_time:21925ms step_avg:94.91ms
step:242/1770 train_time:22021ms step_avg:94.92ms
step:243/1770 train_time:22117ms step_avg:94.92ms
step:244/1770 train_time:22213ms step_avg:94.93ms
step:245/1770 train_time:22307ms step_avg:94.93ms
step:246/1770 train_time:22403ms step_avg:94.93ms
step:247/1770 train_time:22498ms step_avg:94.93ms
step:248/1770 train_time:22593ms step_avg:94.93ms
step:249/1770 train_time:22688ms step_avg:94.93ms
step:250/1770 train_time:22783ms step_avg:94.93ms
step:250/1770 val_loss:4.1125 train_time:22877ms step_avg:95.32ms
step:251/1770 train_time:22902ms step_avg:95.03ms
step:252/1770 train_time:22982ms step_avg:94.97ms
step:253/1770 train_time:23078ms step_avg:94.97ms
step:254/1770 train_time:23173ms step_avg:94.97ms
step:255/1770 train_time:23269ms step_avg:94.97ms
step:256/1770 train_time:23363ms step_avg:94.97ms
step:257/1770 train_time:23458ms step_avg:94.97ms
step:258/1770 train_time:23553ms step_avg:94.97ms
step:259/1770 train_time:23649ms step_avg:94.98ms
step:260/1770 train_time:23744ms step_avg:94.98ms
step:261/1770 train_time:23839ms step_avg:94.98ms
step:262/1770 train_time:23934ms step_avg:94.98ms
step:263/1770 train_time:24030ms step_avg:94.98ms
step:264/1770 train_time:24126ms step_avg:94.99ms
step:265/1770 train_time:24221ms step_avg:94.99ms
step:266/1770 train_time:24317ms step_avg:94.99ms
step:267/1770 train_time:24413ms step_avg:94.99ms
step:268/1770 train_time:24509ms step_avg:95.00ms
step:269/1770 train_time:24604ms step_avg:95.00ms
step:270/1770 train_time:24700ms step_avg:95.00ms
step:271/1770 train_time:24796ms step_avg:95.00ms
step:272/1770 train_time:24893ms step_avg:95.01ms
step:273/1770 train_time:24989ms step_avg:95.02ms
step:274/1770 train_time:25085ms step_avg:95.02ms
step:275/1770 train_time:25181ms step_avg:95.02ms
step:276/1770 train_time:25276ms step_avg:95.02ms
step:277/1770 train_time:25372ms step_avg:95.03ms
step:278/1770 train_time:25468ms step_avg:95.03ms
step:279/1770 train_time:25564ms step_avg:95.03ms
step:280/1770 train_time:25659ms step_avg:95.03ms
step:281/1770 train_time:25755ms step_avg:95.04ms
step:282/1770 train_time:25851ms step_avg:95.04ms
step:283/1770 train_time:25948ms step_avg:95.05ms
step:284/1770 train_time:26043ms step_avg:95.05ms
step:285/1770 train_time:26138ms step_avg:95.05ms
step:286/1770 train_time:26234ms step_avg:95.05ms
step:287/1770 train_time:26331ms step_avg:95.06ms
step:288/1770 train_time:26426ms step_avg:95.06ms
step:289/1770 train_time:26522ms step_avg:95.06ms
step:290/1770 train_time:26617ms step_avg:95.06ms
step:291/1770 train_time:26713ms step_avg:95.06ms
step:292/1770 train_time:26809ms step_avg:95.07ms
step:293/1770 train_time:26905ms step_avg:95.07ms
step:294/1770 train_time:27000ms step_avg:95.07ms
step:295/1770 train_time:27096ms step_avg:95.07ms
step:296/1770 train_time:27192ms step_avg:95.08ms
step:297/1770 train_time:27288ms step_avg:95.08ms
step:298/1770 train_time:27383ms step_avg:95.08ms
step:299/1770 train_time:27479ms step_avg:95.08ms
step:300/1770 train_time:27574ms step_avg:95.08ms
step:301/1770 train_time:27670ms step_avg:95.09ms
step:302/1770 train_time:27765ms step_avg:95.09ms
step:303/1770 train_time:27861ms step_avg:95.09ms
step:304/1770 train_time:27956ms step_avg:95.09ms
step:305/1770 train_time:28053ms step_avg:95.09ms
step:306/1770 train_time:28149ms step_avg:95.10ms
step:307/1770 train_time:28244ms step_avg:95.10ms
step:308/1770 train_time:28340ms step_avg:95.10ms
step:309/1770 train_time:28435ms step_avg:95.10ms
step:310/1770 train_time:28531ms step_avg:95.10ms
step:311/1770 train_time:28627ms step_avg:95.11ms
step:312/1770 train_time:28722ms step_avg:95.11ms
step:313/1770 train_time:28818ms step_avg:95.11ms
step:314/1770 train_time:28914ms step_avg:95.11ms
step:315/1770 train_time:29010ms step_avg:95.11ms
step:316/1770 train_time:29105ms step_avg:95.12ms
step:317/1770 train_time:29201ms step_avg:95.12ms
step:318/1770 train_time:29296ms step_avg:95.12ms
step:319/1770 train_time:29392ms step_avg:95.12ms
step:320/1770 train_time:29488ms step_avg:95.12ms
step:321/1770 train_time:29584ms step_avg:95.12ms
step:322/1770 train_time:29679ms step_avg:95.12ms
step:323/1770 train_time:29774ms step_avg:95.13ms
step:324/1770 train_time:29870ms step_avg:95.13ms
step:325/1770 train_time:29966ms step_avg:95.13ms
step:326/1770 train_time:30062ms step_avg:95.13ms
step:327/1770 train_time:30158ms step_avg:95.13ms
step:328/1770 train_time:30254ms step_avg:95.14ms
step:329/1770 train_time:30349ms step_avg:95.14ms
step:330/1770 train_time:30445ms step_avg:95.14ms
step:331/1770 train_time:30541ms step_avg:95.14ms
step:332/1770 train_time:30636ms step_avg:95.14ms
step:333/1770 train_time:30733ms step_avg:95.15ms
step:334/1770 train_time:30828ms step_avg:95.15ms
step:335/1770 train_time:30924ms step_avg:95.15ms
step:336/1770 train_time:31020ms step_avg:95.15ms
step:337/1770 train_time:31115ms step_avg:95.15ms
step:338/1770 train_time:31212ms step_avg:95.16ms
step:339/1770 train_time:31308ms step_avg:95.16ms
step:340/1770 train_time:31403ms step_avg:95.16ms
step:341/1770 train_time:31498ms step_avg:95.16ms
step:342/1770 train_time:31594ms step_avg:95.16ms
step:343/1770 train_time:31690ms step_avg:95.16ms
step:344/1770 train_time:31785ms step_avg:95.17ms
step:345/1770 train_time:31880ms step_avg:95.17ms
step:346/1770 train_time:31976ms step_avg:95.17ms
step:347/1770 train_time:32072ms step_avg:95.17ms
step:348/1770 train_time:32168ms step_avg:95.17ms
step:349/1770 train_time:32263ms step_avg:95.17ms
step:350/1770 train_time:32359ms step_avg:95.17ms
step:351/1770 train_time:32454ms step_avg:95.17ms
step:352/1770 train_time:32550ms step_avg:95.18ms
step:353/1770 train_time:32646ms step_avg:95.18ms
step:354/1770 train_time:32742ms step_avg:95.18ms
step:355/1770 train_time:32838ms step_avg:95.18ms
step:356/1770 train_time:32933ms step_avg:95.18ms
step:357/1770 train_time:33029ms step_avg:95.19ms
step:358/1770 train_time:33125ms step_avg:95.19ms
step:359/1770 train_time:33220ms step_avg:95.19ms
step:360/1770 train_time:33316ms step_avg:95.19ms
step:361/1770 train_time:33412ms step_avg:95.19ms
step:362/1770 train_time:33508ms step_avg:95.19ms
step:363/1770 train_time:33603ms step_avg:95.19ms
step:364/1770 train_time:33699ms step_avg:95.19ms
step:365/1770 train_time:33794ms step_avg:95.19ms
step:366/1770 train_time:33890ms step_avg:95.20ms
step:367/1770 train_time:33986ms step_avg:95.20ms
step:368/1770 train_time:34081ms step_avg:95.20ms
step:369/1770 train_time:34176ms step_avg:95.20ms
step:370/1770 train_time:34272ms step_avg:95.20ms
step:371/1770 train_time:34368ms step_avg:95.20ms
step:372/1770 train_time:34464ms step_avg:95.20ms
step:373/1770 train_time:34559ms step_avg:95.20ms
step:374/1770 train_time:34655ms step_avg:95.21ms
step:375/1770 train_time:34751ms step_avg:95.21ms
step:375/1770 val_loss:3.9048 train_time:34845ms step_avg:95.47ms
step:376/1770 train_time:34869ms step_avg:95.27ms
step:377/1770 train_time:34951ms step_avg:95.23ms
step:378/1770 train_time:35048ms step_avg:95.24ms
step:379/1770 train_time:35143ms step_avg:95.24ms
step:380/1770 train_time:35239ms step_avg:95.24ms
step:381/1770 train_time:35335ms step_avg:95.24ms
step:382/1770 train_time:35431ms step_avg:95.24ms
step:383/1770 train_time:35526ms step_avg:95.24ms
step:384/1770 train_time:35622ms step_avg:95.25ms
step:385/1770 train_time:35718ms step_avg:95.25ms
step:386/1770 train_time:35813ms step_avg:95.25ms
step:387/1770 train_time:35909ms step_avg:95.25ms
step:388/1770 train_time:36005ms step_avg:95.25ms
step:389/1770 train_time:36101ms step_avg:95.25ms
step:390/1770 train_time:36197ms step_avg:95.25ms
step:391/1770 train_time:36293ms step_avg:95.26ms
step:392/1770 train_time:36388ms step_avg:95.26ms
step:393/1770 train_time:36484ms step_avg:95.26ms
step:394/1770 train_time:36579ms step_avg:95.26ms
step:395/1770 train_time:36675ms step_avg:95.26ms
step:396/1770 train_time:36773ms step_avg:95.27ms
step:397/1770 train_time:36870ms step_avg:95.27ms
step:398/1770 train_time:36968ms step_avg:95.28ms
step:399/1770 train_time:37065ms step_avg:95.28ms
step:400/1770 train_time:37163ms step_avg:95.29ms
step:401/1770 train_time:37260ms step_avg:95.29ms
step:402/1770 train_time:37359ms step_avg:95.30ms
step:403/1770 train_time:37457ms step_avg:95.31ms
step:404/1770 train_time:37555ms step_avg:95.32ms
step:405/1770 train_time:37653ms step_avg:95.32ms
step:406/1770 train_time:37751ms step_avg:95.33ms
step:407/1770 train_time:37848ms step_avg:95.34ms
step:408/1770 train_time:37945ms step_avg:95.34ms
step:409/1770 train_time:38042ms step_avg:95.34ms
step:410/1770 train_time:38140ms step_avg:95.35ms
step:411/1770 train_time:38238ms step_avg:95.36ms
step:412/1770 train_time:38336ms step_avg:95.36ms
step:413/1770 train_time:38434ms step_avg:95.37ms
step:414/1770 train_time:38531ms step_avg:95.37ms
step:415/1770 train_time:38629ms step_avg:95.38ms
step:416/1770 train_time:38726ms step_avg:95.38ms
step:417/1770 train_time:38824ms step_avg:95.39ms
step:418/1770 train_time:38922ms step_avg:95.40ms
step:419/1770 train_time:39020ms step_avg:95.40ms
step:420/1770 train_time:39118ms step_avg:95.41ms
step:421/1770 train_time:39216ms step_avg:95.42ms
step:422/1770 train_time:39314ms step_avg:95.42ms
step:423/1770 train_time:39411ms step_avg:95.43ms
step:424/1770 train_time:39508ms step_avg:95.43ms
step:425/1770 train_time:39605ms step_avg:95.43ms
step:426/1770 train_time:39703ms step_avg:95.44ms
step:427/1770 train_time:39801ms step_avg:95.45ms
step:428/1770 train_time:39898ms step_avg:95.45ms
step:429/1770 train_time:39997ms step_avg:95.46ms
step:430/1770 train_time:40096ms step_avg:95.47ms
step:431/1770 train_time:40194ms step_avg:95.47ms
step:432/1770 train_time:40292ms step_avg:95.48ms
step:433/1770 train_time:40389ms step_avg:95.48ms
step:434/1770 train_time:40486ms step_avg:95.49ms
step:435/1770 train_time:40584ms step_avg:95.49ms
step:436/1770 train_time:40681ms step_avg:95.50ms
step:437/1770 train_time:40779ms step_avg:95.50ms
step:438/1770 train_time:40878ms step_avg:95.51ms
step:439/1770 train_time:40976ms step_avg:95.51ms
step:440/1770 train_time:41073ms step_avg:95.52ms
step:441/1770 train_time:41171ms step_avg:95.52ms
step:442/1770 train_time:41268ms step_avg:95.53ms
step:443/1770 train_time:41365ms step_avg:95.53ms
step:444/1770 train_time:41463ms step_avg:95.54ms
step:445/1770 train_time:41561ms step_avg:95.54ms
step:446/1770 train_time:41658ms step_avg:95.55ms
step:447/1770 train_time:41757ms step_avg:95.55ms
step:448/1770 train_time:41855ms step_avg:95.56ms
step:449/1770 train_time:41953ms step_avg:95.57ms
step:450/1770 train_time:42051ms step_avg:95.57ms
step:451/1770 train_time:42149ms step_avg:95.58ms
step:452/1770 train_time:42246ms step_avg:95.58ms
step:453/1770 train_time:42343ms step_avg:95.58ms
step:454/1770 train_time:42441ms step_avg:95.59ms
step:455/1770 train_time:42539ms step_avg:95.59ms
step:456/1770 train_time:42638ms step_avg:95.60ms
step:457/1770 train_time:42735ms step_avg:95.60ms
step:458/1770 train_time:42833ms step_avg:95.61ms
step:459/1770 train_time:42930ms step_avg:95.61ms
step:460/1770 train_time:43027ms step_avg:95.62ms
step:461/1770 train_time:43125ms step_avg:95.62ms
step:462/1770 train_time:43222ms step_avg:95.62ms
step:463/1770 train_time:43320ms step_avg:95.63ms
step:464/1770 train_time:43417ms step_avg:95.63ms
step:465/1770 train_time:43515ms step_avg:95.64ms
step:466/1770 train_time:43612ms step_avg:95.64ms
step:467/1770 train_time:43709ms step_avg:95.64ms
step:468/1770 train_time:43806ms step_avg:95.65ms
step:469/1770 train_time:43903ms step_avg:95.65ms
step:470/1770 train_time:44001ms step_avg:95.65ms
step:471/1770 train_time:44099ms step_avg:95.66ms
step:472/1770 train_time:44197ms step_avg:95.66ms
step:473/1770 train_time:44295ms step_avg:95.67ms
step:474/1770 train_time:44393ms step_avg:95.67ms
step:475/1770 train_time:44491ms step_avg:95.68ms
step:476/1770 train_time:44588ms step_avg:95.68ms
step:477/1770 train_time:44685ms step_avg:95.69ms
step:478/1770 train_time:44783ms step_avg:95.69ms
step:479/1770 train_time:44880ms step_avg:95.69ms
step:480/1770 train_time:44978ms step_avg:95.70ms
step:481/1770 train_time:45076ms step_avg:95.70ms
step:482/1770 train_time:45173ms step_avg:95.70ms
step:483/1770 train_time:45270ms step_avg:95.71ms
step:484/1770 train_time:45368ms step_avg:95.71ms
step:485/1770 train_time:45465ms step_avg:95.72ms
step:486/1770 train_time:45563ms step_avg:95.72ms
step:487/1770 train_time:45661ms step_avg:95.72ms
step:488/1770 train_time:45759ms step_avg:95.73ms
step:489/1770 train_time:45857ms step_avg:95.73ms
step:490/1770 train_time:45954ms step_avg:95.74ms
step:491/1770 train_time:46052ms step_avg:95.74ms
step:492/1770 train_time:46149ms step_avg:95.75ms
step:493/1770 train_time:46247ms step_avg:95.75ms
step:494/1770 train_time:46344ms step_avg:95.75ms
step:495/1770 train_time:46442ms step_avg:95.76ms
step:496/1770 train_time:46539ms step_avg:95.76ms
step:497/1770 train_time:46637ms step_avg:95.76ms
step:498/1770 train_time:46735ms step_avg:95.77ms
step:499/1770 train_time:46832ms step_avg:95.77ms
step:500/1770 train_time:46929ms step_avg:95.77ms
step:500/1770 val_loss:3.7547 train_time:47025ms step_avg:95.97ms
step:501/1770 train_time:47049ms step_avg:95.82ms
step:502/1770 train_time:47135ms step_avg:95.80ms
step:503/1770 train_time:47235ms step_avg:95.81ms
step:504/1770 train_time:47333ms step_avg:95.82ms
step:505/1770 train_time:47431ms step_avg:95.82ms
step:506/1770 train_time:47528ms step_avg:95.82ms
step:507/1770 train_time:47625ms step_avg:95.82ms
step:508/1770 train_time:47722ms step_avg:95.83ms
step:509/1770 train_time:47819ms step_avg:95.83ms
step:510/1770 train_time:47917ms step_avg:95.83ms
step:511/1770 train_time:48015ms step_avg:95.84ms
step:512/1770 train_time:48113ms step_avg:95.84ms
step:513/1770 train_time:48210ms step_avg:95.85ms
step:514/1770 train_time:48308ms step_avg:95.85ms
step:515/1770 train_time:48405ms step_avg:95.85ms
step:516/1770 train_time:48503ms step_avg:95.86ms
step:517/1770 train_time:48600ms step_avg:95.86ms
step:518/1770 train_time:48698ms step_avg:95.86ms
step:519/1770 train_time:48795ms step_avg:95.87ms
step:520/1770 train_time:48893ms step_avg:95.87ms
step:521/1770 train_time:48991ms step_avg:95.87ms
step:522/1770 train_time:49089ms step_avg:95.88ms
step:523/1770 train_time:49186ms step_avg:95.88ms
step:524/1770 train_time:49284ms step_avg:95.88ms
step:525/1770 train_time:49381ms step_avg:95.89ms
step:526/1770 train_time:49478ms step_avg:95.89ms
step:527/1770 train_time:49576ms step_avg:95.89ms
step:528/1770 train_time:49674ms step_avg:95.90ms
step:529/1770 train_time:49773ms step_avg:95.90ms
step:530/1770 train_time:49871ms step_avg:95.91ms
step:531/1770 train_time:49969ms step_avg:95.91ms
step:532/1770 train_time:50067ms step_avg:95.91ms
step:533/1770 train_time:50164ms step_avg:95.92ms
step:534/1770 train_time:50262ms step_avg:95.92ms
step:535/1770 train_time:50359ms step_avg:95.92ms
step:536/1770 train_time:50457ms step_avg:95.93ms
step:537/1770 train_time:50555ms step_avg:95.93ms
step:538/1770 train_time:50653ms step_avg:95.93ms
step:539/1770 train_time:50751ms step_avg:95.94ms
step:540/1770 train_time:50850ms step_avg:95.94ms
step:541/1770 train_time:50948ms step_avg:95.95ms
step:542/1770 train_time:51046ms step_avg:95.95ms
step:543/1770 train_time:51143ms step_avg:95.95ms
step:544/1770 train_time:51241ms step_avg:95.96ms
step:545/1770 train_time:51339ms step_avg:95.96ms
step:546/1770 train_time:51437ms step_avg:95.96ms
step:547/1770 train_time:51534ms step_avg:95.97ms
step:548/1770 train_time:51632ms step_avg:95.97ms
step:549/1770 train_time:51730ms step_avg:95.97ms
step:550/1770 train_time:51829ms step_avg:95.98ms
step:551/1770 train_time:51927ms step_avg:95.98ms
step:552/1770 train_time:52025ms step_avg:95.99ms
step:553/1770 train_time:52122ms step_avg:95.99ms
step:554/1770 train_time:52220ms step_avg:95.99ms
step:555/1770 train_time:52318ms step_avg:96.00ms
step:556/1770 train_time:52416ms step_avg:96.00ms
step:557/1770 train_time:52514ms step_avg:96.00ms
step:558/1770 train_time:52613ms step_avg:96.01ms
step:559/1770 train_time:52711ms step_avg:96.01ms
step:560/1770 train_time:52809ms step_avg:96.02ms
step:561/1770 train_time:52907ms step_avg:96.02ms
step:562/1770 train_time:53004ms step_avg:96.02ms
step:563/1770 train_time:53102ms step_avg:96.03ms
step:564/1770 train_time:53200ms step_avg:96.03ms
step:565/1770 train_time:53298ms step_avg:96.03ms
step:566/1770 train_time:53396ms step_avg:96.04ms
step:567/1770 train_time:53494ms step_avg:96.04ms
step:568/1770 train_time:53592ms step_avg:96.04ms
step:569/1770 train_time:53691ms step_avg:96.05ms
step:570/1770 train_time:53789ms step_avg:96.05ms
step:571/1770 train_time:53887ms step_avg:96.05ms
step:572/1770 train_time:53984ms step_avg:96.06ms
step:573/1770 train_time:54082ms step_avg:96.06ms
step:574/1770 train_time:54179ms step_avg:96.06ms
step:575/1770 train_time:54277ms step_avg:96.06ms
step:576/1770 train_time:54375ms step_avg:96.07ms
step:577/1770 train_time:54473ms step_avg:96.07ms
step:578/1770 train_time:54571ms step_avg:96.08ms
step:579/1770 train_time:54669ms step_avg:96.08ms
step:580/1770 train_time:54768ms step_avg:96.08ms
step:581/1770 train_time:54865ms step_avg:96.09ms
step:582/1770 train_time:54963ms step_avg:96.09ms
step:583/1770 train_time:55061ms step_avg:96.09ms
step:584/1770 train_time:55158ms step_avg:96.09ms
step:585/1770 train_time:55256ms step_avg:96.10ms
step:586/1770 train_time:55354ms step_avg:96.10ms
step:587/1770 train_time:55452ms step_avg:96.10ms
step:588/1770 train_time:55550ms step_avg:96.11ms
step:589/1770 train_time:55648ms step_avg:96.11ms
step:590/1770 train_time:55746ms step_avg:96.11ms
step:591/1770 train_time:55844ms step_avg:96.12ms
step:592/1770 train_time:55941ms step_avg:96.12ms
step:593/1770 train_time:56039ms step_avg:96.12ms
step:594/1770 train_time:56137ms step_avg:96.12ms
step:595/1770 train_time:56235ms step_avg:96.13ms
step:596/1770 train_time:56333ms step_avg:96.13ms
step:597/1770 train_time:56431ms step_avg:96.14ms
step:598/1770 train_time:56529ms step_avg:96.14ms
step:599/1770 train_time:56627ms step_avg:96.14ms
step:600/1770 train_time:56724ms step_avg:96.14ms
step:601/1770 train_time:56822ms step_avg:96.15ms
step:602/1770 train_time:56920ms step_avg:96.15ms
step:603/1770 train_time:57018ms step_avg:96.15ms
step:604/1770 train_time:57116ms step_avg:96.16ms
step:605/1770 train_time:57214ms step_avg:96.16ms
step:606/1770 train_time:57313ms step_avg:96.16ms
step:607/1770 train_time:57411ms step_avg:96.17ms
step:608/1770 train_time:57509ms step_avg:96.17ms
step:609/1770 train_time:57607ms step_avg:96.17ms
step:610/1770 train_time:57705ms step_avg:96.17ms
step:611/1770 train_time:57802ms step_avg:96.18ms
step:612/1770 train_time:57900ms step_avg:96.18ms
step:613/1770 train_time:57998ms step_avg:96.18ms
step:614/1770 train_time:58096ms step_avg:96.19ms
step:615/1770 train_time:58194ms step_avg:96.19ms
step:616/1770 train_time:58292ms step_avg:96.19ms
step:617/1770 train_time:58390ms step_avg:96.19ms
step:618/1770 train_time:58488ms step_avg:96.20ms
step:619/1770 train_time:58586ms step_avg:96.20ms
step:620/1770 train_time:58683ms step_avg:96.20ms
step:621/1770 train_time:58781ms step_avg:96.20ms
step:622/1770 train_time:58879ms step_avg:96.21ms
step:623/1770 train_time:58976ms step_avg:96.21ms
step:624/1770 train_time:59075ms step_avg:96.21ms
step:625/1770 train_time:59172ms step_avg:96.22ms
step:625/1770 val_loss:3.6657 train_time:59269ms step_avg:96.37ms
step:626/1770 train_time:59290ms step_avg:96.25ms
step:627/1770 train_time:59377ms step_avg:96.23ms
step:628/1770 train_time:59476ms step_avg:96.24ms
step:629/1770 train_time:59575ms step_avg:96.24ms
step:630/1770 train_time:59673ms step_avg:96.25ms
step:631/1770 train_time:59771ms step_avg:96.25ms
step:632/1770 train_time:59868ms step_avg:96.25ms
step:633/1770 train_time:59966ms step_avg:96.25ms
step:634/1770 train_time:60064ms step_avg:96.26ms
step:635/1770 train_time:60162ms step_avg:96.26ms
step:636/1770 train_time:60260ms step_avg:96.26ms
step:637/1770 train_time:60358ms step_avg:96.26ms
step:638/1770 train_time:60457ms step_avg:96.27ms
step:639/1770 train_time:60555ms step_avg:96.27ms
step:640/1770 train_time:60653ms step_avg:96.27ms
step:641/1770 train_time:60750ms step_avg:96.28ms
step:642/1770 train_time:60849ms step_avg:96.28ms
step:643/1770 train_time:60947ms step_avg:96.28ms
step:644/1770 train_time:61045ms step_avg:96.28ms
step:645/1770 train_time:61143ms step_avg:96.29ms
step:646/1770 train_time:61241ms step_avg:96.29ms
step:647/1770 train_time:61339ms step_avg:96.29ms
step:648/1770 train_time:61437ms step_avg:96.30ms
step:649/1770 train_time:61535ms step_avg:96.30ms
step:650/1770 train_time:61632ms step_avg:96.30ms
step:651/1770 train_time:61730ms step_avg:96.30ms
step:652/1770 train_time:61828ms step_avg:96.31ms
step:653/1770 train_time:61926ms step_avg:96.31ms
step:654/1770 train_time:62024ms step_avg:96.31ms
step:655/1770 train_time:62122ms step_avg:96.31ms
step:656/1770 train_time:62220ms step_avg:96.32ms
step:657/1770 train_time:62318ms step_avg:96.32ms
step:658/1770 train_time:62418ms step_avg:96.32ms
step:659/1770 train_time:62517ms step_avg:96.33ms
step:660/1770 train_time:62617ms step_avg:96.33ms
step:661/1770 train_time:62717ms step_avg:96.34ms
step:662/1770 train_time:62818ms step_avg:96.35ms
step:663/1770 train_time:62919ms step_avg:96.35ms
step:664/1770 train_time:63020ms step_avg:96.36ms
step:665/1770 train_time:63120ms step_avg:96.37ms
step:666/1770 train_time:63221ms step_avg:96.37ms
step:667/1770 train_time:63321ms step_avg:96.38ms
step:668/1770 train_time:63422ms step_avg:96.39ms
step:669/1770 train_time:63522ms step_avg:96.39ms
step:670/1770 train_time:63622ms step_avg:96.40ms
step:671/1770 train_time:63723ms step_avg:96.40ms
step:672/1770 train_time:63824ms step_avg:96.41ms
step:673/1770 train_time:63924ms step_avg:96.42ms
step:674/1770 train_time:64024ms step_avg:96.42ms
step:675/1770 train_time:64124ms step_avg:96.43ms
step:676/1770 train_time:64223ms step_avg:96.43ms
step:677/1770 train_time:64323ms step_avg:96.44ms
step:678/1770 train_time:64423ms step_avg:96.44ms
step:679/1770 train_time:64523ms step_avg:96.45ms
step:680/1770 train_time:64623ms step_avg:96.45ms
step:681/1770 train_time:64723ms step_avg:96.46ms
step:682/1770 train_time:64823ms step_avg:96.46ms
step:683/1770 train_time:64923ms step_avg:96.47ms
step:684/1770 train_time:65023ms step_avg:96.47ms
step:685/1770 train_time:65123ms step_avg:96.48ms
step:686/1770 train_time:65223ms step_avg:96.48ms
step:687/1770 train_time:65323ms step_avg:96.49ms
step:688/1770 train_time:65423ms step_avg:96.49ms
step:689/1770 train_time:65523ms step_avg:96.50ms
step:690/1770 train_time:65623ms step_avg:96.50ms
step:691/1770 train_time:65722ms step_avg:96.51ms
step:692/1770 train_time:65822ms step_avg:96.51ms
step:693/1770 train_time:65922ms step_avg:96.52ms
step:694/1770 train_time:66023ms step_avg:96.52ms
step:695/1770 train_time:66123ms step_avg:96.53ms
step:696/1770 train_time:66223ms step_avg:96.53ms
step:697/1770 train_time:66323ms step_avg:96.54ms
step:698/1770 train_time:66423ms step_avg:96.55ms
step:699/1770 train_time:66523ms step_avg:96.55ms
step:700/1770 train_time:66623ms step_avg:96.56ms
step:701/1770 train_time:66723ms step_avg:96.56ms
step:702/1770 train_time:66823ms step_avg:96.56ms
step:703/1770 train_time:66923ms step_avg:96.57ms
step:704/1770 train_time:67023ms step_avg:96.57ms
step:705/1770 train_time:67123ms step_avg:96.58ms
step:706/1770 train_time:67223ms step_avg:96.59ms
step:707/1770 train_time:67324ms step_avg:96.59ms
step:708/1770 train_time:67425ms step_avg:96.60ms
step:709/1770 train_time:67525ms step_avg:96.60ms
step:710/1770 train_time:67625ms step_avg:96.61ms
step:711/1770 train_time:67724ms step_avg:96.61ms
step:712/1770 train_time:67825ms step_avg:96.62ms
step:713/1770 train_time:67925ms step_avg:96.62ms
step:714/1770 train_time:68024ms step_avg:96.63ms
step:715/1770 train_time:68124ms step_avg:96.63ms
step:716/1770 train_time:68224ms step_avg:96.63ms
step:717/1770 train_time:68323ms step_avg:96.64ms
step:718/1770 train_time:68423ms step_avg:96.64ms
step:719/1770 train_time:68523ms step_avg:96.65ms
step:720/1770 train_time:68624ms step_avg:96.65ms
step:721/1770 train_time:68724ms step_avg:96.66ms
step:722/1770 train_time:68823ms step_avg:96.66ms
step:723/1770 train_time:68924ms step_avg:96.67ms
step:724/1770 train_time:69024ms step_avg:96.67ms
step:725/1770 train_time:69124ms step_avg:96.68ms
step:726/1770 train_time:69223ms step_avg:96.68ms
step:727/1770 train_time:69323ms step_avg:96.69ms
step:728/1770 train_time:69423ms step_avg:96.69ms
step:729/1770 train_time:69523ms step_avg:96.69ms
step:730/1770 train_time:69623ms step_avg:96.70ms
step:731/1770 train_time:69723ms step_avg:96.70ms
step:732/1770 train_time:69824ms step_avg:96.71ms
step:733/1770 train_time:69924ms step_avg:96.71ms
step:734/1770 train_time:70024ms step_avg:96.72ms
step:735/1770 train_time:70123ms step_avg:96.72ms
step:736/1770 train_time:70224ms step_avg:96.73ms
step:737/1770 train_time:70324ms step_avg:96.73ms
step:738/1770 train_time:70423ms step_avg:96.74ms
step:739/1770 train_time:70523ms step_avg:96.74ms
step:740/1770 train_time:70622ms step_avg:96.74ms
step:741/1770 train_time:70722ms step_avg:96.75ms
step:742/1770 train_time:70822ms step_avg:96.75ms
step:743/1770 train_time:70922ms step_avg:96.76ms
step:744/1770 train_time:71022ms step_avg:96.76ms
step:745/1770 train_time:71122ms step_avg:96.76ms
step:746/1770 train_time:71223ms step_avg:96.77ms
step:747/1770 train_time:71323ms step_avg:96.77ms
step:748/1770 train_time:71423ms step_avg:96.78ms
step:749/1770 train_time:71523ms step_avg:96.78ms
step:750/1770 train_time:71623ms step_avg:96.79ms
step:750/1770 val_loss:3.6016 train_time:71722ms step_avg:96.92ms
step:751/1770 train_time:71744ms step_avg:96.82ms
step:752/1770 train_time:71831ms step_avg:96.81ms
step:753/1770 train_time:71931ms step_avg:96.81ms
step:754/1770 train_time:72030ms step_avg:96.81ms
step:755/1770 train_time:72130ms step_avg:96.82ms
step:756/1770 train_time:72229ms step_avg:96.82ms
step:757/1770 train_time:72328ms step_avg:96.82ms
step:758/1770 train_time:72427ms step_avg:96.83ms
step:759/1770 train_time:72526ms step_avg:96.83ms
step:760/1770 train_time:72626ms step_avg:96.83ms
step:761/1770 train_time:72725ms step_avg:96.84ms
step:762/1770 train_time:72825ms step_avg:96.84ms
step:763/1770 train_time:72925ms step_avg:96.85ms
step:764/1770 train_time:73024ms step_avg:96.85ms
step:765/1770 train_time:73124ms step_avg:96.85ms
step:766/1770 train_time:73223ms step_avg:96.86ms
step:767/1770 train_time:73323ms step_avg:96.86ms
step:768/1770 train_time:73424ms step_avg:96.87ms
step:769/1770 train_time:73524ms step_avg:96.87ms
step:770/1770 train_time:73624ms step_avg:96.87ms
step:771/1770 train_time:73724ms step_avg:96.88ms
step:772/1770 train_time:73824ms step_avg:96.88ms
step:773/1770 train_time:73923ms step_avg:96.88ms
step:774/1770 train_time:74022ms step_avg:96.89ms
step:775/1770 train_time:74123ms step_avg:96.89ms
step:776/1770 train_time:74222ms step_avg:96.90ms
step:777/1770 train_time:74322ms step_avg:96.90ms
step:778/1770 train_time:74421ms step_avg:96.90ms
step:779/1770 train_time:74520ms step_avg:96.91ms
step:780/1770 train_time:74620ms step_avg:96.91ms
step:781/1770 train_time:74720ms step_avg:96.91ms
step:782/1770 train_time:74819ms step_avg:96.92ms
step:783/1770 train_time:74919ms step_avg:96.92ms
step:784/1770 train_time:75019ms step_avg:96.92ms
step:785/1770 train_time:75119ms step_avg:96.93ms
step:786/1770 train_time:75220ms step_avg:96.93ms
step:787/1770 train_time:75320ms step_avg:96.94ms
step:788/1770 train_time:75420ms step_avg:96.94ms
step:789/1770 train_time:75520ms step_avg:96.94ms
step:790/1770 train_time:75620ms step_avg:96.95ms
step:791/1770 train_time:75720ms step_avg:96.95ms
step:792/1770 train_time:75820ms step_avg:96.96ms
step:793/1770 train_time:75920ms step_avg:96.96ms
step:794/1770 train_time:76020ms step_avg:96.96ms
step:795/1770 train_time:76120ms step_avg:96.97ms
step:796/1770 train_time:76220ms step_avg:96.97ms
step:797/1770 train_time:76321ms step_avg:96.98ms
step:798/1770 train_time:76420ms step_avg:96.98ms
step:799/1770 train_time:76520ms step_avg:96.98ms
step:800/1770 train_time:76620ms step_avg:96.99ms
step:801/1770 train_time:76720ms step_avg:96.99ms
step:802/1770 train_time:76820ms step_avg:96.99ms
step:803/1770 train_time:76920ms step_avg:97.00ms
step:804/1770 train_time:77020ms step_avg:97.00ms
step:805/1770 train_time:77119ms step_avg:97.01ms
step:806/1770 train_time:77219ms step_avg:97.01ms
step:807/1770 train_time:77319ms step_avg:97.01ms
step:808/1770 train_time:77419ms step_avg:97.02ms
step:809/1770 train_time:77519ms step_avg:97.02ms
step:810/1770 train_time:77620ms step_avg:97.02ms
step:811/1770 train_time:77720ms step_avg:97.03ms
step:812/1770 train_time:77820ms step_avg:97.03ms
step:813/1770 train_time:77920ms step_avg:97.04ms
step:814/1770 train_time:78020ms step_avg:97.04ms
step:815/1770 train_time:78119ms step_avg:97.04ms
step:816/1770 train_time:78219ms step_avg:97.05ms
step:817/1770 train_time:78319ms step_avg:97.05ms
step:818/1770 train_time:78419ms step_avg:97.05ms
step:819/1770 train_time:78519ms step_avg:97.06ms
step:820/1770 train_time:78619ms step_avg:97.06ms
step:821/1770 train_time:78720ms step_avg:97.07ms
step:822/1770 train_time:78820ms step_avg:97.07ms
step:823/1770 train_time:78920ms step_avg:97.07ms
step:824/1770 train_time:79020ms step_avg:97.08ms
step:825/1770 train_time:79120ms step_avg:97.08ms
step:826/1770 train_time:79220ms step_avg:97.08ms
step:827/1770 train_time:79320ms step_avg:97.09ms
step:828/1770 train_time:79421ms step_avg:97.09ms
step:829/1770 train_time:79521ms step_avg:97.10ms
step:830/1770 train_time:79622ms step_avg:97.10ms
step:831/1770 train_time:79722ms step_avg:97.10ms
step:832/1770 train_time:79821ms step_avg:97.11ms
step:833/1770 train_time:79921ms step_avg:97.11ms
step:834/1770 train_time:80021ms step_avg:97.11ms
step:835/1770 train_time:80121ms step_avg:97.12ms
step:836/1770 train_time:80221ms step_avg:97.12ms
step:837/1770 train_time:80321ms step_avg:97.12ms
step:838/1770 train_time:80421ms step_avg:97.13ms
step:839/1770 train_time:80521ms step_avg:97.13ms
step:840/1770 train_time:80621ms step_avg:97.13ms
step:841/1770 train_time:80721ms step_avg:97.14ms
step:842/1770 train_time:80821ms step_avg:97.14ms
step:843/1770 train_time:80921ms step_avg:97.14ms
step:844/1770 train_time:81021ms step_avg:97.15ms
step:845/1770 train_time:81121ms step_avg:97.15ms
step:846/1770 train_time:81221ms step_avg:97.15ms
step:847/1770 train_time:81320ms step_avg:97.16ms
step:848/1770 train_time:81420ms step_avg:97.16ms
step:849/1770 train_time:81520ms step_avg:97.16ms
step:850/1770 train_time:81620ms step_avg:97.17ms
step:851/1770 train_time:81720ms step_avg:97.17ms
step:852/1770 train_time:81820ms step_avg:97.17ms
step:853/1770 train_time:81920ms step_avg:97.18ms
step:854/1770 train_time:82020ms step_avg:97.18ms
step:855/1770 train_time:82120ms step_avg:97.18ms
step:856/1770 train_time:82220ms step_avg:97.19ms
step:857/1770 train_time:82320ms step_avg:97.19ms
step:858/1770 train_time:82420ms step_avg:97.19ms
step:859/1770 train_time:82521ms step_avg:97.20ms
step:860/1770 train_time:82620ms step_avg:97.20ms
step:861/1770 train_time:82720ms step_avg:97.20ms
step:862/1770 train_time:82820ms step_avg:97.21ms
step:863/1770 train_time:82920ms step_avg:97.21ms
step:864/1770 train_time:83020ms step_avg:97.21ms
step:865/1770 train_time:83120ms step_avg:97.22ms
step:866/1770 train_time:83220ms step_avg:97.22ms
step:867/1770 train_time:83320ms step_avg:97.22ms
step:868/1770 train_time:83420ms step_avg:97.23ms
step:869/1770 train_time:83521ms step_avg:97.23ms
step:870/1770 train_time:83621ms step_avg:97.23ms
step:871/1770 train_time:83721ms step_avg:97.24ms
step:872/1770 train_time:83821ms step_avg:97.24ms
step:873/1770 train_time:83921ms step_avg:97.24ms
step:874/1770 train_time:84022ms step_avg:97.25ms
step:875/1770 train_time:84122ms step_avg:97.25ms
step:875/1770 val_loss:3.5523 train_time:84220ms step_avg:97.36ms
step:876/1770 train_time:84241ms step_avg:97.28ms
step:877/1770 train_time:84329ms step_avg:97.27ms
step:878/1770 train_time:84430ms step_avg:97.27ms
step:879/1770 train_time:84530ms step_avg:97.27ms
step:880/1770 train_time:84630ms step_avg:97.28ms
step:881/1770 train_time:84730ms step_avg:97.28ms
step:882/1770 train_time:84830ms step_avg:97.28ms
step:883/1770 train_time:84930ms step_avg:97.29ms
step:884/1770 train_time:85030ms step_avg:97.29ms
step:885/1770 train_time:85130ms step_avg:97.29ms
step:886/1770 train_time:85230ms step_avg:97.29ms
step:887/1770 train_time:85331ms step_avg:97.30ms
step:888/1770 train_time:85431ms step_avg:97.30ms
step:889/1770 train_time:85532ms step_avg:97.31ms
step:890/1770 train_time:85632ms step_avg:97.31ms
step:891/1770 train_time:85732ms step_avg:97.31ms
step:892/1770 train_time:85832ms step_avg:97.32ms
step:893/1770 train_time:85932ms step_avg:97.32ms
step:894/1770 train_time:86032ms step_avg:97.32ms
step:895/1770 train_time:86131ms step_avg:97.32ms
step:896/1770 train_time:86231ms step_avg:97.33ms
step:897/1770 train_time:86331ms step_avg:97.33ms
step:898/1770 train_time:86431ms step_avg:97.33ms
step:899/1770 train_time:86531ms step_avg:97.34ms
step:900/1770 train_time:86631ms step_avg:97.34ms
step:901/1770 train_time:86731ms step_avg:97.34ms
step:902/1770 train_time:86831ms step_avg:97.34ms
step:903/1770 train_time:86931ms step_avg:97.35ms
step:904/1770 train_time:87031ms step_avg:97.35ms
step:905/1770 train_time:87131ms step_avg:97.35ms
step:906/1770 train_time:87231ms step_avg:97.36ms
step:907/1770 train_time:87331ms step_avg:97.36ms
step:908/1770 train_time:87430ms step_avg:97.36ms
step:909/1770 train_time:87530ms step_avg:97.36ms
step:910/1770 train_time:87631ms step_avg:97.37ms
step:911/1770 train_time:87731ms step_avg:97.37ms
step:912/1770 train_time:87831ms step_avg:97.37ms
step:913/1770 train_time:87931ms step_avg:97.38ms
step:914/1770 train_time:88031ms step_avg:97.38ms
step:915/1770 train_time:88131ms step_avg:97.38ms
step:916/1770 train_time:88231ms step_avg:97.38ms
step:917/1770 train_time:88331ms step_avg:97.39ms
step:918/1770 train_time:88431ms step_avg:97.39ms
step:919/1770 train_time:88531ms step_avg:97.39ms
step:920/1770 train_time:88633ms step_avg:97.40ms
step:921/1770 train_time:88735ms step_avg:97.40ms
step:922/1770 train_time:88836ms step_avg:97.41ms
step:923/1770 train_time:88937ms step_avg:97.41ms
step:924/1770 train_time:89038ms step_avg:97.42ms
step:925/1770 train_time:89139ms step_avg:97.42ms
step:926/1770 train_time:89240ms step_avg:97.42ms
step:927/1770 train_time:89341ms step_avg:97.43ms
step:928/1770 train_time:89441ms step_avg:97.43ms
step:929/1770 train_time:89542ms step_avg:97.43ms
step:930/1770 train_time:89644ms step_avg:97.44ms
step:931/1770 train_time:89746ms step_avg:97.44ms
step:932/1770 train_time:89848ms step_avg:97.45ms
step:933/1770 train_time:89950ms step_avg:97.45ms
step:934/1770 train_time:90052ms step_avg:97.46ms
step:935/1770 train_time:90153ms step_avg:97.46ms
step:936/1770 train_time:90254ms step_avg:97.47ms
step:937/1770 train_time:90355ms step_avg:97.47ms
step:938/1770 train_time:90456ms step_avg:97.47ms
step:939/1770 train_time:90557ms step_avg:97.48ms
step:940/1770 train_time:90658ms step_avg:97.48ms
step:941/1770 train_time:90759ms step_avg:97.49ms
step:942/1770 train_time:90860ms step_avg:97.49ms
step:943/1770 train_time:90962ms step_avg:97.49ms
step:944/1770 train_time:91062ms step_avg:97.50ms
step:945/1770 train_time:91163ms step_avg:97.50ms
step:946/1770 train_time:91265ms step_avg:97.51ms
step:947/1770 train_time:91367ms step_avg:97.51ms
step:948/1770 train_time:91470ms step_avg:97.52ms
step:949/1770 train_time:91572ms step_avg:97.52ms
step:950/1770 train_time:91673ms step_avg:97.52ms
step:951/1770 train_time:91774ms step_avg:97.53ms
step:952/1770 train_time:91875ms step_avg:97.53ms
step:953/1770 train_time:91976ms step_avg:97.54ms
step:954/1770 train_time:92077ms step_avg:97.54ms
step:955/1770 train_time:92178ms step_avg:97.54ms
step:956/1770 train_time:92278ms step_avg:97.55ms
step:957/1770 train_time:92379ms step_avg:97.55ms
step:958/1770 train_time:92480ms step_avg:97.55ms
step:959/1770 train_time:92581ms step_avg:97.56ms
step:960/1770 train_time:92682ms step_avg:97.56ms
step:961/1770 train_time:92783ms step_avg:97.56ms
step:962/1770 train_time:92885ms step_avg:97.57ms
step:963/1770 train_time:92986ms step_avg:97.57ms
step:964/1770 train_time:93090ms step_avg:97.58ms
step:965/1770 train_time:93192ms step_avg:97.58ms
step:966/1770 train_time:93294ms step_avg:97.59ms
step:967/1770 train_time:93396ms step_avg:97.59ms
step:968/1770 train_time:93497ms step_avg:97.60ms
step:969/1770 train_time:93598ms step_avg:97.60ms
step:970/1770 train_time:93699ms step_avg:97.60ms
step:971/1770 train_time:93800ms step_avg:97.61ms
step:972/1770 train_time:93901ms step_avg:97.61ms
step:973/1770 train_time:94002ms step_avg:97.61ms
step:974/1770 train_time:94103ms step_avg:97.62ms
step:975/1770 train_time:94205ms step_avg:97.62ms
step:976/1770 train_time:94307ms step_avg:97.63ms
step:977/1770 train_time:94410ms step_avg:97.63ms
step:978/1770 train_time:94511ms step_avg:97.64ms
step:979/1770 train_time:94613ms step_avg:97.64ms
step:980/1770 train_time:94714ms step_avg:97.64ms
step:981/1770 train_time:94815ms step_avg:97.65ms
step:982/1770 train_time:94916ms step_avg:97.65ms
step:983/1770 train_time:95018ms step_avg:97.65ms
step:984/1770 train_time:95119ms step_avg:97.66ms
step:985/1770 train_time:95220ms step_avg:97.66ms
step:986/1770 train_time:95321ms step_avg:97.67ms
step:987/1770 train_time:95422ms step_avg:97.67ms
step:988/1770 train_time:95524ms step_avg:97.67ms
step:989/1770 train_time:95626ms step_avg:97.68ms
step:990/1770 train_time:95728ms step_avg:97.68ms
step:991/1770 train_time:95831ms step_avg:97.69ms
step:992/1770 train_time:95932ms step_avg:97.69ms
step:993/1770 train_time:96033ms step_avg:97.69ms
step:994/1770 train_time:96135ms step_avg:97.70ms
step:995/1770 train_time:96236ms step_avg:97.70ms
step:996/1770 train_time:96337ms step_avg:97.70ms
step:997/1770 train_time:96438ms step_avg:97.71ms
step:998/1770 train_time:96539ms step_avg:97.71ms
step:999/1770 train_time:96640ms step_avg:97.71ms
step:1000/1770 train_time:96741ms step_avg:97.72ms
step:1000/1770 val_loss:3.5154 train_time:96840ms step_avg:97.82ms
step:1001/1770 train_time:96864ms step_avg:97.74ms
step:1002/1770 train_time:96950ms step_avg:97.73ms
step:1003/1770 train_time:97052ms step_avg:97.74ms
step:1004/1770 train_time:97154ms step_avg:97.74ms
step:1005/1770 train_time:97256ms step_avg:97.74ms
step:1006/1770 train_time:97357ms step_avg:97.75ms
step:1007/1770 train_time:97457ms step_avg:97.75ms
step:1008/1770 train_time:97559ms step_avg:97.75ms
step:1009/1770 train_time:97659ms step_avg:97.76ms
step:1010/1770 train_time:97760ms step_avg:97.76ms
step:1011/1770 train_time:97865ms step_avg:97.77ms
step:1012/1770 train_time:97966ms step_avg:97.77ms
step:1013/1770 train_time:98067ms step_avg:97.77ms
step:1014/1770 train_time:98169ms step_avg:97.78ms
step:1015/1770 train_time:98269ms step_avg:97.78ms
step:1016/1770 train_time:98370ms step_avg:97.78ms
step:1017/1770 train_time:98472ms step_avg:97.79ms
step:1018/1770 train_time:98574ms step_avg:97.79ms
step:1019/1770 train_time:98676ms step_avg:97.80ms
step:1020/1770 train_time:98778ms step_avg:97.80ms
step:1021/1770 train_time:98880ms step_avg:97.80ms
step:1022/1770 train_time:98981ms step_avg:97.81ms
step:1023/1770 train_time:99082ms step_avg:97.81ms
step:1024/1770 train_time:99183ms step_avg:97.81ms
step:1025/1770 train_time:99284ms step_avg:97.82ms
step:1026/1770 train_time:99385ms step_avg:97.82ms
step:1027/1770 train_time:99486ms step_avg:97.82ms
step:1028/1770 train_time:99587ms step_avg:97.83ms
step:1029/1770 train_time:99688ms step_avg:97.83ms
step:1030/1770 train_time:99789ms step_avg:97.83ms
step:1031/1770 train_time:99892ms step_avg:97.84ms
step:1032/1770 train_time:99994ms step_avg:97.84ms
step:1033/1770 train_time:100097ms step_avg:97.85ms
step:1034/1770 train_time:100198ms step_avg:97.85ms
step:1035/1770 train_time:100299ms step_avg:97.85ms
step:1036/1770 train_time:100400ms step_avg:97.86ms
step:1037/1770 train_time:100501ms step_avg:97.86ms
step:1038/1770 train_time:100602ms step_avg:97.86ms
step:1039/1770 train_time:100702ms step_avg:97.86ms
step:1040/1770 train_time:100804ms step_avg:97.87ms
step:1041/1770 train_time:100904ms step_avg:97.87ms
step:1042/1770 train_time:101005ms step_avg:97.87ms
step:1043/1770 train_time:101106ms step_avg:97.88ms
step:1044/1770 train_time:101207ms step_avg:97.88ms
step:1045/1770 train_time:101309ms step_avg:97.88ms
step:1046/1770 train_time:101409ms step_avg:97.89ms
step:1047/1770 train_time:101512ms step_avg:97.89ms
step:1048/1770 train_time:101614ms step_avg:97.89ms
step:1049/1770 train_time:101715ms step_avg:97.90ms
step:1050/1770 train_time:101818ms step_avg:97.90ms
step:1051/1770 train_time:101920ms step_avg:97.91ms
step:1052/1770 train_time:102021ms step_avg:97.91ms
step:1053/1770 train_time:102122ms step_avg:97.91ms
step:1054/1770 train_time:102223ms step_avg:97.91ms
step:1055/1770 train_time:102325ms step_avg:97.92ms
step:1056/1770 train_time:102425ms step_avg:97.92ms
step:1057/1770 train_time:102527ms step_avg:97.92ms
step:1058/1770 train_time:102628ms step_avg:97.93ms
step:1059/1770 train_time:102729ms step_avg:97.93ms
step:1060/1770 train_time:102830ms step_avg:97.93ms
step:1061/1770 train_time:102933ms step_avg:97.94ms
step:1062/1770 train_time:103037ms step_avg:97.94ms
step:1063/1770 train_time:103139ms step_avg:97.95ms
step:1064/1770 train_time:103241ms step_avg:97.95ms
step:1065/1770 train_time:103342ms step_avg:97.95ms
step:1066/1770 train_time:103444ms step_avg:97.96ms
step:1067/1770 train_time:103545ms step_avg:97.96ms
step:1068/1770 train_time:103647ms step_avg:97.96ms
step:1069/1770 train_time:103748ms step_avg:97.97ms
step:1070/1770 train_time:103849ms step_avg:97.97ms
step:1071/1770 train_time:103951ms step_avg:97.97ms
step:1072/1770 train_time:104053ms step_avg:97.98ms
step:1073/1770 train_time:104155ms step_avg:97.98ms
step:1074/1770 train_time:104257ms step_avg:97.99ms
step:1075/1770 train_time:104359ms step_avg:97.99ms
step:1076/1770 train_time:104461ms step_avg:97.99ms
step:1077/1770 train_time:104562ms step_avg:98.00ms
step:1078/1770 train_time:104663ms step_avg:98.00ms
step:1079/1770 train_time:104764ms step_avg:98.00ms
step:1080/1770 train_time:104865ms step_avg:98.00ms
step:1081/1770 train_time:104966ms step_avg:98.01ms
step:1082/1770 train_time:105067ms step_avg:98.01ms
step:1083/1770 train_time:105168ms step_avg:98.01ms
step:1084/1770 train_time:105271ms step_avg:98.02ms
step:1085/1770 train_time:105374ms step_avg:98.02ms
step:1086/1770 train_time:105476ms step_avg:98.03ms
step:1087/1770 train_time:105578ms step_avg:98.03ms
step:1088/1770 train_time:105679ms step_avg:98.03ms
step:1089/1770 train_time:105780ms step_avg:98.04ms
step:1090/1770 train_time:105882ms step_avg:98.04ms
step:1091/1770 train_time:105983ms step_avg:98.04ms
step:1092/1770 train_time:106084ms step_avg:98.04ms
step:1093/1770 train_time:106185ms step_avg:98.05ms
step:1094/1770 train_time:106286ms step_avg:98.05ms
step:1095/1770 train_time:106388ms step_avg:98.05ms
step:1096/1770 train_time:106488ms step_avg:98.06ms
step:1097/1770 train_time:106591ms step_avg:98.06ms
step:1098/1770 train_time:106692ms step_avg:98.06ms
step:1099/1770 train_time:106795ms step_avg:98.07ms
step:1100/1770 train_time:106898ms step_avg:98.07ms
step:1101/1770 train_time:106999ms step_avg:98.07ms
step:1102/1770 train_time:107101ms step_avg:98.08ms
step:1103/1770 train_time:107202ms step_avg:98.08ms
step:1104/1770 train_time:107303ms step_avg:98.08ms
step:1105/1770 train_time:107404ms step_avg:98.09ms
step:1106/1770 train_time:107506ms step_avg:98.09ms
step:1107/1770 train_time:107607ms step_avg:98.09ms
step:1108/1770 train_time:107708ms step_avg:98.09ms
step:1109/1770 train_time:107810ms step_avg:98.10ms
step:1110/1770 train_time:107911ms step_avg:98.10ms
step:1111/1770 train_time:108013ms step_avg:98.10ms
step:1112/1770 train_time:108115ms step_avg:98.11ms
step:1113/1770 train_time:108217ms step_avg:98.11ms
step:1114/1770 train_time:108319ms step_avg:98.11ms
step:1115/1770 train_time:108420ms step_avg:98.12ms
step:1116/1770 train_time:108521ms step_avg:98.12ms
step:1117/1770 train_time:108623ms step_avg:98.12ms
step:1118/1770 train_time:108724ms step_avg:98.13ms
step:1119/1770 train_time:108825ms step_avg:98.13ms
step:1120/1770 train_time:108927ms step_avg:98.13ms
step:1121/1770 train_time:109027ms step_avg:98.13ms
step:1122/1770 train_time:109129ms step_avg:98.14ms
step:1123/1770 train_time:109231ms step_avg:98.14ms
step:1124/1770 train_time:109334ms step_avg:98.15ms
step:1125/1770 train_time:109437ms step_avg:98.15ms
step:1125/1770 val_loss:3.4749 train_time:109536ms step_avg:98.24ms
step:1126/1770 train_time:109558ms step_avg:98.17ms
step:1127/1770 train_time:109650ms step_avg:98.16ms
step:1128/1770 train_time:109753ms step_avg:98.17ms
step:1129/1770 train_time:109854ms step_avg:98.17ms
step:1130/1770 train_time:109956ms step_avg:98.17ms
step:1131/1770 train_time:110058ms step_avg:98.18ms
step:1132/1770 train_time:110160ms step_avg:98.18ms
step:1133/1770 train_time:110262ms step_avg:98.18ms
step:1134/1770 train_time:110363ms step_avg:98.19ms
step:1135/1770 train_time:110465ms step_avg:98.19ms
step:1136/1770 train_time:110566ms step_avg:98.19ms
step:1137/1770 train_time:110669ms step_avg:98.20ms
step:1138/1770 train_time:110770ms step_avg:98.20ms
step:1139/1770 train_time:110871ms step_avg:98.20ms
step:1140/1770 train_time:110973ms step_avg:98.21ms
step:1141/1770 train_time:111074ms step_avg:98.21ms
step:1142/1770 train_time:111176ms step_avg:98.21ms
step:1143/1770 train_time:111278ms step_avg:98.22ms
step:1144/1770 train_time:111380ms step_avg:98.22ms
step:1145/1770 train_time:111482ms step_avg:98.22ms
step:1146/1770 train_time:111584ms step_avg:98.23ms
step:1147/1770 train_time:111685ms step_avg:98.23ms
step:1148/1770 train_time:111786ms step_avg:98.23ms
step:1149/1770 train_time:111887ms step_avg:98.23ms
step:1150/1770 train_time:111988ms step_avg:98.24ms
step:1151/1770 train_time:112090ms step_avg:98.24ms
step:1152/1770 train_time:112193ms step_avg:98.24ms
step:1153/1770 train_time:112294ms step_avg:98.24ms
step:1154/1770 train_time:112395ms step_avg:98.25ms
step:1155/1770 train_time:112498ms step_avg:98.25ms
step:1156/1770 train_time:112602ms step_avg:98.26ms
step:1157/1770 train_time:112705ms step_avg:98.26ms
step:1158/1770 train_time:112806ms step_avg:98.26ms
step:1159/1770 train_time:112907ms step_avg:98.27ms
step:1160/1770 train_time:113008ms step_avg:98.27ms
step:1161/1770 train_time:113109ms step_avg:98.27ms
step:1162/1770 train_time:113210ms step_avg:98.27ms
step:1163/1770 train_time:113311ms step_avg:98.28ms
step:1164/1770 train_time:113413ms step_avg:98.28ms
step:1165/1770 train_time:113514ms step_avg:98.28ms
step:1166/1770 train_time:113616ms step_avg:98.28ms
step:1167/1770 train_time:113719ms step_avg:98.29ms
step:1168/1770 train_time:113821ms step_avg:98.29ms
step:1169/1770 train_time:113922ms step_avg:98.29ms
step:1170/1770 train_time:114024ms step_avg:98.30ms
step:1171/1770 train_time:114125ms step_avg:98.30ms
step:1172/1770 train_time:114226ms step_avg:98.30ms
step:1173/1770 train_time:114327ms step_avg:98.30ms
step:1174/1770 train_time:114429ms step_avg:98.31ms
step:1175/1770 train_time:114530ms step_avg:98.31ms
step:1176/1770 train_time:114631ms step_avg:98.31ms
step:1177/1770 train_time:114732ms step_avg:98.31ms
step:1178/1770 train_time:114833ms step_avg:98.32ms
step:1179/1770 train_time:114935ms step_avg:98.32ms
step:1180/1770 train_time:115036ms step_avg:98.32ms
step:1181/1770 train_time:115140ms step_avg:98.33ms
step:1182/1770 train_time:115241ms step_avg:98.33ms
step:1183/1770 train_time:115344ms step_avg:98.33ms
step:1184/1770 train_time:115448ms step_avg:98.34ms
step:1185/1770 train_time:115550ms step_avg:98.34ms
step:1186/1770 train_time:115653ms step_avg:98.34ms
step:1187/1770 train_time:115758ms step_avg:98.35ms
step:1188/1770 train_time:115861ms step_avg:98.35ms
step:1189/1770 train_time:115963ms step_avg:98.36ms
step:1190/1770 train_time:116066ms step_avg:98.36ms
step:1191/1770 train_time:116169ms step_avg:98.37ms
step:1192/1770 train_time:116272ms step_avg:98.37ms
step:1193/1770 train_time:116375ms step_avg:98.37ms
step:1194/1770 train_time:116478ms step_avg:98.38ms
step:1195/1770 train_time:116582ms step_avg:98.38ms
step:1196/1770 train_time:116685ms step_avg:98.39ms
step:1197/1770 train_time:116787ms step_avg:98.39ms
step:1198/1770 train_time:116889ms step_avg:98.39ms
step:1199/1770 train_time:116992ms step_avg:98.40ms
step:1200/1770 train_time:117094ms step_avg:98.40ms
step:1201/1770 train_time:117198ms step_avg:98.40ms
step:1202/1770 train_time:117300ms step_avg:98.41ms
step:1203/1770 train_time:117403ms step_avg:98.41ms
step:1204/1770 train_time:117505ms step_avg:98.41ms
step:1205/1770 train_time:117607ms step_avg:98.42ms
step:1206/1770 train_time:117710ms step_avg:98.42ms
step:1207/1770 train_time:117812ms step_avg:98.42ms
step:1208/1770 train_time:117915ms step_avg:98.43ms
step:1209/1770 train_time:118018ms step_avg:98.43ms
step:1210/1770 train_time:118121ms step_avg:98.43ms
step:1211/1770 train_time:118224ms step_avg:98.44ms
step:1212/1770 train_time:118329ms step_avg:98.44ms
step:1213/1770 train_time:118431ms step_avg:98.45ms
step:1214/1770 train_time:118533ms step_avg:98.45ms
step:1215/1770 train_time:118637ms step_avg:98.45ms
step:1216/1770 train_time:118743ms step_avg:98.46ms
step:1217/1770 train_time:118845ms step_avg:98.46ms
step:1218/1770 train_time:118947ms step_avg:98.47ms
step:1219/1770 train_time:119049ms step_avg:98.47ms
step:1220/1770 train_time:119152ms step_avg:98.47ms
step:1221/1770 train_time:119255ms step_avg:98.48ms
step:1222/1770 train_time:119360ms step_avg:98.48ms
step:1223/1770 train_time:119462ms step_avg:98.48ms
step:1224/1770 train_time:119565ms step_avg:98.49ms
step:1225/1770 train_time:119668ms step_avg:98.49ms
step:1226/1770 train_time:119770ms step_avg:98.49ms
step:1227/1770 train_time:119875ms step_avg:98.50ms
step:1228/1770 train_time:119980ms step_avg:98.51ms
step:1229/1770 train_time:120083ms step_avg:98.51ms
step:1230/1770 train_time:120185ms step_avg:98.51ms
step:1231/1770 train_time:120288ms step_avg:98.52ms
step:1232/1770 train_time:120391ms step_avg:98.52ms
step:1233/1770 train_time:120492ms step_avg:98.52ms
step:1234/1770 train_time:120595ms step_avg:98.53ms
step:1235/1770 train_time:120699ms step_avg:98.53ms
step:1236/1770 train_time:120801ms step_avg:98.53ms
step:1237/1770 train_time:120904ms step_avg:98.54ms
step:1238/1770 train_time:121007ms step_avg:98.54ms
step:1239/1770 train_time:121109ms step_avg:98.54ms
step:1240/1770 train_time:121211ms step_avg:98.55ms
step:1241/1770 train_time:121315ms step_avg:98.55ms
step:1242/1770 train_time:121418ms step_avg:98.55ms
step:1243/1770 train_time:121521ms step_avg:98.56ms
step:1244/1770 train_time:121623ms step_avg:98.56ms
step:1245/1770 train_time:121725ms step_avg:98.56ms
step:1246/1770 train_time:121828ms step_avg:98.57ms
step:1247/1770 train_time:121931ms step_avg:98.57ms
step:1248/1770 train_time:122035ms step_avg:98.57ms
step:1249/1770 train_time:122138ms step_avg:98.58ms
step:1250/1770 train_time:122240ms step_avg:98.58ms
step:1250/1770 val_loss:3.4287 train_time:122342ms step_avg:98.66ms
step:1251/1770 train_time:122364ms step_avg:98.60ms
step:1252/1770 train_time:122452ms step_avg:98.59ms
step:1253/1770 train_time:122557ms step_avg:98.60ms
step:1254/1770 train_time:122659ms step_avg:98.60ms
step:1255/1770 train_time:122764ms step_avg:98.61ms
step:1256/1770 train_time:122866ms step_avg:98.61ms
step:1257/1770 train_time:122968ms step_avg:98.61ms
step:1258/1770 train_time:123072ms step_avg:98.62ms
step:1259/1770 train_time:123175ms step_avg:98.62ms
step:1260/1770 train_time:123277ms step_avg:98.62ms
step:1261/1770 train_time:123380ms step_avg:98.63ms
step:1262/1770 train_time:123484ms step_avg:98.63ms
step:1263/1770 train_time:123586ms step_avg:98.63ms
step:1264/1770 train_time:123689ms step_avg:98.64ms
step:1265/1770 train_time:123792ms step_avg:98.64ms
step:1266/1770 train_time:123895ms step_avg:98.64ms
step:1267/1770 train_time:123999ms step_avg:98.65ms
step:1268/1770 train_time:124102ms step_avg:98.65ms
step:1269/1770 train_time:124204ms step_avg:98.65ms
step:1270/1770 train_time:124307ms step_avg:98.66ms
step:1271/1770 train_time:124410ms step_avg:98.66ms
step:1272/1770 train_time:124512ms step_avg:98.66ms
step:1273/1770 train_time:124615ms step_avg:98.67ms
step:1274/1770 train_time:124718ms step_avg:98.67ms
step:1275/1770 train_time:124819ms step_avg:98.67ms
step:1276/1770 train_time:124922ms step_avg:98.67ms
step:1277/1770 train_time:125024ms step_avg:98.68ms
step:1278/1770 train_time:125128ms step_avg:98.68ms
step:1279/1770 train_time:125232ms step_avg:98.69ms
step:1280/1770 train_time:125336ms step_avg:98.69ms
step:1281/1770 train_time:125438ms step_avg:98.69ms
step:1282/1770 train_time:125542ms step_avg:98.70ms
step:1283/1770 train_time:125645ms step_avg:98.70ms
step:1284/1770 train_time:125750ms step_avg:98.70ms
step:1285/1770 train_time:125852ms step_avg:98.71ms
step:1286/1770 train_time:125957ms step_avg:98.71ms
step:1287/1770 train_time:126061ms step_avg:98.72ms
step:1288/1770 train_time:126164ms step_avg:98.72ms
step:1289/1770 train_time:126268ms step_avg:98.72ms
step:1290/1770 train_time:126370ms step_avg:98.73ms
step:1291/1770 train_time:126475ms step_avg:98.73ms
step:1292/1770 train_time:126577ms step_avg:98.73ms
step:1293/1770 train_time:126680ms step_avg:98.74ms
step:1294/1770 train_time:126782ms step_avg:98.74ms
step:1295/1770 train_time:126885ms step_avg:98.74ms
step:1296/1770 train_time:126987ms step_avg:98.75ms
step:1297/1770 train_time:127090ms step_avg:98.75ms
step:1298/1770 train_time:127194ms step_avg:98.75ms
step:1299/1770 train_time:127297ms step_avg:98.76ms
step:1300/1770 train_time:127399ms step_avg:98.76ms
step:1301/1770 train_time:127502ms step_avg:98.76ms
step:1302/1770 train_time:127604ms step_avg:98.77ms
step:1303/1770 train_time:127707ms step_avg:98.77ms
step:1304/1770 train_time:127810ms step_avg:98.77ms
step:1305/1770 train_time:127913ms step_avg:98.77ms
step:1306/1770 train_time:128015ms step_avg:98.78ms
step:1307/1770 train_time:128117ms step_avg:98.78ms
step:1308/1770 train_time:128220ms step_avg:98.78ms
step:1309/1770 train_time:128322ms step_avg:98.79ms
step:1310/1770 train_time:128425ms step_avg:98.79ms
step:1311/1770 train_time:128527ms step_avg:98.79ms
step:1312/1770 train_time:128629ms step_avg:98.79ms
step:1313/1770 train_time:128732ms step_avg:98.80ms
step:1314/1770 train_time:128835ms step_avg:98.80ms
step:1315/1770 train_time:128938ms step_avg:98.80ms
step:1316/1770 train_time:129041ms step_avg:98.81ms
step:1317/1770 train_time:129144ms step_avg:98.81ms
step:1318/1770 train_time:129250ms step_avg:98.81ms
step:1319/1770 train_time:129354ms step_avg:98.82ms
step:1320/1770 train_time:129456ms step_avg:98.82ms
step:1321/1770 train_time:129560ms step_avg:98.83ms
step:1322/1770 train_time:129662ms step_avg:98.83ms
step:1323/1770 train_time:129766ms step_avg:98.83ms
step:1324/1770 train_time:129870ms step_avg:98.84ms
step:1325/1770 train_time:129974ms step_avg:98.84ms
step:1326/1770 train_time:130077ms step_avg:98.84ms
step:1327/1770 train_time:130183ms step_avg:98.85ms
step:1328/1770 train_time:130285ms step_avg:98.85ms
step:1329/1770 train_time:130388ms step_avg:98.85ms
step:1330/1770 train_time:130490ms step_avg:98.86ms
step:1331/1770 train_time:130594ms step_avg:98.86ms
step:1332/1770 train_time:130697ms step_avg:98.86ms
step:1333/1770 train_time:130799ms step_avg:98.87ms
step:1334/1770 train_time:130900ms step_avg:98.87ms
step:1335/1770 train_time:131003ms step_avg:98.87ms
step:1336/1770 train_time:131104ms step_avg:98.87ms
step:1337/1770 train_time:131207ms step_avg:98.87ms
step:1338/1770 train_time:131309ms step_avg:98.88ms
step:1339/1770 train_time:131413ms step_avg:98.88ms
step:1340/1770 train_time:131517ms step_avg:98.88ms
step:1341/1770 train_time:131619ms step_avg:98.89ms
step:1342/1770 train_time:131722ms step_avg:98.89ms
step:1343/1770 train_time:131825ms step_avg:98.89ms
step:1344/1770 train_time:131929ms step_avg:98.90ms
step:1345/1770 train_time:132032ms step_avg:98.90ms
step:1346/1770 train_time:132135ms step_avg:98.90ms
step:1347/1770 train_time:132237ms step_avg:98.91ms
step:1348/1770 train_time:132341ms step_avg:98.91ms
step:1349/1770 train_time:132444ms step_avg:98.91ms
step:1350/1770 train_time:132547ms step_avg:98.92ms
step:1351/1770 train_time:132650ms step_avg:98.92ms
step:1352/1770 train_time:132753ms step_avg:98.92ms
step:1353/1770 train_time:132857ms step_avg:98.93ms
step:1354/1770 train_time:132959ms step_avg:98.93ms
step:1355/1770 train_time:133062ms step_avg:98.93ms
step:1356/1770 train_time:133164ms step_avg:98.93ms
step:1357/1770 train_time:133266ms step_avg:98.94ms
step:1358/1770 train_time:133369ms step_avg:98.94ms
step:1359/1770 train_time:133473ms step_avg:98.94ms
step:1360/1770 train_time:133576ms step_avg:98.95ms
step:1361/1770 train_time:133680ms step_avg:98.95ms
step:1362/1770 train_time:133783ms step_avg:98.95ms
step:1363/1770 train_time:133887ms step_avg:98.96ms
step:1364/1770 train_time:133991ms step_avg:98.96ms
step:1365/1770 train_time:134095ms step_avg:98.96ms
step:1366/1770 train_time:134196ms step_avg:98.96ms
step:1367/1770 train_time:134300ms step_avg:98.97ms
step:1368/1770 train_time:134402ms step_avg:98.97ms
step:1369/1770 train_time:134505ms step_avg:98.97ms
step:1370/1770 train_time:134608ms step_avg:98.98ms
step:1371/1770 train_time:134712ms step_avg:98.98ms
step:1372/1770 train_time:134815ms step_avg:98.98ms
step:1373/1770 train_time:134917ms step_avg:98.99ms
step:1374/1770 train_time:135020ms step_avg:98.99ms
step:1375/1770 train_time:135123ms step_avg:98.99ms
step:1375/1770 val_loss:3.3865 train_time:135224ms step_avg:99.07ms
step:1376/1770 train_time:135247ms step_avg:99.01ms
step:1377/1770 train_time:135337ms step_avg:99.00ms
step:1378/1770 train_time:135439ms step_avg:99.01ms
step:1379/1770 train_time:135541ms step_avg:99.01ms
step:1380/1770 train_time:135644ms step_avg:99.01ms
step:1381/1770 train_time:135747ms step_avg:99.01ms
step:1382/1770 train_time:135850ms step_avg:99.02ms
step:1383/1770 train_time:135953ms step_avg:99.02ms
step:1384/1770 train_time:136056ms step_avg:99.02ms
step:1385/1770 train_time:136158ms step_avg:99.02ms
step:1386/1770 train_time:136261ms step_avg:99.03ms
step:1387/1770 train_time:136364ms step_avg:99.03ms
step:1388/1770 train_time:136466ms step_avg:99.03ms
step:1389/1770 train_time:136570ms step_avg:99.04ms
step:1390/1770 train_time:136672ms step_avg:99.04ms
step:1391/1770 train_time:136774ms step_avg:99.04ms
step:1392/1770 train_time:136877ms step_avg:99.04ms
step:1393/1770 train_time:136979ms step_avg:99.05ms
step:1394/1770 train_time:137082ms step_avg:99.05ms
step:1395/1770 train_time:137186ms step_avg:99.05ms
step:1396/1770 train_time:137290ms step_avg:99.06ms
step:1397/1770 train_time:137393ms step_avg:99.06ms
step:1398/1770 train_time:137496ms step_avg:99.06ms
step:1399/1770 train_time:137598ms step_avg:99.06ms
step:1400/1770 train_time:137701ms step_avg:99.07ms
step:1401/1770 train_time:137805ms step_avg:99.07ms
step:1402/1770 train_time:137909ms step_avg:99.07ms
step:1403/1770 train_time:138012ms step_avg:99.08ms
step:1404/1770 train_time:138115ms step_avg:99.08ms
step:1405/1770 train_time:138217ms step_avg:99.08ms
step:1406/1770 train_time:138320ms step_avg:99.08ms
step:1407/1770 train_time:138422ms step_avg:99.09ms
step:1408/1770 train_time:138525ms step_avg:99.09ms
step:1409/1770 train_time:138629ms step_avg:99.09ms
step:1410/1770 train_time:138732ms step_avg:99.09ms
step:1411/1770 train_time:138834ms step_avg:99.10ms
step:1412/1770 train_time:138936ms step_avg:99.10ms
step:1413/1770 train_time:139038ms step_avg:99.10ms
step:1414/1770 train_time:139142ms step_avg:99.10ms
step:1415/1770 train_time:139245ms step_avg:99.11ms
step:1416/1770 train_time:139349ms step_avg:99.11ms
step:1417/1770 train_time:139452ms step_avg:99.11ms
step:1418/1770 train_time:139555ms step_avg:99.12ms
step:1419/1770 train_time:139658ms step_avg:99.12ms
step:1420/1770 train_time:139760ms step_avg:99.12ms
step:1421/1770 train_time:139863ms step_avg:99.12ms
step:1422/1770 train_time:139965ms step_avg:99.13ms
step:1423/1770 train_time:140069ms step_avg:99.13ms
step:1424/1770 train_time:140172ms step_avg:99.13ms
step:1425/1770 train_time:140274ms step_avg:99.13ms
step:1426/1770 train_time:140377ms step_avg:99.14ms
step:1427/1770 train_time:140478ms step_avg:99.14ms
step:1428/1770 train_time:140582ms step_avg:99.14ms
step:1429/1770 train_time:140685ms step_avg:99.14ms
step:1430/1770 train_time:140788ms step_avg:99.15ms
step:1431/1770 train_time:140892ms step_avg:99.15ms
step:1432/1770 train_time:140994ms step_avg:99.15ms
step:1433/1770 train_time:141096ms step_avg:99.15ms
step:1434/1770 train_time:141198ms step_avg:99.16ms
step:1435/1770 train_time:141300ms step_avg:99.16ms
step:1436/1770 train_time:141405ms step_avg:99.16ms
step:1437/1770 train_time:141508ms step_avg:99.16ms
step:1438/1770 train_time:141610ms step_avg:99.17ms
step:1439/1770 train_time:141712ms step_avg:99.17ms
step:1440/1770 train_time:141815ms step_avg:99.17ms
step:1441/1770 train_time:141920ms step_avg:99.18ms
step:1442/1770 train_time:142023ms step_avg:99.18ms
step:1443/1770 train_time:142126ms step_avg:99.18ms
step:1444/1770 train_time:142229ms step_avg:99.18ms
step:1445/1770 train_time:142332ms step_avg:99.19ms
step:1446/1770 train_time:142436ms step_avg:99.19ms
step:1447/1770 train_time:142540ms step_avg:99.19ms
step:1448/1770 train_time:142643ms step_avg:99.20ms
step:1449/1770 train_time:142748ms step_avg:99.20ms
step:1450/1770 train_time:142852ms step_avg:99.20ms
step:1451/1770 train_time:142956ms step_avg:99.21ms
step:1452/1770 train_time:143060ms step_avg:99.21ms
step:1453/1770 train_time:143163ms step_avg:99.21ms
step:1454/1770 train_time:143267ms step_avg:99.22ms
step:1455/1770 train_time:143372ms step_avg:99.22ms
step:1456/1770 train_time:143478ms step_avg:99.22ms
step:1457/1770 train_time:143581ms step_avg:99.23ms
step:1458/1770 train_time:143686ms step_avg:99.23ms
step:1459/1770 train_time:143792ms step_avg:99.24ms
step:1460/1770 train_time:143896ms step_avg:99.24ms
step:1461/1770 train_time:143999ms step_avg:99.24ms
step:1462/1770 train_time:144103ms step_avg:99.24ms
step:1463/1770 train_time:144207ms step_avg:99.25ms
step:1464/1770 train_time:144312ms step_avg:99.25ms
step:1465/1770 train_time:144416ms step_avg:99.25ms
step:1466/1770 train_time:144521ms step_avg:99.26ms
step:1467/1770 train_time:144626ms step_avg:99.26ms
step:1468/1770 train_time:144731ms step_avg:99.27ms
step:1469/1770 train_time:144834ms step_avg:99.27ms
step:1470/1770 train_time:144938ms step_avg:99.27ms
step:1471/1770 train_time:145042ms step_avg:99.28ms
step:1472/1770 train_time:145145ms step_avg:99.28ms
step:1473/1770 train_time:145249ms step_avg:99.28ms
step:1474/1770 train_time:145355ms step_avg:99.29ms
step:1475/1770 train_time:145458ms step_avg:99.29ms
step:1476/1770 train_time:145561ms step_avg:99.29ms
step:1477/1770 train_time:145668ms step_avg:99.30ms
step:1478/1770 train_time:145773ms step_avg:99.30ms
step:1479/1770 train_time:145876ms step_avg:99.30ms
step:1480/1770 train_time:145980ms step_avg:99.31ms
step:1481/1770 train_time:146087ms step_avg:99.31ms
step:1482/1770 train_time:146191ms step_avg:99.31ms
step:1483/1770 train_time:146295ms step_avg:99.32ms
step:1484/1770 train_time:146398ms step_avg:99.32ms
step:1485/1770 train_time:146501ms step_avg:99.32ms
step:1486/1770 train_time:146605ms step_avg:99.33ms
step:1487/1770 train_time:146709ms step_avg:99.33ms
step:1488/1770 train_time:146814ms step_avg:99.33ms
step:1489/1770 train_time:146918ms step_avg:99.34ms
step:1490/1770 train_time:147022ms step_avg:99.34ms
step:1491/1770 train_time:147125ms step_avg:99.34ms
step:1492/1770 train_time:147231ms step_avg:99.35ms
step:1493/1770 train_time:147338ms step_avg:99.35ms
step:1494/1770 train_time:147444ms step_avg:99.36ms
step:1495/1770 train_time:147547ms step_avg:99.36ms
step:1496/1770 train_time:147652ms step_avg:99.36ms
step:1497/1770 train_time:147756ms step_avg:99.36ms
step:1498/1770 train_time:147859ms step_avg:99.37ms
step:1499/1770 train_time:147962ms step_avg:99.37ms
step:1500/1770 train_time:148066ms step_avg:99.37ms
step:1500/1770 val_loss:3.3502 train_time:148168ms step_avg:99.44ms
step:1501/1770 train_time:148189ms step_avg:99.39ms
step:1502/1770 train_time:148278ms step_avg:99.38ms
step:1503/1770 train_time:148381ms step_avg:99.38ms
step:1504/1770 train_time:148485ms step_avg:99.39ms
step:1505/1770 train_time:148591ms step_avg:99.39ms
step:1506/1770 train_time:148696ms step_avg:99.40ms
step:1507/1770 train_time:148801ms step_avg:99.40ms
step:1508/1770 train_time:148906ms step_avg:99.40ms
step:1509/1770 train_time:149010ms step_avg:99.41ms
step:1510/1770 train_time:149114ms step_avg:99.41ms
step:1511/1770 train_time:149218ms step_avg:99.41ms
step:1512/1770 train_time:149322ms step_avg:99.42ms
step:1513/1770 train_time:149426ms step_avg:99.42ms
step:1514/1770 train_time:149530ms step_avg:99.42ms
step:1515/1770 train_time:149634ms step_avg:99.42ms
step:1516/1770 train_time:149738ms step_avg:99.43ms
step:1517/1770 train_time:149842ms step_avg:99.43ms
step:1518/1770 train_time:149948ms step_avg:99.43ms
step:1519/1770 train_time:150053ms step_avg:99.44ms
step:1520/1770 train_time:150157ms step_avg:99.44ms
step:1521/1770 train_time:150260ms step_avg:99.44ms
step:1522/1770 train_time:150364ms step_avg:99.45ms
step:1523/1770 train_time:150469ms step_avg:99.45ms
step:1524/1770 train_time:150573ms step_avg:99.45ms
step:1525/1770 train_time:150676ms step_avg:99.46ms
step:1526/1770 train_time:150780ms step_avg:99.46ms
step:1527/1770 train_time:150884ms step_avg:99.46ms
step:1528/1770 train_time:150990ms step_avg:99.47ms
step:1529/1770 train_time:151094ms step_avg:99.47ms
step:1530/1770 train_time:151197ms step_avg:99.47ms
step:1531/1770 train_time:151301ms step_avg:99.47ms
step:1532/1770 train_time:151405ms step_avg:99.48ms
step:1533/1770 train_time:151510ms step_avg:99.48ms
step:1534/1770 train_time:151615ms step_avg:99.49ms
step:1535/1770 train_time:151718ms step_avg:99.49ms
step:1536/1770 train_time:151821ms step_avg:99.49ms
step:1537/1770 train_time:151925ms step_avg:99.49ms
step:1538/1770 train_time:152030ms step_avg:99.50ms
step:1539/1770 train_time:152134ms step_avg:99.50ms
step:1540/1770 train_time:152241ms step_avg:99.50ms
step:1541/1770 train_time:152347ms step_avg:99.51ms
step:1542/1770 train_time:152450ms step_avg:99.51ms
step:1543/1770 train_time:152554ms step_avg:99.51ms
step:1544/1770 train_time:152660ms step_avg:99.52ms
step:1545/1770 train_time:152764ms step_avg:99.52ms
step:1546/1770 train_time:152868ms step_avg:99.52ms
step:1547/1770 train_time:152972ms step_avg:99.53ms
step:1548/1770 train_time:153076ms step_avg:99.53ms
step:1549/1770 train_time:153178ms step_avg:99.53ms
step:1550/1770 train_time:153282ms step_avg:99.53ms
step:1551/1770 train_time:153385ms step_avg:99.54ms
step:1552/1770 train_time:153490ms step_avg:99.54ms
step:1553/1770 train_time:153595ms step_avg:99.54ms
step:1554/1770 train_time:153699ms step_avg:99.55ms
step:1555/1770 train_time:153803ms step_avg:99.55ms
step:1556/1770 train_time:153907ms step_avg:99.55ms
step:1557/1770 train_time:154011ms step_avg:99.55ms
step:1558/1770 train_time:154115ms step_avg:99.56ms
step:1559/1770 train_time:154219ms step_avg:99.56ms
step:1560/1770 train_time:154322ms step_avg:99.56ms
step:1561/1770 train_time:154428ms step_avg:99.57ms
step:1562/1770 train_time:154532ms step_avg:99.57ms
step:1563/1770 train_time:154636ms step_avg:99.57ms
step:1564/1770 train_time:154738ms step_avg:99.57ms
step:1565/1770 train_time:154842ms step_avg:99.58ms
step:1566/1770 train_time:154945ms step_avg:99.58ms
step:1567/1770 train_time:155050ms step_avg:99.58ms
step:1568/1770 train_time:155153ms step_avg:99.58ms
step:1569/1770 train_time:155261ms step_avg:99.59ms
step:1570/1770 train_time:155364ms step_avg:99.59ms
step:1571/1770 train_time:155468ms step_avg:99.60ms
step:1572/1770 train_time:155575ms step_avg:99.60ms
step:1573/1770 train_time:155681ms step_avg:99.60ms
step:1574/1770 train_time:155784ms step_avg:99.61ms
step:1575/1770 train_time:155887ms step_avg:99.61ms
step:1576/1770 train_time:155991ms step_avg:99.61ms
step:1577/1770 train_time:156096ms step_avg:99.61ms
step:1578/1770 train_time:156203ms step_avg:99.62ms
step:1579/1770 train_time:156306ms step_avg:99.62ms
step:1580/1770 train_time:156411ms step_avg:99.62ms
step:1581/1770 train_time:156518ms step_avg:99.63ms
step:1582/1770 train_time:156622ms step_avg:99.63ms
step:1583/1770 train_time:156726ms step_avg:99.64ms
step:1584/1770 train_time:156832ms step_avg:99.64ms
step:1585/1770 train_time:156936ms step_avg:99.64ms
step:1586/1770 train_time:157044ms step_avg:99.65ms
step:1587/1770 train_time:157148ms step_avg:99.65ms
step:1588/1770 train_time:157253ms step_avg:99.65ms
step:1589/1770 train_time:157358ms step_avg:99.66ms
step:1590/1770 train_time:157462ms step_avg:99.66ms
step:1591/1770 train_time:157565ms step_avg:99.66ms
step:1592/1770 train_time:157670ms step_avg:99.66ms
step:1593/1770 train_time:157774ms step_avg:99.67ms
step:1594/1770 train_time:157878ms step_avg:99.67ms
step:1595/1770 train_time:157983ms step_avg:99.67ms
step:1596/1770 train_time:158088ms step_avg:99.68ms
step:1597/1770 train_time:158192ms step_avg:99.68ms
step:1598/1770 train_time:158296ms step_avg:99.68ms
step:1599/1770 train_time:158401ms step_avg:99.69ms
step:1600/1770 train_time:158508ms step_avg:99.69ms
step:1601/1770 train_time:158613ms step_avg:99.69ms
step:1602/1770 train_time:158718ms step_avg:99.70ms
step:1603/1770 train_time:158822ms step_avg:99.70ms
step:1604/1770 train_time:158924ms step_avg:99.70ms
step:1605/1770 train_time:159027ms step_avg:99.70ms
step:1606/1770 train_time:159132ms step_avg:99.71ms
step:1607/1770 train_time:159240ms step_avg:99.71ms
step:1608/1770 train_time:159344ms step_avg:99.71ms
step:1609/1770 train_time:159447ms step_avg:99.72ms
step:1610/1770 train_time:159553ms step_avg:99.72ms
step:1611/1770 train_time:159659ms step_avg:99.72ms
step:1612/1770 train_time:159764ms step_avg:99.73ms
step:1613/1770 train_time:159868ms step_avg:99.73ms
step:1614/1770 train_time:159973ms step_avg:99.73ms
step:1615/1770 train_time:160078ms step_avg:99.74ms
step:1616/1770 train_time:160181ms step_avg:99.74ms
step:1617/1770 train_time:160288ms step_avg:99.74ms
step:1618/1770 train_time:160393ms step_avg:99.75ms
step:1619/1770 train_time:160498ms step_avg:99.75ms
step:1620/1770 train_time:160602ms step_avg:99.75ms
step:1621/1770 train_time:160705ms step_avg:99.75ms
step:1622/1770 train_time:160811ms step_avg:99.76ms
step:1623/1770 train_time:160917ms step_avg:99.76ms
step:1624/1770 train_time:161021ms step_avg:99.77ms
step:1625/1770 train_time:161125ms step_avg:99.77ms
step:1625/1770 val_loss:3.3175 train_time:161227ms step_avg:99.83ms
step:1626/1770 train_time:161248ms step_avg:99.78ms
step:1627/1770 train_time:161338ms step_avg:99.78ms
step:1628/1770 train_time:161441ms step_avg:99.78ms
step:1629/1770 train_time:161545ms step_avg:99.78ms
step:1630/1770 train_time:161649ms step_avg:99.78ms
step:1631/1770 train_time:161753ms step_avg:99.79ms
step:1632/1770 train_time:161857ms step_avg:99.79ms
step:1633/1770 train_time:161962ms step_avg:99.79ms
step:1634/1770 train_time:162065ms step_avg:99.79ms
step:1635/1770 train_time:162168ms step_avg:99.80ms
step:1636/1770 train_time:162273ms step_avg:99.80ms
step:1637/1770 train_time:162379ms step_avg:99.80ms
step:1638/1770 train_time:162482ms step_avg:99.80ms
step:1639/1770 train_time:162586ms step_avg:99.81ms
step:1640/1770 train_time:162691ms step_avg:99.81ms
step:1641/1770 train_time:162795ms step_avg:99.81ms
step:1642/1770 train_time:162899ms step_avg:99.82ms
step:1643/1770 train_time:163003ms step_avg:99.82ms
step:1644/1770 train_time:163109ms step_avg:99.82ms
step:1645/1770 train_time:163213ms step_avg:99.82ms
step:1646/1770 train_time:163319ms step_avg:99.83ms
step:1647/1770 train_time:163424ms step_avg:99.83ms
step:1648/1770 train_time:163527ms step_avg:99.83ms
step:1649/1770 train_time:163632ms step_avg:99.84ms
step:1650/1770 train_time:163737ms step_avg:99.84ms
step:1651/1770 train_time:163840ms step_avg:99.84ms
step:1652/1770 train_time:163944ms step_avg:99.84ms
step:1653/1770 train_time:164049ms step_avg:99.85ms
step:1654/1770 train_time:164157ms step_avg:99.85ms
step:1655/1770 train_time:164263ms step_avg:99.86ms
step:1656/1770 train_time:164367ms step_avg:99.86ms
step:1657/1770 train_time:164473ms step_avg:99.86ms
step:1658/1770 train_time:164577ms step_avg:99.86ms
step:1659/1770 train_time:164683ms step_avg:99.87ms
step:1660/1770 train_time:164786ms step_avg:99.87ms
step:1661/1770 train_time:164892ms step_avg:99.87ms
step:1662/1770 train_time:164996ms step_avg:99.88ms
step:1663/1770 train_time:165100ms step_avg:99.88ms
step:1664/1770 train_time:165204ms step_avg:99.88ms
step:1665/1770 train_time:165307ms step_avg:99.88ms
step:1666/1770 train_time:165412ms step_avg:99.89ms
step:1667/1770 train_time:165516ms step_avg:99.89ms
step:1668/1770 train_time:165619ms step_avg:99.89ms
step:1669/1770 train_time:165722ms step_avg:99.89ms
step:1670/1770 train_time:165826ms step_avg:99.89ms
step:1671/1770 train_time:165930ms step_avg:99.90ms
step:1672/1770 train_time:166035ms step_avg:99.90ms
step:1673/1770 train_time:166140ms step_avg:99.90ms
step:1674/1770 train_time:166244ms step_avg:99.91ms
step:1675/1770 train_time:166347ms step_avg:99.91ms
step:1676/1770 train_time:166453ms step_avg:99.91ms
step:1677/1770 train_time:166560ms step_avg:99.92ms
step:1678/1770 train_time:166664ms step_avg:99.92ms
step:1679/1770 train_time:166768ms step_avg:99.92ms
step:1680/1770 train_time:166872ms step_avg:99.92ms
step:1681/1770 train_time:166977ms step_avg:99.93ms
step:1682/1770 train_time:167083ms step_avg:99.93ms
step:1683/1770 train_time:167186ms step_avg:99.93ms
step:1684/1770 train_time:167290ms step_avg:99.93ms
step:1685/1770 train_time:167395ms step_avg:99.94ms
step:1686/1770 train_time:167500ms step_avg:99.94ms
step:1687/1770 train_time:167605ms step_avg:99.94ms
step:1688/1770 train_time:167708ms step_avg:99.95ms
step:1689/1770 train_time:167812ms step_avg:99.95ms
step:1690/1770 train_time:167916ms step_avg:99.95ms
step:1691/1770 train_time:168020ms step_avg:99.95ms
step:1692/1770 train_time:168124ms step_avg:99.95ms
step:1693/1770 train_time:168229ms step_avg:99.96ms
step:1694/1770 train_time:168333ms step_avg:99.96ms
step:1695/1770 train_time:168438ms step_avg:99.96ms
step:1696/1770 train_time:168544ms step_avg:99.97ms
step:1697/1770 train_time:168650ms step_avg:99.97ms
step:1698/1770 train_time:168754ms step_avg:99.97ms
step:1699/1770 train_time:168858ms step_avg:99.98ms
step:1700/1770 train_time:168962ms step_avg:99.98ms
step:1701/1770 train_time:169065ms step_avg:99.98ms
step:1702/1770 train_time:169170ms step_avg:99.98ms
step:1703/1770 train_time:169273ms step_avg:99.98ms
step:1704/1770 train_time:169377ms step_avg:99.99ms
step:1705/1770 train_time:169481ms step_avg:99.99ms
step:1706/1770 train_time:169584ms step_avg:99.99ms
step:1707/1770 train_time:169688ms step_avg:99.99ms
step:1708/1770 train_time:169794ms step_avg:100.00ms
step:1709/1770 train_time:169900ms step_avg:100.00ms
step:1710/1770 train_time:170007ms step_avg:100.00ms
step:1711/1770 train_time:170114ms step_avg:100.01ms
step:1712/1770 train_time:170219ms step_avg:100.01ms
step:1713/1770 train_time:170323ms step_avg:100.01ms
step:1714/1770 train_time:170427ms step_avg:100.02ms
step:1715/1770 train_time:170532ms step_avg:100.02ms
step:1716/1770 train_time:170637ms step_avg:100.02ms
step:1717/1770 train_time:170741ms step_avg:100.02ms
step:1718/1770 train_time:170847ms step_avg:100.03ms
step:1719/1770 train_time:170954ms step_avg:100.03ms
step:1720/1770 train_time:171059ms step_avg:100.03ms
step:1721/1770 train_time:171163ms step_avg:100.04ms
step:1722/1770 train_time:171271ms step_avg:100.04ms
step:1723/1770 train_time:171378ms step_avg:100.05ms
step:1724/1770 train_time:171485ms step_avg:100.05ms
step:1725/1770 train_time:171592ms step_avg:100.05ms
step:1726/1770 train_time:171699ms step_avg:100.06ms
step:1727/1770 train_time:171803ms step_avg:100.06ms
step:1728/1770 train_time:171909ms step_avg:100.06ms
step:1729/1770 train_time:172014ms step_avg:100.07ms
step:1730/1770 train_time:172120ms step_avg:100.07ms
step:1731/1770 train_time:172226ms step_avg:100.07ms
step:1732/1770 train_time:172330ms step_avg:100.08ms
step:1733/1770 train_time:172437ms step_avg:100.08ms
step:1734/1770 train_time:172541ms step_avg:100.08ms
step:1735/1770 train_time:172646ms step_avg:100.08ms
step:1736/1770 train_time:172751ms step_avg:100.09ms
step:1737/1770 train_time:172856ms step_avg:100.09ms
step:1738/1770 train_time:172961ms step_avg:100.09ms
step:1739/1770 train_time:173066ms step_avg:100.10ms
step:1740/1770 train_time:173171ms step_avg:100.10ms
step:1741/1770 train_time:173279ms step_avg:100.10ms
step:1742/1770 train_time:173387ms step_avg:100.11ms
step:1743/1770 train_time:173493ms step_avg:100.11ms
step:1744/1770 train_time:173598ms step_avg:100.11ms
step:1745/1770 train_time:173703ms step_avg:100.12ms
step:1746/1770 train_time:173810ms step_avg:100.12ms
step:1747/1770 train_time:173914ms step_avg:100.12ms
step:1748/1770 train_time:174021ms step_avg:100.13ms
step:1749/1770 train_time:174126ms step_avg:100.13ms
step:1750/1770 train_time:174230ms step_avg:100.13ms
step:1750/1770 val_loss:3.2939 train_time:174334ms step_avg:100.19ms
step:1751/1770 train_time:174355ms step_avg:100.15ms
step:1752/1770 train_time:174445ms step_avg:100.14ms
step:1753/1770 train_time:174550ms step_avg:100.14ms
step:1754/1770 train_time:174655ms step_avg:100.15ms
step:1755/1770 train_time:174760ms step_avg:100.15ms
step:1756/1770 train_time:174864ms step_avg:100.15ms
step:1757/1770 train_time:174970ms step_avg:100.15ms
step:1758/1770 train_time:175075ms step_avg:100.16ms
step:1759/1770 train_time:175181ms step_avg:100.16ms
step:1760/1770 train_time:175286ms step_avg:100.16ms
step:1761/1770 train_time:175394ms step_avg:100.17ms
step:1762/1770 train_time:175502ms step_avg:100.17ms
step:1763/1770 train_time:175606ms step_avg:100.17ms
step:1764/1770 train_time:175711ms step_avg:100.18ms
step:1765/1770 train_time:175817ms step_avg:100.18ms
step:1766/1770 train_time:175926ms step_avg:100.19ms
step:1767/1770 train_time:176030ms step_avg:100.19ms
step:1768/1770 train_time:176135ms step_avg:100.19ms
step:1769/1770 train_time:176239ms step_avg:100.19ms
step:1770/1770 train_time:176343ms step_avg:100.19ms
step:1770/1770 val_loss:3.2910 train_time:176448ms step_avg:100.25ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
