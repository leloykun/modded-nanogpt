import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:07:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23518ms step_avg:nanms
step:2/1770 train_time:23933ms step_avg:nanms
step:3/1770 train_time:24027ms step_avg:nanms
step:4/1770 train_time:24203ms step_avg:nanms
step:5/1770 train_time:24297ms step_avg:nanms
step:6/1770 train_time:24391ms step_avg:nanms
step:7/1770 train_time:24486ms step_avg:nanms
step:8/1770 train_time:24580ms step_avg:nanms
step:9/1770 train_time:24675ms step_avg:nanms
step:10/1770 train_time:24769ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.19ms
step:14/1770 train_time:379ms step_avg:94.64ms
step:15/1770 train_time:474ms step_avg:94.71ms
step:16/1770 train_time:568ms step_avg:94.68ms
step:17/1770 train_time:663ms step_avg:94.66ms
step:18/1770 train_time:758ms step_avg:94.76ms
step:19/1770 train_time:853ms step_avg:94.76ms
step:20/1770 train_time:948ms step_avg:94.81ms
step:21/1770 train_time:1042ms step_avg:94.71ms
step:22/1770 train_time:1137ms step_avg:94.75ms
step:23/1770 train_time:1231ms step_avg:94.71ms
step:24/1770 train_time:1326ms step_avg:94.69ms
step:25/1770 train_time:1420ms step_avg:94.70ms
step:26/1770 train_time:1515ms step_avg:94.69ms
step:27/1770 train_time:1609ms step_avg:94.66ms
step:28/1770 train_time:1704ms step_avg:94.67ms
step:29/1770 train_time:1800ms step_avg:94.74ms
step:30/1770 train_time:1895ms step_avg:94.75ms
step:31/1770 train_time:1989ms step_avg:94.72ms
step:32/1770 train_time:2084ms step_avg:94.73ms
step:33/1770 train_time:2179ms step_avg:94.73ms
step:34/1770 train_time:2273ms step_avg:94.71ms
step:35/1770 train_time:2367ms step_avg:94.70ms
step:36/1770 train_time:2462ms step_avg:94.69ms
step:37/1770 train_time:2557ms step_avg:94.69ms
step:38/1770 train_time:2651ms step_avg:94.68ms
step:39/1770 train_time:2745ms step_avg:94.67ms
step:40/1770 train_time:2840ms step_avg:94.68ms
step:41/1770 train_time:2935ms step_avg:94.67ms
step:42/1770 train_time:3029ms step_avg:94.67ms
step:43/1770 train_time:3124ms step_avg:94.66ms
step:44/1770 train_time:3219ms step_avg:94.69ms
step:45/1770 train_time:3314ms step_avg:94.68ms
step:46/1770 train_time:3408ms step_avg:94.67ms
step:47/1770 train_time:3503ms step_avg:94.67ms
step:48/1770 train_time:3599ms step_avg:94.70ms
step:49/1770 train_time:3693ms step_avg:94.70ms
step:50/1770 train_time:3787ms step_avg:94.68ms
step:51/1770 train_time:3882ms step_avg:94.68ms
step:52/1770 train_time:3977ms step_avg:94.69ms
step:53/1770 train_time:4071ms step_avg:94.69ms
step:54/1770 train_time:4166ms step_avg:94.68ms
step:55/1770 train_time:4261ms step_avg:94.68ms
step:56/1770 train_time:4356ms step_avg:94.69ms
step:57/1770 train_time:4450ms step_avg:94.68ms
step:58/1770 train_time:4544ms step_avg:94.68ms
step:59/1770 train_time:4639ms step_avg:94.67ms
step:60/1770 train_time:4733ms step_avg:94.66ms
step:61/1770 train_time:4828ms step_avg:94.67ms
step:62/1770 train_time:4922ms step_avg:94.65ms
step:63/1770 train_time:5017ms step_avg:94.66ms
step:64/1770 train_time:5112ms step_avg:94.67ms
step:65/1770 train_time:5207ms step_avg:94.67ms
step:66/1770 train_time:5302ms step_avg:94.68ms
step:67/1770 train_time:5397ms step_avg:94.68ms
step:68/1770 train_time:5492ms step_avg:94.68ms
step:69/1770 train_time:5586ms step_avg:94.68ms
step:70/1770 train_time:5681ms step_avg:94.68ms
step:71/1770 train_time:5776ms step_avg:94.68ms
step:72/1770 train_time:5870ms step_avg:94.67ms
step:73/1770 train_time:5965ms step_avg:94.68ms
step:74/1770 train_time:6059ms step_avg:94.67ms
step:75/1770 train_time:6154ms step_avg:94.67ms
step:76/1770 train_time:6248ms step_avg:94.67ms
step:77/1770 train_time:6343ms step_avg:94.67ms
step:78/1770 train_time:6438ms step_avg:94.68ms
step:79/1770 train_time:6532ms step_avg:94.67ms
step:80/1770 train_time:6627ms step_avg:94.67ms
step:81/1770 train_time:6722ms step_avg:94.67ms
step:82/1770 train_time:6816ms step_avg:94.67ms
step:83/1770 train_time:6911ms step_avg:94.67ms
step:84/1770 train_time:7006ms step_avg:94.67ms
step:85/1770 train_time:7101ms step_avg:94.67ms
step:86/1770 train_time:7195ms step_avg:94.67ms
step:87/1770 train_time:7290ms step_avg:94.68ms
step:88/1770 train_time:7385ms step_avg:94.67ms
step:89/1770 train_time:7479ms step_avg:94.67ms
step:90/1770 train_time:7574ms step_avg:94.67ms
step:91/1770 train_time:7668ms step_avg:94.67ms
step:92/1770 train_time:7763ms step_avg:94.67ms
step:93/1770 train_time:7857ms step_avg:94.66ms
step:94/1770 train_time:7951ms step_avg:94.66ms
step:95/1770 train_time:8046ms step_avg:94.66ms
step:96/1770 train_time:8140ms step_avg:94.65ms
step:97/1770 train_time:8235ms step_avg:94.65ms
step:98/1770 train_time:8329ms step_avg:94.65ms
step:99/1770 train_time:8424ms step_avg:94.66ms
step:100/1770 train_time:8519ms step_avg:94.66ms
step:101/1770 train_time:8613ms step_avg:94.65ms
step:102/1770 train_time:8708ms step_avg:94.65ms
step:103/1770 train_time:8803ms step_avg:94.65ms
step:104/1770 train_time:8897ms step_avg:94.65ms
step:105/1770 train_time:8992ms step_avg:94.66ms
step:106/1770 train_time:9087ms step_avg:94.65ms
step:107/1770 train_time:9182ms step_avg:94.65ms
step:108/1770 train_time:9276ms step_avg:94.65ms
step:109/1770 train_time:9371ms step_avg:94.65ms
step:110/1770 train_time:9465ms step_avg:94.65ms
step:111/1770 train_time:9560ms step_avg:94.65ms
step:112/1770 train_time:9655ms step_avg:94.66ms
step:113/1770 train_time:9750ms step_avg:94.66ms
step:114/1770 train_time:9843ms step_avg:94.65ms
step:115/1770 train_time:9939ms step_avg:94.65ms
step:116/1770 train_time:10034ms step_avg:94.66ms
step:117/1770 train_time:10128ms step_avg:94.65ms
step:118/1770 train_time:10223ms step_avg:94.66ms
step:119/1770 train_time:10318ms step_avg:94.66ms
step:120/1770 train_time:10413ms step_avg:94.66ms
step:121/1770 train_time:10507ms step_avg:94.66ms
step:122/1770 train_time:10602ms step_avg:94.66ms
step:123/1770 train_time:10696ms step_avg:94.66ms
step:124/1770 train_time:10791ms step_avg:94.66ms
step:125/1770 train_time:10885ms step_avg:94.65ms
step:125/1770 val_loss:4.6552 train_time:10979ms step_avg:95.47ms
step:126/1770 train_time:11002ms step_avg:94.84ms
step:127/1770 train_time:11084ms step_avg:94.73ms
step:128/1770 train_time:11185ms step_avg:94.78ms
step:129/1770 train_time:11280ms step_avg:94.79ms
step:130/1770 train_time:11375ms step_avg:94.79ms
step:131/1770 train_time:11469ms step_avg:94.79ms
step:132/1770 train_time:11564ms step_avg:94.78ms
step:133/1770 train_time:11659ms step_avg:94.79ms
step:134/1770 train_time:11754ms step_avg:94.79ms
step:135/1770 train_time:11848ms step_avg:94.79ms
step:136/1770 train_time:11943ms step_avg:94.79ms
step:137/1770 train_time:12039ms step_avg:94.80ms
step:138/1770 train_time:12134ms step_avg:94.80ms
step:139/1770 train_time:12230ms step_avg:94.80ms
step:140/1770 train_time:12325ms step_avg:94.81ms
step:141/1770 train_time:12420ms step_avg:94.81ms
step:142/1770 train_time:12516ms step_avg:94.82ms
step:143/1770 train_time:12611ms step_avg:94.82ms
step:144/1770 train_time:12706ms step_avg:94.82ms
step:145/1770 train_time:12801ms step_avg:94.82ms
step:146/1770 train_time:12896ms step_avg:94.83ms
step:147/1770 train_time:12991ms step_avg:94.83ms
step:148/1770 train_time:13086ms step_avg:94.83ms
step:149/1770 train_time:13181ms step_avg:94.83ms
step:150/1770 train_time:13277ms step_avg:94.84ms
step:151/1770 train_time:13378ms step_avg:94.88ms
step:152/1770 train_time:13473ms step_avg:94.88ms
step:153/1770 train_time:13568ms step_avg:94.88ms
step:154/1770 train_time:13663ms step_avg:94.88ms
step:155/1770 train_time:13759ms step_avg:94.89ms
step:156/1770 train_time:13854ms step_avg:94.89ms
step:157/1770 train_time:13949ms step_avg:94.89ms
step:158/1770 train_time:14043ms step_avg:94.89ms
step:159/1770 train_time:14139ms step_avg:94.89ms
step:160/1770 train_time:14234ms step_avg:94.89ms
step:161/1770 train_time:14329ms step_avg:94.89ms
step:162/1770 train_time:14424ms step_avg:94.90ms
step:163/1770 train_time:14521ms step_avg:94.91ms
step:164/1770 train_time:14616ms step_avg:94.91ms
step:165/1770 train_time:14711ms step_avg:94.91ms
step:166/1770 train_time:14806ms step_avg:94.91ms
step:167/1770 train_time:14901ms step_avg:94.91ms
step:168/1770 train_time:14996ms step_avg:94.91ms
step:169/1770 train_time:15092ms step_avg:94.92ms
step:170/1770 train_time:15186ms step_avg:94.92ms
step:171/1770 train_time:15282ms step_avg:94.92ms
step:172/1770 train_time:15377ms step_avg:94.92ms
step:173/1770 train_time:15472ms step_avg:94.92ms
step:174/1770 train_time:15567ms step_avg:94.92ms
step:175/1770 train_time:15662ms step_avg:94.92ms
step:176/1770 train_time:15758ms step_avg:94.93ms
step:177/1770 train_time:15852ms step_avg:94.92ms
step:178/1770 train_time:15947ms step_avg:94.92ms
step:179/1770 train_time:16042ms step_avg:94.92ms
step:180/1770 train_time:16138ms step_avg:94.93ms
step:181/1770 train_time:16233ms step_avg:94.93ms
step:182/1770 train_time:16329ms step_avg:94.93ms
step:183/1770 train_time:16423ms step_avg:94.93ms
step:184/1770 train_time:16518ms step_avg:94.93ms
step:185/1770 train_time:16614ms step_avg:94.94ms
step:186/1770 train_time:16709ms step_avg:94.94ms
step:187/1770 train_time:16803ms step_avg:94.93ms
step:188/1770 train_time:16899ms step_avg:94.94ms
step:189/1770 train_time:16993ms step_avg:94.94ms
step:190/1770 train_time:17088ms step_avg:94.93ms
step:191/1770 train_time:17184ms step_avg:94.94ms
step:192/1770 train_time:17280ms step_avg:94.95ms
step:193/1770 train_time:17376ms step_avg:94.95ms
step:194/1770 train_time:17471ms step_avg:94.95ms
step:195/1770 train_time:17566ms step_avg:94.95ms
step:196/1770 train_time:17662ms step_avg:94.95ms
step:197/1770 train_time:17758ms step_avg:94.96ms
step:198/1770 train_time:17853ms step_avg:94.96ms
step:199/1770 train_time:17948ms step_avg:94.96ms
step:200/1770 train_time:18043ms step_avg:94.96ms
step:201/1770 train_time:18138ms step_avg:94.97ms
step:202/1770 train_time:18234ms step_avg:94.97ms
step:203/1770 train_time:18329ms step_avg:94.97ms
step:204/1770 train_time:18424ms step_avg:94.97ms
step:205/1770 train_time:18519ms step_avg:94.97ms
step:206/1770 train_time:18614ms step_avg:94.97ms
step:207/1770 train_time:18710ms step_avg:94.97ms
step:208/1770 train_time:18805ms step_avg:94.97ms
step:209/1770 train_time:18900ms step_avg:94.97ms
step:210/1770 train_time:18996ms step_avg:94.98ms
step:211/1770 train_time:19091ms step_avg:94.98ms
step:212/1770 train_time:19186ms step_avg:94.98ms
step:213/1770 train_time:19281ms step_avg:94.98ms
step:214/1770 train_time:19376ms step_avg:94.98ms
step:215/1770 train_time:19472ms step_avg:94.98ms
step:216/1770 train_time:19566ms step_avg:94.98ms
step:217/1770 train_time:19661ms step_avg:94.98ms
step:218/1770 train_time:19757ms step_avg:94.98ms
step:219/1770 train_time:19852ms step_avg:94.99ms
step:220/1770 train_time:19947ms step_avg:94.98ms
step:221/1770 train_time:20042ms step_avg:94.99ms
step:222/1770 train_time:20137ms step_avg:94.99ms
step:223/1770 train_time:20233ms step_avg:94.99ms
step:224/1770 train_time:20327ms step_avg:94.99ms
step:225/1770 train_time:20422ms step_avg:94.99ms
step:226/1770 train_time:20518ms step_avg:94.99ms
step:227/1770 train_time:20613ms step_avg:94.99ms
step:228/1770 train_time:20708ms step_avg:94.99ms
step:229/1770 train_time:20803ms step_avg:94.99ms
step:230/1770 train_time:20898ms step_avg:94.99ms
step:231/1770 train_time:20993ms step_avg:94.99ms
step:232/1770 train_time:21088ms step_avg:94.99ms
step:233/1770 train_time:21183ms step_avg:94.99ms
step:234/1770 train_time:21279ms step_avg:94.99ms
step:235/1770 train_time:21375ms step_avg:95.00ms
step:236/1770 train_time:21471ms step_avg:95.01ms
step:237/1770 train_time:21566ms step_avg:95.00ms
step:238/1770 train_time:21661ms step_avg:95.00ms
step:239/1770 train_time:21756ms step_avg:95.00ms
step:240/1770 train_time:21851ms step_avg:95.01ms
step:241/1770 train_time:21946ms step_avg:95.00ms
step:242/1770 train_time:22041ms step_avg:95.01ms
step:243/1770 train_time:22136ms step_avg:95.01ms
step:244/1770 train_time:22232ms step_avg:95.01ms
step:245/1770 train_time:22327ms step_avg:95.01ms
step:246/1770 train_time:22422ms step_avg:95.01ms
step:247/1770 train_time:22517ms step_avg:95.01ms
step:248/1770 train_time:22613ms step_avg:95.01ms
step:249/1770 train_time:22708ms step_avg:95.01ms
step:250/1770 train_time:22803ms step_avg:95.01ms
step:250/1770 val_loss:4.1059 train_time:22896ms step_avg:95.40ms
step:251/1770 train_time:22920ms step_avg:95.11ms
step:252/1770 train_time:23004ms step_avg:95.06ms
step:253/1770 train_time:23104ms step_avg:95.08ms
step:254/1770 train_time:23202ms step_avg:95.09ms
step:255/1770 train_time:23297ms step_avg:95.09ms
step:256/1770 train_time:23392ms step_avg:95.09ms
step:257/1770 train_time:23488ms step_avg:95.09ms
step:258/1770 train_time:23583ms step_avg:95.09ms
step:259/1770 train_time:23678ms step_avg:95.09ms
step:260/1770 train_time:23773ms step_avg:95.09ms
step:261/1770 train_time:23868ms step_avg:95.09ms
step:262/1770 train_time:23963ms step_avg:95.09ms
step:263/1770 train_time:24058ms step_avg:95.09ms
step:264/1770 train_time:24155ms step_avg:95.10ms
step:265/1770 train_time:24251ms step_avg:95.10ms
step:266/1770 train_time:24346ms step_avg:95.10ms
step:267/1770 train_time:24442ms step_avg:95.10ms
step:268/1770 train_time:24537ms step_avg:95.10ms
step:269/1770 train_time:24633ms step_avg:95.11ms
step:270/1770 train_time:24728ms step_avg:95.11ms
step:271/1770 train_time:24824ms step_avg:95.11ms
step:272/1770 train_time:24919ms step_avg:95.11ms
step:273/1770 train_time:25015ms step_avg:95.11ms
step:274/1770 train_time:25111ms step_avg:95.12ms
step:275/1770 train_time:25207ms step_avg:95.12ms
step:276/1770 train_time:25303ms step_avg:95.12ms
step:277/1770 train_time:25398ms step_avg:95.12ms
step:278/1770 train_time:25494ms step_avg:95.13ms
step:279/1770 train_time:25591ms step_avg:95.13ms
step:280/1770 train_time:25686ms step_avg:95.13ms
step:281/1770 train_time:25782ms step_avg:95.14ms
step:282/1770 train_time:25877ms step_avg:95.14ms
step:283/1770 train_time:25973ms step_avg:95.14ms
step:284/1770 train_time:26068ms step_avg:95.14ms
step:285/1770 train_time:26164ms step_avg:95.14ms
step:286/1770 train_time:26260ms step_avg:95.14ms
step:287/1770 train_time:26356ms step_avg:95.15ms
step:288/1770 train_time:26452ms step_avg:95.15ms
step:289/1770 train_time:26548ms step_avg:95.15ms
step:290/1770 train_time:26644ms step_avg:95.16ms
step:291/1770 train_time:26739ms step_avg:95.16ms
step:292/1770 train_time:26834ms step_avg:95.16ms
step:293/1770 train_time:26930ms step_avg:95.16ms
step:294/1770 train_time:27026ms step_avg:95.16ms
step:295/1770 train_time:27122ms step_avg:95.16ms
step:296/1770 train_time:27217ms step_avg:95.16ms
step:297/1770 train_time:27313ms step_avg:95.17ms
step:298/1770 train_time:27408ms step_avg:95.17ms
step:299/1770 train_time:27504ms step_avg:95.17ms
step:300/1770 train_time:27599ms step_avg:95.17ms
step:301/1770 train_time:27695ms step_avg:95.17ms
step:302/1770 train_time:27791ms step_avg:95.17ms
step:303/1770 train_time:27886ms step_avg:95.18ms
step:304/1770 train_time:27982ms step_avg:95.18ms
step:305/1770 train_time:28077ms step_avg:95.18ms
step:306/1770 train_time:28172ms step_avg:95.18ms
step:307/1770 train_time:28268ms step_avg:95.18ms
step:308/1770 train_time:28364ms step_avg:95.18ms
step:309/1770 train_time:28459ms step_avg:95.18ms
step:310/1770 train_time:28555ms step_avg:95.18ms
step:311/1770 train_time:28651ms step_avg:95.19ms
step:312/1770 train_time:28746ms step_avg:95.19ms
step:313/1770 train_time:28842ms step_avg:95.19ms
step:314/1770 train_time:28937ms step_avg:95.19ms
step:315/1770 train_time:29033ms step_avg:95.19ms
step:316/1770 train_time:29129ms step_avg:95.19ms
step:317/1770 train_time:29224ms step_avg:95.19ms
step:318/1770 train_time:29319ms step_avg:95.19ms
step:319/1770 train_time:29414ms step_avg:95.19ms
step:320/1770 train_time:29510ms step_avg:95.20ms
step:321/1770 train_time:29606ms step_avg:95.20ms
step:322/1770 train_time:29702ms step_avg:95.20ms
step:323/1770 train_time:29798ms step_avg:95.20ms
step:324/1770 train_time:29894ms step_avg:95.20ms
step:325/1770 train_time:29989ms step_avg:95.20ms
step:326/1770 train_time:30085ms step_avg:95.21ms
step:327/1770 train_time:30180ms step_avg:95.21ms
step:328/1770 train_time:30276ms step_avg:95.21ms
step:329/1770 train_time:30372ms step_avg:95.21ms
step:330/1770 train_time:30467ms step_avg:95.21ms
step:331/1770 train_time:30563ms step_avg:95.21ms
step:332/1770 train_time:30658ms step_avg:95.21ms
step:333/1770 train_time:30755ms step_avg:95.22ms
step:334/1770 train_time:30851ms step_avg:95.22ms
step:335/1770 train_time:30947ms step_avg:95.22ms
step:336/1770 train_time:31043ms step_avg:95.22ms
step:337/1770 train_time:31138ms step_avg:95.22ms
step:338/1770 train_time:31234ms step_avg:95.23ms
step:339/1770 train_time:31330ms step_avg:95.23ms
step:340/1770 train_time:31425ms step_avg:95.23ms
step:341/1770 train_time:31520ms step_avg:95.23ms
step:342/1770 train_time:31616ms step_avg:95.23ms
step:343/1770 train_time:31711ms step_avg:95.23ms
step:344/1770 train_time:31807ms step_avg:95.23ms
step:345/1770 train_time:31903ms step_avg:95.23ms
step:346/1770 train_time:31998ms step_avg:95.23ms
step:347/1770 train_time:32094ms step_avg:95.24ms
step:348/1770 train_time:32191ms step_avg:95.24ms
step:349/1770 train_time:32287ms step_avg:95.24ms
step:350/1770 train_time:32382ms step_avg:95.24ms
step:351/1770 train_time:32477ms step_avg:95.24ms
step:352/1770 train_time:32573ms step_avg:95.24ms
step:353/1770 train_time:32668ms step_avg:95.24ms
step:354/1770 train_time:32765ms step_avg:95.25ms
step:355/1770 train_time:32860ms step_avg:95.25ms
step:356/1770 train_time:32956ms step_avg:95.25ms
step:357/1770 train_time:33052ms step_avg:95.25ms
step:358/1770 train_time:33148ms step_avg:95.25ms
step:359/1770 train_time:33243ms step_avg:95.25ms
step:360/1770 train_time:33339ms step_avg:95.25ms
step:361/1770 train_time:33434ms step_avg:95.25ms
step:362/1770 train_time:33531ms step_avg:95.26ms
step:363/1770 train_time:33627ms step_avg:95.26ms
step:364/1770 train_time:33723ms step_avg:95.26ms
step:365/1770 train_time:33819ms step_avg:95.27ms
step:366/1770 train_time:33915ms step_avg:95.27ms
step:367/1770 train_time:34011ms step_avg:95.27ms
step:368/1770 train_time:34107ms step_avg:95.27ms
step:369/1770 train_time:34203ms step_avg:95.27ms
step:370/1770 train_time:34298ms step_avg:95.27ms
step:371/1770 train_time:34394ms step_avg:95.27ms
step:372/1770 train_time:34490ms step_avg:95.28ms
step:373/1770 train_time:34586ms step_avg:95.28ms
step:374/1770 train_time:34681ms step_avg:95.28ms
step:375/1770 train_time:34776ms step_avg:95.28ms
step:375/1770 val_loss:3.9013 train_time:34871ms step_avg:95.54ms
step:376/1770 train_time:34892ms step_avg:95.33ms
step:377/1770 train_time:34979ms step_avg:95.31ms
step:378/1770 train_time:35081ms step_avg:95.33ms
step:379/1770 train_time:35178ms step_avg:95.33ms
step:380/1770 train_time:35274ms step_avg:95.33ms
step:381/1770 train_time:35369ms step_avg:95.33ms
step:382/1770 train_time:35464ms step_avg:95.33ms
step:383/1770 train_time:35560ms step_avg:95.33ms
step:384/1770 train_time:35655ms step_avg:95.33ms
step:385/1770 train_time:35750ms step_avg:95.33ms
step:386/1770 train_time:35845ms step_avg:95.33ms
step:387/1770 train_time:35941ms step_avg:95.33ms
step:388/1770 train_time:36037ms step_avg:95.34ms
step:389/1770 train_time:36133ms step_avg:95.34ms
step:390/1770 train_time:36229ms step_avg:95.34ms
step:391/1770 train_time:36324ms step_avg:95.34ms
step:392/1770 train_time:36420ms step_avg:95.34ms
step:393/1770 train_time:36516ms step_avg:95.34ms
step:394/1770 train_time:36612ms step_avg:95.34ms
step:395/1770 train_time:36707ms step_avg:95.34ms
step:396/1770 train_time:36805ms step_avg:95.35ms
step:397/1770 train_time:36902ms step_avg:95.35ms
step:398/1770 train_time:37000ms step_avg:95.36ms
step:399/1770 train_time:37098ms step_avg:95.37ms
step:400/1770 train_time:37196ms step_avg:95.37ms
step:401/1770 train_time:37293ms step_avg:95.38ms
step:402/1770 train_time:37390ms step_avg:95.38ms
step:403/1770 train_time:37487ms step_avg:95.39ms
step:404/1770 train_time:37584ms step_avg:95.39ms
step:405/1770 train_time:37682ms step_avg:95.40ms
step:406/1770 train_time:37780ms step_avg:95.40ms
step:407/1770 train_time:37877ms step_avg:95.41ms
step:408/1770 train_time:37974ms step_avg:95.41ms
step:409/1770 train_time:38071ms step_avg:95.42ms
step:410/1770 train_time:38168ms step_avg:95.42ms
step:411/1770 train_time:38267ms step_avg:95.43ms
step:412/1770 train_time:38364ms step_avg:95.43ms
step:413/1770 train_time:38462ms step_avg:95.44ms
step:414/1770 train_time:38560ms step_avg:95.45ms
step:415/1770 train_time:38657ms step_avg:95.45ms
step:416/1770 train_time:38755ms step_avg:95.45ms
step:417/1770 train_time:38852ms step_avg:95.46ms
step:418/1770 train_time:38949ms step_avg:95.46ms
step:419/1770 train_time:39047ms step_avg:95.47ms
step:420/1770 train_time:39144ms step_avg:95.47ms
step:421/1770 train_time:39242ms step_avg:95.48ms
step:422/1770 train_time:39339ms step_avg:95.48ms
step:423/1770 train_time:39436ms step_avg:95.49ms
step:424/1770 train_time:39534ms step_avg:95.49ms
step:425/1770 train_time:39631ms step_avg:95.50ms
step:426/1770 train_time:39729ms step_avg:95.50ms
step:427/1770 train_time:39825ms step_avg:95.50ms
step:428/1770 train_time:39924ms step_avg:95.51ms
step:429/1770 train_time:40022ms step_avg:95.52ms
step:430/1770 train_time:40119ms step_avg:95.52ms
step:431/1770 train_time:40216ms step_avg:95.53ms
step:432/1770 train_time:40314ms step_avg:95.53ms
step:433/1770 train_time:40411ms step_avg:95.53ms
step:434/1770 train_time:40509ms step_avg:95.54ms
step:435/1770 train_time:40606ms step_avg:95.54ms
step:436/1770 train_time:40704ms step_avg:95.55ms
step:437/1770 train_time:40801ms step_avg:95.55ms
step:438/1770 train_time:40899ms step_avg:95.56ms
step:439/1770 train_time:40997ms step_avg:95.56ms
step:440/1770 train_time:41094ms step_avg:95.57ms
step:441/1770 train_time:41191ms step_avg:95.57ms
step:442/1770 train_time:41288ms step_avg:95.57ms
step:443/1770 train_time:41385ms step_avg:95.58ms
step:444/1770 train_time:41483ms step_avg:95.58ms
step:445/1770 train_time:41581ms step_avg:95.59ms
step:446/1770 train_time:41678ms step_avg:95.59ms
step:447/1770 train_time:41776ms step_avg:95.60ms
step:448/1770 train_time:41873ms step_avg:95.60ms
step:449/1770 train_time:41970ms step_avg:95.60ms
step:450/1770 train_time:42068ms step_avg:95.61ms
step:451/1770 train_time:42166ms step_avg:95.61ms
step:452/1770 train_time:42263ms step_avg:95.62ms
step:453/1770 train_time:42361ms step_avg:95.62ms
step:454/1770 train_time:42458ms step_avg:95.63ms
step:455/1770 train_time:42556ms step_avg:95.63ms
step:456/1770 train_time:42653ms step_avg:95.63ms
step:457/1770 train_time:42750ms step_avg:95.64ms
step:458/1770 train_time:42847ms step_avg:95.64ms
step:459/1770 train_time:42945ms step_avg:95.65ms
step:460/1770 train_time:43042ms step_avg:95.65ms
step:461/1770 train_time:43140ms step_avg:95.65ms
step:462/1770 train_time:43237ms step_avg:95.66ms
step:463/1770 train_time:43334ms step_avg:95.66ms
step:464/1770 train_time:43432ms step_avg:95.67ms
step:465/1770 train_time:43529ms step_avg:95.67ms
step:466/1770 train_time:43626ms step_avg:95.67ms
step:467/1770 train_time:43724ms step_avg:95.68ms
step:468/1770 train_time:43821ms step_avg:95.68ms
step:469/1770 train_time:43919ms step_avg:95.68ms
step:470/1770 train_time:44017ms step_avg:95.69ms
step:471/1770 train_time:44114ms step_avg:95.69ms
step:472/1770 train_time:44211ms step_avg:95.70ms
step:473/1770 train_time:44309ms step_avg:95.70ms
step:474/1770 train_time:44407ms step_avg:95.70ms
step:475/1770 train_time:44504ms step_avg:95.71ms
step:476/1770 train_time:44601ms step_avg:95.71ms
step:477/1770 train_time:44699ms step_avg:95.71ms
step:478/1770 train_time:44796ms step_avg:95.72ms
step:479/1770 train_time:44893ms step_avg:95.72ms
step:480/1770 train_time:44991ms step_avg:95.73ms
step:481/1770 train_time:45088ms step_avg:95.73ms
step:482/1770 train_time:45185ms step_avg:95.73ms
step:483/1770 train_time:45283ms step_avg:95.74ms
step:484/1770 train_time:45383ms step_avg:95.74ms
step:485/1770 train_time:45481ms step_avg:95.75ms
step:486/1770 train_time:45578ms step_avg:95.75ms
step:487/1770 train_time:45675ms step_avg:95.76ms
step:488/1770 train_time:45773ms step_avg:95.76ms
step:489/1770 train_time:45869ms step_avg:95.76ms
step:490/1770 train_time:45967ms step_avg:95.76ms
step:491/1770 train_time:46065ms step_avg:95.77ms
step:492/1770 train_time:46164ms step_avg:95.78ms
step:493/1770 train_time:46261ms step_avg:95.78ms
step:494/1770 train_time:46360ms step_avg:95.78ms
step:495/1770 train_time:46457ms step_avg:95.79ms
step:496/1770 train_time:46554ms step_avg:95.79ms
step:497/1770 train_time:46651ms step_avg:95.79ms
step:498/1770 train_time:46748ms step_avg:95.80ms
step:499/1770 train_time:46846ms step_avg:95.80ms
step:500/1770 train_time:46944ms step_avg:95.80ms
step:500/1770 val_loss:3.7551 train_time:47040ms step_avg:96.00ms
step:501/1770 train_time:47062ms step_avg:95.85ms
step:502/1770 train_time:47152ms step_avg:95.84ms
step:503/1770 train_time:47253ms step_avg:95.85ms
step:504/1770 train_time:47351ms step_avg:95.85ms
step:505/1770 train_time:47448ms step_avg:95.85ms
step:506/1770 train_time:47545ms step_avg:95.86ms
step:507/1770 train_time:47642ms step_avg:95.86ms
step:508/1770 train_time:47740ms step_avg:95.86ms
step:509/1770 train_time:47838ms step_avg:95.87ms
step:510/1770 train_time:47935ms step_avg:95.87ms
step:511/1770 train_time:48032ms step_avg:95.87ms
step:512/1770 train_time:48129ms step_avg:95.87ms
step:513/1770 train_time:48226ms step_avg:95.88ms
step:514/1770 train_time:48323ms step_avg:95.88ms
step:515/1770 train_time:48421ms step_avg:95.88ms
step:516/1770 train_time:48520ms step_avg:95.89ms
step:517/1770 train_time:48617ms step_avg:95.89ms
step:518/1770 train_time:48715ms step_avg:95.90ms
step:519/1770 train_time:48812ms step_avg:95.90ms
step:520/1770 train_time:48910ms step_avg:95.90ms
step:521/1770 train_time:49007ms step_avg:95.90ms
step:522/1770 train_time:49104ms step_avg:95.91ms
step:523/1770 train_time:49202ms step_avg:95.91ms
step:524/1770 train_time:49299ms step_avg:95.91ms
step:525/1770 train_time:49398ms step_avg:95.92ms
step:526/1770 train_time:49495ms step_avg:95.92ms
step:527/1770 train_time:49593ms step_avg:95.92ms
step:528/1770 train_time:49691ms step_avg:95.93ms
step:529/1770 train_time:49788ms step_avg:95.93ms
step:530/1770 train_time:49886ms step_avg:95.93ms
step:531/1770 train_time:49984ms step_avg:95.94ms
step:532/1770 train_time:50081ms step_avg:95.94ms
step:533/1770 train_time:50179ms step_avg:95.95ms
step:534/1770 train_time:50277ms step_avg:95.95ms
step:535/1770 train_time:50375ms step_avg:95.95ms
step:536/1770 train_time:50474ms step_avg:95.96ms
step:537/1770 train_time:50572ms step_avg:95.96ms
step:538/1770 train_time:50670ms step_avg:95.97ms
step:539/1770 train_time:50767ms step_avg:95.97ms
step:540/1770 train_time:50865ms step_avg:95.97ms
step:541/1770 train_time:50963ms step_avg:95.98ms
step:542/1770 train_time:51061ms step_avg:95.98ms
step:543/1770 train_time:51159ms step_avg:95.98ms
step:544/1770 train_time:51257ms step_avg:95.99ms
step:545/1770 train_time:51355ms step_avg:95.99ms
step:546/1770 train_time:51453ms step_avg:95.99ms
step:547/1770 train_time:51551ms step_avg:96.00ms
step:548/1770 train_time:51648ms step_avg:96.00ms
step:549/1770 train_time:51746ms step_avg:96.00ms
step:550/1770 train_time:51844ms step_avg:96.01ms
step:551/1770 train_time:51942ms step_avg:96.01ms
step:552/1770 train_time:52041ms step_avg:96.02ms
step:553/1770 train_time:52139ms step_avg:96.02ms
step:554/1770 train_time:52237ms step_avg:96.02ms
step:555/1770 train_time:52334ms step_avg:96.03ms
step:556/1770 train_time:52432ms step_avg:96.03ms
step:557/1770 train_time:52530ms step_avg:96.03ms
step:558/1770 train_time:52627ms step_avg:96.04ms
step:559/1770 train_time:52725ms step_avg:96.04ms
step:560/1770 train_time:52823ms step_avg:96.04ms
step:561/1770 train_time:52921ms step_avg:96.04ms
step:562/1770 train_time:53019ms step_avg:96.05ms
step:563/1770 train_time:53118ms step_avg:96.05ms
step:564/1770 train_time:53216ms step_avg:96.06ms
step:565/1770 train_time:53313ms step_avg:96.06ms
step:566/1770 train_time:53411ms step_avg:96.06ms
step:567/1770 train_time:53509ms step_avg:96.07ms
step:568/1770 train_time:53607ms step_avg:96.07ms
step:569/1770 train_time:53704ms step_avg:96.07ms
step:570/1770 train_time:53802ms step_avg:96.08ms
step:571/1770 train_time:53900ms step_avg:96.08ms
step:572/1770 train_time:53998ms step_avg:96.08ms
step:573/1770 train_time:54096ms step_avg:96.09ms
step:574/1770 train_time:54194ms step_avg:96.09ms
step:575/1770 train_time:54292ms step_avg:96.09ms
step:576/1770 train_time:54390ms step_avg:96.09ms
step:577/1770 train_time:54487ms step_avg:96.10ms
step:578/1770 train_time:54585ms step_avg:96.10ms
step:579/1770 train_time:54682ms step_avg:96.10ms
step:580/1770 train_time:54780ms step_avg:96.11ms
step:581/1770 train_time:54878ms step_avg:96.11ms
step:582/1770 train_time:54976ms step_avg:96.11ms
step:583/1770 train_time:55075ms step_avg:96.12ms
step:584/1770 train_time:55173ms step_avg:96.12ms
step:585/1770 train_time:55270ms step_avg:96.12ms
step:586/1770 train_time:55368ms step_avg:96.12ms
step:587/1770 train_time:55465ms step_avg:96.13ms
step:588/1770 train_time:55563ms step_avg:96.13ms
step:589/1770 train_time:55661ms step_avg:96.13ms
step:590/1770 train_time:55759ms step_avg:96.14ms
step:591/1770 train_time:55858ms step_avg:96.14ms
step:592/1770 train_time:55956ms step_avg:96.14ms
step:593/1770 train_time:56053ms step_avg:96.15ms
step:594/1770 train_time:56151ms step_avg:96.15ms
step:595/1770 train_time:56249ms step_avg:96.15ms
step:596/1770 train_time:56347ms step_avg:96.15ms
step:597/1770 train_time:56444ms step_avg:96.16ms
step:598/1770 train_time:56542ms step_avg:96.16ms
step:599/1770 train_time:56640ms step_avg:96.16ms
step:600/1770 train_time:56739ms step_avg:96.17ms
step:601/1770 train_time:56837ms step_avg:96.17ms
step:602/1770 train_time:56935ms step_avg:96.17ms
step:603/1770 train_time:57033ms step_avg:96.18ms
step:604/1770 train_time:57130ms step_avg:96.18ms
step:605/1770 train_time:57227ms step_avg:96.18ms
step:606/1770 train_time:57325ms step_avg:96.18ms
step:607/1770 train_time:57423ms step_avg:96.19ms
step:608/1770 train_time:57521ms step_avg:96.19ms
step:609/1770 train_time:57619ms step_avg:96.19ms
step:610/1770 train_time:57718ms step_avg:96.20ms
step:611/1770 train_time:57816ms step_avg:96.20ms
step:612/1770 train_time:57914ms step_avg:96.20ms
step:613/1770 train_time:58011ms step_avg:96.20ms
step:614/1770 train_time:58109ms step_avg:96.21ms
step:615/1770 train_time:58207ms step_avg:96.21ms
step:616/1770 train_time:58305ms step_avg:96.21ms
step:617/1770 train_time:58402ms step_avg:96.21ms
step:618/1770 train_time:58500ms step_avg:96.22ms
step:619/1770 train_time:58599ms step_avg:96.22ms
step:620/1770 train_time:58697ms step_avg:96.22ms
step:621/1770 train_time:58795ms step_avg:96.23ms
step:622/1770 train_time:58893ms step_avg:96.23ms
step:623/1770 train_time:58991ms step_avg:96.23ms
step:624/1770 train_time:59088ms step_avg:96.23ms
step:625/1770 train_time:59186ms step_avg:96.24ms
step:625/1770 val_loss:3.6699 train_time:59281ms step_avg:96.39ms
step:626/1770 train_time:59304ms step_avg:96.27ms
step:627/1770 train_time:59394ms step_avg:96.26ms
step:628/1770 train_time:59495ms step_avg:96.27ms
step:629/1770 train_time:59594ms step_avg:96.27ms
step:630/1770 train_time:59692ms step_avg:96.28ms
step:631/1770 train_time:59790ms step_avg:96.28ms
step:632/1770 train_time:59888ms step_avg:96.28ms
step:633/1770 train_time:59986ms step_avg:96.29ms
step:634/1770 train_time:60083ms step_avg:96.29ms
step:635/1770 train_time:60181ms step_avg:96.29ms
step:636/1770 train_time:60278ms step_avg:96.29ms
step:637/1770 train_time:60376ms step_avg:96.29ms
step:638/1770 train_time:60474ms step_avg:96.30ms
step:639/1770 train_time:60573ms step_avg:96.30ms
step:640/1770 train_time:60671ms step_avg:96.30ms
step:641/1770 train_time:60769ms step_avg:96.31ms
step:642/1770 train_time:60866ms step_avg:96.31ms
step:643/1770 train_time:60964ms step_avg:96.31ms
step:644/1770 train_time:61061ms step_avg:96.31ms
step:645/1770 train_time:61159ms step_avg:96.31ms
step:646/1770 train_time:61257ms step_avg:96.32ms
step:647/1770 train_time:61355ms step_avg:96.32ms
step:648/1770 train_time:61453ms step_avg:96.32ms
step:649/1770 train_time:61552ms step_avg:96.32ms
step:650/1770 train_time:61650ms step_avg:96.33ms
step:651/1770 train_time:61748ms step_avg:96.33ms
step:652/1770 train_time:61846ms step_avg:96.33ms
step:653/1770 train_time:61943ms step_avg:96.33ms
step:654/1770 train_time:62041ms step_avg:96.34ms
step:655/1770 train_time:62139ms step_avg:96.34ms
step:656/1770 train_time:62237ms step_avg:96.34ms
step:657/1770 train_time:62335ms step_avg:96.34ms
step:658/1770 train_time:62435ms step_avg:96.35ms
step:659/1770 train_time:62535ms step_avg:96.36ms
step:660/1770 train_time:62635ms step_avg:96.36ms
step:661/1770 train_time:62735ms step_avg:96.37ms
step:662/1770 train_time:62835ms step_avg:96.37ms
step:663/1770 train_time:62935ms step_avg:96.38ms
step:664/1770 train_time:63036ms step_avg:96.39ms
step:665/1770 train_time:63136ms step_avg:96.39ms
step:666/1770 train_time:63236ms step_avg:96.40ms
step:667/1770 train_time:63335ms step_avg:96.40ms
step:668/1770 train_time:63434ms step_avg:96.40ms
step:669/1770 train_time:63534ms step_avg:96.41ms
step:670/1770 train_time:63634ms step_avg:96.42ms
step:671/1770 train_time:63734ms step_avg:96.42ms
step:672/1770 train_time:63834ms step_avg:96.43ms
step:673/1770 train_time:63934ms step_avg:96.43ms
step:674/1770 train_time:64033ms step_avg:96.44ms
step:675/1770 train_time:64133ms step_avg:96.44ms
step:676/1770 train_time:64233ms step_avg:96.45ms
step:677/1770 train_time:64334ms step_avg:96.45ms
step:678/1770 train_time:64434ms step_avg:96.46ms
step:679/1770 train_time:64534ms step_avg:96.46ms
step:680/1770 train_time:64634ms step_avg:96.47ms
step:681/1770 train_time:64733ms step_avg:96.47ms
step:682/1770 train_time:64834ms step_avg:96.48ms
step:683/1770 train_time:64933ms step_avg:96.48ms
step:684/1770 train_time:65034ms step_avg:96.49ms
step:685/1770 train_time:65134ms step_avg:96.50ms
step:686/1770 train_time:65234ms step_avg:96.50ms
step:687/1770 train_time:65335ms step_avg:96.51ms
step:688/1770 train_time:65435ms step_avg:96.51ms
step:689/1770 train_time:65535ms step_avg:96.52ms
step:690/1770 train_time:65635ms step_avg:96.52ms
step:691/1770 train_time:65735ms step_avg:96.53ms
step:692/1770 train_time:65835ms step_avg:96.53ms
step:693/1770 train_time:65934ms step_avg:96.54ms
step:694/1770 train_time:66034ms step_avg:96.54ms
step:695/1770 train_time:66134ms step_avg:96.55ms
step:696/1770 train_time:66233ms step_avg:96.55ms
step:697/1770 train_time:66333ms step_avg:96.56ms
step:698/1770 train_time:66434ms step_avg:96.56ms
step:699/1770 train_time:66534ms step_avg:96.57ms
step:700/1770 train_time:66635ms step_avg:96.57ms
step:701/1770 train_time:66735ms step_avg:96.58ms
step:702/1770 train_time:66835ms step_avg:96.58ms
step:703/1770 train_time:66935ms step_avg:96.59ms
step:704/1770 train_time:67034ms step_avg:96.59ms
step:705/1770 train_time:67134ms step_avg:96.60ms
step:706/1770 train_time:67234ms step_avg:96.60ms
step:707/1770 train_time:67334ms step_avg:96.61ms
step:708/1770 train_time:67434ms step_avg:96.61ms
step:709/1770 train_time:67534ms step_avg:96.62ms
step:710/1770 train_time:67634ms step_avg:96.62ms
step:711/1770 train_time:67734ms step_avg:96.62ms
step:712/1770 train_time:67835ms step_avg:96.63ms
step:713/1770 train_time:67936ms step_avg:96.64ms
step:714/1770 train_time:68036ms step_avg:96.64ms
step:715/1770 train_time:68135ms step_avg:96.65ms
step:716/1770 train_time:68235ms step_avg:96.65ms
step:717/1770 train_time:68335ms step_avg:96.66ms
step:718/1770 train_time:68435ms step_avg:96.66ms
step:719/1770 train_time:68535ms step_avg:96.66ms
step:720/1770 train_time:68635ms step_avg:96.67ms
step:721/1770 train_time:68735ms step_avg:96.67ms
step:722/1770 train_time:68835ms step_avg:96.68ms
step:723/1770 train_time:68935ms step_avg:96.68ms
step:724/1770 train_time:69036ms step_avg:96.69ms
step:725/1770 train_time:69136ms step_avg:96.69ms
step:726/1770 train_time:69235ms step_avg:96.70ms
step:727/1770 train_time:69335ms step_avg:96.70ms
step:728/1770 train_time:69434ms step_avg:96.71ms
step:729/1770 train_time:69534ms step_avg:96.71ms
step:730/1770 train_time:69634ms step_avg:96.71ms
step:731/1770 train_time:69734ms step_avg:96.72ms
step:732/1770 train_time:69835ms step_avg:96.72ms
step:733/1770 train_time:69935ms step_avg:96.73ms
step:734/1770 train_time:70035ms step_avg:96.73ms
step:735/1770 train_time:70135ms step_avg:96.74ms
step:736/1770 train_time:70236ms step_avg:96.74ms
step:737/1770 train_time:70336ms step_avg:96.75ms
step:738/1770 train_time:70435ms step_avg:96.75ms
step:739/1770 train_time:70535ms step_avg:96.76ms
step:740/1770 train_time:70635ms step_avg:96.76ms
step:741/1770 train_time:70735ms step_avg:96.76ms
step:742/1770 train_time:70834ms step_avg:96.77ms
step:743/1770 train_time:70933ms step_avg:96.77ms
step:744/1770 train_time:71033ms step_avg:96.78ms
step:745/1770 train_time:71133ms step_avg:96.78ms
step:746/1770 train_time:71233ms step_avg:96.78ms
step:747/1770 train_time:71332ms step_avg:96.79ms
step:748/1770 train_time:71433ms step_avg:96.79ms
step:749/1770 train_time:71533ms step_avg:96.80ms
step:750/1770 train_time:71633ms step_avg:96.80ms
step:750/1770 val_loss:3.6026 train_time:71732ms step_avg:96.94ms
step:751/1770 train_time:71754ms step_avg:96.83ms
step:752/1770 train_time:71847ms step_avg:96.83ms
step:753/1770 train_time:71949ms step_avg:96.84ms
step:754/1770 train_time:72050ms step_avg:96.84ms
step:755/1770 train_time:72150ms step_avg:96.85ms
step:756/1770 train_time:72249ms step_avg:96.85ms
step:757/1770 train_time:72349ms step_avg:96.85ms
step:758/1770 train_time:72448ms step_avg:96.86ms
step:759/1770 train_time:72548ms step_avg:96.86ms
step:760/1770 train_time:72648ms step_avg:96.86ms
step:761/1770 train_time:72749ms step_avg:96.87ms
step:762/1770 train_time:72850ms step_avg:96.88ms
step:763/1770 train_time:72952ms step_avg:96.88ms
step:764/1770 train_time:73052ms step_avg:96.89ms
step:765/1770 train_time:73153ms step_avg:96.89ms
step:766/1770 train_time:73253ms step_avg:96.90ms
step:767/1770 train_time:73354ms step_avg:96.90ms
step:768/1770 train_time:73453ms step_avg:96.90ms
step:769/1770 train_time:73553ms step_avg:96.91ms
step:770/1770 train_time:73652ms step_avg:96.91ms
step:771/1770 train_time:73752ms step_avg:96.92ms
step:772/1770 train_time:73853ms step_avg:96.92ms
step:773/1770 train_time:73953ms step_avg:96.92ms
step:774/1770 train_time:74053ms step_avg:96.93ms
step:775/1770 train_time:74153ms step_avg:96.93ms
step:776/1770 train_time:74253ms step_avg:96.94ms
step:777/1770 train_time:74353ms step_avg:96.94ms
step:778/1770 train_time:74453ms step_avg:96.94ms
step:779/1770 train_time:74553ms step_avg:96.95ms
step:780/1770 train_time:74652ms step_avg:96.95ms
step:781/1770 train_time:74753ms step_avg:96.96ms
step:782/1770 train_time:74852ms step_avg:96.96ms
step:783/1770 train_time:74952ms step_avg:96.96ms
step:784/1770 train_time:75052ms step_avg:96.97ms
step:785/1770 train_time:75153ms step_avg:96.97ms
step:786/1770 train_time:75252ms step_avg:96.97ms
step:787/1770 train_time:75353ms step_avg:96.98ms
step:788/1770 train_time:75454ms step_avg:96.98ms
step:789/1770 train_time:75554ms step_avg:96.99ms
step:790/1770 train_time:75654ms step_avg:96.99ms
step:791/1770 train_time:75755ms step_avg:97.00ms
step:792/1770 train_time:75855ms step_avg:97.00ms
step:793/1770 train_time:75955ms step_avg:97.00ms
step:794/1770 train_time:76055ms step_avg:97.01ms
step:795/1770 train_time:76155ms step_avg:97.01ms
step:796/1770 train_time:76255ms step_avg:97.02ms
step:797/1770 train_time:76355ms step_avg:97.02ms
step:798/1770 train_time:76455ms step_avg:97.02ms
step:799/1770 train_time:76555ms step_avg:97.03ms
step:800/1770 train_time:76655ms step_avg:97.03ms
step:801/1770 train_time:76754ms step_avg:97.03ms
step:802/1770 train_time:76854ms step_avg:97.04ms
step:803/1770 train_time:76954ms step_avg:97.04ms
step:804/1770 train_time:77054ms step_avg:97.05ms
step:805/1770 train_time:77154ms step_avg:97.05ms
step:806/1770 train_time:77254ms step_avg:97.05ms
step:807/1770 train_time:77354ms step_avg:97.06ms
step:808/1770 train_time:77454ms step_avg:97.06ms
step:809/1770 train_time:77554ms step_avg:97.06ms
step:810/1770 train_time:77654ms step_avg:97.07ms
step:811/1770 train_time:77754ms step_avg:97.07ms
step:812/1770 train_time:77854ms step_avg:97.07ms
step:813/1770 train_time:77954ms step_avg:97.08ms
step:814/1770 train_time:78054ms step_avg:97.08ms
step:815/1770 train_time:78154ms step_avg:97.09ms
step:816/1770 train_time:78254ms step_avg:97.09ms
step:817/1770 train_time:78354ms step_avg:97.09ms
step:818/1770 train_time:78454ms step_avg:97.10ms
step:819/1770 train_time:78554ms step_avg:97.10ms
step:820/1770 train_time:78654ms step_avg:97.10ms
step:821/1770 train_time:78754ms step_avg:97.11ms
step:822/1770 train_time:78854ms step_avg:97.11ms
step:823/1770 train_time:78954ms step_avg:97.11ms
step:824/1770 train_time:79053ms step_avg:97.12ms
step:825/1770 train_time:79153ms step_avg:97.12ms
step:826/1770 train_time:79253ms step_avg:97.12ms
step:827/1770 train_time:79353ms step_avg:97.13ms
step:828/1770 train_time:79453ms step_avg:97.13ms
step:829/1770 train_time:79554ms step_avg:97.13ms
step:830/1770 train_time:79653ms step_avg:97.14ms
step:831/1770 train_time:79753ms step_avg:97.14ms
step:832/1770 train_time:79853ms step_avg:97.14ms
step:833/1770 train_time:79953ms step_avg:97.15ms
step:834/1770 train_time:80053ms step_avg:97.15ms
step:835/1770 train_time:80153ms step_avg:97.16ms
step:836/1770 train_time:80254ms step_avg:97.16ms
step:837/1770 train_time:80354ms step_avg:97.16ms
step:838/1770 train_time:80455ms step_avg:97.17ms
step:839/1770 train_time:80554ms step_avg:97.17ms
step:840/1770 train_time:80654ms step_avg:97.17ms
step:841/1770 train_time:80755ms step_avg:97.18ms
step:842/1770 train_time:80853ms step_avg:97.18ms
step:843/1770 train_time:80953ms step_avg:97.18ms
step:844/1770 train_time:81053ms step_avg:97.19ms
step:845/1770 train_time:81154ms step_avg:97.19ms
step:846/1770 train_time:81253ms step_avg:97.19ms
step:847/1770 train_time:81353ms step_avg:97.20ms
step:848/1770 train_time:81453ms step_avg:97.20ms
step:849/1770 train_time:81553ms step_avg:97.20ms
step:850/1770 train_time:81653ms step_avg:97.21ms
step:851/1770 train_time:81753ms step_avg:97.21ms
step:852/1770 train_time:81853ms step_avg:97.21ms
step:853/1770 train_time:81953ms step_avg:97.22ms
step:854/1770 train_time:82053ms step_avg:97.22ms
step:855/1770 train_time:82153ms step_avg:97.22ms
step:856/1770 train_time:82253ms step_avg:97.23ms
step:857/1770 train_time:82353ms step_avg:97.23ms
step:858/1770 train_time:82453ms step_avg:97.23ms
step:859/1770 train_time:82553ms step_avg:97.24ms
step:860/1770 train_time:82654ms step_avg:97.24ms
step:861/1770 train_time:82754ms step_avg:97.24ms
step:862/1770 train_time:82854ms step_avg:97.25ms
step:863/1770 train_time:82954ms step_avg:97.25ms
step:864/1770 train_time:83053ms step_avg:97.25ms
step:865/1770 train_time:83154ms step_avg:97.26ms
step:866/1770 train_time:83254ms step_avg:97.26ms
step:867/1770 train_time:83354ms step_avg:97.26ms
step:868/1770 train_time:83454ms step_avg:97.27ms
step:869/1770 train_time:83554ms step_avg:97.27ms
step:870/1770 train_time:83655ms step_avg:97.27ms
step:871/1770 train_time:83754ms step_avg:97.28ms
step:872/1770 train_time:83854ms step_avg:97.28ms
step:873/1770 train_time:83954ms step_avg:97.28ms
step:874/1770 train_time:84054ms step_avg:97.28ms
step:875/1770 train_time:84153ms step_avg:97.29ms
step:875/1770 val_loss:3.5551 train_time:84252ms step_avg:97.40ms
step:876/1770 train_time:84275ms step_avg:97.31ms
step:877/1770 train_time:84364ms step_avg:97.31ms
step:878/1770 train_time:84467ms step_avg:97.31ms
step:879/1770 train_time:84567ms step_avg:97.32ms
step:880/1770 train_time:84667ms step_avg:97.32ms
step:881/1770 train_time:84767ms step_avg:97.32ms
step:882/1770 train_time:84866ms step_avg:97.32ms
step:883/1770 train_time:84966ms step_avg:97.33ms
step:884/1770 train_time:85066ms step_avg:97.33ms
step:885/1770 train_time:85166ms step_avg:97.33ms
step:886/1770 train_time:85266ms step_avg:97.34ms
step:887/1770 train_time:85368ms step_avg:97.34ms
step:888/1770 train_time:85469ms step_avg:97.35ms
step:889/1770 train_time:85570ms step_avg:97.35ms
step:890/1770 train_time:85669ms step_avg:97.35ms
step:891/1770 train_time:85768ms step_avg:97.35ms
step:892/1770 train_time:85868ms step_avg:97.36ms
step:893/1770 train_time:85968ms step_avg:97.36ms
step:894/1770 train_time:86068ms step_avg:97.36ms
step:895/1770 train_time:86168ms step_avg:97.36ms
step:896/1770 train_time:86267ms step_avg:97.37ms
step:897/1770 train_time:86368ms step_avg:97.37ms
step:898/1770 train_time:86468ms step_avg:97.37ms
step:899/1770 train_time:86568ms step_avg:97.38ms
step:900/1770 train_time:86668ms step_avg:97.38ms
step:901/1770 train_time:86768ms step_avg:97.38ms
step:902/1770 train_time:86868ms step_avg:97.39ms
step:903/1770 train_time:86968ms step_avg:97.39ms
step:904/1770 train_time:87068ms step_avg:97.39ms
step:905/1770 train_time:87168ms step_avg:97.39ms
step:906/1770 train_time:87269ms step_avg:97.40ms
step:907/1770 train_time:87369ms step_avg:97.40ms
step:908/1770 train_time:87469ms step_avg:97.40ms
step:909/1770 train_time:87570ms step_avg:97.41ms
step:910/1770 train_time:87670ms step_avg:97.41ms
step:911/1770 train_time:87770ms step_avg:97.41ms
step:912/1770 train_time:87870ms step_avg:97.42ms
step:913/1770 train_time:87969ms step_avg:97.42ms
step:914/1770 train_time:88069ms step_avg:97.42ms
step:915/1770 train_time:88169ms step_avg:97.42ms
step:916/1770 train_time:88268ms step_avg:97.43ms
step:917/1770 train_time:88368ms step_avg:97.43ms
step:918/1770 train_time:88468ms step_avg:97.43ms
step:919/1770 train_time:88568ms step_avg:97.43ms
step:920/1770 train_time:88669ms step_avg:97.44ms
step:921/1770 train_time:88772ms step_avg:97.44ms
step:922/1770 train_time:88874ms step_avg:97.45ms
step:923/1770 train_time:88975ms step_avg:97.45ms
step:924/1770 train_time:89076ms step_avg:97.46ms
step:925/1770 train_time:89176ms step_avg:97.46ms
step:926/1770 train_time:89277ms step_avg:97.46ms
step:927/1770 train_time:89378ms step_avg:97.47ms
step:928/1770 train_time:89479ms step_avg:97.47ms
step:929/1770 train_time:89581ms step_avg:97.48ms
step:930/1770 train_time:89683ms step_avg:97.48ms
step:931/1770 train_time:89785ms step_avg:97.49ms
step:932/1770 train_time:89887ms step_avg:97.49ms
step:933/1770 train_time:89988ms step_avg:97.50ms
step:934/1770 train_time:90089ms step_avg:97.50ms
step:935/1770 train_time:90191ms step_avg:97.50ms
step:936/1770 train_time:90292ms step_avg:97.51ms
step:937/1770 train_time:90393ms step_avg:97.51ms
step:938/1770 train_time:90494ms step_avg:97.51ms
step:939/1770 train_time:90595ms step_avg:97.52ms
step:940/1770 train_time:90696ms step_avg:97.52ms
step:941/1770 train_time:90797ms step_avg:97.53ms
step:942/1770 train_time:90898ms step_avg:97.53ms
step:943/1770 train_time:90999ms step_avg:97.53ms
step:944/1770 train_time:91100ms step_avg:97.54ms
step:945/1770 train_time:91201ms step_avg:97.54ms
step:946/1770 train_time:91303ms step_avg:97.55ms
step:947/1770 train_time:91406ms step_avg:97.55ms
step:948/1770 train_time:91507ms step_avg:97.56ms
step:949/1770 train_time:91609ms step_avg:97.56ms
step:950/1770 train_time:91710ms step_avg:97.56ms
step:951/1770 train_time:91811ms step_avg:97.57ms
step:952/1770 train_time:91912ms step_avg:97.57ms
step:953/1770 train_time:92013ms step_avg:97.57ms
step:954/1770 train_time:92114ms step_avg:97.58ms
step:955/1770 train_time:92215ms step_avg:97.58ms
step:956/1770 train_time:92318ms step_avg:97.59ms
step:957/1770 train_time:92420ms step_avg:97.59ms
step:958/1770 train_time:92520ms step_avg:97.60ms
step:959/1770 train_time:92622ms step_avg:97.60ms
step:960/1770 train_time:92723ms step_avg:97.60ms
step:961/1770 train_time:92825ms step_avg:97.61ms
step:962/1770 train_time:92927ms step_avg:97.61ms
step:963/1770 train_time:93028ms step_avg:97.62ms
step:964/1770 train_time:93129ms step_avg:97.62ms
step:965/1770 train_time:93231ms step_avg:97.62ms
step:966/1770 train_time:93332ms step_avg:97.63ms
step:967/1770 train_time:93433ms step_avg:97.63ms
step:968/1770 train_time:93534ms step_avg:97.63ms
step:969/1770 train_time:93635ms step_avg:97.64ms
step:970/1770 train_time:93735ms step_avg:97.64ms
step:971/1770 train_time:93836ms step_avg:97.64ms
step:972/1770 train_time:93937ms step_avg:97.65ms
step:973/1770 train_time:94038ms step_avg:97.65ms
step:974/1770 train_time:94140ms step_avg:97.66ms
step:975/1770 train_time:94243ms step_avg:97.66ms
step:976/1770 train_time:94345ms step_avg:97.67ms
step:977/1770 train_time:94447ms step_avg:97.67ms
step:978/1770 train_time:94549ms step_avg:97.67ms
step:979/1770 train_time:94650ms step_avg:97.68ms
step:980/1770 train_time:94751ms step_avg:97.68ms
step:981/1770 train_time:94852ms step_avg:97.68ms
step:982/1770 train_time:94953ms step_avg:97.69ms
step:983/1770 train_time:95054ms step_avg:97.69ms
step:984/1770 train_time:95155ms step_avg:97.70ms
step:985/1770 train_time:95256ms step_avg:97.70ms
step:986/1770 train_time:95357ms step_avg:97.70ms
step:987/1770 train_time:95459ms step_avg:97.71ms
step:988/1770 train_time:95559ms step_avg:97.71ms
step:989/1770 train_time:95662ms step_avg:97.71ms
step:990/1770 train_time:95763ms step_avg:97.72ms
step:991/1770 train_time:95866ms step_avg:97.72ms
step:992/1770 train_time:95968ms step_avg:97.73ms
step:993/1770 train_time:96070ms step_avg:97.73ms
step:994/1770 train_time:96171ms step_avg:97.73ms
step:995/1770 train_time:96271ms step_avg:97.74ms
step:996/1770 train_time:96372ms step_avg:97.74ms
step:997/1770 train_time:96473ms step_avg:97.74ms
step:998/1770 train_time:96574ms step_avg:97.75ms
step:999/1770 train_time:96674ms step_avg:97.75ms
step:1000/1770 train_time:96775ms step_avg:97.75ms
step:1000/1770 val_loss:3.5179 train_time:96874ms step_avg:97.85ms
step:1001/1770 train_time:96896ms step_avg:97.78ms
step:1002/1770 train_time:96988ms step_avg:97.77ms
step:1003/1770 train_time:97091ms step_avg:97.78ms
step:1004/1770 train_time:97193ms step_avg:97.78ms
step:1005/1770 train_time:97293ms step_avg:97.78ms
step:1006/1770 train_time:97394ms step_avg:97.79ms
step:1007/1770 train_time:97495ms step_avg:97.79ms
step:1008/1770 train_time:97596ms step_avg:97.79ms
step:1009/1770 train_time:97696ms step_avg:97.79ms
step:1010/1770 train_time:97796ms step_avg:97.80ms
step:1011/1770 train_time:97898ms step_avg:97.80ms
step:1012/1770 train_time:97999ms step_avg:97.80ms
step:1013/1770 train_time:98100ms step_avg:97.81ms
step:1014/1770 train_time:98202ms step_avg:97.81ms
step:1015/1770 train_time:98303ms step_avg:97.81ms
step:1016/1770 train_time:98404ms step_avg:97.82ms
step:1017/1770 train_time:98506ms step_avg:97.82ms
step:1018/1770 train_time:98608ms step_avg:97.83ms
step:1019/1770 train_time:98711ms step_avg:97.83ms
step:1020/1770 train_time:98813ms step_avg:97.83ms
step:1021/1770 train_time:98914ms step_avg:97.84ms
step:1022/1770 train_time:99015ms step_avg:97.84ms
step:1023/1770 train_time:99116ms step_avg:97.84ms
step:1024/1770 train_time:99217ms step_avg:97.85ms
step:1025/1770 train_time:99318ms step_avg:97.85ms
step:1026/1770 train_time:99419ms step_avg:97.85ms
step:1027/1770 train_time:99520ms step_avg:97.86ms
step:1028/1770 train_time:99621ms step_avg:97.86ms
step:1029/1770 train_time:99723ms step_avg:97.86ms
step:1030/1770 train_time:99824ms step_avg:97.87ms
step:1031/1770 train_time:99925ms step_avg:97.87ms
step:1032/1770 train_time:100028ms step_avg:97.87ms
step:1033/1770 train_time:100131ms step_avg:97.88ms
step:1034/1770 train_time:100232ms step_avg:97.88ms
step:1035/1770 train_time:100333ms step_avg:97.89ms
step:1036/1770 train_time:100434ms step_avg:97.89ms
step:1037/1770 train_time:100535ms step_avg:97.89ms
step:1038/1770 train_time:100636ms step_avg:97.90ms
step:1039/1770 train_time:100737ms step_avg:97.90ms
step:1040/1770 train_time:100838ms step_avg:97.90ms
step:1041/1770 train_time:100939ms step_avg:97.90ms
step:1042/1770 train_time:101041ms step_avg:97.91ms
step:1043/1770 train_time:101141ms step_avg:97.91ms
step:1044/1770 train_time:101243ms step_avg:97.91ms
step:1045/1770 train_time:101344ms step_avg:97.92ms
step:1046/1770 train_time:101446ms step_avg:97.92ms
step:1047/1770 train_time:101548ms step_avg:97.92ms
step:1048/1770 train_time:101650ms step_avg:97.93ms
step:1049/1770 train_time:101752ms step_avg:97.93ms
step:1050/1770 train_time:101854ms step_avg:97.94ms
step:1051/1770 train_time:101956ms step_avg:97.94ms
step:1052/1770 train_time:102057ms step_avg:97.94ms
step:1053/1770 train_time:102158ms step_avg:97.95ms
step:1054/1770 train_time:102259ms step_avg:97.95ms
step:1055/1770 train_time:102360ms step_avg:97.95ms
step:1056/1770 train_time:102461ms step_avg:97.95ms
step:1057/1770 train_time:102562ms step_avg:97.96ms
step:1058/1770 train_time:102664ms step_avg:97.96ms
step:1059/1770 train_time:102765ms step_avg:97.96ms
step:1060/1770 train_time:102866ms step_avg:97.97ms
step:1061/1770 train_time:102969ms step_avg:97.97ms
step:1062/1770 train_time:103073ms step_avg:97.98ms
step:1063/1770 train_time:103175ms step_avg:97.98ms
step:1064/1770 train_time:103277ms step_avg:97.99ms
step:1065/1770 train_time:103379ms step_avg:97.99ms
step:1066/1770 train_time:103480ms step_avg:97.99ms
step:1067/1770 train_time:103581ms step_avg:98.00ms
step:1068/1770 train_time:103682ms step_avg:98.00ms
step:1069/1770 train_time:103784ms step_avg:98.00ms
step:1070/1770 train_time:103886ms step_avg:98.01ms
step:1071/1770 train_time:103987ms step_avg:98.01ms
step:1072/1770 train_time:104089ms step_avg:98.01ms
step:1073/1770 train_time:104191ms step_avg:98.02ms
step:1074/1770 train_time:104293ms step_avg:98.02ms
step:1075/1770 train_time:104395ms step_avg:98.02ms
step:1076/1770 train_time:104496ms step_avg:98.03ms
step:1077/1770 train_time:104598ms step_avg:98.03ms
step:1078/1770 train_time:104698ms step_avg:98.03ms
step:1079/1770 train_time:104799ms step_avg:98.03ms
step:1080/1770 train_time:104900ms step_avg:98.04ms
step:1081/1770 train_time:105000ms step_avg:98.04ms
step:1082/1770 train_time:105102ms step_avg:98.04ms
step:1083/1770 train_time:105203ms step_avg:98.05ms
step:1084/1770 train_time:105305ms step_avg:98.05ms
step:1085/1770 train_time:105408ms step_avg:98.05ms
step:1086/1770 train_time:105511ms step_avg:98.06ms
step:1087/1770 train_time:105612ms step_avg:98.06ms
step:1088/1770 train_time:105713ms step_avg:98.06ms
step:1089/1770 train_time:105815ms step_avg:98.07ms
step:1090/1770 train_time:105917ms step_avg:98.07ms
step:1091/1770 train_time:106018ms step_avg:98.07ms
step:1092/1770 train_time:106119ms step_avg:98.08ms
step:1093/1770 train_time:106220ms step_avg:98.08ms
step:1094/1770 train_time:106322ms step_avg:98.08ms
step:1095/1770 train_time:106423ms step_avg:98.09ms
step:1096/1770 train_time:106524ms step_avg:98.09ms
step:1097/1770 train_time:106626ms step_avg:98.09ms
step:1098/1770 train_time:106728ms step_avg:98.10ms
step:1099/1770 train_time:106830ms step_avg:98.10ms
step:1100/1770 train_time:106932ms step_avg:98.10ms
step:1101/1770 train_time:107034ms step_avg:98.11ms
step:1102/1770 train_time:107135ms step_avg:98.11ms
step:1103/1770 train_time:107237ms step_avg:98.11ms
step:1104/1770 train_time:107339ms step_avg:98.12ms
step:1105/1770 train_time:107441ms step_avg:98.12ms
step:1106/1770 train_time:107543ms step_avg:98.12ms
step:1107/1770 train_time:107644ms step_avg:98.13ms
step:1108/1770 train_time:107746ms step_avg:98.13ms
step:1109/1770 train_time:107848ms step_avg:98.13ms
step:1110/1770 train_time:107950ms step_avg:98.14ms
step:1111/1770 train_time:108052ms step_avg:98.14ms
step:1112/1770 train_time:108154ms step_avg:98.14ms
step:1113/1770 train_time:108255ms step_avg:98.15ms
step:1114/1770 train_time:108357ms step_avg:98.15ms
step:1115/1770 train_time:108458ms step_avg:98.15ms
step:1116/1770 train_time:108559ms step_avg:98.15ms
step:1117/1770 train_time:108661ms step_avg:98.16ms
step:1118/1770 train_time:108761ms step_avg:98.16ms
step:1119/1770 train_time:108863ms step_avg:98.16ms
step:1120/1770 train_time:108964ms step_avg:98.17ms
step:1121/1770 train_time:109066ms step_avg:98.17ms
step:1122/1770 train_time:109170ms step_avg:98.17ms
step:1123/1770 train_time:109271ms step_avg:98.18ms
step:1124/1770 train_time:109373ms step_avg:98.18ms
step:1125/1770 train_time:109475ms step_avg:98.18ms
step:1125/1770 val_loss:3.4773 train_time:109574ms step_avg:98.27ms
step:1126/1770 train_time:109596ms step_avg:98.20ms
step:1127/1770 train_time:109688ms step_avg:98.20ms
step:1128/1770 train_time:109792ms step_avg:98.20ms
step:1129/1770 train_time:109893ms step_avg:98.21ms
step:1130/1770 train_time:109995ms step_avg:98.21ms
step:1131/1770 train_time:110097ms step_avg:98.21ms
step:1132/1770 train_time:110199ms step_avg:98.22ms
step:1133/1770 train_time:110300ms step_avg:98.22ms
step:1134/1770 train_time:110401ms step_avg:98.22ms
step:1135/1770 train_time:110502ms step_avg:98.22ms
step:1136/1770 train_time:110603ms step_avg:98.23ms
step:1137/1770 train_time:110705ms step_avg:98.23ms
step:1138/1770 train_time:110806ms step_avg:98.23ms
step:1139/1770 train_time:110907ms step_avg:98.23ms
step:1140/1770 train_time:111009ms step_avg:98.24ms
step:1141/1770 train_time:111109ms step_avg:98.24ms
step:1142/1770 train_time:111211ms step_avg:98.24ms
step:1143/1770 train_time:111313ms step_avg:98.25ms
step:1144/1770 train_time:111415ms step_avg:98.25ms
step:1145/1770 train_time:111517ms step_avg:98.25ms
step:1146/1770 train_time:111619ms step_avg:98.26ms
step:1147/1770 train_time:111720ms step_avg:98.26ms
step:1148/1770 train_time:111821ms step_avg:98.26ms
step:1149/1770 train_time:111922ms step_avg:98.26ms
step:1150/1770 train_time:112023ms step_avg:98.27ms
step:1151/1770 train_time:112125ms step_avg:98.27ms
step:1152/1770 train_time:112227ms step_avg:98.27ms
step:1153/1770 train_time:112328ms step_avg:98.28ms
step:1154/1770 train_time:112430ms step_avg:98.28ms
step:1155/1770 train_time:112532ms step_avg:98.28ms
step:1156/1770 train_time:112634ms step_avg:98.28ms
step:1157/1770 train_time:112737ms step_avg:98.29ms
step:1158/1770 train_time:112839ms step_avg:98.29ms
step:1159/1770 train_time:112940ms step_avg:98.29ms
step:1160/1770 train_time:113041ms step_avg:98.30ms
step:1161/1770 train_time:113142ms step_avg:98.30ms
step:1162/1770 train_time:113244ms step_avg:98.30ms
step:1163/1770 train_time:113344ms step_avg:98.30ms
step:1164/1770 train_time:113446ms step_avg:98.31ms
step:1165/1770 train_time:113547ms step_avg:98.31ms
step:1166/1770 train_time:113650ms step_avg:98.31ms
step:1167/1770 train_time:113752ms step_avg:98.32ms
step:1168/1770 train_time:113854ms step_avg:98.32ms
step:1169/1770 train_time:113956ms step_avg:98.32ms
step:1170/1770 train_time:114057ms step_avg:98.32ms
step:1171/1770 train_time:114159ms step_avg:98.33ms
step:1172/1770 train_time:114260ms step_avg:98.33ms
step:1173/1770 train_time:114360ms step_avg:98.33ms
step:1174/1770 train_time:114462ms step_avg:98.33ms
step:1175/1770 train_time:114563ms step_avg:98.34ms
step:1176/1770 train_time:114664ms step_avg:98.34ms
step:1177/1770 train_time:114765ms step_avg:98.34ms
step:1178/1770 train_time:114865ms step_avg:98.34ms
step:1179/1770 train_time:114967ms step_avg:98.35ms
step:1180/1770 train_time:115070ms step_avg:98.35ms
step:1181/1770 train_time:115173ms step_avg:98.35ms
step:1182/1770 train_time:115275ms step_avg:98.36ms
step:1183/1770 train_time:115377ms step_avg:98.36ms
step:1184/1770 train_time:115481ms step_avg:98.37ms
step:1185/1770 train_time:115582ms step_avg:98.37ms
step:1186/1770 train_time:115686ms step_avg:98.37ms
step:1187/1770 train_time:115791ms step_avg:98.38ms
step:1188/1770 train_time:115895ms step_avg:98.38ms
step:1189/1770 train_time:115997ms step_avg:98.39ms
step:1190/1770 train_time:116099ms step_avg:98.39ms
step:1191/1770 train_time:116202ms step_avg:98.39ms
step:1192/1770 train_time:116305ms step_avg:98.40ms
step:1193/1770 train_time:116407ms step_avg:98.40ms
step:1194/1770 train_time:116510ms step_avg:98.40ms
step:1195/1770 train_time:116613ms step_avg:98.41ms
step:1196/1770 train_time:116717ms step_avg:98.41ms
step:1197/1770 train_time:116820ms step_avg:98.42ms
step:1198/1770 train_time:116922ms step_avg:98.42ms
step:1199/1770 train_time:117025ms step_avg:98.42ms
step:1200/1770 train_time:117129ms step_avg:98.43ms
step:1201/1770 train_time:117232ms step_avg:98.43ms
step:1202/1770 train_time:117334ms step_avg:98.43ms
step:1203/1770 train_time:117437ms step_avg:98.44ms
step:1204/1770 train_time:117539ms step_avg:98.44ms
step:1205/1770 train_time:117642ms step_avg:98.45ms
step:1206/1770 train_time:117745ms step_avg:98.45ms
step:1207/1770 train_time:117848ms step_avg:98.45ms
step:1208/1770 train_time:117950ms step_avg:98.46ms
step:1209/1770 train_time:118053ms step_avg:98.46ms
step:1210/1770 train_time:118156ms step_avg:98.46ms
step:1211/1770 train_time:118259ms step_avg:98.47ms
step:1212/1770 train_time:118364ms step_avg:98.47ms
step:1213/1770 train_time:118467ms step_avg:98.48ms
step:1214/1770 train_time:118569ms step_avg:98.48ms
step:1215/1770 train_time:118672ms step_avg:98.48ms
step:1216/1770 train_time:118777ms step_avg:98.49ms
step:1217/1770 train_time:118880ms step_avg:98.49ms
step:1218/1770 train_time:118982ms step_avg:98.50ms
step:1219/1770 train_time:119085ms step_avg:98.50ms
step:1220/1770 train_time:119188ms step_avg:98.50ms
step:1221/1770 train_time:119290ms step_avg:98.51ms
step:1222/1770 train_time:119394ms step_avg:98.51ms
step:1223/1770 train_time:119497ms step_avg:98.51ms
step:1224/1770 train_time:119600ms step_avg:98.52ms
step:1225/1770 train_time:119703ms step_avg:98.52ms
step:1226/1770 train_time:119805ms step_avg:98.52ms
step:1227/1770 train_time:119910ms step_avg:98.53ms
step:1228/1770 train_time:120015ms step_avg:98.53ms
step:1229/1770 train_time:120117ms step_avg:98.54ms
step:1230/1770 train_time:120220ms step_avg:98.54ms
step:1231/1770 train_time:120323ms step_avg:98.54ms
step:1232/1770 train_time:120425ms step_avg:98.55ms
step:1233/1770 train_time:120527ms step_avg:98.55ms
step:1234/1770 train_time:120629ms step_avg:98.55ms
step:1235/1770 train_time:120731ms step_avg:98.56ms
step:1236/1770 train_time:120836ms step_avg:98.56ms
step:1237/1770 train_time:120938ms step_avg:98.56ms
step:1238/1770 train_time:121041ms step_avg:98.57ms
step:1239/1770 train_time:121144ms step_avg:98.57ms
step:1240/1770 train_time:121246ms step_avg:98.57ms
step:1241/1770 train_time:121349ms step_avg:98.58ms
step:1242/1770 train_time:121453ms step_avg:98.58ms
step:1243/1770 train_time:121556ms step_avg:98.59ms
step:1244/1770 train_time:121658ms step_avg:98.59ms
step:1245/1770 train_time:121760ms step_avg:98.59ms
step:1246/1770 train_time:121863ms step_avg:98.59ms
step:1247/1770 train_time:121965ms step_avg:98.60ms
step:1248/1770 train_time:122069ms step_avg:98.60ms
step:1249/1770 train_time:122171ms step_avg:98.60ms
step:1250/1770 train_time:122274ms step_avg:98.61ms
step:1250/1770 val_loss:3.4317 train_time:122377ms step_avg:98.69ms
step:1251/1770 train_time:122400ms step_avg:98.63ms
step:1252/1770 train_time:122494ms step_avg:98.63ms
step:1253/1770 train_time:122597ms step_avg:98.63ms
step:1254/1770 train_time:122700ms step_avg:98.63ms
step:1255/1770 train_time:122805ms step_avg:98.64ms
step:1256/1770 train_time:122908ms step_avg:98.64ms
step:1257/1770 train_time:123010ms step_avg:98.65ms
step:1258/1770 train_time:123113ms step_avg:98.65ms
step:1259/1770 train_time:123216ms step_avg:98.65ms
step:1260/1770 train_time:123318ms step_avg:98.65ms
step:1261/1770 train_time:123422ms step_avg:98.66ms
step:1262/1770 train_time:123525ms step_avg:98.66ms
step:1263/1770 train_time:123627ms step_avg:98.67ms
step:1264/1770 train_time:123732ms step_avg:98.67ms
step:1265/1770 train_time:123834ms step_avg:98.67ms
step:1266/1770 train_time:123936ms step_avg:98.68ms
step:1267/1770 train_time:124039ms step_avg:98.68ms
step:1268/1770 train_time:124143ms step_avg:98.68ms
step:1269/1770 train_time:124246ms step_avg:98.69ms
step:1270/1770 train_time:124349ms step_avg:98.69ms
step:1271/1770 train_time:124451ms step_avg:98.69ms
step:1272/1770 train_time:124553ms step_avg:98.69ms
step:1273/1770 train_time:124656ms step_avg:98.70ms
step:1274/1770 train_time:124758ms step_avg:98.70ms
step:1275/1770 train_time:124861ms step_avg:98.70ms
step:1276/1770 train_time:124964ms step_avg:98.71ms
step:1277/1770 train_time:125067ms step_avg:98.71ms
step:1278/1770 train_time:125170ms step_avg:98.71ms
step:1279/1770 train_time:125273ms step_avg:98.72ms
step:1280/1770 train_time:125377ms step_avg:98.72ms
step:1281/1770 train_time:125479ms step_avg:98.72ms
step:1282/1770 train_time:125583ms step_avg:98.73ms
step:1283/1770 train_time:125688ms step_avg:98.73ms
step:1284/1770 train_time:125791ms step_avg:98.74ms
step:1285/1770 train_time:125894ms step_avg:98.74ms
step:1286/1770 train_time:125997ms step_avg:98.74ms
step:1287/1770 train_time:126102ms step_avg:98.75ms
step:1288/1770 train_time:126206ms step_avg:98.75ms
step:1289/1770 train_time:126309ms step_avg:98.76ms
step:1290/1770 train_time:126410ms step_avg:98.76ms
step:1291/1770 train_time:126513ms step_avg:98.76ms
step:1292/1770 train_time:126615ms step_avg:98.76ms
step:1293/1770 train_time:126718ms step_avg:98.77ms
step:1294/1770 train_time:126820ms step_avg:98.77ms
step:1295/1770 train_time:126923ms step_avg:98.77ms
step:1296/1770 train_time:127026ms step_avg:98.78ms
step:1297/1770 train_time:127129ms step_avg:98.78ms
step:1298/1770 train_time:127232ms step_avg:98.78ms
step:1299/1770 train_time:127334ms step_avg:98.79ms
step:1300/1770 train_time:127436ms step_avg:98.79ms
step:1301/1770 train_time:127540ms step_avg:98.79ms
step:1302/1770 train_time:127642ms step_avg:98.79ms
step:1303/1770 train_time:127745ms step_avg:98.80ms
step:1304/1770 train_time:127848ms step_avg:98.80ms
step:1305/1770 train_time:127951ms step_avg:98.80ms
step:1306/1770 train_time:128053ms step_avg:98.81ms
step:1307/1770 train_time:128156ms step_avg:98.81ms
step:1308/1770 train_time:128258ms step_avg:98.81ms
step:1309/1770 train_time:128361ms step_avg:98.82ms
step:1310/1770 train_time:128463ms step_avg:98.82ms
step:1311/1770 train_time:128566ms step_avg:98.82ms
step:1312/1770 train_time:128668ms step_avg:98.82ms
step:1313/1770 train_time:128771ms step_avg:98.83ms
step:1314/1770 train_time:128874ms step_avg:98.83ms
step:1315/1770 train_time:128976ms step_avg:98.83ms
step:1316/1770 train_time:129079ms step_avg:98.84ms
step:1317/1770 train_time:129182ms step_avg:98.84ms
step:1318/1770 train_time:129288ms step_avg:98.84ms
step:1319/1770 train_time:129391ms step_avg:98.85ms
step:1320/1770 train_time:129494ms step_avg:98.85ms
step:1321/1770 train_time:129596ms step_avg:98.85ms
step:1322/1770 train_time:129699ms step_avg:98.86ms
step:1323/1770 train_time:129804ms step_avg:98.86ms
step:1324/1770 train_time:129908ms step_avg:98.86ms
step:1325/1770 train_time:130013ms step_avg:98.87ms
step:1326/1770 train_time:130115ms step_avg:98.87ms
step:1327/1770 train_time:130221ms step_avg:98.88ms
step:1328/1770 train_time:130323ms step_avg:98.88ms
step:1329/1770 train_time:130426ms step_avg:98.88ms
step:1330/1770 train_time:130529ms step_avg:98.89ms
step:1331/1770 train_time:130631ms step_avg:98.89ms
step:1332/1770 train_time:130733ms step_avg:98.89ms
step:1333/1770 train_time:130835ms step_avg:98.89ms
step:1334/1770 train_time:130937ms step_avg:98.90ms
step:1335/1770 train_time:131040ms step_avg:98.90ms
step:1336/1770 train_time:131142ms step_avg:98.90ms
step:1337/1770 train_time:131245ms step_avg:98.90ms
step:1338/1770 train_time:131347ms step_avg:98.91ms
step:1339/1770 train_time:131451ms step_avg:98.91ms
step:1340/1770 train_time:131555ms step_avg:98.91ms
step:1341/1770 train_time:131657ms step_avg:98.92ms
step:1342/1770 train_time:131761ms step_avg:98.92ms
step:1343/1770 train_time:131864ms step_avg:98.92ms
step:1344/1770 train_time:131968ms step_avg:98.93ms
step:1345/1770 train_time:132070ms step_avg:98.93ms
step:1346/1770 train_time:132173ms step_avg:98.93ms
step:1347/1770 train_time:132275ms step_avg:98.93ms
step:1348/1770 train_time:132381ms step_avg:98.94ms
step:1349/1770 train_time:132484ms step_avg:98.94ms
step:1350/1770 train_time:132587ms step_avg:98.95ms
step:1351/1770 train_time:132689ms step_avg:98.95ms
step:1352/1770 train_time:132792ms step_avg:98.95ms
step:1353/1770 train_time:132895ms step_avg:98.95ms
step:1354/1770 train_time:132997ms step_avg:98.96ms
step:1355/1770 train_time:133100ms step_avg:98.96ms
step:1356/1770 train_time:133203ms step_avg:98.96ms
step:1357/1770 train_time:133306ms step_avg:98.97ms
step:1358/1770 train_time:133410ms step_avg:98.97ms
step:1359/1770 train_time:133512ms step_avg:98.97ms
step:1360/1770 train_time:133615ms step_avg:98.97ms
step:1361/1770 train_time:133719ms step_avg:98.98ms
step:1362/1770 train_time:133822ms step_avg:98.98ms
step:1363/1770 train_time:133926ms step_avg:98.98ms
step:1364/1770 train_time:134029ms step_avg:98.99ms
step:1365/1770 train_time:134132ms step_avg:98.99ms
step:1366/1770 train_time:134234ms step_avg:98.99ms
step:1367/1770 train_time:134337ms step_avg:99.00ms
step:1368/1770 train_time:134439ms step_avg:99.00ms
step:1369/1770 train_time:134544ms step_avg:99.00ms
step:1370/1770 train_time:134647ms step_avg:99.01ms
step:1371/1770 train_time:134751ms step_avg:99.01ms
step:1372/1770 train_time:134852ms step_avg:99.01ms
step:1373/1770 train_time:134955ms step_avg:99.01ms
step:1374/1770 train_time:135059ms step_avg:99.02ms
step:1375/1770 train_time:135162ms step_avg:99.02ms
step:1375/1770 val_loss:3.3891 train_time:135264ms step_avg:99.09ms
step:1376/1770 train_time:135286ms step_avg:99.04ms
step:1377/1770 train_time:135380ms step_avg:99.03ms
step:1378/1770 train_time:135483ms step_avg:99.04ms
step:1379/1770 train_time:135585ms step_avg:99.04ms
step:1380/1770 train_time:135687ms step_avg:99.04ms
step:1381/1770 train_time:135790ms step_avg:99.04ms
step:1382/1770 train_time:135892ms step_avg:99.05ms
step:1383/1770 train_time:135996ms step_avg:99.05ms
step:1384/1770 train_time:136098ms step_avg:99.05ms
step:1385/1770 train_time:136202ms step_avg:99.06ms
step:1386/1770 train_time:136305ms step_avg:99.06ms
step:1387/1770 train_time:136408ms step_avg:99.06ms
step:1388/1770 train_time:136510ms step_avg:99.06ms
step:1389/1770 train_time:136613ms step_avg:99.07ms
step:1390/1770 train_time:136716ms step_avg:99.07ms
step:1391/1770 train_time:136819ms step_avg:99.07ms
step:1392/1770 train_time:136922ms step_avg:99.08ms
step:1393/1770 train_time:137024ms step_avg:99.08ms
step:1394/1770 train_time:137127ms step_avg:99.08ms
step:1395/1770 train_time:137231ms step_avg:99.08ms
step:1396/1770 train_time:137336ms step_avg:99.09ms
step:1397/1770 train_time:137438ms step_avg:99.09ms
step:1398/1770 train_time:137541ms step_avg:99.09ms
step:1399/1770 train_time:137643ms step_avg:99.10ms
step:1400/1770 train_time:137746ms step_avg:99.10ms
step:1401/1770 train_time:137848ms step_avg:99.10ms
step:1402/1770 train_time:137951ms step_avg:99.10ms
step:1403/1770 train_time:138054ms step_avg:99.11ms
step:1404/1770 train_time:138158ms step_avg:99.11ms
step:1405/1770 train_time:138260ms step_avg:99.11ms
step:1406/1770 train_time:138362ms step_avg:99.11ms
step:1407/1770 train_time:138464ms step_avg:99.12ms
step:1408/1770 train_time:138566ms step_avg:99.12ms
step:1409/1770 train_time:138669ms step_avg:99.12ms
step:1410/1770 train_time:138773ms step_avg:99.12ms
step:1411/1770 train_time:138875ms step_avg:99.13ms
step:1412/1770 train_time:138979ms step_avg:99.13ms
step:1413/1770 train_time:139082ms step_avg:99.13ms
step:1414/1770 train_time:139185ms step_avg:99.13ms
step:1415/1770 train_time:139288ms step_avg:99.14ms
step:1416/1770 train_time:139392ms step_avg:99.14ms
step:1417/1770 train_time:139495ms step_avg:99.14ms
step:1418/1770 train_time:139599ms step_avg:99.15ms
step:1419/1770 train_time:139702ms step_avg:99.15ms
step:1420/1770 train_time:139805ms step_avg:99.15ms
step:1421/1770 train_time:139907ms step_avg:99.15ms
step:1422/1770 train_time:140009ms step_avg:99.16ms
step:1423/1770 train_time:140112ms step_avg:99.16ms
step:1424/1770 train_time:140215ms step_avg:99.16ms
step:1425/1770 train_time:140318ms step_avg:99.16ms
step:1426/1770 train_time:140421ms step_avg:99.17ms
step:1427/1770 train_time:140524ms step_avg:99.17ms
step:1428/1770 train_time:140630ms step_avg:99.17ms
step:1429/1770 train_time:140732ms step_avg:99.18ms
step:1430/1770 train_time:140835ms step_avg:99.18ms
step:1431/1770 train_time:140938ms step_avg:99.18ms
step:1432/1770 train_time:141040ms step_avg:99.18ms
step:1433/1770 train_time:141142ms step_avg:99.19ms
step:1434/1770 train_time:141244ms step_avg:99.19ms
step:1435/1770 train_time:141346ms step_avg:99.19ms
step:1436/1770 train_time:141449ms step_avg:99.19ms
step:1437/1770 train_time:141552ms step_avg:99.20ms
step:1438/1770 train_time:141655ms step_avg:99.20ms
step:1439/1770 train_time:141758ms step_avg:99.20ms
step:1440/1770 train_time:141861ms step_avg:99.20ms
step:1441/1770 train_time:141966ms step_avg:99.21ms
step:1442/1770 train_time:142069ms step_avg:99.21ms
step:1443/1770 train_time:142171ms step_avg:99.21ms
step:1444/1770 train_time:142274ms step_avg:99.22ms
step:1445/1770 train_time:142379ms step_avg:99.22ms
step:1446/1770 train_time:142483ms step_avg:99.22ms
step:1447/1770 train_time:142586ms step_avg:99.22ms
step:1448/1770 train_time:142690ms step_avg:99.23ms
step:1449/1770 train_time:142795ms step_avg:99.23ms
step:1450/1770 train_time:142899ms step_avg:99.24ms
step:1451/1770 train_time:143004ms step_avg:99.24ms
step:1452/1770 train_time:143109ms step_avg:99.24ms
step:1453/1770 train_time:143213ms step_avg:99.25ms
step:1454/1770 train_time:143316ms step_avg:99.25ms
step:1455/1770 train_time:143421ms step_avg:99.25ms
step:1456/1770 train_time:143525ms step_avg:99.26ms
step:1457/1770 train_time:143629ms step_avg:99.26ms
step:1458/1770 train_time:143733ms step_avg:99.26ms
step:1459/1770 train_time:143839ms step_avg:99.27ms
step:1460/1770 train_time:143942ms step_avg:99.27ms
step:1461/1770 train_time:144046ms step_avg:99.27ms
step:1462/1770 train_time:144149ms step_avg:99.28ms
step:1463/1770 train_time:144253ms step_avg:99.28ms
step:1464/1770 train_time:144358ms step_avg:99.28ms
step:1465/1770 train_time:144462ms step_avg:99.29ms
step:1466/1770 train_time:144566ms step_avg:99.29ms
step:1467/1770 train_time:144672ms step_avg:99.29ms
step:1468/1770 train_time:144776ms step_avg:99.30ms
step:1469/1770 train_time:144880ms step_avg:99.30ms
step:1470/1770 train_time:144983ms step_avg:99.30ms
step:1471/1770 train_time:145087ms step_avg:99.31ms
step:1472/1770 train_time:145190ms step_avg:99.31ms
step:1473/1770 train_time:145295ms step_avg:99.31ms
step:1474/1770 train_time:145400ms step_avg:99.32ms
step:1475/1770 train_time:145503ms step_avg:99.32ms
step:1476/1770 train_time:145607ms step_avg:99.32ms
step:1477/1770 train_time:145713ms step_avg:99.33ms
step:1478/1770 train_time:145817ms step_avg:99.33ms
step:1479/1770 train_time:145921ms step_avg:99.33ms
step:1480/1770 train_time:146024ms step_avg:99.34ms
step:1481/1770 train_time:146132ms step_avg:99.34ms
step:1482/1770 train_time:146235ms step_avg:99.34ms
step:1483/1770 train_time:146340ms step_avg:99.35ms
step:1484/1770 train_time:146444ms step_avg:99.35ms
step:1485/1770 train_time:146547ms step_avg:99.35ms
step:1486/1770 train_time:146650ms step_avg:99.36ms
step:1487/1770 train_time:146754ms step_avg:99.36ms
step:1488/1770 train_time:146859ms step_avg:99.36ms
step:1489/1770 train_time:146965ms step_avg:99.37ms
step:1490/1770 train_time:147068ms step_avg:99.37ms
step:1491/1770 train_time:147172ms step_avg:99.37ms
step:1492/1770 train_time:147276ms step_avg:99.38ms
step:1493/1770 train_time:147383ms step_avg:99.38ms
step:1494/1770 train_time:147490ms step_avg:99.39ms
step:1495/1770 train_time:147593ms step_avg:99.39ms
step:1496/1770 train_time:147697ms step_avg:99.39ms
step:1497/1770 train_time:147801ms step_avg:99.40ms
step:1498/1770 train_time:147904ms step_avg:99.40ms
step:1499/1770 train_time:148007ms step_avg:99.40ms
step:1500/1770 train_time:148110ms step_avg:99.40ms
step:1500/1770 val_loss:3.3521 train_time:148212ms step_avg:99.47ms
step:1501/1770 train_time:148233ms step_avg:99.42ms
step:1502/1770 train_time:148326ms step_avg:99.41ms
step:1503/1770 train_time:148429ms step_avg:99.42ms
step:1504/1770 train_time:148533ms step_avg:99.42ms
step:1505/1770 train_time:148641ms step_avg:99.43ms
step:1506/1770 train_time:148744ms step_avg:99.43ms
step:1507/1770 train_time:148848ms step_avg:99.43ms
step:1508/1770 train_time:148954ms step_avg:99.43ms
step:1509/1770 train_time:149058ms step_avg:99.44ms
step:1510/1770 train_time:149161ms step_avg:99.44ms
step:1511/1770 train_time:149265ms step_avg:99.44ms
step:1512/1770 train_time:149369ms step_avg:99.45ms
step:1513/1770 train_time:149474ms step_avg:99.45ms
step:1514/1770 train_time:149578ms step_avg:99.45ms
step:1515/1770 train_time:149681ms step_avg:99.46ms
step:1516/1770 train_time:149785ms step_avg:99.46ms
step:1517/1770 train_time:149888ms step_avg:99.46ms
step:1518/1770 train_time:149994ms step_avg:99.47ms
step:1519/1770 train_time:150098ms step_avg:99.47ms
step:1520/1770 train_time:150202ms step_avg:99.47ms
step:1521/1770 train_time:150306ms step_avg:99.47ms
step:1522/1770 train_time:150410ms step_avg:99.48ms
step:1523/1770 train_time:150514ms step_avg:99.48ms
step:1524/1770 train_time:150618ms step_avg:99.48ms
step:1525/1770 train_time:150721ms step_avg:99.49ms
step:1526/1770 train_time:150825ms step_avg:99.49ms
step:1527/1770 train_time:150929ms step_avg:99.49ms
step:1528/1770 train_time:151034ms step_avg:99.50ms
step:1529/1770 train_time:151138ms step_avg:99.50ms
step:1530/1770 train_time:151241ms step_avg:99.50ms
step:1531/1770 train_time:151345ms step_avg:99.50ms
step:1532/1770 train_time:151450ms step_avg:99.51ms
step:1533/1770 train_time:151555ms step_avg:99.51ms
step:1534/1770 train_time:151659ms step_avg:99.51ms
step:1535/1770 train_time:151762ms step_avg:99.52ms
step:1536/1770 train_time:151866ms step_avg:99.52ms
step:1537/1770 train_time:151970ms step_avg:99.52ms
step:1538/1770 train_time:152076ms step_avg:99.53ms
step:1539/1770 train_time:152180ms step_avg:99.53ms
step:1540/1770 train_time:152287ms step_avg:99.53ms
step:1541/1770 train_time:152391ms step_avg:99.54ms
step:1542/1770 train_time:152496ms step_avg:99.54ms
step:1543/1770 train_time:152599ms step_avg:99.54ms
step:1544/1770 train_time:152706ms step_avg:99.55ms
step:1545/1770 train_time:152809ms step_avg:99.55ms
step:1546/1770 train_time:152913ms step_avg:99.55ms
step:1547/1770 train_time:153017ms step_avg:99.56ms
step:1548/1770 train_time:153121ms step_avg:99.56ms
step:1549/1770 train_time:153225ms step_avg:99.56ms
step:1550/1770 train_time:153329ms step_avg:99.56ms
step:1551/1770 train_time:153431ms step_avg:99.57ms
step:1552/1770 train_time:153537ms step_avg:99.57ms
step:1553/1770 train_time:153641ms step_avg:99.57ms
step:1554/1770 train_time:153745ms step_avg:99.58ms
step:1555/1770 train_time:153850ms step_avg:99.58ms
step:1556/1770 train_time:153952ms step_avg:99.58ms
step:1557/1770 train_time:154056ms step_avg:99.58ms
step:1558/1770 train_time:154160ms step_avg:99.59ms
step:1559/1770 train_time:154264ms step_avg:99.59ms
step:1560/1770 train_time:154367ms step_avg:99.59ms
step:1561/1770 train_time:154473ms step_avg:99.60ms
step:1562/1770 train_time:154577ms step_avg:99.60ms
step:1563/1770 train_time:154681ms step_avg:99.60ms
step:1564/1770 train_time:154785ms step_avg:99.60ms
step:1565/1770 train_time:154889ms step_avg:99.61ms
step:1566/1770 train_time:154993ms step_avg:99.61ms
step:1567/1770 train_time:155098ms step_avg:99.61ms
step:1568/1770 train_time:155202ms step_avg:99.62ms
step:1569/1770 train_time:155309ms step_avg:99.62ms
step:1570/1770 train_time:155412ms step_avg:99.62ms
step:1571/1770 train_time:155516ms step_avg:99.63ms
step:1572/1770 train_time:155621ms step_avg:99.63ms
step:1573/1770 train_time:155727ms step_avg:99.63ms
step:1574/1770 train_time:155831ms step_avg:99.64ms
step:1575/1770 train_time:155934ms step_avg:99.64ms
step:1576/1770 train_time:156038ms step_avg:99.64ms
step:1577/1770 train_time:156143ms step_avg:99.64ms
step:1578/1770 train_time:156248ms step_avg:99.65ms
step:1579/1770 train_time:156352ms step_avg:99.65ms
step:1580/1770 train_time:156455ms step_avg:99.65ms
step:1581/1770 train_time:156562ms step_avg:99.66ms
step:1582/1770 train_time:156668ms step_avg:99.66ms
step:1583/1770 train_time:156772ms step_avg:99.66ms
step:1584/1770 train_time:156877ms step_avg:99.67ms
step:1585/1770 train_time:156981ms step_avg:99.67ms
step:1586/1770 train_time:157088ms step_avg:99.68ms
step:1587/1770 train_time:157193ms step_avg:99.68ms
step:1588/1770 train_time:157297ms step_avg:99.68ms
step:1589/1770 train_time:157403ms step_avg:99.69ms
step:1590/1770 train_time:157507ms step_avg:99.69ms
step:1591/1770 train_time:157610ms step_avg:99.69ms
step:1592/1770 train_time:157715ms step_avg:99.69ms
step:1593/1770 train_time:157819ms step_avg:99.70ms
step:1594/1770 train_time:157922ms step_avg:99.70ms
step:1595/1770 train_time:158026ms step_avg:99.70ms
step:1596/1770 train_time:158131ms step_avg:99.70ms
step:1597/1770 train_time:158235ms step_avg:99.71ms
step:1598/1770 train_time:158338ms step_avg:99.71ms
step:1599/1770 train_time:158444ms step_avg:99.71ms
step:1600/1770 train_time:158551ms step_avg:99.72ms
step:1601/1770 train_time:158655ms step_avg:99.72ms
step:1602/1770 train_time:158761ms step_avg:99.72ms
step:1603/1770 train_time:158865ms step_avg:99.73ms
step:1604/1770 train_time:158968ms step_avg:99.73ms
step:1605/1770 train_time:159071ms step_avg:99.73ms
step:1606/1770 train_time:159175ms step_avg:99.73ms
step:1607/1770 train_time:159283ms step_avg:99.74ms
step:1608/1770 train_time:159387ms step_avg:99.74ms
step:1609/1770 train_time:159491ms step_avg:99.74ms
step:1610/1770 train_time:159597ms step_avg:99.75ms
step:1611/1770 train_time:159703ms step_avg:99.75ms
step:1612/1770 train_time:159809ms step_avg:99.76ms
step:1613/1770 train_time:159912ms step_avg:99.76ms
step:1614/1770 train_time:160016ms step_avg:99.76ms
step:1615/1770 train_time:160121ms step_avg:99.76ms
step:1616/1770 train_time:160225ms step_avg:99.77ms
step:1617/1770 train_time:160331ms step_avg:99.77ms
step:1618/1770 train_time:160437ms step_avg:99.77ms
step:1619/1770 train_time:160542ms step_avg:99.78ms
step:1620/1770 train_time:160646ms step_avg:99.78ms
step:1621/1770 train_time:160750ms step_avg:99.78ms
step:1622/1770 train_time:160855ms step_avg:99.79ms
step:1623/1770 train_time:160963ms step_avg:99.79ms
step:1624/1770 train_time:161066ms step_avg:99.79ms
step:1625/1770 train_time:161170ms step_avg:99.80ms
step:1625/1770 val_loss:3.3207 train_time:161272ms step_avg:99.86ms
step:1626/1770 train_time:161295ms step_avg:99.81ms
step:1627/1770 train_time:161381ms step_avg:99.80ms
step:1628/1770 train_time:161485ms step_avg:99.81ms
step:1629/1770 train_time:161589ms step_avg:99.81ms
step:1630/1770 train_time:161693ms step_avg:99.81ms
step:1631/1770 train_time:161796ms step_avg:99.81ms
step:1632/1770 train_time:161900ms step_avg:99.82ms
step:1633/1770 train_time:162004ms step_avg:99.82ms
step:1634/1770 train_time:162108ms step_avg:99.82ms
step:1635/1770 train_time:162212ms step_avg:99.82ms
step:1636/1770 train_time:162316ms step_avg:99.83ms
step:1637/1770 train_time:162421ms step_avg:99.83ms
step:1638/1770 train_time:162525ms step_avg:99.83ms
step:1639/1770 train_time:162629ms step_avg:99.83ms
step:1640/1770 train_time:162734ms step_avg:99.84ms
step:1641/1770 train_time:162838ms step_avg:99.84ms
step:1642/1770 train_time:162940ms step_avg:99.84ms
step:1643/1770 train_time:163044ms step_avg:99.84ms
step:1644/1770 train_time:163151ms step_avg:99.85ms
step:1645/1770 train_time:163254ms step_avg:99.85ms
step:1646/1770 train_time:163360ms step_avg:99.85ms
step:1647/1770 train_time:163464ms step_avg:99.86ms
step:1648/1770 train_time:163568ms step_avg:99.86ms
step:1649/1770 train_time:163672ms step_avg:99.86ms
step:1650/1770 train_time:163777ms step_avg:99.86ms
step:1651/1770 train_time:163879ms step_avg:99.87ms
step:1652/1770 train_time:163983ms step_avg:99.87ms
step:1653/1770 train_time:164087ms step_avg:99.87ms
step:1654/1770 train_time:164195ms step_avg:99.88ms
step:1655/1770 train_time:164301ms step_avg:99.88ms
step:1656/1770 train_time:164406ms step_avg:99.88ms
step:1657/1770 train_time:164511ms step_avg:99.89ms
step:1658/1770 train_time:164616ms step_avg:99.89ms
step:1659/1770 train_time:164722ms step_avg:99.89ms
step:1660/1770 train_time:164826ms step_avg:99.89ms
step:1661/1770 train_time:164932ms step_avg:99.90ms
step:1662/1770 train_time:165036ms step_avg:99.90ms
step:1663/1770 train_time:165139ms step_avg:99.90ms
step:1664/1770 train_time:165243ms step_avg:99.90ms
step:1665/1770 train_time:165346ms step_avg:99.91ms
step:1666/1770 train_time:165451ms step_avg:99.91ms
step:1667/1770 train_time:165554ms step_avg:99.91ms
step:1668/1770 train_time:165658ms step_avg:99.91ms
step:1669/1770 train_time:165761ms step_avg:99.92ms
step:1670/1770 train_time:165864ms step_avg:99.92ms
step:1671/1770 train_time:165968ms step_avg:99.92ms
step:1672/1770 train_time:166074ms step_avg:99.92ms
step:1673/1770 train_time:166179ms step_avg:99.93ms
step:1674/1770 train_time:166282ms step_avg:99.93ms
step:1675/1770 train_time:166385ms step_avg:99.93ms
step:1676/1770 train_time:166491ms step_avg:99.93ms
step:1677/1770 train_time:166599ms step_avg:99.94ms
step:1678/1770 train_time:166702ms step_avg:99.94ms
step:1679/1770 train_time:166806ms step_avg:99.94ms
step:1680/1770 train_time:166910ms step_avg:99.95ms
step:1681/1770 train_time:167016ms step_avg:99.95ms
step:1682/1770 train_time:167122ms step_avg:99.95ms
step:1683/1770 train_time:167225ms step_avg:99.96ms
step:1684/1770 train_time:167329ms step_avg:99.96ms
step:1685/1770 train_time:167433ms step_avg:99.96ms
step:1686/1770 train_time:167537ms step_avg:99.96ms
step:1687/1770 train_time:167643ms step_avg:99.97ms
step:1688/1770 train_time:167747ms step_avg:99.97ms
step:1689/1770 train_time:167851ms step_avg:99.97ms
step:1690/1770 train_time:167956ms step_avg:99.97ms
step:1691/1770 train_time:168060ms step_avg:99.98ms
step:1692/1770 train_time:168163ms step_avg:99.98ms
step:1693/1770 train_time:168269ms step_avg:99.98ms
step:1694/1770 train_time:168373ms step_avg:99.98ms
step:1695/1770 train_time:168477ms step_avg:99.99ms
step:1696/1770 train_time:168583ms step_avg:99.99ms
step:1697/1770 train_time:168689ms step_avg:99.99ms
step:1698/1770 train_time:168794ms step_avg:100.00ms
step:1699/1770 train_time:168897ms step_avg:100.00ms
step:1700/1770 train_time:169001ms step_avg:100.00ms
step:1701/1770 train_time:169105ms step_avg:100.00ms
step:1702/1770 train_time:169210ms step_avg:100.01ms
step:1703/1770 train_time:169313ms step_avg:100.01ms
step:1704/1770 train_time:169417ms step_avg:100.01ms
step:1705/1770 train_time:169520ms step_avg:100.01ms
step:1706/1770 train_time:169623ms step_avg:100.01ms
step:1707/1770 train_time:169728ms step_avg:100.02ms
step:1708/1770 train_time:169834ms step_avg:100.02ms
step:1709/1770 train_time:169940ms step_avg:100.02ms
step:1710/1770 train_time:170047ms step_avg:100.03ms
step:1711/1770 train_time:170154ms step_avg:100.03ms
step:1712/1770 train_time:170259ms step_avg:100.03ms
step:1713/1770 train_time:170363ms step_avg:100.04ms
step:1714/1770 train_time:170468ms step_avg:100.04ms
step:1715/1770 train_time:170573ms step_avg:100.04ms
step:1716/1770 train_time:170678ms step_avg:100.05ms
step:1717/1770 train_time:170782ms step_avg:100.05ms
step:1718/1770 train_time:170888ms step_avg:100.05ms
step:1719/1770 train_time:170994ms step_avg:100.05ms
step:1720/1770 train_time:171099ms step_avg:100.06ms
step:1721/1770 train_time:171203ms step_avg:100.06ms
step:1722/1770 train_time:171312ms step_avg:100.07ms
step:1723/1770 train_time:171418ms step_avg:100.07ms
step:1724/1770 train_time:171525ms step_avg:100.07ms
step:1725/1770 train_time:171633ms step_avg:100.08ms
step:1726/1770 train_time:171739ms step_avg:100.08ms
step:1727/1770 train_time:171844ms step_avg:100.08ms
step:1728/1770 train_time:171951ms step_avg:100.09ms
step:1729/1770 train_time:172056ms step_avg:100.09ms
step:1730/1770 train_time:172162ms step_avg:100.09ms
step:1731/1770 train_time:172269ms step_avg:100.10ms
step:1732/1770 train_time:172374ms step_avg:100.10ms
step:1733/1770 train_time:172480ms step_avg:100.10ms
step:1734/1770 train_time:172584ms step_avg:100.11ms
step:1735/1770 train_time:172690ms step_avg:100.11ms
step:1736/1770 train_time:172794ms step_avg:100.11ms
step:1737/1770 train_time:172899ms step_avg:100.12ms
step:1738/1770 train_time:173003ms step_avg:100.12ms
step:1739/1770 train_time:173109ms step_avg:100.12ms
step:1740/1770 train_time:173214ms step_avg:100.12ms
step:1741/1770 train_time:173321ms step_avg:100.13ms
step:1742/1770 train_time:173428ms step_avg:100.13ms
step:1743/1770 train_time:173534ms step_avg:100.14ms
step:1744/1770 train_time:173639ms step_avg:100.14ms
step:1745/1770 train_time:173743ms step_avg:100.14ms
step:1746/1770 train_time:173852ms step_avg:100.15ms
step:1747/1770 train_time:173955ms step_avg:100.15ms
step:1748/1770 train_time:174063ms step_avg:100.15ms
step:1749/1770 train_time:174169ms step_avg:100.15ms
step:1750/1770 train_time:174274ms step_avg:100.16ms
step:1750/1770 val_loss:3.2967 train_time:174376ms step_avg:100.22ms
step:1751/1770 train_time:174398ms step_avg:100.17ms
step:1752/1770 train_time:174493ms step_avg:100.17ms
step:1753/1770 train_time:174598ms step_avg:100.17ms
step:1754/1770 train_time:174704ms step_avg:100.17ms
step:1755/1770 train_time:174808ms step_avg:100.18ms
step:1756/1770 train_time:174914ms step_avg:100.18ms
step:1757/1770 train_time:175019ms step_avg:100.18ms
step:1758/1770 train_time:175124ms step_avg:100.19ms
step:1759/1770 train_time:175229ms step_avg:100.19ms
step:1760/1770 train_time:175334ms step_avg:100.19ms
step:1761/1770 train_time:175442ms step_avg:100.20ms
step:1762/1770 train_time:175551ms step_avg:100.20ms
step:1763/1770 train_time:175654ms step_avg:100.20ms
step:1764/1770 train_time:175759ms step_avg:100.20ms
step:1765/1770 train_time:175864ms step_avg:100.21ms
step:1766/1770 train_time:175973ms step_avg:100.21ms
step:1767/1770 train_time:176077ms step_avg:100.21ms
step:1768/1770 train_time:176182ms step_avg:100.22ms
step:1769/1770 train_time:176287ms step_avg:100.22ms
step:1770/1770 train_time:176391ms step_avg:100.22ms
step:1770/1770 val_loss:3.2933 train_time:176495ms step_avg:100.28ms
peak memory allocated: 28840 MiB reserved: 32272 MiB
