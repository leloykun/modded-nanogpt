import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:50:25 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23641ms step_avg:nanms
step:2/1770 train_time:24054ms step_avg:nanms
step:3/1770 train_time:24148ms step_avg:nanms
step:4/1770 train_time:24241ms step_avg:nanms
step:5/1770 train_time:24419ms step_avg:nanms
step:6/1770 train_time:24513ms step_avg:nanms
step:7/1770 train_time:24608ms step_avg:nanms
step:8/1770 train_time:24703ms step_avg:nanms
step:9/1770 train_time:24796ms step_avg:nanms
step:10/1770 train_time:24891ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.34ms
step:14/1770 train_time:378ms step_avg:94.52ms
step:15/1770 train_time:474ms step_avg:94.75ms
step:16/1770 train_time:569ms step_avg:94.82ms
step:17/1770 train_time:664ms step_avg:94.93ms
step:18/1770 train_time:759ms step_avg:94.89ms
step:19/1770 train_time:854ms step_avg:94.86ms
step:20/1770 train_time:949ms step_avg:94.85ms
step:21/1770 train_time:1043ms step_avg:94.86ms
step:22/1770 train_time:1139ms step_avg:94.88ms
step:23/1770 train_time:1233ms step_avg:94.85ms
step:24/1770 train_time:1327ms step_avg:94.82ms
step:25/1770 train_time:1422ms step_avg:94.83ms
step:26/1770 train_time:1517ms step_avg:94.80ms
step:27/1770 train_time:1612ms step_avg:94.80ms
step:28/1770 train_time:1706ms step_avg:94.75ms
step:29/1770 train_time:1800ms step_avg:94.76ms
step:30/1770 train_time:1895ms step_avg:94.77ms
step:31/1770 train_time:1990ms step_avg:94.77ms
step:32/1770 train_time:2085ms step_avg:94.76ms
step:33/1770 train_time:2180ms step_avg:94.77ms
step:34/1770 train_time:2275ms step_avg:94.79ms
step:35/1770 train_time:2369ms step_avg:94.77ms
step:36/1770 train_time:2464ms step_avg:94.76ms
step:37/1770 train_time:2559ms step_avg:94.77ms
step:38/1770 train_time:2654ms step_avg:94.78ms
step:39/1770 train_time:2748ms step_avg:94.77ms
step:40/1770 train_time:2843ms step_avg:94.77ms
step:41/1770 train_time:2938ms step_avg:94.77ms
step:42/1770 train_time:3032ms step_avg:94.75ms
step:43/1770 train_time:3127ms step_avg:94.75ms
step:44/1770 train_time:3221ms step_avg:94.75ms
step:45/1770 train_time:3317ms step_avg:94.76ms
step:46/1770 train_time:3411ms step_avg:94.76ms
step:47/1770 train_time:3506ms step_avg:94.75ms
step:48/1770 train_time:3601ms step_avg:94.76ms
step:49/1770 train_time:3696ms step_avg:94.77ms
step:50/1770 train_time:3790ms step_avg:94.76ms
step:51/1770 train_time:3885ms step_avg:94.75ms
step:52/1770 train_time:3979ms step_avg:94.75ms
step:53/1770 train_time:4074ms step_avg:94.75ms
step:54/1770 train_time:4168ms step_avg:94.74ms
step:55/1770 train_time:4263ms step_avg:94.73ms
step:56/1770 train_time:4358ms step_avg:94.74ms
step:57/1770 train_time:4453ms step_avg:94.74ms
step:58/1770 train_time:4548ms step_avg:94.74ms
step:59/1770 train_time:4642ms step_avg:94.74ms
step:60/1770 train_time:4738ms step_avg:94.76ms
step:61/1770 train_time:4832ms step_avg:94.75ms
step:62/1770 train_time:4927ms step_avg:94.75ms
step:63/1770 train_time:5022ms step_avg:94.75ms
step:64/1770 train_time:5117ms step_avg:94.75ms
step:65/1770 train_time:5211ms step_avg:94.75ms
step:66/1770 train_time:5306ms step_avg:94.74ms
step:67/1770 train_time:5400ms step_avg:94.74ms
step:68/1770 train_time:5495ms step_avg:94.75ms
step:69/1770 train_time:5590ms step_avg:94.74ms
step:70/1770 train_time:5684ms step_avg:94.73ms
step:71/1770 train_time:5780ms step_avg:94.75ms
step:72/1770 train_time:5874ms step_avg:94.74ms
step:73/1770 train_time:5968ms step_avg:94.73ms
step:74/1770 train_time:6062ms step_avg:94.72ms
step:75/1770 train_time:6158ms step_avg:94.73ms
step:76/1770 train_time:6253ms step_avg:94.74ms
step:77/1770 train_time:6347ms step_avg:94.73ms
step:78/1770 train_time:6442ms step_avg:94.73ms
step:79/1770 train_time:6537ms step_avg:94.73ms
step:80/1770 train_time:6631ms step_avg:94.73ms
step:81/1770 train_time:6726ms step_avg:94.73ms
step:82/1770 train_time:6821ms step_avg:94.73ms
step:83/1770 train_time:6916ms step_avg:94.74ms
step:84/1770 train_time:7011ms step_avg:94.74ms
step:85/1770 train_time:7105ms step_avg:94.74ms
step:86/1770 train_time:7200ms step_avg:94.74ms
step:87/1770 train_time:7295ms step_avg:94.74ms
step:88/1770 train_time:7389ms step_avg:94.73ms
step:89/1770 train_time:7483ms step_avg:94.73ms
step:90/1770 train_time:7578ms step_avg:94.73ms
step:91/1770 train_time:7673ms step_avg:94.73ms
step:92/1770 train_time:7767ms step_avg:94.72ms
step:93/1770 train_time:7862ms step_avg:94.72ms
step:94/1770 train_time:7957ms step_avg:94.73ms
step:95/1770 train_time:8052ms step_avg:94.73ms
step:96/1770 train_time:8146ms step_avg:94.72ms
step:97/1770 train_time:8241ms step_avg:94.72ms
step:98/1770 train_time:8336ms step_avg:94.73ms
step:99/1770 train_time:8430ms step_avg:94.72ms
step:100/1770 train_time:8525ms step_avg:94.72ms
step:101/1770 train_time:8620ms step_avg:94.72ms
step:102/1770 train_time:8714ms step_avg:94.72ms
step:103/1770 train_time:8809ms step_avg:94.72ms
step:104/1770 train_time:8903ms step_avg:94.71ms
step:105/1770 train_time:9000ms step_avg:94.73ms
step:106/1770 train_time:9093ms step_avg:94.72ms
step:107/1770 train_time:9187ms step_avg:94.72ms
step:108/1770 train_time:9282ms step_avg:94.72ms
step:109/1770 train_time:9378ms step_avg:94.72ms
step:110/1770 train_time:9472ms step_avg:94.72ms
step:111/1770 train_time:9566ms step_avg:94.72ms
step:112/1770 train_time:9662ms step_avg:94.72ms
step:113/1770 train_time:9756ms step_avg:94.72ms
step:114/1770 train_time:9851ms step_avg:94.72ms
step:115/1770 train_time:9945ms step_avg:94.71ms
step:116/1770 train_time:10040ms step_avg:94.72ms
step:117/1770 train_time:10135ms step_avg:94.72ms
step:118/1770 train_time:10229ms step_avg:94.71ms
step:119/1770 train_time:10324ms step_avg:94.71ms
step:120/1770 train_time:10419ms step_avg:94.72ms
step:121/1770 train_time:10514ms step_avg:94.72ms
step:122/1770 train_time:10608ms step_avg:94.72ms
step:123/1770 train_time:10703ms step_avg:94.72ms
step:124/1770 train_time:10798ms step_avg:94.72ms
step:125/1770 train_time:10892ms step_avg:94.72ms
step:125/1770 val_loss:4.6646 train_time:10985ms step_avg:95.52ms
step:126/1770 train_time:11009ms step_avg:94.90ms
step:127/1770 train_time:11092ms step_avg:94.81ms
step:128/1770 train_time:11193ms step_avg:94.85ms
step:129/1770 train_time:11289ms step_avg:94.87ms
step:130/1770 train_time:11384ms step_avg:94.87ms
step:131/1770 train_time:11478ms step_avg:94.86ms
step:132/1770 train_time:11573ms step_avg:94.86ms
step:133/1770 train_time:11668ms step_avg:94.86ms
step:134/1770 train_time:11763ms step_avg:94.86ms
step:135/1770 train_time:11858ms step_avg:94.86ms
step:136/1770 train_time:11954ms step_avg:94.87ms
step:137/1770 train_time:12049ms step_avg:94.87ms
step:138/1770 train_time:12145ms step_avg:94.88ms
step:139/1770 train_time:12240ms step_avg:94.88ms
step:140/1770 train_time:12335ms step_avg:94.89ms
step:141/1770 train_time:12431ms step_avg:94.89ms
step:142/1770 train_time:12526ms step_avg:94.89ms
step:143/1770 train_time:12621ms step_avg:94.89ms
step:144/1770 train_time:12716ms step_avg:94.90ms
step:145/1770 train_time:12812ms step_avg:94.90ms
step:146/1770 train_time:12907ms step_avg:94.90ms
step:147/1770 train_time:13002ms step_avg:94.90ms
step:148/1770 train_time:13097ms step_avg:94.90ms
step:149/1770 train_time:13192ms step_avg:94.91ms
step:150/1770 train_time:13288ms step_avg:94.91ms
step:151/1770 train_time:13382ms step_avg:94.91ms
step:152/1770 train_time:13477ms step_avg:94.91ms
step:153/1770 train_time:13572ms step_avg:94.91ms
step:154/1770 train_time:13667ms step_avg:94.91ms
step:155/1770 train_time:13763ms step_avg:94.91ms
step:156/1770 train_time:13857ms step_avg:94.91ms
step:157/1770 train_time:13953ms step_avg:94.92ms
step:158/1770 train_time:14048ms step_avg:94.92ms
step:159/1770 train_time:14143ms step_avg:94.92ms
step:160/1770 train_time:14239ms step_avg:94.93ms
step:161/1770 train_time:14335ms step_avg:94.93ms
step:162/1770 train_time:14430ms step_avg:94.93ms
step:163/1770 train_time:14525ms step_avg:94.93ms
step:164/1770 train_time:14620ms step_avg:94.93ms
step:165/1770 train_time:14715ms step_avg:94.94ms
step:166/1770 train_time:14810ms step_avg:94.94ms
step:167/1770 train_time:14905ms step_avg:94.94ms
step:168/1770 train_time:15000ms step_avg:94.94ms
step:169/1770 train_time:15095ms step_avg:94.94ms
step:170/1770 train_time:15190ms step_avg:94.94ms
step:171/1770 train_time:15285ms step_avg:94.94ms
step:172/1770 train_time:15381ms step_avg:94.94ms
step:173/1770 train_time:15476ms step_avg:94.94ms
step:174/1770 train_time:15571ms step_avg:94.95ms
step:175/1770 train_time:15666ms step_avg:94.95ms
step:176/1770 train_time:15761ms step_avg:94.95ms
step:177/1770 train_time:15857ms step_avg:94.95ms
step:178/1770 train_time:15953ms step_avg:94.96ms
step:179/1770 train_time:16048ms step_avg:94.96ms
step:180/1770 train_time:16143ms step_avg:94.96ms
step:181/1770 train_time:16238ms step_avg:94.96ms
step:182/1770 train_time:16334ms step_avg:94.96ms
step:183/1770 train_time:16429ms step_avg:94.96ms
step:184/1770 train_time:16524ms step_avg:94.96ms
step:185/1770 train_time:16619ms step_avg:94.96ms
step:186/1770 train_time:16714ms step_avg:94.97ms
step:187/1770 train_time:16810ms step_avg:94.97ms
step:188/1770 train_time:16904ms step_avg:94.97ms
step:189/1770 train_time:16999ms step_avg:94.97ms
step:190/1770 train_time:17095ms step_avg:94.97ms
step:191/1770 train_time:17190ms step_avg:94.97ms
step:192/1770 train_time:17285ms step_avg:94.97ms
step:193/1770 train_time:17380ms step_avg:94.97ms
step:194/1770 train_time:17475ms step_avg:94.97ms
step:195/1770 train_time:17570ms step_avg:94.97ms
step:196/1770 train_time:17665ms step_avg:94.97ms
step:197/1770 train_time:17759ms step_avg:94.97ms
step:198/1770 train_time:17855ms step_avg:94.97ms
step:199/1770 train_time:17950ms step_avg:94.97ms
step:200/1770 train_time:18045ms step_avg:94.97ms
step:201/1770 train_time:18140ms step_avg:94.97ms
step:202/1770 train_time:18236ms step_avg:94.98ms
step:203/1770 train_time:18331ms step_avg:94.98ms
step:204/1770 train_time:18426ms step_avg:94.98ms
step:205/1770 train_time:18521ms step_avg:94.98ms
step:206/1770 train_time:18616ms step_avg:94.98ms
step:207/1770 train_time:18711ms step_avg:94.98ms
step:208/1770 train_time:18805ms step_avg:94.98ms
step:209/1770 train_time:18901ms step_avg:94.98ms
step:210/1770 train_time:18996ms step_avg:94.98ms
step:211/1770 train_time:19091ms step_avg:94.98ms
step:212/1770 train_time:19186ms step_avg:94.98ms
step:213/1770 train_time:19281ms step_avg:94.98ms
step:214/1770 train_time:19376ms step_avg:94.98ms
step:215/1770 train_time:19472ms step_avg:94.98ms
step:216/1770 train_time:19567ms step_avg:94.98ms
step:217/1770 train_time:19662ms step_avg:94.98ms
step:218/1770 train_time:19757ms step_avg:94.98ms
step:219/1770 train_time:19852ms step_avg:94.98ms
step:220/1770 train_time:19947ms step_avg:94.98ms
step:221/1770 train_time:20042ms step_avg:94.99ms
step:222/1770 train_time:20138ms step_avg:94.99ms
step:223/1770 train_time:20233ms step_avg:94.99ms
step:224/1770 train_time:20329ms step_avg:94.99ms
step:225/1770 train_time:20424ms step_avg:94.99ms
step:226/1770 train_time:20519ms step_avg:94.99ms
step:227/1770 train_time:20614ms step_avg:95.00ms
step:228/1770 train_time:20710ms step_avg:95.00ms
step:229/1770 train_time:20805ms step_avg:95.00ms
step:230/1770 train_time:20900ms step_avg:95.00ms
step:231/1770 train_time:20996ms step_avg:95.00ms
step:232/1770 train_time:21090ms step_avg:95.00ms
step:233/1770 train_time:21185ms step_avg:95.00ms
step:234/1770 train_time:21280ms step_avg:95.00ms
step:235/1770 train_time:21376ms step_avg:95.00ms
step:236/1770 train_time:21471ms step_avg:95.01ms
step:237/1770 train_time:21566ms step_avg:95.00ms
step:238/1770 train_time:21661ms step_avg:95.01ms
step:239/1770 train_time:21757ms step_avg:95.01ms
step:240/1770 train_time:21852ms step_avg:95.01ms
step:241/1770 train_time:21947ms step_avg:95.01ms
step:242/1770 train_time:22043ms step_avg:95.01ms
step:243/1770 train_time:22138ms step_avg:95.01ms
step:244/1770 train_time:22233ms step_avg:95.01ms
step:245/1770 train_time:22328ms step_avg:95.01ms
step:246/1770 train_time:22423ms step_avg:95.01ms
step:247/1770 train_time:22518ms step_avg:95.01ms
step:248/1770 train_time:22613ms step_avg:95.01ms
step:249/1770 train_time:22709ms step_avg:95.01ms
step:250/1770 train_time:22803ms step_avg:95.01ms
step:250/1770 val_loss:4.1197 train_time:22896ms step_avg:95.40ms
step:251/1770 train_time:22920ms step_avg:95.11ms
step:252/1770 train_time:23003ms step_avg:95.06ms
step:253/1770 train_time:23102ms step_avg:95.07ms
step:254/1770 train_time:23198ms step_avg:95.07ms
step:255/1770 train_time:23293ms step_avg:95.07ms
step:256/1770 train_time:23389ms step_avg:95.08ms
step:257/1770 train_time:23484ms step_avg:95.08ms
step:258/1770 train_time:23579ms step_avg:95.08ms
step:259/1770 train_time:23674ms step_avg:95.08ms
step:260/1770 train_time:23769ms step_avg:95.08ms
step:261/1770 train_time:23864ms step_avg:95.08ms
step:262/1770 train_time:23960ms step_avg:95.08ms
step:263/1770 train_time:24056ms step_avg:95.08ms
step:264/1770 train_time:24152ms step_avg:95.09ms
step:265/1770 train_time:24247ms step_avg:95.09ms
step:266/1770 train_time:24343ms step_avg:95.09ms
step:267/1770 train_time:24438ms step_avg:95.09ms
step:268/1770 train_time:24533ms step_avg:95.09ms
step:269/1770 train_time:24629ms step_avg:95.09ms
step:270/1770 train_time:24724ms step_avg:95.09ms
step:271/1770 train_time:24820ms step_avg:95.10ms
step:272/1770 train_time:24916ms step_avg:95.10ms
step:273/1770 train_time:25012ms step_avg:95.10ms
step:274/1770 train_time:25107ms step_avg:95.10ms
step:275/1770 train_time:25202ms step_avg:95.10ms
step:276/1770 train_time:25298ms step_avg:95.10ms
step:277/1770 train_time:25393ms step_avg:95.11ms
step:278/1770 train_time:25489ms step_avg:95.11ms
step:279/1770 train_time:25585ms step_avg:95.11ms
step:280/1770 train_time:25680ms step_avg:95.11ms
step:281/1770 train_time:25777ms step_avg:95.12ms
step:282/1770 train_time:25872ms step_avg:95.12ms
step:283/1770 train_time:25969ms step_avg:95.12ms
step:284/1770 train_time:26065ms step_avg:95.13ms
step:285/1770 train_time:26160ms step_avg:95.13ms
step:286/1770 train_time:26256ms step_avg:95.13ms
step:287/1770 train_time:26352ms step_avg:95.13ms
step:288/1770 train_time:26448ms step_avg:95.13ms
step:289/1770 train_time:26543ms step_avg:95.14ms
step:290/1770 train_time:26639ms step_avg:95.14ms
step:291/1770 train_time:26734ms step_avg:95.14ms
step:292/1770 train_time:26830ms step_avg:95.14ms
step:293/1770 train_time:26926ms step_avg:95.14ms
step:294/1770 train_time:27021ms step_avg:95.15ms
step:295/1770 train_time:27117ms step_avg:95.15ms
step:296/1770 train_time:27213ms step_avg:95.15ms
step:297/1770 train_time:27308ms step_avg:95.15ms
step:298/1770 train_time:27404ms step_avg:95.15ms
step:299/1770 train_time:27500ms step_avg:95.16ms
step:300/1770 train_time:27596ms step_avg:95.16ms
step:301/1770 train_time:27692ms step_avg:95.16ms
step:302/1770 train_time:27788ms step_avg:95.16ms
step:303/1770 train_time:27883ms step_avg:95.16ms
step:304/1770 train_time:27978ms step_avg:95.16ms
step:305/1770 train_time:28074ms step_avg:95.17ms
step:306/1770 train_time:28170ms step_avg:95.17ms
step:307/1770 train_time:28266ms step_avg:95.17ms
step:308/1770 train_time:28361ms step_avg:95.17ms
step:309/1770 train_time:28457ms step_avg:95.17ms
step:310/1770 train_time:28553ms step_avg:95.18ms
step:311/1770 train_time:28648ms step_avg:95.18ms
step:312/1770 train_time:28743ms step_avg:95.18ms
step:313/1770 train_time:28839ms step_avg:95.18ms
step:314/1770 train_time:28935ms step_avg:95.18ms
step:315/1770 train_time:29031ms step_avg:95.18ms
step:316/1770 train_time:29126ms step_avg:95.18ms
step:317/1770 train_time:29222ms step_avg:95.18ms
step:318/1770 train_time:29317ms step_avg:95.19ms
step:319/1770 train_time:29413ms step_avg:95.19ms
step:320/1770 train_time:29509ms step_avg:95.19ms
step:321/1770 train_time:29605ms step_avg:95.19ms
step:322/1770 train_time:29701ms step_avg:95.19ms
step:323/1770 train_time:29796ms step_avg:95.19ms
step:324/1770 train_time:29892ms step_avg:95.20ms
step:325/1770 train_time:29987ms step_avg:95.20ms
step:326/1770 train_time:30083ms step_avg:95.20ms
step:327/1770 train_time:30179ms step_avg:95.20ms
step:328/1770 train_time:30275ms step_avg:95.20ms
step:329/1770 train_time:30371ms step_avg:95.21ms
step:330/1770 train_time:30466ms step_avg:95.21ms
step:331/1770 train_time:30561ms step_avg:95.21ms
step:332/1770 train_time:30657ms step_avg:95.21ms
step:333/1770 train_time:30752ms step_avg:95.21ms
step:334/1770 train_time:30849ms step_avg:95.21ms
step:335/1770 train_time:30944ms step_avg:95.21ms
step:336/1770 train_time:31040ms step_avg:95.21ms
step:337/1770 train_time:31136ms step_avg:95.22ms
step:338/1770 train_time:31232ms step_avg:95.22ms
step:339/1770 train_time:31327ms step_avg:95.22ms
step:340/1770 train_time:31423ms step_avg:95.22ms
step:341/1770 train_time:31519ms step_avg:95.22ms
step:342/1770 train_time:31615ms step_avg:95.23ms
step:343/1770 train_time:31711ms step_avg:95.23ms
step:344/1770 train_time:31806ms step_avg:95.23ms
step:345/1770 train_time:31901ms step_avg:95.23ms
step:346/1770 train_time:31997ms step_avg:95.23ms
step:347/1770 train_time:32093ms step_avg:95.23ms
step:348/1770 train_time:32189ms step_avg:95.23ms
step:349/1770 train_time:32284ms step_avg:95.23ms
step:350/1770 train_time:32380ms step_avg:95.24ms
step:351/1770 train_time:32476ms step_avg:95.24ms
step:352/1770 train_time:32572ms step_avg:95.24ms
step:353/1770 train_time:32668ms step_avg:95.24ms
step:354/1770 train_time:32763ms step_avg:95.24ms
step:355/1770 train_time:32858ms step_avg:95.24ms
step:356/1770 train_time:32954ms step_avg:95.24ms
step:357/1770 train_time:33050ms step_avg:95.25ms
step:358/1770 train_time:33146ms step_avg:95.25ms
step:359/1770 train_time:33241ms step_avg:95.25ms
step:360/1770 train_time:33336ms step_avg:95.25ms
step:361/1770 train_time:33432ms step_avg:95.25ms
step:362/1770 train_time:33528ms step_avg:95.25ms
step:363/1770 train_time:33624ms step_avg:95.25ms
step:364/1770 train_time:33719ms step_avg:95.25ms
step:365/1770 train_time:33815ms step_avg:95.25ms
step:366/1770 train_time:33912ms step_avg:95.26ms
step:367/1770 train_time:34008ms step_avg:95.26ms
step:368/1770 train_time:34104ms step_avg:95.26ms
step:369/1770 train_time:34199ms step_avg:95.26ms
step:370/1770 train_time:34295ms step_avg:95.26ms
step:371/1770 train_time:34392ms step_avg:95.27ms
step:372/1770 train_time:34487ms step_avg:95.27ms
step:373/1770 train_time:34582ms step_avg:95.27ms
step:374/1770 train_time:34677ms step_avg:95.27ms
step:375/1770 train_time:34774ms step_avg:95.27ms
step:375/1770 val_loss:3.9112 train_time:34868ms step_avg:95.53ms
step:376/1770 train_time:34891ms step_avg:95.33ms
step:377/1770 train_time:34977ms step_avg:95.31ms
step:378/1770 train_time:35076ms step_avg:95.31ms
step:379/1770 train_time:35171ms step_avg:95.31ms
step:380/1770 train_time:35266ms step_avg:95.31ms
step:381/1770 train_time:35362ms step_avg:95.32ms
step:382/1770 train_time:35459ms step_avg:95.32ms
step:383/1770 train_time:35553ms step_avg:95.32ms
step:384/1770 train_time:35648ms step_avg:95.32ms
step:385/1770 train_time:35744ms step_avg:95.32ms
step:386/1770 train_time:35840ms step_avg:95.32ms
step:387/1770 train_time:35935ms step_avg:95.32ms
step:388/1770 train_time:36031ms step_avg:95.32ms
step:389/1770 train_time:36127ms step_avg:95.32ms
step:390/1770 train_time:36224ms step_avg:95.33ms
step:391/1770 train_time:36320ms step_avg:95.33ms
step:392/1770 train_time:36416ms step_avg:95.33ms
step:393/1770 train_time:36512ms step_avg:95.33ms
step:394/1770 train_time:36607ms step_avg:95.33ms
step:395/1770 train_time:36703ms step_avg:95.33ms
step:396/1770 train_time:36800ms step_avg:95.34ms
step:397/1770 train_time:36898ms step_avg:95.34ms
step:398/1770 train_time:36995ms step_avg:95.35ms
step:399/1770 train_time:37093ms step_avg:95.35ms
step:400/1770 train_time:37190ms step_avg:95.36ms
step:401/1770 train_time:37288ms step_avg:95.37ms
step:402/1770 train_time:37385ms step_avg:95.37ms
step:403/1770 train_time:37483ms step_avg:95.38ms
step:404/1770 train_time:37580ms step_avg:95.38ms
step:405/1770 train_time:37678ms step_avg:95.39ms
step:406/1770 train_time:37775ms step_avg:95.39ms
step:407/1770 train_time:37872ms step_avg:95.40ms
step:408/1770 train_time:37970ms step_avg:95.40ms
step:409/1770 train_time:38068ms step_avg:95.41ms
step:410/1770 train_time:38165ms step_avg:95.41ms
step:411/1770 train_time:38263ms step_avg:95.42ms
step:412/1770 train_time:38361ms step_avg:95.42ms
step:413/1770 train_time:38458ms step_avg:95.43ms
step:414/1770 train_time:38556ms step_avg:95.44ms
step:415/1770 train_time:38653ms step_avg:95.44ms
step:416/1770 train_time:38751ms step_avg:95.45ms
step:417/1770 train_time:38849ms step_avg:95.45ms
step:418/1770 train_time:38946ms step_avg:95.46ms
step:419/1770 train_time:39044ms step_avg:95.46ms
step:420/1770 train_time:39142ms step_avg:95.47ms
step:421/1770 train_time:39240ms step_avg:95.47ms
step:422/1770 train_time:39338ms step_avg:95.48ms
step:423/1770 train_time:39436ms step_avg:95.49ms
step:424/1770 train_time:39533ms step_avg:95.49ms
step:425/1770 train_time:39630ms step_avg:95.49ms
step:426/1770 train_time:39728ms step_avg:95.50ms
step:427/1770 train_time:39825ms step_avg:95.50ms
step:428/1770 train_time:39923ms step_avg:95.51ms
step:429/1770 train_time:40021ms step_avg:95.51ms
step:430/1770 train_time:40119ms step_avg:95.52ms
step:431/1770 train_time:40216ms step_avg:95.52ms
step:432/1770 train_time:40313ms step_avg:95.53ms
step:433/1770 train_time:40411ms step_avg:95.53ms
step:434/1770 train_time:40508ms step_avg:95.54ms
step:435/1770 train_time:40606ms step_avg:95.54ms
step:436/1770 train_time:40703ms step_avg:95.55ms
step:437/1770 train_time:40801ms step_avg:95.55ms
step:438/1770 train_time:40899ms step_avg:95.56ms
step:439/1770 train_time:40997ms step_avg:95.56ms
step:440/1770 train_time:41095ms step_avg:95.57ms
step:441/1770 train_time:41192ms step_avg:95.57ms
step:442/1770 train_time:41289ms step_avg:95.58ms
step:443/1770 train_time:41386ms step_avg:95.58ms
step:444/1770 train_time:41485ms step_avg:95.59ms
step:445/1770 train_time:41582ms step_avg:95.59ms
step:446/1770 train_time:41680ms step_avg:95.60ms
step:447/1770 train_time:41778ms step_avg:95.60ms
step:448/1770 train_time:41876ms step_avg:95.61ms
step:449/1770 train_time:41973ms step_avg:95.61ms
step:450/1770 train_time:42071ms step_avg:95.62ms
step:451/1770 train_time:42168ms step_avg:95.62ms
step:452/1770 train_time:42266ms step_avg:95.62ms
step:453/1770 train_time:42364ms step_avg:95.63ms
step:454/1770 train_time:42462ms step_avg:95.63ms
step:455/1770 train_time:42560ms step_avg:95.64ms
step:456/1770 train_time:42657ms step_avg:95.64ms
step:457/1770 train_time:42755ms step_avg:95.65ms
step:458/1770 train_time:42852ms step_avg:95.65ms
step:459/1770 train_time:42949ms step_avg:95.65ms
step:460/1770 train_time:43046ms step_avg:95.66ms
step:461/1770 train_time:43145ms step_avg:95.67ms
step:462/1770 train_time:43243ms step_avg:95.67ms
step:463/1770 train_time:43341ms step_avg:95.67ms
step:464/1770 train_time:43438ms step_avg:95.68ms
step:465/1770 train_time:43536ms step_avg:95.68ms
step:466/1770 train_time:43633ms step_avg:95.69ms
step:467/1770 train_time:43730ms step_avg:95.69ms
step:468/1770 train_time:43827ms step_avg:95.69ms
step:469/1770 train_time:43925ms step_avg:95.70ms
step:470/1770 train_time:44023ms step_avg:95.70ms
step:471/1770 train_time:44120ms step_avg:95.71ms
step:472/1770 train_time:44218ms step_avg:95.71ms
step:473/1770 train_time:44316ms step_avg:95.71ms
step:474/1770 train_time:44413ms step_avg:95.72ms
step:475/1770 train_time:44510ms step_avg:95.72ms
step:476/1770 train_time:44608ms step_avg:95.72ms
step:477/1770 train_time:44705ms step_avg:95.73ms
step:478/1770 train_time:44803ms step_avg:95.73ms
step:479/1770 train_time:44901ms step_avg:95.74ms
step:480/1770 train_time:44999ms step_avg:95.74ms
step:481/1770 train_time:45096ms step_avg:95.75ms
step:482/1770 train_time:45193ms step_avg:95.75ms
step:483/1770 train_time:45291ms step_avg:95.75ms
step:484/1770 train_time:45388ms step_avg:95.76ms
step:485/1770 train_time:45486ms step_avg:95.76ms
step:486/1770 train_time:45584ms step_avg:95.76ms
step:487/1770 train_time:45682ms step_avg:95.77ms
step:488/1770 train_time:45780ms step_avg:95.77ms
step:489/1770 train_time:45877ms step_avg:95.78ms
step:490/1770 train_time:45974ms step_avg:95.78ms
step:491/1770 train_time:46072ms step_avg:95.78ms
step:492/1770 train_time:46170ms step_avg:95.79ms
step:493/1770 train_time:46268ms step_avg:95.79ms
step:494/1770 train_time:46365ms step_avg:95.80ms
step:495/1770 train_time:46463ms step_avg:95.80ms
step:496/1770 train_time:46561ms step_avg:95.80ms
step:497/1770 train_time:46658ms step_avg:95.81ms
step:498/1770 train_time:46756ms step_avg:95.81ms
step:499/1770 train_time:46854ms step_avg:95.82ms
step:500/1770 train_time:46951ms step_avg:95.82ms
step:500/1770 val_loss:3.7581 train_time:47047ms step_avg:96.01ms
step:501/1770 train_time:47069ms step_avg:95.86ms
step:502/1770 train_time:47157ms step_avg:95.85ms
step:503/1770 train_time:47256ms step_avg:95.85ms
step:504/1770 train_time:47354ms step_avg:95.86ms
step:505/1770 train_time:47452ms step_avg:95.86ms
step:506/1770 train_time:47549ms step_avg:95.86ms
step:507/1770 train_time:47646ms step_avg:95.87ms
step:508/1770 train_time:47744ms step_avg:95.87ms
step:509/1770 train_time:47841ms step_avg:95.87ms
step:510/1770 train_time:47939ms step_avg:95.88ms
step:511/1770 train_time:48036ms step_avg:95.88ms
step:512/1770 train_time:48134ms step_avg:95.88ms
step:513/1770 train_time:48232ms step_avg:95.89ms
step:514/1770 train_time:48329ms step_avg:95.89ms
step:515/1770 train_time:48426ms step_avg:95.89ms
step:516/1770 train_time:48524ms step_avg:95.90ms
step:517/1770 train_time:48622ms step_avg:95.90ms
step:518/1770 train_time:48720ms step_avg:95.91ms
step:519/1770 train_time:48818ms step_avg:95.91ms
step:520/1770 train_time:48915ms step_avg:95.91ms
step:521/1770 train_time:49013ms step_avg:95.92ms
step:522/1770 train_time:49110ms step_avg:95.92ms
step:523/1770 train_time:49208ms step_avg:95.92ms
step:524/1770 train_time:49305ms step_avg:95.92ms
step:525/1770 train_time:49402ms step_avg:95.93ms
step:526/1770 train_time:49500ms step_avg:95.93ms
step:527/1770 train_time:49599ms step_avg:95.94ms
step:528/1770 train_time:49697ms step_avg:95.94ms
step:529/1770 train_time:49794ms step_avg:95.94ms
step:530/1770 train_time:49892ms step_avg:95.95ms
step:531/1770 train_time:49989ms step_avg:95.95ms
step:532/1770 train_time:50087ms step_avg:95.95ms
step:533/1770 train_time:50185ms step_avg:95.96ms
step:534/1770 train_time:50283ms step_avg:95.96ms
step:535/1770 train_time:50381ms step_avg:95.96ms
step:536/1770 train_time:50480ms step_avg:95.97ms
step:537/1770 train_time:50578ms step_avg:95.97ms
step:538/1770 train_time:50676ms step_avg:95.98ms
step:539/1770 train_time:50774ms step_avg:95.98ms
step:540/1770 train_time:50872ms step_avg:95.98ms
step:541/1770 train_time:50970ms step_avg:95.99ms
step:542/1770 train_time:51067ms step_avg:95.99ms
step:543/1770 train_time:51165ms step_avg:95.99ms
step:544/1770 train_time:51263ms step_avg:96.00ms
step:545/1770 train_time:51361ms step_avg:96.00ms
step:546/1770 train_time:51459ms step_avg:96.01ms
step:547/1770 train_time:51557ms step_avg:96.01ms
step:548/1770 train_time:51655ms step_avg:96.01ms
step:549/1770 train_time:51752ms step_avg:96.02ms
step:550/1770 train_time:51850ms step_avg:96.02ms
step:551/1770 train_time:51948ms step_avg:96.02ms
step:552/1770 train_time:52045ms step_avg:96.02ms
step:553/1770 train_time:52144ms step_avg:96.03ms
step:554/1770 train_time:52242ms step_avg:96.03ms
step:555/1770 train_time:52340ms step_avg:96.04ms
step:556/1770 train_time:52439ms step_avg:96.04ms
step:557/1770 train_time:52537ms step_avg:96.04ms
step:558/1770 train_time:52635ms step_avg:96.05ms
step:559/1770 train_time:52733ms step_avg:96.05ms
step:560/1770 train_time:52831ms step_avg:96.06ms
step:561/1770 train_time:52929ms step_avg:96.06ms
step:562/1770 train_time:53026ms step_avg:96.06ms
step:563/1770 train_time:53125ms step_avg:96.07ms
step:564/1770 train_time:53222ms step_avg:96.07ms
step:565/1770 train_time:53321ms step_avg:96.07ms
step:566/1770 train_time:53419ms step_avg:96.08ms
step:567/1770 train_time:53517ms step_avg:96.08ms
step:568/1770 train_time:53616ms step_avg:96.09ms
step:569/1770 train_time:53714ms step_avg:96.09ms
step:570/1770 train_time:53812ms step_avg:96.09ms
step:571/1770 train_time:53910ms step_avg:96.10ms
step:572/1770 train_time:54008ms step_avg:96.10ms
step:573/1770 train_time:54106ms step_avg:96.10ms
step:574/1770 train_time:54204ms step_avg:96.11ms
step:575/1770 train_time:54302ms step_avg:96.11ms
step:576/1770 train_time:54400ms step_avg:96.11ms
step:577/1770 train_time:54498ms step_avg:96.12ms
step:578/1770 train_time:54596ms step_avg:96.12ms
step:579/1770 train_time:54694ms step_avg:96.12ms
step:580/1770 train_time:54792ms step_avg:96.13ms
step:581/1770 train_time:54891ms step_avg:96.13ms
step:582/1770 train_time:54988ms step_avg:96.13ms
step:583/1770 train_time:55086ms step_avg:96.14ms
step:584/1770 train_time:55184ms step_avg:96.14ms
step:585/1770 train_time:55282ms step_avg:96.14ms
step:586/1770 train_time:55380ms step_avg:96.15ms
step:587/1770 train_time:55478ms step_avg:96.15ms
step:588/1770 train_time:55576ms step_avg:96.15ms
step:589/1770 train_time:55674ms step_avg:96.16ms
step:590/1770 train_time:55772ms step_avg:96.16ms
step:591/1770 train_time:55869ms step_avg:96.16ms
step:592/1770 train_time:55967ms step_avg:96.16ms
step:593/1770 train_time:56065ms step_avg:96.17ms
step:594/1770 train_time:56163ms step_avg:96.17ms
step:595/1770 train_time:56261ms step_avg:96.17ms
step:596/1770 train_time:56360ms step_avg:96.18ms
step:597/1770 train_time:56458ms step_avg:96.18ms
step:598/1770 train_time:56556ms step_avg:96.18ms
step:599/1770 train_time:56654ms step_avg:96.19ms
step:600/1770 train_time:56752ms step_avg:96.19ms
step:601/1770 train_time:56850ms step_avg:96.19ms
step:602/1770 train_time:56948ms step_avg:96.20ms
step:603/1770 train_time:57045ms step_avg:96.20ms
step:604/1770 train_time:57144ms step_avg:96.20ms
step:605/1770 train_time:57242ms step_avg:96.20ms
step:606/1770 train_time:57340ms step_avg:96.21ms
step:607/1770 train_time:57438ms step_avg:96.21ms
step:608/1770 train_time:57536ms step_avg:96.21ms
step:609/1770 train_time:57633ms step_avg:96.22ms
step:610/1770 train_time:57732ms step_avg:96.22ms
step:611/1770 train_time:57830ms step_avg:96.22ms
step:612/1770 train_time:57928ms step_avg:96.23ms
step:613/1770 train_time:58025ms step_avg:96.23ms
step:614/1770 train_time:58123ms step_avg:96.23ms
step:615/1770 train_time:58222ms step_avg:96.23ms
step:616/1770 train_time:58320ms step_avg:96.24ms
step:617/1770 train_time:58418ms step_avg:96.24ms
step:618/1770 train_time:58516ms step_avg:96.24ms
step:619/1770 train_time:58614ms step_avg:96.25ms
step:620/1770 train_time:58712ms step_avg:96.25ms
step:621/1770 train_time:58810ms step_avg:96.25ms
step:622/1770 train_time:58908ms step_avg:96.25ms
step:623/1770 train_time:59005ms step_avg:96.26ms
step:624/1770 train_time:59103ms step_avg:96.26ms
step:625/1770 train_time:59201ms step_avg:96.26ms
step:625/1770 val_loss:3.6679 train_time:59298ms step_avg:96.42ms
step:626/1770 train_time:59322ms step_avg:96.30ms
step:627/1770 train_time:59409ms step_avg:96.29ms
step:628/1770 train_time:59512ms step_avg:96.30ms
step:629/1770 train_time:59610ms step_avg:96.30ms
step:630/1770 train_time:59707ms step_avg:96.30ms
step:631/1770 train_time:59806ms step_avg:96.31ms
step:632/1770 train_time:59903ms step_avg:96.31ms
step:633/1770 train_time:60001ms step_avg:96.31ms
step:634/1770 train_time:60100ms step_avg:96.31ms
step:635/1770 train_time:60198ms step_avg:96.32ms
step:636/1770 train_time:60296ms step_avg:96.32ms
step:637/1770 train_time:60394ms step_avg:96.32ms
step:638/1770 train_time:60491ms step_avg:96.32ms
step:639/1770 train_time:60589ms step_avg:96.33ms
step:640/1770 train_time:60686ms step_avg:96.33ms
step:641/1770 train_time:60784ms step_avg:96.33ms
step:642/1770 train_time:60882ms step_avg:96.33ms
step:643/1770 train_time:60981ms step_avg:96.34ms
step:644/1770 train_time:61079ms step_avg:96.34ms
step:645/1770 train_time:61177ms step_avg:96.34ms
step:646/1770 train_time:61275ms step_avg:96.34ms
step:647/1770 train_time:61373ms step_avg:96.35ms
step:648/1770 train_time:61471ms step_avg:96.35ms
step:649/1770 train_time:61569ms step_avg:96.35ms
step:650/1770 train_time:61667ms step_avg:96.35ms
step:651/1770 train_time:61765ms step_avg:96.36ms
step:652/1770 train_time:61863ms step_avg:96.36ms
step:653/1770 train_time:61961ms step_avg:96.36ms
step:654/1770 train_time:62059ms step_avg:96.37ms
step:655/1770 train_time:62158ms step_avg:96.37ms
step:656/1770 train_time:62256ms step_avg:96.37ms
step:657/1770 train_time:62354ms step_avg:96.37ms
step:658/1770 train_time:62454ms step_avg:96.38ms
step:659/1770 train_time:62553ms step_avg:96.38ms
step:660/1770 train_time:62653ms step_avg:96.39ms
step:661/1770 train_time:62753ms step_avg:96.39ms
step:662/1770 train_time:62853ms step_avg:96.40ms
step:663/1770 train_time:62953ms step_avg:96.41ms
step:664/1770 train_time:63053ms step_avg:96.41ms
step:665/1770 train_time:63153ms step_avg:96.42ms
step:666/1770 train_time:63252ms step_avg:96.42ms
step:667/1770 train_time:63352ms step_avg:96.43ms
step:668/1770 train_time:63452ms step_avg:96.43ms
step:669/1770 train_time:63551ms step_avg:96.44ms
step:670/1770 train_time:63650ms step_avg:96.44ms
step:671/1770 train_time:63749ms step_avg:96.44ms
step:672/1770 train_time:63848ms step_avg:96.45ms
step:673/1770 train_time:63948ms step_avg:96.45ms
step:674/1770 train_time:64047ms step_avg:96.46ms
step:675/1770 train_time:64146ms step_avg:96.46ms
step:676/1770 train_time:64245ms step_avg:96.46ms
step:677/1770 train_time:64345ms step_avg:96.47ms
step:678/1770 train_time:64445ms step_avg:96.47ms
step:679/1770 train_time:64544ms step_avg:96.48ms
step:680/1770 train_time:64644ms step_avg:96.48ms
step:681/1770 train_time:64743ms step_avg:96.49ms
step:682/1770 train_time:64843ms step_avg:96.49ms
step:683/1770 train_time:64943ms step_avg:96.50ms
step:684/1770 train_time:65043ms step_avg:96.50ms
step:685/1770 train_time:65142ms step_avg:96.51ms
step:686/1770 train_time:65242ms step_avg:96.51ms
step:687/1770 train_time:65342ms step_avg:96.52ms
step:688/1770 train_time:65443ms step_avg:96.52ms
step:689/1770 train_time:65543ms step_avg:96.53ms
step:690/1770 train_time:65643ms step_avg:96.53ms
step:691/1770 train_time:65743ms step_avg:96.54ms
step:692/1770 train_time:65843ms step_avg:96.54ms
step:693/1770 train_time:65942ms step_avg:96.55ms
step:694/1770 train_time:66043ms step_avg:96.55ms
step:695/1770 train_time:66142ms step_avg:96.56ms
step:696/1770 train_time:66243ms step_avg:96.56ms
step:697/1770 train_time:66343ms step_avg:96.57ms
step:698/1770 train_time:66443ms step_avg:96.57ms
step:699/1770 train_time:66543ms step_avg:96.58ms
step:700/1770 train_time:66643ms step_avg:96.58ms
step:701/1770 train_time:66743ms step_avg:96.59ms
step:702/1770 train_time:66842ms step_avg:96.59ms
step:703/1770 train_time:66942ms step_avg:96.60ms
step:704/1770 train_time:67042ms step_avg:96.60ms
step:705/1770 train_time:67142ms step_avg:96.61ms
step:706/1770 train_time:67242ms step_avg:96.61ms
step:707/1770 train_time:67342ms step_avg:96.62ms
step:708/1770 train_time:67442ms step_avg:96.62ms
step:709/1770 train_time:67544ms step_avg:96.63ms
step:710/1770 train_time:67644ms step_avg:96.63ms
step:711/1770 train_time:67744ms step_avg:96.64ms
step:712/1770 train_time:67844ms step_avg:96.64ms
step:713/1770 train_time:67944ms step_avg:96.65ms
step:714/1770 train_time:68044ms step_avg:96.65ms
step:715/1770 train_time:68143ms step_avg:96.66ms
step:716/1770 train_time:68243ms step_avg:96.66ms
step:717/1770 train_time:68342ms step_avg:96.67ms
step:718/1770 train_time:68442ms step_avg:96.67ms
step:719/1770 train_time:68542ms step_avg:96.67ms
step:720/1770 train_time:68642ms step_avg:96.68ms
step:721/1770 train_time:68742ms step_avg:96.68ms
step:722/1770 train_time:68842ms step_avg:96.69ms
step:723/1770 train_time:68943ms step_avg:96.69ms
step:724/1770 train_time:69044ms step_avg:96.70ms
step:725/1770 train_time:69143ms step_avg:96.70ms
step:726/1770 train_time:69243ms step_avg:96.71ms
step:727/1770 train_time:69343ms step_avg:96.71ms
step:728/1770 train_time:69442ms step_avg:96.72ms
step:729/1770 train_time:69542ms step_avg:96.72ms
step:730/1770 train_time:69643ms step_avg:96.73ms
step:731/1770 train_time:69743ms step_avg:96.73ms
step:732/1770 train_time:69843ms step_avg:96.74ms
step:733/1770 train_time:69944ms step_avg:96.74ms
step:734/1770 train_time:70044ms step_avg:96.75ms
step:735/1770 train_time:70143ms step_avg:96.75ms
step:736/1770 train_time:70242ms step_avg:96.75ms
step:737/1770 train_time:70342ms step_avg:96.76ms
step:738/1770 train_time:70441ms step_avg:96.76ms
step:739/1770 train_time:70541ms step_avg:96.76ms
step:740/1770 train_time:70641ms step_avg:96.77ms
step:741/1770 train_time:70741ms step_avg:96.77ms
step:742/1770 train_time:70842ms step_avg:96.78ms
step:743/1770 train_time:70942ms step_avg:96.78ms
step:744/1770 train_time:71043ms step_avg:96.79ms
step:745/1770 train_time:71143ms step_avg:96.79ms
step:746/1770 train_time:71243ms step_avg:96.80ms
step:747/1770 train_time:71343ms step_avg:96.80ms
step:748/1770 train_time:71443ms step_avg:96.81ms
step:749/1770 train_time:71542ms step_avg:96.81ms
step:750/1770 train_time:71642ms step_avg:96.81ms
step:750/1770 val_loss:3.6038 train_time:71741ms step_avg:96.95ms
step:751/1770 train_time:71763ms step_avg:96.85ms
step:752/1770 train_time:71854ms step_avg:96.84ms
step:753/1770 train_time:71956ms step_avg:96.84ms
step:754/1770 train_time:72056ms step_avg:96.85ms
step:755/1770 train_time:72156ms step_avg:96.85ms
step:756/1770 train_time:72256ms step_avg:96.86ms
step:757/1770 train_time:72356ms step_avg:96.86ms
step:758/1770 train_time:72455ms step_avg:96.87ms
step:759/1770 train_time:72554ms step_avg:96.87ms
step:760/1770 train_time:72653ms step_avg:96.87ms
step:761/1770 train_time:72752ms step_avg:96.87ms
step:762/1770 train_time:72852ms step_avg:96.88ms
step:763/1770 train_time:72953ms step_avg:96.88ms
step:764/1770 train_time:73053ms step_avg:96.89ms
step:765/1770 train_time:73154ms step_avg:96.89ms
step:766/1770 train_time:73253ms step_avg:96.90ms
step:767/1770 train_time:73353ms step_avg:96.90ms
step:768/1770 train_time:73453ms step_avg:96.90ms
step:769/1770 train_time:73553ms step_avg:96.91ms
step:770/1770 train_time:73652ms step_avg:96.91ms
step:771/1770 train_time:73751ms step_avg:96.91ms
step:772/1770 train_time:73850ms step_avg:96.92ms
step:773/1770 train_time:73950ms step_avg:96.92ms
step:774/1770 train_time:74050ms step_avg:96.92ms
step:775/1770 train_time:74149ms step_avg:96.93ms
step:776/1770 train_time:74249ms step_avg:96.93ms
step:777/1770 train_time:74349ms step_avg:96.93ms
step:778/1770 train_time:74449ms step_avg:96.94ms
step:779/1770 train_time:74548ms step_avg:96.94ms
step:780/1770 train_time:74649ms step_avg:96.95ms
step:781/1770 train_time:74749ms step_avg:96.95ms
step:782/1770 train_time:74848ms step_avg:96.95ms
step:783/1770 train_time:74948ms step_avg:96.96ms
step:784/1770 train_time:75048ms step_avg:96.96ms
step:785/1770 train_time:75148ms step_avg:96.96ms
step:786/1770 train_time:75248ms step_avg:96.97ms
step:787/1770 train_time:75348ms step_avg:96.97ms
step:788/1770 train_time:75448ms step_avg:96.98ms
step:789/1770 train_time:75549ms step_avg:96.98ms
step:790/1770 train_time:75650ms step_avg:96.99ms
step:791/1770 train_time:75750ms step_avg:96.99ms
step:792/1770 train_time:75850ms step_avg:96.99ms
step:793/1770 train_time:75950ms step_avg:97.00ms
step:794/1770 train_time:76049ms step_avg:97.00ms
step:795/1770 train_time:76149ms step_avg:97.01ms
step:796/1770 train_time:76250ms step_avg:97.01ms
step:797/1770 train_time:76350ms step_avg:97.01ms
step:798/1770 train_time:76450ms step_avg:97.02ms
step:799/1770 train_time:76550ms step_avg:97.02ms
step:800/1770 train_time:76649ms step_avg:97.02ms
step:801/1770 train_time:76749ms step_avg:97.03ms
step:802/1770 train_time:76849ms step_avg:97.03ms
step:803/1770 train_time:76949ms step_avg:97.04ms
step:804/1770 train_time:77049ms step_avg:97.04ms
step:805/1770 train_time:77149ms step_avg:97.04ms
step:806/1770 train_time:77249ms step_avg:97.05ms
step:807/1770 train_time:77349ms step_avg:97.05ms
step:808/1770 train_time:77449ms step_avg:97.05ms
step:809/1770 train_time:77549ms step_avg:97.06ms
step:810/1770 train_time:77650ms step_avg:97.06ms
step:811/1770 train_time:77749ms step_avg:97.07ms
step:812/1770 train_time:77849ms step_avg:97.07ms
step:813/1770 train_time:77950ms step_avg:97.07ms
step:814/1770 train_time:78050ms step_avg:97.08ms
step:815/1770 train_time:78150ms step_avg:97.08ms
step:816/1770 train_time:78249ms step_avg:97.08ms
step:817/1770 train_time:78349ms step_avg:97.09ms
step:818/1770 train_time:78449ms step_avg:97.09ms
step:819/1770 train_time:78549ms step_avg:97.09ms
step:820/1770 train_time:78649ms step_avg:97.10ms
step:821/1770 train_time:78749ms step_avg:97.10ms
step:822/1770 train_time:78849ms step_avg:97.11ms
step:823/1770 train_time:78950ms step_avg:97.11ms
step:824/1770 train_time:79049ms step_avg:97.11ms
step:825/1770 train_time:79149ms step_avg:97.11ms
step:826/1770 train_time:79248ms step_avg:97.12ms
step:827/1770 train_time:79349ms step_avg:97.12ms
step:828/1770 train_time:79449ms step_avg:97.13ms
step:829/1770 train_time:79549ms step_avg:97.13ms
step:830/1770 train_time:79649ms step_avg:97.13ms
step:831/1770 train_time:79750ms step_avg:97.14ms
step:832/1770 train_time:79850ms step_avg:97.14ms
step:833/1770 train_time:79950ms step_avg:97.14ms
step:834/1770 train_time:80050ms step_avg:97.15ms
step:835/1770 train_time:80150ms step_avg:97.15ms
step:836/1770 train_time:80250ms step_avg:97.15ms
step:837/1770 train_time:80350ms step_avg:97.16ms
step:838/1770 train_time:80450ms step_avg:97.16ms
step:839/1770 train_time:80549ms step_avg:97.16ms
step:840/1770 train_time:80649ms step_avg:97.17ms
step:841/1770 train_time:80749ms step_avg:97.17ms
step:842/1770 train_time:80849ms step_avg:97.17ms
step:843/1770 train_time:80948ms step_avg:97.18ms
step:844/1770 train_time:81048ms step_avg:97.18ms
step:845/1770 train_time:81148ms step_avg:97.18ms
step:846/1770 train_time:81248ms step_avg:97.19ms
step:847/1770 train_time:81348ms step_avg:97.19ms
step:848/1770 train_time:81449ms step_avg:97.19ms
step:849/1770 train_time:81549ms step_avg:97.20ms
step:850/1770 train_time:81649ms step_avg:97.20ms
step:851/1770 train_time:81749ms step_avg:97.20ms
step:852/1770 train_time:81850ms step_avg:97.21ms
step:853/1770 train_time:81950ms step_avg:97.21ms
step:854/1770 train_time:82049ms step_avg:97.21ms
step:855/1770 train_time:82149ms step_avg:97.22ms
step:856/1770 train_time:82249ms step_avg:97.22ms
step:857/1770 train_time:82349ms step_avg:97.22ms
step:858/1770 train_time:82449ms step_avg:97.23ms
step:859/1770 train_time:82550ms step_avg:97.23ms
step:860/1770 train_time:82650ms step_avg:97.23ms
step:861/1770 train_time:82750ms step_avg:97.24ms
step:862/1770 train_time:82850ms step_avg:97.24ms
step:863/1770 train_time:82950ms step_avg:97.25ms
step:864/1770 train_time:83049ms step_avg:97.25ms
step:865/1770 train_time:83149ms step_avg:97.25ms
step:866/1770 train_time:83250ms step_avg:97.25ms
step:867/1770 train_time:83350ms step_avg:97.26ms
step:868/1770 train_time:83450ms step_avg:97.26ms
step:869/1770 train_time:83550ms step_avg:97.26ms
step:870/1770 train_time:83649ms step_avg:97.27ms
step:871/1770 train_time:83749ms step_avg:97.27ms
step:872/1770 train_time:83849ms step_avg:97.27ms
step:873/1770 train_time:83950ms step_avg:97.28ms
step:874/1770 train_time:84049ms step_avg:97.28ms
step:875/1770 train_time:84149ms step_avg:97.28ms
step:875/1770 val_loss:3.5551 train_time:84248ms step_avg:97.40ms
step:876/1770 train_time:84270ms step_avg:97.31ms
step:877/1770 train_time:84358ms step_avg:97.30ms
step:878/1770 train_time:84460ms step_avg:97.30ms
step:879/1770 train_time:84560ms step_avg:97.31ms
step:880/1770 train_time:84660ms step_avg:97.31ms
step:881/1770 train_time:84760ms step_avg:97.31ms
step:882/1770 train_time:84859ms step_avg:97.32ms
step:883/1770 train_time:84959ms step_avg:97.32ms
step:884/1770 train_time:85059ms step_avg:97.32ms
step:885/1770 train_time:85158ms step_avg:97.32ms
step:886/1770 train_time:85258ms step_avg:97.33ms
step:887/1770 train_time:85358ms step_avg:97.33ms
step:888/1770 train_time:85459ms step_avg:97.33ms
step:889/1770 train_time:85560ms step_avg:97.34ms
step:890/1770 train_time:85659ms step_avg:97.34ms
step:891/1770 train_time:85760ms step_avg:97.34ms
step:892/1770 train_time:85859ms step_avg:97.35ms
step:893/1770 train_time:85959ms step_avg:97.35ms
step:894/1770 train_time:86059ms step_avg:97.35ms
step:895/1770 train_time:86159ms step_avg:97.35ms
step:896/1770 train_time:86259ms step_avg:97.36ms
step:897/1770 train_time:86360ms step_avg:97.36ms
step:898/1770 train_time:86461ms step_avg:97.37ms
step:899/1770 train_time:86561ms step_avg:97.37ms
step:900/1770 train_time:86661ms step_avg:97.37ms
step:901/1770 train_time:86761ms step_avg:97.37ms
step:902/1770 train_time:86861ms step_avg:97.38ms
step:903/1770 train_time:86961ms step_avg:97.38ms
step:904/1770 train_time:87061ms step_avg:97.38ms
step:905/1770 train_time:87161ms step_avg:97.39ms
step:906/1770 train_time:87260ms step_avg:97.39ms
step:907/1770 train_time:87360ms step_avg:97.39ms
step:908/1770 train_time:87460ms step_avg:97.39ms
step:909/1770 train_time:87560ms step_avg:97.40ms
step:910/1770 train_time:87661ms step_avg:97.40ms
step:911/1770 train_time:87761ms step_avg:97.40ms
step:912/1770 train_time:87861ms step_avg:97.41ms
step:913/1770 train_time:87961ms step_avg:97.41ms
step:914/1770 train_time:88061ms step_avg:97.41ms
step:915/1770 train_time:88162ms step_avg:97.42ms
step:916/1770 train_time:88262ms step_avg:97.42ms
step:917/1770 train_time:88362ms step_avg:97.42ms
step:918/1770 train_time:88462ms step_avg:97.43ms
step:919/1770 train_time:88562ms step_avg:97.43ms
step:920/1770 train_time:88664ms step_avg:97.43ms
step:921/1770 train_time:88767ms step_avg:97.44ms
step:922/1770 train_time:88869ms step_avg:97.44ms
step:923/1770 train_time:88970ms step_avg:97.45ms
step:924/1770 train_time:89071ms step_avg:97.45ms
step:925/1770 train_time:89173ms step_avg:97.46ms
step:926/1770 train_time:89273ms step_avg:97.46ms
step:927/1770 train_time:89374ms step_avg:97.46ms
step:928/1770 train_time:89475ms step_avg:97.47ms
step:929/1770 train_time:89575ms step_avg:97.47ms
step:930/1770 train_time:89676ms step_avg:97.47ms
step:931/1770 train_time:89777ms step_avg:97.48ms
step:932/1770 train_time:89879ms step_avg:97.48ms
step:933/1770 train_time:89980ms step_avg:97.49ms
step:934/1770 train_time:90082ms step_avg:97.49ms
step:935/1770 train_time:90182ms step_avg:97.49ms
step:936/1770 train_time:90285ms step_avg:97.50ms
step:937/1770 train_time:90386ms step_avg:97.50ms
step:938/1770 train_time:90489ms step_avg:97.51ms
step:939/1770 train_time:90591ms step_avg:97.51ms
step:940/1770 train_time:90692ms step_avg:97.52ms
step:941/1770 train_time:90793ms step_avg:97.52ms
step:942/1770 train_time:90894ms step_avg:97.53ms
step:943/1770 train_time:90996ms step_avg:97.53ms
step:944/1770 train_time:91096ms step_avg:97.53ms
step:945/1770 train_time:91198ms step_avg:97.54ms
step:946/1770 train_time:91300ms step_avg:97.54ms
step:947/1770 train_time:91402ms step_avg:97.55ms
step:948/1770 train_time:91502ms step_avg:97.55ms
step:949/1770 train_time:91604ms step_avg:97.56ms
step:950/1770 train_time:91706ms step_avg:97.56ms
step:951/1770 train_time:91809ms step_avg:97.57ms
step:952/1770 train_time:91910ms step_avg:97.57ms
step:953/1770 train_time:92012ms step_avg:97.57ms
step:954/1770 train_time:92113ms step_avg:97.58ms
step:955/1770 train_time:92214ms step_avg:97.58ms
step:956/1770 train_time:92315ms step_avg:97.58ms
step:957/1770 train_time:92416ms step_avg:97.59ms
step:958/1770 train_time:92517ms step_avg:97.59ms
step:959/1770 train_time:92618ms step_avg:97.60ms
step:960/1770 train_time:92719ms step_avg:97.60ms
step:961/1770 train_time:92820ms step_avg:97.60ms
step:962/1770 train_time:92923ms step_avg:97.61ms
step:963/1770 train_time:93024ms step_avg:97.61ms
step:964/1770 train_time:93126ms step_avg:97.62ms
step:965/1770 train_time:93227ms step_avg:97.62ms
step:966/1770 train_time:93329ms step_avg:97.62ms
step:967/1770 train_time:93431ms step_avg:97.63ms
step:968/1770 train_time:93532ms step_avg:97.63ms
step:969/1770 train_time:93633ms step_avg:97.64ms
step:970/1770 train_time:93734ms step_avg:97.64ms
step:971/1770 train_time:93836ms step_avg:97.64ms
step:972/1770 train_time:93938ms step_avg:97.65ms
step:973/1770 train_time:94039ms step_avg:97.65ms
step:974/1770 train_time:94141ms step_avg:97.66ms
step:975/1770 train_time:94243ms step_avg:97.66ms
step:976/1770 train_time:94343ms step_avg:97.66ms
step:977/1770 train_time:94445ms step_avg:97.67ms
step:978/1770 train_time:94547ms step_avg:97.67ms
step:979/1770 train_time:94649ms step_avg:97.68ms
step:980/1770 train_time:94752ms step_avg:97.68ms
step:981/1770 train_time:94853ms step_avg:97.69ms
step:982/1770 train_time:94954ms step_avg:97.69ms
step:983/1770 train_time:95055ms step_avg:97.69ms
step:984/1770 train_time:95156ms step_avg:97.70ms
step:985/1770 train_time:95257ms step_avg:97.70ms
step:986/1770 train_time:95358ms step_avg:97.70ms
step:987/1770 train_time:95460ms step_avg:97.71ms
step:988/1770 train_time:95561ms step_avg:97.71ms
step:989/1770 train_time:95663ms step_avg:97.72ms
step:990/1770 train_time:95764ms step_avg:97.72ms
step:991/1770 train_time:95866ms step_avg:97.72ms
step:992/1770 train_time:95968ms step_avg:97.73ms
step:993/1770 train_time:96070ms step_avg:97.73ms
step:994/1770 train_time:96171ms step_avg:97.74ms
step:995/1770 train_time:96273ms step_avg:97.74ms
step:996/1770 train_time:96374ms step_avg:97.74ms
step:997/1770 train_time:96475ms step_avg:97.75ms
step:998/1770 train_time:96575ms step_avg:97.75ms
step:999/1770 train_time:96677ms step_avg:97.75ms
step:1000/1770 train_time:96779ms step_avg:97.76ms
step:1000/1770 val_loss:3.5144 train_time:96878ms step_avg:97.86ms
step:1001/1770 train_time:96903ms step_avg:97.78ms
step:1002/1770 train_time:96992ms step_avg:97.77ms
step:1003/1770 train_time:97095ms step_avg:97.78ms
step:1004/1770 train_time:97196ms step_avg:97.78ms
step:1005/1770 train_time:97298ms step_avg:97.79ms
step:1006/1770 train_time:97399ms step_avg:97.79ms
step:1007/1770 train_time:97500ms step_avg:97.79ms
step:1008/1770 train_time:97602ms step_avg:97.80ms
step:1009/1770 train_time:97703ms step_avg:97.80ms
step:1010/1770 train_time:97803ms step_avg:97.80ms
step:1011/1770 train_time:97906ms step_avg:97.81ms
step:1012/1770 train_time:98007ms step_avg:97.81ms
step:1013/1770 train_time:98109ms step_avg:97.82ms
step:1014/1770 train_time:98210ms step_avg:97.82ms
step:1015/1770 train_time:98311ms step_avg:97.82ms
step:1016/1770 train_time:98412ms step_avg:97.83ms
step:1017/1770 train_time:98514ms step_avg:97.83ms
step:1018/1770 train_time:98615ms step_avg:97.83ms
step:1019/1770 train_time:98716ms step_avg:97.84ms
step:1020/1770 train_time:98818ms step_avg:97.84ms
step:1021/1770 train_time:98921ms step_avg:97.84ms
step:1022/1770 train_time:99024ms step_avg:97.85ms
step:1023/1770 train_time:99125ms step_avg:97.85ms
step:1024/1770 train_time:99226ms step_avg:97.86ms
step:1025/1770 train_time:99327ms step_avg:97.86ms
step:1026/1770 train_time:99428ms step_avg:97.86ms
step:1027/1770 train_time:99531ms step_avg:97.87ms
step:1028/1770 train_time:99632ms step_avg:97.87ms
step:1029/1770 train_time:99733ms step_avg:97.87ms
step:1030/1770 train_time:99834ms step_avg:97.88ms
step:1031/1770 train_time:99935ms step_avg:97.88ms
step:1032/1770 train_time:100038ms step_avg:97.88ms
step:1033/1770 train_time:100139ms step_avg:97.89ms
step:1034/1770 train_time:100241ms step_avg:97.89ms
step:1035/1770 train_time:100343ms step_avg:97.90ms
step:1036/1770 train_time:100444ms step_avg:97.90ms
step:1037/1770 train_time:100546ms step_avg:97.90ms
step:1038/1770 train_time:100646ms step_avg:97.91ms
step:1039/1770 train_time:100747ms step_avg:97.91ms
step:1040/1770 train_time:100848ms step_avg:97.91ms
step:1041/1770 train_time:100949ms step_avg:97.91ms
step:1042/1770 train_time:101051ms step_avg:97.92ms
step:1043/1770 train_time:101152ms step_avg:97.92ms
step:1044/1770 train_time:101254ms step_avg:97.92ms
step:1045/1770 train_time:101357ms step_avg:97.93ms
step:1046/1770 train_time:101458ms step_avg:97.93ms
step:1047/1770 train_time:101559ms step_avg:97.94ms
step:1048/1770 train_time:101660ms step_avg:97.94ms
step:1049/1770 train_time:101761ms step_avg:97.94ms
step:1050/1770 train_time:101863ms step_avg:97.95ms
step:1051/1770 train_time:101965ms step_avg:97.95ms
step:1052/1770 train_time:102067ms step_avg:97.95ms
step:1053/1770 train_time:102168ms step_avg:97.96ms
step:1054/1770 train_time:102269ms step_avg:97.96ms
step:1055/1770 train_time:102370ms step_avg:97.96ms
step:1056/1770 train_time:102472ms step_avg:97.97ms
step:1057/1770 train_time:102572ms step_avg:97.97ms
step:1058/1770 train_time:102674ms step_avg:97.97ms
step:1059/1770 train_time:102776ms step_avg:97.97ms
step:1060/1770 train_time:102878ms step_avg:97.98ms
step:1061/1770 train_time:102981ms step_avg:97.98ms
step:1062/1770 train_time:103083ms step_avg:97.99ms
step:1063/1770 train_time:103186ms step_avg:97.99ms
step:1064/1770 train_time:103288ms step_avg:98.00ms
step:1065/1770 train_time:103389ms step_avg:98.00ms
step:1066/1770 train_time:103491ms step_avg:98.00ms
step:1067/1770 train_time:103592ms step_avg:98.01ms
step:1068/1770 train_time:103693ms step_avg:98.01ms
step:1069/1770 train_time:103795ms step_avg:98.01ms
step:1070/1770 train_time:103897ms step_avg:98.02ms
step:1071/1770 train_time:103998ms step_avg:98.02ms
step:1072/1770 train_time:104101ms step_avg:98.02ms
step:1073/1770 train_time:104203ms step_avg:98.03ms
step:1074/1770 train_time:104304ms step_avg:98.03ms
step:1075/1770 train_time:104405ms step_avg:98.03ms
step:1076/1770 train_time:104507ms step_avg:98.04ms
step:1077/1770 train_time:104608ms step_avg:98.04ms
step:1078/1770 train_time:104709ms step_avg:98.04ms
step:1079/1770 train_time:104811ms step_avg:98.05ms
step:1080/1770 train_time:104913ms step_avg:98.05ms
step:1081/1770 train_time:105014ms step_avg:98.05ms
step:1082/1770 train_time:105116ms step_avg:98.06ms
step:1083/1770 train_time:105218ms step_avg:98.06ms
step:1084/1770 train_time:105320ms step_avg:98.06ms
step:1085/1770 train_time:105422ms step_avg:98.07ms
step:1086/1770 train_time:105523ms step_avg:98.07ms
step:1087/1770 train_time:105625ms step_avg:98.07ms
step:1088/1770 train_time:105726ms step_avg:98.08ms
step:1089/1770 train_time:105827ms step_avg:98.08ms
step:1090/1770 train_time:105929ms step_avg:98.08ms
step:1091/1770 train_time:106030ms step_avg:98.09ms
step:1092/1770 train_time:106131ms step_avg:98.09ms
step:1093/1770 train_time:106232ms step_avg:98.09ms
step:1094/1770 train_time:106335ms step_avg:98.10ms
step:1095/1770 train_time:106436ms step_avg:98.10ms
step:1096/1770 train_time:106538ms step_avg:98.10ms
step:1097/1770 train_time:106640ms step_avg:98.10ms
step:1098/1770 train_time:106741ms step_avg:98.11ms
step:1099/1770 train_time:106843ms step_avg:98.11ms
step:1100/1770 train_time:106945ms step_avg:98.11ms
step:1101/1770 train_time:107046ms step_avg:98.12ms
step:1102/1770 train_time:107147ms step_avg:98.12ms
step:1103/1770 train_time:107248ms step_avg:98.12ms
step:1104/1770 train_time:107351ms step_avg:98.13ms
step:1105/1770 train_time:107452ms step_avg:98.13ms
step:1106/1770 train_time:107553ms step_avg:98.13ms
step:1107/1770 train_time:107654ms step_avg:98.13ms
step:1108/1770 train_time:107756ms step_avg:98.14ms
step:1109/1770 train_time:107857ms step_avg:98.14ms
step:1110/1770 train_time:107959ms step_avg:98.14ms
step:1111/1770 train_time:108062ms step_avg:98.15ms
step:1112/1770 train_time:108164ms step_avg:98.15ms
step:1113/1770 train_time:108267ms step_avg:98.16ms
step:1114/1770 train_time:108367ms step_avg:98.16ms
step:1115/1770 train_time:108469ms step_avg:98.16ms
step:1116/1770 train_time:108570ms step_avg:98.16ms
step:1117/1770 train_time:108671ms step_avg:98.17ms
step:1118/1770 train_time:108772ms step_avg:98.17ms
step:1119/1770 train_time:108873ms step_avg:98.17ms
step:1120/1770 train_time:108975ms step_avg:98.18ms
step:1121/1770 train_time:109076ms step_avg:98.18ms
step:1122/1770 train_time:109179ms step_avg:98.18ms
step:1123/1770 train_time:109281ms step_avg:98.19ms
step:1124/1770 train_time:109384ms step_avg:98.19ms
step:1125/1770 train_time:109485ms step_avg:98.19ms
step:1125/1770 val_loss:3.4746 train_time:109584ms step_avg:98.28ms
step:1126/1770 train_time:109607ms step_avg:98.21ms
step:1127/1770 train_time:109696ms step_avg:98.21ms
step:1128/1770 train_time:109798ms step_avg:98.21ms
step:1129/1770 train_time:109900ms step_avg:98.21ms
step:1130/1770 train_time:110001ms step_avg:98.22ms
step:1131/1770 train_time:110103ms step_avg:98.22ms
step:1132/1770 train_time:110205ms step_avg:98.22ms
step:1133/1770 train_time:110307ms step_avg:98.23ms
step:1134/1770 train_time:110409ms step_avg:98.23ms
step:1135/1770 train_time:110511ms step_avg:98.23ms
step:1136/1770 train_time:110613ms step_avg:98.24ms
step:1137/1770 train_time:110715ms step_avg:98.24ms
step:1138/1770 train_time:110816ms step_avg:98.24ms
step:1139/1770 train_time:110917ms step_avg:98.24ms
step:1140/1770 train_time:111018ms step_avg:98.25ms
step:1141/1770 train_time:111119ms step_avg:98.25ms
step:1142/1770 train_time:111221ms step_avg:98.25ms
step:1143/1770 train_time:111322ms step_avg:98.25ms
step:1144/1770 train_time:111423ms step_avg:98.26ms
step:1145/1770 train_time:111525ms step_avg:98.26ms
step:1146/1770 train_time:111626ms step_avg:98.26ms
step:1147/1770 train_time:111729ms step_avg:98.27ms
step:1148/1770 train_time:111831ms step_avg:98.27ms
step:1149/1770 train_time:111932ms step_avg:98.27ms
step:1150/1770 train_time:112033ms step_avg:98.27ms
step:1151/1770 train_time:112135ms step_avg:98.28ms
step:1152/1770 train_time:112236ms step_avg:98.28ms
step:1153/1770 train_time:112337ms step_avg:98.28ms
step:1154/1770 train_time:112439ms step_avg:98.29ms
step:1155/1770 train_time:112540ms step_avg:98.29ms
step:1156/1770 train_time:112642ms step_avg:98.29ms
step:1157/1770 train_time:112745ms step_avg:98.30ms
step:1158/1770 train_time:112847ms step_avg:98.30ms
step:1159/1770 train_time:112949ms step_avg:98.30ms
step:1160/1770 train_time:113051ms step_avg:98.31ms
step:1161/1770 train_time:113152ms step_avg:98.31ms
step:1162/1770 train_time:113254ms step_avg:98.31ms
step:1163/1770 train_time:113356ms step_avg:98.31ms
step:1164/1770 train_time:113457ms step_avg:98.32ms
step:1165/1770 train_time:113558ms step_avg:98.32ms
step:1166/1770 train_time:113660ms step_avg:98.32ms
step:1167/1770 train_time:113761ms step_avg:98.32ms
step:1168/1770 train_time:113863ms step_avg:98.33ms
step:1169/1770 train_time:113965ms step_avg:98.33ms
step:1170/1770 train_time:114067ms step_avg:98.33ms
step:1171/1770 train_time:114169ms step_avg:98.34ms
step:1172/1770 train_time:114272ms step_avg:98.34ms
step:1173/1770 train_time:114373ms step_avg:98.34ms
step:1174/1770 train_time:114474ms step_avg:98.35ms
step:1175/1770 train_time:114575ms step_avg:98.35ms
step:1176/1770 train_time:114676ms step_avg:98.35ms
step:1177/1770 train_time:114777ms step_avg:98.35ms
step:1178/1770 train_time:114880ms step_avg:98.36ms
step:1179/1770 train_time:114981ms step_avg:98.36ms
step:1180/1770 train_time:115083ms step_avg:98.36ms
step:1181/1770 train_time:115185ms step_avg:98.36ms
step:1182/1770 train_time:115287ms step_avg:98.37ms
step:1183/1770 train_time:115389ms step_avg:98.37ms
step:1184/1770 train_time:115493ms step_avg:98.38ms
step:1185/1770 train_time:115595ms step_avg:98.38ms
step:1186/1770 train_time:115699ms step_avg:98.38ms
step:1187/1770 train_time:115804ms step_avg:98.39ms
step:1188/1770 train_time:115907ms step_avg:98.39ms
step:1189/1770 train_time:116010ms step_avg:98.40ms
step:1190/1770 train_time:116112ms step_avg:98.40ms
step:1191/1770 train_time:116215ms step_avg:98.40ms
step:1192/1770 train_time:116317ms step_avg:98.41ms
step:1193/1770 train_time:116419ms step_avg:98.41ms
step:1194/1770 train_time:116522ms step_avg:98.41ms
step:1195/1770 train_time:116625ms step_avg:98.42ms
step:1196/1770 train_time:116729ms step_avg:98.42ms
step:1197/1770 train_time:116831ms step_avg:98.43ms
step:1198/1770 train_time:116934ms step_avg:98.43ms
step:1199/1770 train_time:117037ms step_avg:98.43ms
step:1200/1770 train_time:117140ms step_avg:98.44ms
step:1201/1770 train_time:117243ms step_avg:98.44ms
step:1202/1770 train_time:117345ms step_avg:98.44ms
step:1203/1770 train_time:117448ms step_avg:98.45ms
step:1204/1770 train_time:117551ms step_avg:98.45ms
step:1205/1770 train_time:117653ms step_avg:98.45ms
step:1206/1770 train_time:117757ms step_avg:98.46ms
step:1207/1770 train_time:117859ms step_avg:98.46ms
step:1208/1770 train_time:117962ms step_avg:98.47ms
step:1209/1770 train_time:118065ms step_avg:98.47ms
step:1210/1770 train_time:118167ms step_avg:98.47ms
step:1211/1770 train_time:118271ms step_avg:98.48ms
step:1212/1770 train_time:118376ms step_avg:98.48ms
step:1213/1770 train_time:118479ms step_avg:98.49ms
step:1214/1770 train_time:118581ms step_avg:98.49ms
step:1215/1770 train_time:118684ms step_avg:98.49ms
step:1216/1770 train_time:118789ms step_avg:98.50ms
step:1217/1770 train_time:118893ms step_avg:98.50ms
step:1218/1770 train_time:118995ms step_avg:98.51ms
step:1219/1770 train_time:119098ms step_avg:98.51ms
step:1220/1770 train_time:119200ms step_avg:98.51ms
step:1221/1770 train_time:119303ms step_avg:98.52ms
step:1222/1770 train_time:119406ms step_avg:98.52ms
step:1223/1770 train_time:119509ms step_avg:98.52ms
step:1224/1770 train_time:119613ms step_avg:98.53ms
step:1225/1770 train_time:119716ms step_avg:98.53ms
step:1226/1770 train_time:119818ms step_avg:98.53ms
step:1227/1770 train_time:119923ms step_avg:98.54ms
step:1228/1770 train_time:120028ms step_avg:98.55ms
step:1229/1770 train_time:120130ms step_avg:98.55ms
step:1230/1770 train_time:120234ms step_avg:98.55ms
step:1231/1770 train_time:120337ms step_avg:98.56ms
step:1232/1770 train_time:120439ms step_avg:98.56ms
step:1233/1770 train_time:120541ms step_avg:98.56ms
step:1234/1770 train_time:120644ms step_avg:98.57ms
step:1235/1770 train_time:120748ms step_avg:98.57ms
step:1236/1770 train_time:120851ms step_avg:98.57ms
step:1237/1770 train_time:120954ms step_avg:98.58ms
step:1238/1770 train_time:121057ms step_avg:98.58ms
step:1239/1770 train_time:121159ms step_avg:98.58ms
step:1240/1770 train_time:121262ms step_avg:98.59ms
step:1241/1770 train_time:121365ms step_avg:98.59ms
step:1242/1770 train_time:121468ms step_avg:98.59ms
step:1243/1770 train_time:121571ms step_avg:98.60ms
step:1244/1770 train_time:121674ms step_avg:98.60ms
step:1245/1770 train_time:121776ms step_avg:98.60ms
step:1246/1770 train_time:121879ms step_avg:98.61ms
step:1247/1770 train_time:121982ms step_avg:98.61ms
step:1248/1770 train_time:122085ms step_avg:98.61ms
step:1249/1770 train_time:122188ms step_avg:98.62ms
step:1250/1770 train_time:122290ms step_avg:98.62ms
step:1250/1770 val_loss:3.4273 train_time:122392ms step_avg:98.70ms
step:1251/1770 train_time:122415ms step_avg:98.64ms
step:1252/1770 train_time:122508ms step_avg:98.64ms
step:1253/1770 train_time:122612ms step_avg:98.64ms
step:1254/1770 train_time:122714ms step_avg:98.64ms
step:1255/1770 train_time:122819ms step_avg:98.65ms
step:1256/1770 train_time:122921ms step_avg:98.65ms
step:1257/1770 train_time:123023ms step_avg:98.65ms
step:1258/1770 train_time:123127ms step_avg:98.66ms
step:1259/1770 train_time:123231ms step_avg:98.66ms
step:1260/1770 train_time:123333ms step_avg:98.67ms
step:1261/1770 train_time:123436ms step_avg:98.67ms
step:1262/1770 train_time:123540ms step_avg:98.67ms
step:1263/1770 train_time:123642ms step_avg:98.68ms
step:1264/1770 train_time:123746ms step_avg:98.68ms
step:1265/1770 train_time:123849ms step_avg:98.68ms
step:1266/1770 train_time:123952ms step_avg:98.69ms
step:1267/1770 train_time:124055ms step_avg:98.69ms
step:1268/1770 train_time:124158ms step_avg:98.69ms
step:1269/1770 train_time:124260ms step_avg:98.70ms
step:1270/1770 train_time:124363ms step_avg:98.70ms
step:1271/1770 train_time:124466ms step_avg:98.70ms
step:1272/1770 train_time:124570ms step_avg:98.71ms
step:1273/1770 train_time:124673ms step_avg:98.71ms
step:1274/1770 train_time:124775ms step_avg:98.71ms
step:1275/1770 train_time:124877ms step_avg:98.72ms
step:1276/1770 train_time:124981ms step_avg:98.72ms
step:1277/1770 train_time:125083ms step_avg:98.72ms
step:1278/1770 train_time:125187ms step_avg:98.73ms
step:1279/1770 train_time:125291ms step_avg:98.73ms
step:1280/1770 train_time:125395ms step_avg:98.74ms
step:1281/1770 train_time:125497ms step_avg:98.74ms
step:1282/1770 train_time:125601ms step_avg:98.74ms
step:1283/1770 train_time:125704ms step_avg:98.75ms
step:1284/1770 train_time:125808ms step_avg:98.75ms
step:1285/1770 train_time:125910ms step_avg:98.75ms
step:1286/1770 train_time:126013ms step_avg:98.76ms
step:1287/1770 train_time:126118ms step_avg:98.76ms
step:1288/1770 train_time:126221ms step_avg:98.76ms
step:1289/1770 train_time:126324ms step_avg:98.77ms
step:1290/1770 train_time:126426ms step_avg:98.77ms
step:1291/1770 train_time:126529ms step_avg:98.77ms
step:1292/1770 train_time:126633ms step_avg:98.78ms
step:1293/1770 train_time:126735ms step_avg:98.78ms
step:1294/1770 train_time:126837ms step_avg:98.78ms
step:1295/1770 train_time:126940ms step_avg:98.79ms
step:1296/1770 train_time:127043ms step_avg:98.79ms
step:1297/1770 train_time:127145ms step_avg:98.79ms
step:1298/1770 train_time:127248ms step_avg:98.80ms
step:1299/1770 train_time:127350ms step_avg:98.80ms
step:1300/1770 train_time:127452ms step_avg:98.80ms
step:1301/1770 train_time:127556ms step_avg:98.80ms
step:1302/1770 train_time:127659ms step_avg:98.81ms
step:1303/1770 train_time:127761ms step_avg:98.81ms
step:1304/1770 train_time:127864ms step_avg:98.81ms
step:1305/1770 train_time:127967ms step_avg:98.82ms
step:1306/1770 train_time:128070ms step_avg:98.82ms
step:1307/1770 train_time:128173ms step_avg:98.82ms
step:1308/1770 train_time:128275ms step_avg:98.82ms
step:1309/1770 train_time:128377ms step_avg:98.83ms
step:1310/1770 train_time:128480ms step_avg:98.83ms
step:1311/1770 train_time:128582ms step_avg:98.83ms
step:1312/1770 train_time:128685ms step_avg:98.84ms
step:1313/1770 train_time:128787ms step_avg:98.84ms
step:1314/1770 train_time:128890ms step_avg:98.84ms
step:1315/1770 train_time:128993ms step_avg:98.84ms
step:1316/1770 train_time:129095ms step_avg:98.85ms
step:1317/1770 train_time:129198ms step_avg:98.85ms
step:1318/1770 train_time:129304ms step_avg:98.86ms
step:1319/1770 train_time:129407ms step_avg:98.86ms
step:1320/1770 train_time:129511ms step_avg:98.86ms
step:1321/1770 train_time:129613ms step_avg:98.87ms
step:1322/1770 train_time:129715ms step_avg:98.87ms
step:1323/1770 train_time:129819ms step_avg:98.87ms
step:1324/1770 train_time:129922ms step_avg:98.88ms
step:1325/1770 train_time:130025ms step_avg:98.88ms
step:1326/1770 train_time:130128ms step_avg:98.88ms
step:1327/1770 train_time:130234ms step_avg:98.89ms
step:1328/1770 train_time:130336ms step_avg:98.89ms
step:1329/1770 train_time:130439ms step_avg:98.89ms
step:1330/1770 train_time:130541ms step_avg:98.89ms
step:1331/1770 train_time:130643ms step_avg:98.90ms
step:1332/1770 train_time:130746ms step_avg:98.90ms
step:1333/1770 train_time:130849ms step_avg:98.90ms
step:1334/1770 train_time:130951ms step_avg:98.91ms
step:1335/1770 train_time:131054ms step_avg:98.91ms
step:1336/1770 train_time:131156ms step_avg:98.91ms
step:1337/1770 train_time:131260ms step_avg:98.91ms
step:1338/1770 train_time:131362ms step_avg:98.92ms
step:1339/1770 train_time:131466ms step_avg:98.92ms
step:1340/1770 train_time:131571ms step_avg:98.93ms
step:1341/1770 train_time:131674ms step_avg:98.93ms
step:1342/1770 train_time:131778ms step_avg:98.93ms
step:1343/1770 train_time:131882ms step_avg:98.94ms
step:1344/1770 train_time:131985ms step_avg:98.94ms
step:1345/1770 train_time:132088ms step_avg:98.94ms
step:1346/1770 train_time:132191ms step_avg:98.95ms
step:1347/1770 train_time:132294ms step_avg:98.95ms
step:1348/1770 train_time:132399ms step_avg:98.95ms
step:1349/1770 train_time:132502ms step_avg:98.96ms
step:1350/1770 train_time:132606ms step_avg:98.96ms
step:1351/1770 train_time:132709ms step_avg:98.96ms
step:1352/1770 train_time:132812ms step_avg:98.97ms
step:1353/1770 train_time:132916ms step_avg:98.97ms
step:1354/1770 train_time:133018ms step_avg:98.97ms
step:1355/1770 train_time:133121ms step_avg:98.97ms
step:1356/1770 train_time:133223ms step_avg:98.98ms
step:1357/1770 train_time:133326ms step_avg:98.98ms
step:1358/1770 train_time:133429ms step_avg:98.98ms
step:1359/1770 train_time:133532ms step_avg:98.99ms
step:1360/1770 train_time:133636ms step_avg:98.99ms
step:1361/1770 train_time:133740ms step_avg:98.99ms
step:1362/1770 train_time:133842ms step_avg:99.00ms
step:1363/1770 train_time:133947ms step_avg:99.00ms
step:1364/1770 train_time:134051ms step_avg:99.00ms
step:1365/1770 train_time:134153ms step_avg:99.01ms
step:1366/1770 train_time:134255ms step_avg:99.01ms
step:1367/1770 train_time:134358ms step_avg:99.01ms
step:1368/1770 train_time:134461ms step_avg:99.01ms
step:1369/1770 train_time:134564ms step_avg:99.02ms
step:1370/1770 train_time:134668ms step_avg:99.02ms
step:1371/1770 train_time:134770ms step_avg:99.02ms
step:1372/1770 train_time:134872ms step_avg:99.03ms
step:1373/1770 train_time:134976ms step_avg:99.03ms
step:1374/1770 train_time:135079ms step_avg:99.03ms
step:1375/1770 train_time:135182ms step_avg:99.03ms
step:1375/1770 val_loss:3.3856 train_time:135284ms step_avg:99.11ms
step:1376/1770 train_time:135306ms step_avg:99.05ms
step:1377/1770 train_time:135397ms step_avg:99.05ms
step:1378/1770 train_time:135500ms step_avg:99.05ms
step:1379/1770 train_time:135603ms step_avg:99.05ms
step:1380/1770 train_time:135705ms step_avg:99.05ms
step:1381/1770 train_time:135808ms step_avg:99.06ms
step:1382/1770 train_time:135911ms step_avg:99.06ms
step:1383/1770 train_time:136014ms step_avg:99.06ms
step:1384/1770 train_time:136117ms step_avg:99.07ms
step:1385/1770 train_time:136220ms step_avg:99.07ms
step:1386/1770 train_time:136323ms step_avg:99.07ms
step:1387/1770 train_time:136427ms step_avg:99.08ms
step:1388/1770 train_time:136529ms step_avg:99.08ms
step:1389/1770 train_time:136632ms step_avg:99.08ms
step:1390/1770 train_time:136735ms step_avg:99.08ms
step:1391/1770 train_time:136838ms step_avg:99.09ms
step:1392/1770 train_time:136940ms step_avg:99.09ms
step:1393/1770 train_time:137044ms step_avg:99.09ms
step:1394/1770 train_time:137148ms step_avg:99.10ms
step:1395/1770 train_time:137251ms step_avg:99.10ms
step:1396/1770 train_time:137354ms step_avg:99.10ms
step:1397/1770 train_time:137458ms step_avg:99.10ms
step:1398/1770 train_time:137560ms step_avg:99.11ms
step:1399/1770 train_time:137663ms step_avg:99.11ms
step:1400/1770 train_time:137766ms step_avg:99.11ms
step:1401/1770 train_time:137869ms step_avg:99.12ms
step:1402/1770 train_time:137972ms step_avg:99.12ms
step:1403/1770 train_time:138074ms step_avg:99.12ms
step:1404/1770 train_time:138177ms step_avg:99.12ms
step:1405/1770 train_time:138280ms step_avg:99.13ms
step:1406/1770 train_time:138384ms step_avg:99.13ms
step:1407/1770 train_time:138486ms step_avg:99.13ms
step:1408/1770 train_time:138588ms step_avg:99.13ms
step:1409/1770 train_time:138691ms step_avg:99.14ms
step:1410/1770 train_time:138794ms step_avg:99.14ms
step:1411/1770 train_time:138896ms step_avg:99.14ms
step:1412/1770 train_time:138998ms step_avg:99.14ms
step:1413/1770 train_time:139101ms step_avg:99.15ms
step:1414/1770 train_time:139204ms step_avg:99.15ms
step:1415/1770 train_time:139308ms step_avg:99.15ms
step:1416/1770 train_time:139411ms step_avg:99.15ms
step:1417/1770 train_time:139514ms step_avg:99.16ms
step:1418/1770 train_time:139616ms step_avg:99.16ms
step:1419/1770 train_time:139721ms step_avg:99.16ms
step:1420/1770 train_time:139824ms step_avg:99.17ms
step:1421/1770 train_time:139927ms step_avg:99.17ms
step:1422/1770 train_time:140029ms step_avg:99.17ms
step:1423/1770 train_time:140131ms step_avg:99.17ms
step:1424/1770 train_time:140234ms step_avg:99.18ms
step:1425/1770 train_time:140336ms step_avg:99.18ms
step:1426/1770 train_time:140439ms step_avg:99.18ms
step:1427/1770 train_time:140541ms step_avg:99.18ms
step:1428/1770 train_time:140645ms step_avg:99.19ms
step:1429/1770 train_time:140748ms step_avg:99.19ms
step:1430/1770 train_time:140850ms step_avg:99.19ms
step:1431/1770 train_time:140954ms step_avg:99.19ms
step:1432/1770 train_time:141056ms step_avg:99.20ms
step:1433/1770 train_time:141158ms step_avg:99.20ms
step:1434/1770 train_time:141260ms step_avg:99.20ms
step:1435/1770 train_time:141363ms step_avg:99.20ms
step:1436/1770 train_time:141467ms step_avg:99.21ms
step:1437/1770 train_time:141570ms step_avg:99.21ms
step:1438/1770 train_time:141672ms step_avg:99.21ms
step:1439/1770 train_time:141775ms step_avg:99.21ms
step:1440/1770 train_time:141877ms step_avg:99.21ms
step:1441/1770 train_time:141983ms step_avg:99.22ms
step:1442/1770 train_time:142085ms step_avg:99.22ms
step:1443/1770 train_time:142189ms step_avg:99.22ms
step:1444/1770 train_time:142292ms step_avg:99.23ms
step:1445/1770 train_time:142396ms step_avg:99.23ms
step:1446/1770 train_time:142500ms step_avg:99.23ms
step:1447/1770 train_time:142604ms step_avg:99.24ms
step:1448/1770 train_time:142707ms step_avg:99.24ms
step:1449/1770 train_time:142812ms step_avg:99.24ms
step:1450/1770 train_time:142916ms step_avg:99.25ms
step:1451/1770 train_time:143020ms step_avg:99.25ms
step:1452/1770 train_time:143125ms step_avg:99.25ms
step:1453/1770 train_time:143229ms step_avg:99.26ms
step:1454/1770 train_time:143332ms step_avg:99.26ms
step:1455/1770 train_time:143437ms step_avg:99.26ms
step:1456/1770 train_time:143542ms step_avg:99.27ms
step:1457/1770 train_time:143646ms step_avg:99.27ms
step:1458/1770 train_time:143751ms step_avg:99.28ms
step:1459/1770 train_time:143856ms step_avg:99.28ms
step:1460/1770 train_time:143960ms step_avg:99.28ms
step:1461/1770 train_time:144064ms step_avg:99.29ms
step:1462/1770 train_time:144168ms step_avg:99.29ms
step:1463/1770 train_time:144272ms step_avg:99.29ms
step:1464/1770 train_time:144377ms step_avg:99.30ms
step:1465/1770 train_time:144481ms step_avg:99.30ms
step:1466/1770 train_time:144585ms step_avg:99.30ms
step:1467/1770 train_time:144690ms step_avg:99.31ms
step:1468/1770 train_time:144795ms step_avg:99.31ms
step:1469/1770 train_time:144898ms step_avg:99.31ms
step:1470/1770 train_time:145002ms step_avg:99.32ms
step:1471/1770 train_time:145106ms step_avg:99.32ms
step:1472/1770 train_time:145209ms step_avg:99.32ms
step:1473/1770 train_time:145315ms step_avg:99.33ms
step:1474/1770 train_time:145419ms step_avg:99.33ms
step:1475/1770 train_time:145523ms step_avg:99.33ms
step:1476/1770 train_time:145627ms step_avg:99.34ms
step:1477/1770 train_time:145733ms step_avg:99.34ms
step:1478/1770 train_time:145838ms step_avg:99.34ms
step:1479/1770 train_time:145941ms step_avg:99.35ms
step:1480/1770 train_time:146045ms step_avg:99.35ms
step:1481/1770 train_time:146153ms step_avg:99.36ms
step:1482/1770 train_time:146256ms step_avg:99.36ms
step:1483/1770 train_time:146361ms step_avg:99.36ms
step:1484/1770 train_time:146464ms step_avg:99.36ms
step:1485/1770 train_time:146568ms step_avg:99.37ms
step:1486/1770 train_time:146672ms step_avg:99.37ms
step:1487/1770 train_time:146775ms step_avg:99.37ms
step:1488/1770 train_time:146879ms step_avg:99.38ms
step:1489/1770 train_time:146984ms step_avg:99.38ms
step:1490/1770 train_time:147088ms step_avg:99.38ms
step:1491/1770 train_time:147192ms step_avg:99.39ms
step:1492/1770 train_time:147299ms step_avg:99.39ms
step:1493/1770 train_time:147404ms step_avg:99.40ms
step:1494/1770 train_time:147511ms step_avg:99.40ms
step:1495/1770 train_time:147614ms step_avg:99.40ms
step:1496/1770 train_time:147717ms step_avg:99.41ms
step:1497/1770 train_time:147822ms step_avg:99.41ms
step:1498/1770 train_time:147926ms step_avg:99.41ms
step:1499/1770 train_time:148029ms step_avg:99.42ms
step:1500/1770 train_time:148133ms step_avg:99.42ms
step:1500/1770 val_loss:3.3491 train_time:148234ms step_avg:99.49ms
step:1501/1770 train_time:148257ms step_avg:99.43ms
step:1502/1770 train_time:148346ms step_avg:99.43ms
step:1503/1770 train_time:148450ms step_avg:99.43ms
step:1504/1770 train_time:148553ms step_avg:99.43ms
step:1505/1770 train_time:148660ms step_avg:99.44ms
step:1506/1770 train_time:148764ms step_avg:99.44ms
step:1507/1770 train_time:148868ms step_avg:99.44ms
step:1508/1770 train_time:148974ms step_avg:99.45ms
step:1509/1770 train_time:149078ms step_avg:99.45ms
step:1510/1770 train_time:149181ms step_avg:99.45ms
step:1511/1770 train_time:149285ms step_avg:99.46ms
step:1512/1770 train_time:149389ms step_avg:99.46ms
step:1513/1770 train_time:149494ms step_avg:99.46ms
step:1514/1770 train_time:149598ms step_avg:99.47ms
step:1515/1770 train_time:149702ms step_avg:99.47ms
step:1516/1770 train_time:149806ms step_avg:99.47ms
step:1517/1770 train_time:149910ms step_avg:99.48ms
step:1518/1770 train_time:150016ms step_avg:99.48ms
step:1519/1770 train_time:150120ms step_avg:99.48ms
step:1520/1770 train_time:150224ms step_avg:99.49ms
step:1521/1770 train_time:150328ms step_avg:99.49ms
step:1522/1770 train_time:150432ms step_avg:99.49ms
step:1523/1770 train_time:150537ms step_avg:99.50ms
step:1524/1770 train_time:150640ms step_avg:99.50ms
step:1525/1770 train_time:150744ms step_avg:99.50ms
step:1526/1770 train_time:150848ms step_avg:99.50ms
step:1527/1770 train_time:150952ms step_avg:99.51ms
step:1528/1770 train_time:151058ms step_avg:99.51ms
step:1529/1770 train_time:151162ms step_avg:99.51ms
step:1530/1770 train_time:151265ms step_avg:99.52ms
step:1531/1770 train_time:151369ms step_avg:99.52ms
step:1532/1770 train_time:151473ms step_avg:99.52ms
step:1533/1770 train_time:151578ms step_avg:99.53ms
step:1534/1770 train_time:151682ms step_avg:99.53ms
step:1535/1770 train_time:151785ms step_avg:99.53ms
step:1536/1770 train_time:151889ms step_avg:99.53ms
step:1537/1770 train_time:151993ms step_avg:99.54ms
step:1538/1770 train_time:152098ms step_avg:99.54ms
step:1539/1770 train_time:152202ms step_avg:99.54ms
step:1540/1770 train_time:152309ms step_avg:99.55ms
step:1541/1770 train_time:152414ms step_avg:99.55ms
step:1542/1770 train_time:152519ms step_avg:99.56ms
step:1543/1770 train_time:152622ms step_avg:99.56ms
step:1544/1770 train_time:152728ms step_avg:99.56ms
step:1545/1770 train_time:152832ms step_avg:99.56ms
step:1546/1770 train_time:152936ms step_avg:99.57ms
step:1547/1770 train_time:153040ms step_avg:99.57ms
step:1548/1770 train_time:153144ms step_avg:99.57ms
step:1549/1770 train_time:153248ms step_avg:99.58ms
step:1550/1770 train_time:153352ms step_avg:99.58ms
step:1551/1770 train_time:153456ms step_avg:99.58ms
step:1552/1770 train_time:153562ms step_avg:99.59ms
step:1553/1770 train_time:153666ms step_avg:99.59ms
step:1554/1770 train_time:153769ms step_avg:99.59ms
step:1555/1770 train_time:153873ms step_avg:99.59ms
step:1556/1770 train_time:153976ms step_avg:99.60ms
step:1557/1770 train_time:154080ms step_avg:99.60ms
step:1558/1770 train_time:154184ms step_avg:99.60ms
step:1559/1770 train_time:154288ms step_avg:99.60ms
step:1560/1770 train_time:154391ms step_avg:99.61ms
step:1561/1770 train_time:154497ms step_avg:99.61ms
step:1562/1770 train_time:154601ms step_avg:99.61ms
step:1563/1770 train_time:154705ms step_avg:99.62ms
step:1564/1770 train_time:154808ms step_avg:99.62ms
step:1565/1770 train_time:154912ms step_avg:99.62ms
step:1566/1770 train_time:155016ms step_avg:99.62ms
step:1567/1770 train_time:155121ms step_avg:99.63ms
step:1568/1770 train_time:155225ms step_avg:99.63ms
step:1569/1770 train_time:155332ms step_avg:99.64ms
step:1570/1770 train_time:155436ms step_avg:99.64ms
step:1571/1770 train_time:155540ms step_avg:99.64ms
step:1572/1770 train_time:155645ms step_avg:99.64ms
step:1573/1770 train_time:155750ms step_avg:99.65ms
step:1574/1770 train_time:155855ms step_avg:99.65ms
step:1575/1770 train_time:155958ms step_avg:99.65ms
step:1576/1770 train_time:156061ms step_avg:99.66ms
step:1577/1770 train_time:156167ms step_avg:99.66ms
step:1578/1770 train_time:156272ms step_avg:99.66ms
step:1579/1770 train_time:156376ms step_avg:99.67ms
step:1580/1770 train_time:156480ms step_avg:99.67ms
step:1581/1770 train_time:156588ms step_avg:99.67ms
step:1582/1770 train_time:156692ms step_avg:99.68ms
step:1583/1770 train_time:156796ms step_avg:99.68ms
step:1584/1770 train_time:156901ms step_avg:99.68ms
step:1585/1770 train_time:157004ms step_avg:99.69ms
step:1586/1770 train_time:157112ms step_avg:99.69ms
step:1587/1770 train_time:157218ms step_avg:99.69ms
step:1588/1770 train_time:157321ms step_avg:99.70ms
step:1589/1770 train_time:157427ms step_avg:99.70ms
step:1590/1770 train_time:157531ms step_avg:99.70ms
step:1591/1770 train_time:157635ms step_avg:99.71ms
step:1592/1770 train_time:157739ms step_avg:99.71ms
step:1593/1770 train_time:157843ms step_avg:99.71ms
step:1594/1770 train_time:157947ms step_avg:99.71ms
step:1595/1770 train_time:158050ms step_avg:99.72ms
step:1596/1770 train_time:158156ms step_avg:99.72ms
step:1597/1770 train_time:158260ms step_avg:99.72ms
step:1598/1770 train_time:158364ms step_avg:99.73ms
step:1599/1770 train_time:158470ms step_avg:99.73ms
step:1600/1770 train_time:158577ms step_avg:99.73ms
step:1601/1770 train_time:158681ms step_avg:99.74ms
step:1602/1770 train_time:158786ms step_avg:99.74ms
step:1603/1770 train_time:158890ms step_avg:99.74ms
step:1604/1770 train_time:158993ms step_avg:99.74ms
step:1605/1770 train_time:159096ms step_avg:99.75ms
step:1606/1770 train_time:159200ms step_avg:99.75ms
step:1607/1770 train_time:159308ms step_avg:99.75ms
step:1608/1770 train_time:159411ms step_avg:99.76ms
step:1609/1770 train_time:159515ms step_avg:99.76ms
step:1610/1770 train_time:159621ms step_avg:99.76ms
step:1611/1770 train_time:159727ms step_avg:99.77ms
step:1612/1770 train_time:159832ms step_avg:99.77ms
step:1613/1770 train_time:159936ms step_avg:99.77ms
step:1614/1770 train_time:160040ms step_avg:99.78ms
step:1615/1770 train_time:160145ms step_avg:99.78ms
step:1616/1770 train_time:160249ms step_avg:99.78ms
step:1617/1770 train_time:160355ms step_avg:99.79ms
step:1618/1770 train_time:160461ms step_avg:99.79ms
step:1619/1770 train_time:160566ms step_avg:99.79ms
step:1620/1770 train_time:160670ms step_avg:99.79ms
step:1621/1770 train_time:160775ms step_avg:99.80ms
step:1622/1770 train_time:160880ms step_avg:99.80ms
step:1623/1770 train_time:160987ms step_avg:99.81ms
step:1624/1770 train_time:161091ms step_avg:99.81ms
step:1625/1770 train_time:161194ms step_avg:99.81ms
step:1625/1770 val_loss:3.3162 train_time:161296ms step_avg:99.87ms
step:1626/1770 train_time:161318ms step_avg:99.83ms
step:1627/1770 train_time:161409ms step_avg:99.82ms
step:1628/1770 train_time:161513ms step_avg:99.82ms
step:1629/1770 train_time:161616ms step_avg:99.82ms
step:1630/1770 train_time:161721ms step_avg:99.83ms
step:1631/1770 train_time:161825ms step_avg:99.83ms
step:1632/1770 train_time:161929ms step_avg:99.83ms
step:1633/1770 train_time:162033ms step_avg:99.84ms
step:1634/1770 train_time:162137ms step_avg:99.84ms
step:1635/1770 train_time:162241ms step_avg:99.84ms
step:1636/1770 train_time:162346ms step_avg:99.84ms
step:1637/1770 train_time:162450ms step_avg:99.85ms
step:1638/1770 train_time:162554ms step_avg:99.85ms
step:1639/1770 train_time:162658ms step_avg:99.85ms
step:1640/1770 train_time:162763ms step_avg:99.85ms
step:1641/1770 train_time:162866ms step_avg:99.86ms
step:1642/1770 train_time:162970ms step_avg:99.86ms
step:1643/1770 train_time:163073ms step_avg:99.86ms
step:1644/1770 train_time:163179ms step_avg:99.86ms
step:1645/1770 train_time:163283ms step_avg:99.87ms
step:1646/1770 train_time:163389ms step_avg:99.87ms
step:1647/1770 train_time:163494ms step_avg:99.87ms
step:1648/1770 train_time:163597ms step_avg:99.88ms
step:1649/1770 train_time:163701ms step_avg:99.88ms
step:1650/1770 train_time:163806ms step_avg:99.88ms
step:1651/1770 train_time:163910ms step_avg:99.88ms
step:1652/1770 train_time:164014ms step_avg:99.89ms
step:1653/1770 train_time:164118ms step_avg:99.89ms
step:1654/1770 train_time:164226ms step_avg:99.89ms
step:1655/1770 train_time:164332ms step_avg:99.90ms
step:1656/1770 train_time:164436ms step_avg:99.90ms
step:1657/1770 train_time:164543ms step_avg:99.90ms
step:1658/1770 train_time:164647ms step_avg:99.91ms
step:1659/1770 train_time:164752ms step_avg:99.91ms
step:1660/1770 train_time:164856ms step_avg:99.91ms
step:1661/1770 train_time:164962ms step_avg:99.92ms
step:1662/1770 train_time:165066ms step_avg:99.92ms
step:1663/1770 train_time:165169ms step_avg:99.92ms
step:1664/1770 train_time:165273ms step_avg:99.92ms
step:1665/1770 train_time:165376ms step_avg:99.93ms
step:1666/1770 train_time:165480ms step_avg:99.93ms
step:1667/1770 train_time:165584ms step_avg:99.93ms
step:1668/1770 train_time:165688ms step_avg:99.93ms
step:1669/1770 train_time:165791ms step_avg:99.93ms
step:1670/1770 train_time:165894ms step_avg:99.94ms
step:1671/1770 train_time:165998ms step_avg:99.94ms
step:1672/1770 train_time:166103ms step_avg:99.94ms
step:1673/1770 train_time:166209ms step_avg:99.95ms
step:1674/1770 train_time:166313ms step_avg:99.95ms
step:1675/1770 train_time:166416ms step_avg:99.95ms
step:1676/1770 train_time:166522ms step_avg:99.95ms
step:1677/1770 train_time:166631ms step_avg:99.96ms
step:1678/1770 train_time:166734ms step_avg:99.96ms
step:1679/1770 train_time:166838ms step_avg:99.96ms
step:1680/1770 train_time:166942ms step_avg:99.97ms
step:1681/1770 train_time:167047ms step_avg:99.97ms
step:1682/1770 train_time:167153ms step_avg:99.97ms
step:1683/1770 train_time:167257ms step_avg:99.97ms
step:1684/1770 train_time:167361ms step_avg:99.98ms
step:1685/1770 train_time:167465ms step_avg:99.98ms
step:1686/1770 train_time:167569ms step_avg:99.98ms
step:1687/1770 train_time:167676ms step_avg:99.99ms
step:1688/1770 train_time:167779ms step_avg:99.99ms
step:1689/1770 train_time:167884ms step_avg:99.99ms
step:1690/1770 train_time:167987ms step_avg:99.99ms
step:1691/1770 train_time:168092ms step_avg:100.00ms
step:1692/1770 train_time:168196ms step_avg:100.00ms
step:1693/1770 train_time:168301ms step_avg:100.00ms
step:1694/1770 train_time:168405ms step_avg:100.00ms
step:1695/1770 train_time:168509ms step_avg:100.01ms
step:1696/1770 train_time:168615ms step_avg:100.01ms
step:1697/1770 train_time:168721ms step_avg:100.01ms
step:1698/1770 train_time:168825ms step_avg:100.02ms
step:1699/1770 train_time:168929ms step_avg:100.02ms
step:1700/1770 train_time:169033ms step_avg:100.02ms
step:1701/1770 train_time:169137ms step_avg:100.02ms
step:1702/1770 train_time:169242ms step_avg:100.02ms
step:1703/1770 train_time:169346ms step_avg:100.03ms
step:1704/1770 train_time:169450ms step_avg:100.03ms
step:1705/1770 train_time:169553ms step_avg:100.03ms
step:1706/1770 train_time:169656ms step_avg:100.03ms
step:1707/1770 train_time:169762ms step_avg:100.04ms
step:1708/1770 train_time:169867ms step_avg:100.04ms
step:1709/1770 train_time:169973ms step_avg:100.04ms
step:1710/1770 train_time:170080ms step_avg:100.05ms
step:1711/1770 train_time:170187ms step_avg:100.05ms
step:1712/1770 train_time:170292ms step_avg:100.05ms
step:1713/1770 train_time:170396ms step_avg:100.06ms
step:1714/1770 train_time:170501ms step_avg:100.06ms
step:1715/1770 train_time:170605ms step_avg:100.06ms
step:1716/1770 train_time:170710ms step_avg:100.06ms
step:1717/1770 train_time:170814ms step_avg:100.07ms
step:1718/1770 train_time:170921ms step_avg:100.07ms
step:1719/1770 train_time:171026ms step_avg:100.07ms
step:1720/1770 train_time:171133ms step_avg:100.08ms
step:1721/1770 train_time:171237ms step_avg:100.08ms
step:1722/1770 train_time:171344ms step_avg:100.08ms
step:1723/1770 train_time:171451ms step_avg:100.09ms
step:1724/1770 train_time:171558ms step_avg:100.09ms
step:1725/1770 train_time:171665ms step_avg:100.10ms
step:1726/1770 train_time:171772ms step_avg:100.10ms
step:1727/1770 train_time:171876ms step_avg:100.10ms
step:1728/1770 train_time:171984ms step_avg:100.11ms
step:1729/1770 train_time:172087ms step_avg:100.11ms
step:1730/1770 train_time:172194ms step_avg:100.11ms
step:1731/1770 train_time:172300ms step_avg:100.12ms
step:1732/1770 train_time:172404ms step_avg:100.12ms
step:1733/1770 train_time:172511ms step_avg:100.12ms
step:1734/1770 train_time:172615ms step_avg:100.12ms
step:1735/1770 train_time:172721ms step_avg:100.13ms
step:1736/1770 train_time:172825ms step_avg:100.13ms
step:1737/1770 train_time:172930ms step_avg:100.13ms
step:1738/1770 train_time:173035ms step_avg:100.14ms
step:1739/1770 train_time:173140ms step_avg:100.14ms
step:1740/1770 train_time:173246ms step_avg:100.14ms
step:1741/1770 train_time:173353ms step_avg:100.15ms
step:1742/1770 train_time:173460ms step_avg:100.15ms
step:1743/1770 train_time:173567ms step_avg:100.15ms
step:1744/1770 train_time:173672ms step_avg:100.16ms
step:1745/1770 train_time:173777ms step_avg:100.16ms
step:1746/1770 train_time:173884ms step_avg:100.16ms
step:1747/1770 train_time:173987ms step_avg:100.17ms
step:1748/1770 train_time:174094ms step_avg:100.17ms
step:1749/1770 train_time:174201ms step_avg:100.17ms
step:1750/1770 train_time:174306ms step_avg:100.18ms
step:1750/1770 val_loss:3.2915 train_time:174409ms step_avg:100.24ms
step:1751/1770 train_time:174432ms step_avg:100.19ms
step:1752/1770 train_time:174524ms step_avg:100.19ms
step:1753/1770 train_time:174629ms step_avg:100.19ms
step:1754/1770 train_time:174735ms step_avg:100.19ms
step:1755/1770 train_time:174839ms step_avg:100.19ms
step:1756/1770 train_time:174944ms step_avg:100.20ms
step:1757/1770 train_time:175050ms step_avg:100.20ms
step:1758/1770 train_time:175154ms step_avg:100.20ms
step:1759/1770 train_time:175260ms step_avg:100.21ms
step:1760/1770 train_time:175365ms step_avg:100.21ms
step:1761/1770 train_time:175472ms step_avg:100.21ms
step:1762/1770 train_time:175581ms step_avg:100.22ms
step:1763/1770 train_time:175685ms step_avg:100.22ms
step:1764/1770 train_time:175790ms step_avg:100.22ms
step:1765/1770 train_time:175895ms step_avg:100.22ms
step:1766/1770 train_time:176005ms step_avg:100.23ms
step:1767/1770 train_time:176108ms step_avg:100.23ms
step:1768/1770 train_time:176214ms step_avg:100.24ms
step:1769/1770 train_time:176318ms step_avg:100.24ms
step:1770/1770 train_time:176423ms step_avg:100.24ms
step:1770/1770 val_loss:3.2879 train_time:176528ms step_avg:100.30ms
peak memory allocated: 28840 MiB reserved: 32192 MiB
