import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:46:03 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   32C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24516ms step_avg:nanms
step:2/1770 train_time:24963ms step_avg:nanms
step:3/1770 train_time:25060ms step_avg:nanms
step:4/1770 train_time:25153ms step_avg:nanms
step:5/1770 train_time:25247ms step_avg:nanms
step:6/1770 train_time:25341ms step_avg:nanms
step:7/1770 train_time:25435ms step_avg:nanms
step:8/1770 train_time:25530ms step_avg:nanms
step:9/1770 train_time:25623ms step_avg:nanms
step:10/1770 train_time:25718ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.15ms
step:14/1770 train_time:377ms step_avg:94.35ms
step:15/1770 train_time:473ms step_avg:94.53ms
step:16/1770 train_time:567ms step_avg:94.55ms
step:17/1770 train_time:662ms step_avg:94.51ms
step:18/1770 train_time:756ms step_avg:94.52ms
step:19/1770 train_time:851ms step_avg:94.54ms
step:20/1770 train_time:946ms step_avg:94.60ms
step:21/1770 train_time:1040ms step_avg:94.57ms
step:22/1770 train_time:1135ms step_avg:94.57ms
step:23/1770 train_time:1229ms step_avg:94.55ms
step:24/1770 train_time:1323ms step_avg:94.53ms
step:25/1770 train_time:1418ms step_avg:94.55ms
step:26/1770 train_time:1512ms step_avg:94.53ms
step:27/1770 train_time:1606ms step_avg:94.50ms
step:28/1770 train_time:1701ms step_avg:94.51ms
step:29/1770 train_time:1796ms step_avg:94.53ms
step:30/1770 train_time:1891ms step_avg:94.54ms
step:31/1770 train_time:1985ms step_avg:94.53ms
step:32/1770 train_time:2080ms step_avg:94.56ms
step:33/1770 train_time:2175ms step_avg:94.57ms
step:34/1770 train_time:2269ms step_avg:94.55ms
step:35/1770 train_time:2363ms step_avg:94.54ms
step:36/1770 train_time:2459ms step_avg:94.57ms
step:37/1770 train_time:2553ms step_avg:94.56ms
step:38/1770 train_time:2648ms step_avg:94.56ms
step:39/1770 train_time:2742ms step_avg:94.56ms
step:40/1770 train_time:2837ms step_avg:94.57ms
step:41/1770 train_time:2931ms step_avg:94.56ms
step:42/1770 train_time:3026ms step_avg:94.55ms
step:43/1770 train_time:3120ms step_avg:94.55ms
step:44/1770 train_time:3215ms step_avg:94.55ms
step:45/1770 train_time:3310ms step_avg:94.56ms
step:46/1770 train_time:3404ms step_avg:94.57ms
step:47/1770 train_time:3499ms step_avg:94.57ms
step:48/1770 train_time:3594ms step_avg:94.58ms
step:49/1770 train_time:3689ms step_avg:94.58ms
step:50/1770 train_time:3784ms step_avg:94.60ms
step:51/1770 train_time:3878ms step_avg:94.60ms
step:52/1770 train_time:3973ms step_avg:94.60ms
step:53/1770 train_time:4067ms step_avg:94.59ms
step:54/1770 train_time:4162ms step_avg:94.59ms
step:55/1770 train_time:4256ms step_avg:94.59ms
step:56/1770 train_time:4351ms step_avg:94.58ms
step:57/1770 train_time:4445ms step_avg:94.57ms
step:58/1770 train_time:4539ms step_avg:94.57ms
step:59/1770 train_time:4634ms step_avg:94.57ms
step:60/1770 train_time:4728ms step_avg:94.56ms
step:61/1770 train_time:4823ms step_avg:94.58ms
step:62/1770 train_time:4918ms step_avg:94.58ms
step:63/1770 train_time:5013ms step_avg:94.59ms
step:64/1770 train_time:5108ms step_avg:94.59ms
step:65/1770 train_time:5202ms step_avg:94.59ms
step:66/1770 train_time:5297ms step_avg:94.59ms
step:67/1770 train_time:5392ms step_avg:94.59ms
step:68/1770 train_time:5486ms step_avg:94.58ms
step:69/1770 train_time:5580ms step_avg:94.58ms
step:70/1770 train_time:5675ms step_avg:94.58ms
step:71/1770 train_time:5769ms step_avg:94.58ms
step:72/1770 train_time:5864ms step_avg:94.58ms
step:73/1770 train_time:5959ms step_avg:94.58ms
step:74/1770 train_time:6053ms step_avg:94.58ms
step:75/1770 train_time:6147ms step_avg:94.57ms
step:76/1770 train_time:6242ms step_avg:94.58ms
step:77/1770 train_time:6337ms step_avg:94.58ms
step:78/1770 train_time:6431ms step_avg:94.57ms
step:79/1770 train_time:6526ms step_avg:94.57ms
step:80/1770 train_time:6620ms step_avg:94.57ms
step:81/1770 train_time:6715ms step_avg:94.57ms
step:82/1770 train_time:6809ms step_avg:94.57ms
step:83/1770 train_time:6903ms step_avg:94.56ms
step:84/1770 train_time:6998ms step_avg:94.57ms
step:85/1770 train_time:7092ms step_avg:94.57ms
step:86/1770 train_time:7187ms step_avg:94.56ms
step:87/1770 train_time:7281ms step_avg:94.56ms
step:88/1770 train_time:7376ms step_avg:94.57ms
step:89/1770 train_time:7471ms step_avg:94.56ms
step:90/1770 train_time:7565ms step_avg:94.56ms
step:91/1770 train_time:7660ms step_avg:94.56ms
step:92/1770 train_time:7754ms step_avg:94.56ms
step:93/1770 train_time:7848ms step_avg:94.56ms
step:94/1770 train_time:7943ms step_avg:94.56ms
step:95/1770 train_time:8037ms step_avg:94.56ms
step:96/1770 train_time:8132ms step_avg:94.56ms
step:97/1770 train_time:8226ms step_avg:94.55ms
step:98/1770 train_time:8320ms step_avg:94.55ms
step:99/1770 train_time:8415ms step_avg:94.55ms
step:100/1770 train_time:8509ms step_avg:94.55ms
step:101/1770 train_time:8604ms step_avg:94.55ms
step:102/1770 train_time:8698ms step_avg:94.55ms
step:103/1770 train_time:8793ms step_avg:94.55ms
step:104/1770 train_time:8887ms step_avg:94.54ms
step:105/1770 train_time:8981ms step_avg:94.54ms
step:106/1770 train_time:9076ms step_avg:94.54ms
step:107/1770 train_time:9170ms step_avg:94.54ms
step:108/1770 train_time:9265ms step_avg:94.54ms
step:109/1770 train_time:9359ms step_avg:94.54ms
step:110/1770 train_time:9454ms step_avg:94.54ms
step:111/1770 train_time:9548ms step_avg:94.54ms
step:112/1770 train_time:9643ms step_avg:94.54ms
step:113/1770 train_time:9737ms step_avg:94.54ms
step:114/1770 train_time:9832ms step_avg:94.54ms
step:115/1770 train_time:9926ms step_avg:94.53ms
step:116/1770 train_time:10021ms step_avg:94.53ms
step:117/1770 train_time:10115ms step_avg:94.53ms
step:118/1770 train_time:10209ms step_avg:94.53ms
step:119/1770 train_time:10304ms step_avg:94.53ms
step:120/1770 train_time:10398ms step_avg:94.53ms
step:121/1770 train_time:10492ms step_avg:94.53ms
step:122/1770 train_time:10587ms step_avg:94.53ms
step:123/1770 train_time:10681ms step_avg:94.53ms
step:124/1770 train_time:10776ms step_avg:94.53ms
step:125/1770 train_time:10871ms step_avg:94.53ms
step:125/1770 val_loss:4.6512 train_time:10964ms step_avg:95.34ms
step:126/1770 train_time:10986ms step_avg:94.71ms
step:127/1770 train_time:11068ms step_avg:94.60ms
step:128/1770 train_time:11171ms step_avg:94.67ms
step:129/1770 train_time:11267ms step_avg:94.68ms
step:130/1770 train_time:11362ms step_avg:94.68ms
step:131/1770 train_time:11457ms step_avg:94.68ms
step:132/1770 train_time:11551ms step_avg:94.68ms
step:133/1770 train_time:11645ms step_avg:94.68ms
step:134/1770 train_time:11741ms step_avg:94.68ms
step:135/1770 train_time:11836ms step_avg:94.69ms
step:136/1770 train_time:11930ms step_avg:94.68ms
step:137/1770 train_time:12025ms step_avg:94.69ms
step:138/1770 train_time:12120ms step_avg:94.69ms
step:139/1770 train_time:12216ms step_avg:94.69ms
step:140/1770 train_time:12311ms step_avg:94.70ms
step:141/1770 train_time:12406ms step_avg:94.70ms
step:142/1770 train_time:12501ms step_avg:94.71ms
step:143/1770 train_time:12596ms step_avg:94.71ms
step:144/1770 train_time:12692ms step_avg:94.71ms
step:145/1770 train_time:12786ms step_avg:94.71ms
step:146/1770 train_time:12882ms step_avg:94.72ms
step:147/1770 train_time:12977ms step_avg:94.72ms
step:148/1770 train_time:13073ms step_avg:94.73ms
step:149/1770 train_time:13168ms step_avg:94.73ms
step:150/1770 train_time:13263ms step_avg:94.74ms
step:151/1770 train_time:13358ms step_avg:94.74ms
step:152/1770 train_time:13453ms step_avg:94.74ms
step:153/1770 train_time:13548ms step_avg:94.74ms
step:154/1770 train_time:13643ms step_avg:94.75ms
step:155/1770 train_time:13739ms step_avg:94.75ms
step:156/1770 train_time:13834ms step_avg:94.75ms
step:157/1770 train_time:13929ms step_avg:94.75ms
step:158/1770 train_time:14024ms step_avg:94.75ms
step:159/1770 train_time:14119ms step_avg:94.76ms
step:160/1770 train_time:14214ms step_avg:94.76ms
step:161/1770 train_time:14308ms step_avg:94.76ms
step:162/1770 train_time:14404ms step_avg:94.76ms
step:163/1770 train_time:14500ms step_avg:94.77ms
step:164/1770 train_time:14595ms step_avg:94.78ms
step:165/1770 train_time:14690ms step_avg:94.77ms
step:166/1770 train_time:14785ms step_avg:94.78ms
step:167/1770 train_time:14881ms step_avg:94.78ms
step:168/1770 train_time:14975ms step_avg:94.78ms
step:169/1770 train_time:15070ms step_avg:94.78ms
step:170/1770 train_time:15165ms step_avg:94.78ms
step:171/1770 train_time:15261ms step_avg:94.79ms
step:172/1770 train_time:15355ms step_avg:94.79ms
step:173/1770 train_time:15451ms step_avg:94.79ms
step:174/1770 train_time:15546ms step_avg:94.79ms
step:175/1770 train_time:15642ms step_avg:94.80ms
step:176/1770 train_time:15738ms step_avg:94.81ms
step:177/1770 train_time:15833ms step_avg:94.81ms
step:178/1770 train_time:15928ms step_avg:94.81ms
step:179/1770 train_time:16024ms step_avg:94.81ms
step:180/1770 train_time:16119ms step_avg:94.82ms
step:181/1770 train_time:16213ms step_avg:94.82ms
step:182/1770 train_time:16309ms step_avg:94.82ms
step:183/1770 train_time:16404ms step_avg:94.82ms
step:184/1770 train_time:16499ms step_avg:94.82ms
step:185/1770 train_time:16595ms step_avg:94.83ms
step:186/1770 train_time:16690ms step_avg:94.83ms
step:187/1770 train_time:16785ms step_avg:94.83ms
step:188/1770 train_time:16881ms step_avg:94.84ms
step:189/1770 train_time:16976ms step_avg:94.84ms
step:190/1770 train_time:17071ms step_avg:94.84ms
step:191/1770 train_time:17166ms step_avg:94.84ms
step:192/1770 train_time:17262ms step_avg:94.84ms
step:193/1770 train_time:17357ms step_avg:94.85ms
step:194/1770 train_time:17452ms step_avg:94.85ms
step:195/1770 train_time:17547ms step_avg:94.85ms
step:196/1770 train_time:17642ms step_avg:94.85ms
step:197/1770 train_time:17738ms step_avg:94.85ms
step:198/1770 train_time:17832ms step_avg:94.85ms
step:199/1770 train_time:17928ms step_avg:94.86ms
step:200/1770 train_time:18023ms step_avg:94.86ms
step:201/1770 train_time:18119ms step_avg:94.86ms
step:202/1770 train_time:18214ms step_avg:94.86ms
step:203/1770 train_time:18309ms step_avg:94.87ms
step:204/1770 train_time:18404ms step_avg:94.87ms
step:205/1770 train_time:18499ms step_avg:94.87ms
step:206/1770 train_time:18594ms step_avg:94.87ms
step:207/1770 train_time:18689ms step_avg:94.87ms
step:208/1770 train_time:18784ms step_avg:94.87ms
step:209/1770 train_time:18880ms step_avg:94.87ms
step:210/1770 train_time:18975ms step_avg:94.87ms
step:211/1770 train_time:19070ms step_avg:94.88ms
step:212/1770 train_time:19165ms step_avg:94.88ms
step:213/1770 train_time:19261ms step_avg:94.88ms
step:214/1770 train_time:19357ms step_avg:94.89ms
step:215/1770 train_time:19453ms step_avg:94.89ms
step:216/1770 train_time:19548ms step_avg:94.89ms
step:217/1770 train_time:19644ms step_avg:94.90ms
step:218/1770 train_time:19740ms step_avg:94.90ms
step:219/1770 train_time:19834ms step_avg:94.90ms
step:220/1770 train_time:19929ms step_avg:94.90ms
step:221/1770 train_time:20025ms step_avg:94.90ms
step:222/1770 train_time:20120ms step_avg:94.91ms
step:223/1770 train_time:20216ms step_avg:94.91ms
step:224/1770 train_time:20310ms step_avg:94.91ms
step:225/1770 train_time:20406ms step_avg:94.91ms
step:226/1770 train_time:20502ms step_avg:94.92ms
step:227/1770 train_time:20597ms step_avg:94.92ms
step:228/1770 train_time:20692ms step_avg:94.92ms
step:229/1770 train_time:20787ms step_avg:94.92ms
step:230/1770 train_time:20882ms step_avg:94.92ms
step:231/1770 train_time:20978ms step_avg:94.92ms
step:232/1770 train_time:21072ms step_avg:94.92ms
step:233/1770 train_time:21167ms step_avg:94.92ms
step:234/1770 train_time:21263ms step_avg:94.92ms
step:235/1770 train_time:21358ms step_avg:94.93ms
step:236/1770 train_time:21453ms step_avg:94.93ms
step:237/1770 train_time:21549ms step_avg:94.93ms
step:238/1770 train_time:21645ms step_avg:94.93ms
step:239/1770 train_time:21740ms step_avg:94.93ms
step:240/1770 train_time:21835ms step_avg:94.93ms
step:241/1770 train_time:21930ms step_avg:94.93ms
step:242/1770 train_time:22025ms step_avg:94.94ms
step:243/1770 train_time:22120ms step_avg:94.94ms
step:244/1770 train_time:22215ms step_avg:94.94ms
step:245/1770 train_time:22310ms step_avg:94.94ms
step:246/1770 train_time:22405ms step_avg:94.94ms
step:247/1770 train_time:22501ms step_avg:94.94ms
step:248/1770 train_time:22597ms step_avg:94.94ms
step:249/1770 train_time:22692ms step_avg:94.95ms
step:250/1770 train_time:22787ms step_avg:94.95ms
step:250/1770 val_loss:4.1100 train_time:22881ms step_avg:95.34ms
step:251/1770 train_time:22902ms step_avg:95.03ms
step:252/1770 train_time:22989ms step_avg:94.99ms
step:253/1770 train_time:23088ms step_avg:95.01ms
step:254/1770 train_time:23184ms step_avg:95.02ms
step:255/1770 train_time:23280ms step_avg:95.02ms
step:256/1770 train_time:23375ms step_avg:95.02ms
step:257/1770 train_time:23470ms step_avg:95.02ms
step:258/1770 train_time:23565ms step_avg:95.02ms
step:259/1770 train_time:23660ms step_avg:95.02ms
step:260/1770 train_time:23756ms step_avg:95.02ms
step:261/1770 train_time:23850ms step_avg:95.02ms
step:262/1770 train_time:23946ms step_avg:95.02ms
step:263/1770 train_time:24041ms step_avg:95.03ms
step:264/1770 train_time:24137ms step_avg:95.03ms
step:265/1770 train_time:24232ms step_avg:95.03ms
step:266/1770 train_time:24328ms step_avg:95.03ms
step:267/1770 train_time:24424ms step_avg:95.03ms
step:268/1770 train_time:24520ms step_avg:95.04ms
step:269/1770 train_time:24616ms step_avg:95.04ms
step:270/1770 train_time:24711ms step_avg:95.04ms
step:271/1770 train_time:24807ms step_avg:95.04ms
step:272/1770 train_time:24902ms step_avg:95.05ms
step:273/1770 train_time:24999ms step_avg:95.05ms
step:274/1770 train_time:25094ms step_avg:95.05ms
step:275/1770 train_time:25189ms step_avg:95.05ms
step:276/1770 train_time:25285ms step_avg:95.06ms
step:277/1770 train_time:25381ms step_avg:95.06ms
step:278/1770 train_time:25477ms step_avg:95.06ms
step:279/1770 train_time:25573ms step_avg:95.07ms
step:280/1770 train_time:25668ms step_avg:95.07ms
step:281/1770 train_time:25764ms step_avg:95.07ms
step:282/1770 train_time:25860ms step_avg:95.07ms
step:283/1770 train_time:25956ms step_avg:95.08ms
step:284/1770 train_time:26051ms step_avg:95.08ms
step:285/1770 train_time:26147ms step_avg:95.08ms
step:286/1770 train_time:26243ms step_avg:95.08ms
step:287/1770 train_time:26339ms step_avg:95.09ms
step:288/1770 train_time:26435ms step_avg:95.09ms
step:289/1770 train_time:26530ms step_avg:95.09ms
step:290/1770 train_time:26626ms step_avg:95.09ms
step:291/1770 train_time:26722ms step_avg:95.10ms
step:292/1770 train_time:26818ms step_avg:95.10ms
step:293/1770 train_time:26913ms step_avg:95.10ms
step:294/1770 train_time:27008ms step_avg:95.10ms
step:295/1770 train_time:27104ms step_avg:95.10ms
step:296/1770 train_time:27201ms step_avg:95.11ms
step:297/1770 train_time:27296ms step_avg:95.11ms
step:298/1770 train_time:27392ms step_avg:95.11ms
step:299/1770 train_time:27486ms step_avg:95.11ms
step:300/1770 train_time:27583ms step_avg:95.11ms
step:301/1770 train_time:27737ms step_avg:95.32ms
step:302/1770 train_time:27786ms step_avg:95.16ms
step:303/1770 train_time:27882ms step_avg:95.16ms
step:304/1770 train_time:27978ms step_avg:95.16ms
step:305/1770 train_time:28073ms step_avg:95.16ms
step:306/1770 train_time:28169ms step_avg:95.16ms
step:307/1770 train_time:28264ms step_avg:95.17ms
step:308/1770 train_time:28360ms step_avg:95.17ms
step:309/1770 train_time:28456ms step_avg:95.17ms
step:310/1770 train_time:28552ms step_avg:95.17ms
step:311/1770 train_time:28647ms step_avg:95.17ms
step:312/1770 train_time:28743ms step_avg:95.18ms
step:313/1770 train_time:28840ms step_avg:95.18ms
step:314/1770 train_time:28936ms step_avg:95.18ms
step:315/1770 train_time:29031ms step_avg:95.18ms
step:316/1770 train_time:29127ms step_avg:95.18ms
step:317/1770 train_time:29223ms step_avg:95.19ms
step:318/1770 train_time:29319ms step_avg:95.19ms
step:319/1770 train_time:29415ms step_avg:95.20ms
step:320/1770 train_time:29511ms step_avg:95.20ms
step:321/1770 train_time:29607ms step_avg:95.20ms
step:322/1770 train_time:29704ms step_avg:95.20ms
step:323/1770 train_time:29799ms step_avg:95.20ms
step:324/1770 train_time:29895ms step_avg:95.21ms
step:325/1770 train_time:29990ms step_avg:95.21ms
step:326/1770 train_time:30085ms step_avg:95.21ms
step:327/1770 train_time:30181ms step_avg:95.21ms
step:328/1770 train_time:30277ms step_avg:95.21ms
step:329/1770 train_time:30373ms step_avg:95.21ms
step:330/1770 train_time:30468ms step_avg:95.21ms
step:331/1770 train_time:30564ms step_avg:95.21ms
step:332/1770 train_time:30660ms step_avg:95.22ms
step:333/1770 train_time:30756ms step_avg:95.22ms
step:334/1770 train_time:30851ms step_avg:95.22ms
step:335/1770 train_time:30947ms step_avg:95.22ms
step:336/1770 train_time:31043ms step_avg:95.22ms
step:337/1770 train_time:31139ms step_avg:95.23ms
step:338/1770 train_time:31234ms step_avg:95.23ms
step:339/1770 train_time:31330ms step_avg:95.23ms
step:340/1770 train_time:31425ms step_avg:95.23ms
step:341/1770 train_time:31521ms step_avg:95.23ms
step:342/1770 train_time:31618ms step_avg:95.23ms
step:343/1770 train_time:31713ms step_avg:95.24ms
step:344/1770 train_time:31809ms step_avg:95.24ms
step:345/1770 train_time:31905ms step_avg:95.24ms
step:346/1770 train_time:32001ms step_avg:95.24ms
step:347/1770 train_time:32097ms step_avg:95.24ms
step:348/1770 train_time:32193ms step_avg:95.25ms
step:349/1770 train_time:32288ms step_avg:95.25ms
step:350/1770 train_time:32384ms step_avg:95.25ms
step:351/1770 train_time:32480ms step_avg:95.25ms
step:352/1770 train_time:32576ms step_avg:95.25ms
step:353/1770 train_time:32671ms step_avg:95.25ms
step:354/1770 train_time:32767ms step_avg:95.25ms
step:355/1770 train_time:32863ms step_avg:95.26ms
step:356/1770 train_time:32959ms step_avg:95.26ms
step:357/1770 train_time:33055ms step_avg:95.26ms
step:358/1770 train_time:33151ms step_avg:95.26ms
step:359/1770 train_time:33246ms step_avg:95.26ms
step:360/1770 train_time:33342ms step_avg:95.26ms
step:361/1770 train_time:33438ms step_avg:95.26ms
step:362/1770 train_time:33534ms step_avg:95.27ms
step:363/1770 train_time:33629ms step_avg:95.27ms
step:364/1770 train_time:33724ms step_avg:95.27ms
step:365/1770 train_time:33820ms step_avg:95.27ms
step:366/1770 train_time:33916ms step_avg:95.27ms
step:367/1770 train_time:34011ms step_avg:95.27ms
step:368/1770 train_time:34107ms step_avg:95.27ms
step:369/1770 train_time:34203ms step_avg:95.27ms
step:370/1770 train_time:34299ms step_avg:95.28ms
step:371/1770 train_time:34395ms step_avg:95.28ms
step:372/1770 train_time:34490ms step_avg:95.28ms
step:373/1770 train_time:34585ms step_avg:95.28ms
step:374/1770 train_time:34682ms step_avg:95.28ms
step:375/1770 train_time:34777ms step_avg:95.28ms
step:375/1770 val_loss:3.9036 train_time:34871ms step_avg:95.54ms
step:376/1770 train_time:34893ms step_avg:95.33ms
step:377/1770 train_time:34980ms step_avg:95.31ms
step:378/1770 train_time:35079ms step_avg:95.32ms
step:379/1770 train_time:35176ms step_avg:95.33ms
step:380/1770 train_time:35271ms step_avg:95.33ms
step:381/1770 train_time:35366ms step_avg:95.33ms
step:382/1770 train_time:35462ms step_avg:95.33ms
step:383/1770 train_time:35558ms step_avg:95.33ms
step:384/1770 train_time:35653ms step_avg:95.33ms
step:385/1770 train_time:35748ms step_avg:95.33ms
step:386/1770 train_time:35843ms step_avg:95.33ms
step:387/1770 train_time:35939ms step_avg:95.33ms
step:388/1770 train_time:36035ms step_avg:95.33ms
step:389/1770 train_time:36130ms step_avg:95.33ms
step:390/1770 train_time:36226ms step_avg:95.33ms
step:391/1770 train_time:36322ms step_avg:95.33ms
step:392/1770 train_time:36418ms step_avg:95.33ms
step:393/1770 train_time:36513ms step_avg:95.34ms
step:394/1770 train_time:36609ms step_avg:95.34ms
step:395/1770 train_time:36704ms step_avg:95.34ms
step:396/1770 train_time:36802ms step_avg:95.34ms
step:397/1770 train_time:36899ms step_avg:95.35ms
step:398/1770 train_time:36996ms step_avg:95.35ms
step:399/1770 train_time:37094ms step_avg:95.36ms
step:400/1770 train_time:37191ms step_avg:95.36ms
step:401/1770 train_time:37289ms step_avg:95.37ms
step:402/1770 train_time:37386ms step_avg:95.37ms
step:403/1770 train_time:37483ms step_avg:95.38ms
step:404/1770 train_time:37581ms step_avg:95.38ms
step:405/1770 train_time:37679ms step_avg:95.39ms
step:406/1770 train_time:37776ms step_avg:95.39ms
step:407/1770 train_time:37874ms step_avg:95.40ms
step:408/1770 train_time:37971ms step_avg:95.40ms
step:409/1770 train_time:38068ms step_avg:95.41ms
step:410/1770 train_time:38165ms step_avg:95.41ms
step:411/1770 train_time:38264ms step_avg:95.42ms
step:412/1770 train_time:38362ms step_avg:95.43ms
step:413/1770 train_time:38460ms step_avg:95.43ms
step:414/1770 train_time:38557ms step_avg:95.44ms
step:415/1770 train_time:38654ms step_avg:95.44ms
step:416/1770 train_time:38752ms step_avg:95.45ms
step:417/1770 train_time:38849ms step_avg:95.45ms
step:418/1770 train_time:38946ms step_avg:95.46ms
step:419/1770 train_time:39043ms step_avg:95.46ms
step:420/1770 train_time:39142ms step_avg:95.47ms
step:421/1770 train_time:39240ms step_avg:95.47ms
step:422/1770 train_time:39337ms step_avg:95.48ms
step:423/1770 train_time:39434ms step_avg:95.48ms
step:424/1770 train_time:39531ms step_avg:95.49ms
step:425/1770 train_time:39628ms step_avg:95.49ms
step:426/1770 train_time:39726ms step_avg:95.49ms
step:427/1770 train_time:39823ms step_avg:95.50ms
step:428/1770 train_time:39921ms step_avg:95.50ms
step:429/1770 train_time:40018ms step_avg:95.51ms
step:430/1770 train_time:40116ms step_avg:95.51ms
step:431/1770 train_time:40213ms step_avg:95.52ms
step:432/1770 train_time:40310ms step_avg:95.52ms
step:433/1770 train_time:40407ms step_avg:95.52ms
step:434/1770 train_time:40504ms step_avg:95.53ms
step:435/1770 train_time:40602ms step_avg:95.53ms
step:436/1770 train_time:40699ms step_avg:95.54ms
step:437/1770 train_time:40797ms step_avg:95.54ms
step:438/1770 train_time:40894ms step_avg:95.55ms
step:439/1770 train_time:40992ms step_avg:95.55ms
step:440/1770 train_time:41089ms step_avg:95.56ms
step:441/1770 train_time:41186ms step_avg:95.56ms
step:442/1770 train_time:41284ms step_avg:95.56ms
step:443/1770 train_time:41382ms step_avg:95.57ms
step:444/1770 train_time:41480ms step_avg:95.58ms
step:445/1770 train_time:41577ms step_avg:95.58ms
step:446/1770 train_time:41674ms step_avg:95.58ms
step:447/1770 train_time:41772ms step_avg:95.59ms
step:448/1770 train_time:41869ms step_avg:95.59ms
step:449/1770 train_time:41966ms step_avg:95.59ms
step:450/1770 train_time:42064ms step_avg:95.60ms
step:451/1770 train_time:42162ms step_avg:95.61ms
step:452/1770 train_time:42260ms step_avg:95.61ms
step:453/1770 train_time:42358ms step_avg:95.62ms
step:454/1770 train_time:42455ms step_avg:95.62ms
step:455/1770 train_time:42553ms step_avg:95.62ms
step:456/1770 train_time:42650ms step_avg:95.63ms
step:457/1770 train_time:42748ms step_avg:95.63ms
step:458/1770 train_time:42845ms step_avg:95.64ms
step:459/1770 train_time:42943ms step_avg:95.64ms
step:460/1770 train_time:43041ms step_avg:95.65ms
step:461/1770 train_time:43139ms step_avg:95.65ms
step:462/1770 train_time:43236ms step_avg:95.65ms
step:463/1770 train_time:43333ms step_avg:95.66ms
step:464/1770 train_time:43431ms step_avg:95.66ms
step:465/1770 train_time:43528ms step_avg:95.67ms
step:466/1770 train_time:43625ms step_avg:95.67ms
step:467/1770 train_time:43723ms step_avg:95.67ms
step:468/1770 train_time:43820ms step_avg:95.68ms
step:469/1770 train_time:43918ms step_avg:95.68ms
step:470/1770 train_time:44015ms step_avg:95.69ms
step:471/1770 train_time:44112ms step_avg:95.69ms
step:472/1770 train_time:44209ms step_avg:95.69ms
step:473/1770 train_time:44306ms step_avg:95.69ms
step:474/1770 train_time:44404ms step_avg:95.70ms
step:475/1770 train_time:44502ms step_avg:95.70ms
step:476/1770 train_time:44599ms step_avg:95.71ms
step:477/1770 train_time:44697ms step_avg:95.71ms
step:478/1770 train_time:44794ms step_avg:95.71ms
step:479/1770 train_time:44891ms step_avg:95.72ms
step:480/1770 train_time:44989ms step_avg:95.72ms
step:481/1770 train_time:45086ms step_avg:95.72ms
step:482/1770 train_time:45184ms step_avg:95.73ms
step:483/1770 train_time:45282ms step_avg:95.73ms
step:484/1770 train_time:45379ms step_avg:95.74ms
step:485/1770 train_time:45477ms step_avg:95.74ms
step:486/1770 train_time:45574ms step_avg:95.74ms
step:487/1770 train_time:45671ms step_avg:95.75ms
step:488/1770 train_time:45768ms step_avg:95.75ms
step:489/1770 train_time:45866ms step_avg:95.75ms
step:490/1770 train_time:45963ms step_avg:95.76ms
step:491/1770 train_time:46061ms step_avg:95.76ms
step:492/1770 train_time:46159ms step_avg:95.76ms
step:493/1770 train_time:46256ms step_avg:95.77ms
step:494/1770 train_time:46354ms step_avg:95.77ms
step:495/1770 train_time:46451ms step_avg:95.78ms
step:496/1770 train_time:46549ms step_avg:95.78ms
step:497/1770 train_time:46646ms step_avg:95.78ms
step:498/1770 train_time:46744ms step_avg:95.79ms
step:499/1770 train_time:46842ms step_avg:95.79ms
step:500/1770 train_time:46940ms step_avg:95.79ms
step:500/1770 val_loss:3.7515 train_time:47035ms step_avg:95.99ms
step:501/1770 train_time:47057ms step_avg:95.84ms
step:502/1770 train_time:47146ms step_avg:95.83ms
step:503/1770 train_time:47248ms step_avg:95.84ms
step:504/1770 train_time:47347ms step_avg:95.84ms
step:505/1770 train_time:47444ms step_avg:95.85ms
step:506/1770 train_time:47542ms step_avg:95.85ms
step:507/1770 train_time:47639ms step_avg:95.85ms
step:508/1770 train_time:47737ms step_avg:95.86ms
step:509/1770 train_time:47834ms step_avg:95.86ms
step:510/1770 train_time:47931ms step_avg:95.86ms
step:511/1770 train_time:48028ms step_avg:95.86ms
step:512/1770 train_time:48126ms step_avg:95.87ms
step:513/1770 train_time:48225ms step_avg:95.87ms
step:514/1770 train_time:48323ms step_avg:95.88ms
step:515/1770 train_time:48420ms step_avg:95.88ms
step:516/1770 train_time:48517ms step_avg:95.88ms
step:517/1770 train_time:48615ms step_avg:95.89ms
step:518/1770 train_time:48712ms step_avg:95.89ms
step:519/1770 train_time:48809ms step_avg:95.89ms
step:520/1770 train_time:48907ms step_avg:95.90ms
step:521/1770 train_time:49005ms step_avg:95.90ms
step:522/1770 train_time:49103ms step_avg:95.90ms
step:523/1770 train_time:49200ms step_avg:95.91ms
step:524/1770 train_time:49297ms step_avg:95.91ms
step:525/1770 train_time:49395ms step_avg:95.91ms
step:526/1770 train_time:49493ms step_avg:95.92ms
step:527/1770 train_time:49590ms step_avg:95.92ms
step:528/1770 train_time:49688ms step_avg:95.92ms
step:529/1770 train_time:49786ms step_avg:95.93ms
step:530/1770 train_time:49885ms step_avg:95.93ms
step:531/1770 train_time:49983ms step_avg:95.94ms
step:532/1770 train_time:50080ms step_avg:95.94ms
step:533/1770 train_time:50178ms step_avg:95.94ms
step:534/1770 train_time:50276ms step_avg:95.95ms
step:535/1770 train_time:50373ms step_avg:95.95ms
step:536/1770 train_time:50471ms step_avg:95.95ms
step:537/1770 train_time:50569ms step_avg:95.96ms
step:538/1770 train_time:50667ms step_avg:95.96ms
step:539/1770 train_time:50765ms step_avg:95.96ms
step:540/1770 train_time:50863ms step_avg:95.97ms
step:541/1770 train_time:50961ms step_avg:95.97ms
step:542/1770 train_time:51058ms step_avg:95.97ms
step:543/1770 train_time:51156ms step_avg:95.98ms
step:544/1770 train_time:51254ms step_avg:95.98ms
step:545/1770 train_time:51351ms step_avg:95.98ms
step:546/1770 train_time:51450ms step_avg:95.99ms
step:547/1770 train_time:51548ms step_avg:95.99ms
step:548/1770 train_time:51646ms step_avg:96.00ms
step:549/1770 train_time:51744ms step_avg:96.00ms
step:550/1770 train_time:51842ms step_avg:96.00ms
step:551/1770 train_time:51940ms step_avg:96.01ms
step:552/1770 train_time:52038ms step_avg:96.01ms
step:553/1770 train_time:52135ms step_avg:96.01ms
step:554/1770 train_time:52233ms step_avg:96.02ms
step:555/1770 train_time:52331ms step_avg:96.02ms
step:556/1770 train_time:52430ms step_avg:96.02ms
step:557/1770 train_time:52528ms step_avg:96.03ms
step:558/1770 train_time:52627ms step_avg:96.03ms
step:559/1770 train_time:52724ms step_avg:96.04ms
step:560/1770 train_time:52822ms step_avg:96.04ms
step:561/1770 train_time:52919ms step_avg:96.04ms
step:562/1770 train_time:53017ms step_avg:96.05ms
step:563/1770 train_time:53115ms step_avg:96.05ms
step:564/1770 train_time:53213ms step_avg:96.05ms
step:565/1770 train_time:53310ms step_avg:96.05ms
step:566/1770 train_time:53408ms step_avg:96.06ms
step:567/1770 train_time:53507ms step_avg:96.06ms
step:568/1770 train_time:53605ms step_avg:96.07ms
step:569/1770 train_time:53703ms step_avg:96.07ms
step:570/1770 train_time:53801ms step_avg:96.07ms
step:571/1770 train_time:53899ms step_avg:96.08ms
step:572/1770 train_time:53996ms step_avg:96.08ms
step:573/1770 train_time:54094ms step_avg:96.08ms
step:574/1770 train_time:54192ms step_avg:96.09ms
step:575/1770 train_time:54290ms step_avg:96.09ms
step:576/1770 train_time:54388ms step_avg:96.09ms
step:577/1770 train_time:54486ms step_avg:96.10ms
step:578/1770 train_time:54584ms step_avg:96.10ms
step:579/1770 train_time:54682ms step_avg:96.10ms
step:580/1770 train_time:54780ms step_avg:96.10ms
step:581/1770 train_time:54878ms step_avg:96.11ms
step:582/1770 train_time:54975ms step_avg:96.11ms
step:583/1770 train_time:55072ms step_avg:96.11ms
step:584/1770 train_time:55170ms step_avg:96.12ms
step:585/1770 train_time:55269ms step_avg:96.12ms
step:586/1770 train_time:55367ms step_avg:96.12ms
step:587/1770 train_time:55465ms step_avg:96.13ms
step:588/1770 train_time:55563ms step_avg:96.13ms
step:589/1770 train_time:55661ms step_avg:96.13ms
step:590/1770 train_time:55759ms step_avg:96.14ms
step:591/1770 train_time:55857ms step_avg:96.14ms
step:592/1770 train_time:55955ms step_avg:96.14ms
step:593/1770 train_time:56053ms step_avg:96.15ms
step:594/1770 train_time:56150ms step_avg:96.15ms
step:595/1770 train_time:56248ms step_avg:96.15ms
step:596/1770 train_time:56346ms step_avg:96.15ms
step:597/1770 train_time:56445ms step_avg:96.16ms
step:598/1770 train_time:56543ms step_avg:96.16ms
step:599/1770 train_time:56641ms step_avg:96.16ms
step:600/1770 train_time:56739ms step_avg:96.17ms
step:601/1770 train_time:56836ms step_avg:96.17ms
step:602/1770 train_time:56934ms step_avg:96.17ms
step:603/1770 train_time:57031ms step_avg:96.17ms
step:604/1770 train_time:57130ms step_avg:96.18ms
step:605/1770 train_time:57227ms step_avg:96.18ms
step:606/1770 train_time:57325ms step_avg:96.18ms
step:607/1770 train_time:57423ms step_avg:96.19ms
step:608/1770 train_time:57522ms step_avg:96.19ms
step:609/1770 train_time:57620ms step_avg:96.19ms
step:610/1770 train_time:57718ms step_avg:96.20ms
step:611/1770 train_time:57816ms step_avg:96.20ms
step:612/1770 train_time:57914ms step_avg:96.20ms
step:613/1770 train_time:58011ms step_avg:96.20ms
step:614/1770 train_time:58109ms step_avg:96.21ms
step:615/1770 train_time:58207ms step_avg:96.21ms
step:616/1770 train_time:58306ms step_avg:96.21ms
step:617/1770 train_time:58404ms step_avg:96.22ms
step:618/1770 train_time:58502ms step_avg:96.22ms
step:619/1770 train_time:58600ms step_avg:96.22ms
step:620/1770 train_time:58698ms step_avg:96.23ms
step:621/1770 train_time:58796ms step_avg:96.23ms
step:622/1770 train_time:58894ms step_avg:96.23ms
step:623/1770 train_time:58992ms step_avg:96.23ms
step:624/1770 train_time:59090ms step_avg:96.24ms
step:625/1770 train_time:59188ms step_avg:96.24ms
step:625/1770 val_loss:3.6633 train_time:59285ms step_avg:96.40ms
step:626/1770 train_time:59307ms step_avg:96.28ms
step:627/1770 train_time:59394ms step_avg:96.26ms
step:628/1770 train_time:59496ms step_avg:96.27ms
step:629/1770 train_time:59594ms step_avg:96.27ms
step:630/1770 train_time:59692ms step_avg:96.28ms
step:631/1770 train_time:59789ms step_avg:96.28ms
step:632/1770 train_time:59887ms step_avg:96.28ms
step:633/1770 train_time:59985ms step_avg:96.28ms
step:634/1770 train_time:60084ms step_avg:96.29ms
step:635/1770 train_time:60182ms step_avg:96.29ms
step:636/1770 train_time:60280ms step_avg:96.29ms
step:637/1770 train_time:60377ms step_avg:96.30ms
step:638/1770 train_time:60476ms step_avg:96.30ms
step:639/1770 train_time:60574ms step_avg:96.30ms
step:640/1770 train_time:60672ms step_avg:96.30ms
step:641/1770 train_time:60770ms step_avg:96.31ms
step:642/1770 train_time:60867ms step_avg:96.31ms
step:643/1770 train_time:60965ms step_avg:96.31ms
step:644/1770 train_time:61063ms step_avg:96.31ms
step:645/1770 train_time:61162ms step_avg:96.32ms
step:646/1770 train_time:61259ms step_avg:96.32ms
step:647/1770 train_time:61357ms step_avg:96.32ms
step:648/1770 train_time:61455ms step_avg:96.32ms
step:649/1770 train_time:61552ms step_avg:96.33ms
step:650/1770 train_time:61650ms step_avg:96.33ms
step:651/1770 train_time:61748ms step_avg:96.33ms
step:652/1770 train_time:61846ms step_avg:96.33ms
step:653/1770 train_time:61944ms step_avg:96.34ms
step:654/1770 train_time:62041ms step_avg:96.34ms
step:655/1770 train_time:62139ms step_avg:96.34ms
step:656/1770 train_time:62238ms step_avg:96.34ms
step:657/1770 train_time:62335ms step_avg:96.35ms
step:658/1770 train_time:62435ms step_avg:96.35ms
step:659/1770 train_time:62534ms step_avg:96.35ms
step:660/1770 train_time:62633ms step_avg:96.36ms
step:661/1770 train_time:62733ms step_avg:96.36ms
step:662/1770 train_time:62832ms step_avg:96.37ms
step:663/1770 train_time:62932ms step_avg:96.37ms
step:664/1770 train_time:63031ms step_avg:96.38ms
step:665/1770 train_time:63131ms step_avg:96.38ms
step:666/1770 train_time:63231ms step_avg:96.39ms
step:667/1770 train_time:63331ms step_avg:96.39ms
step:668/1770 train_time:63430ms step_avg:96.40ms
step:669/1770 train_time:63530ms step_avg:96.40ms
step:670/1770 train_time:63629ms step_avg:96.41ms
step:671/1770 train_time:63729ms step_avg:96.41ms
step:672/1770 train_time:63828ms step_avg:96.42ms
step:673/1770 train_time:63928ms step_avg:96.42ms
step:674/1770 train_time:64027ms step_avg:96.43ms
step:675/1770 train_time:64127ms step_avg:96.43ms
step:676/1770 train_time:64226ms step_avg:96.44ms
step:677/1770 train_time:64326ms step_avg:96.44ms
step:678/1770 train_time:64426ms step_avg:96.45ms
step:679/1770 train_time:64526ms step_avg:96.45ms
step:680/1770 train_time:64625ms step_avg:96.46ms
step:681/1770 train_time:64725ms step_avg:96.46ms
step:682/1770 train_time:64824ms step_avg:96.46ms
step:683/1770 train_time:64925ms step_avg:96.47ms
step:684/1770 train_time:65025ms step_avg:96.48ms
step:685/1770 train_time:65125ms step_avg:96.48ms
step:686/1770 train_time:65225ms step_avg:96.49ms
step:687/1770 train_time:65326ms step_avg:96.49ms
step:688/1770 train_time:65425ms step_avg:96.50ms
step:689/1770 train_time:65525ms step_avg:96.50ms
step:690/1770 train_time:65626ms step_avg:96.51ms
step:691/1770 train_time:65725ms step_avg:96.51ms
step:692/1770 train_time:65825ms step_avg:96.52ms
step:693/1770 train_time:65925ms step_avg:96.52ms
step:694/1770 train_time:66026ms step_avg:96.53ms
step:695/1770 train_time:66126ms step_avg:96.53ms
step:696/1770 train_time:66225ms step_avg:96.54ms
step:697/1770 train_time:66325ms step_avg:96.54ms
step:698/1770 train_time:66425ms step_avg:96.55ms
step:699/1770 train_time:66525ms step_avg:96.55ms
step:700/1770 train_time:66625ms step_avg:96.56ms
step:701/1770 train_time:66725ms step_avg:96.56ms
step:702/1770 train_time:66825ms step_avg:96.57ms
step:703/1770 train_time:66925ms step_avg:96.57ms
step:704/1770 train_time:67025ms step_avg:96.58ms
step:705/1770 train_time:67125ms step_avg:96.58ms
step:706/1770 train_time:67225ms step_avg:96.59ms
step:707/1770 train_time:67325ms step_avg:96.59ms
step:708/1770 train_time:67425ms step_avg:96.60ms
step:709/1770 train_time:67525ms step_avg:96.60ms
step:710/1770 train_time:67625ms step_avg:96.61ms
step:711/1770 train_time:67725ms step_avg:96.61ms
step:712/1770 train_time:67826ms step_avg:96.62ms
step:713/1770 train_time:67926ms step_avg:96.62ms
step:714/1770 train_time:68026ms step_avg:96.63ms
step:715/1770 train_time:68126ms step_avg:96.63ms
step:716/1770 train_time:68226ms step_avg:96.64ms
step:717/1770 train_time:68326ms step_avg:96.64ms
step:718/1770 train_time:68425ms step_avg:96.65ms
step:719/1770 train_time:68525ms step_avg:96.65ms
step:720/1770 train_time:68625ms step_avg:96.66ms
step:721/1770 train_time:68725ms step_avg:96.66ms
step:722/1770 train_time:68825ms step_avg:96.66ms
step:723/1770 train_time:68926ms step_avg:96.67ms
step:724/1770 train_time:69027ms step_avg:96.68ms
step:725/1770 train_time:69127ms step_avg:96.68ms
step:726/1770 train_time:69226ms step_avg:96.68ms
step:727/1770 train_time:69326ms step_avg:96.69ms
step:728/1770 train_time:69426ms step_avg:96.69ms
step:729/1770 train_time:69525ms step_avg:96.70ms
step:730/1770 train_time:69626ms step_avg:96.70ms
step:731/1770 train_time:69727ms step_avg:96.71ms
step:732/1770 train_time:69827ms step_avg:96.71ms
step:733/1770 train_time:69926ms step_avg:96.72ms
step:734/1770 train_time:70026ms step_avg:96.72ms
step:735/1770 train_time:70126ms step_avg:96.73ms
step:736/1770 train_time:70226ms step_avg:96.73ms
step:737/1770 train_time:70326ms step_avg:96.73ms
step:738/1770 train_time:70426ms step_avg:96.74ms
step:739/1770 train_time:70526ms step_avg:96.74ms
step:740/1770 train_time:70625ms step_avg:96.75ms
step:741/1770 train_time:70725ms step_avg:96.75ms
step:742/1770 train_time:70825ms step_avg:96.76ms
step:743/1770 train_time:70925ms step_avg:96.76ms
step:744/1770 train_time:71024ms step_avg:96.76ms
step:745/1770 train_time:71124ms step_avg:96.77ms
step:746/1770 train_time:71224ms step_avg:96.77ms
step:747/1770 train_time:71324ms step_avg:96.78ms
step:748/1770 train_time:71425ms step_avg:96.78ms
step:749/1770 train_time:71526ms step_avg:96.79ms
step:750/1770 train_time:71626ms step_avg:96.79ms
step:750/1770 val_loss:3.6024 train_time:71724ms step_avg:96.92ms
step:751/1770 train_time:71745ms step_avg:96.82ms
step:752/1770 train_time:71838ms step_avg:96.82ms
step:753/1770 train_time:71940ms step_avg:96.82ms
step:754/1770 train_time:72039ms step_avg:96.83ms
step:755/1770 train_time:72138ms step_avg:96.83ms
step:756/1770 train_time:72238ms step_avg:96.83ms
step:757/1770 train_time:72336ms step_avg:96.84ms
step:758/1770 train_time:72436ms step_avg:96.84ms
step:759/1770 train_time:72535ms step_avg:96.84ms
step:760/1770 train_time:72634ms step_avg:96.85ms
step:761/1770 train_time:72733ms step_avg:96.85ms
step:762/1770 train_time:72833ms step_avg:96.85ms
step:763/1770 train_time:72934ms step_avg:96.86ms
step:764/1770 train_time:73033ms step_avg:96.86ms
step:765/1770 train_time:73133ms step_avg:96.86ms
step:766/1770 train_time:73233ms step_avg:96.87ms
step:767/1770 train_time:73334ms step_avg:96.87ms
step:768/1770 train_time:73433ms step_avg:96.88ms
step:769/1770 train_time:73533ms step_avg:96.88ms
step:770/1770 train_time:73632ms step_avg:96.88ms
step:771/1770 train_time:73732ms step_avg:96.89ms
step:772/1770 train_time:73831ms step_avg:96.89ms
step:773/1770 train_time:73931ms step_avg:96.90ms
step:774/1770 train_time:74031ms step_avg:96.90ms
step:775/1770 train_time:74132ms step_avg:96.90ms
step:776/1770 train_time:74232ms step_avg:96.91ms
step:777/1770 train_time:74332ms step_avg:96.91ms
step:778/1770 train_time:74432ms step_avg:96.92ms
step:779/1770 train_time:74532ms step_avg:96.92ms
step:780/1770 train_time:74632ms step_avg:96.92ms
step:781/1770 train_time:74732ms step_avg:96.93ms
step:782/1770 train_time:74832ms step_avg:96.93ms
step:783/1770 train_time:74932ms step_avg:96.94ms
step:784/1770 train_time:75032ms step_avg:96.94ms
step:785/1770 train_time:75132ms step_avg:96.94ms
step:786/1770 train_time:75232ms step_avg:96.95ms
step:787/1770 train_time:75332ms step_avg:96.95ms
step:788/1770 train_time:75432ms step_avg:96.96ms
step:789/1770 train_time:75533ms step_avg:96.96ms
step:790/1770 train_time:75633ms step_avg:96.97ms
step:791/1770 train_time:75733ms step_avg:96.97ms
step:792/1770 train_time:75833ms step_avg:96.97ms
step:793/1770 train_time:75933ms step_avg:96.98ms
step:794/1770 train_time:76033ms step_avg:96.98ms
step:795/1770 train_time:76133ms step_avg:96.99ms
step:796/1770 train_time:76234ms step_avg:96.99ms
step:797/1770 train_time:76333ms step_avg:96.99ms
step:798/1770 train_time:76433ms step_avg:97.00ms
step:799/1770 train_time:76533ms step_avg:97.00ms
step:800/1770 train_time:76633ms step_avg:97.00ms
step:801/1770 train_time:76733ms step_avg:97.01ms
step:802/1770 train_time:76833ms step_avg:97.01ms
step:803/1770 train_time:76932ms step_avg:97.01ms
step:804/1770 train_time:77033ms step_avg:97.02ms
step:805/1770 train_time:77132ms step_avg:97.02ms
step:806/1770 train_time:77232ms step_avg:97.03ms
step:807/1770 train_time:77333ms step_avg:97.03ms
step:808/1770 train_time:77433ms step_avg:97.03ms
step:809/1770 train_time:77533ms step_avg:97.04ms
step:810/1770 train_time:77633ms step_avg:97.04ms
step:811/1770 train_time:77733ms step_avg:97.05ms
step:812/1770 train_time:77833ms step_avg:97.05ms
step:813/1770 train_time:77933ms step_avg:97.05ms
step:814/1770 train_time:78033ms step_avg:97.06ms
step:815/1770 train_time:78133ms step_avg:97.06ms
step:816/1770 train_time:78233ms step_avg:97.06ms
step:817/1770 train_time:78332ms step_avg:97.07ms
step:818/1770 train_time:78432ms step_avg:97.07ms
step:819/1770 train_time:78532ms step_avg:97.07ms
step:820/1770 train_time:78632ms step_avg:97.08ms
step:821/1770 train_time:78732ms step_avg:97.08ms
step:822/1770 train_time:78832ms step_avg:97.08ms
step:823/1770 train_time:78932ms step_avg:97.09ms
step:824/1770 train_time:79032ms step_avg:97.09ms
step:825/1770 train_time:79133ms step_avg:97.10ms
step:826/1770 train_time:79233ms step_avg:97.10ms
step:827/1770 train_time:79333ms step_avg:97.10ms
step:828/1770 train_time:79434ms step_avg:97.11ms
step:829/1770 train_time:79534ms step_avg:97.11ms
step:830/1770 train_time:79633ms step_avg:97.11ms
step:831/1770 train_time:79733ms step_avg:97.12ms
step:832/1770 train_time:79833ms step_avg:97.12ms
step:833/1770 train_time:79933ms step_avg:97.12ms
step:834/1770 train_time:80033ms step_avg:97.13ms
step:835/1770 train_time:80134ms step_avg:97.13ms
step:836/1770 train_time:80234ms step_avg:97.14ms
step:837/1770 train_time:80334ms step_avg:97.14ms
step:838/1770 train_time:80434ms step_avg:97.14ms
step:839/1770 train_time:80534ms step_avg:97.15ms
step:840/1770 train_time:80634ms step_avg:97.15ms
step:841/1770 train_time:80734ms step_avg:97.15ms
step:842/1770 train_time:80834ms step_avg:97.16ms
step:843/1770 train_time:80933ms step_avg:97.16ms
step:844/1770 train_time:81033ms step_avg:97.16ms
step:845/1770 train_time:81133ms step_avg:97.17ms
step:846/1770 train_time:81233ms step_avg:97.17ms
step:847/1770 train_time:81332ms step_avg:97.17ms
step:848/1770 train_time:81432ms step_avg:97.17ms
step:849/1770 train_time:81532ms step_avg:97.18ms
step:850/1770 train_time:81632ms step_avg:97.18ms
step:851/1770 train_time:81733ms step_avg:97.18ms
step:852/1770 train_time:81832ms step_avg:97.19ms
step:853/1770 train_time:81932ms step_avg:97.19ms
step:854/1770 train_time:82032ms step_avg:97.19ms
step:855/1770 train_time:82133ms step_avg:97.20ms
step:856/1770 train_time:82233ms step_avg:97.20ms
step:857/1770 train_time:82333ms step_avg:97.21ms
step:858/1770 train_time:82433ms step_avg:97.21ms
step:859/1770 train_time:82533ms step_avg:97.21ms
step:860/1770 train_time:82633ms step_avg:97.21ms
step:861/1770 train_time:82733ms step_avg:97.22ms
step:862/1770 train_time:82832ms step_avg:97.22ms
step:863/1770 train_time:82932ms step_avg:97.22ms
step:864/1770 train_time:83032ms step_avg:97.23ms
step:865/1770 train_time:83132ms step_avg:97.23ms
step:866/1770 train_time:83234ms step_avg:97.24ms
step:867/1770 train_time:83334ms step_avg:97.24ms
step:868/1770 train_time:83434ms step_avg:97.24ms
step:869/1770 train_time:83534ms step_avg:97.25ms
step:870/1770 train_time:83634ms step_avg:97.25ms
step:871/1770 train_time:83735ms step_avg:97.25ms
step:872/1770 train_time:83834ms step_avg:97.26ms
step:873/1770 train_time:83934ms step_avg:97.26ms
step:874/1770 train_time:84034ms step_avg:97.26ms
step:875/1770 train_time:84134ms step_avg:97.26ms
step:875/1770 val_loss:3.5528 train_time:84232ms step_avg:97.38ms
step:876/1770 train_time:84256ms step_avg:97.29ms
step:877/1770 train_time:84345ms step_avg:97.28ms
step:878/1770 train_time:84446ms step_avg:97.29ms
step:879/1770 train_time:84546ms step_avg:97.29ms
step:880/1770 train_time:84646ms step_avg:97.29ms
step:881/1770 train_time:84745ms step_avg:97.30ms
step:882/1770 train_time:84845ms step_avg:97.30ms
step:883/1770 train_time:84944ms step_avg:97.30ms
step:884/1770 train_time:85044ms step_avg:97.30ms
step:885/1770 train_time:85143ms step_avg:97.31ms
step:886/1770 train_time:85242ms step_avg:97.31ms
step:887/1770 train_time:85342ms step_avg:97.31ms
step:888/1770 train_time:85443ms step_avg:97.32ms
step:889/1770 train_time:85543ms step_avg:97.32ms
step:890/1770 train_time:85643ms step_avg:97.32ms
step:891/1770 train_time:85743ms step_avg:97.32ms
step:892/1770 train_time:85843ms step_avg:97.33ms
step:893/1770 train_time:85942ms step_avg:97.33ms
step:894/1770 train_time:86043ms step_avg:97.33ms
step:895/1770 train_time:86143ms step_avg:97.34ms
step:896/1770 train_time:86242ms step_avg:97.34ms
step:897/1770 train_time:86342ms step_avg:97.34ms
step:898/1770 train_time:86442ms step_avg:97.34ms
step:899/1770 train_time:86541ms step_avg:97.35ms
step:900/1770 train_time:86641ms step_avg:97.35ms
step:901/1770 train_time:86741ms step_avg:97.35ms
step:902/1770 train_time:86840ms step_avg:97.35ms
step:903/1770 train_time:86940ms step_avg:97.36ms
step:904/1770 train_time:87040ms step_avg:97.36ms
step:905/1770 train_time:87139ms step_avg:97.36ms
step:906/1770 train_time:87239ms step_avg:97.36ms
step:907/1770 train_time:87339ms step_avg:97.37ms
step:908/1770 train_time:87439ms step_avg:97.37ms
step:909/1770 train_time:87539ms step_avg:97.37ms
step:910/1770 train_time:87639ms step_avg:97.38ms
step:911/1770 train_time:87739ms step_avg:97.38ms
step:912/1770 train_time:87839ms step_avg:97.38ms
step:913/1770 train_time:87939ms step_avg:97.38ms
step:914/1770 train_time:88038ms step_avg:97.39ms
step:915/1770 train_time:88138ms step_avg:97.39ms
step:916/1770 train_time:88238ms step_avg:97.39ms
step:917/1770 train_time:88338ms step_avg:97.40ms
step:918/1770 train_time:88438ms step_avg:97.40ms
step:919/1770 train_time:88538ms step_avg:97.40ms
step:920/1770 train_time:88640ms step_avg:97.41ms
step:921/1770 train_time:88742ms step_avg:97.41ms
step:922/1770 train_time:88842ms step_avg:97.41ms
step:923/1770 train_time:88943ms step_avg:97.42ms
step:924/1770 train_time:89045ms step_avg:97.42ms
step:925/1770 train_time:89145ms step_avg:97.43ms
step:926/1770 train_time:89247ms step_avg:97.43ms
step:927/1770 train_time:89348ms step_avg:97.44ms
step:928/1770 train_time:89449ms step_avg:97.44ms
step:929/1770 train_time:89550ms step_avg:97.44ms
step:930/1770 train_time:89651ms step_avg:97.45ms
step:931/1770 train_time:89752ms step_avg:97.45ms
step:932/1770 train_time:89854ms step_avg:97.46ms
step:933/1770 train_time:89955ms step_avg:97.46ms
step:934/1770 train_time:90058ms step_avg:97.47ms
step:935/1770 train_time:90160ms step_avg:97.47ms
step:936/1770 train_time:90261ms step_avg:97.47ms
step:937/1770 train_time:90362ms step_avg:97.48ms
step:938/1770 train_time:90463ms step_avg:97.48ms
step:939/1770 train_time:90565ms step_avg:97.49ms
step:940/1770 train_time:90667ms step_avg:97.49ms
step:941/1770 train_time:90768ms step_avg:97.50ms
step:942/1770 train_time:90869ms step_avg:97.50ms
step:943/1770 train_time:90971ms step_avg:97.50ms
step:944/1770 train_time:91072ms step_avg:97.51ms
step:945/1770 train_time:91172ms step_avg:97.51ms
step:946/1770 train_time:91276ms step_avg:97.52ms
step:947/1770 train_time:91379ms step_avg:97.52ms
step:948/1770 train_time:91480ms step_avg:97.53ms
step:949/1770 train_time:91582ms step_avg:97.53ms
step:950/1770 train_time:91683ms step_avg:97.54ms
step:951/1770 train_time:91784ms step_avg:97.54ms
step:952/1770 train_time:91885ms step_avg:97.54ms
step:953/1770 train_time:91987ms step_avg:97.55ms
step:954/1770 train_time:92088ms step_avg:97.55ms
step:955/1770 train_time:92190ms step_avg:97.56ms
step:956/1770 train_time:92291ms step_avg:97.56ms
step:957/1770 train_time:92392ms step_avg:97.56ms
step:958/1770 train_time:92493ms step_avg:97.57ms
step:959/1770 train_time:92595ms step_avg:97.57ms
step:960/1770 train_time:92696ms step_avg:97.57ms
step:961/1770 train_time:92798ms step_avg:97.58ms
step:962/1770 train_time:92900ms step_avg:97.58ms
step:963/1770 train_time:93001ms step_avg:97.59ms
step:964/1770 train_time:93102ms step_avg:97.59ms
step:965/1770 train_time:93202ms step_avg:97.59ms
step:966/1770 train_time:93303ms step_avg:97.60ms
step:967/1770 train_time:93405ms step_avg:97.60ms
step:968/1770 train_time:93506ms step_avg:97.61ms
step:969/1770 train_time:93608ms step_avg:97.61ms
step:970/1770 train_time:93709ms step_avg:97.61ms
step:971/1770 train_time:93811ms step_avg:97.62ms
step:972/1770 train_time:93911ms step_avg:97.62ms
step:973/1770 train_time:94013ms step_avg:97.63ms
step:974/1770 train_time:94115ms step_avg:97.63ms
step:975/1770 train_time:94218ms step_avg:97.63ms
step:976/1770 train_time:94321ms step_avg:97.64ms
step:977/1770 train_time:94422ms step_avg:97.64ms
step:978/1770 train_time:94523ms step_avg:97.65ms
step:979/1770 train_time:94625ms step_avg:97.65ms
step:980/1770 train_time:94725ms step_avg:97.66ms
step:981/1770 train_time:94826ms step_avg:97.66ms
step:982/1770 train_time:94927ms step_avg:97.66ms
step:983/1770 train_time:95028ms step_avg:97.67ms
step:984/1770 train_time:95130ms step_avg:97.67ms
step:985/1770 train_time:95231ms step_avg:97.67ms
step:986/1770 train_time:95332ms step_avg:97.68ms
step:987/1770 train_time:95435ms step_avg:97.68ms
step:988/1770 train_time:95537ms step_avg:97.69ms
step:989/1770 train_time:95640ms step_avg:97.69ms
step:990/1770 train_time:95741ms step_avg:97.69ms
step:991/1770 train_time:95842ms step_avg:97.70ms
step:992/1770 train_time:95942ms step_avg:97.70ms
step:993/1770 train_time:96043ms step_avg:97.70ms
step:994/1770 train_time:96144ms step_avg:97.71ms
step:995/1770 train_time:96245ms step_avg:97.71ms
step:996/1770 train_time:96346ms step_avg:97.71ms
step:997/1770 train_time:96448ms step_avg:97.72ms
step:998/1770 train_time:96549ms step_avg:97.72ms
step:999/1770 train_time:96650ms step_avg:97.73ms
step:1000/1770 train_time:96751ms step_avg:97.73ms
step:1000/1770 val_loss:3.5141 train_time:96850ms step_avg:97.83ms
step:1001/1770 train_time:96872ms step_avg:97.75ms
step:1002/1770 train_time:96961ms step_avg:97.74ms
step:1003/1770 train_time:97066ms step_avg:97.75ms
step:1004/1770 train_time:97168ms step_avg:97.75ms
step:1005/1770 train_time:97269ms step_avg:97.76ms
step:1006/1770 train_time:97370ms step_avg:97.76ms
step:1007/1770 train_time:97470ms step_avg:97.76ms
step:1008/1770 train_time:97571ms step_avg:97.77ms
step:1009/1770 train_time:97672ms step_avg:97.77ms
step:1010/1770 train_time:97772ms step_avg:97.77ms
step:1011/1770 train_time:97874ms step_avg:97.78ms
step:1012/1770 train_time:97975ms step_avg:97.78ms
step:1013/1770 train_time:98077ms step_avg:97.78ms
step:1014/1770 train_time:98178ms step_avg:97.79ms
step:1015/1770 train_time:98279ms step_avg:97.79ms
step:1016/1770 train_time:98381ms step_avg:97.79ms
step:1017/1770 train_time:98483ms step_avg:97.80ms
step:1018/1770 train_time:98585ms step_avg:97.80ms
step:1019/1770 train_time:98687ms step_avg:97.81ms
step:1020/1770 train_time:98789ms step_avg:97.81ms
step:1021/1770 train_time:98891ms step_avg:97.81ms
step:1022/1770 train_time:98991ms step_avg:97.82ms
step:1023/1770 train_time:99092ms step_avg:97.82ms
step:1024/1770 train_time:99193ms step_avg:97.82ms
step:1025/1770 train_time:99295ms step_avg:97.83ms
step:1026/1770 train_time:99396ms step_avg:97.83ms
step:1027/1770 train_time:99498ms step_avg:97.84ms
step:1028/1770 train_time:99600ms step_avg:97.84ms
step:1029/1770 train_time:99701ms step_avg:97.84ms
step:1030/1770 train_time:99803ms step_avg:97.85ms
step:1031/1770 train_time:99905ms step_avg:97.85ms
step:1032/1770 train_time:100007ms step_avg:97.85ms
step:1033/1770 train_time:100109ms step_avg:97.86ms
step:1034/1770 train_time:100210ms step_avg:97.86ms
step:1035/1770 train_time:100310ms step_avg:97.86ms
step:1036/1770 train_time:100411ms step_avg:97.87ms
step:1037/1770 train_time:100511ms step_avg:97.87ms
step:1038/1770 train_time:100612ms step_avg:97.87ms
step:1039/1770 train_time:100713ms step_avg:97.87ms
step:1040/1770 train_time:100814ms step_avg:97.88ms
step:1041/1770 train_time:100916ms step_avg:97.88ms
step:1042/1770 train_time:101017ms step_avg:97.89ms
step:1043/1770 train_time:101119ms step_avg:97.89ms
step:1044/1770 train_time:101220ms step_avg:97.89ms
step:1045/1770 train_time:101320ms step_avg:97.89ms
step:1046/1770 train_time:101422ms step_avg:97.90ms
step:1047/1770 train_time:101524ms step_avg:97.90ms
step:1048/1770 train_time:101626ms step_avg:97.91ms
step:1049/1770 train_time:101727ms step_avg:97.91ms
step:1050/1770 train_time:101829ms step_avg:97.91ms
step:1051/1770 train_time:101931ms step_avg:97.92ms
step:1052/1770 train_time:102032ms step_avg:97.92ms
step:1053/1770 train_time:102133ms step_avg:97.92ms
step:1054/1770 train_time:102235ms step_avg:97.93ms
step:1055/1770 train_time:102336ms step_avg:97.93ms
step:1056/1770 train_time:102437ms step_avg:97.93ms
step:1057/1770 train_time:102538ms step_avg:97.94ms
step:1058/1770 train_time:102640ms step_avg:97.94ms
step:1059/1770 train_time:102743ms step_avg:97.94ms
step:1060/1770 train_time:102845ms step_avg:97.95ms
step:1061/1770 train_time:102947ms step_avg:97.95ms
step:1062/1770 train_time:103050ms step_avg:97.96ms
step:1063/1770 train_time:103152ms step_avg:97.96ms
step:1064/1770 train_time:103255ms step_avg:97.96ms
step:1065/1770 train_time:103356ms step_avg:97.97ms
step:1066/1770 train_time:103457ms step_avg:97.97ms
step:1067/1770 train_time:103558ms step_avg:97.97ms
step:1068/1770 train_time:103660ms step_avg:97.98ms
step:1069/1770 train_time:103761ms step_avg:97.98ms
step:1070/1770 train_time:103863ms step_avg:97.98ms
step:1071/1770 train_time:103964ms step_avg:97.99ms
step:1072/1770 train_time:104067ms step_avg:97.99ms
step:1073/1770 train_time:104169ms step_avg:98.00ms
step:1074/1770 train_time:104270ms step_avg:98.00ms
step:1075/1770 train_time:104371ms step_avg:98.00ms
step:1076/1770 train_time:104473ms step_avg:98.00ms
step:1077/1770 train_time:104574ms step_avg:98.01ms
step:1078/1770 train_time:104675ms step_avg:98.01ms
step:1079/1770 train_time:104776ms step_avg:98.01ms
step:1080/1770 train_time:104878ms step_avg:98.02ms
step:1081/1770 train_time:104979ms step_avg:98.02ms
step:1082/1770 train_time:105082ms step_avg:98.02ms
step:1083/1770 train_time:105184ms step_avg:98.03ms
step:1084/1770 train_time:105285ms step_avg:98.03ms
step:1085/1770 train_time:105387ms step_avg:98.03ms
step:1086/1770 train_time:105488ms step_avg:98.04ms
step:1087/1770 train_time:105591ms step_avg:98.04ms
step:1088/1770 train_time:105692ms step_avg:98.04ms
step:1089/1770 train_time:105793ms step_avg:98.05ms
step:1090/1770 train_time:105895ms step_avg:98.05ms
step:1091/1770 train_time:105997ms step_avg:98.05ms
step:1092/1770 train_time:106098ms step_avg:98.06ms
step:1093/1770 train_time:106200ms step_avg:98.06ms
step:1094/1770 train_time:106302ms step_avg:98.06ms
step:1095/1770 train_time:106403ms step_avg:98.07ms
step:1096/1770 train_time:106505ms step_avg:98.07ms
step:1097/1770 train_time:106607ms step_avg:98.07ms
step:1098/1770 train_time:106709ms step_avg:98.08ms
step:1099/1770 train_time:106810ms step_avg:98.08ms
step:1100/1770 train_time:106911ms step_avg:98.08ms
step:1101/1770 train_time:107012ms step_avg:98.09ms
step:1102/1770 train_time:107114ms step_avg:98.09ms
step:1103/1770 train_time:107215ms step_avg:98.09ms
step:1104/1770 train_time:107317ms step_avg:98.10ms
step:1105/1770 train_time:107418ms step_avg:98.10ms
step:1106/1770 train_time:107520ms step_avg:98.10ms
step:1107/1770 train_time:107621ms step_avg:98.11ms
step:1108/1770 train_time:107724ms step_avg:98.11ms
step:1109/1770 train_time:107826ms step_avg:98.11ms
step:1110/1770 train_time:107928ms step_avg:98.12ms
step:1111/1770 train_time:108030ms step_avg:98.12ms
step:1112/1770 train_time:108132ms step_avg:98.12ms
step:1113/1770 train_time:108233ms step_avg:98.13ms
step:1114/1770 train_time:108335ms step_avg:98.13ms
step:1115/1770 train_time:108436ms step_avg:98.13ms
step:1116/1770 train_time:108538ms step_avg:98.14ms
step:1117/1770 train_time:108639ms step_avg:98.14ms
step:1118/1770 train_time:108740ms step_avg:98.14ms
step:1119/1770 train_time:108841ms step_avg:98.14ms
step:1120/1770 train_time:108943ms step_avg:98.15ms
step:1121/1770 train_time:109046ms step_avg:98.15ms
step:1122/1770 train_time:109149ms step_avg:98.16ms
step:1123/1770 train_time:109250ms step_avg:98.16ms
step:1124/1770 train_time:109351ms step_avg:98.16ms
step:1125/1770 train_time:109453ms step_avg:98.16ms
step:1125/1770 val_loss:3.4744 train_time:109553ms step_avg:98.25ms
step:1126/1770 train_time:109575ms step_avg:98.19ms
step:1127/1770 train_time:109665ms step_avg:98.18ms
step:1128/1770 train_time:109768ms step_avg:98.18ms
step:1129/1770 train_time:109869ms step_avg:98.19ms
step:1130/1770 train_time:109972ms step_avg:98.19ms
step:1131/1770 train_time:110074ms step_avg:98.19ms
step:1132/1770 train_time:110175ms step_avg:98.20ms
step:1133/1770 train_time:110277ms step_avg:98.20ms
step:1134/1770 train_time:110378ms step_avg:98.20ms
step:1135/1770 train_time:110479ms step_avg:98.20ms
step:1136/1770 train_time:110580ms step_avg:98.21ms
step:1137/1770 train_time:110682ms step_avg:98.21ms
step:1138/1770 train_time:110783ms step_avg:98.21ms
step:1139/1770 train_time:110885ms step_avg:98.21ms
step:1140/1770 train_time:110986ms step_avg:98.22ms
step:1141/1770 train_time:111087ms step_avg:98.22ms
step:1142/1770 train_time:111188ms step_avg:98.22ms
step:1143/1770 train_time:111289ms step_avg:98.22ms
step:1144/1770 train_time:111390ms step_avg:98.23ms
step:1145/1770 train_time:111493ms step_avg:98.23ms
step:1146/1770 train_time:111596ms step_avg:98.24ms
step:1147/1770 train_time:111698ms step_avg:98.24ms
step:1148/1770 train_time:111799ms step_avg:98.24ms
step:1149/1770 train_time:111901ms step_avg:98.25ms
step:1150/1770 train_time:112002ms step_avg:98.25ms
step:1151/1770 train_time:112105ms step_avg:98.25ms
step:1152/1770 train_time:112207ms step_avg:98.25ms
step:1153/1770 train_time:112308ms step_avg:98.26ms
step:1154/1770 train_time:112409ms step_avg:98.26ms
step:1155/1770 train_time:112511ms step_avg:98.26ms
step:1156/1770 train_time:112613ms step_avg:98.27ms
step:1157/1770 train_time:112715ms step_avg:98.27ms
step:1158/1770 train_time:112818ms step_avg:98.27ms
step:1159/1770 train_time:112919ms step_avg:98.28ms
step:1160/1770 train_time:113020ms step_avg:98.28ms
step:1161/1770 train_time:113121ms step_avg:98.28ms
step:1162/1770 train_time:113223ms step_avg:98.28ms
step:1163/1770 train_time:113325ms step_avg:98.29ms
step:1164/1770 train_time:113427ms step_avg:98.29ms
step:1165/1770 train_time:113529ms step_avg:98.29ms
step:1166/1770 train_time:113631ms step_avg:98.30ms
step:1167/1770 train_time:113732ms step_avg:98.30ms
step:1168/1770 train_time:113834ms step_avg:98.30ms
step:1169/1770 train_time:113937ms step_avg:98.31ms
step:1170/1770 train_time:114038ms step_avg:98.31ms
step:1171/1770 train_time:114139ms step_avg:98.31ms
step:1172/1770 train_time:114240ms step_avg:98.31ms
step:1173/1770 train_time:114342ms step_avg:98.32ms
step:1174/1770 train_time:114443ms step_avg:98.32ms
step:1175/1770 train_time:114545ms step_avg:98.32ms
step:1176/1770 train_time:114646ms step_avg:98.32ms
step:1177/1770 train_time:114748ms step_avg:98.33ms
step:1178/1770 train_time:114849ms step_avg:98.33ms
step:1179/1770 train_time:114951ms step_avg:98.33ms
step:1180/1770 train_time:115053ms step_avg:98.34ms
step:1181/1770 train_time:115156ms step_avg:98.34ms
step:1182/1770 train_time:115258ms step_avg:98.34ms
step:1183/1770 train_time:115360ms step_avg:98.35ms
step:1184/1770 train_time:115464ms step_avg:98.35ms
step:1185/1770 train_time:115566ms step_avg:98.35ms
step:1186/1770 train_time:115669ms step_avg:98.36ms
step:1187/1770 train_time:115775ms step_avg:98.36ms
step:1188/1770 train_time:115877ms step_avg:98.37ms
step:1189/1770 train_time:115979ms step_avg:98.37ms
step:1190/1770 train_time:116081ms step_avg:98.37ms
step:1191/1770 train_time:116184ms step_avg:98.38ms
step:1192/1770 train_time:116286ms step_avg:98.38ms
step:1193/1770 train_time:116389ms step_avg:98.38ms
step:1194/1770 train_time:116492ms step_avg:98.39ms
step:1195/1770 train_time:116595ms step_avg:98.39ms
step:1196/1770 train_time:116700ms step_avg:98.40ms
step:1197/1770 train_time:116803ms step_avg:98.40ms
step:1198/1770 train_time:116906ms step_avg:98.41ms
step:1199/1770 train_time:117010ms step_avg:98.41ms
step:1200/1770 train_time:117112ms step_avg:98.41ms
step:1201/1770 train_time:117214ms step_avg:98.42ms
step:1202/1770 train_time:117317ms step_avg:98.42ms
step:1203/1770 train_time:117419ms step_avg:98.42ms
step:1204/1770 train_time:117521ms step_avg:98.43ms
step:1205/1770 train_time:117624ms step_avg:98.43ms
step:1206/1770 train_time:117727ms step_avg:98.43ms
step:1207/1770 train_time:117829ms step_avg:98.44ms
step:1208/1770 train_time:117931ms step_avg:98.44ms
step:1209/1770 train_time:118034ms step_avg:98.44ms
step:1210/1770 train_time:118136ms step_avg:98.45ms
step:1211/1770 train_time:118240ms step_avg:98.45ms
step:1212/1770 train_time:118345ms step_avg:98.46ms
step:1213/1770 train_time:118447ms step_avg:98.46ms
step:1214/1770 train_time:118549ms step_avg:98.46ms
step:1215/1770 train_time:118652ms step_avg:98.47ms
step:1216/1770 train_time:118758ms step_avg:98.47ms
step:1217/1770 train_time:118860ms step_avg:98.48ms
step:1218/1770 train_time:118962ms step_avg:98.48ms
step:1219/1770 train_time:119066ms step_avg:98.48ms
step:1220/1770 train_time:119169ms step_avg:98.49ms
step:1221/1770 train_time:119272ms step_avg:98.49ms
step:1222/1770 train_time:119376ms step_avg:98.49ms
step:1223/1770 train_time:119478ms step_avg:98.50ms
step:1224/1770 train_time:119581ms step_avg:98.50ms
step:1225/1770 train_time:119684ms step_avg:98.51ms
step:1226/1770 train_time:119787ms step_avg:98.51ms
step:1227/1770 train_time:119892ms step_avg:98.51ms
step:1228/1770 train_time:119997ms step_avg:98.52ms
step:1229/1770 train_time:120099ms step_avg:98.52ms
step:1230/1770 train_time:120201ms step_avg:98.53ms
step:1231/1770 train_time:120304ms step_avg:98.53ms
step:1232/1770 train_time:120407ms step_avg:98.53ms
step:1233/1770 train_time:120509ms step_avg:98.54ms
step:1234/1770 train_time:120611ms step_avg:98.54ms
step:1235/1770 train_time:120715ms step_avg:98.54ms
step:1236/1770 train_time:120818ms step_avg:98.55ms
step:1237/1770 train_time:120920ms step_avg:98.55ms
step:1238/1770 train_time:121023ms step_avg:98.55ms
step:1239/1770 train_time:121126ms step_avg:98.56ms
step:1240/1770 train_time:121228ms step_avg:98.56ms
step:1241/1770 train_time:121333ms step_avg:98.56ms
step:1242/1770 train_time:121435ms step_avg:98.57ms
step:1243/1770 train_time:121538ms step_avg:98.57ms
step:1244/1770 train_time:121640ms step_avg:98.57ms
step:1245/1770 train_time:121742ms step_avg:98.58ms
step:1246/1770 train_time:121845ms step_avg:98.58ms
step:1247/1770 train_time:121948ms step_avg:98.58ms
step:1248/1770 train_time:122051ms step_avg:98.59ms
step:1249/1770 train_time:122155ms step_avg:98.59ms
step:1250/1770 train_time:122257ms step_avg:98.59ms
step:1250/1770 val_loss:3.4278 train_time:122360ms step_avg:98.68ms
step:1251/1770 train_time:122381ms step_avg:98.62ms
step:1252/1770 train_time:122476ms step_avg:98.61ms
step:1253/1770 train_time:122580ms step_avg:98.62ms
step:1254/1770 train_time:122683ms step_avg:98.62ms
step:1255/1770 train_time:122788ms step_avg:98.62ms
step:1256/1770 train_time:122890ms step_avg:98.63ms
step:1257/1770 train_time:122992ms step_avg:98.63ms
step:1258/1770 train_time:123096ms step_avg:98.63ms
step:1259/1770 train_time:123199ms step_avg:98.64ms
step:1260/1770 train_time:123301ms step_avg:98.64ms
step:1261/1770 train_time:123404ms step_avg:98.64ms
step:1262/1770 train_time:123508ms step_avg:98.65ms
step:1263/1770 train_time:123610ms step_avg:98.65ms
step:1264/1770 train_time:123713ms step_avg:98.65ms
step:1265/1770 train_time:123815ms step_avg:98.66ms
step:1266/1770 train_time:123919ms step_avg:98.66ms
step:1267/1770 train_time:124022ms step_avg:98.66ms
step:1268/1770 train_time:124124ms step_avg:98.67ms
step:1269/1770 train_time:124227ms step_avg:98.67ms
step:1270/1770 train_time:124331ms step_avg:98.68ms
step:1271/1770 train_time:124433ms step_avg:98.68ms
step:1272/1770 train_time:124536ms step_avg:98.68ms
step:1273/1770 train_time:124640ms step_avg:98.69ms
step:1274/1770 train_time:124742ms step_avg:98.69ms
step:1275/1770 train_time:124844ms step_avg:98.69ms
step:1276/1770 train_time:124947ms step_avg:98.69ms
step:1277/1770 train_time:125049ms step_avg:98.70ms
step:1278/1770 train_time:125153ms step_avg:98.70ms
step:1279/1770 train_time:125257ms step_avg:98.71ms
step:1280/1770 train_time:125361ms step_avg:98.71ms
step:1281/1770 train_time:125463ms step_avg:98.71ms
step:1282/1770 train_time:125566ms step_avg:98.72ms
step:1283/1770 train_time:125670ms step_avg:98.72ms
step:1284/1770 train_time:125773ms step_avg:98.72ms
step:1285/1770 train_time:125875ms step_avg:98.73ms
step:1286/1770 train_time:125978ms step_avg:98.73ms
step:1287/1770 train_time:126082ms step_avg:98.73ms
step:1288/1770 train_time:126185ms step_avg:98.74ms
step:1289/1770 train_time:126288ms step_avg:98.74ms
step:1290/1770 train_time:126390ms step_avg:98.74ms
step:1291/1770 train_time:126493ms step_avg:98.75ms
step:1292/1770 train_time:126596ms step_avg:98.75ms
step:1293/1770 train_time:126699ms step_avg:98.75ms
step:1294/1770 train_time:126801ms step_avg:98.75ms
step:1295/1770 train_time:126904ms step_avg:98.76ms
step:1296/1770 train_time:127006ms step_avg:98.76ms
step:1297/1770 train_time:127109ms step_avg:98.76ms
step:1298/1770 train_time:127212ms step_avg:98.77ms
step:1299/1770 train_time:127314ms step_avg:98.77ms
step:1300/1770 train_time:127417ms step_avg:98.77ms
step:1301/1770 train_time:127520ms step_avg:98.78ms
step:1302/1770 train_time:127623ms step_avg:98.78ms
step:1303/1770 train_time:127726ms step_avg:98.78ms
step:1304/1770 train_time:127829ms step_avg:98.79ms
step:1305/1770 train_time:127932ms step_avg:98.79ms
step:1306/1770 train_time:128035ms step_avg:98.79ms
step:1307/1770 train_time:128139ms step_avg:98.80ms
step:1308/1770 train_time:128241ms step_avg:98.80ms
step:1309/1770 train_time:128343ms step_avg:98.80ms
step:1310/1770 train_time:128445ms step_avg:98.80ms
step:1311/1770 train_time:128547ms step_avg:98.81ms
step:1312/1770 train_time:128650ms step_avg:98.81ms
step:1313/1770 train_time:128751ms step_avg:98.81ms
step:1314/1770 train_time:128855ms step_avg:98.81ms
step:1315/1770 train_time:128958ms step_avg:98.82ms
step:1316/1770 train_time:129061ms step_avg:98.82ms
step:1317/1770 train_time:129164ms step_avg:98.82ms
step:1318/1770 train_time:129270ms step_avg:98.83ms
step:1319/1770 train_time:129372ms step_avg:98.83ms
step:1320/1770 train_time:129475ms step_avg:98.84ms
step:1321/1770 train_time:129579ms step_avg:98.84ms
step:1322/1770 train_time:129682ms step_avg:98.84ms
step:1323/1770 train_time:129786ms step_avg:98.85ms
step:1324/1770 train_time:129889ms step_avg:98.85ms
step:1325/1770 train_time:129992ms step_avg:98.85ms
step:1326/1770 train_time:130095ms step_avg:98.86ms
step:1327/1770 train_time:130201ms step_avg:98.86ms
step:1328/1770 train_time:130303ms step_avg:98.86ms
step:1329/1770 train_time:130406ms step_avg:98.87ms
step:1330/1770 train_time:130508ms step_avg:98.87ms
step:1331/1770 train_time:130610ms step_avg:98.87ms
step:1332/1770 train_time:130713ms step_avg:98.87ms
step:1333/1770 train_time:130815ms step_avg:98.88ms
step:1334/1770 train_time:130917ms step_avg:98.88ms
step:1335/1770 train_time:131020ms step_avg:98.88ms
step:1336/1770 train_time:131122ms step_avg:98.89ms
step:1337/1770 train_time:131225ms step_avg:98.89ms
step:1338/1770 train_time:131327ms step_avg:98.89ms
step:1339/1770 train_time:131430ms step_avg:98.89ms
step:1340/1770 train_time:131535ms step_avg:98.90ms
step:1341/1770 train_time:131638ms step_avg:98.90ms
step:1342/1770 train_time:131742ms step_avg:98.91ms
step:1343/1770 train_time:131845ms step_avg:98.91ms
step:1344/1770 train_time:131949ms step_avg:98.91ms
step:1345/1770 train_time:132050ms step_avg:98.91ms
step:1346/1770 train_time:132153ms step_avg:98.92ms
step:1347/1770 train_time:132256ms step_avg:98.92ms
step:1348/1770 train_time:132361ms step_avg:98.92ms
step:1349/1770 train_time:132463ms step_avg:98.93ms
step:1350/1770 train_time:132566ms step_avg:98.93ms
step:1351/1770 train_time:132669ms step_avg:98.93ms
step:1352/1770 train_time:132773ms step_avg:98.94ms
step:1353/1770 train_time:132876ms step_avg:98.94ms
step:1354/1770 train_time:132979ms step_avg:98.94ms
step:1355/1770 train_time:133081ms step_avg:98.95ms
step:1356/1770 train_time:133183ms step_avg:98.95ms
step:1357/1770 train_time:133286ms step_avg:98.95ms
step:1358/1770 train_time:133390ms step_avg:98.95ms
step:1359/1770 train_time:133493ms step_avg:98.96ms
step:1360/1770 train_time:133596ms step_avg:98.96ms
step:1361/1770 train_time:133700ms step_avg:98.96ms
step:1362/1770 train_time:133803ms step_avg:98.97ms
step:1363/1770 train_time:133906ms step_avg:98.97ms
step:1364/1770 train_time:134009ms step_avg:98.97ms
step:1365/1770 train_time:134112ms step_avg:98.98ms
step:1366/1770 train_time:134215ms step_avg:98.98ms
step:1367/1770 train_time:134318ms step_avg:98.98ms
step:1368/1770 train_time:134421ms step_avg:98.98ms
step:1369/1770 train_time:134524ms step_avg:98.99ms
step:1370/1770 train_time:134627ms step_avg:98.99ms
step:1371/1770 train_time:134730ms step_avg:98.99ms
step:1372/1770 train_time:134832ms step_avg:99.00ms
step:1373/1770 train_time:134936ms step_avg:99.00ms
step:1374/1770 train_time:135039ms step_avg:99.00ms
step:1375/1770 train_time:135142ms step_avg:99.00ms
step:1375/1770 val_loss:3.3853 train_time:135243ms step_avg:99.08ms
step:1376/1770 train_time:135264ms step_avg:99.02ms
step:1377/1770 train_time:135358ms step_avg:99.02ms
step:1378/1770 train_time:135461ms step_avg:99.02ms
step:1379/1770 train_time:135563ms step_avg:99.02ms
step:1380/1770 train_time:135666ms step_avg:99.03ms
step:1381/1770 train_time:135769ms step_avg:99.03ms
step:1382/1770 train_time:135871ms step_avg:99.03ms
step:1383/1770 train_time:135975ms step_avg:99.04ms
step:1384/1770 train_time:136078ms step_avg:99.04ms
step:1385/1770 train_time:136181ms step_avg:99.04ms
step:1386/1770 train_time:136284ms step_avg:99.04ms
step:1387/1770 train_time:136387ms step_avg:99.05ms
step:1388/1770 train_time:136490ms step_avg:99.05ms
step:1389/1770 train_time:136593ms step_avg:99.05ms
step:1390/1770 train_time:136696ms step_avg:99.06ms
step:1391/1770 train_time:136799ms step_avg:99.06ms
step:1392/1770 train_time:136901ms step_avg:99.06ms
step:1393/1770 train_time:137004ms step_avg:99.06ms
step:1394/1770 train_time:137106ms step_avg:99.07ms
step:1395/1770 train_time:137210ms step_avg:99.07ms
step:1396/1770 train_time:137314ms step_avg:99.07ms
step:1397/1770 train_time:137417ms step_avg:99.07ms
step:1398/1770 train_time:137521ms step_avg:99.08ms
step:1399/1770 train_time:137623ms step_avg:99.08ms
step:1400/1770 train_time:137726ms step_avg:99.08ms
step:1401/1770 train_time:137829ms step_avg:99.09ms
step:1402/1770 train_time:137931ms step_avg:99.09ms
step:1403/1770 train_time:138035ms step_avg:99.09ms
step:1404/1770 train_time:138137ms step_avg:99.09ms
step:1405/1770 train_time:138240ms step_avg:99.10ms
step:1406/1770 train_time:138343ms step_avg:99.10ms
step:1407/1770 train_time:138446ms step_avg:99.10ms
step:1408/1770 train_time:138548ms step_avg:99.10ms
step:1409/1770 train_time:138651ms step_avg:99.11ms
step:1410/1770 train_time:138755ms step_avg:99.11ms
step:1411/1770 train_time:138857ms step_avg:99.11ms
step:1412/1770 train_time:138959ms step_avg:99.12ms
step:1413/1770 train_time:139061ms step_avg:99.12ms
step:1414/1770 train_time:139165ms step_avg:99.12ms
step:1415/1770 train_time:139268ms step_avg:99.12ms
step:1416/1770 train_time:139371ms step_avg:99.13ms
step:1417/1770 train_time:139474ms step_avg:99.13ms
step:1418/1770 train_time:139577ms step_avg:99.13ms
step:1419/1770 train_time:139681ms step_avg:99.13ms
step:1420/1770 train_time:139783ms step_avg:99.14ms
step:1421/1770 train_time:139886ms step_avg:99.14ms
step:1422/1770 train_time:139988ms step_avg:99.14ms
step:1423/1770 train_time:140091ms step_avg:99.14ms
step:1424/1770 train_time:140195ms step_avg:99.15ms
step:1425/1770 train_time:140298ms step_avg:99.15ms
step:1426/1770 train_time:140401ms step_avg:99.15ms
step:1427/1770 train_time:140503ms step_avg:99.16ms
step:1428/1770 train_time:140608ms step_avg:99.16ms
step:1429/1770 train_time:140711ms step_avg:99.16ms
step:1430/1770 train_time:140813ms step_avg:99.16ms
step:1431/1770 train_time:140917ms step_avg:99.17ms
step:1432/1770 train_time:141019ms step_avg:99.17ms
step:1433/1770 train_time:141122ms step_avg:99.17ms
step:1434/1770 train_time:141224ms step_avg:99.17ms
step:1435/1770 train_time:141326ms step_avg:99.18ms
step:1436/1770 train_time:141430ms step_avg:99.18ms
step:1437/1770 train_time:141532ms step_avg:99.18ms
step:1438/1770 train_time:141635ms step_avg:99.18ms
step:1439/1770 train_time:141738ms step_avg:99.19ms
step:1440/1770 train_time:141841ms step_avg:99.19ms
step:1441/1770 train_time:141946ms step_avg:99.19ms
step:1442/1770 train_time:142048ms step_avg:99.20ms
step:1443/1770 train_time:142151ms step_avg:99.20ms
step:1444/1770 train_time:142255ms step_avg:99.20ms
step:1445/1770 train_time:142360ms step_avg:99.21ms
step:1446/1770 train_time:142464ms step_avg:99.21ms
step:1447/1770 train_time:142567ms step_avg:99.21ms
step:1448/1770 train_time:142672ms step_avg:99.22ms
step:1449/1770 train_time:142777ms step_avg:99.22ms
step:1450/1770 train_time:142880ms step_avg:99.22ms
step:1451/1770 train_time:142984ms step_avg:99.23ms
step:1452/1770 train_time:143088ms step_avg:99.23ms
step:1453/1770 train_time:143192ms step_avg:99.23ms
step:1454/1770 train_time:143296ms step_avg:99.24ms
step:1455/1770 train_time:143400ms step_avg:99.24ms
step:1456/1770 train_time:143504ms step_avg:99.24ms
step:1457/1770 train_time:143609ms step_avg:99.25ms
step:1458/1770 train_time:143713ms step_avg:99.25ms
step:1459/1770 train_time:143818ms step_avg:99.25ms
step:1460/1770 train_time:143921ms step_avg:99.26ms
step:1461/1770 train_time:144025ms step_avg:99.26ms
step:1462/1770 train_time:144129ms step_avg:99.26ms
step:1463/1770 train_time:144232ms step_avg:99.27ms
step:1464/1770 train_time:144338ms step_avg:99.27ms
step:1465/1770 train_time:144441ms step_avg:99.27ms
step:1466/1770 train_time:144546ms step_avg:99.28ms
step:1467/1770 train_time:144652ms step_avg:99.28ms
step:1468/1770 train_time:144756ms step_avg:99.28ms
step:1469/1770 train_time:144860ms step_avg:99.29ms
step:1470/1770 train_time:144963ms step_avg:99.29ms
step:1471/1770 train_time:145067ms step_avg:99.29ms
step:1472/1770 train_time:145171ms step_avg:99.30ms
step:1473/1770 train_time:145275ms step_avg:99.30ms
step:1474/1770 train_time:145380ms step_avg:99.30ms
step:1475/1770 train_time:145484ms step_avg:99.31ms
step:1476/1770 train_time:145587ms step_avg:99.31ms
step:1477/1770 train_time:145693ms step_avg:99.31ms
step:1478/1770 train_time:145798ms step_avg:99.32ms
step:1479/1770 train_time:145901ms step_avg:99.32ms
step:1480/1770 train_time:146005ms step_avg:99.32ms
step:1481/1770 train_time:146113ms step_avg:99.33ms
step:1482/1770 train_time:146217ms step_avg:99.33ms
step:1483/1770 train_time:146321ms step_avg:99.34ms
step:1484/1770 train_time:146425ms step_avg:99.34ms
step:1485/1770 train_time:146529ms step_avg:99.34ms
step:1486/1770 train_time:146632ms step_avg:99.34ms
step:1487/1770 train_time:146736ms step_avg:99.35ms
step:1488/1770 train_time:146841ms step_avg:99.35ms
step:1489/1770 train_time:146946ms step_avg:99.36ms
step:1490/1770 train_time:147050ms step_avg:99.36ms
step:1491/1770 train_time:147154ms step_avg:99.36ms
step:1492/1770 train_time:147257ms step_avg:99.36ms
step:1493/1770 train_time:147365ms step_avg:99.37ms
step:1494/1770 train_time:147472ms step_avg:99.37ms
step:1495/1770 train_time:147576ms step_avg:99.38ms
step:1496/1770 train_time:147679ms step_avg:99.38ms
step:1497/1770 train_time:147783ms step_avg:99.38ms
step:1498/1770 train_time:147887ms step_avg:99.39ms
step:1499/1770 train_time:147990ms step_avg:99.39ms
step:1500/1770 train_time:148093ms step_avg:99.39ms
step:1500/1770 val_loss:3.3510 train_time:148195ms step_avg:99.46ms
step:1501/1770 train_time:148216ms step_avg:99.41ms
step:1502/1770 train_time:148309ms step_avg:99.40ms
step:1503/1770 train_time:148415ms step_avg:99.41ms
step:1504/1770 train_time:148519ms step_avg:99.41ms
step:1505/1770 train_time:148625ms step_avg:99.41ms
step:1506/1770 train_time:148728ms step_avg:99.42ms
step:1507/1770 train_time:148833ms step_avg:99.42ms
step:1508/1770 train_time:148938ms step_avg:99.42ms
step:1509/1770 train_time:149042ms step_avg:99.43ms
step:1510/1770 train_time:149145ms step_avg:99.43ms
step:1511/1770 train_time:149250ms step_avg:99.43ms
step:1512/1770 train_time:149356ms step_avg:99.44ms
step:1513/1770 train_time:149460ms step_avg:99.44ms
step:1514/1770 train_time:149564ms step_avg:99.44ms
step:1515/1770 train_time:149669ms step_avg:99.45ms
step:1516/1770 train_time:149773ms step_avg:99.45ms
step:1517/1770 train_time:149876ms step_avg:99.45ms
step:1518/1770 train_time:149983ms step_avg:99.46ms
step:1519/1770 train_time:150085ms step_avg:99.46ms
step:1520/1770 train_time:150190ms step_avg:99.46ms
step:1521/1770 train_time:150294ms step_avg:99.47ms
step:1522/1770 train_time:150397ms step_avg:99.47ms
step:1523/1770 train_time:150503ms step_avg:99.47ms
step:1524/1770 train_time:150607ms step_avg:99.48ms
step:1525/1770 train_time:150710ms step_avg:99.48ms
step:1526/1770 train_time:150814ms step_avg:99.48ms
step:1527/1770 train_time:150918ms step_avg:99.48ms
step:1528/1770 train_time:151023ms step_avg:99.49ms
step:1529/1770 train_time:151127ms step_avg:99.49ms
step:1530/1770 train_time:151231ms step_avg:99.49ms
step:1531/1770 train_time:151335ms step_avg:99.50ms
step:1532/1770 train_time:151438ms step_avg:99.50ms
step:1533/1770 train_time:151542ms step_avg:99.50ms
step:1534/1770 train_time:151647ms step_avg:99.51ms
step:1535/1770 train_time:151751ms step_avg:99.51ms
step:1536/1770 train_time:151855ms step_avg:99.51ms
step:1537/1770 train_time:151959ms step_avg:99.51ms
step:1538/1770 train_time:152064ms step_avg:99.52ms
step:1539/1770 train_time:152168ms step_avg:99.52ms
step:1540/1770 train_time:152275ms step_avg:99.53ms
step:1541/1770 train_time:152380ms step_avg:99.53ms
step:1542/1770 train_time:152484ms step_avg:99.53ms
step:1543/1770 train_time:152587ms step_avg:99.53ms
step:1544/1770 train_time:152694ms step_avg:99.54ms
step:1545/1770 train_time:152798ms step_avg:99.54ms
step:1546/1770 train_time:152903ms step_avg:99.55ms
step:1547/1770 train_time:153006ms step_avg:99.55ms
step:1548/1770 train_time:153110ms step_avg:99.55ms
step:1549/1770 train_time:153215ms step_avg:99.55ms
step:1550/1770 train_time:153319ms step_avg:99.56ms
step:1551/1770 train_time:153423ms step_avg:99.56ms
step:1552/1770 train_time:153528ms step_avg:99.56ms
step:1553/1770 train_time:153632ms step_avg:99.57ms
step:1554/1770 train_time:153735ms step_avg:99.57ms
step:1555/1770 train_time:153840ms step_avg:99.57ms
step:1556/1770 train_time:153943ms step_avg:99.57ms
step:1557/1770 train_time:154046ms step_avg:99.58ms
step:1558/1770 train_time:154150ms step_avg:99.58ms
step:1559/1770 train_time:154254ms step_avg:99.58ms
step:1560/1770 train_time:154357ms step_avg:99.59ms
step:1561/1770 train_time:154464ms step_avg:99.59ms
step:1562/1770 train_time:154567ms step_avg:99.59ms
step:1563/1770 train_time:154672ms step_avg:99.60ms
step:1564/1770 train_time:154775ms step_avg:99.60ms
step:1565/1770 train_time:154879ms step_avg:99.60ms
step:1566/1770 train_time:154982ms step_avg:99.60ms
step:1567/1770 train_time:155086ms step_avg:99.61ms
step:1568/1770 train_time:155189ms step_avg:99.61ms
step:1569/1770 train_time:155296ms step_avg:99.61ms
step:1570/1770 train_time:155400ms step_avg:99.62ms
step:1571/1770 train_time:155504ms step_avg:99.62ms
step:1572/1770 train_time:155609ms step_avg:99.62ms
step:1573/1770 train_time:155715ms step_avg:99.63ms
step:1574/1770 train_time:155819ms step_avg:99.63ms
step:1575/1770 train_time:155922ms step_avg:99.63ms
step:1576/1770 train_time:156026ms step_avg:99.63ms
step:1577/1770 train_time:156132ms step_avg:99.64ms
step:1578/1770 train_time:156237ms step_avg:99.64ms
step:1579/1770 train_time:156341ms step_avg:99.64ms
step:1580/1770 train_time:156445ms step_avg:99.65ms
step:1581/1770 train_time:156551ms step_avg:99.65ms
step:1582/1770 train_time:156656ms step_avg:99.65ms
step:1583/1770 train_time:156760ms step_avg:99.66ms
step:1584/1770 train_time:156865ms step_avg:99.66ms
step:1585/1770 train_time:156969ms step_avg:99.66ms
step:1586/1770 train_time:157077ms step_avg:99.67ms
step:1587/1770 train_time:157182ms step_avg:99.67ms
step:1588/1770 train_time:157287ms step_avg:99.67ms
step:1589/1770 train_time:157393ms step_avg:99.68ms
step:1590/1770 train_time:157497ms step_avg:99.68ms
step:1591/1770 train_time:157600ms step_avg:99.68ms
step:1592/1770 train_time:157705ms step_avg:99.69ms
step:1593/1770 train_time:157808ms step_avg:99.69ms
step:1594/1770 train_time:157912ms step_avg:99.69ms
step:1595/1770 train_time:158016ms step_avg:99.69ms
step:1596/1770 train_time:158121ms step_avg:99.70ms
step:1597/1770 train_time:158224ms step_avg:99.70ms
step:1598/1770 train_time:158328ms step_avg:99.70ms
step:1599/1770 train_time:158433ms step_avg:99.71ms
step:1600/1770 train_time:158539ms step_avg:99.71ms
step:1601/1770 train_time:158643ms step_avg:99.71ms
step:1602/1770 train_time:158749ms step_avg:99.72ms
step:1603/1770 train_time:158853ms step_avg:99.72ms
step:1604/1770 train_time:158956ms step_avg:99.72ms
step:1605/1770 train_time:159058ms step_avg:99.72ms
step:1606/1770 train_time:159163ms step_avg:99.73ms
step:1607/1770 train_time:159270ms step_avg:99.73ms
step:1608/1770 train_time:159374ms step_avg:99.73ms
step:1609/1770 train_time:159479ms step_avg:99.74ms
step:1610/1770 train_time:159584ms step_avg:99.74ms
step:1611/1770 train_time:159690ms step_avg:99.74ms
step:1612/1770 train_time:159795ms step_avg:99.75ms
step:1613/1770 train_time:159899ms step_avg:99.75ms
step:1614/1770 train_time:160004ms step_avg:99.75ms
step:1615/1770 train_time:160108ms step_avg:99.76ms
step:1616/1770 train_time:160212ms step_avg:99.76ms
step:1617/1770 train_time:160319ms step_avg:99.76ms
step:1618/1770 train_time:160423ms step_avg:99.77ms
step:1619/1770 train_time:160528ms step_avg:99.77ms
step:1620/1770 train_time:160632ms step_avg:99.77ms
step:1621/1770 train_time:160736ms step_avg:99.77ms
step:1622/1770 train_time:160841ms step_avg:99.78ms
step:1623/1770 train_time:160947ms step_avg:99.78ms
step:1624/1770 train_time:161051ms step_avg:99.78ms
step:1625/1770 train_time:161154ms step_avg:99.79ms
step:1625/1770 val_loss:3.3187 train_time:161257ms step_avg:99.85ms
step:1626/1770 train_time:161278ms step_avg:99.80ms
step:1627/1770 train_time:161371ms step_avg:99.80ms
step:1628/1770 train_time:161475ms step_avg:99.80ms
step:1629/1770 train_time:161578ms step_avg:99.80ms
step:1630/1770 train_time:161682ms step_avg:99.80ms
step:1631/1770 train_time:161786ms step_avg:99.81ms
step:1632/1770 train_time:161890ms step_avg:99.81ms
step:1633/1770 train_time:161994ms step_avg:99.81ms
step:1634/1770 train_time:162098ms step_avg:99.81ms
step:1635/1770 train_time:162203ms step_avg:99.82ms
step:1636/1770 train_time:162307ms step_avg:99.82ms
step:1637/1770 train_time:162411ms step_avg:99.82ms
step:1638/1770 train_time:162514ms step_avg:99.82ms
step:1639/1770 train_time:162619ms step_avg:99.83ms
step:1640/1770 train_time:162725ms step_avg:99.83ms
step:1641/1770 train_time:162829ms step_avg:99.83ms
step:1642/1770 train_time:162933ms step_avg:99.84ms
step:1643/1770 train_time:163037ms step_avg:99.84ms
step:1644/1770 train_time:163144ms step_avg:99.84ms
step:1645/1770 train_time:163247ms step_avg:99.85ms
step:1646/1770 train_time:163353ms step_avg:99.85ms
step:1647/1770 train_time:163457ms step_avg:99.85ms
step:1648/1770 train_time:163561ms step_avg:99.85ms
step:1649/1770 train_time:163665ms step_avg:99.86ms
step:1650/1770 train_time:163768ms step_avg:99.86ms
step:1651/1770 train_time:163872ms step_avg:99.86ms
step:1652/1770 train_time:163976ms step_avg:99.86ms
step:1653/1770 train_time:164080ms step_avg:99.87ms
step:1654/1770 train_time:164188ms step_avg:99.87ms
step:1655/1770 train_time:164295ms step_avg:99.88ms
step:1656/1770 train_time:164399ms step_avg:99.88ms
step:1657/1770 train_time:164505ms step_avg:99.88ms
step:1658/1770 train_time:164610ms step_avg:99.88ms
step:1659/1770 train_time:164716ms step_avg:99.89ms
step:1660/1770 train_time:164820ms step_avg:99.89ms
step:1661/1770 train_time:164925ms step_avg:99.89ms
step:1662/1770 train_time:165029ms step_avg:99.90ms
step:1663/1770 train_time:165132ms step_avg:99.90ms
step:1664/1770 train_time:165235ms step_avg:99.90ms
step:1665/1770 train_time:165339ms step_avg:99.90ms
step:1666/1770 train_time:165444ms step_avg:99.91ms
step:1667/1770 train_time:165547ms step_avg:99.91ms
step:1668/1770 train_time:165650ms step_avg:99.91ms
step:1669/1770 train_time:165753ms step_avg:99.91ms
step:1670/1770 train_time:165857ms step_avg:99.91ms
step:1671/1770 train_time:165961ms step_avg:99.92ms
step:1672/1770 train_time:166067ms step_avg:99.92ms
step:1673/1770 train_time:166172ms step_avg:99.92ms
step:1674/1770 train_time:166275ms step_avg:99.93ms
step:1675/1770 train_time:166379ms step_avg:99.93ms
step:1676/1770 train_time:166484ms step_avg:99.93ms
step:1677/1770 train_time:166593ms step_avg:99.94ms
step:1678/1770 train_time:166696ms step_avg:99.94ms
step:1679/1770 train_time:166800ms step_avg:99.94ms
step:1680/1770 train_time:166904ms step_avg:99.94ms
step:1681/1770 train_time:167009ms step_avg:99.95ms
step:1682/1770 train_time:167115ms step_avg:99.95ms
step:1683/1770 train_time:167219ms step_avg:99.95ms
step:1684/1770 train_time:167323ms step_avg:99.95ms
step:1685/1770 train_time:167427ms step_avg:99.96ms
step:1686/1770 train_time:167532ms step_avg:99.96ms
step:1687/1770 train_time:167637ms step_avg:99.96ms
step:1688/1770 train_time:167740ms step_avg:99.96ms
step:1689/1770 train_time:167845ms step_avg:99.97ms
step:1690/1770 train_time:167948ms step_avg:99.97ms
step:1691/1770 train_time:168053ms step_avg:99.97ms
step:1692/1770 train_time:168157ms step_avg:99.97ms
step:1693/1770 train_time:168263ms step_avg:99.98ms
step:1694/1770 train_time:168367ms step_avg:99.98ms
step:1695/1770 train_time:168471ms step_avg:99.98ms
step:1696/1770 train_time:168578ms step_avg:99.99ms
step:1697/1770 train_time:168684ms step_avg:99.99ms
step:1698/1770 train_time:168789ms step_avg:99.99ms
step:1699/1770 train_time:168893ms step_avg:100.00ms
step:1700/1770 train_time:168996ms step_avg:100.00ms
step:1701/1770 train_time:169099ms step_avg:100.00ms
step:1702/1770 train_time:169204ms step_avg:100.00ms
step:1703/1770 train_time:169307ms step_avg:100.00ms
step:1704/1770 train_time:169411ms step_avg:100.01ms
step:1705/1770 train_time:169515ms step_avg:100.01ms
step:1706/1770 train_time:169618ms step_avg:100.01ms
step:1707/1770 train_time:169724ms step_avg:100.01ms
step:1708/1770 train_time:169828ms step_avg:100.02ms
step:1709/1770 train_time:169934ms step_avg:100.02ms
step:1710/1770 train_time:170043ms step_avg:100.03ms
step:1711/1770 train_time:170149ms step_avg:100.03ms
step:1712/1770 train_time:170254ms step_avg:100.03ms
step:1713/1770 train_time:170359ms step_avg:100.03ms
step:1714/1770 train_time:170463ms step_avg:100.04ms
step:1715/1770 train_time:170567ms step_avg:100.04ms
step:1716/1770 train_time:170672ms step_avg:100.04ms
step:1717/1770 train_time:170775ms step_avg:100.04ms
step:1718/1770 train_time:170881ms step_avg:100.05ms
step:1719/1770 train_time:170987ms step_avg:100.05ms
step:1720/1770 train_time:171094ms step_avg:100.05ms
step:1721/1770 train_time:171198ms step_avg:100.06ms
step:1722/1770 train_time:171304ms step_avg:100.06ms
step:1723/1770 train_time:171411ms step_avg:100.06ms
step:1724/1770 train_time:171517ms step_avg:100.07ms
step:1725/1770 train_time:171624ms step_avg:100.07ms
step:1726/1770 train_time:171731ms step_avg:100.08ms
step:1727/1770 train_time:171835ms step_avg:100.08ms
step:1728/1770 train_time:171942ms step_avg:100.08ms
step:1729/1770 train_time:172046ms step_avg:100.08ms
step:1730/1770 train_time:172152ms step_avg:100.09ms
step:1731/1770 train_time:172258ms step_avg:100.09ms
step:1732/1770 train_time:172362ms step_avg:100.09ms
step:1733/1770 train_time:172469ms step_avg:100.10ms
step:1734/1770 train_time:172573ms step_avg:100.10ms
step:1735/1770 train_time:172679ms step_avg:100.10ms
step:1736/1770 train_time:172783ms step_avg:100.11ms
step:1737/1770 train_time:172888ms step_avg:100.11ms
step:1738/1770 train_time:172993ms step_avg:100.11ms
step:1739/1770 train_time:173098ms step_avg:100.11ms
step:1740/1770 train_time:173203ms step_avg:100.12ms
step:1741/1770 train_time:173310ms step_avg:100.12ms
step:1742/1770 train_time:173418ms step_avg:100.13ms
step:1743/1770 train_time:173524ms step_avg:100.13ms
step:1744/1770 train_time:173629ms step_avg:100.13ms
step:1745/1770 train_time:173734ms step_avg:100.13ms
step:1746/1770 train_time:173841ms step_avg:100.14ms
step:1747/1770 train_time:173945ms step_avg:100.14ms
step:1748/1770 train_time:174051ms step_avg:100.14ms
step:1749/1770 train_time:174158ms step_avg:100.15ms
step:1750/1770 train_time:174262ms step_avg:100.15ms
step:1750/1770 val_loss:3.2954 train_time:174365ms step_avg:100.21ms
step:1751/1770 train_time:174388ms step_avg:100.17ms
step:1752/1770 train_time:174482ms step_avg:100.16ms
step:1753/1770 train_time:174587ms step_avg:100.16ms
step:1754/1770 train_time:174693ms step_avg:100.17ms
step:1755/1770 train_time:174797ms step_avg:100.17ms
step:1756/1770 train_time:174903ms step_avg:100.17ms
step:1757/1770 train_time:175007ms step_avg:100.18ms
step:1758/1770 train_time:175112ms step_avg:100.18ms
step:1759/1770 train_time:175218ms step_avg:100.18ms
step:1760/1770 train_time:175324ms step_avg:100.18ms
step:1761/1770 train_time:175431ms step_avg:100.19ms
step:1762/1770 train_time:175539ms step_avg:100.19ms
step:1763/1770 train_time:175642ms step_avg:100.20ms
step:1764/1770 train_time:175748ms step_avg:100.20ms
step:1765/1770 train_time:175852ms step_avg:100.20ms
step:1766/1770 train_time:175962ms step_avg:100.21ms
step:1767/1770 train_time:176067ms step_avg:100.21ms
step:1768/1770 train_time:176172ms step_avg:100.21ms
step:1769/1770 train_time:176276ms step_avg:100.21ms
step:1770/1770 train_time:176380ms step_avg:100.22ms
step:1770/1770 val_loss:3.2913 train_time:176486ms step_avg:100.28ms
peak memory allocated: 28840 MiB reserved: 32232 MiB
