import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:54:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24474ms step_avg:nanms
step:2/1770 train_time:24885ms step_avg:nanms
step:3/1770 train_time:24980ms step_avg:nanms
step:4/1770 train_time:25073ms step_avg:nanms
step:5/1770 train_time:25168ms step_avg:nanms
step:6/1770 train_time:25262ms step_avg:nanms
step:7/1770 train_time:25357ms step_avg:nanms
step:8/1770 train_time:25451ms step_avg:nanms
step:9/1770 train_time:25545ms step_avg:nanms
step:10/1770 train_time:25639ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.43ms
step:14/1770 train_time:378ms step_avg:94.60ms
step:15/1770 train_time:473ms step_avg:94.51ms
step:16/1770 train_time:567ms step_avg:94.52ms
step:17/1770 train_time:662ms step_avg:94.52ms
step:18/1770 train_time:756ms step_avg:94.56ms
step:19/1770 train_time:852ms step_avg:94.63ms
step:20/1770 train_time:946ms step_avg:94.59ms
step:21/1770 train_time:1072ms step_avg:97.49ms
step:22/1770 train_time:1166ms step_avg:97.19ms
step:23/1770 train_time:1261ms step_avg:96.97ms
step:24/1770 train_time:1356ms step_avg:96.84ms
step:25/1770 train_time:1450ms step_avg:96.67ms
step:26/1770 train_time:1545ms step_avg:96.55ms
step:27/1770 train_time:1639ms step_avg:96.40ms
step:28/1770 train_time:1734ms step_avg:96.31ms
step:29/1770 train_time:1828ms step_avg:96.22ms
step:30/1770 train_time:1922ms step_avg:96.12ms
step:31/1770 train_time:2018ms step_avg:96.09ms
step:32/1770 train_time:2113ms step_avg:96.04ms
step:33/1770 train_time:2208ms step_avg:96.00ms
step:34/1770 train_time:2303ms step_avg:95.96ms
step:35/1770 train_time:2398ms step_avg:95.91ms
step:36/1770 train_time:2492ms step_avg:95.86ms
step:37/1770 train_time:2587ms step_avg:95.81ms
step:38/1770 train_time:2681ms step_avg:95.75ms
step:39/1770 train_time:2776ms step_avg:95.72ms
step:40/1770 train_time:2870ms step_avg:95.68ms
step:41/1770 train_time:2965ms step_avg:95.64ms
step:42/1770 train_time:3060ms step_avg:95.62ms
step:43/1770 train_time:3155ms step_avg:95.59ms
step:44/1770 train_time:3249ms step_avg:95.56ms
step:45/1770 train_time:3344ms step_avg:95.54ms
step:46/1770 train_time:3438ms step_avg:95.51ms
step:47/1770 train_time:3534ms step_avg:95.50ms
step:48/1770 train_time:3629ms step_avg:95.49ms
step:49/1770 train_time:3723ms step_avg:95.47ms
step:50/1770 train_time:3818ms step_avg:95.44ms
step:51/1770 train_time:3913ms step_avg:95.43ms
step:52/1770 train_time:4007ms step_avg:95.41ms
step:53/1770 train_time:4102ms step_avg:95.39ms
step:54/1770 train_time:4196ms step_avg:95.37ms
step:55/1770 train_time:4291ms step_avg:95.35ms
step:56/1770 train_time:4385ms step_avg:95.33ms
step:57/1770 train_time:4480ms step_avg:95.32ms
step:58/1770 train_time:4575ms step_avg:95.31ms
step:59/1770 train_time:4669ms step_avg:95.29ms
step:60/1770 train_time:4764ms step_avg:95.28ms
step:61/1770 train_time:4859ms step_avg:95.27ms
step:62/1770 train_time:4954ms step_avg:95.26ms
step:63/1770 train_time:5048ms step_avg:95.25ms
step:64/1770 train_time:5143ms step_avg:95.25ms
step:65/1770 train_time:5237ms step_avg:95.23ms
step:66/1770 train_time:5332ms step_avg:95.22ms
step:67/1770 train_time:5427ms step_avg:95.20ms
step:68/1770 train_time:5521ms step_avg:95.19ms
step:69/1770 train_time:5616ms step_avg:95.18ms
step:70/1770 train_time:5711ms step_avg:95.18ms
step:71/1770 train_time:5805ms step_avg:95.17ms
step:72/1770 train_time:5900ms step_avg:95.16ms
step:73/1770 train_time:5995ms step_avg:95.15ms
step:74/1770 train_time:6089ms step_avg:95.15ms
step:75/1770 train_time:6184ms step_avg:95.14ms
step:76/1770 train_time:6279ms step_avg:95.14ms
step:77/1770 train_time:6374ms step_avg:95.14ms
step:78/1770 train_time:6469ms step_avg:95.13ms
step:79/1770 train_time:6564ms step_avg:95.13ms
step:80/1770 train_time:6658ms step_avg:95.11ms
step:81/1770 train_time:6753ms step_avg:95.11ms
step:82/1770 train_time:6847ms step_avg:95.10ms
step:83/1770 train_time:6941ms step_avg:95.09ms
step:84/1770 train_time:7036ms step_avg:95.08ms
step:85/1770 train_time:7131ms step_avg:95.08ms
step:86/1770 train_time:7225ms step_avg:95.07ms
step:87/1770 train_time:7320ms step_avg:95.07ms
step:88/1770 train_time:7415ms step_avg:95.06ms
step:89/1770 train_time:7510ms step_avg:95.06ms
step:90/1770 train_time:7604ms step_avg:95.05ms
step:91/1770 train_time:7698ms step_avg:95.04ms
step:92/1770 train_time:7793ms step_avg:95.04ms
step:93/1770 train_time:7889ms step_avg:95.04ms
step:94/1770 train_time:7988ms step_avg:95.10ms
step:95/1770 train_time:8082ms step_avg:95.09ms
step:96/1770 train_time:8177ms step_avg:95.08ms
step:97/1770 train_time:8272ms step_avg:95.08ms
step:98/1770 train_time:8367ms step_avg:95.08ms
step:99/1770 train_time:8461ms step_avg:95.07ms
step:100/1770 train_time:8556ms step_avg:95.06ms
step:101/1770 train_time:8650ms step_avg:95.06ms
step:102/1770 train_time:8745ms step_avg:95.05ms
step:103/1770 train_time:8839ms step_avg:95.04ms
step:104/1770 train_time:8934ms step_avg:95.04ms
step:105/1770 train_time:9029ms step_avg:95.04ms
step:106/1770 train_time:9123ms step_avg:95.03ms
step:107/1770 train_time:9218ms step_avg:95.03ms
step:108/1770 train_time:9313ms step_avg:95.03ms
step:109/1770 train_time:9408ms step_avg:95.03ms
step:110/1770 train_time:9502ms step_avg:95.02ms
step:111/1770 train_time:9596ms step_avg:95.01ms
step:112/1770 train_time:9692ms step_avg:95.01ms
step:113/1770 train_time:9786ms step_avg:95.01ms
step:114/1770 train_time:9880ms step_avg:95.00ms
step:115/1770 train_time:9975ms step_avg:95.00ms
step:116/1770 train_time:10069ms step_avg:94.99ms
step:117/1770 train_time:10164ms step_avg:94.99ms
step:118/1770 train_time:10258ms step_avg:94.98ms
step:119/1770 train_time:10353ms step_avg:94.98ms
step:120/1770 train_time:10447ms step_avg:94.98ms
step:121/1770 train_time:10541ms step_avg:94.97ms
step:122/1770 train_time:10636ms step_avg:94.96ms
step:123/1770 train_time:10732ms step_avg:94.97ms
step:124/1770 train_time:10825ms step_avg:94.96ms
step:125/1770 train_time:10920ms step_avg:94.95ms
step:125/1770 val_loss:4.6632 train_time:11013ms step_avg:95.77ms
step:126/1770 train_time:11045ms step_avg:95.22ms
step:127/1770 train_time:11122ms step_avg:95.06ms
step:128/1770 train_time:11222ms step_avg:95.10ms
step:129/1770 train_time:11318ms step_avg:95.11ms
step:130/1770 train_time:11413ms step_avg:95.11ms
step:131/1770 train_time:11507ms step_avg:95.10ms
step:132/1770 train_time:11602ms step_avg:95.10ms
step:133/1770 train_time:11697ms step_avg:95.10ms
step:134/1770 train_time:11792ms step_avg:95.10ms
step:135/1770 train_time:11887ms step_avg:95.09ms
step:136/1770 train_time:11982ms step_avg:95.09ms
step:137/1770 train_time:12077ms step_avg:95.09ms
step:138/1770 train_time:12172ms step_avg:95.10ms
step:139/1770 train_time:12268ms step_avg:95.10ms
step:140/1770 train_time:12363ms step_avg:95.10ms
step:141/1770 train_time:12459ms step_avg:95.10ms
step:142/1770 train_time:12554ms step_avg:95.11ms
step:143/1770 train_time:12649ms step_avg:95.10ms
step:144/1770 train_time:12744ms step_avg:95.10ms
step:145/1770 train_time:12839ms step_avg:95.11ms
step:146/1770 train_time:12935ms step_avg:95.11ms
step:147/1770 train_time:13030ms step_avg:95.11ms
step:148/1770 train_time:13124ms step_avg:95.10ms
step:149/1770 train_time:13220ms step_avg:95.11ms
step:150/1770 train_time:13316ms step_avg:95.11ms
step:151/1770 train_time:13411ms step_avg:95.12ms
step:152/1770 train_time:13506ms step_avg:95.11ms
step:153/1770 train_time:13602ms step_avg:95.12ms
step:154/1770 train_time:13697ms step_avg:95.12ms
step:155/1770 train_time:13792ms step_avg:95.11ms
step:156/1770 train_time:13887ms step_avg:95.11ms
step:157/1770 train_time:13982ms step_avg:95.11ms
step:158/1770 train_time:14077ms step_avg:95.12ms
step:159/1770 train_time:14172ms step_avg:95.12ms
step:160/1770 train_time:14268ms step_avg:95.12ms
step:161/1770 train_time:14362ms step_avg:95.11ms
step:162/1770 train_time:14458ms step_avg:95.12ms
step:163/1770 train_time:14554ms step_avg:95.12ms
step:164/1770 train_time:14649ms step_avg:95.12ms
step:165/1770 train_time:14744ms step_avg:95.12ms
step:166/1770 train_time:14840ms step_avg:95.13ms
step:167/1770 train_time:14936ms step_avg:95.13ms
step:168/1770 train_time:15031ms step_avg:95.13ms
step:169/1770 train_time:15126ms step_avg:95.13ms
step:170/1770 train_time:15221ms step_avg:95.13ms
step:171/1770 train_time:15316ms step_avg:95.13ms
step:172/1770 train_time:15412ms step_avg:95.13ms
step:173/1770 train_time:15507ms step_avg:95.13ms
step:174/1770 train_time:15602ms step_avg:95.13ms
step:175/1770 train_time:15697ms step_avg:95.13ms
step:176/1770 train_time:15792ms step_avg:95.13ms
step:177/1770 train_time:15887ms step_avg:95.13ms
step:178/1770 train_time:15982ms step_avg:95.13ms
step:179/1770 train_time:16079ms step_avg:95.14ms
step:180/1770 train_time:16175ms step_avg:95.15ms
step:181/1770 train_time:16270ms step_avg:95.14ms
step:182/1770 train_time:16365ms step_avg:95.15ms
step:183/1770 train_time:16461ms step_avg:95.15ms
step:184/1770 train_time:16557ms step_avg:95.15ms
step:185/1770 train_time:16651ms step_avg:95.15ms
step:186/1770 train_time:16746ms step_avg:95.15ms
step:187/1770 train_time:16841ms step_avg:95.15ms
step:188/1770 train_time:16937ms step_avg:95.15ms
step:189/1770 train_time:17032ms step_avg:95.15ms
step:190/1770 train_time:17127ms step_avg:95.15ms
step:191/1770 train_time:17222ms step_avg:95.15ms
step:192/1770 train_time:17317ms step_avg:95.15ms
step:193/1770 train_time:17412ms step_avg:95.15ms
step:194/1770 train_time:17507ms step_avg:95.15ms
step:195/1770 train_time:17602ms step_avg:95.15ms
step:196/1770 train_time:17698ms step_avg:95.15ms
step:197/1770 train_time:17793ms step_avg:95.15ms
step:198/1770 train_time:17889ms step_avg:95.15ms
step:199/1770 train_time:17984ms step_avg:95.15ms
step:200/1770 train_time:18079ms step_avg:95.15ms
step:201/1770 train_time:18174ms step_avg:95.15ms
step:202/1770 train_time:18269ms step_avg:95.15ms
step:203/1770 train_time:18364ms step_avg:95.15ms
step:204/1770 train_time:18460ms step_avg:95.15ms
step:205/1770 train_time:18555ms step_avg:95.15ms
step:206/1770 train_time:18650ms step_avg:95.15ms
step:207/1770 train_time:18746ms step_avg:95.16ms
step:208/1770 train_time:18841ms step_avg:95.16ms
step:209/1770 train_time:18937ms step_avg:95.16ms
step:210/1770 train_time:19032ms step_avg:95.16ms
step:211/1770 train_time:19127ms step_avg:95.16ms
step:212/1770 train_time:19222ms step_avg:95.16ms
step:213/1770 train_time:19317ms step_avg:95.16ms
step:214/1770 train_time:19413ms step_avg:95.16ms
step:215/1770 train_time:19507ms step_avg:95.16ms
step:216/1770 train_time:19602ms step_avg:95.16ms
step:217/1770 train_time:19698ms step_avg:95.16ms
step:218/1770 train_time:19794ms step_avg:95.16ms
step:219/1770 train_time:19888ms step_avg:95.16ms
step:220/1770 train_time:19984ms step_avg:95.16ms
step:221/1770 train_time:20080ms step_avg:95.17ms
step:222/1770 train_time:20176ms step_avg:95.17ms
step:223/1770 train_time:20271ms step_avg:95.17ms
step:224/1770 train_time:20366ms step_avg:95.17ms
step:225/1770 train_time:20461ms step_avg:95.17ms
step:226/1770 train_time:20557ms step_avg:95.17ms
step:227/1770 train_time:20653ms step_avg:95.17ms
step:228/1770 train_time:20748ms step_avg:95.17ms
step:229/1770 train_time:20843ms step_avg:95.17ms
step:230/1770 train_time:20938ms step_avg:95.17ms
step:231/1770 train_time:21033ms step_avg:95.17ms
step:232/1770 train_time:21128ms step_avg:95.17ms
step:233/1770 train_time:21224ms step_avg:95.17ms
step:234/1770 train_time:21319ms step_avg:95.18ms
step:235/1770 train_time:21415ms step_avg:95.18ms
step:236/1770 train_time:21510ms step_avg:95.18ms
step:237/1770 train_time:21606ms step_avg:95.18ms
step:238/1770 train_time:21701ms step_avg:95.18ms
step:239/1770 train_time:21796ms step_avg:95.18ms
step:240/1770 train_time:21891ms step_avg:95.18ms
step:241/1770 train_time:21986ms step_avg:95.18ms
step:242/1770 train_time:22082ms step_avg:95.18ms
step:243/1770 train_time:22177ms step_avg:95.18ms
step:244/1770 train_time:22272ms step_avg:95.18ms
step:245/1770 train_time:22367ms step_avg:95.18ms
step:246/1770 train_time:22462ms step_avg:95.18ms
step:247/1770 train_time:22557ms step_avg:95.18ms
step:248/1770 train_time:22653ms step_avg:95.18ms
step:249/1770 train_time:22748ms step_avg:95.18ms
step:250/1770 train_time:22844ms step_avg:95.18ms
step:250/1770 val_loss:4.1129 train_time:22938ms step_avg:95.57ms
step:251/1770 train_time:22961ms step_avg:95.27ms
step:252/1770 train_time:23047ms step_avg:95.23ms
step:253/1770 train_time:23146ms step_avg:95.25ms
step:254/1770 train_time:23241ms step_avg:95.25ms
step:255/1770 train_time:23337ms step_avg:95.25ms
step:256/1770 train_time:23433ms step_avg:95.25ms
step:257/1770 train_time:23528ms step_avg:95.25ms
step:258/1770 train_time:23623ms step_avg:95.25ms
step:259/1770 train_time:23718ms step_avg:95.25ms
step:260/1770 train_time:23813ms step_avg:95.25ms
step:261/1770 train_time:23909ms step_avg:95.25ms
step:262/1770 train_time:24004ms step_avg:95.25ms
step:263/1770 train_time:24099ms step_avg:95.25ms
step:264/1770 train_time:24195ms step_avg:95.25ms
step:265/1770 train_time:24290ms step_avg:95.26ms
step:266/1770 train_time:24386ms step_avg:95.26ms
step:267/1770 train_time:24482ms step_avg:95.26ms
step:268/1770 train_time:24577ms step_avg:95.26ms
step:269/1770 train_time:24673ms step_avg:95.26ms
step:270/1770 train_time:24769ms step_avg:95.26ms
step:271/1770 train_time:24864ms step_avg:95.27ms
step:272/1770 train_time:24960ms step_avg:95.27ms
step:273/1770 train_time:25057ms step_avg:95.27ms
step:274/1770 train_time:25153ms step_avg:95.28ms
step:275/1770 train_time:25249ms step_avg:95.28ms
step:276/1770 train_time:25345ms step_avg:95.28ms
step:277/1770 train_time:25440ms step_avg:95.28ms
step:278/1770 train_time:25536ms step_avg:95.28ms
step:279/1770 train_time:25632ms step_avg:95.28ms
step:280/1770 train_time:25727ms step_avg:95.29ms
step:281/1770 train_time:25823ms step_avg:95.29ms
step:282/1770 train_time:25918ms step_avg:95.29ms
step:283/1770 train_time:26015ms step_avg:95.29ms
step:284/1770 train_time:26111ms step_avg:95.30ms
step:285/1770 train_time:26207ms step_avg:95.30ms
step:286/1770 train_time:26303ms step_avg:95.30ms
step:287/1770 train_time:26399ms step_avg:95.30ms
step:288/1770 train_time:26495ms step_avg:95.31ms
step:289/1770 train_time:26590ms step_avg:95.31ms
step:290/1770 train_time:26686ms step_avg:95.31ms
step:291/1770 train_time:26781ms step_avg:95.31ms
step:292/1770 train_time:26877ms step_avg:95.31ms
step:293/1770 train_time:26974ms step_avg:95.31ms
step:294/1770 train_time:27069ms step_avg:95.31ms
step:295/1770 train_time:27165ms step_avg:95.32ms
step:296/1770 train_time:27261ms step_avg:95.32ms
step:297/1770 train_time:27357ms step_avg:95.32ms
step:298/1770 train_time:27454ms step_avg:95.33ms
step:299/1770 train_time:27550ms step_avg:95.33ms
step:300/1770 train_time:27646ms step_avg:95.33ms
step:301/1770 train_time:27742ms step_avg:95.33ms
step:302/1770 train_time:27838ms step_avg:95.33ms
step:303/1770 train_time:27933ms step_avg:95.34ms
step:304/1770 train_time:28029ms step_avg:95.34ms
step:305/1770 train_time:28124ms step_avg:95.34ms
step:306/1770 train_time:28220ms step_avg:95.34ms
step:307/1770 train_time:28315ms step_avg:95.34ms
step:308/1770 train_time:28411ms step_avg:95.34ms
step:309/1770 train_time:28507ms step_avg:95.34ms
step:310/1770 train_time:28603ms step_avg:95.34ms
step:311/1770 train_time:28699ms step_avg:95.34ms
step:312/1770 train_time:28795ms step_avg:95.35ms
step:313/1770 train_time:28890ms step_avg:95.35ms
step:314/1770 train_time:28986ms step_avg:95.35ms
step:315/1770 train_time:29081ms step_avg:95.35ms
step:316/1770 train_time:29177ms step_avg:95.35ms
step:317/1770 train_time:29273ms step_avg:95.35ms
step:318/1770 train_time:29368ms step_avg:95.35ms
step:319/1770 train_time:29464ms step_avg:95.35ms
step:320/1770 train_time:29560ms step_avg:95.36ms
step:321/1770 train_time:29656ms step_avg:95.36ms
step:322/1770 train_time:29752ms step_avg:95.36ms
step:323/1770 train_time:29847ms step_avg:95.36ms
step:324/1770 train_time:29942ms step_avg:95.36ms
step:325/1770 train_time:30038ms step_avg:95.36ms
step:326/1770 train_time:30134ms step_avg:95.36ms
step:327/1770 train_time:30229ms step_avg:95.36ms
step:328/1770 train_time:30324ms step_avg:95.36ms
step:329/1770 train_time:30420ms step_avg:95.36ms
step:330/1770 train_time:30517ms step_avg:95.37ms
step:331/1770 train_time:30613ms step_avg:95.37ms
step:332/1770 train_time:30708ms step_avg:95.37ms
step:333/1770 train_time:30804ms step_avg:95.37ms
step:334/1770 train_time:30900ms step_avg:95.37ms
step:335/1770 train_time:30995ms step_avg:95.37ms
step:336/1770 train_time:31091ms step_avg:95.37ms
step:337/1770 train_time:31187ms step_avg:95.37ms
step:338/1770 train_time:31283ms step_avg:95.38ms
step:339/1770 train_time:31379ms step_avg:95.38ms
step:340/1770 train_time:31475ms step_avg:95.38ms
step:341/1770 train_time:31571ms step_avg:95.38ms
step:342/1770 train_time:31667ms step_avg:95.38ms
step:343/1770 train_time:31763ms step_avg:95.38ms
step:344/1770 train_time:31859ms step_avg:95.38ms
step:345/1770 train_time:31954ms step_avg:95.39ms
step:346/1770 train_time:32049ms step_avg:95.39ms
step:347/1770 train_time:32145ms step_avg:95.38ms
step:348/1770 train_time:32240ms step_avg:95.38ms
step:349/1770 train_time:32335ms step_avg:95.38ms
step:350/1770 train_time:32431ms step_avg:95.39ms
step:351/1770 train_time:32526ms step_avg:95.38ms
step:352/1770 train_time:32621ms step_avg:95.38ms
step:353/1770 train_time:32717ms step_avg:95.38ms
step:354/1770 train_time:32813ms step_avg:95.39ms
step:355/1770 train_time:32908ms step_avg:95.39ms
step:356/1770 train_time:33003ms step_avg:95.39ms
step:357/1770 train_time:33099ms step_avg:95.39ms
step:358/1770 train_time:33195ms step_avg:95.39ms
step:359/1770 train_time:33291ms step_avg:95.39ms
step:360/1770 train_time:33386ms step_avg:95.39ms
step:361/1770 train_time:33481ms step_avg:95.39ms
step:362/1770 train_time:33577ms step_avg:95.39ms
step:363/1770 train_time:33672ms step_avg:95.39ms
step:364/1770 train_time:33768ms step_avg:95.39ms
step:365/1770 train_time:33863ms step_avg:95.39ms
step:366/1770 train_time:33959ms step_avg:95.39ms
step:367/1770 train_time:34054ms step_avg:95.39ms
step:368/1770 train_time:34150ms step_avg:95.39ms
step:369/1770 train_time:34245ms step_avg:95.39ms
step:370/1770 train_time:34340ms step_avg:95.39ms
step:371/1770 train_time:34436ms step_avg:95.39ms
step:372/1770 train_time:34532ms step_avg:95.39ms
step:373/1770 train_time:34628ms step_avg:95.39ms
step:374/1770 train_time:34723ms step_avg:95.39ms
step:375/1770 train_time:34819ms step_avg:95.39ms
step:375/1770 val_loss:3.9035 train_time:34913ms step_avg:95.65ms
step:376/1770 train_time:34935ms step_avg:95.45ms
step:377/1770 train_time:35023ms step_avg:95.43ms
step:378/1770 train_time:35123ms step_avg:95.44ms
step:379/1770 train_time:35221ms step_avg:95.45ms
step:380/1770 train_time:35316ms step_avg:95.45ms
step:381/1770 train_time:35411ms step_avg:95.45ms
step:382/1770 train_time:35507ms step_avg:95.45ms
step:383/1770 train_time:35602ms step_avg:95.45ms
step:384/1770 train_time:35697ms step_avg:95.45ms
step:385/1770 train_time:35793ms step_avg:95.45ms
step:386/1770 train_time:35889ms step_avg:95.45ms
step:387/1770 train_time:35984ms step_avg:95.45ms
step:388/1770 train_time:36080ms step_avg:95.45ms
step:389/1770 train_time:36177ms step_avg:95.45ms
step:390/1770 train_time:36274ms step_avg:95.46ms
step:391/1770 train_time:36370ms step_avg:95.46ms
step:392/1770 train_time:36465ms step_avg:95.46ms
step:393/1770 train_time:36560ms step_avg:95.46ms
step:394/1770 train_time:36656ms step_avg:95.46ms
step:395/1770 train_time:36752ms step_avg:95.46ms
step:396/1770 train_time:36849ms step_avg:95.46ms
step:397/1770 train_time:36946ms step_avg:95.47ms
step:398/1770 train_time:37044ms step_avg:95.47ms
step:399/1770 train_time:37141ms step_avg:95.48ms
step:400/1770 train_time:37239ms step_avg:95.48ms
step:401/1770 train_time:37336ms step_avg:95.49ms
step:402/1770 train_time:37434ms step_avg:95.49ms
step:403/1770 train_time:37531ms step_avg:95.50ms
step:404/1770 train_time:37629ms step_avg:95.50ms
step:405/1770 train_time:37726ms step_avg:95.51ms
step:406/1770 train_time:37823ms step_avg:95.51ms
step:407/1770 train_time:37921ms step_avg:95.52ms
step:408/1770 train_time:38018ms step_avg:95.52ms
step:409/1770 train_time:38116ms step_avg:95.53ms
step:410/1770 train_time:38214ms step_avg:95.54ms
step:411/1770 train_time:38312ms step_avg:95.54ms
step:412/1770 train_time:38409ms step_avg:95.55ms
step:413/1770 train_time:38506ms step_avg:95.55ms
step:414/1770 train_time:38604ms step_avg:95.56ms
step:415/1770 train_time:38702ms step_avg:95.56ms
step:416/1770 train_time:38799ms step_avg:95.56ms
step:417/1770 train_time:38896ms step_avg:95.57ms
step:418/1770 train_time:38994ms step_avg:95.57ms
step:419/1770 train_time:39092ms step_avg:95.58ms
step:420/1770 train_time:39189ms step_avg:95.58ms
step:421/1770 train_time:39287ms step_avg:95.59ms
step:422/1770 train_time:39384ms step_avg:95.59ms
step:423/1770 train_time:39482ms step_avg:95.60ms
step:424/1770 train_time:39579ms step_avg:95.60ms
step:425/1770 train_time:39677ms step_avg:95.61ms
step:426/1770 train_time:39775ms step_avg:95.61ms
step:427/1770 train_time:39872ms step_avg:95.62ms
step:428/1770 train_time:39970ms step_avg:95.62ms
step:429/1770 train_time:40067ms step_avg:95.63ms
step:430/1770 train_time:40164ms step_avg:95.63ms
step:431/1770 train_time:40261ms step_avg:95.63ms
step:432/1770 train_time:40359ms step_avg:95.64ms
step:433/1770 train_time:40457ms step_avg:95.64ms
step:434/1770 train_time:40554ms step_avg:95.65ms
step:435/1770 train_time:40652ms step_avg:95.65ms
step:436/1770 train_time:40750ms step_avg:95.66ms
step:437/1770 train_time:40848ms step_avg:95.66ms
step:438/1770 train_time:40945ms step_avg:95.67ms
step:439/1770 train_time:41042ms step_avg:95.67ms
step:440/1770 train_time:41139ms step_avg:95.67ms
step:441/1770 train_time:41237ms step_avg:95.68ms
step:442/1770 train_time:41335ms step_avg:95.68ms
step:443/1770 train_time:41433ms step_avg:95.69ms
step:444/1770 train_time:41530ms step_avg:95.69ms
step:445/1770 train_time:41627ms step_avg:95.69ms
step:446/1770 train_time:41725ms step_avg:95.70ms
step:447/1770 train_time:41822ms step_avg:95.70ms
step:448/1770 train_time:41919ms step_avg:95.71ms
step:449/1770 train_time:42017ms step_avg:95.71ms
step:450/1770 train_time:42115ms step_avg:95.72ms
step:451/1770 train_time:42213ms step_avg:95.72ms
step:452/1770 train_time:42311ms step_avg:95.73ms
step:453/1770 train_time:42408ms step_avg:95.73ms
step:454/1770 train_time:42505ms step_avg:95.73ms
step:455/1770 train_time:42603ms step_avg:95.74ms
step:456/1770 train_time:42700ms step_avg:95.74ms
step:457/1770 train_time:42797ms step_avg:95.74ms
step:458/1770 train_time:42895ms step_avg:95.75ms
step:459/1770 train_time:42993ms step_avg:95.75ms
step:460/1770 train_time:43092ms step_avg:95.76ms
step:461/1770 train_time:43189ms step_avg:95.76ms
step:462/1770 train_time:43287ms step_avg:95.77ms
step:463/1770 train_time:43384ms step_avg:95.77ms
step:464/1770 train_time:43482ms step_avg:95.77ms
step:465/1770 train_time:43579ms step_avg:95.78ms
step:466/1770 train_time:43677ms step_avg:95.78ms
step:467/1770 train_time:43774ms step_avg:95.79ms
step:468/1770 train_time:43872ms step_avg:95.79ms
step:469/1770 train_time:43970ms step_avg:95.79ms
step:470/1770 train_time:44067ms step_avg:95.80ms
step:471/1770 train_time:44165ms step_avg:95.80ms
step:472/1770 train_time:44262ms step_avg:95.80ms
step:473/1770 train_time:44359ms step_avg:95.81ms
step:474/1770 train_time:44456ms step_avg:95.81ms
step:475/1770 train_time:44554ms step_avg:95.82ms
step:476/1770 train_time:44652ms step_avg:95.82ms
step:477/1770 train_time:44750ms step_avg:95.82ms
step:478/1770 train_time:44848ms step_avg:95.83ms
step:479/1770 train_time:44945ms step_avg:95.83ms
step:480/1770 train_time:45043ms step_avg:95.84ms
step:481/1770 train_time:45140ms step_avg:95.84ms
step:482/1770 train_time:45237ms step_avg:95.84ms
step:483/1770 train_time:45336ms step_avg:95.85ms
step:484/1770 train_time:45434ms step_avg:95.85ms
step:485/1770 train_time:45531ms step_avg:95.86ms
step:486/1770 train_time:45629ms step_avg:95.86ms
step:487/1770 train_time:45727ms step_avg:95.86ms
step:488/1770 train_time:45824ms step_avg:95.87ms
step:489/1770 train_time:45922ms step_avg:95.87ms
step:490/1770 train_time:46019ms step_avg:95.87ms
step:491/1770 train_time:46116ms step_avg:95.88ms
step:492/1770 train_time:46214ms step_avg:95.88ms
step:493/1770 train_time:46312ms step_avg:95.88ms
step:494/1770 train_time:46410ms step_avg:95.89ms
step:495/1770 train_time:46507ms step_avg:95.89ms
step:496/1770 train_time:46604ms step_avg:95.89ms
step:497/1770 train_time:46702ms step_avg:95.90ms
step:498/1770 train_time:46799ms step_avg:95.90ms
step:499/1770 train_time:46896ms step_avg:95.90ms
step:500/1770 train_time:46994ms step_avg:95.91ms
step:500/1770 val_loss:3.7554 train_time:47090ms step_avg:96.10ms
step:501/1770 train_time:47114ms step_avg:95.95ms
step:502/1770 train_time:47202ms step_avg:95.94ms
step:503/1770 train_time:47301ms step_avg:95.95ms
step:504/1770 train_time:47399ms step_avg:95.95ms
step:505/1770 train_time:47497ms step_avg:95.95ms
step:506/1770 train_time:47595ms step_avg:95.96ms
step:507/1770 train_time:47692ms step_avg:95.96ms
step:508/1770 train_time:47790ms step_avg:95.96ms
step:509/1770 train_time:47887ms step_avg:95.97ms
step:510/1770 train_time:47984ms step_avg:95.97ms
step:511/1770 train_time:48081ms step_avg:95.97ms
step:512/1770 train_time:48179ms step_avg:95.97ms
step:513/1770 train_time:48277ms step_avg:95.98ms
step:514/1770 train_time:48375ms step_avg:95.98ms
step:515/1770 train_time:48472ms step_avg:95.98ms
step:516/1770 train_time:48570ms step_avg:95.99ms
step:517/1770 train_time:48667ms step_avg:95.99ms
step:518/1770 train_time:48764ms step_avg:95.99ms
step:519/1770 train_time:48861ms step_avg:95.99ms
step:520/1770 train_time:48959ms step_avg:96.00ms
step:521/1770 train_time:49057ms step_avg:96.00ms
step:522/1770 train_time:49155ms step_avg:96.01ms
step:523/1770 train_time:49253ms step_avg:96.01ms
step:524/1770 train_time:49350ms step_avg:96.01ms
step:525/1770 train_time:49448ms step_avg:96.01ms
step:526/1770 train_time:49545ms step_avg:96.02ms
step:527/1770 train_time:49643ms step_avg:96.02ms
step:528/1770 train_time:49740ms step_avg:96.02ms
step:529/1770 train_time:49839ms step_avg:96.03ms
step:530/1770 train_time:49936ms step_avg:96.03ms
step:531/1770 train_time:50035ms step_avg:96.04ms
step:532/1770 train_time:50133ms step_avg:96.04ms
step:533/1770 train_time:50231ms step_avg:96.04ms
step:534/1770 train_time:50328ms step_avg:96.05ms
step:535/1770 train_time:50426ms step_avg:96.05ms
step:536/1770 train_time:50524ms step_avg:96.05ms
step:537/1770 train_time:50621ms step_avg:96.06ms
step:538/1770 train_time:50720ms step_avg:96.06ms
step:539/1770 train_time:50819ms step_avg:96.07ms
step:540/1770 train_time:50917ms step_avg:96.07ms
step:541/1770 train_time:51015ms step_avg:96.07ms
step:542/1770 train_time:51112ms step_avg:96.08ms
step:543/1770 train_time:51210ms step_avg:96.08ms
step:544/1770 train_time:51307ms step_avg:96.08ms
step:545/1770 train_time:51405ms step_avg:96.08ms
step:546/1770 train_time:51503ms step_avg:96.09ms
step:547/1770 train_time:51601ms step_avg:96.09ms
step:548/1770 train_time:51699ms step_avg:96.10ms
step:549/1770 train_time:51797ms step_avg:96.10ms
step:550/1770 train_time:51896ms step_avg:96.10ms
step:551/1770 train_time:51994ms step_avg:96.11ms
step:552/1770 train_time:52092ms step_avg:96.11ms
step:553/1770 train_time:52190ms step_avg:96.11ms
step:554/1770 train_time:52288ms step_avg:96.12ms
step:555/1770 train_time:52387ms step_avg:96.12ms
step:556/1770 train_time:52484ms step_avg:96.12ms
step:557/1770 train_time:52582ms step_avg:96.13ms
step:558/1770 train_time:52680ms step_avg:96.13ms
step:559/1770 train_time:52778ms step_avg:96.14ms
step:560/1770 train_time:52876ms step_avg:96.14ms
step:561/1770 train_time:52974ms step_avg:96.14ms
step:562/1770 train_time:53071ms step_avg:96.14ms
step:563/1770 train_time:53169ms step_avg:96.15ms
step:564/1770 train_time:53267ms step_avg:96.15ms
step:565/1770 train_time:53365ms step_avg:96.15ms
step:566/1770 train_time:53462ms step_avg:96.16ms
step:567/1770 train_time:53561ms step_avg:96.16ms
step:568/1770 train_time:53659ms step_avg:96.16ms
step:569/1770 train_time:53758ms step_avg:96.17ms
step:570/1770 train_time:53856ms step_avg:96.17ms
step:571/1770 train_time:53954ms step_avg:96.17ms
step:572/1770 train_time:54052ms step_avg:96.18ms
step:573/1770 train_time:54150ms step_avg:96.18ms
step:574/1770 train_time:54248ms step_avg:96.18ms
step:575/1770 train_time:54346ms step_avg:96.19ms
step:576/1770 train_time:54444ms step_avg:96.19ms
step:577/1770 train_time:54542ms step_avg:96.19ms
step:578/1770 train_time:54640ms step_avg:96.20ms
step:579/1770 train_time:54738ms step_avg:96.20ms
step:580/1770 train_time:54836ms step_avg:96.20ms
step:581/1770 train_time:54934ms step_avg:96.21ms
step:582/1770 train_time:55032ms step_avg:96.21ms
step:583/1770 train_time:55130ms step_avg:96.21ms
step:584/1770 train_time:55229ms step_avg:96.22ms
step:585/1770 train_time:55327ms step_avg:96.22ms
step:586/1770 train_time:55424ms step_avg:96.22ms
step:587/1770 train_time:55522ms step_avg:96.22ms
step:588/1770 train_time:55620ms step_avg:96.23ms
step:589/1770 train_time:55718ms step_avg:96.23ms
step:590/1770 train_time:55815ms step_avg:96.23ms
step:591/1770 train_time:55913ms step_avg:96.24ms
step:592/1770 train_time:56011ms step_avg:96.24ms
step:593/1770 train_time:56108ms step_avg:96.24ms
step:594/1770 train_time:56206ms step_avg:96.24ms
step:595/1770 train_time:56304ms step_avg:96.25ms
step:596/1770 train_time:56401ms step_avg:96.25ms
step:597/1770 train_time:56499ms step_avg:96.25ms
step:598/1770 train_time:56597ms step_avg:96.25ms
step:599/1770 train_time:56695ms step_avg:96.26ms
step:600/1770 train_time:56794ms step_avg:96.26ms
step:601/1770 train_time:56891ms step_avg:96.26ms
step:602/1770 train_time:56989ms step_avg:96.26ms
step:603/1770 train_time:57087ms step_avg:96.27ms
step:604/1770 train_time:57184ms step_avg:96.27ms
step:605/1770 train_time:57282ms step_avg:96.27ms
step:606/1770 train_time:57380ms step_avg:96.27ms
step:607/1770 train_time:57479ms step_avg:96.28ms
step:608/1770 train_time:57577ms step_avg:96.28ms
step:609/1770 train_time:57675ms step_avg:96.28ms
step:610/1770 train_time:57773ms step_avg:96.29ms
step:611/1770 train_time:57871ms step_avg:96.29ms
step:612/1770 train_time:57968ms step_avg:96.29ms
step:613/1770 train_time:58066ms step_avg:96.29ms
step:614/1770 train_time:58164ms step_avg:96.30ms
step:615/1770 train_time:58261ms step_avg:96.30ms
step:616/1770 train_time:58359ms step_avg:96.30ms
step:617/1770 train_time:58458ms step_avg:96.31ms
step:618/1770 train_time:58556ms step_avg:96.31ms
step:619/1770 train_time:58655ms step_avg:96.31ms
step:620/1770 train_time:58753ms step_avg:96.32ms
step:621/1770 train_time:58851ms step_avg:96.32ms
step:622/1770 train_time:58949ms step_avg:96.32ms
step:623/1770 train_time:59047ms step_avg:96.32ms
step:624/1770 train_time:59145ms step_avg:96.33ms
step:625/1770 train_time:59242ms step_avg:96.33ms
step:625/1770 val_loss:3.6676 train_time:59339ms step_avg:96.49ms
step:626/1770 train_time:59360ms step_avg:96.36ms
step:627/1770 train_time:59449ms step_avg:96.35ms
step:628/1770 train_time:59550ms step_avg:96.36ms
step:629/1770 train_time:59648ms step_avg:96.36ms
step:630/1770 train_time:59746ms step_avg:96.36ms
step:631/1770 train_time:59843ms step_avg:96.37ms
step:632/1770 train_time:59941ms step_avg:96.37ms
step:633/1770 train_time:60038ms step_avg:96.37ms
step:634/1770 train_time:60137ms step_avg:96.37ms
step:635/1770 train_time:60235ms step_avg:96.38ms
step:636/1770 train_time:60333ms step_avg:96.38ms
step:637/1770 train_time:60431ms step_avg:96.38ms
step:638/1770 train_time:60529ms step_avg:96.38ms
step:639/1770 train_time:60627ms step_avg:96.39ms
step:640/1770 train_time:60725ms step_avg:96.39ms
step:641/1770 train_time:60823ms step_avg:96.39ms
step:642/1770 train_time:60920ms step_avg:96.39ms
step:643/1770 train_time:61018ms step_avg:96.39ms
step:644/1770 train_time:61116ms step_avg:96.40ms
step:645/1770 train_time:61214ms step_avg:96.40ms
step:646/1770 train_time:61312ms step_avg:96.40ms
step:647/1770 train_time:61410ms step_avg:96.40ms
step:648/1770 train_time:61508ms step_avg:96.41ms
step:649/1770 train_time:61605ms step_avg:96.41ms
step:650/1770 train_time:61703ms step_avg:96.41ms
step:651/1770 train_time:61800ms step_avg:96.41ms
step:652/1770 train_time:61898ms step_avg:96.41ms
step:653/1770 train_time:61996ms step_avg:96.42ms
step:654/1770 train_time:62094ms step_avg:96.42ms
step:655/1770 train_time:62193ms step_avg:96.42ms
step:656/1770 train_time:62290ms step_avg:96.42ms
step:657/1770 train_time:62388ms step_avg:96.43ms
step:658/1770 train_time:62488ms step_avg:96.43ms
step:659/1770 train_time:62588ms step_avg:96.44ms
step:660/1770 train_time:62688ms step_avg:96.44ms
step:661/1770 train_time:62787ms step_avg:96.45ms
step:662/1770 train_time:62887ms step_avg:96.45ms
step:663/1770 train_time:62986ms step_avg:96.46ms
step:664/1770 train_time:63086ms step_avg:96.46ms
step:665/1770 train_time:63185ms step_avg:96.47ms
step:666/1770 train_time:63284ms step_avg:96.47ms
step:667/1770 train_time:63383ms step_avg:96.47ms
step:668/1770 train_time:63482ms step_avg:96.48ms
step:669/1770 train_time:63581ms step_avg:96.48ms
step:670/1770 train_time:63681ms step_avg:96.49ms
step:671/1770 train_time:63781ms step_avg:96.49ms
step:672/1770 train_time:63881ms step_avg:96.50ms
step:673/1770 train_time:63981ms step_avg:96.50ms
step:674/1770 train_time:64081ms step_avg:96.51ms
step:675/1770 train_time:64181ms step_avg:96.51ms
step:676/1770 train_time:64281ms step_avg:96.52ms
step:677/1770 train_time:64380ms step_avg:96.52ms
step:678/1770 train_time:64480ms step_avg:96.53ms
step:679/1770 train_time:64579ms step_avg:96.53ms
step:680/1770 train_time:64678ms step_avg:96.53ms
step:681/1770 train_time:64779ms step_avg:96.54ms
step:682/1770 train_time:64880ms step_avg:96.55ms
step:683/1770 train_time:64979ms step_avg:96.55ms
step:684/1770 train_time:65080ms step_avg:96.56ms
step:685/1770 train_time:65179ms step_avg:96.56ms
step:686/1770 train_time:65279ms step_avg:96.57ms
step:687/1770 train_time:65378ms step_avg:96.57ms
step:688/1770 train_time:65478ms step_avg:96.58ms
step:689/1770 train_time:65578ms step_avg:96.58ms
step:690/1770 train_time:65677ms step_avg:96.58ms
step:691/1770 train_time:65777ms step_avg:96.59ms
step:692/1770 train_time:65877ms step_avg:96.59ms
step:693/1770 train_time:65977ms step_avg:96.60ms
step:694/1770 train_time:66077ms step_avg:96.60ms
step:695/1770 train_time:66177ms step_avg:96.61ms
step:696/1770 train_time:66277ms step_avg:96.61ms
step:697/1770 train_time:66377ms step_avg:96.62ms
step:698/1770 train_time:66477ms step_avg:96.62ms
step:699/1770 train_time:66577ms step_avg:96.63ms
step:700/1770 train_time:66677ms step_avg:96.63ms
step:701/1770 train_time:66777ms step_avg:96.64ms
step:702/1770 train_time:66877ms step_avg:96.64ms
step:703/1770 train_time:66977ms step_avg:96.65ms
step:704/1770 train_time:67076ms step_avg:96.65ms
step:705/1770 train_time:67177ms step_avg:96.66ms
step:706/1770 train_time:67277ms step_avg:96.66ms
step:707/1770 train_time:67377ms step_avg:96.67ms
step:708/1770 train_time:67477ms step_avg:96.67ms
step:709/1770 train_time:67577ms step_avg:96.68ms
step:710/1770 train_time:67677ms step_avg:96.68ms
step:711/1770 train_time:67777ms step_avg:96.69ms
step:712/1770 train_time:67878ms step_avg:96.69ms
step:713/1770 train_time:67977ms step_avg:96.70ms
step:714/1770 train_time:68077ms step_avg:96.70ms
step:715/1770 train_time:68177ms step_avg:96.70ms
step:716/1770 train_time:68277ms step_avg:96.71ms
step:717/1770 train_time:68377ms step_avg:96.71ms
step:718/1770 train_time:68477ms step_avg:96.72ms
step:719/1770 train_time:68577ms step_avg:96.72ms
step:720/1770 train_time:68677ms step_avg:96.73ms
step:721/1770 train_time:68777ms step_avg:96.73ms
step:722/1770 train_time:68878ms step_avg:96.74ms
step:723/1770 train_time:68978ms step_avg:96.74ms
step:724/1770 train_time:69077ms step_avg:96.75ms
step:725/1770 train_time:69177ms step_avg:96.75ms
step:726/1770 train_time:69277ms step_avg:96.76ms
step:727/1770 train_time:69377ms step_avg:96.76ms
step:728/1770 train_time:69477ms step_avg:96.76ms
step:729/1770 train_time:69577ms step_avg:96.77ms
step:730/1770 train_time:69677ms step_avg:96.77ms
step:731/1770 train_time:69776ms step_avg:96.78ms
step:732/1770 train_time:69876ms step_avg:96.78ms
step:733/1770 train_time:69976ms step_avg:96.79ms
step:734/1770 train_time:70076ms step_avg:96.79ms
step:735/1770 train_time:70176ms step_avg:96.79ms
step:736/1770 train_time:70276ms step_avg:96.80ms
step:737/1770 train_time:70376ms step_avg:96.80ms
step:738/1770 train_time:70476ms step_avg:96.81ms
step:739/1770 train_time:70576ms step_avg:96.81ms
step:740/1770 train_time:70676ms step_avg:96.82ms
step:741/1770 train_time:70777ms step_avg:96.82ms
step:742/1770 train_time:70877ms step_avg:96.83ms
step:743/1770 train_time:70977ms step_avg:96.83ms
step:744/1770 train_time:71077ms step_avg:96.83ms
step:745/1770 train_time:71177ms step_avg:96.84ms
step:746/1770 train_time:71277ms step_avg:96.84ms
step:747/1770 train_time:71376ms step_avg:96.85ms
step:748/1770 train_time:71476ms step_avg:96.85ms
step:749/1770 train_time:71576ms step_avg:96.85ms
step:750/1770 train_time:71676ms step_avg:96.86ms
step:750/1770 val_loss:3.6040 train_time:71774ms step_avg:96.99ms
step:751/1770 train_time:71796ms step_avg:96.89ms
step:752/1770 train_time:71889ms step_avg:96.89ms
step:753/1770 train_time:71990ms step_avg:96.89ms
step:754/1770 train_time:72090ms step_avg:96.90ms
step:755/1770 train_time:72189ms step_avg:96.90ms
step:756/1770 train_time:72288ms step_avg:96.90ms
step:757/1770 train_time:72387ms step_avg:96.90ms
step:758/1770 train_time:72486ms step_avg:96.91ms
step:759/1770 train_time:72585ms step_avg:96.91ms
step:760/1770 train_time:72685ms step_avg:96.91ms
step:761/1770 train_time:72785ms step_avg:96.92ms
step:762/1770 train_time:72886ms step_avg:96.92ms
step:763/1770 train_time:72987ms step_avg:96.93ms
step:764/1770 train_time:73086ms step_avg:96.93ms
step:765/1770 train_time:73186ms step_avg:96.94ms
step:766/1770 train_time:73285ms step_avg:96.94ms
step:767/1770 train_time:73386ms step_avg:96.94ms
step:768/1770 train_time:73485ms step_avg:96.95ms
step:769/1770 train_time:73585ms step_avg:96.95ms
step:770/1770 train_time:73684ms step_avg:96.95ms
step:771/1770 train_time:73784ms step_avg:96.96ms
step:772/1770 train_time:73884ms step_avg:96.96ms
step:773/1770 train_time:73984ms step_avg:96.96ms
step:774/1770 train_time:74085ms step_avg:96.97ms
step:775/1770 train_time:74185ms step_avg:96.97ms
step:776/1770 train_time:74285ms step_avg:96.98ms
step:777/1770 train_time:74386ms step_avg:96.98ms
step:778/1770 train_time:74486ms step_avg:96.99ms
step:779/1770 train_time:74586ms step_avg:96.99ms
step:780/1770 train_time:74685ms step_avg:96.99ms
step:781/1770 train_time:74785ms step_avg:97.00ms
step:782/1770 train_time:74884ms step_avg:97.00ms
step:783/1770 train_time:74984ms step_avg:97.00ms
step:784/1770 train_time:75084ms step_avg:97.01ms
step:785/1770 train_time:75184ms step_avg:97.01ms
step:786/1770 train_time:75284ms step_avg:97.02ms
step:787/1770 train_time:75385ms step_avg:97.02ms
step:788/1770 train_time:75486ms step_avg:97.03ms
step:789/1770 train_time:75586ms step_avg:97.03ms
step:790/1770 train_time:75686ms step_avg:97.03ms
step:791/1770 train_time:75787ms step_avg:97.04ms
step:792/1770 train_time:75886ms step_avg:97.04ms
step:793/1770 train_time:75986ms step_avg:97.05ms
step:794/1770 train_time:76086ms step_avg:97.05ms
step:795/1770 train_time:76186ms step_avg:97.05ms
step:796/1770 train_time:76287ms step_avg:97.06ms
step:797/1770 train_time:76386ms step_avg:97.06ms
step:798/1770 train_time:76486ms step_avg:97.06ms
step:799/1770 train_time:76587ms step_avg:97.07ms
step:800/1770 train_time:76686ms step_avg:97.07ms
step:801/1770 train_time:76786ms step_avg:97.07ms
step:802/1770 train_time:76886ms step_avg:97.08ms
step:803/1770 train_time:76986ms step_avg:97.08ms
step:804/1770 train_time:77085ms step_avg:97.08ms
step:805/1770 train_time:77185ms step_avg:97.09ms
step:806/1770 train_time:77285ms step_avg:97.09ms
step:807/1770 train_time:77386ms step_avg:97.10ms
step:808/1770 train_time:77485ms step_avg:97.10ms
step:809/1770 train_time:77586ms step_avg:97.10ms
step:810/1770 train_time:77686ms step_avg:97.11ms
step:811/1770 train_time:77786ms step_avg:97.11ms
step:812/1770 train_time:77886ms step_avg:97.11ms
step:813/1770 train_time:77986ms step_avg:97.12ms
step:814/1770 train_time:78086ms step_avg:97.12ms
step:815/1770 train_time:78186ms step_avg:97.13ms
step:816/1770 train_time:78286ms step_avg:97.13ms
step:817/1770 train_time:78385ms step_avg:97.13ms
step:818/1770 train_time:78486ms step_avg:97.14ms
step:819/1770 train_time:78586ms step_avg:97.14ms
step:820/1770 train_time:78685ms step_avg:97.14ms
step:821/1770 train_time:78786ms step_avg:97.15ms
step:822/1770 train_time:78885ms step_avg:97.15ms
step:823/1770 train_time:78985ms step_avg:97.15ms
step:824/1770 train_time:79085ms step_avg:97.16ms
step:825/1770 train_time:79186ms step_avg:97.16ms
step:826/1770 train_time:79285ms step_avg:97.16ms
step:827/1770 train_time:79385ms step_avg:97.17ms
step:828/1770 train_time:79485ms step_avg:97.17ms
step:829/1770 train_time:79585ms step_avg:97.17ms
step:830/1770 train_time:79685ms step_avg:97.18ms
step:831/1770 train_time:79785ms step_avg:97.18ms
step:832/1770 train_time:79885ms step_avg:97.18ms
step:833/1770 train_time:79985ms step_avg:97.19ms
step:834/1770 train_time:80085ms step_avg:97.19ms
step:835/1770 train_time:80186ms step_avg:97.19ms
step:836/1770 train_time:80286ms step_avg:97.20ms
step:837/1770 train_time:80385ms step_avg:97.20ms
step:838/1770 train_time:80485ms step_avg:97.20ms
step:839/1770 train_time:80585ms step_avg:97.21ms
step:840/1770 train_time:80685ms step_avg:97.21ms
step:841/1770 train_time:80786ms step_avg:97.21ms
step:842/1770 train_time:80885ms step_avg:97.22ms
step:843/1770 train_time:80985ms step_avg:97.22ms
step:844/1770 train_time:81085ms step_avg:97.22ms
step:845/1770 train_time:81186ms step_avg:97.23ms
step:846/1770 train_time:81286ms step_avg:97.23ms
step:847/1770 train_time:81386ms step_avg:97.24ms
step:848/1770 train_time:81485ms step_avg:97.24ms
step:849/1770 train_time:81585ms step_avg:97.24ms
step:850/1770 train_time:81685ms step_avg:97.24ms
step:851/1770 train_time:81785ms step_avg:97.25ms
step:852/1770 train_time:81885ms step_avg:97.25ms
step:853/1770 train_time:81985ms step_avg:97.25ms
step:854/1770 train_time:82085ms step_avg:97.26ms
step:855/1770 train_time:82185ms step_avg:97.26ms
step:856/1770 train_time:82285ms step_avg:97.26ms
step:857/1770 train_time:82385ms step_avg:97.27ms
step:858/1770 train_time:82485ms step_avg:97.27ms
step:859/1770 train_time:82585ms step_avg:97.27ms
step:860/1770 train_time:82685ms step_avg:97.28ms
step:861/1770 train_time:82785ms step_avg:97.28ms
step:862/1770 train_time:82885ms step_avg:97.28ms
step:863/1770 train_time:82986ms step_avg:97.29ms
step:864/1770 train_time:83086ms step_avg:97.29ms
step:865/1770 train_time:83185ms step_avg:97.29ms
step:866/1770 train_time:83286ms step_avg:97.30ms
step:867/1770 train_time:83386ms step_avg:97.30ms
step:868/1770 train_time:83486ms step_avg:97.30ms
step:869/1770 train_time:83586ms step_avg:97.31ms
step:870/1770 train_time:83686ms step_avg:97.31ms
step:871/1770 train_time:83785ms step_avg:97.31ms
step:872/1770 train_time:83886ms step_avg:97.32ms
step:873/1770 train_time:83986ms step_avg:97.32ms
step:874/1770 train_time:84086ms step_avg:97.32ms
step:875/1770 train_time:84186ms step_avg:97.33ms
step:875/1770 val_loss:3.5542 train_time:84284ms step_avg:97.44ms
step:876/1770 train_time:84306ms step_avg:97.35ms
step:877/1770 train_time:84397ms step_avg:97.34ms
step:878/1770 train_time:84499ms step_avg:97.35ms
step:879/1770 train_time:84598ms step_avg:97.35ms
step:880/1770 train_time:84698ms step_avg:97.35ms
step:881/1770 train_time:84798ms step_avg:97.36ms
step:882/1770 train_time:84898ms step_avg:97.36ms
step:883/1770 train_time:84997ms step_avg:97.36ms
step:884/1770 train_time:85096ms step_avg:97.36ms
step:885/1770 train_time:85196ms step_avg:97.37ms
step:886/1770 train_time:85295ms step_avg:97.37ms
step:887/1770 train_time:85395ms step_avg:97.37ms
step:888/1770 train_time:85495ms step_avg:97.37ms
step:889/1770 train_time:85594ms step_avg:97.38ms
step:890/1770 train_time:85695ms step_avg:97.38ms
step:891/1770 train_time:85795ms step_avg:97.38ms
step:892/1770 train_time:85895ms step_avg:97.39ms
step:893/1770 train_time:85994ms step_avg:97.39ms
step:894/1770 train_time:86094ms step_avg:97.39ms
step:895/1770 train_time:86194ms step_avg:97.39ms
step:896/1770 train_time:86294ms step_avg:97.40ms
step:897/1770 train_time:86393ms step_avg:97.40ms
step:898/1770 train_time:86492ms step_avg:97.40ms
step:899/1770 train_time:86592ms step_avg:97.40ms
step:900/1770 train_time:86693ms step_avg:97.41ms
step:901/1770 train_time:86793ms step_avg:97.41ms
step:902/1770 train_time:86893ms step_avg:97.41ms
step:903/1770 train_time:86993ms step_avg:97.42ms
step:904/1770 train_time:87092ms step_avg:97.42ms
step:905/1770 train_time:87192ms step_avg:97.42ms
step:906/1770 train_time:87292ms step_avg:97.42ms
step:907/1770 train_time:87391ms step_avg:97.43ms
step:908/1770 train_time:87491ms step_avg:97.43ms
step:909/1770 train_time:87591ms step_avg:97.43ms
step:910/1770 train_time:87691ms step_avg:97.43ms
step:911/1770 train_time:87791ms step_avg:97.44ms
step:912/1770 train_time:87891ms step_avg:97.44ms
step:913/1770 train_time:87991ms step_avg:97.44ms
step:914/1770 train_time:88090ms step_avg:97.45ms
step:915/1770 train_time:88190ms step_avg:97.45ms
step:916/1770 train_time:88290ms step_avg:97.45ms
step:917/1770 train_time:88389ms step_avg:97.45ms
step:918/1770 train_time:88489ms step_avg:97.45ms
step:919/1770 train_time:88589ms step_avg:97.46ms
step:920/1770 train_time:88691ms step_avg:97.46ms
step:921/1770 train_time:88793ms step_avg:97.47ms
step:922/1770 train_time:88894ms step_avg:97.47ms
step:923/1770 train_time:88994ms step_avg:97.47ms
step:924/1770 train_time:89095ms step_avg:97.48ms
step:925/1770 train_time:89196ms step_avg:97.48ms
step:926/1770 train_time:89297ms step_avg:97.49ms
step:927/1770 train_time:89398ms step_avg:97.49ms
step:928/1770 train_time:89499ms step_avg:97.49ms
step:929/1770 train_time:89599ms step_avg:97.50ms
step:930/1770 train_time:89700ms step_avg:97.50ms
step:931/1770 train_time:89801ms step_avg:97.50ms
step:932/1770 train_time:89903ms step_avg:97.51ms
step:933/1770 train_time:90004ms step_avg:97.51ms
step:934/1770 train_time:90106ms step_avg:97.52ms
step:935/1770 train_time:90208ms step_avg:97.52ms
step:936/1770 train_time:90310ms step_avg:97.53ms
step:937/1770 train_time:90412ms step_avg:97.53ms
step:938/1770 train_time:90513ms step_avg:97.54ms
step:939/1770 train_time:90614ms step_avg:97.54ms
step:940/1770 train_time:90715ms step_avg:97.54ms
step:941/1770 train_time:90816ms step_avg:97.55ms
step:942/1770 train_time:90918ms step_avg:97.55ms
step:943/1770 train_time:91019ms step_avg:97.56ms
step:944/1770 train_time:91120ms step_avg:97.56ms
step:945/1770 train_time:91221ms step_avg:97.56ms
step:946/1770 train_time:91323ms step_avg:97.57ms
step:947/1770 train_time:91425ms step_avg:97.57ms
step:948/1770 train_time:91526ms step_avg:97.58ms
step:949/1770 train_time:91630ms step_avg:97.58ms
step:950/1770 train_time:91732ms step_avg:97.59ms
step:951/1770 train_time:91834ms step_avg:97.59ms
step:952/1770 train_time:91934ms step_avg:97.59ms
step:953/1770 train_time:92036ms step_avg:97.60ms
step:954/1770 train_time:92137ms step_avg:97.60ms
step:955/1770 train_time:92238ms step_avg:97.61ms
step:956/1770 train_time:92338ms step_avg:97.61ms
step:957/1770 train_time:92439ms step_avg:97.61ms
step:958/1770 train_time:92540ms step_avg:97.62ms
step:959/1770 train_time:92642ms step_avg:97.62ms
step:960/1770 train_time:92743ms step_avg:97.62ms
step:961/1770 train_time:92844ms step_avg:97.63ms
step:962/1770 train_time:92946ms step_avg:97.63ms
step:963/1770 train_time:93048ms step_avg:97.64ms
step:964/1770 train_time:93150ms step_avg:97.64ms
step:965/1770 train_time:93252ms step_avg:97.65ms
step:966/1770 train_time:93353ms step_avg:97.65ms
step:967/1770 train_time:93455ms step_avg:97.65ms
step:968/1770 train_time:93557ms step_avg:97.66ms
step:969/1770 train_time:93658ms step_avg:97.66ms
step:970/1770 train_time:93758ms step_avg:97.66ms
step:971/1770 train_time:93859ms step_avg:97.67ms
step:972/1770 train_time:93960ms step_avg:97.67ms
step:973/1770 train_time:94061ms step_avg:97.67ms
step:974/1770 train_time:94162ms step_avg:97.68ms
step:975/1770 train_time:94264ms step_avg:97.68ms
step:976/1770 train_time:94366ms step_avg:97.69ms
step:977/1770 train_time:94468ms step_avg:97.69ms
step:978/1770 train_time:94570ms step_avg:97.70ms
step:979/1770 train_time:94672ms step_avg:97.70ms
step:980/1770 train_time:94772ms step_avg:97.70ms
step:981/1770 train_time:94873ms step_avg:97.71ms
step:982/1770 train_time:94974ms step_avg:97.71ms
step:983/1770 train_time:95076ms step_avg:97.71ms
step:984/1770 train_time:95177ms step_avg:97.72ms
step:985/1770 train_time:95278ms step_avg:97.72ms
step:986/1770 train_time:95379ms step_avg:97.72ms
step:987/1770 train_time:95480ms step_avg:97.73ms
step:988/1770 train_time:95581ms step_avg:97.73ms
step:989/1770 train_time:95684ms step_avg:97.74ms
step:990/1770 train_time:95785ms step_avg:97.74ms
step:991/1770 train_time:95888ms step_avg:97.75ms
step:992/1770 train_time:95991ms step_avg:97.75ms
step:993/1770 train_time:96093ms step_avg:97.75ms
step:994/1770 train_time:96194ms step_avg:97.76ms
step:995/1770 train_time:96295ms step_avg:97.76ms
step:996/1770 train_time:96396ms step_avg:97.76ms
step:997/1770 train_time:96496ms step_avg:97.77ms
step:998/1770 train_time:96597ms step_avg:97.77ms
step:999/1770 train_time:96697ms step_avg:97.77ms
step:1000/1770 train_time:96799ms step_avg:97.78ms
step:1000/1770 val_loss:3.5164 train_time:96898ms step_avg:97.88ms
step:1001/1770 train_time:96920ms step_avg:97.80ms
step:1002/1770 train_time:97012ms step_avg:97.79ms
step:1003/1770 train_time:97117ms step_avg:97.80ms
step:1004/1770 train_time:97218ms step_avg:97.80ms
step:1005/1770 train_time:97319ms step_avg:97.81ms
step:1006/1770 train_time:97420ms step_avg:97.81ms
step:1007/1770 train_time:97521ms step_avg:97.81ms
step:1008/1770 train_time:97622ms step_avg:97.82ms
step:1009/1770 train_time:97723ms step_avg:97.82ms
step:1010/1770 train_time:97823ms step_avg:97.82ms
step:1011/1770 train_time:97925ms step_avg:97.83ms
step:1012/1770 train_time:98027ms step_avg:97.83ms
step:1013/1770 train_time:98128ms step_avg:97.83ms
step:1014/1770 train_time:98230ms step_avg:97.84ms
step:1015/1770 train_time:98331ms step_avg:97.84ms
step:1016/1770 train_time:98433ms step_avg:97.85ms
step:1017/1770 train_time:98535ms step_avg:97.85ms
step:1018/1770 train_time:98637ms step_avg:97.85ms
step:1019/1770 train_time:98738ms step_avg:97.86ms
step:1020/1770 train_time:98839ms step_avg:97.86ms
step:1021/1770 train_time:98940ms step_avg:97.86ms
step:1022/1770 train_time:99041ms step_avg:97.87ms
step:1023/1770 train_time:99143ms step_avg:97.87ms
step:1024/1770 train_time:99245ms step_avg:97.87ms
step:1025/1770 train_time:99346ms step_avg:97.88ms
step:1026/1770 train_time:99447ms step_avg:97.88ms
step:1027/1770 train_time:99548ms step_avg:97.88ms
step:1028/1770 train_time:99649ms step_avg:97.89ms
step:1029/1770 train_time:99750ms step_avg:97.89ms
step:1030/1770 train_time:99852ms step_avg:97.89ms
step:1031/1770 train_time:99954ms step_avg:97.90ms
step:1032/1770 train_time:100057ms step_avg:97.90ms
step:1033/1770 train_time:100159ms step_avg:97.91ms
step:1034/1770 train_time:100260ms step_avg:97.91ms
step:1035/1770 train_time:100361ms step_avg:97.91ms
step:1036/1770 train_time:100462ms step_avg:97.92ms
step:1037/1770 train_time:100562ms step_avg:97.92ms
step:1038/1770 train_time:100663ms step_avg:97.92ms
step:1039/1770 train_time:100764ms step_avg:97.92ms
step:1040/1770 train_time:100865ms step_avg:97.93ms
step:1041/1770 train_time:100966ms step_avg:97.93ms
step:1042/1770 train_time:101067ms step_avg:97.93ms
step:1043/1770 train_time:101168ms step_avg:97.94ms
step:1044/1770 train_time:101269ms step_avg:97.94ms
step:1045/1770 train_time:101370ms step_avg:97.94ms
step:1046/1770 train_time:101472ms step_avg:97.95ms
step:1047/1770 train_time:101574ms step_avg:97.95ms
step:1048/1770 train_time:101676ms step_avg:97.95ms
step:1049/1770 train_time:101777ms step_avg:97.96ms
step:1050/1770 train_time:101879ms step_avg:97.96ms
step:1051/1770 train_time:101980ms step_avg:97.96ms
step:1052/1770 train_time:102082ms step_avg:97.97ms
step:1053/1770 train_time:102183ms step_avg:97.97ms
step:1054/1770 train_time:102284ms step_avg:97.97ms
step:1055/1770 train_time:102385ms step_avg:97.98ms
step:1056/1770 train_time:102486ms step_avg:97.98ms
step:1057/1770 train_time:102587ms step_avg:97.98ms
step:1058/1770 train_time:102689ms step_avg:97.99ms
step:1059/1770 train_time:102790ms step_avg:97.99ms
step:1060/1770 train_time:102893ms step_avg:97.99ms
step:1061/1770 train_time:102996ms step_avg:98.00ms
step:1062/1770 train_time:103099ms step_avg:98.00ms
step:1063/1770 train_time:103201ms step_avg:98.01ms
step:1064/1770 train_time:103303ms step_avg:98.01ms
step:1065/1770 train_time:103404ms step_avg:98.01ms
step:1066/1770 train_time:103506ms step_avg:98.02ms
step:1067/1770 train_time:103607ms step_avg:98.02ms
step:1068/1770 train_time:103709ms step_avg:98.02ms
step:1069/1770 train_time:103810ms step_avg:98.03ms
step:1070/1770 train_time:103911ms step_avg:98.03ms
step:1071/1770 train_time:104013ms step_avg:98.03ms
step:1072/1770 train_time:104115ms step_avg:98.04ms
step:1073/1770 train_time:104217ms step_avg:98.04ms
step:1074/1770 train_time:104319ms step_avg:98.04ms
step:1075/1770 train_time:104421ms step_avg:98.05ms
step:1076/1770 train_time:104522ms step_avg:98.05ms
step:1077/1770 train_time:104623ms step_avg:98.05ms
step:1078/1770 train_time:104724ms step_avg:98.06ms
step:1079/1770 train_time:104825ms step_avg:98.06ms
step:1080/1770 train_time:104925ms step_avg:98.06ms
step:1081/1770 train_time:105027ms step_avg:98.06ms
step:1082/1770 train_time:105128ms step_avg:98.07ms
step:1083/1770 train_time:105230ms step_avg:98.07ms
step:1084/1770 train_time:105332ms step_avg:98.07ms
step:1085/1770 train_time:105435ms step_avg:98.08ms
step:1086/1770 train_time:105537ms step_avg:98.08ms
step:1087/1770 train_time:105639ms step_avg:98.09ms
step:1088/1770 train_time:105741ms step_avg:98.09ms
step:1089/1770 train_time:105842ms step_avg:98.09ms
step:1090/1770 train_time:105943ms step_avg:98.10ms
step:1091/1770 train_time:106045ms step_avg:98.10ms
step:1092/1770 train_time:106146ms step_avg:98.10ms
step:1093/1770 train_time:106247ms step_avg:98.10ms
step:1094/1770 train_time:106349ms step_avg:98.11ms
step:1095/1770 train_time:106451ms step_avg:98.11ms
step:1096/1770 train_time:106552ms step_avg:98.11ms
step:1097/1770 train_time:106653ms step_avg:98.12ms
step:1098/1770 train_time:106755ms step_avg:98.12ms
step:1099/1770 train_time:106857ms step_avg:98.12ms
step:1100/1770 train_time:106959ms step_avg:98.13ms
step:1101/1770 train_time:107060ms step_avg:98.13ms
step:1102/1770 train_time:107162ms step_avg:98.13ms
step:1103/1770 train_time:107262ms step_avg:98.14ms
step:1104/1770 train_time:107364ms step_avg:98.14ms
step:1105/1770 train_time:107465ms step_avg:98.14ms
step:1106/1770 train_time:107566ms step_avg:98.14ms
step:1107/1770 train_time:107667ms step_avg:98.15ms
step:1108/1770 train_time:107769ms step_avg:98.15ms
step:1109/1770 train_time:107872ms step_avg:98.15ms
step:1110/1770 train_time:107974ms step_avg:98.16ms
step:1111/1770 train_time:108076ms step_avg:98.16ms
step:1112/1770 train_time:108178ms step_avg:98.17ms
step:1113/1770 train_time:108279ms step_avg:98.17ms
step:1114/1770 train_time:108381ms step_avg:98.17ms
step:1115/1770 train_time:108482ms step_avg:98.17ms
step:1116/1770 train_time:108583ms step_avg:98.18ms
step:1117/1770 train_time:108684ms step_avg:98.18ms
step:1118/1770 train_time:108785ms step_avg:98.18ms
step:1119/1770 train_time:108886ms step_avg:98.18ms
step:1120/1770 train_time:108987ms step_avg:98.19ms
step:1121/1770 train_time:109088ms step_avg:98.19ms
step:1122/1770 train_time:109190ms step_avg:98.19ms
step:1123/1770 train_time:109292ms step_avg:98.20ms
step:1124/1770 train_time:109395ms step_avg:98.20ms
step:1125/1770 train_time:109497ms step_avg:98.20ms
step:1125/1770 val_loss:3.4775 train_time:109597ms step_avg:98.29ms
step:1126/1770 train_time:109618ms step_avg:98.22ms
step:1127/1770 train_time:109708ms step_avg:98.22ms
step:1128/1770 train_time:109811ms step_avg:98.22ms
step:1129/1770 train_time:109913ms step_avg:98.22ms
step:1130/1770 train_time:110016ms step_avg:98.23ms
step:1131/1770 train_time:110117ms step_avg:98.23ms
step:1132/1770 train_time:110219ms step_avg:98.23ms
step:1133/1770 train_time:110320ms step_avg:98.24ms
step:1134/1770 train_time:110421ms step_avg:98.24ms
step:1135/1770 train_time:110522ms step_avg:98.24ms
step:1136/1770 train_time:110624ms step_avg:98.24ms
step:1137/1770 train_time:110725ms step_avg:98.25ms
step:1138/1770 train_time:110826ms step_avg:98.25ms
step:1139/1770 train_time:110927ms step_avg:98.25ms
step:1140/1770 train_time:111029ms step_avg:98.26ms
step:1141/1770 train_time:111131ms step_avg:98.26ms
step:1142/1770 train_time:111233ms step_avg:98.26ms
step:1143/1770 train_time:111336ms step_avg:98.27ms
step:1144/1770 train_time:111437ms step_avg:98.27ms
step:1145/1770 train_time:111539ms step_avg:98.27ms
step:1146/1770 train_time:111641ms step_avg:98.28ms
step:1147/1770 train_time:111743ms step_avg:98.28ms
step:1148/1770 train_time:111844ms step_avg:98.28ms
step:1149/1770 train_time:111944ms step_avg:98.28ms
step:1150/1770 train_time:112045ms step_avg:98.29ms
step:1151/1770 train_time:112148ms step_avg:98.29ms
step:1152/1770 train_time:112250ms step_avg:98.29ms
step:1153/1770 train_time:112351ms step_avg:98.30ms
step:1154/1770 train_time:112454ms step_avg:98.30ms
step:1155/1770 train_time:112556ms step_avg:98.30ms
step:1156/1770 train_time:112657ms step_avg:98.30ms
step:1157/1770 train_time:112760ms step_avg:98.31ms
step:1158/1770 train_time:112862ms step_avg:98.31ms
step:1159/1770 train_time:112962ms step_avg:98.31ms
step:1160/1770 train_time:113063ms step_avg:98.32ms
step:1161/1770 train_time:113164ms step_avg:98.32ms
step:1162/1770 train_time:113267ms step_avg:98.32ms
step:1163/1770 train_time:113368ms step_avg:98.32ms
step:1164/1770 train_time:113469ms step_avg:98.33ms
step:1165/1770 train_time:113572ms step_avg:98.33ms
step:1166/1770 train_time:113674ms step_avg:98.33ms
step:1167/1770 train_time:113775ms step_avg:98.34ms
step:1168/1770 train_time:113877ms step_avg:98.34ms
step:1169/1770 train_time:113978ms step_avg:98.34ms
step:1170/1770 train_time:114079ms step_avg:98.34ms
step:1171/1770 train_time:114182ms step_avg:98.35ms
step:1172/1770 train_time:114283ms step_avg:98.35ms
step:1173/1770 train_time:114385ms step_avg:98.35ms
step:1174/1770 train_time:114486ms step_avg:98.36ms
step:1175/1770 train_time:114587ms step_avg:98.36ms
step:1176/1770 train_time:114689ms step_avg:98.36ms
step:1177/1770 train_time:114791ms step_avg:98.36ms
step:1178/1770 train_time:114892ms step_avg:98.37ms
step:1179/1770 train_time:114995ms step_avg:98.37ms
step:1180/1770 train_time:115096ms step_avg:98.37ms
step:1181/1770 train_time:115198ms step_avg:98.38ms
step:1182/1770 train_time:115300ms step_avg:98.38ms
step:1183/1770 train_time:115403ms step_avg:98.38ms
step:1184/1770 train_time:115507ms step_avg:98.39ms
step:1185/1770 train_time:115609ms step_avg:98.39ms
step:1186/1770 train_time:115711ms step_avg:98.39ms
step:1187/1770 train_time:115817ms step_avg:98.40ms
step:1188/1770 train_time:115919ms step_avg:98.40ms
step:1189/1770 train_time:116021ms step_avg:98.41ms
step:1190/1770 train_time:116123ms step_avg:98.41ms
step:1191/1770 train_time:116226ms step_avg:98.41ms
step:1192/1770 train_time:116329ms step_avg:98.42ms
step:1193/1770 train_time:116431ms step_avg:98.42ms
step:1194/1770 train_time:116533ms step_avg:98.42ms
step:1195/1770 train_time:116637ms step_avg:98.43ms
step:1196/1770 train_time:116740ms step_avg:98.43ms
step:1197/1770 train_time:116843ms step_avg:98.44ms
step:1198/1770 train_time:116946ms step_avg:98.44ms
step:1199/1770 train_time:117048ms step_avg:98.44ms
step:1200/1770 train_time:117151ms step_avg:98.45ms
step:1201/1770 train_time:117254ms step_avg:98.45ms
step:1202/1770 train_time:117356ms step_avg:98.45ms
step:1203/1770 train_time:117459ms step_avg:98.46ms
step:1204/1770 train_time:117562ms step_avg:98.46ms
step:1205/1770 train_time:117665ms step_avg:98.46ms
step:1206/1770 train_time:117768ms step_avg:98.47ms
step:1207/1770 train_time:117870ms step_avg:98.47ms
step:1208/1770 train_time:117972ms step_avg:98.47ms
step:1209/1770 train_time:118075ms step_avg:98.48ms
step:1210/1770 train_time:118177ms step_avg:98.48ms
step:1211/1770 train_time:118280ms step_avg:98.48ms
step:1212/1770 train_time:118385ms step_avg:98.49ms
step:1213/1770 train_time:118487ms step_avg:98.49ms
step:1214/1770 train_time:118589ms step_avg:98.50ms
step:1215/1770 train_time:118692ms step_avg:98.50ms
step:1216/1770 train_time:118798ms step_avg:98.51ms
step:1217/1770 train_time:118900ms step_avg:98.51ms
step:1218/1770 train_time:119003ms step_avg:98.51ms
step:1219/1770 train_time:119105ms step_avg:98.52ms
step:1220/1770 train_time:119208ms step_avg:98.52ms
step:1221/1770 train_time:119310ms step_avg:98.52ms
step:1222/1770 train_time:119414ms step_avg:98.53ms
step:1223/1770 train_time:119517ms step_avg:98.53ms
step:1224/1770 train_time:119621ms step_avg:98.53ms
step:1225/1770 train_time:119724ms step_avg:98.54ms
step:1226/1770 train_time:119826ms step_avg:98.54ms
step:1227/1770 train_time:119931ms step_avg:98.55ms
step:1228/1770 train_time:120037ms step_avg:98.55ms
step:1229/1770 train_time:120139ms step_avg:98.56ms
step:1230/1770 train_time:120241ms step_avg:98.56ms
step:1231/1770 train_time:120344ms step_avg:98.56ms
step:1232/1770 train_time:120446ms step_avg:98.57ms
step:1233/1770 train_time:120549ms step_avg:98.57ms
step:1234/1770 train_time:120651ms step_avg:98.57ms
step:1235/1770 train_time:120754ms step_avg:98.57ms
step:1236/1770 train_time:120857ms step_avg:98.58ms
step:1237/1770 train_time:120960ms step_avg:98.58ms
step:1238/1770 train_time:121063ms step_avg:98.59ms
step:1239/1770 train_time:121165ms step_avg:98.59ms
step:1240/1770 train_time:121268ms step_avg:98.59ms
step:1241/1770 train_time:121371ms step_avg:98.60ms
step:1242/1770 train_time:121474ms step_avg:98.60ms
step:1243/1770 train_time:121577ms step_avg:98.60ms
step:1244/1770 train_time:121679ms step_avg:98.61ms
step:1245/1770 train_time:121781ms step_avg:98.61ms
step:1246/1770 train_time:121884ms step_avg:98.61ms
step:1247/1770 train_time:121987ms step_avg:98.61ms
step:1248/1770 train_time:122091ms step_avg:98.62ms
step:1249/1770 train_time:122193ms step_avg:98.62ms
step:1250/1770 train_time:122296ms step_avg:98.63ms
step:1250/1770 val_loss:3.4302 train_time:122399ms step_avg:98.71ms
step:1251/1770 train_time:122421ms step_avg:98.65ms
step:1252/1770 train_time:122516ms step_avg:98.64ms
step:1253/1770 train_time:122620ms step_avg:98.65ms
step:1254/1770 train_time:122722ms step_avg:98.65ms
step:1255/1770 train_time:122827ms step_avg:98.66ms
step:1256/1770 train_time:122929ms step_avg:98.66ms
step:1257/1770 train_time:123032ms step_avg:98.66ms
step:1258/1770 train_time:123135ms step_avg:98.67ms
step:1259/1770 train_time:123238ms step_avg:98.67ms
step:1260/1770 train_time:123339ms step_avg:98.67ms
step:1261/1770 train_time:123443ms step_avg:98.68ms
step:1262/1770 train_time:123546ms step_avg:98.68ms
step:1263/1770 train_time:123648ms step_avg:98.68ms
step:1264/1770 train_time:123753ms step_avg:98.69ms
step:1265/1770 train_time:123855ms step_avg:98.69ms
step:1266/1770 train_time:123957ms step_avg:98.69ms
step:1267/1770 train_time:124060ms step_avg:98.70ms
step:1268/1770 train_time:124163ms step_avg:98.70ms
step:1269/1770 train_time:124266ms step_avg:98.70ms
step:1270/1770 train_time:124369ms step_avg:98.71ms
step:1271/1770 train_time:124472ms step_avg:98.71ms
step:1272/1770 train_time:124575ms step_avg:98.71ms
step:1273/1770 train_time:124678ms step_avg:98.72ms
step:1274/1770 train_time:124781ms step_avg:98.72ms
step:1275/1770 train_time:124883ms step_avg:98.72ms
step:1276/1770 train_time:124986ms step_avg:98.72ms
step:1277/1770 train_time:125088ms step_avg:98.73ms
step:1278/1770 train_time:125192ms step_avg:98.73ms
step:1279/1770 train_time:125296ms step_avg:98.74ms
step:1280/1770 train_time:125400ms step_avg:98.74ms
step:1281/1770 train_time:125502ms step_avg:98.74ms
step:1282/1770 train_time:125606ms step_avg:98.75ms
step:1283/1770 train_time:125709ms step_avg:98.75ms
step:1284/1770 train_time:125812ms step_avg:98.75ms
step:1285/1770 train_time:125914ms step_avg:98.76ms
step:1286/1770 train_time:126018ms step_avg:98.76ms
step:1287/1770 train_time:126122ms step_avg:98.76ms
step:1288/1770 train_time:126225ms step_avg:98.77ms
step:1289/1770 train_time:126328ms step_avg:98.77ms
step:1290/1770 train_time:126430ms step_avg:98.77ms
step:1291/1770 train_time:126533ms step_avg:98.78ms
step:1292/1770 train_time:126636ms step_avg:98.78ms
step:1293/1770 train_time:126740ms step_avg:98.78ms
step:1294/1770 train_time:126841ms step_avg:98.79ms
step:1295/1770 train_time:126944ms step_avg:98.79ms
step:1296/1770 train_time:127047ms step_avg:98.79ms
step:1297/1770 train_time:127149ms step_avg:98.79ms
step:1298/1770 train_time:127252ms step_avg:98.80ms
step:1299/1770 train_time:127355ms step_avg:98.80ms
step:1300/1770 train_time:127457ms step_avg:98.80ms
step:1301/1770 train_time:127560ms step_avg:98.81ms
step:1302/1770 train_time:127663ms step_avg:98.81ms
step:1303/1770 train_time:127765ms step_avg:98.81ms
step:1304/1770 train_time:127868ms step_avg:98.82ms
step:1305/1770 train_time:127971ms step_avg:98.82ms
step:1306/1770 train_time:128074ms step_avg:98.82ms
step:1307/1770 train_time:128177ms step_avg:98.83ms
step:1308/1770 train_time:128279ms step_avg:98.83ms
step:1309/1770 train_time:128381ms step_avg:98.83ms
step:1310/1770 train_time:128484ms step_avg:98.83ms
step:1311/1770 train_time:128585ms step_avg:98.84ms
step:1312/1770 train_time:128688ms step_avg:98.84ms
step:1313/1770 train_time:128790ms step_avg:98.84ms
step:1314/1770 train_time:128893ms step_avg:98.84ms
step:1315/1770 train_time:128996ms step_avg:98.85ms
step:1316/1770 train_time:129098ms step_avg:98.85ms
step:1317/1770 train_time:129201ms step_avg:98.85ms
step:1318/1770 train_time:129307ms step_avg:98.86ms
step:1319/1770 train_time:129411ms step_avg:98.86ms
step:1320/1770 train_time:129513ms step_avg:98.87ms
step:1321/1770 train_time:129616ms step_avg:98.87ms
step:1322/1770 train_time:129719ms step_avg:98.87ms
step:1323/1770 train_time:129822ms step_avg:98.87ms
step:1324/1770 train_time:129926ms step_avg:98.88ms
step:1325/1770 train_time:130029ms step_avg:98.88ms
step:1326/1770 train_time:130133ms step_avg:98.88ms
step:1327/1770 train_time:130238ms step_avg:98.89ms
step:1328/1770 train_time:130340ms step_avg:98.89ms
step:1329/1770 train_time:130443ms step_avg:98.90ms
step:1330/1770 train_time:130545ms step_avg:98.90ms
step:1331/1770 train_time:130647ms step_avg:98.90ms
step:1332/1770 train_time:130749ms step_avg:98.90ms
step:1333/1770 train_time:130852ms step_avg:98.91ms
step:1334/1770 train_time:130954ms step_avg:98.91ms
step:1335/1770 train_time:131056ms step_avg:98.91ms
step:1336/1770 train_time:131159ms step_avg:98.91ms
step:1337/1770 train_time:131262ms step_avg:98.92ms
step:1338/1770 train_time:131365ms step_avg:98.92ms
step:1339/1770 train_time:131469ms step_avg:98.92ms
step:1340/1770 train_time:131573ms step_avg:98.93ms
step:1341/1770 train_time:131675ms step_avg:98.93ms
step:1342/1770 train_time:131779ms step_avg:98.93ms
step:1343/1770 train_time:131882ms step_avg:98.94ms
step:1344/1770 train_time:131985ms step_avg:98.94ms
step:1345/1770 train_time:132087ms step_avg:98.94ms
step:1346/1770 train_time:132190ms step_avg:98.94ms
step:1347/1770 train_time:132294ms step_avg:98.95ms
step:1348/1770 train_time:132399ms step_avg:98.95ms
step:1349/1770 train_time:132501ms step_avg:98.96ms
step:1350/1770 train_time:132604ms step_avg:98.96ms
step:1351/1770 train_time:132706ms step_avg:98.96ms
step:1352/1770 train_time:132809ms step_avg:98.96ms
step:1353/1770 train_time:132912ms step_avg:98.97ms
step:1354/1770 train_time:133015ms step_avg:98.97ms
step:1355/1770 train_time:133118ms step_avg:98.97ms
step:1356/1770 train_time:133221ms step_avg:98.98ms
step:1357/1770 train_time:133324ms step_avg:98.98ms
step:1358/1770 train_time:133426ms step_avg:98.98ms
step:1359/1770 train_time:133530ms step_avg:98.98ms
step:1360/1770 train_time:133634ms step_avg:98.99ms
step:1361/1770 train_time:133736ms step_avg:98.99ms
step:1362/1770 train_time:133839ms step_avg:98.99ms
step:1363/1770 train_time:133942ms step_avg:99.00ms
step:1364/1770 train_time:134045ms step_avg:99.00ms
step:1365/1770 train_time:134148ms step_avg:99.00ms
step:1366/1770 train_time:134251ms step_avg:99.00ms
step:1367/1770 train_time:134354ms step_avg:99.01ms
step:1368/1770 train_time:134456ms step_avg:99.01ms
step:1369/1770 train_time:134559ms step_avg:99.01ms
step:1370/1770 train_time:134662ms step_avg:99.02ms
step:1371/1770 train_time:134765ms step_avg:99.02ms
step:1372/1770 train_time:134868ms step_avg:99.02ms
step:1373/1770 train_time:134971ms step_avg:99.03ms
step:1374/1770 train_time:135074ms step_avg:99.03ms
step:1375/1770 train_time:135177ms step_avg:99.03ms
step:1375/1770 val_loss:3.3882 train_time:135279ms step_avg:99.11ms
step:1376/1770 train_time:135300ms step_avg:99.05ms
step:1377/1770 train_time:135393ms step_avg:99.04ms
step:1378/1770 train_time:135497ms step_avg:99.05ms
step:1379/1770 train_time:135599ms step_avg:99.05ms
step:1380/1770 train_time:135701ms step_avg:99.05ms
step:1381/1770 train_time:135805ms step_avg:99.06ms
step:1382/1770 train_time:135907ms step_avg:99.06ms
step:1383/1770 train_time:136011ms step_avg:99.06ms
step:1384/1770 train_time:136114ms step_avg:99.06ms
step:1385/1770 train_time:136216ms step_avg:99.07ms
step:1386/1770 train_time:136320ms step_avg:99.07ms
step:1387/1770 train_time:136424ms step_avg:99.07ms
step:1388/1770 train_time:136526ms step_avg:99.08ms
step:1389/1770 train_time:136629ms step_avg:99.08ms
step:1390/1770 train_time:136732ms step_avg:99.08ms
step:1391/1770 train_time:136834ms step_avg:99.08ms
step:1392/1770 train_time:136937ms step_avg:99.09ms
step:1393/1770 train_time:137039ms step_avg:99.09ms
step:1394/1770 train_time:137142ms step_avg:99.09ms
step:1395/1770 train_time:137245ms step_avg:99.09ms
step:1396/1770 train_time:137350ms step_avg:99.10ms
step:1397/1770 train_time:137452ms step_avg:99.10ms
step:1398/1770 train_time:137555ms step_avg:99.10ms
step:1399/1770 train_time:137658ms step_avg:99.11ms
step:1400/1770 train_time:137762ms step_avg:99.11ms
step:1401/1770 train_time:137866ms step_avg:99.11ms
step:1402/1770 train_time:137969ms step_avg:99.12ms
step:1403/1770 train_time:138071ms step_avg:99.12ms
step:1404/1770 train_time:138174ms step_avg:99.12ms
step:1405/1770 train_time:138276ms step_avg:99.12ms
step:1406/1770 train_time:138379ms step_avg:99.13ms
step:1407/1770 train_time:138481ms step_avg:99.13ms
step:1408/1770 train_time:138585ms step_avg:99.13ms
step:1409/1770 train_time:138687ms step_avg:99.13ms
step:1410/1770 train_time:138790ms step_avg:99.14ms
step:1411/1770 train_time:138892ms step_avg:99.14ms
step:1412/1770 train_time:138994ms step_avg:99.14ms
step:1413/1770 train_time:139097ms step_avg:99.14ms
step:1414/1770 train_time:139200ms step_avg:99.15ms
step:1415/1770 train_time:139303ms step_avg:99.15ms
step:1416/1770 train_time:139407ms step_avg:99.15ms
step:1417/1770 train_time:139509ms step_avg:99.15ms
step:1418/1770 train_time:139611ms step_avg:99.16ms
step:1419/1770 train_time:139714ms step_avg:99.16ms
step:1420/1770 train_time:139817ms step_avg:99.16ms
step:1421/1770 train_time:139919ms step_avg:99.16ms
step:1422/1770 train_time:140022ms step_avg:99.17ms
step:1423/1770 train_time:140125ms step_avg:99.17ms
step:1424/1770 train_time:140229ms step_avg:99.17ms
step:1425/1770 train_time:140331ms step_avg:99.17ms
step:1426/1770 train_time:140435ms step_avg:99.18ms
step:1427/1770 train_time:140537ms step_avg:99.18ms
step:1428/1770 train_time:140641ms step_avg:99.18ms
step:1429/1770 train_time:140744ms step_avg:99.19ms
step:1430/1770 train_time:140846ms step_avg:99.19ms
step:1431/1770 train_time:140950ms step_avg:99.19ms
step:1432/1770 train_time:141052ms step_avg:99.19ms
step:1433/1770 train_time:141155ms step_avg:99.20ms
step:1434/1770 train_time:141257ms step_avg:99.20ms
step:1435/1770 train_time:141359ms step_avg:99.20ms
step:1436/1770 train_time:141464ms step_avg:99.20ms
step:1437/1770 train_time:141567ms step_avg:99.21ms
step:1438/1770 train_time:141669ms step_avg:99.21ms
step:1439/1770 train_time:141771ms step_avg:99.21ms
step:1440/1770 train_time:141873ms step_avg:99.21ms
step:1441/1770 train_time:141978ms step_avg:99.22ms
step:1442/1770 train_time:142081ms step_avg:99.22ms
step:1443/1770 train_time:142185ms step_avg:99.22ms
step:1444/1770 train_time:142288ms step_avg:99.22ms
step:1445/1770 train_time:142391ms step_avg:99.23ms
step:1446/1770 train_time:142495ms step_avg:99.23ms
step:1447/1770 train_time:142599ms step_avg:99.23ms
step:1448/1770 train_time:142703ms step_avg:99.24ms
step:1449/1770 train_time:142808ms step_avg:99.24ms
step:1450/1770 train_time:142912ms step_avg:99.24ms
step:1451/1770 train_time:143016ms step_avg:99.25ms
step:1452/1770 train_time:143119ms step_avg:99.25ms
step:1453/1770 train_time:143223ms step_avg:99.25ms
step:1454/1770 train_time:143327ms step_avg:99.26ms
step:1455/1770 train_time:143432ms step_avg:99.26ms
step:1456/1770 train_time:143537ms step_avg:99.26ms
step:1457/1770 train_time:143642ms step_avg:99.27ms
step:1458/1770 train_time:143747ms step_avg:99.27ms
step:1459/1770 train_time:143852ms step_avg:99.28ms
step:1460/1770 train_time:143956ms step_avg:99.28ms
step:1461/1770 train_time:144060ms step_avg:99.28ms
step:1462/1770 train_time:144163ms step_avg:99.29ms
step:1463/1770 train_time:144267ms step_avg:99.29ms
step:1464/1770 train_time:144373ms step_avg:99.29ms
step:1465/1770 train_time:144477ms step_avg:99.30ms
step:1466/1770 train_time:144582ms step_avg:99.30ms
step:1467/1770 train_time:144687ms step_avg:99.30ms
step:1468/1770 train_time:144791ms step_avg:99.31ms
step:1469/1770 train_time:144894ms step_avg:99.31ms
step:1470/1770 train_time:144998ms step_avg:99.31ms
step:1471/1770 train_time:145101ms step_avg:99.32ms
step:1472/1770 train_time:145205ms step_avg:99.32ms
step:1473/1770 train_time:145310ms step_avg:99.32ms
step:1474/1770 train_time:145415ms step_avg:99.33ms
step:1475/1770 train_time:145518ms step_avg:99.33ms
step:1476/1770 train_time:145621ms step_avg:99.33ms
step:1477/1770 train_time:145728ms step_avg:99.34ms
step:1478/1770 train_time:145833ms step_avg:99.34ms
step:1479/1770 train_time:145937ms step_avg:99.34ms
step:1480/1770 train_time:146041ms step_avg:99.35ms
step:1481/1770 train_time:146149ms step_avg:99.35ms
step:1482/1770 train_time:146253ms step_avg:99.36ms
step:1483/1770 train_time:146357ms step_avg:99.36ms
step:1484/1770 train_time:146460ms step_avg:99.36ms
step:1485/1770 train_time:146564ms step_avg:99.37ms
step:1486/1770 train_time:146668ms step_avg:99.37ms
step:1487/1770 train_time:146771ms step_avg:99.37ms
step:1488/1770 train_time:146875ms step_avg:99.37ms
step:1489/1770 train_time:146980ms step_avg:99.38ms
step:1490/1770 train_time:147084ms step_avg:99.38ms
step:1491/1770 train_time:147188ms step_avg:99.38ms
step:1492/1770 train_time:147292ms step_avg:99.39ms
step:1493/1770 train_time:147399ms step_avg:99.39ms
step:1494/1770 train_time:147506ms step_avg:99.40ms
step:1495/1770 train_time:147610ms step_avg:99.40ms
step:1496/1770 train_time:147713ms step_avg:99.40ms
step:1497/1770 train_time:147817ms step_avg:99.41ms
step:1498/1770 train_time:147921ms step_avg:99.41ms
step:1499/1770 train_time:148024ms step_avg:99.41ms
step:1500/1770 train_time:148128ms step_avg:99.41ms
step:1500/1770 val_loss:3.3528 train_time:148229ms step_avg:99.48ms
step:1501/1770 train_time:148251ms step_avg:99.43ms
step:1502/1770 train_time:148343ms step_avg:99.43ms
step:1503/1770 train_time:148448ms step_avg:99.43ms
step:1504/1770 train_time:148552ms step_avg:99.43ms
step:1505/1770 train_time:148658ms step_avg:99.44ms
step:1506/1770 train_time:148763ms step_avg:99.44ms
step:1507/1770 train_time:148868ms step_avg:99.44ms
step:1508/1770 train_time:148974ms step_avg:99.45ms
step:1509/1770 train_time:149078ms step_avg:99.45ms
step:1510/1770 train_time:149181ms step_avg:99.45ms
step:1511/1770 train_time:149285ms step_avg:99.46ms
step:1512/1770 train_time:149391ms step_avg:99.46ms
step:1513/1770 train_time:149495ms step_avg:99.46ms
step:1514/1770 train_time:149600ms step_avg:99.47ms
step:1515/1770 train_time:149704ms step_avg:99.47ms
step:1516/1770 train_time:149808ms step_avg:99.47ms
step:1517/1770 train_time:149912ms step_avg:99.48ms
step:1518/1770 train_time:150018ms step_avg:99.48ms
step:1519/1770 train_time:150121ms step_avg:99.48ms
step:1520/1770 train_time:150226ms step_avg:99.49ms
step:1521/1770 train_time:150330ms step_avg:99.49ms
step:1522/1770 train_time:150434ms step_avg:99.49ms
step:1523/1770 train_time:150540ms step_avg:99.50ms
step:1524/1770 train_time:150643ms step_avg:99.50ms
step:1525/1770 train_time:150747ms step_avg:99.50ms
step:1526/1770 train_time:150851ms step_avg:99.51ms
step:1527/1770 train_time:150954ms step_avg:99.51ms
step:1528/1770 train_time:151060ms step_avg:99.51ms
step:1529/1770 train_time:151163ms step_avg:99.51ms
step:1530/1770 train_time:151267ms step_avg:99.52ms
step:1531/1770 train_time:151372ms step_avg:99.52ms
step:1532/1770 train_time:151476ms step_avg:99.52ms
step:1533/1770 train_time:151581ms step_avg:99.53ms
step:1534/1770 train_time:151685ms step_avg:99.53ms
step:1535/1770 train_time:151789ms step_avg:99.53ms
step:1536/1770 train_time:151893ms step_avg:99.54ms
step:1537/1770 train_time:151997ms step_avg:99.54ms
step:1538/1770 train_time:152102ms step_avg:99.54ms
step:1539/1770 train_time:152206ms step_avg:99.55ms
step:1540/1770 train_time:152313ms step_avg:99.55ms
step:1541/1770 train_time:152418ms step_avg:99.55ms
step:1542/1770 train_time:152522ms step_avg:99.56ms
step:1543/1770 train_time:152626ms step_avg:99.56ms
step:1544/1770 train_time:152732ms step_avg:99.56ms
step:1545/1770 train_time:152835ms step_avg:99.57ms
step:1546/1770 train_time:152940ms step_avg:99.57ms
step:1547/1770 train_time:153043ms step_avg:99.57ms
step:1548/1770 train_time:153147ms step_avg:99.58ms
step:1549/1770 train_time:153252ms step_avg:99.58ms
step:1550/1770 train_time:153356ms step_avg:99.58ms
step:1551/1770 train_time:153459ms step_avg:99.58ms
step:1552/1770 train_time:153564ms step_avg:99.59ms
step:1553/1770 train_time:153669ms step_avg:99.59ms
step:1554/1770 train_time:153772ms step_avg:99.59ms
step:1555/1770 train_time:153877ms step_avg:99.60ms
step:1556/1770 train_time:153980ms step_avg:99.60ms
step:1557/1770 train_time:154084ms step_avg:99.60ms
step:1558/1770 train_time:154188ms step_avg:99.60ms
step:1559/1770 train_time:154292ms step_avg:99.61ms
step:1560/1770 train_time:154395ms step_avg:99.61ms
step:1561/1770 train_time:154502ms step_avg:99.61ms
step:1562/1770 train_time:154606ms step_avg:99.62ms
step:1563/1770 train_time:154710ms step_avg:99.62ms
step:1564/1770 train_time:154813ms step_avg:99.62ms
step:1565/1770 train_time:154917ms step_avg:99.62ms
step:1566/1770 train_time:155020ms step_avg:99.63ms
step:1567/1770 train_time:155124ms step_avg:99.63ms
step:1568/1770 train_time:155228ms step_avg:99.63ms
step:1569/1770 train_time:155335ms step_avg:99.64ms
step:1570/1770 train_time:155438ms step_avg:99.64ms
step:1571/1770 train_time:155542ms step_avg:99.64ms
step:1572/1770 train_time:155647ms step_avg:99.65ms
step:1573/1770 train_time:155754ms step_avg:99.65ms
step:1574/1770 train_time:155858ms step_avg:99.65ms
step:1575/1770 train_time:155961ms step_avg:99.66ms
step:1576/1770 train_time:156064ms step_avg:99.66ms
step:1577/1770 train_time:156171ms step_avg:99.66ms
step:1578/1770 train_time:156276ms step_avg:99.67ms
step:1579/1770 train_time:156379ms step_avg:99.67ms
step:1580/1770 train_time:156483ms step_avg:99.67ms
step:1581/1770 train_time:156590ms step_avg:99.68ms
step:1582/1770 train_time:156696ms step_avg:99.68ms
step:1583/1770 train_time:156800ms step_avg:99.68ms
step:1584/1770 train_time:156905ms step_avg:99.69ms
step:1585/1770 train_time:157010ms step_avg:99.69ms
step:1586/1770 train_time:157117ms step_avg:99.69ms
step:1587/1770 train_time:157221ms step_avg:99.70ms
step:1588/1770 train_time:157326ms step_avg:99.70ms
step:1589/1770 train_time:157432ms step_avg:99.70ms
step:1590/1770 train_time:157537ms step_avg:99.71ms
step:1591/1770 train_time:157640ms step_avg:99.71ms
step:1592/1770 train_time:157745ms step_avg:99.71ms
step:1593/1770 train_time:157848ms step_avg:99.71ms
step:1594/1770 train_time:157953ms step_avg:99.72ms
step:1595/1770 train_time:158056ms step_avg:99.72ms
step:1596/1770 train_time:158161ms step_avg:99.72ms
step:1597/1770 train_time:158265ms step_avg:99.73ms
step:1598/1770 train_time:158369ms step_avg:99.73ms
step:1599/1770 train_time:158474ms step_avg:99.73ms
step:1600/1770 train_time:158581ms step_avg:99.74ms
step:1601/1770 train_time:158685ms step_avg:99.74ms
step:1602/1770 train_time:158791ms step_avg:99.74ms
step:1603/1770 train_time:158894ms step_avg:99.75ms
step:1604/1770 train_time:158997ms step_avg:99.75ms
step:1605/1770 train_time:159100ms step_avg:99.75ms
step:1606/1770 train_time:159204ms step_avg:99.75ms
step:1607/1770 train_time:159313ms step_avg:99.76ms
step:1608/1770 train_time:159417ms step_avg:99.76ms
step:1609/1770 train_time:159520ms step_avg:99.76ms
step:1610/1770 train_time:159626ms step_avg:99.77ms
step:1611/1770 train_time:159733ms step_avg:99.77ms
step:1612/1770 train_time:159838ms step_avg:99.77ms
step:1613/1770 train_time:159941ms step_avg:99.78ms
step:1614/1770 train_time:160045ms step_avg:99.78ms
step:1615/1770 train_time:160149ms step_avg:99.78ms
step:1616/1770 train_time:160253ms step_avg:99.78ms
step:1617/1770 train_time:160359ms step_avg:99.79ms
step:1618/1770 train_time:160464ms step_avg:99.79ms
step:1619/1770 train_time:160568ms step_avg:99.79ms
step:1620/1770 train_time:160672ms step_avg:99.80ms
step:1621/1770 train_time:160776ms step_avg:99.80ms
step:1622/1770 train_time:160881ms step_avg:99.80ms
step:1623/1770 train_time:160988ms step_avg:99.81ms
step:1624/1770 train_time:161091ms step_avg:99.81ms
step:1625/1770 train_time:161194ms step_avg:99.81ms
step:1625/1770 val_loss:3.3220 train_time:161297ms step_avg:99.87ms
step:1626/1770 train_time:161319ms step_avg:99.83ms
step:1627/1770 train_time:161409ms step_avg:99.82ms
step:1628/1770 train_time:161515ms step_avg:99.82ms
step:1629/1770 train_time:161618ms step_avg:99.83ms
step:1630/1770 train_time:161722ms step_avg:99.83ms
step:1631/1770 train_time:161826ms step_avg:99.83ms
step:1632/1770 train_time:161930ms step_avg:99.83ms
step:1633/1770 train_time:162033ms step_avg:99.84ms
step:1634/1770 train_time:162137ms step_avg:99.84ms
step:1635/1770 train_time:162241ms step_avg:99.84ms
step:1636/1770 train_time:162345ms step_avg:99.84ms
step:1637/1770 train_time:162450ms step_avg:99.85ms
step:1638/1770 train_time:162553ms step_avg:99.85ms
step:1639/1770 train_time:162657ms step_avg:99.85ms
step:1640/1770 train_time:162762ms step_avg:99.85ms
step:1641/1770 train_time:162865ms step_avg:99.86ms
step:1642/1770 train_time:162968ms step_avg:99.86ms
step:1643/1770 train_time:163073ms step_avg:99.86ms
step:1644/1770 train_time:163179ms step_avg:99.86ms
step:1645/1770 train_time:163282ms step_avg:99.87ms
step:1646/1770 train_time:163388ms step_avg:99.87ms
step:1647/1770 train_time:163495ms step_avg:99.87ms
step:1648/1770 train_time:163599ms step_avg:99.88ms
step:1649/1770 train_time:163702ms step_avg:99.88ms
step:1650/1770 train_time:163806ms step_avg:99.88ms
step:1651/1770 train_time:163909ms step_avg:99.88ms
step:1652/1770 train_time:164013ms step_avg:99.89ms
step:1653/1770 train_time:164118ms step_avg:99.89ms
step:1654/1770 train_time:164226ms step_avg:99.89ms
step:1655/1770 train_time:164332ms step_avg:99.90ms
step:1656/1770 train_time:164436ms step_avg:99.90ms
step:1657/1770 train_time:164542ms step_avg:99.90ms
step:1658/1770 train_time:164646ms step_avg:99.91ms
step:1659/1770 train_time:164752ms step_avg:99.91ms
step:1660/1770 train_time:164856ms step_avg:99.91ms
step:1661/1770 train_time:164960ms step_avg:99.91ms
step:1662/1770 train_time:165064ms step_avg:99.92ms
step:1663/1770 train_time:165168ms step_avg:99.92ms
step:1664/1770 train_time:165272ms step_avg:99.92ms
step:1665/1770 train_time:165376ms step_avg:99.93ms
step:1666/1770 train_time:165482ms step_avg:99.93ms
step:1667/1770 train_time:165585ms step_avg:99.93ms
step:1668/1770 train_time:165689ms step_avg:99.93ms
step:1669/1770 train_time:165793ms step_avg:99.94ms
step:1670/1770 train_time:165897ms step_avg:99.94ms
step:1671/1770 train_time:166002ms step_avg:99.94ms
step:1672/1770 train_time:166106ms step_avg:99.94ms
step:1673/1770 train_time:166212ms step_avg:99.95ms
step:1674/1770 train_time:166315ms step_avg:99.95ms
step:1675/1770 train_time:166418ms step_avg:99.95ms
step:1676/1770 train_time:166523ms step_avg:99.95ms
step:1677/1770 train_time:166632ms step_avg:99.96ms
step:1678/1770 train_time:166735ms step_avg:99.96ms
step:1679/1770 train_time:166839ms step_avg:99.96ms
step:1680/1770 train_time:166942ms step_avg:99.97ms
step:1681/1770 train_time:167048ms step_avg:99.97ms
step:1682/1770 train_time:167154ms step_avg:99.97ms
step:1683/1770 train_time:167257ms step_avg:99.97ms
step:1684/1770 train_time:167360ms step_avg:99.98ms
step:1685/1770 train_time:167464ms step_avg:99.98ms
step:1686/1770 train_time:167569ms step_avg:99.98ms
step:1687/1770 train_time:167676ms step_avg:99.99ms
step:1688/1770 train_time:167780ms step_avg:99.99ms
step:1689/1770 train_time:167884ms step_avg:99.99ms
step:1690/1770 train_time:167988ms step_avg:99.99ms
step:1691/1770 train_time:168092ms step_avg:100.00ms
step:1692/1770 train_time:168196ms step_avg:100.00ms
step:1693/1770 train_time:168302ms step_avg:100.00ms
step:1694/1770 train_time:168405ms step_avg:100.00ms
step:1695/1770 train_time:168510ms step_avg:100.01ms
step:1696/1770 train_time:168615ms step_avg:100.01ms
step:1697/1770 train_time:168722ms step_avg:100.01ms
step:1698/1770 train_time:168827ms step_avg:100.02ms
step:1699/1770 train_time:168931ms step_avg:100.02ms
step:1700/1770 train_time:169035ms step_avg:100.02ms
step:1701/1770 train_time:169138ms step_avg:100.02ms
step:1702/1770 train_time:169243ms step_avg:100.03ms
step:1703/1770 train_time:169347ms step_avg:100.03ms
step:1704/1770 train_time:169451ms step_avg:100.03ms
step:1705/1770 train_time:169555ms step_avg:100.03ms
step:1706/1770 train_time:169658ms step_avg:100.03ms
step:1707/1770 train_time:169763ms step_avg:100.04ms
step:1708/1770 train_time:169868ms step_avg:100.04ms
step:1709/1770 train_time:169973ms step_avg:100.04ms
step:1710/1770 train_time:170081ms step_avg:100.05ms
step:1711/1770 train_time:170187ms step_avg:100.05ms
step:1712/1770 train_time:170293ms step_avg:100.05ms
step:1713/1770 train_time:170397ms step_avg:100.06ms
step:1714/1770 train_time:170501ms step_avg:100.06ms
step:1715/1770 train_time:170606ms step_avg:100.06ms
step:1716/1770 train_time:170711ms step_avg:100.07ms
step:1717/1770 train_time:170815ms step_avg:100.07ms
step:1718/1770 train_time:170921ms step_avg:100.07ms
step:1719/1770 train_time:171027ms step_avg:100.07ms
step:1720/1770 train_time:171132ms step_avg:100.08ms
step:1721/1770 train_time:171237ms step_avg:100.08ms
step:1722/1770 train_time:171344ms step_avg:100.08ms
step:1723/1770 train_time:171450ms step_avg:100.09ms
step:1724/1770 train_time:171556ms step_avg:100.09ms
step:1725/1770 train_time:171663ms step_avg:100.10ms
step:1726/1770 train_time:171771ms step_avg:100.10ms
step:1727/1770 train_time:171875ms step_avg:100.10ms
step:1728/1770 train_time:171983ms step_avg:100.11ms
step:1729/1770 train_time:172087ms step_avg:100.11ms
step:1730/1770 train_time:172194ms step_avg:100.11ms
step:1731/1770 train_time:172301ms step_avg:100.12ms
step:1732/1770 train_time:172405ms step_avg:100.12ms
step:1733/1770 train_time:172511ms step_avg:100.12ms
step:1734/1770 train_time:172616ms step_avg:100.13ms
step:1735/1770 train_time:172721ms step_avg:100.13ms
step:1736/1770 train_time:172826ms step_avg:100.13ms
step:1737/1770 train_time:172931ms step_avg:100.13ms
step:1738/1770 train_time:173036ms step_avg:100.14ms
step:1739/1770 train_time:173140ms step_avg:100.14ms
step:1740/1770 train_time:173244ms step_avg:100.14ms
step:1741/1770 train_time:173352ms step_avg:100.15ms
step:1742/1770 train_time:173460ms step_avg:100.15ms
step:1743/1770 train_time:173566ms step_avg:100.15ms
step:1744/1770 train_time:173671ms step_avg:100.16ms
step:1745/1770 train_time:173776ms step_avg:100.16ms
step:1746/1770 train_time:173884ms step_avg:100.16ms
step:1747/1770 train_time:173987ms step_avg:100.17ms
step:1748/1770 train_time:174094ms step_avg:100.17ms
step:1749/1770 train_time:174199ms step_avg:100.17ms
step:1750/1770 train_time:174304ms step_avg:100.17ms
step:1750/1770 val_loss:3.2969 train_time:174407ms step_avg:100.23ms
step:1751/1770 train_time:174429ms step_avg:100.19ms
step:1752/1770 train_time:174519ms step_avg:100.18ms
step:1753/1770 train_time:174625ms step_avg:100.19ms
step:1754/1770 train_time:174731ms step_avg:100.19ms
step:1755/1770 train_time:174835ms step_avg:100.19ms
step:1756/1770 train_time:174940ms step_avg:100.19ms
step:1757/1770 train_time:175046ms step_avg:100.20ms
step:1758/1770 train_time:175150ms step_avg:100.20ms
step:1759/1770 train_time:175257ms step_avg:100.20ms
step:1760/1770 train_time:175362ms step_avg:100.21ms
step:1761/1770 train_time:175470ms step_avg:100.21ms
step:1762/1770 train_time:175578ms step_avg:100.22ms
step:1763/1770 train_time:175682ms step_avg:100.22ms
step:1764/1770 train_time:175788ms step_avg:100.22ms
step:1765/1770 train_time:175893ms step_avg:100.22ms
step:1766/1770 train_time:176002ms step_avg:100.23ms
step:1767/1770 train_time:176106ms step_avg:100.23ms
step:1768/1770 train_time:176212ms step_avg:100.23ms
step:1769/1770 train_time:176316ms step_avg:100.24ms
step:1770/1770 train_time:176420ms step_avg:100.24ms
step:1770/1770 val_loss:3.2934 train_time:176525ms step_avg:100.30ms
peak memory allocated: 28840 MiB reserved: 32212 MiB
