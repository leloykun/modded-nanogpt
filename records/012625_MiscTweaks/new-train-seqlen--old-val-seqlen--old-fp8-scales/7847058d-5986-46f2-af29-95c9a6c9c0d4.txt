import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:03:35 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24435ms step_avg:nanms
step:2/1770 train_time:24912ms step_avg:nanms
step:3/1770 train_time:25054ms step_avg:nanms
step:4/1770 train_time:25166ms step_avg:nanms
step:5/1770 train_time:25259ms step_avg:nanms
step:6/1770 train_time:25353ms step_avg:nanms
step:7/1770 train_time:25448ms step_avg:nanms
step:8/1770 train_time:25543ms step_avg:nanms
step:9/1770 train_time:25637ms step_avg:nanms
step:10/1770 train_time:25731ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.47ms
step:14/1770 train_time:378ms step_avg:94.50ms
step:15/1770 train_time:473ms step_avg:94.52ms
step:16/1770 train_time:567ms step_avg:94.48ms
step:17/1770 train_time:661ms step_avg:94.48ms
step:18/1770 train_time:757ms step_avg:94.62ms
step:19/1770 train_time:851ms step_avg:94.58ms
step:20/1770 train_time:946ms step_avg:94.59ms
step:21/1770 train_time:1041ms step_avg:94.60ms
step:22/1770 train_time:1136ms step_avg:94.64ms
step:23/1770 train_time:1230ms step_avg:94.62ms
step:24/1770 train_time:1324ms step_avg:94.59ms
step:25/1770 train_time:1419ms step_avg:94.60ms
step:26/1770 train_time:1514ms step_avg:94.61ms
step:27/1770 train_time:1608ms step_avg:94.60ms
step:28/1770 train_time:1703ms step_avg:94.60ms
step:29/1770 train_time:1798ms step_avg:94.61ms
step:30/1770 train_time:1892ms step_avg:94.62ms
step:31/1770 train_time:1987ms step_avg:94.64ms
step:32/1770 train_time:2081ms step_avg:94.61ms
step:33/1770 train_time:2176ms step_avg:94.61ms
step:34/1770 train_time:2271ms step_avg:94.61ms
step:35/1770 train_time:2365ms step_avg:94.60ms
step:36/1770 train_time:2460ms step_avg:94.61ms
step:37/1770 train_time:2555ms step_avg:94.64ms
step:38/1770 train_time:2650ms step_avg:94.63ms
step:39/1770 train_time:2744ms step_avg:94.62ms
step:40/1770 train_time:2839ms step_avg:94.64ms
step:41/1770 train_time:2934ms step_avg:94.65ms
step:42/1770 train_time:3029ms step_avg:94.65ms
step:43/1770 train_time:3123ms step_avg:94.64ms
step:44/1770 train_time:3218ms step_avg:94.65ms
step:45/1770 train_time:3313ms step_avg:94.65ms
step:46/1770 train_time:3407ms step_avg:94.64ms
step:47/1770 train_time:3503ms step_avg:94.67ms
step:48/1770 train_time:3598ms step_avg:94.70ms
step:49/1770 train_time:3693ms step_avg:94.70ms
step:50/1770 train_time:3788ms step_avg:94.69ms
step:51/1770 train_time:3883ms step_avg:94.70ms
step:52/1770 train_time:3977ms step_avg:94.70ms
step:53/1770 train_time:4073ms step_avg:94.72ms
step:54/1770 train_time:4167ms step_avg:94.70ms
step:55/1770 train_time:4261ms step_avg:94.69ms
step:56/1770 train_time:4356ms step_avg:94.70ms
step:57/1770 train_time:4451ms step_avg:94.70ms
step:58/1770 train_time:4545ms step_avg:94.69ms
step:59/1770 train_time:4640ms step_avg:94.69ms
step:60/1770 train_time:4735ms step_avg:94.69ms
step:61/1770 train_time:4830ms step_avg:94.71ms
step:62/1770 train_time:4924ms step_avg:94.70ms
step:63/1770 train_time:5019ms step_avg:94.70ms
step:64/1770 train_time:5114ms step_avg:94.70ms
step:65/1770 train_time:5208ms step_avg:94.70ms
step:66/1770 train_time:5303ms step_avg:94.69ms
step:67/1770 train_time:5398ms step_avg:94.70ms
step:68/1770 train_time:5492ms step_avg:94.69ms
step:69/1770 train_time:5586ms step_avg:94.68ms
step:70/1770 train_time:5681ms step_avg:94.68ms
step:71/1770 train_time:5777ms step_avg:94.70ms
step:72/1770 train_time:5871ms step_avg:94.69ms
step:73/1770 train_time:5965ms step_avg:94.69ms
step:74/1770 train_time:6060ms step_avg:94.69ms
step:75/1770 train_time:6155ms step_avg:94.69ms
step:76/1770 train_time:6250ms step_avg:94.70ms
step:77/1770 train_time:6344ms step_avg:94.69ms
step:78/1770 train_time:6439ms step_avg:94.70ms
step:79/1770 train_time:6534ms step_avg:94.70ms
step:80/1770 train_time:6628ms step_avg:94.69ms
step:81/1770 train_time:6723ms step_avg:94.69ms
step:82/1770 train_time:6818ms step_avg:94.69ms
step:83/1770 train_time:6913ms step_avg:94.70ms
step:84/1770 train_time:7008ms step_avg:94.70ms
step:85/1770 train_time:7102ms step_avg:94.69ms
step:86/1770 train_time:7197ms step_avg:94.70ms
step:87/1770 train_time:7292ms step_avg:94.70ms
step:88/1770 train_time:7386ms step_avg:94.69ms
step:89/1770 train_time:7480ms step_avg:94.68ms
step:90/1770 train_time:7575ms step_avg:94.68ms
step:91/1770 train_time:7670ms step_avg:94.69ms
step:92/1770 train_time:7764ms step_avg:94.68ms
step:93/1770 train_time:7859ms step_avg:94.69ms
step:94/1770 train_time:7954ms step_avg:94.69ms
step:95/1770 train_time:8051ms step_avg:94.72ms
step:96/1770 train_time:8142ms step_avg:94.68ms
step:97/1770 train_time:8237ms step_avg:94.68ms
step:98/1770 train_time:8332ms step_avg:94.68ms
step:99/1770 train_time:8426ms step_avg:94.68ms
step:100/1770 train_time:8520ms step_avg:94.67ms
step:101/1770 train_time:8616ms step_avg:94.68ms
step:102/1770 train_time:8711ms step_avg:94.68ms
step:103/1770 train_time:8805ms step_avg:94.68ms
step:104/1770 train_time:8900ms step_avg:94.68ms
step:105/1770 train_time:8995ms step_avg:94.68ms
step:106/1770 train_time:9089ms step_avg:94.68ms
step:107/1770 train_time:9183ms step_avg:94.67ms
step:108/1770 train_time:9278ms step_avg:94.67ms
step:109/1770 train_time:9373ms step_avg:94.67ms
step:110/1770 train_time:9468ms step_avg:94.68ms
step:111/1770 train_time:9562ms step_avg:94.67ms
step:112/1770 train_time:9657ms step_avg:94.67ms
step:113/1770 train_time:9752ms step_avg:94.68ms
step:114/1770 train_time:9846ms step_avg:94.68ms
step:115/1770 train_time:9941ms step_avg:94.68ms
step:116/1770 train_time:10036ms step_avg:94.68ms
step:117/1770 train_time:10131ms step_avg:94.68ms
step:118/1770 train_time:10226ms step_avg:94.68ms
step:119/1770 train_time:10320ms step_avg:94.68ms
step:120/1770 train_time:10416ms step_avg:94.69ms
step:121/1770 train_time:10511ms step_avg:94.69ms
step:122/1770 train_time:10605ms step_avg:94.69ms
step:123/1770 train_time:10700ms step_avg:94.69ms
step:124/1770 train_time:10795ms step_avg:94.69ms
step:125/1770 train_time:10890ms step_avg:94.70ms
step:125/1770 val_loss:4.6548 train_time:10983ms step_avg:95.50ms
step:126/1770 train_time:11004ms step_avg:94.86ms
step:127/1770 train_time:11090ms step_avg:94.78ms
step:128/1770 train_time:11188ms step_avg:94.81ms
step:129/1770 train_time:11286ms step_avg:94.84ms
step:130/1770 train_time:11382ms step_avg:94.85ms
step:131/1770 train_time:11476ms step_avg:94.84ms
step:132/1770 train_time:11571ms step_avg:94.84ms
step:133/1770 train_time:11665ms step_avg:94.84ms
step:134/1770 train_time:11760ms step_avg:94.84ms
step:135/1770 train_time:11856ms step_avg:94.85ms
step:136/1770 train_time:11951ms step_avg:94.85ms
step:137/1770 train_time:12046ms step_avg:94.85ms
step:138/1770 train_time:12141ms step_avg:94.85ms
step:139/1770 train_time:12237ms step_avg:94.86ms
step:140/1770 train_time:12334ms step_avg:94.88ms
step:141/1770 train_time:12428ms step_avg:94.87ms
step:142/1770 train_time:12523ms step_avg:94.87ms
step:143/1770 train_time:12618ms step_avg:94.87ms
step:144/1770 train_time:12714ms step_avg:94.88ms
step:145/1770 train_time:12809ms step_avg:94.88ms
step:146/1770 train_time:12904ms step_avg:94.89ms
step:147/1770 train_time:12999ms step_avg:94.89ms
step:148/1770 train_time:13095ms step_avg:94.89ms
step:149/1770 train_time:13191ms step_avg:94.90ms
step:150/1770 train_time:13286ms step_avg:94.90ms
step:151/1770 train_time:13381ms step_avg:94.90ms
step:152/1770 train_time:13477ms step_avg:94.91ms
step:153/1770 train_time:13572ms step_avg:94.91ms
step:154/1770 train_time:13667ms step_avg:94.91ms
step:155/1770 train_time:13761ms step_avg:94.91ms
step:156/1770 train_time:13857ms step_avg:94.91ms
step:157/1770 train_time:13952ms step_avg:94.91ms
step:158/1770 train_time:14048ms step_avg:94.92ms
step:159/1770 train_time:14142ms step_avg:94.91ms
step:160/1770 train_time:14237ms step_avg:94.92ms
step:161/1770 train_time:14333ms step_avg:94.92ms
step:162/1770 train_time:14428ms step_avg:94.92ms
step:163/1770 train_time:14523ms step_avg:94.92ms
step:164/1770 train_time:14619ms step_avg:94.93ms
step:165/1770 train_time:14714ms step_avg:94.93ms
step:166/1770 train_time:14809ms step_avg:94.93ms
step:167/1770 train_time:14904ms step_avg:94.93ms
step:168/1770 train_time:14999ms step_avg:94.93ms
step:169/1770 train_time:15094ms step_avg:94.93ms
step:170/1770 train_time:15190ms step_avg:94.94ms
step:171/1770 train_time:15284ms step_avg:94.93ms
step:172/1770 train_time:15380ms step_avg:94.94ms
step:173/1770 train_time:15475ms step_avg:94.94ms
step:174/1770 train_time:15571ms step_avg:94.94ms
step:175/1770 train_time:15665ms step_avg:94.94ms
step:176/1770 train_time:15760ms step_avg:94.94ms
step:177/1770 train_time:15855ms step_avg:94.94ms
step:178/1770 train_time:15951ms step_avg:94.94ms
step:179/1770 train_time:16045ms step_avg:94.94ms
step:180/1770 train_time:16140ms step_avg:94.94ms
step:181/1770 train_time:16236ms step_avg:94.95ms
step:182/1770 train_time:16332ms step_avg:94.95ms
step:183/1770 train_time:16427ms step_avg:94.95ms
step:184/1770 train_time:16522ms step_avg:94.95ms
step:185/1770 train_time:16617ms step_avg:94.96ms
step:186/1770 train_time:16713ms step_avg:94.96ms
step:187/1770 train_time:16808ms step_avg:94.96ms
step:188/1770 train_time:16903ms step_avg:94.96ms
step:189/1770 train_time:16998ms step_avg:94.96ms
step:190/1770 train_time:17094ms step_avg:94.97ms
step:191/1770 train_time:17189ms step_avg:94.97ms
step:192/1770 train_time:17284ms step_avg:94.97ms
step:193/1770 train_time:17379ms step_avg:94.97ms
step:194/1770 train_time:17475ms step_avg:94.97ms
step:195/1770 train_time:17572ms step_avg:94.98ms
step:196/1770 train_time:17667ms step_avg:94.98ms
step:197/1770 train_time:17761ms step_avg:94.98ms
step:198/1770 train_time:17857ms step_avg:94.98ms
step:199/1770 train_time:17952ms step_avg:94.98ms
step:200/1770 train_time:18047ms step_avg:94.98ms
step:201/1770 train_time:18143ms step_avg:94.99ms
step:202/1770 train_time:18237ms step_avg:94.99ms
step:203/1770 train_time:18333ms step_avg:94.99ms
step:204/1770 train_time:18428ms step_avg:94.99ms
step:205/1770 train_time:18523ms step_avg:94.99ms
step:206/1770 train_time:18618ms step_avg:94.99ms
step:207/1770 train_time:18713ms step_avg:94.99ms
step:208/1770 train_time:18809ms step_avg:94.99ms
step:209/1770 train_time:18904ms step_avg:94.99ms
step:210/1770 train_time:18999ms step_avg:94.99ms
step:211/1770 train_time:19094ms step_avg:95.00ms
step:212/1770 train_time:19190ms step_avg:95.00ms
step:213/1770 train_time:19285ms step_avg:95.00ms
step:214/1770 train_time:19381ms step_avg:95.00ms
step:215/1770 train_time:19476ms step_avg:95.01ms
step:216/1770 train_time:19572ms step_avg:95.01ms
step:217/1770 train_time:19667ms step_avg:95.01ms
step:218/1770 train_time:19762ms step_avg:95.01ms
step:219/1770 train_time:19857ms step_avg:95.01ms
step:220/1770 train_time:19952ms step_avg:95.01ms
step:221/1770 train_time:20047ms step_avg:95.01ms
step:222/1770 train_time:20142ms step_avg:95.01ms
step:223/1770 train_time:20237ms step_avg:95.01ms
step:224/1770 train_time:20333ms step_avg:95.01ms
step:225/1770 train_time:20429ms step_avg:95.02ms
step:226/1770 train_time:20524ms step_avg:95.02ms
step:227/1770 train_time:20619ms step_avg:95.02ms
step:228/1770 train_time:20714ms step_avg:95.02ms
step:229/1770 train_time:20809ms step_avg:95.02ms
step:230/1770 train_time:20904ms step_avg:95.02ms
step:231/1770 train_time:20999ms step_avg:95.02ms
step:232/1770 train_time:21095ms step_avg:95.02ms
step:233/1770 train_time:21190ms step_avg:95.02ms
step:234/1770 train_time:21285ms step_avg:95.02ms
step:235/1770 train_time:21380ms step_avg:95.02ms
step:236/1770 train_time:21476ms step_avg:95.02ms
step:237/1770 train_time:21571ms step_avg:95.02ms
step:238/1770 train_time:21665ms step_avg:95.02ms
step:239/1770 train_time:21760ms step_avg:95.02ms
step:240/1770 train_time:21856ms step_avg:95.02ms
step:241/1770 train_time:21952ms step_avg:95.03ms
step:242/1770 train_time:22047ms step_avg:95.03ms
step:243/1770 train_time:22141ms step_avg:95.03ms
step:244/1770 train_time:22237ms step_avg:95.03ms
step:245/1770 train_time:22332ms step_avg:95.03ms
step:246/1770 train_time:22428ms step_avg:95.03ms
step:247/1770 train_time:22523ms step_avg:95.03ms
step:248/1770 train_time:22619ms step_avg:95.04ms
step:249/1770 train_time:22714ms step_avg:95.04ms
step:250/1770 train_time:22810ms step_avg:95.04ms
step:250/1770 val_loss:4.1057 train_time:22903ms step_avg:95.43ms
step:251/1770 train_time:22925ms step_avg:95.12ms
step:252/1770 train_time:23010ms step_avg:95.08ms
step:253/1770 train_time:23108ms step_avg:95.09ms
step:254/1770 train_time:23204ms step_avg:95.10ms
step:255/1770 train_time:23299ms step_avg:95.10ms
step:256/1770 train_time:23394ms step_avg:95.10ms
step:257/1770 train_time:23489ms step_avg:95.10ms
step:258/1770 train_time:23584ms step_avg:95.10ms
step:259/1770 train_time:23679ms step_avg:95.10ms
step:260/1770 train_time:23774ms step_avg:95.09ms
step:261/1770 train_time:23869ms step_avg:95.10ms
step:262/1770 train_time:23964ms step_avg:95.10ms
step:263/1770 train_time:24060ms step_avg:95.10ms
step:264/1770 train_time:24155ms step_avg:95.10ms
step:265/1770 train_time:24251ms step_avg:95.10ms
step:266/1770 train_time:24347ms step_avg:95.10ms
step:267/1770 train_time:24442ms step_avg:95.10ms
step:268/1770 train_time:24538ms step_avg:95.11ms
step:269/1770 train_time:24633ms step_avg:95.11ms
step:270/1770 train_time:24729ms step_avg:95.11ms
step:271/1770 train_time:24825ms step_avg:95.12ms
step:272/1770 train_time:24921ms step_avg:95.12ms
step:273/1770 train_time:25016ms step_avg:95.12ms
step:274/1770 train_time:25113ms step_avg:95.12ms
step:275/1770 train_time:25208ms step_avg:95.13ms
step:276/1770 train_time:25304ms step_avg:95.13ms
step:277/1770 train_time:25400ms step_avg:95.13ms
step:278/1770 train_time:25495ms step_avg:95.13ms
step:279/1770 train_time:25592ms step_avg:95.14ms
step:280/1770 train_time:25687ms step_avg:95.14ms
step:281/1770 train_time:25783ms step_avg:95.14ms
step:282/1770 train_time:25878ms step_avg:95.14ms
step:283/1770 train_time:25973ms step_avg:95.14ms
step:284/1770 train_time:26069ms step_avg:95.14ms
step:285/1770 train_time:26164ms step_avg:95.14ms
step:286/1770 train_time:26260ms step_avg:95.14ms
step:287/1770 train_time:26355ms step_avg:95.15ms
step:288/1770 train_time:26451ms step_avg:95.15ms
step:289/1770 train_time:26552ms step_avg:95.17ms
step:290/1770 train_time:26643ms step_avg:95.15ms
step:291/1770 train_time:26739ms step_avg:95.16ms
step:292/1770 train_time:26835ms step_avg:95.16ms
step:293/1770 train_time:26932ms step_avg:95.17ms
step:294/1770 train_time:27028ms step_avg:95.17ms
step:295/1770 train_time:27123ms step_avg:95.17ms
step:296/1770 train_time:27219ms step_avg:95.17ms
step:297/1770 train_time:27315ms step_avg:95.17ms
step:298/1770 train_time:27410ms step_avg:95.18ms
step:299/1770 train_time:27506ms step_avg:95.18ms
step:300/1770 train_time:27602ms step_avg:95.18ms
step:301/1770 train_time:27697ms step_avg:95.18ms
step:302/1770 train_time:27793ms step_avg:95.18ms
step:303/1770 train_time:27889ms step_avg:95.19ms
step:304/1770 train_time:27985ms step_avg:95.19ms
step:305/1770 train_time:28080ms step_avg:95.19ms
step:306/1770 train_time:28177ms step_avg:95.19ms
step:307/1770 train_time:28272ms step_avg:95.19ms
step:308/1770 train_time:28368ms step_avg:95.19ms
step:309/1770 train_time:28463ms step_avg:95.19ms
step:310/1770 train_time:28559ms step_avg:95.20ms
step:311/1770 train_time:28654ms step_avg:95.20ms
step:312/1770 train_time:28751ms step_avg:95.20ms
step:313/1770 train_time:28847ms step_avg:95.20ms
step:314/1770 train_time:28943ms step_avg:95.21ms
step:315/1770 train_time:29038ms step_avg:95.21ms
step:316/1770 train_time:29134ms step_avg:95.21ms
step:317/1770 train_time:29229ms step_avg:95.21ms
step:318/1770 train_time:29325ms step_avg:95.21ms
step:319/1770 train_time:29420ms step_avg:95.21ms
step:320/1770 train_time:29516ms step_avg:95.21ms
step:321/1770 train_time:29611ms step_avg:95.21ms
step:322/1770 train_time:29707ms step_avg:95.21ms
step:323/1770 train_time:29802ms step_avg:95.22ms
step:324/1770 train_time:29898ms step_avg:95.22ms
step:325/1770 train_time:29994ms step_avg:95.22ms
step:326/1770 train_time:30089ms step_avg:95.22ms
step:327/1770 train_time:30186ms step_avg:95.22ms
step:328/1770 train_time:30280ms step_avg:95.22ms
step:329/1770 train_time:30376ms step_avg:95.22ms
step:330/1770 train_time:30473ms step_avg:95.23ms
step:331/1770 train_time:30568ms step_avg:95.23ms
step:332/1770 train_time:30663ms step_avg:95.23ms
step:333/1770 train_time:30758ms step_avg:95.23ms
step:334/1770 train_time:30855ms step_avg:95.23ms
step:335/1770 train_time:30953ms step_avg:95.24ms
step:336/1770 train_time:31050ms step_avg:95.25ms
step:337/1770 train_time:31146ms step_avg:95.25ms
step:338/1770 train_time:31241ms step_avg:95.25ms
step:339/1770 train_time:31336ms step_avg:95.25ms
step:340/1770 train_time:31432ms step_avg:95.25ms
step:341/1770 train_time:31528ms step_avg:95.25ms
step:342/1770 train_time:31625ms step_avg:95.25ms
step:343/1770 train_time:31720ms step_avg:95.25ms
step:344/1770 train_time:31815ms step_avg:95.25ms
step:345/1770 train_time:31911ms step_avg:95.26ms
step:346/1770 train_time:32007ms step_avg:95.26ms
step:347/1770 train_time:32102ms step_avg:95.26ms
step:348/1770 train_time:32198ms step_avg:95.26ms
step:349/1770 train_time:32294ms step_avg:95.26ms
step:350/1770 train_time:32390ms step_avg:95.26ms
step:351/1770 train_time:32486ms step_avg:95.27ms
step:352/1770 train_time:32581ms step_avg:95.27ms
step:353/1770 train_time:32676ms step_avg:95.27ms
step:354/1770 train_time:32773ms step_avg:95.27ms
step:355/1770 train_time:32869ms step_avg:95.27ms
step:356/1770 train_time:32965ms step_avg:95.27ms
step:357/1770 train_time:33061ms step_avg:95.28ms
step:358/1770 train_time:33156ms step_avg:95.28ms
step:359/1770 train_time:33252ms step_avg:95.28ms
step:360/1770 train_time:33347ms step_avg:95.28ms
step:361/1770 train_time:33443ms step_avg:95.28ms
step:362/1770 train_time:33538ms step_avg:95.28ms
step:363/1770 train_time:33634ms step_avg:95.28ms
step:364/1770 train_time:33730ms step_avg:95.28ms
step:365/1770 train_time:33826ms step_avg:95.28ms
step:366/1770 train_time:33921ms step_avg:95.28ms
step:367/1770 train_time:34017ms step_avg:95.29ms
step:368/1770 train_time:34114ms step_avg:95.29ms
step:369/1770 train_time:34210ms step_avg:95.29ms
step:370/1770 train_time:34307ms step_avg:95.30ms
step:371/1770 train_time:34402ms step_avg:95.30ms
step:372/1770 train_time:34498ms step_avg:95.30ms
step:373/1770 train_time:34595ms step_avg:95.30ms
step:374/1770 train_time:34691ms step_avg:95.30ms
step:375/1770 train_time:34787ms step_avg:95.31ms
step:375/1770 val_loss:3.9029 train_time:34881ms step_avg:95.57ms
step:376/1770 train_time:34902ms step_avg:95.36ms
step:377/1770 train_time:34989ms step_avg:95.34ms
step:378/1770 train_time:35087ms step_avg:95.34ms
step:379/1770 train_time:35182ms step_avg:95.34ms
step:380/1770 train_time:35277ms step_avg:95.34ms
step:381/1770 train_time:35373ms step_avg:95.34ms
step:382/1770 train_time:35468ms step_avg:95.35ms
step:383/1770 train_time:35563ms step_avg:95.34ms
step:384/1770 train_time:35659ms step_avg:95.34ms
step:385/1770 train_time:35754ms step_avg:95.34ms
step:386/1770 train_time:35849ms step_avg:95.34ms
step:387/1770 train_time:35945ms step_avg:95.34ms
step:388/1770 train_time:36041ms step_avg:95.35ms
step:389/1770 train_time:36137ms step_avg:95.35ms
step:390/1770 train_time:36233ms step_avg:95.35ms
step:391/1770 train_time:36328ms step_avg:95.35ms
step:392/1770 train_time:36424ms step_avg:95.35ms
step:393/1770 train_time:36520ms step_avg:95.35ms
step:394/1770 train_time:36616ms step_avg:95.35ms
step:395/1770 train_time:36711ms step_avg:95.35ms
step:396/1770 train_time:36808ms step_avg:95.36ms
step:397/1770 train_time:36906ms step_avg:95.36ms
step:398/1770 train_time:37003ms step_avg:95.37ms
step:399/1770 train_time:37101ms step_avg:95.38ms
step:400/1770 train_time:37199ms step_avg:95.38ms
step:401/1770 train_time:37296ms step_avg:95.39ms
step:402/1770 train_time:37394ms step_avg:95.39ms
step:403/1770 train_time:37491ms step_avg:95.40ms
step:404/1770 train_time:37588ms step_avg:95.40ms
step:405/1770 train_time:37685ms step_avg:95.41ms
step:406/1770 train_time:37782ms step_avg:95.41ms
step:407/1770 train_time:37880ms step_avg:95.42ms
step:408/1770 train_time:37978ms step_avg:95.42ms
step:409/1770 train_time:38076ms step_avg:95.43ms
step:410/1770 train_time:38174ms step_avg:95.44ms
step:411/1770 train_time:38272ms step_avg:95.44ms
step:412/1770 train_time:38369ms step_avg:95.45ms
step:413/1770 train_time:38467ms step_avg:95.45ms
step:414/1770 train_time:38564ms step_avg:95.45ms
step:415/1770 train_time:38662ms step_avg:95.46ms
step:416/1770 train_time:38759ms step_avg:95.47ms
step:417/1770 train_time:38857ms step_avg:95.47ms
step:418/1770 train_time:38955ms step_avg:95.48ms
step:419/1770 train_time:39052ms step_avg:95.48ms
step:420/1770 train_time:39150ms step_avg:95.49ms
step:421/1770 train_time:39247ms step_avg:95.49ms
step:422/1770 train_time:39344ms step_avg:95.50ms
step:423/1770 train_time:39442ms step_avg:95.50ms
step:424/1770 train_time:39540ms step_avg:95.51ms
step:425/1770 train_time:39638ms step_avg:95.51ms
step:426/1770 train_time:39736ms step_avg:95.52ms
step:427/1770 train_time:39833ms step_avg:95.52ms
step:428/1770 train_time:39930ms step_avg:95.53ms
step:429/1770 train_time:40027ms step_avg:95.53ms
step:430/1770 train_time:40125ms step_avg:95.53ms
step:431/1770 train_time:40222ms step_avg:95.54ms
step:432/1770 train_time:40320ms step_avg:95.54ms
step:433/1770 train_time:40418ms step_avg:95.55ms
step:434/1770 train_time:40515ms step_avg:95.55ms
step:435/1770 train_time:40613ms step_avg:95.56ms
step:436/1770 train_time:40710ms step_avg:95.56ms
step:437/1770 train_time:40807ms step_avg:95.57ms
step:438/1770 train_time:40905ms step_avg:95.57ms
step:439/1770 train_time:41002ms step_avg:95.58ms
step:440/1770 train_time:41099ms step_avg:95.58ms
step:441/1770 train_time:41197ms step_avg:95.59ms
step:442/1770 train_time:41295ms step_avg:95.59ms
step:443/1770 train_time:41393ms step_avg:95.60ms
step:444/1770 train_time:41490ms step_avg:95.60ms
step:445/1770 train_time:41587ms step_avg:95.60ms
step:446/1770 train_time:41685ms step_avg:95.61ms
step:447/1770 train_time:41781ms step_avg:95.61ms
step:448/1770 train_time:41879ms step_avg:95.61ms
step:449/1770 train_time:41978ms step_avg:95.62ms
step:450/1770 train_time:42075ms step_avg:95.63ms
step:451/1770 train_time:42173ms step_avg:95.63ms
step:452/1770 train_time:42271ms step_avg:95.64ms
step:453/1770 train_time:42368ms step_avg:95.64ms
step:454/1770 train_time:42465ms step_avg:95.64ms
step:455/1770 train_time:42562ms step_avg:95.65ms
step:456/1770 train_time:42660ms step_avg:95.65ms
step:457/1770 train_time:42758ms step_avg:95.65ms
step:458/1770 train_time:42856ms step_avg:95.66ms
step:459/1770 train_time:42953ms step_avg:95.66ms
step:460/1770 train_time:43050ms step_avg:95.67ms
step:461/1770 train_time:43148ms step_avg:95.67ms
step:462/1770 train_time:43245ms step_avg:95.67ms
step:463/1770 train_time:43343ms step_avg:95.68ms
step:464/1770 train_time:43441ms step_avg:95.68ms
step:465/1770 train_time:43539ms step_avg:95.69ms
step:466/1770 train_time:43637ms step_avg:95.69ms
step:467/1770 train_time:43734ms step_avg:95.70ms
step:468/1770 train_time:43832ms step_avg:95.70ms
step:469/1770 train_time:43929ms step_avg:95.71ms
step:470/1770 train_time:44026ms step_avg:95.71ms
step:471/1770 train_time:44123ms step_avg:95.71ms
step:472/1770 train_time:44221ms step_avg:95.72ms
step:473/1770 train_time:44319ms step_avg:95.72ms
step:474/1770 train_time:44417ms step_avg:95.73ms
step:475/1770 train_time:44515ms step_avg:95.73ms
step:476/1770 train_time:44612ms step_avg:95.73ms
step:477/1770 train_time:44710ms step_avg:95.74ms
step:478/1770 train_time:44807ms step_avg:95.74ms
step:479/1770 train_time:44905ms step_avg:95.75ms
step:480/1770 train_time:45002ms step_avg:95.75ms
step:481/1770 train_time:45100ms step_avg:95.75ms
step:482/1770 train_time:45198ms step_avg:95.76ms
step:483/1770 train_time:45296ms step_avg:95.76ms
step:484/1770 train_time:45394ms step_avg:95.77ms
step:485/1770 train_time:45491ms step_avg:95.77ms
step:486/1770 train_time:45588ms step_avg:95.77ms
step:487/1770 train_time:45685ms step_avg:95.78ms
step:488/1770 train_time:45782ms step_avg:95.78ms
step:489/1770 train_time:45880ms step_avg:95.78ms
step:490/1770 train_time:45978ms step_avg:95.79ms
step:491/1770 train_time:46077ms step_avg:95.79ms
step:492/1770 train_time:46173ms step_avg:95.80ms
step:493/1770 train_time:46271ms step_avg:95.80ms
step:494/1770 train_time:46368ms step_avg:95.80ms
step:495/1770 train_time:46466ms step_avg:95.81ms
step:496/1770 train_time:46563ms step_avg:95.81ms
step:497/1770 train_time:46661ms step_avg:95.81ms
step:498/1770 train_time:46759ms step_avg:95.82ms
step:499/1770 train_time:46857ms step_avg:95.82ms
step:500/1770 train_time:46955ms step_avg:95.83ms
step:500/1770 val_loss:3.7545 train_time:47051ms step_avg:96.02ms
step:501/1770 train_time:47072ms step_avg:95.87ms
step:502/1770 train_time:47163ms step_avg:95.86ms
step:503/1770 train_time:47263ms step_avg:95.87ms
step:504/1770 train_time:47360ms step_avg:95.87ms
step:505/1770 train_time:47457ms step_avg:95.87ms
step:506/1770 train_time:47555ms step_avg:95.88ms
step:507/1770 train_time:47653ms step_avg:95.88ms
step:508/1770 train_time:47750ms step_avg:95.88ms
step:509/1770 train_time:47848ms step_avg:95.89ms
step:510/1770 train_time:47945ms step_avg:95.89ms
step:511/1770 train_time:48042ms step_avg:95.89ms
step:512/1770 train_time:48140ms step_avg:95.90ms
step:513/1770 train_time:48238ms step_avg:95.90ms
step:514/1770 train_time:48336ms step_avg:95.90ms
step:515/1770 train_time:48434ms step_avg:95.91ms
step:516/1770 train_time:48532ms step_avg:95.91ms
step:517/1770 train_time:48630ms step_avg:95.92ms
step:518/1770 train_time:48727ms step_avg:95.92ms
step:519/1770 train_time:48825ms step_avg:95.92ms
step:520/1770 train_time:48922ms step_avg:95.93ms
step:521/1770 train_time:49019ms step_avg:95.93ms
step:522/1770 train_time:49117ms step_avg:95.93ms
step:523/1770 train_time:49215ms step_avg:95.94ms
step:524/1770 train_time:49313ms step_avg:95.94ms
step:525/1770 train_time:49410ms step_avg:95.94ms
step:526/1770 train_time:49509ms step_avg:95.95ms
step:527/1770 train_time:49607ms step_avg:95.95ms
step:528/1770 train_time:49704ms step_avg:95.95ms
step:529/1770 train_time:49801ms step_avg:95.96ms
step:530/1770 train_time:49899ms step_avg:95.96ms
step:531/1770 train_time:49997ms step_avg:95.96ms
step:532/1770 train_time:50095ms step_avg:95.97ms
step:533/1770 train_time:50193ms step_avg:95.97ms
step:534/1770 train_time:50291ms step_avg:95.98ms
step:535/1770 train_time:50389ms step_avg:95.98ms
step:536/1770 train_time:50487ms step_avg:95.98ms
step:537/1770 train_time:50584ms step_avg:95.99ms
step:538/1770 train_time:50682ms step_avg:95.99ms
step:539/1770 train_time:50780ms step_avg:95.99ms
step:540/1770 train_time:50879ms step_avg:96.00ms
step:541/1770 train_time:50977ms step_avg:96.00ms
step:542/1770 train_time:51075ms step_avg:96.01ms
step:543/1770 train_time:51173ms step_avg:96.01ms
step:544/1770 train_time:51271ms step_avg:96.01ms
step:545/1770 train_time:51370ms step_avg:96.02ms
step:546/1770 train_time:51469ms step_avg:96.02ms
step:547/1770 train_time:51567ms step_avg:96.03ms
step:548/1770 train_time:51665ms step_avg:96.03ms
step:549/1770 train_time:51763ms step_avg:96.04ms
step:550/1770 train_time:51861ms step_avg:96.04ms
step:551/1770 train_time:51959ms step_avg:96.04ms
step:552/1770 train_time:52057ms step_avg:96.05ms
step:553/1770 train_time:52155ms step_avg:96.05ms
step:554/1770 train_time:52253ms step_avg:96.05ms
step:555/1770 train_time:52351ms step_avg:96.06ms
step:556/1770 train_time:52449ms step_avg:96.06ms
step:557/1770 train_time:52547ms step_avg:96.06ms
step:558/1770 train_time:52644ms step_avg:96.07ms
step:559/1770 train_time:52742ms step_avg:96.07ms
step:560/1770 train_time:52840ms step_avg:96.07ms
step:561/1770 train_time:52937ms step_avg:96.08ms
step:562/1770 train_time:53035ms step_avg:96.08ms
step:563/1770 train_time:53133ms step_avg:96.08ms
step:564/1770 train_time:53231ms step_avg:96.09ms
step:565/1770 train_time:53329ms step_avg:96.09ms
step:566/1770 train_time:53427ms step_avg:96.09ms
step:567/1770 train_time:53524ms step_avg:96.09ms
step:568/1770 train_time:53622ms step_avg:96.10ms
step:569/1770 train_time:53719ms step_avg:96.10ms
step:570/1770 train_time:53817ms step_avg:96.10ms
step:571/1770 train_time:53916ms step_avg:96.11ms
step:572/1770 train_time:54014ms step_avg:96.11ms
step:573/1770 train_time:54112ms step_avg:96.11ms
step:574/1770 train_time:54210ms step_avg:96.12ms
step:575/1770 train_time:54308ms step_avg:96.12ms
step:576/1770 train_time:54406ms step_avg:96.12ms
step:577/1770 train_time:54504ms step_avg:96.13ms
step:578/1770 train_time:54601ms step_avg:96.13ms
step:579/1770 train_time:54699ms step_avg:96.13ms
step:580/1770 train_time:54797ms step_avg:96.14ms
step:581/1770 train_time:54896ms step_avg:96.14ms
step:582/1770 train_time:54994ms step_avg:96.14ms
step:583/1770 train_time:55091ms step_avg:96.15ms
step:584/1770 train_time:55189ms step_avg:96.15ms
step:585/1770 train_time:55287ms step_avg:96.15ms
step:586/1770 train_time:55384ms step_avg:96.15ms
step:587/1770 train_time:55482ms step_avg:96.16ms
step:588/1770 train_time:55580ms step_avg:96.16ms
step:589/1770 train_time:55679ms step_avg:96.16ms
step:590/1770 train_time:55778ms step_avg:96.17ms
step:591/1770 train_time:55876ms step_avg:96.17ms
step:592/1770 train_time:55975ms step_avg:96.18ms
step:593/1770 train_time:56073ms step_avg:96.18ms
step:594/1770 train_time:56171ms step_avg:96.18ms
step:595/1770 train_time:56268ms step_avg:96.19ms
step:596/1770 train_time:56366ms step_avg:96.19ms
step:597/1770 train_time:56464ms step_avg:96.19ms
step:598/1770 train_time:56561ms step_avg:96.19ms
step:599/1770 train_time:56659ms step_avg:96.20ms
step:600/1770 train_time:56757ms step_avg:96.20ms
step:601/1770 train_time:56855ms step_avg:96.20ms
step:602/1770 train_time:56953ms step_avg:96.20ms
step:603/1770 train_time:57051ms step_avg:96.21ms
step:604/1770 train_time:57149ms step_avg:96.21ms
step:605/1770 train_time:57246ms step_avg:96.21ms
step:606/1770 train_time:57344ms step_avg:96.21ms
step:607/1770 train_time:57441ms step_avg:96.22ms
step:608/1770 train_time:57539ms step_avg:96.22ms
step:609/1770 train_time:57637ms step_avg:96.22ms
step:610/1770 train_time:57736ms step_avg:96.23ms
step:611/1770 train_time:57834ms step_avg:96.23ms
step:612/1770 train_time:57932ms step_avg:96.23ms
step:613/1770 train_time:58030ms step_avg:96.24ms
step:614/1770 train_time:58128ms step_avg:96.24ms
step:615/1770 train_time:58226ms step_avg:96.24ms
step:616/1770 train_time:58324ms step_avg:96.24ms
step:617/1770 train_time:58421ms step_avg:96.25ms
step:618/1770 train_time:58519ms step_avg:96.25ms
step:619/1770 train_time:58617ms step_avg:96.25ms
step:620/1770 train_time:58716ms step_avg:96.26ms
step:621/1770 train_time:58815ms step_avg:96.26ms
step:622/1770 train_time:58913ms step_avg:96.26ms
step:623/1770 train_time:59011ms step_avg:96.27ms
step:624/1770 train_time:59110ms step_avg:96.27ms
step:625/1770 train_time:59208ms step_avg:96.27ms
step:625/1770 val_loss:3.6676 train_time:59304ms step_avg:96.43ms
step:626/1770 train_time:59325ms step_avg:96.31ms
step:627/1770 train_time:59417ms step_avg:96.30ms
step:628/1770 train_time:59518ms step_avg:96.31ms
step:629/1770 train_time:59616ms step_avg:96.31ms
step:630/1770 train_time:59714ms step_avg:96.31ms
step:631/1770 train_time:59812ms step_avg:96.32ms
step:632/1770 train_time:59910ms step_avg:96.32ms
step:633/1770 train_time:60008ms step_avg:96.32ms
step:634/1770 train_time:60105ms step_avg:96.32ms
step:635/1770 train_time:60203ms step_avg:96.32ms
step:636/1770 train_time:60301ms step_avg:96.33ms
step:637/1770 train_time:60399ms step_avg:96.33ms
step:638/1770 train_time:60497ms step_avg:96.33ms
step:639/1770 train_time:60595ms step_avg:96.34ms
step:640/1770 train_time:60694ms step_avg:96.34ms
step:641/1770 train_time:60792ms step_avg:96.34ms
step:642/1770 train_time:60890ms step_avg:96.34ms
step:643/1770 train_time:60988ms step_avg:96.35ms
step:644/1770 train_time:61085ms step_avg:96.35ms
step:645/1770 train_time:61183ms step_avg:96.35ms
step:646/1770 train_time:61280ms step_avg:96.35ms
step:647/1770 train_time:61378ms step_avg:96.36ms
step:648/1770 train_time:61475ms step_avg:96.36ms
step:649/1770 train_time:61574ms step_avg:96.36ms
step:650/1770 train_time:61673ms step_avg:96.36ms
step:651/1770 train_time:61771ms step_avg:96.37ms
step:652/1770 train_time:61869ms step_avg:96.37ms
step:653/1770 train_time:61966ms step_avg:96.37ms
step:654/1770 train_time:62064ms step_avg:96.37ms
step:655/1770 train_time:62162ms step_avg:96.38ms
step:656/1770 train_time:62260ms step_avg:96.38ms
step:657/1770 train_time:62358ms step_avg:96.38ms
step:658/1770 train_time:62457ms step_avg:96.38ms
step:659/1770 train_time:62557ms step_avg:96.39ms
step:660/1770 train_time:62657ms step_avg:96.39ms
step:661/1770 train_time:62756ms step_avg:96.40ms
step:662/1770 train_time:62856ms step_avg:96.40ms
step:663/1770 train_time:62956ms step_avg:96.41ms
step:664/1770 train_time:63056ms step_avg:96.42ms
step:665/1770 train_time:63156ms step_avg:96.42ms
step:666/1770 train_time:63256ms step_avg:96.43ms
step:667/1770 train_time:63356ms step_avg:96.43ms
step:668/1770 train_time:63456ms step_avg:96.44ms
step:669/1770 train_time:63557ms step_avg:96.44ms
step:670/1770 train_time:63657ms step_avg:96.45ms
step:671/1770 train_time:63757ms step_avg:96.45ms
step:672/1770 train_time:63856ms step_avg:96.46ms
step:673/1770 train_time:63956ms step_avg:96.46ms
step:674/1770 train_time:64056ms step_avg:96.47ms
step:675/1770 train_time:64156ms step_avg:96.48ms
step:676/1770 train_time:64256ms step_avg:96.48ms
step:677/1770 train_time:64356ms step_avg:96.49ms
step:678/1770 train_time:64457ms step_avg:96.49ms
step:679/1770 train_time:64557ms step_avg:96.50ms
step:680/1770 train_time:64657ms step_avg:96.50ms
step:681/1770 train_time:64762ms step_avg:96.52ms
step:682/1770 train_time:64857ms step_avg:96.51ms
step:683/1770 train_time:64957ms step_avg:96.52ms
step:684/1770 train_time:65057ms step_avg:96.52ms
step:685/1770 train_time:65157ms step_avg:96.53ms
step:686/1770 train_time:65256ms step_avg:96.53ms
step:687/1770 train_time:65356ms step_avg:96.54ms
step:688/1770 train_time:65457ms step_avg:96.54ms
step:689/1770 train_time:65557ms step_avg:96.55ms
step:690/1770 train_time:65657ms step_avg:96.55ms
step:691/1770 train_time:65757ms step_avg:96.56ms
step:692/1770 train_time:65857ms step_avg:96.56ms
step:693/1770 train_time:65957ms step_avg:96.57ms
step:694/1770 train_time:66057ms step_avg:96.57ms
step:695/1770 train_time:66157ms step_avg:96.58ms
step:696/1770 train_time:66257ms step_avg:96.58ms
step:697/1770 train_time:66356ms step_avg:96.59ms
step:698/1770 train_time:66459ms step_avg:96.60ms
step:699/1770 train_time:66556ms step_avg:96.60ms
step:700/1770 train_time:66656ms step_avg:96.60ms
step:701/1770 train_time:66756ms step_avg:96.61ms
step:702/1770 train_time:66856ms step_avg:96.61ms
step:703/1770 train_time:66956ms step_avg:96.62ms
step:704/1770 train_time:67057ms step_avg:96.62ms
step:705/1770 train_time:67157ms step_avg:96.63ms
step:706/1770 train_time:67258ms step_avg:96.63ms
step:707/1770 train_time:67358ms step_avg:96.64ms
step:708/1770 train_time:67458ms step_avg:96.64ms
step:709/1770 train_time:67557ms step_avg:96.65ms
step:710/1770 train_time:67657ms step_avg:96.65ms
step:711/1770 train_time:67757ms step_avg:96.66ms
step:712/1770 train_time:67857ms step_avg:96.66ms
step:713/1770 train_time:67957ms step_avg:96.67ms
step:714/1770 train_time:68057ms step_avg:96.67ms
step:715/1770 train_time:68157ms step_avg:96.68ms
step:716/1770 train_time:68262ms step_avg:96.69ms
step:717/1770 train_time:68357ms step_avg:96.69ms
step:718/1770 train_time:68458ms step_avg:96.69ms
step:719/1770 train_time:68557ms step_avg:96.70ms
step:720/1770 train_time:68657ms step_avg:96.70ms
step:721/1770 train_time:68757ms step_avg:96.71ms
step:722/1770 train_time:68858ms step_avg:96.71ms
step:723/1770 train_time:68957ms step_avg:96.71ms
step:724/1770 train_time:69057ms step_avg:96.72ms
step:725/1770 train_time:69157ms step_avg:96.72ms
step:726/1770 train_time:69258ms step_avg:96.73ms
step:727/1770 train_time:69358ms step_avg:96.73ms
step:728/1770 train_time:69458ms step_avg:96.74ms
step:729/1770 train_time:69557ms step_avg:96.74ms
step:730/1770 train_time:69657ms step_avg:96.75ms
step:731/1770 train_time:69758ms step_avg:96.75ms
step:732/1770 train_time:69857ms step_avg:96.76ms
step:733/1770 train_time:69957ms step_avg:96.76ms
step:734/1770 train_time:70057ms step_avg:96.76ms
step:735/1770 train_time:70157ms step_avg:96.77ms
step:736/1770 train_time:70257ms step_avg:96.77ms
step:737/1770 train_time:70356ms step_avg:96.78ms
step:738/1770 train_time:70456ms step_avg:96.78ms
step:739/1770 train_time:70556ms step_avg:96.78ms
step:740/1770 train_time:70656ms step_avg:96.79ms
step:741/1770 train_time:70756ms step_avg:96.79ms
step:742/1770 train_time:70856ms step_avg:96.80ms
step:743/1770 train_time:70957ms step_avg:96.80ms
step:744/1770 train_time:71057ms step_avg:96.81ms
step:745/1770 train_time:71157ms step_avg:96.81ms
step:746/1770 train_time:71258ms step_avg:96.82ms
step:747/1770 train_time:71362ms step_avg:96.83ms
step:748/1770 train_time:71456ms step_avg:96.82ms
step:749/1770 train_time:71556ms step_avg:96.83ms
step:750/1770 train_time:71655ms step_avg:96.83ms
step:750/1770 val_loss:3.6052 train_time:71754ms step_avg:96.96ms
step:751/1770 train_time:71774ms step_avg:96.86ms
step:752/1770 train_time:71865ms step_avg:96.85ms
step:753/1770 train_time:71965ms step_avg:96.86ms
step:754/1770 train_time:72064ms step_avg:96.86ms
step:755/1770 train_time:72163ms step_avg:96.86ms
step:756/1770 train_time:72262ms step_avg:96.87ms
step:757/1770 train_time:72362ms step_avg:96.87ms
step:758/1770 train_time:72461ms step_avg:96.87ms
step:759/1770 train_time:72560ms step_avg:96.88ms
step:760/1770 train_time:72659ms step_avg:96.88ms
step:761/1770 train_time:72758ms step_avg:96.88ms
step:762/1770 train_time:72858ms step_avg:96.89ms
step:763/1770 train_time:72958ms step_avg:96.89ms
step:764/1770 train_time:73057ms step_avg:96.89ms
step:765/1770 train_time:73158ms step_avg:96.90ms
step:766/1770 train_time:73258ms step_avg:96.90ms
step:767/1770 train_time:73359ms step_avg:96.91ms
step:768/1770 train_time:73458ms step_avg:96.91ms
step:769/1770 train_time:73558ms step_avg:96.91ms
step:770/1770 train_time:73658ms step_avg:96.92ms
step:771/1770 train_time:73758ms step_avg:96.92ms
step:772/1770 train_time:73857ms step_avg:96.93ms
step:773/1770 train_time:73957ms step_avg:96.93ms
step:774/1770 train_time:74058ms step_avg:96.93ms
step:775/1770 train_time:74158ms step_avg:96.94ms
step:776/1770 train_time:74258ms step_avg:96.94ms
step:777/1770 train_time:74358ms step_avg:96.95ms
step:778/1770 train_time:74458ms step_avg:96.95ms
step:779/1770 train_time:74560ms step_avg:96.96ms
step:780/1770 train_time:74658ms step_avg:96.96ms
step:781/1770 train_time:74757ms step_avg:96.96ms
step:782/1770 train_time:74857ms step_avg:96.97ms
step:783/1770 train_time:74958ms step_avg:96.97ms
step:784/1770 train_time:75058ms step_avg:96.97ms
step:785/1770 train_time:75158ms step_avg:96.98ms
step:786/1770 train_time:75258ms step_avg:96.98ms
step:787/1770 train_time:75358ms step_avg:96.99ms
step:788/1770 train_time:75458ms step_avg:96.99ms
step:789/1770 train_time:75558ms step_avg:96.99ms
step:790/1770 train_time:75657ms step_avg:97.00ms
step:791/1770 train_time:75758ms step_avg:97.00ms
step:792/1770 train_time:75858ms step_avg:97.01ms
step:793/1770 train_time:75959ms step_avg:97.01ms
step:794/1770 train_time:76058ms step_avg:97.01ms
step:795/1770 train_time:76159ms step_avg:97.02ms
step:796/1770 train_time:76259ms step_avg:97.02ms
step:797/1770 train_time:76359ms step_avg:97.03ms
step:798/1770 train_time:76460ms step_avg:97.03ms
step:799/1770 train_time:76559ms step_avg:97.03ms
step:800/1770 train_time:76659ms step_avg:97.04ms
step:801/1770 train_time:76759ms step_avg:97.04ms
step:802/1770 train_time:76859ms step_avg:97.04ms
step:803/1770 train_time:76959ms step_avg:97.05ms
step:804/1770 train_time:77058ms step_avg:97.05ms
step:805/1770 train_time:77158ms step_avg:97.05ms
step:806/1770 train_time:77258ms step_avg:97.06ms
step:807/1770 train_time:77358ms step_avg:97.06ms
step:808/1770 train_time:77458ms step_avg:97.06ms
step:809/1770 train_time:77559ms step_avg:97.07ms
step:810/1770 train_time:77659ms step_avg:97.07ms
step:811/1770 train_time:77758ms step_avg:97.08ms
step:812/1770 train_time:77858ms step_avg:97.08ms
step:813/1770 train_time:77958ms step_avg:97.08ms
step:814/1770 train_time:78059ms step_avg:97.09ms
step:815/1770 train_time:78159ms step_avg:97.09ms
step:816/1770 train_time:78259ms step_avg:97.10ms
step:817/1770 train_time:78358ms step_avg:97.10ms
step:818/1770 train_time:78459ms step_avg:97.10ms
step:819/1770 train_time:78558ms step_avg:97.11ms
step:820/1770 train_time:78658ms step_avg:97.11ms
step:821/1770 train_time:78759ms step_avg:97.11ms
step:822/1770 train_time:78859ms step_avg:97.12ms
step:823/1770 train_time:78959ms step_avg:97.12ms
step:824/1770 train_time:79059ms step_avg:97.12ms
step:825/1770 train_time:79159ms step_avg:97.13ms
step:826/1770 train_time:79259ms step_avg:97.13ms
step:827/1770 train_time:79360ms step_avg:97.14ms
step:828/1770 train_time:79460ms step_avg:97.14ms
step:829/1770 train_time:79560ms step_avg:97.14ms
step:830/1770 train_time:79659ms step_avg:97.15ms
step:831/1770 train_time:79759ms step_avg:97.15ms
step:832/1770 train_time:79859ms step_avg:97.15ms
step:833/1770 train_time:79959ms step_avg:97.16ms
step:834/1770 train_time:80059ms step_avg:97.16ms
step:835/1770 train_time:80158ms step_avg:97.16ms
step:836/1770 train_time:80258ms step_avg:97.16ms
step:837/1770 train_time:80358ms step_avg:97.17ms
step:838/1770 train_time:80458ms step_avg:97.17ms
step:839/1770 train_time:80558ms step_avg:97.18ms
step:840/1770 train_time:80658ms step_avg:97.18ms
step:841/1770 train_time:80759ms step_avg:97.18ms
step:842/1770 train_time:80858ms step_avg:97.19ms
step:843/1770 train_time:80958ms step_avg:97.19ms
step:844/1770 train_time:81058ms step_avg:97.19ms
step:845/1770 train_time:81158ms step_avg:97.20ms
step:846/1770 train_time:81258ms step_avg:97.20ms
step:847/1770 train_time:81359ms step_avg:97.20ms
step:848/1770 train_time:81459ms step_avg:97.21ms
step:849/1770 train_time:81559ms step_avg:97.21ms
step:850/1770 train_time:81659ms step_avg:97.21ms
step:851/1770 train_time:81759ms step_avg:97.22ms
step:852/1770 train_time:81859ms step_avg:97.22ms
step:853/1770 train_time:81959ms step_avg:97.22ms
step:854/1770 train_time:82059ms step_avg:97.23ms
step:855/1770 train_time:82159ms step_avg:97.23ms
step:856/1770 train_time:82259ms step_avg:97.23ms
step:857/1770 train_time:82359ms step_avg:97.24ms
step:858/1770 train_time:82458ms step_avg:97.24ms
step:859/1770 train_time:82558ms step_avg:97.24ms
step:860/1770 train_time:82658ms step_avg:97.25ms
step:861/1770 train_time:82758ms step_avg:97.25ms
step:862/1770 train_time:82858ms step_avg:97.25ms
step:863/1770 train_time:82958ms step_avg:97.25ms
step:864/1770 train_time:83058ms step_avg:97.26ms
step:865/1770 train_time:83158ms step_avg:97.26ms
step:866/1770 train_time:83259ms step_avg:97.26ms
step:867/1770 train_time:83359ms step_avg:97.27ms
step:868/1770 train_time:83459ms step_avg:97.27ms
step:869/1770 train_time:83559ms step_avg:97.27ms
step:870/1770 train_time:83658ms step_avg:97.28ms
step:871/1770 train_time:83759ms step_avg:97.28ms
step:872/1770 train_time:83859ms step_avg:97.28ms
step:873/1770 train_time:83959ms step_avg:97.29ms
step:874/1770 train_time:84059ms step_avg:97.29ms
step:875/1770 train_time:84159ms step_avg:97.29ms
step:875/1770 val_loss:3.5557 train_time:84257ms step_avg:97.41ms
step:876/1770 train_time:84278ms step_avg:97.32ms
step:877/1770 train_time:84370ms step_avg:97.31ms
step:878/1770 train_time:84472ms step_avg:97.32ms
step:879/1770 train_time:84573ms step_avg:97.32ms
step:880/1770 train_time:84672ms step_avg:97.32ms
step:881/1770 train_time:84771ms step_avg:97.33ms
step:882/1770 train_time:84871ms step_avg:97.33ms
step:883/1770 train_time:84971ms step_avg:97.33ms
step:884/1770 train_time:85070ms step_avg:97.33ms
step:885/1770 train_time:85169ms step_avg:97.34ms
step:886/1770 train_time:85269ms step_avg:97.34ms
step:887/1770 train_time:85369ms step_avg:97.34ms
step:888/1770 train_time:85469ms step_avg:97.34ms
step:889/1770 train_time:85568ms step_avg:97.35ms
step:890/1770 train_time:85668ms step_avg:97.35ms
step:891/1770 train_time:85768ms step_avg:97.35ms
step:892/1770 train_time:85868ms step_avg:97.36ms
step:893/1770 train_time:85967ms step_avg:97.36ms
step:894/1770 train_time:86067ms step_avg:97.36ms
step:895/1770 train_time:86167ms step_avg:97.36ms
step:896/1770 train_time:86267ms step_avg:97.37ms
step:897/1770 train_time:86366ms step_avg:97.37ms
step:898/1770 train_time:86466ms step_avg:97.37ms
step:899/1770 train_time:86566ms step_avg:97.37ms
step:900/1770 train_time:86665ms step_avg:97.38ms
step:901/1770 train_time:86765ms step_avg:97.38ms
step:902/1770 train_time:86864ms step_avg:97.38ms
step:903/1770 train_time:86964ms step_avg:97.38ms
step:904/1770 train_time:87064ms step_avg:97.39ms
step:905/1770 train_time:87163ms step_avg:97.39ms
step:906/1770 train_time:87263ms step_avg:97.39ms
step:907/1770 train_time:87363ms step_avg:97.39ms
step:908/1770 train_time:87463ms step_avg:97.40ms
step:909/1770 train_time:87563ms step_avg:97.40ms
step:910/1770 train_time:87663ms step_avg:97.40ms
step:911/1770 train_time:87763ms step_avg:97.41ms
step:912/1770 train_time:87862ms step_avg:97.41ms
step:913/1770 train_time:87963ms step_avg:97.41ms
step:914/1770 train_time:88063ms step_avg:97.41ms
step:915/1770 train_time:88163ms step_avg:97.42ms
step:916/1770 train_time:88263ms step_avg:97.42ms
step:917/1770 train_time:88363ms step_avg:97.42ms
step:918/1770 train_time:88463ms step_avg:97.43ms
step:919/1770 train_time:88563ms step_avg:97.43ms
step:920/1770 train_time:88664ms step_avg:97.43ms
step:921/1770 train_time:88766ms step_avg:97.44ms
step:922/1770 train_time:88867ms step_avg:97.44ms
step:923/1770 train_time:88968ms step_avg:97.45ms
step:924/1770 train_time:89069ms step_avg:97.45ms
step:925/1770 train_time:89170ms step_avg:97.45ms
step:926/1770 train_time:89271ms step_avg:97.46ms
step:927/1770 train_time:89372ms step_avg:97.46ms
step:928/1770 train_time:89473ms step_avg:97.46ms
step:929/1770 train_time:89575ms step_avg:97.47ms
step:930/1770 train_time:89677ms step_avg:97.47ms
step:931/1770 train_time:89778ms step_avg:97.48ms
step:932/1770 train_time:89880ms step_avg:97.48ms
step:933/1770 train_time:89982ms step_avg:97.49ms
step:934/1770 train_time:90084ms step_avg:97.49ms
step:935/1770 train_time:90184ms step_avg:97.50ms
step:936/1770 train_time:90286ms step_avg:97.50ms
step:937/1770 train_time:90387ms step_avg:97.50ms
step:938/1770 train_time:90489ms step_avg:97.51ms
step:939/1770 train_time:90589ms step_avg:97.51ms
step:940/1770 train_time:90690ms step_avg:97.52ms
step:941/1770 train_time:90791ms step_avg:97.52ms
step:942/1770 train_time:90892ms step_avg:97.52ms
step:943/1770 train_time:90993ms step_avg:97.53ms
step:944/1770 train_time:91094ms step_avg:97.53ms
step:945/1770 train_time:91196ms step_avg:97.54ms
step:946/1770 train_time:91298ms step_avg:97.54ms
step:947/1770 train_time:91402ms step_avg:97.55ms
step:948/1770 train_time:91504ms step_avg:97.55ms
step:949/1770 train_time:91604ms step_avg:97.56ms
step:950/1770 train_time:91706ms step_avg:97.56ms
step:951/1770 train_time:91807ms step_avg:97.56ms
step:952/1770 train_time:91908ms step_avg:97.57ms
step:953/1770 train_time:92009ms step_avg:97.57ms
step:954/1770 train_time:92110ms step_avg:97.57ms
step:955/1770 train_time:92211ms step_avg:97.58ms
step:956/1770 train_time:92312ms step_avg:97.58ms
step:957/1770 train_time:92413ms step_avg:97.59ms
step:958/1770 train_time:92516ms step_avg:97.59ms
step:959/1770 train_time:92617ms step_avg:97.59ms
step:960/1770 train_time:92718ms step_avg:97.60ms
step:961/1770 train_time:92821ms step_avg:97.60ms
step:962/1770 train_time:92922ms step_avg:97.61ms
step:963/1770 train_time:93023ms step_avg:97.61ms
step:964/1770 train_time:93124ms step_avg:97.61ms
step:965/1770 train_time:93225ms step_avg:97.62ms
step:966/1770 train_time:93326ms step_avg:97.62ms
step:967/1770 train_time:93427ms step_avg:97.62ms
step:968/1770 train_time:93528ms step_avg:97.63ms
step:969/1770 train_time:93632ms step_avg:97.63ms
step:970/1770 train_time:93736ms step_avg:97.64ms
step:971/1770 train_time:93835ms step_avg:97.64ms
step:972/1770 train_time:93937ms step_avg:97.65ms
step:973/1770 train_time:94039ms step_avg:97.65ms
step:974/1770 train_time:94140ms step_avg:97.66ms
step:975/1770 train_time:94242ms step_avg:97.66ms
step:976/1770 train_time:94343ms step_avg:97.66ms
step:977/1770 train_time:94444ms step_avg:97.67ms
step:978/1770 train_time:94545ms step_avg:97.67ms
step:979/1770 train_time:94647ms step_avg:97.67ms
step:980/1770 train_time:94748ms step_avg:97.68ms
step:981/1770 train_time:94849ms step_avg:97.68ms
step:982/1770 train_time:94950ms step_avg:97.69ms
step:983/1770 train_time:95051ms step_avg:97.69ms
step:984/1770 train_time:95153ms step_avg:97.69ms
step:985/1770 train_time:95255ms step_avg:97.70ms
step:986/1770 train_time:95357ms step_avg:97.70ms
step:987/1770 train_time:95459ms step_avg:97.71ms
step:988/1770 train_time:95560ms step_avg:97.71ms
step:989/1770 train_time:95664ms step_avg:97.72ms
step:990/1770 train_time:95765ms step_avg:97.72ms
step:991/1770 train_time:95865ms step_avg:97.72ms
step:992/1770 train_time:95967ms step_avg:97.73ms
step:993/1770 train_time:96068ms step_avg:97.73ms
step:994/1770 train_time:96169ms step_avg:97.73ms
step:995/1770 train_time:96270ms step_avg:97.74ms
step:996/1770 train_time:96370ms step_avg:97.74ms
step:997/1770 train_time:96472ms step_avg:97.74ms
step:998/1770 train_time:96573ms step_avg:97.75ms
step:999/1770 train_time:96674ms step_avg:97.75ms
step:1000/1770 train_time:96776ms step_avg:97.75ms
step:1000/1770 val_loss:3.5181 train_time:96877ms step_avg:97.86ms
step:1001/1770 train_time:96898ms step_avg:97.78ms
step:1002/1770 train_time:96990ms step_avg:97.77ms
step:1003/1770 train_time:97095ms step_avg:97.78ms
step:1004/1770 train_time:97197ms step_avg:97.78ms
step:1005/1770 train_time:97297ms step_avg:97.79ms
step:1006/1770 train_time:97398ms step_avg:97.79ms
step:1007/1770 train_time:97498ms step_avg:97.79ms
step:1008/1770 train_time:97599ms step_avg:97.79ms
step:1009/1770 train_time:97700ms step_avg:97.80ms
step:1010/1770 train_time:97801ms step_avg:97.80ms
step:1011/1770 train_time:97905ms step_avg:97.81ms
step:1012/1770 train_time:98007ms step_avg:97.81ms
step:1013/1770 train_time:98111ms step_avg:97.82ms
step:1014/1770 train_time:98210ms step_avg:97.82ms
step:1015/1770 train_time:98311ms step_avg:97.82ms
step:1016/1770 train_time:98412ms step_avg:97.83ms
step:1017/1770 train_time:98513ms step_avg:97.83ms
step:1018/1770 train_time:98614ms step_avg:97.83ms
step:1019/1770 train_time:98714ms step_avg:97.83ms
step:1020/1770 train_time:98816ms step_avg:97.84ms
step:1021/1770 train_time:98917ms step_avg:97.84ms
step:1022/1770 train_time:99018ms step_avg:97.84ms
step:1023/1770 train_time:99119ms step_avg:97.85ms
step:1024/1770 train_time:99221ms step_avg:97.85ms
step:1025/1770 train_time:99322ms step_avg:97.85ms
step:1026/1770 train_time:99424ms step_avg:97.86ms
step:1027/1770 train_time:99526ms step_avg:97.86ms
step:1028/1770 train_time:99629ms step_avg:97.87ms
step:1029/1770 train_time:99729ms step_avg:97.87ms
step:1030/1770 train_time:99831ms step_avg:97.87ms
step:1031/1770 train_time:99932ms step_avg:97.88ms
step:1032/1770 train_time:100033ms step_avg:97.88ms
step:1033/1770 train_time:100135ms step_avg:97.88ms
step:1034/1770 train_time:100237ms step_avg:97.89ms
step:1035/1770 train_time:100339ms step_avg:97.89ms
step:1036/1770 train_time:100439ms step_avg:97.89ms
step:1037/1770 train_time:100541ms step_avg:97.90ms
step:1038/1770 train_time:100641ms step_avg:97.90ms
step:1039/1770 train_time:100743ms step_avg:97.90ms
step:1040/1770 train_time:100843ms step_avg:97.91ms
step:1041/1770 train_time:100945ms step_avg:97.91ms
step:1042/1770 train_time:101048ms step_avg:97.91ms
step:1043/1770 train_time:101150ms step_avg:97.92ms
step:1044/1770 train_time:101252ms step_avg:97.92ms
step:1045/1770 train_time:101353ms step_avg:97.93ms
step:1046/1770 train_time:101454ms step_avg:97.93ms
step:1047/1770 train_time:101555ms step_avg:97.93ms
step:1048/1770 train_time:101656ms step_avg:97.93ms
step:1049/1770 train_time:101757ms step_avg:97.94ms
step:1050/1770 train_time:101858ms step_avg:97.94ms
step:1051/1770 train_time:101959ms step_avg:97.94ms
step:1052/1770 train_time:102061ms step_avg:97.95ms
step:1053/1770 train_time:102162ms step_avg:97.95ms
step:1054/1770 train_time:102263ms step_avg:97.95ms
step:1055/1770 train_time:102366ms step_avg:97.96ms
step:1056/1770 train_time:102468ms step_avg:97.96ms
step:1057/1770 train_time:102570ms step_avg:97.97ms
step:1058/1770 train_time:102672ms step_avg:97.97ms
step:1059/1770 train_time:102773ms step_avg:97.97ms
step:1060/1770 train_time:102875ms step_avg:97.98ms
step:1061/1770 train_time:102976ms step_avg:97.98ms
step:1062/1770 train_time:103078ms step_avg:97.98ms
step:1063/1770 train_time:103180ms step_avg:97.99ms
step:1064/1770 train_time:103282ms step_avg:97.99ms
step:1065/1770 train_time:103383ms step_avg:97.99ms
step:1066/1770 train_time:103485ms step_avg:98.00ms
step:1067/1770 train_time:103587ms step_avg:98.00ms
step:1068/1770 train_time:103689ms step_avg:98.01ms
step:1069/1770 train_time:103791ms step_avg:98.01ms
step:1070/1770 train_time:103893ms step_avg:98.01ms
step:1071/1770 train_time:103994ms step_avg:98.02ms
step:1072/1770 train_time:104095ms step_avg:98.02ms
step:1073/1770 train_time:104196ms step_avg:98.02ms
step:1074/1770 train_time:104297ms step_avg:98.02ms
step:1075/1770 train_time:104398ms step_avg:98.03ms
step:1076/1770 train_time:104500ms step_avg:98.03ms
step:1077/1770 train_time:104602ms step_avg:98.03ms
step:1078/1770 train_time:104705ms step_avg:98.04ms
step:1079/1770 train_time:104806ms step_avg:98.04ms
step:1080/1770 train_time:104909ms step_avg:98.05ms
step:1081/1770 train_time:105011ms step_avg:98.05ms
step:1082/1770 train_time:105114ms step_avg:98.05ms
step:1083/1770 train_time:105214ms step_avg:98.06ms
step:1084/1770 train_time:105315ms step_avg:98.06ms
step:1085/1770 train_time:105416ms step_avg:98.06ms
step:1086/1770 train_time:105517ms step_avg:98.06ms
step:1087/1770 train_time:105618ms step_avg:98.07ms
step:1088/1770 train_time:105719ms step_avg:98.07ms
step:1089/1770 train_time:105820ms step_avg:98.07ms
step:1090/1770 train_time:105922ms step_avg:98.08ms
step:1091/1770 train_time:106025ms step_avg:98.08ms
step:1092/1770 train_time:106127ms step_avg:98.08ms
step:1093/1770 train_time:106229ms step_avg:98.09ms
step:1094/1770 train_time:106331ms step_avg:98.09ms
step:1095/1770 train_time:106432ms step_avg:98.09ms
step:1096/1770 train_time:106534ms step_avg:98.10ms
step:1097/1770 train_time:106635ms step_avg:98.10ms
step:1098/1770 train_time:106737ms step_avg:98.10ms
step:1099/1770 train_time:106838ms step_avg:98.11ms
step:1100/1770 train_time:106939ms step_avg:98.11ms
step:1101/1770 train_time:107041ms step_avg:98.11ms
step:1102/1770 train_time:107141ms step_avg:98.11ms
step:1103/1770 train_time:107243ms step_avg:98.12ms
step:1104/1770 train_time:107345ms step_avg:98.12ms
step:1105/1770 train_time:107446ms step_avg:98.12ms
step:1106/1770 train_time:107549ms step_avg:98.13ms
step:1107/1770 train_time:107651ms step_avg:98.13ms
step:1108/1770 train_time:107753ms step_avg:98.14ms
step:1109/1770 train_time:107854ms step_avg:98.14ms
step:1110/1770 train_time:107956ms step_avg:98.14ms
step:1111/1770 train_time:108058ms step_avg:98.15ms
step:1112/1770 train_time:108160ms step_avg:98.15ms
step:1113/1770 train_time:108261ms step_avg:98.15ms
step:1114/1770 train_time:108362ms step_avg:98.15ms
step:1115/1770 train_time:108464ms step_avg:98.16ms
step:1116/1770 train_time:108566ms step_avg:98.16ms
step:1117/1770 train_time:108668ms step_avg:98.16ms
step:1118/1770 train_time:108770ms step_avg:98.17ms
step:1119/1770 train_time:108871ms step_avg:98.17ms
step:1120/1770 train_time:108973ms step_avg:98.17ms
step:1121/1770 train_time:109074ms step_avg:98.18ms
step:1122/1770 train_time:109175ms step_avg:98.18ms
step:1123/1770 train_time:109276ms step_avg:98.18ms
step:1124/1770 train_time:109377ms step_avg:98.18ms
step:1125/1770 train_time:109478ms step_avg:98.19ms
step:1125/1770 val_loss:3.4807 train_time:109578ms step_avg:98.28ms
step:1126/1770 train_time:109598ms step_avg:98.21ms
step:1127/1770 train_time:109690ms step_avg:98.20ms
step:1128/1770 train_time:109793ms step_avg:98.20ms
step:1129/1770 train_time:109894ms step_avg:98.21ms
step:1130/1770 train_time:109997ms step_avg:98.21ms
step:1131/1770 train_time:110099ms step_avg:98.22ms
step:1132/1770 train_time:110201ms step_avg:98.22ms
step:1133/1770 train_time:110302ms step_avg:98.22ms
step:1134/1770 train_time:110403ms step_avg:98.22ms
step:1135/1770 train_time:110504ms step_avg:98.23ms
step:1136/1770 train_time:110605ms step_avg:98.23ms
step:1137/1770 train_time:110707ms step_avg:98.23ms
step:1138/1770 train_time:110808ms step_avg:98.23ms
step:1139/1770 train_time:110909ms step_avg:98.24ms
step:1140/1770 train_time:111011ms step_avg:98.24ms
step:1141/1770 train_time:111112ms step_avg:98.24ms
step:1142/1770 train_time:111214ms step_avg:98.25ms
step:1143/1770 train_time:111317ms step_avg:98.25ms
step:1144/1770 train_time:111419ms step_avg:98.25ms
step:1145/1770 train_time:111521ms step_avg:98.26ms
step:1146/1770 train_time:111622ms step_avg:98.26ms
step:1147/1770 train_time:111724ms step_avg:98.26ms
step:1148/1770 train_time:111824ms step_avg:98.26ms
step:1149/1770 train_time:111926ms step_avg:98.27ms
step:1150/1770 train_time:112026ms step_avg:98.27ms
step:1151/1770 train_time:112128ms step_avg:98.27ms
step:1152/1770 train_time:112230ms step_avg:98.27ms
step:1153/1770 train_time:112331ms step_avg:98.28ms
step:1154/1770 train_time:112434ms step_avg:98.28ms
step:1155/1770 train_time:112537ms step_avg:98.29ms
step:1156/1770 train_time:112639ms step_avg:98.29ms
step:1157/1770 train_time:112742ms step_avg:98.29ms
step:1158/1770 train_time:112844ms step_avg:98.30ms
step:1159/1770 train_time:112944ms step_avg:98.30ms
step:1160/1770 train_time:113045ms step_avg:98.30ms
step:1161/1770 train_time:113146ms step_avg:98.30ms
step:1162/1770 train_time:113249ms step_avg:98.31ms
step:1163/1770 train_time:113350ms step_avg:98.31ms
step:1164/1770 train_time:113451ms step_avg:98.31ms
step:1165/1770 train_time:113553ms step_avg:98.31ms
step:1166/1770 train_time:113656ms step_avg:98.32ms
step:1167/1770 train_time:113758ms step_avg:98.32ms
step:1168/1770 train_time:113860ms step_avg:98.32ms
step:1169/1770 train_time:113960ms step_avg:98.33ms
step:1170/1770 train_time:114062ms step_avg:98.33ms
step:1171/1770 train_time:114163ms step_avg:98.33ms
step:1172/1770 train_time:114264ms step_avg:98.33ms
step:1173/1770 train_time:114365ms step_avg:98.34ms
step:1174/1770 train_time:114468ms step_avg:98.34ms
step:1175/1770 train_time:114569ms step_avg:98.34ms
step:1176/1770 train_time:114671ms step_avg:98.35ms
step:1177/1770 train_time:114772ms step_avg:98.35ms
step:1178/1770 train_time:114874ms step_avg:98.35ms
step:1179/1770 train_time:114976ms step_avg:98.35ms
step:1180/1770 train_time:115078ms step_avg:98.36ms
step:1181/1770 train_time:115180ms step_avg:98.36ms
step:1182/1770 train_time:115281ms step_avg:98.36ms
step:1183/1770 train_time:115384ms step_avg:98.37ms
step:1184/1770 train_time:115488ms step_avg:98.37ms
step:1185/1770 train_time:115590ms step_avg:98.37ms
step:1186/1770 train_time:115693ms step_avg:98.38ms
step:1187/1770 train_time:115798ms step_avg:98.38ms
step:1188/1770 train_time:115900ms step_avg:98.39ms
step:1189/1770 train_time:116002ms step_avg:98.39ms
step:1190/1770 train_time:116104ms step_avg:98.39ms
step:1191/1770 train_time:116206ms step_avg:98.40ms
step:1192/1770 train_time:116309ms step_avg:98.40ms
step:1193/1770 train_time:116411ms step_avg:98.40ms
step:1194/1770 train_time:116514ms step_avg:98.41ms
step:1195/1770 train_time:116618ms step_avg:98.41ms
step:1196/1770 train_time:116721ms step_avg:98.42ms
step:1197/1770 train_time:116823ms step_avg:98.42ms
step:1198/1770 train_time:116926ms step_avg:98.42ms
step:1199/1770 train_time:117028ms step_avg:98.43ms
step:1200/1770 train_time:117131ms step_avg:98.43ms
step:1201/1770 train_time:117234ms step_avg:98.43ms
step:1202/1770 train_time:117336ms step_avg:98.44ms
step:1203/1770 train_time:117439ms step_avg:98.44ms
step:1204/1770 train_time:117542ms step_avg:98.44ms
step:1205/1770 train_time:117643ms step_avg:98.45ms
step:1206/1770 train_time:117746ms step_avg:98.45ms
step:1207/1770 train_time:117848ms step_avg:98.45ms
step:1208/1770 train_time:117950ms step_avg:98.46ms
step:1209/1770 train_time:118053ms step_avg:98.46ms
step:1210/1770 train_time:118155ms step_avg:98.46ms
step:1211/1770 train_time:118259ms step_avg:98.47ms
step:1212/1770 train_time:118364ms step_avg:98.47ms
step:1213/1770 train_time:118466ms step_avg:98.48ms
step:1214/1770 train_time:118568ms step_avg:98.48ms
step:1215/1770 train_time:118671ms step_avg:98.48ms
step:1216/1770 train_time:118778ms step_avg:98.49ms
step:1217/1770 train_time:118880ms step_avg:98.49ms
step:1218/1770 train_time:118982ms step_avg:98.49ms
step:1219/1770 train_time:119084ms step_avg:98.50ms
step:1220/1770 train_time:119186ms step_avg:98.50ms
step:1221/1770 train_time:119289ms step_avg:98.50ms
step:1222/1770 train_time:119392ms step_avg:98.51ms
step:1223/1770 train_time:119495ms step_avg:98.51ms
step:1224/1770 train_time:119599ms step_avg:98.52ms
step:1225/1770 train_time:119703ms step_avg:98.52ms
step:1226/1770 train_time:119805ms step_avg:98.52ms
step:1227/1770 train_time:119909ms step_avg:98.53ms
step:1228/1770 train_time:120014ms step_avg:98.53ms
step:1229/1770 train_time:120117ms step_avg:98.54ms
step:1230/1770 train_time:120221ms step_avg:98.54ms
step:1231/1770 train_time:120324ms step_avg:98.55ms
step:1232/1770 train_time:120426ms step_avg:98.55ms
step:1233/1770 train_time:120528ms step_avg:98.55ms
step:1234/1770 train_time:120631ms step_avg:98.55ms
step:1235/1770 train_time:120733ms step_avg:98.56ms
step:1236/1770 train_time:120837ms step_avg:98.56ms
step:1237/1770 train_time:120940ms step_avg:98.57ms
step:1238/1770 train_time:121043ms step_avg:98.57ms
step:1239/1770 train_time:121146ms step_avg:98.57ms
step:1240/1770 train_time:121248ms step_avg:98.58ms
step:1241/1770 train_time:121351ms step_avg:98.58ms
step:1242/1770 train_time:121454ms step_avg:98.58ms
step:1243/1770 train_time:121558ms step_avg:98.59ms
step:1244/1770 train_time:121660ms step_avg:98.59ms
step:1245/1770 train_time:121762ms step_avg:98.59ms
step:1246/1770 train_time:121866ms step_avg:98.60ms
step:1247/1770 train_time:121968ms step_avg:98.60ms
step:1248/1770 train_time:122072ms step_avg:98.60ms
step:1249/1770 train_time:122174ms step_avg:98.61ms
step:1250/1770 train_time:122277ms step_avg:98.61ms
step:1250/1770 val_loss:3.4323 train_time:122379ms step_avg:98.69ms
step:1251/1770 train_time:122400ms step_avg:98.63ms
step:1252/1770 train_time:122491ms step_avg:98.62ms
step:1253/1770 train_time:122597ms step_avg:98.63ms
step:1254/1770 train_time:122700ms step_avg:98.63ms
step:1255/1770 train_time:122805ms step_avg:98.64ms
step:1256/1770 train_time:122906ms step_avg:98.64ms
step:1257/1770 train_time:123009ms step_avg:98.64ms
step:1258/1770 train_time:123112ms step_avg:98.65ms
step:1259/1770 train_time:123215ms step_avg:98.65ms
step:1260/1770 train_time:123317ms step_avg:98.65ms
step:1261/1770 train_time:123421ms step_avg:98.66ms
step:1262/1770 train_time:123524ms step_avg:98.66ms
step:1263/1770 train_time:123626ms step_avg:98.66ms
step:1264/1770 train_time:123731ms step_avg:98.67ms
step:1265/1770 train_time:123834ms step_avg:98.67ms
step:1266/1770 train_time:123937ms step_avg:98.68ms
step:1267/1770 train_time:124040ms step_avg:98.68ms
step:1268/1770 train_time:124143ms step_avg:98.68ms
step:1269/1770 train_time:124246ms step_avg:98.69ms
step:1270/1770 train_time:124348ms step_avg:98.69ms
step:1271/1770 train_time:124452ms step_avg:98.69ms
step:1272/1770 train_time:124555ms step_avg:98.70ms
step:1273/1770 train_time:124657ms step_avg:98.70ms
step:1274/1770 train_time:124760ms step_avg:98.70ms
step:1275/1770 train_time:124862ms step_avg:98.71ms
step:1276/1770 train_time:124965ms step_avg:98.71ms
step:1277/1770 train_time:125067ms step_avg:98.71ms
step:1278/1770 train_time:125171ms step_avg:98.72ms
step:1279/1770 train_time:125274ms step_avg:98.72ms
step:1280/1770 train_time:125378ms step_avg:98.72ms
step:1281/1770 train_time:125480ms step_avg:98.73ms
step:1282/1770 train_time:125583ms step_avg:98.73ms
step:1283/1770 train_time:125687ms step_avg:98.73ms
step:1284/1770 train_time:125790ms step_avg:98.74ms
step:1285/1770 train_time:125893ms step_avg:98.74ms
step:1286/1770 train_time:125996ms step_avg:98.74ms
step:1287/1770 train_time:126101ms step_avg:98.75ms
step:1288/1770 train_time:126203ms step_avg:98.75ms
step:1289/1770 train_time:126306ms step_avg:98.75ms
step:1290/1770 train_time:126409ms step_avg:98.76ms
step:1291/1770 train_time:126512ms step_avg:98.76ms
step:1292/1770 train_time:126615ms step_avg:98.76ms
step:1293/1770 train_time:126718ms step_avg:98.77ms
step:1294/1770 train_time:126820ms step_avg:98.77ms
step:1295/1770 train_time:126922ms step_avg:98.77ms
step:1296/1770 train_time:127025ms step_avg:98.78ms
step:1297/1770 train_time:127127ms step_avg:98.78ms
step:1298/1770 train_time:127235ms step_avg:98.78ms
step:1299/1770 train_time:127333ms step_avg:98.78ms
step:1300/1770 train_time:127435ms step_avg:98.79ms
step:1301/1770 train_time:127538ms step_avg:98.79ms
step:1302/1770 train_time:127640ms step_avg:98.79ms
step:1303/1770 train_time:127743ms step_avg:98.80ms
step:1304/1770 train_time:127845ms step_avg:98.80ms
step:1305/1770 train_time:127948ms step_avg:98.80ms
step:1306/1770 train_time:128051ms step_avg:98.80ms
step:1307/1770 train_time:128153ms step_avg:98.81ms
step:1308/1770 train_time:128256ms step_avg:98.81ms
step:1309/1770 train_time:128359ms step_avg:98.81ms
step:1310/1770 train_time:128461ms step_avg:98.82ms
step:1311/1770 train_time:128563ms step_avg:98.82ms
step:1312/1770 train_time:128665ms step_avg:98.82ms
step:1313/1770 train_time:128767ms step_avg:98.82ms
step:1314/1770 train_time:128870ms step_avg:98.83ms
step:1315/1770 train_time:128973ms step_avg:98.83ms
step:1316/1770 train_time:129076ms step_avg:98.83ms
step:1317/1770 train_time:129179ms step_avg:98.84ms
step:1318/1770 train_time:129284ms step_avg:98.84ms
step:1319/1770 train_time:129387ms step_avg:98.84ms
step:1320/1770 train_time:129490ms step_avg:98.85ms
step:1321/1770 train_time:129594ms step_avg:98.85ms
step:1322/1770 train_time:129697ms step_avg:98.85ms
step:1323/1770 train_time:129801ms step_avg:98.86ms
step:1324/1770 train_time:129904ms step_avg:98.86ms
step:1325/1770 train_time:130007ms step_avg:98.86ms
step:1326/1770 train_time:130110ms step_avg:98.87ms
step:1327/1770 train_time:130217ms step_avg:98.87ms
step:1328/1770 train_time:130319ms step_avg:98.88ms
step:1329/1770 train_time:130422ms step_avg:98.88ms
step:1330/1770 train_time:130524ms step_avg:98.88ms
step:1331/1770 train_time:130626ms step_avg:98.88ms
step:1332/1770 train_time:130729ms step_avg:98.89ms
step:1333/1770 train_time:130831ms step_avg:98.89ms
step:1334/1770 train_time:130935ms step_avg:98.89ms
step:1335/1770 train_time:131038ms step_avg:98.90ms
step:1336/1770 train_time:131142ms step_avg:98.90ms
step:1337/1770 train_time:131242ms step_avg:98.90ms
step:1338/1770 train_time:131344ms step_avg:98.90ms
step:1339/1770 train_time:131447ms step_avg:98.91ms
step:1340/1770 train_time:131552ms step_avg:98.91ms
step:1341/1770 train_time:131655ms step_avg:98.91ms
step:1342/1770 train_time:131758ms step_avg:98.92ms
step:1343/1770 train_time:131862ms step_avg:98.92ms
step:1344/1770 train_time:131965ms step_avg:98.92ms
step:1345/1770 train_time:132067ms step_avg:98.93ms
step:1346/1770 train_time:132170ms step_avg:98.93ms
step:1347/1770 train_time:132273ms step_avg:98.93ms
step:1348/1770 train_time:132378ms step_avg:98.94ms
step:1349/1770 train_time:132480ms step_avg:98.94ms
step:1350/1770 train_time:132583ms step_avg:98.94ms
step:1351/1770 train_time:132685ms step_avg:98.94ms
step:1352/1770 train_time:132788ms step_avg:98.95ms
step:1353/1770 train_time:132892ms step_avg:98.95ms
step:1354/1770 train_time:132995ms step_avg:98.95ms
step:1355/1770 train_time:133098ms step_avg:98.96ms
step:1356/1770 train_time:133200ms step_avg:98.96ms
step:1357/1770 train_time:133302ms step_avg:98.96ms
step:1358/1770 train_time:133405ms step_avg:98.96ms
step:1359/1770 train_time:133507ms step_avg:98.97ms
step:1360/1770 train_time:133611ms step_avg:98.97ms
step:1361/1770 train_time:133714ms step_avg:98.97ms
step:1362/1770 train_time:133817ms step_avg:98.98ms
step:1363/1770 train_time:133921ms step_avg:98.98ms
step:1364/1770 train_time:134024ms step_avg:98.98ms
step:1365/1770 train_time:134126ms step_avg:98.99ms
step:1366/1770 train_time:134228ms step_avg:98.99ms
step:1367/1770 train_time:134332ms step_avg:98.99ms
step:1368/1770 train_time:134435ms step_avg:99.00ms
step:1369/1770 train_time:134541ms step_avg:99.00ms
step:1370/1770 train_time:134642ms step_avg:99.00ms
step:1371/1770 train_time:134744ms step_avg:99.00ms
step:1372/1770 train_time:134846ms step_avg:99.01ms
step:1373/1770 train_time:134949ms step_avg:99.01ms
step:1374/1770 train_time:135053ms step_avg:99.01ms
step:1375/1770 train_time:135156ms step_avg:99.02ms
step:1375/1770 val_loss:3.3922 train_time:135258ms step_avg:99.09ms
step:1376/1770 train_time:135279ms step_avg:99.03ms
step:1377/1770 train_time:135372ms step_avg:99.03ms
step:1378/1770 train_time:135475ms step_avg:99.03ms
step:1379/1770 train_time:135577ms step_avg:99.03ms
step:1380/1770 train_time:135680ms step_avg:99.04ms
step:1381/1770 train_time:135784ms step_avg:99.04ms
step:1382/1770 train_time:135887ms step_avg:99.04ms
step:1383/1770 train_time:135990ms step_avg:99.05ms
step:1384/1770 train_time:136093ms step_avg:99.05ms
step:1385/1770 train_time:136196ms step_avg:99.05ms
step:1386/1770 train_time:136299ms step_avg:99.05ms
step:1387/1770 train_time:136403ms step_avg:99.06ms
step:1388/1770 train_time:136505ms step_avg:99.06ms
step:1389/1770 train_time:136608ms step_avg:99.06ms
step:1390/1770 train_time:136710ms step_avg:99.07ms
step:1391/1770 train_time:136813ms step_avg:99.07ms
step:1392/1770 train_time:136916ms step_avg:99.07ms
step:1393/1770 train_time:137018ms step_avg:99.07ms
step:1394/1770 train_time:137120ms step_avg:99.08ms
step:1395/1770 train_time:137224ms step_avg:99.08ms
step:1396/1770 train_time:137328ms step_avg:99.08ms
step:1397/1770 train_time:137430ms step_avg:99.08ms
step:1398/1770 train_time:137533ms step_avg:99.09ms
step:1399/1770 train_time:137635ms step_avg:99.09ms
step:1400/1770 train_time:137739ms step_avg:99.09ms
step:1401/1770 train_time:137841ms step_avg:99.09ms
step:1402/1770 train_time:137944ms step_avg:99.10ms
step:1403/1770 train_time:138047ms step_avg:99.10ms
step:1404/1770 train_time:138150ms step_avg:99.10ms
step:1405/1770 train_time:138252ms step_avg:99.11ms
step:1406/1770 train_time:138354ms step_avg:99.11ms
step:1407/1770 train_time:138456ms step_avg:99.11ms
step:1408/1770 train_time:138559ms step_avg:99.11ms
step:1409/1770 train_time:138662ms step_avg:99.12ms
step:1410/1770 train_time:138766ms step_avg:99.12ms
step:1411/1770 train_time:138868ms step_avg:99.12ms
step:1412/1770 train_time:138971ms step_avg:99.12ms
step:1413/1770 train_time:139073ms step_avg:99.13ms
step:1414/1770 train_time:139176ms step_avg:99.13ms
step:1415/1770 train_time:139278ms step_avg:99.13ms
step:1416/1770 train_time:139383ms step_avg:99.13ms
step:1417/1770 train_time:139486ms step_avg:99.14ms
step:1418/1770 train_time:139588ms step_avg:99.14ms
step:1419/1770 train_time:139691ms step_avg:99.14ms
step:1420/1770 train_time:139794ms step_avg:99.14ms
step:1421/1770 train_time:139897ms step_avg:99.15ms
step:1422/1770 train_time:139999ms step_avg:99.15ms
step:1423/1770 train_time:140102ms step_avg:99.15ms
step:1424/1770 train_time:140206ms step_avg:99.16ms
step:1425/1770 train_time:140308ms step_avg:99.16ms
step:1426/1770 train_time:140412ms step_avg:99.16ms
step:1427/1770 train_time:140514ms step_avg:99.16ms
step:1428/1770 train_time:140618ms step_avg:99.17ms
step:1429/1770 train_time:140721ms step_avg:99.17ms
step:1430/1770 train_time:140823ms step_avg:99.17ms
step:1431/1770 train_time:140927ms step_avg:99.17ms
step:1432/1770 train_time:141030ms step_avg:99.18ms
step:1433/1770 train_time:141132ms step_avg:99.18ms
step:1434/1770 train_time:141234ms step_avg:99.18ms
step:1435/1770 train_time:141335ms step_avg:99.18ms
step:1436/1770 train_time:141439ms step_avg:99.19ms
step:1437/1770 train_time:141542ms step_avg:99.19ms
step:1438/1770 train_time:141645ms step_avg:99.19ms
step:1439/1770 train_time:141748ms step_avg:99.19ms
step:1440/1770 train_time:141850ms step_avg:99.20ms
step:1441/1770 train_time:141955ms step_avg:99.20ms
step:1442/1770 train_time:142058ms step_avg:99.20ms
step:1443/1770 train_time:142160ms step_avg:99.20ms
step:1444/1770 train_time:142263ms step_avg:99.21ms
step:1445/1770 train_time:142367ms step_avg:99.21ms
step:1446/1770 train_time:142471ms step_avg:99.21ms
step:1447/1770 train_time:142574ms step_avg:99.22ms
step:1448/1770 train_time:142678ms step_avg:99.22ms
step:1449/1770 train_time:142783ms step_avg:99.22ms
step:1450/1770 train_time:142887ms step_avg:99.23ms
step:1451/1770 train_time:142991ms step_avg:99.23ms
step:1452/1770 train_time:143095ms step_avg:99.23ms
step:1453/1770 train_time:143199ms step_avg:99.24ms
step:1454/1770 train_time:143303ms step_avg:99.24ms
step:1455/1770 train_time:143407ms step_avg:99.24ms
step:1456/1770 train_time:143512ms step_avg:99.25ms
step:1457/1770 train_time:143616ms step_avg:99.25ms
step:1458/1770 train_time:143720ms step_avg:99.25ms
step:1459/1770 train_time:143824ms step_avg:99.26ms
step:1460/1770 train_time:143928ms step_avg:99.26ms
step:1461/1770 train_time:144032ms step_avg:99.26ms
step:1462/1770 train_time:144135ms step_avg:99.27ms
step:1463/1770 train_time:144239ms step_avg:99.27ms
step:1464/1770 train_time:144344ms step_avg:99.27ms
step:1465/1770 train_time:144448ms step_avg:99.28ms
step:1466/1770 train_time:144553ms step_avg:99.28ms
step:1467/1770 train_time:144658ms step_avg:99.28ms
step:1468/1770 train_time:144762ms step_avg:99.29ms
step:1469/1770 train_time:144866ms step_avg:99.29ms
step:1470/1770 train_time:144969ms step_avg:99.29ms
step:1471/1770 train_time:145073ms step_avg:99.30ms
step:1472/1770 train_time:145176ms step_avg:99.30ms
step:1473/1770 train_time:145281ms step_avg:99.30ms
step:1474/1770 train_time:145386ms step_avg:99.31ms
step:1475/1770 train_time:145489ms step_avg:99.31ms
step:1476/1770 train_time:145592ms step_avg:99.31ms
step:1477/1770 train_time:145699ms step_avg:99.32ms
step:1478/1770 train_time:145803ms step_avg:99.32ms
step:1479/1770 train_time:145907ms step_avg:99.32ms
step:1480/1770 train_time:146010ms step_avg:99.33ms
step:1481/1770 train_time:146117ms step_avg:99.33ms
step:1482/1770 train_time:146220ms step_avg:99.33ms
step:1483/1770 train_time:146324ms step_avg:99.34ms
step:1484/1770 train_time:146428ms step_avg:99.34ms
step:1485/1770 train_time:146532ms step_avg:99.34ms
step:1486/1770 train_time:146635ms step_avg:99.35ms
step:1487/1770 train_time:146738ms step_avg:99.35ms
step:1488/1770 train_time:146842ms step_avg:99.35ms
step:1489/1770 train_time:146947ms step_avg:99.36ms
step:1490/1770 train_time:147052ms step_avg:99.36ms
step:1491/1770 train_time:147156ms step_avg:99.36ms
step:1492/1770 train_time:147261ms step_avg:99.37ms
step:1493/1770 train_time:147367ms step_avg:99.37ms
step:1494/1770 train_time:147475ms step_avg:99.38ms
step:1495/1770 train_time:147577ms step_avg:99.38ms
step:1496/1770 train_time:147680ms step_avg:99.38ms
step:1497/1770 train_time:147785ms step_avg:99.38ms
step:1498/1770 train_time:147888ms step_avg:99.39ms
step:1499/1770 train_time:147994ms step_avg:99.39ms
step:1500/1770 train_time:148094ms step_avg:99.39ms
step:1500/1770 val_loss:3.3557 train_time:148196ms step_avg:99.46ms
step:1501/1770 train_time:148217ms step_avg:99.41ms
step:1502/1770 train_time:148309ms step_avg:99.40ms
step:1503/1770 train_time:148413ms step_avg:99.41ms
step:1504/1770 train_time:148517ms step_avg:99.41ms
step:1505/1770 train_time:148623ms step_avg:99.41ms
step:1506/1770 train_time:148727ms step_avg:99.42ms
step:1507/1770 train_time:148832ms step_avg:99.42ms
step:1508/1770 train_time:148938ms step_avg:99.42ms
step:1509/1770 train_time:149041ms step_avg:99.43ms
step:1510/1770 train_time:149144ms step_avg:99.43ms
step:1511/1770 train_time:149250ms step_avg:99.43ms
step:1512/1770 train_time:149355ms step_avg:99.44ms
step:1513/1770 train_time:149458ms step_avg:99.44ms
step:1514/1770 train_time:149562ms step_avg:99.44ms
step:1515/1770 train_time:149666ms step_avg:99.45ms
step:1516/1770 train_time:149770ms step_avg:99.45ms
step:1517/1770 train_time:149873ms step_avg:99.45ms
step:1518/1770 train_time:149979ms step_avg:99.46ms
step:1519/1770 train_time:150081ms step_avg:99.46ms
step:1520/1770 train_time:150186ms step_avg:99.46ms
step:1521/1770 train_time:150290ms step_avg:99.46ms
step:1522/1770 train_time:150394ms step_avg:99.47ms
step:1523/1770 train_time:150499ms step_avg:99.47ms
step:1524/1770 train_time:150602ms step_avg:99.47ms
step:1525/1770 train_time:150706ms step_avg:99.48ms
step:1526/1770 train_time:150810ms step_avg:99.48ms
step:1527/1770 train_time:150914ms step_avg:99.48ms
step:1528/1770 train_time:151020ms step_avg:99.49ms
step:1529/1770 train_time:151123ms step_avg:99.49ms
step:1530/1770 train_time:151226ms step_avg:99.49ms
step:1531/1770 train_time:151330ms step_avg:99.49ms
step:1532/1770 train_time:151435ms step_avg:99.50ms
step:1533/1770 train_time:151539ms step_avg:99.50ms
step:1534/1770 train_time:151644ms step_avg:99.50ms
step:1535/1770 train_time:151746ms step_avg:99.51ms
step:1536/1770 train_time:151850ms step_avg:99.51ms
step:1537/1770 train_time:151955ms step_avg:99.51ms
step:1538/1770 train_time:152062ms step_avg:99.52ms
step:1539/1770 train_time:152165ms step_avg:99.52ms
step:1540/1770 train_time:152272ms step_avg:99.52ms
step:1541/1770 train_time:152377ms step_avg:99.53ms
step:1542/1770 train_time:152481ms step_avg:99.53ms
step:1543/1770 train_time:152584ms step_avg:99.53ms
step:1544/1770 train_time:152691ms step_avg:99.54ms
step:1545/1770 train_time:152795ms step_avg:99.54ms
step:1546/1770 train_time:152900ms step_avg:99.54ms
step:1547/1770 train_time:153003ms step_avg:99.55ms
step:1548/1770 train_time:153107ms step_avg:99.55ms
step:1549/1770 train_time:153211ms step_avg:99.55ms
step:1550/1770 train_time:153316ms step_avg:99.56ms
step:1551/1770 train_time:153419ms step_avg:99.56ms
step:1552/1770 train_time:153524ms step_avg:99.56ms
step:1553/1770 train_time:153629ms step_avg:99.57ms
step:1554/1770 train_time:153732ms step_avg:99.57ms
step:1555/1770 train_time:153837ms step_avg:99.57ms
step:1556/1770 train_time:153941ms step_avg:99.57ms
step:1557/1770 train_time:154045ms step_avg:99.58ms
step:1558/1770 train_time:154149ms step_avg:99.58ms
step:1559/1770 train_time:154255ms step_avg:99.58ms
step:1560/1770 train_time:154358ms step_avg:99.59ms
step:1561/1770 train_time:154465ms step_avg:99.59ms
step:1562/1770 train_time:154568ms step_avg:99.59ms
step:1563/1770 train_time:154673ms step_avg:99.60ms
step:1564/1770 train_time:154776ms step_avg:99.60ms
step:1565/1770 train_time:154879ms step_avg:99.60ms
step:1566/1770 train_time:154983ms step_avg:99.60ms
step:1567/1770 train_time:155087ms step_avg:99.61ms
step:1568/1770 train_time:155191ms step_avg:99.61ms
step:1569/1770 train_time:155298ms step_avg:99.61ms
step:1570/1770 train_time:155402ms step_avg:99.62ms
step:1571/1770 train_time:155506ms step_avg:99.62ms
step:1572/1770 train_time:155611ms step_avg:99.62ms
step:1573/1770 train_time:155718ms step_avg:99.63ms
step:1574/1770 train_time:155822ms step_avg:99.63ms
step:1575/1770 train_time:155924ms step_avg:99.63ms
step:1576/1770 train_time:156029ms step_avg:99.64ms
step:1577/1770 train_time:156134ms step_avg:99.64ms
step:1578/1770 train_time:156239ms step_avg:99.64ms
step:1579/1770 train_time:156344ms step_avg:99.65ms
step:1580/1770 train_time:156447ms step_avg:99.65ms
step:1581/1770 train_time:156554ms step_avg:99.65ms
step:1582/1770 train_time:156660ms step_avg:99.66ms
step:1583/1770 train_time:156764ms step_avg:99.66ms
step:1584/1770 train_time:156869ms step_avg:99.66ms
step:1585/1770 train_time:156974ms step_avg:99.67ms
step:1586/1770 train_time:157082ms step_avg:99.67ms
step:1587/1770 train_time:157187ms step_avg:99.67ms
step:1588/1770 train_time:157291ms step_avg:99.68ms
step:1589/1770 train_time:157397ms step_avg:99.68ms
step:1590/1770 train_time:157501ms step_avg:99.68ms
step:1591/1770 train_time:157604ms step_avg:99.69ms
step:1592/1770 train_time:157709ms step_avg:99.69ms
step:1593/1770 train_time:157813ms step_avg:99.69ms
step:1594/1770 train_time:157916ms step_avg:99.69ms
step:1595/1770 train_time:158020ms step_avg:99.70ms
step:1596/1770 train_time:158125ms step_avg:99.70ms
step:1597/1770 train_time:158228ms step_avg:99.70ms
step:1598/1770 train_time:158332ms step_avg:99.71ms
step:1599/1770 train_time:158438ms step_avg:99.71ms
step:1600/1770 train_time:158544ms step_avg:99.71ms
step:1601/1770 train_time:158648ms step_avg:99.72ms
step:1602/1770 train_time:158754ms step_avg:99.72ms
step:1603/1770 train_time:158858ms step_avg:99.72ms
step:1604/1770 train_time:158961ms step_avg:99.72ms
step:1605/1770 train_time:159064ms step_avg:99.73ms
step:1606/1770 train_time:159168ms step_avg:99.73ms
step:1607/1770 train_time:159276ms step_avg:99.73ms
step:1608/1770 train_time:159379ms step_avg:99.74ms
step:1609/1770 train_time:159483ms step_avg:99.74ms
step:1610/1770 train_time:159588ms step_avg:99.74ms
step:1611/1770 train_time:159695ms step_avg:99.75ms
step:1612/1770 train_time:159801ms step_avg:99.75ms
step:1613/1770 train_time:159904ms step_avg:99.75ms
step:1614/1770 train_time:160007ms step_avg:99.76ms
step:1615/1770 train_time:160112ms step_avg:99.76ms
step:1616/1770 train_time:160216ms step_avg:99.76ms
step:1617/1770 train_time:160322ms step_avg:99.76ms
step:1618/1770 train_time:160427ms step_avg:99.77ms
step:1619/1770 train_time:160531ms step_avg:99.77ms
step:1620/1770 train_time:160635ms step_avg:99.77ms
step:1621/1770 train_time:160739ms step_avg:99.78ms
step:1622/1770 train_time:160844ms step_avg:99.78ms
step:1623/1770 train_time:160951ms step_avg:99.78ms
step:1624/1770 train_time:161054ms step_avg:99.79ms
step:1625/1770 train_time:161157ms step_avg:99.79ms
step:1625/1770 val_loss:3.3228 train_time:161260ms step_avg:99.85ms
step:1626/1770 train_time:161280ms step_avg:99.80ms
step:1627/1770 train_time:161371ms step_avg:99.80ms
step:1628/1770 train_time:161476ms step_avg:99.80ms
step:1629/1770 train_time:161579ms step_avg:99.80ms
step:1630/1770 train_time:161682ms step_avg:99.80ms
step:1631/1770 train_time:161786ms step_avg:99.81ms
step:1632/1770 train_time:161889ms step_avg:99.81ms
step:1633/1770 train_time:161993ms step_avg:99.81ms
step:1634/1770 train_time:162097ms step_avg:99.81ms
step:1635/1770 train_time:162202ms step_avg:99.82ms
step:1636/1770 train_time:162306ms step_avg:99.82ms
step:1637/1770 train_time:162410ms step_avg:99.82ms
step:1638/1770 train_time:162514ms step_avg:99.82ms
step:1639/1770 train_time:162618ms step_avg:99.83ms
step:1640/1770 train_time:162723ms step_avg:99.83ms
step:1641/1770 train_time:162827ms step_avg:99.83ms
step:1642/1770 train_time:162930ms step_avg:99.83ms
step:1643/1770 train_time:163034ms step_avg:99.84ms
step:1644/1770 train_time:163140ms step_avg:99.84ms
step:1645/1770 train_time:163243ms step_avg:99.84ms
step:1646/1770 train_time:163349ms step_avg:99.85ms
step:1647/1770 train_time:163454ms step_avg:99.85ms
step:1648/1770 train_time:163557ms step_avg:99.85ms
step:1649/1770 train_time:163662ms step_avg:99.85ms
step:1650/1770 train_time:163765ms step_avg:99.86ms
step:1651/1770 train_time:163868ms step_avg:99.86ms
step:1652/1770 train_time:163973ms step_avg:99.86ms
step:1653/1770 train_time:164077ms step_avg:99.86ms
step:1654/1770 train_time:164190ms step_avg:99.87ms
step:1655/1770 train_time:164291ms step_avg:99.87ms
step:1656/1770 train_time:164395ms step_avg:99.88ms
step:1657/1770 train_time:164502ms step_avg:99.88ms
step:1658/1770 train_time:164606ms step_avg:99.88ms
step:1659/1770 train_time:164711ms step_avg:99.89ms
step:1660/1770 train_time:164814ms step_avg:99.89ms
step:1661/1770 train_time:164920ms step_avg:99.89ms
step:1662/1770 train_time:165024ms step_avg:99.89ms
step:1663/1770 train_time:165127ms step_avg:99.90ms
step:1664/1770 train_time:165230ms step_avg:99.90ms
step:1665/1770 train_time:165333ms step_avg:99.90ms
step:1666/1770 train_time:165438ms step_avg:99.90ms
step:1667/1770 train_time:165542ms step_avg:99.90ms
step:1668/1770 train_time:165646ms step_avg:99.91ms
step:1669/1770 train_time:165749ms step_avg:99.91ms
step:1670/1770 train_time:165853ms step_avg:99.91ms
step:1671/1770 train_time:165957ms step_avg:99.91ms
step:1672/1770 train_time:166062ms step_avg:99.92ms
step:1673/1770 train_time:166167ms step_avg:99.92ms
step:1674/1770 train_time:166271ms step_avg:99.92ms
step:1675/1770 train_time:166374ms step_avg:99.92ms
step:1676/1770 train_time:166480ms step_avg:99.93ms
step:1677/1770 train_time:166588ms step_avg:99.93ms
step:1678/1770 train_time:166691ms step_avg:99.93ms
step:1679/1770 train_time:166795ms step_avg:99.94ms
step:1680/1770 train_time:166900ms step_avg:99.94ms
step:1681/1770 train_time:167005ms step_avg:99.94ms
step:1682/1770 train_time:167110ms step_avg:99.95ms
step:1683/1770 train_time:167214ms step_avg:99.95ms
step:1684/1770 train_time:167318ms step_avg:99.95ms
step:1685/1770 train_time:167422ms step_avg:99.95ms
step:1686/1770 train_time:167527ms step_avg:99.96ms
step:1687/1770 train_time:167632ms step_avg:99.96ms
step:1688/1770 train_time:167736ms step_avg:99.96ms
step:1689/1770 train_time:167841ms step_avg:99.96ms
step:1690/1770 train_time:167944ms step_avg:99.97ms
step:1691/1770 train_time:168048ms step_avg:99.97ms
step:1692/1770 train_time:168152ms step_avg:99.97ms
step:1693/1770 train_time:168258ms step_avg:99.98ms
step:1694/1770 train_time:168362ms step_avg:99.98ms
step:1695/1770 train_time:168467ms step_avg:99.98ms
step:1696/1770 train_time:168572ms step_avg:99.98ms
step:1697/1770 train_time:168678ms step_avg:99.99ms
step:1698/1770 train_time:168782ms step_avg:99.99ms
step:1699/1770 train_time:168886ms step_avg:99.99ms
step:1700/1770 train_time:168993ms step_avg:100.00ms
step:1701/1770 train_time:169093ms step_avg:100.00ms
step:1702/1770 train_time:169199ms step_avg:100.00ms
step:1703/1770 train_time:169302ms step_avg:100.00ms
step:1704/1770 train_time:169406ms step_avg:100.00ms
step:1705/1770 train_time:169509ms step_avg:100.01ms
step:1706/1770 train_time:169612ms step_avg:100.01ms
step:1707/1770 train_time:169717ms step_avg:100.01ms
step:1708/1770 train_time:169823ms step_avg:100.01ms
step:1709/1770 train_time:169929ms step_avg:100.02ms
step:1710/1770 train_time:170036ms step_avg:100.02ms
step:1711/1770 train_time:170142ms step_avg:100.02ms
step:1712/1770 train_time:170247ms step_avg:100.03ms
step:1713/1770 train_time:170351ms step_avg:100.03ms
step:1714/1770 train_time:170457ms step_avg:100.03ms
step:1715/1770 train_time:170561ms step_avg:100.04ms
step:1716/1770 train_time:170666ms step_avg:100.04ms
step:1717/1770 train_time:170770ms step_avg:100.04ms
step:1718/1770 train_time:170876ms step_avg:100.04ms
step:1719/1770 train_time:170982ms step_avg:100.05ms
step:1720/1770 train_time:171087ms step_avg:100.05ms
step:1721/1770 train_time:171191ms step_avg:100.05ms
step:1722/1770 train_time:171299ms step_avg:100.06ms
step:1723/1770 train_time:171405ms step_avg:100.06ms
step:1724/1770 train_time:171512ms step_avg:100.07ms
step:1725/1770 train_time:171620ms step_avg:100.07ms
step:1726/1770 train_time:171727ms step_avg:100.07ms
step:1727/1770 train_time:171831ms step_avg:100.08ms
step:1728/1770 train_time:171938ms step_avg:100.08ms
step:1729/1770 train_time:172043ms step_avg:100.08ms
step:1730/1770 train_time:172148ms step_avg:100.09ms
step:1731/1770 train_time:172255ms step_avg:100.09ms
step:1732/1770 train_time:172359ms step_avg:100.09ms
step:1733/1770 train_time:172466ms step_avg:100.10ms
step:1734/1770 train_time:172570ms step_avg:100.10ms
step:1735/1770 train_time:172676ms step_avg:100.10ms
step:1736/1770 train_time:172780ms step_avg:100.10ms
step:1737/1770 train_time:172885ms step_avg:100.11ms
step:1738/1770 train_time:172989ms step_avg:100.11ms
step:1739/1770 train_time:173095ms step_avg:100.11ms
step:1740/1770 train_time:173199ms step_avg:100.12ms
step:1741/1770 train_time:173307ms step_avg:100.12ms
step:1742/1770 train_time:173415ms step_avg:100.12ms
step:1743/1770 train_time:173521ms step_avg:100.13ms
step:1744/1770 train_time:173626ms step_avg:100.13ms
step:1745/1770 train_time:173730ms step_avg:100.13ms
step:1746/1770 train_time:173838ms step_avg:100.14ms
step:1747/1770 train_time:173942ms step_avg:100.14ms
step:1748/1770 train_time:174048ms step_avg:100.14ms
step:1749/1770 train_time:174154ms step_avg:100.15ms
step:1750/1770 train_time:174259ms step_avg:100.15ms
step:1750/1770 val_loss:3.2986 train_time:174362ms step_avg:100.21ms
step:1751/1770 train_time:174384ms step_avg:100.16ms
step:1752/1770 train_time:174475ms step_avg:100.16ms
step:1753/1770 train_time:174581ms step_avg:100.16ms
step:1754/1770 train_time:174686ms step_avg:100.16ms
step:1755/1770 train_time:174791ms step_avg:100.17ms
step:1756/1770 train_time:174895ms step_avg:100.17ms
step:1757/1770 train_time:175001ms step_avg:100.17ms
step:1758/1770 train_time:175106ms step_avg:100.17ms
step:1759/1770 train_time:175212ms step_avg:100.18ms
step:1760/1770 train_time:175317ms step_avg:100.18ms
step:1761/1770 train_time:175424ms step_avg:100.19ms
step:1762/1770 train_time:175536ms step_avg:100.19ms
step:1763/1770 train_time:175637ms step_avg:100.19ms
step:1764/1770 train_time:175743ms step_avg:100.20ms
step:1765/1770 train_time:175848ms step_avg:100.20ms
step:1766/1770 train_time:175958ms step_avg:100.20ms
step:1767/1770 train_time:176062ms step_avg:100.21ms
step:1768/1770 train_time:176168ms step_avg:100.21ms
step:1769/1770 train_time:176272ms step_avg:100.21ms
step:1770/1770 train_time:176376ms step_avg:100.21ms
step:1770/1770 val_loss:3.2957 train_time:176481ms step_avg:100.27ms
peak memory allocated: 28840 MiB reserved: 32232 MiB
