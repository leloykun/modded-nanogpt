import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:31:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0             110W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24343ms step_avg:nanms
step:2/1770 train_time:24809ms step_avg:nanms
step:3/1770 train_time:24906ms step_avg:nanms
step:4/1770 train_time:25000ms step_avg:nanms
step:5/1770 train_time:25093ms step_avg:nanms
step:6/1770 train_time:25186ms step_avg:nanms
step:7/1770 train_time:25279ms step_avg:nanms
step:8/1770 train_time:25372ms step_avg:nanms
step:9/1770 train_time:25466ms step_avg:nanms
step:10/1770 train_time:25559ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.52ms
step:14/1770 train_time:375ms step_avg:93.86ms
step:15/1770 train_time:470ms step_avg:93.92ms
step:16/1770 train_time:563ms step_avg:93.80ms
step:17/1770 train_time:656ms step_avg:93.78ms
step:18/1770 train_time:750ms step_avg:93.73ms
step:19/1770 train_time:844ms step_avg:93.74ms
step:20/1770 train_time:937ms step_avg:93.71ms
step:21/1770 train_time:1030ms step_avg:93.67ms
step:22/1770 train_time:1124ms step_avg:93.63ms
step:23/1770 train_time:1217ms step_avg:93.63ms
step:24/1770 train_time:1311ms step_avg:93.62ms
step:25/1770 train_time:1404ms step_avg:93.62ms
step:26/1770 train_time:1498ms step_avg:93.61ms
step:27/1770 train_time:1592ms step_avg:93.62ms
step:28/1770 train_time:1685ms step_avg:93.63ms
step:29/1770 train_time:1779ms step_avg:93.64ms
step:30/1770 train_time:1873ms step_avg:93.65ms
step:31/1770 train_time:1967ms step_avg:93.65ms
step:32/1770 train_time:2060ms step_avg:93.64ms
step:33/1770 train_time:2153ms step_avg:93.63ms
step:34/1770 train_time:2247ms step_avg:93.63ms
step:35/1770 train_time:2342ms step_avg:93.66ms
step:36/1770 train_time:2435ms step_avg:93.64ms
step:37/1770 train_time:2529ms step_avg:93.65ms
step:38/1770 train_time:2622ms step_avg:93.65ms
step:39/1770 train_time:2715ms step_avg:93.64ms
step:40/1770 train_time:2809ms step_avg:93.63ms
step:41/1770 train_time:2903ms step_avg:93.64ms
step:42/1770 train_time:2996ms step_avg:93.64ms
step:43/1770 train_time:3090ms step_avg:93.64ms
step:44/1770 train_time:3184ms step_avg:93.64ms
step:45/1770 train_time:3278ms step_avg:93.65ms
step:46/1770 train_time:3371ms step_avg:93.65ms
step:47/1770 train_time:3465ms step_avg:93.66ms
step:48/1770 train_time:3559ms step_avg:93.65ms
step:49/1770 train_time:3652ms step_avg:93.65ms
step:50/1770 train_time:3747ms step_avg:93.67ms
step:51/1770 train_time:3840ms step_avg:93.65ms
step:52/1770 train_time:3933ms step_avg:93.65ms
step:53/1770 train_time:4027ms step_avg:93.64ms
step:54/1770 train_time:4120ms step_avg:93.63ms
step:55/1770 train_time:4214ms step_avg:93.65ms
step:56/1770 train_time:4308ms step_avg:93.65ms
step:57/1770 train_time:4402ms step_avg:93.66ms
step:58/1770 train_time:4496ms step_avg:93.66ms
step:59/1770 train_time:4589ms step_avg:93.66ms
step:60/1770 train_time:4683ms step_avg:93.66ms
step:61/1770 train_time:4777ms step_avg:93.66ms
step:62/1770 train_time:4870ms step_avg:93.65ms
step:63/1770 train_time:4963ms step_avg:93.65ms
step:64/1770 train_time:5057ms step_avg:93.65ms
step:65/1770 train_time:5151ms step_avg:93.66ms
step:66/1770 train_time:5244ms step_avg:93.64ms
step:67/1770 train_time:5338ms step_avg:93.65ms
step:68/1770 train_time:5431ms step_avg:93.64ms
step:69/1770 train_time:5525ms step_avg:93.65ms
step:70/1770 train_time:5619ms step_avg:93.65ms
step:71/1770 train_time:5713ms step_avg:93.65ms
step:72/1770 train_time:5806ms step_avg:93.65ms
step:73/1770 train_time:5900ms step_avg:93.65ms
step:74/1770 train_time:5993ms step_avg:93.65ms
step:75/1770 train_time:6087ms step_avg:93.64ms
step:76/1770 train_time:6180ms step_avg:93.63ms
step:77/1770 train_time:6273ms step_avg:93.63ms
step:78/1770 train_time:6366ms step_avg:93.62ms
step:79/1770 train_time:6460ms step_avg:93.62ms
step:80/1770 train_time:6554ms step_avg:93.62ms
step:81/1770 train_time:6647ms step_avg:93.62ms
step:82/1770 train_time:6741ms step_avg:93.62ms
step:83/1770 train_time:6834ms step_avg:93.62ms
step:84/1770 train_time:6928ms step_avg:93.62ms
step:85/1770 train_time:7023ms step_avg:93.63ms
step:86/1770 train_time:7117ms step_avg:93.64ms
step:87/1770 train_time:7210ms step_avg:93.64ms
step:88/1770 train_time:7303ms step_avg:93.63ms
step:89/1770 train_time:7397ms step_avg:93.63ms
step:90/1770 train_time:7493ms step_avg:93.66ms
step:91/1770 train_time:7584ms step_avg:93.63ms
step:92/1770 train_time:7677ms step_avg:93.63ms
step:93/1770 train_time:7771ms step_avg:93.62ms
step:94/1770 train_time:7865ms step_avg:93.63ms
step:95/1770 train_time:7958ms step_avg:93.62ms
step:96/1770 train_time:8052ms step_avg:93.62ms
step:97/1770 train_time:8145ms step_avg:93.62ms
step:98/1770 train_time:8239ms step_avg:93.62ms
step:99/1770 train_time:8332ms step_avg:93.62ms
step:100/1770 train_time:8426ms step_avg:93.62ms
step:101/1770 train_time:8519ms step_avg:93.62ms
step:102/1770 train_time:8613ms step_avg:93.62ms
step:103/1770 train_time:8707ms step_avg:93.62ms
step:104/1770 train_time:8800ms step_avg:93.62ms
step:105/1770 train_time:8894ms step_avg:93.62ms
step:106/1770 train_time:8987ms step_avg:93.62ms
step:107/1770 train_time:9081ms step_avg:93.62ms
step:108/1770 train_time:9175ms step_avg:93.62ms
step:109/1770 train_time:9268ms step_avg:93.62ms
step:110/1770 train_time:9362ms step_avg:93.62ms
step:111/1770 train_time:9455ms step_avg:93.62ms
step:112/1770 train_time:9549ms step_avg:93.62ms
step:113/1770 train_time:9643ms step_avg:93.62ms
step:114/1770 train_time:9736ms step_avg:93.62ms
step:115/1770 train_time:9830ms step_avg:93.62ms
step:116/1770 train_time:9924ms step_avg:93.62ms
step:117/1770 train_time:10017ms step_avg:93.62ms
step:118/1770 train_time:10110ms step_avg:93.62ms
step:119/1770 train_time:10204ms step_avg:93.61ms
step:120/1770 train_time:10298ms step_avg:93.62ms
step:121/1770 train_time:10391ms step_avg:93.62ms
step:122/1770 train_time:10485ms step_avg:93.62ms
step:123/1770 train_time:10579ms step_avg:93.62ms
step:124/1770 train_time:10672ms step_avg:93.61ms
step:125/1770 train_time:10765ms step_avg:93.61ms
step:125/1770 val_loss:4.6419 train_time:10858ms step_avg:94.41ms
step:126/1770 train_time:10879ms step_avg:93.79ms
step:127/1770 train_time:10955ms step_avg:93.64ms
step:128/1770 train_time:11052ms step_avg:93.66ms
step:129/1770 train_time:11152ms step_avg:93.71ms
step:130/1770 train_time:11247ms step_avg:93.72ms
step:131/1770 train_time:11340ms step_avg:93.72ms
step:132/1770 train_time:11434ms step_avg:93.72ms
step:133/1770 train_time:11527ms step_avg:93.71ms
step:134/1770 train_time:11621ms step_avg:93.72ms
step:135/1770 train_time:11714ms step_avg:93.72ms
step:136/1770 train_time:11808ms step_avg:93.71ms
step:137/1770 train_time:11902ms step_avg:93.72ms
step:138/1770 train_time:11996ms step_avg:93.72ms
step:139/1770 train_time:12090ms step_avg:93.72ms
step:140/1770 train_time:12184ms step_avg:93.73ms
step:141/1770 train_time:12279ms step_avg:93.73ms
step:142/1770 train_time:12373ms step_avg:93.74ms
step:143/1770 train_time:12467ms step_avg:93.74ms
step:144/1770 train_time:12561ms step_avg:93.74ms
step:145/1770 train_time:12655ms step_avg:93.74ms
step:146/1770 train_time:12748ms step_avg:93.74ms
step:147/1770 train_time:12843ms step_avg:93.74ms
step:148/1770 train_time:12937ms step_avg:93.75ms
step:149/1770 train_time:13031ms step_avg:93.75ms
step:150/1770 train_time:13125ms step_avg:93.75ms
step:151/1770 train_time:13219ms step_avg:93.75ms
step:152/1770 train_time:13313ms step_avg:93.76ms
step:153/1770 train_time:13407ms step_avg:93.76ms
step:154/1770 train_time:13502ms step_avg:93.76ms
step:155/1770 train_time:13596ms step_avg:93.77ms
step:156/1770 train_time:13690ms step_avg:93.77ms
step:157/1770 train_time:13784ms step_avg:93.77ms
step:158/1770 train_time:13878ms step_avg:93.77ms
step:159/1770 train_time:13972ms step_avg:93.77ms
step:160/1770 train_time:14067ms step_avg:93.78ms
step:161/1770 train_time:14161ms step_avg:93.78ms
step:162/1770 train_time:14255ms step_avg:93.78ms
step:163/1770 train_time:14350ms step_avg:93.79ms
step:164/1770 train_time:14444ms step_avg:93.79ms
step:165/1770 train_time:14538ms step_avg:93.80ms
step:166/1770 train_time:14632ms step_avg:93.80ms
step:167/1770 train_time:14726ms step_avg:93.80ms
step:168/1770 train_time:14820ms step_avg:93.80ms
step:169/1770 train_time:14914ms step_avg:93.80ms
step:170/1770 train_time:15009ms step_avg:93.80ms
step:171/1770 train_time:15102ms step_avg:93.80ms
step:172/1770 train_time:15196ms step_avg:93.80ms
step:173/1770 train_time:15290ms step_avg:93.80ms
step:174/1770 train_time:15384ms step_avg:93.81ms
step:175/1770 train_time:15478ms step_avg:93.81ms
step:176/1770 train_time:15572ms step_avg:93.81ms
step:177/1770 train_time:15667ms step_avg:93.81ms
step:178/1770 train_time:15761ms step_avg:93.82ms
step:179/1770 train_time:15855ms step_avg:93.82ms
step:180/1770 train_time:15949ms step_avg:93.82ms
step:181/1770 train_time:16044ms step_avg:93.82ms
step:182/1770 train_time:16137ms step_avg:93.82ms
step:183/1770 train_time:16231ms step_avg:93.82ms
step:184/1770 train_time:16325ms step_avg:93.82ms
step:185/1770 train_time:16419ms step_avg:93.82ms
step:186/1770 train_time:16513ms step_avg:93.82ms
step:187/1770 train_time:16608ms step_avg:93.83ms
step:188/1770 train_time:16701ms step_avg:93.83ms
step:189/1770 train_time:16795ms step_avg:93.83ms
step:190/1770 train_time:16889ms step_avg:93.83ms
step:191/1770 train_time:16983ms step_avg:93.83ms
step:192/1770 train_time:17077ms step_avg:93.83ms
step:193/1770 train_time:17171ms step_avg:93.83ms
step:194/1770 train_time:17266ms step_avg:93.84ms
step:195/1770 train_time:17361ms step_avg:93.84ms
step:196/1770 train_time:17455ms step_avg:93.84ms
step:197/1770 train_time:17549ms step_avg:93.84ms
step:198/1770 train_time:17643ms step_avg:93.85ms
step:199/1770 train_time:17737ms step_avg:93.85ms
step:200/1770 train_time:17831ms step_avg:93.85ms
step:201/1770 train_time:17926ms step_avg:93.85ms
step:202/1770 train_time:18020ms step_avg:93.86ms
step:203/1770 train_time:18115ms step_avg:93.86ms
step:204/1770 train_time:18209ms step_avg:93.86ms
step:205/1770 train_time:18303ms step_avg:93.86ms
step:206/1770 train_time:18397ms step_avg:93.86ms
step:207/1770 train_time:18491ms step_avg:93.86ms
step:208/1770 train_time:18586ms step_avg:93.87ms
step:209/1770 train_time:18680ms step_avg:93.87ms
step:210/1770 train_time:18774ms step_avg:93.87ms
step:211/1770 train_time:18868ms step_avg:93.87ms
step:212/1770 train_time:18962ms step_avg:93.87ms
step:213/1770 train_time:19057ms step_avg:93.88ms
step:214/1770 train_time:19151ms step_avg:93.88ms
step:215/1770 train_time:19245ms step_avg:93.88ms
step:216/1770 train_time:19339ms step_avg:93.88ms
step:217/1770 train_time:19434ms step_avg:93.88ms
step:218/1770 train_time:19529ms step_avg:93.89ms
step:219/1770 train_time:19623ms step_avg:93.89ms
step:220/1770 train_time:19717ms step_avg:93.89ms
step:221/1770 train_time:19811ms step_avg:93.89ms
step:222/1770 train_time:19905ms step_avg:93.89ms
step:223/1770 train_time:20000ms step_avg:93.89ms
step:224/1770 train_time:20094ms step_avg:93.90ms
step:225/1770 train_time:20188ms step_avg:93.90ms
step:226/1770 train_time:20282ms step_avg:93.90ms
step:227/1770 train_time:20376ms step_avg:93.90ms
step:228/1770 train_time:20470ms step_avg:93.90ms
step:229/1770 train_time:20565ms step_avg:93.90ms
step:230/1770 train_time:20659ms step_avg:93.90ms
step:231/1770 train_time:20753ms step_avg:93.90ms
step:232/1770 train_time:20847ms step_avg:93.91ms
step:233/1770 train_time:20941ms step_avg:93.91ms
step:234/1770 train_time:21036ms step_avg:93.91ms
step:235/1770 train_time:21130ms step_avg:93.91ms
step:236/1770 train_time:21224ms step_avg:93.91ms
step:237/1770 train_time:21319ms step_avg:93.92ms
step:238/1770 train_time:21413ms step_avg:93.92ms
step:239/1770 train_time:21508ms step_avg:93.92ms
step:240/1770 train_time:21602ms step_avg:93.92ms
step:241/1770 train_time:21697ms step_avg:93.93ms
step:242/1770 train_time:21791ms step_avg:93.93ms
step:243/1770 train_time:21885ms step_avg:93.93ms
step:244/1770 train_time:21979ms step_avg:93.93ms
step:245/1770 train_time:22073ms step_avg:93.93ms
step:246/1770 train_time:22168ms step_avg:93.93ms
step:247/1770 train_time:22262ms step_avg:93.93ms
step:248/1770 train_time:22356ms step_avg:93.93ms
step:249/1770 train_time:22451ms step_avg:93.94ms
step:250/1770 train_time:22545ms step_avg:93.94ms
step:250/1770 val_loss:4.1094 train_time:22638ms step_avg:94.32ms
step:251/1770 train_time:22660ms step_avg:94.03ms
step:252/1770 train_time:22740ms step_avg:93.97ms
step:253/1770 train_time:22841ms step_avg:94.00ms
step:254/1770 train_time:22936ms step_avg:94.00ms
step:255/1770 train_time:23030ms step_avg:94.00ms
step:256/1770 train_time:23124ms step_avg:94.00ms
step:257/1770 train_time:23218ms step_avg:94.00ms
step:258/1770 train_time:23311ms step_avg:94.00ms
step:259/1770 train_time:23405ms step_avg:94.00ms
step:260/1770 train_time:23499ms step_avg:94.00ms
step:261/1770 train_time:23593ms step_avg:94.00ms
step:262/1770 train_time:23687ms step_avg:94.00ms
step:263/1770 train_time:23782ms step_avg:94.00ms
step:264/1770 train_time:23877ms step_avg:94.01ms
step:265/1770 train_time:23974ms step_avg:94.01ms
step:266/1770 train_time:24069ms step_avg:94.02ms
step:267/1770 train_time:24163ms step_avg:94.02ms
step:268/1770 train_time:24259ms step_avg:94.03ms
step:269/1770 train_time:24352ms step_avg:94.02ms
step:270/1770 train_time:24447ms step_avg:94.03ms
step:271/1770 train_time:24541ms step_avg:94.03ms
step:272/1770 train_time:24636ms step_avg:94.03ms
step:273/1770 train_time:24730ms step_avg:94.03ms
step:274/1770 train_time:24825ms step_avg:94.03ms
step:275/1770 train_time:24921ms step_avg:94.04ms
step:276/1770 train_time:25016ms step_avg:94.05ms
step:277/1770 train_time:25111ms step_avg:94.05ms
step:278/1770 train_time:25206ms step_avg:94.05ms
step:279/1770 train_time:25300ms step_avg:94.05ms
step:280/1770 train_time:25395ms step_avg:94.05ms
step:281/1770 train_time:25489ms step_avg:94.06ms
step:282/1770 train_time:25584ms step_avg:94.06ms
step:283/1770 train_time:25678ms step_avg:94.06ms
step:284/1770 train_time:25773ms step_avg:94.06ms
step:285/1770 train_time:25868ms step_avg:94.07ms
step:286/1770 train_time:25963ms step_avg:94.07ms
step:287/1770 train_time:26058ms step_avg:94.07ms
step:288/1770 train_time:26153ms step_avg:94.07ms
step:289/1770 train_time:26248ms step_avg:94.08ms
step:290/1770 train_time:26343ms step_avg:94.08ms
step:291/1770 train_time:26438ms step_avg:94.09ms
step:292/1770 train_time:26532ms step_avg:94.09ms
step:293/1770 train_time:26627ms step_avg:94.09ms
step:294/1770 train_time:26721ms step_avg:94.09ms
step:295/1770 train_time:26817ms step_avg:94.09ms
step:296/1770 train_time:26912ms step_avg:94.10ms
step:297/1770 train_time:27006ms step_avg:94.10ms
step:298/1770 train_time:27101ms step_avg:94.10ms
step:299/1770 train_time:27196ms step_avg:94.10ms
step:300/1770 train_time:27290ms step_avg:94.10ms
step:301/1770 train_time:27385ms step_avg:94.11ms
step:302/1770 train_time:27480ms step_avg:94.11ms
step:303/1770 train_time:27575ms step_avg:94.11ms
step:304/1770 train_time:27670ms step_avg:94.12ms
step:305/1770 train_time:27765ms step_avg:94.12ms
step:306/1770 train_time:27860ms step_avg:94.12ms
step:307/1770 train_time:27956ms step_avg:94.13ms
step:308/1770 train_time:28050ms step_avg:94.13ms
step:309/1770 train_time:28145ms step_avg:94.13ms
step:310/1770 train_time:28241ms step_avg:94.14ms
step:311/1770 train_time:28336ms step_avg:94.14ms
step:312/1770 train_time:28431ms step_avg:94.14ms
step:313/1770 train_time:28526ms step_avg:94.14ms
step:314/1770 train_time:28621ms step_avg:94.15ms
step:315/1770 train_time:28715ms step_avg:94.15ms
step:316/1770 train_time:28810ms step_avg:94.15ms
step:317/1770 train_time:28905ms step_avg:94.15ms
step:318/1770 train_time:29000ms step_avg:94.16ms
step:319/1770 train_time:29095ms step_avg:94.16ms
step:320/1770 train_time:29191ms step_avg:94.16ms
step:321/1770 train_time:29285ms step_avg:94.17ms
step:322/1770 train_time:29380ms step_avg:94.17ms
step:323/1770 train_time:29475ms step_avg:94.17ms
step:324/1770 train_time:29570ms step_avg:94.17ms
step:325/1770 train_time:29665ms step_avg:94.18ms
step:326/1770 train_time:29761ms step_avg:94.18ms
step:327/1770 train_time:29855ms step_avg:94.18ms
step:328/1770 train_time:29950ms step_avg:94.18ms
step:329/1770 train_time:30045ms step_avg:94.18ms
step:330/1770 train_time:30140ms step_avg:94.19ms
step:331/1770 train_time:30234ms step_avg:94.19ms
step:332/1770 train_time:30330ms step_avg:94.19ms
step:333/1770 train_time:30424ms step_avg:94.19ms
step:334/1770 train_time:30519ms step_avg:94.19ms
step:335/1770 train_time:30613ms step_avg:94.20ms
step:336/1770 train_time:30708ms step_avg:94.20ms
step:337/1770 train_time:30803ms step_avg:94.20ms
step:338/1770 train_time:30897ms step_avg:94.20ms
step:339/1770 train_time:30992ms step_avg:94.20ms
step:340/1770 train_time:31087ms step_avg:94.20ms
step:341/1770 train_time:31183ms step_avg:94.21ms
step:342/1770 train_time:31278ms step_avg:94.21ms
step:343/1770 train_time:31373ms step_avg:94.21ms
step:344/1770 train_time:31468ms step_avg:94.21ms
step:345/1770 train_time:31562ms step_avg:94.22ms
step:346/1770 train_time:31657ms step_avg:94.22ms
step:347/1770 train_time:31752ms step_avg:94.22ms
step:348/1770 train_time:31847ms step_avg:94.22ms
step:349/1770 train_time:31942ms step_avg:94.22ms
step:350/1770 train_time:32037ms step_avg:94.23ms
step:351/1770 train_time:32132ms step_avg:94.23ms
step:352/1770 train_time:32227ms step_avg:94.23ms
step:353/1770 train_time:32321ms step_avg:94.23ms
step:354/1770 train_time:32416ms step_avg:94.23ms
step:355/1770 train_time:32511ms step_avg:94.23ms
step:356/1770 train_time:32606ms step_avg:94.24ms
step:357/1770 train_time:32700ms step_avg:94.24ms
step:358/1770 train_time:32795ms step_avg:94.24ms
step:359/1770 train_time:32890ms step_avg:94.24ms
step:360/1770 train_time:32985ms step_avg:94.24ms
step:361/1770 train_time:33081ms step_avg:94.25ms
step:362/1770 train_time:33176ms step_avg:94.25ms
step:363/1770 train_time:33270ms step_avg:94.25ms
step:364/1770 train_time:33365ms step_avg:94.25ms
step:365/1770 train_time:33460ms step_avg:94.25ms
step:366/1770 train_time:33555ms step_avg:94.26ms
step:367/1770 train_time:33650ms step_avg:94.26ms
step:368/1770 train_time:33745ms step_avg:94.26ms
step:369/1770 train_time:33839ms step_avg:94.26ms
step:370/1770 train_time:33934ms step_avg:94.26ms
step:371/1770 train_time:34029ms step_avg:94.26ms
step:372/1770 train_time:34124ms step_avg:94.26ms
step:373/1770 train_time:34219ms step_avg:94.27ms
step:374/1770 train_time:34313ms step_avg:94.27ms
step:375/1770 train_time:34408ms step_avg:94.27ms
step:375/1770 val_loss:3.8999 train_time:34502ms step_avg:94.53ms
step:376/1770 train_time:34524ms step_avg:94.33ms
step:377/1770 train_time:34608ms step_avg:94.30ms
step:378/1770 train_time:34709ms step_avg:94.32ms
step:379/1770 train_time:34804ms step_avg:94.32ms
step:380/1770 train_time:34899ms step_avg:94.32ms
step:381/1770 train_time:34993ms step_avg:94.32ms
step:382/1770 train_time:35088ms step_avg:94.32ms
step:383/1770 train_time:35184ms step_avg:94.33ms
step:384/1770 train_time:35277ms step_avg:94.32ms
step:385/1770 train_time:35371ms step_avg:94.32ms
step:386/1770 train_time:35465ms step_avg:94.32ms
step:387/1770 train_time:35559ms step_avg:94.32ms
step:388/1770 train_time:35655ms step_avg:94.32ms
step:389/1770 train_time:35750ms step_avg:94.33ms
step:390/1770 train_time:35845ms step_avg:94.33ms
step:391/1770 train_time:35940ms step_avg:94.33ms
step:392/1770 train_time:36035ms step_avg:94.33ms
step:393/1770 train_time:36129ms step_avg:94.33ms
step:394/1770 train_time:36224ms step_avg:94.33ms
step:395/1770 train_time:36319ms step_avg:94.33ms
step:396/1770 train_time:36415ms step_avg:94.34ms
step:397/1770 train_time:36512ms step_avg:94.35ms
step:398/1770 train_time:36608ms step_avg:94.35ms
step:399/1770 train_time:36705ms step_avg:94.36ms
step:400/1770 train_time:36801ms step_avg:94.36ms
step:401/1770 train_time:36898ms step_avg:94.37ms
step:402/1770 train_time:36995ms step_avg:94.37ms
step:403/1770 train_time:37091ms step_avg:94.38ms
step:404/1770 train_time:37188ms step_avg:94.39ms
step:405/1770 train_time:37285ms step_avg:94.39ms
step:406/1770 train_time:37382ms step_avg:94.40ms
step:407/1770 train_time:37479ms step_avg:94.41ms
step:408/1770 train_time:37576ms step_avg:94.41ms
step:409/1770 train_time:37673ms step_avg:94.42ms
step:410/1770 train_time:37770ms step_avg:94.42ms
step:411/1770 train_time:37867ms step_avg:94.43ms
step:412/1770 train_time:37963ms step_avg:94.44ms
step:413/1770 train_time:38060ms step_avg:94.44ms
step:414/1770 train_time:38157ms step_avg:94.45ms
step:415/1770 train_time:38253ms step_avg:94.45ms
step:416/1770 train_time:38351ms step_avg:94.46ms
step:417/1770 train_time:38448ms step_avg:94.47ms
step:418/1770 train_time:38544ms step_avg:94.47ms
step:419/1770 train_time:38641ms step_avg:94.48ms
step:420/1770 train_time:38739ms step_avg:94.48ms
step:421/1770 train_time:38836ms step_avg:94.49ms
step:422/1770 train_time:38933ms step_avg:94.50ms
step:423/1770 train_time:39029ms step_avg:94.50ms
step:424/1770 train_time:39126ms step_avg:94.51ms
step:425/1770 train_time:39222ms step_avg:94.51ms
step:426/1770 train_time:39319ms step_avg:94.52ms
step:427/1770 train_time:39416ms step_avg:94.52ms
step:428/1770 train_time:39512ms step_avg:94.53ms
step:429/1770 train_time:39609ms step_avg:94.53ms
step:430/1770 train_time:39705ms step_avg:94.54ms
step:431/1770 train_time:39802ms step_avg:94.54ms
step:432/1770 train_time:39899ms step_avg:94.55ms
step:433/1770 train_time:39996ms step_avg:94.55ms
step:434/1770 train_time:40093ms step_avg:94.56ms
step:435/1770 train_time:40190ms step_avg:94.57ms
step:436/1770 train_time:40286ms step_avg:94.57ms
step:437/1770 train_time:40383ms step_avg:94.57ms
step:438/1770 train_time:40480ms step_avg:94.58ms
step:439/1770 train_time:40576ms step_avg:94.58ms
step:440/1770 train_time:40673ms step_avg:94.59ms
step:441/1770 train_time:40770ms step_avg:94.59ms
step:442/1770 train_time:40866ms step_avg:94.60ms
step:443/1770 train_time:40963ms step_avg:94.60ms
step:444/1770 train_time:41060ms step_avg:94.61ms
step:445/1770 train_time:41157ms step_avg:94.61ms
step:446/1770 train_time:41254ms step_avg:94.62ms
step:447/1770 train_time:41350ms step_avg:94.62ms
step:448/1770 train_time:41447ms step_avg:94.63ms
step:449/1770 train_time:41544ms step_avg:94.63ms
step:450/1770 train_time:41641ms step_avg:94.64ms
step:451/1770 train_time:41737ms step_avg:94.64ms
step:452/1770 train_time:41834ms step_avg:94.65ms
step:453/1770 train_time:41931ms step_avg:94.65ms
step:454/1770 train_time:42028ms step_avg:94.66ms
step:455/1770 train_time:42124ms step_avg:94.66ms
step:456/1770 train_time:42221ms step_avg:94.67ms
step:457/1770 train_time:42318ms step_avg:94.67ms
step:458/1770 train_time:42415ms step_avg:94.68ms
step:459/1770 train_time:42511ms step_avg:94.68ms
step:460/1770 train_time:42607ms step_avg:94.68ms
step:461/1770 train_time:42704ms step_avg:94.69ms
step:462/1770 train_time:42801ms step_avg:94.69ms
step:463/1770 train_time:42898ms step_avg:94.70ms
step:464/1770 train_time:42994ms step_avg:94.70ms
step:465/1770 train_time:43091ms step_avg:94.71ms
step:466/1770 train_time:43188ms step_avg:94.71ms
step:467/1770 train_time:43285ms step_avg:94.71ms
step:468/1770 train_time:43381ms step_avg:94.72ms
step:469/1770 train_time:43478ms step_avg:94.72ms
step:470/1770 train_time:43575ms step_avg:94.73ms
step:471/1770 train_time:43672ms step_avg:94.73ms
step:472/1770 train_time:43770ms step_avg:94.74ms
step:473/1770 train_time:43867ms step_avg:94.75ms
step:474/1770 train_time:43964ms step_avg:94.75ms
step:475/1770 train_time:44061ms step_avg:94.75ms
step:476/1770 train_time:44158ms step_avg:94.76ms
step:477/1770 train_time:44255ms step_avg:94.76ms
step:478/1770 train_time:44351ms step_avg:94.77ms
step:479/1770 train_time:44448ms step_avg:94.77ms
step:480/1770 train_time:44544ms step_avg:94.78ms
step:481/1770 train_time:44641ms step_avg:94.78ms
step:482/1770 train_time:44739ms step_avg:94.79ms
step:483/1770 train_time:44836ms step_avg:94.79ms
step:484/1770 train_time:44932ms step_avg:94.79ms
step:485/1770 train_time:45029ms step_avg:94.80ms
step:486/1770 train_time:45126ms step_avg:94.80ms
step:487/1770 train_time:45223ms step_avg:94.81ms
step:488/1770 train_time:45320ms step_avg:94.81ms
step:489/1770 train_time:45416ms step_avg:94.81ms
step:490/1770 train_time:45512ms step_avg:94.82ms
step:491/1770 train_time:45609ms step_avg:94.82ms
step:492/1770 train_time:45706ms step_avg:94.83ms
step:493/1770 train_time:45802ms step_avg:94.83ms
step:494/1770 train_time:45900ms step_avg:94.83ms
step:495/1770 train_time:45997ms step_avg:94.84ms
step:496/1770 train_time:46093ms step_avg:94.84ms
step:497/1770 train_time:46190ms step_avg:94.85ms
step:498/1770 train_time:46289ms step_avg:94.85ms
step:499/1770 train_time:46385ms step_avg:94.86ms
step:500/1770 train_time:46481ms step_avg:94.86ms
step:500/1770 val_loss:3.7501 train_time:46576ms step_avg:95.05ms
step:501/1770 train_time:46598ms step_avg:94.90ms
step:502/1770 train_time:46687ms step_avg:94.89ms
step:503/1770 train_time:46788ms step_avg:94.90ms
step:504/1770 train_time:46885ms step_avg:94.91ms
step:505/1770 train_time:46982ms step_avg:94.91ms
step:506/1770 train_time:47081ms step_avg:94.92ms
step:507/1770 train_time:47175ms step_avg:94.92ms
step:508/1770 train_time:47272ms step_avg:94.92ms
step:509/1770 train_time:47368ms step_avg:94.93ms
step:510/1770 train_time:47464ms step_avg:94.93ms
step:511/1770 train_time:47561ms step_avg:94.93ms
step:512/1770 train_time:47658ms step_avg:94.94ms
step:513/1770 train_time:47755ms step_avg:94.94ms
step:514/1770 train_time:47853ms step_avg:94.95ms
step:515/1770 train_time:47951ms step_avg:94.95ms
step:516/1770 train_time:48049ms step_avg:94.96ms
step:517/1770 train_time:48146ms step_avg:94.96ms
step:518/1770 train_time:48242ms step_avg:94.96ms
step:519/1770 train_time:48339ms step_avg:94.97ms
step:520/1770 train_time:48436ms step_avg:94.97ms
step:521/1770 train_time:48533ms step_avg:94.98ms
step:522/1770 train_time:48631ms step_avg:94.98ms
step:523/1770 train_time:48728ms step_avg:94.99ms
step:524/1770 train_time:48824ms step_avg:94.99ms
step:525/1770 train_time:48922ms step_avg:94.99ms
step:526/1770 train_time:49020ms step_avg:95.00ms
step:527/1770 train_time:49117ms step_avg:95.00ms
step:528/1770 train_time:49215ms step_avg:95.01ms
step:529/1770 train_time:49312ms step_avg:95.01ms
step:530/1770 train_time:49410ms step_avg:95.02ms
step:531/1770 train_time:49507ms step_avg:95.02ms
step:532/1770 train_time:49604ms step_avg:95.03ms
step:533/1770 train_time:49701ms step_avg:95.03ms
step:534/1770 train_time:49798ms step_avg:95.03ms
step:535/1770 train_time:49895ms step_avg:95.04ms
step:536/1770 train_time:49992ms step_avg:95.04ms
step:537/1770 train_time:50090ms step_avg:95.05ms
step:538/1770 train_time:50189ms step_avg:95.05ms
step:539/1770 train_time:50284ms step_avg:95.06ms
step:540/1770 train_time:50382ms step_avg:95.06ms
step:541/1770 train_time:50479ms step_avg:95.06ms
step:542/1770 train_time:50577ms step_avg:95.07ms
step:543/1770 train_time:50674ms step_avg:95.07ms
step:544/1770 train_time:50771ms step_avg:95.08ms
step:545/1770 train_time:50869ms step_avg:95.08ms
step:546/1770 train_time:50966ms step_avg:95.09ms
step:547/1770 train_time:51064ms step_avg:95.09ms
step:548/1770 train_time:51161ms step_avg:95.09ms
step:549/1770 train_time:51258ms step_avg:95.10ms
step:550/1770 train_time:51355ms step_avg:95.10ms
step:551/1770 train_time:51453ms step_avg:95.11ms
step:552/1770 train_time:51551ms step_avg:95.11ms
step:553/1770 train_time:51648ms step_avg:95.12ms
step:554/1770 train_time:51745ms step_avg:95.12ms
step:555/1770 train_time:51843ms step_avg:95.12ms
step:556/1770 train_time:51940ms step_avg:95.13ms
step:557/1770 train_time:52038ms step_avg:95.13ms
step:558/1770 train_time:52134ms step_avg:95.14ms
step:559/1770 train_time:52231ms step_avg:95.14ms
step:560/1770 train_time:52328ms step_avg:95.14ms
step:561/1770 train_time:52425ms step_avg:95.15ms
step:562/1770 train_time:52523ms step_avg:95.15ms
step:563/1770 train_time:52620ms step_avg:95.15ms
step:564/1770 train_time:52718ms step_avg:95.16ms
step:565/1770 train_time:52815ms step_avg:95.16ms
step:566/1770 train_time:52913ms step_avg:95.17ms
step:567/1770 train_time:53010ms step_avg:95.17ms
step:568/1770 train_time:53108ms step_avg:95.18ms
step:569/1770 train_time:53205ms step_avg:95.18ms
step:570/1770 train_time:53302ms step_avg:95.18ms
step:571/1770 train_time:53400ms step_avg:95.19ms
step:572/1770 train_time:53498ms step_avg:95.19ms
step:573/1770 train_time:53595ms step_avg:95.20ms
step:574/1770 train_time:53693ms step_avg:95.20ms
step:575/1770 train_time:53790ms step_avg:95.20ms
step:576/1770 train_time:53888ms step_avg:95.21ms
step:577/1770 train_time:53986ms step_avg:95.21ms
step:578/1770 train_time:54086ms step_avg:95.22ms
step:579/1770 train_time:54181ms step_avg:95.22ms
step:580/1770 train_time:54278ms step_avg:95.23ms
step:581/1770 train_time:54376ms step_avg:95.23ms
step:582/1770 train_time:54473ms step_avg:95.23ms
step:583/1770 train_time:54570ms step_avg:95.24ms
step:584/1770 train_time:54667ms step_avg:95.24ms
step:585/1770 train_time:54765ms step_avg:95.24ms
step:586/1770 train_time:54862ms step_avg:95.25ms
step:587/1770 train_time:54959ms step_avg:95.25ms
step:588/1770 train_time:55057ms step_avg:95.25ms
step:589/1770 train_time:55154ms step_avg:95.26ms
step:590/1770 train_time:55252ms step_avg:95.26ms
step:591/1770 train_time:55349ms step_avg:95.27ms
step:592/1770 train_time:55446ms step_avg:95.27ms
step:593/1770 train_time:55544ms step_avg:95.27ms
step:594/1770 train_time:55641ms step_avg:95.28ms
step:595/1770 train_time:55739ms step_avg:95.28ms
step:596/1770 train_time:55836ms step_avg:95.28ms
step:597/1770 train_time:55933ms step_avg:95.29ms
step:598/1770 train_time:56031ms step_avg:95.29ms
step:599/1770 train_time:56128ms step_avg:95.29ms
step:600/1770 train_time:56225ms step_avg:95.30ms
step:601/1770 train_time:56323ms step_avg:95.30ms
step:602/1770 train_time:56420ms step_avg:95.30ms
step:603/1770 train_time:56517ms step_avg:95.31ms
step:604/1770 train_time:56615ms step_avg:95.31ms
step:605/1770 train_time:56712ms step_avg:95.31ms
step:606/1770 train_time:56809ms step_avg:95.32ms
step:607/1770 train_time:56907ms step_avg:95.32ms
step:608/1770 train_time:57003ms step_avg:95.32ms
step:609/1770 train_time:57101ms step_avg:95.33ms
step:610/1770 train_time:57199ms step_avg:95.33ms
step:611/1770 train_time:57295ms step_avg:95.33ms
step:612/1770 train_time:57393ms step_avg:95.34ms
step:613/1770 train_time:57491ms step_avg:95.34ms
step:614/1770 train_time:57589ms step_avg:95.35ms
step:615/1770 train_time:57686ms step_avg:95.35ms
step:616/1770 train_time:57785ms step_avg:95.35ms
step:617/1770 train_time:57883ms step_avg:95.36ms
step:618/1770 train_time:57981ms step_avg:95.36ms
step:619/1770 train_time:58082ms step_avg:95.37ms
step:620/1770 train_time:58175ms step_avg:95.37ms
step:621/1770 train_time:58272ms step_avg:95.37ms
step:622/1770 train_time:58369ms step_avg:95.37ms
step:623/1770 train_time:58466ms step_avg:95.38ms
step:624/1770 train_time:58564ms step_avg:95.38ms
step:625/1770 train_time:58662ms step_avg:95.39ms
step:625/1770 val_loss:3.6623 train_time:58758ms step_avg:95.54ms
step:626/1770 train_time:58779ms step_avg:95.42ms
step:627/1770 train_time:58868ms step_avg:95.41ms
step:628/1770 train_time:58968ms step_avg:95.42ms
step:629/1770 train_time:59066ms step_avg:95.42ms
step:630/1770 train_time:59163ms step_avg:95.42ms
step:631/1770 train_time:59260ms step_avg:95.43ms
step:632/1770 train_time:59356ms step_avg:95.43ms
step:633/1770 train_time:59453ms step_avg:95.43ms
step:634/1770 train_time:59550ms step_avg:95.43ms
step:635/1770 train_time:59646ms step_avg:95.43ms
step:636/1770 train_time:59743ms step_avg:95.44ms
step:637/1770 train_time:59841ms step_avg:95.44ms
step:638/1770 train_time:59939ms step_avg:95.44ms
step:639/1770 train_time:60037ms step_avg:95.45ms
step:640/1770 train_time:60135ms step_avg:95.45ms
step:641/1770 train_time:60232ms step_avg:95.45ms
step:642/1770 train_time:60328ms step_avg:95.46ms
step:643/1770 train_time:60425ms step_avg:95.46ms
step:644/1770 train_time:60522ms step_avg:95.46ms
step:645/1770 train_time:60619ms step_avg:95.46ms
step:646/1770 train_time:60715ms step_avg:95.46ms
step:647/1770 train_time:60813ms step_avg:95.47ms
step:648/1770 train_time:60910ms step_avg:95.47ms
step:649/1770 train_time:61008ms step_avg:95.47ms
step:650/1770 train_time:61106ms step_avg:95.48ms
step:651/1770 train_time:61203ms step_avg:95.48ms
step:652/1770 train_time:61301ms step_avg:95.48ms
step:653/1770 train_time:61398ms step_avg:95.49ms
step:654/1770 train_time:61494ms step_avg:95.49ms
step:655/1770 train_time:61591ms step_avg:95.49ms
step:656/1770 train_time:61687ms step_avg:95.49ms
step:657/1770 train_time:61785ms step_avg:95.49ms
step:658/1770 train_time:61884ms step_avg:95.50ms
step:659/1770 train_time:61986ms step_avg:95.51ms
step:660/1770 train_time:62083ms step_avg:95.51ms
step:661/1770 train_time:62182ms step_avg:95.52ms
step:662/1770 train_time:62280ms step_avg:95.52ms
step:663/1770 train_time:62379ms step_avg:95.53ms
step:664/1770 train_time:62477ms step_avg:95.53ms
step:665/1770 train_time:62576ms step_avg:95.54ms
step:666/1770 train_time:62675ms step_avg:95.54ms
step:667/1770 train_time:62774ms step_avg:95.55ms
step:668/1770 train_time:62872ms step_avg:95.55ms
step:669/1770 train_time:62972ms step_avg:95.56ms
step:670/1770 train_time:63071ms step_avg:95.56ms
step:671/1770 train_time:63171ms step_avg:95.57ms
step:672/1770 train_time:63270ms step_avg:95.57ms
step:673/1770 train_time:63370ms step_avg:95.58ms
step:674/1770 train_time:63469ms step_avg:95.59ms
step:675/1770 train_time:63567ms step_avg:95.59ms
step:676/1770 train_time:63666ms step_avg:95.59ms
step:677/1770 train_time:63766ms step_avg:95.60ms
step:678/1770 train_time:63865ms step_avg:95.61ms
step:679/1770 train_time:63965ms step_avg:95.61ms
step:680/1770 train_time:64064ms step_avg:95.62ms
step:681/1770 train_time:64163ms step_avg:95.62ms
step:682/1770 train_time:64262ms step_avg:95.63ms
step:683/1770 train_time:64361ms step_avg:95.63ms
step:684/1770 train_time:64460ms step_avg:95.64ms
step:685/1770 train_time:64558ms step_avg:95.64ms
step:686/1770 train_time:64657ms step_avg:95.65ms
step:687/1770 train_time:64755ms step_avg:95.65ms
step:688/1770 train_time:64854ms step_avg:95.66ms
step:689/1770 train_time:64953ms step_avg:95.66ms
step:690/1770 train_time:65052ms step_avg:95.66ms
step:691/1770 train_time:65151ms step_avg:95.67ms
step:692/1770 train_time:65250ms step_avg:95.67ms
step:693/1770 train_time:65348ms step_avg:95.68ms
step:694/1770 train_time:65447ms step_avg:95.68ms
step:695/1770 train_time:65546ms step_avg:95.69ms
step:696/1770 train_time:65645ms step_avg:95.69ms
step:697/1770 train_time:65744ms step_avg:95.70ms
step:698/1770 train_time:65844ms step_avg:95.70ms
step:699/1770 train_time:65944ms step_avg:95.71ms
step:700/1770 train_time:66044ms step_avg:95.72ms
step:701/1770 train_time:66144ms step_avg:95.72ms
step:702/1770 train_time:66244ms step_avg:95.73ms
step:703/1770 train_time:66343ms step_avg:95.73ms
step:704/1770 train_time:66442ms step_avg:95.74ms
step:705/1770 train_time:66541ms step_avg:95.74ms
step:706/1770 train_time:66640ms step_avg:95.75ms
step:707/1770 train_time:66739ms step_avg:95.75ms
step:708/1770 train_time:66837ms step_avg:95.76ms
step:709/1770 train_time:66937ms step_avg:95.76ms
step:710/1770 train_time:67036ms step_avg:95.77ms
step:711/1770 train_time:67135ms step_avg:95.77ms
step:712/1770 train_time:67233ms step_avg:95.77ms
step:713/1770 train_time:67333ms step_avg:95.78ms
step:714/1770 train_time:67432ms step_avg:95.78ms
step:715/1770 train_time:67531ms step_avg:95.79ms
step:716/1770 train_time:67631ms step_avg:95.79ms
step:717/1770 train_time:67730ms step_avg:95.80ms
step:718/1770 train_time:67828ms step_avg:95.80ms
step:719/1770 train_time:67927ms step_avg:95.81ms
step:720/1770 train_time:68026ms step_avg:95.81ms
step:721/1770 train_time:68125ms step_avg:95.82ms
step:722/1770 train_time:68225ms step_avg:95.82ms
step:723/1770 train_time:68324ms step_avg:95.83ms
step:724/1770 train_time:68424ms step_avg:95.83ms
step:725/1770 train_time:68524ms step_avg:95.84ms
step:726/1770 train_time:68623ms step_avg:95.84ms
step:727/1770 train_time:68722ms step_avg:95.85ms
step:728/1770 train_time:68822ms step_avg:95.85ms
step:729/1770 train_time:68921ms step_avg:95.86ms
step:730/1770 train_time:69019ms step_avg:95.86ms
step:731/1770 train_time:69118ms step_avg:95.86ms
step:732/1770 train_time:69217ms step_avg:95.87ms
step:733/1770 train_time:69316ms step_avg:95.87ms
step:734/1770 train_time:69416ms step_avg:95.88ms
step:735/1770 train_time:69516ms step_avg:95.88ms
step:736/1770 train_time:69615ms step_avg:95.89ms
step:737/1770 train_time:69715ms step_avg:95.89ms
step:738/1770 train_time:69816ms step_avg:95.90ms
step:739/1770 train_time:69915ms step_avg:95.91ms
step:740/1770 train_time:70014ms step_avg:95.91ms
step:741/1770 train_time:70113ms step_avg:95.91ms
step:742/1770 train_time:70212ms step_avg:95.92ms
step:743/1770 train_time:70312ms step_avg:95.92ms
step:744/1770 train_time:70410ms step_avg:95.93ms
step:745/1770 train_time:70509ms step_avg:95.93ms
step:746/1770 train_time:70608ms step_avg:95.94ms
step:747/1770 train_time:70707ms step_avg:95.94ms
step:748/1770 train_time:70806ms step_avg:95.94ms
step:749/1770 train_time:70905ms step_avg:95.95ms
step:750/1770 train_time:71005ms step_avg:95.95ms
step:750/1770 val_loss:3.5989 train_time:71102ms step_avg:96.08ms
step:751/1770 train_time:71123ms step_avg:95.98ms
step:752/1770 train_time:71215ms step_avg:95.98ms
step:753/1770 train_time:71316ms step_avg:95.98ms
step:754/1770 train_time:71415ms step_avg:95.99ms
step:755/1770 train_time:71514ms step_avg:95.99ms
step:756/1770 train_time:71613ms step_avg:96.00ms
step:757/1770 train_time:71711ms step_avg:96.00ms
step:758/1770 train_time:71809ms step_avg:96.00ms
step:759/1770 train_time:71907ms step_avg:96.00ms
step:760/1770 train_time:72006ms step_avg:96.01ms
step:761/1770 train_time:72105ms step_avg:96.01ms
step:762/1770 train_time:72205ms step_avg:96.02ms
step:763/1770 train_time:72305ms step_avg:96.02ms
step:764/1770 train_time:72404ms step_avg:96.03ms
step:765/1770 train_time:72504ms step_avg:96.03ms
step:766/1770 train_time:72603ms step_avg:96.04ms
step:767/1770 train_time:72703ms step_avg:96.04ms
step:768/1770 train_time:72803ms step_avg:96.05ms
step:769/1770 train_time:72902ms step_avg:96.05ms
step:770/1770 train_time:73001ms step_avg:96.05ms
step:771/1770 train_time:73100ms step_avg:96.06ms
step:772/1770 train_time:73199ms step_avg:96.06ms
step:773/1770 train_time:73298ms step_avg:96.07ms
step:774/1770 train_time:73397ms step_avg:96.07ms
step:775/1770 train_time:73495ms step_avg:96.07ms
step:776/1770 train_time:73595ms step_avg:96.08ms
step:777/1770 train_time:73694ms step_avg:96.08ms
step:778/1770 train_time:73793ms step_avg:96.08ms
step:779/1770 train_time:73892ms step_avg:96.09ms
step:780/1770 train_time:73991ms step_avg:96.09ms
step:781/1770 train_time:74090ms step_avg:96.10ms
step:782/1770 train_time:74190ms step_avg:96.10ms
step:783/1770 train_time:74289ms step_avg:96.11ms
step:784/1770 train_time:74389ms step_avg:96.11ms
step:785/1770 train_time:74488ms step_avg:96.11ms
step:786/1770 train_time:74587ms step_avg:96.12ms
step:787/1770 train_time:74686ms step_avg:96.12ms
step:788/1770 train_time:74785ms step_avg:96.13ms
step:789/1770 train_time:74884ms step_avg:96.13ms
step:790/1770 train_time:74984ms step_avg:96.13ms
step:791/1770 train_time:75084ms step_avg:96.14ms
step:792/1770 train_time:75184ms step_avg:96.14ms
step:793/1770 train_time:75284ms step_avg:96.15ms
step:794/1770 train_time:75384ms step_avg:96.15ms
step:795/1770 train_time:75485ms step_avg:96.16ms
step:796/1770 train_time:75584ms step_avg:96.16ms
step:797/1770 train_time:75683ms step_avg:96.17ms
step:798/1770 train_time:75782ms step_avg:96.17ms
step:799/1770 train_time:75881ms step_avg:96.17ms
step:800/1770 train_time:75981ms step_avg:96.18ms
step:801/1770 train_time:76082ms step_avg:96.18ms
step:802/1770 train_time:76180ms step_avg:96.19ms
step:803/1770 train_time:76280ms step_avg:96.19ms
step:804/1770 train_time:76379ms step_avg:96.20ms
step:805/1770 train_time:76479ms step_avg:96.20ms
step:806/1770 train_time:76579ms step_avg:96.20ms
step:807/1770 train_time:76678ms step_avg:96.21ms
step:808/1770 train_time:76777ms step_avg:96.21ms
step:809/1770 train_time:76876ms step_avg:96.22ms
step:810/1770 train_time:76975ms step_avg:96.22ms
step:811/1770 train_time:77074ms step_avg:96.22ms
step:812/1770 train_time:77174ms step_avg:96.23ms
step:813/1770 train_time:77274ms step_avg:96.23ms
step:814/1770 train_time:77372ms step_avg:96.23ms
step:815/1770 train_time:77472ms step_avg:96.24ms
step:816/1770 train_time:77572ms step_avg:96.24ms
step:817/1770 train_time:77672ms step_avg:96.25ms
step:818/1770 train_time:77772ms step_avg:96.25ms
step:819/1770 train_time:77872ms step_avg:96.26ms
step:820/1770 train_time:77972ms step_avg:96.26ms
step:821/1770 train_time:78071ms step_avg:96.27ms
step:822/1770 train_time:78174ms step_avg:96.27ms
step:823/1770 train_time:78271ms step_avg:96.27ms
step:824/1770 train_time:78371ms step_avg:96.28ms
step:825/1770 train_time:78470ms step_avg:96.28ms
step:826/1770 train_time:78569ms step_avg:96.29ms
step:827/1770 train_time:78669ms step_avg:96.29ms
step:828/1770 train_time:78769ms step_avg:96.29ms
step:829/1770 train_time:78868ms step_avg:96.30ms
step:830/1770 train_time:78967ms step_avg:96.30ms
step:831/1770 train_time:79066ms step_avg:96.30ms
step:832/1770 train_time:79164ms step_avg:96.31ms
step:833/1770 train_time:79264ms step_avg:96.31ms
step:834/1770 train_time:79363ms step_avg:96.31ms
step:835/1770 train_time:79462ms step_avg:96.32ms
step:836/1770 train_time:79563ms step_avg:96.32ms
step:837/1770 train_time:79662ms step_avg:96.33ms
step:838/1770 train_time:79762ms step_avg:96.33ms
step:839/1770 train_time:79861ms step_avg:96.33ms
step:840/1770 train_time:79961ms step_avg:96.34ms
step:841/1770 train_time:80061ms step_avg:96.34ms
step:842/1770 train_time:80159ms step_avg:96.35ms
step:843/1770 train_time:80259ms step_avg:96.35ms
step:844/1770 train_time:80357ms step_avg:96.35ms
step:845/1770 train_time:80456ms step_avg:96.35ms
step:846/1770 train_time:80555ms step_avg:96.36ms
step:847/1770 train_time:80654ms step_avg:96.36ms
step:848/1770 train_time:80754ms step_avg:96.36ms
step:849/1770 train_time:80854ms step_avg:96.37ms
step:850/1770 train_time:80954ms step_avg:96.37ms
step:851/1770 train_time:81053ms step_avg:96.38ms
step:852/1770 train_time:81153ms step_avg:96.38ms
step:853/1770 train_time:81253ms step_avg:96.39ms
step:854/1770 train_time:81352ms step_avg:96.39ms
step:855/1770 train_time:81451ms step_avg:96.39ms
step:856/1770 train_time:81550ms step_avg:96.39ms
step:857/1770 train_time:81650ms step_avg:96.40ms
step:858/1770 train_time:81748ms step_avg:96.40ms
step:859/1770 train_time:81847ms step_avg:96.40ms
step:860/1770 train_time:81946ms step_avg:96.41ms
step:861/1770 train_time:82046ms step_avg:96.41ms
step:862/1770 train_time:82145ms step_avg:96.41ms
step:863/1770 train_time:82244ms step_avg:96.42ms
step:864/1770 train_time:82343ms step_avg:96.42ms
step:865/1770 train_time:82442ms step_avg:96.42ms
step:866/1770 train_time:82542ms step_avg:96.43ms
step:867/1770 train_time:82642ms step_avg:96.43ms
step:868/1770 train_time:82742ms step_avg:96.44ms
step:869/1770 train_time:82841ms step_avg:96.44ms
step:870/1770 train_time:82941ms step_avg:96.44ms
step:871/1770 train_time:83041ms step_avg:96.45ms
step:872/1770 train_time:83141ms step_avg:96.45ms
step:873/1770 train_time:83240ms step_avg:96.45ms
step:874/1770 train_time:83339ms step_avg:96.46ms
step:875/1770 train_time:83438ms step_avg:96.46ms
step:875/1770 val_loss:3.5519 train_time:83536ms step_avg:96.57ms
step:876/1770 train_time:83557ms step_avg:96.49ms
step:877/1770 train_time:83648ms step_avg:96.48ms
step:878/1770 train_time:83750ms step_avg:96.49ms
step:879/1770 train_time:83850ms step_avg:96.49ms
step:880/1770 train_time:83948ms step_avg:96.49ms
step:881/1770 train_time:84047ms step_avg:96.49ms
step:882/1770 train_time:84145ms step_avg:96.50ms
step:883/1770 train_time:84243ms step_avg:96.50ms
step:884/1770 train_time:84342ms step_avg:96.50ms
step:885/1770 train_time:84441ms step_avg:96.50ms
step:886/1770 train_time:84540ms step_avg:96.51ms
step:887/1770 train_time:84640ms step_avg:96.51ms
step:888/1770 train_time:84740ms step_avg:96.51ms
step:889/1770 train_time:84841ms step_avg:96.52ms
step:890/1770 train_time:84940ms step_avg:96.52ms
step:891/1770 train_time:85041ms step_avg:96.53ms
step:892/1770 train_time:85141ms step_avg:96.53ms
step:893/1770 train_time:85241ms step_avg:96.54ms
step:894/1770 train_time:85340ms step_avg:96.54ms
step:895/1770 train_time:85439ms step_avg:96.54ms
step:896/1770 train_time:85538ms step_avg:96.54ms
step:897/1770 train_time:85637ms step_avg:96.55ms
step:898/1770 train_time:85737ms step_avg:96.55ms
step:899/1770 train_time:85837ms step_avg:96.55ms
step:900/1770 train_time:85936ms step_avg:96.56ms
step:901/1770 train_time:86037ms step_avg:96.56ms
step:902/1770 train_time:86137ms step_avg:96.57ms
step:903/1770 train_time:86237ms step_avg:96.57ms
step:904/1770 train_time:86336ms step_avg:96.57ms
step:905/1770 train_time:86436ms step_avg:96.58ms
step:906/1770 train_time:86535ms step_avg:96.58ms
step:907/1770 train_time:86634ms step_avg:96.58ms
step:908/1770 train_time:86733ms step_avg:96.58ms
step:909/1770 train_time:86832ms step_avg:96.59ms
step:910/1770 train_time:86931ms step_avg:96.59ms
step:911/1770 train_time:87030ms step_avg:96.59ms
step:912/1770 train_time:87129ms step_avg:96.60ms
step:913/1770 train_time:87228ms step_avg:96.60ms
step:914/1770 train_time:87327ms step_avg:96.60ms
step:915/1770 train_time:87427ms step_avg:96.60ms
step:916/1770 train_time:87527ms step_avg:96.61ms
step:917/1770 train_time:87627ms step_avg:96.61ms
step:918/1770 train_time:87727ms step_avg:96.62ms
step:919/1770 train_time:87826ms step_avg:96.62ms
step:920/1770 train_time:87926ms step_avg:96.62ms
step:921/1770 train_time:88027ms step_avg:96.63ms
step:922/1770 train_time:88128ms step_avg:96.63ms
step:923/1770 train_time:88230ms step_avg:96.64ms
step:924/1770 train_time:88330ms step_avg:96.64ms
step:925/1770 train_time:88430ms step_avg:96.65ms
step:926/1770 train_time:88531ms step_avg:96.65ms
step:927/1770 train_time:88632ms step_avg:96.65ms
step:928/1770 train_time:88732ms step_avg:96.66ms
step:929/1770 train_time:88832ms step_avg:96.66ms
step:930/1770 train_time:88933ms step_avg:96.67ms
step:931/1770 train_time:89033ms step_avg:96.67ms
step:932/1770 train_time:89134ms step_avg:96.67ms
step:933/1770 train_time:89234ms step_avg:96.68ms
step:934/1770 train_time:89335ms step_avg:96.68ms
step:935/1770 train_time:89435ms step_avg:96.69ms
step:936/1770 train_time:89536ms step_avg:96.69ms
step:937/1770 train_time:89637ms step_avg:96.70ms
step:938/1770 train_time:89738ms step_avg:96.70ms
step:939/1770 train_time:89839ms step_avg:96.70ms
step:940/1770 train_time:89939ms step_avg:96.71ms
step:941/1770 train_time:90040ms step_avg:96.71ms
step:942/1770 train_time:90141ms step_avg:96.72ms
step:943/1770 train_time:90243ms step_avg:96.72ms
step:944/1770 train_time:90344ms step_avg:96.73ms
step:945/1770 train_time:90445ms step_avg:96.73ms
step:946/1770 train_time:90545ms step_avg:96.74ms
step:947/1770 train_time:90646ms step_avg:96.74ms
step:948/1770 train_time:90746ms step_avg:96.74ms
step:949/1770 train_time:90846ms step_avg:96.75ms
step:950/1770 train_time:90947ms step_avg:96.75ms
step:951/1770 train_time:91047ms step_avg:96.76ms
step:952/1770 train_time:91148ms step_avg:96.76ms
step:953/1770 train_time:91249ms step_avg:96.76ms
step:954/1770 train_time:91350ms step_avg:96.77ms
step:955/1770 train_time:91451ms step_avg:96.77ms
step:956/1770 train_time:91552ms step_avg:96.78ms
step:957/1770 train_time:91653ms step_avg:96.78ms
step:958/1770 train_time:91754ms step_avg:96.79ms
step:959/1770 train_time:91856ms step_avg:96.79ms
step:960/1770 train_time:91956ms step_avg:96.80ms
step:961/1770 train_time:92057ms step_avg:96.80ms
step:962/1770 train_time:92158ms step_avg:96.80ms
step:963/1770 train_time:92258ms step_avg:96.81ms
step:964/1770 train_time:92359ms step_avg:96.81ms
step:965/1770 train_time:92461ms step_avg:96.82ms
step:966/1770 train_time:92563ms step_avg:96.82ms
step:967/1770 train_time:92663ms step_avg:96.83ms
step:968/1770 train_time:92764ms step_avg:96.83ms
step:969/1770 train_time:92864ms step_avg:96.83ms
step:970/1770 train_time:92964ms step_avg:96.84ms
step:971/1770 train_time:93065ms step_avg:96.84ms
step:972/1770 train_time:93165ms step_avg:96.85ms
step:973/1770 train_time:93266ms step_avg:96.85ms
step:974/1770 train_time:93368ms step_avg:96.86ms
step:975/1770 train_time:93470ms step_avg:96.86ms
step:976/1770 train_time:93571ms step_avg:96.86ms
step:977/1770 train_time:93672ms step_avg:96.87ms
step:978/1770 train_time:93772ms step_avg:96.87ms
step:979/1770 train_time:93873ms step_avg:96.88ms
step:980/1770 train_time:93973ms step_avg:96.88ms
step:981/1770 train_time:94073ms step_avg:96.88ms
step:982/1770 train_time:94174ms step_avg:96.89ms
step:983/1770 train_time:94275ms step_avg:96.89ms
step:984/1770 train_time:94377ms step_avg:96.90ms
step:985/1770 train_time:94479ms step_avg:96.90ms
step:986/1770 train_time:94580ms step_avg:96.91ms
step:987/1770 train_time:94682ms step_avg:96.91ms
step:988/1770 train_time:94784ms step_avg:96.92ms
step:989/1770 train_time:94886ms step_avg:96.92ms
step:990/1770 train_time:94986ms step_avg:96.92ms
step:991/1770 train_time:95086ms step_avg:96.93ms
step:992/1770 train_time:95187ms step_avg:96.93ms
step:993/1770 train_time:95289ms step_avg:96.94ms
step:994/1770 train_time:95390ms step_avg:96.94ms
step:995/1770 train_time:95491ms step_avg:96.95ms
step:996/1770 train_time:95592ms step_avg:96.95ms
step:997/1770 train_time:95693ms step_avg:96.95ms
step:998/1770 train_time:95793ms step_avg:96.96ms
step:999/1770 train_time:95893ms step_avg:96.96ms
step:1000/1770 train_time:95993ms step_avg:96.96ms
step:1000/1770 val_loss:3.5131 train_time:96093ms step_avg:97.06ms
step:1001/1770 train_time:96114ms step_avg:96.99ms
step:1002/1770 train_time:96207ms step_avg:96.98ms
step:1003/1770 train_time:96310ms step_avg:96.99ms
step:1004/1770 train_time:96410ms step_avg:96.99ms
step:1005/1770 train_time:96510ms step_avg:96.99ms
step:1006/1770 train_time:96610ms step_avg:97.00ms
step:1007/1770 train_time:96709ms step_avg:97.00ms
step:1008/1770 train_time:96809ms step_avg:97.00ms
step:1009/1770 train_time:96909ms step_avg:97.01ms
step:1010/1770 train_time:97009ms step_avg:97.01ms
step:1011/1770 train_time:97113ms step_avg:97.02ms
step:1012/1770 train_time:97218ms step_avg:97.02ms
step:1013/1770 train_time:97319ms step_avg:97.03ms
step:1014/1770 train_time:97419ms step_avg:97.03ms
step:1015/1770 train_time:97518ms step_avg:97.03ms
step:1016/1770 train_time:97618ms step_avg:97.04ms
step:1017/1770 train_time:97718ms step_avg:97.04ms
step:1018/1770 train_time:97818ms step_avg:97.04ms
step:1019/1770 train_time:97919ms step_avg:97.05ms
step:1020/1770 train_time:98021ms step_avg:97.05ms
step:1021/1770 train_time:98123ms step_avg:97.05ms
step:1022/1770 train_time:98230ms step_avg:97.07ms
step:1023/1770 train_time:98325ms step_avg:97.06ms
step:1024/1770 train_time:98425ms step_avg:97.07ms
step:1025/1770 train_time:98526ms step_avg:97.07ms
step:1026/1770 train_time:98626ms step_avg:97.07ms
step:1027/1770 train_time:98726ms step_avg:97.08ms
step:1028/1770 train_time:98827ms step_avg:97.08ms
step:1029/1770 train_time:98928ms step_avg:97.08ms
step:1030/1770 train_time:99028ms step_avg:97.09ms
step:1031/1770 train_time:99129ms step_avg:97.09ms
step:1032/1770 train_time:99230ms step_avg:97.09ms
step:1033/1770 train_time:99332ms step_avg:97.10ms
step:1034/1770 train_time:99434ms step_avg:97.10ms
step:1035/1770 train_time:99534ms step_avg:97.11ms
step:1036/1770 train_time:99635ms step_avg:97.11ms
step:1037/1770 train_time:99735ms step_avg:97.11ms
step:1038/1770 train_time:99836ms step_avg:97.12ms
step:1039/1770 train_time:99936ms step_avg:97.12ms
step:1040/1770 train_time:100036ms step_avg:97.12ms
step:1041/1770 train_time:100137ms step_avg:97.13ms
step:1042/1770 train_time:100238ms step_avg:97.13ms
step:1043/1770 train_time:100339ms step_avg:97.13ms
step:1044/1770 train_time:100440ms step_avg:97.14ms
step:1045/1770 train_time:100541ms step_avg:97.14ms
step:1046/1770 train_time:100642ms step_avg:97.14ms
step:1047/1770 train_time:100743ms step_avg:97.15ms
step:1048/1770 train_time:100844ms step_avg:97.15ms
step:1049/1770 train_time:100944ms step_avg:97.16ms
step:1050/1770 train_time:101044ms step_avg:97.16ms
step:1051/1770 train_time:101145ms step_avg:97.16ms
step:1052/1770 train_time:101245ms step_avg:97.16ms
step:1053/1770 train_time:101346ms step_avg:97.17ms
step:1054/1770 train_time:101446ms step_avg:97.17ms
step:1055/1770 train_time:101547ms step_avg:97.17ms
step:1056/1770 train_time:101649ms step_avg:97.18ms
step:1057/1770 train_time:101750ms step_avg:97.18ms
step:1058/1770 train_time:101852ms step_avg:97.19ms
step:1059/1770 train_time:101953ms step_avg:97.19ms
step:1060/1770 train_time:102055ms step_avg:97.19ms
step:1061/1770 train_time:102155ms step_avg:97.20ms
step:1062/1770 train_time:102256ms step_avg:97.20ms
step:1063/1770 train_time:102357ms step_avg:97.21ms
step:1064/1770 train_time:102459ms step_avg:97.21ms
step:1065/1770 train_time:102560ms step_avg:97.21ms
step:1066/1770 train_time:102662ms step_avg:97.22ms
step:1067/1770 train_time:102763ms step_avg:97.22ms
step:1068/1770 train_time:102864ms step_avg:97.23ms
step:1069/1770 train_time:102965ms step_avg:97.23ms
step:1070/1770 train_time:103066ms step_avg:97.23ms
step:1071/1770 train_time:103166ms step_avg:97.24ms
step:1072/1770 train_time:103267ms step_avg:97.24ms
step:1073/1770 train_time:103367ms step_avg:97.24ms
step:1074/1770 train_time:103469ms step_avg:97.24ms
step:1075/1770 train_time:103569ms step_avg:97.25ms
step:1076/1770 train_time:103672ms step_avg:97.25ms
step:1077/1770 train_time:103773ms step_avg:97.26ms
step:1078/1770 train_time:103873ms step_avg:97.26ms
step:1079/1770 train_time:103973ms step_avg:97.26ms
step:1080/1770 train_time:104074ms step_avg:97.27ms
step:1081/1770 train_time:104175ms step_avg:97.27ms
step:1082/1770 train_time:104276ms step_avg:97.27ms
step:1083/1770 train_time:104376ms step_avg:97.28ms
step:1084/1770 train_time:104477ms step_avg:97.28ms
step:1085/1770 train_time:104578ms step_avg:97.28ms
step:1086/1770 train_time:104678ms step_avg:97.28ms
step:1087/1770 train_time:104779ms step_avg:97.29ms
step:1088/1770 train_time:104880ms step_avg:97.29ms
step:1089/1770 train_time:104981ms step_avg:97.29ms
step:1090/1770 train_time:105083ms step_avg:97.30ms
step:1091/1770 train_time:105184ms step_avg:97.30ms
step:1092/1770 train_time:105285ms step_avg:97.31ms
step:1093/1770 train_time:105386ms step_avg:97.31ms
step:1094/1770 train_time:105487ms step_avg:97.31ms
step:1095/1770 train_time:105588ms step_avg:97.32ms
step:1096/1770 train_time:105688ms step_avg:97.32ms
step:1097/1770 train_time:105789ms step_avg:97.32ms
step:1098/1770 train_time:105890ms step_avg:97.33ms
step:1099/1770 train_time:105992ms step_avg:97.33ms
step:1100/1770 train_time:106095ms step_avg:97.34ms
step:1101/1770 train_time:106196ms step_avg:97.34ms
step:1102/1770 train_time:106296ms step_avg:97.34ms
step:1103/1770 train_time:106397ms step_avg:97.34ms
step:1104/1770 train_time:106498ms step_avg:97.35ms
step:1105/1770 train_time:106600ms step_avg:97.35ms
step:1106/1770 train_time:106701ms step_avg:97.35ms
step:1107/1770 train_time:106801ms step_avg:97.36ms
step:1108/1770 train_time:106903ms step_avg:97.36ms
step:1109/1770 train_time:107004ms step_avg:97.36ms
step:1110/1770 train_time:107105ms step_avg:97.37ms
step:1111/1770 train_time:107208ms step_avg:97.37ms
step:1112/1770 train_time:107309ms step_avg:97.38ms
step:1113/1770 train_time:107409ms step_avg:97.38ms
step:1114/1770 train_time:107510ms step_avg:97.38ms
step:1115/1770 train_time:107611ms step_avg:97.39ms
step:1116/1770 train_time:107713ms step_avg:97.39ms
step:1117/1770 train_time:107814ms step_avg:97.39ms
step:1118/1770 train_time:107915ms step_avg:97.40ms
step:1119/1770 train_time:108015ms step_avg:97.40ms
step:1120/1770 train_time:108116ms step_avg:97.40ms
step:1121/1770 train_time:108216ms step_avg:97.40ms
step:1122/1770 train_time:108316ms step_avg:97.41ms
step:1123/1770 train_time:108416ms step_avg:97.41ms
step:1124/1770 train_time:108517ms step_avg:97.41ms
step:1125/1770 train_time:108618ms step_avg:97.42ms
step:1125/1770 val_loss:3.4720 train_time:108718ms step_avg:97.50ms
step:1126/1770 train_time:108739ms step_avg:97.44ms
step:1127/1770 train_time:108831ms step_avg:97.43ms
step:1128/1770 train_time:108932ms step_avg:97.44ms
step:1129/1770 train_time:109033ms step_avg:97.44ms
step:1130/1770 train_time:109133ms step_avg:97.44ms
step:1131/1770 train_time:109234ms step_avg:97.44ms
step:1132/1770 train_time:109334ms step_avg:97.45ms
step:1133/1770 train_time:109434ms step_avg:97.45ms
step:1134/1770 train_time:109535ms step_avg:97.45ms
step:1135/1770 train_time:109634ms step_avg:97.45ms
step:1136/1770 train_time:109736ms step_avg:97.46ms
step:1137/1770 train_time:109839ms step_avg:97.46ms
step:1138/1770 train_time:109940ms step_avg:97.46ms
step:1139/1770 train_time:110042ms step_avg:97.47ms
step:1140/1770 train_time:110143ms step_avg:97.47ms
step:1141/1770 train_time:110244ms step_avg:97.47ms
step:1142/1770 train_time:110344ms step_avg:97.48ms
step:1143/1770 train_time:110444ms step_avg:97.48ms
step:1144/1770 train_time:110544ms step_avg:97.48ms
step:1145/1770 train_time:110645ms step_avg:97.48ms
step:1146/1770 train_time:110746ms step_avg:97.49ms
step:1147/1770 train_time:110847ms step_avg:97.49ms
step:1148/1770 train_time:110947ms step_avg:97.49ms
step:1149/1770 train_time:111048ms step_avg:97.50ms
step:1150/1770 train_time:111149ms step_avg:97.50ms
step:1151/1770 train_time:111250ms step_avg:97.50ms
step:1152/1770 train_time:111351ms step_avg:97.51ms
step:1153/1770 train_time:111452ms step_avg:97.51ms
step:1154/1770 train_time:111554ms step_avg:97.51ms
step:1155/1770 train_time:111655ms step_avg:97.52ms
step:1156/1770 train_time:111755ms step_avg:97.52ms
step:1157/1770 train_time:111857ms step_avg:97.52ms
step:1158/1770 train_time:111959ms step_avg:97.52ms
step:1159/1770 train_time:112059ms step_avg:97.53ms
step:1160/1770 train_time:112160ms step_avg:97.53ms
step:1161/1770 train_time:112261ms step_avg:97.53ms
step:1162/1770 train_time:112362ms step_avg:97.54ms
step:1163/1770 train_time:112463ms step_avg:97.54ms
step:1164/1770 train_time:112564ms step_avg:97.54ms
step:1165/1770 train_time:112665ms step_avg:97.55ms
step:1166/1770 train_time:112766ms step_avg:97.55ms
step:1167/1770 train_time:112867ms step_avg:97.55ms
step:1168/1770 train_time:112968ms step_avg:97.55ms
step:1169/1770 train_time:113069ms step_avg:97.56ms
step:1170/1770 train_time:113170ms step_avg:97.56ms
step:1171/1770 train_time:113271ms step_avg:97.56ms
step:1172/1770 train_time:113371ms step_avg:97.57ms
step:1173/1770 train_time:113472ms step_avg:97.57ms
step:1174/1770 train_time:113574ms step_avg:97.57ms
step:1175/1770 train_time:113676ms step_avg:97.58ms
step:1176/1770 train_time:113777ms step_avg:97.58ms
step:1177/1770 train_time:113878ms step_avg:97.58ms
step:1178/1770 train_time:113979ms step_avg:97.58ms
step:1179/1770 train_time:114080ms step_avg:97.59ms
step:1180/1770 train_time:114182ms step_avg:97.59ms
step:1181/1770 train_time:114284ms step_avg:97.59ms
step:1182/1770 train_time:114385ms step_avg:97.60ms
step:1183/1770 train_time:114487ms step_avg:97.60ms
step:1184/1770 train_time:114590ms step_avg:97.61ms
step:1185/1770 train_time:114691ms step_avg:97.61ms
step:1186/1770 train_time:114793ms step_avg:97.61ms
step:1187/1770 train_time:114896ms step_avg:97.62ms
step:1188/1770 train_time:114998ms step_avg:97.62ms
step:1189/1770 train_time:115099ms step_avg:97.62ms
step:1190/1770 train_time:115202ms step_avg:97.63ms
step:1191/1770 train_time:115304ms step_avg:97.63ms
step:1192/1770 train_time:115407ms step_avg:97.64ms
step:1193/1770 train_time:115508ms step_avg:97.64ms
step:1194/1770 train_time:115611ms step_avg:97.64ms
step:1195/1770 train_time:115713ms step_avg:97.65ms
step:1196/1770 train_time:115816ms step_avg:97.65ms
step:1197/1770 train_time:115917ms step_avg:97.66ms
step:1198/1770 train_time:116019ms step_avg:97.66ms
step:1199/1770 train_time:116121ms step_avg:97.66ms
step:1200/1770 train_time:116223ms step_avg:97.67ms
step:1201/1770 train_time:116325ms step_avg:97.67ms
step:1202/1770 train_time:116426ms step_avg:97.67ms
step:1203/1770 train_time:116528ms step_avg:97.68ms
step:1204/1770 train_time:116630ms step_avg:97.68ms
step:1205/1770 train_time:116732ms step_avg:97.68ms
step:1206/1770 train_time:116835ms step_avg:97.69ms
step:1207/1770 train_time:116938ms step_avg:97.69ms
step:1208/1770 train_time:117039ms step_avg:97.70ms
step:1209/1770 train_time:117141ms step_avg:97.70ms
step:1210/1770 train_time:117243ms step_avg:97.70ms
step:1211/1770 train_time:117345ms step_avg:97.71ms
step:1212/1770 train_time:117448ms step_avg:97.71ms
step:1213/1770 train_time:117550ms step_avg:97.71ms
step:1214/1770 train_time:117651ms step_avg:97.72ms
step:1215/1770 train_time:117753ms step_avg:97.72ms
step:1216/1770 train_time:117857ms step_avg:97.73ms
step:1217/1770 train_time:117959ms step_avg:97.73ms
step:1218/1770 train_time:118060ms step_avg:97.73ms
step:1219/1770 train_time:118162ms step_avg:97.74ms
step:1220/1770 train_time:118265ms step_avg:97.74ms
step:1221/1770 train_time:118366ms step_avg:97.74ms
step:1222/1770 train_time:118470ms step_avg:97.75ms
step:1223/1770 train_time:118571ms step_avg:97.75ms
step:1224/1770 train_time:118674ms step_avg:97.75ms
step:1225/1770 train_time:118777ms step_avg:97.76ms
step:1226/1770 train_time:118878ms step_avg:97.76ms
step:1227/1770 train_time:118982ms step_avg:97.77ms
step:1228/1770 train_time:119085ms step_avg:97.77ms
step:1229/1770 train_time:119187ms step_avg:97.77ms
step:1230/1770 train_time:119289ms step_avg:97.78ms
step:1231/1770 train_time:119391ms step_avg:97.78ms
step:1232/1770 train_time:119493ms step_avg:97.78ms
step:1233/1770 train_time:119594ms step_avg:97.79ms
step:1234/1770 train_time:119697ms step_avg:97.79ms
step:1235/1770 train_time:119801ms step_avg:97.80ms
step:1236/1770 train_time:119901ms step_avg:97.80ms
step:1237/1770 train_time:120003ms step_avg:97.80ms
step:1238/1770 train_time:120105ms step_avg:97.81ms
step:1239/1770 train_time:120207ms step_avg:97.81ms
step:1240/1770 train_time:120308ms step_avg:97.81ms
step:1241/1770 train_time:120410ms step_avg:97.81ms
step:1242/1770 train_time:120511ms step_avg:97.82ms
step:1243/1770 train_time:120614ms step_avg:97.82ms
step:1244/1770 train_time:120715ms step_avg:97.82ms
step:1245/1770 train_time:120817ms step_avg:97.83ms
step:1246/1770 train_time:120920ms step_avg:97.83ms
step:1247/1770 train_time:121021ms step_avg:97.83ms
step:1248/1770 train_time:121124ms step_avg:97.84ms
step:1249/1770 train_time:121226ms step_avg:97.84ms
step:1250/1770 train_time:121329ms step_avg:97.85ms
step:1250/1770 val_loss:3.4241 train_time:121431ms step_avg:97.93ms
step:1251/1770 train_time:121454ms step_avg:97.87ms
step:1252/1770 train_time:121545ms step_avg:97.86ms
step:1253/1770 train_time:121648ms step_avg:97.87ms
step:1254/1770 train_time:121749ms step_avg:97.87ms
step:1255/1770 train_time:121853ms step_avg:97.87ms
step:1256/1770 train_time:121954ms step_avg:97.88ms
step:1257/1770 train_time:122057ms step_avg:97.88ms
step:1258/1770 train_time:122157ms step_avg:97.88ms
step:1259/1770 train_time:122259ms step_avg:97.89ms
step:1260/1770 train_time:122360ms step_avg:97.89ms
step:1261/1770 train_time:122464ms step_avg:97.89ms
step:1262/1770 train_time:122567ms step_avg:97.90ms
step:1263/1770 train_time:122669ms step_avg:97.90ms
step:1264/1770 train_time:122772ms step_avg:97.90ms
step:1265/1770 train_time:122874ms step_avg:97.91ms
step:1266/1770 train_time:122976ms step_avg:97.91ms
step:1267/1770 train_time:123078ms step_avg:97.91ms
step:1268/1770 train_time:123180ms step_avg:97.92ms
step:1269/1770 train_time:123281ms step_avg:97.92ms
step:1270/1770 train_time:123383ms step_avg:97.92ms
step:1271/1770 train_time:123486ms step_avg:97.93ms
step:1272/1770 train_time:123588ms step_avg:97.93ms
step:1273/1770 train_time:123691ms step_avg:97.93ms
step:1274/1770 train_time:123793ms step_avg:97.94ms
step:1275/1770 train_time:123896ms step_avg:97.94ms
step:1276/1770 train_time:123998ms step_avg:97.94ms
step:1277/1770 train_time:124100ms step_avg:97.95ms
step:1278/1770 train_time:124202ms step_avg:97.95ms
step:1279/1770 train_time:124304ms step_avg:97.95ms
step:1280/1770 train_time:124407ms step_avg:97.96ms
step:1281/1770 train_time:124508ms step_avg:97.96ms
step:1282/1770 train_time:124611ms step_avg:97.96ms
step:1283/1770 train_time:124713ms step_avg:97.97ms
step:1284/1770 train_time:124815ms step_avg:97.97ms
step:1285/1770 train_time:124917ms step_avg:97.97ms
step:1286/1770 train_time:125021ms step_avg:97.98ms
step:1287/1770 train_time:125124ms step_avg:97.98ms
step:1288/1770 train_time:125227ms step_avg:97.99ms
step:1289/1770 train_time:125328ms step_avg:97.99ms
step:1290/1770 train_time:125431ms step_avg:97.99ms
step:1291/1770 train_time:125532ms step_avg:98.00ms
step:1292/1770 train_time:125633ms step_avg:98.00ms
step:1293/1770 train_time:125735ms step_avg:98.00ms
step:1294/1770 train_time:125836ms step_avg:98.00ms
step:1295/1770 train_time:125938ms step_avg:98.01ms
step:1296/1770 train_time:126041ms step_avg:98.01ms
step:1297/1770 train_time:126142ms step_avg:98.01ms
step:1298/1770 train_time:126245ms step_avg:98.02ms
step:1299/1770 train_time:126347ms step_avg:98.02ms
step:1300/1770 train_time:126449ms step_avg:98.02ms
step:1301/1770 train_time:126550ms step_avg:98.03ms
step:1302/1770 train_time:126652ms step_avg:98.03ms
step:1303/1770 train_time:126754ms step_avg:98.03ms
step:1304/1770 train_time:126856ms step_avg:98.03ms
step:1305/1770 train_time:126958ms step_avg:98.04ms
step:1306/1770 train_time:127060ms step_avg:98.04ms
step:1307/1770 train_time:127163ms step_avg:98.04ms
step:1308/1770 train_time:127266ms step_avg:98.05ms
step:1309/1770 train_time:127368ms step_avg:98.05ms
step:1310/1770 train_time:127469ms step_avg:98.05ms
step:1311/1770 train_time:127570ms step_avg:98.06ms
step:1312/1770 train_time:127673ms step_avg:98.06ms
step:1313/1770 train_time:127772ms step_avg:98.06ms
step:1314/1770 train_time:127875ms step_avg:98.06ms
step:1315/1770 train_time:127977ms step_avg:98.07ms
step:1316/1770 train_time:128080ms step_avg:98.07ms
step:1317/1770 train_time:128181ms step_avg:98.07ms
step:1318/1770 train_time:128287ms step_avg:98.08ms
step:1319/1770 train_time:128389ms step_avg:98.08ms
step:1320/1770 train_time:128490ms step_avg:98.08ms
step:1321/1770 train_time:128592ms step_avg:98.09ms
step:1322/1770 train_time:128694ms step_avg:98.09ms
step:1323/1770 train_time:128796ms step_avg:98.09ms
step:1324/1770 train_time:128899ms step_avg:98.10ms
step:1325/1770 train_time:129002ms step_avg:98.10ms
step:1326/1770 train_time:129104ms step_avg:98.10ms
step:1327/1770 train_time:129209ms step_avg:98.11ms
step:1328/1770 train_time:129310ms step_avg:98.11ms
step:1329/1770 train_time:129412ms step_avg:98.11ms
step:1330/1770 train_time:129513ms step_avg:98.12ms
step:1331/1770 train_time:129615ms step_avg:98.12ms
step:1332/1770 train_time:129717ms step_avg:98.12ms
step:1333/1770 train_time:129818ms step_avg:98.12ms
step:1334/1770 train_time:129919ms step_avg:98.13ms
step:1335/1770 train_time:130022ms step_avg:98.13ms
step:1336/1770 train_time:130124ms step_avg:98.13ms
step:1337/1770 train_time:130226ms step_avg:98.14ms
step:1338/1770 train_time:130327ms step_avg:98.14ms
step:1339/1770 train_time:130430ms step_avg:98.14ms
step:1340/1770 train_time:130534ms step_avg:98.15ms
step:1341/1770 train_time:130635ms step_avg:98.15ms
step:1342/1770 train_time:130738ms step_avg:98.15ms
step:1343/1770 train_time:130841ms step_avg:98.16ms
step:1344/1770 train_time:130944ms step_avg:98.16ms
step:1345/1770 train_time:131045ms step_avg:98.16ms
step:1346/1770 train_time:131147ms step_avg:98.16ms
step:1347/1770 train_time:131249ms step_avg:98.17ms
step:1348/1770 train_time:131353ms step_avg:98.17ms
step:1349/1770 train_time:131455ms step_avg:98.17ms
step:1350/1770 train_time:131557ms step_avg:98.18ms
step:1351/1770 train_time:131659ms step_avg:98.18ms
step:1352/1770 train_time:131760ms step_avg:98.18ms
step:1353/1770 train_time:131863ms step_avg:98.19ms
step:1354/1770 train_time:131965ms step_avg:98.19ms
step:1355/1770 train_time:132067ms step_avg:98.19ms
step:1356/1770 train_time:132169ms step_avg:98.19ms
step:1357/1770 train_time:132270ms step_avg:98.20ms
step:1358/1770 train_time:132373ms step_avg:98.20ms
step:1359/1770 train_time:132476ms step_avg:98.20ms
step:1360/1770 train_time:132579ms step_avg:98.21ms
step:1361/1770 train_time:132681ms step_avg:98.21ms
step:1362/1770 train_time:132782ms step_avg:98.21ms
step:1363/1770 train_time:132885ms step_avg:98.22ms
step:1364/1770 train_time:132988ms step_avg:98.22ms
step:1365/1770 train_time:133090ms step_avg:98.22ms
step:1366/1770 train_time:133191ms step_avg:98.22ms
step:1367/1770 train_time:133295ms step_avg:98.23ms
step:1368/1770 train_time:133396ms step_avg:98.23ms
step:1369/1770 train_time:133499ms step_avg:98.23ms
step:1370/1770 train_time:133602ms step_avg:98.24ms
step:1371/1770 train_time:133703ms step_avg:98.24ms
step:1372/1770 train_time:133806ms step_avg:98.24ms
step:1373/1770 train_time:133908ms step_avg:98.24ms
step:1374/1770 train_time:134011ms step_avg:98.25ms
step:1375/1770 train_time:134112ms step_avg:98.25ms
step:1375/1770 val_loss:3.3812 train_time:134213ms step_avg:98.32ms
step:1376/1770 train_time:134235ms step_avg:98.27ms
step:1377/1770 train_time:134334ms step_avg:98.27ms
step:1378/1770 train_time:134436ms step_avg:98.27ms
step:1379/1770 train_time:134537ms step_avg:98.27ms
step:1380/1770 train_time:134639ms step_avg:98.28ms
step:1381/1770 train_time:134741ms step_avg:98.28ms
step:1382/1770 train_time:134842ms step_avg:98.28ms
step:1383/1770 train_time:134945ms step_avg:98.28ms
step:1384/1770 train_time:135047ms step_avg:98.29ms
step:1385/1770 train_time:135149ms step_avg:98.29ms
step:1386/1770 train_time:135252ms step_avg:98.29ms
step:1387/1770 train_time:135356ms step_avg:98.30ms
step:1388/1770 train_time:135457ms step_avg:98.30ms
step:1389/1770 train_time:135559ms step_avg:98.30ms
step:1390/1770 train_time:135661ms step_avg:98.30ms
step:1391/1770 train_time:135762ms step_avg:98.31ms
step:1392/1770 train_time:135863ms step_avg:98.31ms
step:1393/1770 train_time:135966ms step_avg:98.31ms
step:1394/1770 train_time:136068ms step_avg:98.31ms
step:1395/1770 train_time:136171ms step_avg:98.32ms
step:1396/1770 train_time:136275ms step_avg:98.32ms
step:1397/1770 train_time:136378ms step_avg:98.33ms
step:1398/1770 train_time:136480ms step_avg:98.33ms
step:1399/1770 train_time:136582ms step_avg:98.33ms
step:1400/1770 train_time:136684ms step_avg:98.33ms
step:1401/1770 train_time:136786ms step_avg:98.34ms
step:1402/1770 train_time:136888ms step_avg:98.34ms
step:1403/1770 train_time:136990ms step_avg:98.34ms
step:1404/1770 train_time:137092ms step_avg:98.34ms
step:1405/1770 train_time:137194ms step_avg:98.35ms
step:1406/1770 train_time:137297ms step_avg:98.35ms
step:1407/1770 train_time:137399ms step_avg:98.35ms
step:1408/1770 train_time:137501ms step_avg:98.36ms
step:1409/1770 train_time:137603ms step_avg:98.36ms
step:1410/1770 train_time:137706ms step_avg:98.36ms
step:1411/1770 train_time:137808ms step_avg:98.36ms
step:1412/1770 train_time:137910ms step_avg:98.37ms
step:1413/1770 train_time:138012ms step_avg:98.37ms
step:1414/1770 train_time:138114ms step_avg:98.37ms
step:1415/1770 train_time:138217ms step_avg:98.37ms
step:1416/1770 train_time:138320ms step_avg:98.38ms
step:1417/1770 train_time:138422ms step_avg:98.38ms
step:1418/1770 train_time:138524ms step_avg:98.38ms
step:1419/1770 train_time:138626ms step_avg:98.39ms
step:1420/1770 train_time:138729ms step_avg:98.39ms
step:1421/1770 train_time:138831ms step_avg:98.39ms
step:1422/1770 train_time:138932ms step_avg:98.39ms
step:1423/1770 train_time:139036ms step_avg:98.40ms
step:1424/1770 train_time:139137ms step_avg:98.40ms
step:1425/1770 train_time:139239ms step_avg:98.40ms
step:1426/1770 train_time:139342ms step_avg:98.41ms
step:1427/1770 train_time:139443ms step_avg:98.41ms
step:1428/1770 train_time:139546ms step_avg:98.41ms
step:1429/1770 train_time:139648ms step_avg:98.41ms
step:1430/1770 train_time:139750ms step_avg:98.42ms
step:1431/1770 train_time:139853ms step_avg:98.42ms
step:1432/1770 train_time:139954ms step_avg:98.42ms
step:1433/1770 train_time:140056ms step_avg:98.42ms
step:1434/1770 train_time:140158ms step_avg:98.43ms
step:1435/1770 train_time:140260ms step_avg:98.43ms
step:1436/1770 train_time:140364ms step_avg:98.43ms
step:1437/1770 train_time:140466ms step_avg:98.43ms
step:1438/1770 train_time:140568ms step_avg:98.44ms
step:1439/1770 train_time:140670ms step_avg:98.44ms
step:1440/1770 train_time:140772ms step_avg:98.44ms
step:1441/1770 train_time:140877ms step_avg:98.45ms
step:1442/1770 train_time:140978ms step_avg:98.45ms
step:1443/1770 train_time:141080ms step_avg:98.45ms
step:1444/1770 train_time:141183ms step_avg:98.45ms
step:1445/1770 train_time:141286ms step_avg:98.46ms
step:1446/1770 train_time:141388ms step_avg:98.46ms
step:1447/1770 train_time:141492ms step_avg:98.46ms
step:1448/1770 train_time:141595ms step_avg:98.47ms
step:1449/1770 train_time:141699ms step_avg:98.47ms
step:1450/1770 train_time:141802ms step_avg:98.47ms
step:1451/1770 train_time:141906ms step_avg:98.48ms
step:1452/1770 train_time:142010ms step_avg:98.48ms
step:1453/1770 train_time:142113ms step_avg:98.48ms
step:1454/1770 train_time:142215ms step_avg:98.49ms
step:1455/1770 train_time:142320ms step_avg:98.49ms
step:1456/1770 train_time:142424ms step_avg:98.49ms
step:1457/1770 train_time:142526ms step_avg:98.50ms
step:1458/1770 train_time:142631ms step_avg:98.50ms
step:1459/1770 train_time:142736ms step_avg:98.51ms
step:1460/1770 train_time:142840ms step_avg:98.51ms
step:1461/1770 train_time:142944ms step_avg:98.51ms
step:1462/1770 train_time:143047ms step_avg:98.52ms
step:1463/1770 train_time:143151ms step_avg:98.52ms
step:1464/1770 train_time:143256ms step_avg:98.53ms
step:1465/1770 train_time:143359ms step_avg:98.53ms
step:1466/1770 train_time:143464ms step_avg:98.53ms
step:1467/1770 train_time:143567ms step_avg:98.54ms
step:1468/1770 train_time:143670ms step_avg:98.54ms
step:1469/1770 train_time:143773ms step_avg:98.54ms
step:1470/1770 train_time:143875ms step_avg:98.54ms
step:1471/1770 train_time:143979ms step_avg:98.55ms
step:1472/1770 train_time:144082ms step_avg:98.55ms
step:1473/1770 train_time:144186ms step_avg:98.55ms
step:1474/1770 train_time:144291ms step_avg:98.56ms
step:1475/1770 train_time:144394ms step_avg:98.56ms
step:1476/1770 train_time:144496ms step_avg:98.56ms
step:1477/1770 train_time:144601ms step_avg:98.57ms
step:1478/1770 train_time:144705ms step_avg:98.57ms
step:1479/1770 train_time:144809ms step_avg:98.58ms
step:1480/1770 train_time:144912ms step_avg:98.58ms
step:1481/1770 train_time:145018ms step_avg:98.58ms
step:1482/1770 train_time:145121ms step_avg:98.59ms
step:1483/1770 train_time:145224ms step_avg:98.59ms
step:1484/1770 train_time:145328ms step_avg:98.59ms
step:1485/1770 train_time:145431ms step_avg:98.60ms
step:1486/1770 train_time:145533ms step_avg:98.60ms
step:1487/1770 train_time:145636ms step_avg:98.60ms
step:1488/1770 train_time:145740ms step_avg:98.61ms
step:1489/1770 train_time:145845ms step_avg:98.61ms
step:1490/1770 train_time:145948ms step_avg:98.61ms
step:1491/1770 train_time:146050ms step_avg:98.62ms
step:1492/1770 train_time:146154ms step_avg:98.62ms
step:1493/1770 train_time:146261ms step_avg:98.63ms
step:1494/1770 train_time:146367ms step_avg:98.63ms
step:1495/1770 train_time:146469ms step_avg:98.63ms
step:1496/1770 train_time:146572ms step_avg:98.64ms
step:1497/1770 train_time:146675ms step_avg:98.64ms
step:1498/1770 train_time:146777ms step_avg:98.64ms
step:1499/1770 train_time:146881ms step_avg:98.64ms
step:1500/1770 train_time:146983ms step_avg:98.65ms
step:1500/1770 val_loss:3.3430 train_time:147084ms step_avg:98.71ms
step:1501/1770 train_time:147107ms step_avg:98.66ms
step:1502/1770 train_time:147200ms step_avg:98.66ms
step:1503/1770 train_time:147303ms step_avg:98.66ms
step:1504/1770 train_time:147406ms step_avg:98.67ms
step:1505/1770 train_time:147511ms step_avg:98.67ms
step:1506/1770 train_time:147615ms step_avg:98.67ms
step:1507/1770 train_time:147718ms step_avg:98.68ms
step:1508/1770 train_time:147822ms step_avg:98.68ms
step:1509/1770 train_time:147925ms step_avg:98.68ms
step:1510/1770 train_time:148027ms step_avg:98.68ms
step:1511/1770 train_time:148132ms step_avg:98.69ms
step:1512/1770 train_time:148236ms step_avg:98.69ms
step:1513/1770 train_time:148341ms step_avg:98.70ms
step:1514/1770 train_time:148444ms step_avg:98.70ms
step:1515/1770 train_time:148547ms step_avg:98.70ms
step:1516/1770 train_time:148651ms step_avg:98.71ms
step:1517/1770 train_time:148754ms step_avg:98.71ms
step:1518/1770 train_time:148860ms step_avg:98.71ms
step:1519/1770 train_time:148962ms step_avg:98.72ms
step:1520/1770 train_time:149067ms step_avg:98.72ms
step:1521/1770 train_time:149169ms step_avg:98.72ms
step:1522/1770 train_time:149274ms step_avg:98.73ms
step:1523/1770 train_time:149378ms step_avg:98.73ms
step:1524/1770 train_time:149481ms step_avg:98.73ms
step:1525/1770 train_time:149584ms step_avg:98.74ms
step:1526/1770 train_time:149687ms step_avg:98.74ms
step:1527/1770 train_time:149790ms step_avg:98.74ms
step:1528/1770 train_time:149896ms step_avg:98.75ms
step:1529/1770 train_time:149998ms step_avg:98.75ms
step:1530/1770 train_time:150100ms step_avg:98.75ms
step:1531/1770 train_time:150203ms step_avg:98.75ms
step:1532/1770 train_time:150307ms step_avg:98.76ms
step:1533/1770 train_time:150411ms step_avg:98.76ms
step:1534/1770 train_time:150515ms step_avg:98.76ms
step:1535/1770 train_time:150619ms step_avg:98.77ms
step:1536/1770 train_time:150722ms step_avg:98.77ms
step:1537/1770 train_time:150825ms step_avg:98.77ms
step:1538/1770 train_time:150930ms step_avg:98.78ms
step:1539/1770 train_time:151032ms step_avg:98.78ms
step:1540/1770 train_time:151138ms step_avg:98.78ms
step:1541/1770 train_time:151242ms step_avg:98.79ms
step:1542/1770 train_time:151346ms step_avg:98.79ms
step:1543/1770 train_time:151448ms step_avg:98.79ms
step:1544/1770 train_time:151553ms step_avg:98.80ms
step:1545/1770 train_time:151656ms step_avg:98.80ms
step:1546/1770 train_time:151759ms step_avg:98.80ms
step:1547/1770 train_time:151864ms step_avg:98.81ms
step:1548/1770 train_time:151967ms step_avg:98.81ms
step:1549/1770 train_time:152069ms step_avg:98.81ms
step:1550/1770 train_time:152177ms step_avg:98.82ms
step:1551/1770 train_time:152276ms step_avg:98.82ms
step:1552/1770 train_time:152380ms step_avg:98.82ms
step:1553/1770 train_time:152484ms step_avg:98.82ms
step:1554/1770 train_time:152587ms step_avg:98.83ms
step:1555/1770 train_time:152691ms step_avg:98.83ms
step:1556/1770 train_time:152794ms step_avg:98.83ms
step:1557/1770 train_time:152897ms step_avg:98.83ms
step:1558/1770 train_time:153000ms step_avg:98.84ms
step:1559/1770 train_time:153105ms step_avg:98.84ms
step:1560/1770 train_time:153207ms step_avg:98.84ms
step:1561/1770 train_time:153314ms step_avg:98.85ms
step:1562/1770 train_time:153417ms step_avg:98.85ms
step:1563/1770 train_time:153520ms step_avg:98.85ms
step:1564/1770 train_time:153623ms step_avg:98.86ms
step:1565/1770 train_time:153726ms step_avg:98.86ms
step:1566/1770 train_time:153829ms step_avg:98.86ms
step:1567/1770 train_time:153933ms step_avg:98.87ms
step:1568/1770 train_time:154036ms step_avg:98.87ms
step:1569/1770 train_time:154142ms step_avg:98.87ms
step:1570/1770 train_time:154245ms step_avg:98.88ms
step:1571/1770 train_time:154348ms step_avg:98.88ms
step:1572/1770 train_time:154452ms step_avg:98.88ms
step:1573/1770 train_time:154557ms step_avg:98.89ms
step:1574/1770 train_time:154661ms step_avg:98.89ms
step:1575/1770 train_time:154762ms step_avg:98.89ms
step:1576/1770 train_time:154865ms step_avg:98.89ms
step:1577/1770 train_time:154970ms step_avg:98.90ms
step:1578/1770 train_time:155075ms step_avg:98.90ms
step:1579/1770 train_time:155183ms step_avg:98.91ms
step:1580/1770 train_time:155282ms step_avg:98.91ms
step:1581/1770 train_time:155388ms step_avg:98.91ms
step:1582/1770 train_time:155493ms step_avg:98.91ms
step:1583/1770 train_time:155596ms step_avg:98.92ms
step:1584/1770 train_time:155701ms step_avg:98.92ms
step:1585/1770 train_time:155805ms step_avg:98.92ms
step:1586/1770 train_time:155912ms step_avg:98.93ms
step:1587/1770 train_time:156016ms step_avg:98.93ms
step:1588/1770 train_time:156119ms step_avg:98.93ms
step:1589/1770 train_time:156225ms step_avg:98.94ms
step:1590/1770 train_time:156328ms step_avg:98.94ms
step:1591/1770 train_time:156429ms step_avg:98.94ms
step:1592/1770 train_time:156534ms step_avg:98.95ms
step:1593/1770 train_time:156637ms step_avg:98.95ms
step:1594/1770 train_time:156740ms step_avg:98.95ms
step:1595/1770 train_time:156843ms step_avg:98.95ms
step:1596/1770 train_time:156948ms step_avg:98.96ms
step:1597/1770 train_time:157051ms step_avg:98.96ms
step:1598/1770 train_time:157155ms step_avg:98.96ms
step:1599/1770 train_time:157258ms step_avg:98.97ms
step:1600/1770 train_time:157364ms step_avg:98.97ms
step:1601/1770 train_time:157467ms step_avg:98.97ms
step:1602/1770 train_time:157572ms step_avg:98.98ms
step:1603/1770 train_time:157675ms step_avg:98.98ms
step:1604/1770 train_time:157778ms step_avg:98.98ms
step:1605/1770 train_time:157880ms step_avg:98.98ms
step:1606/1770 train_time:157984ms step_avg:98.99ms
step:1607/1770 train_time:158091ms step_avg:98.99ms
step:1608/1770 train_time:158194ms step_avg:99.00ms
step:1609/1770 train_time:158297ms step_avg:99.00ms
step:1610/1770 train_time:158402ms step_avg:99.00ms
step:1611/1770 train_time:158507ms step_avg:99.00ms
step:1612/1770 train_time:158611ms step_avg:99.01ms
step:1613/1770 train_time:158714ms step_avg:99.01ms
step:1614/1770 train_time:158817ms step_avg:99.01ms
step:1615/1770 train_time:158920ms step_avg:99.02ms
step:1616/1770 train_time:159023ms step_avg:99.02ms
step:1617/1770 train_time:159128ms step_avg:99.02ms
step:1618/1770 train_time:159232ms step_avg:99.03ms
step:1619/1770 train_time:159336ms step_avg:99.03ms
step:1620/1770 train_time:159440ms step_avg:99.03ms
step:1621/1770 train_time:159544ms step_avg:99.03ms
step:1622/1770 train_time:159647ms step_avg:99.04ms
step:1623/1770 train_time:159755ms step_avg:99.04ms
step:1624/1770 train_time:159857ms step_avg:99.04ms
step:1625/1770 train_time:159960ms step_avg:99.05ms
step:1625/1770 val_loss:3.3083 train_time:160062ms step_avg:99.11ms
step:1626/1770 train_time:160084ms step_avg:99.06ms
step:1627/1770 train_time:160174ms step_avg:99.06ms
step:1628/1770 train_time:160278ms step_avg:99.06ms
step:1629/1770 train_time:160380ms step_avg:99.06ms
step:1630/1770 train_time:160483ms step_avg:99.06ms
step:1631/1770 train_time:160586ms step_avg:99.07ms
step:1632/1770 train_time:160689ms step_avg:99.07ms
step:1633/1770 train_time:160792ms step_avg:99.07ms
step:1634/1770 train_time:160895ms step_avg:99.07ms
step:1635/1770 train_time:160998ms step_avg:99.08ms
step:1636/1770 train_time:161102ms step_avg:99.08ms
step:1637/1770 train_time:161207ms step_avg:99.08ms
step:1638/1770 train_time:161310ms step_avg:99.08ms
step:1639/1770 train_time:161413ms step_avg:99.09ms
step:1640/1770 train_time:161518ms step_avg:99.09ms
step:1641/1770 train_time:161622ms step_avg:99.09ms
step:1642/1770 train_time:161725ms step_avg:99.10ms
step:1643/1770 train_time:161828ms step_avg:99.10ms
step:1644/1770 train_time:161933ms step_avg:99.10ms
step:1645/1770 train_time:162036ms step_avg:99.10ms
step:1646/1770 train_time:162141ms step_avg:99.11ms
step:1647/1770 train_time:162246ms step_avg:99.11ms
step:1648/1770 train_time:162349ms step_avg:99.11ms
step:1649/1770 train_time:162452ms step_avg:99.12ms
step:1650/1770 train_time:162555ms step_avg:99.12ms
step:1651/1770 train_time:162658ms step_avg:99.12ms
step:1652/1770 train_time:162761ms step_avg:99.12ms
step:1653/1770 train_time:162865ms step_avg:99.13ms
step:1654/1770 train_time:162972ms step_avg:99.13ms
step:1655/1770 train_time:163078ms step_avg:99.14ms
step:1656/1770 train_time:163181ms step_avg:99.14ms
step:1657/1770 train_time:163286ms step_avg:99.14ms
step:1658/1770 train_time:163390ms step_avg:99.14ms
step:1659/1770 train_time:163495ms step_avg:99.15ms
step:1660/1770 train_time:163598ms step_avg:99.15ms
step:1661/1770 train_time:163702ms step_avg:99.15ms
step:1662/1770 train_time:163806ms step_avg:99.16ms
step:1663/1770 train_time:163909ms step_avg:99.16ms
step:1664/1770 train_time:164012ms step_avg:99.16ms
step:1665/1770 train_time:164115ms step_avg:99.16ms
step:1666/1770 train_time:164219ms step_avg:99.17ms
step:1667/1770 train_time:164322ms step_avg:99.17ms
step:1668/1770 train_time:164425ms step_avg:99.17ms
step:1669/1770 train_time:164527ms step_avg:99.17ms
step:1670/1770 train_time:164630ms step_avg:99.17ms
step:1671/1770 train_time:164736ms step_avg:99.18ms
step:1672/1770 train_time:164837ms step_avg:99.18ms
step:1673/1770 train_time:164942ms step_avg:99.18ms
step:1674/1770 train_time:165044ms step_avg:99.18ms
step:1675/1770 train_time:165147ms step_avg:99.19ms
step:1676/1770 train_time:165252ms step_avg:99.19ms
step:1677/1770 train_time:165359ms step_avg:99.20ms
step:1678/1770 train_time:165461ms step_avg:99.20ms
step:1679/1770 train_time:165566ms step_avg:99.20ms
step:1680/1770 train_time:165670ms step_avg:99.20ms
step:1681/1770 train_time:165774ms step_avg:99.21ms
step:1682/1770 train_time:165880ms step_avg:99.21ms
step:1683/1770 train_time:165982ms step_avg:99.21ms
step:1684/1770 train_time:166085ms step_avg:99.21ms
step:1685/1770 train_time:166190ms step_avg:99.22ms
step:1686/1770 train_time:166294ms step_avg:99.22ms
step:1687/1770 train_time:166399ms step_avg:99.22ms
step:1688/1770 train_time:166503ms step_avg:99.23ms
step:1689/1770 train_time:166606ms step_avg:99.23ms
step:1690/1770 train_time:166709ms step_avg:99.23ms
step:1691/1770 train_time:166813ms step_avg:99.23ms
step:1692/1770 train_time:166916ms step_avg:99.24ms
step:1693/1770 train_time:167020ms step_avg:99.24ms
step:1694/1770 train_time:167124ms step_avg:99.24ms
step:1695/1770 train_time:167228ms step_avg:99.25ms
step:1696/1770 train_time:167333ms step_avg:99.25ms
step:1697/1770 train_time:167438ms step_avg:99.25ms
step:1698/1770 train_time:167543ms step_avg:99.26ms
step:1699/1770 train_time:167646ms step_avg:99.26ms
step:1700/1770 train_time:167749ms step_avg:99.26ms
step:1701/1770 train_time:167852ms step_avg:99.26ms
step:1702/1770 train_time:167956ms step_avg:99.26ms
step:1703/1770 train_time:168059ms step_avg:99.27ms
step:1704/1770 train_time:168162ms step_avg:99.27ms
step:1705/1770 train_time:168266ms step_avg:99.27ms
step:1706/1770 train_time:168368ms step_avg:99.27ms
step:1707/1770 train_time:168473ms step_avg:99.28ms
step:1708/1770 train_time:168577ms step_avg:99.28ms
step:1709/1770 train_time:168682ms step_avg:99.28ms
step:1710/1770 train_time:168788ms step_avg:99.29ms
step:1711/1770 train_time:168895ms step_avg:99.29ms
step:1712/1770 train_time:168998ms step_avg:99.29ms
step:1713/1770 train_time:169102ms step_avg:99.30ms
step:1714/1770 train_time:169206ms step_avg:99.30ms
step:1715/1770 train_time:169309ms step_avg:99.30ms
step:1716/1770 train_time:169413ms step_avg:99.30ms
step:1717/1770 train_time:169517ms step_avg:99.31ms
step:1718/1770 train_time:169622ms step_avg:99.31ms
step:1719/1770 train_time:169727ms step_avg:99.31ms
step:1720/1770 train_time:169832ms step_avg:99.32ms
step:1721/1770 train_time:169936ms step_avg:99.32ms
step:1722/1770 train_time:170042ms step_avg:99.32ms
step:1723/1770 train_time:170147ms step_avg:99.33ms
step:1724/1770 train_time:170253ms step_avg:99.33ms
step:1725/1770 train_time:170359ms step_avg:99.33ms
step:1726/1770 train_time:170465ms step_avg:99.34ms
step:1727/1770 train_time:170568ms step_avg:99.34ms
step:1728/1770 train_time:170674ms step_avg:99.34ms
step:1729/1770 train_time:170778ms step_avg:99.35ms
step:1730/1770 train_time:170883ms step_avg:99.35ms
step:1731/1770 train_time:170989ms step_avg:99.35ms
step:1732/1770 train_time:171092ms step_avg:99.36ms
step:1733/1770 train_time:171198ms step_avg:99.36ms
step:1734/1770 train_time:171301ms step_avg:99.36ms
step:1735/1770 train_time:171406ms step_avg:99.37ms
step:1736/1770 train_time:171509ms step_avg:99.37ms
step:1737/1770 train_time:171614ms step_avg:99.37ms
step:1738/1770 train_time:171718ms step_avg:99.37ms
step:1739/1770 train_time:171822ms step_avg:99.38ms
step:1740/1770 train_time:171927ms step_avg:99.38ms
step:1741/1770 train_time:172034ms step_avg:99.38ms
step:1742/1770 train_time:172141ms step_avg:99.39ms
step:1743/1770 train_time:172246ms step_avg:99.39ms
step:1744/1770 train_time:172352ms step_avg:99.40ms
step:1745/1770 train_time:172455ms step_avg:99.40ms
step:1746/1770 train_time:172561ms step_avg:99.40ms
step:1747/1770 train_time:172664ms step_avg:99.40ms
step:1748/1770 train_time:172770ms step_avg:99.41ms
step:1749/1770 train_time:172875ms step_avg:99.41ms
step:1750/1770 train_time:172978ms step_avg:99.41ms
step:1750/1770 val_loss:3.2816 train_time:173081ms step_avg:99.47ms
step:1751/1770 train_time:173102ms step_avg:99.43ms
step:1752/1770 train_time:173197ms step_avg:99.42ms
step:1753/1770 train_time:173301ms step_avg:99.43ms
step:1754/1770 train_time:173405ms step_avg:99.43ms
step:1755/1770 train_time:173509ms step_avg:99.43ms
step:1756/1770 train_time:173613ms step_avg:99.43ms
step:1757/1770 train_time:173717ms step_avg:99.44ms
step:1758/1770 train_time:173821ms step_avg:99.44ms
step:1759/1770 train_time:173925ms step_avg:99.44ms
step:1760/1770 train_time:174030ms step_avg:99.45ms
step:1761/1770 train_time:174137ms step_avg:99.45ms
step:1762/1770 train_time:174245ms step_avg:99.45ms
step:1763/1770 train_time:174347ms step_avg:99.46ms
step:1764/1770 train_time:174452ms step_avg:99.46ms
step:1765/1770 train_time:174556ms step_avg:99.46ms
step:1766/1770 train_time:174664ms step_avg:99.47ms
step:1767/1770 train_time:174767ms step_avg:99.47ms
step:1768/1770 train_time:174872ms step_avg:99.47ms
step:1769/1770 train_time:174974ms step_avg:99.47ms
step:1770/1770 train_time:175077ms step_avg:99.48ms
step:1770/1770 val_loss:3.2785 train_time:175181ms step_avg:99.53ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
