import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:49:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24606ms step_avg:nanms
step:2/1770 train_time:25116ms step_avg:nanms
step:3/1770 train_time:25211ms step_avg:nanms
step:4/1770 train_time:25304ms step_avg:nanms
step:5/1770 train_time:25398ms step_avg:nanms
step:6/1770 train_time:25491ms step_avg:nanms
step:7/1770 train_time:25584ms step_avg:nanms
step:8/1770 train_time:25677ms step_avg:nanms
step:9/1770 train_time:25771ms step_avg:nanms
step:10/1770 train_time:25864ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.75ms
step:14/1770 train_time:376ms step_avg:94.10ms
step:15/1770 train_time:471ms step_avg:94.24ms
step:16/1770 train_time:566ms step_avg:94.27ms
step:17/1770 train_time:660ms step_avg:94.26ms
step:18/1770 train_time:753ms step_avg:94.18ms
step:19/1770 train_time:848ms step_avg:94.19ms
step:20/1770 train_time:942ms step_avg:94.23ms
step:21/1770 train_time:1036ms step_avg:94.19ms
step:22/1770 train_time:1130ms step_avg:94.17ms
step:23/1770 train_time:1224ms step_avg:94.16ms
step:24/1770 train_time:1318ms step_avg:94.12ms
step:25/1770 train_time:1412ms step_avg:94.11ms
step:26/1770 train_time:1506ms step_avg:94.12ms
step:27/1770 train_time:1600ms step_avg:94.12ms
step:28/1770 train_time:1694ms step_avg:94.08ms
step:29/1770 train_time:1787ms step_avg:94.07ms
step:30/1770 train_time:1882ms step_avg:94.09ms
step:31/1770 train_time:1975ms step_avg:94.04ms
step:32/1770 train_time:2069ms step_avg:94.05ms
step:33/1770 train_time:2163ms step_avg:94.04ms
step:34/1770 train_time:2257ms step_avg:94.03ms
step:35/1770 train_time:2350ms step_avg:94.01ms
step:36/1770 train_time:2445ms step_avg:94.02ms
step:37/1770 train_time:2538ms step_avg:94.02ms
step:38/1770 train_time:2633ms step_avg:94.02ms
step:39/1770 train_time:2726ms step_avg:94.00ms
step:40/1770 train_time:2820ms step_avg:94.00ms
step:41/1770 train_time:2914ms step_avg:94.00ms
step:42/1770 train_time:3008ms step_avg:93.99ms
step:43/1770 train_time:3101ms step_avg:93.97ms
step:44/1770 train_time:3195ms step_avg:93.96ms
step:45/1770 train_time:3289ms step_avg:93.96ms
step:46/1770 train_time:3383ms step_avg:93.98ms
step:47/1770 train_time:3477ms step_avg:93.97ms
step:48/1770 train_time:3571ms step_avg:93.99ms
step:49/1770 train_time:3666ms step_avg:93.99ms
step:50/1770 train_time:3759ms step_avg:93.98ms
step:51/1770 train_time:3854ms step_avg:94.00ms
step:52/1770 train_time:3947ms step_avg:93.97ms
step:53/1770 train_time:4040ms step_avg:93.96ms
step:54/1770 train_time:4134ms step_avg:93.95ms
step:55/1770 train_time:4228ms step_avg:93.94ms
step:56/1770 train_time:4321ms step_avg:93.94ms
step:57/1770 train_time:4415ms step_avg:93.94ms
step:58/1770 train_time:4509ms step_avg:93.94ms
step:59/1770 train_time:4603ms step_avg:93.94ms
step:60/1770 train_time:4698ms step_avg:93.96ms
step:61/1770 train_time:4792ms step_avg:93.95ms
step:62/1770 train_time:4886ms step_avg:93.96ms
step:63/1770 train_time:4980ms step_avg:93.97ms
step:64/1770 train_time:5074ms step_avg:93.96ms
step:65/1770 train_time:5167ms step_avg:93.95ms
step:66/1770 train_time:5261ms step_avg:93.94ms
step:67/1770 train_time:5355ms step_avg:93.94ms
step:68/1770 train_time:5448ms step_avg:93.94ms
step:69/1770 train_time:5542ms step_avg:93.93ms
step:70/1770 train_time:5636ms step_avg:93.94ms
step:71/1770 train_time:5730ms step_avg:93.94ms
step:72/1770 train_time:5824ms step_avg:93.94ms
step:73/1770 train_time:5918ms step_avg:93.93ms
step:74/1770 train_time:6012ms step_avg:93.93ms
step:75/1770 train_time:6105ms step_avg:93.92ms
step:76/1770 train_time:6199ms step_avg:93.92ms
step:77/1770 train_time:6292ms step_avg:93.91ms
step:78/1770 train_time:6386ms step_avg:93.91ms
step:79/1770 train_time:6479ms step_avg:93.91ms
step:80/1770 train_time:6574ms step_avg:93.91ms
step:81/1770 train_time:6668ms step_avg:93.91ms
step:82/1770 train_time:6761ms step_avg:93.91ms
step:83/1770 train_time:6857ms step_avg:93.93ms
step:84/1770 train_time:6950ms step_avg:93.92ms
step:85/1770 train_time:7044ms step_avg:93.92ms
step:86/1770 train_time:7137ms step_avg:93.91ms
step:87/1770 train_time:7230ms step_avg:93.90ms
step:88/1770 train_time:7324ms step_avg:93.90ms
step:89/1770 train_time:7418ms step_avg:93.90ms
step:90/1770 train_time:7512ms step_avg:93.90ms
step:91/1770 train_time:7605ms step_avg:93.89ms
step:92/1770 train_time:7699ms step_avg:93.89ms
step:93/1770 train_time:7793ms step_avg:93.90ms
step:94/1770 train_time:7887ms step_avg:93.89ms
step:95/1770 train_time:7981ms step_avg:93.89ms
step:96/1770 train_time:8075ms step_avg:93.90ms
step:97/1770 train_time:8169ms step_avg:93.90ms
step:98/1770 train_time:8263ms step_avg:93.90ms
step:99/1770 train_time:8357ms step_avg:93.90ms
step:100/1770 train_time:8451ms step_avg:93.90ms
step:101/1770 train_time:8544ms step_avg:93.90ms
step:102/1770 train_time:8638ms step_avg:93.90ms
step:103/1770 train_time:8732ms step_avg:93.90ms
step:104/1770 train_time:8826ms step_avg:93.89ms
step:105/1770 train_time:8919ms step_avg:93.89ms
step:106/1770 train_time:9013ms step_avg:93.88ms
step:107/1770 train_time:9107ms step_avg:93.88ms
step:108/1770 train_time:9201ms step_avg:93.89ms
step:109/1770 train_time:9295ms step_avg:93.88ms
step:110/1770 train_time:9388ms step_avg:93.88ms
step:111/1770 train_time:9482ms step_avg:93.88ms
step:112/1770 train_time:9575ms step_avg:93.88ms
step:113/1770 train_time:9669ms step_avg:93.87ms
step:114/1770 train_time:9764ms step_avg:93.89ms
step:115/1770 train_time:9858ms step_avg:93.88ms
step:116/1770 train_time:9951ms step_avg:93.88ms
step:117/1770 train_time:10045ms step_avg:93.88ms
step:118/1770 train_time:10138ms step_avg:93.87ms
step:119/1770 train_time:10232ms step_avg:93.87ms
step:120/1770 train_time:10325ms step_avg:93.87ms
step:121/1770 train_time:10419ms step_avg:93.86ms
step:122/1770 train_time:10513ms step_avg:93.87ms
step:123/1770 train_time:10607ms step_avg:93.87ms
step:124/1770 train_time:10700ms step_avg:93.86ms
step:125/1770 train_time:10794ms step_avg:93.86ms
step:125/1770 val_loss:4.6515 train_time:10886ms step_avg:94.66ms
step:126/1770 train_time:10908ms step_avg:94.04ms
step:127/1770 train_time:10986ms step_avg:93.90ms
step:128/1770 train_time:11085ms step_avg:93.94ms
step:129/1770 train_time:11187ms step_avg:94.00ms
step:130/1770 train_time:11282ms step_avg:94.01ms
step:131/1770 train_time:11375ms step_avg:94.01ms
step:132/1770 train_time:11468ms step_avg:94.00ms
step:133/1770 train_time:11562ms step_avg:94.00ms
step:134/1770 train_time:11656ms step_avg:94.00ms
step:135/1770 train_time:11751ms step_avg:94.00ms
step:136/1770 train_time:11845ms step_avg:94.01ms
step:137/1770 train_time:11939ms step_avg:94.00ms
step:138/1770 train_time:12033ms step_avg:94.01ms
step:139/1770 train_time:12128ms step_avg:94.01ms
step:140/1770 train_time:12223ms step_avg:94.02ms
step:141/1770 train_time:12318ms step_avg:94.03ms
step:142/1770 train_time:12412ms step_avg:94.03ms
step:143/1770 train_time:12507ms step_avg:94.04ms
step:144/1770 train_time:12601ms step_avg:94.04ms
step:145/1770 train_time:12695ms step_avg:94.04ms
step:146/1770 train_time:12789ms step_avg:94.04ms
step:147/1770 train_time:12883ms step_avg:94.04ms
step:148/1770 train_time:12977ms step_avg:94.04ms
step:149/1770 train_time:13072ms step_avg:94.04ms
step:150/1770 train_time:13167ms step_avg:94.05ms
step:151/1770 train_time:13261ms step_avg:94.05ms
step:152/1770 train_time:13356ms step_avg:94.06ms
step:153/1770 train_time:13451ms step_avg:94.06ms
step:154/1770 train_time:13545ms step_avg:94.06ms
step:155/1770 train_time:13639ms step_avg:94.06ms
step:156/1770 train_time:13733ms step_avg:94.06ms
step:157/1770 train_time:13827ms step_avg:94.06ms
step:158/1770 train_time:13921ms step_avg:94.06ms
step:159/1770 train_time:14015ms step_avg:94.06ms
step:160/1770 train_time:14110ms step_avg:94.07ms
step:161/1770 train_time:14205ms step_avg:94.07ms
step:162/1770 train_time:14299ms step_avg:94.07ms
step:163/1770 train_time:14394ms step_avg:94.08ms
step:164/1770 train_time:14488ms step_avg:94.08ms
step:165/1770 train_time:14583ms step_avg:94.08ms
step:166/1770 train_time:14677ms step_avg:94.08ms
step:167/1770 train_time:14771ms step_avg:94.08ms
step:168/1770 train_time:14865ms step_avg:94.08ms
step:169/1770 train_time:14960ms step_avg:94.09ms
step:170/1770 train_time:15053ms step_avg:94.08ms
step:171/1770 train_time:15148ms step_avg:94.09ms
step:172/1770 train_time:15242ms step_avg:94.09ms
step:173/1770 train_time:15337ms step_avg:94.09ms
step:174/1770 train_time:15431ms step_avg:94.09ms
step:175/1770 train_time:15526ms step_avg:94.09ms
step:176/1770 train_time:15620ms step_avg:94.10ms
step:177/1770 train_time:15715ms step_avg:94.10ms
step:178/1770 train_time:15809ms step_avg:94.10ms
step:179/1770 train_time:15904ms step_avg:94.10ms
step:180/1770 train_time:15998ms step_avg:94.10ms
step:181/1770 train_time:16092ms step_avg:94.11ms
step:182/1770 train_time:16187ms step_avg:94.11ms
step:183/1770 train_time:16282ms step_avg:94.11ms
step:184/1770 train_time:16376ms step_avg:94.11ms
step:185/1770 train_time:16470ms step_avg:94.12ms
step:186/1770 train_time:16565ms step_avg:94.12ms
step:187/1770 train_time:16659ms step_avg:94.12ms
step:188/1770 train_time:16753ms step_avg:94.12ms
step:189/1770 train_time:16847ms step_avg:94.12ms
step:190/1770 train_time:16942ms step_avg:94.12ms
step:191/1770 train_time:17036ms step_avg:94.12ms
step:192/1770 train_time:17131ms step_avg:94.13ms
step:193/1770 train_time:17225ms step_avg:94.13ms
step:194/1770 train_time:17319ms step_avg:94.13ms
step:195/1770 train_time:17414ms step_avg:94.13ms
step:196/1770 train_time:17508ms step_avg:94.13ms
step:197/1770 train_time:17603ms step_avg:94.13ms
step:198/1770 train_time:17697ms step_avg:94.13ms
step:199/1770 train_time:17792ms step_avg:94.13ms
step:200/1770 train_time:17886ms step_avg:94.14ms
step:201/1770 train_time:17981ms step_avg:94.14ms
step:202/1770 train_time:18076ms step_avg:94.15ms
step:203/1770 train_time:18171ms step_avg:94.15ms
step:204/1770 train_time:18265ms step_avg:94.15ms
step:205/1770 train_time:18359ms step_avg:94.15ms
step:206/1770 train_time:18453ms step_avg:94.15ms
step:207/1770 train_time:18548ms step_avg:94.15ms
step:208/1770 train_time:18643ms step_avg:94.16ms
step:209/1770 train_time:18737ms step_avg:94.16ms
step:210/1770 train_time:18832ms step_avg:94.16ms
step:211/1770 train_time:18928ms step_avg:94.17ms
step:212/1770 train_time:19022ms step_avg:94.17ms
step:213/1770 train_time:19117ms step_avg:94.17ms
step:214/1770 train_time:19211ms step_avg:94.17ms
step:215/1770 train_time:19306ms step_avg:94.17ms
step:216/1770 train_time:19400ms step_avg:94.17ms
step:217/1770 train_time:19496ms step_avg:94.18ms
step:218/1770 train_time:19589ms step_avg:94.18ms
step:219/1770 train_time:19684ms step_avg:94.18ms
step:220/1770 train_time:19778ms step_avg:94.18ms
step:221/1770 train_time:19873ms step_avg:94.18ms
step:222/1770 train_time:19968ms step_avg:94.19ms
step:223/1770 train_time:20062ms step_avg:94.19ms
step:224/1770 train_time:20157ms step_avg:94.19ms
step:225/1770 train_time:20251ms step_avg:94.19ms
step:226/1770 train_time:20346ms step_avg:94.19ms
step:227/1770 train_time:20440ms step_avg:94.19ms
step:228/1770 train_time:20534ms step_avg:94.19ms
step:229/1770 train_time:20628ms step_avg:94.19ms
step:230/1770 train_time:20723ms step_avg:94.19ms
step:231/1770 train_time:20817ms step_avg:94.20ms
step:232/1770 train_time:20912ms step_avg:94.20ms
step:233/1770 train_time:21006ms step_avg:94.20ms
step:234/1770 train_time:21100ms step_avg:94.20ms
step:235/1770 train_time:21195ms step_avg:94.20ms
step:236/1770 train_time:21290ms step_avg:94.20ms
step:237/1770 train_time:21384ms step_avg:94.20ms
step:238/1770 train_time:21479ms step_avg:94.20ms
step:239/1770 train_time:21574ms step_avg:94.21ms
step:240/1770 train_time:21669ms step_avg:94.21ms
step:241/1770 train_time:21763ms step_avg:94.21ms
step:242/1770 train_time:21858ms step_avg:94.21ms
step:243/1770 train_time:21952ms step_avg:94.22ms
step:244/1770 train_time:22046ms step_avg:94.22ms
step:245/1770 train_time:22141ms step_avg:94.22ms
step:246/1770 train_time:22235ms step_avg:94.21ms
step:247/1770 train_time:22329ms step_avg:94.22ms
step:248/1770 train_time:22424ms step_avg:94.22ms
step:249/1770 train_time:22519ms step_avg:94.22ms
step:250/1770 train_time:22614ms step_avg:94.22ms
step:250/1770 val_loss:4.1124 train_time:22707ms step_avg:94.61ms
step:251/1770 train_time:22733ms step_avg:94.33ms
step:252/1770 train_time:22812ms step_avg:94.26ms
step:253/1770 train_time:22910ms step_avg:94.28ms
step:254/1770 train_time:23006ms step_avg:94.29ms
step:255/1770 train_time:23100ms step_avg:94.29ms
step:256/1770 train_time:23194ms step_avg:94.29ms
step:257/1770 train_time:23288ms step_avg:94.28ms
step:258/1770 train_time:23382ms step_avg:94.28ms
step:259/1770 train_time:23476ms step_avg:94.28ms
step:260/1770 train_time:23571ms step_avg:94.28ms
step:261/1770 train_time:23664ms step_avg:94.28ms
step:262/1770 train_time:23759ms step_avg:94.28ms
step:263/1770 train_time:23854ms step_avg:94.28ms
step:264/1770 train_time:23949ms step_avg:94.29ms
step:265/1770 train_time:24044ms step_avg:94.29ms
step:266/1770 train_time:24140ms step_avg:94.30ms
step:267/1770 train_time:24234ms step_avg:94.30ms
step:268/1770 train_time:24329ms step_avg:94.30ms
step:269/1770 train_time:24424ms step_avg:94.30ms
step:270/1770 train_time:24519ms step_avg:94.30ms
step:271/1770 train_time:24613ms step_avg:94.30ms
step:272/1770 train_time:24708ms step_avg:94.31ms
step:273/1770 train_time:24803ms step_avg:94.31ms
step:274/1770 train_time:24899ms step_avg:94.31ms
step:275/1770 train_time:24995ms step_avg:94.32ms
step:276/1770 train_time:25091ms step_avg:94.33ms
step:277/1770 train_time:25186ms step_avg:94.33ms
step:278/1770 train_time:25281ms step_avg:94.33ms
step:279/1770 train_time:25376ms step_avg:94.33ms
step:280/1770 train_time:25471ms step_avg:94.34ms
step:281/1770 train_time:25565ms step_avg:94.34ms
step:282/1770 train_time:25660ms step_avg:94.34ms
step:283/1770 train_time:25755ms step_avg:94.34ms
step:284/1770 train_time:25851ms step_avg:94.35ms
step:285/1770 train_time:25946ms step_avg:94.35ms
step:286/1770 train_time:26042ms step_avg:94.35ms
step:287/1770 train_time:26137ms step_avg:94.36ms
step:288/1770 train_time:26232ms step_avg:94.36ms
step:289/1770 train_time:26327ms step_avg:94.36ms
step:290/1770 train_time:26422ms step_avg:94.36ms
step:291/1770 train_time:26517ms step_avg:94.37ms
step:292/1770 train_time:26612ms step_avg:94.37ms
step:293/1770 train_time:26707ms step_avg:94.37ms
step:294/1770 train_time:26801ms step_avg:94.37ms
step:295/1770 train_time:26897ms step_avg:94.38ms
step:296/1770 train_time:26993ms step_avg:94.38ms
step:297/1770 train_time:27088ms step_avg:94.38ms
step:298/1770 train_time:27182ms step_avg:94.38ms
step:299/1770 train_time:27278ms step_avg:94.39ms
step:300/1770 train_time:27373ms step_avg:94.39ms
step:301/1770 train_time:27468ms step_avg:94.39ms
step:302/1770 train_time:27563ms step_avg:94.39ms
step:303/1770 train_time:27658ms step_avg:94.40ms
step:304/1770 train_time:27753ms step_avg:94.40ms
step:305/1770 train_time:27848ms step_avg:94.40ms
step:306/1770 train_time:27943ms step_avg:94.40ms
step:307/1770 train_time:28039ms step_avg:94.41ms
step:308/1770 train_time:28134ms step_avg:94.41ms
step:309/1770 train_time:28229ms step_avg:94.41ms
step:310/1770 train_time:28323ms step_avg:94.41ms
step:311/1770 train_time:28418ms step_avg:94.41ms
step:312/1770 train_time:28514ms step_avg:94.42ms
step:313/1770 train_time:28609ms step_avg:94.42ms
step:314/1770 train_time:28704ms step_avg:94.42ms
step:315/1770 train_time:28799ms step_avg:94.42ms
step:316/1770 train_time:28894ms step_avg:94.43ms
step:317/1770 train_time:28989ms step_avg:94.43ms
step:318/1770 train_time:29084ms step_avg:94.43ms
step:319/1770 train_time:29178ms step_avg:94.43ms
step:320/1770 train_time:29273ms step_avg:94.43ms
step:321/1770 train_time:29368ms step_avg:94.43ms
step:322/1770 train_time:29463ms step_avg:94.43ms
step:323/1770 train_time:29559ms step_avg:94.44ms
step:324/1770 train_time:29654ms step_avg:94.44ms
step:325/1770 train_time:29749ms step_avg:94.44ms
step:326/1770 train_time:29845ms step_avg:94.45ms
step:327/1770 train_time:29940ms step_avg:94.45ms
step:328/1770 train_time:30035ms step_avg:94.45ms
step:329/1770 train_time:30130ms step_avg:94.45ms
step:330/1770 train_time:30225ms step_avg:94.45ms
step:331/1770 train_time:30320ms step_avg:94.45ms
step:332/1770 train_time:30415ms step_avg:94.46ms
step:333/1770 train_time:30509ms step_avg:94.46ms
step:334/1770 train_time:30604ms step_avg:94.46ms
step:335/1770 train_time:30700ms step_avg:94.46ms
step:336/1770 train_time:30795ms step_avg:94.46ms
step:337/1770 train_time:30890ms step_avg:94.47ms
step:338/1770 train_time:30985ms step_avg:94.47ms
step:339/1770 train_time:31079ms step_avg:94.47ms
step:340/1770 train_time:31174ms step_avg:94.47ms
step:341/1770 train_time:31269ms step_avg:94.47ms
step:342/1770 train_time:31364ms step_avg:94.47ms
step:343/1770 train_time:31459ms step_avg:94.47ms
step:344/1770 train_time:31555ms step_avg:94.47ms
step:345/1770 train_time:31649ms step_avg:94.48ms
step:346/1770 train_time:31745ms step_avg:94.48ms
step:347/1770 train_time:31840ms step_avg:94.48ms
step:348/1770 train_time:31935ms step_avg:94.48ms
step:349/1770 train_time:32030ms step_avg:94.48ms
step:350/1770 train_time:32125ms step_avg:94.48ms
step:351/1770 train_time:32220ms step_avg:94.49ms
step:352/1770 train_time:32315ms step_avg:94.49ms
step:353/1770 train_time:32410ms step_avg:94.49ms
step:354/1770 train_time:32505ms step_avg:94.49ms
step:355/1770 train_time:32600ms step_avg:94.49ms
step:356/1770 train_time:32695ms step_avg:94.50ms
step:357/1770 train_time:32791ms step_avg:94.50ms
step:358/1770 train_time:32886ms step_avg:94.50ms
step:359/1770 train_time:32981ms step_avg:94.50ms
step:360/1770 train_time:33077ms step_avg:94.50ms
step:361/1770 train_time:33172ms step_avg:94.51ms
step:362/1770 train_time:33268ms step_avg:94.51ms
step:363/1770 train_time:33363ms step_avg:94.51ms
step:364/1770 train_time:33458ms step_avg:94.51ms
step:365/1770 train_time:33553ms step_avg:94.52ms
step:366/1770 train_time:33648ms step_avg:94.52ms
step:367/1770 train_time:33743ms step_avg:94.52ms
step:368/1770 train_time:33838ms step_avg:94.52ms
step:369/1770 train_time:33933ms step_avg:94.52ms
step:370/1770 train_time:34028ms step_avg:94.52ms
step:371/1770 train_time:34123ms step_avg:94.52ms
step:372/1770 train_time:34219ms step_avg:94.53ms
step:373/1770 train_time:34313ms step_avg:94.52ms
step:374/1770 train_time:34407ms step_avg:94.53ms
step:375/1770 train_time:34503ms step_avg:94.53ms
step:375/1770 val_loss:3.9075 train_time:34596ms step_avg:94.78ms
step:376/1770 train_time:34618ms step_avg:94.59ms
step:377/1770 train_time:34703ms step_avg:94.56ms
step:378/1770 train_time:34802ms step_avg:94.57ms
step:379/1770 train_time:34897ms step_avg:94.57ms
step:380/1770 train_time:34992ms step_avg:94.57ms
step:381/1770 train_time:35086ms step_avg:94.57ms
step:382/1770 train_time:35180ms step_avg:94.57ms
step:383/1770 train_time:35275ms step_avg:94.57ms
step:384/1770 train_time:35370ms step_avg:94.57ms
step:385/1770 train_time:35465ms step_avg:94.57ms
step:386/1770 train_time:35559ms step_avg:94.57ms
step:387/1770 train_time:35654ms step_avg:94.57ms
step:388/1770 train_time:35750ms step_avg:94.58ms
step:389/1770 train_time:35846ms step_avg:94.58ms
step:390/1770 train_time:35941ms step_avg:94.58ms
step:391/1770 train_time:36036ms step_avg:94.58ms
step:392/1770 train_time:36131ms step_avg:94.58ms
step:393/1770 train_time:36225ms step_avg:94.58ms
step:394/1770 train_time:36320ms step_avg:94.58ms
step:395/1770 train_time:36415ms step_avg:94.59ms
step:396/1770 train_time:36512ms step_avg:94.59ms
step:397/1770 train_time:36609ms step_avg:94.60ms
step:398/1770 train_time:36705ms step_avg:94.60ms
step:399/1770 train_time:36802ms step_avg:94.61ms
step:400/1770 train_time:36899ms step_avg:94.61ms
step:401/1770 train_time:36996ms step_avg:94.62ms
step:402/1770 train_time:37094ms step_avg:94.63ms
step:403/1770 train_time:37191ms step_avg:94.63ms
step:404/1770 train_time:37288ms step_avg:94.64ms
step:405/1770 train_time:37385ms step_avg:94.65ms
step:406/1770 train_time:37482ms step_avg:94.65ms
step:407/1770 train_time:37578ms step_avg:94.66ms
step:408/1770 train_time:37675ms step_avg:94.66ms
step:409/1770 train_time:37772ms step_avg:94.67ms
step:410/1770 train_time:37869ms step_avg:94.67ms
step:411/1770 train_time:37965ms step_avg:94.68ms
step:412/1770 train_time:38062ms step_avg:94.68ms
step:413/1770 train_time:38160ms step_avg:94.69ms
step:414/1770 train_time:38257ms step_avg:94.70ms
step:415/1770 train_time:38355ms step_avg:94.70ms
step:416/1770 train_time:38452ms step_avg:94.71ms
step:417/1770 train_time:38549ms step_avg:94.72ms
step:418/1770 train_time:38647ms step_avg:94.72ms
step:419/1770 train_time:38746ms step_avg:94.73ms
step:420/1770 train_time:38840ms step_avg:94.73ms
step:421/1770 train_time:38937ms step_avg:94.74ms
step:422/1770 train_time:39035ms step_avg:94.74ms
step:423/1770 train_time:39132ms step_avg:94.75ms
step:424/1770 train_time:39228ms step_avg:94.75ms
step:425/1770 train_time:39325ms step_avg:94.76ms
step:426/1770 train_time:39422ms step_avg:94.76ms
step:427/1770 train_time:39520ms step_avg:94.77ms
step:428/1770 train_time:39616ms step_avg:94.78ms
step:429/1770 train_time:39714ms step_avg:94.78ms
step:430/1770 train_time:39811ms step_avg:94.79ms
step:431/1770 train_time:39907ms step_avg:94.79ms
step:432/1770 train_time:40004ms step_avg:94.80ms
step:433/1770 train_time:40100ms step_avg:94.80ms
step:434/1770 train_time:40197ms step_avg:94.80ms
step:435/1770 train_time:40294ms step_avg:94.81ms
step:436/1770 train_time:40391ms step_avg:94.81ms
step:437/1770 train_time:40488ms step_avg:94.82ms
step:438/1770 train_time:40585ms step_avg:94.82ms
step:439/1770 train_time:40682ms step_avg:94.83ms
step:440/1770 train_time:40779ms step_avg:94.84ms
step:441/1770 train_time:40876ms step_avg:94.84ms
step:442/1770 train_time:40974ms step_avg:94.85ms
step:443/1770 train_time:41070ms step_avg:94.85ms
step:444/1770 train_time:41167ms step_avg:94.85ms
step:445/1770 train_time:41263ms step_avg:94.86ms
step:446/1770 train_time:41361ms step_avg:94.86ms
step:447/1770 train_time:41458ms step_avg:94.87ms
step:448/1770 train_time:41556ms step_avg:94.88ms
step:449/1770 train_time:41653ms step_avg:94.88ms
step:450/1770 train_time:41750ms step_avg:94.89ms
step:451/1770 train_time:41848ms step_avg:94.89ms
step:452/1770 train_time:41945ms step_avg:94.90ms
step:453/1770 train_time:42042ms step_avg:94.90ms
step:454/1770 train_time:42138ms step_avg:94.91ms
step:455/1770 train_time:42236ms step_avg:94.91ms
step:456/1770 train_time:42333ms step_avg:94.92ms
step:457/1770 train_time:42430ms step_avg:94.92ms
step:458/1770 train_time:42526ms step_avg:94.92ms
step:459/1770 train_time:42624ms step_avg:94.93ms
step:460/1770 train_time:42720ms step_avg:94.93ms
step:461/1770 train_time:42818ms step_avg:94.94ms
step:462/1770 train_time:42916ms step_avg:94.95ms
step:463/1770 train_time:43013ms step_avg:94.95ms
step:464/1770 train_time:43110ms step_avg:94.96ms
step:465/1770 train_time:43207ms step_avg:94.96ms
step:466/1770 train_time:43304ms step_avg:94.97ms
step:467/1770 train_time:43401ms step_avg:94.97ms
step:468/1770 train_time:43498ms step_avg:94.97ms
step:469/1770 train_time:43596ms step_avg:94.98ms
step:470/1770 train_time:43693ms step_avg:94.98ms
step:471/1770 train_time:43790ms step_avg:94.99ms
step:472/1770 train_time:43887ms step_avg:94.99ms
step:473/1770 train_time:43984ms step_avg:95.00ms
step:474/1770 train_time:44081ms step_avg:95.00ms
step:475/1770 train_time:44178ms step_avg:95.01ms
step:476/1770 train_time:44275ms step_avg:95.01ms
step:477/1770 train_time:44372ms step_avg:95.01ms
step:478/1770 train_time:44468ms step_avg:95.02ms
step:479/1770 train_time:44565ms step_avg:95.02ms
step:480/1770 train_time:44662ms step_avg:95.03ms
step:481/1770 train_time:44759ms step_avg:95.03ms
step:482/1770 train_time:44857ms step_avg:95.04ms
step:483/1770 train_time:44954ms step_avg:95.04ms
step:484/1770 train_time:45051ms step_avg:95.05ms
step:485/1770 train_time:45148ms step_avg:95.05ms
step:486/1770 train_time:45245ms step_avg:95.05ms
step:487/1770 train_time:45341ms step_avg:95.05ms
step:488/1770 train_time:45438ms step_avg:95.06ms
step:489/1770 train_time:45535ms step_avg:95.06ms
step:490/1770 train_time:45633ms step_avg:95.07ms
step:491/1770 train_time:45730ms step_avg:95.07ms
step:492/1770 train_time:45829ms step_avg:95.08ms
step:493/1770 train_time:45924ms step_avg:95.08ms
step:494/1770 train_time:46020ms step_avg:95.08ms
step:495/1770 train_time:46118ms step_avg:95.09ms
step:496/1770 train_time:46215ms step_avg:95.09ms
step:497/1770 train_time:46312ms step_avg:95.10ms
step:498/1770 train_time:46409ms step_avg:95.10ms
step:499/1770 train_time:46505ms step_avg:95.10ms
step:500/1770 train_time:46602ms step_avg:95.11ms
step:500/1770 val_loss:3.7523 train_time:46697ms step_avg:95.30ms
step:501/1770 train_time:46720ms step_avg:95.15ms
step:502/1770 train_time:46808ms step_avg:95.14ms
step:503/1770 train_time:46909ms step_avg:95.15ms
step:504/1770 train_time:47006ms step_avg:95.15ms
step:505/1770 train_time:47102ms step_avg:95.16ms
step:506/1770 train_time:47199ms step_avg:95.16ms
step:507/1770 train_time:47296ms step_avg:95.16ms
step:508/1770 train_time:47392ms step_avg:95.16ms
step:509/1770 train_time:47489ms step_avg:95.17ms
step:510/1770 train_time:47585ms step_avg:95.17ms
step:511/1770 train_time:47682ms step_avg:95.17ms
step:512/1770 train_time:47779ms step_avg:95.18ms
step:513/1770 train_time:47876ms step_avg:95.18ms
step:514/1770 train_time:47974ms step_avg:95.19ms
step:515/1770 train_time:48071ms step_avg:95.19ms
step:516/1770 train_time:48168ms step_avg:95.19ms
step:517/1770 train_time:48265ms step_avg:95.20ms
step:518/1770 train_time:48362ms step_avg:95.20ms
step:519/1770 train_time:48459ms step_avg:95.20ms
step:520/1770 train_time:48555ms step_avg:95.21ms
step:521/1770 train_time:48652ms step_avg:95.21ms
step:522/1770 train_time:48749ms step_avg:95.21ms
step:523/1770 train_time:48846ms step_avg:95.22ms
step:524/1770 train_time:48943ms step_avg:95.22ms
step:525/1770 train_time:49039ms step_avg:95.22ms
step:526/1770 train_time:49136ms step_avg:95.23ms
step:527/1770 train_time:49233ms step_avg:95.23ms
step:528/1770 train_time:49330ms step_avg:95.23ms
step:529/1770 train_time:49428ms step_avg:95.24ms
step:530/1770 train_time:49525ms step_avg:95.24ms
step:531/1770 train_time:49622ms step_avg:95.24ms
step:532/1770 train_time:49720ms step_avg:95.25ms
step:533/1770 train_time:49817ms step_avg:95.25ms
step:534/1770 train_time:49915ms step_avg:95.26ms
step:535/1770 train_time:50011ms step_avg:95.26ms
step:536/1770 train_time:50108ms step_avg:95.26ms
step:537/1770 train_time:50205ms step_avg:95.27ms
step:538/1770 train_time:50303ms step_avg:95.27ms
step:539/1770 train_time:50399ms step_avg:95.27ms
step:540/1770 train_time:50497ms step_avg:95.28ms
step:541/1770 train_time:50594ms step_avg:95.28ms
step:542/1770 train_time:50692ms step_avg:95.29ms
step:543/1770 train_time:50789ms step_avg:95.29ms
step:544/1770 train_time:50888ms step_avg:95.30ms
step:545/1770 train_time:50986ms step_avg:95.30ms
step:546/1770 train_time:51083ms step_avg:95.30ms
step:547/1770 train_time:51180ms step_avg:95.31ms
step:548/1770 train_time:51277ms step_avg:95.31ms
step:549/1770 train_time:51373ms step_avg:95.31ms
step:550/1770 train_time:51470ms step_avg:95.32ms
step:551/1770 train_time:51568ms step_avg:95.32ms
step:552/1770 train_time:51665ms step_avg:95.32ms
step:553/1770 train_time:51763ms step_avg:95.33ms
step:554/1770 train_time:51861ms step_avg:95.33ms
step:555/1770 train_time:51958ms step_avg:95.34ms
step:556/1770 train_time:52056ms step_avg:95.34ms
step:557/1770 train_time:52152ms step_avg:95.34ms
step:558/1770 train_time:52249ms step_avg:95.34ms
step:559/1770 train_time:52349ms step_avg:95.35ms
step:560/1770 train_time:52443ms step_avg:95.35ms
step:561/1770 train_time:52540ms step_avg:95.35ms
step:562/1770 train_time:52637ms step_avg:95.36ms
step:563/1770 train_time:52734ms step_avg:95.36ms
step:564/1770 train_time:52831ms step_avg:95.36ms
step:565/1770 train_time:52928ms step_avg:95.37ms
step:566/1770 train_time:53026ms step_avg:95.37ms
step:567/1770 train_time:53124ms step_avg:95.37ms
step:568/1770 train_time:53221ms step_avg:95.38ms
step:569/1770 train_time:53318ms step_avg:95.38ms
step:570/1770 train_time:53415ms step_avg:95.38ms
step:571/1770 train_time:53512ms step_avg:95.39ms
step:572/1770 train_time:53609ms step_avg:95.39ms
step:573/1770 train_time:53706ms step_avg:95.39ms
step:574/1770 train_time:53803ms step_avg:95.40ms
step:575/1770 train_time:53901ms step_avg:95.40ms
step:576/1770 train_time:53998ms step_avg:95.40ms
step:577/1770 train_time:54096ms step_avg:95.41ms
step:578/1770 train_time:54193ms step_avg:95.41ms
step:579/1770 train_time:54291ms step_avg:95.41ms
step:580/1770 train_time:54388ms step_avg:95.42ms
step:581/1770 train_time:54485ms step_avg:95.42ms
step:582/1770 train_time:54582ms step_avg:95.42ms
step:583/1770 train_time:54679ms step_avg:95.43ms
step:584/1770 train_time:54777ms step_avg:95.43ms
step:585/1770 train_time:54874ms step_avg:95.43ms
step:586/1770 train_time:54971ms step_avg:95.44ms
step:587/1770 train_time:55068ms step_avg:95.44ms
step:588/1770 train_time:55166ms step_avg:95.44ms
step:589/1770 train_time:55263ms step_avg:95.45ms
step:590/1770 train_time:55361ms step_avg:95.45ms
step:591/1770 train_time:55458ms step_avg:95.45ms
step:592/1770 train_time:55555ms step_avg:95.46ms
step:593/1770 train_time:55652ms step_avg:95.46ms
step:594/1770 train_time:55749ms step_avg:95.46ms
step:595/1770 train_time:55847ms step_avg:95.47ms
step:596/1770 train_time:55944ms step_avg:95.47ms
step:597/1770 train_time:56042ms step_avg:95.47ms
step:598/1770 train_time:56139ms step_avg:95.47ms
step:599/1770 train_time:56235ms step_avg:95.48ms
step:600/1770 train_time:56332ms step_avg:95.48ms
step:601/1770 train_time:56430ms step_avg:95.48ms
step:602/1770 train_time:56528ms step_avg:95.49ms
step:603/1770 train_time:56625ms step_avg:95.49ms
step:604/1770 train_time:56723ms step_avg:95.49ms
step:605/1770 train_time:56820ms step_avg:95.50ms
step:606/1770 train_time:56917ms step_avg:95.50ms
step:607/1770 train_time:57015ms step_avg:95.50ms
step:608/1770 train_time:57112ms step_avg:95.50ms
step:609/1770 train_time:57210ms step_avg:95.51ms
step:610/1770 train_time:57307ms step_avg:95.51ms
step:611/1770 train_time:57404ms step_avg:95.51ms
step:612/1770 train_time:57502ms step_avg:95.52ms
step:613/1770 train_time:57600ms step_avg:95.52ms
step:614/1770 train_time:57697ms step_avg:95.52ms
step:615/1770 train_time:57794ms step_avg:95.53ms
step:616/1770 train_time:57892ms step_avg:95.53ms
step:617/1770 train_time:57989ms step_avg:95.53ms
step:618/1770 train_time:58087ms step_avg:95.54ms
step:619/1770 train_time:58184ms step_avg:95.54ms
step:620/1770 train_time:58281ms step_avg:95.54ms
step:621/1770 train_time:58378ms step_avg:95.55ms
step:622/1770 train_time:58475ms step_avg:95.55ms
step:623/1770 train_time:58572ms step_avg:95.55ms
step:624/1770 train_time:58669ms step_avg:95.55ms
step:625/1770 train_time:58767ms step_avg:95.56ms
step:625/1770 val_loss:3.6628 train_time:58863ms step_avg:95.71ms
step:626/1770 train_time:58885ms step_avg:95.59ms
step:627/1770 train_time:58973ms step_avg:95.58ms
step:628/1770 train_time:59073ms step_avg:95.59ms
step:629/1770 train_time:59172ms step_avg:95.59ms
step:630/1770 train_time:59270ms step_avg:95.60ms
step:631/1770 train_time:59367ms step_avg:95.60ms
step:632/1770 train_time:59463ms step_avg:95.60ms
step:633/1770 train_time:59560ms step_avg:95.60ms
step:634/1770 train_time:59657ms step_avg:95.60ms
step:635/1770 train_time:59753ms step_avg:95.60ms
step:636/1770 train_time:59849ms step_avg:95.61ms
step:637/1770 train_time:59947ms step_avg:95.61ms
step:638/1770 train_time:60045ms step_avg:95.61ms
step:639/1770 train_time:60143ms step_avg:95.62ms
step:640/1770 train_time:60241ms step_avg:95.62ms
step:641/1770 train_time:60339ms step_avg:95.62ms
step:642/1770 train_time:60436ms step_avg:95.63ms
step:643/1770 train_time:60534ms step_avg:95.63ms
step:644/1770 train_time:60631ms step_avg:95.63ms
step:645/1770 train_time:60729ms step_avg:95.64ms
step:646/1770 train_time:60826ms step_avg:95.64ms
step:647/1770 train_time:60923ms step_avg:95.64ms
step:648/1770 train_time:61020ms step_avg:95.64ms
step:649/1770 train_time:61117ms step_avg:95.64ms
step:650/1770 train_time:61213ms step_avg:95.65ms
step:651/1770 train_time:61311ms step_avg:95.65ms
step:652/1770 train_time:61408ms step_avg:95.65ms
step:653/1770 train_time:61506ms step_avg:95.65ms
step:654/1770 train_time:61603ms step_avg:95.66ms
step:655/1770 train_time:61701ms step_avg:95.66ms
step:656/1770 train_time:61799ms step_avg:95.66ms
step:657/1770 train_time:61897ms step_avg:95.67ms
step:658/1770 train_time:61995ms step_avg:95.67ms
step:659/1770 train_time:62093ms step_avg:95.68ms
step:660/1770 train_time:62192ms step_avg:95.68ms
step:661/1770 train_time:62290ms step_avg:95.68ms
step:662/1770 train_time:62389ms step_avg:95.69ms
step:663/1770 train_time:62489ms step_avg:95.70ms
step:664/1770 train_time:62588ms step_avg:95.70ms
step:665/1770 train_time:62687ms step_avg:95.71ms
step:666/1770 train_time:62787ms step_avg:95.71ms
step:667/1770 train_time:62886ms step_avg:95.72ms
step:668/1770 train_time:62986ms step_avg:95.72ms
step:669/1770 train_time:63086ms step_avg:95.73ms
step:670/1770 train_time:63185ms step_avg:95.74ms
step:671/1770 train_time:63285ms step_avg:95.74ms
step:672/1770 train_time:63383ms step_avg:95.75ms
step:673/1770 train_time:63482ms step_avg:95.75ms
step:674/1770 train_time:63580ms step_avg:95.75ms
step:675/1770 train_time:63679ms step_avg:95.76ms
step:676/1770 train_time:63778ms step_avg:95.76ms
step:677/1770 train_time:63878ms step_avg:95.77ms
step:678/1770 train_time:63977ms step_avg:95.77ms
step:679/1770 train_time:64076ms step_avg:95.78ms
step:680/1770 train_time:64175ms step_avg:95.78ms
step:681/1770 train_time:64274ms step_avg:95.79ms
step:682/1770 train_time:64372ms step_avg:95.79ms
step:683/1770 train_time:64472ms step_avg:95.80ms
step:684/1770 train_time:64571ms step_avg:95.80ms
step:685/1770 train_time:64670ms step_avg:95.81ms
step:686/1770 train_time:64770ms step_avg:95.81ms
step:687/1770 train_time:64870ms step_avg:95.82ms
step:688/1770 train_time:64969ms step_avg:95.82ms
step:689/1770 train_time:65068ms step_avg:95.83ms
step:690/1770 train_time:65168ms step_avg:95.84ms
step:691/1770 train_time:65268ms step_avg:95.84ms
step:692/1770 train_time:65367ms step_avg:95.85ms
step:693/1770 train_time:65466ms step_avg:95.85ms
step:694/1770 train_time:65565ms step_avg:95.86ms
step:695/1770 train_time:65665ms step_avg:95.86ms
step:696/1770 train_time:65764ms step_avg:95.87ms
step:697/1770 train_time:65863ms step_avg:95.87ms
step:698/1770 train_time:65962ms step_avg:95.87ms
step:699/1770 train_time:66061ms step_avg:95.88ms
step:700/1770 train_time:66160ms step_avg:95.88ms
step:701/1770 train_time:66260ms step_avg:95.89ms
step:702/1770 train_time:66358ms step_avg:95.89ms
step:703/1770 train_time:66457ms step_avg:95.90ms
step:704/1770 train_time:66556ms step_avg:95.90ms
step:705/1770 train_time:66658ms step_avg:95.91ms
step:706/1770 train_time:66754ms step_avg:95.91ms
step:707/1770 train_time:66853ms step_avg:95.92ms
step:708/1770 train_time:66952ms step_avg:95.92ms
step:709/1770 train_time:67051ms step_avg:95.92ms
step:710/1770 train_time:67152ms step_avg:95.93ms
step:711/1770 train_time:67251ms step_avg:95.94ms
step:712/1770 train_time:67350ms step_avg:95.94ms
step:713/1770 train_time:67449ms step_avg:95.94ms
step:714/1770 train_time:67549ms step_avg:95.95ms
step:715/1770 train_time:67649ms step_avg:95.96ms
step:716/1770 train_time:67747ms step_avg:95.96ms
step:717/1770 train_time:67847ms step_avg:95.96ms
step:718/1770 train_time:67946ms step_avg:95.97ms
step:719/1770 train_time:68045ms step_avg:95.97ms
step:720/1770 train_time:68144ms step_avg:95.98ms
step:721/1770 train_time:68243ms step_avg:95.98ms
step:722/1770 train_time:68342ms step_avg:95.99ms
step:723/1770 train_time:68440ms step_avg:95.99ms
step:724/1770 train_time:68539ms step_avg:95.99ms
step:725/1770 train_time:68639ms step_avg:96.00ms
step:726/1770 train_time:68737ms step_avg:96.00ms
step:727/1770 train_time:68836ms step_avg:96.01ms
step:728/1770 train_time:68934ms step_avg:96.01ms
step:729/1770 train_time:69033ms step_avg:96.01ms
step:730/1770 train_time:69132ms step_avg:96.02ms
step:731/1770 train_time:69230ms step_avg:96.02ms
step:732/1770 train_time:69330ms step_avg:96.02ms
step:733/1770 train_time:69430ms step_avg:96.03ms
step:734/1770 train_time:69530ms step_avg:96.04ms
step:735/1770 train_time:69630ms step_avg:96.04ms
step:736/1770 train_time:69730ms step_avg:96.05ms
step:737/1770 train_time:69829ms step_avg:96.05ms
step:738/1770 train_time:69929ms step_avg:96.06ms
step:739/1770 train_time:70028ms step_avg:96.06ms
step:740/1770 train_time:70127ms step_avg:96.06ms
step:741/1770 train_time:70226ms step_avg:96.07ms
step:742/1770 train_time:70326ms step_avg:96.07ms
step:743/1770 train_time:70425ms step_avg:96.08ms
step:744/1770 train_time:70525ms step_avg:96.08ms
step:745/1770 train_time:70624ms step_avg:96.09ms
step:746/1770 train_time:70722ms step_avg:96.09ms
step:747/1770 train_time:70821ms step_avg:96.09ms
step:748/1770 train_time:70921ms step_avg:96.10ms
step:749/1770 train_time:71020ms step_avg:96.10ms
step:750/1770 train_time:71118ms step_avg:96.11ms
step:750/1770 val_loss:3.5981 train_time:71215ms step_avg:96.24ms
step:751/1770 train_time:71237ms step_avg:96.14ms
step:752/1770 train_time:71329ms step_avg:96.13ms
step:753/1770 train_time:71430ms step_avg:96.14ms
step:754/1770 train_time:71530ms step_avg:96.14ms
step:755/1770 train_time:71628ms step_avg:96.15ms
step:756/1770 train_time:71727ms step_avg:96.15ms
step:757/1770 train_time:71826ms step_avg:96.15ms
step:758/1770 train_time:71925ms step_avg:96.16ms
step:759/1770 train_time:72023ms step_avg:96.16ms
step:760/1770 train_time:72121ms step_avg:96.16ms
step:761/1770 train_time:72221ms step_avg:96.17ms
step:762/1770 train_time:72321ms step_avg:96.17ms
step:763/1770 train_time:72420ms step_avg:96.18ms
step:764/1770 train_time:72519ms step_avg:96.18ms
step:765/1770 train_time:72618ms step_avg:96.18ms
step:766/1770 train_time:72718ms step_avg:96.19ms
step:767/1770 train_time:72819ms step_avg:96.19ms
step:768/1770 train_time:72918ms step_avg:96.20ms
step:769/1770 train_time:73018ms step_avg:96.20ms
step:770/1770 train_time:73117ms step_avg:96.21ms
step:771/1770 train_time:73217ms step_avg:96.21ms
step:772/1770 train_time:73316ms step_avg:96.22ms
step:773/1770 train_time:73415ms step_avg:96.22ms
step:774/1770 train_time:73515ms step_avg:96.22ms
step:775/1770 train_time:73614ms step_avg:96.23ms
step:776/1770 train_time:73713ms step_avg:96.23ms
step:777/1770 train_time:73813ms step_avg:96.24ms
step:778/1770 train_time:73912ms step_avg:96.24ms
step:779/1770 train_time:74011ms step_avg:96.24ms
step:780/1770 train_time:74111ms step_avg:96.25ms
step:781/1770 train_time:74209ms step_avg:96.25ms
step:782/1770 train_time:74308ms step_avg:96.25ms
step:783/1770 train_time:74407ms step_avg:96.26ms
step:784/1770 train_time:74507ms step_avg:96.26ms
step:785/1770 train_time:74606ms step_avg:96.27ms
step:786/1770 train_time:74706ms step_avg:96.27ms
step:787/1770 train_time:74805ms step_avg:96.27ms
step:788/1770 train_time:74905ms step_avg:96.28ms
step:789/1770 train_time:75005ms step_avg:96.28ms
step:790/1770 train_time:75105ms step_avg:96.29ms
step:791/1770 train_time:75204ms step_avg:96.29ms
step:792/1770 train_time:75303ms step_avg:96.30ms
step:793/1770 train_time:75402ms step_avg:96.30ms
step:794/1770 train_time:75501ms step_avg:96.30ms
step:795/1770 train_time:75601ms step_avg:96.31ms
step:796/1770 train_time:75700ms step_avg:96.31ms
step:797/1770 train_time:75799ms step_avg:96.31ms
step:798/1770 train_time:75899ms step_avg:96.32ms
step:799/1770 train_time:75998ms step_avg:96.32ms
step:800/1770 train_time:76098ms step_avg:96.33ms
step:801/1770 train_time:76197ms step_avg:96.33ms
step:802/1770 train_time:76297ms step_avg:96.33ms
step:803/1770 train_time:76397ms step_avg:96.34ms
step:804/1770 train_time:76497ms step_avg:96.34ms
step:805/1770 train_time:76596ms step_avg:96.35ms
step:806/1770 train_time:76696ms step_avg:96.35ms
step:807/1770 train_time:76795ms step_avg:96.35ms
step:808/1770 train_time:76895ms step_avg:96.36ms
step:809/1770 train_time:76995ms step_avg:96.36ms
step:810/1770 train_time:77095ms step_avg:96.37ms
step:811/1770 train_time:77194ms step_avg:96.37ms
step:812/1770 train_time:77293ms step_avg:96.38ms
step:813/1770 train_time:77393ms step_avg:96.38ms
step:814/1770 train_time:77492ms step_avg:96.38ms
step:815/1770 train_time:77591ms step_avg:96.39ms
step:816/1770 train_time:77691ms step_avg:96.39ms
step:817/1770 train_time:77791ms step_avg:96.40ms
step:818/1770 train_time:77890ms step_avg:96.40ms
step:819/1770 train_time:77990ms step_avg:96.40ms
step:820/1770 train_time:78089ms step_avg:96.41ms
step:821/1770 train_time:78188ms step_avg:96.41ms
step:822/1770 train_time:78287ms step_avg:96.41ms
step:823/1770 train_time:78386ms step_avg:96.42ms
step:824/1770 train_time:78486ms step_avg:96.42ms
step:825/1770 train_time:78585ms step_avg:96.42ms
step:826/1770 train_time:78684ms step_avg:96.43ms
step:827/1770 train_time:78783ms step_avg:96.43ms
step:828/1770 train_time:78883ms step_avg:96.43ms
step:829/1770 train_time:78982ms step_avg:96.44ms
step:830/1770 train_time:79081ms step_avg:96.44ms
step:831/1770 train_time:79181ms step_avg:96.44ms
step:832/1770 train_time:79281ms step_avg:96.45ms
step:833/1770 train_time:79381ms step_avg:96.45ms
step:834/1770 train_time:79480ms step_avg:96.46ms
step:835/1770 train_time:79579ms step_avg:96.46ms
step:836/1770 train_time:79679ms step_avg:96.46ms
step:837/1770 train_time:79778ms step_avg:96.47ms
step:838/1770 train_time:79878ms step_avg:96.47ms
step:839/1770 train_time:79977ms step_avg:96.47ms
step:840/1770 train_time:80076ms step_avg:96.48ms
step:841/1770 train_time:80176ms step_avg:96.48ms
step:842/1770 train_time:80276ms step_avg:96.49ms
step:843/1770 train_time:80375ms step_avg:96.49ms
step:844/1770 train_time:80475ms step_avg:96.49ms
step:845/1770 train_time:80575ms step_avg:96.50ms
step:846/1770 train_time:80675ms step_avg:96.50ms
step:847/1770 train_time:80774ms step_avg:96.50ms
step:848/1770 train_time:80874ms step_avg:96.51ms
step:849/1770 train_time:80974ms step_avg:96.51ms
step:850/1770 train_time:81073ms step_avg:96.52ms
step:851/1770 train_time:81172ms step_avg:96.52ms
step:852/1770 train_time:81271ms step_avg:96.52ms
step:853/1770 train_time:81370ms step_avg:96.52ms
step:854/1770 train_time:81469ms step_avg:96.53ms
step:855/1770 train_time:81568ms step_avg:96.53ms
step:856/1770 train_time:81667ms step_avg:96.53ms
step:857/1770 train_time:81767ms step_avg:96.54ms
step:858/1770 train_time:81866ms step_avg:96.54ms
step:859/1770 train_time:81968ms step_avg:96.55ms
step:860/1770 train_time:82067ms step_avg:96.55ms
step:861/1770 train_time:82166ms step_avg:96.55ms
step:862/1770 train_time:82266ms step_avg:96.56ms
step:863/1770 train_time:82365ms step_avg:96.56ms
step:864/1770 train_time:82463ms step_avg:96.56ms
step:865/1770 train_time:82562ms step_avg:96.56ms
step:866/1770 train_time:82662ms step_avg:96.57ms
step:867/1770 train_time:82761ms step_avg:96.57ms
step:868/1770 train_time:82861ms step_avg:96.57ms
step:869/1770 train_time:82961ms step_avg:96.58ms
step:870/1770 train_time:83060ms step_avg:96.58ms
step:871/1770 train_time:83160ms step_avg:96.59ms
step:872/1770 train_time:83259ms step_avg:96.59ms
step:873/1770 train_time:83358ms step_avg:96.59ms
step:874/1770 train_time:83459ms step_avg:96.60ms
step:875/1770 train_time:83561ms step_avg:96.60ms
step:875/1770 val_loss:3.5500 train_time:83655ms step_avg:96.71ms
step:876/1770 train_time:83678ms step_avg:96.63ms
step:877/1770 train_time:83767ms step_avg:96.62ms
step:878/1770 train_time:83869ms step_avg:96.62ms
step:879/1770 train_time:83968ms step_avg:96.63ms
step:880/1770 train_time:84067ms step_avg:96.63ms
step:881/1770 train_time:84166ms step_avg:96.63ms
step:882/1770 train_time:84264ms step_avg:96.63ms
step:883/1770 train_time:84363ms step_avg:96.64ms
step:884/1770 train_time:84462ms step_avg:96.64ms
step:885/1770 train_time:84560ms step_avg:96.64ms
step:886/1770 train_time:84659ms step_avg:96.64ms
step:887/1770 train_time:84761ms step_avg:96.65ms
step:888/1770 train_time:84861ms step_avg:96.65ms
step:889/1770 train_time:84961ms step_avg:96.66ms
step:890/1770 train_time:85060ms step_avg:96.66ms
step:891/1770 train_time:85159ms step_avg:96.66ms
step:892/1770 train_time:85259ms step_avg:96.66ms
step:893/1770 train_time:85358ms step_avg:96.67ms
step:894/1770 train_time:85456ms step_avg:96.67ms
step:895/1770 train_time:85556ms step_avg:96.67ms
step:896/1770 train_time:85655ms step_avg:96.68ms
step:897/1770 train_time:85755ms step_avg:96.68ms
step:898/1770 train_time:85855ms step_avg:96.68ms
step:899/1770 train_time:85955ms step_avg:96.69ms
step:900/1770 train_time:86055ms step_avg:96.69ms
step:901/1770 train_time:86154ms step_avg:96.69ms
step:902/1770 train_time:86253ms step_avg:96.70ms
step:903/1770 train_time:86353ms step_avg:96.70ms
step:904/1770 train_time:86452ms step_avg:96.70ms
step:905/1770 train_time:86551ms step_avg:96.71ms
step:906/1770 train_time:86650ms step_avg:96.71ms
step:907/1770 train_time:86752ms step_avg:96.71ms
step:908/1770 train_time:86849ms step_avg:96.71ms
step:909/1770 train_time:86948ms step_avg:96.72ms
step:910/1770 train_time:87048ms step_avg:96.72ms
step:911/1770 train_time:87147ms step_avg:96.72ms
step:912/1770 train_time:87247ms step_avg:96.73ms
step:913/1770 train_time:87347ms step_avg:96.73ms
step:914/1770 train_time:87448ms step_avg:96.73ms
step:915/1770 train_time:87548ms step_avg:96.74ms
step:916/1770 train_time:87647ms step_avg:96.74ms
step:917/1770 train_time:87747ms step_avg:96.74ms
step:918/1770 train_time:87846ms step_avg:96.75ms
step:919/1770 train_time:87946ms step_avg:96.75ms
step:920/1770 train_time:88047ms step_avg:96.75ms
step:921/1770 train_time:88148ms step_avg:96.76ms
step:922/1770 train_time:88249ms step_avg:96.76ms
step:923/1770 train_time:88349ms step_avg:96.77ms
step:924/1770 train_time:88450ms step_avg:96.77ms
step:925/1770 train_time:88550ms step_avg:96.78ms
step:926/1770 train_time:88653ms step_avg:96.78ms
step:927/1770 train_time:88751ms step_avg:96.78ms
step:928/1770 train_time:88852ms step_avg:96.79ms
step:929/1770 train_time:88952ms step_avg:96.79ms
step:930/1770 train_time:89052ms step_avg:96.80ms
step:931/1770 train_time:89153ms step_avg:96.80ms
step:932/1770 train_time:89253ms step_avg:96.80ms
step:933/1770 train_time:89354ms step_avg:96.81ms
step:934/1770 train_time:89456ms step_avg:96.81ms
step:935/1770 train_time:89557ms step_avg:96.82ms
step:936/1770 train_time:89657ms step_avg:96.82ms
step:937/1770 train_time:89757ms step_avg:96.83ms
step:938/1770 train_time:89859ms step_avg:96.83ms
step:939/1770 train_time:89960ms step_avg:96.83ms
step:940/1770 train_time:90061ms step_avg:96.84ms
step:941/1770 train_time:90162ms step_avg:96.84ms
step:942/1770 train_time:90265ms step_avg:96.85ms
step:943/1770 train_time:90366ms step_avg:96.85ms
step:944/1770 train_time:90466ms step_avg:96.86ms
step:945/1770 train_time:90566ms step_avg:96.86ms
step:946/1770 train_time:90668ms step_avg:96.87ms
step:947/1770 train_time:90769ms step_avg:96.87ms
step:948/1770 train_time:90869ms step_avg:96.88ms
step:949/1770 train_time:90970ms step_avg:96.88ms
step:950/1770 train_time:91071ms step_avg:96.88ms
step:951/1770 train_time:91172ms step_avg:96.89ms
step:952/1770 train_time:91274ms step_avg:96.89ms
step:953/1770 train_time:91374ms step_avg:96.90ms
step:954/1770 train_time:91474ms step_avg:96.90ms
step:955/1770 train_time:91575ms step_avg:96.90ms
step:956/1770 train_time:91676ms step_avg:96.91ms
step:957/1770 train_time:91777ms step_avg:96.91ms
step:958/1770 train_time:91877ms step_avg:96.92ms
step:959/1770 train_time:91979ms step_avg:96.92ms
step:960/1770 train_time:92079ms step_avg:96.93ms
step:961/1770 train_time:92179ms step_avg:96.93ms
step:962/1770 train_time:92282ms step_avg:96.93ms
step:963/1770 train_time:92382ms step_avg:96.94ms
step:964/1770 train_time:92484ms step_avg:96.94ms
step:965/1770 train_time:92586ms step_avg:96.95ms
step:966/1770 train_time:92687ms step_avg:96.95ms
step:967/1770 train_time:92787ms step_avg:96.96ms
step:968/1770 train_time:92889ms step_avg:96.96ms
step:969/1770 train_time:92989ms step_avg:96.96ms
step:970/1770 train_time:93090ms step_avg:96.97ms
step:971/1770 train_time:93192ms step_avg:96.97ms
step:972/1770 train_time:93293ms step_avg:96.98ms
step:973/1770 train_time:93394ms step_avg:96.98ms
step:974/1770 train_time:93495ms step_avg:96.99ms
step:975/1770 train_time:93597ms step_avg:96.99ms
step:976/1770 train_time:93698ms step_avg:97.00ms
step:977/1770 train_time:93799ms step_avg:97.00ms
step:978/1770 train_time:93899ms step_avg:97.00ms
step:979/1770 train_time:94000ms step_avg:97.01ms
step:980/1770 train_time:94101ms step_avg:97.01ms
step:981/1770 train_time:94201ms step_avg:97.01ms
step:982/1770 train_time:94303ms step_avg:97.02ms
step:983/1770 train_time:94404ms step_avg:97.02ms
step:984/1770 train_time:94506ms step_avg:97.03ms
step:985/1770 train_time:94607ms step_avg:97.03ms
step:986/1770 train_time:94708ms step_avg:97.04ms
step:987/1770 train_time:94809ms step_avg:97.04ms
step:988/1770 train_time:94909ms step_avg:97.04ms
step:989/1770 train_time:95012ms step_avg:97.05ms
step:990/1770 train_time:95112ms step_avg:97.05ms
step:991/1770 train_time:95211ms step_avg:97.06ms
step:992/1770 train_time:95312ms step_avg:97.06ms
step:993/1770 train_time:95412ms step_avg:97.06ms
step:994/1770 train_time:95513ms step_avg:97.07ms
step:995/1770 train_time:95614ms step_avg:97.07ms
step:996/1770 train_time:95714ms step_avg:97.07ms
step:997/1770 train_time:95814ms step_avg:97.08ms
step:998/1770 train_time:95914ms step_avg:97.08ms
step:999/1770 train_time:96015ms step_avg:97.08ms
step:1000/1770 train_time:96117ms step_avg:97.09ms
step:1000/1770 val_loss:3.5115 train_time:96216ms step_avg:97.19ms
step:1001/1770 train_time:96238ms step_avg:97.11ms
step:1002/1770 train_time:96332ms step_avg:97.11ms
step:1003/1770 train_time:96434ms step_avg:97.11ms
step:1004/1770 train_time:96534ms step_avg:97.12ms
step:1005/1770 train_time:96634ms step_avg:97.12ms
step:1006/1770 train_time:96734ms step_avg:97.12ms
step:1007/1770 train_time:96834ms step_avg:97.13ms
step:1008/1770 train_time:96934ms step_avg:97.13ms
step:1009/1770 train_time:97035ms step_avg:97.13ms
step:1010/1770 train_time:97134ms step_avg:97.13ms
step:1011/1770 train_time:97238ms step_avg:97.14ms
step:1012/1770 train_time:97340ms step_avg:97.15ms
step:1013/1770 train_time:97443ms step_avg:97.15ms
step:1014/1770 train_time:97543ms step_avg:97.15ms
step:1015/1770 train_time:97644ms step_avg:97.16ms
step:1016/1770 train_time:97744ms step_avg:97.16ms
step:1017/1770 train_time:97845ms step_avg:97.16ms
step:1018/1770 train_time:97946ms step_avg:97.17ms
step:1019/1770 train_time:98046ms step_avg:97.17ms
step:1020/1770 train_time:98147ms step_avg:97.17ms
step:1021/1770 train_time:98248ms step_avg:97.18ms
step:1022/1770 train_time:98349ms step_avg:97.18ms
step:1023/1770 train_time:98450ms step_avg:97.19ms
step:1024/1770 train_time:98551ms step_avg:97.19ms
step:1025/1770 train_time:98652ms step_avg:97.19ms
step:1026/1770 train_time:98753ms step_avg:97.20ms
step:1027/1770 train_time:98854ms step_avg:97.20ms
step:1028/1770 train_time:98955ms step_avg:97.20ms
step:1029/1770 train_time:99055ms step_avg:97.21ms
step:1030/1770 train_time:99157ms step_avg:97.21ms
step:1031/1770 train_time:99258ms step_avg:97.22ms
step:1032/1770 train_time:99359ms step_avg:97.22ms
step:1033/1770 train_time:99460ms step_avg:97.22ms
step:1034/1770 train_time:99560ms step_avg:97.23ms
step:1035/1770 train_time:99661ms step_avg:97.23ms
step:1036/1770 train_time:99763ms step_avg:97.23ms
step:1037/1770 train_time:99865ms step_avg:97.24ms
step:1038/1770 train_time:99965ms step_avg:97.24ms
step:1039/1770 train_time:100067ms step_avg:97.25ms
step:1040/1770 train_time:100167ms step_avg:97.25ms
step:1041/1770 train_time:100268ms step_avg:97.25ms
step:1042/1770 train_time:100369ms step_avg:97.26ms
step:1043/1770 train_time:100470ms step_avg:97.26ms
step:1044/1770 train_time:100571ms step_avg:97.26ms
step:1045/1770 train_time:100672ms step_avg:97.27ms
step:1046/1770 train_time:100772ms step_avg:97.27ms
step:1047/1770 train_time:100872ms step_avg:97.27ms
step:1048/1770 train_time:100973ms step_avg:97.28ms
step:1049/1770 train_time:101074ms step_avg:97.28ms
step:1050/1770 train_time:101174ms step_avg:97.28ms
step:1051/1770 train_time:101275ms step_avg:97.29ms
step:1052/1770 train_time:101376ms step_avg:97.29ms
step:1053/1770 train_time:101477ms step_avg:97.29ms
step:1054/1770 train_time:101577ms step_avg:97.30ms
step:1055/1770 train_time:101678ms step_avg:97.30ms
step:1056/1770 train_time:101779ms step_avg:97.30ms
step:1057/1770 train_time:101881ms step_avg:97.31ms
step:1058/1770 train_time:101982ms step_avg:97.31ms
step:1059/1770 train_time:102084ms step_avg:97.32ms
step:1060/1770 train_time:102185ms step_avg:97.32ms
step:1061/1770 train_time:102287ms step_avg:97.32ms
step:1062/1770 train_time:102388ms step_avg:97.33ms
step:1063/1770 train_time:102491ms step_avg:97.33ms
step:1064/1770 train_time:102593ms step_avg:97.34ms
step:1065/1770 train_time:102693ms step_avg:97.34ms
step:1066/1770 train_time:102794ms step_avg:97.34ms
step:1067/1770 train_time:102895ms step_avg:97.35ms
step:1068/1770 train_time:102996ms step_avg:97.35ms
step:1069/1770 train_time:103097ms step_avg:97.35ms
step:1070/1770 train_time:103198ms step_avg:97.36ms
step:1071/1770 train_time:103300ms step_avg:97.36ms
step:1072/1770 train_time:103400ms step_avg:97.36ms
step:1073/1770 train_time:103501ms step_avg:97.37ms
step:1074/1770 train_time:103603ms step_avg:97.37ms
step:1075/1770 train_time:103705ms step_avg:97.38ms
step:1076/1770 train_time:103806ms step_avg:97.38ms
step:1077/1770 train_time:103907ms step_avg:97.38ms
step:1078/1770 train_time:104008ms step_avg:97.39ms
step:1079/1770 train_time:104108ms step_avg:97.39ms
step:1080/1770 train_time:104209ms step_avg:97.39ms
step:1081/1770 train_time:104310ms step_avg:97.40ms
step:1082/1770 train_time:104412ms step_avg:97.40ms
step:1083/1770 train_time:104512ms step_avg:97.40ms
step:1084/1770 train_time:104613ms step_avg:97.41ms
step:1085/1770 train_time:104714ms step_avg:97.41ms
step:1086/1770 train_time:104815ms step_avg:97.41ms
step:1087/1770 train_time:104915ms step_avg:97.41ms
step:1088/1770 train_time:105017ms step_avg:97.42ms
step:1089/1770 train_time:105118ms step_avg:97.42ms
step:1090/1770 train_time:105220ms step_avg:97.43ms
step:1091/1770 train_time:105321ms step_avg:97.43ms
step:1092/1770 train_time:105423ms step_avg:97.43ms
step:1093/1770 train_time:105527ms step_avg:97.44ms
step:1094/1770 train_time:105625ms step_avg:97.44ms
step:1095/1770 train_time:105725ms step_avg:97.44ms
step:1096/1770 train_time:105827ms step_avg:97.45ms
step:1097/1770 train_time:105927ms step_avg:97.45ms
step:1098/1770 train_time:106029ms step_avg:97.45ms
step:1099/1770 train_time:106130ms step_avg:97.46ms
step:1100/1770 train_time:106231ms step_avg:97.46ms
step:1101/1770 train_time:106331ms step_avg:97.46ms
step:1102/1770 train_time:106432ms step_avg:97.47ms
step:1103/1770 train_time:106533ms step_avg:97.47ms
step:1104/1770 train_time:106635ms step_avg:97.47ms
step:1105/1770 train_time:106737ms step_avg:97.48ms
step:1106/1770 train_time:106839ms step_avg:97.48ms
step:1107/1770 train_time:106939ms step_avg:97.48ms
step:1108/1770 train_time:107040ms step_avg:97.49ms
step:1109/1770 train_time:107142ms step_avg:97.49ms
step:1110/1770 train_time:107243ms step_avg:97.49ms
step:1111/1770 train_time:107344ms step_avg:97.50ms
step:1112/1770 train_time:107446ms step_avg:97.50ms
step:1113/1770 train_time:107546ms step_avg:97.50ms
step:1114/1770 train_time:107648ms step_avg:97.51ms
step:1115/1770 train_time:107749ms step_avg:97.51ms
step:1116/1770 train_time:107849ms step_avg:97.51ms
step:1117/1770 train_time:107950ms step_avg:97.52ms
step:1118/1770 train_time:108052ms step_avg:97.52ms
step:1119/1770 train_time:108153ms step_avg:97.52ms
step:1120/1770 train_time:108253ms step_avg:97.52ms
step:1121/1770 train_time:108353ms step_avg:97.53ms
step:1122/1770 train_time:108455ms step_avg:97.53ms
step:1123/1770 train_time:108556ms step_avg:97.53ms
step:1124/1770 train_time:108657ms step_avg:97.54ms
step:1125/1770 train_time:108758ms step_avg:97.54ms
step:1125/1770 val_loss:3.4718 train_time:108858ms step_avg:97.63ms
step:1126/1770 train_time:108880ms step_avg:97.56ms
step:1127/1770 train_time:108973ms step_avg:97.56ms
step:1128/1770 train_time:109075ms step_avg:97.56ms
step:1129/1770 train_time:109175ms step_avg:97.56ms
step:1130/1770 train_time:109276ms step_avg:97.57ms
step:1131/1770 train_time:109377ms step_avg:97.57ms
step:1132/1770 train_time:109478ms step_avg:97.57ms
step:1133/1770 train_time:109577ms step_avg:97.58ms
step:1134/1770 train_time:109678ms step_avg:97.58ms
step:1135/1770 train_time:109778ms step_avg:97.58ms
step:1136/1770 train_time:109881ms step_avg:97.59ms
step:1137/1770 train_time:109984ms step_avg:97.59ms
step:1138/1770 train_time:110085ms step_avg:97.59ms
step:1139/1770 train_time:110186ms step_avg:97.60ms
step:1140/1770 train_time:110287ms step_avg:97.60ms
step:1141/1770 train_time:110388ms step_avg:97.60ms
step:1142/1770 train_time:110488ms step_avg:97.60ms
step:1143/1770 train_time:110589ms step_avg:97.61ms
step:1144/1770 train_time:110694ms step_avg:97.61ms
step:1145/1770 train_time:110790ms step_avg:97.61ms
step:1146/1770 train_time:110892ms step_avg:97.62ms
step:1147/1770 train_time:110993ms step_avg:97.62ms
step:1148/1770 train_time:111095ms step_avg:97.62ms
step:1149/1770 train_time:111196ms step_avg:97.63ms
step:1150/1770 train_time:111297ms step_avg:97.63ms
step:1151/1770 train_time:111399ms step_avg:97.63ms
step:1152/1770 train_time:111501ms step_avg:97.64ms
step:1153/1770 train_time:111602ms step_avg:97.64ms
step:1154/1770 train_time:111703ms step_avg:97.64ms
step:1155/1770 train_time:111803ms step_avg:97.64ms
step:1156/1770 train_time:111903ms step_avg:97.65ms
step:1157/1770 train_time:112006ms step_avg:97.65ms
step:1158/1770 train_time:112107ms step_avg:97.65ms
step:1159/1770 train_time:112207ms step_avg:97.66ms
step:1160/1770 train_time:112308ms step_avg:97.66ms
step:1161/1770 train_time:112410ms step_avg:97.66ms
step:1162/1770 train_time:112512ms step_avg:97.67ms
step:1163/1770 train_time:112613ms step_avg:97.67ms
step:1164/1770 train_time:112714ms step_avg:97.67ms
step:1165/1770 train_time:112814ms step_avg:97.67ms
step:1166/1770 train_time:112916ms step_avg:97.68ms
step:1167/1770 train_time:113017ms step_avg:97.68ms
step:1168/1770 train_time:113118ms step_avg:97.68ms
step:1169/1770 train_time:113219ms step_avg:97.69ms
step:1170/1770 train_time:113320ms step_avg:97.69ms
step:1171/1770 train_time:113422ms step_avg:97.69ms
step:1172/1770 train_time:113523ms step_avg:97.70ms
step:1173/1770 train_time:113624ms step_avg:97.70ms
step:1174/1770 train_time:113725ms step_avg:97.70ms
step:1175/1770 train_time:113826ms step_avg:97.70ms
step:1176/1770 train_time:113927ms step_avg:97.71ms
step:1177/1770 train_time:114028ms step_avg:97.71ms
step:1178/1770 train_time:114129ms step_avg:97.71ms
step:1179/1770 train_time:114230ms step_avg:97.72ms
step:1180/1770 train_time:114332ms step_avg:97.72ms
step:1181/1770 train_time:114434ms step_avg:97.72ms
step:1182/1770 train_time:114535ms step_avg:97.73ms
step:1183/1770 train_time:114637ms step_avg:97.73ms
step:1184/1770 train_time:114740ms step_avg:97.73ms
step:1185/1770 train_time:114842ms step_avg:97.74ms
step:1186/1770 train_time:114943ms step_avg:97.74ms
step:1187/1770 train_time:115047ms step_avg:97.75ms
step:1188/1770 train_time:115149ms step_avg:97.75ms
step:1189/1770 train_time:115250ms step_avg:97.75ms
step:1190/1770 train_time:115351ms step_avg:97.76ms
step:1191/1770 train_time:115453ms step_avg:97.76ms
step:1192/1770 train_time:115557ms step_avg:97.76ms
step:1193/1770 train_time:115659ms step_avg:97.77ms
step:1194/1770 train_time:115762ms step_avg:97.77ms
step:1195/1770 train_time:115864ms step_avg:97.78ms
step:1196/1770 train_time:115967ms step_avg:97.78ms
step:1197/1770 train_time:116068ms step_avg:97.78ms
step:1198/1770 train_time:116170ms step_avg:97.79ms
step:1199/1770 train_time:116272ms step_avg:97.79ms
step:1200/1770 train_time:116375ms step_avg:97.79ms
step:1201/1770 train_time:116478ms step_avg:97.80ms
step:1202/1770 train_time:116579ms step_avg:97.80ms
step:1203/1770 train_time:116682ms step_avg:97.81ms
step:1204/1770 train_time:116784ms step_avg:97.81ms
step:1205/1770 train_time:116886ms step_avg:97.81ms
step:1206/1770 train_time:116988ms step_avg:97.82ms
step:1207/1770 train_time:117089ms step_avg:97.82ms
step:1208/1770 train_time:117191ms step_avg:97.82ms
step:1209/1770 train_time:117293ms step_avg:97.83ms
step:1210/1770 train_time:117394ms step_avg:97.83ms
step:1211/1770 train_time:117496ms step_avg:97.83ms
step:1212/1770 train_time:117603ms step_avg:97.84ms
step:1213/1770 train_time:117703ms step_avg:97.84ms
step:1214/1770 train_time:117804ms step_avg:97.84ms
step:1215/1770 train_time:117906ms step_avg:97.85ms
step:1216/1770 train_time:118009ms step_avg:97.85ms
step:1217/1770 train_time:118111ms step_avg:97.85ms
step:1218/1770 train_time:118212ms step_avg:97.86ms
step:1219/1770 train_time:118314ms step_avg:97.86ms
step:1220/1770 train_time:118416ms step_avg:97.86ms
step:1221/1770 train_time:118518ms step_avg:97.87ms
step:1222/1770 train_time:118622ms step_avg:97.87ms
step:1223/1770 train_time:118723ms step_avg:97.88ms
step:1224/1770 train_time:118825ms step_avg:97.88ms
step:1225/1770 train_time:118928ms step_avg:97.88ms
step:1226/1770 train_time:119030ms step_avg:97.89ms
step:1227/1770 train_time:119134ms step_avg:97.89ms
step:1228/1770 train_time:119238ms step_avg:97.90ms
step:1229/1770 train_time:119340ms step_avg:97.90ms
step:1230/1770 train_time:119442ms step_avg:97.90ms
step:1231/1770 train_time:119544ms step_avg:97.91ms
step:1232/1770 train_time:119646ms step_avg:97.91ms
step:1233/1770 train_time:119748ms step_avg:97.91ms
step:1234/1770 train_time:119849ms step_avg:97.92ms
step:1235/1770 train_time:119951ms step_avg:97.92ms
step:1236/1770 train_time:120053ms step_avg:97.92ms
step:1237/1770 train_time:120156ms step_avg:97.93ms
step:1238/1770 train_time:120259ms step_avg:97.93ms
step:1239/1770 train_time:120360ms step_avg:97.93ms
step:1240/1770 train_time:120462ms step_avg:97.94ms
step:1241/1770 train_time:120564ms step_avg:97.94ms
step:1242/1770 train_time:120666ms step_avg:97.94ms
step:1243/1770 train_time:120769ms step_avg:97.95ms
step:1244/1770 train_time:120870ms step_avg:97.95ms
step:1245/1770 train_time:120972ms step_avg:97.95ms
step:1246/1770 train_time:121074ms step_avg:97.96ms
step:1247/1770 train_time:121176ms step_avg:97.96ms
step:1248/1770 train_time:121279ms step_avg:97.96ms
step:1249/1770 train_time:121380ms step_avg:97.97ms
step:1250/1770 train_time:121481ms step_avg:97.97ms
step:1250/1770 val_loss:3.4238 train_time:121585ms step_avg:98.05ms
step:1251/1770 train_time:121606ms step_avg:97.99ms
step:1252/1770 train_time:121699ms step_avg:97.99ms
step:1253/1770 train_time:121803ms step_avg:97.99ms
step:1254/1770 train_time:121905ms step_avg:97.99ms
step:1255/1770 train_time:122008ms step_avg:98.00ms
step:1256/1770 train_time:122109ms step_avg:98.00ms
step:1257/1770 train_time:122211ms step_avg:98.00ms
step:1258/1770 train_time:122314ms step_avg:98.01ms
step:1259/1770 train_time:122416ms step_avg:98.01ms
step:1260/1770 train_time:122517ms step_avg:98.01ms
step:1261/1770 train_time:122620ms step_avg:98.02ms
step:1262/1770 train_time:122724ms step_avg:98.02ms
step:1263/1770 train_time:122825ms step_avg:98.02ms
step:1264/1770 train_time:122928ms step_avg:98.03ms
step:1265/1770 train_time:123030ms step_avg:98.03ms
step:1266/1770 train_time:123132ms step_avg:98.03ms
step:1267/1770 train_time:123234ms step_avg:98.04ms
step:1268/1770 train_time:123336ms step_avg:98.04ms
step:1269/1770 train_time:123438ms step_avg:98.04ms
step:1270/1770 train_time:123540ms step_avg:98.05ms
step:1271/1770 train_time:123643ms step_avg:98.05ms
step:1272/1770 train_time:123744ms step_avg:98.05ms
step:1273/1770 train_time:123847ms step_avg:98.06ms
step:1274/1770 train_time:123949ms step_avg:98.06ms
step:1275/1770 train_time:124051ms step_avg:98.06ms
step:1276/1770 train_time:124152ms step_avg:98.07ms
step:1277/1770 train_time:124254ms step_avg:98.07ms
step:1278/1770 train_time:124358ms step_avg:98.07ms
step:1279/1770 train_time:124461ms step_avg:98.08ms
step:1280/1770 train_time:124566ms step_avg:98.08ms
step:1281/1770 train_time:124666ms step_avg:98.09ms
step:1282/1770 train_time:124769ms step_avg:98.09ms
step:1283/1770 train_time:124871ms step_avg:98.09ms
step:1284/1770 train_time:124974ms step_avg:98.10ms
step:1285/1770 train_time:125076ms step_avg:98.10ms
step:1286/1770 train_time:125180ms step_avg:98.10ms
step:1287/1770 train_time:125283ms step_avg:98.11ms
step:1288/1770 train_time:125385ms step_avg:98.11ms
step:1289/1770 train_time:125488ms step_avg:98.11ms
step:1290/1770 train_time:125589ms step_avg:98.12ms
step:1291/1770 train_time:125691ms step_avg:98.12ms
step:1292/1770 train_time:125793ms step_avg:98.12ms
step:1293/1770 train_time:125896ms step_avg:98.13ms
step:1294/1770 train_time:125998ms step_avg:98.13ms
step:1295/1770 train_time:126099ms step_avg:98.13ms
step:1296/1770 train_time:126201ms step_avg:98.13ms
step:1297/1770 train_time:126304ms step_avg:98.14ms
step:1298/1770 train_time:126405ms step_avg:98.14ms
step:1299/1770 train_time:126508ms step_avg:98.14ms
step:1300/1770 train_time:126610ms step_avg:98.15ms
step:1301/1770 train_time:126714ms step_avg:98.15ms
step:1302/1770 train_time:126815ms step_avg:98.15ms
step:1303/1770 train_time:126916ms step_avg:98.16ms
step:1304/1770 train_time:127019ms step_avg:98.16ms
step:1305/1770 train_time:127120ms step_avg:98.16ms
step:1306/1770 train_time:127222ms step_avg:98.17ms
step:1307/1770 train_time:127324ms step_avg:98.17ms
step:1308/1770 train_time:127426ms step_avg:98.17ms
step:1309/1770 train_time:127527ms step_avg:98.17ms
step:1310/1770 train_time:127630ms step_avg:98.18ms
step:1311/1770 train_time:127732ms step_avg:98.18ms
step:1312/1770 train_time:127834ms step_avg:98.18ms
step:1313/1770 train_time:127935ms step_avg:98.19ms
step:1314/1770 train_time:128038ms step_avg:98.19ms
step:1315/1770 train_time:128139ms step_avg:98.19ms
step:1316/1770 train_time:128241ms step_avg:98.19ms
step:1317/1770 train_time:128343ms step_avg:98.20ms
step:1318/1770 train_time:128448ms step_avg:98.20ms
step:1319/1770 train_time:128550ms step_avg:98.20ms
step:1320/1770 train_time:128652ms step_avg:98.21ms
step:1321/1770 train_time:128754ms step_avg:98.21ms
step:1322/1770 train_time:128856ms step_avg:98.21ms
step:1323/1770 train_time:128959ms step_avg:98.22ms
step:1324/1770 train_time:129061ms step_avg:98.22ms
step:1325/1770 train_time:129165ms step_avg:98.22ms
step:1326/1770 train_time:129268ms step_avg:98.23ms
step:1327/1770 train_time:129371ms step_avg:98.23ms
step:1328/1770 train_time:129473ms step_avg:98.23ms
step:1329/1770 train_time:129575ms step_avg:98.24ms
step:1330/1770 train_time:129677ms step_avg:98.24ms
step:1331/1770 train_time:129779ms step_avg:98.24ms
step:1332/1770 train_time:129880ms step_avg:98.25ms
step:1333/1770 train_time:129983ms step_avg:98.25ms
step:1334/1770 train_time:130085ms step_avg:98.25ms
step:1335/1770 train_time:130186ms step_avg:98.25ms
step:1336/1770 train_time:130287ms step_avg:98.26ms
step:1337/1770 train_time:130389ms step_avg:98.26ms
step:1338/1770 train_time:130491ms step_avg:98.26ms
step:1339/1770 train_time:130594ms step_avg:98.26ms
step:1340/1770 train_time:130698ms step_avg:98.27ms
step:1341/1770 train_time:130799ms step_avg:98.27ms
step:1342/1770 train_time:130903ms step_avg:98.28ms
step:1343/1770 train_time:131006ms step_avg:98.28ms
step:1344/1770 train_time:131109ms step_avg:98.28ms
step:1345/1770 train_time:131211ms step_avg:98.29ms
step:1346/1770 train_time:131313ms step_avg:98.29ms
step:1347/1770 train_time:131415ms step_avg:98.29ms
step:1348/1770 train_time:131520ms step_avg:98.30ms
step:1349/1770 train_time:131621ms step_avg:98.30ms
step:1350/1770 train_time:131724ms step_avg:98.30ms
step:1351/1770 train_time:131827ms step_avg:98.30ms
step:1352/1770 train_time:131928ms step_avg:98.31ms
step:1353/1770 train_time:132032ms step_avg:98.31ms
step:1354/1770 train_time:132133ms step_avg:98.31ms
step:1355/1770 train_time:132235ms step_avg:98.32ms
step:1356/1770 train_time:132337ms step_avg:98.32ms
step:1357/1770 train_time:132439ms step_avg:98.32ms
step:1358/1770 train_time:132542ms step_avg:98.33ms
step:1359/1770 train_time:132644ms step_avg:98.33ms
step:1360/1770 train_time:132747ms step_avg:98.33ms
step:1361/1770 train_time:132850ms step_avg:98.33ms
step:1362/1770 train_time:132952ms step_avg:98.34ms
step:1363/1770 train_time:133055ms step_avg:98.34ms
step:1364/1770 train_time:133161ms step_avg:98.35ms
step:1365/1770 train_time:133260ms step_avg:98.35ms
step:1366/1770 train_time:133361ms step_avg:98.35ms
step:1367/1770 train_time:133464ms step_avg:98.35ms
step:1368/1770 train_time:133565ms step_avg:98.35ms
step:1369/1770 train_time:133668ms step_avg:98.36ms
step:1370/1770 train_time:133771ms step_avg:98.36ms
step:1371/1770 train_time:133873ms step_avg:98.36ms
step:1372/1770 train_time:133975ms step_avg:98.37ms
step:1373/1770 train_time:134077ms step_avg:98.37ms
step:1374/1770 train_time:134179ms step_avg:98.37ms
step:1375/1770 train_time:134282ms step_avg:98.37ms
step:1375/1770 val_loss:3.3795 train_time:134383ms step_avg:98.45ms
step:1376/1770 train_time:134405ms step_avg:98.39ms
step:1377/1770 train_time:134499ms step_avg:98.39ms
step:1378/1770 train_time:134601ms step_avg:98.39ms
step:1379/1770 train_time:134703ms step_avg:98.39ms
step:1380/1770 train_time:134805ms step_avg:98.40ms
step:1381/1770 train_time:134907ms step_avg:98.40ms
step:1382/1770 train_time:135008ms step_avg:98.40ms
step:1383/1770 train_time:135110ms step_avg:98.40ms
step:1384/1770 train_time:135212ms step_avg:98.41ms
step:1385/1770 train_time:135314ms step_avg:98.41ms
step:1386/1770 train_time:135418ms step_avg:98.41ms
step:1387/1770 train_time:135522ms step_avg:98.42ms
step:1388/1770 train_time:135624ms step_avg:98.42ms
step:1389/1770 train_time:135726ms step_avg:98.42ms
step:1390/1770 train_time:135828ms step_avg:98.43ms
step:1391/1770 train_time:135930ms step_avg:98.43ms
step:1392/1770 train_time:136033ms step_avg:98.43ms
step:1393/1770 train_time:136133ms step_avg:98.43ms
step:1394/1770 train_time:136234ms step_avg:98.44ms
step:1395/1770 train_time:136337ms step_avg:98.44ms
step:1396/1770 train_time:136440ms step_avg:98.44ms
step:1397/1770 train_time:136543ms step_avg:98.45ms
step:1398/1770 train_time:136646ms step_avg:98.45ms
step:1399/1770 train_time:136747ms step_avg:98.45ms
step:1400/1770 train_time:136850ms step_avg:98.45ms
step:1401/1770 train_time:136952ms step_avg:98.46ms
step:1402/1770 train_time:137054ms step_avg:98.46ms
step:1403/1770 train_time:137156ms step_avg:98.46ms
step:1404/1770 train_time:137258ms step_avg:98.46ms
step:1405/1770 train_time:137359ms step_avg:98.47ms
step:1406/1770 train_time:137462ms step_avg:98.47ms
step:1407/1770 train_time:137564ms step_avg:98.47ms
step:1408/1770 train_time:137667ms step_avg:98.47ms
step:1409/1770 train_time:137770ms step_avg:98.48ms
step:1410/1770 train_time:137872ms step_avg:98.48ms
step:1411/1770 train_time:137974ms step_avg:98.48ms
step:1412/1770 train_time:138076ms step_avg:98.48ms
step:1413/1770 train_time:138177ms step_avg:98.49ms
step:1414/1770 train_time:138280ms step_avg:98.49ms
step:1415/1770 train_time:138383ms step_avg:98.49ms
step:1416/1770 train_time:138486ms step_avg:98.50ms
step:1417/1770 train_time:138588ms step_avg:98.50ms
step:1418/1770 train_time:138690ms step_avg:98.50ms
step:1419/1770 train_time:138793ms step_avg:98.50ms
step:1420/1770 train_time:138894ms step_avg:98.51ms
step:1421/1770 train_time:138997ms step_avg:98.51ms
step:1422/1770 train_time:139099ms step_avg:98.51ms
step:1423/1770 train_time:139201ms step_avg:98.51ms
step:1424/1770 train_time:139305ms step_avg:98.52ms
step:1425/1770 train_time:139407ms step_avg:98.52ms
step:1426/1770 train_time:139513ms step_avg:98.53ms
step:1427/1770 train_time:139612ms step_avg:98.53ms
step:1428/1770 train_time:139716ms step_avg:98.53ms
step:1429/1770 train_time:139818ms step_avg:98.53ms
step:1430/1770 train_time:139920ms step_avg:98.54ms
step:1431/1770 train_time:140023ms step_avg:98.54ms
step:1432/1770 train_time:140124ms step_avg:98.54ms
step:1433/1770 train_time:140225ms step_avg:98.54ms
step:1434/1770 train_time:140327ms step_avg:98.54ms
step:1435/1770 train_time:140429ms step_avg:98.55ms
step:1436/1770 train_time:140533ms step_avg:98.55ms
step:1437/1770 train_time:140636ms step_avg:98.55ms
step:1438/1770 train_time:140737ms step_avg:98.56ms
step:1439/1770 train_time:140839ms step_avg:98.56ms
step:1440/1770 train_time:140941ms step_avg:98.56ms
step:1441/1770 train_time:141047ms step_avg:98.57ms
step:1442/1770 train_time:141148ms step_avg:98.57ms
step:1443/1770 train_time:141250ms step_avg:98.57ms
step:1444/1770 train_time:141352ms step_avg:98.57ms
step:1445/1770 train_time:141455ms step_avg:98.57ms
step:1446/1770 train_time:141558ms step_avg:98.58ms
step:1447/1770 train_time:141662ms step_avg:98.58ms
step:1448/1770 train_time:141765ms step_avg:98.59ms
step:1449/1770 train_time:141869ms step_avg:98.59ms
step:1450/1770 train_time:141972ms step_avg:98.59ms
step:1451/1770 train_time:142075ms step_avg:98.59ms
step:1452/1770 train_time:142180ms step_avg:98.60ms
step:1453/1770 train_time:142284ms step_avg:98.60ms
step:1454/1770 train_time:142386ms step_avg:98.61ms
step:1455/1770 train_time:142490ms step_avg:98.61ms
step:1456/1770 train_time:142596ms step_avg:98.61ms
step:1457/1770 train_time:142699ms step_avg:98.62ms
step:1458/1770 train_time:142803ms step_avg:98.62ms
step:1459/1770 train_time:142907ms step_avg:98.62ms
step:1460/1770 train_time:143010ms step_avg:98.63ms
step:1461/1770 train_time:143115ms step_avg:98.63ms
step:1462/1770 train_time:143218ms step_avg:98.63ms
step:1463/1770 train_time:143326ms step_avg:98.64ms
step:1464/1770 train_time:143427ms step_avg:98.64ms
step:1465/1770 train_time:143530ms step_avg:98.65ms
step:1466/1770 train_time:143634ms step_avg:98.65ms
step:1467/1770 train_time:143738ms step_avg:98.65ms
step:1468/1770 train_time:143841ms step_avg:98.66ms
step:1469/1770 train_time:143944ms step_avg:98.66ms
step:1470/1770 train_time:144046ms step_avg:98.66ms
step:1471/1770 train_time:144150ms step_avg:98.67ms
step:1472/1770 train_time:144254ms step_avg:98.67ms
step:1473/1770 train_time:144357ms step_avg:98.67ms
step:1474/1770 train_time:144462ms step_avg:98.68ms
step:1475/1770 train_time:144565ms step_avg:98.68ms
step:1476/1770 train_time:144667ms step_avg:98.68ms
step:1477/1770 train_time:144772ms step_avg:98.69ms
step:1478/1770 train_time:144876ms step_avg:98.69ms
step:1479/1770 train_time:144979ms step_avg:98.69ms
step:1480/1770 train_time:145083ms step_avg:98.70ms
step:1481/1770 train_time:145190ms step_avg:98.70ms
step:1482/1770 train_time:145292ms step_avg:98.70ms
step:1483/1770 train_time:145396ms step_avg:98.71ms
step:1484/1770 train_time:145498ms step_avg:98.71ms
step:1485/1770 train_time:145601ms step_avg:98.71ms
step:1486/1770 train_time:145704ms step_avg:98.72ms
step:1487/1770 train_time:145808ms step_avg:98.72ms
step:1488/1770 train_time:145911ms step_avg:98.72ms
step:1489/1770 train_time:146015ms step_avg:98.73ms
step:1490/1770 train_time:146119ms step_avg:98.73ms
step:1491/1770 train_time:146222ms step_avg:98.73ms
step:1492/1770 train_time:146326ms step_avg:98.74ms
step:1493/1770 train_time:146432ms step_avg:98.74ms
step:1494/1770 train_time:146539ms step_avg:98.75ms
step:1495/1770 train_time:146641ms step_avg:98.75ms
step:1496/1770 train_time:146745ms step_avg:98.75ms
step:1497/1770 train_time:146848ms step_avg:98.75ms
step:1498/1770 train_time:146950ms step_avg:98.76ms
step:1499/1770 train_time:147053ms step_avg:98.76ms
step:1500/1770 train_time:147155ms step_avg:98.76ms
step:1500/1770 val_loss:3.3414 train_time:147257ms step_avg:98.83ms
step:1501/1770 train_time:147279ms step_avg:98.78ms
step:1502/1770 train_time:147372ms step_avg:98.77ms
step:1503/1770 train_time:147479ms step_avg:98.78ms
step:1504/1770 train_time:147578ms step_avg:98.78ms
step:1505/1770 train_time:147683ms step_avg:98.78ms
step:1506/1770 train_time:147786ms step_avg:98.79ms
step:1507/1770 train_time:147890ms step_avg:98.79ms
step:1508/1770 train_time:147995ms step_avg:98.79ms
step:1509/1770 train_time:148097ms step_avg:98.80ms
step:1510/1770 train_time:148200ms step_avg:98.80ms
step:1511/1770 train_time:148304ms step_avg:98.80ms
step:1512/1770 train_time:148408ms step_avg:98.81ms
step:1513/1770 train_time:148513ms step_avg:98.81ms
step:1514/1770 train_time:148617ms step_avg:98.81ms
step:1515/1770 train_time:148720ms step_avg:98.82ms
step:1516/1770 train_time:148824ms step_avg:98.82ms
step:1517/1770 train_time:148927ms step_avg:98.82ms
step:1518/1770 train_time:149032ms step_avg:98.83ms
step:1519/1770 train_time:149135ms step_avg:98.83ms
step:1520/1770 train_time:149238ms step_avg:98.83ms
step:1521/1770 train_time:149341ms step_avg:98.84ms
step:1522/1770 train_time:149445ms step_avg:98.84ms
step:1523/1770 train_time:149549ms step_avg:98.84ms
step:1524/1770 train_time:149652ms step_avg:98.85ms
step:1525/1770 train_time:149755ms step_avg:98.85ms
step:1526/1770 train_time:149857ms step_avg:98.85ms
step:1527/1770 train_time:149961ms step_avg:98.85ms
step:1528/1770 train_time:150067ms step_avg:98.86ms
step:1529/1770 train_time:150170ms step_avg:98.86ms
step:1530/1770 train_time:150277ms step_avg:98.87ms
step:1531/1770 train_time:150375ms step_avg:98.87ms
step:1532/1770 train_time:150479ms step_avg:98.87ms
step:1533/1770 train_time:150583ms step_avg:98.87ms
step:1534/1770 train_time:150686ms step_avg:98.88ms
step:1535/1770 train_time:150789ms step_avg:98.88ms
step:1536/1770 train_time:150892ms step_avg:98.88ms
step:1537/1770 train_time:150996ms step_avg:98.88ms
step:1538/1770 train_time:151101ms step_avg:98.89ms
step:1539/1770 train_time:151204ms step_avg:98.89ms
step:1540/1770 train_time:151310ms step_avg:98.90ms
step:1541/1770 train_time:151414ms step_avg:98.90ms
step:1542/1770 train_time:151517ms step_avg:98.90ms
step:1543/1770 train_time:151620ms step_avg:98.90ms
step:1544/1770 train_time:151726ms step_avg:98.91ms
step:1545/1770 train_time:151828ms step_avg:98.91ms
step:1546/1770 train_time:151932ms step_avg:98.91ms
step:1547/1770 train_time:152036ms step_avg:98.92ms
step:1548/1770 train_time:152139ms step_avg:98.92ms
step:1549/1770 train_time:152242ms step_avg:98.92ms
step:1550/1770 train_time:152345ms step_avg:98.93ms
step:1551/1770 train_time:152449ms step_avg:98.93ms
step:1552/1770 train_time:152553ms step_avg:98.93ms
step:1553/1770 train_time:152657ms step_avg:98.94ms
step:1554/1770 train_time:152761ms step_avg:98.94ms
step:1555/1770 train_time:152865ms step_avg:98.94ms
step:1556/1770 train_time:152967ms step_avg:98.94ms
step:1557/1770 train_time:153070ms step_avg:98.95ms
step:1558/1770 train_time:153174ms step_avg:98.95ms
step:1559/1770 train_time:153277ms step_avg:98.95ms
step:1560/1770 train_time:153379ms step_avg:98.95ms
step:1561/1770 train_time:153485ms step_avg:98.96ms
step:1562/1770 train_time:153589ms step_avg:98.96ms
step:1563/1770 train_time:153692ms step_avg:98.96ms
step:1564/1770 train_time:153795ms step_avg:98.97ms
step:1565/1770 train_time:153898ms step_avg:98.97ms
step:1566/1770 train_time:154001ms step_avg:98.97ms
step:1567/1770 train_time:154104ms step_avg:98.98ms
step:1568/1770 train_time:154207ms step_avg:98.98ms
step:1569/1770 train_time:154314ms step_avg:98.98ms
step:1570/1770 train_time:154416ms step_avg:98.98ms
step:1571/1770 train_time:154519ms step_avg:98.99ms
step:1572/1770 train_time:154624ms step_avg:98.99ms
step:1573/1770 train_time:154729ms step_avg:99.00ms
step:1574/1770 train_time:154833ms step_avg:99.00ms
step:1575/1770 train_time:154935ms step_avg:99.00ms
step:1576/1770 train_time:155038ms step_avg:99.00ms
step:1577/1770 train_time:155143ms step_avg:99.01ms
step:1578/1770 train_time:155247ms step_avg:99.01ms
step:1579/1770 train_time:155352ms step_avg:99.01ms
step:1580/1770 train_time:155455ms step_avg:99.02ms
step:1581/1770 train_time:155561ms step_avg:99.02ms
step:1582/1770 train_time:155666ms step_avg:99.02ms
step:1583/1770 train_time:155769ms step_avg:99.03ms
step:1584/1770 train_time:155874ms step_avg:99.03ms
step:1585/1770 train_time:155980ms step_avg:99.04ms
step:1586/1770 train_time:156084ms step_avg:99.04ms
step:1587/1770 train_time:156187ms step_avg:99.04ms
step:1588/1770 train_time:156291ms step_avg:99.04ms
step:1589/1770 train_time:156396ms step_avg:99.05ms
step:1590/1770 train_time:156499ms step_avg:99.05ms
step:1591/1770 train_time:156601ms step_avg:99.05ms
step:1592/1770 train_time:156706ms step_avg:99.06ms
step:1593/1770 train_time:156809ms step_avg:99.06ms
step:1594/1770 train_time:156912ms step_avg:99.06ms
step:1595/1770 train_time:157016ms step_avg:99.06ms
step:1596/1770 train_time:157120ms step_avg:99.07ms
step:1597/1770 train_time:157223ms step_avg:99.07ms
step:1598/1770 train_time:157326ms step_avg:99.07ms
step:1599/1770 train_time:157431ms step_avg:99.08ms
step:1600/1770 train_time:157536ms step_avg:99.08ms
step:1601/1770 train_time:157640ms step_avg:99.08ms
step:1602/1770 train_time:157745ms step_avg:99.09ms
step:1603/1770 train_time:157847ms step_avg:99.09ms
step:1604/1770 train_time:157950ms step_avg:99.09ms
step:1605/1770 train_time:158053ms step_avg:99.09ms
step:1606/1770 train_time:158156ms step_avg:99.10ms
step:1607/1770 train_time:158263ms step_avg:99.10ms
step:1608/1770 train_time:158366ms step_avg:99.10ms
step:1609/1770 train_time:158469ms step_avg:99.11ms
step:1610/1770 train_time:158574ms step_avg:99.11ms
step:1611/1770 train_time:158679ms step_avg:99.11ms
step:1612/1770 train_time:158783ms step_avg:99.12ms
step:1613/1770 train_time:158886ms step_avg:99.12ms
step:1614/1770 train_time:158989ms step_avg:99.12ms
step:1615/1770 train_time:159092ms step_avg:99.12ms
step:1616/1770 train_time:159197ms step_avg:99.13ms
step:1617/1770 train_time:159302ms step_avg:99.13ms
step:1618/1770 train_time:159406ms step_avg:99.13ms
step:1619/1770 train_time:159509ms step_avg:99.14ms
step:1620/1770 train_time:159613ms step_avg:99.14ms
step:1621/1770 train_time:159716ms step_avg:99.14ms
step:1622/1770 train_time:159821ms step_avg:99.14ms
step:1623/1770 train_time:159928ms step_avg:99.15ms
step:1624/1770 train_time:160031ms step_avg:99.15ms
step:1625/1770 train_time:160133ms step_avg:99.15ms
step:1625/1770 val_loss:3.3069 train_time:160235ms step_avg:99.22ms
step:1626/1770 train_time:160257ms step_avg:99.17ms
step:1627/1770 train_time:160348ms step_avg:99.16ms
step:1628/1770 train_time:160451ms step_avg:99.17ms
step:1629/1770 train_time:160553ms step_avg:99.17ms
step:1630/1770 train_time:160656ms step_avg:99.17ms
step:1631/1770 train_time:160759ms step_avg:99.17ms
step:1632/1770 train_time:160861ms step_avg:99.17ms
step:1633/1770 train_time:160965ms step_avg:99.18ms
step:1634/1770 train_time:161068ms step_avg:99.18ms
step:1635/1770 train_time:161171ms step_avg:99.18ms
step:1636/1770 train_time:161275ms step_avg:99.19ms
step:1637/1770 train_time:161380ms step_avg:99.19ms
step:1638/1770 train_time:161483ms step_avg:99.19ms
step:1639/1770 train_time:161587ms step_avg:99.19ms
step:1640/1770 train_time:161691ms step_avg:99.20ms
step:1641/1770 train_time:161794ms step_avg:99.20ms
step:1642/1770 train_time:161896ms step_avg:99.20ms
step:1643/1770 train_time:161999ms step_avg:99.20ms
step:1644/1770 train_time:162104ms step_avg:99.21ms
step:1645/1770 train_time:162207ms step_avg:99.21ms
step:1646/1770 train_time:162313ms step_avg:99.21ms
step:1647/1770 train_time:162417ms step_avg:99.22ms
step:1648/1770 train_time:162520ms step_avg:99.22ms
step:1649/1770 train_time:162625ms step_avg:99.22ms
step:1650/1770 train_time:162728ms step_avg:99.22ms
step:1651/1770 train_time:162830ms step_avg:99.23ms
step:1652/1770 train_time:162933ms step_avg:99.23ms
step:1653/1770 train_time:163037ms step_avg:99.23ms
step:1654/1770 train_time:163143ms step_avg:99.24ms
step:1655/1770 train_time:163249ms step_avg:99.24ms
step:1656/1770 train_time:163352ms step_avg:99.24ms
step:1657/1770 train_time:163457ms step_avg:99.25ms
step:1658/1770 train_time:163561ms step_avg:99.25ms
step:1659/1770 train_time:163666ms step_avg:99.25ms
step:1660/1770 train_time:163769ms step_avg:99.25ms
step:1661/1770 train_time:163872ms step_avg:99.26ms
step:1662/1770 train_time:163976ms step_avg:99.26ms
step:1663/1770 train_time:164078ms step_avg:99.26ms
step:1664/1770 train_time:164181ms step_avg:99.26ms
step:1665/1770 train_time:164284ms step_avg:99.27ms
step:1666/1770 train_time:164388ms step_avg:99.27ms
step:1667/1770 train_time:164492ms step_avg:99.27ms
step:1668/1770 train_time:164595ms step_avg:99.27ms
step:1669/1770 train_time:164697ms step_avg:99.27ms
step:1670/1770 train_time:164801ms step_avg:99.28ms
step:1671/1770 train_time:164905ms step_avg:99.28ms
step:1672/1770 train_time:165009ms step_avg:99.28ms
step:1673/1770 train_time:165113ms step_avg:99.29ms
step:1674/1770 train_time:165216ms step_avg:99.29ms
step:1675/1770 train_time:165318ms step_avg:99.29ms
step:1676/1770 train_time:165427ms step_avg:99.30ms
step:1677/1770 train_time:165530ms step_avg:99.30ms
step:1678/1770 train_time:165632ms step_avg:99.30ms
step:1679/1770 train_time:165736ms step_avg:99.30ms
step:1680/1770 train_time:165841ms step_avg:99.31ms
step:1681/1770 train_time:165945ms step_avg:99.31ms
step:1682/1770 train_time:166049ms step_avg:99.31ms
step:1683/1770 train_time:166152ms step_avg:99.31ms
step:1684/1770 train_time:166255ms step_avg:99.32ms
step:1685/1770 train_time:166359ms step_avg:99.32ms
step:1686/1770 train_time:166463ms step_avg:99.32ms
step:1687/1770 train_time:166568ms step_avg:99.33ms
step:1688/1770 train_time:166672ms step_avg:99.33ms
step:1689/1770 train_time:166776ms step_avg:99.33ms
step:1690/1770 train_time:166878ms step_avg:99.33ms
step:1691/1770 train_time:166982ms step_avg:99.33ms
step:1692/1770 train_time:167084ms step_avg:99.34ms
step:1693/1770 train_time:167188ms step_avg:99.34ms
step:1694/1770 train_time:167292ms step_avg:99.34ms
step:1695/1770 train_time:167397ms step_avg:99.35ms
step:1696/1770 train_time:167502ms step_avg:99.35ms
step:1697/1770 train_time:167606ms step_avg:99.35ms
step:1698/1770 train_time:167710ms step_avg:99.35ms
step:1699/1770 train_time:167813ms step_avg:99.36ms
step:1700/1770 train_time:167916ms step_avg:99.36ms
step:1701/1770 train_time:168019ms step_avg:99.36ms
step:1702/1770 train_time:168124ms step_avg:99.36ms
step:1703/1770 train_time:168227ms step_avg:99.37ms
step:1704/1770 train_time:168330ms step_avg:99.37ms
step:1705/1770 train_time:168433ms step_avg:99.37ms
step:1706/1770 train_time:168537ms step_avg:99.37ms
step:1707/1770 train_time:168641ms step_avg:99.38ms
step:1708/1770 train_time:168744ms step_avg:99.38ms
step:1709/1770 train_time:168850ms step_avg:99.38ms
step:1710/1770 train_time:168957ms step_avg:99.39ms
step:1711/1770 train_time:169063ms step_avg:99.39ms
step:1712/1770 train_time:169167ms step_avg:99.39ms
step:1713/1770 train_time:169271ms step_avg:99.40ms
step:1714/1770 train_time:169375ms step_avg:99.40ms
step:1715/1770 train_time:169478ms step_avg:99.40ms
step:1716/1770 train_time:169583ms step_avg:99.40ms
step:1717/1770 train_time:169687ms step_avg:99.41ms
step:1718/1770 train_time:169791ms step_avg:99.41ms
step:1719/1770 train_time:169896ms step_avg:99.41ms
step:1720/1770 train_time:170002ms step_avg:99.42ms
step:1721/1770 train_time:170105ms step_avg:99.42ms
step:1722/1770 train_time:170211ms step_avg:99.42ms
step:1723/1770 train_time:170316ms step_avg:99.43ms
step:1724/1770 train_time:170423ms step_avg:99.43ms
step:1725/1770 train_time:170529ms step_avg:99.43ms
step:1726/1770 train_time:170635ms step_avg:99.44ms
step:1727/1770 train_time:170739ms step_avg:99.44ms
step:1728/1770 train_time:170846ms step_avg:99.44ms
step:1729/1770 train_time:170949ms step_avg:99.45ms
step:1730/1770 train_time:171055ms step_avg:99.45ms
step:1731/1770 train_time:171160ms step_avg:99.45ms
step:1732/1770 train_time:171264ms step_avg:99.46ms
step:1733/1770 train_time:171370ms step_avg:99.46ms
step:1734/1770 train_time:171472ms step_avg:99.46ms
step:1735/1770 train_time:171578ms step_avg:99.47ms
step:1736/1770 train_time:171680ms step_avg:99.47ms
step:1737/1770 train_time:171785ms step_avg:99.47ms
step:1738/1770 train_time:171889ms step_avg:99.47ms
step:1739/1770 train_time:171993ms step_avg:99.48ms
step:1740/1770 train_time:172097ms step_avg:99.48ms
step:1741/1770 train_time:172203ms step_avg:99.48ms
step:1742/1770 train_time:172311ms step_avg:99.49ms
step:1743/1770 train_time:172415ms step_avg:99.49ms
step:1744/1770 train_time:172520ms step_avg:99.49ms
step:1745/1770 train_time:172624ms step_avg:99.49ms
step:1746/1770 train_time:172730ms step_avg:99.50ms
step:1747/1770 train_time:172833ms step_avg:99.50ms
step:1748/1770 train_time:172939ms step_avg:99.50ms
step:1749/1770 train_time:173045ms step_avg:99.51ms
step:1750/1770 train_time:173148ms step_avg:99.51ms
step:1750/1770 val_loss:3.2803 train_time:173251ms step_avg:99.57ms
step:1751/1770 train_time:173273ms step_avg:99.52ms
step:1752/1770 train_time:173366ms step_avg:99.52ms
step:1753/1770 train_time:173470ms step_avg:99.52ms
step:1754/1770 train_time:173575ms step_avg:99.53ms
step:1755/1770 train_time:173678ms step_avg:99.53ms
step:1756/1770 train_time:173783ms step_avg:99.53ms
step:1757/1770 train_time:173887ms step_avg:99.53ms
step:1758/1770 train_time:173991ms step_avg:99.54ms
step:1759/1770 train_time:174096ms step_avg:99.54ms
step:1760/1770 train_time:174201ms step_avg:99.54ms
step:1761/1770 train_time:174307ms step_avg:99.55ms
step:1762/1770 train_time:174415ms step_avg:99.55ms
step:1763/1770 train_time:174518ms step_avg:99.55ms
step:1764/1770 train_time:174623ms step_avg:99.56ms
step:1765/1770 train_time:174726ms step_avg:99.56ms
step:1766/1770 train_time:174835ms step_avg:99.56ms
step:1767/1770 train_time:174938ms step_avg:99.57ms
step:1768/1770 train_time:175042ms step_avg:99.57ms
step:1769/1770 train_time:175145ms step_avg:99.57ms
step:1770/1770 train_time:175248ms step_avg:99.57ms
step:1770/1770 val_loss:3.2776 train_time:175353ms step_avg:99.63ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
