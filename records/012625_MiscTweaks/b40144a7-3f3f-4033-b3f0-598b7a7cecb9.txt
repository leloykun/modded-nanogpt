import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 22:55:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:25184ms step_avg:nanms
step:2/1770 train_time:25629ms step_avg:nanms
step:3/1770 train_time:25725ms step_avg:nanms
step:4/1770 train_time:25818ms step_avg:nanms
step:5/1770 train_time:25911ms step_avg:nanms
step:6/1770 train_time:26004ms step_avg:nanms
step:7/1770 train_time:26098ms step_avg:nanms
step:8/1770 train_time:26191ms step_avg:nanms
step:9/1770 train_time:26285ms step_avg:nanms
step:10/1770 train_time:26379ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.10ms
step:14/1770 train_time:376ms step_avg:94.09ms
step:15/1770 train_time:471ms step_avg:94.14ms
step:16/1770 train_time:564ms step_avg:94.07ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:93.96ms
step:19/1770 train_time:845ms step_avg:93.91ms
step:20/1770 train_time:939ms step_avg:93.88ms
step:21/1770 train_time:1032ms step_avg:93.86ms
step:22/1770 train_time:1126ms step_avg:93.86ms
step:23/1770 train_time:1220ms step_avg:93.85ms
step:24/1770 train_time:1314ms step_avg:93.85ms
step:25/1770 train_time:1409ms step_avg:93.92ms
step:26/1770 train_time:1503ms step_avg:93.93ms
step:27/1770 train_time:1597ms step_avg:93.92ms
step:28/1770 train_time:1691ms step_avg:93.96ms
step:29/1770 train_time:1785ms step_avg:93.96ms
step:30/1770 train_time:1879ms step_avg:93.96ms
step:31/1770 train_time:1973ms step_avg:93.94ms
step:32/1770 train_time:2066ms step_avg:93.93ms
step:33/1770 train_time:2160ms step_avg:93.92ms
step:34/1770 train_time:2254ms step_avg:93.92ms
step:35/1770 train_time:2349ms step_avg:93.95ms
step:36/1770 train_time:2443ms step_avg:93.95ms
step:37/1770 train_time:2537ms step_avg:93.94ms
step:38/1770 train_time:2631ms step_avg:93.95ms
step:39/1770 train_time:2725ms step_avg:93.95ms
step:40/1770 train_time:2819ms step_avg:93.95ms
step:41/1770 train_time:2912ms step_avg:93.95ms
step:42/1770 train_time:3007ms step_avg:93.96ms
step:43/1770 train_time:3100ms step_avg:93.95ms
step:44/1770 train_time:3194ms step_avg:93.95ms
step:45/1770 train_time:3288ms step_avg:93.95ms
step:46/1770 train_time:3382ms step_avg:93.96ms
step:47/1770 train_time:3476ms step_avg:93.94ms
step:48/1770 train_time:3570ms step_avg:93.94ms
step:49/1770 train_time:3663ms step_avg:93.93ms
step:50/1770 train_time:3757ms step_avg:93.93ms
step:51/1770 train_time:3852ms step_avg:93.94ms
step:52/1770 train_time:3946ms step_avg:93.94ms
step:53/1770 train_time:4039ms step_avg:93.94ms
step:54/1770 train_time:4133ms step_avg:93.93ms
step:55/1770 train_time:4227ms step_avg:93.93ms
step:56/1770 train_time:4320ms step_avg:93.92ms
step:57/1770 train_time:4414ms step_avg:93.92ms
step:58/1770 train_time:4508ms step_avg:93.92ms
step:59/1770 train_time:4601ms step_avg:93.90ms
step:60/1770 train_time:4695ms step_avg:93.90ms
step:61/1770 train_time:4788ms step_avg:93.89ms
step:62/1770 train_time:4882ms step_avg:93.88ms
step:63/1770 train_time:4976ms step_avg:93.89ms
step:64/1770 train_time:5070ms step_avg:93.89ms
step:65/1770 train_time:5163ms step_avg:93.88ms
step:66/1770 train_time:5256ms step_avg:93.87ms
step:67/1770 train_time:5350ms step_avg:93.86ms
step:68/1770 train_time:5444ms step_avg:93.86ms
step:69/1770 train_time:5538ms step_avg:93.86ms
step:70/1770 train_time:5632ms step_avg:93.86ms
step:71/1770 train_time:5725ms step_avg:93.86ms
step:72/1770 train_time:5820ms step_avg:93.87ms
step:73/1770 train_time:5914ms step_avg:93.88ms
step:74/1770 train_time:6008ms step_avg:93.88ms
step:75/1770 train_time:6102ms step_avg:93.88ms
step:76/1770 train_time:6196ms step_avg:93.89ms
step:77/1770 train_time:6290ms step_avg:93.88ms
step:78/1770 train_time:6384ms step_avg:93.88ms
step:79/1770 train_time:6478ms step_avg:93.88ms
step:80/1770 train_time:6572ms step_avg:93.88ms
step:81/1770 train_time:6665ms step_avg:93.87ms
step:82/1770 train_time:6759ms step_avg:93.87ms
step:83/1770 train_time:6853ms step_avg:93.87ms
step:84/1770 train_time:6946ms step_avg:93.87ms
step:85/1770 train_time:7040ms step_avg:93.87ms
step:86/1770 train_time:7135ms step_avg:93.88ms
step:87/1770 train_time:7228ms step_avg:93.88ms
step:88/1770 train_time:7322ms step_avg:93.88ms
step:89/1770 train_time:7416ms step_avg:93.87ms
step:90/1770 train_time:7510ms step_avg:93.87ms
step:91/1770 train_time:7604ms step_avg:93.88ms
step:92/1770 train_time:7698ms step_avg:93.87ms
step:93/1770 train_time:7792ms step_avg:93.88ms
step:94/1770 train_time:7885ms step_avg:93.87ms
step:95/1770 train_time:7979ms step_avg:93.87ms
step:96/1770 train_time:8074ms step_avg:93.88ms
step:97/1770 train_time:8168ms step_avg:93.88ms
step:98/1770 train_time:8262ms step_avg:93.88ms
step:99/1770 train_time:8355ms step_avg:93.88ms
step:100/1770 train_time:8449ms step_avg:93.88ms
step:101/1770 train_time:8543ms step_avg:93.88ms
step:102/1770 train_time:8637ms step_avg:93.88ms
step:103/1770 train_time:8730ms step_avg:93.88ms
step:104/1770 train_time:8824ms step_avg:93.88ms
step:105/1770 train_time:8918ms step_avg:93.88ms
step:106/1770 train_time:9012ms step_avg:93.88ms
step:107/1770 train_time:9106ms step_avg:93.88ms
step:108/1770 train_time:9200ms step_avg:93.88ms
step:109/1770 train_time:9295ms step_avg:93.88ms
step:110/1770 train_time:9389ms step_avg:93.89ms
step:111/1770 train_time:9482ms step_avg:93.88ms
step:112/1770 train_time:9576ms step_avg:93.88ms
step:113/1770 train_time:9670ms step_avg:93.88ms
step:114/1770 train_time:9763ms step_avg:93.88ms
step:115/1770 train_time:9857ms step_avg:93.88ms
step:116/1770 train_time:9952ms step_avg:93.88ms
step:117/1770 train_time:10046ms step_avg:93.88ms
step:118/1770 train_time:10139ms step_avg:93.88ms
step:119/1770 train_time:10233ms step_avg:93.88ms
step:120/1770 train_time:10327ms step_avg:93.89ms
step:121/1770 train_time:10421ms step_avg:93.88ms
step:122/1770 train_time:10515ms step_avg:93.89ms
step:123/1770 train_time:10609ms step_avg:93.89ms
step:124/1770 train_time:10703ms step_avg:93.88ms
step:125/1770 train_time:10797ms step_avg:93.89ms
step:125/1770 val_loss:4.6536 train_time:10889ms step_avg:94.69ms
step:126/1770 train_time:10911ms step_avg:94.06ms
step:127/1770 train_time:10989ms step_avg:93.93ms
step:128/1770 train_time:11086ms step_avg:93.95ms
step:129/1770 train_time:11187ms step_avg:94.00ms
step:130/1770 train_time:11284ms step_avg:94.03ms
step:131/1770 train_time:11377ms step_avg:94.03ms
step:132/1770 train_time:11471ms step_avg:94.02ms
step:133/1770 train_time:11565ms step_avg:94.02ms
step:134/1770 train_time:11659ms step_avg:94.02ms
step:135/1770 train_time:11753ms step_avg:94.02ms
step:136/1770 train_time:11846ms step_avg:94.02ms
step:137/1770 train_time:11941ms step_avg:94.02ms
step:138/1770 train_time:12035ms step_avg:94.02ms
step:139/1770 train_time:12129ms step_avg:94.03ms
step:140/1770 train_time:12225ms step_avg:94.04ms
step:141/1770 train_time:12319ms step_avg:94.04ms
step:142/1770 train_time:12414ms step_avg:94.05ms
step:143/1770 train_time:12508ms step_avg:94.04ms
step:144/1770 train_time:12602ms step_avg:94.04ms
step:145/1770 train_time:12696ms step_avg:94.05ms
step:146/1770 train_time:12790ms step_avg:94.04ms
step:147/1770 train_time:12884ms step_avg:94.04ms
step:148/1770 train_time:12979ms step_avg:94.05ms
step:149/1770 train_time:13074ms step_avg:94.05ms
step:150/1770 train_time:13168ms step_avg:94.06ms
step:151/1770 train_time:13263ms step_avg:94.06ms
step:152/1770 train_time:13358ms step_avg:94.07ms
step:153/1770 train_time:13452ms step_avg:94.07ms
step:154/1770 train_time:13546ms step_avg:94.07ms
step:155/1770 train_time:13641ms step_avg:94.08ms
step:156/1770 train_time:13736ms step_avg:94.08ms
step:157/1770 train_time:13830ms step_avg:94.08ms
step:158/1770 train_time:13925ms step_avg:94.09ms
step:159/1770 train_time:14019ms step_avg:94.09ms
step:160/1770 train_time:14114ms step_avg:94.09ms
step:161/1770 train_time:14208ms step_avg:94.09ms
step:162/1770 train_time:14302ms step_avg:94.10ms
step:163/1770 train_time:14397ms step_avg:94.10ms
step:164/1770 train_time:14492ms step_avg:94.10ms
step:165/1770 train_time:14586ms step_avg:94.10ms
step:166/1770 train_time:14681ms step_avg:94.11ms
step:167/1770 train_time:14775ms step_avg:94.11ms
step:168/1770 train_time:14870ms step_avg:94.11ms
step:169/1770 train_time:14964ms step_avg:94.11ms
step:170/1770 train_time:15059ms step_avg:94.12ms
step:171/1770 train_time:15154ms step_avg:94.12ms
step:172/1770 train_time:15248ms step_avg:94.12ms
step:173/1770 train_time:15342ms step_avg:94.13ms
step:174/1770 train_time:15437ms step_avg:94.13ms
step:175/1770 train_time:15532ms step_avg:94.13ms
step:176/1770 train_time:15626ms step_avg:94.13ms
step:177/1770 train_time:15721ms step_avg:94.14ms
step:178/1770 train_time:15815ms step_avg:94.14ms
step:179/1770 train_time:15910ms step_avg:94.14ms
step:180/1770 train_time:16005ms step_avg:94.14ms
step:181/1770 train_time:16099ms step_avg:94.15ms
step:182/1770 train_time:16194ms step_avg:94.15ms
step:183/1770 train_time:16288ms step_avg:94.15ms
step:184/1770 train_time:16383ms step_avg:94.16ms
step:185/1770 train_time:16478ms step_avg:94.16ms
step:186/1770 train_time:16572ms step_avg:94.16ms
step:187/1770 train_time:16667ms step_avg:94.16ms
step:188/1770 train_time:16761ms step_avg:94.16ms
step:189/1770 train_time:16856ms step_avg:94.17ms
step:190/1770 train_time:16950ms step_avg:94.17ms
step:191/1770 train_time:17045ms step_avg:94.17ms
step:192/1770 train_time:17140ms step_avg:94.17ms
step:193/1770 train_time:17234ms step_avg:94.17ms
step:194/1770 train_time:17328ms step_avg:94.17ms
step:195/1770 train_time:17423ms step_avg:94.18ms
step:196/1770 train_time:17518ms step_avg:94.18ms
step:197/1770 train_time:17612ms step_avg:94.18ms
step:198/1770 train_time:17707ms step_avg:94.18ms
step:199/1770 train_time:17801ms step_avg:94.19ms
step:200/1770 train_time:17896ms step_avg:94.19ms
step:201/1770 train_time:17991ms step_avg:94.19ms
step:202/1770 train_time:18085ms step_avg:94.19ms
step:203/1770 train_time:18179ms step_avg:94.19ms
step:204/1770 train_time:18274ms step_avg:94.20ms
step:205/1770 train_time:18369ms step_avg:94.20ms
step:206/1770 train_time:18464ms step_avg:94.20ms
step:207/1770 train_time:18558ms step_avg:94.20ms
step:208/1770 train_time:18653ms step_avg:94.21ms
step:209/1770 train_time:18747ms step_avg:94.21ms
step:210/1770 train_time:18842ms step_avg:94.21ms
step:211/1770 train_time:18936ms step_avg:94.21ms
step:212/1770 train_time:19031ms step_avg:94.21ms
step:213/1770 train_time:19125ms step_avg:94.21ms
step:214/1770 train_time:19220ms step_avg:94.22ms
step:215/1770 train_time:19315ms step_avg:94.22ms
step:216/1770 train_time:19409ms step_avg:94.22ms
step:217/1770 train_time:19504ms step_avg:94.22ms
step:218/1770 train_time:19598ms step_avg:94.22ms
step:219/1770 train_time:19692ms step_avg:94.22ms
step:220/1770 train_time:19787ms step_avg:94.22ms
step:221/1770 train_time:19881ms step_avg:94.23ms
step:222/1770 train_time:19977ms step_avg:94.23ms
step:223/1770 train_time:20071ms step_avg:94.23ms
step:224/1770 train_time:20165ms step_avg:94.23ms
step:225/1770 train_time:20260ms step_avg:94.23ms
step:226/1770 train_time:20354ms step_avg:94.23ms
step:227/1770 train_time:20448ms step_avg:94.23ms
step:228/1770 train_time:20543ms step_avg:94.23ms
step:229/1770 train_time:20637ms step_avg:94.23ms
step:230/1770 train_time:20731ms step_avg:94.23ms
step:231/1770 train_time:20826ms step_avg:94.24ms
step:232/1770 train_time:20921ms step_avg:94.24ms
step:233/1770 train_time:21016ms step_avg:94.24ms
step:234/1770 train_time:21110ms step_avg:94.24ms
step:235/1770 train_time:21204ms step_avg:94.24ms
step:236/1770 train_time:21300ms step_avg:94.25ms
step:237/1770 train_time:21395ms step_avg:94.25ms
step:238/1770 train_time:21490ms step_avg:94.25ms
step:239/1770 train_time:21584ms step_avg:94.25ms
step:240/1770 train_time:21679ms step_avg:94.25ms
step:241/1770 train_time:21773ms step_avg:94.26ms
step:242/1770 train_time:21868ms step_avg:94.26ms
step:243/1770 train_time:21962ms step_avg:94.26ms
step:244/1770 train_time:22057ms step_avg:94.26ms
step:245/1770 train_time:22152ms step_avg:94.26ms
step:246/1770 train_time:22246ms step_avg:94.26ms
step:247/1770 train_time:22341ms step_avg:94.27ms
step:248/1770 train_time:22436ms step_avg:94.27ms
step:249/1770 train_time:22531ms step_avg:94.27ms
step:250/1770 train_time:22625ms step_avg:94.27ms
step:250/1770 val_loss:4.1125 train_time:22719ms step_avg:94.66ms
step:251/1770 train_time:22740ms step_avg:94.36ms
step:252/1770 train_time:22826ms step_avg:94.32ms
step:253/1770 train_time:22926ms step_avg:94.34ms
step:254/1770 train_time:23021ms step_avg:94.35ms
step:255/1770 train_time:23115ms step_avg:94.35ms
step:256/1770 train_time:23209ms step_avg:94.35ms
step:257/1770 train_time:23303ms step_avg:94.34ms
step:258/1770 train_time:23397ms step_avg:94.34ms
step:259/1770 train_time:23491ms step_avg:94.34ms
step:260/1770 train_time:23585ms step_avg:94.34ms
step:261/1770 train_time:23679ms step_avg:94.34ms
step:262/1770 train_time:23775ms step_avg:94.34ms
step:263/1770 train_time:23870ms step_avg:94.35ms
step:264/1770 train_time:23965ms step_avg:94.35ms
step:265/1770 train_time:24060ms step_avg:94.35ms
step:266/1770 train_time:24155ms step_avg:94.36ms
step:267/1770 train_time:24251ms step_avg:94.36ms
step:268/1770 train_time:24346ms step_avg:94.36ms
step:269/1770 train_time:24440ms step_avg:94.36ms
step:270/1770 train_time:24535ms step_avg:94.37ms
step:271/1770 train_time:24630ms step_avg:94.37ms
step:272/1770 train_time:24724ms step_avg:94.37ms
step:273/1770 train_time:24820ms step_avg:94.37ms
step:274/1770 train_time:24916ms step_avg:94.38ms
step:275/1770 train_time:25011ms step_avg:94.38ms
step:276/1770 train_time:25105ms step_avg:94.38ms
step:277/1770 train_time:25200ms step_avg:94.38ms
step:278/1770 train_time:25296ms step_avg:94.39ms
step:279/1770 train_time:25391ms step_avg:94.39ms
step:280/1770 train_time:25485ms step_avg:94.39ms
step:281/1770 train_time:25580ms step_avg:94.39ms
step:282/1770 train_time:25675ms step_avg:94.39ms
step:283/1770 train_time:25770ms step_avg:94.40ms
step:284/1770 train_time:25865ms step_avg:94.40ms
step:285/1770 train_time:25960ms step_avg:94.40ms
step:286/1770 train_time:26056ms step_avg:94.40ms
step:287/1770 train_time:26151ms step_avg:94.41ms
step:288/1770 train_time:26246ms step_avg:94.41ms
step:289/1770 train_time:26341ms step_avg:94.41ms
step:290/1770 train_time:26437ms step_avg:94.42ms
step:291/1770 train_time:26531ms step_avg:94.42ms
step:292/1770 train_time:26626ms step_avg:94.42ms
step:293/1770 train_time:26721ms step_avg:94.42ms
step:294/1770 train_time:26816ms step_avg:94.42ms
step:295/1770 train_time:26911ms step_avg:94.42ms
step:296/1770 train_time:27006ms step_avg:94.43ms
step:297/1770 train_time:27101ms step_avg:94.43ms
step:298/1770 train_time:27196ms step_avg:94.43ms
step:299/1770 train_time:27291ms step_avg:94.43ms
step:300/1770 train_time:27386ms step_avg:94.43ms
step:301/1770 train_time:27482ms step_avg:94.44ms
step:302/1770 train_time:27577ms step_avg:94.44ms
step:303/1770 train_time:27672ms step_avg:94.44ms
step:304/1770 train_time:27766ms step_avg:94.44ms
step:305/1770 train_time:27861ms step_avg:94.45ms
step:306/1770 train_time:27957ms step_avg:94.45ms
step:307/1770 train_time:28051ms step_avg:94.45ms
step:308/1770 train_time:28146ms step_avg:94.45ms
step:309/1770 train_time:28241ms step_avg:94.45ms
step:310/1770 train_time:28337ms step_avg:94.46ms
step:311/1770 train_time:28432ms step_avg:94.46ms
step:312/1770 train_time:28526ms step_avg:94.46ms
step:313/1770 train_time:28622ms step_avg:94.46ms
step:314/1770 train_time:28717ms step_avg:94.46ms
step:315/1770 train_time:28813ms step_avg:94.47ms
step:316/1770 train_time:28907ms step_avg:94.47ms
step:317/1770 train_time:29002ms step_avg:94.47ms
step:318/1770 train_time:29098ms step_avg:94.47ms
step:319/1770 train_time:29193ms step_avg:94.47ms
step:320/1770 train_time:29288ms step_avg:94.48ms
step:321/1770 train_time:29382ms step_avg:94.48ms
step:322/1770 train_time:29478ms step_avg:94.48ms
step:323/1770 train_time:29573ms step_avg:94.48ms
step:324/1770 train_time:29667ms step_avg:94.48ms
step:325/1770 train_time:29763ms step_avg:94.48ms
step:326/1770 train_time:29858ms step_avg:94.49ms
step:327/1770 train_time:29953ms step_avg:94.49ms
step:328/1770 train_time:30047ms step_avg:94.49ms
step:329/1770 train_time:30142ms step_avg:94.49ms
step:330/1770 train_time:30238ms step_avg:94.49ms
step:331/1770 train_time:30333ms step_avg:94.50ms
step:332/1770 train_time:30428ms step_avg:94.50ms
step:333/1770 train_time:30523ms step_avg:94.50ms
step:334/1770 train_time:30618ms step_avg:94.50ms
step:335/1770 train_time:30713ms step_avg:94.50ms
step:336/1770 train_time:30808ms step_avg:94.50ms
step:337/1770 train_time:30903ms step_avg:94.51ms
step:338/1770 train_time:30999ms step_avg:94.51ms
step:339/1770 train_time:31094ms step_avg:94.51ms
step:340/1770 train_time:31188ms step_avg:94.51ms
step:341/1770 train_time:31283ms step_avg:94.51ms
step:342/1770 train_time:31379ms step_avg:94.51ms
step:343/1770 train_time:31474ms step_avg:94.52ms
step:344/1770 train_time:31569ms step_avg:94.52ms
step:345/1770 train_time:31663ms step_avg:94.52ms
step:346/1770 train_time:31759ms step_avg:94.52ms
step:347/1770 train_time:31854ms step_avg:94.52ms
step:348/1770 train_time:31949ms step_avg:94.52ms
step:349/1770 train_time:32044ms step_avg:94.53ms
step:350/1770 train_time:32139ms step_avg:94.53ms
step:351/1770 train_time:32234ms step_avg:94.53ms
step:352/1770 train_time:32329ms step_avg:94.53ms
step:353/1770 train_time:32424ms step_avg:94.53ms
step:354/1770 train_time:32519ms step_avg:94.53ms
step:355/1770 train_time:32614ms step_avg:94.53ms
step:356/1770 train_time:32709ms step_avg:94.53ms
step:357/1770 train_time:32804ms step_avg:94.54ms
step:358/1770 train_time:32899ms step_avg:94.54ms
step:359/1770 train_time:32994ms step_avg:94.54ms
step:360/1770 train_time:33089ms step_avg:94.54ms
step:361/1770 train_time:33184ms step_avg:94.54ms
step:362/1770 train_time:33280ms step_avg:94.54ms
step:363/1770 train_time:33375ms step_avg:94.55ms
step:364/1770 train_time:33470ms step_avg:94.55ms
step:365/1770 train_time:33565ms step_avg:94.55ms
step:366/1770 train_time:33661ms step_avg:94.55ms
step:367/1770 train_time:33756ms step_avg:94.55ms
step:368/1770 train_time:33851ms step_avg:94.55ms
step:369/1770 train_time:33946ms step_avg:94.56ms
step:370/1770 train_time:34041ms step_avg:94.56ms
step:371/1770 train_time:34136ms step_avg:94.56ms
step:372/1770 train_time:34231ms step_avg:94.56ms
step:373/1770 train_time:34326ms step_avg:94.56ms
step:374/1770 train_time:34421ms step_avg:94.56ms
step:375/1770 train_time:34516ms step_avg:94.57ms
step:375/1770 val_loss:3.9082 train_time:34610ms step_avg:94.82ms
step:376/1770 train_time:34632ms step_avg:94.62ms
step:377/1770 train_time:34716ms step_avg:94.59ms
step:378/1770 train_time:34813ms step_avg:94.60ms
step:379/1770 train_time:34908ms step_avg:94.60ms
step:380/1770 train_time:35002ms step_avg:94.60ms
step:381/1770 train_time:35097ms step_avg:94.60ms
step:382/1770 train_time:35192ms step_avg:94.60ms
step:383/1770 train_time:35287ms step_avg:94.60ms
step:384/1770 train_time:35381ms step_avg:94.60ms
step:385/1770 train_time:35476ms step_avg:94.60ms
step:386/1770 train_time:35571ms step_avg:94.60ms
step:387/1770 train_time:35666ms step_avg:94.60ms
step:388/1770 train_time:35761ms step_avg:94.61ms
step:389/1770 train_time:35856ms step_avg:94.61ms
step:390/1770 train_time:35952ms step_avg:94.61ms
step:391/1770 train_time:36047ms step_avg:94.61ms
step:392/1770 train_time:36142ms step_avg:94.61ms
step:393/1770 train_time:36236ms step_avg:94.61ms
step:394/1770 train_time:36331ms step_avg:94.61ms
step:395/1770 train_time:36426ms step_avg:94.61ms
step:396/1770 train_time:36523ms step_avg:94.62ms
step:397/1770 train_time:36619ms step_avg:94.62ms
step:398/1770 train_time:36716ms step_avg:94.63ms
step:399/1770 train_time:36813ms step_avg:94.64ms
step:400/1770 train_time:36911ms step_avg:94.64ms
step:401/1770 train_time:37007ms step_avg:94.65ms
step:402/1770 train_time:37104ms step_avg:94.65ms
step:403/1770 train_time:37201ms step_avg:94.66ms
step:404/1770 train_time:37298ms step_avg:94.66ms
step:405/1770 train_time:37395ms step_avg:94.67ms
step:406/1770 train_time:37492ms step_avg:94.68ms
step:407/1770 train_time:37589ms step_avg:94.68ms
step:408/1770 train_time:37686ms step_avg:94.69ms
step:409/1770 train_time:37782ms step_avg:94.69ms
step:410/1770 train_time:37879ms step_avg:94.70ms
step:411/1770 train_time:37977ms step_avg:94.71ms
step:412/1770 train_time:38075ms step_avg:94.71ms
step:413/1770 train_time:38172ms step_avg:94.72ms
step:414/1770 train_time:38270ms step_avg:94.73ms
step:415/1770 train_time:38366ms step_avg:94.73ms
step:416/1770 train_time:38463ms step_avg:94.74ms
step:417/1770 train_time:38559ms step_avg:94.74ms
step:418/1770 train_time:38656ms step_avg:94.74ms
step:419/1770 train_time:38753ms step_avg:94.75ms
step:420/1770 train_time:38850ms step_avg:94.76ms
step:421/1770 train_time:38947ms step_avg:94.76ms
step:422/1770 train_time:39043ms step_avg:94.77ms
step:423/1770 train_time:39140ms step_avg:94.77ms
step:424/1770 train_time:39236ms step_avg:94.77ms
step:425/1770 train_time:39333ms step_avg:94.78ms
step:426/1770 train_time:39431ms step_avg:94.79ms
step:427/1770 train_time:39528ms step_avg:94.79ms
step:428/1770 train_time:39625ms step_avg:94.80ms
step:429/1770 train_time:39722ms step_avg:94.80ms
step:430/1770 train_time:39818ms step_avg:94.81ms
step:431/1770 train_time:39915ms step_avg:94.81ms
step:432/1770 train_time:40013ms step_avg:94.82ms
step:433/1770 train_time:40109ms step_avg:94.82ms
step:434/1770 train_time:40206ms step_avg:94.83ms
step:435/1770 train_time:40302ms step_avg:94.83ms
step:436/1770 train_time:40399ms step_avg:94.83ms
step:437/1770 train_time:40497ms step_avg:94.84ms
step:438/1770 train_time:40594ms step_avg:94.85ms
step:439/1770 train_time:40691ms step_avg:94.85ms
step:440/1770 train_time:40788ms step_avg:94.86ms
step:441/1770 train_time:40885ms step_avg:94.86ms
step:442/1770 train_time:40982ms step_avg:94.86ms
step:443/1770 train_time:41079ms step_avg:94.87ms
step:444/1770 train_time:41176ms step_avg:94.88ms
step:445/1770 train_time:41273ms step_avg:94.88ms
step:446/1770 train_time:41370ms step_avg:94.89ms
step:447/1770 train_time:41467ms step_avg:94.89ms
step:448/1770 train_time:41564ms step_avg:94.89ms
step:449/1770 train_time:41660ms step_avg:94.90ms
step:450/1770 train_time:41757ms step_avg:94.90ms
step:451/1770 train_time:41854ms step_avg:94.91ms
step:452/1770 train_time:41951ms step_avg:94.91ms
step:453/1770 train_time:42048ms step_avg:94.92ms
step:454/1770 train_time:42145ms step_avg:94.92ms
step:455/1770 train_time:42241ms step_avg:94.92ms
step:456/1770 train_time:42338ms step_avg:94.93ms
step:457/1770 train_time:42436ms step_avg:94.93ms
step:458/1770 train_time:42533ms step_avg:94.94ms
step:459/1770 train_time:42630ms step_avg:94.94ms
step:460/1770 train_time:42727ms step_avg:94.95ms
step:461/1770 train_time:42824ms step_avg:94.95ms
step:462/1770 train_time:42920ms step_avg:94.96ms
step:463/1770 train_time:43018ms step_avg:94.96ms
step:464/1770 train_time:43115ms step_avg:94.97ms
step:465/1770 train_time:43212ms step_avg:94.97ms
step:466/1770 train_time:43309ms step_avg:94.98ms
step:467/1770 train_time:43405ms step_avg:94.98ms
step:468/1770 train_time:43502ms step_avg:94.98ms
step:469/1770 train_time:43599ms step_avg:94.99ms
step:470/1770 train_time:43696ms step_avg:94.99ms
step:471/1770 train_time:43793ms step_avg:95.00ms
step:472/1770 train_time:43890ms step_avg:95.00ms
step:473/1770 train_time:43987ms step_avg:95.00ms
step:474/1770 train_time:44083ms step_avg:95.01ms
step:475/1770 train_time:44180ms step_avg:95.01ms
step:476/1770 train_time:44276ms step_avg:95.01ms
step:477/1770 train_time:44373ms step_avg:95.02ms
step:478/1770 train_time:44471ms step_avg:95.02ms
step:479/1770 train_time:44568ms step_avg:95.03ms
step:480/1770 train_time:44666ms step_avg:95.03ms
step:481/1770 train_time:44762ms step_avg:95.04ms
step:482/1770 train_time:44859ms step_avg:95.04ms
step:483/1770 train_time:44956ms step_avg:95.05ms
step:484/1770 train_time:45054ms step_avg:95.05ms
step:485/1770 train_time:45150ms step_avg:95.05ms
step:486/1770 train_time:45247ms step_avg:95.06ms
step:487/1770 train_time:45343ms step_avg:95.06ms
step:488/1770 train_time:45440ms step_avg:95.06ms
step:489/1770 train_time:45537ms step_avg:95.07ms
step:490/1770 train_time:45634ms step_avg:95.07ms
step:491/1770 train_time:45731ms step_avg:95.07ms
step:492/1770 train_time:45828ms step_avg:95.08ms
step:493/1770 train_time:45925ms step_avg:95.08ms
step:494/1770 train_time:46022ms step_avg:95.09ms
step:495/1770 train_time:46119ms step_avg:95.09ms
step:496/1770 train_time:46217ms step_avg:95.10ms
step:497/1770 train_time:46315ms step_avg:95.10ms
step:498/1770 train_time:46411ms step_avg:95.11ms
step:499/1770 train_time:46508ms step_avg:95.11ms
step:500/1770 train_time:46605ms step_avg:95.11ms
step:500/1770 val_loss:3.7571 train_time:46700ms step_avg:95.31ms
step:501/1770 train_time:46722ms step_avg:95.16ms
step:502/1770 train_time:46806ms step_avg:95.13ms
step:503/1770 train_time:46909ms step_avg:95.15ms
step:504/1770 train_time:47007ms step_avg:95.16ms
step:505/1770 train_time:47104ms step_avg:95.16ms
step:506/1770 train_time:47200ms step_avg:95.16ms
step:507/1770 train_time:47296ms step_avg:95.16ms
step:508/1770 train_time:47393ms step_avg:95.17ms
step:509/1770 train_time:47490ms step_avg:95.17ms
step:510/1770 train_time:47586ms step_avg:95.17ms
step:511/1770 train_time:47683ms step_avg:95.18ms
step:512/1770 train_time:47781ms step_avg:95.18ms
step:513/1770 train_time:47879ms step_avg:95.19ms
step:514/1770 train_time:47977ms step_avg:95.19ms
step:515/1770 train_time:48075ms step_avg:95.20ms
step:516/1770 train_time:48172ms step_avg:95.20ms
step:517/1770 train_time:48269ms step_avg:95.20ms
step:518/1770 train_time:48365ms step_avg:95.21ms
step:519/1770 train_time:48462ms step_avg:95.21ms
step:520/1770 train_time:48559ms step_avg:95.21ms
step:521/1770 train_time:48656ms step_avg:95.22ms
step:522/1770 train_time:48753ms step_avg:95.22ms
step:523/1770 train_time:48850ms step_avg:95.22ms
step:524/1770 train_time:48947ms step_avg:95.23ms
step:525/1770 train_time:49044ms step_avg:95.23ms
step:526/1770 train_time:49141ms step_avg:95.24ms
step:527/1770 train_time:49239ms step_avg:95.24ms
step:528/1770 train_time:49337ms step_avg:95.24ms
step:529/1770 train_time:49434ms step_avg:95.25ms
step:530/1770 train_time:49531ms step_avg:95.25ms
step:531/1770 train_time:49628ms step_avg:95.26ms
step:532/1770 train_time:49725ms step_avg:95.26ms
step:533/1770 train_time:49822ms step_avg:95.26ms
step:534/1770 train_time:49919ms step_avg:95.27ms
step:535/1770 train_time:50017ms step_avg:95.27ms
step:536/1770 train_time:50114ms step_avg:95.27ms
step:537/1770 train_time:50212ms step_avg:95.28ms
step:538/1770 train_time:50310ms step_avg:95.28ms
step:539/1770 train_time:50407ms step_avg:95.29ms
step:540/1770 train_time:50504ms step_avg:95.29ms
step:541/1770 train_time:50601ms step_avg:95.29ms
step:542/1770 train_time:50699ms step_avg:95.30ms
step:543/1770 train_time:50796ms step_avg:95.30ms
step:544/1770 train_time:50894ms step_avg:95.31ms
step:545/1770 train_time:50991ms step_avg:95.31ms
step:546/1770 train_time:51088ms step_avg:95.31ms
step:547/1770 train_time:51185ms step_avg:95.32ms
step:548/1770 train_time:51282ms step_avg:95.32ms
step:549/1770 train_time:51379ms step_avg:95.32ms
step:550/1770 train_time:51476ms step_avg:95.33ms
step:551/1770 train_time:51573ms step_avg:95.33ms
step:552/1770 train_time:51671ms step_avg:95.33ms
step:553/1770 train_time:51768ms step_avg:95.34ms
step:554/1770 train_time:51865ms step_avg:95.34ms
step:555/1770 train_time:51962ms step_avg:95.34ms
step:556/1770 train_time:52059ms step_avg:95.35ms
step:557/1770 train_time:52156ms step_avg:95.35ms
step:558/1770 train_time:52254ms step_avg:95.35ms
step:559/1770 train_time:52351ms step_avg:95.36ms
step:560/1770 train_time:52448ms step_avg:95.36ms
step:561/1770 train_time:52545ms step_avg:95.36ms
step:562/1770 train_time:52643ms step_avg:95.37ms
step:563/1770 train_time:52740ms step_avg:95.37ms
step:564/1770 train_time:52837ms step_avg:95.37ms
step:565/1770 train_time:52934ms step_avg:95.38ms
step:566/1770 train_time:53031ms step_avg:95.38ms
step:567/1770 train_time:53129ms step_avg:95.38ms
step:568/1770 train_time:53226ms step_avg:95.39ms
step:569/1770 train_time:53324ms step_avg:95.39ms
step:570/1770 train_time:53422ms step_avg:95.40ms
step:571/1770 train_time:53519ms step_avg:95.40ms
step:572/1770 train_time:53617ms step_avg:95.40ms
step:573/1770 train_time:53714ms step_avg:95.41ms
step:574/1770 train_time:53811ms step_avg:95.41ms
step:575/1770 train_time:53909ms step_avg:95.41ms
step:576/1770 train_time:54006ms step_avg:95.42ms
step:577/1770 train_time:54102ms step_avg:95.42ms
step:578/1770 train_time:54200ms step_avg:95.42ms
step:579/1770 train_time:54297ms step_avg:95.43ms
step:580/1770 train_time:54395ms step_avg:95.43ms
step:581/1770 train_time:54492ms step_avg:95.43ms
step:582/1770 train_time:54590ms step_avg:95.44ms
step:583/1770 train_time:54687ms step_avg:95.44ms
step:584/1770 train_time:54784ms step_avg:95.44ms
step:585/1770 train_time:54881ms step_avg:95.45ms
step:586/1770 train_time:54978ms step_avg:95.45ms
step:587/1770 train_time:55075ms step_avg:95.45ms
step:588/1770 train_time:55173ms step_avg:95.45ms
step:589/1770 train_time:55271ms step_avg:95.46ms
step:590/1770 train_time:55368ms step_avg:95.46ms
step:591/1770 train_time:55465ms step_avg:95.46ms
step:592/1770 train_time:55562ms step_avg:95.47ms
step:593/1770 train_time:55659ms step_avg:95.47ms
step:594/1770 train_time:55756ms step_avg:95.47ms
step:595/1770 train_time:55854ms step_avg:95.48ms
step:596/1770 train_time:55951ms step_avg:95.48ms
step:597/1770 train_time:56048ms step_avg:95.48ms
step:598/1770 train_time:56146ms step_avg:95.49ms
step:599/1770 train_time:56243ms step_avg:95.49ms
step:600/1770 train_time:56340ms step_avg:95.49ms
step:601/1770 train_time:56437ms step_avg:95.49ms
step:602/1770 train_time:56535ms step_avg:95.50ms
step:603/1770 train_time:56632ms step_avg:95.50ms
step:604/1770 train_time:56729ms step_avg:95.50ms
step:605/1770 train_time:56826ms step_avg:95.51ms
step:606/1770 train_time:56924ms step_avg:95.51ms
step:607/1770 train_time:57022ms step_avg:95.51ms
step:608/1770 train_time:57119ms step_avg:95.52ms
step:609/1770 train_time:57217ms step_avg:95.52ms
step:610/1770 train_time:57314ms step_avg:95.52ms
step:611/1770 train_time:57411ms step_avg:95.53ms
step:612/1770 train_time:57508ms step_avg:95.53ms
step:613/1770 train_time:57605ms step_avg:95.53ms
step:614/1770 train_time:57702ms step_avg:95.53ms
step:615/1770 train_time:57799ms step_avg:95.54ms
step:616/1770 train_time:57896ms step_avg:95.54ms
step:617/1770 train_time:57994ms step_avg:95.54ms
step:618/1770 train_time:58092ms step_avg:95.55ms
step:619/1770 train_time:58189ms step_avg:95.55ms
step:620/1770 train_time:58286ms step_avg:95.55ms
step:621/1770 train_time:58383ms step_avg:95.55ms
step:622/1770 train_time:58480ms step_avg:95.56ms
step:623/1770 train_time:58578ms step_avg:95.56ms
step:624/1770 train_time:58675ms step_avg:95.56ms
step:625/1770 train_time:58773ms step_avg:95.57ms
step:625/1770 val_loss:3.6690 train_time:58869ms step_avg:95.72ms
step:626/1770 train_time:58890ms step_avg:95.60ms
step:627/1770 train_time:58976ms step_avg:95.59ms
step:628/1770 train_time:59076ms step_avg:95.59ms
step:629/1770 train_time:59173ms step_avg:95.60ms
step:630/1770 train_time:59271ms step_avg:95.60ms
step:631/1770 train_time:59368ms step_avg:95.60ms
step:632/1770 train_time:59465ms step_avg:95.60ms
step:633/1770 train_time:59562ms step_avg:95.60ms
step:634/1770 train_time:59658ms step_avg:95.61ms
step:635/1770 train_time:59755ms step_avg:95.61ms
step:636/1770 train_time:59852ms step_avg:95.61ms
step:637/1770 train_time:59950ms step_avg:95.61ms
step:638/1770 train_time:60049ms step_avg:95.62ms
step:639/1770 train_time:60148ms step_avg:95.62ms
step:640/1770 train_time:60245ms step_avg:95.63ms
step:641/1770 train_time:60342ms step_avg:95.63ms
step:642/1770 train_time:60439ms step_avg:95.63ms
step:643/1770 train_time:60535ms step_avg:95.63ms
step:644/1770 train_time:60632ms step_avg:95.63ms
step:645/1770 train_time:60729ms step_avg:95.64ms
step:646/1770 train_time:60827ms step_avg:95.64ms
step:647/1770 train_time:60924ms step_avg:95.64ms
step:648/1770 train_time:61021ms step_avg:95.64ms
step:649/1770 train_time:61120ms step_avg:95.65ms
step:650/1770 train_time:61217ms step_avg:95.65ms
step:651/1770 train_time:61314ms step_avg:95.65ms
step:652/1770 train_time:61411ms step_avg:95.66ms
step:653/1770 train_time:61508ms step_avg:95.66ms
step:654/1770 train_time:61606ms step_avg:95.66ms
step:655/1770 train_time:61703ms step_avg:95.66ms
step:656/1770 train_time:61800ms step_avg:95.67ms
step:657/1770 train_time:61898ms step_avg:95.67ms
step:658/1770 train_time:61997ms step_avg:95.67ms
step:659/1770 train_time:62096ms step_avg:95.68ms
step:660/1770 train_time:62195ms step_avg:95.69ms
step:661/1770 train_time:62294ms step_avg:95.69ms
step:662/1770 train_time:62393ms step_avg:95.70ms
step:663/1770 train_time:62493ms step_avg:95.70ms
step:664/1770 train_time:62592ms step_avg:95.71ms
step:665/1770 train_time:62692ms step_avg:95.71ms
step:666/1770 train_time:62792ms step_avg:95.72ms
step:667/1770 train_time:62891ms step_avg:95.73ms
step:668/1770 train_time:62991ms step_avg:95.73ms
step:669/1770 train_time:63091ms step_avg:95.74ms
step:670/1770 train_time:63191ms step_avg:95.74ms
step:671/1770 train_time:63291ms step_avg:95.75ms
step:672/1770 train_time:63390ms step_avg:95.76ms
step:673/1770 train_time:63489ms step_avg:95.76ms
step:674/1770 train_time:63588ms step_avg:95.77ms
step:675/1770 train_time:63688ms step_avg:95.77ms
step:676/1770 train_time:63786ms step_avg:95.78ms
step:677/1770 train_time:63885ms step_avg:95.78ms
step:678/1770 train_time:63985ms step_avg:95.79ms
step:679/1770 train_time:64084ms step_avg:95.79ms
step:680/1770 train_time:64184ms step_avg:95.80ms
step:681/1770 train_time:64283ms step_avg:95.80ms
step:682/1770 train_time:64383ms step_avg:95.81ms
step:683/1770 train_time:64482ms step_avg:95.81ms
step:684/1770 train_time:64581ms step_avg:95.82ms
step:685/1770 train_time:64679ms step_avg:95.82ms
step:686/1770 train_time:64778ms step_avg:95.83ms
step:687/1770 train_time:64877ms step_avg:95.83ms
step:688/1770 train_time:64976ms step_avg:95.84ms
step:689/1770 train_time:65075ms step_avg:95.84ms
step:690/1770 train_time:65175ms step_avg:95.84ms
step:691/1770 train_time:65274ms step_avg:95.85ms
step:692/1770 train_time:65373ms step_avg:95.85ms
step:693/1770 train_time:65472ms step_avg:95.86ms
step:694/1770 train_time:65572ms step_avg:95.87ms
step:695/1770 train_time:65671ms step_avg:95.87ms
step:696/1770 train_time:65771ms step_avg:95.88ms
step:697/1770 train_time:65870ms step_avg:95.88ms
step:698/1770 train_time:65970ms step_avg:95.89ms
step:699/1770 train_time:66069ms step_avg:95.89ms
step:700/1770 train_time:66169ms step_avg:95.90ms
step:701/1770 train_time:66268ms step_avg:95.90ms
step:702/1770 train_time:66368ms step_avg:95.91ms
step:703/1770 train_time:66467ms step_avg:95.91ms
step:704/1770 train_time:66566ms step_avg:95.92ms
step:705/1770 train_time:66665ms step_avg:95.92ms
step:706/1770 train_time:66764ms step_avg:95.92ms
step:707/1770 train_time:66863ms step_avg:95.93ms
step:708/1770 train_time:66962ms step_avg:95.93ms
step:709/1770 train_time:67061ms step_avg:95.94ms
step:710/1770 train_time:67160ms step_avg:95.94ms
step:711/1770 train_time:67259ms step_avg:95.95ms
step:712/1770 train_time:67358ms step_avg:95.95ms
step:713/1770 train_time:67457ms step_avg:95.96ms
step:714/1770 train_time:67557ms step_avg:95.96ms
step:715/1770 train_time:67655ms step_avg:95.97ms
step:716/1770 train_time:67754ms step_avg:95.97ms
step:717/1770 train_time:67853ms step_avg:95.97ms
step:718/1770 train_time:67952ms step_avg:95.98ms
step:719/1770 train_time:68052ms step_avg:95.98ms
step:720/1770 train_time:68151ms step_avg:95.99ms
step:721/1770 train_time:68252ms step_avg:95.99ms
step:722/1770 train_time:68350ms step_avg:96.00ms
step:723/1770 train_time:68449ms step_avg:96.00ms
step:724/1770 train_time:68548ms step_avg:96.01ms
step:725/1770 train_time:68648ms step_avg:96.01ms
step:726/1770 train_time:68747ms step_avg:96.02ms
step:727/1770 train_time:68846ms step_avg:96.02ms
step:728/1770 train_time:68945ms step_avg:96.02ms
step:729/1770 train_time:69044ms step_avg:96.03ms
step:730/1770 train_time:69143ms step_avg:96.03ms
step:731/1770 train_time:69242ms step_avg:96.04ms
step:732/1770 train_time:69341ms step_avg:96.04ms
step:733/1770 train_time:69439ms step_avg:96.04ms
step:734/1770 train_time:69538ms step_avg:96.05ms
step:735/1770 train_time:69637ms step_avg:96.05ms
step:736/1770 train_time:69737ms step_avg:96.06ms
step:737/1770 train_time:69836ms step_avg:96.06ms
step:738/1770 train_time:69935ms step_avg:96.06ms
step:739/1770 train_time:70034ms step_avg:96.07ms
step:740/1770 train_time:70133ms step_avg:96.07ms
step:741/1770 train_time:70232ms step_avg:96.08ms
step:742/1770 train_time:70332ms step_avg:96.08ms
step:743/1770 train_time:70432ms step_avg:96.09ms
step:744/1770 train_time:70531ms step_avg:96.09ms
step:745/1770 train_time:70630ms step_avg:96.10ms
step:746/1770 train_time:70730ms step_avg:96.10ms
step:747/1770 train_time:70829ms step_avg:96.10ms
step:748/1770 train_time:70928ms step_avg:96.11ms
step:749/1770 train_time:71027ms step_avg:96.11ms
step:750/1770 train_time:71127ms step_avg:96.12ms
step:750/1770 val_loss:3.6061 train_time:71225ms step_avg:96.25ms
step:751/1770 train_time:71246ms step_avg:96.15ms
step:752/1770 train_time:71336ms step_avg:96.14ms
step:753/1770 train_time:71440ms step_avg:96.15ms
step:754/1770 train_time:71538ms step_avg:96.15ms
step:755/1770 train_time:71636ms step_avg:96.16ms
step:756/1770 train_time:71735ms step_avg:96.16ms
step:757/1770 train_time:71834ms step_avg:96.16ms
step:758/1770 train_time:71932ms step_avg:96.17ms
step:759/1770 train_time:72031ms step_avg:96.17ms
step:760/1770 train_time:72129ms step_avg:96.17ms
step:761/1770 train_time:72228ms step_avg:96.18ms
step:762/1770 train_time:72328ms step_avg:96.18ms
step:763/1770 train_time:72429ms step_avg:96.19ms
step:764/1770 train_time:72528ms step_avg:96.19ms
step:765/1770 train_time:72628ms step_avg:96.20ms
step:766/1770 train_time:72728ms step_avg:96.20ms
step:767/1770 train_time:72827ms step_avg:96.20ms
step:768/1770 train_time:72926ms step_avg:96.21ms
step:769/1770 train_time:73025ms step_avg:96.21ms
step:770/1770 train_time:73123ms step_avg:96.22ms
step:771/1770 train_time:73222ms step_avg:96.22ms
step:772/1770 train_time:73320ms step_avg:96.22ms
step:773/1770 train_time:73420ms step_avg:96.22ms
step:774/1770 train_time:73519ms step_avg:96.23ms
step:775/1770 train_time:73618ms step_avg:96.23ms
step:776/1770 train_time:73716ms step_avg:96.24ms
step:777/1770 train_time:73816ms step_avg:96.24ms
step:778/1770 train_time:73916ms step_avg:96.24ms
step:779/1770 train_time:74015ms step_avg:96.25ms
step:780/1770 train_time:74115ms step_avg:96.25ms
step:781/1770 train_time:74214ms step_avg:96.26ms
step:782/1770 train_time:74313ms step_avg:96.26ms
step:783/1770 train_time:74413ms step_avg:96.26ms
step:784/1770 train_time:74512ms step_avg:96.27ms
step:785/1770 train_time:74612ms step_avg:96.27ms
step:786/1770 train_time:74711ms step_avg:96.28ms
step:787/1770 train_time:74810ms step_avg:96.28ms
step:788/1770 train_time:74909ms step_avg:96.28ms
step:789/1770 train_time:75008ms step_avg:96.29ms
step:790/1770 train_time:75108ms step_avg:96.29ms
step:791/1770 train_time:75208ms step_avg:96.30ms
step:792/1770 train_time:75307ms step_avg:96.30ms
step:793/1770 train_time:75407ms step_avg:96.30ms
step:794/1770 train_time:75506ms step_avg:96.31ms
step:795/1770 train_time:75608ms step_avg:96.32ms
step:796/1770 train_time:75707ms step_avg:96.32ms
step:797/1770 train_time:75806ms step_avg:96.32ms
step:798/1770 train_time:75905ms step_avg:96.33ms
step:799/1770 train_time:76004ms step_avg:96.33ms
step:800/1770 train_time:76103ms step_avg:96.33ms
step:801/1770 train_time:76202ms step_avg:96.34ms
step:802/1770 train_time:76301ms step_avg:96.34ms
step:803/1770 train_time:76400ms step_avg:96.34ms
step:804/1770 train_time:76500ms step_avg:96.35ms
step:805/1770 train_time:76600ms step_avg:96.35ms
step:806/1770 train_time:76700ms step_avg:96.36ms
step:807/1770 train_time:76800ms step_avg:96.36ms
step:808/1770 train_time:76899ms step_avg:96.37ms
step:809/1770 train_time:76999ms step_avg:96.37ms
step:810/1770 train_time:77098ms step_avg:96.37ms
step:811/1770 train_time:77197ms step_avg:96.38ms
step:812/1770 train_time:77296ms step_avg:96.38ms
step:813/1770 train_time:77396ms step_avg:96.38ms
step:814/1770 train_time:77496ms step_avg:96.39ms
step:815/1770 train_time:77595ms step_avg:96.39ms
step:816/1770 train_time:77696ms step_avg:96.40ms
step:817/1770 train_time:77796ms step_avg:96.40ms
step:818/1770 train_time:77895ms step_avg:96.40ms
step:819/1770 train_time:77995ms step_avg:96.41ms
step:820/1770 train_time:78095ms step_avg:96.41ms
step:821/1770 train_time:78195ms step_avg:96.42ms
step:822/1770 train_time:78294ms step_avg:96.42ms
step:823/1770 train_time:78393ms step_avg:96.42ms
step:824/1770 train_time:78493ms step_avg:96.43ms
step:825/1770 train_time:78592ms step_avg:96.43ms
step:826/1770 train_time:78691ms step_avg:96.43ms
step:827/1770 train_time:78790ms step_avg:96.44ms
step:828/1770 train_time:78890ms step_avg:96.44ms
step:829/1770 train_time:78990ms step_avg:96.45ms
step:830/1770 train_time:79089ms step_avg:96.45ms
step:831/1770 train_time:79188ms step_avg:96.45ms
step:832/1770 train_time:79288ms step_avg:96.46ms
step:833/1770 train_time:79387ms step_avg:96.46ms
step:834/1770 train_time:79487ms step_avg:96.47ms
step:835/1770 train_time:79586ms step_avg:96.47ms
step:836/1770 train_time:79686ms step_avg:96.47ms
step:837/1770 train_time:79785ms step_avg:96.48ms
step:838/1770 train_time:79885ms step_avg:96.48ms
step:839/1770 train_time:79984ms step_avg:96.48ms
step:840/1770 train_time:80083ms step_avg:96.49ms
step:841/1770 train_time:80182ms step_avg:96.49ms
step:842/1770 train_time:80281ms step_avg:96.49ms
step:843/1770 train_time:80381ms step_avg:96.50ms
step:844/1770 train_time:80481ms step_avg:96.50ms
step:845/1770 train_time:80581ms step_avg:96.50ms
step:846/1770 train_time:80681ms step_avg:96.51ms
step:847/1770 train_time:80780ms step_avg:96.51ms
step:848/1770 train_time:80879ms step_avg:96.51ms
step:849/1770 train_time:80979ms step_avg:96.52ms
step:850/1770 train_time:81078ms step_avg:96.52ms
step:851/1770 train_time:81177ms step_avg:96.52ms
step:852/1770 train_time:81276ms step_avg:96.53ms
step:853/1770 train_time:81376ms step_avg:96.53ms
step:854/1770 train_time:81474ms step_avg:96.53ms
step:855/1770 train_time:81575ms step_avg:96.54ms
step:856/1770 train_time:81674ms step_avg:96.54ms
step:857/1770 train_time:81775ms step_avg:96.55ms
step:858/1770 train_time:81875ms step_avg:96.55ms
step:859/1770 train_time:81975ms step_avg:96.55ms
step:860/1770 train_time:82074ms step_avg:96.56ms
step:861/1770 train_time:82174ms step_avg:96.56ms
step:862/1770 train_time:82274ms step_avg:96.57ms
step:863/1770 train_time:82373ms step_avg:96.57ms
step:864/1770 train_time:82473ms step_avg:96.57ms
step:865/1770 train_time:82572ms step_avg:96.58ms
step:866/1770 train_time:82673ms step_avg:96.58ms
step:867/1770 train_time:82773ms step_avg:96.58ms
step:868/1770 train_time:82872ms step_avg:96.59ms
step:869/1770 train_time:82972ms step_avg:96.59ms
step:870/1770 train_time:83073ms step_avg:96.60ms
step:871/1770 train_time:83172ms step_avg:96.60ms
step:872/1770 train_time:83271ms step_avg:96.60ms
step:873/1770 train_time:83370ms step_avg:96.61ms
step:874/1770 train_time:83469ms step_avg:96.61ms
step:875/1770 train_time:83569ms step_avg:96.61ms
step:875/1770 val_loss:3.5562 train_time:83666ms step_avg:96.72ms
step:876/1770 train_time:83687ms step_avg:96.64ms
step:877/1770 train_time:83776ms step_avg:96.63ms
step:878/1770 train_time:83877ms step_avg:96.63ms
step:879/1770 train_time:83977ms step_avg:96.64ms
step:880/1770 train_time:84076ms step_avg:96.64ms
step:881/1770 train_time:84175ms step_avg:96.64ms
step:882/1770 train_time:84273ms step_avg:96.64ms
step:883/1770 train_time:84372ms step_avg:96.65ms
step:884/1770 train_time:84472ms step_avg:96.65ms
step:885/1770 train_time:84570ms step_avg:96.65ms
step:886/1770 train_time:84671ms step_avg:96.66ms
step:887/1770 train_time:84772ms step_avg:96.66ms
step:888/1770 train_time:84872ms step_avg:96.67ms
step:889/1770 train_time:84973ms step_avg:96.67ms
step:890/1770 train_time:85073ms step_avg:96.67ms
step:891/1770 train_time:85172ms step_avg:96.68ms
step:892/1770 train_time:85271ms step_avg:96.68ms
step:893/1770 train_time:85370ms step_avg:96.68ms
step:894/1770 train_time:85469ms step_avg:96.68ms
step:895/1770 train_time:85569ms step_avg:96.69ms
step:896/1770 train_time:85668ms step_avg:96.69ms
step:897/1770 train_time:85767ms step_avg:96.69ms
step:898/1770 train_time:85866ms step_avg:96.70ms
step:899/1770 train_time:85965ms step_avg:96.70ms
step:900/1770 train_time:86065ms step_avg:96.70ms
step:901/1770 train_time:86164ms step_avg:96.71ms
step:902/1770 train_time:86264ms step_avg:96.71ms
step:903/1770 train_time:86363ms step_avg:96.71ms
step:904/1770 train_time:86463ms step_avg:96.71ms
step:905/1770 train_time:86562ms step_avg:96.72ms
step:906/1770 train_time:86662ms step_avg:96.72ms
step:907/1770 train_time:86761ms step_avg:96.72ms
step:908/1770 train_time:86860ms step_avg:96.73ms
step:909/1770 train_time:86960ms step_avg:96.73ms
step:910/1770 train_time:87059ms step_avg:96.73ms
step:911/1770 train_time:87158ms step_avg:96.73ms
step:912/1770 train_time:87258ms step_avg:96.74ms
step:913/1770 train_time:87358ms step_avg:96.74ms
step:914/1770 train_time:87457ms step_avg:96.74ms
step:915/1770 train_time:87558ms step_avg:96.75ms
step:916/1770 train_time:87657ms step_avg:96.75ms
step:917/1770 train_time:87757ms step_avg:96.76ms
step:918/1770 train_time:87857ms step_avg:96.76ms
step:919/1770 train_time:87956ms step_avg:96.76ms
step:920/1770 train_time:88058ms step_avg:96.77ms
step:921/1770 train_time:88159ms step_avg:96.77ms
step:922/1770 train_time:88259ms step_avg:96.78ms
step:923/1770 train_time:88360ms step_avg:96.78ms
step:924/1770 train_time:88461ms step_avg:96.78ms
step:925/1770 train_time:88561ms step_avg:96.79ms
step:926/1770 train_time:88661ms step_avg:96.79ms
step:927/1770 train_time:88763ms step_avg:96.80ms
step:928/1770 train_time:88865ms step_avg:96.80ms
step:929/1770 train_time:88965ms step_avg:96.81ms
step:930/1770 train_time:89066ms step_avg:96.81ms
step:931/1770 train_time:89166ms step_avg:96.81ms
step:932/1770 train_time:89267ms step_avg:96.82ms
step:933/1770 train_time:89367ms step_avg:96.82ms
step:934/1770 train_time:89469ms step_avg:96.83ms
step:935/1770 train_time:89571ms step_avg:96.83ms
step:936/1770 train_time:89673ms step_avg:96.84ms
step:937/1770 train_time:89774ms step_avg:96.84ms
step:938/1770 train_time:89874ms step_avg:96.85ms
step:939/1770 train_time:89975ms step_avg:96.85ms
step:940/1770 train_time:90076ms step_avg:96.86ms
step:941/1770 train_time:90178ms step_avg:96.86ms
step:942/1770 train_time:90280ms step_avg:96.87ms
step:943/1770 train_time:90381ms step_avg:96.87ms
step:944/1770 train_time:90481ms step_avg:96.87ms
step:945/1770 train_time:90581ms step_avg:96.88ms
step:946/1770 train_time:90682ms step_avg:96.88ms
step:947/1770 train_time:90783ms step_avg:96.89ms
step:948/1770 train_time:90884ms step_avg:96.89ms
step:949/1770 train_time:90985ms step_avg:96.90ms
step:950/1770 train_time:91086ms step_avg:96.90ms
step:951/1770 train_time:91187ms step_avg:96.90ms
step:952/1770 train_time:91288ms step_avg:96.91ms
step:953/1770 train_time:91389ms step_avg:96.91ms
step:954/1770 train_time:91489ms step_avg:96.92ms
step:955/1770 train_time:91590ms step_avg:96.92ms
step:956/1770 train_time:91691ms step_avg:96.92ms
step:957/1770 train_time:91791ms step_avg:96.93ms
step:958/1770 train_time:91892ms step_avg:96.93ms
step:959/1770 train_time:91993ms step_avg:96.94ms
step:960/1770 train_time:92093ms step_avg:96.94ms
step:961/1770 train_time:92194ms step_avg:96.94ms
step:962/1770 train_time:92295ms step_avg:96.95ms
step:963/1770 train_time:92396ms step_avg:96.95ms
step:964/1770 train_time:92498ms step_avg:96.96ms
step:965/1770 train_time:92600ms step_avg:96.96ms
step:966/1770 train_time:92700ms step_avg:96.97ms
step:967/1770 train_time:92801ms step_avg:96.97ms
step:968/1770 train_time:92902ms step_avg:96.97ms
step:969/1770 train_time:93002ms step_avg:96.98ms
step:970/1770 train_time:93103ms step_avg:96.98ms
step:971/1770 train_time:93204ms step_avg:96.99ms
step:972/1770 train_time:93305ms step_avg:96.99ms
step:973/1770 train_time:93405ms step_avg:96.99ms
step:974/1770 train_time:93506ms step_avg:97.00ms
step:975/1770 train_time:93608ms step_avg:97.00ms
step:976/1770 train_time:93709ms step_avg:97.01ms
step:977/1770 train_time:93809ms step_avg:97.01ms
step:978/1770 train_time:93910ms step_avg:97.01ms
step:979/1770 train_time:94010ms step_avg:97.02ms
step:980/1770 train_time:94110ms step_avg:97.02ms
step:981/1770 train_time:94211ms step_avg:97.02ms
step:982/1770 train_time:94312ms step_avg:97.03ms
step:983/1770 train_time:94412ms step_avg:97.03ms
step:984/1770 train_time:94515ms step_avg:97.04ms
step:985/1770 train_time:94616ms step_avg:97.04ms
step:986/1770 train_time:94717ms step_avg:97.05ms
step:987/1770 train_time:94818ms step_avg:97.05ms
step:988/1770 train_time:94918ms step_avg:97.05ms
step:989/1770 train_time:95019ms step_avg:97.06ms
step:990/1770 train_time:95120ms step_avg:97.06ms
step:991/1770 train_time:95221ms step_avg:97.07ms
step:992/1770 train_time:95322ms step_avg:97.07ms
step:993/1770 train_time:95424ms step_avg:97.07ms
step:994/1770 train_time:95527ms step_avg:97.08ms
step:995/1770 train_time:95628ms step_avg:97.08ms
step:996/1770 train_time:95727ms step_avg:97.09ms
step:997/1770 train_time:95827ms step_avg:97.09ms
step:998/1770 train_time:95928ms step_avg:97.09ms
step:999/1770 train_time:96028ms step_avg:97.10ms
step:1000/1770 train_time:96130ms step_avg:97.10ms
step:1000/1770 val_loss:3.5176 train_time:96229ms step_avg:97.20ms
step:1001/1770 train_time:96251ms step_avg:97.12ms
step:1002/1770 train_time:96342ms step_avg:97.12ms
step:1003/1770 train_time:96446ms step_avg:97.13ms
step:1004/1770 train_time:96547ms step_avg:97.13ms
step:1005/1770 train_time:96646ms step_avg:97.13ms
step:1006/1770 train_time:96746ms step_avg:97.13ms
step:1007/1770 train_time:96846ms step_avg:97.14ms
step:1008/1770 train_time:96946ms step_avg:97.14ms
step:1009/1770 train_time:97046ms step_avg:97.14ms
step:1010/1770 train_time:97147ms step_avg:97.15ms
step:1011/1770 train_time:97249ms step_avg:97.15ms
step:1012/1770 train_time:97352ms step_avg:97.16ms
step:1013/1770 train_time:97454ms step_avg:97.16ms
step:1014/1770 train_time:97555ms step_avg:97.17ms
step:1015/1770 train_time:97655ms step_avg:97.17ms
step:1016/1770 train_time:97755ms step_avg:97.17ms
step:1017/1770 train_time:97856ms step_avg:97.18ms
step:1018/1770 train_time:97955ms step_avg:97.18ms
step:1019/1770 train_time:98056ms step_avg:97.18ms
step:1020/1770 train_time:98157ms step_avg:97.19ms
step:1021/1770 train_time:98259ms step_avg:97.19ms
step:1022/1770 train_time:98360ms step_avg:97.19ms
step:1023/1770 train_time:98461ms step_avg:97.20ms
step:1024/1770 train_time:98563ms step_avg:97.20ms
step:1025/1770 train_time:98663ms step_avg:97.21ms
step:1026/1770 train_time:98765ms step_avg:97.21ms
step:1027/1770 train_time:98867ms step_avg:97.21ms
step:1028/1770 train_time:98969ms step_avg:97.22ms
step:1029/1770 train_time:99069ms step_avg:97.22ms
step:1030/1770 train_time:99170ms step_avg:97.23ms
step:1031/1770 train_time:99270ms step_avg:97.23ms
step:1032/1770 train_time:99371ms step_avg:97.23ms
step:1033/1770 train_time:99472ms step_avg:97.24ms
step:1034/1770 train_time:99573ms step_avg:97.24ms
step:1035/1770 train_time:99674ms step_avg:97.24ms
step:1036/1770 train_time:99774ms step_avg:97.25ms
step:1037/1770 train_time:99875ms step_avg:97.25ms
step:1038/1770 train_time:99976ms step_avg:97.25ms
step:1039/1770 train_time:100077ms step_avg:97.26ms
step:1040/1770 train_time:100178ms step_avg:97.26ms
step:1041/1770 train_time:100278ms step_avg:97.26ms
step:1042/1770 train_time:100380ms step_avg:97.27ms
step:1043/1770 train_time:100481ms step_avg:97.27ms
step:1044/1770 train_time:100581ms step_avg:97.27ms
step:1045/1770 train_time:100682ms step_avg:97.28ms
step:1046/1770 train_time:100782ms step_avg:97.28ms
step:1047/1770 train_time:100883ms step_avg:97.28ms
step:1048/1770 train_time:100983ms step_avg:97.29ms
step:1049/1770 train_time:101084ms step_avg:97.29ms
step:1050/1770 train_time:101185ms step_avg:97.29ms
step:1051/1770 train_time:101287ms step_avg:97.30ms
step:1052/1770 train_time:101388ms step_avg:97.30ms
step:1053/1770 train_time:101488ms step_avg:97.30ms
step:1054/1770 train_time:101588ms step_avg:97.31ms
step:1055/1770 train_time:101689ms step_avg:97.31ms
step:1056/1770 train_time:101789ms step_avg:97.31ms
step:1057/1770 train_time:101891ms step_avg:97.32ms
step:1058/1770 train_time:101992ms step_avg:97.32ms
step:1059/1770 train_time:102094ms step_avg:97.32ms
step:1060/1770 train_time:102195ms step_avg:97.33ms
step:1061/1770 train_time:102296ms step_avg:97.33ms
step:1062/1770 train_time:102400ms step_avg:97.34ms
step:1063/1770 train_time:102501ms step_avg:97.34ms
step:1064/1770 train_time:102603ms step_avg:97.35ms
step:1065/1770 train_time:102703ms step_avg:97.35ms
step:1066/1770 train_time:102805ms step_avg:97.35ms
step:1067/1770 train_time:102905ms step_avg:97.36ms
step:1068/1770 train_time:103007ms step_avg:97.36ms
step:1069/1770 train_time:103109ms step_avg:97.36ms
step:1070/1770 train_time:103209ms step_avg:97.37ms
step:1071/1770 train_time:103310ms step_avg:97.37ms
step:1072/1770 train_time:103412ms step_avg:97.37ms
step:1073/1770 train_time:103513ms step_avg:97.38ms
step:1074/1770 train_time:103614ms step_avg:97.38ms
step:1075/1770 train_time:103715ms step_avg:97.39ms
step:1076/1770 train_time:103816ms step_avg:97.39ms
step:1077/1770 train_time:103918ms step_avg:97.39ms
step:1078/1770 train_time:104018ms step_avg:97.40ms
step:1079/1770 train_time:104120ms step_avg:97.40ms
step:1080/1770 train_time:104220ms step_avg:97.40ms
step:1081/1770 train_time:104321ms step_avg:97.41ms
step:1082/1770 train_time:104422ms step_avg:97.41ms
step:1083/1770 train_time:104524ms step_avg:97.41ms
step:1084/1770 train_time:104625ms step_avg:97.42ms
step:1085/1770 train_time:104727ms step_avg:97.42ms
step:1086/1770 train_time:104829ms step_avg:97.42ms
step:1087/1770 train_time:104929ms step_avg:97.43ms
step:1088/1770 train_time:105030ms step_avg:97.43ms
step:1089/1770 train_time:105130ms step_avg:97.43ms
step:1090/1770 train_time:105233ms step_avg:97.44ms
step:1091/1770 train_time:105334ms step_avg:97.44ms
step:1092/1770 train_time:105434ms step_avg:97.44ms
step:1093/1770 train_time:105535ms step_avg:97.45ms
step:1094/1770 train_time:105636ms step_avg:97.45ms
step:1095/1770 train_time:105738ms step_avg:97.45ms
step:1096/1770 train_time:105839ms step_avg:97.46ms
step:1097/1770 train_time:105940ms step_avg:97.46ms
step:1098/1770 train_time:106041ms step_avg:97.46ms
step:1099/1770 train_time:106143ms step_avg:97.47ms
step:1100/1770 train_time:106245ms step_avg:97.47ms
step:1101/1770 train_time:106346ms step_avg:97.48ms
step:1102/1770 train_time:106446ms step_avg:97.48ms
step:1103/1770 train_time:106548ms step_avg:97.48ms
step:1104/1770 train_time:106648ms step_avg:97.48ms
step:1105/1770 train_time:106749ms step_avg:97.49ms
step:1106/1770 train_time:106849ms step_avg:97.49ms
step:1107/1770 train_time:106951ms step_avg:97.49ms
step:1108/1770 train_time:107053ms step_avg:97.50ms
step:1109/1770 train_time:107154ms step_avg:97.50ms
step:1110/1770 train_time:107256ms step_avg:97.51ms
step:1111/1770 train_time:107357ms step_avg:97.51ms
step:1112/1770 train_time:107458ms step_avg:97.51ms
step:1113/1770 train_time:107559ms step_avg:97.51ms
step:1114/1770 train_time:107660ms step_avg:97.52ms
step:1115/1770 train_time:107762ms step_avg:97.52ms
step:1116/1770 train_time:107863ms step_avg:97.53ms
step:1117/1770 train_time:107966ms step_avg:97.53ms
step:1118/1770 train_time:108067ms step_avg:97.53ms
step:1119/1770 train_time:108169ms step_avg:97.54ms
step:1120/1770 train_time:108271ms step_avg:97.54ms
step:1121/1770 train_time:108371ms step_avg:97.54ms
step:1122/1770 train_time:108472ms step_avg:97.55ms
step:1123/1770 train_time:108573ms step_avg:97.55ms
step:1124/1770 train_time:108676ms step_avg:97.55ms
step:1125/1770 train_time:108776ms step_avg:97.56ms
step:1125/1770 val_loss:3.4761 train_time:108875ms step_avg:97.65ms
step:1126/1770 train_time:108897ms step_avg:97.58ms
step:1127/1770 train_time:108987ms step_avg:97.57ms
step:1128/1770 train_time:109091ms step_avg:97.58ms
step:1129/1770 train_time:109191ms step_avg:97.58ms
step:1130/1770 train_time:109292ms step_avg:97.58ms
step:1131/1770 train_time:109392ms step_avg:97.58ms
step:1132/1770 train_time:109493ms step_avg:97.59ms
step:1133/1770 train_time:109593ms step_avg:97.59ms
step:1134/1770 train_time:109694ms step_avg:97.59ms
step:1135/1770 train_time:109794ms step_avg:97.59ms
step:1136/1770 train_time:109897ms step_avg:97.60ms
step:1137/1770 train_time:110000ms step_avg:97.60ms
step:1138/1770 train_time:110101ms step_avg:97.61ms
step:1139/1770 train_time:110201ms step_avg:97.61ms
step:1140/1770 train_time:110303ms step_avg:97.61ms
step:1141/1770 train_time:110404ms step_avg:97.62ms
step:1142/1770 train_time:110505ms step_avg:97.62ms
step:1143/1770 train_time:110605ms step_avg:97.62ms
step:1144/1770 train_time:110706ms step_avg:97.62ms
step:1145/1770 train_time:110807ms step_avg:97.63ms
step:1146/1770 train_time:110908ms step_avg:97.63ms
step:1147/1770 train_time:111010ms step_avg:97.63ms
step:1148/1770 train_time:111111ms step_avg:97.64ms
step:1149/1770 train_time:111212ms step_avg:97.64ms
step:1150/1770 train_time:111313ms step_avg:97.64ms
step:1151/1770 train_time:111414ms step_avg:97.65ms
step:1152/1770 train_time:111515ms step_avg:97.65ms
step:1153/1770 train_time:111616ms step_avg:97.65ms
step:1154/1770 train_time:111718ms step_avg:97.66ms
step:1155/1770 train_time:111818ms step_avg:97.66ms
step:1156/1770 train_time:111919ms step_avg:97.66ms
step:1157/1770 train_time:112023ms step_avg:97.67ms
step:1158/1770 train_time:112124ms step_avg:97.67ms
step:1159/1770 train_time:112225ms step_avg:97.67ms
step:1160/1770 train_time:112325ms step_avg:97.67ms
step:1161/1770 train_time:112426ms step_avg:97.68ms
step:1162/1770 train_time:112528ms step_avg:97.68ms
step:1163/1770 train_time:112630ms step_avg:97.68ms
step:1164/1770 train_time:112730ms step_avg:97.69ms
step:1165/1770 train_time:112832ms step_avg:97.69ms
step:1166/1770 train_time:112933ms step_avg:97.69ms
step:1167/1770 train_time:113033ms step_avg:97.70ms
step:1168/1770 train_time:113135ms step_avg:97.70ms
step:1169/1770 train_time:113236ms step_avg:97.70ms
step:1170/1770 train_time:113337ms step_avg:97.70ms
step:1171/1770 train_time:113438ms step_avg:97.71ms
step:1172/1770 train_time:113539ms step_avg:97.71ms
step:1173/1770 train_time:113641ms step_avg:97.71ms
step:1174/1770 train_time:113743ms step_avg:97.72ms
step:1175/1770 train_time:113844ms step_avg:97.72ms
step:1176/1770 train_time:113946ms step_avg:97.72ms
step:1177/1770 train_time:114047ms step_avg:97.73ms
step:1178/1770 train_time:114148ms step_avg:97.73ms
step:1179/1770 train_time:114250ms step_avg:97.73ms
step:1180/1770 train_time:114351ms step_avg:97.74ms
step:1181/1770 train_time:114453ms step_avg:97.74ms
step:1182/1770 train_time:114553ms step_avg:97.74ms
step:1183/1770 train_time:114656ms step_avg:97.75ms
step:1184/1770 train_time:114759ms step_avg:97.75ms
step:1185/1770 train_time:114861ms step_avg:97.75ms
step:1186/1770 train_time:114963ms step_avg:97.76ms
step:1187/1770 train_time:115067ms step_avg:97.76ms
step:1188/1770 train_time:115169ms step_avg:97.77ms
step:1189/1770 train_time:115271ms step_avg:97.77ms
step:1190/1770 train_time:115373ms step_avg:97.77ms
step:1191/1770 train_time:115474ms step_avg:97.78ms
step:1192/1770 train_time:115576ms step_avg:97.78ms
step:1193/1770 train_time:115678ms step_avg:97.78ms
step:1194/1770 train_time:115779ms step_avg:97.79ms
step:1195/1770 train_time:115881ms step_avg:97.79ms
step:1196/1770 train_time:115984ms step_avg:97.79ms
step:1197/1770 train_time:116086ms step_avg:97.80ms
step:1198/1770 train_time:116189ms step_avg:97.80ms
step:1199/1770 train_time:116291ms step_avg:97.81ms
step:1200/1770 train_time:116394ms step_avg:97.81ms
step:1201/1770 train_time:116497ms step_avg:97.81ms
step:1202/1770 train_time:116597ms step_avg:97.82ms
step:1203/1770 train_time:116699ms step_avg:97.82ms
step:1204/1770 train_time:116802ms step_avg:97.82ms
step:1205/1770 train_time:116904ms step_avg:97.83ms
step:1206/1770 train_time:117007ms step_avg:97.83ms
step:1207/1770 train_time:117110ms step_avg:97.84ms
step:1208/1770 train_time:117211ms step_avg:97.84ms
step:1209/1770 train_time:117313ms step_avg:97.84ms
step:1210/1770 train_time:117414ms step_avg:97.85ms
step:1211/1770 train_time:117516ms step_avg:97.85ms
step:1212/1770 train_time:117621ms step_avg:97.85ms
step:1213/1770 train_time:117722ms step_avg:97.86ms
step:1214/1770 train_time:117824ms step_avg:97.86ms
step:1215/1770 train_time:117926ms step_avg:97.86ms
step:1216/1770 train_time:118030ms step_avg:97.87ms
step:1217/1770 train_time:118132ms step_avg:97.87ms
step:1218/1770 train_time:118234ms step_avg:97.88ms
step:1219/1770 train_time:118336ms step_avg:97.88ms
step:1220/1770 train_time:118438ms step_avg:97.88ms
step:1221/1770 train_time:118540ms step_avg:97.89ms
step:1222/1770 train_time:118645ms step_avg:97.89ms
step:1223/1770 train_time:118747ms step_avg:97.90ms
step:1224/1770 train_time:118850ms step_avg:97.90ms
step:1225/1770 train_time:118952ms step_avg:97.90ms
step:1226/1770 train_time:119053ms step_avg:97.91ms
step:1227/1770 train_time:119157ms step_avg:97.91ms
step:1228/1770 train_time:119261ms step_avg:97.92ms
step:1229/1770 train_time:119363ms step_avg:97.92ms
step:1230/1770 train_time:119465ms step_avg:97.92ms
step:1231/1770 train_time:119567ms step_avg:97.93ms
step:1232/1770 train_time:119669ms step_avg:97.93ms
step:1233/1770 train_time:119771ms step_avg:97.93ms
step:1234/1770 train_time:119873ms step_avg:97.94ms
step:1235/1770 train_time:119974ms step_avg:97.94ms
step:1236/1770 train_time:120077ms step_avg:97.94ms
step:1237/1770 train_time:120179ms step_avg:97.95ms
step:1238/1770 train_time:120283ms step_avg:97.95ms
step:1239/1770 train_time:120386ms step_avg:97.95ms
step:1240/1770 train_time:120487ms step_avg:97.96ms
step:1241/1770 train_time:120591ms step_avg:97.96ms
step:1242/1770 train_time:120692ms step_avg:97.96ms
step:1243/1770 train_time:120794ms step_avg:97.97ms
step:1244/1770 train_time:120896ms step_avg:97.97ms
step:1245/1770 train_time:120998ms step_avg:97.97ms
step:1246/1770 train_time:121101ms step_avg:97.98ms
step:1247/1770 train_time:121203ms step_avg:97.98ms
step:1248/1770 train_time:121306ms step_avg:97.99ms
step:1249/1770 train_time:121408ms step_avg:97.99ms
step:1250/1770 train_time:121509ms step_avg:97.99ms
step:1250/1770 val_loss:3.4281 train_time:121611ms step_avg:98.07ms
step:1251/1770 train_time:121633ms step_avg:98.01ms
step:1252/1770 train_time:121723ms step_avg:98.01ms
step:1253/1770 train_time:121826ms step_avg:98.01ms
step:1254/1770 train_time:121928ms step_avg:98.01ms
step:1255/1770 train_time:122032ms step_avg:98.02ms
step:1256/1770 train_time:122133ms step_avg:98.02ms
step:1257/1770 train_time:122235ms step_avg:98.02ms
step:1258/1770 train_time:122337ms step_avg:98.03ms
step:1259/1770 train_time:122439ms step_avg:98.03ms
step:1260/1770 train_time:122541ms step_avg:98.03ms
step:1261/1770 train_time:122647ms step_avg:98.04ms
step:1262/1770 train_time:122750ms step_avg:98.04ms
step:1263/1770 train_time:122852ms step_avg:98.05ms
step:1264/1770 train_time:122957ms step_avg:98.05ms
step:1265/1770 train_time:123058ms step_avg:98.05ms
step:1266/1770 train_time:123160ms step_avg:98.06ms
step:1267/1770 train_time:123262ms step_avg:98.06ms
step:1268/1770 train_time:123364ms step_avg:98.06ms
step:1269/1770 train_time:123466ms step_avg:98.07ms
step:1270/1770 train_time:123569ms step_avg:98.07ms
step:1271/1770 train_time:123671ms step_avg:98.07ms
step:1272/1770 train_time:123773ms step_avg:98.08ms
step:1273/1770 train_time:123876ms step_avg:98.08ms
step:1274/1770 train_time:123979ms step_avg:98.08ms
step:1275/1770 train_time:124080ms step_avg:98.09ms
step:1276/1770 train_time:124182ms step_avg:98.09ms
step:1277/1770 train_time:124284ms step_avg:98.09ms
step:1278/1770 train_time:124387ms step_avg:98.10ms
step:1279/1770 train_time:124490ms step_avg:98.10ms
step:1280/1770 train_time:124593ms step_avg:98.10ms
step:1281/1770 train_time:124694ms step_avg:98.11ms
step:1282/1770 train_time:124797ms step_avg:98.11ms
step:1283/1770 train_time:124900ms step_avg:98.12ms
step:1284/1770 train_time:125003ms step_avg:98.12ms
step:1285/1770 train_time:125105ms step_avg:98.12ms
step:1286/1770 train_time:125208ms step_avg:98.13ms
step:1287/1770 train_time:125312ms step_avg:98.13ms
step:1288/1770 train_time:125414ms step_avg:98.13ms
step:1289/1770 train_time:125516ms step_avg:98.14ms
step:1290/1770 train_time:125618ms step_avg:98.14ms
step:1291/1770 train_time:125720ms step_avg:98.14ms
step:1292/1770 train_time:125821ms step_avg:98.14ms
step:1293/1770 train_time:125924ms step_avg:98.15ms
step:1294/1770 train_time:126025ms step_avg:98.15ms
step:1295/1770 train_time:126127ms step_avg:98.15ms
step:1296/1770 train_time:126229ms step_avg:98.16ms
step:1297/1770 train_time:126331ms step_avg:98.16ms
step:1298/1770 train_time:126433ms step_avg:98.16ms
step:1299/1770 train_time:126536ms step_avg:98.17ms
step:1300/1770 train_time:126637ms step_avg:98.17ms
step:1301/1770 train_time:126740ms step_avg:98.17ms
step:1302/1770 train_time:126842ms step_avg:98.18ms
step:1303/1770 train_time:126944ms step_avg:98.18ms
step:1304/1770 train_time:127046ms step_avg:98.18ms
step:1305/1770 train_time:127148ms step_avg:98.18ms
step:1306/1770 train_time:127250ms step_avg:98.19ms
step:1307/1770 train_time:127352ms step_avg:98.19ms
step:1308/1770 train_time:127454ms step_avg:98.19ms
step:1309/1770 train_time:127556ms step_avg:98.20ms
step:1310/1770 train_time:127658ms step_avg:98.20ms
step:1311/1770 train_time:127760ms step_avg:98.20ms
step:1312/1770 train_time:127862ms step_avg:98.20ms
step:1313/1770 train_time:127964ms step_avg:98.21ms
step:1314/1770 train_time:128065ms step_avg:98.21ms
step:1315/1770 train_time:128167ms step_avg:98.21ms
step:1316/1770 train_time:128270ms step_avg:98.22ms
step:1317/1770 train_time:128372ms step_avg:98.22ms
step:1318/1770 train_time:128477ms step_avg:98.22ms
step:1319/1770 train_time:128580ms step_avg:98.23ms
step:1320/1770 train_time:128682ms step_avg:98.23ms
step:1321/1770 train_time:128784ms step_avg:98.23ms
step:1322/1770 train_time:128886ms step_avg:98.24ms
step:1323/1770 train_time:128989ms step_avg:98.24ms
step:1324/1770 train_time:129092ms step_avg:98.24ms
step:1325/1770 train_time:129195ms step_avg:98.25ms
step:1326/1770 train_time:129297ms step_avg:98.25ms
step:1327/1770 train_time:129402ms step_avg:98.26ms
step:1328/1770 train_time:129504ms step_avg:98.26ms
step:1329/1770 train_time:129605ms step_avg:98.26ms
step:1330/1770 train_time:129706ms step_avg:98.26ms
step:1331/1770 train_time:129808ms step_avg:98.26ms
step:1332/1770 train_time:129909ms step_avg:98.27ms
step:1333/1770 train_time:130010ms step_avg:98.27ms
step:1334/1770 train_time:130113ms step_avg:98.27ms
step:1335/1770 train_time:130216ms step_avg:98.28ms
step:1336/1770 train_time:130319ms step_avg:98.28ms
step:1337/1770 train_time:130421ms step_avg:98.28ms
step:1338/1770 train_time:130523ms step_avg:98.29ms
step:1339/1770 train_time:130625ms step_avg:98.29ms
step:1340/1770 train_time:130729ms step_avg:98.29ms
step:1341/1770 train_time:130830ms step_avg:98.29ms
step:1342/1770 train_time:130933ms step_avg:98.30ms
step:1343/1770 train_time:131036ms step_avg:98.30ms
step:1344/1770 train_time:131138ms step_avg:98.30ms
step:1345/1770 train_time:131240ms step_avg:98.31ms
step:1346/1770 train_time:131342ms step_avg:98.31ms
step:1347/1770 train_time:131444ms step_avg:98.31ms
step:1348/1770 train_time:131549ms step_avg:98.32ms
step:1349/1770 train_time:131651ms step_avg:98.32ms
step:1350/1770 train_time:131753ms step_avg:98.32ms
step:1351/1770 train_time:131855ms step_avg:98.33ms
step:1352/1770 train_time:131957ms step_avg:98.33ms
step:1353/1770 train_time:132060ms step_avg:98.33ms
step:1354/1770 train_time:132162ms step_avg:98.33ms
step:1355/1770 train_time:132264ms step_avg:98.34ms
step:1356/1770 train_time:132366ms step_avg:98.34ms
step:1357/1770 train_time:132469ms step_avg:98.34ms
step:1358/1770 train_time:132573ms step_avg:98.35ms
step:1359/1770 train_time:132675ms step_avg:98.35ms
step:1360/1770 train_time:132778ms step_avg:98.35ms
step:1361/1770 train_time:132880ms step_avg:98.36ms
step:1362/1770 train_time:132982ms step_avg:98.36ms
step:1363/1770 train_time:133085ms step_avg:98.36ms
step:1364/1770 train_time:133187ms step_avg:98.37ms
step:1365/1770 train_time:133288ms step_avg:98.37ms
step:1366/1770 train_time:133390ms step_avg:98.37ms
step:1367/1770 train_time:133493ms step_avg:98.37ms
step:1368/1770 train_time:133595ms step_avg:98.38ms
step:1369/1770 train_time:133698ms step_avg:98.38ms
step:1370/1770 train_time:133800ms step_avg:98.38ms
step:1371/1770 train_time:133902ms step_avg:98.38ms
step:1372/1770 train_time:134004ms step_avg:98.39ms
step:1373/1770 train_time:134107ms step_avg:98.39ms
step:1374/1770 train_time:134210ms step_avg:98.39ms
step:1375/1770 train_time:134312ms step_avg:98.40ms
step:1375/1770 val_loss:3.3841 train_time:134413ms step_avg:98.47ms
step:1376/1770 train_time:134434ms step_avg:98.41ms
step:1377/1770 train_time:134522ms step_avg:98.41ms
step:1378/1770 train_time:134624ms step_avg:98.41ms
step:1379/1770 train_time:134726ms step_avg:98.41ms
step:1380/1770 train_time:134827ms step_avg:98.41ms
step:1381/1770 train_time:134929ms step_avg:98.42ms
step:1382/1770 train_time:135031ms step_avg:98.42ms
step:1383/1770 train_time:135134ms step_avg:98.42ms
step:1384/1770 train_time:135236ms step_avg:98.42ms
step:1385/1770 train_time:135338ms step_avg:98.43ms
step:1386/1770 train_time:135441ms step_avg:98.43ms
step:1387/1770 train_time:135545ms step_avg:98.43ms
step:1388/1770 train_time:135646ms step_avg:98.44ms
step:1389/1770 train_time:135748ms step_avg:98.44ms
step:1390/1770 train_time:135851ms step_avg:98.44ms
step:1391/1770 train_time:135952ms step_avg:98.44ms
step:1392/1770 train_time:136055ms step_avg:98.45ms
step:1393/1770 train_time:136156ms step_avg:98.45ms
step:1394/1770 train_time:136258ms step_avg:98.45ms
step:1395/1770 train_time:136361ms step_avg:98.46ms
step:1396/1770 train_time:136465ms step_avg:98.46ms
step:1397/1770 train_time:136568ms step_avg:98.46ms
step:1398/1770 train_time:136670ms step_avg:98.47ms
step:1399/1770 train_time:136773ms step_avg:98.47ms
step:1400/1770 train_time:136875ms step_avg:98.47ms
step:1401/1770 train_time:136977ms step_avg:98.47ms
step:1402/1770 train_time:137079ms step_avg:98.48ms
step:1403/1770 train_time:137181ms step_avg:98.48ms
step:1404/1770 train_time:137284ms step_avg:98.48ms
step:1405/1770 train_time:137386ms step_avg:98.48ms
step:1406/1770 train_time:137488ms step_avg:98.49ms
step:1407/1770 train_time:137590ms step_avg:98.49ms
step:1408/1770 train_time:137693ms step_avg:98.49ms
step:1409/1770 train_time:137795ms step_avg:98.50ms
step:1410/1770 train_time:137898ms step_avg:98.50ms
step:1411/1770 train_time:138000ms step_avg:98.50ms
step:1412/1770 train_time:138103ms step_avg:98.50ms
step:1413/1770 train_time:138204ms step_avg:98.51ms
step:1414/1770 train_time:138308ms step_avg:98.51ms
step:1415/1770 train_time:138410ms step_avg:98.51ms
step:1416/1770 train_time:138514ms step_avg:98.52ms
step:1417/1770 train_time:138616ms step_avg:98.52ms
step:1418/1770 train_time:138718ms step_avg:98.52ms
step:1419/1770 train_time:138821ms step_avg:98.52ms
step:1420/1770 train_time:138923ms step_avg:98.53ms
step:1421/1770 train_time:139025ms step_avg:98.53ms
step:1422/1770 train_time:139127ms step_avg:98.53ms
step:1423/1770 train_time:139230ms step_avg:98.53ms
step:1424/1770 train_time:139332ms step_avg:98.54ms
step:1425/1770 train_time:139434ms step_avg:98.54ms
step:1426/1770 train_time:139536ms step_avg:98.54ms
step:1427/1770 train_time:139638ms step_avg:98.54ms
step:1428/1770 train_time:139742ms step_avg:98.55ms
step:1429/1770 train_time:139845ms step_avg:98.55ms
step:1430/1770 train_time:139946ms step_avg:98.55ms
step:1431/1770 train_time:140049ms step_avg:98.56ms
step:1432/1770 train_time:140151ms step_avg:98.56ms
step:1433/1770 train_time:140253ms step_avg:98.56ms
step:1434/1770 train_time:140355ms step_avg:98.56ms
step:1435/1770 train_time:140457ms step_avg:98.57ms
step:1436/1770 train_time:140561ms step_avg:98.57ms
step:1437/1770 train_time:140663ms step_avg:98.57ms
step:1438/1770 train_time:140765ms step_avg:98.58ms
step:1439/1770 train_time:140867ms step_avg:98.58ms
step:1440/1770 train_time:140970ms step_avg:98.58ms
step:1441/1770 train_time:141075ms step_avg:98.58ms
step:1442/1770 train_time:141176ms step_avg:98.59ms
step:1443/1770 train_time:141278ms step_avg:98.59ms
step:1444/1770 train_time:141380ms step_avg:98.59ms
step:1445/1770 train_time:141482ms step_avg:98.59ms
step:1446/1770 train_time:141585ms step_avg:98.60ms
step:1447/1770 train_time:141689ms step_avg:98.60ms
step:1448/1770 train_time:141792ms step_avg:98.60ms
step:1449/1770 train_time:141896ms step_avg:98.61ms
step:1450/1770 train_time:141999ms step_avg:98.61ms
step:1451/1770 train_time:142104ms step_avg:98.61ms
step:1452/1770 train_time:142208ms step_avg:98.62ms
step:1453/1770 train_time:142311ms step_avg:98.62ms
step:1454/1770 train_time:142414ms step_avg:98.62ms
step:1455/1770 train_time:142519ms step_avg:98.63ms
step:1456/1770 train_time:142624ms step_avg:98.63ms
step:1457/1770 train_time:142727ms step_avg:98.64ms
step:1458/1770 train_time:142831ms step_avg:98.64ms
step:1459/1770 train_time:142935ms step_avg:98.64ms
step:1460/1770 train_time:143039ms step_avg:98.65ms
step:1461/1770 train_time:143142ms step_avg:98.65ms
step:1462/1770 train_time:143246ms step_avg:98.65ms
step:1463/1770 train_time:143350ms step_avg:98.66ms
step:1464/1770 train_time:143456ms step_avg:98.66ms
step:1465/1770 train_time:143559ms step_avg:98.67ms
step:1466/1770 train_time:143663ms step_avg:98.67ms
step:1467/1770 train_time:143767ms step_avg:98.67ms
step:1468/1770 train_time:143871ms step_avg:98.68ms
step:1469/1770 train_time:143973ms step_avg:98.68ms
step:1470/1770 train_time:144077ms step_avg:98.68ms
step:1471/1770 train_time:144180ms step_avg:98.69ms
step:1472/1770 train_time:144283ms step_avg:98.69ms
step:1473/1770 train_time:144387ms step_avg:98.69ms
step:1474/1770 train_time:144492ms step_avg:98.70ms
step:1475/1770 train_time:144595ms step_avg:98.70ms
step:1476/1770 train_time:144698ms step_avg:98.70ms
step:1477/1770 train_time:144803ms step_avg:98.71ms
step:1478/1770 train_time:144907ms step_avg:98.71ms
step:1479/1770 train_time:145010ms step_avg:98.71ms
step:1480/1770 train_time:145115ms step_avg:98.72ms
step:1481/1770 train_time:145222ms step_avg:98.72ms
step:1482/1770 train_time:145325ms step_avg:98.73ms
step:1483/1770 train_time:145428ms step_avg:98.73ms
step:1484/1770 train_time:145530ms step_avg:98.73ms
step:1485/1770 train_time:145633ms step_avg:98.73ms
step:1486/1770 train_time:145736ms step_avg:98.74ms
step:1487/1770 train_time:145839ms step_avg:98.74ms
step:1488/1770 train_time:145943ms step_avg:98.74ms
step:1489/1770 train_time:146048ms step_avg:98.75ms
step:1490/1770 train_time:146153ms step_avg:98.75ms
step:1491/1770 train_time:146256ms step_avg:98.75ms
step:1492/1770 train_time:146360ms step_avg:98.76ms
step:1493/1770 train_time:146466ms step_avg:98.76ms
step:1494/1770 train_time:146572ms step_avg:98.77ms
step:1495/1770 train_time:146675ms step_avg:98.77ms
step:1496/1770 train_time:146778ms step_avg:98.77ms
step:1497/1770 train_time:146881ms step_avg:98.78ms
step:1498/1770 train_time:146983ms step_avg:98.78ms
step:1499/1770 train_time:147086ms step_avg:98.78ms
step:1500/1770 train_time:147189ms step_avg:98.78ms
step:1500/1770 val_loss:3.3459 train_time:147290ms step_avg:98.85ms
step:1501/1770 train_time:147312ms step_avg:98.80ms
step:1502/1770 train_time:147405ms step_avg:98.80ms
step:1503/1770 train_time:147508ms step_avg:98.80ms
step:1504/1770 train_time:147611ms step_avg:98.80ms
step:1505/1770 train_time:147716ms step_avg:98.81ms
step:1506/1770 train_time:147819ms step_avg:98.81ms
step:1507/1770 train_time:147923ms step_avg:98.81ms
step:1508/1770 train_time:148027ms step_avg:98.82ms
step:1509/1770 train_time:148130ms step_avg:98.82ms
step:1510/1770 train_time:148232ms step_avg:98.82ms
step:1511/1770 train_time:148337ms step_avg:98.83ms
step:1512/1770 train_time:148442ms step_avg:98.83ms
step:1513/1770 train_time:148547ms step_avg:98.83ms
step:1514/1770 train_time:148650ms step_avg:98.84ms
step:1515/1770 train_time:148753ms step_avg:98.84ms
step:1516/1770 train_time:148856ms step_avg:98.84ms
step:1517/1770 train_time:148959ms step_avg:98.84ms
step:1518/1770 train_time:149065ms step_avg:98.85ms
step:1519/1770 train_time:149167ms step_avg:98.85ms
step:1520/1770 train_time:149272ms step_avg:98.86ms
step:1521/1770 train_time:149375ms step_avg:98.86ms
step:1522/1770 train_time:149479ms step_avg:98.86ms
step:1523/1770 train_time:149583ms step_avg:98.87ms
step:1524/1770 train_time:149685ms step_avg:98.87ms
step:1525/1770 train_time:149789ms step_avg:98.87ms
step:1526/1770 train_time:149891ms step_avg:98.87ms
step:1527/1770 train_time:149995ms step_avg:98.88ms
step:1528/1770 train_time:150100ms step_avg:98.88ms
step:1529/1770 train_time:150203ms step_avg:98.88ms
step:1530/1770 train_time:150306ms step_avg:98.89ms
step:1531/1770 train_time:150409ms step_avg:98.89ms
step:1532/1770 train_time:150512ms step_avg:98.89ms
step:1533/1770 train_time:150617ms step_avg:98.89ms
step:1534/1770 train_time:150721ms step_avg:98.90ms
step:1535/1770 train_time:150825ms step_avg:98.90ms
step:1536/1770 train_time:150927ms step_avg:98.90ms
step:1537/1770 train_time:151031ms step_avg:98.91ms
step:1538/1770 train_time:151135ms step_avg:98.91ms
step:1539/1770 train_time:151238ms step_avg:98.91ms
step:1540/1770 train_time:151345ms step_avg:98.92ms
step:1541/1770 train_time:151449ms step_avg:98.92ms
step:1542/1770 train_time:151553ms step_avg:98.92ms
step:1543/1770 train_time:151656ms step_avg:98.93ms
step:1544/1770 train_time:151761ms step_avg:98.93ms
step:1545/1770 train_time:151865ms step_avg:98.94ms
step:1546/1770 train_time:151969ms step_avg:98.94ms
step:1547/1770 train_time:152072ms step_avg:98.94ms
step:1548/1770 train_time:152175ms step_avg:98.94ms
step:1549/1770 train_time:152279ms step_avg:98.95ms
step:1550/1770 train_time:152383ms step_avg:98.95ms
step:1551/1770 train_time:152486ms step_avg:98.95ms
step:1552/1770 train_time:152590ms step_avg:98.96ms
step:1553/1770 train_time:152693ms step_avg:98.96ms
step:1554/1770 train_time:152797ms step_avg:98.96ms
step:1555/1770 train_time:152902ms step_avg:98.97ms
step:1556/1770 train_time:153004ms step_avg:98.97ms
step:1557/1770 train_time:153107ms step_avg:98.97ms
step:1558/1770 train_time:153212ms step_avg:98.97ms
step:1559/1770 train_time:153315ms step_avg:98.98ms
step:1560/1770 train_time:153418ms step_avg:98.98ms
step:1561/1770 train_time:153523ms step_avg:98.98ms
step:1562/1770 train_time:153626ms step_avg:98.99ms
step:1563/1770 train_time:153731ms step_avg:98.99ms
step:1564/1770 train_time:153834ms step_avg:98.99ms
step:1565/1770 train_time:153937ms step_avg:98.99ms
step:1566/1770 train_time:154041ms step_avg:99.00ms
step:1567/1770 train_time:154145ms step_avg:99.00ms
step:1568/1770 train_time:154248ms step_avg:99.00ms
step:1569/1770 train_time:154355ms step_avg:99.01ms
step:1570/1770 train_time:154458ms step_avg:99.01ms
step:1571/1770 train_time:154561ms step_avg:99.01ms
step:1572/1770 train_time:154666ms step_avg:99.02ms
step:1573/1770 train_time:154772ms step_avg:99.02ms
step:1574/1770 train_time:154875ms step_avg:99.02ms
step:1575/1770 train_time:154977ms step_avg:99.03ms
step:1576/1770 train_time:155079ms step_avg:99.03ms
step:1577/1770 train_time:155185ms step_avg:99.03ms
step:1578/1770 train_time:155290ms step_avg:99.04ms
step:1579/1770 train_time:155392ms step_avg:99.04ms
step:1580/1770 train_time:155495ms step_avg:99.04ms
step:1581/1770 train_time:155601ms step_avg:99.05ms
step:1582/1770 train_time:155706ms step_avg:99.05ms
step:1583/1770 train_time:155810ms step_avg:99.05ms
step:1584/1770 train_time:155915ms step_avg:99.06ms
step:1585/1770 train_time:156019ms step_avg:99.06ms
step:1586/1770 train_time:156127ms step_avg:99.07ms
step:1587/1770 train_time:156230ms step_avg:99.07ms
step:1588/1770 train_time:156333ms step_avg:99.07ms
step:1589/1770 train_time:156439ms step_avg:99.07ms
step:1590/1770 train_time:156541ms step_avg:99.08ms
step:1591/1770 train_time:156646ms step_avg:99.08ms
step:1592/1770 train_time:156751ms step_avg:99.08ms
step:1593/1770 train_time:156854ms step_avg:99.09ms
step:1594/1770 train_time:156957ms step_avg:99.09ms
step:1595/1770 train_time:157060ms step_avg:99.09ms
step:1596/1770 train_time:157165ms step_avg:99.09ms
step:1597/1770 train_time:157267ms step_avg:99.10ms
step:1598/1770 train_time:157370ms step_avg:99.10ms
step:1599/1770 train_time:157476ms step_avg:99.10ms
step:1600/1770 train_time:157582ms step_avg:99.11ms
step:1601/1770 train_time:157686ms step_avg:99.11ms
step:1602/1770 train_time:157790ms step_avg:99.11ms
step:1603/1770 train_time:157894ms step_avg:99.12ms
step:1604/1770 train_time:157997ms step_avg:99.12ms
step:1605/1770 train_time:158099ms step_avg:99.12ms
step:1606/1770 train_time:158203ms step_avg:99.12ms
step:1607/1770 train_time:158310ms step_avg:99.13ms
step:1608/1770 train_time:158413ms step_avg:99.13ms
step:1609/1770 train_time:158517ms step_avg:99.13ms
step:1610/1770 train_time:158622ms step_avg:99.14ms
step:1611/1770 train_time:158728ms step_avg:99.14ms
step:1612/1770 train_time:158833ms step_avg:99.15ms
step:1613/1770 train_time:158936ms step_avg:99.15ms
step:1614/1770 train_time:159040ms step_avg:99.15ms
step:1615/1770 train_time:159143ms step_avg:99.15ms
step:1616/1770 train_time:159246ms step_avg:99.16ms
step:1617/1770 train_time:159351ms step_avg:99.16ms
step:1618/1770 train_time:159455ms step_avg:99.16ms
step:1619/1770 train_time:159559ms step_avg:99.17ms
step:1620/1770 train_time:159663ms step_avg:99.17ms
step:1621/1770 train_time:159768ms step_avg:99.17ms
step:1622/1770 train_time:159872ms step_avg:99.18ms
step:1623/1770 train_time:159978ms step_avg:99.18ms
step:1624/1770 train_time:160081ms step_avg:99.18ms
step:1625/1770 train_time:160183ms step_avg:99.18ms
step:1625/1770 val_loss:3.3115 train_time:160286ms step_avg:99.25ms
step:1626/1770 train_time:160307ms step_avg:99.20ms
step:1627/1770 train_time:160398ms step_avg:99.19ms
step:1628/1770 train_time:160501ms step_avg:99.20ms
step:1629/1770 train_time:160604ms step_avg:99.20ms
step:1630/1770 train_time:160707ms step_avg:99.20ms
step:1631/1770 train_time:160809ms step_avg:99.20ms
step:1632/1770 train_time:160912ms step_avg:99.21ms
step:1633/1770 train_time:161016ms step_avg:99.21ms
step:1634/1770 train_time:161119ms step_avg:99.21ms
step:1635/1770 train_time:161221ms step_avg:99.21ms
step:1636/1770 train_time:161327ms step_avg:99.22ms
step:1637/1770 train_time:161433ms step_avg:99.22ms
step:1638/1770 train_time:161535ms step_avg:99.22ms
step:1639/1770 train_time:161639ms step_avg:99.23ms
step:1640/1770 train_time:161743ms step_avg:99.23ms
step:1641/1770 train_time:161847ms step_avg:99.23ms
step:1642/1770 train_time:161950ms step_avg:99.23ms
step:1643/1770 train_time:162053ms step_avg:99.24ms
step:1644/1770 train_time:162159ms step_avg:99.24ms
step:1645/1770 train_time:162262ms step_avg:99.24ms
step:1646/1770 train_time:162367ms step_avg:99.25ms
step:1647/1770 train_time:162472ms step_avg:99.25ms
step:1648/1770 train_time:162575ms step_avg:99.25ms
step:1649/1770 train_time:162679ms step_avg:99.25ms
step:1650/1770 train_time:162782ms step_avg:99.26ms
step:1651/1770 train_time:162885ms step_avg:99.26ms
step:1652/1770 train_time:162989ms step_avg:99.26ms
step:1653/1770 train_time:163091ms step_avg:99.26ms
step:1654/1770 train_time:163198ms step_avg:99.27ms
step:1655/1770 train_time:163305ms step_avg:99.27ms
step:1656/1770 train_time:163408ms step_avg:99.28ms
step:1657/1770 train_time:163513ms step_avg:99.28ms
step:1658/1770 train_time:163616ms step_avg:99.28ms
step:1659/1770 train_time:163723ms step_avg:99.29ms
step:1660/1770 train_time:163826ms step_avg:99.29ms
step:1661/1770 train_time:163930ms step_avg:99.29ms
step:1662/1770 train_time:164034ms step_avg:99.29ms
step:1663/1770 train_time:164136ms step_avg:99.30ms
step:1664/1770 train_time:164240ms step_avg:99.30ms
step:1665/1770 train_time:164343ms step_avg:99.30ms
step:1666/1770 train_time:164447ms step_avg:99.30ms
step:1667/1770 train_time:164550ms step_avg:99.31ms
step:1668/1770 train_time:164653ms step_avg:99.31ms
step:1669/1770 train_time:164756ms step_avg:99.31ms
step:1670/1770 train_time:164860ms step_avg:99.31ms
step:1671/1770 train_time:164964ms step_avg:99.32ms
step:1672/1770 train_time:165068ms step_avg:99.32ms
step:1673/1770 train_time:165174ms step_avg:99.32ms
step:1674/1770 train_time:165277ms step_avg:99.32ms
step:1675/1770 train_time:165379ms step_avg:99.33ms
step:1676/1770 train_time:165483ms step_avg:99.33ms
step:1677/1770 train_time:165590ms step_avg:99.33ms
step:1678/1770 train_time:165693ms step_avg:99.34ms
step:1679/1770 train_time:165797ms step_avg:99.34ms
step:1680/1770 train_time:165900ms step_avg:99.34ms
step:1681/1770 train_time:166005ms step_avg:99.34ms
step:1682/1770 train_time:166111ms step_avg:99.35ms
step:1683/1770 train_time:166214ms step_avg:99.35ms
step:1684/1770 train_time:166317ms step_avg:99.35ms
step:1685/1770 train_time:166421ms step_avg:99.36ms
step:1686/1770 train_time:166525ms step_avg:99.36ms
step:1687/1770 train_time:166630ms step_avg:99.36ms
step:1688/1770 train_time:166732ms step_avg:99.36ms
step:1689/1770 train_time:166835ms step_avg:99.37ms
step:1690/1770 train_time:166938ms step_avg:99.37ms
step:1691/1770 train_time:167042ms step_avg:99.37ms
step:1692/1770 train_time:167146ms step_avg:99.37ms
step:1693/1770 train_time:167251ms step_avg:99.38ms
step:1694/1770 train_time:167355ms step_avg:99.38ms
step:1695/1770 train_time:167459ms step_avg:99.38ms
step:1696/1770 train_time:167565ms step_avg:99.39ms
step:1697/1770 train_time:167670ms step_avg:99.39ms
step:1698/1770 train_time:167774ms step_avg:99.39ms
step:1699/1770 train_time:167876ms step_avg:99.39ms
step:1700/1770 train_time:167980ms step_avg:99.40ms
step:1701/1770 train_time:168082ms step_avg:99.40ms
step:1702/1770 train_time:168186ms step_avg:99.40ms
step:1703/1770 train_time:168289ms step_avg:99.40ms
step:1704/1770 train_time:168392ms step_avg:99.41ms
step:1705/1770 train_time:168497ms step_avg:99.41ms
step:1706/1770 train_time:168600ms step_avg:99.41ms
step:1707/1770 train_time:168704ms step_avg:99.41ms
step:1708/1770 train_time:168807ms step_avg:99.42ms
step:1709/1770 train_time:168913ms step_avg:99.42ms
step:1710/1770 train_time:169020ms step_avg:99.42ms
step:1711/1770 train_time:169126ms step_avg:99.43ms
step:1712/1770 train_time:169231ms step_avg:99.43ms
step:1713/1770 train_time:169334ms step_avg:99.43ms
step:1714/1770 train_time:169438ms step_avg:99.44ms
step:1715/1770 train_time:169542ms step_avg:99.44ms
step:1716/1770 train_time:169647ms step_avg:99.44ms
step:1717/1770 train_time:169750ms step_avg:99.44ms
step:1718/1770 train_time:169855ms step_avg:99.45ms
step:1719/1770 train_time:169960ms step_avg:99.45ms
step:1720/1770 train_time:170065ms step_avg:99.45ms
step:1721/1770 train_time:170168ms step_avg:99.46ms
step:1722/1770 train_time:170275ms step_avg:99.46ms
step:1723/1770 train_time:170380ms step_avg:99.46ms
step:1724/1770 train_time:170486ms step_avg:99.47ms
step:1725/1770 train_time:170592ms step_avg:99.47ms
step:1726/1770 train_time:170699ms step_avg:99.47ms
step:1727/1770 train_time:170803ms step_avg:99.48ms
step:1728/1770 train_time:170909ms step_avg:99.48ms
step:1729/1770 train_time:171012ms step_avg:99.48ms
step:1730/1770 train_time:171117ms step_avg:99.49ms
step:1731/1770 train_time:171223ms step_avg:99.49ms
step:1732/1770 train_time:171326ms step_avg:99.49ms
step:1733/1770 train_time:171432ms step_avg:99.50ms
step:1734/1770 train_time:171536ms step_avg:99.50ms
step:1735/1770 train_time:171641ms step_avg:99.50ms
step:1736/1770 train_time:171744ms step_avg:99.50ms
step:1737/1770 train_time:171849ms step_avg:99.51ms
step:1738/1770 train_time:171953ms step_avg:99.51ms
step:1739/1770 train_time:172057ms step_avg:99.51ms
step:1740/1770 train_time:172161ms step_avg:99.51ms
step:1741/1770 train_time:172269ms step_avg:99.52ms
step:1742/1770 train_time:172377ms step_avg:99.52ms
step:1743/1770 train_time:172482ms step_avg:99.53ms
step:1744/1770 train_time:172586ms step_avg:99.53ms
step:1745/1770 train_time:172690ms step_avg:99.53ms
step:1746/1770 train_time:172797ms step_avg:99.54ms
step:1747/1770 train_time:172900ms step_avg:99.54ms
step:1748/1770 train_time:173007ms step_avg:99.54ms
step:1749/1770 train_time:173112ms step_avg:99.55ms
step:1750/1770 train_time:173216ms step_avg:99.55ms
step:1750/1770 val_loss:3.2846 train_time:173318ms step_avg:99.61ms
step:1751/1770 train_time:173340ms step_avg:99.56ms
step:1752/1770 train_time:173431ms step_avg:99.56ms
step:1753/1770 train_time:173535ms step_avg:99.56ms
step:1754/1770 train_time:173640ms step_avg:99.56ms
step:1755/1770 train_time:173744ms step_avg:99.57ms
step:1756/1770 train_time:173849ms step_avg:99.57ms
step:1757/1770 train_time:173953ms step_avg:99.57ms
step:1758/1770 train_time:174057ms step_avg:99.57ms
step:1759/1770 train_time:174162ms step_avg:99.58ms
step:1760/1770 train_time:174266ms step_avg:99.58ms
step:1761/1770 train_time:174373ms step_avg:99.58ms
step:1762/1770 train_time:174481ms step_avg:99.59ms
step:1763/1770 train_time:174584ms step_avg:99.59ms
step:1764/1770 train_time:174689ms step_avg:99.59ms
step:1765/1770 train_time:174794ms step_avg:99.60ms
step:1766/1770 train_time:174902ms step_avg:99.60ms
step:1767/1770 train_time:175005ms step_avg:99.60ms
step:1768/1770 train_time:175110ms step_avg:99.61ms
step:1769/1770 train_time:175213ms step_avg:99.61ms
step:1770/1770 train_time:175317ms step_avg:99.61ms
step:1770/1770 val_loss:3.2813 train_time:175422ms step_avg:99.67ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
