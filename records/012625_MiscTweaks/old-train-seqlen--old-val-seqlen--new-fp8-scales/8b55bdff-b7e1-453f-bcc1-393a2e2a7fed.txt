import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:11:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:36539ms step_avg:nanms
step:2/1393 train_time:36963ms step_avg:nanms
step:3/1393 train_time:37108ms step_avg:nanms
step:4/1393 train_time:37227ms step_avg:nanms
step:5/1393 train_time:37348ms step_avg:nanms
step:6/1393 train_time:37468ms step_avg:nanms
step:7/1393 train_time:37589ms step_avg:nanms
step:8/1393 train_time:37710ms step_avg:nanms
step:9/1393 train_time:37830ms step_avg:nanms
step:10/1393 train_time:37950ms step_avg:nanms
step:11/1393 train_time:124ms step_avg:nanms
step:12/1393 train_time:247ms step_avg:nanms
step:13/1393 train_time:368ms step_avg:122.64ms
step:14/1393 train_time:489ms step_avg:122.26ms
step:15/1393 train_time:609ms step_avg:121.90ms
step:16/1393 train_time:730ms step_avg:121.74ms
step:17/1393 train_time:851ms step_avg:121.62ms
step:18/1393 train_time:972ms step_avg:121.50ms
step:19/1393 train_time:1096ms step_avg:121.77ms
step:20/1393 train_time:1217ms step_avg:121.70ms
step:21/1393 train_time:1339ms step_avg:121.70ms
step:22/1393 train_time:1460ms step_avg:121.64ms
step:23/1393 train_time:1581ms step_avg:121.59ms
step:24/1393 train_time:1702ms step_avg:121.60ms
step:25/1393 train_time:1823ms step_avg:121.52ms
step:26/1393 train_time:1944ms step_avg:121.50ms
step:27/1393 train_time:2065ms step_avg:121.48ms
step:28/1393 train_time:2187ms step_avg:121.51ms
step:29/1393 train_time:2311ms step_avg:121.63ms
step:30/1393 train_time:2434ms step_avg:121.70ms
step:31/1393 train_time:2555ms step_avg:121.66ms
step:32/1393 train_time:2676ms step_avg:121.62ms
step:33/1393 train_time:2796ms step_avg:121.56ms
step:34/1393 train_time:2917ms step_avg:121.53ms
step:35/1393 train_time:3038ms step_avg:121.53ms
step:36/1393 train_time:3160ms step_avg:121.53ms
step:37/1393 train_time:3282ms step_avg:121.54ms
step:38/1393 train_time:3403ms step_avg:121.54ms
step:39/1393 train_time:3525ms step_avg:121.55ms
step:40/1393 train_time:3645ms step_avg:121.51ms
step:41/1393 train_time:3767ms step_avg:121.51ms
step:42/1393 train_time:3887ms step_avg:121.48ms
step:43/1393 train_time:4008ms step_avg:121.45ms
step:44/1393 train_time:4130ms step_avg:121.48ms
step:45/1393 train_time:4253ms step_avg:121.51ms
step:46/1393 train_time:4374ms step_avg:121.49ms
step:47/1393 train_time:4494ms step_avg:121.47ms
step:48/1393 train_time:4615ms step_avg:121.46ms
step:49/1393 train_time:4736ms step_avg:121.45ms
step:50/1393 train_time:4857ms step_avg:121.43ms
step:51/1393 train_time:4979ms step_avg:121.43ms
step:52/1393 train_time:5099ms step_avg:121.41ms
step:53/1393 train_time:5221ms step_avg:121.41ms
step:54/1393 train_time:5342ms step_avg:121.40ms
step:55/1393 train_time:5464ms step_avg:121.42ms
step:56/1393 train_time:5585ms step_avg:121.40ms
step:57/1393 train_time:5705ms step_avg:121.38ms
step:58/1393 train_time:5827ms step_avg:121.39ms
step:59/1393 train_time:5948ms step_avg:121.40ms
step:60/1393 train_time:6070ms step_avg:121.39ms
step:61/1393 train_time:6191ms step_avg:121.38ms
step:62/1393 train_time:6312ms step_avg:121.38ms
step:63/1393 train_time:6433ms step_avg:121.38ms
step:64/1393 train_time:6555ms step_avg:121.38ms
step:65/1393 train_time:6675ms step_avg:121.37ms
step:66/1393 train_time:6798ms step_avg:121.40ms
step:67/1393 train_time:6919ms step_avg:121.38ms
step:68/1393 train_time:7040ms step_avg:121.37ms
step:69/1393 train_time:7160ms step_avg:121.36ms
step:70/1393 train_time:7281ms step_avg:121.35ms
step:71/1393 train_time:7402ms step_avg:121.34ms
step:72/1393 train_time:7524ms step_avg:121.36ms
step:73/1393 train_time:7645ms step_avg:121.34ms
step:74/1393 train_time:7766ms step_avg:121.34ms
step:75/1393 train_time:7886ms step_avg:121.33ms
step:76/1393 train_time:8007ms step_avg:121.33ms
step:77/1393 train_time:8129ms step_avg:121.33ms
step:78/1393 train_time:8251ms step_avg:121.34ms
step:79/1393 train_time:8371ms step_avg:121.32ms
step:80/1393 train_time:8492ms step_avg:121.32ms
step:81/1393 train_time:8615ms step_avg:121.33ms
step:82/1393 train_time:8736ms step_avg:121.33ms
step:83/1393 train_time:8858ms step_avg:121.35ms
step:84/1393 train_time:8979ms step_avg:121.34ms
step:85/1393 train_time:9100ms step_avg:121.33ms
step:86/1393 train_time:9221ms step_avg:121.33ms
step:87/1393 train_time:9342ms step_avg:121.32ms
step:88/1393 train_time:9463ms step_avg:121.32ms
step:89/1393 train_time:9584ms step_avg:121.32ms
step:90/1393 train_time:9705ms step_avg:121.32ms
step:91/1393 train_time:9827ms step_avg:121.32ms
step:92/1393 train_time:9948ms step_avg:121.32ms
step:93/1393 train_time:10071ms step_avg:121.34ms
step:94/1393 train_time:10192ms step_avg:121.33ms
step:95/1393 train_time:10313ms step_avg:121.33ms
step:96/1393 train_time:10434ms step_avg:121.32ms
step:97/1393 train_time:10554ms step_avg:121.31ms
step:98/1393 train_time:10675ms step_avg:121.31ms
step:99/1393 train_time:10796ms step_avg:121.30ms
step:100/1393 train_time:10918ms step_avg:121.31ms
step:101/1393 train_time:11038ms step_avg:121.30ms
step:102/1393 train_time:11159ms step_avg:121.29ms
step:103/1393 train_time:11281ms step_avg:121.30ms
step:104/1393 train_time:11403ms step_avg:121.30ms
step:105/1393 train_time:11525ms step_avg:121.31ms
step:106/1393 train_time:11647ms step_avg:121.32ms
step:107/1393 train_time:11769ms step_avg:121.33ms
step:108/1393 train_time:11889ms step_avg:121.32ms
step:109/1393 train_time:12012ms step_avg:121.33ms
step:110/1393 train_time:12133ms step_avg:121.33ms
step:111/1393 train_time:12255ms step_avg:121.34ms
step:112/1393 train_time:12377ms step_avg:121.34ms
step:113/1393 train_time:12498ms step_avg:121.34ms
step:114/1393 train_time:12620ms step_avg:121.35ms
step:115/1393 train_time:12742ms step_avg:121.36ms
step:116/1393 train_time:12865ms step_avg:121.37ms
step:117/1393 train_time:12987ms step_avg:121.37ms
step:118/1393 train_time:13109ms step_avg:121.38ms
step:119/1393 train_time:13231ms step_avg:121.39ms
step:120/1393 train_time:13354ms step_avg:121.40ms
step:121/1393 train_time:13476ms step_avg:121.40ms
step:122/1393 train_time:13597ms step_avg:121.40ms
step:123/1393 train_time:13719ms step_avg:121.41ms
step:124/1393 train_time:13840ms step_avg:121.40ms
step:125/1393 train_time:13962ms step_avg:121.41ms
step:125/1393 val_loss:4.3909 train_time:14082ms step_avg:122.45ms
step:126/1393 train_time:14105ms step_avg:121.59ms
step:127/1393 train_time:14220ms step_avg:121.54ms
step:128/1393 train_time:14345ms step_avg:121.57ms
step:129/1393 train_time:14466ms step_avg:121.57ms
step:130/1393 train_time:14588ms step_avg:121.56ms
step:131/1393 train_time:14709ms step_avg:121.56ms
step:132/1393 train_time:14831ms step_avg:121.57ms
step:133/1393 train_time:14952ms step_avg:121.56ms
step:134/1393 train_time:15073ms step_avg:121.56ms
step:135/1393 train_time:15196ms step_avg:121.57ms
step:136/1393 train_time:15319ms step_avg:121.58ms
step:137/1393 train_time:15444ms step_avg:121.61ms
step:138/1393 train_time:15563ms step_avg:121.58ms
step:139/1393 train_time:15684ms step_avg:121.59ms
step:140/1393 train_time:15807ms step_avg:121.59ms
step:141/1393 train_time:15928ms step_avg:121.59ms
step:142/1393 train_time:16049ms step_avg:121.59ms
step:143/1393 train_time:16172ms step_avg:121.59ms
step:144/1393 train_time:16294ms step_avg:121.60ms
step:145/1393 train_time:16416ms step_avg:121.60ms
step:146/1393 train_time:16537ms step_avg:121.60ms
step:147/1393 train_time:16659ms step_avg:121.60ms
step:148/1393 train_time:16781ms step_avg:121.60ms
step:149/1393 train_time:16903ms step_avg:121.60ms
step:150/1393 train_time:17024ms step_avg:121.60ms
step:151/1393 train_time:17146ms step_avg:121.60ms
step:152/1393 train_time:17268ms step_avg:121.61ms
step:153/1393 train_time:17391ms step_avg:121.62ms
step:154/1393 train_time:17513ms step_avg:121.62ms
step:155/1393 train_time:17636ms step_avg:121.63ms
step:156/1393 train_time:17758ms step_avg:121.63ms
step:157/1393 train_time:17879ms step_avg:121.63ms
step:158/1393 train_time:18002ms step_avg:121.63ms
step:159/1393 train_time:18123ms step_avg:121.63ms
step:160/1393 train_time:18246ms step_avg:121.64ms
step:161/1393 train_time:18367ms step_avg:121.64ms
step:162/1393 train_time:18491ms step_avg:121.65ms
step:163/1393 train_time:18613ms step_avg:121.65ms
step:164/1393 train_time:18734ms step_avg:121.65ms
step:165/1393 train_time:18856ms step_avg:121.65ms
step:166/1393 train_time:18977ms step_avg:121.65ms
step:167/1393 train_time:19100ms step_avg:121.66ms
step:168/1393 train_time:19224ms step_avg:121.67ms
step:169/1393 train_time:19345ms step_avg:121.67ms
step:170/1393 train_time:19467ms step_avg:121.67ms
step:171/1393 train_time:19589ms step_avg:121.67ms
step:172/1393 train_time:19710ms step_avg:121.67ms
step:173/1393 train_time:19832ms step_avg:121.67ms
step:174/1393 train_time:19953ms step_avg:121.67ms
step:175/1393 train_time:20076ms step_avg:121.67ms
step:176/1393 train_time:20198ms step_avg:121.68ms
step:177/1393 train_time:20321ms step_avg:121.68ms
step:178/1393 train_time:20443ms step_avg:121.68ms
step:179/1393 train_time:20564ms step_avg:121.68ms
step:180/1393 train_time:20686ms step_avg:121.68ms
step:181/1393 train_time:20807ms step_avg:121.68ms
step:182/1393 train_time:20929ms step_avg:121.68ms
step:183/1393 train_time:21052ms step_avg:121.69ms
step:184/1393 train_time:21174ms step_avg:121.69ms
step:185/1393 train_time:21296ms step_avg:121.69ms
step:186/1393 train_time:21418ms step_avg:121.69ms
step:187/1393 train_time:21540ms step_avg:121.69ms
step:188/1393 train_time:21661ms step_avg:121.69ms
step:189/1393 train_time:21782ms step_avg:121.69ms
step:190/1393 train_time:21905ms step_avg:121.69ms
step:191/1393 train_time:22027ms step_avg:121.70ms
step:192/1393 train_time:22149ms step_avg:121.70ms
step:193/1393 train_time:22271ms step_avg:121.70ms
step:194/1393 train_time:22394ms step_avg:121.71ms
step:195/1393 train_time:22517ms step_avg:121.71ms
step:196/1393 train_time:22638ms step_avg:121.71ms
step:197/1393 train_time:22760ms step_avg:121.71ms
step:198/1393 train_time:22883ms step_avg:121.72ms
step:199/1393 train_time:23006ms step_avg:121.72ms
step:200/1393 train_time:23128ms step_avg:121.72ms
step:201/1393 train_time:23250ms step_avg:121.73ms
step:202/1393 train_time:23372ms step_avg:121.73ms
step:203/1393 train_time:23494ms step_avg:121.73ms
step:204/1393 train_time:23616ms step_avg:121.73ms
step:205/1393 train_time:23738ms step_avg:121.73ms
step:206/1393 train_time:23859ms step_avg:121.73ms
step:207/1393 train_time:23981ms step_avg:121.73ms
step:208/1393 train_time:24103ms step_avg:121.73ms
step:209/1393 train_time:24227ms step_avg:121.74ms
step:210/1393 train_time:24349ms step_avg:121.75ms
step:211/1393 train_time:24472ms step_avg:121.75ms
step:212/1393 train_time:24595ms step_avg:121.76ms
step:213/1393 train_time:24718ms step_avg:121.77ms
step:214/1393 train_time:24841ms step_avg:121.77ms
step:215/1393 train_time:24963ms step_avg:121.77ms
step:216/1393 train_time:25085ms step_avg:121.77ms
step:217/1393 train_time:25207ms step_avg:121.78ms
step:218/1393 train_time:25329ms step_avg:121.78ms
step:219/1393 train_time:25452ms step_avg:121.78ms
step:220/1393 train_time:25574ms step_avg:121.78ms
step:221/1393 train_time:25697ms step_avg:121.78ms
step:222/1393 train_time:25819ms step_avg:121.79ms
step:223/1393 train_time:25942ms step_avg:121.79ms
step:224/1393 train_time:26065ms step_avg:121.80ms
step:225/1393 train_time:26188ms step_avg:121.80ms
step:226/1393 train_time:26310ms step_avg:121.81ms
step:227/1393 train_time:26433ms step_avg:121.81ms
step:228/1393 train_time:26555ms step_avg:121.81ms
step:229/1393 train_time:26678ms step_avg:121.82ms
step:230/1393 train_time:26801ms step_avg:121.82ms
step:231/1393 train_time:26923ms step_avg:121.82ms
step:232/1393 train_time:27046ms step_avg:121.83ms
step:233/1393 train_time:27169ms step_avg:121.83ms
step:234/1393 train_time:27291ms step_avg:121.83ms
step:235/1393 train_time:27413ms step_avg:121.84ms
step:236/1393 train_time:27537ms step_avg:121.84ms
step:237/1393 train_time:27659ms step_avg:121.85ms
step:238/1393 train_time:27782ms step_avg:121.85ms
step:239/1393 train_time:27905ms step_avg:121.86ms
step:240/1393 train_time:28029ms step_avg:121.87ms
step:241/1393 train_time:28151ms step_avg:121.87ms
step:242/1393 train_time:28274ms step_avg:121.87ms
step:243/1393 train_time:28397ms step_avg:121.88ms
step:244/1393 train_time:28521ms step_avg:121.88ms
step:245/1393 train_time:28644ms step_avg:121.89ms
step:246/1393 train_time:28767ms step_avg:121.89ms
step:247/1393 train_time:28890ms step_avg:121.90ms
step:248/1393 train_time:29012ms step_avg:121.90ms
step:249/1393 train_time:29134ms step_avg:121.90ms
step:250/1393 train_time:29257ms step_avg:121.91ms
step:250/1393 val_loss:3.9816 train_time:29380ms step_avg:122.42ms
step:251/1393 train_time:29402ms step_avg:122.00ms
step:252/1393 train_time:29518ms step_avg:121.97ms
step:253/1393 train_time:29643ms step_avg:121.99ms
step:254/1393 train_time:29765ms step_avg:121.99ms
step:255/1393 train_time:29887ms step_avg:121.99ms
step:256/1393 train_time:30009ms step_avg:121.99ms
step:257/1393 train_time:30131ms step_avg:121.99ms
step:258/1393 train_time:30253ms step_avg:121.99ms
step:259/1393 train_time:30375ms step_avg:121.99ms
step:260/1393 train_time:30499ms step_avg:122.00ms
step:261/1393 train_time:30623ms step_avg:122.01ms
step:262/1393 train_time:30746ms step_avg:122.01ms
step:263/1393 train_time:30868ms step_avg:122.01ms
step:264/1393 train_time:30991ms step_avg:122.01ms
step:265/1393 train_time:31113ms step_avg:122.01ms
step:266/1393 train_time:31235ms step_avg:122.01ms
step:267/1393 train_time:31357ms step_avg:122.01ms
step:268/1393 train_time:31481ms step_avg:122.02ms
step:269/1393 train_time:31603ms step_avg:122.02ms
step:270/1393 train_time:31726ms step_avg:122.02ms
step:271/1393 train_time:31848ms step_avg:122.02ms
step:272/1393 train_time:31972ms step_avg:122.03ms
step:273/1393 train_time:32095ms step_avg:122.03ms
step:274/1393 train_time:32217ms step_avg:122.04ms
step:275/1393 train_time:32340ms step_avg:122.04ms
step:276/1393 train_time:32463ms step_avg:122.04ms
step:277/1393 train_time:32586ms step_avg:122.04ms
step:278/1393 train_time:32709ms step_avg:122.05ms
step:279/1393 train_time:32832ms step_avg:122.05ms
step:280/1393 train_time:32955ms step_avg:122.06ms
step:281/1393 train_time:33078ms step_avg:122.06ms
step:282/1393 train_time:33200ms step_avg:122.06ms
step:283/1393 train_time:33323ms step_avg:122.06ms
step:284/1393 train_time:33446ms step_avg:122.07ms
step:285/1393 train_time:33569ms step_avg:122.07ms
step:286/1393 train_time:33690ms step_avg:122.07ms
step:287/1393 train_time:33814ms step_avg:122.07ms
step:288/1393 train_time:33938ms step_avg:122.08ms
step:289/1393 train_time:34061ms step_avg:122.08ms
step:290/1393 train_time:34184ms step_avg:122.08ms
step:291/1393 train_time:34306ms step_avg:122.08ms
step:292/1393 train_time:34429ms step_avg:122.09ms
step:293/1393 train_time:34551ms step_avg:122.09ms
step:294/1393 train_time:34674ms step_avg:122.09ms
step:295/1393 train_time:34796ms step_avg:122.09ms
step:296/1393 train_time:34919ms step_avg:122.09ms
step:297/1393 train_time:35041ms step_avg:122.10ms
step:298/1393 train_time:35165ms step_avg:122.10ms
step:299/1393 train_time:35287ms step_avg:122.10ms
step:300/1393 train_time:35410ms step_avg:122.10ms
step:301/1393 train_time:35532ms step_avg:122.10ms
step:302/1393 train_time:35655ms step_avg:122.11ms
step:303/1393 train_time:35778ms step_avg:122.11ms
step:304/1393 train_time:35901ms step_avg:122.11ms
step:305/1393 train_time:36024ms step_avg:122.11ms
step:306/1393 train_time:36147ms step_avg:122.12ms
step:307/1393 train_time:36270ms step_avg:122.12ms
step:308/1393 train_time:36393ms step_avg:122.12ms
step:309/1393 train_time:36515ms step_avg:122.12ms
step:310/1393 train_time:36637ms step_avg:122.12ms
step:311/1393 train_time:36759ms step_avg:122.12ms
step:312/1393 train_time:36883ms step_avg:122.13ms
step:313/1393 train_time:37008ms step_avg:122.14ms
step:314/1393 train_time:37134ms step_avg:122.15ms
step:315/1393 train_time:37259ms step_avg:122.16ms
step:316/1393 train_time:37384ms step_avg:122.17ms
step:317/1393 train_time:37509ms step_avg:122.18ms
step:318/1393 train_time:37635ms step_avg:122.19ms
step:319/1393 train_time:37760ms step_avg:122.20ms
step:320/1393 train_time:37886ms step_avg:122.21ms
step:321/1393 train_time:38010ms step_avg:122.22ms
step:322/1393 train_time:38136ms step_avg:122.23ms
step:323/1393 train_time:38261ms step_avg:122.24ms
step:324/1393 train_time:38385ms step_avg:122.25ms
step:325/1393 train_time:38511ms step_avg:122.26ms
step:326/1393 train_time:38636ms step_avg:122.26ms
step:327/1393 train_time:38761ms step_avg:122.28ms
step:328/1393 train_time:38887ms step_avg:122.29ms
step:329/1393 train_time:39012ms step_avg:122.29ms
step:330/1393 train_time:39138ms step_avg:122.31ms
step:331/1393 train_time:39265ms step_avg:122.32ms
step:332/1393 train_time:39388ms step_avg:122.32ms
step:333/1393 train_time:39515ms step_avg:122.34ms
step:334/1393 train_time:39641ms step_avg:122.35ms
step:335/1393 train_time:39766ms step_avg:122.36ms
step:336/1393 train_time:39891ms step_avg:122.37ms
step:337/1393 train_time:40016ms step_avg:122.37ms
step:338/1393 train_time:40143ms step_avg:122.39ms
step:339/1393 train_time:40269ms step_avg:122.40ms
step:340/1393 train_time:40394ms step_avg:122.41ms
step:341/1393 train_time:40519ms step_avg:122.41ms
step:342/1393 train_time:40644ms step_avg:122.42ms
step:343/1393 train_time:40770ms step_avg:122.43ms
step:344/1393 train_time:40895ms step_avg:122.44ms
step:345/1393 train_time:41021ms step_avg:122.45ms
step:346/1393 train_time:41146ms step_avg:122.46ms
step:347/1393 train_time:41272ms step_avg:122.47ms
step:348/1393 train_time:41396ms step_avg:122.47ms
step:349/1393 train_time:41521ms step_avg:122.48ms
step:350/1393 train_time:41646ms step_avg:122.49ms
step:351/1393 train_time:41774ms step_avg:122.50ms
step:352/1393 train_time:41900ms step_avg:122.51ms
step:353/1393 train_time:42024ms step_avg:122.52ms
step:354/1393 train_time:42149ms step_avg:122.53ms
step:355/1393 train_time:42274ms step_avg:122.53ms
step:356/1393 train_time:42399ms step_avg:122.54ms
step:357/1393 train_time:42523ms step_avg:122.54ms
step:358/1393 train_time:42654ms step_avg:122.57ms
step:359/1393 train_time:42774ms step_avg:122.56ms
step:360/1393 train_time:42899ms step_avg:122.57ms
step:361/1393 train_time:43024ms step_avg:122.58ms
step:362/1393 train_time:43149ms step_avg:122.58ms
step:363/1393 train_time:43274ms step_avg:122.59ms
step:364/1393 train_time:43400ms step_avg:122.60ms
step:365/1393 train_time:43525ms step_avg:122.61ms
step:366/1393 train_time:43649ms step_avg:122.61ms
step:367/1393 train_time:43775ms step_avg:122.62ms
step:368/1393 train_time:43900ms step_avg:122.62ms
step:369/1393 train_time:44025ms step_avg:122.63ms
step:370/1393 train_time:44157ms step_avg:122.66ms
step:371/1393 train_time:44277ms step_avg:122.65ms
step:372/1393 train_time:44402ms step_avg:122.66ms
step:373/1393 train_time:44527ms step_avg:122.66ms
step:374/1393 train_time:44652ms step_avg:122.67ms
step:375/1393 train_time:44778ms step_avg:122.68ms
step:375/1393 val_loss:3.7788 train_time:44901ms step_avg:123.02ms
step:376/1393 train_time:44924ms step_avg:122.74ms
step:377/1393 train_time:45042ms step_avg:122.73ms
step:378/1393 train_time:45169ms step_avg:122.74ms
step:379/1393 train_time:45294ms step_avg:122.75ms
step:380/1393 train_time:45420ms step_avg:122.76ms
step:381/1393 train_time:45545ms step_avg:122.76ms
step:382/1393 train_time:45669ms step_avg:122.77ms
step:383/1393 train_time:45793ms step_avg:122.77ms
step:384/1393 train_time:45919ms step_avg:122.78ms
step:385/1393 train_time:46045ms step_avg:122.79ms
step:386/1393 train_time:46172ms step_avg:122.80ms
step:387/1393 train_time:46298ms step_avg:122.81ms
step:388/1393 train_time:46423ms step_avg:122.81ms
step:389/1393 train_time:46548ms step_avg:122.82ms
step:390/1393 train_time:46672ms step_avg:122.82ms
step:391/1393 train_time:46797ms step_avg:122.83ms
step:392/1393 train_time:46921ms step_avg:122.83ms
step:393/1393 train_time:47047ms step_avg:122.84ms
step:394/1393 train_time:47174ms step_avg:122.85ms
step:395/1393 train_time:47300ms step_avg:122.86ms
step:396/1393 train_time:47425ms step_avg:122.86ms
step:397/1393 train_time:47549ms step_avg:122.87ms
step:398/1393 train_time:47674ms step_avg:122.87ms
step:399/1393 train_time:47799ms step_avg:122.88ms
step:400/1393 train_time:47925ms step_avg:122.88ms
step:401/1393 train_time:48051ms step_avg:122.89ms
step:402/1393 train_time:48177ms step_avg:122.90ms
step:403/1393 train_time:48302ms step_avg:122.91ms
step:404/1393 train_time:48427ms step_avg:122.91ms
step:405/1393 train_time:48552ms step_avg:122.92ms
step:406/1393 train_time:48676ms step_avg:122.92ms
step:407/1393 train_time:48801ms step_avg:122.92ms
step:408/1393 train_time:48926ms step_avg:122.93ms
step:409/1393 train_time:49052ms step_avg:122.94ms
step:410/1393 train_time:49177ms step_avg:122.94ms
step:411/1393 train_time:49303ms step_avg:122.95ms
step:412/1393 train_time:49430ms step_avg:122.96ms
step:413/1393 train_time:49555ms step_avg:122.97ms
step:414/1393 train_time:49680ms step_avg:122.97ms
step:415/1393 train_time:49806ms step_avg:122.98ms
step:416/1393 train_time:49932ms step_avg:122.99ms
step:417/1393 train_time:50058ms step_avg:122.99ms
step:418/1393 train_time:50183ms step_avg:123.00ms
step:419/1393 train_time:50308ms step_avg:123.00ms
step:420/1393 train_time:50435ms step_avg:123.01ms
step:421/1393 train_time:50561ms step_avg:123.02ms
step:422/1393 train_time:50686ms step_avg:123.02ms
step:423/1393 train_time:50811ms step_avg:123.03ms
step:424/1393 train_time:50937ms step_avg:123.04ms
step:425/1393 train_time:51063ms step_avg:123.04ms
step:426/1393 train_time:51189ms step_avg:123.05ms
step:427/1393 train_time:51315ms step_avg:123.06ms
step:428/1393 train_time:51441ms step_avg:123.07ms
step:429/1393 train_time:51567ms step_avg:123.07ms
step:430/1393 train_time:51693ms step_avg:123.08ms
step:431/1393 train_time:51819ms step_avg:123.09ms
step:432/1393 train_time:51945ms step_avg:123.09ms
step:433/1393 train_time:52071ms step_avg:123.10ms
step:434/1393 train_time:52196ms step_avg:123.10ms
step:435/1393 train_time:52322ms step_avg:123.11ms
step:436/1393 train_time:52448ms step_avg:123.12ms
step:437/1393 train_time:52573ms step_avg:123.12ms
step:438/1393 train_time:52699ms step_avg:123.13ms
step:439/1393 train_time:52825ms step_avg:123.13ms
step:440/1393 train_time:52950ms step_avg:123.14ms
step:441/1393 train_time:53077ms step_avg:123.15ms
step:442/1393 train_time:53203ms step_avg:123.15ms
step:443/1393 train_time:53329ms step_avg:123.16ms
step:444/1393 train_time:53455ms step_avg:123.17ms
step:445/1393 train_time:53580ms step_avg:123.17ms
step:446/1393 train_time:53705ms step_avg:123.18ms
step:447/1393 train_time:53831ms step_avg:123.18ms
step:448/1393 train_time:53957ms step_avg:123.19ms
step:449/1393 train_time:54083ms step_avg:123.20ms
step:450/1393 train_time:54209ms step_avg:123.20ms
step:451/1393 train_time:54335ms step_avg:123.21ms
step:452/1393 train_time:54461ms step_avg:123.21ms
step:453/1393 train_time:54587ms step_avg:123.22ms
step:454/1393 train_time:54713ms step_avg:123.23ms
step:455/1393 train_time:54839ms step_avg:123.23ms
step:456/1393 train_time:54965ms step_avg:123.24ms
step:457/1393 train_time:55092ms step_avg:123.25ms
step:458/1393 train_time:55217ms step_avg:123.25ms
step:459/1393 train_time:55343ms step_avg:123.26ms
step:460/1393 train_time:55469ms step_avg:123.26ms
step:461/1393 train_time:55594ms step_avg:123.27ms
step:462/1393 train_time:55719ms step_avg:123.27ms
step:463/1393 train_time:55845ms step_avg:123.28ms
step:464/1393 train_time:55972ms step_avg:123.29ms
step:465/1393 train_time:56098ms step_avg:123.29ms
step:466/1393 train_time:56223ms step_avg:123.30ms
step:467/1393 train_time:56350ms step_avg:123.30ms
step:468/1393 train_time:56476ms step_avg:123.31ms
step:469/1393 train_time:56601ms step_avg:123.31ms
step:470/1393 train_time:56728ms step_avg:123.32ms
step:471/1393 train_time:56853ms step_avg:123.33ms
step:472/1393 train_time:56979ms step_avg:123.33ms
step:473/1393 train_time:57104ms step_avg:123.34ms
step:474/1393 train_time:57230ms step_avg:123.34ms
step:475/1393 train_time:57355ms step_avg:123.35ms
step:476/1393 train_time:57481ms step_avg:123.35ms
step:477/1393 train_time:57606ms step_avg:123.35ms
step:478/1393 train_time:57731ms step_avg:123.36ms
step:479/1393 train_time:57857ms step_avg:123.36ms
step:480/1393 train_time:57982ms step_avg:123.37ms
step:481/1393 train_time:58109ms step_avg:123.37ms
step:482/1393 train_time:58234ms step_avg:123.38ms
step:483/1393 train_time:58360ms step_avg:123.38ms
step:484/1393 train_time:58487ms step_avg:123.39ms
step:485/1393 train_time:58613ms step_avg:123.40ms
step:486/1393 train_time:58739ms step_avg:123.40ms
step:487/1393 train_time:58865ms step_avg:123.41ms
step:488/1393 train_time:58991ms step_avg:123.41ms
step:489/1393 train_time:59118ms step_avg:123.42ms
step:490/1393 train_time:59244ms step_avg:123.42ms
step:491/1393 train_time:59370ms step_avg:123.43ms
step:492/1393 train_time:59496ms step_avg:123.43ms
step:493/1393 train_time:59621ms step_avg:123.44ms
step:494/1393 train_time:59748ms step_avg:123.45ms
step:495/1393 train_time:59873ms step_avg:123.45ms
step:496/1393 train_time:59998ms step_avg:123.45ms
step:497/1393 train_time:60124ms step_avg:123.46ms
step:498/1393 train_time:60251ms step_avg:123.47ms
step:499/1393 train_time:60376ms step_avg:123.47ms
step:500/1393 train_time:60502ms step_avg:123.47ms
step:500/1393 val_loss:3.6617 train_time:60626ms step_avg:123.73ms
step:501/1393 train_time:60648ms step_avg:123.52ms
step:502/1393 train_time:60768ms step_avg:123.51ms
step:503/1393 train_time:60896ms step_avg:123.52ms
step:504/1393 train_time:61020ms step_avg:123.52ms
step:505/1393 train_time:61145ms step_avg:123.52ms
step:506/1393 train_time:61270ms step_avg:123.53ms
step:507/1393 train_time:61394ms step_avg:123.53ms
step:508/1393 train_time:61520ms step_avg:123.53ms
step:509/1393 train_time:61646ms step_avg:123.54ms
step:510/1393 train_time:61773ms step_avg:123.55ms
step:511/1393 train_time:61900ms step_avg:123.55ms
step:512/1393 train_time:62026ms step_avg:123.56ms
step:513/1393 train_time:62152ms step_avg:123.56ms
step:514/1393 train_time:62276ms step_avg:123.56ms
step:515/1393 train_time:62402ms step_avg:123.57ms
step:516/1393 train_time:62527ms step_avg:123.57ms
step:517/1393 train_time:62654ms step_avg:123.58ms
step:518/1393 train_time:62782ms step_avg:123.59ms
step:519/1393 train_time:62910ms step_avg:123.59ms
step:520/1393 train_time:63037ms step_avg:123.60ms
step:521/1393 train_time:63164ms step_avg:123.61ms
step:522/1393 train_time:63292ms step_avg:123.62ms
step:523/1393 train_time:63419ms step_avg:123.62ms
step:524/1393 train_time:63546ms step_avg:123.63ms
step:525/1393 train_time:63675ms step_avg:123.64ms
step:526/1393 train_time:63802ms step_avg:123.65ms
step:527/1393 train_time:63930ms step_avg:123.66ms
step:528/1393 train_time:64056ms step_avg:123.66ms
step:529/1393 train_time:64183ms step_avg:123.67ms
step:530/1393 train_time:64311ms step_avg:123.68ms
step:531/1393 train_time:64444ms step_avg:123.69ms
step:532/1393 train_time:64566ms step_avg:123.69ms
step:533/1393 train_time:64694ms step_avg:123.70ms
step:534/1393 train_time:64822ms step_avg:123.71ms
step:535/1393 train_time:64950ms step_avg:123.71ms
step:536/1393 train_time:65078ms step_avg:123.72ms
step:537/1393 train_time:65206ms step_avg:123.73ms
step:538/1393 train_time:65333ms step_avg:123.74ms
step:539/1393 train_time:65462ms step_avg:123.75ms
step:540/1393 train_time:65590ms step_avg:123.75ms
step:541/1393 train_time:65718ms step_avg:123.76ms
step:542/1393 train_time:65846ms step_avg:123.77ms
step:543/1393 train_time:65973ms step_avg:123.78ms
step:544/1393 train_time:66100ms step_avg:123.78ms
step:545/1393 train_time:66227ms step_avg:123.79ms
step:546/1393 train_time:66355ms step_avg:123.80ms
step:547/1393 train_time:66484ms step_avg:123.81ms
step:548/1393 train_time:66611ms step_avg:123.81ms
step:549/1393 train_time:66739ms step_avg:123.82ms
step:550/1393 train_time:66866ms step_avg:123.83ms
step:551/1393 train_time:66994ms step_avg:123.83ms
step:552/1393 train_time:67121ms step_avg:123.84ms
step:553/1393 train_time:67249ms step_avg:123.85ms
step:554/1393 train_time:67376ms step_avg:123.85ms
step:555/1393 train_time:67504ms step_avg:123.86ms
step:556/1393 train_time:67631ms step_avg:123.87ms
step:557/1393 train_time:67758ms step_avg:123.87ms
step:558/1393 train_time:67885ms step_avg:123.88ms
step:559/1393 train_time:68013ms step_avg:123.89ms
step:560/1393 train_time:68146ms step_avg:123.90ms
step:561/1393 train_time:68269ms step_avg:123.90ms
step:562/1393 train_time:68397ms step_avg:123.91ms
step:563/1393 train_time:68524ms step_avg:123.91ms
step:564/1393 train_time:68651ms step_avg:123.92ms
step:565/1393 train_time:68778ms step_avg:123.92ms
step:566/1393 train_time:68905ms step_avg:123.93ms
step:567/1393 train_time:69032ms step_avg:123.94ms
step:568/1393 train_time:69160ms step_avg:123.94ms
step:569/1393 train_time:69288ms step_avg:123.95ms
step:570/1393 train_time:69416ms step_avg:123.96ms
step:571/1393 train_time:69545ms step_avg:123.97ms
step:572/1393 train_time:69672ms step_avg:123.97ms
step:573/1393 train_time:69800ms step_avg:123.98ms
step:574/1393 train_time:69928ms step_avg:123.99ms
step:575/1393 train_time:70057ms step_avg:124.00ms
step:576/1393 train_time:70184ms step_avg:124.00ms
step:577/1393 train_time:70311ms step_avg:124.01ms
step:578/1393 train_time:70439ms step_avg:124.01ms
step:579/1393 train_time:70566ms step_avg:124.02ms
step:580/1393 train_time:70693ms step_avg:124.02ms
step:581/1393 train_time:70821ms step_avg:124.03ms
step:582/1393 train_time:70948ms step_avg:124.04ms
step:583/1393 train_time:71076ms step_avg:124.04ms
step:584/1393 train_time:71204ms step_avg:124.05ms
step:585/1393 train_time:71331ms step_avg:124.05ms
step:586/1393 train_time:71459ms step_avg:124.06ms
step:587/1393 train_time:71587ms step_avg:124.07ms
step:588/1393 train_time:71715ms step_avg:124.07ms
step:589/1393 train_time:71845ms step_avg:124.09ms
step:590/1393 train_time:71970ms step_avg:124.09ms
step:591/1393 train_time:72098ms step_avg:124.09ms
step:592/1393 train_time:72225ms step_avg:124.10ms
step:593/1393 train_time:72353ms step_avg:124.10ms
step:594/1393 train_time:72481ms step_avg:124.11ms
step:595/1393 train_time:72608ms step_avg:124.12ms
step:596/1393 train_time:72736ms step_avg:124.12ms
step:597/1393 train_time:72864ms step_avg:124.13ms
step:598/1393 train_time:72991ms step_avg:124.13ms
step:599/1393 train_time:73119ms step_avg:124.14ms
step:600/1393 train_time:73246ms step_avg:124.15ms
step:601/1393 train_time:73374ms step_avg:124.15ms
step:602/1393 train_time:73501ms step_avg:124.16ms
step:603/1393 train_time:73629ms step_avg:124.16ms
step:604/1393 train_time:73757ms step_avg:124.17ms
step:605/1393 train_time:73885ms step_avg:124.18ms
step:606/1393 train_time:74012ms step_avg:124.18ms
step:607/1393 train_time:74139ms step_avg:124.19ms
step:608/1393 train_time:74266ms step_avg:124.19ms
step:609/1393 train_time:74394ms step_avg:124.20ms
step:610/1393 train_time:74521ms step_avg:124.20ms
step:611/1393 train_time:74648ms step_avg:124.21ms
step:612/1393 train_time:74776ms step_avg:124.21ms
step:613/1393 train_time:74904ms step_avg:124.22ms
step:614/1393 train_time:75032ms step_avg:124.23ms
step:615/1393 train_time:75159ms step_avg:124.23ms
step:616/1393 train_time:75286ms step_avg:124.23ms
step:617/1393 train_time:75414ms step_avg:124.24ms
step:618/1393 train_time:75542ms step_avg:124.25ms
step:619/1393 train_time:75669ms step_avg:124.25ms
step:620/1393 train_time:75797ms step_avg:124.26ms
step:621/1393 train_time:75924ms step_avg:124.26ms
step:622/1393 train_time:76052ms step_avg:124.27ms
step:623/1393 train_time:76180ms step_avg:124.27ms
step:624/1393 train_time:76309ms step_avg:124.28ms
step:625/1393 train_time:76436ms step_avg:124.29ms
step:625/1393 val_loss:3.5792 train_time:76562ms step_avg:124.49ms
step:626/1393 train_time:76583ms step_avg:124.32ms
step:627/1393 train_time:76702ms step_avg:124.31ms
step:628/1393 train_time:76832ms step_avg:124.32ms
step:629/1393 train_time:76960ms step_avg:124.33ms
step:630/1393 train_time:77086ms step_avg:124.33ms
step:631/1393 train_time:77213ms step_avg:124.34ms
step:632/1393 train_time:77340ms step_avg:124.34ms
step:633/1393 train_time:77467ms step_avg:124.35ms
step:634/1393 train_time:77597ms step_avg:124.35ms
step:635/1393 train_time:77729ms step_avg:124.37ms
step:636/1393 train_time:77856ms step_avg:124.37ms
step:637/1393 train_time:77984ms step_avg:124.38ms
step:638/1393 train_time:78111ms step_avg:124.38ms
step:639/1393 train_time:78239ms step_avg:124.39ms
step:640/1393 train_time:78366ms step_avg:124.39ms
step:641/1393 train_time:78494ms step_avg:124.40ms
step:642/1393 train_time:78622ms step_avg:124.40ms
step:643/1393 train_time:78750ms step_avg:124.41ms
step:644/1393 train_time:78879ms step_avg:124.41ms
step:645/1393 train_time:79007ms step_avg:124.42ms
step:646/1393 train_time:79135ms step_avg:124.43ms
step:647/1393 train_time:79262ms step_avg:124.43ms
step:648/1393 train_time:79390ms step_avg:124.44ms
step:649/1393 train_time:79523ms step_avg:124.45ms
step:650/1393 train_time:79647ms step_avg:124.45ms
step:651/1393 train_time:79775ms step_avg:124.45ms
step:652/1393 train_time:79903ms step_avg:124.46ms
step:653/1393 train_time:80031ms step_avg:124.47ms
step:654/1393 train_time:80159ms step_avg:124.47ms
step:655/1393 train_time:80287ms step_avg:124.48ms
step:656/1393 train_time:80415ms step_avg:124.48ms
step:657/1393 train_time:80542ms step_avg:124.49ms
step:658/1393 train_time:80670ms step_avg:124.49ms
step:659/1393 train_time:80799ms step_avg:124.50ms
step:660/1393 train_time:80927ms step_avg:124.50ms
step:661/1393 train_time:81054ms step_avg:124.51ms
step:662/1393 train_time:81182ms step_avg:124.51ms
step:663/1393 train_time:81310ms step_avg:124.52ms
step:664/1393 train_time:81438ms step_avg:124.52ms
step:665/1393 train_time:81566ms step_avg:124.53ms
step:666/1393 train_time:81694ms step_avg:124.53ms
step:667/1393 train_time:81822ms step_avg:124.54ms
step:668/1393 train_time:81950ms step_avg:124.54ms
step:669/1393 train_time:82079ms step_avg:124.55ms
step:670/1393 train_time:82206ms step_avg:124.56ms
step:671/1393 train_time:82334ms step_avg:124.56ms
step:672/1393 train_time:82462ms step_avg:124.56ms
step:673/1393 train_time:82590ms step_avg:124.57ms
step:674/1393 train_time:82718ms step_avg:124.57ms
step:675/1393 train_time:82846ms step_avg:124.58ms
step:676/1393 train_time:82974ms step_avg:124.58ms
step:677/1393 train_time:83102ms step_avg:124.59ms
step:678/1393 train_time:83230ms step_avg:124.60ms
step:679/1393 train_time:83359ms step_avg:124.60ms
step:680/1393 train_time:83488ms step_avg:124.61ms
step:681/1393 train_time:83616ms step_avg:124.61ms
step:682/1393 train_time:83743ms step_avg:124.62ms
step:683/1393 train_time:83870ms step_avg:124.62ms
step:684/1393 train_time:83999ms step_avg:124.63ms
step:685/1393 train_time:84127ms step_avg:124.63ms
step:686/1393 train_time:84255ms step_avg:124.64ms
step:687/1393 train_time:84383ms step_avg:124.64ms
step:688/1393 train_time:84511ms step_avg:124.65ms
step:689/1393 train_time:84640ms step_avg:124.65ms
step:690/1393 train_time:84768ms step_avg:124.66ms
step:691/1393 train_time:84896ms step_avg:124.66ms
step:692/1393 train_time:85024ms step_avg:124.67ms
step:693/1393 train_time:85151ms step_avg:124.67ms
step:694/1393 train_time:85280ms step_avg:124.68ms
step:695/1393 train_time:85408ms step_avg:124.68ms
step:696/1393 train_time:85536ms step_avg:124.69ms
step:697/1393 train_time:85664ms step_avg:124.69ms
step:698/1393 train_time:85792ms step_avg:124.70ms
step:699/1393 train_time:85920ms step_avg:124.70ms
step:700/1393 train_time:86047ms step_avg:124.71ms
step:701/1393 train_time:86176ms step_avg:124.71ms
step:702/1393 train_time:86305ms step_avg:124.72ms
step:703/1393 train_time:86433ms step_avg:124.72ms
step:704/1393 train_time:86560ms step_avg:124.73ms
step:705/1393 train_time:86689ms step_avg:124.73ms
step:706/1393 train_time:86817ms step_avg:124.74ms
step:707/1393 train_time:86945ms step_avg:124.74ms
step:708/1393 train_time:87072ms step_avg:124.75ms
step:709/1393 train_time:87200ms step_avg:124.75ms
step:710/1393 train_time:87329ms step_avg:124.76ms
step:711/1393 train_time:87457ms step_avg:124.76ms
step:712/1393 train_time:87585ms step_avg:124.77ms
step:713/1393 train_time:87713ms step_avg:124.77ms
step:714/1393 train_time:87840ms step_avg:124.77ms
step:715/1393 train_time:87969ms step_avg:124.78ms
step:716/1393 train_time:88097ms step_avg:124.78ms
step:717/1393 train_time:88224ms step_avg:124.79ms
step:718/1393 train_time:88352ms step_avg:124.79ms
step:719/1393 train_time:88481ms step_avg:124.80ms
step:720/1393 train_time:88609ms step_avg:124.80ms
step:721/1393 train_time:88738ms step_avg:124.81ms
step:722/1393 train_time:88866ms step_avg:124.81ms
step:723/1393 train_time:88993ms step_avg:124.82ms
step:724/1393 train_time:89121ms step_avg:124.82ms
step:725/1393 train_time:89251ms step_avg:124.83ms
step:726/1393 train_time:89381ms step_avg:124.83ms
step:727/1393 train_time:89512ms step_avg:124.84ms
step:728/1393 train_time:89642ms step_avg:124.85ms
step:729/1393 train_time:89772ms step_avg:124.86ms
step:730/1393 train_time:89901ms step_avg:124.86ms
step:731/1393 train_time:90031ms step_avg:124.87ms
step:732/1393 train_time:90160ms step_avg:124.88ms
step:733/1393 train_time:90290ms step_avg:124.88ms
step:734/1393 train_time:90419ms step_avg:124.89ms
step:735/1393 train_time:90549ms step_avg:124.89ms
step:736/1393 train_time:90677ms step_avg:124.90ms
step:737/1393 train_time:90807ms step_avg:124.91ms
step:738/1393 train_time:90937ms step_avg:124.91ms
step:739/1393 train_time:91067ms step_avg:124.92ms
step:740/1393 train_time:91196ms step_avg:124.93ms
step:741/1393 train_time:91327ms step_avg:124.93ms
step:742/1393 train_time:91458ms step_avg:124.94ms
step:743/1393 train_time:91587ms step_avg:124.95ms
step:744/1393 train_time:91716ms step_avg:124.95ms
step:745/1393 train_time:91847ms step_avg:124.96ms
step:746/1393 train_time:91976ms step_avg:124.97ms
step:747/1393 train_time:92106ms step_avg:124.97ms
step:748/1393 train_time:92236ms step_avg:124.98ms
step:749/1393 train_time:92366ms step_avg:124.99ms
step:750/1393 train_time:92497ms step_avg:125.00ms
step:750/1393 val_loss:3.5258 train_time:92624ms step_avg:125.17ms
step:751/1393 train_time:92646ms step_avg:125.03ms
step:752/1393 train_time:92767ms step_avg:125.02ms
step:753/1393 train_time:92898ms step_avg:125.03ms
step:754/1393 train_time:93027ms step_avg:125.04ms
step:755/1393 train_time:93157ms step_avg:125.04ms
step:756/1393 train_time:93285ms step_avg:125.05ms
step:757/1393 train_time:93416ms step_avg:125.05ms
step:758/1393 train_time:93544ms step_avg:125.06ms
step:759/1393 train_time:93675ms step_avg:125.07ms
step:760/1393 train_time:93807ms step_avg:125.08ms
step:761/1393 train_time:93937ms step_avg:125.08ms
step:762/1393 train_time:94067ms step_avg:125.09ms
step:763/1393 train_time:94196ms step_avg:125.09ms
step:764/1393 train_time:94326ms step_avg:125.10ms
step:765/1393 train_time:94455ms step_avg:125.11ms
step:766/1393 train_time:94584ms step_avg:125.11ms
step:767/1393 train_time:94715ms step_avg:125.12ms
step:768/1393 train_time:94845ms step_avg:125.13ms
step:769/1393 train_time:94976ms step_avg:125.13ms
step:770/1393 train_time:95107ms step_avg:125.14ms
step:771/1393 train_time:95236ms step_avg:125.15ms
step:772/1393 train_time:95366ms step_avg:125.15ms
step:773/1393 train_time:95496ms step_avg:125.16ms
step:774/1393 train_time:95625ms step_avg:125.16ms
step:775/1393 train_time:95754ms step_avg:125.17ms
step:776/1393 train_time:95884ms step_avg:125.17ms
step:777/1393 train_time:96013ms step_avg:125.18ms
step:778/1393 train_time:96145ms step_avg:125.19ms
step:779/1393 train_time:96275ms step_avg:125.19ms
step:780/1393 train_time:96404ms step_avg:125.20ms
step:781/1393 train_time:96534ms step_avg:125.21ms
step:782/1393 train_time:96664ms step_avg:125.21ms
step:783/1393 train_time:96794ms step_avg:125.22ms
step:784/1393 train_time:96924ms step_avg:125.22ms
step:785/1393 train_time:97055ms step_avg:125.23ms
step:786/1393 train_time:97185ms step_avg:125.24ms
step:787/1393 train_time:97315ms step_avg:125.24ms
step:788/1393 train_time:97444ms step_avg:125.25ms
step:789/1393 train_time:97575ms step_avg:125.26ms
step:790/1393 train_time:97703ms step_avg:125.26ms
step:791/1393 train_time:97834ms step_avg:125.27ms
step:792/1393 train_time:97964ms step_avg:125.27ms
step:793/1393 train_time:98093ms step_avg:125.28ms
step:794/1393 train_time:98223ms step_avg:125.28ms
step:795/1393 train_time:98356ms step_avg:125.29ms
step:796/1393 train_time:98486ms step_avg:125.30ms
step:797/1393 train_time:98616ms step_avg:125.31ms
step:798/1393 train_time:98745ms step_avg:125.31ms
step:799/1393 train_time:98879ms step_avg:125.32ms
step:800/1393 train_time:99004ms step_avg:125.32ms
step:801/1393 train_time:99136ms step_avg:125.33ms
step:802/1393 train_time:99266ms step_avg:125.34ms
step:803/1393 train_time:99396ms step_avg:125.34ms
step:804/1393 train_time:99527ms step_avg:125.35ms
step:805/1393 train_time:99657ms step_avg:125.35ms
step:806/1393 train_time:99787ms step_avg:125.36ms
step:807/1393 train_time:99917ms step_avg:125.37ms
step:808/1393 train_time:100046ms step_avg:125.37ms
step:809/1393 train_time:100176ms step_avg:125.38ms
step:810/1393 train_time:100306ms step_avg:125.38ms
step:811/1393 train_time:100437ms step_avg:125.39ms
step:812/1393 train_time:100567ms step_avg:125.40ms
step:813/1393 train_time:100697ms step_avg:125.40ms
step:814/1393 train_time:100826ms step_avg:125.41ms
step:815/1393 train_time:100956ms step_avg:125.41ms
step:816/1393 train_time:101086ms step_avg:125.42ms
step:817/1393 train_time:101216ms step_avg:125.42ms
step:818/1393 train_time:101346ms step_avg:125.43ms
step:819/1393 train_time:101477ms step_avg:125.43ms
step:820/1393 train_time:101607ms step_avg:125.44ms
step:821/1393 train_time:101737ms step_avg:125.45ms
step:822/1393 train_time:101867ms step_avg:125.45ms
step:823/1393 train_time:101997ms step_avg:125.46ms
step:824/1393 train_time:102127ms step_avg:125.46ms
step:825/1393 train_time:102257ms step_avg:125.47ms
step:826/1393 train_time:102388ms step_avg:125.48ms
step:827/1393 train_time:102517ms step_avg:125.48ms
step:828/1393 train_time:102649ms step_avg:125.49ms
step:829/1393 train_time:102778ms step_avg:125.49ms
step:830/1393 train_time:102908ms step_avg:125.50ms
step:831/1393 train_time:103039ms step_avg:125.50ms
step:832/1393 train_time:103169ms step_avg:125.51ms
step:833/1393 train_time:103299ms step_avg:125.52ms
step:834/1393 train_time:103431ms step_avg:125.52ms
step:835/1393 train_time:103561ms step_avg:125.53ms
step:836/1393 train_time:103691ms step_avg:125.53ms
step:837/1393 train_time:103821ms step_avg:125.54ms
step:838/1393 train_time:103950ms step_avg:125.54ms
step:839/1393 train_time:104080ms step_avg:125.55ms
step:840/1393 train_time:104210ms step_avg:125.55ms
step:841/1393 train_time:104340ms step_avg:125.56ms
step:842/1393 train_time:104470ms step_avg:125.57ms
step:843/1393 train_time:104600ms step_avg:125.57ms
step:844/1393 train_time:104730ms step_avg:125.58ms
step:845/1393 train_time:104860ms step_avg:125.58ms
step:846/1393 train_time:104990ms step_avg:125.59ms
step:847/1393 train_time:105121ms step_avg:125.59ms
step:848/1393 train_time:105251ms step_avg:125.60ms
step:849/1393 train_time:105381ms step_avg:125.60ms
step:850/1393 train_time:105512ms step_avg:125.61ms
step:851/1393 train_time:105642ms step_avg:125.62ms
step:852/1393 train_time:105773ms step_avg:125.62ms
step:853/1393 train_time:105903ms step_avg:125.63ms
step:854/1393 train_time:106034ms step_avg:125.63ms
step:855/1393 train_time:106164ms step_avg:125.64ms
step:856/1393 train_time:106295ms step_avg:125.64ms
step:857/1393 train_time:106425ms step_avg:125.65ms
step:858/1393 train_time:106556ms step_avg:125.66ms
step:859/1393 train_time:106686ms step_avg:125.66ms
step:860/1393 train_time:106817ms step_avg:125.67ms
step:861/1393 train_time:106946ms step_avg:125.67ms
step:862/1393 train_time:107078ms step_avg:125.68ms
step:863/1393 train_time:107207ms step_avg:125.68ms
step:864/1393 train_time:107337ms step_avg:125.69ms
step:865/1393 train_time:107468ms step_avg:125.69ms
step:866/1393 train_time:107599ms step_avg:125.70ms
step:867/1393 train_time:107730ms step_avg:125.71ms
step:868/1393 train_time:107860ms step_avg:125.71ms
step:869/1393 train_time:107989ms step_avg:125.72ms
step:870/1393 train_time:108121ms step_avg:125.72ms
step:871/1393 train_time:108251ms step_avg:125.73ms
step:872/1393 train_time:108380ms step_avg:125.73ms
step:873/1393 train_time:108511ms step_avg:125.74ms
step:874/1393 train_time:108641ms step_avg:125.74ms
step:875/1393 train_time:108771ms step_avg:125.75ms
step:875/1393 val_loss:3.4763 train_time:108900ms step_avg:125.90ms
step:876/1393 train_time:108922ms step_avg:125.78ms
step:877/1393 train_time:109043ms step_avg:125.77ms
step:878/1393 train_time:109174ms step_avg:125.78ms
step:879/1393 train_time:109304ms step_avg:125.78ms
step:880/1393 train_time:109433ms step_avg:125.78ms
step:881/1393 train_time:109561ms step_avg:125.79ms
step:882/1393 train_time:109691ms step_avg:125.79ms
step:883/1393 train_time:109820ms step_avg:125.80ms
step:884/1393 train_time:109950ms step_avg:125.80ms
step:885/1393 train_time:110084ms step_avg:125.81ms
step:886/1393 train_time:110214ms step_avg:125.82ms
step:887/1393 train_time:110344ms step_avg:125.82ms
step:888/1393 train_time:110474ms step_avg:125.82ms
step:889/1393 train_time:110605ms step_avg:125.83ms
step:890/1393 train_time:110735ms step_avg:125.84ms
step:891/1393 train_time:110864ms step_avg:125.84ms
step:892/1393 train_time:110994ms step_avg:125.84ms
step:893/1393 train_time:111125ms step_avg:125.85ms
step:894/1393 train_time:111256ms step_avg:125.86ms
step:895/1393 train_time:111386ms step_avg:125.86ms
step:896/1393 train_time:111516ms step_avg:125.86ms
step:897/1393 train_time:111646ms step_avg:125.87ms
step:898/1393 train_time:111777ms step_avg:125.87ms
step:899/1393 train_time:111908ms step_avg:125.88ms
step:900/1393 train_time:112037ms step_avg:125.88ms
step:901/1393 train_time:112168ms step_avg:125.89ms
step:902/1393 train_time:112298ms step_avg:125.89ms
step:903/1393 train_time:112428ms step_avg:125.90ms
step:904/1393 train_time:112558ms step_avg:125.90ms
step:905/1393 train_time:112688ms step_avg:125.91ms
step:906/1393 train_time:112818ms step_avg:125.91ms
step:907/1393 train_time:112950ms step_avg:125.92ms
step:908/1393 train_time:113080ms step_avg:125.92ms
step:909/1393 train_time:113211ms step_avg:125.93ms
step:910/1393 train_time:113342ms step_avg:125.94ms
step:911/1393 train_time:113472ms step_avg:125.94ms
step:912/1393 train_time:113601ms step_avg:125.94ms
step:913/1393 train_time:113731ms step_avg:125.95ms
step:914/1393 train_time:113861ms step_avg:125.95ms
step:915/1393 train_time:113991ms step_avg:125.96ms
step:916/1393 train_time:114122ms step_avg:125.96ms
step:917/1393 train_time:114254ms step_avg:125.97ms
step:918/1393 train_time:114384ms step_avg:125.97ms
step:919/1393 train_time:114517ms step_avg:125.98ms
step:920/1393 train_time:114647ms step_avg:125.99ms
step:921/1393 train_time:114777ms step_avg:125.99ms
step:922/1393 train_time:114907ms step_avg:125.99ms
step:923/1393 train_time:115036ms step_avg:126.00ms
step:924/1393 train_time:115167ms step_avg:126.00ms
step:925/1393 train_time:115298ms step_avg:126.01ms
step:926/1393 train_time:115429ms step_avg:126.01ms
step:927/1393 train_time:115558ms step_avg:126.02ms
step:928/1393 train_time:115688ms step_avg:126.02ms
step:929/1393 train_time:115819ms step_avg:126.03ms
step:930/1393 train_time:115949ms step_avg:126.03ms
step:931/1393 train_time:116081ms step_avg:126.04ms
step:932/1393 train_time:116213ms step_avg:126.04ms
step:933/1393 train_time:116346ms step_avg:126.05ms
step:934/1393 train_time:116477ms step_avg:126.06ms
step:935/1393 train_time:116611ms step_avg:126.07ms
step:936/1393 train_time:116743ms step_avg:126.07ms
step:937/1393 train_time:116877ms step_avg:126.08ms
step:938/1393 train_time:117010ms step_avg:126.09ms
step:939/1393 train_time:117141ms step_avg:126.09ms
step:940/1393 train_time:117274ms step_avg:126.10ms
step:941/1393 train_time:117405ms step_avg:126.11ms
step:942/1393 train_time:117537ms step_avg:126.11ms
step:943/1393 train_time:117671ms step_avg:126.12ms
step:944/1393 train_time:117804ms step_avg:126.13ms
step:945/1393 train_time:117938ms step_avg:126.14ms
step:946/1393 train_time:118070ms step_avg:126.14ms
step:947/1393 train_time:118203ms step_avg:126.15ms
step:948/1393 train_time:118340ms step_avg:126.16ms
step:949/1393 train_time:118468ms step_avg:126.16ms
step:950/1393 train_time:118601ms step_avg:126.17ms
step:951/1393 train_time:118733ms step_avg:126.18ms
step:952/1393 train_time:118865ms step_avg:126.18ms
step:953/1393 train_time:118996ms step_avg:126.19ms
step:954/1393 train_time:119129ms step_avg:126.20ms
step:955/1393 train_time:119262ms step_avg:126.20ms
step:956/1393 train_time:119395ms step_avg:126.21ms
step:957/1393 train_time:119527ms step_avg:126.22ms
step:958/1393 train_time:119660ms step_avg:126.22ms
step:959/1393 train_time:119791ms step_avg:126.23ms
step:960/1393 train_time:119924ms step_avg:126.24ms
step:961/1393 train_time:120056ms step_avg:126.24ms
step:962/1393 train_time:120189ms step_avg:126.25ms
step:963/1393 train_time:120322ms step_avg:126.26ms
step:964/1393 train_time:120454ms step_avg:126.26ms
step:965/1393 train_time:120586ms step_avg:126.27ms
step:966/1393 train_time:120717ms step_avg:126.27ms
step:967/1393 train_time:120851ms step_avg:126.28ms
step:968/1393 train_time:120981ms step_avg:126.29ms
step:969/1393 train_time:121113ms step_avg:126.29ms
step:970/1393 train_time:121245ms step_avg:126.30ms
step:971/1393 train_time:121377ms step_avg:126.30ms
step:972/1393 train_time:121510ms step_avg:126.31ms
step:973/1393 train_time:121641ms step_avg:126.31ms
step:974/1393 train_time:121773ms step_avg:126.32ms
step:975/1393 train_time:121903ms step_avg:126.32ms
step:976/1393 train_time:122035ms step_avg:126.33ms
step:977/1393 train_time:122165ms step_avg:126.33ms
step:978/1393 train_time:122297ms step_avg:126.34ms
step:979/1393 train_time:122430ms step_avg:126.35ms
step:980/1393 train_time:122562ms step_avg:126.35ms
step:981/1393 train_time:122694ms step_avg:126.36ms
step:982/1393 train_time:122826ms step_avg:126.36ms
step:983/1393 train_time:122957ms step_avg:126.37ms
step:984/1393 train_time:123088ms step_avg:126.37ms
step:985/1393 train_time:123219ms step_avg:126.38ms
step:986/1393 train_time:123354ms step_avg:126.39ms
step:987/1393 train_time:123485ms step_avg:126.39ms
step:988/1393 train_time:123617ms step_avg:126.40ms
step:989/1393 train_time:123749ms step_avg:126.40ms
step:990/1393 train_time:123882ms step_avg:126.41ms
step:991/1393 train_time:124014ms step_avg:126.42ms
step:992/1393 train_time:124145ms step_avg:126.42ms
step:993/1393 train_time:124281ms step_avg:126.43ms
step:994/1393 train_time:124413ms step_avg:126.44ms
step:995/1393 train_time:124544ms step_avg:126.44ms
step:996/1393 train_time:124675ms step_avg:126.45ms
step:997/1393 train_time:124807ms step_avg:126.45ms
step:998/1393 train_time:124938ms step_avg:126.46ms
step:999/1393 train_time:125071ms step_avg:126.46ms
step:1000/1393 train_time:125202ms step_avg:126.47ms
step:1000/1393 val_loss:3.4135 train_time:125333ms step_avg:126.60ms
step:1001/1393 train_time:125355ms step_avg:126.49ms
step:1002/1393 train_time:125477ms step_avg:126.49ms
step:1003/1393 train_time:125609ms step_avg:126.49ms
step:1004/1393 train_time:125741ms step_avg:126.50ms
step:1005/1393 train_time:125873ms step_avg:126.51ms
step:1006/1393 train_time:126004ms step_avg:126.51ms
step:1007/1393 train_time:126135ms step_avg:126.51ms
step:1008/1393 train_time:126266ms step_avg:126.52ms
step:1009/1393 train_time:126400ms step_avg:126.53ms
step:1010/1393 train_time:126533ms step_avg:126.53ms
step:1011/1393 train_time:126666ms step_avg:126.54ms
step:1012/1393 train_time:126797ms step_avg:126.54ms
step:1013/1393 train_time:126931ms step_avg:126.55ms
step:1014/1393 train_time:127063ms step_avg:126.56ms
step:1015/1393 train_time:127194ms step_avg:126.56ms
step:1016/1393 train_time:127326ms step_avg:126.57ms
step:1017/1393 train_time:127458ms step_avg:126.57ms
step:1018/1393 train_time:127591ms step_avg:126.58ms
step:1019/1393 train_time:127724ms step_avg:126.58ms
step:1020/1393 train_time:127856ms step_avg:126.59ms
step:1021/1393 train_time:127988ms step_avg:126.60ms
step:1022/1393 train_time:128120ms step_avg:126.60ms
step:1023/1393 train_time:128253ms step_avg:126.61ms
step:1024/1393 train_time:128386ms step_avg:126.61ms
step:1025/1393 train_time:128517ms step_avg:126.62ms
step:1026/1393 train_time:128649ms step_avg:126.62ms
step:1027/1393 train_time:128781ms step_avg:126.63ms
step:1028/1393 train_time:128914ms step_avg:126.63ms
step:1029/1393 train_time:129047ms step_avg:126.64ms
step:1030/1393 train_time:129178ms step_avg:126.64ms
step:1031/1393 train_time:129309ms step_avg:126.65ms
step:1032/1393 train_time:129440ms step_avg:126.65ms
step:1033/1393 train_time:129572ms step_avg:126.66ms
step:1034/1393 train_time:129704ms step_avg:126.66ms
step:1035/1393 train_time:129836ms step_avg:126.67ms
step:1036/1393 train_time:129968ms step_avg:126.67ms
step:1037/1393 train_time:130103ms step_avg:126.68ms
step:1038/1393 train_time:130233ms step_avg:126.69ms
step:1039/1393 train_time:130364ms step_avg:126.69ms
step:1040/1393 train_time:130496ms step_avg:126.70ms
step:1041/1393 train_time:130629ms step_avg:126.70ms
step:1042/1393 train_time:130762ms step_avg:126.71ms
step:1043/1393 train_time:130895ms step_avg:126.71ms
step:1044/1393 train_time:131029ms step_avg:126.72ms
step:1045/1393 train_time:131162ms step_avg:126.73ms
step:1046/1393 train_time:131294ms step_avg:126.73ms
step:1047/1393 train_time:131425ms step_avg:126.74ms
step:1048/1393 train_time:131558ms step_avg:126.74ms
step:1049/1393 train_time:131690ms step_avg:126.75ms
step:1050/1393 train_time:131822ms step_avg:126.75ms
step:1051/1393 train_time:131956ms step_avg:126.76ms
step:1052/1393 train_time:132088ms step_avg:126.76ms
step:1053/1393 train_time:132219ms step_avg:126.77ms
step:1054/1393 train_time:132351ms step_avg:126.77ms
step:1055/1393 train_time:132484ms step_avg:126.78ms
step:1056/1393 train_time:132616ms step_avg:126.78ms
step:1057/1393 train_time:132749ms step_avg:126.79ms
step:1058/1393 train_time:132881ms step_avg:126.80ms
step:1059/1393 train_time:133014ms step_avg:126.80ms
step:1060/1393 train_time:133148ms step_avg:126.81ms
step:1061/1393 train_time:133279ms step_avg:126.81ms
step:1062/1393 train_time:133411ms step_avg:126.82ms
step:1063/1393 train_time:133542ms step_avg:126.82ms
step:1064/1393 train_time:133674ms step_avg:126.83ms
step:1065/1393 train_time:133805ms step_avg:126.83ms
step:1066/1393 train_time:133939ms step_avg:126.84ms
step:1067/1393 train_time:134072ms step_avg:126.84ms
step:1068/1393 train_time:134204ms step_avg:126.85ms
step:1069/1393 train_time:134338ms step_avg:126.85ms
step:1070/1393 train_time:134470ms step_avg:126.86ms
step:1071/1393 train_time:134603ms step_avg:126.86ms
step:1072/1393 train_time:134734ms step_avg:126.87ms
step:1073/1393 train_time:134866ms step_avg:126.87ms
step:1074/1393 train_time:134998ms step_avg:126.88ms
step:1075/1393 train_time:135131ms step_avg:126.88ms
step:1076/1393 train_time:135264ms step_avg:126.89ms
step:1077/1393 train_time:135396ms step_avg:126.89ms
step:1078/1393 train_time:135528ms step_avg:126.90ms
step:1079/1393 train_time:135664ms step_avg:126.91ms
step:1080/1393 train_time:135795ms step_avg:126.91ms
step:1081/1393 train_time:135927ms step_avg:126.92ms
step:1082/1393 train_time:136058ms step_avg:126.92ms
step:1083/1393 train_time:136190ms step_avg:126.92ms
step:1084/1393 train_time:136324ms step_avg:126.93ms
step:1085/1393 train_time:136456ms step_avg:126.94ms
step:1086/1393 train_time:136588ms step_avg:126.94ms
step:1087/1393 train_time:136721ms step_avg:126.95ms
step:1088/1393 train_time:136852ms step_avg:126.95ms
step:1089/1393 train_time:136985ms step_avg:126.96ms
step:1090/1393 train_time:137118ms step_avg:126.96ms
step:1091/1393 train_time:137250ms step_avg:126.97ms
step:1092/1393 train_time:137383ms step_avg:126.97ms
step:1093/1393 train_time:137514ms step_avg:126.98ms
step:1094/1393 train_time:137646ms step_avg:126.98ms
step:1095/1393 train_time:137778ms step_avg:126.98ms
step:1096/1393 train_time:137911ms step_avg:126.99ms
step:1097/1393 train_time:138044ms step_avg:127.00ms
step:1098/1393 train_time:138178ms step_avg:127.00ms
step:1099/1393 train_time:138309ms step_avg:127.01ms
step:1100/1393 train_time:138441ms step_avg:127.01ms
step:1101/1393 train_time:138573ms step_avg:127.01ms
step:1102/1393 train_time:138705ms step_avg:127.02ms
step:1103/1393 train_time:138838ms step_avg:127.03ms
step:1104/1393 train_time:138970ms step_avg:127.03ms
step:1105/1393 train_time:139104ms step_avg:127.04ms
step:1106/1393 train_time:139237ms step_avg:127.04ms
step:1107/1393 train_time:139368ms step_avg:127.04ms
step:1108/1393 train_time:139503ms step_avg:127.05ms
step:1109/1393 train_time:139636ms step_avg:127.06ms
step:1110/1393 train_time:139768ms step_avg:127.06ms
step:1111/1393 train_time:139900ms step_avg:127.07ms
step:1112/1393 train_time:140032ms step_avg:127.07ms
step:1113/1393 train_time:140164ms step_avg:127.08ms
step:1114/1393 train_time:140297ms step_avg:127.08ms
step:1115/1393 train_time:140429ms step_avg:127.09ms
step:1116/1393 train_time:140562ms step_avg:127.09ms
step:1117/1393 train_time:140694ms step_avg:127.09ms
step:1118/1393 train_time:140829ms step_avg:127.10ms
step:1119/1393 train_time:140961ms step_avg:127.11ms
step:1120/1393 train_time:141092ms step_avg:127.11ms
step:1121/1393 train_time:141224ms step_avg:127.11ms
step:1122/1393 train_time:141355ms step_avg:127.12ms
step:1123/1393 train_time:141488ms step_avg:127.12ms
step:1124/1393 train_time:141621ms step_avg:127.13ms
step:1125/1393 train_time:141752ms step_avg:127.13ms
step:1125/1393 val_loss:3.3633 train_time:141885ms step_avg:127.25ms
step:1126/1393 train_time:141907ms step_avg:127.16ms
step:1127/1393 train_time:142029ms step_avg:127.15ms
step:1128/1393 train_time:142161ms step_avg:127.16ms
step:1129/1393 train_time:142294ms step_avg:127.16ms
step:1130/1393 train_time:142425ms step_avg:127.17ms
step:1131/1393 train_time:142558ms step_avg:127.17ms
step:1132/1393 train_time:142690ms step_avg:127.17ms
step:1133/1393 train_time:142821ms step_avg:127.18ms
step:1134/1393 train_time:142956ms step_avg:127.18ms
step:1135/1393 train_time:143090ms step_avg:127.19ms
step:1136/1393 train_time:143224ms step_avg:127.20ms
step:1137/1393 train_time:143355ms step_avg:127.20ms
step:1138/1393 train_time:143490ms step_avg:127.21ms
step:1139/1393 train_time:143623ms step_avg:127.21ms
step:1140/1393 train_time:143756ms step_avg:127.22ms
step:1141/1393 train_time:143890ms step_avg:127.22ms
step:1142/1393 train_time:144026ms step_avg:127.23ms
step:1143/1393 train_time:144162ms step_avg:127.24ms
step:1144/1393 train_time:144296ms step_avg:127.25ms
step:1145/1393 train_time:144430ms step_avg:127.25ms
step:1146/1393 train_time:144563ms step_avg:127.26ms
step:1147/1393 train_time:144697ms step_avg:127.26ms
step:1148/1393 train_time:144831ms step_avg:127.27ms
step:1149/1393 train_time:144965ms step_avg:127.27ms
step:1150/1393 train_time:145098ms step_avg:127.28ms
step:1151/1393 train_time:145233ms step_avg:127.29ms
step:1152/1393 train_time:145366ms step_avg:127.29ms
step:1153/1393 train_time:145501ms step_avg:127.30ms
step:1154/1393 train_time:145635ms step_avg:127.30ms
step:1155/1393 train_time:145768ms step_avg:127.31ms
step:1156/1393 train_time:145905ms step_avg:127.32ms
step:1157/1393 train_time:146039ms step_avg:127.32ms
step:1158/1393 train_time:146173ms step_avg:127.33ms
step:1159/1393 train_time:146306ms step_avg:127.33ms
step:1160/1393 train_time:146440ms step_avg:127.34ms
step:1161/1393 train_time:146574ms step_avg:127.34ms
step:1162/1393 train_time:146708ms step_avg:127.35ms
step:1163/1393 train_time:146841ms step_avg:127.36ms
step:1164/1393 train_time:146975ms step_avg:127.36ms
step:1165/1393 train_time:147107ms step_avg:127.37ms
step:1166/1393 train_time:147240ms step_avg:127.37ms
step:1167/1393 train_time:147373ms step_avg:127.38ms
step:1168/1393 train_time:147507ms step_avg:127.38ms
step:1169/1393 train_time:147642ms step_avg:127.39ms
step:1170/1393 train_time:147775ms step_avg:127.39ms
step:1171/1393 train_time:147909ms step_avg:127.40ms
step:1172/1393 train_time:148042ms step_avg:127.40ms
step:1173/1393 train_time:148176ms step_avg:127.41ms
step:1174/1393 train_time:148314ms step_avg:127.42ms
step:1175/1393 train_time:148447ms step_avg:127.42ms
step:1176/1393 train_time:148582ms step_avg:127.43ms
step:1177/1393 train_time:148719ms step_avg:127.44ms
step:1178/1393 train_time:148851ms step_avg:127.44ms
step:1179/1393 train_time:148984ms step_avg:127.45ms
step:1180/1393 train_time:149120ms step_avg:127.45ms
step:1181/1393 train_time:149256ms step_avg:127.46ms
step:1182/1393 train_time:149389ms step_avg:127.47ms
step:1183/1393 train_time:149525ms step_avg:127.47ms
step:1184/1393 train_time:149658ms step_avg:127.48ms
step:1185/1393 train_time:149792ms step_avg:127.48ms
step:1186/1393 train_time:149924ms step_avg:127.49ms
step:1187/1393 train_time:150066ms step_avg:127.50ms
step:1188/1393 train_time:150198ms step_avg:127.50ms
step:1189/1393 train_time:150333ms step_avg:127.51ms
step:1190/1393 train_time:150466ms step_avg:127.51ms
step:1191/1393 train_time:150600ms step_avg:127.52ms
step:1192/1393 train_time:150733ms step_avg:127.52ms
step:1193/1393 train_time:150866ms step_avg:127.53ms
step:1194/1393 train_time:150999ms step_avg:127.53ms
step:1195/1393 train_time:151134ms step_avg:127.54ms
step:1196/1393 train_time:151267ms step_avg:127.54ms
step:1197/1393 train_time:151401ms step_avg:127.55ms
step:1198/1393 train_time:151538ms step_avg:127.56ms
step:1199/1393 train_time:151671ms step_avg:127.56ms
step:1200/1393 train_time:151805ms step_avg:127.57ms
step:1201/1393 train_time:151937ms step_avg:127.57ms
step:1202/1393 train_time:152074ms step_avg:127.58ms
step:1203/1393 train_time:152210ms step_avg:127.59ms
step:1204/1393 train_time:152344ms step_avg:127.59ms
step:1205/1393 train_time:152478ms step_avg:127.60ms
step:1206/1393 train_time:152613ms step_avg:127.60ms
step:1207/1393 train_time:152746ms step_avg:127.61ms
step:1208/1393 train_time:152880ms step_avg:127.61ms
step:1209/1393 train_time:153014ms step_avg:127.62ms
step:1210/1393 train_time:153149ms step_avg:127.62ms
step:1211/1393 train_time:153284ms step_avg:127.63ms
step:1212/1393 train_time:153417ms step_avg:127.63ms
step:1213/1393 train_time:153550ms step_avg:127.64ms
step:1214/1393 train_time:153685ms step_avg:127.65ms
step:1215/1393 train_time:153820ms step_avg:127.65ms
step:1216/1393 train_time:153952ms step_avg:127.66ms
step:1217/1393 train_time:154088ms step_avg:127.66ms
step:1218/1393 train_time:154221ms step_avg:127.67ms
step:1219/1393 train_time:154354ms step_avg:127.67ms
step:1220/1393 train_time:154488ms step_avg:127.68ms
step:1221/1393 train_time:154620ms step_avg:127.68ms
step:1222/1393 train_time:154754ms step_avg:127.69ms
step:1223/1393 train_time:154888ms step_avg:127.69ms
step:1224/1393 train_time:155021ms step_avg:127.69ms
step:1225/1393 train_time:155157ms step_avg:127.70ms
step:1226/1393 train_time:155290ms step_avg:127.71ms
step:1227/1393 train_time:155423ms step_avg:127.71ms
step:1228/1393 train_time:155557ms step_avg:127.72ms
step:1229/1393 train_time:155690ms step_avg:127.72ms
step:1230/1393 train_time:155826ms step_avg:127.73ms
step:1231/1393 train_time:155963ms step_avg:127.73ms
step:1232/1393 train_time:156097ms step_avg:127.74ms
step:1233/1393 train_time:156230ms step_avg:127.74ms
step:1234/1393 train_time:156363ms step_avg:127.75ms
step:1235/1393 train_time:156497ms step_avg:127.75ms
step:1236/1393 train_time:156630ms step_avg:127.76ms
step:1237/1393 train_time:156763ms step_avg:127.76ms
step:1238/1393 train_time:156899ms step_avg:127.77ms
step:1239/1393 train_time:157033ms step_avg:127.77ms
step:1240/1393 train_time:157167ms step_avg:127.78ms
step:1241/1393 train_time:157303ms step_avg:127.78ms
step:1242/1393 train_time:157436ms step_avg:127.79ms
step:1243/1393 train_time:157571ms step_avg:127.79ms
step:1244/1393 train_time:157705ms step_avg:127.80ms
step:1245/1393 train_time:157838ms step_avg:127.80ms
step:1246/1393 train_time:157971ms step_avg:127.81ms
step:1247/1393 train_time:158105ms step_avg:127.81ms
step:1248/1393 train_time:158238ms step_avg:127.82ms
step:1249/1393 train_time:158370ms step_avg:127.82ms
step:1250/1393 train_time:158504ms step_avg:127.83ms
step:1250/1393 val_loss:3.3168 train_time:158638ms step_avg:127.93ms
step:1251/1393 train_time:158659ms step_avg:127.85ms
step:1252/1393 train_time:158783ms step_avg:127.84ms
step:1253/1393 train_time:158915ms step_avg:127.85ms
step:1254/1393 train_time:159047ms step_avg:127.85ms
step:1255/1393 train_time:159186ms step_avg:127.86ms
step:1256/1393 train_time:159319ms step_avg:127.86ms
step:1257/1393 train_time:159451ms step_avg:127.87ms
step:1258/1393 train_time:159584ms step_avg:127.87ms
step:1259/1393 train_time:159721ms step_avg:127.88ms
step:1260/1393 train_time:159857ms step_avg:127.89ms
step:1261/1393 train_time:159994ms step_avg:127.89ms
step:1262/1393 train_time:160125ms step_avg:127.90ms
step:1263/1393 train_time:160259ms step_avg:127.90ms
step:1264/1393 train_time:160391ms step_avg:127.90ms
step:1265/1393 train_time:160524ms step_avg:127.91ms
step:1266/1393 train_time:160658ms step_avg:127.91ms
step:1267/1393 train_time:160792ms step_avg:127.92ms
step:1268/1393 train_time:160926ms step_avg:127.92ms
step:1269/1393 train_time:161061ms step_avg:127.93ms
step:1270/1393 train_time:161195ms step_avg:127.93ms
step:1271/1393 train_time:161330ms step_avg:127.94ms
step:1272/1393 train_time:161463ms step_avg:127.94ms
step:1273/1393 train_time:161596ms step_avg:127.95ms
step:1274/1393 train_time:161730ms step_avg:127.95ms
step:1275/1393 train_time:161865ms step_avg:127.96ms
step:1276/1393 train_time:161998ms step_avg:127.96ms
step:1277/1393 train_time:162130ms step_avg:127.96ms
step:1278/1393 train_time:162264ms step_avg:127.97ms
step:1279/1393 train_time:162398ms step_avg:127.97ms
step:1280/1393 train_time:162535ms step_avg:127.98ms
step:1281/1393 train_time:162669ms step_avg:127.99ms
step:1282/1393 train_time:162802ms step_avg:127.99ms
step:1283/1393 train_time:162935ms step_avg:127.99ms
step:1284/1393 train_time:163069ms step_avg:128.00ms
step:1285/1393 train_time:163202ms step_avg:128.00ms
step:1286/1393 train_time:163336ms step_avg:128.01ms
step:1287/1393 train_time:163470ms step_avg:128.01ms
step:1288/1393 train_time:163605ms step_avg:128.02ms
step:1289/1393 train_time:163741ms step_avg:128.02ms
step:1290/1393 train_time:163876ms step_avg:128.03ms
step:1291/1393 train_time:164012ms step_avg:128.03ms
step:1292/1393 train_time:164146ms step_avg:128.04ms
step:1293/1393 train_time:164283ms step_avg:128.05ms
step:1294/1393 train_time:164416ms step_avg:128.05ms
step:1295/1393 train_time:164550ms step_avg:128.05ms
step:1296/1393 train_time:164684ms step_avg:128.06ms
step:1297/1393 train_time:164818ms step_avg:128.06ms
step:1298/1393 train_time:164951ms step_avg:128.07ms
step:1299/1393 train_time:165085ms step_avg:128.07ms
step:1300/1393 train_time:165218ms step_avg:128.08ms
step:1301/1393 train_time:165351ms step_avg:128.08ms
step:1302/1393 train_time:165485ms step_avg:128.08ms
step:1303/1393 train_time:165619ms step_avg:128.09ms
step:1304/1393 train_time:165755ms step_avg:128.09ms
step:1305/1393 train_time:165888ms step_avg:128.10ms
step:1306/1393 train_time:166022ms step_avg:128.10ms
step:1307/1393 train_time:166155ms step_avg:128.11ms
step:1308/1393 train_time:166290ms step_avg:128.11ms
step:1309/1393 train_time:166424ms step_avg:128.12ms
step:1310/1393 train_time:166559ms step_avg:128.12ms
step:1311/1393 train_time:166692ms step_avg:128.13ms
step:1312/1393 train_time:166825ms step_avg:128.13ms
step:1313/1393 train_time:166960ms step_avg:128.13ms
step:1314/1393 train_time:167093ms step_avg:128.14ms
step:1315/1393 train_time:167228ms step_avg:128.14ms
step:1316/1393 train_time:167362ms step_avg:128.15ms
step:1317/1393 train_time:167495ms step_avg:128.15ms
step:1318/1393 train_time:167631ms step_avg:128.16ms
step:1319/1393 train_time:167766ms step_avg:128.16ms
step:1320/1393 train_time:167900ms step_avg:128.17ms
step:1321/1393 train_time:168033ms step_avg:128.17ms
step:1322/1393 train_time:168171ms step_avg:128.18ms
step:1323/1393 train_time:168304ms step_avg:128.18ms
step:1324/1393 train_time:168437ms step_avg:128.19ms
step:1325/1393 train_time:168571ms step_avg:128.19ms
step:1326/1393 train_time:168705ms step_avg:128.20ms
step:1327/1393 train_time:168839ms step_avg:128.20ms
step:1328/1393 train_time:168972ms step_avg:128.20ms
step:1329/1393 train_time:169110ms step_avg:128.21ms
step:1330/1393 train_time:169245ms step_avg:128.22ms
step:1331/1393 train_time:169382ms step_avg:128.22ms
step:1332/1393 train_time:169520ms step_avg:128.23ms
step:1333/1393 train_time:169654ms step_avg:128.23ms
step:1334/1393 train_time:169788ms step_avg:128.24ms
step:1335/1393 train_time:169920ms step_avg:128.24ms
step:1336/1393 train_time:170056ms step_avg:128.25ms
step:1337/1393 train_time:170190ms step_avg:128.25ms
step:1338/1393 train_time:170323ms step_avg:128.26ms
step:1339/1393 train_time:170457ms step_avg:128.26ms
step:1340/1393 train_time:170592ms step_avg:128.26ms
step:1341/1393 train_time:170725ms step_avg:128.27ms
step:1342/1393 train_time:170859ms step_avg:128.27ms
step:1343/1393 train_time:170992ms step_avg:128.28ms
step:1344/1393 train_time:171127ms step_avg:128.28ms
step:1345/1393 train_time:171263ms step_avg:128.29ms
step:1346/1393 train_time:171397ms step_avg:128.29ms
step:1347/1393 train_time:171534ms step_avg:128.30ms
step:1348/1393 train_time:171668ms step_avg:128.30ms
step:1349/1393 train_time:171803ms step_avg:128.31ms
step:1350/1393 train_time:171936ms step_avg:128.31ms
step:1351/1393 train_time:172073ms step_avg:128.32ms
step:1352/1393 train_time:172211ms step_avg:128.32ms
step:1353/1393 train_time:172347ms step_avg:128.33ms
step:1354/1393 train_time:172482ms step_avg:128.33ms
step:1355/1393 train_time:172615ms step_avg:128.34ms
step:1356/1393 train_time:172749ms step_avg:128.34ms
step:1357/1393 train_time:172883ms step_avg:128.35ms
step:1358/1393 train_time:173020ms step_avg:128.35ms
step:1359/1393 train_time:173154ms step_avg:128.36ms
step:1360/1393 train_time:173291ms step_avg:128.36ms
step:1361/1393 train_time:173427ms step_avg:128.37ms
step:1362/1393 train_time:173564ms step_avg:128.38ms
step:1363/1393 train_time:173703ms step_avg:128.38ms
step:1364/1393 train_time:173839ms step_avg:128.39ms
step:1365/1393 train_time:173971ms step_avg:128.39ms
step:1366/1393 train_time:174107ms step_avg:128.40ms
step:1367/1393 train_time:174243ms step_avg:128.40ms
step:1368/1393 train_time:174379ms step_avg:128.41ms
step:1369/1393 train_time:174516ms step_avg:128.42ms
step:1370/1393 train_time:174654ms step_avg:128.42ms
step:1371/1393 train_time:174789ms step_avg:128.43ms
step:1372/1393 train_time:174927ms step_avg:128.43ms
step:1373/1393 train_time:175062ms step_avg:128.44ms
step:1374/1393 train_time:175199ms step_avg:128.45ms
step:1375/1393 train_time:175332ms step_avg:128.45ms
step:1375/1393 val_loss:3.2830 train_time:175464ms step_avg:128.55ms
step:1376/1393 train_time:175485ms step_avg:128.47ms
step:1377/1393 train_time:175608ms step_avg:128.46ms
step:1378/1393 train_time:175743ms step_avg:128.47ms
step:1379/1393 train_time:175878ms step_avg:128.47ms
step:1380/1393 train_time:176013ms step_avg:128.48ms
step:1381/1393 train_time:176149ms step_avg:128.48ms
step:1382/1393 train_time:176284ms step_avg:128.49ms
step:1383/1393 train_time:176419ms step_avg:128.49ms
step:1384/1393 train_time:176557ms step_avg:128.50ms
step:1385/1393 train_time:176690ms step_avg:128.50ms
step:1386/1393 train_time:176825ms step_avg:128.51ms
step:1387/1393 train_time:176960ms step_avg:128.51ms
step:1388/1393 train_time:177094ms step_avg:128.52ms
step:1389/1393 train_time:177231ms step_avg:128.52ms
step:1390/1393 train_time:177364ms step_avg:128.52ms
step:1391/1393 train_time:177499ms step_avg:128.53ms
step:1392/1393 train_time:177635ms step_avg:128.53ms
step:1393/1393 train_time:177769ms step_avg:128.54ms
step:1393/1393 val_loss:3.2793 train_time:177902ms step_avg:128.64ms
peak memory allocated: 37653 MiB reserved: 41736 MiB
