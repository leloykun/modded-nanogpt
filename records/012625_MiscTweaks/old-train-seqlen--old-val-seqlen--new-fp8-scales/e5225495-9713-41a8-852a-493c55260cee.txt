import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:33:20 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23530ms step_avg:nanms
step:2/1393 train_time:23936ms step_avg:nanms
step:3/1393 train_time:24057ms step_avg:nanms
step:4/1393 train_time:24177ms step_avg:nanms
step:5/1393 train_time:24297ms step_avg:nanms
step:6/1393 train_time:24418ms step_avg:nanms
step:7/1393 train_time:24538ms step_avg:nanms
step:8/1393 train_time:24660ms step_avg:nanms
step:9/1393 train_time:24780ms step_avg:nanms
step:10/1393 train_time:24903ms step_avg:nanms
step:11/1393 train_time:127ms step_avg:nanms
step:12/1393 train_time:249ms step_avg:nanms
step:13/1393 train_time:370ms step_avg:123.38ms
step:14/1393 train_time:491ms step_avg:122.63ms
step:15/1393 train_time:611ms step_avg:122.18ms
step:16/1393 train_time:733ms step_avg:122.14ms
step:17/1393 train_time:853ms step_avg:121.93ms
step:18/1393 train_time:975ms step_avg:121.91ms
step:19/1393 train_time:1100ms step_avg:122.21ms
step:20/1393 train_time:1223ms step_avg:122.32ms
step:21/1393 train_time:1345ms step_avg:122.26ms
step:22/1393 train_time:1467ms step_avg:122.22ms
step:23/1393 train_time:1587ms step_avg:122.09ms
step:24/1393 train_time:1708ms step_avg:121.99ms
step:25/1393 train_time:1828ms step_avg:121.90ms
step:26/1393 train_time:1949ms step_avg:121.83ms
step:27/1393 train_time:2072ms step_avg:121.86ms
step:28/1393 train_time:2194ms step_avg:121.90ms
step:29/1393 train_time:2317ms step_avg:121.93ms
step:30/1393 train_time:2439ms step_avg:121.97ms
step:31/1393 train_time:2562ms step_avg:122.01ms
step:32/1393 train_time:2684ms step_avg:121.98ms
step:33/1393 train_time:2806ms step_avg:121.99ms
step:34/1393 train_time:2928ms step_avg:121.99ms
step:35/1393 train_time:3048ms step_avg:121.93ms
step:36/1393 train_time:3169ms step_avg:121.90ms
step:37/1393 train_time:3293ms step_avg:121.95ms
step:38/1393 train_time:3414ms step_avg:121.94ms
step:39/1393 train_time:3537ms step_avg:121.95ms
step:40/1393 train_time:3658ms step_avg:121.94ms
step:41/1393 train_time:3779ms step_avg:121.92ms
step:42/1393 train_time:3900ms step_avg:121.89ms
step:43/1393 train_time:4022ms step_avg:121.87ms
step:44/1393 train_time:4143ms step_avg:121.86ms
step:45/1393 train_time:4265ms step_avg:121.85ms
step:46/1393 train_time:4386ms step_avg:121.84ms
step:47/1393 train_time:4508ms step_avg:121.84ms
step:48/1393 train_time:4630ms step_avg:121.83ms
step:49/1393 train_time:4750ms step_avg:121.80ms
step:50/1393 train_time:4871ms step_avg:121.78ms
step:51/1393 train_time:4992ms step_avg:121.75ms
step:52/1393 train_time:5113ms step_avg:121.74ms
step:53/1393 train_time:5235ms step_avg:121.75ms
step:54/1393 train_time:5358ms step_avg:121.76ms
step:55/1393 train_time:5480ms step_avg:121.79ms
step:56/1393 train_time:5602ms step_avg:121.78ms
step:57/1393 train_time:5723ms step_avg:121.77ms
step:58/1393 train_time:5844ms step_avg:121.75ms
step:59/1393 train_time:5966ms step_avg:121.75ms
step:60/1393 train_time:6087ms step_avg:121.75ms
step:61/1393 train_time:6208ms step_avg:121.73ms
step:62/1393 train_time:6329ms step_avg:121.71ms
step:63/1393 train_time:6450ms step_avg:121.70ms
step:64/1393 train_time:6572ms step_avg:121.71ms
step:65/1393 train_time:6693ms step_avg:121.69ms
step:66/1393 train_time:6815ms step_avg:121.70ms
step:67/1393 train_time:6936ms step_avg:121.68ms
step:68/1393 train_time:7057ms step_avg:121.67ms
step:69/1393 train_time:7178ms step_avg:121.66ms
step:70/1393 train_time:7299ms step_avg:121.65ms
step:71/1393 train_time:7420ms step_avg:121.64ms
step:72/1393 train_time:7542ms step_avg:121.64ms
step:73/1393 train_time:7663ms step_avg:121.64ms
step:74/1393 train_time:7785ms step_avg:121.65ms
step:75/1393 train_time:7907ms step_avg:121.64ms
step:76/1393 train_time:8028ms step_avg:121.64ms
step:77/1393 train_time:8148ms step_avg:121.62ms
step:78/1393 train_time:8270ms step_avg:121.61ms
step:79/1393 train_time:8391ms step_avg:121.61ms
step:80/1393 train_time:8514ms step_avg:121.63ms
step:81/1393 train_time:8635ms step_avg:121.63ms
step:82/1393 train_time:8757ms step_avg:121.62ms
step:83/1393 train_time:8878ms step_avg:121.61ms
step:84/1393 train_time:8998ms step_avg:121.60ms
step:85/1393 train_time:9119ms step_avg:121.59ms
step:86/1393 train_time:9241ms step_avg:121.59ms
step:87/1393 train_time:9361ms step_avg:121.58ms
step:88/1393 train_time:9484ms step_avg:121.58ms
step:89/1393 train_time:9605ms step_avg:121.58ms
step:90/1393 train_time:9726ms step_avg:121.58ms
step:91/1393 train_time:9848ms step_avg:121.58ms
step:92/1393 train_time:9969ms step_avg:121.57ms
step:93/1393 train_time:10089ms step_avg:121.56ms
step:94/1393 train_time:10210ms step_avg:121.55ms
step:95/1393 train_time:10332ms step_avg:121.55ms
step:96/1393 train_time:10453ms step_avg:121.54ms
step:97/1393 train_time:10573ms step_avg:121.53ms
step:98/1393 train_time:10695ms step_avg:121.54ms
step:99/1393 train_time:10817ms step_avg:121.54ms
step:100/1393 train_time:10939ms step_avg:121.54ms
step:101/1393 train_time:11061ms step_avg:121.55ms
step:102/1393 train_time:11182ms step_avg:121.54ms
step:103/1393 train_time:11303ms step_avg:121.53ms
step:104/1393 train_time:11424ms step_avg:121.53ms
step:105/1393 train_time:11546ms step_avg:121.54ms
step:106/1393 train_time:11668ms step_avg:121.54ms
step:107/1393 train_time:11791ms step_avg:121.56ms
step:108/1393 train_time:11914ms step_avg:121.57ms
step:109/1393 train_time:12037ms step_avg:121.58ms
step:110/1393 train_time:12159ms step_avg:121.59ms
step:111/1393 train_time:12281ms step_avg:121.59ms
step:112/1393 train_time:12402ms step_avg:121.59ms
step:113/1393 train_time:12525ms step_avg:121.60ms
step:114/1393 train_time:12647ms step_avg:121.60ms
step:115/1393 train_time:12769ms step_avg:121.61ms
step:116/1393 train_time:12891ms step_avg:121.61ms
step:117/1393 train_time:13014ms step_avg:121.62ms
step:118/1393 train_time:13136ms step_avg:121.63ms
step:119/1393 train_time:13258ms step_avg:121.64ms
step:120/1393 train_time:13380ms step_avg:121.64ms
step:121/1393 train_time:13502ms step_avg:121.64ms
step:122/1393 train_time:13624ms step_avg:121.64ms
step:123/1393 train_time:13747ms step_avg:121.66ms
step:124/1393 train_time:13869ms step_avg:121.66ms
step:125/1393 train_time:13991ms step_avg:121.66ms
step:125/1393 val_loss:4.3889 train_time:14112ms step_avg:122.72ms
step:126/1393 train_time:14136ms step_avg:121.86ms
step:127/1393 train_time:14252ms step_avg:121.81ms
step:128/1393 train_time:14378ms step_avg:121.84ms
step:129/1393 train_time:14499ms step_avg:121.84ms
step:130/1393 train_time:14620ms step_avg:121.84ms
step:131/1393 train_time:14741ms step_avg:121.83ms
step:132/1393 train_time:14862ms step_avg:121.82ms
step:133/1393 train_time:14983ms step_avg:121.81ms
step:134/1393 train_time:15104ms step_avg:121.81ms
step:135/1393 train_time:15229ms step_avg:121.83ms
step:136/1393 train_time:15352ms step_avg:121.84ms
step:137/1393 train_time:15475ms step_avg:121.85ms
step:138/1393 train_time:15597ms step_avg:121.85ms
step:139/1393 train_time:15719ms step_avg:121.85ms
step:140/1393 train_time:15840ms step_avg:121.85ms
step:141/1393 train_time:15962ms step_avg:121.85ms
step:142/1393 train_time:16084ms step_avg:121.85ms
step:143/1393 train_time:16206ms step_avg:121.85ms
step:144/1393 train_time:16329ms step_avg:121.86ms
step:145/1393 train_time:16451ms step_avg:121.86ms
step:146/1393 train_time:16574ms step_avg:121.87ms
step:147/1393 train_time:16696ms step_avg:121.87ms
step:148/1393 train_time:16818ms step_avg:121.87ms
step:149/1393 train_time:16941ms step_avg:121.88ms
step:150/1393 train_time:17063ms step_avg:121.88ms
step:151/1393 train_time:17185ms step_avg:121.88ms
step:152/1393 train_time:17306ms step_avg:121.88ms
step:153/1393 train_time:17429ms step_avg:121.88ms
step:154/1393 train_time:17551ms step_avg:121.89ms
step:155/1393 train_time:17674ms step_avg:121.89ms
step:156/1393 train_time:17796ms step_avg:121.89ms
step:157/1393 train_time:17918ms step_avg:121.89ms
step:158/1393 train_time:18040ms step_avg:121.89ms
step:159/1393 train_time:18162ms step_avg:121.89ms
step:160/1393 train_time:18285ms step_avg:121.90ms
step:161/1393 train_time:18406ms step_avg:121.90ms
step:162/1393 train_time:18529ms step_avg:121.90ms
step:163/1393 train_time:18651ms step_avg:121.90ms
step:164/1393 train_time:18773ms step_avg:121.90ms
step:165/1393 train_time:18896ms step_avg:121.91ms
step:166/1393 train_time:19018ms step_avg:121.91ms
step:167/1393 train_time:19141ms step_avg:121.92ms
step:168/1393 train_time:19261ms step_avg:121.91ms
step:169/1393 train_time:19383ms step_avg:121.90ms
step:170/1393 train_time:19505ms step_avg:121.91ms
step:171/1393 train_time:19627ms step_avg:121.91ms
step:172/1393 train_time:19750ms step_avg:121.91ms
step:173/1393 train_time:19872ms step_avg:121.91ms
step:174/1393 train_time:19994ms step_avg:121.92ms
step:175/1393 train_time:20116ms step_avg:121.92ms
step:176/1393 train_time:20238ms step_avg:121.91ms
step:177/1393 train_time:20360ms step_avg:121.92ms
step:178/1393 train_time:20482ms step_avg:121.92ms
step:179/1393 train_time:20604ms step_avg:121.92ms
step:180/1393 train_time:20726ms step_avg:121.92ms
step:181/1393 train_time:20848ms step_avg:121.92ms
step:182/1393 train_time:20970ms step_avg:121.92ms
step:183/1393 train_time:21092ms step_avg:121.92ms
step:184/1393 train_time:21214ms step_avg:121.92ms
step:185/1393 train_time:21337ms step_avg:121.92ms
step:186/1393 train_time:21459ms step_avg:121.93ms
step:187/1393 train_time:21581ms step_avg:121.93ms
step:188/1393 train_time:21704ms step_avg:121.93ms
step:189/1393 train_time:21827ms step_avg:121.94ms
step:190/1393 train_time:21949ms step_avg:121.94ms
step:191/1393 train_time:22071ms step_avg:121.94ms
step:192/1393 train_time:22192ms step_avg:121.93ms
step:193/1393 train_time:22314ms step_avg:121.93ms
step:194/1393 train_time:22436ms step_avg:121.93ms
step:195/1393 train_time:22558ms step_avg:121.93ms
step:196/1393 train_time:22679ms step_avg:121.93ms
step:197/1393 train_time:22801ms step_avg:121.93ms
step:198/1393 train_time:22923ms step_avg:121.93ms
step:199/1393 train_time:23046ms step_avg:121.93ms
step:200/1393 train_time:23168ms step_avg:121.94ms
step:201/1393 train_time:23291ms step_avg:121.94ms
step:202/1393 train_time:23413ms step_avg:121.94ms
step:203/1393 train_time:23535ms step_avg:121.94ms
step:204/1393 train_time:23657ms step_avg:121.95ms
step:205/1393 train_time:23779ms step_avg:121.94ms
step:206/1393 train_time:23901ms step_avg:121.95ms
step:207/1393 train_time:24023ms step_avg:121.95ms
step:208/1393 train_time:24145ms step_avg:121.95ms
step:209/1393 train_time:24268ms step_avg:121.95ms
step:210/1393 train_time:24391ms step_avg:121.96ms
step:211/1393 train_time:24514ms step_avg:121.96ms
step:212/1393 train_time:24637ms step_avg:121.96ms
step:213/1393 train_time:24759ms step_avg:121.96ms
step:214/1393 train_time:24882ms step_avg:121.97ms
step:215/1393 train_time:25004ms step_avg:121.97ms
step:216/1393 train_time:25127ms step_avg:121.98ms
step:217/1393 train_time:25250ms step_avg:121.98ms
step:218/1393 train_time:25372ms step_avg:121.98ms
step:219/1393 train_time:25496ms step_avg:121.99ms
step:220/1393 train_time:25619ms step_avg:121.99ms
step:221/1393 train_time:25741ms step_avg:121.99ms
step:222/1393 train_time:25864ms step_avg:122.00ms
step:223/1393 train_time:25985ms step_avg:122.00ms
step:224/1393 train_time:26108ms step_avg:122.00ms
step:225/1393 train_time:26231ms step_avg:122.01ms
step:226/1393 train_time:26354ms step_avg:122.01ms
step:227/1393 train_time:26477ms step_avg:122.01ms
step:228/1393 train_time:26600ms step_avg:122.02ms
step:229/1393 train_time:26722ms step_avg:122.02ms
step:230/1393 train_time:26844ms step_avg:122.02ms
step:231/1393 train_time:26967ms step_avg:122.02ms
step:232/1393 train_time:27091ms step_avg:122.03ms
step:233/1393 train_time:27215ms step_avg:122.04ms
step:234/1393 train_time:27339ms step_avg:122.05ms
step:235/1393 train_time:27462ms step_avg:122.05ms
step:236/1393 train_time:27585ms step_avg:122.06ms
step:237/1393 train_time:27708ms step_avg:122.06ms
step:238/1393 train_time:27830ms step_avg:122.06ms
step:239/1393 train_time:27953ms step_avg:122.07ms
step:240/1393 train_time:28076ms step_avg:122.07ms
step:241/1393 train_time:28199ms step_avg:122.08ms
step:242/1393 train_time:28323ms step_avg:122.08ms
step:243/1393 train_time:28446ms step_avg:122.09ms
step:244/1393 train_time:28569ms step_avg:122.09ms
step:245/1393 train_time:28692ms step_avg:122.10ms
step:246/1393 train_time:28815ms step_avg:122.10ms
step:247/1393 train_time:28938ms step_avg:122.10ms
step:248/1393 train_time:29060ms step_avg:122.10ms
step:249/1393 train_time:29183ms step_avg:122.10ms
step:250/1393 train_time:29306ms step_avg:122.11ms
step:250/1393 val_loss:3.9791 train_time:29427ms step_avg:122.61ms
step:251/1393 train_time:29449ms step_avg:122.20ms
step:252/1393 train_time:29569ms step_avg:122.19ms
step:253/1393 train_time:29693ms step_avg:122.19ms
step:254/1393 train_time:29816ms step_avg:122.20ms
step:255/1393 train_time:29938ms step_avg:122.20ms
step:256/1393 train_time:30061ms step_avg:122.20ms
step:257/1393 train_time:30183ms step_avg:122.20ms
step:258/1393 train_time:30304ms step_avg:122.19ms
step:259/1393 train_time:30426ms step_avg:122.19ms
step:260/1393 train_time:30551ms step_avg:122.21ms
step:261/1393 train_time:30676ms step_avg:122.21ms
step:262/1393 train_time:30798ms step_avg:122.21ms
step:263/1393 train_time:30922ms step_avg:122.22ms
step:264/1393 train_time:31043ms step_avg:122.22ms
step:265/1393 train_time:31165ms step_avg:122.22ms
step:266/1393 train_time:31287ms step_avg:122.21ms
step:267/1393 train_time:31409ms step_avg:122.22ms
step:268/1393 train_time:31533ms step_avg:122.22ms
step:269/1393 train_time:31657ms step_avg:122.23ms
step:270/1393 train_time:31780ms step_avg:122.23ms
step:271/1393 train_time:31902ms step_avg:122.23ms
step:272/1393 train_time:32025ms step_avg:122.23ms
step:273/1393 train_time:32148ms step_avg:122.24ms
step:274/1393 train_time:32270ms step_avg:122.24ms
step:275/1393 train_time:32392ms step_avg:122.24ms
step:276/1393 train_time:32515ms step_avg:122.24ms
step:277/1393 train_time:32638ms step_avg:122.24ms
step:278/1393 train_time:32761ms step_avg:122.24ms
step:279/1393 train_time:32883ms step_avg:122.24ms
step:280/1393 train_time:33006ms step_avg:122.24ms
step:281/1393 train_time:33128ms step_avg:122.24ms
step:282/1393 train_time:33250ms step_avg:122.24ms
step:283/1393 train_time:33372ms step_avg:122.24ms
step:284/1393 train_time:33495ms step_avg:122.25ms
step:285/1393 train_time:33619ms step_avg:122.25ms
step:286/1393 train_time:33742ms step_avg:122.25ms
step:287/1393 train_time:33865ms step_avg:122.26ms
step:288/1393 train_time:33987ms step_avg:122.26ms
step:289/1393 train_time:34110ms step_avg:122.26ms
step:290/1393 train_time:34232ms step_avg:122.26ms
step:291/1393 train_time:34354ms step_avg:122.26ms
step:292/1393 train_time:34477ms step_avg:122.26ms
step:293/1393 train_time:34599ms step_avg:122.26ms
step:294/1393 train_time:34722ms step_avg:122.26ms
step:295/1393 train_time:34845ms step_avg:122.26ms
step:296/1393 train_time:34968ms step_avg:122.26ms
step:297/1393 train_time:35090ms step_avg:122.26ms
step:298/1393 train_time:35212ms step_avg:122.27ms
step:299/1393 train_time:35336ms step_avg:122.27ms
step:300/1393 train_time:35459ms step_avg:122.27ms
step:301/1393 train_time:35581ms step_avg:122.27ms
step:302/1393 train_time:35704ms step_avg:122.27ms
step:303/1393 train_time:35827ms step_avg:122.28ms
step:304/1393 train_time:35949ms step_avg:122.28ms
step:305/1393 train_time:36072ms step_avg:122.28ms
step:306/1393 train_time:36195ms step_avg:122.28ms
step:307/1393 train_time:36317ms step_avg:122.28ms
step:308/1393 train_time:36440ms step_avg:122.28ms
step:309/1393 train_time:36563ms step_avg:122.28ms
step:310/1393 train_time:36685ms step_avg:122.28ms
step:311/1393 train_time:36808ms step_avg:122.28ms
step:312/1393 train_time:36933ms step_avg:122.30ms
step:313/1393 train_time:37059ms step_avg:122.31ms
step:314/1393 train_time:37185ms step_avg:122.32ms
step:315/1393 train_time:37309ms step_avg:122.33ms
step:316/1393 train_time:37435ms step_avg:122.34ms
step:317/1393 train_time:37560ms step_avg:122.35ms
step:318/1393 train_time:37686ms step_avg:122.36ms
step:319/1393 train_time:37811ms step_avg:122.36ms
step:320/1393 train_time:37936ms step_avg:122.37ms
step:321/1393 train_time:38061ms step_avg:122.38ms
step:322/1393 train_time:38186ms step_avg:122.39ms
step:323/1393 train_time:38311ms step_avg:122.40ms
step:324/1393 train_time:38437ms step_avg:122.41ms
step:325/1393 train_time:38562ms step_avg:122.42ms
step:326/1393 train_time:38688ms step_avg:122.43ms
step:327/1393 train_time:38813ms step_avg:122.44ms
step:328/1393 train_time:38939ms step_avg:122.45ms
step:329/1393 train_time:39063ms step_avg:122.46ms
step:330/1393 train_time:39189ms step_avg:122.46ms
step:331/1393 train_time:39314ms step_avg:122.47ms
step:332/1393 train_time:39439ms step_avg:122.48ms
step:333/1393 train_time:39563ms step_avg:122.49ms
step:334/1393 train_time:39689ms step_avg:122.50ms
step:335/1393 train_time:39815ms step_avg:122.51ms
step:336/1393 train_time:39940ms step_avg:122.52ms
step:337/1393 train_time:40065ms step_avg:122.52ms
step:338/1393 train_time:40191ms step_avg:122.53ms
step:339/1393 train_time:40317ms step_avg:122.54ms
step:340/1393 train_time:40442ms step_avg:122.55ms
step:341/1393 train_time:40566ms step_avg:122.55ms
step:342/1393 train_time:40691ms step_avg:122.56ms
step:343/1393 train_time:40816ms step_avg:122.57ms
step:344/1393 train_time:40941ms step_avg:122.58ms
step:345/1393 train_time:41067ms step_avg:122.59ms
step:346/1393 train_time:41192ms step_avg:122.59ms
step:347/1393 train_time:41317ms step_avg:122.60ms
step:348/1393 train_time:41442ms step_avg:122.61ms
step:349/1393 train_time:41568ms step_avg:122.62ms
step:350/1393 train_time:41693ms step_avg:122.63ms
step:351/1393 train_time:41818ms step_avg:122.63ms
step:352/1393 train_time:41944ms step_avg:122.64ms
step:353/1393 train_time:42068ms step_avg:122.65ms
step:354/1393 train_time:42193ms step_avg:122.65ms
step:355/1393 train_time:42318ms step_avg:122.66ms
step:356/1393 train_time:42443ms step_avg:122.67ms
step:357/1393 train_time:42568ms step_avg:122.68ms
step:358/1393 train_time:42694ms step_avg:122.69ms
step:359/1393 train_time:42820ms step_avg:122.69ms
step:360/1393 train_time:42945ms step_avg:122.70ms
step:361/1393 train_time:43070ms step_avg:122.71ms
step:362/1393 train_time:43196ms step_avg:122.71ms
step:363/1393 train_time:43320ms step_avg:122.72ms
step:364/1393 train_time:43446ms step_avg:122.73ms
step:365/1393 train_time:43571ms step_avg:122.74ms
step:366/1393 train_time:43698ms step_avg:122.75ms
step:367/1393 train_time:43822ms step_avg:122.75ms
step:368/1393 train_time:43948ms step_avg:122.76ms
step:369/1393 train_time:44073ms step_avg:122.77ms
step:370/1393 train_time:44200ms step_avg:122.78ms
step:371/1393 train_time:44324ms step_avg:122.78ms
step:372/1393 train_time:44449ms step_avg:122.79ms
step:373/1393 train_time:44575ms step_avg:122.80ms
step:374/1393 train_time:44700ms step_avg:122.80ms
step:375/1393 train_time:44825ms step_avg:122.81ms
step:375/1393 val_loss:3.7771 train_time:44949ms step_avg:123.15ms
step:376/1393 train_time:44970ms step_avg:122.87ms
step:377/1393 train_time:45093ms step_avg:122.87ms
step:378/1393 train_time:45218ms step_avg:122.88ms
step:379/1393 train_time:45343ms step_avg:122.88ms
step:380/1393 train_time:45468ms step_avg:122.89ms
step:381/1393 train_time:45592ms step_avg:122.89ms
step:382/1393 train_time:45717ms step_avg:122.89ms
step:383/1393 train_time:45841ms step_avg:122.90ms
step:384/1393 train_time:45966ms step_avg:122.90ms
step:385/1393 train_time:46094ms step_avg:122.92ms
step:386/1393 train_time:46219ms step_avg:122.92ms
step:387/1393 train_time:46345ms step_avg:122.93ms
step:388/1393 train_time:46469ms step_avg:122.93ms
step:389/1393 train_time:46595ms step_avg:122.94ms
step:390/1393 train_time:46720ms step_avg:122.95ms
step:391/1393 train_time:46846ms step_avg:122.95ms
step:392/1393 train_time:46971ms step_avg:122.96ms
step:393/1393 train_time:47096ms step_avg:122.97ms
step:394/1393 train_time:47223ms step_avg:122.98ms
step:395/1393 train_time:47348ms step_avg:122.98ms
step:396/1393 train_time:47473ms step_avg:122.99ms
step:397/1393 train_time:47598ms step_avg:122.99ms
step:398/1393 train_time:47723ms step_avg:123.00ms
step:399/1393 train_time:47848ms step_avg:123.00ms
step:400/1393 train_time:47973ms step_avg:123.01ms
step:401/1393 train_time:48098ms step_avg:123.01ms
step:402/1393 train_time:48224ms step_avg:123.02ms
step:403/1393 train_time:48349ms step_avg:123.03ms
step:404/1393 train_time:48475ms step_avg:123.03ms
step:405/1393 train_time:48600ms step_avg:123.04ms
step:406/1393 train_time:48725ms step_avg:123.04ms
step:407/1393 train_time:48850ms step_avg:123.05ms
step:408/1393 train_time:48975ms step_avg:123.05ms
step:409/1393 train_time:49101ms step_avg:123.06ms
step:410/1393 train_time:49226ms step_avg:123.07ms
step:411/1393 train_time:49351ms step_avg:123.07ms
step:412/1393 train_time:49477ms step_avg:123.08ms
step:413/1393 train_time:49602ms step_avg:123.08ms
step:414/1393 train_time:49727ms step_avg:123.09ms
step:415/1393 train_time:49852ms step_avg:123.09ms
step:416/1393 train_time:49978ms step_avg:123.10ms
step:417/1393 train_time:50105ms step_avg:123.11ms
step:418/1393 train_time:50231ms step_avg:123.11ms
step:419/1393 train_time:50356ms step_avg:123.12ms
step:420/1393 train_time:50483ms step_avg:123.13ms
step:421/1393 train_time:50608ms step_avg:123.13ms
step:422/1393 train_time:50733ms step_avg:123.14ms
step:423/1393 train_time:50859ms step_avg:123.14ms
step:424/1393 train_time:50984ms step_avg:123.15ms
step:425/1393 train_time:51110ms step_avg:123.16ms
step:426/1393 train_time:51235ms step_avg:123.16ms
step:427/1393 train_time:51361ms step_avg:123.17ms
step:428/1393 train_time:51488ms step_avg:123.18ms
step:429/1393 train_time:51613ms step_avg:123.18ms
step:430/1393 train_time:51739ms step_avg:123.19ms
step:431/1393 train_time:51865ms step_avg:123.19ms
step:432/1393 train_time:51991ms step_avg:123.20ms
step:433/1393 train_time:52116ms step_avg:123.21ms
step:434/1393 train_time:52241ms step_avg:123.21ms
step:435/1393 train_time:52367ms step_avg:123.22ms
step:436/1393 train_time:52493ms step_avg:123.22ms
step:437/1393 train_time:52620ms step_avg:123.23ms
step:438/1393 train_time:52746ms step_avg:123.24ms
step:439/1393 train_time:52871ms step_avg:123.24ms
step:440/1393 train_time:52997ms step_avg:123.25ms
step:441/1393 train_time:53123ms step_avg:123.25ms
step:442/1393 train_time:53248ms step_avg:123.26ms
step:443/1393 train_time:53373ms step_avg:123.26ms
step:444/1393 train_time:53499ms step_avg:123.27ms
step:445/1393 train_time:53625ms step_avg:123.28ms
step:446/1393 train_time:53751ms step_avg:123.28ms
step:447/1393 train_time:53876ms step_avg:123.29ms
step:448/1393 train_time:54002ms step_avg:123.29ms
step:449/1393 train_time:54127ms step_avg:123.30ms
step:450/1393 train_time:54253ms step_avg:123.30ms
step:451/1393 train_time:54378ms step_avg:123.31ms
step:452/1393 train_time:54505ms step_avg:123.31ms
step:453/1393 train_time:54630ms step_avg:123.32ms
step:454/1393 train_time:54756ms step_avg:123.32ms
step:455/1393 train_time:54882ms step_avg:123.33ms
step:456/1393 train_time:55007ms step_avg:123.33ms
step:457/1393 train_time:55133ms step_avg:123.34ms
step:458/1393 train_time:55259ms step_avg:123.35ms
step:459/1393 train_time:55385ms step_avg:123.35ms
step:460/1393 train_time:55510ms step_avg:123.36ms
step:461/1393 train_time:55636ms step_avg:123.36ms
step:462/1393 train_time:55763ms step_avg:123.37ms
step:463/1393 train_time:55889ms step_avg:123.37ms
step:464/1393 train_time:56014ms step_avg:123.38ms
step:465/1393 train_time:56141ms step_avg:123.39ms
step:466/1393 train_time:56267ms step_avg:123.39ms
step:467/1393 train_time:56392ms step_avg:123.40ms
step:468/1393 train_time:56518ms step_avg:123.40ms
step:469/1393 train_time:56644ms step_avg:123.41ms
step:470/1393 train_time:56769ms step_avg:123.41ms
step:471/1393 train_time:56895ms step_avg:123.42ms
step:472/1393 train_time:57021ms step_avg:123.42ms
step:473/1393 train_time:57147ms step_avg:123.43ms
step:474/1393 train_time:57273ms step_avg:123.43ms
step:475/1393 train_time:57398ms step_avg:123.44ms
step:476/1393 train_time:57524ms step_avg:123.44ms
step:477/1393 train_time:57649ms step_avg:123.45ms
step:478/1393 train_time:57774ms step_avg:123.45ms
step:479/1393 train_time:57900ms step_avg:123.46ms
step:480/1393 train_time:58027ms step_avg:123.46ms
step:481/1393 train_time:58152ms step_avg:123.46ms
step:482/1393 train_time:58279ms step_avg:123.47ms
step:483/1393 train_time:58405ms step_avg:123.48ms
step:484/1393 train_time:58531ms step_avg:123.48ms
step:485/1393 train_time:58656ms step_avg:123.49ms
step:486/1393 train_time:58782ms step_avg:123.49ms
step:487/1393 train_time:58907ms step_avg:123.49ms
step:488/1393 train_time:59033ms step_avg:123.50ms
step:489/1393 train_time:59159ms step_avg:123.51ms
step:490/1393 train_time:59285ms step_avg:123.51ms
step:491/1393 train_time:59411ms step_avg:123.52ms
step:492/1393 train_time:59537ms step_avg:123.52ms
step:493/1393 train_time:59664ms step_avg:123.53ms
step:494/1393 train_time:59789ms step_avg:123.53ms
step:495/1393 train_time:59914ms step_avg:123.53ms
step:496/1393 train_time:60039ms step_avg:123.54ms
step:497/1393 train_time:60165ms step_avg:123.54ms
step:498/1393 train_time:60290ms step_avg:123.55ms
step:499/1393 train_time:60416ms step_avg:123.55ms
step:500/1393 train_time:60542ms step_avg:123.55ms
step:500/1393 val_loss:3.6605 train_time:60665ms step_avg:123.81ms
step:501/1393 train_time:60687ms step_avg:123.60ms
step:502/1393 train_time:60808ms step_avg:123.59ms
step:503/1393 train_time:60935ms step_avg:123.60ms
step:504/1393 train_time:61060ms step_avg:123.60ms
step:505/1393 train_time:61185ms step_avg:123.61ms
step:506/1393 train_time:61310ms step_avg:123.61ms
step:507/1393 train_time:61435ms step_avg:123.61ms
step:508/1393 train_time:61560ms step_avg:123.61ms
step:509/1393 train_time:61686ms step_avg:123.62ms
step:510/1393 train_time:61815ms step_avg:123.63ms
step:511/1393 train_time:61942ms step_avg:123.64ms
step:512/1393 train_time:62068ms step_avg:123.64ms
step:513/1393 train_time:62193ms step_avg:123.64ms
step:514/1393 train_time:62318ms step_avg:123.65ms
step:515/1393 train_time:62444ms step_avg:123.65ms
step:516/1393 train_time:62568ms step_avg:123.65ms
step:517/1393 train_time:62694ms step_avg:123.66ms
step:518/1393 train_time:62822ms step_avg:123.67ms
step:519/1393 train_time:62951ms step_avg:123.68ms
step:520/1393 train_time:63079ms step_avg:123.68ms
step:521/1393 train_time:63207ms step_avg:123.69ms
step:522/1393 train_time:63334ms step_avg:123.70ms
step:523/1393 train_time:63462ms step_avg:123.71ms
step:524/1393 train_time:63589ms step_avg:123.71ms
step:525/1393 train_time:63716ms step_avg:123.72ms
step:526/1393 train_time:63844ms step_avg:123.73ms
step:527/1393 train_time:63973ms step_avg:123.74ms
step:528/1393 train_time:64102ms step_avg:123.75ms
step:529/1393 train_time:64230ms step_avg:123.76ms
step:530/1393 train_time:64357ms step_avg:123.76ms
step:531/1393 train_time:64485ms step_avg:123.77ms
step:532/1393 train_time:64612ms step_avg:123.78ms
step:533/1393 train_time:64740ms step_avg:123.79ms
step:534/1393 train_time:64868ms step_avg:123.79ms
step:535/1393 train_time:64995ms step_avg:123.80ms
step:536/1393 train_time:65124ms step_avg:123.81ms
step:537/1393 train_time:65253ms step_avg:123.82ms
step:538/1393 train_time:65381ms step_avg:123.83ms
step:539/1393 train_time:65509ms step_avg:123.84ms
step:540/1393 train_time:65637ms step_avg:123.84ms
step:541/1393 train_time:65765ms step_avg:123.85ms
step:542/1393 train_time:65893ms step_avg:123.86ms
step:543/1393 train_time:66021ms step_avg:123.87ms
step:544/1393 train_time:66148ms step_avg:123.87ms
step:545/1393 train_time:66276ms step_avg:123.88ms
step:546/1393 train_time:66404ms step_avg:123.89ms
step:547/1393 train_time:66531ms step_avg:123.89ms
step:548/1393 train_time:66660ms step_avg:123.90ms
step:549/1393 train_time:66787ms step_avg:123.91ms
step:550/1393 train_time:66914ms step_avg:123.92ms
step:551/1393 train_time:67043ms step_avg:123.92ms
step:552/1393 train_time:67170ms step_avg:123.93ms
step:553/1393 train_time:67298ms step_avg:123.94ms
step:554/1393 train_time:67426ms step_avg:123.94ms
step:555/1393 train_time:67553ms step_avg:123.95ms
step:556/1393 train_time:67681ms step_avg:123.96ms
step:557/1393 train_time:67809ms step_avg:123.97ms
step:558/1393 train_time:67937ms step_avg:123.97ms
step:559/1393 train_time:68066ms step_avg:123.98ms
step:560/1393 train_time:68193ms step_avg:123.99ms
step:561/1393 train_time:68320ms step_avg:123.99ms
step:562/1393 train_time:68448ms step_avg:124.00ms
step:563/1393 train_time:68575ms step_avg:124.01ms
step:564/1393 train_time:68704ms step_avg:124.01ms
step:565/1393 train_time:68832ms step_avg:124.02ms
step:566/1393 train_time:68960ms step_avg:124.03ms
step:567/1393 train_time:69087ms step_avg:124.03ms
step:568/1393 train_time:69215ms step_avg:124.04ms
step:569/1393 train_time:69343ms step_avg:124.05ms
step:570/1393 train_time:69470ms step_avg:124.05ms
step:571/1393 train_time:69598ms step_avg:124.06ms
step:572/1393 train_time:69725ms step_avg:124.07ms
step:573/1393 train_time:69853ms step_avg:124.07ms
step:574/1393 train_time:69982ms step_avg:124.08ms
step:575/1393 train_time:70109ms step_avg:124.09ms
step:576/1393 train_time:70237ms step_avg:124.09ms
step:577/1393 train_time:70366ms step_avg:124.10ms
step:578/1393 train_time:70494ms step_avg:124.11ms
step:579/1393 train_time:70622ms step_avg:124.12ms
step:580/1393 train_time:70750ms step_avg:124.12ms
step:581/1393 train_time:70877ms step_avg:124.13ms
step:582/1393 train_time:71005ms step_avg:124.13ms
step:583/1393 train_time:71133ms step_avg:124.14ms
step:584/1393 train_time:71260ms step_avg:124.15ms
step:585/1393 train_time:71388ms step_avg:124.15ms
step:586/1393 train_time:71516ms step_avg:124.16ms
step:587/1393 train_time:71645ms step_avg:124.17ms
step:588/1393 train_time:71773ms step_avg:124.18ms
step:589/1393 train_time:71901ms step_avg:124.18ms
step:590/1393 train_time:72028ms step_avg:124.19ms
step:591/1393 train_time:72156ms step_avg:124.19ms
step:592/1393 train_time:72284ms step_avg:124.20ms
step:593/1393 train_time:72411ms step_avg:124.20ms
step:594/1393 train_time:72539ms step_avg:124.21ms
step:595/1393 train_time:72667ms step_avg:124.22ms
step:596/1393 train_time:72794ms step_avg:124.22ms
step:597/1393 train_time:72923ms step_avg:124.23ms
step:598/1393 train_time:73050ms step_avg:124.24ms
step:599/1393 train_time:73178ms step_avg:124.24ms
step:600/1393 train_time:73306ms step_avg:124.25ms
step:601/1393 train_time:73434ms step_avg:124.25ms
step:602/1393 train_time:73563ms step_avg:124.26ms
step:603/1393 train_time:73690ms step_avg:124.27ms
step:604/1393 train_time:73818ms step_avg:124.27ms
step:605/1393 train_time:73945ms step_avg:124.28ms
step:606/1393 train_time:74073ms step_avg:124.28ms
step:607/1393 train_time:74202ms step_avg:124.29ms
step:608/1393 train_time:74329ms step_avg:124.30ms
step:609/1393 train_time:74457ms step_avg:124.30ms
step:610/1393 train_time:74585ms step_avg:124.31ms
step:611/1393 train_time:74713ms step_avg:124.31ms
step:612/1393 train_time:74841ms step_avg:124.32ms
step:613/1393 train_time:74969ms step_avg:124.33ms
step:614/1393 train_time:75096ms step_avg:124.33ms
step:615/1393 train_time:75224ms step_avg:124.34ms
step:616/1393 train_time:75351ms step_avg:124.34ms
step:617/1393 train_time:75478ms step_avg:124.35ms
step:618/1393 train_time:75606ms step_avg:124.35ms
step:619/1393 train_time:75734ms step_avg:124.36ms
step:620/1393 train_time:75862ms step_avg:124.36ms
step:621/1393 train_time:75990ms step_avg:124.37ms
step:622/1393 train_time:76118ms step_avg:124.38ms
step:623/1393 train_time:76246ms step_avg:124.38ms
step:624/1393 train_time:76374ms step_avg:124.39ms
step:625/1393 train_time:76502ms step_avg:124.39ms
step:625/1393 val_loss:3.5784 train_time:76629ms step_avg:124.60ms
step:626/1393 train_time:76650ms step_avg:124.43ms
step:627/1393 train_time:76771ms step_avg:124.43ms
step:628/1393 train_time:76901ms step_avg:124.43ms
step:629/1393 train_time:77028ms step_avg:124.44ms
step:630/1393 train_time:77156ms step_avg:124.44ms
step:631/1393 train_time:77283ms step_avg:124.45ms
step:632/1393 train_time:77411ms step_avg:124.45ms
step:633/1393 train_time:77538ms step_avg:124.46ms
step:634/1393 train_time:77667ms step_avg:124.47ms
step:635/1393 train_time:77797ms step_avg:124.47ms
step:636/1393 train_time:77926ms step_avg:124.48ms
step:637/1393 train_time:78054ms step_avg:124.49ms
step:638/1393 train_time:78181ms step_avg:124.49ms
step:639/1393 train_time:78309ms step_avg:124.50ms
step:640/1393 train_time:78437ms step_avg:124.50ms
step:641/1393 train_time:78565ms step_avg:124.51ms
step:642/1393 train_time:78692ms step_avg:124.51ms
step:643/1393 train_time:78821ms step_avg:124.52ms
step:644/1393 train_time:78949ms step_avg:124.53ms
step:645/1393 train_time:79077ms step_avg:124.53ms
step:646/1393 train_time:79205ms step_avg:124.54ms
step:647/1393 train_time:79332ms step_avg:124.54ms
step:648/1393 train_time:79460ms step_avg:124.55ms
step:649/1393 train_time:79589ms step_avg:124.55ms
step:650/1393 train_time:79718ms step_avg:124.56ms
step:651/1393 train_time:79848ms step_avg:124.57ms
step:652/1393 train_time:79976ms step_avg:124.57ms
step:653/1393 train_time:80104ms step_avg:124.58ms
step:654/1393 train_time:80232ms step_avg:124.58ms
step:655/1393 train_time:80360ms step_avg:124.59ms
step:656/1393 train_time:80488ms step_avg:124.59ms
step:657/1393 train_time:80616ms step_avg:124.60ms
step:658/1393 train_time:80744ms step_avg:124.60ms
step:659/1393 train_time:80872ms step_avg:124.61ms
step:660/1393 train_time:81000ms step_avg:124.62ms
step:661/1393 train_time:81128ms step_avg:124.62ms
step:662/1393 train_time:81257ms step_avg:124.63ms
step:663/1393 train_time:81385ms step_avg:124.63ms
step:664/1393 train_time:81513ms step_avg:124.64ms
step:665/1393 train_time:81641ms step_avg:124.64ms
step:666/1393 train_time:81769ms step_avg:124.65ms
step:667/1393 train_time:81897ms step_avg:124.65ms
step:668/1393 train_time:82026ms step_avg:124.66ms
step:669/1393 train_time:82154ms step_avg:124.67ms
step:670/1393 train_time:82282ms step_avg:124.67ms
step:671/1393 train_time:82411ms step_avg:124.68ms
step:672/1393 train_time:82539ms step_avg:124.68ms
step:673/1393 train_time:82667ms step_avg:124.69ms
step:674/1393 train_time:82796ms step_avg:124.69ms
step:675/1393 train_time:82924ms step_avg:124.70ms
step:676/1393 train_time:83052ms step_avg:124.70ms
step:677/1393 train_time:83181ms step_avg:124.71ms
step:678/1393 train_time:83309ms step_avg:124.71ms
step:679/1393 train_time:83437ms step_avg:124.72ms
step:680/1393 train_time:83565ms step_avg:124.72ms
step:681/1393 train_time:83693ms step_avg:124.73ms
step:682/1393 train_time:83821ms step_avg:124.73ms
step:683/1393 train_time:83950ms step_avg:124.74ms
step:684/1393 train_time:84079ms step_avg:124.75ms
step:685/1393 train_time:84208ms step_avg:124.75ms
step:686/1393 train_time:84337ms step_avg:124.76ms
step:687/1393 train_time:84465ms step_avg:124.76ms
step:688/1393 train_time:84594ms step_avg:124.77ms
step:689/1393 train_time:84722ms step_avg:124.77ms
step:690/1393 train_time:84851ms step_avg:124.78ms
step:691/1393 train_time:84979ms step_avg:124.79ms
step:692/1393 train_time:85108ms step_avg:124.79ms
step:693/1393 train_time:85237ms step_avg:124.80ms
step:694/1393 train_time:85365ms step_avg:124.80ms
step:695/1393 train_time:85493ms step_avg:124.81ms
step:696/1393 train_time:85621ms step_avg:124.81ms
step:697/1393 train_time:85749ms step_avg:124.82ms
step:698/1393 train_time:85878ms step_avg:124.82ms
step:699/1393 train_time:86006ms step_avg:124.83ms
step:700/1393 train_time:86134ms step_avg:124.83ms
step:701/1393 train_time:86261ms step_avg:124.84ms
step:702/1393 train_time:86389ms step_avg:124.84ms
step:703/1393 train_time:86517ms step_avg:124.84ms
step:704/1393 train_time:86645ms step_avg:124.85ms
step:705/1393 train_time:86773ms step_avg:124.85ms
step:706/1393 train_time:86901ms step_avg:124.86ms
step:707/1393 train_time:87029ms step_avg:124.86ms
step:708/1393 train_time:87157ms step_avg:124.87ms
step:709/1393 train_time:87286ms step_avg:124.87ms
step:710/1393 train_time:87414ms step_avg:124.88ms
step:711/1393 train_time:87543ms step_avg:124.88ms
step:712/1393 train_time:87672ms step_avg:124.89ms
step:713/1393 train_time:87800ms step_avg:124.89ms
step:714/1393 train_time:87928ms step_avg:124.90ms
step:715/1393 train_time:88055ms step_avg:124.90ms
step:716/1393 train_time:88184ms step_avg:124.91ms
step:717/1393 train_time:88311ms step_avg:124.91ms
step:718/1393 train_time:88439ms step_avg:124.91ms
step:719/1393 train_time:88568ms step_avg:124.92ms
step:720/1393 train_time:88696ms step_avg:124.92ms
step:721/1393 train_time:88824ms step_avg:124.93ms
step:722/1393 train_time:88952ms step_avg:124.93ms
step:723/1393 train_time:89080ms step_avg:124.94ms
step:724/1393 train_time:89208ms step_avg:124.94ms
step:725/1393 train_time:89338ms step_avg:124.95ms
step:726/1393 train_time:89469ms step_avg:124.96ms
step:727/1393 train_time:89600ms step_avg:124.96ms
step:728/1393 train_time:89730ms step_avg:124.97ms
step:729/1393 train_time:89859ms step_avg:124.98ms
step:730/1393 train_time:89989ms step_avg:124.98ms
step:731/1393 train_time:90119ms step_avg:124.99ms
step:732/1393 train_time:90249ms step_avg:125.00ms
step:733/1393 train_time:90380ms step_avg:125.01ms
step:734/1393 train_time:90510ms step_avg:125.01ms
step:735/1393 train_time:90641ms step_avg:125.02ms
step:736/1393 train_time:90770ms step_avg:125.03ms
step:737/1393 train_time:90901ms step_avg:125.04ms
step:738/1393 train_time:91031ms step_avg:125.04ms
step:739/1393 train_time:91161ms step_avg:125.05ms
step:740/1393 train_time:91292ms step_avg:125.06ms
step:741/1393 train_time:91424ms step_avg:125.07ms
step:742/1393 train_time:91555ms step_avg:125.07ms
step:743/1393 train_time:91684ms step_avg:125.08ms
step:744/1393 train_time:91814ms step_avg:125.09ms
step:745/1393 train_time:91944ms step_avg:125.09ms
step:746/1393 train_time:92073ms step_avg:125.10ms
step:747/1393 train_time:92203ms step_avg:125.11ms
step:748/1393 train_time:92333ms step_avg:125.11ms
step:749/1393 train_time:92464ms step_avg:125.12ms
step:750/1393 train_time:92593ms step_avg:125.13ms
step:750/1393 val_loss:3.5253 train_time:92722ms step_avg:125.30ms
step:751/1393 train_time:92743ms step_avg:125.16ms
step:752/1393 train_time:92868ms step_avg:125.16ms
step:753/1393 train_time:92998ms step_avg:125.17ms
step:754/1393 train_time:93128ms step_avg:125.17ms
step:755/1393 train_time:93257ms step_avg:125.18ms
step:756/1393 train_time:93386ms step_avg:125.18ms
step:757/1393 train_time:93516ms step_avg:125.19ms
step:758/1393 train_time:93646ms step_avg:125.20ms
step:759/1393 train_time:93776ms step_avg:125.20ms
step:760/1393 train_time:93908ms step_avg:125.21ms
step:761/1393 train_time:94040ms step_avg:125.22ms
step:762/1393 train_time:94168ms step_avg:125.22ms
step:763/1393 train_time:94297ms step_avg:125.23ms
step:764/1393 train_time:94426ms step_avg:125.23ms
step:765/1393 train_time:94556ms step_avg:125.24ms
step:766/1393 train_time:94687ms step_avg:125.25ms
step:767/1393 train_time:94818ms step_avg:125.25ms
step:768/1393 train_time:94949ms step_avg:125.26ms
step:769/1393 train_time:95079ms step_avg:125.27ms
step:770/1393 train_time:95209ms step_avg:125.28ms
step:771/1393 train_time:95339ms step_avg:125.28ms
step:772/1393 train_time:95469ms step_avg:125.29ms
step:773/1393 train_time:95599ms step_avg:125.29ms
step:774/1393 train_time:95729ms step_avg:125.30ms
step:775/1393 train_time:95859ms step_avg:125.31ms
step:776/1393 train_time:95989ms step_avg:125.31ms
step:777/1393 train_time:96119ms step_avg:125.32ms
step:778/1393 train_time:96248ms step_avg:125.32ms
step:779/1393 train_time:96379ms step_avg:125.33ms
step:780/1393 train_time:96509ms step_avg:125.34ms
step:781/1393 train_time:96639ms step_avg:125.34ms
step:782/1393 train_time:96769ms step_avg:125.35ms
step:783/1393 train_time:96899ms step_avg:125.35ms
step:784/1393 train_time:97028ms step_avg:125.36ms
step:785/1393 train_time:97158ms step_avg:125.37ms
step:786/1393 train_time:97289ms step_avg:125.37ms
step:787/1393 train_time:97419ms step_avg:125.38ms
step:788/1393 train_time:97549ms step_avg:125.38ms
step:789/1393 train_time:97680ms step_avg:125.39ms
step:790/1393 train_time:97809ms step_avg:125.40ms
step:791/1393 train_time:97939ms step_avg:125.40ms
step:792/1393 train_time:98070ms step_avg:125.41ms
step:793/1393 train_time:98200ms step_avg:125.42ms
step:794/1393 train_time:98330ms step_avg:125.42ms
step:795/1393 train_time:98461ms step_avg:125.43ms
step:796/1393 train_time:98590ms step_avg:125.43ms
step:797/1393 train_time:98720ms step_avg:125.44ms
step:798/1393 train_time:98851ms step_avg:125.45ms
step:799/1393 train_time:98983ms step_avg:125.45ms
step:800/1393 train_time:99113ms step_avg:125.46ms
step:801/1393 train_time:99243ms step_avg:125.47ms
step:802/1393 train_time:99373ms step_avg:125.47ms
step:803/1393 train_time:99502ms step_avg:125.48ms
step:804/1393 train_time:99631ms step_avg:125.48ms
step:805/1393 train_time:99762ms step_avg:125.49ms
step:806/1393 train_time:99893ms step_avg:125.49ms
step:807/1393 train_time:100022ms step_avg:125.50ms
step:808/1393 train_time:100152ms step_avg:125.50ms
step:809/1393 train_time:100281ms step_avg:125.51ms
step:810/1393 train_time:100411ms step_avg:125.51ms
step:811/1393 train_time:100541ms step_avg:125.52ms
step:812/1393 train_time:100671ms step_avg:125.53ms
step:813/1393 train_time:100801ms step_avg:125.53ms
step:814/1393 train_time:100931ms step_avg:125.54ms
step:815/1393 train_time:101061ms step_avg:125.54ms
step:816/1393 train_time:101191ms step_avg:125.55ms
step:817/1393 train_time:101321ms step_avg:125.55ms
step:818/1393 train_time:101451ms step_avg:125.56ms
step:819/1393 train_time:101582ms step_avg:125.56ms
step:820/1393 train_time:101711ms step_avg:125.57ms
step:821/1393 train_time:101842ms step_avg:125.58ms
step:822/1393 train_time:101971ms step_avg:125.58ms
step:823/1393 train_time:102101ms step_avg:125.59ms
step:824/1393 train_time:102230ms step_avg:125.59ms
step:825/1393 train_time:102361ms step_avg:125.60ms
step:826/1393 train_time:102491ms step_avg:125.60ms
step:827/1393 train_time:102621ms step_avg:125.61ms
step:828/1393 train_time:102751ms step_avg:125.61ms
step:829/1393 train_time:102881ms step_avg:125.62ms
step:830/1393 train_time:103013ms step_avg:125.63ms
step:831/1393 train_time:103143ms step_avg:125.63ms
step:832/1393 train_time:103274ms step_avg:125.64ms
step:833/1393 train_time:103404ms step_avg:125.64ms
step:834/1393 train_time:103536ms step_avg:125.65ms
step:835/1393 train_time:103667ms step_avg:125.66ms
step:836/1393 train_time:103797ms step_avg:125.66ms
step:837/1393 train_time:103927ms step_avg:125.67ms
step:838/1393 train_time:104058ms step_avg:125.67ms
step:839/1393 train_time:104187ms step_avg:125.68ms
step:840/1393 train_time:104318ms step_avg:125.68ms
step:841/1393 train_time:104448ms step_avg:125.69ms
step:842/1393 train_time:104578ms step_avg:125.69ms
step:843/1393 train_time:104707ms step_avg:125.70ms
step:844/1393 train_time:104839ms step_avg:125.71ms
step:845/1393 train_time:104969ms step_avg:125.71ms
step:846/1393 train_time:105099ms step_avg:125.72ms
step:847/1393 train_time:105230ms step_avg:125.72ms
step:848/1393 train_time:105360ms step_avg:125.73ms
step:849/1393 train_time:105491ms step_avg:125.73ms
step:850/1393 train_time:105621ms step_avg:125.74ms
step:851/1393 train_time:105751ms step_avg:125.74ms
step:852/1393 train_time:105882ms step_avg:125.75ms
step:853/1393 train_time:106014ms step_avg:125.76ms
step:854/1393 train_time:106143ms step_avg:125.76ms
step:855/1393 train_time:106273ms step_avg:125.77ms
step:856/1393 train_time:106403ms step_avg:125.77ms
step:857/1393 train_time:106533ms step_avg:125.78ms
step:858/1393 train_time:106664ms step_avg:125.78ms
step:859/1393 train_time:106795ms step_avg:125.79ms
step:860/1393 train_time:106926ms step_avg:125.79ms
step:861/1393 train_time:107056ms step_avg:125.80ms
step:862/1393 train_time:107187ms step_avg:125.81ms
step:863/1393 train_time:107317ms step_avg:125.81ms
step:864/1393 train_time:107447ms step_avg:125.82ms
step:865/1393 train_time:107577ms step_avg:125.82ms
step:866/1393 train_time:107708ms step_avg:125.83ms
step:867/1393 train_time:107838ms step_avg:125.83ms
step:868/1393 train_time:107969ms step_avg:125.84ms
step:869/1393 train_time:108100ms step_avg:125.84ms
step:870/1393 train_time:108230ms step_avg:125.85ms
step:871/1393 train_time:108360ms step_avg:125.85ms
step:872/1393 train_time:108490ms step_avg:125.86ms
step:873/1393 train_time:108620ms step_avg:125.86ms
step:874/1393 train_time:108751ms step_avg:125.87ms
step:875/1393 train_time:108880ms step_avg:125.87ms
step:875/1393 val_loss:3.4755 train_time:109009ms step_avg:126.02ms
step:876/1393 train_time:109030ms step_avg:125.90ms
step:877/1393 train_time:109150ms step_avg:125.89ms
step:878/1393 train_time:109282ms step_avg:125.90ms
step:879/1393 train_time:109411ms step_avg:125.90ms
step:880/1393 train_time:109541ms step_avg:125.91ms
step:881/1393 train_time:109671ms step_avg:125.91ms
step:882/1393 train_time:109800ms step_avg:125.92ms
step:883/1393 train_time:109930ms step_avg:125.92ms
step:884/1393 train_time:110064ms step_avg:125.93ms
step:885/1393 train_time:110196ms step_avg:125.94ms
step:886/1393 train_time:110326ms step_avg:125.94ms
step:887/1393 train_time:110456ms step_avg:125.95ms
step:888/1393 train_time:110586ms step_avg:125.95ms
step:889/1393 train_time:110718ms step_avg:125.96ms
step:890/1393 train_time:110847ms step_avg:125.96ms
step:891/1393 train_time:110976ms step_avg:125.97ms
step:892/1393 train_time:111107ms step_avg:125.97ms
step:893/1393 train_time:111238ms step_avg:125.98ms
step:894/1393 train_time:111368ms step_avg:125.98ms
step:895/1393 train_time:111498ms step_avg:125.99ms
step:896/1393 train_time:111629ms step_avg:125.99ms
step:897/1393 train_time:111759ms step_avg:126.00ms
step:898/1393 train_time:111889ms step_avg:126.00ms
step:899/1393 train_time:112020ms step_avg:126.01ms
step:900/1393 train_time:112150ms step_avg:126.01ms
step:901/1393 train_time:112281ms step_avg:126.02ms
step:902/1393 train_time:112410ms step_avg:126.02ms
step:903/1393 train_time:112541ms step_avg:126.03ms
step:904/1393 train_time:112671ms step_avg:126.03ms
step:905/1393 train_time:112802ms step_avg:126.04ms
step:906/1393 train_time:112932ms step_avg:126.04ms
step:907/1393 train_time:113063ms step_avg:126.05ms
step:908/1393 train_time:113193ms step_avg:126.05ms
step:909/1393 train_time:113324ms step_avg:126.06ms
step:910/1393 train_time:113456ms step_avg:126.06ms
step:911/1393 train_time:113586ms step_avg:126.07ms
step:912/1393 train_time:113716ms step_avg:126.07ms
step:913/1393 train_time:113846ms step_avg:126.08ms
step:914/1393 train_time:113976ms step_avg:126.08ms
step:915/1393 train_time:114108ms step_avg:126.09ms
step:916/1393 train_time:114237ms step_avg:126.09ms
step:917/1393 train_time:114368ms step_avg:126.09ms
step:918/1393 train_time:114498ms step_avg:126.10ms
step:919/1393 train_time:114630ms step_avg:126.11ms
step:920/1393 train_time:114762ms step_avg:126.11ms
step:921/1393 train_time:114892ms step_avg:126.12ms
step:922/1393 train_time:115024ms step_avg:126.12ms
step:923/1393 train_time:115153ms step_avg:126.13ms
step:924/1393 train_time:115284ms step_avg:126.13ms
step:925/1393 train_time:115414ms step_avg:126.14ms
step:926/1393 train_time:115545ms step_avg:126.14ms
step:927/1393 train_time:115675ms step_avg:126.15ms
step:928/1393 train_time:115806ms step_avg:126.15ms
step:929/1393 train_time:115936ms step_avg:126.15ms
step:930/1393 train_time:116066ms step_avg:126.16ms
step:931/1393 train_time:116198ms step_avg:126.17ms
step:932/1393 train_time:116331ms step_avg:126.17ms
step:933/1393 train_time:116463ms step_avg:126.18ms
step:934/1393 train_time:116596ms step_avg:126.19ms
step:935/1393 train_time:116729ms step_avg:126.19ms
step:936/1393 train_time:116860ms step_avg:126.20ms
step:937/1393 train_time:116995ms step_avg:126.21ms
step:938/1393 train_time:117127ms step_avg:126.21ms
step:939/1393 train_time:117259ms step_avg:126.22ms
step:940/1393 train_time:117392ms step_avg:126.23ms
step:941/1393 train_time:117525ms step_avg:126.23ms
step:942/1393 train_time:117656ms step_avg:126.24ms
step:943/1393 train_time:117790ms step_avg:126.25ms
step:944/1393 train_time:117923ms step_avg:126.26ms
step:945/1393 train_time:118055ms step_avg:126.26ms
step:946/1393 train_time:118186ms step_avg:126.27ms
step:947/1393 train_time:118320ms step_avg:126.28ms
step:948/1393 train_time:118452ms step_avg:126.28ms
step:949/1393 train_time:118584ms step_avg:126.29ms
step:950/1393 train_time:118716ms step_avg:126.29ms
step:951/1393 train_time:118849ms step_avg:126.30ms
step:952/1393 train_time:118980ms step_avg:126.31ms
step:953/1393 train_time:119113ms step_avg:126.31ms
step:954/1393 train_time:119244ms step_avg:126.32ms
step:955/1393 train_time:119376ms step_avg:126.32ms
step:956/1393 train_time:119510ms step_avg:126.33ms
step:957/1393 train_time:119642ms step_avg:126.34ms
step:958/1393 train_time:119775ms step_avg:126.34ms
step:959/1393 train_time:119907ms step_avg:126.35ms
step:960/1393 train_time:120040ms step_avg:126.36ms
step:961/1393 train_time:120172ms step_avg:126.36ms
step:962/1393 train_time:120305ms step_avg:126.37ms
step:963/1393 train_time:120437ms step_avg:126.38ms
step:964/1393 train_time:120568ms step_avg:126.38ms
step:965/1393 train_time:120701ms step_avg:126.39ms
step:966/1393 train_time:120832ms step_avg:126.39ms
step:967/1393 train_time:120965ms step_avg:126.40ms
step:968/1393 train_time:121097ms step_avg:126.41ms
step:969/1393 train_time:121229ms step_avg:126.41ms
step:970/1393 train_time:121361ms step_avg:126.42ms
step:971/1393 train_time:121493ms step_avg:126.42ms
step:972/1393 train_time:121626ms step_avg:126.43ms
step:973/1393 train_time:121759ms step_avg:126.44ms
step:974/1393 train_time:121890ms step_avg:126.44ms
step:975/1393 train_time:122022ms step_avg:126.45ms
step:976/1393 train_time:122155ms step_avg:126.45ms
step:977/1393 train_time:122287ms step_avg:126.46ms
step:978/1393 train_time:122419ms step_avg:126.47ms
step:979/1393 train_time:122551ms step_avg:126.47ms
step:980/1393 train_time:122683ms step_avg:126.48ms
step:981/1393 train_time:122814ms step_avg:126.48ms
step:982/1393 train_time:122946ms step_avg:126.49ms
step:983/1393 train_time:123078ms step_avg:126.49ms
step:984/1393 train_time:123209ms step_avg:126.50ms
step:985/1393 train_time:123341ms step_avg:126.50ms
step:986/1393 train_time:123476ms step_avg:126.51ms
step:987/1393 train_time:123606ms step_avg:126.52ms
step:988/1393 train_time:123738ms step_avg:126.52ms
step:989/1393 train_time:123870ms step_avg:126.53ms
step:990/1393 train_time:124001ms step_avg:126.53ms
step:991/1393 train_time:124133ms step_avg:126.54ms
step:992/1393 train_time:124266ms step_avg:126.54ms
step:993/1393 train_time:124401ms step_avg:126.55ms
step:994/1393 train_time:124533ms step_avg:126.56ms
step:995/1393 train_time:124664ms step_avg:126.56ms
step:996/1393 train_time:124795ms step_avg:126.57ms
step:997/1393 train_time:124927ms step_avg:126.57ms
step:998/1393 train_time:125058ms step_avg:126.58ms
step:999/1393 train_time:125192ms step_avg:126.58ms
step:1000/1393 train_time:125324ms step_avg:126.59ms
step:1000/1393 val_loss:3.4139 train_time:125456ms step_avg:126.72ms
step:1001/1393 train_time:125478ms step_avg:126.62ms
step:1002/1393 train_time:125598ms step_avg:126.61ms
step:1003/1393 train_time:125731ms step_avg:126.62ms
step:1004/1393 train_time:125862ms step_avg:126.62ms
step:1005/1393 train_time:125994ms step_avg:126.63ms
step:1006/1393 train_time:126124ms step_avg:126.63ms
step:1007/1393 train_time:126255ms step_avg:126.64ms
step:1008/1393 train_time:126387ms step_avg:126.64ms
step:1009/1393 train_time:126522ms step_avg:126.65ms
step:1010/1393 train_time:126654ms step_avg:126.65ms
step:1011/1393 train_time:126787ms step_avg:126.66ms
step:1012/1393 train_time:126918ms step_avg:126.66ms
step:1013/1393 train_time:127051ms step_avg:126.67ms
step:1014/1393 train_time:127181ms step_avg:126.67ms
step:1015/1393 train_time:127313ms step_avg:126.68ms
step:1016/1393 train_time:127446ms step_avg:126.69ms
step:1017/1393 train_time:127577ms step_avg:126.69ms
step:1018/1393 train_time:127710ms step_avg:126.70ms
step:1019/1393 train_time:127843ms step_avg:126.70ms
step:1020/1393 train_time:127976ms step_avg:126.71ms
step:1021/1393 train_time:128108ms step_avg:126.71ms
step:1022/1393 train_time:128240ms step_avg:126.72ms
step:1023/1393 train_time:128373ms step_avg:126.73ms
step:1024/1393 train_time:128506ms step_avg:126.73ms
step:1025/1393 train_time:128639ms step_avg:126.74ms
step:1026/1393 train_time:128771ms step_avg:126.74ms
step:1027/1393 train_time:128903ms step_avg:126.75ms
step:1028/1393 train_time:129035ms step_avg:126.75ms
step:1029/1393 train_time:129167ms step_avg:126.76ms
step:1030/1393 train_time:129299ms step_avg:126.76ms
step:1031/1393 train_time:129430ms step_avg:126.77ms
step:1032/1393 train_time:129561ms step_avg:126.77ms
step:1033/1393 train_time:129692ms step_avg:126.78ms
step:1034/1393 train_time:129826ms step_avg:126.78ms
step:1035/1393 train_time:129959ms step_avg:126.79ms
step:1036/1393 train_time:130091ms step_avg:126.79ms
step:1037/1393 train_time:130225ms step_avg:126.80ms
step:1038/1393 train_time:130358ms step_avg:126.81ms
step:1039/1393 train_time:130490ms step_avg:126.81ms
step:1040/1393 train_time:130621ms step_avg:126.82ms
step:1041/1393 train_time:130753ms step_avg:126.82ms
step:1042/1393 train_time:130886ms step_avg:126.83ms
step:1043/1393 train_time:131018ms step_avg:126.83ms
step:1044/1393 train_time:131152ms step_avg:126.84ms
step:1045/1393 train_time:131286ms step_avg:126.85ms
step:1046/1393 train_time:131418ms step_avg:126.85ms
step:1047/1393 train_time:131550ms step_avg:126.86ms
step:1048/1393 train_time:131682ms step_avg:126.86ms
step:1049/1393 train_time:131815ms step_avg:126.87ms
step:1050/1393 train_time:131947ms step_avg:126.87ms
step:1051/1393 train_time:132080ms step_avg:126.88ms
step:1052/1393 train_time:132212ms step_avg:126.88ms
step:1053/1393 train_time:132343ms step_avg:126.89ms
step:1054/1393 train_time:132475ms step_avg:126.89ms
step:1055/1393 train_time:132606ms step_avg:126.90ms
step:1056/1393 train_time:132738ms step_avg:126.90ms
step:1057/1393 train_time:132870ms step_avg:126.91ms
step:1058/1393 train_time:133002ms step_avg:126.91ms
step:1059/1393 train_time:133134ms step_avg:126.92ms
step:1060/1393 train_time:133267ms step_avg:126.92ms
step:1061/1393 train_time:133399ms step_avg:126.93ms
step:1062/1393 train_time:133532ms step_avg:126.93ms
step:1063/1393 train_time:133663ms step_avg:126.94ms
step:1064/1393 train_time:133796ms step_avg:126.94ms
step:1065/1393 train_time:133927ms step_avg:126.94ms
step:1066/1393 train_time:134060ms step_avg:126.95ms
step:1067/1393 train_time:134193ms step_avg:126.96ms
step:1068/1393 train_time:134325ms step_avg:126.96ms
step:1069/1393 train_time:134459ms step_avg:126.97ms
step:1070/1393 train_time:134591ms step_avg:126.97ms
step:1071/1393 train_time:134724ms step_avg:126.98ms
step:1072/1393 train_time:134856ms step_avg:126.98ms
step:1073/1393 train_time:134988ms step_avg:126.99ms
step:1074/1393 train_time:135121ms step_avg:126.99ms
step:1075/1393 train_time:135254ms step_avg:127.00ms
step:1076/1393 train_time:135386ms step_avg:127.00ms
step:1077/1393 train_time:135519ms step_avg:127.01ms
step:1078/1393 train_time:135650ms step_avg:127.01ms
step:1079/1393 train_time:135787ms step_avg:127.02ms
step:1080/1393 train_time:135918ms step_avg:127.03ms
step:1081/1393 train_time:136051ms step_avg:127.03ms
step:1082/1393 train_time:136182ms step_avg:127.04ms
step:1083/1393 train_time:136315ms step_avg:127.04ms
step:1084/1393 train_time:136447ms step_avg:127.05ms
step:1085/1393 train_time:136579ms step_avg:127.05ms
step:1086/1393 train_time:136712ms step_avg:127.06ms
step:1087/1393 train_time:136845ms step_avg:127.06ms
step:1088/1393 train_time:136977ms step_avg:127.07ms
step:1089/1393 train_time:137111ms step_avg:127.07ms
step:1090/1393 train_time:137245ms step_avg:127.08ms
step:1091/1393 train_time:137376ms step_avg:127.08ms
step:1092/1393 train_time:137509ms step_avg:127.09ms
step:1093/1393 train_time:137641ms step_avg:127.09ms
step:1094/1393 train_time:137773ms step_avg:127.10ms
step:1095/1393 train_time:137905ms step_avg:127.10ms
step:1096/1393 train_time:138038ms step_avg:127.11ms
step:1097/1393 train_time:138169ms step_avg:127.11ms
step:1098/1393 train_time:138303ms step_avg:127.12ms
step:1099/1393 train_time:138436ms step_avg:127.12ms
step:1100/1393 train_time:138566ms step_avg:127.12ms
step:1101/1393 train_time:138698ms step_avg:127.13ms
step:1102/1393 train_time:138830ms step_avg:127.13ms
step:1103/1393 train_time:138963ms step_avg:127.14ms
step:1104/1393 train_time:139096ms step_avg:127.14ms
step:1105/1393 train_time:139231ms step_avg:127.15ms
step:1106/1393 train_time:139362ms step_avg:127.16ms
step:1107/1393 train_time:139494ms step_avg:127.16ms
step:1108/1393 train_time:139628ms step_avg:127.17ms
step:1109/1393 train_time:139759ms step_avg:127.17ms
step:1110/1393 train_time:139891ms step_avg:127.17ms
step:1111/1393 train_time:140023ms step_avg:127.18ms
step:1112/1393 train_time:140155ms step_avg:127.18ms
step:1113/1393 train_time:140287ms step_avg:127.19ms
step:1114/1393 train_time:140420ms step_avg:127.19ms
step:1115/1393 train_time:140552ms step_avg:127.20ms
step:1116/1393 train_time:140685ms step_avg:127.20ms
step:1117/1393 train_time:140818ms step_avg:127.21ms
step:1118/1393 train_time:140952ms step_avg:127.21ms
step:1119/1393 train_time:141083ms step_avg:127.22ms
step:1120/1393 train_time:141215ms step_avg:127.22ms
step:1121/1393 train_time:141347ms step_avg:127.23ms
step:1122/1393 train_time:141480ms step_avg:127.23ms
step:1123/1393 train_time:141612ms step_avg:127.23ms
step:1124/1393 train_time:141745ms step_avg:127.24ms
step:1125/1393 train_time:141876ms step_avg:127.24ms
step:1125/1393 val_loss:3.3632 train_time:142009ms step_avg:127.36ms
step:1126/1393 train_time:142031ms step_avg:127.27ms
step:1127/1393 train_time:142152ms step_avg:127.26ms
step:1128/1393 train_time:142285ms step_avg:127.27ms
step:1129/1393 train_time:142417ms step_avg:127.27ms
step:1130/1393 train_time:142548ms step_avg:127.28ms
step:1131/1393 train_time:142681ms step_avg:127.28ms
step:1132/1393 train_time:142813ms step_avg:127.28ms
step:1133/1393 train_time:142943ms step_avg:127.29ms
step:1134/1393 train_time:143078ms step_avg:127.29ms
step:1135/1393 train_time:143211ms step_avg:127.30ms
step:1136/1393 train_time:143347ms step_avg:127.31ms
step:1137/1393 train_time:143478ms step_avg:127.31ms
step:1138/1393 train_time:143613ms step_avg:127.32ms
step:1139/1393 train_time:143745ms step_avg:127.32ms
step:1140/1393 train_time:143879ms step_avg:127.33ms
step:1141/1393 train_time:144013ms step_avg:127.33ms
step:1142/1393 train_time:144147ms step_avg:127.34ms
step:1143/1393 train_time:144282ms step_avg:127.35ms
step:1144/1393 train_time:144415ms step_avg:127.35ms
step:1145/1393 train_time:144548ms step_avg:127.36ms
step:1146/1393 train_time:144681ms step_avg:127.36ms
step:1147/1393 train_time:144816ms step_avg:127.37ms
step:1148/1393 train_time:144949ms step_avg:127.37ms
step:1149/1393 train_time:145083ms step_avg:127.38ms
step:1150/1393 train_time:145217ms step_avg:127.38ms
step:1151/1393 train_time:145352ms step_avg:127.39ms
step:1152/1393 train_time:145485ms step_avg:127.39ms
step:1153/1393 train_time:145620ms step_avg:127.40ms
step:1154/1393 train_time:145754ms step_avg:127.41ms
step:1155/1393 train_time:145888ms step_avg:127.41ms
step:1156/1393 train_time:146023ms step_avg:127.42ms
step:1157/1393 train_time:146157ms step_avg:127.43ms
step:1158/1393 train_time:146290ms step_avg:127.43ms
step:1159/1393 train_time:146424ms step_avg:127.44ms
step:1160/1393 train_time:146557ms step_avg:127.44ms
step:1161/1393 train_time:146691ms step_avg:127.45ms
step:1162/1393 train_time:146826ms step_avg:127.45ms
step:1163/1393 train_time:146961ms step_avg:127.46ms
step:1164/1393 train_time:147095ms step_avg:127.47ms
step:1165/1393 train_time:147228ms step_avg:127.47ms
step:1166/1393 train_time:147361ms step_avg:127.47ms
step:1167/1393 train_time:147494ms step_avg:127.48ms
step:1168/1393 train_time:147629ms step_avg:127.49ms
step:1169/1393 train_time:147762ms step_avg:127.49ms
step:1170/1393 train_time:147897ms step_avg:127.50ms
step:1171/1393 train_time:148030ms step_avg:127.50ms
step:1172/1393 train_time:148164ms step_avg:127.51ms
step:1173/1393 train_time:148298ms step_avg:127.51ms
step:1174/1393 train_time:148436ms step_avg:127.52ms
step:1175/1393 train_time:148569ms step_avg:127.53ms
step:1176/1393 train_time:148703ms step_avg:127.53ms
step:1177/1393 train_time:148840ms step_avg:127.54ms
step:1178/1393 train_time:148974ms step_avg:127.55ms
step:1179/1393 train_time:149107ms step_avg:127.55ms
step:1180/1393 train_time:149243ms step_avg:127.56ms
step:1181/1393 train_time:149378ms step_avg:127.56ms
step:1182/1393 train_time:149512ms step_avg:127.57ms
step:1183/1393 train_time:149646ms step_avg:127.58ms
step:1184/1393 train_time:149779ms step_avg:127.58ms
step:1185/1393 train_time:149913ms step_avg:127.59ms
step:1186/1393 train_time:150047ms step_avg:127.59ms
step:1187/1393 train_time:150185ms step_avg:127.60ms
step:1188/1393 train_time:150318ms step_avg:127.60ms
step:1189/1393 train_time:150455ms step_avg:127.61ms
step:1190/1393 train_time:150588ms step_avg:127.62ms
step:1191/1393 train_time:150721ms step_avg:127.62ms
step:1192/1393 train_time:150855ms step_avg:127.63ms
step:1193/1393 train_time:150989ms step_avg:127.63ms
step:1194/1393 train_time:151122ms step_avg:127.64ms
step:1195/1393 train_time:151256ms step_avg:127.64ms
step:1196/1393 train_time:151391ms step_avg:127.65ms
step:1197/1393 train_time:151524ms step_avg:127.65ms
step:1198/1393 train_time:151661ms step_avg:127.66ms
step:1199/1393 train_time:151793ms step_avg:127.66ms
step:1200/1393 train_time:151926ms step_avg:127.67ms
step:1201/1393 train_time:152059ms step_avg:127.67ms
step:1202/1393 train_time:152196ms step_avg:127.68ms
step:1203/1393 train_time:152333ms step_avg:127.69ms
step:1204/1393 train_time:152467ms step_avg:127.69ms
step:1205/1393 train_time:152602ms step_avg:127.70ms
step:1206/1393 train_time:152736ms step_avg:127.71ms
step:1207/1393 train_time:152870ms step_avg:127.71ms
step:1208/1393 train_time:153004ms step_avg:127.72ms
step:1209/1393 train_time:153138ms step_avg:127.72ms
step:1210/1393 train_time:153274ms step_avg:127.73ms
step:1211/1393 train_time:153408ms step_avg:127.73ms
step:1212/1393 train_time:153542ms step_avg:127.74ms
step:1213/1393 train_time:153675ms step_avg:127.74ms
step:1214/1393 train_time:153810ms step_avg:127.75ms
step:1215/1393 train_time:153946ms step_avg:127.76ms
step:1216/1393 train_time:154077ms step_avg:127.76ms
step:1217/1393 train_time:154213ms step_avg:127.77ms
step:1218/1393 train_time:154346ms step_avg:127.77ms
step:1219/1393 train_time:154478ms step_avg:127.77ms
step:1220/1393 train_time:154612ms step_avg:127.78ms
step:1221/1393 train_time:154745ms step_avg:127.78ms
step:1222/1393 train_time:154879ms step_avg:127.79ms
step:1223/1393 train_time:155013ms step_avg:127.79ms
step:1224/1393 train_time:155146ms step_avg:127.80ms
step:1225/1393 train_time:155282ms step_avg:127.80ms
step:1226/1393 train_time:155415ms step_avg:127.81ms
step:1227/1393 train_time:155549ms step_avg:127.81ms
step:1228/1393 train_time:155686ms step_avg:127.82ms
step:1229/1393 train_time:155818ms step_avg:127.82ms
step:1230/1393 train_time:155953ms step_avg:127.83ms
step:1231/1393 train_time:156087ms step_avg:127.84ms
step:1232/1393 train_time:156223ms step_avg:127.84ms
step:1233/1393 train_time:156357ms step_avg:127.85ms
step:1234/1393 train_time:156491ms step_avg:127.85ms
step:1235/1393 train_time:156626ms step_avg:127.86ms
step:1236/1393 train_time:156761ms step_avg:127.86ms
step:1237/1393 train_time:156893ms step_avg:127.87ms
step:1238/1393 train_time:157031ms step_avg:127.88ms
step:1239/1393 train_time:157166ms step_avg:127.88ms
step:1240/1393 train_time:157301ms step_avg:127.89ms
step:1241/1393 train_time:157436ms step_avg:127.89ms
step:1242/1393 train_time:157569ms step_avg:127.90ms
step:1243/1393 train_time:157703ms step_avg:127.90ms
step:1244/1393 train_time:157837ms step_avg:127.91ms
step:1245/1393 train_time:157971ms step_avg:127.91ms
step:1246/1393 train_time:158104ms step_avg:127.92ms
step:1247/1393 train_time:158239ms step_avg:127.92ms
step:1248/1393 train_time:158372ms step_avg:127.93ms
step:1249/1393 train_time:158505ms step_avg:127.93ms
step:1250/1393 train_time:158638ms step_avg:127.93ms
step:1250/1393 val_loss:3.3168 train_time:158773ms step_avg:128.04ms
step:1251/1393 train_time:158795ms step_avg:127.96ms
step:1252/1393 train_time:158918ms step_avg:127.95ms
step:1253/1393 train_time:159051ms step_avg:127.96ms
step:1254/1393 train_time:159183ms step_avg:127.96ms
step:1255/1393 train_time:159323ms step_avg:127.97ms
step:1256/1393 train_time:159455ms step_avg:127.97ms
step:1257/1393 train_time:159588ms step_avg:127.98ms
step:1258/1393 train_time:159721ms step_avg:127.98ms
step:1259/1393 train_time:159858ms step_avg:127.99ms
step:1260/1393 train_time:159992ms step_avg:127.99ms
step:1261/1393 train_time:160125ms step_avg:128.00ms
step:1262/1393 train_time:160261ms step_avg:128.00ms
step:1263/1393 train_time:160395ms step_avg:128.01ms
step:1264/1393 train_time:160528ms step_avg:128.01ms
step:1265/1393 train_time:160661ms step_avg:128.02ms
step:1266/1393 train_time:160796ms step_avg:128.02ms
step:1267/1393 train_time:160930ms step_avg:128.03ms
step:1268/1393 train_time:161063ms step_avg:128.03ms
step:1269/1393 train_time:161199ms step_avg:128.04ms
step:1270/1393 train_time:161333ms step_avg:128.04ms
step:1271/1393 train_time:161467ms step_avg:128.05ms
step:1272/1393 train_time:161601ms step_avg:128.05ms
step:1273/1393 train_time:161733ms step_avg:128.05ms
step:1274/1393 train_time:161868ms step_avg:128.06ms
step:1275/1393 train_time:162001ms step_avg:128.06ms
step:1276/1393 train_time:162135ms step_avg:128.07ms
step:1277/1393 train_time:162269ms step_avg:128.07ms
step:1278/1393 train_time:162402ms step_avg:128.08ms
step:1279/1393 train_time:162536ms step_avg:128.08ms
step:1280/1393 train_time:162671ms step_avg:128.09ms
step:1281/1393 train_time:162805ms step_avg:128.09ms
step:1282/1393 train_time:162938ms step_avg:128.10ms
step:1283/1393 train_time:163071ms step_avg:128.10ms
step:1284/1393 train_time:163206ms step_avg:128.10ms
step:1285/1393 train_time:163339ms step_avg:128.11ms
step:1286/1393 train_time:163473ms step_avg:128.11ms
step:1287/1393 train_time:163607ms step_avg:128.12ms
step:1288/1393 train_time:163741ms step_avg:128.12ms
step:1289/1393 train_time:163877ms step_avg:128.13ms
step:1290/1393 train_time:164014ms step_avg:128.14ms
step:1291/1393 train_time:164149ms step_avg:128.14ms
step:1292/1393 train_time:164285ms step_avg:128.15ms
step:1293/1393 train_time:164420ms step_avg:128.15ms
step:1294/1393 train_time:164553ms step_avg:128.16ms
step:1295/1393 train_time:164688ms step_avg:128.16ms
step:1296/1393 train_time:164822ms step_avg:128.17ms
step:1297/1393 train_time:164956ms step_avg:128.17ms
step:1298/1393 train_time:165091ms step_avg:128.18ms
step:1299/1393 train_time:165224ms step_avg:128.18ms
step:1300/1393 train_time:165357ms step_avg:128.18ms
step:1301/1393 train_time:165490ms step_avg:128.19ms
step:1302/1393 train_time:165625ms step_avg:128.19ms
step:1303/1393 train_time:165760ms step_avg:128.20ms
step:1304/1393 train_time:165894ms step_avg:128.20ms
step:1305/1393 train_time:166030ms step_avg:128.21ms
step:1306/1393 train_time:166163ms step_avg:128.21ms
step:1307/1393 train_time:166297ms step_avg:128.22ms
step:1308/1393 train_time:166432ms step_avg:128.22ms
step:1309/1393 train_time:166565ms step_avg:128.23ms
step:1310/1393 train_time:166701ms step_avg:128.23ms
step:1311/1393 train_time:166834ms step_avg:128.24ms
step:1312/1393 train_time:166968ms step_avg:128.24ms
step:1313/1393 train_time:167101ms step_avg:128.24ms
step:1314/1393 train_time:167236ms step_avg:128.25ms
step:1315/1393 train_time:167370ms step_avg:128.25ms
step:1316/1393 train_time:167503ms step_avg:128.26ms
step:1317/1393 train_time:167637ms step_avg:128.26ms
step:1318/1393 train_time:167772ms step_avg:128.27ms
step:1319/1393 train_time:167908ms step_avg:128.27ms
step:1320/1393 train_time:168041ms step_avg:128.28ms
step:1321/1393 train_time:168174ms step_avg:128.28ms
step:1322/1393 train_time:168311ms step_avg:128.29ms
step:1323/1393 train_time:168446ms step_avg:128.29ms
step:1324/1393 train_time:168579ms step_avg:128.29ms
step:1325/1393 train_time:168714ms step_avg:128.30ms
step:1326/1393 train_time:168849ms step_avg:128.30ms
step:1327/1393 train_time:168983ms step_avg:128.31ms
step:1328/1393 train_time:169116ms step_avg:128.31ms
step:1329/1393 train_time:169253ms step_avg:128.32ms
step:1330/1393 train_time:169389ms step_avg:128.33ms
step:1331/1393 train_time:169526ms step_avg:128.33ms
step:1332/1393 train_time:169662ms step_avg:128.34ms
step:1333/1393 train_time:169797ms step_avg:128.34ms
step:1334/1393 train_time:169931ms step_avg:128.35ms
step:1335/1393 train_time:170064ms step_avg:128.35ms
step:1336/1393 train_time:170200ms step_avg:128.36ms
step:1337/1393 train_time:170335ms step_avg:128.36ms
step:1338/1393 train_time:170470ms step_avg:128.37ms
step:1339/1393 train_time:170604ms step_avg:128.37ms
step:1340/1393 train_time:170740ms step_avg:128.38ms
step:1341/1393 train_time:170872ms step_avg:128.38ms
step:1342/1393 train_time:171006ms step_avg:128.38ms
step:1343/1393 train_time:171139ms step_avg:128.39ms
step:1344/1393 train_time:171274ms step_avg:128.39ms
step:1345/1393 train_time:171410ms step_avg:128.40ms
step:1346/1393 train_time:171544ms step_avg:128.40ms
step:1347/1393 train_time:171680ms step_avg:128.41ms
step:1348/1393 train_time:171813ms step_avg:128.41ms
step:1349/1393 train_time:171949ms step_avg:128.42ms
step:1350/1393 train_time:172082ms step_avg:128.42ms
step:1351/1393 train_time:172216ms step_avg:128.42ms
step:1352/1393 train_time:172355ms step_avg:128.43ms
step:1353/1393 train_time:172491ms step_avg:128.44ms
step:1354/1393 train_time:172627ms step_avg:128.44ms
step:1355/1393 train_time:172761ms step_avg:128.45ms
step:1356/1393 train_time:172894ms step_avg:128.45ms
step:1357/1393 train_time:173030ms step_avg:128.46ms
step:1358/1393 train_time:173166ms step_avg:128.46ms
step:1359/1393 train_time:173301ms step_avg:128.47ms
step:1360/1393 train_time:173438ms step_avg:128.47ms
step:1361/1393 train_time:173573ms step_avg:128.48ms
step:1362/1393 train_time:173711ms step_avg:128.48ms
step:1363/1393 train_time:173848ms step_avg:128.49ms
step:1364/1393 train_time:173983ms step_avg:128.50ms
step:1365/1393 train_time:174116ms step_avg:128.50ms
step:1366/1393 train_time:174251ms step_avg:128.50ms
step:1367/1393 train_time:174386ms step_avg:128.51ms
step:1368/1393 train_time:174522ms step_avg:128.51ms
step:1369/1393 train_time:174661ms step_avg:128.52ms
step:1370/1393 train_time:174798ms step_avg:128.53ms
step:1371/1393 train_time:174935ms step_avg:128.53ms
step:1372/1393 train_time:175072ms step_avg:128.54ms
step:1373/1393 train_time:175205ms step_avg:128.54ms
step:1374/1393 train_time:175342ms step_avg:128.55ms
step:1375/1393 train_time:175476ms step_avg:128.55ms
step:1375/1393 val_loss:3.2829 train_time:175609ms step_avg:128.65ms
step:1376/1393 train_time:175631ms step_avg:128.57ms
step:1377/1393 train_time:175754ms step_avg:128.57ms
step:1378/1393 train_time:175889ms step_avg:128.57ms
step:1379/1393 train_time:176023ms step_avg:128.58ms
step:1380/1393 train_time:176158ms step_avg:128.58ms
step:1381/1393 train_time:176294ms step_avg:128.59ms
step:1382/1393 train_time:176429ms step_avg:128.59ms
step:1383/1393 train_time:176563ms step_avg:128.60ms
step:1384/1393 train_time:176701ms step_avg:128.60ms
step:1385/1393 train_time:176835ms step_avg:128.61ms
step:1386/1393 train_time:176970ms step_avg:128.61ms
step:1387/1393 train_time:177107ms step_avg:128.62ms
step:1388/1393 train_time:177241ms step_avg:128.62ms
step:1389/1393 train_time:177376ms step_avg:128.63ms
step:1390/1393 train_time:177511ms step_avg:128.63ms
step:1391/1393 train_time:177646ms step_avg:128.64ms
step:1392/1393 train_time:177782ms step_avg:128.64ms
step:1393/1393 train_time:177916ms step_avg:128.64ms
step:1393/1393 val_loss:3.2794 train_time:178049ms step_avg:128.74ms
peak memory allocated: 37653 MiB reserved: 41736 MiB
