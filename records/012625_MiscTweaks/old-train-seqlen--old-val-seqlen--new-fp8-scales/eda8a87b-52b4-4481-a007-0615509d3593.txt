import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:15:51 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23408ms step_avg:nanms
step:2/1393 train_time:23825ms step_avg:nanms
step:3/1393 train_time:23951ms step_avg:nanms
step:4/1393 train_time:24071ms step_avg:nanms
step:5/1393 train_time:24192ms step_avg:nanms
step:6/1393 train_time:24313ms step_avg:nanms
step:7/1393 train_time:24434ms step_avg:nanms
step:8/1393 train_time:24555ms step_avg:nanms
step:9/1393 train_time:24676ms step_avg:nanms
step:10/1393 train_time:24798ms step_avg:nanms
step:11/1393 train_time:122ms step_avg:nanms
step:12/1393 train_time:245ms step_avg:nanms
step:13/1393 train_time:366ms step_avg:122.00ms
step:14/1393 train_time:487ms step_avg:121.71ms
step:15/1393 train_time:608ms step_avg:121.52ms
step:16/1393 train_time:728ms step_avg:121.35ms
step:17/1393 train_time:849ms step_avg:121.31ms
step:18/1393 train_time:971ms step_avg:121.33ms
step:19/1393 train_time:1093ms step_avg:121.46ms
step:20/1393 train_time:1214ms step_avg:121.43ms
step:21/1393 train_time:1336ms step_avg:121.46ms
step:22/1393 train_time:1458ms step_avg:121.51ms
step:23/1393 train_time:1579ms step_avg:121.50ms
step:24/1393 train_time:1701ms step_avg:121.50ms
step:25/1393 train_time:1822ms step_avg:121.47ms
step:26/1393 train_time:1945ms step_avg:121.58ms
step:27/1393 train_time:2067ms step_avg:121.60ms
step:28/1393 train_time:2188ms step_avg:121.57ms
step:29/1393 train_time:2308ms step_avg:121.50ms
step:30/1393 train_time:2429ms step_avg:121.47ms
step:31/1393 train_time:2551ms step_avg:121.47ms
step:32/1393 train_time:2672ms step_avg:121.46ms
step:33/1393 train_time:2793ms step_avg:121.43ms
step:34/1393 train_time:2914ms step_avg:121.43ms
step:35/1393 train_time:3037ms step_avg:121.47ms
step:36/1393 train_time:3158ms step_avg:121.47ms
step:37/1393 train_time:3279ms step_avg:121.45ms
step:38/1393 train_time:3402ms step_avg:121.48ms
step:39/1393 train_time:3525ms step_avg:121.54ms
step:40/1393 train_time:3646ms step_avg:121.54ms
step:41/1393 train_time:3768ms step_avg:121.54ms
step:42/1393 train_time:3888ms step_avg:121.50ms
step:43/1393 train_time:4010ms step_avg:121.53ms
step:44/1393 train_time:4132ms step_avg:121.52ms
step:45/1393 train_time:4254ms step_avg:121.54ms
step:46/1393 train_time:4375ms step_avg:121.52ms
step:47/1393 train_time:4497ms step_avg:121.55ms
step:48/1393 train_time:4618ms step_avg:121.53ms
step:49/1393 train_time:4740ms step_avg:121.53ms
step:50/1393 train_time:4861ms step_avg:121.52ms
step:51/1393 train_time:4984ms step_avg:121.55ms
step:52/1393 train_time:5104ms step_avg:121.54ms
step:53/1393 train_time:5225ms step_avg:121.52ms
step:54/1393 train_time:5346ms step_avg:121.51ms
step:55/1393 train_time:5467ms step_avg:121.50ms
step:56/1393 train_time:5588ms step_avg:121.49ms
step:57/1393 train_time:5712ms step_avg:121.52ms
step:58/1393 train_time:5833ms step_avg:121.52ms
step:59/1393 train_time:5954ms step_avg:121.52ms
step:60/1393 train_time:6076ms step_avg:121.52ms
step:61/1393 train_time:6197ms step_avg:121.51ms
step:62/1393 train_time:6317ms step_avg:121.49ms
step:63/1393 train_time:6439ms step_avg:121.48ms
step:64/1393 train_time:6560ms step_avg:121.48ms
step:65/1393 train_time:6682ms step_avg:121.48ms
step:66/1393 train_time:6803ms step_avg:121.48ms
step:67/1393 train_time:6924ms step_avg:121.47ms
step:68/1393 train_time:7045ms step_avg:121.46ms
step:69/1393 train_time:7167ms step_avg:121.47ms
step:70/1393 train_time:7286ms step_avg:121.44ms
step:71/1393 train_time:7406ms step_avg:121.41ms
step:72/1393 train_time:7528ms step_avg:121.41ms
step:73/1393 train_time:7649ms step_avg:121.41ms
step:74/1393 train_time:7772ms step_avg:121.43ms
step:75/1393 train_time:7893ms step_avg:121.43ms
step:76/1393 train_time:8014ms step_avg:121.42ms
step:77/1393 train_time:8136ms step_avg:121.43ms
step:78/1393 train_time:8257ms step_avg:121.42ms
step:79/1393 train_time:8379ms step_avg:121.43ms
step:80/1393 train_time:8499ms step_avg:121.42ms
step:81/1393 train_time:8620ms step_avg:121.41ms
step:82/1393 train_time:8742ms step_avg:121.41ms
step:83/1393 train_time:8862ms step_avg:121.40ms
step:84/1393 train_time:8983ms step_avg:121.39ms
step:85/1393 train_time:9105ms step_avg:121.40ms
step:86/1393 train_time:9228ms step_avg:121.42ms
step:87/1393 train_time:9349ms step_avg:121.41ms
step:88/1393 train_time:9469ms step_avg:121.40ms
step:89/1393 train_time:9590ms step_avg:121.39ms
step:90/1393 train_time:9711ms step_avg:121.38ms
step:91/1393 train_time:9833ms step_avg:121.39ms
step:92/1393 train_time:9954ms step_avg:121.39ms
step:93/1393 train_time:10077ms step_avg:121.41ms
step:94/1393 train_time:10199ms step_avg:121.42ms
step:95/1393 train_time:10321ms step_avg:121.43ms
step:96/1393 train_time:10443ms step_avg:121.43ms
step:97/1393 train_time:10564ms step_avg:121.43ms
step:98/1393 train_time:10685ms step_avg:121.42ms
step:99/1393 train_time:10807ms step_avg:121.43ms
step:100/1393 train_time:10928ms step_avg:121.42ms
step:101/1393 train_time:11049ms step_avg:121.42ms
step:102/1393 train_time:11172ms step_avg:121.44ms
step:103/1393 train_time:11294ms step_avg:121.45ms
step:104/1393 train_time:11415ms step_avg:121.44ms
step:105/1393 train_time:11536ms step_avg:121.44ms
step:106/1393 train_time:11659ms step_avg:121.45ms
step:107/1393 train_time:11781ms step_avg:121.45ms
step:108/1393 train_time:11903ms step_avg:121.46ms
step:109/1393 train_time:12025ms step_avg:121.46ms
step:110/1393 train_time:12147ms step_avg:121.47ms
step:111/1393 train_time:12269ms step_avg:121.47ms
step:112/1393 train_time:12391ms step_avg:121.48ms
step:113/1393 train_time:12514ms step_avg:121.49ms
step:114/1393 train_time:12635ms step_avg:121.49ms
step:115/1393 train_time:12757ms step_avg:121.50ms
step:116/1393 train_time:12880ms step_avg:121.50ms
step:117/1393 train_time:13002ms step_avg:121.52ms
step:118/1393 train_time:13124ms step_avg:121.52ms
step:119/1393 train_time:13247ms step_avg:121.53ms
step:120/1393 train_time:13368ms step_avg:121.53ms
step:121/1393 train_time:13491ms step_avg:121.54ms
step:122/1393 train_time:13613ms step_avg:121.54ms
step:123/1393 train_time:13736ms step_avg:121.56ms
step:124/1393 train_time:13858ms step_avg:121.56ms
step:125/1393 train_time:13979ms step_avg:121.56ms
step:125/1393 val_loss:4.4305 train_time:14099ms step_avg:122.60ms
step:126/1393 train_time:14127ms step_avg:121.78ms
step:127/1393 train_time:14238ms step_avg:121.69ms
step:128/1393 train_time:14362ms step_avg:121.72ms
step:129/1393 train_time:14485ms step_avg:121.72ms
step:130/1393 train_time:14607ms step_avg:121.72ms
step:131/1393 train_time:14727ms step_avg:121.71ms
step:132/1393 train_time:14848ms step_avg:121.70ms
step:133/1393 train_time:14970ms step_avg:121.70ms
step:134/1393 train_time:15091ms step_avg:121.71ms
step:135/1393 train_time:15217ms step_avg:121.74ms
step:136/1393 train_time:15341ms step_avg:121.75ms
step:137/1393 train_time:15464ms step_avg:121.76ms
step:138/1393 train_time:15585ms step_avg:121.76ms
step:139/1393 train_time:15706ms step_avg:121.75ms
step:140/1393 train_time:15828ms step_avg:121.75ms
step:141/1393 train_time:15950ms step_avg:121.75ms
step:142/1393 train_time:16071ms step_avg:121.75ms
step:143/1393 train_time:16194ms step_avg:121.76ms
step:144/1393 train_time:16316ms step_avg:121.76ms
step:145/1393 train_time:16439ms step_avg:121.77ms
step:146/1393 train_time:16563ms step_avg:121.78ms
step:147/1393 train_time:16685ms step_avg:121.79ms
step:148/1393 train_time:16808ms step_avg:121.80ms
step:149/1393 train_time:16930ms step_avg:121.80ms
step:150/1393 train_time:17052ms step_avg:121.80ms
step:151/1393 train_time:17174ms step_avg:121.80ms
step:152/1393 train_time:17297ms step_avg:121.81ms
step:153/1393 train_time:17420ms step_avg:121.82ms
step:154/1393 train_time:17542ms step_avg:121.82ms
step:155/1393 train_time:17663ms step_avg:121.82ms
step:156/1393 train_time:17786ms step_avg:121.83ms
step:157/1393 train_time:17908ms step_avg:121.83ms
step:158/1393 train_time:18031ms step_avg:121.83ms
step:159/1393 train_time:18152ms step_avg:121.82ms
step:160/1393 train_time:18274ms step_avg:121.83ms
step:161/1393 train_time:18398ms step_avg:121.84ms
step:162/1393 train_time:18520ms step_avg:121.84ms
step:163/1393 train_time:18643ms step_avg:121.85ms
step:164/1393 train_time:18765ms step_avg:121.85ms
step:165/1393 train_time:18888ms step_avg:121.86ms
step:166/1393 train_time:19010ms step_avg:121.86ms
step:167/1393 train_time:19133ms step_avg:121.86ms
step:168/1393 train_time:19254ms step_avg:121.86ms
step:169/1393 train_time:19378ms step_avg:121.88ms
step:170/1393 train_time:19500ms step_avg:121.88ms
step:171/1393 train_time:19623ms step_avg:121.88ms
step:172/1393 train_time:19745ms step_avg:121.88ms
step:173/1393 train_time:19867ms step_avg:121.88ms
step:174/1393 train_time:19989ms step_avg:121.88ms
step:175/1393 train_time:20110ms step_avg:121.88ms
step:176/1393 train_time:20232ms step_avg:121.88ms
step:177/1393 train_time:20355ms step_avg:121.89ms
step:178/1393 train_time:20478ms step_avg:121.89ms
step:179/1393 train_time:20600ms step_avg:121.89ms
step:180/1393 train_time:20722ms step_avg:121.90ms
step:181/1393 train_time:20844ms step_avg:121.90ms
step:182/1393 train_time:20966ms step_avg:121.90ms
step:183/1393 train_time:21088ms step_avg:121.90ms
step:184/1393 train_time:21211ms step_avg:121.90ms
step:185/1393 train_time:21333ms step_avg:121.90ms
step:186/1393 train_time:21455ms step_avg:121.91ms
step:187/1393 train_time:21578ms step_avg:121.91ms
step:188/1393 train_time:21700ms step_avg:121.91ms
step:189/1393 train_time:21821ms step_avg:121.91ms
step:190/1393 train_time:21943ms step_avg:121.91ms
step:191/1393 train_time:22068ms step_avg:121.92ms
step:192/1393 train_time:22189ms step_avg:121.92ms
step:193/1393 train_time:22311ms step_avg:121.92ms
step:194/1393 train_time:22434ms step_avg:121.92ms
step:195/1393 train_time:22557ms step_avg:121.93ms
step:196/1393 train_time:22679ms step_avg:121.93ms
step:197/1393 train_time:22801ms step_avg:121.93ms
step:198/1393 train_time:22927ms step_avg:121.95ms
step:199/1393 train_time:23046ms step_avg:121.94ms
step:200/1393 train_time:23169ms step_avg:121.94ms
step:201/1393 train_time:23290ms step_avg:121.94ms
step:202/1393 train_time:23413ms step_avg:121.94ms
step:203/1393 train_time:23534ms step_avg:121.94ms
step:204/1393 train_time:23656ms step_avg:121.94ms
step:205/1393 train_time:23778ms step_avg:121.94ms
step:206/1393 train_time:23900ms step_avg:121.94ms
step:207/1393 train_time:24023ms step_avg:121.95ms
step:208/1393 train_time:24145ms step_avg:121.95ms
step:209/1393 train_time:24268ms step_avg:121.95ms
step:210/1393 train_time:24391ms step_avg:121.96ms
step:211/1393 train_time:24515ms step_avg:121.96ms
step:212/1393 train_time:24639ms step_avg:121.98ms
step:213/1393 train_time:24760ms step_avg:121.97ms
step:214/1393 train_time:24883ms step_avg:121.97ms
step:215/1393 train_time:25006ms step_avg:121.98ms
step:216/1393 train_time:25128ms step_avg:121.98ms
step:217/1393 train_time:25251ms step_avg:121.99ms
step:218/1393 train_time:25375ms step_avg:121.99ms
step:219/1393 train_time:25498ms step_avg:122.00ms
step:220/1393 train_time:25621ms step_avg:122.00ms
step:221/1393 train_time:25743ms step_avg:122.00ms
step:222/1393 train_time:25866ms step_avg:122.01ms
step:223/1393 train_time:25988ms step_avg:122.01ms
step:224/1393 train_time:26112ms step_avg:122.02ms
step:225/1393 train_time:26235ms step_avg:122.02ms
step:226/1393 train_time:26358ms step_avg:122.03ms
step:227/1393 train_time:26480ms step_avg:122.03ms
step:228/1393 train_time:26603ms step_avg:122.03ms
step:229/1393 train_time:26725ms step_avg:122.03ms
step:230/1393 train_time:26848ms step_avg:122.03ms
step:231/1393 train_time:26970ms step_avg:122.04ms
step:232/1393 train_time:27093ms step_avg:122.04ms
step:233/1393 train_time:27215ms step_avg:122.04ms
step:234/1393 train_time:27338ms step_avg:122.05ms
step:235/1393 train_time:27461ms step_avg:122.05ms
step:236/1393 train_time:27583ms step_avg:122.05ms
step:237/1393 train_time:27707ms step_avg:122.06ms
step:238/1393 train_time:27829ms step_avg:122.06ms
step:239/1393 train_time:27951ms step_avg:122.06ms
step:240/1393 train_time:28075ms step_avg:122.06ms
step:241/1393 train_time:28198ms step_avg:122.07ms
step:242/1393 train_time:28321ms step_avg:122.07ms
step:243/1393 train_time:28444ms step_avg:122.08ms
step:244/1393 train_time:28565ms step_avg:122.07ms
step:245/1393 train_time:28688ms step_avg:122.08ms
step:246/1393 train_time:28811ms step_avg:122.08ms
step:247/1393 train_time:28933ms step_avg:122.08ms
step:248/1393 train_time:29056ms step_avg:122.08ms
step:249/1393 train_time:29179ms step_avg:122.09ms
step:250/1393 train_time:29302ms step_avg:122.09ms
step:250/1393 val_loss:3.9924 train_time:29424ms step_avg:122.60ms
step:251/1393 train_time:29445ms step_avg:122.18ms
step:252/1393 train_time:29561ms step_avg:122.15ms
step:253/1393 train_time:29686ms step_avg:122.16ms
step:254/1393 train_time:29807ms step_avg:122.16ms
step:255/1393 train_time:29929ms step_avg:122.16ms
step:256/1393 train_time:30051ms step_avg:122.16ms
step:257/1393 train_time:30173ms step_avg:122.16ms
step:258/1393 train_time:30295ms step_avg:122.16ms
step:259/1393 train_time:30418ms step_avg:122.16ms
step:260/1393 train_time:30544ms step_avg:122.17ms
step:261/1393 train_time:30668ms step_avg:122.18ms
step:262/1393 train_time:30791ms step_avg:122.19ms
step:263/1393 train_time:30913ms step_avg:122.18ms
step:264/1393 train_time:31034ms step_avg:122.18ms
step:265/1393 train_time:31157ms step_avg:122.18ms
step:266/1393 train_time:31278ms step_avg:122.18ms
step:267/1393 train_time:31401ms step_avg:122.18ms
step:268/1393 train_time:31525ms step_avg:122.19ms
step:269/1393 train_time:31649ms step_avg:122.20ms
step:270/1393 train_time:31772ms step_avg:122.20ms
step:271/1393 train_time:31895ms step_avg:122.20ms
step:272/1393 train_time:32017ms step_avg:122.20ms
step:273/1393 train_time:32140ms step_avg:122.20ms
step:274/1393 train_time:32262ms step_avg:122.21ms
step:275/1393 train_time:32385ms step_avg:122.21ms
step:276/1393 train_time:32507ms step_avg:122.21ms
step:277/1393 train_time:32631ms step_avg:122.21ms
step:278/1393 train_time:32754ms step_avg:122.22ms
step:279/1393 train_time:32876ms step_avg:122.22ms
step:280/1393 train_time:32998ms step_avg:122.22ms
step:281/1393 train_time:33122ms step_avg:122.22ms
step:282/1393 train_time:33245ms step_avg:122.22ms
step:283/1393 train_time:33368ms step_avg:122.23ms
step:284/1393 train_time:33491ms step_avg:122.23ms
step:285/1393 train_time:33614ms step_avg:122.23ms
step:286/1393 train_time:33737ms step_avg:122.23ms
step:287/1393 train_time:33859ms step_avg:122.24ms
step:288/1393 train_time:33982ms step_avg:122.24ms
step:289/1393 train_time:34104ms step_avg:122.24ms
step:290/1393 train_time:34226ms step_avg:122.24ms
step:291/1393 train_time:34349ms step_avg:122.24ms
step:292/1393 train_time:34471ms step_avg:122.24ms
step:293/1393 train_time:34596ms step_avg:122.25ms
step:294/1393 train_time:34720ms step_avg:122.25ms
step:295/1393 train_time:34843ms step_avg:122.26ms
step:296/1393 train_time:34966ms step_avg:122.26ms
step:297/1393 train_time:35088ms step_avg:122.26ms
step:298/1393 train_time:35211ms step_avg:122.26ms
step:299/1393 train_time:35333ms step_avg:122.26ms
step:300/1393 train_time:35456ms step_avg:122.26ms
step:301/1393 train_time:35580ms step_avg:122.27ms
step:302/1393 train_time:35702ms step_avg:122.27ms
step:303/1393 train_time:35825ms step_avg:122.27ms
step:304/1393 train_time:35948ms step_avg:122.27ms
step:305/1393 train_time:36071ms step_avg:122.28ms
step:306/1393 train_time:36195ms step_avg:122.28ms
step:307/1393 train_time:36318ms step_avg:122.28ms
step:308/1393 train_time:36440ms step_avg:122.28ms
step:309/1393 train_time:36562ms step_avg:122.28ms
step:310/1393 train_time:36686ms step_avg:122.29ms
step:311/1393 train_time:36809ms step_avg:122.29ms
step:312/1393 train_time:36934ms step_avg:122.30ms
step:313/1393 train_time:37059ms step_avg:122.31ms
step:314/1393 train_time:37185ms step_avg:122.32ms
step:315/1393 train_time:37310ms step_avg:122.33ms
step:316/1393 train_time:37436ms step_avg:122.34ms
step:317/1393 train_time:37561ms step_avg:122.35ms
step:318/1393 train_time:37687ms step_avg:122.36ms
step:319/1393 train_time:37812ms step_avg:122.37ms
step:320/1393 train_time:37937ms step_avg:122.38ms
step:321/1393 train_time:38062ms step_avg:122.39ms
step:322/1393 train_time:38187ms step_avg:122.39ms
step:323/1393 train_time:38312ms step_avg:122.40ms
step:324/1393 train_time:38438ms step_avg:122.41ms
step:325/1393 train_time:38563ms step_avg:122.42ms
step:326/1393 train_time:38688ms step_avg:122.43ms
step:327/1393 train_time:38813ms step_avg:122.44ms
step:328/1393 train_time:38938ms step_avg:122.45ms
step:329/1393 train_time:39063ms step_avg:122.45ms
step:330/1393 train_time:39188ms step_avg:122.46ms
step:331/1393 train_time:39313ms step_avg:122.47ms
step:332/1393 train_time:39438ms step_avg:122.48ms
step:333/1393 train_time:39564ms step_avg:122.49ms
step:334/1393 train_time:39689ms step_avg:122.50ms
step:335/1393 train_time:39814ms step_avg:122.50ms
step:336/1393 train_time:39939ms step_avg:122.51ms
step:337/1393 train_time:40064ms step_avg:122.52ms
step:338/1393 train_time:40189ms step_avg:122.53ms
step:339/1393 train_time:40314ms step_avg:122.54ms
step:340/1393 train_time:40439ms step_avg:122.54ms
step:341/1393 train_time:40564ms step_avg:122.55ms
step:342/1393 train_time:40690ms step_avg:122.56ms
step:343/1393 train_time:40815ms step_avg:122.57ms
step:344/1393 train_time:40940ms step_avg:122.58ms
step:345/1393 train_time:41065ms step_avg:122.58ms
step:346/1393 train_time:41190ms step_avg:122.59ms
step:347/1393 train_time:41316ms step_avg:122.60ms
step:348/1393 train_time:41442ms step_avg:122.61ms
step:349/1393 train_time:41566ms step_avg:122.61ms
step:350/1393 train_time:41692ms step_avg:122.62ms
step:351/1393 train_time:41817ms step_avg:122.63ms
step:352/1393 train_time:41943ms step_avg:122.64ms
step:353/1393 train_time:42067ms step_avg:122.64ms
step:354/1393 train_time:42192ms step_avg:122.65ms
step:355/1393 train_time:42317ms step_avg:122.66ms
step:356/1393 train_time:42442ms step_avg:122.67ms
step:357/1393 train_time:42568ms step_avg:122.67ms
step:358/1393 train_time:42692ms step_avg:122.68ms
step:359/1393 train_time:42817ms step_avg:122.69ms
step:360/1393 train_time:42942ms step_avg:122.69ms
step:361/1393 train_time:43067ms step_avg:122.70ms
step:362/1393 train_time:43192ms step_avg:122.70ms
step:363/1393 train_time:43317ms step_avg:122.71ms
step:364/1393 train_time:43443ms step_avg:122.72ms
step:365/1393 train_time:43568ms step_avg:122.73ms
step:366/1393 train_time:43693ms step_avg:122.73ms
step:367/1393 train_time:43819ms step_avg:122.74ms
step:368/1393 train_time:43945ms step_avg:122.75ms
step:369/1393 train_time:44069ms step_avg:122.76ms
step:370/1393 train_time:44194ms step_avg:122.76ms
step:371/1393 train_time:44319ms step_avg:122.77ms
step:372/1393 train_time:44445ms step_avg:122.78ms
step:373/1393 train_time:44570ms step_avg:122.78ms
step:374/1393 train_time:44695ms step_avg:122.79ms
step:375/1393 train_time:44822ms step_avg:122.80ms
step:375/1393 val_loss:3.7894 train_time:44945ms step_avg:123.14ms
step:376/1393 train_time:44966ms step_avg:122.86ms
step:377/1393 train_time:45083ms step_avg:122.84ms
step:378/1393 train_time:45210ms step_avg:122.85ms
step:379/1393 train_time:45335ms step_avg:122.86ms
step:380/1393 train_time:45460ms step_avg:122.86ms
step:381/1393 train_time:45585ms step_avg:122.87ms
step:382/1393 train_time:45709ms step_avg:122.87ms
step:383/1393 train_time:45835ms step_avg:122.88ms
step:384/1393 train_time:45959ms step_avg:122.89ms
step:385/1393 train_time:46085ms step_avg:122.89ms
step:386/1393 train_time:46212ms step_avg:122.90ms
step:387/1393 train_time:46337ms step_avg:122.91ms
step:388/1393 train_time:46461ms step_avg:122.91ms
step:389/1393 train_time:46586ms step_avg:122.92ms
step:390/1393 train_time:46710ms step_avg:122.92ms
step:391/1393 train_time:46834ms step_avg:122.93ms
step:392/1393 train_time:46960ms step_avg:122.93ms
step:393/1393 train_time:47085ms step_avg:122.94ms
step:394/1393 train_time:47211ms step_avg:122.94ms
step:395/1393 train_time:47336ms step_avg:122.95ms
step:396/1393 train_time:47461ms step_avg:122.96ms
step:397/1393 train_time:47587ms step_avg:122.96ms
step:398/1393 train_time:47712ms step_avg:122.97ms
step:399/1393 train_time:47837ms step_avg:122.97ms
step:400/1393 train_time:47962ms step_avg:122.98ms
step:401/1393 train_time:48088ms step_avg:122.99ms
step:402/1393 train_time:48213ms step_avg:122.99ms
step:403/1393 train_time:48339ms step_avg:123.00ms
step:404/1393 train_time:48464ms step_avg:123.01ms
step:405/1393 train_time:48591ms step_avg:123.01ms
step:406/1393 train_time:48716ms step_avg:123.02ms
step:407/1393 train_time:48842ms step_avg:123.03ms
step:408/1393 train_time:48967ms step_avg:123.03ms
step:409/1393 train_time:49092ms step_avg:123.04ms
step:410/1393 train_time:49217ms step_avg:123.04ms
step:411/1393 train_time:49343ms step_avg:123.05ms
step:412/1393 train_time:49468ms step_avg:123.05ms
step:413/1393 train_time:49593ms step_avg:123.06ms
step:414/1393 train_time:49718ms step_avg:123.06ms
step:415/1393 train_time:49845ms step_avg:123.07ms
step:416/1393 train_time:49970ms step_avg:123.08ms
step:417/1393 train_time:50096ms step_avg:123.09ms
step:418/1393 train_time:50222ms step_avg:123.09ms
step:419/1393 train_time:50348ms step_avg:123.10ms
step:420/1393 train_time:50473ms step_avg:123.10ms
step:421/1393 train_time:50598ms step_avg:123.11ms
step:422/1393 train_time:50724ms step_avg:123.12ms
step:423/1393 train_time:50850ms step_avg:123.12ms
step:424/1393 train_time:50974ms step_avg:123.13ms
step:425/1393 train_time:51100ms step_avg:123.13ms
step:426/1393 train_time:51227ms step_avg:123.14ms
step:427/1393 train_time:51352ms step_avg:123.15ms
step:428/1393 train_time:51478ms step_avg:123.15ms
step:429/1393 train_time:51604ms step_avg:123.16ms
step:430/1393 train_time:51730ms step_avg:123.17ms
step:431/1393 train_time:51855ms step_avg:123.17ms
step:432/1393 train_time:51981ms step_avg:123.18ms
step:433/1393 train_time:52107ms step_avg:123.18ms
step:434/1393 train_time:52232ms step_avg:123.19ms
step:435/1393 train_time:52358ms step_avg:123.20ms
step:436/1393 train_time:52484ms step_avg:123.20ms
step:437/1393 train_time:52610ms step_avg:123.21ms
step:438/1393 train_time:52735ms step_avg:123.21ms
step:439/1393 train_time:52861ms step_avg:123.22ms
step:440/1393 train_time:52988ms step_avg:123.23ms
step:441/1393 train_time:53114ms step_avg:123.24ms
step:442/1393 train_time:53240ms step_avg:123.24ms
step:443/1393 train_time:53366ms step_avg:123.25ms
step:444/1393 train_time:53491ms step_avg:123.25ms
step:445/1393 train_time:53617ms step_avg:123.26ms
step:446/1393 train_time:53742ms step_avg:123.26ms
step:447/1393 train_time:53867ms step_avg:123.27ms
step:448/1393 train_time:53993ms step_avg:123.27ms
step:449/1393 train_time:54119ms step_avg:123.28ms
step:450/1393 train_time:54245ms step_avg:123.28ms
step:451/1393 train_time:54371ms step_avg:123.29ms
step:452/1393 train_time:54496ms step_avg:123.29ms
step:453/1393 train_time:54621ms step_avg:123.30ms
step:454/1393 train_time:54747ms step_avg:123.30ms
step:455/1393 train_time:54873ms step_avg:123.31ms
step:456/1393 train_time:54998ms step_avg:123.31ms
step:457/1393 train_time:55124ms step_avg:123.32ms
step:458/1393 train_time:55249ms step_avg:123.32ms
step:459/1393 train_time:55376ms step_avg:123.33ms
step:460/1393 train_time:55501ms step_avg:123.34ms
step:461/1393 train_time:55628ms step_avg:123.34ms
step:462/1393 train_time:55753ms step_avg:123.35ms
step:463/1393 train_time:55879ms step_avg:123.35ms
step:464/1393 train_time:56004ms step_avg:123.36ms
step:465/1393 train_time:56130ms step_avg:123.36ms
step:466/1393 train_time:56256ms step_avg:123.37ms
step:467/1393 train_time:56383ms step_avg:123.38ms
step:468/1393 train_time:56508ms step_avg:123.38ms
step:469/1393 train_time:56634ms step_avg:123.39ms
step:470/1393 train_time:56760ms step_avg:123.39ms
step:471/1393 train_time:56886ms step_avg:123.40ms
step:472/1393 train_time:57011ms step_avg:123.40ms
step:473/1393 train_time:57137ms step_avg:123.41ms
step:474/1393 train_time:57263ms step_avg:123.41ms
step:475/1393 train_time:57388ms step_avg:123.42ms
step:476/1393 train_time:57515ms step_avg:123.42ms
step:477/1393 train_time:57640ms step_avg:123.43ms
step:478/1393 train_time:57766ms step_avg:123.43ms
step:479/1393 train_time:57891ms step_avg:123.43ms
step:480/1393 train_time:58017ms step_avg:123.44ms
step:481/1393 train_time:58143ms step_avg:123.45ms
step:482/1393 train_time:58268ms step_avg:123.45ms
step:483/1393 train_time:58394ms step_avg:123.45ms
step:484/1393 train_time:58519ms step_avg:123.46ms
step:485/1393 train_time:58645ms step_avg:123.46ms
step:486/1393 train_time:58770ms step_avg:123.47ms
step:487/1393 train_time:58896ms step_avg:123.47ms
step:488/1393 train_time:59022ms step_avg:123.48ms
step:489/1393 train_time:59147ms step_avg:123.48ms
step:490/1393 train_time:59272ms step_avg:123.48ms
step:491/1393 train_time:59398ms step_avg:123.49ms
step:492/1393 train_time:59523ms step_avg:123.49ms
step:493/1393 train_time:59648ms step_avg:123.50ms
step:494/1393 train_time:59774ms step_avg:123.50ms
step:495/1393 train_time:59900ms step_avg:123.50ms
step:496/1393 train_time:60029ms step_avg:123.52ms
step:497/1393 train_time:60151ms step_avg:123.51ms
step:498/1393 train_time:60277ms step_avg:123.52ms
step:499/1393 train_time:60403ms step_avg:123.52ms
step:500/1393 train_time:60529ms step_avg:123.53ms
step:500/1393 val_loss:3.6687 train_time:60652ms step_avg:123.78ms
step:501/1393 train_time:60673ms step_avg:123.57ms
step:502/1393 train_time:60794ms step_avg:123.57ms
step:503/1393 train_time:60921ms step_avg:123.57ms
step:504/1393 train_time:61047ms step_avg:123.58ms
step:505/1393 train_time:61172ms step_avg:123.58ms
step:506/1393 train_time:61296ms step_avg:123.58ms
step:507/1393 train_time:61421ms step_avg:123.58ms
step:508/1393 train_time:61546ms step_avg:123.59ms
step:509/1393 train_time:61672ms step_avg:123.59ms
step:510/1393 train_time:61799ms step_avg:123.60ms
step:511/1393 train_time:61926ms step_avg:123.61ms
step:512/1393 train_time:62052ms step_avg:123.61ms
step:513/1393 train_time:62177ms step_avg:123.61ms
step:514/1393 train_time:62302ms step_avg:123.62ms
step:515/1393 train_time:62427ms step_avg:123.62ms
step:516/1393 train_time:62552ms step_avg:123.62ms
step:517/1393 train_time:62678ms step_avg:123.63ms
step:518/1393 train_time:62806ms step_avg:123.63ms
step:519/1393 train_time:62934ms step_avg:123.64ms
step:520/1393 train_time:63062ms step_avg:123.65ms
step:521/1393 train_time:63190ms step_avg:123.66ms
step:522/1393 train_time:63317ms step_avg:123.67ms
step:523/1393 train_time:63444ms step_avg:123.67ms
step:524/1393 train_time:63572ms step_avg:123.68ms
step:525/1393 train_time:63699ms step_avg:123.69ms
step:526/1393 train_time:63827ms step_avg:123.70ms
step:527/1393 train_time:63955ms step_avg:123.70ms
step:528/1393 train_time:64082ms step_avg:123.71ms
step:529/1393 train_time:64210ms step_avg:123.72ms
step:530/1393 train_time:64338ms step_avg:123.73ms
step:531/1393 train_time:64465ms step_avg:123.73ms
step:532/1393 train_time:64594ms step_avg:123.74ms
step:533/1393 train_time:64721ms step_avg:123.75ms
step:534/1393 train_time:64849ms step_avg:123.76ms
step:535/1393 train_time:64978ms step_avg:123.77ms
step:536/1393 train_time:65106ms step_avg:123.78ms
step:537/1393 train_time:65234ms step_avg:123.78ms
step:538/1393 train_time:65362ms step_avg:123.79ms
step:539/1393 train_time:65489ms step_avg:123.80ms
step:540/1393 train_time:65617ms step_avg:123.81ms
step:541/1393 train_time:65746ms step_avg:123.81ms
step:542/1393 train_time:65874ms step_avg:123.82ms
step:543/1393 train_time:66002ms step_avg:123.83ms
step:544/1393 train_time:66130ms step_avg:123.84ms
step:545/1393 train_time:66258ms step_avg:123.85ms
step:546/1393 train_time:66385ms step_avg:123.85ms
step:547/1393 train_time:66512ms step_avg:123.86ms
step:548/1393 train_time:66639ms step_avg:123.87ms
step:549/1393 train_time:66767ms step_avg:123.87ms
step:550/1393 train_time:66894ms step_avg:123.88ms
step:551/1393 train_time:67023ms step_avg:123.89ms
step:552/1393 train_time:67150ms step_avg:123.89ms
step:553/1393 train_time:67278ms step_avg:123.90ms
step:554/1393 train_time:67406ms step_avg:123.91ms
step:555/1393 train_time:67533ms step_avg:123.91ms
step:556/1393 train_time:67660ms step_avg:123.92ms
step:557/1393 train_time:67788ms step_avg:123.93ms
step:558/1393 train_time:67916ms step_avg:123.93ms
step:559/1393 train_time:68044ms step_avg:123.94ms
step:560/1393 train_time:68172ms step_avg:123.95ms
step:561/1393 train_time:68299ms step_avg:123.95ms
step:562/1393 train_time:68427ms step_avg:123.96ms
step:563/1393 train_time:68555ms step_avg:123.97ms
step:564/1393 train_time:68682ms step_avg:123.97ms
step:565/1393 train_time:68809ms step_avg:123.98ms
step:566/1393 train_time:68936ms step_avg:123.99ms
step:567/1393 train_time:69064ms step_avg:123.99ms
step:568/1393 train_time:69192ms step_avg:124.00ms
step:569/1393 train_time:69320ms step_avg:124.01ms
step:570/1393 train_time:69449ms step_avg:124.02ms
step:571/1393 train_time:69577ms step_avg:124.02ms
step:572/1393 train_time:69705ms step_avg:124.03ms
step:573/1393 train_time:69833ms step_avg:124.04ms
step:574/1393 train_time:69962ms step_avg:124.05ms
step:575/1393 train_time:70090ms step_avg:124.05ms
step:576/1393 train_time:70218ms step_avg:124.06ms
step:577/1393 train_time:70346ms step_avg:124.07ms
step:578/1393 train_time:70474ms step_avg:124.07ms
step:579/1393 train_time:70601ms step_avg:124.08ms
step:580/1393 train_time:70729ms step_avg:124.09ms
step:581/1393 train_time:70856ms step_avg:124.09ms
step:582/1393 train_time:70984ms step_avg:124.10ms
step:583/1393 train_time:71112ms step_avg:124.10ms
step:584/1393 train_time:71239ms step_avg:124.11ms
step:585/1393 train_time:71367ms step_avg:124.12ms
step:586/1393 train_time:71496ms step_avg:124.12ms
step:587/1393 train_time:71624ms step_avg:124.13ms
step:588/1393 train_time:71751ms step_avg:124.14ms
step:589/1393 train_time:71879ms step_avg:124.14ms
step:590/1393 train_time:72007ms step_avg:124.15ms
step:591/1393 train_time:72135ms step_avg:124.16ms
step:592/1393 train_time:72263ms step_avg:124.16ms
step:593/1393 train_time:72390ms step_avg:124.17ms
step:594/1393 train_time:72518ms step_avg:124.17ms
step:595/1393 train_time:72646ms step_avg:124.18ms
step:596/1393 train_time:72774ms step_avg:124.19ms
step:597/1393 train_time:72902ms step_avg:124.19ms
step:598/1393 train_time:73030ms step_avg:124.20ms
step:599/1393 train_time:73157ms step_avg:124.21ms
step:600/1393 train_time:73284ms step_avg:124.21ms
step:601/1393 train_time:73413ms step_avg:124.22ms
step:602/1393 train_time:73540ms step_avg:124.22ms
step:603/1393 train_time:73668ms step_avg:124.23ms
step:604/1393 train_time:73796ms step_avg:124.24ms
step:605/1393 train_time:73924ms step_avg:124.24ms
step:606/1393 train_time:74052ms step_avg:124.25ms
step:607/1393 train_time:74180ms step_avg:124.26ms
step:608/1393 train_time:74308ms step_avg:124.26ms
step:609/1393 train_time:74435ms step_avg:124.27ms
step:610/1393 train_time:74562ms step_avg:124.27ms
step:611/1393 train_time:74690ms step_avg:124.28ms
step:612/1393 train_time:74818ms step_avg:124.28ms
step:613/1393 train_time:74945ms step_avg:124.29ms
step:614/1393 train_time:75074ms step_avg:124.29ms
step:615/1393 train_time:75201ms step_avg:124.30ms
step:616/1393 train_time:75329ms step_avg:124.31ms
step:617/1393 train_time:75456ms step_avg:124.31ms
step:618/1393 train_time:75583ms step_avg:124.31ms
step:619/1393 train_time:75711ms step_avg:124.32ms
step:620/1393 train_time:75839ms step_avg:124.33ms
step:621/1393 train_time:75967ms step_avg:124.33ms
step:622/1393 train_time:76095ms step_avg:124.34ms
step:623/1393 train_time:76222ms step_avg:124.34ms
step:624/1393 train_time:76350ms step_avg:124.35ms
step:625/1393 train_time:76478ms step_avg:124.35ms
step:625/1393 val_loss:3.5829 train_time:76604ms step_avg:124.56ms
step:626/1393 train_time:76625ms step_avg:124.39ms
step:627/1393 train_time:76746ms step_avg:124.39ms
step:628/1393 train_time:76875ms step_avg:124.39ms
step:629/1393 train_time:77003ms step_avg:124.40ms
step:630/1393 train_time:77129ms step_avg:124.40ms
step:631/1393 train_time:77256ms step_avg:124.41ms
step:632/1393 train_time:77383ms step_avg:124.41ms
step:633/1393 train_time:77510ms step_avg:124.41ms
step:634/1393 train_time:77640ms step_avg:124.42ms
step:635/1393 train_time:77770ms step_avg:124.43ms
step:636/1393 train_time:77900ms step_avg:124.44ms
step:637/1393 train_time:78028ms step_avg:124.45ms
step:638/1393 train_time:78156ms step_avg:124.45ms
step:639/1393 train_time:78284ms step_avg:124.46ms
step:640/1393 train_time:78411ms step_avg:124.46ms
step:641/1393 train_time:78539ms step_avg:124.47ms
step:642/1393 train_time:78668ms step_avg:124.47ms
step:643/1393 train_time:78796ms step_avg:124.48ms
step:644/1393 train_time:78924ms step_avg:124.49ms
step:645/1393 train_time:79053ms step_avg:124.49ms
step:646/1393 train_time:79181ms step_avg:124.50ms
step:647/1393 train_time:79309ms step_avg:124.50ms
step:648/1393 train_time:79437ms step_avg:124.51ms
step:649/1393 train_time:79565ms step_avg:124.51ms
step:650/1393 train_time:79693ms step_avg:124.52ms
step:651/1393 train_time:79822ms step_avg:124.53ms
step:652/1393 train_time:79950ms step_avg:124.53ms
step:653/1393 train_time:80079ms step_avg:124.54ms
step:654/1393 train_time:80207ms step_avg:124.54ms
step:655/1393 train_time:80335ms step_avg:124.55ms
step:656/1393 train_time:80463ms step_avg:124.56ms
step:657/1393 train_time:80591ms step_avg:124.56ms
step:658/1393 train_time:80718ms step_avg:124.57ms
step:659/1393 train_time:80846ms step_avg:124.57ms
step:660/1393 train_time:80974ms step_avg:124.58ms
step:661/1393 train_time:81102ms step_avg:124.58ms
step:662/1393 train_time:81230ms step_avg:124.59ms
step:663/1393 train_time:81358ms step_avg:124.59ms
step:664/1393 train_time:81486ms step_avg:124.60ms
step:665/1393 train_time:81614ms step_avg:124.60ms
step:666/1393 train_time:81743ms step_avg:124.61ms
step:667/1393 train_time:81871ms step_avg:124.61ms
step:668/1393 train_time:81999ms step_avg:124.62ms
step:669/1393 train_time:82127ms step_avg:124.62ms
step:670/1393 train_time:82256ms step_avg:124.63ms
step:671/1393 train_time:82383ms step_avg:124.63ms
step:672/1393 train_time:82511ms step_avg:124.64ms
step:673/1393 train_time:82640ms step_avg:124.65ms
step:674/1393 train_time:82768ms step_avg:124.65ms
step:675/1393 train_time:82896ms step_avg:124.66ms
step:676/1393 train_time:83024ms step_avg:124.66ms
step:677/1393 train_time:83152ms step_avg:124.67ms
step:678/1393 train_time:83280ms step_avg:124.67ms
step:679/1393 train_time:83409ms step_avg:124.68ms
step:680/1393 train_time:83538ms step_avg:124.68ms
step:681/1393 train_time:83666ms step_avg:124.69ms
step:682/1393 train_time:83793ms step_avg:124.69ms
step:683/1393 train_time:83922ms step_avg:124.70ms
step:684/1393 train_time:84050ms step_avg:124.70ms
step:685/1393 train_time:84180ms step_avg:124.71ms
step:686/1393 train_time:84308ms step_avg:124.72ms
step:687/1393 train_time:84436ms step_avg:124.72ms
step:688/1393 train_time:84564ms step_avg:124.73ms
step:689/1393 train_time:84692ms step_avg:124.73ms
step:690/1393 train_time:84820ms step_avg:124.74ms
step:691/1393 train_time:84948ms step_avg:124.74ms
step:692/1393 train_time:85076ms step_avg:124.74ms
step:693/1393 train_time:85203ms step_avg:124.75ms
step:694/1393 train_time:85331ms step_avg:124.75ms
step:695/1393 train_time:85460ms step_avg:124.76ms
step:696/1393 train_time:85591ms step_avg:124.77ms
step:697/1393 train_time:85716ms step_avg:124.77ms
step:698/1393 train_time:85844ms step_avg:124.77ms
step:699/1393 train_time:85973ms step_avg:124.78ms
step:700/1393 train_time:86101ms step_avg:124.78ms
step:701/1393 train_time:86229ms step_avg:124.79ms
step:702/1393 train_time:86357ms step_avg:124.79ms
step:703/1393 train_time:86485ms step_avg:124.80ms
step:704/1393 train_time:86613ms step_avg:124.80ms
step:705/1393 train_time:86741ms step_avg:124.81ms
step:706/1393 train_time:86870ms step_avg:124.81ms
step:707/1393 train_time:86998ms step_avg:124.82ms
step:708/1393 train_time:87126ms step_avg:124.82ms
step:709/1393 train_time:87255ms step_avg:124.83ms
step:710/1393 train_time:87383ms step_avg:124.83ms
step:711/1393 train_time:87512ms step_avg:124.84ms
step:712/1393 train_time:87640ms step_avg:124.84ms
step:713/1393 train_time:87767ms step_avg:124.85ms
step:714/1393 train_time:87896ms step_avg:124.85ms
step:715/1393 train_time:88024ms step_avg:124.86ms
step:716/1393 train_time:88152ms step_avg:124.86ms
step:717/1393 train_time:88280ms step_avg:124.87ms
step:718/1393 train_time:88409ms step_avg:124.87ms
step:719/1393 train_time:88537ms step_avg:124.88ms
step:720/1393 train_time:88665ms step_avg:124.88ms
step:721/1393 train_time:88793ms step_avg:124.88ms
step:722/1393 train_time:88921ms step_avg:124.89ms
step:723/1393 train_time:89048ms step_avg:124.89ms
step:724/1393 train_time:89177ms step_avg:124.90ms
step:725/1393 train_time:89307ms step_avg:124.91ms
step:726/1393 train_time:89438ms step_avg:124.91ms
step:727/1393 train_time:89568ms step_avg:124.92ms
step:728/1393 train_time:89697ms step_avg:124.93ms
step:729/1393 train_time:89827ms step_avg:124.93ms
step:730/1393 train_time:89958ms step_avg:124.94ms
step:731/1393 train_time:90086ms step_avg:124.95ms
step:732/1393 train_time:90216ms step_avg:124.95ms
step:733/1393 train_time:90346ms step_avg:124.96ms
step:734/1393 train_time:90475ms step_avg:124.97ms
step:735/1393 train_time:90606ms step_avg:124.97ms
step:736/1393 train_time:90736ms step_avg:124.98ms
step:737/1393 train_time:90867ms step_avg:124.99ms
step:738/1393 train_time:90997ms step_avg:125.00ms
step:739/1393 train_time:91127ms step_avg:125.00ms
step:740/1393 train_time:91257ms step_avg:125.01ms
step:741/1393 train_time:91389ms step_avg:125.02ms
step:742/1393 train_time:91519ms step_avg:125.03ms
step:743/1393 train_time:91648ms step_avg:125.03ms
step:744/1393 train_time:91777ms step_avg:125.04ms
step:745/1393 train_time:91908ms step_avg:125.04ms
step:746/1393 train_time:92038ms step_avg:125.05ms
step:747/1393 train_time:92168ms step_avg:125.06ms
step:748/1393 train_time:92298ms step_avg:125.06ms
step:749/1393 train_time:92429ms step_avg:125.07ms
step:750/1393 train_time:92559ms step_avg:125.08ms
step:750/1393 val_loss:3.5284 train_time:92688ms step_avg:125.25ms
step:751/1393 train_time:92709ms step_avg:125.11ms
step:752/1393 train_time:92830ms step_avg:125.11ms
step:753/1393 train_time:92960ms step_avg:125.11ms
step:754/1393 train_time:93090ms step_avg:125.12ms
step:755/1393 train_time:93219ms step_avg:125.13ms
step:756/1393 train_time:93348ms step_avg:125.13ms
step:757/1393 train_time:93479ms step_avg:125.14ms
step:758/1393 train_time:93609ms step_avg:125.15ms
step:759/1393 train_time:93739ms step_avg:125.15ms
step:760/1393 train_time:93870ms step_avg:125.16ms
step:761/1393 train_time:94001ms step_avg:125.17ms
step:762/1393 train_time:94131ms step_avg:125.17ms
step:763/1393 train_time:94260ms step_avg:125.18ms
step:764/1393 train_time:94390ms step_avg:125.19ms
step:765/1393 train_time:94520ms step_avg:125.19ms
step:766/1393 train_time:94651ms step_avg:125.20ms
step:767/1393 train_time:94782ms step_avg:125.21ms
step:768/1393 train_time:94913ms step_avg:125.22ms
step:769/1393 train_time:95046ms step_avg:125.22ms
step:770/1393 train_time:95174ms step_avg:125.23ms
step:771/1393 train_time:95304ms step_avg:125.23ms
step:772/1393 train_time:95434ms step_avg:125.24ms
step:773/1393 train_time:95564ms step_avg:125.25ms
step:774/1393 train_time:95694ms step_avg:125.25ms
step:775/1393 train_time:95823ms step_avg:125.26ms
step:776/1393 train_time:95953ms step_avg:125.26ms
step:777/1393 train_time:96084ms step_avg:125.27ms
step:778/1393 train_time:96214ms step_avg:125.28ms
step:779/1393 train_time:96343ms step_avg:125.28ms
step:780/1393 train_time:96474ms step_avg:125.29ms
step:781/1393 train_time:96604ms step_avg:125.30ms
step:782/1393 train_time:96734ms step_avg:125.30ms
step:783/1393 train_time:96864ms step_avg:125.31ms
step:784/1393 train_time:96994ms step_avg:125.32ms
step:785/1393 train_time:97124ms step_avg:125.32ms
step:786/1393 train_time:97255ms step_avg:125.33ms
step:787/1393 train_time:97385ms step_avg:125.33ms
step:788/1393 train_time:97515ms step_avg:125.34ms
step:789/1393 train_time:97645ms step_avg:125.35ms
step:790/1393 train_time:97775ms step_avg:125.35ms
step:791/1393 train_time:97904ms step_avg:125.36ms
step:792/1393 train_time:98035ms step_avg:125.36ms
step:793/1393 train_time:98165ms step_avg:125.37ms
step:794/1393 train_time:98295ms step_avg:125.38ms
step:795/1393 train_time:98425ms step_avg:125.38ms
step:796/1393 train_time:98556ms step_avg:125.39ms
step:797/1393 train_time:98685ms step_avg:125.39ms
step:798/1393 train_time:98815ms step_avg:125.40ms
step:799/1393 train_time:98952ms step_avg:125.41ms
step:800/1393 train_time:99077ms step_avg:125.41ms
step:801/1393 train_time:99207ms step_avg:125.42ms
step:802/1393 train_time:99338ms step_avg:125.43ms
step:803/1393 train_time:99467ms step_avg:125.43ms
step:804/1393 train_time:99598ms step_avg:125.44ms
step:805/1393 train_time:99729ms step_avg:125.44ms
step:806/1393 train_time:99858ms step_avg:125.45ms
step:807/1393 train_time:99987ms step_avg:125.45ms
step:808/1393 train_time:100118ms step_avg:125.46ms
step:809/1393 train_time:100247ms step_avg:125.47ms
step:810/1393 train_time:100378ms step_avg:125.47ms
step:811/1393 train_time:100508ms step_avg:125.48ms
step:812/1393 train_time:100638ms step_avg:125.48ms
step:813/1393 train_time:100768ms step_avg:125.49ms
step:814/1393 train_time:100898ms step_avg:125.50ms
step:815/1393 train_time:101028ms step_avg:125.50ms
step:816/1393 train_time:101159ms step_avg:125.51ms
step:817/1393 train_time:101289ms step_avg:125.51ms
step:818/1393 train_time:101419ms step_avg:125.52ms
step:819/1393 train_time:101548ms step_avg:125.52ms
step:820/1393 train_time:101679ms step_avg:125.53ms
step:821/1393 train_time:101808ms step_avg:125.53ms
step:822/1393 train_time:101938ms step_avg:125.54ms
step:823/1393 train_time:102068ms step_avg:125.54ms
step:824/1393 train_time:102197ms step_avg:125.55ms
step:825/1393 train_time:102328ms step_avg:125.56ms
step:826/1393 train_time:102459ms step_avg:125.56ms
step:827/1393 train_time:102589ms step_avg:125.57ms
step:828/1393 train_time:102719ms step_avg:125.57ms
step:829/1393 train_time:102849ms step_avg:125.58ms
step:830/1393 train_time:102980ms step_avg:125.59ms
step:831/1393 train_time:103111ms step_avg:125.59ms
step:832/1393 train_time:103241ms step_avg:125.60ms
step:833/1393 train_time:103371ms step_avg:125.60ms
step:834/1393 train_time:103502ms step_avg:125.61ms
step:835/1393 train_time:103632ms step_avg:125.62ms
step:836/1393 train_time:103763ms step_avg:125.62ms
step:837/1393 train_time:103893ms step_avg:125.63ms
step:838/1393 train_time:104024ms step_avg:125.63ms
step:839/1393 train_time:104153ms step_avg:125.64ms
step:840/1393 train_time:104285ms step_avg:125.64ms
step:841/1393 train_time:104416ms step_avg:125.65ms
step:842/1393 train_time:104546ms step_avg:125.66ms
step:843/1393 train_time:104676ms step_avg:125.66ms
step:844/1393 train_time:104806ms step_avg:125.67ms
step:845/1393 train_time:104936ms step_avg:125.67ms
step:846/1393 train_time:105067ms step_avg:125.68ms
step:847/1393 train_time:105198ms step_avg:125.68ms
step:848/1393 train_time:105328ms step_avg:125.69ms
step:849/1393 train_time:105458ms step_avg:125.70ms
step:850/1393 train_time:105589ms step_avg:125.70ms
step:851/1393 train_time:105720ms step_avg:125.71ms
step:852/1393 train_time:105850ms step_avg:125.71ms
step:853/1393 train_time:105980ms step_avg:125.72ms
step:854/1393 train_time:106110ms step_avg:125.72ms
step:855/1393 train_time:106240ms step_avg:125.73ms
step:856/1393 train_time:106370ms step_avg:125.73ms
step:857/1393 train_time:106500ms step_avg:125.74ms
step:858/1393 train_time:106631ms step_avg:125.74ms
step:859/1393 train_time:106762ms step_avg:125.75ms
step:860/1393 train_time:106893ms step_avg:125.76ms
step:861/1393 train_time:107023ms step_avg:125.76ms
step:862/1393 train_time:107155ms step_avg:125.77ms
step:863/1393 train_time:107285ms step_avg:125.77ms
step:864/1393 train_time:107415ms step_avg:125.78ms
step:865/1393 train_time:107547ms step_avg:125.79ms
step:866/1393 train_time:107678ms step_avg:125.79ms
step:867/1393 train_time:107808ms step_avg:125.80ms
step:868/1393 train_time:107938ms step_avg:125.80ms
step:869/1393 train_time:108068ms step_avg:125.81ms
step:870/1393 train_time:108199ms step_avg:125.81ms
step:871/1393 train_time:108330ms step_avg:125.82ms
step:872/1393 train_time:108460ms step_avg:125.82ms
step:873/1393 train_time:108591ms step_avg:125.83ms
step:874/1393 train_time:108721ms step_avg:125.83ms
step:875/1393 train_time:108851ms step_avg:125.84ms
step:875/1393 val_loss:3.4794 train_time:108980ms step_avg:125.99ms
step:876/1393 train_time:109007ms step_avg:125.87ms
step:877/1393 train_time:109122ms step_avg:125.86ms
step:878/1393 train_time:109252ms step_avg:125.87ms
step:879/1393 train_time:109382ms step_avg:125.87ms
step:880/1393 train_time:109511ms step_avg:125.87ms
step:881/1393 train_time:109640ms step_avg:125.88ms
step:882/1393 train_time:109770ms step_avg:125.88ms
step:883/1393 train_time:109899ms step_avg:125.89ms
step:884/1393 train_time:110031ms step_avg:125.89ms
step:885/1393 train_time:110164ms step_avg:125.90ms
step:886/1393 train_time:110295ms step_avg:125.91ms
step:887/1393 train_time:110424ms step_avg:125.91ms
step:888/1393 train_time:110555ms step_avg:125.92ms
step:889/1393 train_time:110686ms step_avg:125.92ms
step:890/1393 train_time:110815ms step_avg:125.93ms
step:891/1393 train_time:110945ms step_avg:125.93ms
step:892/1393 train_time:111076ms step_avg:125.94ms
step:893/1393 train_time:111207ms step_avg:125.94ms
step:894/1393 train_time:111337ms step_avg:125.95ms
step:895/1393 train_time:111468ms step_avg:125.95ms
step:896/1393 train_time:111597ms step_avg:125.96ms
step:897/1393 train_time:111727ms step_avg:125.96ms
step:898/1393 train_time:111857ms step_avg:125.97ms
step:899/1393 train_time:111988ms step_avg:125.97ms
step:900/1393 train_time:112118ms step_avg:125.98ms
step:901/1393 train_time:112248ms step_avg:125.98ms
step:902/1393 train_time:112378ms step_avg:125.98ms
step:903/1393 train_time:112508ms step_avg:125.99ms
step:904/1393 train_time:112638ms step_avg:125.99ms
step:905/1393 train_time:112768ms step_avg:126.00ms
step:906/1393 train_time:112898ms step_avg:126.00ms
step:907/1393 train_time:113030ms step_avg:126.01ms
step:908/1393 train_time:113160ms step_avg:126.01ms
step:909/1393 train_time:113291ms step_avg:126.02ms
step:910/1393 train_time:113425ms step_avg:126.03ms
step:911/1393 train_time:113554ms step_avg:126.03ms
step:912/1393 train_time:113684ms step_avg:126.04ms
step:913/1393 train_time:113815ms step_avg:126.04ms
step:914/1393 train_time:113945ms step_avg:126.05ms
step:915/1393 train_time:114076ms step_avg:126.05ms
step:916/1393 train_time:114206ms step_avg:126.06ms
step:917/1393 train_time:114338ms step_avg:126.06ms
step:918/1393 train_time:114468ms step_avg:126.07ms
step:919/1393 train_time:114601ms step_avg:126.07ms
step:920/1393 train_time:114731ms step_avg:126.08ms
step:921/1393 train_time:114861ms step_avg:126.08ms
step:922/1393 train_time:114992ms step_avg:126.09ms
step:923/1393 train_time:115121ms step_avg:126.09ms
step:924/1393 train_time:115251ms step_avg:126.10ms
step:925/1393 train_time:115384ms step_avg:126.10ms
step:926/1393 train_time:115515ms step_avg:126.11ms
step:927/1393 train_time:115646ms step_avg:126.11ms
step:928/1393 train_time:115775ms step_avg:126.12ms
step:929/1393 train_time:115906ms step_avg:126.12ms
step:930/1393 train_time:116036ms step_avg:126.13ms
step:931/1393 train_time:116168ms step_avg:126.13ms
step:932/1393 train_time:116300ms step_avg:126.14ms
step:933/1393 train_time:116433ms step_avg:126.15ms
step:934/1393 train_time:116565ms step_avg:126.15ms
step:935/1393 train_time:116697ms step_avg:126.16ms
step:936/1393 train_time:116829ms step_avg:126.16ms
step:937/1393 train_time:116963ms step_avg:126.17ms
step:938/1393 train_time:117096ms step_avg:126.18ms
step:939/1393 train_time:117228ms step_avg:126.19ms
step:940/1393 train_time:117362ms step_avg:126.20ms
step:941/1393 train_time:117493ms step_avg:126.20ms
step:942/1393 train_time:117626ms step_avg:126.21ms
step:943/1393 train_time:117760ms step_avg:126.22ms
step:944/1393 train_time:117893ms step_avg:126.22ms
step:945/1393 train_time:118026ms step_avg:126.23ms
step:946/1393 train_time:118159ms step_avg:126.24ms
step:947/1393 train_time:118293ms step_avg:126.25ms
step:948/1393 train_time:118426ms step_avg:126.25ms
step:949/1393 train_time:118558ms step_avg:126.26ms
step:950/1393 train_time:118690ms step_avg:126.27ms
step:951/1393 train_time:118824ms step_avg:126.27ms
step:952/1393 train_time:118955ms step_avg:126.28ms
step:953/1393 train_time:119087ms step_avg:126.28ms
step:954/1393 train_time:119219ms step_avg:126.29ms
step:955/1393 train_time:119350ms step_avg:126.30ms
step:956/1393 train_time:119483ms step_avg:126.30ms
step:957/1393 train_time:119616ms step_avg:126.31ms
step:958/1393 train_time:119749ms step_avg:126.32ms
step:959/1393 train_time:119881ms step_avg:126.32ms
step:960/1393 train_time:120014ms step_avg:126.33ms
step:961/1393 train_time:120146ms step_avg:126.34ms
step:962/1393 train_time:120279ms step_avg:126.34ms
step:963/1393 train_time:120412ms step_avg:126.35ms
step:964/1393 train_time:120543ms step_avg:126.36ms
step:965/1393 train_time:120677ms step_avg:126.36ms
step:966/1393 train_time:120809ms step_avg:126.37ms
step:967/1393 train_time:120941ms step_avg:126.38ms
step:968/1393 train_time:121073ms step_avg:126.38ms
step:969/1393 train_time:121205ms step_avg:126.39ms
step:970/1393 train_time:121336ms step_avg:126.39ms
step:971/1393 train_time:121468ms step_avg:126.40ms
step:972/1393 train_time:121601ms step_avg:126.40ms
step:973/1393 train_time:121734ms step_avg:126.41ms
step:974/1393 train_time:121866ms step_avg:126.42ms
step:975/1393 train_time:121996ms step_avg:126.42ms
step:976/1393 train_time:122128ms step_avg:126.43ms
step:977/1393 train_time:122259ms step_avg:126.43ms
step:978/1393 train_time:122390ms step_avg:126.44ms
step:979/1393 train_time:122523ms step_avg:126.44ms
step:980/1393 train_time:122656ms step_avg:126.45ms
step:981/1393 train_time:122788ms step_avg:126.46ms
step:982/1393 train_time:122919ms step_avg:126.46ms
step:983/1393 train_time:123050ms step_avg:126.46ms
step:984/1393 train_time:123182ms step_avg:126.47ms
step:985/1393 train_time:123313ms step_avg:126.47ms
step:986/1393 train_time:123447ms step_avg:126.48ms
step:987/1393 train_time:123579ms step_avg:126.49ms
step:988/1393 train_time:123711ms step_avg:126.49ms
step:989/1393 train_time:123843ms step_avg:126.50ms
step:990/1393 train_time:123975ms step_avg:126.51ms
step:991/1393 train_time:124107ms step_avg:126.51ms
step:992/1393 train_time:124240ms step_avg:126.52ms
step:993/1393 train_time:124375ms step_avg:126.53ms
step:994/1393 train_time:124507ms step_avg:126.53ms
step:995/1393 train_time:124639ms step_avg:126.54ms
step:996/1393 train_time:124771ms step_avg:126.54ms
step:997/1393 train_time:124902ms step_avg:126.55ms
step:998/1393 train_time:125033ms step_avg:126.55ms
step:999/1393 train_time:125165ms step_avg:126.56ms
step:1000/1393 train_time:125296ms step_avg:126.56ms
step:1000/1393 val_loss:3.4153 train_time:125427ms step_avg:126.69ms
step:1001/1393 train_time:125448ms step_avg:126.59ms
step:1002/1393 train_time:125571ms step_avg:126.58ms
step:1003/1393 train_time:125705ms step_avg:126.59ms
step:1004/1393 train_time:125836ms step_avg:126.60ms
step:1005/1393 train_time:125968ms step_avg:126.60ms
step:1006/1393 train_time:126099ms step_avg:126.60ms
step:1007/1393 train_time:126230ms step_avg:126.61ms
step:1008/1393 train_time:126362ms step_avg:126.61ms
step:1009/1393 train_time:126495ms step_avg:126.62ms
step:1010/1393 train_time:126629ms step_avg:126.63ms
step:1011/1393 train_time:126763ms step_avg:126.64ms
step:1012/1393 train_time:126894ms step_avg:126.64ms
step:1013/1393 train_time:127026ms step_avg:126.65ms
step:1014/1393 train_time:127157ms step_avg:126.65ms
step:1015/1393 train_time:127287ms step_avg:126.65ms
step:1016/1393 train_time:127418ms step_avg:126.66ms
step:1017/1393 train_time:127551ms step_avg:126.66ms
step:1018/1393 train_time:127683ms step_avg:126.67ms
step:1019/1393 train_time:127817ms step_avg:126.68ms
step:1020/1393 train_time:127948ms step_avg:126.68ms
step:1021/1393 train_time:128080ms step_avg:126.69ms
step:1022/1393 train_time:128212ms step_avg:126.69ms
step:1023/1393 train_time:128345ms step_avg:126.70ms
step:1024/1393 train_time:128476ms step_avg:126.70ms
step:1025/1393 train_time:128608ms step_avg:126.71ms
step:1026/1393 train_time:128741ms step_avg:126.71ms
step:1027/1393 train_time:128873ms step_avg:126.72ms
step:1028/1393 train_time:129006ms step_avg:126.73ms
step:1029/1393 train_time:129139ms step_avg:126.73ms
step:1030/1393 train_time:129271ms step_avg:126.74ms
step:1031/1393 train_time:129403ms step_avg:126.74ms
step:1032/1393 train_time:129534ms step_avg:126.75ms
step:1033/1393 train_time:129665ms step_avg:126.75ms
step:1034/1393 train_time:129798ms step_avg:126.76ms
step:1035/1393 train_time:129931ms step_avg:126.76ms
step:1036/1393 train_time:130064ms step_avg:126.77ms
step:1037/1393 train_time:130196ms step_avg:126.77ms
step:1038/1393 train_time:130329ms step_avg:126.78ms
step:1039/1393 train_time:130460ms step_avg:126.78ms
step:1040/1393 train_time:130592ms step_avg:126.79ms
step:1041/1393 train_time:130726ms step_avg:126.80ms
step:1042/1393 train_time:130858ms step_avg:126.80ms
step:1043/1393 train_time:130991ms step_avg:126.81ms
step:1044/1393 train_time:131125ms step_avg:126.81ms
step:1045/1393 train_time:131258ms step_avg:126.82ms
step:1046/1393 train_time:131391ms step_avg:126.83ms
step:1047/1393 train_time:131524ms step_avg:126.83ms
step:1048/1393 train_time:131656ms step_avg:126.84ms
step:1049/1393 train_time:131788ms step_avg:126.84ms
step:1050/1393 train_time:131920ms step_avg:126.85ms
step:1051/1393 train_time:132053ms step_avg:126.85ms
step:1052/1393 train_time:132185ms step_avg:126.86ms
step:1053/1393 train_time:132318ms step_avg:126.86ms
step:1054/1393 train_time:132455ms step_avg:126.87ms
step:1055/1393 train_time:132582ms step_avg:126.87ms
step:1056/1393 train_time:132714ms step_avg:126.88ms
step:1057/1393 train_time:132845ms step_avg:126.88ms
step:1058/1393 train_time:132979ms step_avg:126.89ms
step:1059/1393 train_time:133111ms step_avg:126.89ms
step:1060/1393 train_time:133245ms step_avg:126.90ms
step:1061/1393 train_time:133376ms step_avg:126.90ms
step:1062/1393 train_time:133510ms step_avg:126.91ms
step:1063/1393 train_time:133642ms step_avg:126.92ms
step:1064/1393 train_time:133774ms step_avg:126.92ms
step:1065/1393 train_time:133906ms step_avg:126.92ms
step:1066/1393 train_time:134038ms step_avg:126.93ms
step:1067/1393 train_time:134171ms step_avg:126.94ms
step:1068/1393 train_time:134303ms step_avg:126.94ms
step:1069/1393 train_time:134436ms step_avg:126.95ms
step:1070/1393 train_time:134568ms step_avg:126.95ms
step:1071/1393 train_time:134704ms step_avg:126.96ms
step:1072/1393 train_time:134836ms step_avg:126.96ms
step:1073/1393 train_time:134967ms step_avg:126.97ms
step:1074/1393 train_time:135099ms step_avg:126.97ms
step:1075/1393 train_time:135231ms step_avg:126.98ms
step:1076/1393 train_time:135363ms step_avg:126.98ms
step:1077/1393 train_time:135495ms step_avg:126.99ms
step:1078/1393 train_time:135627ms step_avg:126.99ms
step:1079/1393 train_time:135764ms step_avg:127.00ms
step:1080/1393 train_time:135895ms step_avg:127.01ms
step:1081/1393 train_time:136027ms step_avg:127.01ms
step:1082/1393 train_time:136159ms step_avg:127.01ms
step:1083/1393 train_time:136291ms step_avg:127.02ms
step:1084/1393 train_time:136423ms step_avg:127.02ms
step:1085/1393 train_time:136556ms step_avg:127.03ms
step:1086/1393 train_time:136689ms step_avg:127.03ms
step:1087/1393 train_time:136822ms step_avg:127.04ms
step:1088/1393 train_time:136953ms step_avg:127.04ms
step:1089/1393 train_time:137088ms step_avg:127.05ms
step:1090/1393 train_time:137220ms step_avg:127.06ms
step:1091/1393 train_time:137354ms step_avg:127.06ms
step:1092/1393 train_time:137486ms step_avg:127.07ms
step:1093/1393 train_time:137618ms step_avg:127.07ms
step:1094/1393 train_time:137749ms step_avg:127.08ms
step:1095/1393 train_time:137882ms step_avg:127.08ms
step:1096/1393 train_time:138013ms step_avg:127.08ms
step:1097/1393 train_time:138145ms step_avg:127.09ms
step:1098/1393 train_time:138276ms step_avg:127.09ms
step:1099/1393 train_time:138409ms step_avg:127.10ms
step:1100/1393 train_time:138541ms step_avg:127.10ms
step:1101/1393 train_time:138674ms step_avg:127.11ms
step:1102/1393 train_time:138806ms step_avg:127.11ms
step:1103/1393 train_time:138940ms step_avg:127.12ms
step:1104/1393 train_time:139071ms step_avg:127.12ms
step:1105/1393 train_time:139205ms step_avg:127.13ms
step:1106/1393 train_time:139337ms step_avg:127.13ms
step:1107/1393 train_time:139468ms step_avg:127.14ms
step:1108/1393 train_time:139603ms step_avg:127.14ms
step:1109/1393 train_time:139735ms step_avg:127.15ms
step:1110/1393 train_time:139869ms step_avg:127.15ms
step:1111/1393 train_time:140002ms step_avg:127.16ms
step:1112/1393 train_time:140134ms step_avg:127.16ms
step:1113/1393 train_time:140266ms step_avg:127.17ms
step:1114/1393 train_time:140399ms step_avg:127.17ms
step:1115/1393 train_time:140533ms step_avg:127.18ms
step:1116/1393 train_time:140664ms step_avg:127.18ms
step:1117/1393 train_time:140797ms step_avg:127.19ms
step:1118/1393 train_time:140932ms step_avg:127.19ms
step:1119/1393 train_time:141064ms step_avg:127.20ms
step:1120/1393 train_time:141195ms step_avg:127.20ms
step:1121/1393 train_time:141327ms step_avg:127.21ms
step:1122/1393 train_time:141459ms step_avg:127.21ms
step:1123/1393 train_time:141591ms step_avg:127.22ms
step:1124/1393 train_time:141723ms step_avg:127.22ms
step:1125/1393 train_time:141854ms step_avg:127.22ms
step:1125/1393 val_loss:3.3651 train_time:141985ms step_avg:127.34ms
step:1126/1393 train_time:142006ms step_avg:127.25ms
step:1127/1393 train_time:142129ms step_avg:127.24ms
step:1128/1393 train_time:142262ms step_avg:127.25ms
step:1129/1393 train_time:142394ms step_avg:127.25ms
step:1130/1393 train_time:142525ms step_avg:127.25ms
step:1131/1393 train_time:142657ms step_avg:127.26ms
step:1132/1393 train_time:142788ms step_avg:127.26ms
step:1133/1393 train_time:142919ms step_avg:127.27ms
step:1134/1393 train_time:143054ms step_avg:127.27ms
step:1135/1393 train_time:143187ms step_avg:127.28ms
step:1136/1393 train_time:143322ms step_avg:127.28ms
step:1137/1393 train_time:143454ms step_avg:127.29ms
step:1138/1393 train_time:143589ms step_avg:127.30ms
step:1139/1393 train_time:143722ms step_avg:127.30ms
step:1140/1393 train_time:143855ms step_avg:127.31ms
step:1141/1393 train_time:143989ms step_avg:127.31ms
step:1142/1393 train_time:144122ms step_avg:127.32ms
step:1143/1393 train_time:144258ms step_avg:127.32ms
step:1144/1393 train_time:144392ms step_avg:127.33ms
step:1145/1393 train_time:144525ms step_avg:127.34ms
step:1146/1393 train_time:144658ms step_avg:127.34ms
step:1147/1393 train_time:144792ms step_avg:127.35ms
step:1148/1393 train_time:144927ms step_avg:127.35ms
step:1149/1393 train_time:145059ms step_avg:127.36ms
step:1150/1393 train_time:145192ms step_avg:127.36ms
step:1151/1393 train_time:145328ms step_avg:127.37ms
step:1152/1393 train_time:145462ms step_avg:127.37ms
step:1153/1393 train_time:145598ms step_avg:127.38ms
step:1154/1393 train_time:145733ms step_avg:127.39ms
step:1155/1393 train_time:145865ms step_avg:127.39ms
step:1156/1393 train_time:146002ms step_avg:127.40ms
step:1157/1393 train_time:146136ms step_avg:127.41ms
step:1158/1393 train_time:146269ms step_avg:127.41ms
step:1159/1393 train_time:146403ms step_avg:127.42ms
step:1160/1393 train_time:146537ms step_avg:127.42ms
step:1161/1393 train_time:146672ms step_avg:127.43ms
step:1162/1393 train_time:146806ms step_avg:127.44ms
step:1163/1393 train_time:146938ms step_avg:127.44ms
step:1164/1393 train_time:147073ms step_avg:127.45ms
step:1165/1393 train_time:147206ms step_avg:127.45ms
step:1166/1393 train_time:147338ms step_avg:127.46ms
step:1167/1393 train_time:147471ms step_avg:127.46ms
step:1168/1393 train_time:147607ms step_avg:127.47ms
step:1169/1393 train_time:147742ms step_avg:127.47ms
step:1170/1393 train_time:147876ms step_avg:127.48ms
step:1171/1393 train_time:148010ms step_avg:127.48ms
step:1172/1393 train_time:148144ms step_avg:127.49ms
step:1173/1393 train_time:148279ms step_avg:127.50ms
step:1174/1393 train_time:148416ms step_avg:127.51ms
step:1175/1393 train_time:148550ms step_avg:127.51ms
step:1176/1393 train_time:148685ms step_avg:127.52ms
step:1177/1393 train_time:148821ms step_avg:127.52ms
step:1178/1393 train_time:148954ms step_avg:127.53ms
step:1179/1393 train_time:149087ms step_avg:127.53ms
step:1180/1393 train_time:149223ms step_avg:127.54ms
step:1181/1393 train_time:149358ms step_avg:127.55ms
step:1182/1393 train_time:149491ms step_avg:127.55ms
step:1183/1393 train_time:149625ms step_avg:127.56ms
step:1184/1393 train_time:149758ms step_avg:127.56ms
step:1185/1393 train_time:149892ms step_avg:127.57ms
step:1186/1393 train_time:150026ms step_avg:127.57ms
step:1187/1393 train_time:150165ms step_avg:127.58ms
step:1188/1393 train_time:150297ms step_avg:127.59ms
step:1189/1393 train_time:150433ms step_avg:127.59ms
step:1190/1393 train_time:150567ms step_avg:127.60ms
step:1191/1393 train_time:150700ms step_avg:127.60ms
step:1192/1393 train_time:150834ms step_avg:127.61ms
step:1193/1393 train_time:150966ms step_avg:127.61ms
step:1194/1393 train_time:151102ms step_avg:127.62ms
step:1195/1393 train_time:151234ms step_avg:127.62ms
step:1196/1393 train_time:151367ms step_avg:127.63ms
step:1197/1393 train_time:151502ms step_avg:127.63ms
step:1198/1393 train_time:151638ms step_avg:127.64ms
step:1199/1393 train_time:151771ms step_avg:127.65ms
step:1200/1393 train_time:151904ms step_avg:127.65ms
step:1201/1393 train_time:152036ms step_avg:127.65ms
step:1202/1393 train_time:152173ms step_avg:127.66ms
step:1203/1393 train_time:152311ms step_avg:127.67ms
step:1204/1393 train_time:152444ms step_avg:127.67ms
step:1205/1393 train_time:152578ms step_avg:127.68ms
step:1206/1393 train_time:152712ms step_avg:127.69ms
step:1207/1393 train_time:152846ms step_avg:127.69ms
step:1208/1393 train_time:152980ms step_avg:127.70ms
step:1209/1393 train_time:153116ms step_avg:127.70ms
step:1210/1393 train_time:153250ms step_avg:127.71ms
step:1211/1393 train_time:153385ms step_avg:127.71ms
step:1212/1393 train_time:153517ms step_avg:127.72ms
step:1213/1393 train_time:153651ms step_avg:127.72ms
step:1214/1393 train_time:153786ms step_avg:127.73ms
step:1215/1393 train_time:153922ms step_avg:127.74ms
step:1216/1393 train_time:154054ms step_avg:127.74ms
step:1217/1393 train_time:154189ms step_avg:127.75ms
step:1218/1393 train_time:154323ms step_avg:127.75ms
step:1219/1393 train_time:154455ms step_avg:127.75ms
step:1220/1393 train_time:154588ms step_avg:127.76ms
step:1221/1393 train_time:154721ms step_avg:127.76ms
step:1222/1393 train_time:154855ms step_avg:127.77ms
step:1223/1393 train_time:154987ms step_avg:127.77ms
step:1224/1393 train_time:155121ms step_avg:127.78ms
step:1225/1393 train_time:155258ms step_avg:127.78ms
step:1226/1393 train_time:155391ms step_avg:127.79ms
step:1227/1393 train_time:155524ms step_avg:127.79ms
step:1228/1393 train_time:155658ms step_avg:127.80ms
step:1229/1393 train_time:155790ms step_avg:127.80ms
step:1230/1393 train_time:155924ms step_avg:127.81ms
step:1231/1393 train_time:156059ms step_avg:127.81ms
step:1232/1393 train_time:156195ms step_avg:127.82ms
step:1233/1393 train_time:156329ms step_avg:127.82ms
step:1234/1393 train_time:156462ms step_avg:127.83ms
step:1235/1393 train_time:156596ms step_avg:127.83ms
step:1236/1393 train_time:156731ms step_avg:127.84ms
step:1237/1393 train_time:156864ms step_avg:127.84ms
step:1238/1393 train_time:157004ms step_avg:127.85ms
step:1239/1393 train_time:157133ms step_avg:127.85ms
step:1240/1393 train_time:157268ms step_avg:127.86ms
step:1241/1393 train_time:157404ms step_avg:127.87ms
step:1242/1393 train_time:157536ms step_avg:127.87ms
step:1243/1393 train_time:157670ms step_avg:127.87ms
step:1244/1393 train_time:157805ms step_avg:127.88ms
step:1245/1393 train_time:157938ms step_avg:127.88ms
step:1246/1393 train_time:158072ms step_avg:127.89ms
step:1247/1393 train_time:158206ms step_avg:127.89ms
step:1248/1393 train_time:158338ms step_avg:127.90ms
step:1249/1393 train_time:158471ms step_avg:127.90ms
step:1250/1393 train_time:158604ms step_avg:127.91ms
step:1250/1393 val_loss:3.3182 train_time:158737ms step_avg:128.01ms
step:1251/1393 train_time:158758ms step_avg:127.93ms
step:1252/1393 train_time:158882ms step_avg:127.92ms
step:1253/1393 train_time:159014ms step_avg:127.93ms
step:1254/1393 train_time:159146ms step_avg:127.93ms
step:1255/1393 train_time:159285ms step_avg:127.94ms
step:1256/1393 train_time:159417ms step_avg:127.94ms
step:1257/1393 train_time:159551ms step_avg:127.95ms
step:1258/1393 train_time:159685ms step_avg:127.95ms
step:1259/1393 train_time:159822ms step_avg:127.96ms
step:1260/1393 train_time:159956ms step_avg:127.96ms
step:1261/1393 train_time:160089ms step_avg:127.97ms
step:1262/1393 train_time:160226ms step_avg:127.98ms
step:1263/1393 train_time:160359ms step_avg:127.98ms
step:1264/1393 train_time:160492ms step_avg:127.98ms
step:1265/1393 train_time:160626ms step_avg:127.99ms
step:1266/1393 train_time:160760ms step_avg:127.99ms
step:1267/1393 train_time:160894ms step_avg:128.00ms
step:1268/1393 train_time:161029ms step_avg:128.00ms
step:1269/1393 train_time:161162ms step_avg:128.01ms
step:1270/1393 train_time:161297ms step_avg:128.01ms
step:1271/1393 train_time:161432ms step_avg:128.02ms
step:1272/1393 train_time:161564ms step_avg:128.02ms
step:1273/1393 train_time:161697ms step_avg:128.03ms
step:1274/1393 train_time:161831ms step_avg:128.03ms
step:1275/1393 train_time:161965ms step_avg:128.04ms
step:1276/1393 train_time:162099ms step_avg:128.04ms
step:1277/1393 train_time:162233ms step_avg:128.04ms
step:1278/1393 train_time:162366ms step_avg:128.05ms
step:1279/1393 train_time:162500ms step_avg:128.05ms
step:1280/1393 train_time:162636ms step_avg:128.06ms
step:1281/1393 train_time:162769ms step_avg:128.06ms
step:1282/1393 train_time:162902ms step_avg:128.07ms
step:1283/1393 train_time:163035ms step_avg:128.07ms
step:1284/1393 train_time:163169ms step_avg:128.08ms
step:1285/1393 train_time:163303ms step_avg:128.08ms
step:1286/1393 train_time:163437ms step_avg:128.09ms
step:1287/1393 train_time:163570ms step_avg:128.09ms
step:1288/1393 train_time:163704ms step_avg:128.09ms
step:1289/1393 train_time:163840ms step_avg:128.10ms
step:1290/1393 train_time:163977ms step_avg:128.11ms
step:1291/1393 train_time:164113ms step_avg:128.11ms
step:1292/1393 train_time:164247ms step_avg:128.12ms
step:1293/1393 train_time:164383ms step_avg:128.12ms
step:1294/1393 train_time:164517ms step_avg:128.13ms
step:1295/1393 train_time:164652ms step_avg:128.13ms
step:1296/1393 train_time:164786ms step_avg:128.14ms
step:1297/1393 train_time:164921ms step_avg:128.14ms
step:1298/1393 train_time:165054ms step_avg:128.15ms
step:1299/1393 train_time:165188ms step_avg:128.15ms
step:1300/1393 train_time:165321ms step_avg:128.16ms
step:1301/1393 train_time:165454ms step_avg:128.16ms
step:1302/1393 train_time:165588ms step_avg:128.16ms
step:1303/1393 train_time:165722ms step_avg:128.17ms
step:1304/1393 train_time:165858ms step_avg:128.17ms
step:1305/1393 train_time:165991ms step_avg:128.18ms
step:1306/1393 train_time:166124ms step_avg:128.18ms
step:1307/1393 train_time:166258ms step_avg:128.19ms
step:1308/1393 train_time:166393ms step_avg:128.19ms
step:1309/1393 train_time:166533ms step_avg:128.20ms
step:1310/1393 train_time:166662ms step_avg:128.20ms
step:1311/1393 train_time:166794ms step_avg:128.20ms
step:1312/1393 train_time:166927ms step_avg:128.21ms
step:1313/1393 train_time:167061ms step_avg:128.21ms
step:1314/1393 train_time:167194ms step_avg:128.22ms
step:1315/1393 train_time:167329ms step_avg:128.22ms
step:1316/1393 train_time:167462ms step_avg:128.23ms
step:1317/1393 train_time:167596ms step_avg:128.23ms
step:1318/1393 train_time:167731ms step_avg:128.23ms
step:1319/1393 train_time:167867ms step_avg:128.24ms
step:1320/1393 train_time:167999ms step_avg:128.24ms
step:1321/1393 train_time:168133ms step_avg:128.25ms
step:1322/1393 train_time:168269ms step_avg:128.25ms
step:1323/1393 train_time:168403ms step_avg:128.26ms
step:1324/1393 train_time:168536ms step_avg:128.26ms
step:1325/1393 train_time:168670ms step_avg:128.27ms
step:1326/1393 train_time:168805ms step_avg:128.27ms
step:1327/1393 train_time:168938ms step_avg:128.27ms
step:1328/1393 train_time:169070ms step_avg:128.28ms
step:1329/1393 train_time:169209ms step_avg:128.29ms
step:1330/1393 train_time:169344ms step_avg:128.29ms
step:1331/1393 train_time:169481ms step_avg:128.30ms
step:1332/1393 train_time:169617ms step_avg:128.30ms
step:1333/1393 train_time:169751ms step_avg:128.31ms
step:1334/1393 train_time:169885ms step_avg:128.31ms
step:1335/1393 train_time:170017ms step_avg:128.31ms
step:1336/1393 train_time:170155ms step_avg:128.32ms
step:1337/1393 train_time:170288ms step_avg:128.33ms
step:1338/1393 train_time:170423ms step_avg:128.33ms
step:1339/1393 train_time:170556ms step_avg:128.33ms
step:1340/1393 train_time:170693ms step_avg:128.34ms
step:1341/1393 train_time:170825ms step_avg:128.34ms
step:1342/1393 train_time:170959ms step_avg:128.35ms
step:1343/1393 train_time:171094ms step_avg:128.35ms
step:1344/1393 train_time:171228ms step_avg:128.36ms
step:1345/1393 train_time:171365ms step_avg:128.36ms
step:1346/1393 train_time:171499ms step_avg:128.37ms
step:1347/1393 train_time:171636ms step_avg:128.37ms
step:1348/1393 train_time:171769ms step_avg:128.38ms
step:1349/1393 train_time:171903ms step_avg:128.38ms
step:1350/1393 train_time:172037ms step_avg:128.39ms
step:1351/1393 train_time:172171ms step_avg:128.39ms
step:1352/1393 train_time:172308ms step_avg:128.40ms
step:1353/1393 train_time:172445ms step_avg:128.40ms
step:1354/1393 train_time:172581ms step_avg:128.41ms
step:1355/1393 train_time:172715ms step_avg:128.41ms
step:1356/1393 train_time:172849ms step_avg:128.42ms
step:1357/1393 train_time:172984ms step_avg:128.42ms
step:1358/1393 train_time:173121ms step_avg:128.43ms
step:1359/1393 train_time:173255ms step_avg:128.43ms
step:1360/1393 train_time:173392ms step_avg:128.44ms
step:1361/1393 train_time:173527ms step_avg:128.44ms
step:1362/1393 train_time:173665ms step_avg:128.45ms
step:1363/1393 train_time:173803ms step_avg:128.46ms
step:1364/1393 train_time:173938ms step_avg:128.46ms
step:1365/1393 train_time:174071ms step_avg:128.47ms
step:1366/1393 train_time:174206ms step_avg:128.47ms
step:1367/1393 train_time:174342ms step_avg:128.48ms
step:1368/1393 train_time:174477ms step_avg:128.48ms
step:1369/1393 train_time:174615ms step_avg:128.49ms
step:1370/1393 train_time:174753ms step_avg:128.49ms
step:1371/1393 train_time:174889ms step_avg:128.50ms
step:1372/1393 train_time:175027ms step_avg:128.51ms
step:1373/1393 train_time:175162ms step_avg:128.51ms
step:1374/1393 train_time:175299ms step_avg:128.52ms
step:1375/1393 train_time:175432ms step_avg:128.52ms
step:1375/1393 val_loss:3.2842 train_time:175565ms step_avg:128.62ms
step:1376/1393 train_time:175585ms step_avg:128.54ms
step:1377/1393 train_time:175708ms step_avg:128.54ms
step:1378/1393 train_time:175843ms step_avg:128.54ms
step:1379/1393 train_time:175977ms step_avg:128.54ms
step:1380/1393 train_time:176111ms step_avg:128.55ms
step:1381/1393 train_time:176246ms step_avg:128.55ms
step:1382/1393 train_time:176381ms step_avg:128.56ms
step:1383/1393 train_time:176515ms step_avg:128.56ms
step:1384/1393 train_time:176654ms step_avg:128.57ms
step:1385/1393 train_time:176788ms step_avg:128.57ms
step:1386/1393 train_time:176922ms step_avg:128.58ms
step:1387/1393 train_time:177059ms step_avg:128.58ms
step:1388/1393 train_time:177193ms step_avg:128.59ms
step:1389/1393 train_time:177327ms step_avg:128.59ms
step:1390/1393 train_time:177462ms step_avg:128.60ms
step:1391/1393 train_time:177597ms step_avg:128.60ms
step:1392/1393 train_time:177734ms step_avg:128.61ms
step:1393/1393 train_time:177868ms step_avg:128.61ms
step:1393/1393 val_loss:3.2805 train_time:178002ms step_avg:128.71ms
peak memory allocated: 37653 MiB reserved: 41736 MiB
