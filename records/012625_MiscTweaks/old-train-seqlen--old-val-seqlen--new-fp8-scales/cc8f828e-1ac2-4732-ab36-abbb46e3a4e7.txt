import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:28:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:24492ms step_avg:nanms
step:2/1393 train_time:24816ms step_avg:nanms
step:3/1393 train_time:24936ms step_avg:nanms
step:4/1393 train_time:25056ms step_avg:nanms
step:5/1393 train_time:25177ms step_avg:nanms
step:6/1393 train_time:25297ms step_avg:nanms
step:7/1393 train_time:25417ms step_avg:nanms
step:8/1393 train_time:25538ms step_avg:nanms
step:9/1393 train_time:25658ms step_avg:nanms
step:10/1393 train_time:25782ms step_avg:nanms
step:11/1393 train_time:124ms step_avg:nanms
step:12/1393 train_time:250ms step_avg:nanms
step:13/1393 train_time:372ms step_avg:123.85ms
step:14/1393 train_time:492ms step_avg:123.04ms
step:15/1393 train_time:612ms step_avg:122.49ms
step:16/1393 train_time:733ms step_avg:122.14ms
step:17/1393 train_time:854ms step_avg:121.95ms
step:18/1393 train_time:975ms step_avg:121.84ms
step:19/1393 train_time:1098ms step_avg:122.03ms
step:20/1393 train_time:1222ms step_avg:122.24ms
step:21/1393 train_time:1344ms step_avg:122.22ms
step:22/1393 train_time:1467ms step_avg:122.22ms
step:23/1393 train_time:1589ms step_avg:122.20ms
step:24/1393 train_time:1709ms step_avg:122.09ms
step:25/1393 train_time:1830ms step_avg:122.01ms
step:26/1393 train_time:1951ms step_avg:121.95ms
step:27/1393 train_time:2072ms step_avg:121.89ms
step:28/1393 train_time:2195ms step_avg:121.94ms
step:29/1393 train_time:2316ms step_avg:121.90ms
step:30/1393 train_time:2439ms step_avg:121.94ms
step:31/1393 train_time:2562ms step_avg:121.98ms
step:32/1393 train_time:2684ms step_avg:122.01ms
step:33/1393 train_time:2805ms step_avg:121.97ms
step:34/1393 train_time:2926ms step_avg:121.92ms
step:35/1393 train_time:3048ms step_avg:121.92ms
step:36/1393 train_time:3170ms step_avg:121.91ms
step:37/1393 train_time:3291ms step_avg:121.90ms
step:38/1393 train_time:3413ms step_avg:121.89ms
step:39/1393 train_time:3534ms step_avg:121.87ms
step:40/1393 train_time:3657ms step_avg:121.89ms
step:41/1393 train_time:3777ms step_avg:121.83ms
step:42/1393 train_time:3898ms step_avg:121.80ms
step:43/1393 train_time:4021ms step_avg:121.84ms
step:44/1393 train_time:4142ms step_avg:121.82ms
step:45/1393 train_time:4263ms step_avg:121.80ms
step:46/1393 train_time:4385ms step_avg:121.80ms
step:47/1393 train_time:4506ms step_avg:121.79ms
step:48/1393 train_time:4628ms step_avg:121.79ms
step:49/1393 train_time:4751ms step_avg:121.81ms
step:50/1393 train_time:4871ms step_avg:121.79ms
step:51/1393 train_time:4994ms step_avg:121.80ms
step:52/1393 train_time:5116ms step_avg:121.81ms
step:53/1393 train_time:5238ms step_avg:121.81ms
step:54/1393 train_time:5359ms step_avg:121.79ms
step:55/1393 train_time:5481ms step_avg:121.80ms
step:56/1393 train_time:5602ms step_avg:121.79ms
step:57/1393 train_time:5724ms step_avg:121.79ms
step:58/1393 train_time:5845ms step_avg:121.78ms
step:59/1393 train_time:5967ms step_avg:121.77ms
step:60/1393 train_time:6088ms step_avg:121.75ms
step:61/1393 train_time:6210ms step_avg:121.76ms
step:62/1393 train_time:6332ms step_avg:121.78ms
step:63/1393 train_time:6453ms step_avg:121.76ms
step:64/1393 train_time:6574ms step_avg:121.74ms
step:65/1393 train_time:6696ms step_avg:121.74ms
step:66/1393 train_time:6818ms step_avg:121.75ms
step:67/1393 train_time:6940ms step_avg:121.76ms
step:68/1393 train_time:7062ms step_avg:121.76ms
step:69/1393 train_time:7183ms step_avg:121.75ms
step:70/1393 train_time:7305ms step_avg:121.74ms
step:71/1393 train_time:7426ms step_avg:121.73ms
step:72/1393 train_time:7546ms step_avg:121.71ms
step:73/1393 train_time:7668ms step_avg:121.71ms
step:74/1393 train_time:7789ms step_avg:121.70ms
step:75/1393 train_time:7910ms step_avg:121.70ms
step:76/1393 train_time:8032ms step_avg:121.69ms
step:77/1393 train_time:8153ms step_avg:121.69ms
step:78/1393 train_time:8276ms step_avg:121.70ms
step:79/1393 train_time:8398ms step_avg:121.70ms
step:80/1393 train_time:8518ms step_avg:121.69ms
step:81/1393 train_time:8640ms step_avg:121.69ms
step:82/1393 train_time:8762ms step_avg:121.70ms
step:83/1393 train_time:8883ms step_avg:121.68ms
step:84/1393 train_time:9005ms step_avg:121.68ms
step:85/1393 train_time:9125ms step_avg:121.67ms
step:86/1393 train_time:9247ms step_avg:121.67ms
step:87/1393 train_time:9369ms step_avg:121.68ms
step:88/1393 train_time:9491ms step_avg:121.68ms
step:89/1393 train_time:9613ms step_avg:121.68ms
step:90/1393 train_time:9734ms step_avg:121.68ms
step:91/1393 train_time:9855ms step_avg:121.67ms
step:92/1393 train_time:9977ms step_avg:121.67ms
step:93/1393 train_time:10098ms step_avg:121.67ms
step:94/1393 train_time:10221ms step_avg:121.68ms
step:95/1393 train_time:10343ms step_avg:121.68ms
step:96/1393 train_time:10464ms step_avg:121.68ms
step:97/1393 train_time:10586ms step_avg:121.68ms
step:98/1393 train_time:10707ms step_avg:121.67ms
step:99/1393 train_time:10828ms step_avg:121.66ms
step:100/1393 train_time:10950ms step_avg:121.67ms
step:101/1393 train_time:11071ms step_avg:121.66ms
step:102/1393 train_time:11193ms step_avg:121.66ms
step:103/1393 train_time:11314ms step_avg:121.65ms
step:104/1393 train_time:11436ms step_avg:121.66ms
step:105/1393 train_time:11558ms step_avg:121.66ms
step:106/1393 train_time:11681ms step_avg:121.67ms
step:107/1393 train_time:11805ms step_avg:121.70ms
step:108/1393 train_time:11927ms step_avg:121.70ms
step:109/1393 train_time:12049ms step_avg:121.70ms
step:110/1393 train_time:12171ms step_avg:121.71ms
step:111/1393 train_time:12293ms step_avg:121.72ms
step:112/1393 train_time:12415ms step_avg:121.71ms
step:113/1393 train_time:12537ms step_avg:121.71ms
step:114/1393 train_time:12660ms step_avg:121.73ms
step:115/1393 train_time:12782ms step_avg:121.73ms
step:116/1393 train_time:12904ms step_avg:121.74ms
step:117/1393 train_time:13027ms step_avg:121.75ms
step:118/1393 train_time:13149ms step_avg:121.75ms
step:119/1393 train_time:13270ms step_avg:121.74ms
step:120/1393 train_time:13393ms step_avg:121.75ms
step:121/1393 train_time:13515ms step_avg:121.75ms
step:122/1393 train_time:13637ms step_avg:121.75ms
step:123/1393 train_time:13759ms step_avg:121.76ms
step:124/1393 train_time:13882ms step_avg:121.77ms
step:125/1393 train_time:14004ms step_avg:121.77ms
step:125/1393 val_loss:4.3933 train_time:14123ms step_avg:122.81ms
step:126/1393 train_time:14145ms step_avg:121.94ms
step:127/1393 train_time:14263ms step_avg:121.90ms
step:128/1393 train_time:14389ms step_avg:121.94ms
step:129/1393 train_time:14511ms step_avg:121.94ms
step:130/1393 train_time:14632ms step_avg:121.93ms
step:131/1393 train_time:14752ms step_avg:121.92ms
step:132/1393 train_time:14874ms step_avg:121.91ms
step:133/1393 train_time:14996ms step_avg:121.92ms
step:134/1393 train_time:15117ms step_avg:121.91ms
step:135/1393 train_time:15239ms step_avg:121.91ms
step:136/1393 train_time:15362ms step_avg:121.92ms
step:137/1393 train_time:15486ms step_avg:121.93ms
step:138/1393 train_time:15607ms step_avg:121.93ms
step:139/1393 train_time:15728ms step_avg:121.92ms
step:140/1393 train_time:15849ms step_avg:121.92ms
step:141/1393 train_time:15971ms step_avg:121.92ms
step:142/1393 train_time:16093ms step_avg:121.91ms
step:143/1393 train_time:16214ms step_avg:121.91ms
step:144/1393 train_time:16336ms step_avg:121.91ms
step:145/1393 train_time:16459ms step_avg:121.92ms
step:146/1393 train_time:16582ms step_avg:121.92ms
step:147/1393 train_time:16702ms step_avg:121.91ms
step:148/1393 train_time:16823ms step_avg:121.91ms
step:149/1393 train_time:16946ms step_avg:121.91ms
step:150/1393 train_time:17067ms step_avg:121.91ms
step:151/1393 train_time:17189ms step_avg:121.91ms
step:152/1393 train_time:17311ms step_avg:121.91ms
step:153/1393 train_time:17433ms step_avg:121.91ms
step:154/1393 train_time:17556ms step_avg:121.92ms
step:155/1393 train_time:17679ms step_avg:121.92ms
step:156/1393 train_time:17801ms step_avg:121.92ms
step:157/1393 train_time:17922ms step_avg:121.92ms
step:158/1393 train_time:18044ms step_avg:121.92ms
step:159/1393 train_time:18166ms step_avg:121.92ms
step:160/1393 train_time:18288ms step_avg:121.92ms
step:161/1393 train_time:18410ms step_avg:121.92ms
step:162/1393 train_time:18532ms step_avg:121.92ms
step:163/1393 train_time:18655ms step_avg:121.93ms
step:164/1393 train_time:18778ms step_avg:121.94ms
step:165/1393 train_time:18900ms step_avg:121.93ms
step:166/1393 train_time:19022ms step_avg:121.93ms
step:167/1393 train_time:19142ms step_avg:121.93ms
step:168/1393 train_time:19264ms step_avg:121.92ms
step:169/1393 train_time:19387ms step_avg:121.93ms
step:170/1393 train_time:19509ms step_avg:121.93ms
step:171/1393 train_time:19632ms step_avg:121.94ms
step:172/1393 train_time:19753ms step_avg:121.93ms
step:173/1393 train_time:19876ms step_avg:121.94ms
step:174/1393 train_time:19997ms step_avg:121.94ms
step:175/1393 train_time:20119ms step_avg:121.93ms
step:176/1393 train_time:20241ms step_avg:121.94ms
step:177/1393 train_time:20363ms step_avg:121.94ms
step:178/1393 train_time:20486ms step_avg:121.94ms
step:179/1393 train_time:20609ms step_avg:121.95ms
step:180/1393 train_time:20731ms step_avg:121.95ms
step:181/1393 train_time:20854ms step_avg:121.95ms
step:182/1393 train_time:20975ms step_avg:121.95ms
step:183/1393 train_time:21097ms step_avg:121.95ms
step:184/1393 train_time:21219ms step_avg:121.95ms
step:185/1393 train_time:21341ms step_avg:121.95ms
step:186/1393 train_time:21465ms step_avg:121.96ms
step:187/1393 train_time:21587ms step_avg:121.96ms
step:188/1393 train_time:21709ms step_avg:121.96ms
step:189/1393 train_time:21831ms step_avg:121.96ms
step:190/1393 train_time:21952ms step_avg:121.96ms
step:191/1393 train_time:22075ms step_avg:121.96ms
step:192/1393 train_time:22197ms step_avg:121.96ms
step:193/1393 train_time:22318ms step_avg:121.96ms
step:194/1393 train_time:22441ms step_avg:121.96ms
step:195/1393 train_time:22562ms step_avg:121.96ms
step:196/1393 train_time:22685ms step_avg:121.96ms
step:197/1393 train_time:22807ms step_avg:121.96ms
step:198/1393 train_time:22929ms step_avg:121.97ms
step:199/1393 train_time:23052ms step_avg:121.97ms
step:200/1393 train_time:23173ms step_avg:121.96ms
step:201/1393 train_time:23296ms step_avg:121.97ms
step:202/1393 train_time:23419ms step_avg:121.97ms
step:203/1393 train_time:23542ms step_avg:121.98ms
step:204/1393 train_time:23663ms step_avg:121.98ms
step:205/1393 train_time:23786ms step_avg:121.98ms
step:206/1393 train_time:23907ms step_avg:121.98ms
step:207/1393 train_time:24029ms step_avg:121.98ms
step:208/1393 train_time:24152ms step_avg:121.98ms
step:209/1393 train_time:24276ms step_avg:121.99ms
step:210/1393 train_time:24398ms step_avg:121.99ms
step:211/1393 train_time:24520ms step_avg:121.99ms
step:212/1393 train_time:24643ms step_avg:122.00ms
step:213/1393 train_time:24767ms step_avg:122.00ms
step:214/1393 train_time:24889ms step_avg:122.00ms
step:215/1393 train_time:25012ms step_avg:122.01ms
step:216/1393 train_time:25135ms step_avg:122.01ms
step:217/1393 train_time:25257ms step_avg:122.02ms
step:218/1393 train_time:25380ms step_avg:122.02ms
step:219/1393 train_time:25502ms step_avg:122.02ms
step:220/1393 train_time:25623ms step_avg:122.02ms
step:221/1393 train_time:25748ms step_avg:122.03ms
step:222/1393 train_time:25869ms step_avg:122.03ms
step:223/1393 train_time:25992ms step_avg:122.03ms
step:224/1393 train_time:26115ms step_avg:122.03ms
step:225/1393 train_time:26239ms step_avg:122.04ms
step:226/1393 train_time:26361ms step_avg:122.04ms
step:227/1393 train_time:26483ms step_avg:122.04ms
step:228/1393 train_time:26605ms step_avg:122.04ms
step:229/1393 train_time:26729ms step_avg:122.05ms
step:230/1393 train_time:26852ms step_avg:122.05ms
step:231/1393 train_time:26975ms step_avg:122.06ms
step:232/1393 train_time:27098ms step_avg:122.06ms
step:233/1393 train_time:27220ms step_avg:122.06ms
step:234/1393 train_time:27343ms step_avg:122.07ms
step:235/1393 train_time:27464ms step_avg:122.06ms
step:236/1393 train_time:27586ms step_avg:122.06ms
step:237/1393 train_time:27710ms step_avg:122.07ms
step:238/1393 train_time:27832ms step_avg:122.07ms
step:239/1393 train_time:27956ms step_avg:122.08ms
step:240/1393 train_time:28078ms step_avg:122.08ms
step:241/1393 train_time:28200ms step_avg:122.08ms
step:242/1393 train_time:28323ms step_avg:122.08ms
step:243/1393 train_time:28445ms step_avg:122.08ms
step:244/1393 train_time:28567ms step_avg:122.08ms
step:245/1393 train_time:28690ms step_avg:122.09ms
step:246/1393 train_time:28813ms step_avg:122.09ms
step:247/1393 train_time:28937ms step_avg:122.10ms
step:248/1393 train_time:29060ms step_avg:122.10ms
step:249/1393 train_time:29183ms step_avg:122.10ms
step:250/1393 train_time:29305ms step_avg:122.10ms
step:250/1393 val_loss:3.9779 train_time:29425ms step_avg:122.60ms
step:251/1393 train_time:29448ms step_avg:122.19ms
step:252/1393 train_time:29562ms step_avg:122.16ms
step:253/1393 train_time:29689ms step_avg:122.18ms
step:254/1393 train_time:29812ms step_avg:122.18ms
step:255/1393 train_time:29933ms step_avg:122.18ms
step:256/1393 train_time:30055ms step_avg:122.17ms
step:257/1393 train_time:30177ms step_avg:122.17ms
step:258/1393 train_time:30299ms step_avg:122.17ms
step:259/1393 train_time:30421ms step_avg:122.17ms
step:260/1393 train_time:30544ms step_avg:122.18ms
step:261/1393 train_time:30667ms step_avg:122.18ms
step:262/1393 train_time:30792ms step_avg:122.19ms
step:263/1393 train_time:30915ms step_avg:122.19ms
step:264/1393 train_time:31036ms step_avg:122.19ms
step:265/1393 train_time:31159ms step_avg:122.19ms
step:266/1393 train_time:31281ms step_avg:122.19ms
step:267/1393 train_time:31404ms step_avg:122.19ms
step:268/1393 train_time:31527ms step_avg:122.20ms
step:269/1393 train_time:31650ms step_avg:122.20ms
step:270/1393 train_time:31773ms step_avg:122.20ms
step:271/1393 train_time:31895ms step_avg:122.20ms
step:272/1393 train_time:32018ms step_avg:122.21ms
step:273/1393 train_time:32141ms step_avg:122.21ms
step:274/1393 train_time:32263ms step_avg:122.21ms
step:275/1393 train_time:32385ms step_avg:122.21ms
step:276/1393 train_time:32508ms step_avg:122.21ms
step:277/1393 train_time:32631ms step_avg:122.21ms
step:278/1393 train_time:32754ms step_avg:122.22ms
step:279/1393 train_time:32878ms step_avg:122.22ms
step:280/1393 train_time:33001ms step_avg:122.23ms
step:281/1393 train_time:33124ms step_avg:122.23ms
step:282/1393 train_time:33247ms step_avg:122.23ms
step:283/1393 train_time:33370ms step_avg:122.23ms
step:284/1393 train_time:33492ms step_avg:122.23ms
step:285/1393 train_time:33616ms step_avg:122.24ms
step:286/1393 train_time:33739ms step_avg:122.24ms
step:287/1393 train_time:33862ms step_avg:122.25ms
step:288/1393 train_time:33985ms step_avg:122.25ms
step:289/1393 train_time:34107ms step_avg:122.25ms
step:290/1393 train_time:34230ms step_avg:122.25ms
step:291/1393 train_time:34352ms step_avg:122.25ms
step:292/1393 train_time:34475ms step_avg:122.25ms
step:293/1393 train_time:34597ms step_avg:122.25ms
step:294/1393 train_time:34721ms step_avg:122.26ms
step:295/1393 train_time:34845ms step_avg:122.26ms
step:296/1393 train_time:34968ms step_avg:122.27ms
step:297/1393 train_time:35090ms step_avg:122.27ms
step:298/1393 train_time:35213ms step_avg:122.27ms
step:299/1393 train_time:35335ms step_avg:122.27ms
step:300/1393 train_time:35458ms step_avg:122.27ms
step:301/1393 train_time:35580ms step_avg:122.27ms
step:302/1393 train_time:35705ms step_avg:122.28ms
step:303/1393 train_time:35826ms step_avg:122.27ms
step:304/1393 train_time:35949ms step_avg:122.27ms
step:305/1393 train_time:36071ms step_avg:122.28ms
step:306/1393 train_time:36194ms step_avg:122.28ms
step:307/1393 train_time:36316ms step_avg:122.28ms
step:308/1393 train_time:36439ms step_avg:122.28ms
step:309/1393 train_time:36562ms step_avg:122.28ms
step:310/1393 train_time:36684ms step_avg:122.28ms
step:311/1393 train_time:36808ms step_avg:122.28ms
step:312/1393 train_time:36934ms step_avg:122.30ms
step:313/1393 train_time:37059ms step_avg:122.31ms
step:314/1393 train_time:37184ms step_avg:122.32ms
step:315/1393 train_time:37309ms step_avg:122.32ms
step:316/1393 train_time:37433ms step_avg:122.33ms
step:317/1393 train_time:37559ms step_avg:122.34ms
step:318/1393 train_time:37684ms step_avg:122.35ms
step:319/1393 train_time:37809ms step_avg:122.36ms
step:320/1393 train_time:37935ms step_avg:122.37ms
step:321/1393 train_time:38060ms step_avg:122.38ms
step:322/1393 train_time:38185ms step_avg:122.39ms
step:323/1393 train_time:38310ms step_avg:122.40ms
step:324/1393 train_time:38435ms step_avg:122.40ms
step:325/1393 train_time:38560ms step_avg:122.41ms
step:326/1393 train_time:38685ms step_avg:122.42ms
step:327/1393 train_time:38811ms step_avg:122.43ms
step:328/1393 train_time:38936ms step_avg:122.44ms
step:329/1393 train_time:39061ms step_avg:122.45ms
step:330/1393 train_time:39186ms step_avg:122.46ms
step:331/1393 train_time:39313ms step_avg:122.47ms
step:332/1393 train_time:39438ms step_avg:122.48ms
step:333/1393 train_time:39563ms step_avg:122.48ms
step:334/1393 train_time:39688ms step_avg:122.49ms
step:335/1393 train_time:39814ms step_avg:122.50ms
step:336/1393 train_time:39939ms step_avg:122.51ms
step:337/1393 train_time:40064ms step_avg:122.52ms
step:338/1393 train_time:40190ms step_avg:122.53ms
step:339/1393 train_time:40316ms step_avg:122.54ms
step:340/1393 train_time:40441ms step_avg:122.55ms
step:341/1393 train_time:40565ms step_avg:122.55ms
step:342/1393 train_time:40690ms step_avg:122.56ms
step:343/1393 train_time:40818ms step_avg:122.58ms
step:344/1393 train_time:40942ms step_avg:122.58ms
step:345/1393 train_time:41069ms step_avg:122.59ms
step:346/1393 train_time:41195ms step_avg:122.60ms
step:347/1393 train_time:41321ms step_avg:122.61ms
step:348/1393 train_time:41446ms step_avg:122.62ms
step:349/1393 train_time:41570ms step_avg:122.63ms
step:350/1393 train_time:41696ms step_avg:122.63ms
step:351/1393 train_time:41821ms step_avg:122.64ms
step:352/1393 train_time:41947ms step_avg:122.65ms
step:353/1393 train_time:42073ms step_avg:122.66ms
step:354/1393 train_time:42198ms step_avg:122.67ms
step:355/1393 train_time:42324ms step_avg:122.68ms
step:356/1393 train_time:42450ms step_avg:122.69ms
step:357/1393 train_time:42575ms step_avg:122.69ms
step:358/1393 train_time:42700ms step_avg:122.70ms
step:359/1393 train_time:42825ms step_avg:122.71ms
step:360/1393 train_time:42951ms step_avg:122.72ms
step:361/1393 train_time:43077ms step_avg:122.73ms
step:362/1393 train_time:43202ms step_avg:122.73ms
step:363/1393 train_time:43328ms step_avg:122.74ms
step:364/1393 train_time:43454ms step_avg:122.75ms
step:365/1393 train_time:43579ms step_avg:122.76ms
step:366/1393 train_time:43704ms step_avg:122.76ms
step:367/1393 train_time:43830ms step_avg:122.77ms
step:368/1393 train_time:43956ms step_avg:122.78ms
step:369/1393 train_time:44081ms step_avg:122.79ms
step:370/1393 train_time:44206ms step_avg:122.79ms
step:371/1393 train_time:44331ms step_avg:122.80ms
step:372/1393 train_time:44456ms step_avg:122.81ms
step:373/1393 train_time:44581ms step_avg:122.81ms
step:374/1393 train_time:44706ms step_avg:122.82ms
step:375/1393 train_time:44831ms step_avg:122.83ms
step:375/1393 val_loss:3.7815 train_time:44955ms step_avg:123.17ms
step:376/1393 train_time:44977ms step_avg:122.89ms
step:377/1393 train_time:45095ms step_avg:122.87ms
step:378/1393 train_time:45221ms step_avg:122.88ms
step:379/1393 train_time:45346ms step_avg:122.89ms
step:380/1393 train_time:45471ms step_avg:122.89ms
step:381/1393 train_time:45595ms step_avg:122.90ms
step:382/1393 train_time:45719ms step_avg:122.90ms
step:383/1393 train_time:45844ms step_avg:122.90ms
step:384/1393 train_time:45969ms step_avg:122.91ms
step:385/1393 train_time:46096ms step_avg:122.92ms
step:386/1393 train_time:46221ms step_avg:122.93ms
step:387/1393 train_time:46346ms step_avg:122.93ms
step:388/1393 train_time:46472ms step_avg:122.94ms
step:389/1393 train_time:46597ms step_avg:122.95ms
step:390/1393 train_time:46721ms step_avg:122.95ms
step:391/1393 train_time:46846ms step_avg:122.96ms
step:392/1393 train_time:46971ms step_avg:122.96ms
step:393/1393 train_time:47096ms step_avg:122.97ms
step:394/1393 train_time:47222ms step_avg:122.97ms
step:395/1393 train_time:47347ms step_avg:122.98ms
step:396/1393 train_time:47473ms step_avg:122.99ms
step:397/1393 train_time:47598ms step_avg:122.99ms
step:398/1393 train_time:47723ms step_avg:123.00ms
step:399/1393 train_time:47848ms step_avg:123.00ms
step:400/1393 train_time:47975ms step_avg:123.01ms
step:401/1393 train_time:48100ms step_avg:123.02ms
step:402/1393 train_time:48225ms step_avg:123.02ms
step:403/1393 train_time:48350ms step_avg:123.03ms
step:404/1393 train_time:48475ms step_avg:123.03ms
step:405/1393 train_time:48601ms step_avg:123.04ms
step:406/1393 train_time:48726ms step_avg:123.05ms
step:407/1393 train_time:48852ms step_avg:123.05ms
step:408/1393 train_time:48977ms step_avg:123.06ms
step:409/1393 train_time:49102ms step_avg:123.06ms
step:410/1393 train_time:49228ms step_avg:123.07ms
step:411/1393 train_time:49354ms step_avg:123.08ms
step:412/1393 train_time:49479ms step_avg:123.08ms
step:413/1393 train_time:49604ms step_avg:123.09ms
step:414/1393 train_time:49729ms step_avg:123.09ms
step:415/1393 train_time:49856ms step_avg:123.10ms
step:416/1393 train_time:49982ms step_avg:123.11ms
step:417/1393 train_time:50107ms step_avg:123.11ms
step:418/1393 train_time:50233ms step_avg:123.12ms
step:419/1393 train_time:50358ms step_avg:123.13ms
step:420/1393 train_time:50484ms step_avg:123.13ms
step:421/1393 train_time:50609ms step_avg:123.14ms
step:422/1393 train_time:50735ms step_avg:123.14ms
step:423/1393 train_time:50860ms step_avg:123.15ms
step:424/1393 train_time:50986ms step_avg:123.15ms
step:425/1393 train_time:51111ms step_avg:123.16ms
step:426/1393 train_time:51239ms step_avg:123.17ms
step:427/1393 train_time:51364ms step_avg:123.18ms
step:428/1393 train_time:51490ms step_avg:123.18ms
step:429/1393 train_time:51617ms step_avg:123.19ms
step:430/1393 train_time:51744ms step_avg:123.20ms
step:431/1393 train_time:51869ms step_avg:123.20ms
step:432/1393 train_time:51995ms step_avg:123.21ms
step:433/1393 train_time:52120ms step_avg:123.21ms
step:434/1393 train_time:52246ms step_avg:123.22ms
step:435/1393 train_time:52373ms step_avg:123.23ms
step:436/1393 train_time:52498ms step_avg:123.24ms
step:437/1393 train_time:52623ms step_avg:123.24ms
step:438/1393 train_time:52749ms step_avg:123.25ms
step:439/1393 train_time:52876ms step_avg:123.25ms
step:440/1393 train_time:53001ms step_avg:123.26ms
step:441/1393 train_time:53127ms step_avg:123.26ms
step:442/1393 train_time:53253ms step_avg:123.27ms
step:443/1393 train_time:53379ms step_avg:123.28ms
step:444/1393 train_time:53505ms step_avg:123.28ms
step:445/1393 train_time:53632ms step_avg:123.29ms
step:446/1393 train_time:53757ms step_avg:123.30ms
step:447/1393 train_time:53882ms step_avg:123.30ms
step:448/1393 train_time:54009ms step_avg:123.31ms
step:449/1393 train_time:54134ms step_avg:123.31ms
step:450/1393 train_time:54260ms step_avg:123.32ms
step:451/1393 train_time:54385ms step_avg:123.32ms
step:452/1393 train_time:54511ms step_avg:123.33ms
step:453/1393 train_time:54637ms step_avg:123.33ms
step:454/1393 train_time:54763ms step_avg:123.34ms
step:455/1393 train_time:54889ms step_avg:123.35ms
step:456/1393 train_time:55015ms step_avg:123.35ms
step:457/1393 train_time:55141ms step_avg:123.36ms
step:458/1393 train_time:55266ms step_avg:123.36ms
step:459/1393 train_time:55391ms step_avg:123.37ms
step:460/1393 train_time:55517ms step_avg:123.37ms
step:461/1393 train_time:55642ms step_avg:123.38ms
step:462/1393 train_time:55768ms step_avg:123.38ms
step:463/1393 train_time:55895ms step_avg:123.39ms
step:464/1393 train_time:56020ms step_avg:123.39ms
step:465/1393 train_time:56145ms step_avg:123.40ms
step:466/1393 train_time:56271ms step_avg:123.40ms
step:467/1393 train_time:56397ms step_avg:123.41ms
step:468/1393 train_time:56523ms step_avg:123.41ms
step:469/1393 train_time:56648ms step_avg:123.42ms
step:470/1393 train_time:56775ms step_avg:123.42ms
step:471/1393 train_time:56900ms step_avg:123.43ms
step:472/1393 train_time:57025ms step_avg:123.43ms
step:473/1393 train_time:57151ms step_avg:123.44ms
step:474/1393 train_time:57277ms step_avg:123.44ms
step:475/1393 train_time:57402ms step_avg:123.45ms
step:476/1393 train_time:57528ms step_avg:123.45ms
step:477/1393 train_time:57654ms step_avg:123.46ms
step:478/1393 train_time:57779ms step_avg:123.46ms
step:479/1393 train_time:57905ms step_avg:123.46ms
step:480/1393 train_time:58030ms step_avg:123.47ms
step:481/1393 train_time:58156ms step_avg:123.47ms
step:482/1393 train_time:58282ms step_avg:123.48ms
step:483/1393 train_time:58407ms step_avg:123.48ms
step:484/1393 train_time:58533ms step_avg:123.49ms
step:485/1393 train_time:58658ms step_avg:123.49ms
step:486/1393 train_time:58784ms step_avg:123.50ms
step:487/1393 train_time:58910ms step_avg:123.50ms
step:488/1393 train_time:59035ms step_avg:123.51ms
step:489/1393 train_time:59161ms step_avg:123.51ms
step:490/1393 train_time:59287ms step_avg:123.52ms
step:491/1393 train_time:59413ms step_avg:123.52ms
step:492/1393 train_time:59539ms step_avg:123.53ms
step:493/1393 train_time:59665ms step_avg:123.53ms
step:494/1393 train_time:59791ms step_avg:123.53ms
step:495/1393 train_time:59917ms step_avg:123.54ms
step:496/1393 train_time:60042ms step_avg:123.54ms
step:497/1393 train_time:60167ms step_avg:123.55ms
step:498/1393 train_time:60293ms step_avg:123.55ms
step:499/1393 train_time:60419ms step_avg:123.56ms
step:500/1393 train_time:60546ms step_avg:123.56ms
step:500/1393 val_loss:3.6656 train_time:60670ms step_avg:123.82ms
step:501/1393 train_time:60693ms step_avg:123.61ms
step:502/1393 train_time:60812ms step_avg:123.60ms
step:503/1393 train_time:60939ms step_avg:123.61ms
step:504/1393 train_time:61064ms step_avg:123.61ms
step:505/1393 train_time:61188ms step_avg:123.61ms
step:506/1393 train_time:61313ms step_avg:123.61ms
step:507/1393 train_time:61438ms step_avg:123.62ms
step:508/1393 train_time:61562ms step_avg:123.62ms
step:509/1393 train_time:61688ms step_avg:123.62ms
step:510/1393 train_time:61816ms step_avg:123.63ms
step:511/1393 train_time:61943ms step_avg:123.64ms
step:512/1393 train_time:62069ms step_avg:123.64ms
step:513/1393 train_time:62195ms step_avg:123.65ms
step:514/1393 train_time:62320ms step_avg:123.65ms
step:515/1393 train_time:62444ms step_avg:123.65ms
step:516/1393 train_time:62570ms step_avg:123.66ms
step:517/1393 train_time:62696ms step_avg:123.66ms
step:518/1393 train_time:62823ms step_avg:123.67ms
step:519/1393 train_time:62951ms step_avg:123.68ms
step:520/1393 train_time:63079ms step_avg:123.68ms
step:521/1393 train_time:63207ms step_avg:123.69ms
step:522/1393 train_time:63335ms step_avg:123.70ms
step:523/1393 train_time:63461ms step_avg:123.71ms
step:524/1393 train_time:63589ms step_avg:123.71ms
step:525/1393 train_time:63717ms step_avg:123.72ms
step:526/1393 train_time:63845ms step_avg:123.73ms
step:527/1393 train_time:63973ms step_avg:123.74ms
step:528/1393 train_time:64102ms step_avg:123.75ms
step:529/1393 train_time:64229ms step_avg:123.76ms
step:530/1393 train_time:64357ms step_avg:123.76ms
step:531/1393 train_time:64485ms step_avg:123.77ms
step:532/1393 train_time:64613ms step_avg:123.78ms
step:533/1393 train_time:64740ms step_avg:123.79ms
step:534/1393 train_time:64869ms step_avg:123.80ms
step:535/1393 train_time:64996ms step_avg:123.80ms
step:536/1393 train_time:65124ms step_avg:123.81ms
step:537/1393 train_time:65253ms step_avg:123.82ms
step:538/1393 train_time:65381ms step_avg:123.83ms
step:539/1393 train_time:65508ms step_avg:123.83ms
step:540/1393 train_time:65635ms step_avg:123.84ms
step:541/1393 train_time:65763ms step_avg:123.85ms
step:542/1393 train_time:65891ms step_avg:123.85ms
step:543/1393 train_time:66019ms step_avg:123.86ms
step:544/1393 train_time:66147ms step_avg:123.87ms
step:545/1393 train_time:66275ms step_avg:123.88ms
step:546/1393 train_time:66402ms step_avg:123.88ms
step:547/1393 train_time:66529ms step_avg:123.89ms
step:548/1393 train_time:66657ms step_avg:123.90ms
step:549/1393 train_time:66785ms step_avg:123.91ms
step:550/1393 train_time:66913ms step_avg:123.91ms
step:551/1393 train_time:67041ms step_avg:123.92ms
step:552/1393 train_time:67170ms step_avg:123.93ms
step:553/1393 train_time:67299ms step_avg:123.94ms
step:554/1393 train_time:67426ms step_avg:123.94ms
step:555/1393 train_time:67553ms step_avg:123.95ms
step:556/1393 train_time:67682ms step_avg:123.96ms
step:557/1393 train_time:67809ms step_avg:123.97ms
step:558/1393 train_time:67937ms step_avg:123.97ms
step:559/1393 train_time:68064ms step_avg:123.98ms
step:560/1393 train_time:68192ms step_avg:123.98ms
step:561/1393 train_time:68319ms step_avg:123.99ms
step:562/1393 train_time:68447ms step_avg:124.00ms
step:563/1393 train_time:68575ms step_avg:124.00ms
step:564/1393 train_time:68702ms step_avg:124.01ms
step:565/1393 train_time:68830ms step_avg:124.02ms
step:566/1393 train_time:68958ms step_avg:124.03ms
step:567/1393 train_time:69086ms step_avg:124.03ms
step:568/1393 train_time:69215ms step_avg:124.04ms
step:569/1393 train_time:69342ms step_avg:124.05ms
step:570/1393 train_time:69470ms step_avg:124.05ms
step:571/1393 train_time:69597ms step_avg:124.06ms
step:572/1393 train_time:69724ms step_avg:124.06ms
step:573/1393 train_time:69852ms step_avg:124.07ms
step:574/1393 train_time:69980ms step_avg:124.08ms
step:575/1393 train_time:70108ms step_avg:124.08ms
step:576/1393 train_time:70236ms step_avg:124.09ms
step:577/1393 train_time:70364ms step_avg:124.10ms
step:578/1393 train_time:70492ms step_avg:124.11ms
step:579/1393 train_time:70620ms step_avg:124.11ms
step:580/1393 train_time:70747ms step_avg:124.12ms
step:581/1393 train_time:70875ms step_avg:124.12ms
step:582/1393 train_time:71002ms step_avg:124.13ms
step:583/1393 train_time:71130ms step_avg:124.14ms
step:584/1393 train_time:71257ms step_avg:124.14ms
step:585/1393 train_time:71385ms step_avg:124.15ms
step:586/1393 train_time:71513ms step_avg:124.16ms
step:587/1393 train_time:71641ms step_avg:124.16ms
step:588/1393 train_time:71769ms step_avg:124.17ms
step:589/1393 train_time:71896ms step_avg:124.17ms
step:590/1393 train_time:72024ms step_avg:124.18ms
step:591/1393 train_time:72152ms step_avg:124.19ms
step:592/1393 train_time:72280ms step_avg:124.19ms
step:593/1393 train_time:72408ms step_avg:124.20ms
step:594/1393 train_time:72536ms step_avg:124.21ms
step:595/1393 train_time:72664ms step_avg:124.21ms
step:596/1393 train_time:72792ms step_avg:124.22ms
step:597/1393 train_time:72921ms step_avg:124.23ms
step:598/1393 train_time:73049ms step_avg:124.23ms
step:599/1393 train_time:73176ms step_avg:124.24ms
step:600/1393 train_time:73304ms step_avg:124.24ms
step:601/1393 train_time:73431ms step_avg:124.25ms
step:602/1393 train_time:73559ms step_avg:124.25ms
step:603/1393 train_time:73686ms step_avg:124.26ms
step:604/1393 train_time:73814ms step_avg:124.27ms
step:605/1393 train_time:73943ms step_avg:124.27ms
step:606/1393 train_time:74071ms step_avg:124.28ms
step:607/1393 train_time:74199ms step_avg:124.29ms
step:608/1393 train_time:74327ms step_avg:124.29ms
step:609/1393 train_time:74455ms step_avg:124.30ms
step:610/1393 train_time:74582ms step_avg:124.30ms
step:611/1393 train_time:74710ms step_avg:124.31ms
step:612/1393 train_time:74837ms step_avg:124.31ms
step:613/1393 train_time:74965ms step_avg:124.32ms
step:614/1393 train_time:75092ms step_avg:124.33ms
step:615/1393 train_time:75220ms step_avg:124.33ms
step:616/1393 train_time:75348ms step_avg:124.34ms
step:617/1393 train_time:75475ms step_avg:124.34ms
step:618/1393 train_time:75602ms step_avg:124.35ms
step:619/1393 train_time:75731ms step_avg:124.35ms
step:620/1393 train_time:75858ms step_avg:124.36ms
step:621/1393 train_time:75986ms step_avg:124.36ms
step:622/1393 train_time:76114ms step_avg:124.37ms
step:623/1393 train_time:76242ms step_avg:124.37ms
step:624/1393 train_time:76370ms step_avg:124.38ms
step:625/1393 train_time:76498ms step_avg:124.39ms
step:625/1393 val_loss:3.5827 train_time:76625ms step_avg:124.59ms
step:626/1393 train_time:76646ms step_avg:124.43ms
step:627/1393 train_time:76764ms step_avg:124.42ms
step:628/1393 train_time:76893ms step_avg:124.42ms
step:629/1393 train_time:77020ms step_avg:124.43ms
step:630/1393 train_time:77147ms step_avg:124.43ms
step:631/1393 train_time:77274ms step_avg:124.44ms
step:632/1393 train_time:77401ms step_avg:124.44ms
step:633/1393 train_time:77529ms step_avg:124.44ms
step:634/1393 train_time:77658ms step_avg:124.45ms
step:635/1393 train_time:77788ms step_avg:124.46ms
step:636/1393 train_time:77916ms step_avg:124.47ms
step:637/1393 train_time:78043ms step_avg:124.47ms
step:638/1393 train_time:78171ms step_avg:124.48ms
step:639/1393 train_time:78298ms step_avg:124.48ms
step:640/1393 train_time:78425ms step_avg:124.48ms
step:641/1393 train_time:78553ms step_avg:124.49ms
step:642/1393 train_time:78682ms step_avg:124.50ms
step:643/1393 train_time:78810ms step_avg:124.50ms
step:644/1393 train_time:78938ms step_avg:124.51ms
step:645/1393 train_time:79066ms step_avg:124.51ms
step:646/1393 train_time:79194ms step_avg:124.52ms
step:647/1393 train_time:79322ms step_avg:124.52ms
step:648/1393 train_time:79450ms step_avg:124.53ms
step:649/1393 train_time:79578ms step_avg:124.54ms
step:650/1393 train_time:79706ms step_avg:124.54ms
step:651/1393 train_time:79835ms step_avg:124.55ms
step:652/1393 train_time:79963ms step_avg:124.55ms
step:653/1393 train_time:80091ms step_avg:124.56ms
step:654/1393 train_time:80219ms step_avg:124.56ms
step:655/1393 train_time:80347ms step_avg:124.57ms
step:656/1393 train_time:80475ms step_avg:124.57ms
step:657/1393 train_time:80603ms step_avg:124.58ms
step:658/1393 train_time:80732ms step_avg:124.59ms
step:659/1393 train_time:80860ms step_avg:124.59ms
step:660/1393 train_time:80989ms step_avg:124.60ms
step:661/1393 train_time:81117ms step_avg:124.60ms
step:662/1393 train_time:81247ms step_avg:124.61ms
step:663/1393 train_time:81375ms step_avg:124.62ms
step:664/1393 train_time:81503ms step_avg:124.62ms
step:665/1393 train_time:81630ms step_avg:124.63ms
step:666/1393 train_time:81758ms step_avg:124.63ms
step:667/1393 train_time:81886ms step_avg:124.64ms
step:668/1393 train_time:82015ms step_avg:124.64ms
step:669/1393 train_time:82143ms step_avg:124.65ms
step:670/1393 train_time:82272ms step_avg:124.65ms
step:671/1393 train_time:82400ms step_avg:124.66ms
step:672/1393 train_time:82528ms step_avg:124.66ms
step:673/1393 train_time:82656ms step_avg:124.67ms
step:674/1393 train_time:82785ms step_avg:124.68ms
step:675/1393 train_time:82913ms step_avg:124.68ms
step:676/1393 train_time:83042ms step_avg:124.69ms
step:677/1393 train_time:83170ms step_avg:124.69ms
step:678/1393 train_time:83298ms step_avg:124.70ms
step:679/1393 train_time:83426ms step_avg:124.70ms
step:680/1393 train_time:83554ms step_avg:124.71ms
step:681/1393 train_time:83682ms step_avg:124.71ms
step:682/1393 train_time:83810ms step_avg:124.72ms
step:683/1393 train_time:83939ms step_avg:124.72ms
step:684/1393 train_time:84067ms step_avg:124.73ms
step:685/1393 train_time:84195ms step_avg:124.73ms
step:686/1393 train_time:84323ms step_avg:124.74ms
step:687/1393 train_time:84451ms step_avg:124.74ms
step:688/1393 train_time:84579ms step_avg:124.75ms
step:689/1393 train_time:84707ms step_avg:124.75ms
step:690/1393 train_time:84835ms step_avg:124.76ms
step:691/1393 train_time:84965ms step_avg:124.76ms
step:692/1393 train_time:85093ms step_avg:124.77ms
step:693/1393 train_time:85221ms step_avg:124.77ms
step:694/1393 train_time:85349ms step_avg:124.78ms
step:695/1393 train_time:85477ms step_avg:124.78ms
step:696/1393 train_time:85605ms step_avg:124.79ms
step:697/1393 train_time:85734ms step_avg:124.79ms
step:698/1393 train_time:85862ms step_avg:124.80ms
step:699/1393 train_time:85991ms step_avg:124.81ms
step:700/1393 train_time:86119ms step_avg:124.81ms
step:701/1393 train_time:86247ms step_avg:124.81ms
step:702/1393 train_time:86375ms step_avg:124.82ms
step:703/1393 train_time:86503ms step_avg:124.82ms
step:704/1393 train_time:86630ms step_avg:124.83ms
step:705/1393 train_time:86759ms step_avg:124.83ms
step:706/1393 train_time:86887ms step_avg:124.84ms
step:707/1393 train_time:87016ms step_avg:124.84ms
step:708/1393 train_time:87144ms step_avg:124.85ms
step:709/1393 train_time:87272ms step_avg:124.85ms
step:710/1393 train_time:87400ms step_avg:124.86ms
step:711/1393 train_time:87528ms step_avg:124.86ms
step:712/1393 train_time:87655ms step_avg:124.87ms
step:713/1393 train_time:87783ms step_avg:124.87ms
step:714/1393 train_time:87911ms step_avg:124.87ms
step:715/1393 train_time:88039ms step_avg:124.88ms
step:716/1393 train_time:88168ms step_avg:124.88ms
step:717/1393 train_time:88297ms step_avg:124.89ms
step:718/1393 train_time:88425ms step_avg:124.89ms
step:719/1393 train_time:88554ms step_avg:124.90ms
step:720/1393 train_time:88681ms step_avg:124.90ms
step:721/1393 train_time:88810ms step_avg:124.91ms
step:722/1393 train_time:88938ms step_avg:124.91ms
step:723/1393 train_time:89065ms step_avg:124.92ms
step:724/1393 train_time:89194ms step_avg:124.92ms
step:725/1393 train_time:89324ms step_avg:124.93ms
step:726/1393 train_time:89454ms step_avg:124.94ms
step:727/1393 train_time:89586ms step_avg:124.95ms
step:728/1393 train_time:89716ms step_avg:124.95ms
step:729/1393 train_time:89846ms step_avg:124.96ms
step:730/1393 train_time:89976ms step_avg:124.97ms
step:731/1393 train_time:90107ms step_avg:124.97ms
step:732/1393 train_time:90236ms step_avg:124.98ms
step:733/1393 train_time:90367ms step_avg:124.99ms
step:734/1393 train_time:90497ms step_avg:125.00ms
step:735/1393 train_time:90627ms step_avg:125.00ms
step:736/1393 train_time:90756ms step_avg:125.01ms
step:737/1393 train_time:90886ms step_avg:125.02ms
step:738/1393 train_time:91016ms step_avg:125.02ms
step:739/1393 train_time:91146ms step_avg:125.03ms
step:740/1393 train_time:91277ms step_avg:125.04ms
step:741/1393 train_time:91408ms step_avg:125.04ms
step:742/1393 train_time:91538ms step_avg:125.05ms
step:743/1393 train_time:91668ms step_avg:125.06ms
step:744/1393 train_time:91798ms step_avg:125.07ms
step:745/1393 train_time:91928ms step_avg:125.07ms
step:746/1393 train_time:92057ms step_avg:125.08ms
step:747/1393 train_time:92188ms step_avg:125.09ms
step:748/1393 train_time:92318ms step_avg:125.09ms
step:749/1393 train_time:92448ms step_avg:125.10ms
step:750/1393 train_time:92579ms step_avg:125.11ms
step:750/1393 val_loss:3.5286 train_time:92708ms step_avg:125.28ms
step:751/1393 train_time:92729ms step_avg:125.14ms
step:752/1393 train_time:92849ms step_avg:125.13ms
step:753/1393 train_time:92980ms step_avg:125.14ms
step:754/1393 train_time:93109ms step_avg:125.15ms
step:755/1393 train_time:93238ms step_avg:125.15ms
step:756/1393 train_time:93367ms step_avg:125.16ms
step:757/1393 train_time:93497ms step_avg:125.16ms
step:758/1393 train_time:93627ms step_avg:125.17ms
step:759/1393 train_time:93758ms step_avg:125.18ms
step:760/1393 train_time:93890ms step_avg:125.19ms
step:761/1393 train_time:94020ms step_avg:125.19ms
step:762/1393 train_time:94149ms step_avg:125.20ms
step:763/1393 train_time:94279ms step_avg:125.20ms
step:764/1393 train_time:94409ms step_avg:125.21ms
step:765/1393 train_time:94539ms step_avg:125.22ms
step:766/1393 train_time:94669ms step_avg:125.22ms
step:767/1393 train_time:94800ms step_avg:125.23ms
step:768/1393 train_time:94932ms step_avg:125.24ms
step:769/1393 train_time:95062ms step_avg:125.25ms
step:770/1393 train_time:95192ms step_avg:125.25ms
step:771/1393 train_time:95322ms step_avg:125.26ms
step:772/1393 train_time:95453ms step_avg:125.27ms
step:773/1393 train_time:95583ms step_avg:125.27ms
step:774/1393 train_time:95712ms step_avg:125.28ms
step:775/1393 train_time:95842ms step_avg:125.28ms
step:776/1393 train_time:95973ms step_avg:125.29ms
step:777/1393 train_time:96103ms step_avg:125.30ms
step:778/1393 train_time:96234ms step_avg:125.30ms
step:779/1393 train_time:96362ms step_avg:125.31ms
step:780/1393 train_time:96492ms step_avg:125.31ms
step:781/1393 train_time:96622ms step_avg:125.32ms
step:782/1393 train_time:96752ms step_avg:125.33ms
step:783/1393 train_time:96883ms step_avg:125.33ms
step:784/1393 train_time:97012ms step_avg:125.34ms
step:785/1393 train_time:97142ms step_avg:125.35ms
step:786/1393 train_time:97272ms step_avg:125.35ms
step:787/1393 train_time:97402ms step_avg:125.36ms
step:788/1393 train_time:97532ms step_avg:125.36ms
step:789/1393 train_time:97663ms step_avg:125.37ms
step:790/1393 train_time:97793ms step_avg:125.38ms
step:791/1393 train_time:97923ms step_avg:125.38ms
step:792/1393 train_time:98053ms step_avg:125.39ms
step:793/1393 train_time:98182ms step_avg:125.39ms
step:794/1393 train_time:98313ms step_avg:125.40ms
step:795/1393 train_time:98444ms step_avg:125.41ms
step:796/1393 train_time:98574ms step_avg:125.41ms
step:797/1393 train_time:98704ms step_avg:125.42ms
step:798/1393 train_time:98834ms step_avg:125.42ms
step:799/1393 train_time:98964ms step_avg:125.43ms
step:800/1393 train_time:99094ms step_avg:125.44ms
step:801/1393 train_time:99224ms step_avg:125.44ms
step:802/1393 train_time:99354ms step_avg:125.45ms
step:803/1393 train_time:99484ms step_avg:125.45ms
step:804/1393 train_time:99613ms step_avg:125.46ms
step:805/1393 train_time:99745ms step_avg:125.47ms
step:806/1393 train_time:99874ms step_avg:125.47ms
step:807/1393 train_time:100003ms step_avg:125.47ms
step:808/1393 train_time:100133ms step_avg:125.48ms
step:809/1393 train_time:100264ms step_avg:125.49ms
step:810/1393 train_time:100395ms step_avg:125.49ms
step:811/1393 train_time:100525ms step_avg:125.50ms
step:812/1393 train_time:100655ms step_avg:125.50ms
step:813/1393 train_time:100785ms step_avg:125.51ms
step:814/1393 train_time:100916ms step_avg:125.52ms
step:815/1393 train_time:101046ms step_avg:125.52ms
step:816/1393 train_time:101176ms step_avg:125.53ms
step:817/1393 train_time:101306ms step_avg:125.53ms
step:818/1393 train_time:101435ms step_avg:125.54ms
step:819/1393 train_time:101566ms step_avg:125.54ms
step:820/1393 train_time:101695ms step_avg:125.55ms
step:821/1393 train_time:101825ms step_avg:125.55ms
step:822/1393 train_time:101954ms step_avg:125.56ms
step:823/1393 train_time:102085ms step_avg:125.57ms
step:824/1393 train_time:102215ms step_avg:125.57ms
step:825/1393 train_time:102345ms step_avg:125.58ms
step:826/1393 train_time:102475ms step_avg:125.58ms
step:827/1393 train_time:102605ms step_avg:125.59ms
step:828/1393 train_time:102736ms step_avg:125.59ms
step:829/1393 train_time:102866ms step_avg:125.60ms
step:830/1393 train_time:102997ms step_avg:125.61ms
step:831/1393 train_time:103127ms step_avg:125.61ms
step:832/1393 train_time:103258ms step_avg:125.62ms
step:833/1393 train_time:103387ms step_avg:125.62ms
step:834/1393 train_time:103519ms step_avg:125.63ms
step:835/1393 train_time:103649ms step_avg:125.63ms
step:836/1393 train_time:103779ms step_avg:125.64ms
step:837/1393 train_time:103910ms step_avg:125.65ms
step:838/1393 train_time:104040ms step_avg:125.65ms
step:839/1393 train_time:104169ms step_avg:125.66ms
step:840/1393 train_time:104300ms step_avg:125.66ms
step:841/1393 train_time:104430ms step_avg:125.67ms
step:842/1393 train_time:104560ms step_avg:125.67ms
step:843/1393 train_time:104690ms step_avg:125.68ms
step:844/1393 train_time:104820ms step_avg:125.68ms
step:845/1393 train_time:104950ms step_avg:125.69ms
step:846/1393 train_time:105080ms step_avg:125.69ms
step:847/1393 train_time:105211ms step_avg:125.70ms
step:848/1393 train_time:105342ms step_avg:125.71ms
step:849/1393 train_time:105473ms step_avg:125.71ms
step:850/1393 train_time:105603ms step_avg:125.72ms
step:851/1393 train_time:105733ms step_avg:125.72ms
step:852/1393 train_time:105863ms step_avg:125.73ms
step:853/1393 train_time:105993ms step_avg:125.73ms
step:854/1393 train_time:106122ms step_avg:125.74ms
step:855/1393 train_time:106253ms step_avg:125.74ms
step:856/1393 train_time:106383ms step_avg:125.75ms
step:857/1393 train_time:106514ms step_avg:125.75ms
step:858/1393 train_time:106645ms step_avg:125.76ms
step:859/1393 train_time:106776ms step_avg:125.77ms
step:860/1393 train_time:106906ms step_avg:125.77ms
step:861/1393 train_time:107036ms step_avg:125.78ms
step:862/1393 train_time:107167ms step_avg:125.78ms
step:863/1393 train_time:107296ms step_avg:125.79ms
step:864/1393 train_time:107426ms step_avg:125.79ms
step:865/1393 train_time:107557ms step_avg:125.80ms
step:866/1393 train_time:107689ms step_avg:125.80ms
step:867/1393 train_time:107821ms step_avg:125.81ms
step:868/1393 train_time:107950ms step_avg:125.82ms
step:869/1393 train_time:108079ms step_avg:125.82ms
step:870/1393 train_time:108211ms step_avg:125.83ms
step:871/1393 train_time:108341ms step_avg:125.83ms
step:872/1393 train_time:108471ms step_avg:125.84ms
step:873/1393 train_time:108602ms step_avg:125.84ms
step:874/1393 train_time:108732ms step_avg:125.85ms
step:875/1393 train_time:108863ms step_avg:125.85ms
step:875/1393 val_loss:3.4782 train_time:108992ms step_avg:126.00ms
step:876/1393 train_time:109013ms step_avg:125.88ms
step:877/1393 train_time:109135ms step_avg:125.88ms
step:878/1393 train_time:109266ms step_avg:125.88ms
step:879/1393 train_time:109395ms step_avg:125.89ms
step:880/1393 train_time:109524ms step_avg:125.89ms
step:881/1393 train_time:109653ms step_avg:125.89ms
step:882/1393 train_time:109782ms step_avg:125.90ms
step:883/1393 train_time:109912ms step_avg:125.90ms
step:884/1393 train_time:110043ms step_avg:125.91ms
step:885/1393 train_time:110176ms step_avg:125.91ms
step:886/1393 train_time:110307ms step_avg:125.92ms
step:887/1393 train_time:110437ms step_avg:125.93ms
step:888/1393 train_time:110567ms step_avg:125.93ms
step:889/1393 train_time:110698ms step_avg:125.94ms
step:890/1393 train_time:110827ms step_avg:125.94ms
step:891/1393 train_time:110957ms step_avg:125.94ms
step:892/1393 train_time:111089ms step_avg:125.95ms
step:893/1393 train_time:111220ms step_avg:125.96ms
step:894/1393 train_time:111351ms step_avg:125.96ms
step:895/1393 train_time:111481ms step_avg:125.97ms
step:896/1393 train_time:111612ms step_avg:125.97ms
step:897/1393 train_time:111740ms step_avg:125.98ms
step:898/1393 train_time:111871ms step_avg:125.98ms
step:899/1393 train_time:112002ms step_avg:125.99ms
step:900/1393 train_time:112133ms step_avg:125.99ms
step:901/1393 train_time:112263ms step_avg:126.00ms
step:902/1393 train_time:112393ms step_avg:126.00ms
step:903/1393 train_time:112523ms step_avg:126.01ms
step:904/1393 train_time:112652ms step_avg:126.01ms
step:905/1393 train_time:112783ms step_avg:126.01ms
step:906/1393 train_time:112913ms step_avg:126.02ms
step:907/1393 train_time:113043ms step_avg:126.02ms
step:908/1393 train_time:113174ms step_avg:126.03ms
step:909/1393 train_time:113304ms step_avg:126.03ms
step:910/1393 train_time:113435ms step_avg:126.04ms
step:911/1393 train_time:113564ms step_avg:126.04ms
step:912/1393 train_time:113695ms step_avg:126.05ms
step:913/1393 train_time:113825ms step_avg:126.05ms
step:914/1393 train_time:113955ms step_avg:126.06ms
step:915/1393 train_time:114087ms step_avg:126.06ms
step:916/1393 train_time:114218ms step_avg:126.07ms
step:917/1393 train_time:114349ms step_avg:126.07ms
step:918/1393 train_time:114478ms step_avg:126.08ms
step:919/1393 train_time:114611ms step_avg:126.08ms
step:920/1393 train_time:114742ms step_avg:126.09ms
step:921/1393 train_time:114873ms step_avg:126.10ms
step:922/1393 train_time:115003ms step_avg:126.10ms
step:923/1393 train_time:115133ms step_avg:126.10ms
step:924/1393 train_time:115262ms step_avg:126.11ms
step:925/1393 train_time:115393ms step_avg:126.11ms
step:926/1393 train_time:115523ms step_avg:126.12ms
step:927/1393 train_time:115653ms step_avg:126.12ms
step:928/1393 train_time:115784ms step_avg:126.13ms
step:929/1393 train_time:115915ms step_avg:126.13ms
step:930/1393 train_time:116045ms step_avg:126.14ms
step:931/1393 train_time:116177ms step_avg:126.14ms
step:932/1393 train_time:116309ms step_avg:126.15ms
step:933/1393 train_time:116441ms step_avg:126.16ms
step:934/1393 train_time:116573ms step_avg:126.16ms
step:935/1393 train_time:116705ms step_avg:126.17ms
step:936/1393 train_time:116837ms step_avg:126.17ms
step:937/1393 train_time:116970ms step_avg:126.18ms
step:938/1393 train_time:117102ms step_avg:126.19ms
step:939/1393 train_time:117233ms step_avg:126.19ms
step:940/1393 train_time:117367ms step_avg:126.20ms
step:941/1393 train_time:117499ms step_avg:126.21ms
step:942/1393 train_time:117633ms step_avg:126.22ms
step:943/1393 train_time:117766ms step_avg:126.22ms
step:944/1393 train_time:117902ms step_avg:126.23ms
step:945/1393 train_time:118034ms step_avg:126.24ms
step:946/1393 train_time:118166ms step_avg:126.25ms
step:947/1393 train_time:118299ms step_avg:126.25ms
step:948/1393 train_time:118431ms step_avg:126.26ms
step:949/1393 train_time:118563ms step_avg:126.27ms
step:950/1393 train_time:118695ms step_avg:126.27ms
step:951/1393 train_time:118830ms step_avg:126.28ms
step:952/1393 train_time:118961ms step_avg:126.29ms
step:953/1393 train_time:119094ms step_avg:126.29ms
step:954/1393 train_time:119225ms step_avg:126.30ms
step:955/1393 train_time:119356ms step_avg:126.30ms
step:956/1393 train_time:119491ms step_avg:126.31ms
step:957/1393 train_time:119623ms step_avg:126.32ms
step:958/1393 train_time:119755ms step_avg:126.32ms
step:959/1393 train_time:119888ms step_avg:126.33ms
step:960/1393 train_time:120020ms step_avg:126.34ms
step:961/1393 train_time:120152ms step_avg:126.34ms
step:962/1393 train_time:120284ms step_avg:126.35ms
step:963/1393 train_time:120416ms step_avg:126.36ms
step:964/1393 train_time:120549ms step_avg:126.36ms
step:965/1393 train_time:120681ms step_avg:126.37ms
step:966/1393 train_time:120814ms step_avg:126.37ms
step:967/1393 train_time:120948ms step_avg:126.38ms
step:968/1393 train_time:121079ms step_avg:126.39ms
step:969/1393 train_time:121211ms step_avg:126.39ms
step:970/1393 train_time:121342ms step_avg:126.40ms
step:971/1393 train_time:121474ms step_avg:126.40ms
step:972/1393 train_time:121604ms step_avg:126.41ms
step:973/1393 train_time:121737ms step_avg:126.41ms
step:974/1393 train_time:121868ms step_avg:126.42ms
step:975/1393 train_time:121999ms step_avg:126.42ms
step:976/1393 train_time:122131ms step_avg:126.43ms
step:977/1393 train_time:122263ms step_avg:126.44ms
step:978/1393 train_time:122395ms step_avg:126.44ms
step:979/1393 train_time:122527ms step_avg:126.45ms
step:980/1393 train_time:122659ms step_avg:126.45ms
step:981/1393 train_time:122792ms step_avg:126.46ms
step:982/1393 train_time:122923ms step_avg:126.46ms
step:983/1393 train_time:123055ms step_avg:126.47ms
step:984/1393 train_time:123187ms step_avg:126.47ms
step:985/1393 train_time:123318ms step_avg:126.48ms
step:986/1393 train_time:123452ms step_avg:126.49ms
step:987/1393 train_time:123584ms step_avg:126.49ms
step:988/1393 train_time:123717ms step_avg:126.50ms
step:989/1393 train_time:123850ms step_avg:126.51ms
step:990/1393 train_time:123983ms step_avg:126.51ms
step:991/1393 train_time:124115ms step_avg:126.52ms
step:992/1393 train_time:124248ms step_avg:126.53ms
step:993/1393 train_time:124383ms step_avg:126.53ms
step:994/1393 train_time:124514ms step_avg:126.54ms
step:995/1393 train_time:124646ms step_avg:126.54ms
step:996/1393 train_time:124777ms step_avg:126.55ms
step:997/1393 train_time:124909ms step_avg:126.55ms
step:998/1393 train_time:125040ms step_avg:126.56ms
step:999/1393 train_time:125172ms step_avg:126.56ms
step:1000/1393 train_time:125303ms step_avg:126.57ms
step:1000/1393 val_loss:3.4156 train_time:125433ms step_avg:126.70ms
step:1001/1393 train_time:125454ms step_avg:126.59ms
step:1002/1393 train_time:125574ms step_avg:126.59ms
step:1003/1393 train_time:125707ms step_avg:126.59ms
step:1004/1393 train_time:125840ms step_avg:126.60ms
step:1005/1393 train_time:125971ms step_avg:126.60ms
step:1006/1393 train_time:126102ms step_avg:126.61ms
step:1007/1393 train_time:126233ms step_avg:126.61ms
step:1008/1393 train_time:126364ms step_avg:126.62ms
step:1009/1393 train_time:126498ms step_avg:126.62ms
step:1010/1393 train_time:126631ms step_avg:126.63ms
step:1011/1393 train_time:126764ms step_avg:126.64ms
step:1012/1393 train_time:126896ms step_avg:126.64ms
step:1013/1393 train_time:127028ms step_avg:126.65ms
step:1014/1393 train_time:127159ms step_avg:126.65ms
step:1015/1393 train_time:127290ms step_avg:126.66ms
step:1016/1393 train_time:127421ms step_avg:126.66ms
step:1017/1393 train_time:127554ms step_avg:126.67ms
step:1018/1393 train_time:127687ms step_avg:126.67ms
step:1019/1393 train_time:127820ms step_avg:126.68ms
step:1020/1393 train_time:127952ms step_avg:126.69ms
step:1021/1393 train_time:128083ms step_avg:126.69ms
step:1022/1393 train_time:128214ms step_avg:126.69ms
step:1023/1393 train_time:128346ms step_avg:126.70ms
step:1024/1393 train_time:128478ms step_avg:126.70ms
step:1025/1393 train_time:128610ms step_avg:126.71ms
step:1026/1393 train_time:128742ms step_avg:126.71ms
step:1027/1393 train_time:128874ms step_avg:126.72ms
step:1028/1393 train_time:129006ms step_avg:126.73ms
step:1029/1393 train_time:129140ms step_avg:126.73ms
step:1030/1393 train_time:129272ms step_avg:126.74ms
step:1031/1393 train_time:129404ms step_avg:126.74ms
step:1032/1393 train_time:129534ms step_avg:126.75ms
step:1033/1393 train_time:129666ms step_avg:126.75ms
step:1034/1393 train_time:129799ms step_avg:126.76ms
step:1035/1393 train_time:129931ms step_avg:126.76ms
step:1036/1393 train_time:130062ms step_avg:126.77ms
step:1037/1393 train_time:130195ms step_avg:126.77ms
step:1038/1393 train_time:130329ms step_avg:126.78ms
step:1039/1393 train_time:130460ms step_avg:126.78ms
step:1040/1393 train_time:130592ms step_avg:126.79ms
step:1041/1393 train_time:130724ms step_avg:126.79ms
step:1042/1393 train_time:130856ms step_avg:126.80ms
step:1043/1393 train_time:130990ms step_avg:126.81ms
step:1044/1393 train_time:131123ms step_avg:126.81ms
step:1045/1393 train_time:131256ms step_avg:126.82ms
step:1046/1393 train_time:131388ms step_avg:126.82ms
step:1047/1393 train_time:131519ms step_avg:126.83ms
step:1048/1393 train_time:131652ms step_avg:126.83ms
step:1049/1393 train_time:131784ms step_avg:126.84ms
step:1050/1393 train_time:131917ms step_avg:126.84ms
step:1051/1393 train_time:132050ms step_avg:126.85ms
step:1052/1393 train_time:132181ms step_avg:126.85ms
step:1053/1393 train_time:132314ms step_avg:126.86ms
step:1054/1393 train_time:132446ms step_avg:126.86ms
step:1055/1393 train_time:132577ms step_avg:126.87ms
step:1056/1393 train_time:132708ms step_avg:126.87ms
step:1057/1393 train_time:132840ms step_avg:126.88ms
step:1058/1393 train_time:132972ms step_avg:126.88ms
step:1059/1393 train_time:133104ms step_avg:126.89ms
step:1060/1393 train_time:133238ms step_avg:126.89ms
step:1061/1393 train_time:133371ms step_avg:126.90ms
step:1062/1393 train_time:133504ms step_avg:126.91ms
step:1063/1393 train_time:133636ms step_avg:126.91ms
step:1064/1393 train_time:133768ms step_avg:126.91ms
step:1065/1393 train_time:133899ms step_avg:126.92ms
step:1066/1393 train_time:134033ms step_avg:126.92ms
step:1067/1393 train_time:134165ms step_avg:126.93ms
step:1068/1393 train_time:134297ms step_avg:126.93ms
step:1069/1393 train_time:134431ms step_avg:126.94ms
step:1070/1393 train_time:134564ms step_avg:126.95ms
step:1071/1393 train_time:134698ms step_avg:126.95ms
step:1072/1393 train_time:134830ms step_avg:126.96ms
step:1073/1393 train_time:134962ms step_avg:126.96ms
step:1074/1393 train_time:135094ms step_avg:126.97ms
step:1075/1393 train_time:135226ms step_avg:126.97ms
step:1076/1393 train_time:135359ms step_avg:126.98ms
step:1077/1393 train_time:135492ms step_avg:126.98ms
step:1078/1393 train_time:135623ms step_avg:126.99ms
step:1079/1393 train_time:135758ms step_avg:127.00ms
step:1080/1393 train_time:135890ms step_avg:127.00ms
step:1081/1393 train_time:136022ms step_avg:127.00ms
step:1082/1393 train_time:136153ms step_avg:127.01ms
step:1083/1393 train_time:136286ms step_avg:127.01ms
step:1084/1393 train_time:136419ms step_avg:127.02ms
step:1085/1393 train_time:136551ms step_avg:127.02ms
step:1086/1393 train_time:136683ms step_avg:127.03ms
step:1087/1393 train_time:136816ms step_avg:127.03ms
step:1088/1393 train_time:136949ms step_avg:127.04ms
step:1089/1393 train_time:137083ms step_avg:127.05ms
step:1090/1393 train_time:137218ms step_avg:127.05ms
step:1091/1393 train_time:137349ms step_avg:127.06ms
step:1092/1393 train_time:137481ms step_avg:127.06ms
step:1093/1393 train_time:137614ms step_avg:127.07ms
step:1094/1393 train_time:137746ms step_avg:127.07ms
step:1095/1393 train_time:137878ms step_avg:127.08ms
step:1096/1393 train_time:138012ms step_avg:127.08ms
step:1097/1393 train_time:138144ms step_avg:127.09ms
step:1098/1393 train_time:138277ms step_avg:127.09ms
step:1099/1393 train_time:138409ms step_avg:127.10ms
step:1100/1393 train_time:138541ms step_avg:127.10ms
step:1101/1393 train_time:138673ms step_avg:127.11ms
step:1102/1393 train_time:138807ms step_avg:127.11ms
step:1103/1393 train_time:138940ms step_avg:127.12ms
step:1104/1393 train_time:139071ms step_avg:127.12ms
step:1105/1393 train_time:139205ms step_avg:127.13ms
step:1106/1393 train_time:139338ms step_avg:127.13ms
step:1107/1393 train_time:139469ms step_avg:127.14ms
step:1108/1393 train_time:139603ms step_avg:127.14ms
step:1109/1393 train_time:139735ms step_avg:127.15ms
step:1110/1393 train_time:139869ms step_avg:127.15ms
step:1111/1393 train_time:140001ms step_avg:127.16ms
step:1112/1393 train_time:140132ms step_avg:127.16ms
step:1113/1393 train_time:140263ms step_avg:127.17ms
step:1114/1393 train_time:140396ms step_avg:127.17ms
step:1115/1393 train_time:140529ms step_avg:127.18ms
step:1116/1393 train_time:140662ms step_avg:127.18ms
step:1117/1393 train_time:140795ms step_avg:127.19ms
step:1118/1393 train_time:140929ms step_avg:127.19ms
step:1119/1393 train_time:141061ms step_avg:127.20ms
step:1120/1393 train_time:141194ms step_avg:127.20ms
step:1121/1393 train_time:141326ms step_avg:127.21ms
step:1122/1393 train_time:141459ms step_avg:127.21ms
step:1123/1393 train_time:141592ms step_avg:127.22ms
step:1124/1393 train_time:141723ms step_avg:127.22ms
step:1125/1393 train_time:141855ms step_avg:127.22ms
step:1125/1393 val_loss:3.3649 train_time:141986ms step_avg:127.34ms
step:1126/1393 train_time:142007ms step_avg:127.25ms
step:1127/1393 train_time:142128ms step_avg:127.24ms
step:1128/1393 train_time:142259ms step_avg:127.24ms
step:1129/1393 train_time:142393ms step_avg:127.25ms
step:1130/1393 train_time:142524ms step_avg:127.25ms
step:1131/1393 train_time:142657ms step_avg:127.26ms
step:1132/1393 train_time:142789ms step_avg:127.26ms
step:1133/1393 train_time:142919ms step_avg:127.27ms
step:1134/1393 train_time:143054ms step_avg:127.27ms
step:1135/1393 train_time:143186ms step_avg:127.28ms
step:1136/1393 train_time:143321ms step_avg:127.28ms
step:1137/1393 train_time:143453ms step_avg:127.29ms
step:1138/1393 train_time:143587ms step_avg:127.29ms
step:1139/1393 train_time:143721ms step_avg:127.30ms
step:1140/1393 train_time:143855ms step_avg:127.31ms
step:1141/1393 train_time:143989ms step_avg:127.31ms
step:1142/1393 train_time:144123ms step_avg:127.32ms
step:1143/1393 train_time:144258ms step_avg:127.32ms
step:1144/1393 train_time:144391ms step_avg:127.33ms
step:1145/1393 train_time:144526ms step_avg:127.34ms
step:1146/1393 train_time:144659ms step_avg:127.34ms
step:1147/1393 train_time:144793ms step_avg:127.35ms
step:1148/1393 train_time:144928ms step_avg:127.35ms
step:1149/1393 train_time:145062ms step_avg:127.36ms
step:1150/1393 train_time:145196ms step_avg:127.36ms
step:1151/1393 train_time:145331ms step_avg:127.37ms
step:1152/1393 train_time:145464ms step_avg:127.38ms
step:1153/1393 train_time:145600ms step_avg:127.38ms
step:1154/1393 train_time:145733ms step_avg:127.39ms
step:1155/1393 train_time:145866ms step_avg:127.39ms
step:1156/1393 train_time:146004ms step_avg:127.40ms
step:1157/1393 train_time:146137ms step_avg:127.41ms
step:1158/1393 train_time:146272ms step_avg:127.41ms
step:1159/1393 train_time:146405ms step_avg:127.42ms
step:1160/1393 train_time:146539ms step_avg:127.43ms
step:1161/1393 train_time:146673ms step_avg:127.43ms
step:1162/1393 train_time:146807ms step_avg:127.44ms
step:1163/1393 train_time:146940ms step_avg:127.44ms
step:1164/1393 train_time:147076ms step_avg:127.45ms
step:1165/1393 train_time:147208ms step_avg:127.45ms
step:1166/1393 train_time:147343ms step_avg:127.46ms
step:1167/1393 train_time:147477ms step_avg:127.46ms
step:1168/1393 train_time:147611ms step_avg:127.47ms
step:1169/1393 train_time:147746ms step_avg:127.48ms
step:1170/1393 train_time:147879ms step_avg:127.48ms
step:1171/1393 train_time:148012ms step_avg:127.49ms
step:1172/1393 train_time:148147ms step_avg:127.49ms
step:1173/1393 train_time:148280ms step_avg:127.50ms
step:1174/1393 train_time:148417ms step_avg:127.51ms
step:1175/1393 train_time:148551ms step_avg:127.51ms
step:1176/1393 train_time:148685ms step_avg:127.52ms
step:1177/1393 train_time:148822ms step_avg:127.53ms
step:1178/1393 train_time:148956ms step_avg:127.53ms
step:1179/1393 train_time:149089ms step_avg:127.54ms
step:1180/1393 train_time:149224ms step_avg:127.54ms
step:1181/1393 train_time:149360ms step_avg:127.55ms
step:1182/1393 train_time:149493ms step_avg:127.55ms
step:1183/1393 train_time:149628ms step_avg:127.56ms
step:1184/1393 train_time:149762ms step_avg:127.57ms
step:1185/1393 train_time:149896ms step_avg:127.57ms
step:1186/1393 train_time:150030ms step_avg:127.58ms
step:1187/1393 train_time:150170ms step_avg:127.59ms
step:1188/1393 train_time:150303ms step_avg:127.59ms
step:1189/1393 train_time:150437ms step_avg:127.60ms
step:1190/1393 train_time:150570ms step_avg:127.60ms
step:1191/1393 train_time:150704ms step_avg:127.61ms
step:1192/1393 train_time:150837ms step_avg:127.61ms
step:1193/1393 train_time:150971ms step_avg:127.62ms
step:1194/1393 train_time:151105ms step_avg:127.62ms
step:1195/1393 train_time:151239ms step_avg:127.63ms
step:1196/1393 train_time:151372ms step_avg:127.63ms
step:1197/1393 train_time:151506ms step_avg:127.64ms
step:1198/1393 train_time:151642ms step_avg:127.64ms
step:1199/1393 train_time:151775ms step_avg:127.65ms
step:1200/1393 train_time:151908ms step_avg:127.65ms
step:1201/1393 train_time:152041ms step_avg:127.66ms
step:1202/1393 train_time:152178ms step_avg:127.67ms
step:1203/1393 train_time:152316ms step_avg:127.67ms
step:1204/1393 train_time:152449ms step_avg:127.68ms
step:1205/1393 train_time:152583ms step_avg:127.68ms
step:1206/1393 train_time:152718ms step_avg:127.69ms
step:1207/1393 train_time:152851ms step_avg:127.70ms
step:1208/1393 train_time:152985ms step_avg:127.70ms
step:1209/1393 train_time:153119ms step_avg:127.71ms
step:1210/1393 train_time:153256ms step_avg:127.71ms
step:1211/1393 train_time:153390ms step_avg:127.72ms
step:1212/1393 train_time:153523ms step_avg:127.72ms
step:1213/1393 train_time:153656ms step_avg:127.73ms
step:1214/1393 train_time:153792ms step_avg:127.73ms
step:1215/1393 train_time:153927ms step_avg:127.74ms
step:1216/1393 train_time:154059ms step_avg:127.74ms
step:1217/1393 train_time:154193ms step_avg:127.75ms
step:1218/1393 train_time:154325ms step_avg:127.75ms
step:1219/1393 train_time:154457ms step_avg:127.76ms
step:1220/1393 train_time:154591ms step_avg:127.76ms
step:1221/1393 train_time:154724ms step_avg:127.77ms
step:1222/1393 train_time:154858ms step_avg:127.77ms
step:1223/1393 train_time:154991ms step_avg:127.78ms
step:1224/1393 train_time:155125ms step_avg:127.78ms
step:1225/1393 train_time:155261ms step_avg:127.79ms
step:1226/1393 train_time:155394ms step_avg:127.79ms
step:1227/1393 train_time:155528ms step_avg:127.80ms
step:1228/1393 train_time:155662ms step_avg:127.80ms
step:1229/1393 train_time:155794ms step_avg:127.80ms
step:1230/1393 train_time:155929ms step_avg:127.81ms
step:1231/1393 train_time:156065ms step_avg:127.82ms
step:1232/1393 train_time:156201ms step_avg:127.82ms
step:1233/1393 train_time:156335ms step_avg:127.83ms
step:1234/1393 train_time:156468ms step_avg:127.83ms
step:1235/1393 train_time:156601ms step_avg:127.84ms
step:1236/1393 train_time:156735ms step_avg:127.84ms
step:1237/1393 train_time:156868ms step_avg:127.85ms
step:1238/1393 train_time:157005ms step_avg:127.85ms
step:1239/1393 train_time:157139ms step_avg:127.86ms
step:1240/1393 train_time:157273ms step_avg:127.86ms
step:1241/1393 train_time:157410ms step_avg:127.87ms
step:1242/1393 train_time:157543ms step_avg:127.88ms
step:1243/1393 train_time:157678ms step_avg:127.88ms
step:1244/1393 train_time:157812ms step_avg:127.89ms
step:1245/1393 train_time:157945ms step_avg:127.89ms
step:1246/1393 train_time:158078ms step_avg:127.89ms
step:1247/1393 train_time:158212ms step_avg:127.90ms
step:1248/1393 train_time:158346ms step_avg:127.90ms
step:1249/1393 train_time:158479ms step_avg:127.91ms
step:1250/1393 train_time:158613ms step_avg:127.91ms
step:1250/1393 val_loss:3.3188 train_time:158745ms step_avg:128.02ms
step:1251/1393 train_time:158766ms step_avg:127.93ms
step:1252/1393 train_time:158892ms step_avg:127.93ms
step:1253/1393 train_time:159024ms step_avg:127.94ms
step:1254/1393 train_time:159156ms step_avg:127.94ms
step:1255/1393 train_time:159294ms step_avg:127.95ms
step:1256/1393 train_time:159426ms step_avg:127.95ms
step:1257/1393 train_time:159559ms step_avg:127.95ms
step:1258/1393 train_time:159693ms step_avg:127.96ms
step:1259/1393 train_time:159830ms step_avg:127.97ms
step:1260/1393 train_time:159963ms step_avg:127.97ms
step:1261/1393 train_time:160097ms step_avg:127.98ms
step:1262/1393 train_time:160232ms step_avg:127.98ms
step:1263/1393 train_time:160366ms step_avg:127.99ms
step:1264/1393 train_time:160499ms step_avg:127.99ms
step:1265/1393 train_time:160633ms step_avg:127.99ms
step:1266/1393 train_time:160767ms step_avg:128.00ms
step:1267/1393 train_time:160902ms step_avg:128.00ms
step:1268/1393 train_time:161036ms step_avg:128.01ms
step:1269/1393 train_time:161171ms step_avg:128.02ms
step:1270/1393 train_time:161305ms step_avg:128.02ms
step:1271/1393 train_time:161438ms step_avg:128.02ms
step:1272/1393 train_time:161571ms step_avg:128.03ms
step:1273/1393 train_time:161704ms step_avg:128.03ms
step:1274/1393 train_time:161838ms step_avg:128.04ms
step:1275/1393 train_time:161972ms step_avg:128.04ms
step:1276/1393 train_time:162105ms step_avg:128.05ms
step:1277/1393 train_time:162239ms step_avg:128.05ms
step:1278/1393 train_time:162373ms step_avg:128.05ms
step:1279/1393 train_time:162506ms step_avg:128.06ms
step:1280/1393 train_time:162643ms step_avg:128.07ms
step:1281/1393 train_time:162777ms step_avg:128.07ms
step:1282/1393 train_time:162912ms step_avg:128.08ms
step:1283/1393 train_time:163046ms step_avg:128.08ms
step:1284/1393 train_time:163181ms step_avg:128.09ms
step:1285/1393 train_time:163315ms step_avg:128.09ms
step:1286/1393 train_time:163448ms step_avg:128.09ms
step:1287/1393 train_time:163582ms step_avg:128.10ms
step:1288/1393 train_time:163716ms step_avg:128.10ms
step:1289/1393 train_time:163851ms step_avg:128.11ms
step:1290/1393 train_time:163986ms step_avg:128.11ms
step:1291/1393 train_time:164122ms step_avg:128.12ms
step:1292/1393 train_time:164256ms step_avg:128.12ms
step:1293/1393 train_time:164393ms step_avg:128.13ms
step:1294/1393 train_time:164526ms step_avg:128.14ms
step:1295/1393 train_time:164660ms step_avg:128.14ms
step:1296/1393 train_time:164795ms step_avg:128.15ms
step:1297/1393 train_time:164928ms step_avg:128.15ms
step:1298/1393 train_time:165061ms step_avg:128.15ms
step:1299/1393 train_time:165193ms step_avg:128.16ms
step:1300/1393 train_time:165328ms step_avg:128.16ms
step:1301/1393 train_time:165461ms step_avg:128.17ms
step:1302/1393 train_time:165595ms step_avg:128.17ms
step:1303/1393 train_time:165729ms step_avg:128.17ms
step:1304/1393 train_time:165864ms step_avg:128.18ms
step:1305/1393 train_time:165998ms step_avg:128.18ms
step:1306/1393 train_time:166132ms step_avg:128.19ms
step:1307/1393 train_time:166267ms step_avg:128.19ms
step:1308/1393 train_time:166401ms step_avg:128.20ms
step:1309/1393 train_time:166535ms step_avg:128.20ms
step:1310/1393 train_time:166670ms step_avg:128.21ms
step:1311/1393 train_time:166802ms step_avg:128.21ms
step:1312/1393 train_time:166936ms step_avg:128.22ms
step:1313/1393 train_time:167070ms step_avg:128.22ms
step:1314/1393 train_time:167203ms step_avg:128.22ms
step:1315/1393 train_time:167337ms step_avg:128.23ms
step:1316/1393 train_time:167470ms step_avg:128.23ms
step:1317/1393 train_time:167603ms step_avg:128.24ms
step:1318/1393 train_time:167738ms step_avg:128.24ms
step:1319/1393 train_time:167873ms step_avg:128.24ms
step:1320/1393 train_time:168007ms step_avg:128.25ms
step:1321/1393 train_time:168140ms step_avg:128.25ms
step:1322/1393 train_time:168277ms step_avg:128.26ms
step:1323/1393 train_time:168411ms step_avg:128.26ms
step:1324/1393 train_time:168544ms step_avg:128.27ms
step:1325/1393 train_time:168679ms step_avg:128.27ms
step:1326/1393 train_time:168812ms step_avg:128.28ms
step:1327/1393 train_time:168946ms step_avg:128.28ms
step:1328/1393 train_time:169079ms step_avg:128.28ms
step:1329/1393 train_time:169217ms step_avg:128.29ms
step:1330/1393 train_time:169353ms step_avg:128.30ms
step:1331/1393 train_time:169490ms step_avg:128.30ms
step:1332/1393 train_time:169627ms step_avg:128.31ms
step:1333/1393 train_time:169762ms step_avg:128.32ms
step:1334/1393 train_time:169896ms step_avg:128.32ms
step:1335/1393 train_time:170028ms step_avg:128.32ms
step:1336/1393 train_time:170166ms step_avg:128.33ms
step:1337/1393 train_time:170301ms step_avg:128.34ms
step:1338/1393 train_time:170435ms step_avg:128.34ms
step:1339/1393 train_time:170569ms step_avg:128.34ms
step:1340/1393 train_time:170703ms step_avg:128.35ms
step:1341/1393 train_time:170837ms step_avg:128.35ms
step:1342/1393 train_time:170971ms step_avg:128.36ms
step:1343/1393 train_time:171106ms step_avg:128.36ms
step:1344/1393 train_time:171241ms step_avg:128.37ms
step:1345/1393 train_time:171376ms step_avg:128.37ms
step:1346/1393 train_time:171511ms step_avg:128.38ms
step:1347/1393 train_time:171647ms step_avg:128.38ms
step:1348/1393 train_time:171780ms step_avg:128.39ms
step:1349/1393 train_time:171915ms step_avg:128.39ms
step:1350/1393 train_time:172049ms step_avg:128.39ms
step:1351/1393 train_time:172184ms step_avg:128.40ms
step:1352/1393 train_time:172322ms step_avg:128.41ms
step:1353/1393 train_time:172458ms step_avg:128.41ms
step:1354/1393 train_time:172594ms step_avg:128.42ms
step:1355/1393 train_time:172728ms step_avg:128.42ms
step:1356/1393 train_time:172861ms step_avg:128.43ms
step:1357/1393 train_time:172997ms step_avg:128.43ms
step:1358/1393 train_time:173133ms step_avg:128.44ms
step:1359/1393 train_time:173268ms step_avg:128.44ms
step:1360/1393 train_time:173404ms step_avg:128.45ms
step:1361/1393 train_time:173539ms step_avg:128.45ms
step:1362/1393 train_time:173676ms step_avg:128.46ms
step:1363/1393 train_time:173813ms step_avg:128.47ms
step:1364/1393 train_time:173949ms step_avg:128.47ms
step:1365/1393 train_time:174082ms step_avg:128.47ms
step:1366/1393 train_time:174217ms step_avg:128.48ms
step:1367/1393 train_time:174353ms step_avg:128.48ms
step:1368/1393 train_time:174488ms step_avg:128.49ms
step:1369/1393 train_time:174626ms step_avg:128.50ms
step:1370/1393 train_time:174764ms step_avg:128.50ms
step:1371/1393 train_time:174901ms step_avg:128.51ms
step:1372/1393 train_time:175039ms step_avg:128.52ms
step:1373/1393 train_time:175173ms step_avg:128.52ms
step:1374/1393 train_time:175309ms step_avg:128.53ms
step:1375/1393 train_time:175443ms step_avg:128.53ms
step:1375/1393 val_loss:3.2849 train_time:175576ms step_avg:128.63ms
step:1376/1393 train_time:175597ms step_avg:128.55ms
step:1377/1393 train_time:175720ms step_avg:128.54ms
step:1378/1393 train_time:175856ms step_avg:128.55ms
step:1379/1393 train_time:175990ms step_avg:128.55ms
step:1380/1393 train_time:176126ms step_avg:128.56ms
step:1381/1393 train_time:176261ms step_avg:128.56ms
step:1382/1393 train_time:176396ms step_avg:128.57ms
step:1383/1393 train_time:176530ms step_avg:128.57ms
step:1384/1393 train_time:176669ms step_avg:128.58ms
step:1385/1393 train_time:176802ms step_avg:128.58ms
step:1386/1393 train_time:176938ms step_avg:128.59ms
step:1387/1393 train_time:177073ms step_avg:128.59ms
step:1388/1393 train_time:177208ms step_avg:128.60ms
step:1389/1393 train_time:177342ms step_avg:128.60ms
step:1390/1393 train_time:177476ms step_avg:128.61ms
step:1391/1393 train_time:177611ms step_avg:128.61ms
step:1392/1393 train_time:177748ms step_avg:128.62ms
step:1393/1393 train_time:177881ms step_avg:128.62ms
step:1393/1393 val_loss:3.2811 train_time:178014ms step_avg:128.72ms
peak memory allocated: 37653 MiB reserved: 41736 MiB
