import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 05:24:36 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23413ms step_avg:nanms
step:2/1393 train_time:23893ms step_avg:nanms
step:3/1393 train_time:24013ms step_avg:nanms
step:4/1393 train_time:24133ms step_avg:nanms
step:5/1393 train_time:24254ms step_avg:nanms
step:6/1393 train_time:24374ms step_avg:nanms
step:7/1393 train_time:24495ms step_avg:nanms
step:8/1393 train_time:24616ms step_avg:nanms
step:9/1393 train_time:24737ms step_avg:nanms
step:10/1393 train_time:24859ms step_avg:nanms
step:11/1393 train_time:123ms step_avg:nanms
step:12/1393 train_time:246ms step_avg:nanms
step:13/1393 train_time:367ms step_avg:122.40ms
step:14/1393 train_time:488ms step_avg:121.93ms
step:15/1393 train_time:609ms step_avg:121.76ms
step:16/1393 train_time:729ms step_avg:121.58ms
step:17/1393 train_time:851ms step_avg:121.50ms
step:18/1393 train_time:973ms step_avg:121.57ms
step:19/1393 train_time:1094ms step_avg:121.60ms
step:20/1393 train_time:1217ms step_avg:121.68ms
step:21/1393 train_time:1338ms step_avg:121.64ms
step:22/1393 train_time:1459ms step_avg:121.59ms
step:23/1393 train_time:1580ms step_avg:121.55ms
step:24/1393 train_time:1701ms step_avg:121.53ms
step:25/1393 train_time:1823ms step_avg:121.51ms
step:26/1393 train_time:1944ms step_avg:121.51ms
step:27/1393 train_time:2066ms step_avg:121.55ms
step:28/1393 train_time:2189ms step_avg:121.62ms
step:29/1393 train_time:2311ms step_avg:121.64ms
step:30/1393 train_time:2433ms step_avg:121.64ms
step:31/1393 train_time:2554ms step_avg:121.62ms
step:32/1393 train_time:2675ms step_avg:121.59ms
step:33/1393 train_time:2796ms step_avg:121.59ms
step:34/1393 train_time:2917ms step_avg:121.56ms
step:35/1393 train_time:3038ms step_avg:121.53ms
step:36/1393 train_time:3160ms step_avg:121.52ms
step:37/1393 train_time:3281ms step_avg:121.52ms
step:38/1393 train_time:3403ms step_avg:121.55ms
step:39/1393 train_time:3524ms step_avg:121.53ms
step:40/1393 train_time:3646ms step_avg:121.55ms
step:41/1393 train_time:3768ms step_avg:121.54ms
step:42/1393 train_time:3889ms step_avg:121.52ms
step:43/1393 train_time:4011ms step_avg:121.55ms
step:44/1393 train_time:4132ms step_avg:121.52ms
step:45/1393 train_time:4253ms step_avg:121.52ms
step:46/1393 train_time:4375ms step_avg:121.53ms
step:47/1393 train_time:4496ms step_avg:121.52ms
step:48/1393 train_time:4618ms step_avg:121.52ms
step:49/1393 train_time:4738ms step_avg:121.49ms
step:50/1393 train_time:4859ms step_avg:121.46ms
step:51/1393 train_time:4981ms step_avg:121.48ms
step:52/1393 train_time:5103ms step_avg:121.49ms
step:53/1393 train_time:5224ms step_avg:121.49ms
step:54/1393 train_time:5345ms step_avg:121.48ms
step:55/1393 train_time:5467ms step_avg:121.48ms
step:56/1393 train_time:5589ms step_avg:121.50ms
step:57/1393 train_time:5712ms step_avg:121.53ms
step:58/1393 train_time:5832ms step_avg:121.49ms
step:59/1393 train_time:5953ms step_avg:121.48ms
step:60/1393 train_time:6076ms step_avg:121.53ms
step:61/1393 train_time:6197ms step_avg:121.51ms
step:62/1393 train_time:6319ms step_avg:121.53ms
step:63/1393 train_time:6441ms step_avg:121.52ms
step:64/1393 train_time:6562ms step_avg:121.52ms
step:65/1393 train_time:6684ms step_avg:121.52ms
step:66/1393 train_time:6805ms step_avg:121.52ms
step:67/1393 train_time:6928ms step_avg:121.54ms
step:68/1393 train_time:7048ms step_avg:121.52ms
step:69/1393 train_time:7170ms step_avg:121.52ms
step:70/1393 train_time:7292ms step_avg:121.53ms
step:71/1393 train_time:7413ms step_avg:121.53ms
step:72/1393 train_time:7534ms step_avg:121.52ms
step:73/1393 train_time:7655ms step_avg:121.50ms
step:74/1393 train_time:7778ms step_avg:121.54ms
step:75/1393 train_time:7901ms step_avg:121.56ms
step:76/1393 train_time:8024ms step_avg:121.58ms
step:77/1393 train_time:8144ms step_avg:121.55ms
step:78/1393 train_time:8264ms step_avg:121.53ms
step:79/1393 train_time:8385ms step_avg:121.53ms
step:80/1393 train_time:8507ms step_avg:121.53ms
step:81/1393 train_time:8629ms step_avg:121.53ms
step:82/1393 train_time:8751ms step_avg:121.54ms
step:83/1393 train_time:8873ms step_avg:121.54ms
step:84/1393 train_time:8993ms step_avg:121.53ms
step:85/1393 train_time:9114ms step_avg:121.52ms
step:86/1393 train_time:9235ms step_avg:121.51ms
step:87/1393 train_time:9356ms step_avg:121.51ms
step:88/1393 train_time:9479ms step_avg:121.53ms
step:89/1393 train_time:9600ms step_avg:121.52ms
step:90/1393 train_time:9723ms step_avg:121.54ms
step:91/1393 train_time:9842ms step_avg:121.51ms
step:92/1393 train_time:9964ms step_avg:121.51ms
step:93/1393 train_time:10085ms step_avg:121.50ms
step:94/1393 train_time:10205ms step_avg:121.49ms
step:95/1393 train_time:10326ms step_avg:121.48ms
step:96/1393 train_time:10447ms step_avg:121.47ms
step:97/1393 train_time:10569ms step_avg:121.48ms
step:98/1393 train_time:10689ms step_avg:121.47ms
step:99/1393 train_time:10811ms step_avg:121.47ms
step:100/1393 train_time:10933ms step_avg:121.47ms
step:101/1393 train_time:11053ms step_avg:121.46ms
step:102/1393 train_time:11174ms step_avg:121.46ms
step:103/1393 train_time:11295ms step_avg:121.46ms
step:104/1393 train_time:11417ms step_avg:121.46ms
step:105/1393 train_time:11539ms step_avg:121.46ms
step:106/1393 train_time:11661ms step_avg:121.46ms
step:107/1393 train_time:11782ms step_avg:121.47ms
step:108/1393 train_time:11905ms step_avg:121.47ms
step:109/1393 train_time:12027ms step_avg:121.48ms
step:110/1393 train_time:12150ms step_avg:121.50ms
step:111/1393 train_time:12272ms step_avg:121.51ms
step:112/1393 train_time:12394ms step_avg:121.51ms
step:113/1393 train_time:12516ms step_avg:121.51ms
step:114/1393 train_time:12638ms step_avg:121.52ms
step:115/1393 train_time:12760ms step_avg:121.52ms
step:116/1393 train_time:12882ms step_avg:121.53ms
step:117/1393 train_time:13004ms step_avg:121.53ms
step:118/1393 train_time:13126ms step_avg:121.54ms
step:119/1393 train_time:13249ms step_avg:121.55ms
step:120/1393 train_time:13370ms step_avg:121.55ms
step:121/1393 train_time:13492ms step_avg:121.55ms
step:122/1393 train_time:13615ms step_avg:121.56ms
step:123/1393 train_time:13737ms step_avg:121.57ms
step:124/1393 train_time:13860ms step_avg:121.58ms
step:125/1393 train_time:13982ms step_avg:121.58ms
step:125/1393 val_loss:4.3946 train_time:14103ms step_avg:122.63ms
step:126/1393 train_time:14125ms step_avg:121.77ms
step:127/1393 train_time:14243ms step_avg:121.74ms
step:128/1393 train_time:14369ms step_avg:121.77ms
step:129/1393 train_time:14491ms step_avg:121.77ms
step:130/1393 train_time:14613ms step_avg:121.77ms
step:131/1393 train_time:14734ms step_avg:121.77ms
step:132/1393 train_time:14856ms step_avg:121.77ms
step:133/1393 train_time:14977ms step_avg:121.77ms
step:134/1393 train_time:15100ms step_avg:121.77ms
step:135/1393 train_time:15225ms step_avg:121.80ms
step:136/1393 train_time:15350ms step_avg:121.83ms
step:137/1393 train_time:15473ms step_avg:121.83ms
step:138/1393 train_time:15595ms step_avg:121.83ms
step:139/1393 train_time:15716ms step_avg:121.83ms
step:140/1393 train_time:15838ms step_avg:121.83ms
step:141/1393 train_time:15960ms step_avg:121.83ms
step:142/1393 train_time:16081ms step_avg:121.82ms
step:143/1393 train_time:16205ms step_avg:121.84ms
step:144/1393 train_time:16328ms step_avg:121.85ms
step:145/1393 train_time:16452ms step_avg:121.87ms
step:146/1393 train_time:16575ms step_avg:121.88ms
step:147/1393 train_time:16697ms step_avg:121.88ms
step:148/1393 train_time:16818ms step_avg:121.87ms
step:149/1393 train_time:16940ms step_avg:121.87ms
step:150/1393 train_time:17062ms step_avg:121.87ms
step:151/1393 train_time:17185ms step_avg:121.88ms
step:152/1393 train_time:17307ms step_avg:121.88ms
step:153/1393 train_time:17429ms step_avg:121.88ms
step:154/1393 train_time:17552ms step_avg:121.89ms
step:155/1393 train_time:17674ms step_avg:121.89ms
step:156/1393 train_time:17796ms step_avg:121.89ms
step:157/1393 train_time:17918ms step_avg:121.89ms
step:158/1393 train_time:18040ms step_avg:121.89ms
step:159/1393 train_time:18162ms step_avg:121.89ms
step:160/1393 train_time:18285ms step_avg:121.90ms
step:161/1393 train_time:18407ms step_avg:121.90ms
step:162/1393 train_time:18529ms step_avg:121.90ms
step:163/1393 train_time:18652ms step_avg:121.91ms
step:164/1393 train_time:18774ms step_avg:121.91ms
step:165/1393 train_time:18896ms step_avg:121.91ms
step:166/1393 train_time:19018ms step_avg:121.91ms
step:167/1393 train_time:19139ms step_avg:121.91ms
step:168/1393 train_time:19262ms step_avg:121.91ms
step:169/1393 train_time:19384ms step_avg:121.91ms
step:170/1393 train_time:19507ms step_avg:121.92ms
step:171/1393 train_time:19630ms step_avg:121.92ms
step:172/1393 train_time:19752ms step_avg:121.92ms
step:173/1393 train_time:19874ms step_avg:121.93ms
step:174/1393 train_time:19997ms step_avg:121.93ms
step:175/1393 train_time:20118ms step_avg:121.93ms
step:176/1393 train_time:20240ms step_avg:121.93ms
step:177/1393 train_time:20362ms step_avg:121.93ms
step:178/1393 train_time:20485ms step_avg:121.94ms
step:179/1393 train_time:20608ms step_avg:121.94ms
step:180/1393 train_time:20729ms step_avg:121.94ms
step:181/1393 train_time:20852ms step_avg:121.94ms
step:182/1393 train_time:20973ms step_avg:121.94ms
step:183/1393 train_time:21095ms step_avg:121.94ms
step:184/1393 train_time:21217ms step_avg:121.94ms
step:185/1393 train_time:21338ms step_avg:121.93ms
step:186/1393 train_time:21462ms step_avg:121.94ms
step:187/1393 train_time:21584ms step_avg:121.94ms
step:188/1393 train_time:21706ms step_avg:121.94ms
step:189/1393 train_time:21828ms step_avg:121.94ms
step:190/1393 train_time:21950ms step_avg:121.94ms
step:191/1393 train_time:22072ms step_avg:121.94ms
step:192/1393 train_time:22193ms step_avg:121.94ms
step:193/1393 train_time:22315ms step_avg:121.94ms
step:194/1393 train_time:22436ms step_avg:121.94ms
step:195/1393 train_time:22559ms step_avg:121.94ms
step:196/1393 train_time:22681ms step_avg:121.94ms
step:197/1393 train_time:22803ms step_avg:121.94ms
step:198/1393 train_time:22924ms step_avg:121.94ms
step:199/1393 train_time:23047ms step_avg:121.94ms
step:200/1393 train_time:23170ms step_avg:121.95ms
step:201/1393 train_time:23296ms step_avg:121.97ms
step:202/1393 train_time:23414ms step_avg:121.95ms
step:203/1393 train_time:23536ms step_avg:121.95ms
step:204/1393 train_time:23658ms step_avg:121.95ms
step:205/1393 train_time:23779ms step_avg:121.94ms
step:206/1393 train_time:23901ms step_avg:121.94ms
step:207/1393 train_time:24023ms step_avg:121.94ms
step:208/1393 train_time:24145ms step_avg:121.95ms
step:209/1393 train_time:24268ms step_avg:121.95ms
step:210/1393 train_time:24390ms step_avg:121.95ms
step:211/1393 train_time:24513ms step_avg:121.95ms
step:212/1393 train_time:24635ms step_avg:121.96ms
step:213/1393 train_time:24758ms step_avg:121.96ms
step:214/1393 train_time:24880ms step_avg:121.96ms
step:215/1393 train_time:25003ms step_avg:121.97ms
step:216/1393 train_time:25125ms step_avg:121.97ms
step:217/1393 train_time:25248ms step_avg:121.97ms
step:218/1393 train_time:25371ms step_avg:121.97ms
step:219/1393 train_time:25493ms step_avg:121.98ms
step:220/1393 train_time:25617ms step_avg:121.98ms
step:221/1393 train_time:25739ms step_avg:121.99ms
step:222/1393 train_time:25861ms step_avg:121.99ms
step:223/1393 train_time:25984ms step_avg:121.99ms
step:224/1393 train_time:26108ms step_avg:122.00ms
step:225/1393 train_time:26230ms step_avg:122.00ms
step:226/1393 train_time:26354ms step_avg:122.01ms
step:227/1393 train_time:26477ms step_avg:122.01ms
step:228/1393 train_time:26600ms step_avg:122.02ms
step:229/1393 train_time:26722ms step_avg:122.02ms
step:230/1393 train_time:26844ms step_avg:122.02ms
step:231/1393 train_time:26966ms step_avg:122.02ms
step:232/1393 train_time:27089ms step_avg:122.02ms
step:233/1393 train_time:27213ms step_avg:122.03ms
step:234/1393 train_time:27335ms step_avg:122.03ms
step:235/1393 train_time:27457ms step_avg:122.03ms
step:236/1393 train_time:27580ms step_avg:122.03ms
step:237/1393 train_time:27702ms step_avg:122.04ms
step:238/1393 train_time:27826ms step_avg:122.04ms
step:239/1393 train_time:27948ms step_avg:122.04ms
step:240/1393 train_time:28070ms step_avg:122.04ms
step:241/1393 train_time:28193ms step_avg:122.05ms
step:242/1393 train_time:28316ms step_avg:122.05ms
step:243/1393 train_time:28438ms step_avg:122.05ms
step:244/1393 train_time:28560ms step_avg:122.05ms
step:245/1393 train_time:28683ms step_avg:122.06ms
step:246/1393 train_time:28807ms step_avg:122.06ms
step:247/1393 train_time:28930ms step_avg:122.07ms
step:248/1393 train_time:29053ms step_avg:122.07ms
step:249/1393 train_time:29175ms step_avg:122.07ms
step:250/1393 train_time:29297ms step_avg:122.07ms
step:250/1393 val_loss:3.9840 train_time:29419ms step_avg:122.58ms
step:251/1393 train_time:29441ms step_avg:122.16ms
step:252/1393 train_time:29558ms step_avg:122.14ms
step:253/1393 train_time:29683ms step_avg:122.15ms
step:254/1393 train_time:29805ms step_avg:122.15ms
step:255/1393 train_time:29927ms step_avg:122.15ms
step:256/1393 train_time:30049ms step_avg:122.15ms
step:257/1393 train_time:30170ms step_avg:122.15ms
step:258/1393 train_time:30293ms step_avg:122.15ms
step:259/1393 train_time:30415ms step_avg:122.15ms
step:260/1393 train_time:30540ms step_avg:122.16ms
step:261/1393 train_time:30663ms step_avg:122.16ms
step:262/1393 train_time:30786ms step_avg:122.17ms
step:263/1393 train_time:30909ms step_avg:122.17ms
step:264/1393 train_time:31032ms step_avg:122.17ms
step:265/1393 train_time:31154ms step_avg:122.17ms
step:266/1393 train_time:31277ms step_avg:122.17ms
step:267/1393 train_time:31400ms step_avg:122.18ms
step:268/1393 train_time:31522ms step_avg:122.18ms
step:269/1393 train_time:31645ms step_avg:122.18ms
step:270/1393 train_time:31768ms step_avg:122.19ms
step:271/1393 train_time:31892ms step_avg:122.19ms
step:272/1393 train_time:32014ms step_avg:122.19ms
step:273/1393 train_time:32136ms step_avg:122.19ms
step:274/1393 train_time:32259ms step_avg:122.19ms
step:275/1393 train_time:32381ms step_avg:122.19ms
step:276/1393 train_time:32503ms step_avg:122.19ms
step:277/1393 train_time:32626ms step_avg:122.19ms
step:278/1393 train_time:32749ms step_avg:122.20ms
step:279/1393 train_time:32872ms step_avg:122.20ms
step:280/1393 train_time:32995ms step_avg:122.21ms
step:281/1393 train_time:33118ms step_avg:122.21ms
step:282/1393 train_time:33240ms step_avg:122.21ms
step:283/1393 train_time:33362ms step_avg:122.21ms
step:284/1393 train_time:33484ms step_avg:122.21ms
step:285/1393 train_time:33607ms step_avg:122.21ms
step:286/1393 train_time:33730ms step_avg:122.21ms
step:287/1393 train_time:33853ms step_avg:122.21ms
step:288/1393 train_time:33976ms step_avg:122.22ms
step:289/1393 train_time:34098ms step_avg:122.22ms
step:290/1393 train_time:34221ms step_avg:122.22ms
step:291/1393 train_time:34344ms step_avg:122.22ms
step:292/1393 train_time:34466ms step_avg:122.22ms
step:293/1393 train_time:34590ms step_avg:122.22ms
step:294/1393 train_time:34713ms step_avg:122.23ms
step:295/1393 train_time:34836ms step_avg:122.23ms
step:296/1393 train_time:34959ms step_avg:122.24ms
step:297/1393 train_time:35081ms step_avg:122.23ms
step:298/1393 train_time:35204ms step_avg:122.24ms
step:299/1393 train_time:35327ms step_avg:122.24ms
step:300/1393 train_time:35449ms step_avg:122.24ms
step:301/1393 train_time:35571ms step_avg:122.24ms
step:302/1393 train_time:35693ms step_avg:122.24ms
step:303/1393 train_time:35816ms step_avg:122.24ms
step:304/1393 train_time:35939ms step_avg:122.24ms
step:305/1393 train_time:36063ms step_avg:122.25ms
step:306/1393 train_time:36186ms step_avg:122.25ms
step:307/1393 train_time:36310ms step_avg:122.26ms
step:308/1393 train_time:36430ms step_avg:122.25ms
step:309/1393 train_time:36553ms step_avg:122.25ms
step:310/1393 train_time:36675ms step_avg:122.25ms
step:311/1393 train_time:36799ms step_avg:122.26ms
step:312/1393 train_time:36925ms step_avg:122.27ms
step:313/1393 train_time:37050ms step_avg:122.28ms
step:314/1393 train_time:37176ms step_avg:122.29ms
step:315/1393 train_time:37301ms step_avg:122.30ms
step:316/1393 train_time:37426ms step_avg:122.31ms
step:317/1393 train_time:37552ms step_avg:122.32ms
step:318/1393 train_time:37677ms step_avg:122.33ms
step:319/1393 train_time:37802ms step_avg:122.34ms
step:320/1393 train_time:37927ms step_avg:122.35ms
step:321/1393 train_time:38052ms step_avg:122.35ms
step:322/1393 train_time:38178ms step_avg:122.36ms
step:323/1393 train_time:38303ms step_avg:122.37ms
step:324/1393 train_time:38429ms step_avg:122.38ms
step:325/1393 train_time:38554ms step_avg:122.39ms
step:326/1393 train_time:38680ms step_avg:122.40ms
step:327/1393 train_time:38805ms step_avg:122.41ms
step:328/1393 train_time:38929ms step_avg:122.42ms
step:329/1393 train_time:39055ms step_avg:122.43ms
step:330/1393 train_time:39180ms step_avg:122.44ms
step:331/1393 train_time:39305ms step_avg:122.44ms
step:332/1393 train_time:39430ms step_avg:122.45ms
step:333/1393 train_time:39555ms step_avg:122.46ms
step:334/1393 train_time:39680ms step_avg:122.47ms
step:335/1393 train_time:39806ms step_avg:122.48ms
step:336/1393 train_time:39931ms step_avg:122.49ms
step:337/1393 train_time:40057ms step_avg:122.50ms
step:338/1393 train_time:40183ms step_avg:122.51ms
step:339/1393 train_time:40307ms step_avg:122.51ms
step:340/1393 train_time:40433ms step_avg:122.52ms
step:341/1393 train_time:40557ms step_avg:122.53ms
step:342/1393 train_time:40682ms step_avg:122.54ms
step:343/1393 train_time:40807ms step_avg:122.54ms
step:344/1393 train_time:40932ms step_avg:122.55ms
step:345/1393 train_time:41058ms step_avg:122.56ms
step:346/1393 train_time:41183ms step_avg:122.57ms
step:347/1393 train_time:41308ms step_avg:122.58ms
step:348/1393 train_time:41434ms step_avg:122.59ms
step:349/1393 train_time:41560ms step_avg:122.60ms
step:350/1393 train_time:41685ms step_avg:122.60ms
step:351/1393 train_time:41811ms step_avg:122.61ms
step:352/1393 train_time:41936ms step_avg:122.62ms
step:353/1393 train_time:42061ms step_avg:122.63ms
step:354/1393 train_time:42186ms step_avg:122.63ms
step:355/1393 train_time:42311ms step_avg:122.64ms
step:356/1393 train_time:42437ms step_avg:122.65ms
step:357/1393 train_time:42562ms step_avg:122.66ms
step:358/1393 train_time:42688ms step_avg:122.67ms
step:359/1393 train_time:42815ms step_avg:122.68ms
step:360/1393 train_time:42939ms step_avg:122.68ms
step:361/1393 train_time:43064ms step_avg:122.69ms
step:362/1393 train_time:43189ms step_avg:122.70ms
step:363/1393 train_time:43315ms step_avg:122.70ms
step:364/1393 train_time:43440ms step_avg:122.71ms
step:365/1393 train_time:43564ms step_avg:122.72ms
step:366/1393 train_time:43690ms step_avg:122.72ms
step:367/1393 train_time:43816ms step_avg:122.73ms
step:368/1393 train_time:43942ms step_avg:122.74ms
step:369/1393 train_time:44066ms step_avg:122.75ms
step:370/1393 train_time:44191ms step_avg:122.75ms
step:371/1393 train_time:44317ms step_avg:122.76ms
step:372/1393 train_time:44442ms step_avg:122.77ms
step:373/1393 train_time:44568ms step_avg:122.78ms
step:374/1393 train_time:44693ms step_avg:122.78ms
step:375/1393 train_time:44818ms step_avg:122.79ms
step:375/1393 val_loss:3.7837 train_time:44941ms step_avg:123.13ms
step:376/1393 train_time:44964ms step_avg:122.85ms
step:377/1393 train_time:45085ms step_avg:122.85ms
step:378/1393 train_time:45207ms step_avg:122.85ms
step:379/1393 train_time:45332ms step_avg:122.85ms
step:380/1393 train_time:45457ms step_avg:122.86ms
step:381/1393 train_time:45581ms step_avg:122.86ms
step:382/1393 train_time:45705ms step_avg:122.86ms
step:383/1393 train_time:45829ms step_avg:122.87ms
step:384/1393 train_time:45954ms step_avg:122.87ms
step:385/1393 train_time:46081ms step_avg:122.88ms
step:386/1393 train_time:46208ms step_avg:122.89ms
step:387/1393 train_time:46333ms step_avg:122.90ms
step:388/1393 train_time:46459ms step_avg:122.91ms
step:389/1393 train_time:46584ms step_avg:122.91ms
step:390/1393 train_time:46709ms step_avg:122.92ms
step:391/1393 train_time:46833ms step_avg:122.92ms
step:392/1393 train_time:46958ms step_avg:122.93ms
step:393/1393 train_time:47083ms step_avg:122.93ms
step:394/1393 train_time:47209ms step_avg:122.94ms
step:395/1393 train_time:47334ms step_avg:122.95ms
step:396/1393 train_time:47459ms step_avg:122.95ms
step:397/1393 train_time:47585ms step_avg:122.96ms
step:398/1393 train_time:47710ms step_avg:122.96ms
step:399/1393 train_time:47835ms step_avg:122.97ms
step:400/1393 train_time:47961ms step_avg:122.98ms
step:401/1393 train_time:48086ms step_avg:122.98ms
step:402/1393 train_time:48212ms step_avg:122.99ms
step:403/1393 train_time:48337ms step_avg:122.99ms
step:404/1393 train_time:48463ms step_avg:123.00ms
step:405/1393 train_time:48589ms step_avg:123.01ms
step:406/1393 train_time:48714ms step_avg:123.01ms
step:407/1393 train_time:48839ms step_avg:123.02ms
step:408/1393 train_time:48964ms step_avg:123.03ms
step:409/1393 train_time:49089ms step_avg:123.03ms
step:410/1393 train_time:49214ms step_avg:123.04ms
step:411/1393 train_time:49340ms step_avg:123.04ms
step:412/1393 train_time:49464ms step_avg:123.05ms
step:413/1393 train_time:49590ms step_avg:123.05ms
step:414/1393 train_time:49715ms step_avg:123.06ms
step:415/1393 train_time:49841ms step_avg:123.06ms
step:416/1393 train_time:49966ms step_avg:123.07ms
step:417/1393 train_time:50092ms step_avg:123.08ms
step:418/1393 train_time:50218ms step_avg:123.08ms
step:419/1393 train_time:50345ms step_avg:123.09ms
step:420/1393 train_time:50470ms step_avg:123.10ms
step:421/1393 train_time:50596ms step_avg:123.11ms
step:422/1393 train_time:50722ms step_avg:123.11ms
step:423/1393 train_time:50848ms step_avg:123.12ms
step:424/1393 train_time:50973ms step_avg:123.12ms
step:425/1393 train_time:51099ms step_avg:123.13ms
step:426/1393 train_time:51224ms step_avg:123.13ms
step:427/1393 train_time:51350ms step_avg:123.14ms
step:428/1393 train_time:51476ms step_avg:123.15ms
step:429/1393 train_time:51602ms step_avg:123.16ms
step:430/1393 train_time:51728ms step_avg:123.16ms
step:431/1393 train_time:51854ms step_avg:123.17ms
step:432/1393 train_time:51980ms step_avg:123.18ms
step:433/1393 train_time:52106ms step_avg:123.18ms
step:434/1393 train_time:52231ms step_avg:123.19ms
step:435/1393 train_time:52356ms step_avg:123.19ms
step:436/1393 train_time:52483ms step_avg:123.20ms
step:437/1393 train_time:52608ms step_avg:123.20ms
step:438/1393 train_time:52734ms step_avg:123.21ms
step:439/1393 train_time:52860ms step_avg:123.22ms
step:440/1393 train_time:52985ms step_avg:123.22ms
step:441/1393 train_time:53110ms step_avg:123.23ms
step:442/1393 train_time:53236ms step_avg:123.23ms
step:443/1393 train_time:53362ms step_avg:123.24ms
step:444/1393 train_time:53488ms step_avg:123.24ms
step:445/1393 train_time:53613ms step_avg:123.25ms
step:446/1393 train_time:53738ms step_avg:123.25ms
step:447/1393 train_time:53864ms step_avg:123.26ms
step:448/1393 train_time:53989ms step_avg:123.26ms
step:449/1393 train_time:54115ms step_avg:123.27ms
step:450/1393 train_time:54241ms step_avg:123.27ms
step:451/1393 train_time:54367ms step_avg:123.28ms
step:452/1393 train_time:54492ms step_avg:123.28ms
step:453/1393 train_time:54618ms step_avg:123.29ms
step:454/1393 train_time:54744ms step_avg:123.30ms
step:455/1393 train_time:54870ms step_avg:123.30ms
step:456/1393 train_time:54996ms step_avg:123.31ms
step:457/1393 train_time:55121ms step_avg:123.31ms
step:458/1393 train_time:55247ms step_avg:123.32ms
step:459/1393 train_time:55372ms step_avg:123.32ms
step:460/1393 train_time:55498ms step_avg:123.33ms
step:461/1393 train_time:55624ms step_avg:123.34ms
step:462/1393 train_time:55750ms step_avg:123.34ms
step:463/1393 train_time:55876ms step_avg:123.35ms
step:464/1393 train_time:56002ms step_avg:123.35ms
step:465/1393 train_time:56127ms step_avg:123.36ms
step:466/1393 train_time:56252ms step_avg:123.36ms
step:467/1393 train_time:56378ms step_avg:123.37ms
step:468/1393 train_time:56504ms step_avg:123.37ms
step:469/1393 train_time:56628ms step_avg:123.37ms
step:470/1393 train_time:56753ms step_avg:123.38ms
step:471/1393 train_time:56879ms step_avg:123.38ms
step:472/1393 train_time:57004ms step_avg:123.39ms
step:473/1393 train_time:57130ms step_avg:123.39ms
step:474/1393 train_time:57255ms step_avg:123.39ms
step:475/1393 train_time:57381ms step_avg:123.40ms
step:476/1393 train_time:57508ms step_avg:123.41ms
step:477/1393 train_time:57633ms step_avg:123.41ms
step:478/1393 train_time:57759ms step_avg:123.42ms
step:479/1393 train_time:57884ms step_avg:123.42ms
step:480/1393 train_time:58011ms step_avg:123.43ms
step:481/1393 train_time:58136ms step_avg:123.43ms
step:482/1393 train_time:58262ms step_avg:123.44ms
step:483/1393 train_time:58388ms step_avg:123.44ms
step:484/1393 train_time:58514ms step_avg:123.45ms
step:485/1393 train_time:58639ms step_avg:123.45ms
step:486/1393 train_time:58765ms step_avg:123.46ms
step:487/1393 train_time:58890ms step_avg:123.46ms
step:488/1393 train_time:59016ms step_avg:123.46ms
step:489/1393 train_time:59142ms step_avg:123.47ms
step:490/1393 train_time:59268ms step_avg:123.47ms
step:491/1393 train_time:59394ms step_avg:123.48ms
step:492/1393 train_time:59520ms step_avg:123.48ms
step:493/1393 train_time:59645ms step_avg:123.49ms
step:494/1393 train_time:59772ms step_avg:123.50ms
step:495/1393 train_time:59898ms step_avg:123.50ms
step:496/1393 train_time:60024ms step_avg:123.51ms
step:497/1393 train_time:60149ms step_avg:123.51ms
step:498/1393 train_time:60276ms step_avg:123.52ms
step:499/1393 train_time:60402ms step_avg:123.52ms
step:500/1393 train_time:60528ms step_avg:123.53ms
step:500/1393 val_loss:3.6652 train_time:60652ms step_avg:123.78ms
step:501/1393 train_time:60674ms step_avg:123.57ms
step:502/1393 train_time:60796ms step_avg:123.57ms
step:503/1393 train_time:60924ms step_avg:123.58ms
step:504/1393 train_time:61049ms step_avg:123.58ms
step:505/1393 train_time:61173ms step_avg:123.58ms
step:506/1393 train_time:61298ms step_avg:123.58ms
step:507/1393 train_time:61423ms step_avg:123.59ms
step:508/1393 train_time:61548ms step_avg:123.59ms
step:509/1393 train_time:61674ms step_avg:123.59ms
step:510/1393 train_time:61803ms step_avg:123.61ms
step:511/1393 train_time:61930ms step_avg:123.61ms
step:512/1393 train_time:62055ms step_avg:123.62ms
step:513/1393 train_time:62180ms step_avg:123.62ms
step:514/1393 train_time:62306ms step_avg:123.62ms
step:515/1393 train_time:62431ms step_avg:123.63ms
step:516/1393 train_time:62556ms step_avg:123.63ms
step:517/1393 train_time:62681ms step_avg:123.63ms
step:518/1393 train_time:62810ms step_avg:123.64ms
step:519/1393 train_time:62939ms step_avg:123.65ms
step:520/1393 train_time:63067ms step_avg:123.66ms
step:521/1393 train_time:63194ms step_avg:123.67ms
step:522/1393 train_time:63321ms step_avg:123.67ms
step:523/1393 train_time:63449ms step_avg:123.68ms
step:524/1393 train_time:63576ms step_avg:123.69ms
step:525/1393 train_time:63704ms step_avg:123.70ms
step:526/1393 train_time:63832ms step_avg:123.71ms
step:527/1393 train_time:63960ms step_avg:123.71ms
step:528/1393 train_time:64087ms step_avg:123.72ms
step:529/1393 train_time:64214ms step_avg:123.73ms
step:530/1393 train_time:64342ms step_avg:123.73ms
step:531/1393 train_time:64470ms step_avg:123.74ms
step:532/1393 train_time:64597ms step_avg:123.75ms
step:533/1393 train_time:64725ms step_avg:123.76ms
step:534/1393 train_time:64853ms step_avg:123.77ms
step:535/1393 train_time:64981ms step_avg:123.77ms
step:536/1393 train_time:65110ms step_avg:123.78ms
step:537/1393 train_time:65237ms step_avg:123.79ms
step:538/1393 train_time:65365ms step_avg:123.80ms
step:539/1393 train_time:65492ms step_avg:123.80ms
step:540/1393 train_time:65620ms step_avg:123.81ms
step:541/1393 train_time:65748ms step_avg:123.82ms
step:542/1393 train_time:65875ms step_avg:123.83ms
step:543/1393 train_time:66004ms step_avg:123.84ms
step:544/1393 train_time:66132ms step_avg:123.84ms
step:545/1393 train_time:66260ms step_avg:123.85ms
step:546/1393 train_time:66387ms step_avg:123.86ms
step:547/1393 train_time:66514ms step_avg:123.86ms
step:548/1393 train_time:66641ms step_avg:123.87ms
step:549/1393 train_time:66769ms step_avg:123.88ms
step:550/1393 train_time:66897ms step_avg:123.88ms
step:551/1393 train_time:67025ms step_avg:123.89ms
step:552/1393 train_time:67153ms step_avg:123.90ms
step:553/1393 train_time:67282ms step_avg:123.91ms
step:554/1393 train_time:67410ms step_avg:123.91ms
step:555/1393 train_time:67536ms step_avg:123.92ms
step:556/1393 train_time:67663ms step_avg:123.93ms
step:557/1393 train_time:67790ms step_avg:123.93ms
step:558/1393 train_time:67919ms step_avg:123.94ms
step:559/1393 train_time:68046ms step_avg:123.95ms
step:560/1393 train_time:68174ms step_avg:123.95ms
step:561/1393 train_time:68302ms step_avg:123.96ms
step:562/1393 train_time:68430ms step_avg:123.97ms
step:563/1393 train_time:68558ms step_avg:123.97ms
step:564/1393 train_time:68686ms step_avg:123.98ms
step:565/1393 train_time:68813ms step_avg:123.99ms
step:566/1393 train_time:68942ms step_avg:124.00ms
step:567/1393 train_time:69070ms step_avg:124.00ms
step:568/1393 train_time:69198ms step_avg:124.01ms
step:569/1393 train_time:69326ms step_avg:124.02ms
step:570/1393 train_time:69454ms step_avg:124.03ms
step:571/1393 train_time:69582ms step_avg:124.03ms
step:572/1393 train_time:69709ms step_avg:124.04ms
step:573/1393 train_time:69837ms step_avg:124.04ms
step:574/1393 train_time:69966ms step_avg:124.05ms
step:575/1393 train_time:70093ms step_avg:124.06ms
step:576/1393 train_time:70221ms step_avg:124.07ms
step:577/1393 train_time:70349ms step_avg:124.07ms
step:578/1393 train_time:70478ms step_avg:124.08ms
step:579/1393 train_time:70605ms step_avg:124.09ms
step:580/1393 train_time:70733ms step_avg:124.09ms
step:581/1393 train_time:70861ms step_avg:124.10ms
step:582/1393 train_time:70990ms step_avg:124.11ms
step:583/1393 train_time:71118ms step_avg:124.12ms
step:584/1393 train_time:71245ms step_avg:124.12ms
step:585/1393 train_time:71372ms step_avg:124.13ms
step:586/1393 train_time:71499ms step_avg:124.13ms
step:587/1393 train_time:71627ms step_avg:124.14ms
step:588/1393 train_time:71755ms step_avg:124.14ms
step:589/1393 train_time:71883ms step_avg:124.15ms
step:590/1393 train_time:72011ms step_avg:124.16ms
step:591/1393 train_time:72139ms step_avg:124.16ms
step:592/1393 train_time:72272ms step_avg:124.18ms
step:593/1393 train_time:72395ms step_avg:124.18ms
step:594/1393 train_time:72523ms step_avg:124.18ms
step:595/1393 train_time:72650ms step_avg:124.19ms
step:596/1393 train_time:72778ms step_avg:124.19ms
step:597/1393 train_time:72905ms step_avg:124.20ms
step:598/1393 train_time:73033ms step_avg:124.21ms
step:599/1393 train_time:73161ms step_avg:124.21ms
step:600/1393 train_time:73289ms step_avg:124.22ms
step:601/1393 train_time:73417ms step_avg:124.22ms
step:602/1393 train_time:73544ms step_avg:124.23ms
step:603/1393 train_time:73672ms step_avg:124.24ms
step:604/1393 train_time:73799ms step_avg:124.24ms
step:605/1393 train_time:73927ms step_avg:124.25ms
step:606/1393 train_time:74054ms step_avg:124.25ms
step:607/1393 train_time:74182ms step_avg:124.26ms
step:608/1393 train_time:74310ms step_avg:124.26ms
step:609/1393 train_time:74439ms step_avg:124.27ms
step:610/1393 train_time:74567ms step_avg:124.28ms
step:611/1393 train_time:74694ms step_avg:124.28ms
step:612/1393 train_time:74822ms step_avg:124.29ms
step:613/1393 train_time:74949ms step_avg:124.29ms
step:614/1393 train_time:75076ms step_avg:124.30ms
step:615/1393 train_time:75204ms step_avg:124.30ms
step:616/1393 train_time:75332ms step_avg:124.31ms
step:617/1393 train_time:75459ms step_avg:124.32ms
step:618/1393 train_time:75588ms step_avg:124.32ms
step:619/1393 train_time:75715ms step_avg:124.33ms
step:620/1393 train_time:75843ms step_avg:124.33ms
step:621/1393 train_time:75971ms step_avg:124.34ms
step:622/1393 train_time:76099ms step_avg:124.34ms
step:623/1393 train_time:76227ms step_avg:124.35ms
step:624/1393 train_time:76356ms step_avg:124.36ms
step:625/1393 train_time:76485ms step_avg:124.37ms
step:625/1393 val_loss:3.5823 train_time:76612ms step_avg:124.57ms
step:626/1393 train_time:76633ms step_avg:124.40ms
step:627/1393 train_time:76752ms step_avg:124.40ms
step:628/1393 train_time:76880ms step_avg:124.40ms
step:629/1393 train_time:77008ms step_avg:124.41ms
step:630/1393 train_time:77135ms step_avg:124.41ms
step:631/1393 train_time:77263ms step_avg:124.42ms
step:632/1393 train_time:77390ms step_avg:124.42ms
step:633/1393 train_time:77517ms step_avg:124.43ms
step:634/1393 train_time:77647ms step_avg:124.43ms
step:635/1393 train_time:77777ms step_avg:124.44ms
step:636/1393 train_time:77906ms step_avg:124.45ms
step:637/1393 train_time:78034ms step_avg:124.46ms
step:638/1393 train_time:78160ms step_avg:124.46ms
step:639/1393 train_time:78289ms step_avg:124.47ms
step:640/1393 train_time:78416ms step_avg:124.47ms
step:641/1393 train_time:78543ms step_avg:124.47ms
step:642/1393 train_time:78672ms step_avg:124.48ms
step:643/1393 train_time:78801ms step_avg:124.49ms
step:644/1393 train_time:78929ms step_avg:124.49ms
step:645/1393 train_time:79058ms step_avg:124.50ms
step:646/1393 train_time:79185ms step_avg:124.50ms
step:647/1393 train_time:79314ms step_avg:124.51ms
step:648/1393 train_time:79442ms step_avg:124.52ms
step:649/1393 train_time:79570ms step_avg:124.52ms
step:650/1393 train_time:79698ms step_avg:124.53ms
step:651/1393 train_time:79827ms step_avg:124.54ms
step:652/1393 train_time:79956ms step_avg:124.54ms
step:653/1393 train_time:80084ms step_avg:124.55ms
step:654/1393 train_time:80213ms step_avg:124.55ms
step:655/1393 train_time:80340ms step_avg:124.56ms
step:656/1393 train_time:80468ms step_avg:124.56ms
step:657/1393 train_time:80596ms step_avg:124.57ms
step:658/1393 train_time:80724ms step_avg:124.57ms
step:659/1393 train_time:80853ms step_avg:124.58ms
step:660/1393 train_time:80981ms step_avg:124.59ms
step:661/1393 train_time:81110ms step_avg:124.59ms
step:662/1393 train_time:81238ms step_avg:124.60ms
step:663/1393 train_time:81366ms step_avg:124.60ms
step:664/1393 train_time:81494ms step_avg:124.61ms
step:665/1393 train_time:81622ms step_avg:124.61ms
step:666/1393 train_time:81750ms step_avg:124.62ms
step:667/1393 train_time:81877ms step_avg:124.62ms
step:668/1393 train_time:82006ms step_avg:124.63ms
step:669/1393 train_time:82134ms step_avg:124.63ms
step:670/1393 train_time:82262ms step_avg:124.64ms
step:671/1393 train_time:82390ms step_avg:124.65ms
step:672/1393 train_time:82518ms step_avg:124.65ms
step:673/1393 train_time:82646ms step_avg:124.65ms
step:674/1393 train_time:82775ms step_avg:124.66ms
step:675/1393 train_time:82903ms step_avg:124.67ms
step:676/1393 train_time:83031ms step_avg:124.67ms
step:677/1393 train_time:83159ms step_avg:124.68ms
step:678/1393 train_time:83287ms step_avg:124.68ms
step:679/1393 train_time:83415ms step_avg:124.69ms
step:680/1393 train_time:83545ms step_avg:124.69ms
step:681/1393 train_time:83673ms step_avg:124.70ms
step:682/1393 train_time:83800ms step_avg:124.70ms
step:683/1393 train_time:83929ms step_avg:124.71ms
step:684/1393 train_time:84057ms step_avg:124.71ms
step:685/1393 train_time:84184ms step_avg:124.72ms
step:686/1393 train_time:84313ms step_avg:124.72ms
step:687/1393 train_time:84440ms step_avg:124.73ms
step:688/1393 train_time:84569ms step_avg:124.73ms
step:689/1393 train_time:84697ms step_avg:124.74ms
step:690/1393 train_time:84825ms step_avg:124.74ms
step:691/1393 train_time:84953ms step_avg:124.75ms
step:692/1393 train_time:85082ms step_avg:124.75ms
step:693/1393 train_time:85210ms step_avg:124.76ms
step:694/1393 train_time:85337ms step_avg:124.76ms
step:695/1393 train_time:85465ms step_avg:124.77ms
step:696/1393 train_time:85593ms step_avg:124.77ms
step:697/1393 train_time:85721ms step_avg:124.78ms
step:698/1393 train_time:85849ms step_avg:124.78ms
step:699/1393 train_time:85977ms step_avg:124.78ms
step:700/1393 train_time:86105ms step_avg:124.79ms
step:701/1393 train_time:86233ms step_avg:124.79ms
step:702/1393 train_time:86362ms step_avg:124.80ms
step:703/1393 train_time:86490ms step_avg:124.80ms
step:704/1393 train_time:86618ms step_avg:124.81ms
step:705/1393 train_time:86746ms step_avg:124.81ms
step:706/1393 train_time:86875ms step_avg:124.82ms
step:707/1393 train_time:87003ms step_avg:124.83ms
step:708/1393 train_time:87132ms step_avg:124.83ms
step:709/1393 train_time:87260ms step_avg:124.84ms
step:710/1393 train_time:87387ms step_avg:124.84ms
step:711/1393 train_time:87516ms step_avg:124.84ms
step:712/1393 train_time:87644ms step_avg:124.85ms
step:713/1393 train_time:87772ms step_avg:124.85ms
step:714/1393 train_time:87900ms step_avg:124.86ms
step:715/1393 train_time:88028ms step_avg:124.86ms
step:716/1393 train_time:88156ms step_avg:124.87ms
step:717/1393 train_time:88283ms step_avg:124.87ms
step:718/1393 train_time:88412ms step_avg:124.88ms
step:719/1393 train_time:88539ms step_avg:124.88ms
step:720/1393 train_time:88667ms step_avg:124.88ms
step:721/1393 train_time:88795ms step_avg:124.89ms
step:722/1393 train_time:88923ms step_avg:124.89ms
step:723/1393 train_time:89051ms step_avg:124.90ms
step:724/1393 train_time:89179ms step_avg:124.90ms
step:725/1393 train_time:89310ms step_avg:124.91ms
step:726/1393 train_time:89439ms step_avg:124.92ms
step:727/1393 train_time:89570ms step_avg:124.92ms
step:728/1393 train_time:89700ms step_avg:124.93ms
step:729/1393 train_time:89831ms step_avg:124.94ms
step:730/1393 train_time:89961ms step_avg:124.95ms
step:731/1393 train_time:90092ms step_avg:124.95ms
step:732/1393 train_time:90222ms step_avg:124.96ms
step:733/1393 train_time:90352ms step_avg:124.97ms
step:734/1393 train_time:90481ms step_avg:124.97ms
step:735/1393 train_time:90612ms step_avg:124.98ms
step:736/1393 train_time:90743ms step_avg:124.99ms
step:737/1393 train_time:90874ms step_avg:125.00ms
step:738/1393 train_time:91004ms step_avg:125.01ms
step:739/1393 train_time:91134ms step_avg:125.01ms
step:740/1393 train_time:91263ms step_avg:125.02ms
step:741/1393 train_time:91395ms step_avg:125.03ms
step:742/1393 train_time:91525ms step_avg:125.03ms
step:743/1393 train_time:91655ms step_avg:125.04ms
step:744/1393 train_time:91784ms step_avg:125.05ms
step:745/1393 train_time:91915ms step_avg:125.06ms
step:746/1393 train_time:92045ms step_avg:125.06ms
step:747/1393 train_time:92176ms step_avg:125.07ms
step:748/1393 train_time:92305ms step_avg:125.08ms
step:749/1393 train_time:92436ms step_avg:125.08ms
step:750/1393 train_time:92566ms step_avg:125.09ms
step:750/1393 val_loss:3.5277 train_time:92695ms step_avg:125.26ms
step:751/1393 train_time:92718ms step_avg:125.13ms
step:752/1393 train_time:92840ms step_avg:125.12ms
step:753/1393 train_time:92970ms step_avg:125.13ms
step:754/1393 train_time:93099ms step_avg:125.13ms
step:755/1393 train_time:93228ms step_avg:125.14ms
step:756/1393 train_time:93356ms step_avg:125.14ms
step:757/1393 train_time:93487ms step_avg:125.15ms
step:758/1393 train_time:93616ms step_avg:125.15ms
step:759/1393 train_time:93747ms step_avg:125.16ms
step:760/1393 train_time:93879ms step_avg:125.17ms
step:761/1393 train_time:94009ms step_avg:125.18ms
step:762/1393 train_time:94139ms step_avg:125.18ms
step:763/1393 train_time:94268ms step_avg:125.19ms
step:764/1393 train_time:94399ms step_avg:125.20ms
step:765/1393 train_time:94529ms step_avg:125.20ms
step:766/1393 train_time:94658ms step_avg:125.21ms
step:767/1393 train_time:94789ms step_avg:125.22ms
step:768/1393 train_time:94920ms step_avg:125.22ms
step:769/1393 train_time:95050ms step_avg:125.23ms
step:770/1393 train_time:95179ms step_avg:125.24ms
step:771/1393 train_time:95310ms step_avg:125.24ms
step:772/1393 train_time:95440ms step_avg:125.25ms
step:773/1393 train_time:95570ms step_avg:125.26ms
step:774/1393 train_time:95699ms step_avg:125.26ms
step:775/1393 train_time:95829ms step_avg:125.27ms
step:776/1393 train_time:95960ms step_avg:125.27ms
step:777/1393 train_time:96090ms step_avg:125.28ms
step:778/1393 train_time:96221ms step_avg:125.29ms
step:779/1393 train_time:96350ms step_avg:125.29ms
step:780/1393 train_time:96481ms step_avg:125.30ms
step:781/1393 train_time:96611ms step_avg:125.31ms
step:782/1393 train_time:96741ms step_avg:125.31ms
step:783/1393 train_time:96871ms step_avg:125.32ms
step:784/1393 train_time:97002ms step_avg:125.33ms
step:785/1393 train_time:97132ms step_avg:125.33ms
step:786/1393 train_time:97263ms step_avg:125.34ms
step:787/1393 train_time:97393ms step_avg:125.34ms
step:788/1393 train_time:97523ms step_avg:125.35ms
step:789/1393 train_time:97653ms step_avg:125.36ms
step:790/1393 train_time:97783ms step_avg:125.36ms
step:791/1393 train_time:97913ms step_avg:125.37ms
step:792/1393 train_time:98043ms step_avg:125.38ms
step:793/1393 train_time:98173ms step_avg:125.38ms
step:794/1393 train_time:98303ms step_avg:125.39ms
step:795/1393 train_time:98434ms step_avg:125.39ms
step:796/1393 train_time:98565ms step_avg:125.40ms
step:797/1393 train_time:98695ms step_avg:125.41ms
step:798/1393 train_time:98825ms step_avg:125.41ms
step:799/1393 train_time:98955ms step_avg:125.42ms
step:800/1393 train_time:99085ms step_avg:125.42ms
step:801/1393 train_time:99216ms step_avg:125.43ms
step:802/1393 train_time:99345ms step_avg:125.44ms
step:803/1393 train_time:99475ms step_avg:125.44ms
step:804/1393 train_time:99605ms step_avg:125.45ms
step:805/1393 train_time:99736ms step_avg:125.45ms
step:806/1393 train_time:99866ms step_avg:125.46ms
step:807/1393 train_time:99995ms step_avg:125.46ms
step:808/1393 train_time:100127ms step_avg:125.47ms
step:809/1393 train_time:100257ms step_avg:125.48ms
step:810/1393 train_time:100387ms step_avg:125.48ms
step:811/1393 train_time:100517ms step_avg:125.49ms
step:812/1393 train_time:100647ms step_avg:125.50ms
step:813/1393 train_time:100777ms step_avg:125.50ms
step:814/1393 train_time:100907ms step_avg:125.51ms
step:815/1393 train_time:101037ms step_avg:125.51ms
step:816/1393 train_time:101169ms step_avg:125.52ms
step:817/1393 train_time:101299ms step_avg:125.52ms
step:818/1393 train_time:101428ms step_avg:125.53ms
step:819/1393 train_time:101558ms step_avg:125.54ms
step:820/1393 train_time:101688ms step_avg:125.54ms
step:821/1393 train_time:101818ms step_avg:125.55ms
step:822/1393 train_time:101947ms step_avg:125.55ms
step:823/1393 train_time:102078ms step_avg:125.56ms
step:824/1393 train_time:102207ms step_avg:125.56ms
step:825/1393 train_time:102337ms step_avg:125.57ms
step:826/1393 train_time:102467ms step_avg:125.57ms
step:827/1393 train_time:102598ms step_avg:125.58ms
step:828/1393 train_time:102730ms step_avg:125.59ms
step:829/1393 train_time:102860ms step_avg:125.59ms
step:830/1393 train_time:102989ms step_avg:125.60ms
step:831/1393 train_time:103119ms step_avg:125.60ms
step:832/1393 train_time:103249ms step_avg:125.61ms
step:833/1393 train_time:103378ms step_avg:125.61ms
step:834/1393 train_time:103509ms step_avg:125.62ms
step:835/1393 train_time:103640ms step_avg:125.62ms
step:836/1393 train_time:103770ms step_avg:125.63ms
step:837/1393 train_time:103901ms step_avg:125.64ms
step:838/1393 train_time:104031ms step_avg:125.64ms
step:839/1393 train_time:104162ms step_avg:125.65ms
step:840/1393 train_time:104292ms step_avg:125.65ms
step:841/1393 train_time:104422ms step_avg:125.66ms
step:842/1393 train_time:104553ms step_avg:125.66ms
step:843/1393 train_time:104684ms step_avg:125.67ms
step:844/1393 train_time:104814ms step_avg:125.68ms
step:845/1393 train_time:104943ms step_avg:125.68ms
step:846/1393 train_time:105074ms step_avg:125.69ms
step:847/1393 train_time:105203ms step_avg:125.69ms
step:848/1393 train_time:105333ms step_avg:125.70ms
step:849/1393 train_time:105464ms step_avg:125.70ms
step:850/1393 train_time:105594ms step_avg:125.71ms
step:851/1393 train_time:105724ms step_avg:125.71ms
step:852/1393 train_time:105854ms step_avg:125.72ms
step:853/1393 train_time:105983ms step_avg:125.72ms
step:854/1393 train_time:106113ms step_avg:125.73ms
step:855/1393 train_time:106242ms step_avg:125.73ms
step:856/1393 train_time:106373ms step_avg:125.74ms
step:857/1393 train_time:106503ms step_avg:125.74ms
step:858/1393 train_time:106635ms step_avg:125.75ms
step:859/1393 train_time:106766ms step_avg:125.76ms
step:860/1393 train_time:106896ms step_avg:125.76ms
step:861/1393 train_time:107027ms step_avg:125.77ms
step:862/1393 train_time:107158ms step_avg:125.77ms
step:863/1393 train_time:107288ms step_avg:125.78ms
step:864/1393 train_time:107419ms step_avg:125.78ms
step:865/1393 train_time:107548ms step_avg:125.79ms
step:866/1393 train_time:107682ms step_avg:125.80ms
step:867/1393 train_time:107813ms step_avg:125.80ms
step:868/1393 train_time:107943ms step_avg:125.81ms
step:869/1393 train_time:108073ms step_avg:125.81ms
step:870/1393 train_time:108203ms step_avg:125.82ms
step:871/1393 train_time:108334ms step_avg:125.82ms
step:872/1393 train_time:108463ms step_avg:125.83ms
step:873/1393 train_time:108593ms step_avg:125.83ms
step:874/1393 train_time:108723ms step_avg:125.84ms
step:875/1393 train_time:108855ms step_avg:125.84ms
step:875/1393 val_loss:3.4775 train_time:108984ms step_avg:125.99ms
step:876/1393 train_time:109005ms step_avg:125.87ms
step:877/1393 train_time:109125ms step_avg:125.87ms
step:878/1393 train_time:109257ms step_avg:125.87ms
step:879/1393 train_time:109387ms step_avg:125.88ms
step:880/1393 train_time:109517ms step_avg:125.88ms
step:881/1393 train_time:109646ms step_avg:125.88ms
step:882/1393 train_time:109776ms step_avg:125.89ms
step:883/1393 train_time:109904ms step_avg:125.89ms
step:884/1393 train_time:110037ms step_avg:125.90ms
step:885/1393 train_time:110170ms step_avg:125.91ms
step:886/1393 train_time:110302ms step_avg:125.92ms
step:887/1393 train_time:110432ms step_avg:125.92ms
step:888/1393 train_time:110562ms step_avg:125.92ms
step:889/1393 train_time:110694ms step_avg:125.93ms
step:890/1393 train_time:110822ms step_avg:125.93ms
step:891/1393 train_time:110952ms step_avg:125.94ms
step:892/1393 train_time:111083ms step_avg:125.94ms
step:893/1393 train_time:111215ms step_avg:125.95ms
step:894/1393 train_time:111344ms step_avg:125.95ms
step:895/1393 train_time:111476ms step_avg:125.96ms
step:896/1393 train_time:111606ms step_avg:125.97ms
step:897/1393 train_time:111736ms step_avg:125.97ms
step:898/1393 train_time:111866ms step_avg:125.98ms
step:899/1393 train_time:111997ms step_avg:125.98ms
step:900/1393 train_time:112128ms step_avg:125.99ms
step:901/1393 train_time:112259ms step_avg:125.99ms
step:902/1393 train_time:112388ms step_avg:126.00ms
step:903/1393 train_time:112520ms step_avg:126.00ms
step:904/1393 train_time:112649ms step_avg:126.01ms
step:905/1393 train_time:112779ms step_avg:126.01ms
step:906/1393 train_time:112908ms step_avg:126.01ms
step:907/1393 train_time:113039ms step_avg:126.02ms
step:908/1393 train_time:113168ms step_avg:126.02ms
step:909/1393 train_time:113299ms step_avg:126.03ms
step:910/1393 train_time:113430ms step_avg:126.03ms
step:911/1393 train_time:113560ms step_avg:126.04ms
step:912/1393 train_time:113690ms step_avg:126.04ms
step:913/1393 train_time:113820ms step_avg:126.05ms
step:914/1393 train_time:113950ms step_avg:126.05ms
step:915/1393 train_time:114080ms step_avg:126.06ms
step:916/1393 train_time:114211ms step_avg:126.06ms
step:917/1393 train_time:114342ms step_avg:126.07ms
step:918/1393 train_time:114472ms step_avg:126.07ms
step:919/1393 train_time:114604ms step_avg:126.08ms
step:920/1393 train_time:114735ms step_avg:126.08ms
step:921/1393 train_time:114864ms step_avg:126.09ms
step:922/1393 train_time:114995ms step_avg:126.09ms
step:923/1393 train_time:115124ms step_avg:126.09ms
step:924/1393 train_time:115254ms step_avg:126.10ms
step:925/1393 train_time:115384ms step_avg:126.10ms
step:926/1393 train_time:115515ms step_avg:126.11ms
step:927/1393 train_time:115645ms step_avg:126.11ms
step:928/1393 train_time:115775ms step_avg:126.12ms
step:929/1393 train_time:115906ms step_avg:126.12ms
step:930/1393 train_time:116036ms step_avg:126.13ms
step:931/1393 train_time:116168ms step_avg:126.13ms
step:932/1393 train_time:116300ms step_avg:126.14ms
step:933/1393 train_time:116432ms step_avg:126.15ms
step:934/1393 train_time:116565ms step_avg:126.15ms
step:935/1393 train_time:116698ms step_avg:126.16ms
step:936/1393 train_time:116830ms step_avg:126.17ms
step:937/1393 train_time:116965ms step_avg:126.18ms
step:938/1393 train_time:117096ms step_avg:126.18ms
step:939/1393 train_time:117227ms step_avg:126.19ms
step:940/1393 train_time:117360ms step_avg:126.19ms
step:941/1393 train_time:117493ms step_avg:126.20ms
step:942/1393 train_time:117625ms step_avg:126.21ms
step:943/1393 train_time:117759ms step_avg:126.22ms
step:944/1393 train_time:117893ms step_avg:126.22ms
step:945/1393 train_time:118025ms step_avg:126.23ms
step:946/1393 train_time:118156ms step_avg:126.24ms
step:947/1393 train_time:118289ms step_avg:126.24ms
step:948/1393 train_time:118421ms step_avg:126.25ms
step:949/1393 train_time:118554ms step_avg:126.26ms
step:950/1393 train_time:118686ms step_avg:126.26ms
step:951/1393 train_time:118820ms step_avg:126.27ms
step:952/1393 train_time:118951ms step_avg:126.28ms
step:953/1393 train_time:119083ms step_avg:126.28ms
step:954/1393 train_time:119216ms step_avg:126.29ms
step:955/1393 train_time:119348ms step_avg:126.29ms
step:956/1393 train_time:119482ms step_avg:126.30ms
step:957/1393 train_time:119613ms step_avg:126.31ms
step:958/1393 train_time:119745ms step_avg:126.31ms
step:959/1393 train_time:119877ms step_avg:126.32ms
step:960/1393 train_time:120009ms step_avg:126.33ms
step:961/1393 train_time:120141ms step_avg:126.33ms
step:962/1393 train_time:120274ms step_avg:126.34ms
step:963/1393 train_time:120407ms step_avg:126.34ms
step:964/1393 train_time:120539ms step_avg:126.35ms
step:965/1393 train_time:120671ms step_avg:126.36ms
step:966/1393 train_time:120802ms step_avg:126.36ms
step:967/1393 train_time:120934ms step_avg:126.37ms
step:968/1393 train_time:121066ms step_avg:126.37ms
step:969/1393 train_time:121199ms step_avg:126.38ms
step:970/1393 train_time:121331ms step_avg:126.39ms
step:971/1393 train_time:121463ms step_avg:126.39ms
step:972/1393 train_time:121596ms step_avg:126.40ms
step:973/1393 train_time:121728ms step_avg:126.40ms
step:974/1393 train_time:121860ms step_avg:126.41ms
step:975/1393 train_time:121991ms step_avg:126.42ms
step:976/1393 train_time:122122ms step_avg:126.42ms
step:977/1393 train_time:122254ms step_avg:126.43ms
step:978/1393 train_time:122385ms step_avg:126.43ms
step:979/1393 train_time:122517ms step_avg:126.44ms
step:980/1393 train_time:122650ms step_avg:126.44ms
step:981/1393 train_time:122781ms step_avg:126.45ms
step:982/1393 train_time:122912ms step_avg:126.45ms
step:983/1393 train_time:123043ms step_avg:126.46ms
step:984/1393 train_time:123176ms step_avg:126.46ms
step:985/1393 train_time:123308ms step_avg:126.47ms
step:986/1393 train_time:123442ms step_avg:126.48ms
step:987/1393 train_time:123574ms step_avg:126.48ms
step:988/1393 train_time:123705ms step_avg:126.49ms
step:989/1393 train_time:123838ms step_avg:126.49ms
step:990/1393 train_time:123970ms step_avg:126.50ms
step:991/1393 train_time:124102ms step_avg:126.51ms
step:992/1393 train_time:124236ms step_avg:126.51ms
step:993/1393 train_time:124370ms step_avg:126.52ms
step:994/1393 train_time:124502ms step_avg:126.53ms
step:995/1393 train_time:124633ms step_avg:126.53ms
step:996/1393 train_time:124764ms step_avg:126.54ms
step:997/1393 train_time:124895ms step_avg:126.54ms
step:998/1393 train_time:125027ms step_avg:126.55ms
step:999/1393 train_time:125159ms step_avg:126.55ms
step:1000/1393 train_time:125292ms step_avg:126.56ms
step:1000/1393 val_loss:3.4146 train_time:125422ms step_avg:126.69ms
step:1001/1393 train_time:125443ms step_avg:126.58ms
step:1002/1393 train_time:125563ms step_avg:126.58ms
step:1003/1393 train_time:125696ms step_avg:126.58ms
step:1004/1393 train_time:125827ms step_avg:126.59ms
step:1005/1393 train_time:125960ms step_avg:126.59ms
step:1006/1393 train_time:126090ms step_avg:126.60ms
step:1007/1393 train_time:126221ms step_avg:126.60ms
step:1008/1393 train_time:126352ms step_avg:126.60ms
step:1009/1393 train_time:126487ms step_avg:126.61ms
step:1010/1393 train_time:126620ms step_avg:126.62ms
step:1011/1393 train_time:126753ms step_avg:126.63ms
step:1012/1393 train_time:126884ms step_avg:126.63ms
step:1013/1393 train_time:127016ms step_avg:126.64ms
step:1014/1393 train_time:127147ms step_avg:126.64ms
step:1015/1393 train_time:127279ms step_avg:126.65ms
step:1016/1393 train_time:127410ms step_avg:126.65ms
step:1017/1393 train_time:127542ms step_avg:126.66ms
step:1018/1393 train_time:127673ms step_avg:126.66ms
step:1019/1393 train_time:127807ms step_avg:126.67ms
step:1020/1393 train_time:127938ms step_avg:126.67ms
step:1021/1393 train_time:128070ms step_avg:126.68ms
step:1022/1393 train_time:128201ms step_avg:126.68ms
step:1023/1393 train_time:128333ms step_avg:126.69ms
step:1024/1393 train_time:128466ms step_avg:126.69ms
step:1025/1393 train_time:128599ms step_avg:126.70ms
step:1026/1393 train_time:128731ms step_avg:126.70ms
step:1027/1393 train_time:128862ms step_avg:126.71ms
step:1028/1393 train_time:128995ms step_avg:126.71ms
step:1029/1393 train_time:129128ms step_avg:126.72ms
step:1030/1393 train_time:129260ms step_avg:126.73ms
step:1031/1393 train_time:129391ms step_avg:126.73ms
step:1032/1393 train_time:129523ms step_avg:126.74ms
step:1033/1393 train_time:129655ms step_avg:126.74ms
step:1034/1393 train_time:129787ms step_avg:126.75ms
step:1035/1393 train_time:129921ms step_avg:126.75ms
step:1036/1393 train_time:130053ms step_avg:126.76ms
step:1037/1393 train_time:130185ms step_avg:126.76ms
step:1038/1393 train_time:130318ms step_avg:126.77ms
step:1039/1393 train_time:130449ms step_avg:126.77ms
step:1040/1393 train_time:130580ms step_avg:126.78ms
step:1041/1393 train_time:130713ms step_avg:126.78ms
step:1042/1393 train_time:130846ms step_avg:126.79ms
step:1043/1393 train_time:130980ms step_avg:126.80ms
step:1044/1393 train_time:131114ms step_avg:126.80ms
step:1045/1393 train_time:131247ms step_avg:126.81ms
step:1046/1393 train_time:131380ms step_avg:126.81ms
step:1047/1393 train_time:131511ms step_avg:126.82ms
step:1048/1393 train_time:131643ms step_avg:126.82ms
step:1049/1393 train_time:131776ms step_avg:126.83ms
step:1050/1393 train_time:131908ms step_avg:126.83ms
step:1051/1393 train_time:132042ms step_avg:126.84ms
step:1052/1393 train_time:132176ms step_avg:126.85ms
step:1053/1393 train_time:132309ms step_avg:126.85ms
step:1054/1393 train_time:132440ms step_avg:126.86ms
step:1055/1393 train_time:132573ms step_avg:126.86ms
step:1056/1393 train_time:132705ms step_avg:126.87ms
step:1057/1393 train_time:132836ms step_avg:126.87ms
step:1058/1393 train_time:132969ms step_avg:126.88ms
step:1059/1393 train_time:133101ms step_avg:126.88ms
step:1060/1393 train_time:133235ms step_avg:126.89ms
step:1061/1393 train_time:133366ms step_avg:126.89ms
step:1062/1393 train_time:133499ms step_avg:126.90ms
step:1063/1393 train_time:133630ms step_avg:126.90ms
step:1064/1393 train_time:133762ms step_avg:126.91ms
step:1065/1393 train_time:133894ms step_avg:126.91ms
step:1066/1393 train_time:134027ms step_avg:126.92ms
step:1067/1393 train_time:134159ms step_avg:126.92ms
step:1068/1393 train_time:134291ms step_avg:126.93ms
step:1069/1393 train_time:134425ms step_avg:126.94ms
step:1070/1393 train_time:134557ms step_avg:126.94ms
step:1071/1393 train_time:134691ms step_avg:126.95ms
step:1072/1393 train_time:134823ms step_avg:126.95ms
step:1073/1393 train_time:134955ms step_avg:126.96ms
step:1074/1393 train_time:135086ms step_avg:126.96ms
step:1075/1393 train_time:135218ms step_avg:126.97ms
step:1076/1393 train_time:135349ms step_avg:126.97ms
step:1077/1393 train_time:135481ms step_avg:126.97ms
step:1078/1393 train_time:135613ms step_avg:126.98ms
step:1079/1393 train_time:135749ms step_avg:126.99ms
step:1080/1393 train_time:135881ms step_avg:126.99ms
step:1081/1393 train_time:136014ms step_avg:127.00ms
step:1082/1393 train_time:136146ms step_avg:127.00ms
step:1083/1393 train_time:136278ms step_avg:127.01ms
step:1084/1393 train_time:136411ms step_avg:127.01ms
step:1085/1393 train_time:136542ms step_avg:127.02ms
step:1086/1393 train_time:136675ms step_avg:127.02ms
step:1087/1393 train_time:136807ms step_avg:127.03ms
step:1088/1393 train_time:136940ms step_avg:127.03ms
step:1089/1393 train_time:137073ms step_avg:127.04ms
step:1090/1393 train_time:137207ms step_avg:127.04ms
step:1091/1393 train_time:137339ms step_avg:127.05ms
step:1092/1393 train_time:137471ms step_avg:127.05ms
step:1093/1393 train_time:137603ms step_avg:127.06ms
step:1094/1393 train_time:137736ms step_avg:127.06ms
step:1095/1393 train_time:137867ms step_avg:127.07ms
step:1096/1393 train_time:138000ms step_avg:127.07ms
step:1097/1393 train_time:138132ms step_avg:127.08ms
step:1098/1393 train_time:138265ms step_avg:127.08ms
step:1099/1393 train_time:138396ms step_avg:127.09ms
step:1100/1393 train_time:138528ms step_avg:127.09ms
step:1101/1393 train_time:138662ms step_avg:127.10ms
step:1102/1393 train_time:138794ms step_avg:127.10ms
step:1103/1393 train_time:138929ms step_avg:127.11ms
step:1104/1393 train_time:139061ms step_avg:127.11ms
step:1105/1393 train_time:139196ms step_avg:127.12ms
step:1106/1393 train_time:139328ms step_avg:127.12ms
step:1107/1393 train_time:139460ms step_avg:127.13ms
step:1108/1393 train_time:139595ms step_avg:127.14ms
step:1109/1393 train_time:139726ms step_avg:127.14ms
step:1110/1393 train_time:139860ms step_avg:127.15ms
step:1111/1393 train_time:139992ms step_avg:127.15ms
step:1112/1393 train_time:140123ms step_avg:127.15ms
step:1113/1393 train_time:140256ms step_avg:127.16ms
step:1114/1393 train_time:140389ms step_avg:127.16ms
step:1115/1393 train_time:140521ms step_avg:127.17ms
step:1116/1393 train_time:140653ms step_avg:127.17ms
step:1117/1393 train_time:140786ms step_avg:127.18ms
step:1118/1393 train_time:140919ms step_avg:127.18ms
step:1119/1393 train_time:141051ms step_avg:127.19ms
step:1120/1393 train_time:141183ms step_avg:127.19ms
step:1121/1393 train_time:141315ms step_avg:127.20ms
step:1122/1393 train_time:141447ms step_avg:127.20ms
step:1123/1393 train_time:141579ms step_avg:127.20ms
step:1124/1393 train_time:141710ms step_avg:127.21ms
step:1125/1393 train_time:141842ms step_avg:127.21ms
step:1125/1393 val_loss:3.3642 train_time:141975ms step_avg:127.33ms
step:1126/1393 train_time:141996ms step_avg:127.24ms
step:1127/1393 train_time:142119ms step_avg:127.23ms
step:1128/1393 train_time:142252ms step_avg:127.24ms
step:1129/1393 train_time:142386ms step_avg:127.24ms
step:1130/1393 train_time:142517ms step_avg:127.25ms
step:1131/1393 train_time:142651ms step_avg:127.25ms
step:1132/1393 train_time:142782ms step_avg:127.26ms
step:1133/1393 train_time:142912ms step_avg:127.26ms
step:1134/1393 train_time:143047ms step_avg:127.27ms
step:1135/1393 train_time:143180ms step_avg:127.27ms
step:1136/1393 train_time:143317ms step_avg:127.28ms
step:1137/1393 train_time:143449ms step_avg:127.28ms
step:1138/1393 train_time:143584ms step_avg:127.29ms
step:1139/1393 train_time:143717ms step_avg:127.30ms
step:1140/1393 train_time:143850ms step_avg:127.30ms
step:1141/1393 train_time:143984ms step_avg:127.31ms
step:1142/1393 train_time:144117ms step_avg:127.31ms
step:1143/1393 train_time:144254ms step_avg:127.32ms
step:1144/1393 train_time:144387ms step_avg:127.33ms
step:1145/1393 train_time:144522ms step_avg:127.33ms
step:1146/1393 train_time:144655ms step_avg:127.34ms
step:1147/1393 train_time:144789ms step_avg:127.34ms
step:1148/1393 train_time:144923ms step_avg:127.35ms
step:1149/1393 train_time:145056ms step_avg:127.35ms
step:1150/1393 train_time:145190ms step_avg:127.36ms
step:1151/1393 train_time:145326ms step_avg:127.37ms
step:1152/1393 train_time:145459ms step_avg:127.37ms
step:1153/1393 train_time:145595ms step_avg:127.38ms
step:1154/1393 train_time:145729ms step_avg:127.39ms
step:1155/1393 train_time:145863ms step_avg:127.39ms
step:1156/1393 train_time:146000ms step_avg:127.40ms
step:1157/1393 train_time:146133ms step_avg:127.40ms
step:1158/1393 train_time:146267ms step_avg:127.41ms
step:1159/1393 train_time:146401ms step_avg:127.42ms
step:1160/1393 train_time:146535ms step_avg:127.42ms
step:1161/1393 train_time:146669ms step_avg:127.43ms
step:1162/1393 train_time:146802ms step_avg:127.43ms
step:1163/1393 train_time:146938ms step_avg:127.44ms
step:1164/1393 train_time:147069ms step_avg:127.44ms
step:1165/1393 train_time:147202ms step_avg:127.45ms
step:1166/1393 train_time:147336ms step_avg:127.45ms
step:1167/1393 train_time:147469ms step_avg:127.46ms
step:1168/1393 train_time:147604ms step_avg:127.46ms
step:1169/1393 train_time:147736ms step_avg:127.47ms
step:1170/1393 train_time:147869ms step_avg:127.47ms
step:1171/1393 train_time:148003ms step_avg:127.48ms
step:1172/1393 train_time:148137ms step_avg:127.48ms
step:1173/1393 train_time:148271ms step_avg:127.49ms
step:1174/1393 train_time:148409ms step_avg:127.50ms
step:1175/1393 train_time:148545ms step_avg:127.51ms
step:1176/1393 train_time:148678ms step_avg:127.51ms
step:1177/1393 train_time:148814ms step_avg:127.52ms
step:1178/1393 train_time:148949ms step_avg:127.52ms
step:1179/1393 train_time:149081ms step_avg:127.53ms
step:1180/1393 train_time:149216ms step_avg:127.54ms
step:1181/1393 train_time:149352ms step_avg:127.54ms
step:1182/1393 train_time:149484ms step_avg:127.55ms
step:1183/1393 train_time:149619ms step_avg:127.55ms
step:1184/1393 train_time:149753ms step_avg:127.56ms
step:1185/1393 train_time:149888ms step_avg:127.56ms
step:1186/1393 train_time:150022ms step_avg:127.57ms
step:1187/1393 train_time:150161ms step_avg:127.58ms
step:1188/1393 train_time:150293ms step_avg:127.58ms
step:1189/1393 train_time:150427ms step_avg:127.59ms
step:1190/1393 train_time:150562ms step_avg:127.59ms
step:1191/1393 train_time:150695ms step_avg:127.60ms
step:1192/1393 train_time:150828ms step_avg:127.60ms
step:1193/1393 train_time:150961ms step_avg:127.61ms
step:1194/1393 train_time:151094ms step_avg:127.61ms
step:1195/1393 train_time:151227ms step_avg:127.62ms
step:1196/1393 train_time:151361ms step_avg:127.62ms
step:1197/1393 train_time:151496ms step_avg:127.63ms
step:1198/1393 train_time:151632ms step_avg:127.64ms
step:1199/1393 train_time:151766ms step_avg:127.64ms
step:1200/1393 train_time:151898ms step_avg:127.65ms
step:1201/1393 train_time:152030ms step_avg:127.65ms
step:1202/1393 train_time:152166ms step_avg:127.66ms
step:1203/1393 train_time:152303ms step_avg:127.66ms
step:1204/1393 train_time:152436ms step_avg:127.67ms
step:1205/1393 train_time:152570ms step_avg:127.67ms
step:1206/1393 train_time:152706ms step_avg:127.68ms
step:1207/1393 train_time:152839ms step_avg:127.69ms
step:1208/1393 train_time:152974ms step_avg:127.69ms
step:1209/1393 train_time:153107ms step_avg:127.70ms
step:1210/1393 train_time:153244ms step_avg:127.70ms
step:1211/1393 train_time:153378ms step_avg:127.71ms
step:1212/1393 train_time:153511ms step_avg:127.71ms
step:1213/1393 train_time:153644ms step_avg:127.72ms
step:1214/1393 train_time:153778ms step_avg:127.72ms
step:1215/1393 train_time:153913ms step_avg:127.73ms
step:1216/1393 train_time:154045ms step_avg:127.73ms
step:1217/1393 train_time:154180ms step_avg:127.74ms
step:1218/1393 train_time:154313ms step_avg:127.74ms
step:1219/1393 train_time:154447ms step_avg:127.75ms
step:1220/1393 train_time:154580ms step_avg:127.75ms
step:1221/1393 train_time:154713ms step_avg:127.76ms
step:1222/1393 train_time:154847ms step_avg:127.76ms
step:1223/1393 train_time:154981ms step_avg:127.77ms
step:1224/1393 train_time:155115ms step_avg:127.77ms
step:1225/1393 train_time:155250ms step_avg:127.78ms
step:1226/1393 train_time:155384ms step_avg:127.78ms
step:1227/1393 train_time:155517ms step_avg:127.79ms
step:1228/1393 train_time:155652ms step_avg:127.79ms
step:1229/1393 train_time:155785ms step_avg:127.80ms
step:1230/1393 train_time:155920ms step_avg:127.80ms
step:1231/1393 train_time:156055ms step_avg:127.81ms
step:1232/1393 train_time:156191ms step_avg:127.82ms
step:1233/1393 train_time:156324ms step_avg:127.82ms
step:1234/1393 train_time:156460ms step_avg:127.83ms
step:1235/1393 train_time:156593ms step_avg:127.83ms
step:1236/1393 train_time:156727ms step_avg:127.84ms
step:1237/1393 train_time:156859ms step_avg:127.84ms
step:1238/1393 train_time:156997ms step_avg:127.85ms
step:1239/1393 train_time:157130ms step_avg:127.85ms
step:1240/1393 train_time:157264ms step_avg:127.86ms
step:1241/1393 train_time:157401ms step_avg:127.86ms
step:1242/1393 train_time:157535ms step_avg:127.87ms
step:1243/1393 train_time:157669ms step_avg:127.87ms
step:1244/1393 train_time:157803ms step_avg:127.88ms
step:1245/1393 train_time:157937ms step_avg:127.88ms
step:1246/1393 train_time:158070ms step_avg:127.89ms
step:1247/1393 train_time:158205ms step_avg:127.89ms
step:1248/1393 train_time:158338ms step_avg:127.90ms
step:1249/1393 train_time:158470ms step_avg:127.90ms
step:1250/1393 train_time:158603ms step_avg:127.91ms
step:1250/1393 val_loss:3.3179 train_time:158736ms step_avg:128.01ms
step:1251/1393 train_time:158759ms step_avg:127.93ms
step:1252/1393 train_time:158881ms step_avg:127.92ms
step:1253/1393 train_time:159014ms step_avg:127.93ms
step:1254/1393 train_time:159146ms step_avg:127.93ms
step:1255/1393 train_time:159284ms step_avg:127.94ms
step:1256/1393 train_time:159416ms step_avg:127.94ms
step:1257/1393 train_time:159549ms step_avg:127.95ms
step:1258/1393 train_time:159682ms step_avg:127.95ms
step:1259/1393 train_time:159819ms step_avg:127.96ms
step:1260/1393 train_time:159956ms step_avg:127.96ms
step:1261/1393 train_time:160086ms step_avg:127.97ms
step:1262/1393 train_time:160221ms step_avg:127.97ms
step:1263/1393 train_time:160355ms step_avg:127.98ms
step:1264/1393 train_time:160489ms step_avg:127.98ms
step:1265/1393 train_time:160622ms step_avg:127.99ms
step:1266/1393 train_time:160756ms step_avg:127.99ms
step:1267/1393 train_time:160890ms step_avg:128.00ms
step:1268/1393 train_time:161024ms step_avg:128.00ms
step:1269/1393 train_time:161159ms step_avg:128.01ms
step:1270/1393 train_time:161293ms step_avg:128.01ms
step:1271/1393 train_time:161427ms step_avg:128.02ms
step:1272/1393 train_time:161564ms step_avg:128.02ms
step:1273/1393 train_time:161692ms step_avg:128.02ms
step:1274/1393 train_time:161827ms step_avg:128.03ms
step:1275/1393 train_time:161962ms step_avg:128.03ms
step:1276/1393 train_time:162096ms step_avg:128.04ms
step:1277/1393 train_time:162229ms step_avg:128.04ms
step:1278/1393 train_time:162363ms step_avg:128.05ms
step:1279/1393 train_time:162496ms step_avg:128.05ms
step:1280/1393 train_time:162634ms step_avg:128.06ms
step:1281/1393 train_time:162769ms step_avg:128.06ms
step:1282/1393 train_time:162902ms step_avg:128.07ms
step:1283/1393 train_time:163035ms step_avg:128.07ms
step:1284/1393 train_time:163170ms step_avg:128.08ms
step:1285/1393 train_time:163304ms step_avg:128.08ms
step:1286/1393 train_time:163438ms step_avg:128.09ms
step:1287/1393 train_time:163572ms step_avg:128.09ms
step:1288/1393 train_time:163706ms step_avg:128.10ms
step:1289/1393 train_time:163842ms step_avg:128.10ms
step:1290/1393 train_time:163977ms step_avg:128.11ms
step:1291/1393 train_time:164113ms step_avg:128.11ms
step:1292/1393 train_time:164247ms step_avg:128.12ms
step:1293/1393 train_time:164383ms step_avg:128.12ms
step:1294/1393 train_time:164516ms step_avg:128.13ms
step:1295/1393 train_time:164649ms step_avg:128.13ms
step:1296/1393 train_time:164786ms step_avg:128.14ms
step:1297/1393 train_time:164920ms step_avg:128.14ms
step:1298/1393 train_time:165053ms step_avg:128.15ms
step:1299/1393 train_time:165187ms step_avg:128.15ms
step:1300/1393 train_time:165319ms step_avg:128.15ms
step:1301/1393 train_time:165454ms step_avg:128.16ms
step:1302/1393 train_time:165588ms step_avg:128.16ms
step:1303/1393 train_time:165724ms step_avg:128.17ms
step:1304/1393 train_time:165859ms step_avg:128.18ms
step:1305/1393 train_time:165993ms step_avg:128.18ms
step:1306/1393 train_time:166128ms step_avg:128.18ms
step:1307/1393 train_time:166262ms step_avg:128.19ms
step:1308/1393 train_time:166397ms step_avg:128.19ms
step:1309/1393 train_time:166531ms step_avg:128.20ms
step:1310/1393 train_time:166664ms step_avg:128.20ms
step:1311/1393 train_time:166797ms step_avg:128.21ms
step:1312/1393 train_time:166931ms step_avg:128.21ms
step:1313/1393 train_time:167066ms step_avg:128.22ms
step:1314/1393 train_time:167199ms step_avg:128.22ms
step:1315/1393 train_time:167334ms step_avg:128.23ms
step:1316/1393 train_time:167467ms step_avg:128.23ms
step:1317/1393 train_time:167600ms step_avg:128.23ms
step:1318/1393 train_time:167734ms step_avg:128.24ms
step:1319/1393 train_time:167870ms step_avg:128.24ms
step:1320/1393 train_time:168004ms step_avg:128.25ms
step:1321/1393 train_time:168137ms step_avg:128.25ms
step:1322/1393 train_time:168275ms step_avg:128.26ms
step:1323/1393 train_time:168409ms step_avg:128.26ms
step:1324/1393 train_time:168542ms step_avg:128.27ms
step:1325/1393 train_time:168677ms step_avg:128.27ms
step:1326/1393 train_time:168812ms step_avg:128.28ms
step:1327/1393 train_time:168946ms step_avg:128.28ms
step:1328/1393 train_time:169079ms step_avg:128.28ms
step:1329/1393 train_time:169217ms step_avg:128.29ms
step:1330/1393 train_time:169351ms step_avg:128.30ms
step:1331/1393 train_time:169489ms step_avg:128.30ms
step:1332/1393 train_time:169624ms step_avg:128.31ms
step:1333/1393 train_time:169759ms step_avg:128.31ms
step:1334/1393 train_time:169892ms step_avg:128.32ms
step:1335/1393 train_time:170025ms step_avg:128.32ms
step:1336/1393 train_time:170161ms step_avg:128.33ms
step:1337/1393 train_time:170295ms step_avg:128.33ms
step:1338/1393 train_time:170429ms step_avg:128.34ms
step:1339/1393 train_time:170565ms step_avg:128.34ms
step:1340/1393 train_time:170699ms step_avg:128.35ms
step:1341/1393 train_time:170831ms step_avg:128.35ms
step:1342/1393 train_time:170965ms step_avg:128.35ms
step:1343/1393 train_time:171098ms step_avg:128.36ms
step:1344/1393 train_time:171232ms step_avg:128.36ms
step:1345/1393 train_time:171367ms step_avg:128.36ms
step:1346/1393 train_time:171501ms step_avg:128.37ms
step:1347/1393 train_time:171637ms step_avg:128.37ms
step:1348/1393 train_time:171771ms step_avg:128.38ms
step:1349/1393 train_time:171906ms step_avg:128.38ms
step:1350/1393 train_time:172040ms step_avg:128.39ms
step:1351/1393 train_time:172175ms step_avg:128.39ms
step:1352/1393 train_time:172312ms step_avg:128.40ms
step:1353/1393 train_time:172449ms step_avg:128.41ms
step:1354/1393 train_time:172584ms step_avg:128.41ms
step:1355/1393 train_time:172718ms step_avg:128.42ms
step:1356/1393 train_time:172851ms step_avg:128.42ms
step:1357/1393 train_time:172987ms step_avg:128.42ms
step:1358/1393 train_time:173123ms step_avg:128.43ms
step:1359/1393 train_time:173258ms step_avg:128.43ms
step:1360/1393 train_time:173394ms step_avg:128.44ms
step:1361/1393 train_time:173531ms step_avg:128.45ms
step:1362/1393 train_time:173668ms step_avg:128.45ms
step:1363/1393 train_time:173805ms step_avg:128.46ms
step:1364/1393 train_time:173939ms step_avg:128.46ms
step:1365/1393 train_time:174072ms step_avg:128.47ms
step:1366/1393 train_time:174207ms step_avg:128.47ms
step:1367/1393 train_time:174343ms step_avg:128.48ms
step:1368/1393 train_time:174478ms step_avg:128.48ms
step:1369/1393 train_time:174616ms step_avg:128.49ms
step:1370/1393 train_time:174755ms step_avg:128.50ms
step:1371/1393 train_time:174891ms step_avg:128.50ms
step:1372/1393 train_time:175029ms step_avg:128.51ms
step:1373/1393 train_time:175163ms step_avg:128.51ms
step:1374/1393 train_time:175300ms step_avg:128.52ms
step:1375/1393 train_time:175434ms step_avg:128.52ms
step:1375/1393 val_loss:3.2837 train_time:175566ms step_avg:128.62ms
step:1376/1393 train_time:175588ms step_avg:128.54ms
step:1377/1393 train_time:175711ms step_avg:128.54ms
step:1378/1393 train_time:175846ms step_avg:128.54ms
step:1379/1393 train_time:175980ms step_avg:128.55ms
step:1380/1393 train_time:176115ms step_avg:128.55ms
step:1381/1393 train_time:176252ms step_avg:128.56ms
step:1382/1393 train_time:176387ms step_avg:128.56ms
step:1383/1393 train_time:176521ms step_avg:128.57ms
step:1384/1393 train_time:176659ms step_avg:128.57ms
step:1385/1393 train_time:176794ms step_avg:128.58ms
step:1386/1393 train_time:176930ms step_avg:128.58ms
step:1387/1393 train_time:177065ms step_avg:128.59ms
step:1388/1393 train_time:177199ms step_avg:128.59ms
step:1389/1393 train_time:177335ms step_avg:128.60ms
step:1390/1393 train_time:177469ms step_avg:128.60ms
step:1391/1393 train_time:177604ms step_avg:128.61ms
step:1392/1393 train_time:177740ms step_avg:128.61ms
step:1393/1393 train_time:177875ms step_avg:128.62ms
step:1393/1393 val_loss:3.2800 train_time:178009ms step_avg:128.71ms
peak memory allocated: 37653 MiB reserved: 41736 MiB
