import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:00:50 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23544ms step_avg:nanms
step:2/1770 train_time:23949ms step_avg:nanms
step:3/1770 train_time:24110ms step_avg:nanms
step:4/1770 train_time:24219ms step_avg:nanms
step:5/1770 train_time:24312ms step_avg:nanms
step:6/1770 train_time:24406ms step_avg:nanms
step:7/1770 train_time:24500ms step_avg:nanms
step:8/1770 train_time:24593ms step_avg:nanms
step:9/1770 train_time:24687ms step_avg:nanms
step:10/1770 train_time:24781ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.81ms
step:14/1770 train_time:377ms step_avg:94.20ms
step:15/1770 train_time:471ms step_avg:94.25ms
step:16/1770 train_time:565ms step_avg:94.22ms
step:17/1770 train_time:660ms step_avg:94.28ms
step:18/1770 train_time:754ms step_avg:94.25ms
step:19/1770 train_time:848ms step_avg:94.19ms
step:20/1770 train_time:942ms step_avg:94.18ms
step:21/1770 train_time:1036ms step_avg:94.18ms
step:22/1770 train_time:1129ms step_avg:94.12ms
step:23/1770 train_time:1223ms step_avg:94.10ms
step:24/1770 train_time:1317ms step_avg:94.09ms
step:25/1770 train_time:1411ms step_avg:94.09ms
step:26/1770 train_time:1506ms step_avg:94.11ms
step:27/1770 train_time:1600ms step_avg:94.10ms
step:28/1770 train_time:1695ms step_avg:94.14ms
step:29/1770 train_time:1789ms step_avg:94.14ms
step:30/1770 train_time:1883ms step_avg:94.15ms
step:31/1770 train_time:1977ms step_avg:94.15ms
step:32/1770 train_time:2071ms step_avg:94.13ms
step:33/1770 train_time:2165ms step_avg:94.11ms
step:34/1770 train_time:2259ms step_avg:94.13ms
step:35/1770 train_time:2354ms step_avg:94.15ms
step:36/1770 train_time:2447ms step_avg:94.13ms
step:37/1770 train_time:2541ms step_avg:94.13ms
step:38/1770 train_time:2636ms step_avg:94.14ms
step:39/1770 train_time:2730ms step_avg:94.13ms
step:40/1770 train_time:2823ms step_avg:94.11ms
step:41/1770 train_time:2917ms step_avg:94.11ms
step:42/1770 train_time:3011ms step_avg:94.10ms
step:43/1770 train_time:3104ms step_avg:94.07ms
step:44/1770 train_time:3199ms step_avg:94.08ms
step:45/1770 train_time:3293ms step_avg:94.07ms
step:46/1770 train_time:3387ms step_avg:94.07ms
step:47/1770 train_time:3480ms step_avg:94.07ms
step:48/1770 train_time:3575ms step_avg:94.08ms
step:49/1770 train_time:3669ms step_avg:94.08ms
step:50/1770 train_time:3763ms step_avg:94.08ms
step:51/1770 train_time:3857ms step_avg:94.08ms
step:52/1770 train_time:3951ms step_avg:94.08ms
step:53/1770 train_time:4045ms step_avg:94.07ms
step:54/1770 train_time:4139ms step_avg:94.06ms
step:55/1770 train_time:4232ms step_avg:94.05ms
step:56/1770 train_time:4326ms step_avg:94.04ms
step:57/1770 train_time:4420ms step_avg:94.04ms
step:58/1770 train_time:4513ms step_avg:94.02ms
step:59/1770 train_time:4608ms step_avg:94.03ms
step:60/1770 train_time:4702ms step_avg:94.04ms
step:61/1770 train_time:4796ms step_avg:94.03ms
step:62/1770 train_time:4889ms step_avg:94.03ms
step:63/1770 train_time:4984ms step_avg:94.03ms
step:64/1770 train_time:5079ms step_avg:94.05ms
step:65/1770 train_time:5173ms step_avg:94.05ms
step:66/1770 train_time:5266ms step_avg:94.04ms
step:67/1770 train_time:5361ms step_avg:94.05ms
step:68/1770 train_time:5455ms step_avg:94.05ms
step:69/1770 train_time:5549ms step_avg:94.05ms
step:70/1770 train_time:5643ms step_avg:94.05ms
step:71/1770 train_time:5738ms step_avg:94.06ms
step:72/1770 train_time:5832ms step_avg:94.07ms
step:73/1770 train_time:5926ms step_avg:94.06ms
step:74/1770 train_time:6019ms step_avg:94.05ms
step:75/1770 train_time:6114ms step_avg:94.06ms
step:76/1770 train_time:6207ms step_avg:94.05ms
step:77/1770 train_time:6301ms step_avg:94.05ms
step:78/1770 train_time:6396ms step_avg:94.06ms
step:79/1770 train_time:6490ms step_avg:94.05ms
step:80/1770 train_time:6584ms step_avg:94.06ms
step:81/1770 train_time:6678ms step_avg:94.05ms
step:82/1770 train_time:6771ms step_avg:94.05ms
step:83/1770 train_time:6865ms step_avg:94.04ms
step:84/1770 train_time:6959ms step_avg:94.04ms
step:85/1770 train_time:7053ms step_avg:94.04ms
step:86/1770 train_time:7147ms step_avg:94.03ms
step:87/1770 train_time:7241ms step_avg:94.04ms
step:88/1770 train_time:7335ms step_avg:94.04ms
step:89/1770 train_time:7428ms step_avg:94.03ms
step:90/1770 train_time:7522ms step_avg:94.02ms
step:91/1770 train_time:7616ms step_avg:94.02ms
step:92/1770 train_time:7709ms step_avg:94.02ms
step:93/1770 train_time:7804ms step_avg:94.02ms
step:94/1770 train_time:7897ms step_avg:94.02ms
step:95/1770 train_time:7991ms step_avg:94.02ms
step:96/1770 train_time:8085ms step_avg:94.02ms
step:97/1770 train_time:8179ms step_avg:94.01ms
step:98/1770 train_time:8274ms step_avg:94.02ms
step:99/1770 train_time:8368ms step_avg:94.02ms
step:100/1770 train_time:8463ms step_avg:94.03ms
step:101/1770 train_time:8557ms step_avg:94.03ms
step:102/1770 train_time:8651ms step_avg:94.03ms
step:103/1770 train_time:8745ms step_avg:94.03ms
step:104/1770 train_time:8839ms step_avg:94.03ms
step:105/1770 train_time:8933ms step_avg:94.03ms
step:106/1770 train_time:9027ms step_avg:94.03ms
step:107/1770 train_time:9121ms step_avg:94.03ms
step:108/1770 train_time:9215ms step_avg:94.03ms
step:109/1770 train_time:9308ms step_avg:94.02ms
step:110/1770 train_time:9402ms step_avg:94.02ms
step:111/1770 train_time:9497ms step_avg:94.03ms
step:112/1770 train_time:9591ms step_avg:94.03ms
step:113/1770 train_time:9684ms step_avg:94.02ms
step:114/1770 train_time:9778ms step_avg:94.02ms
step:115/1770 train_time:9872ms step_avg:94.02ms
step:116/1770 train_time:9967ms step_avg:94.03ms
step:117/1770 train_time:10061ms step_avg:94.03ms
step:118/1770 train_time:10155ms step_avg:94.03ms
step:119/1770 train_time:10249ms step_avg:94.03ms
step:120/1770 train_time:10343ms step_avg:94.03ms
step:121/1770 train_time:10437ms step_avg:94.03ms
step:122/1770 train_time:10531ms step_avg:94.03ms
step:123/1770 train_time:10625ms step_avg:94.03ms
step:124/1770 train_time:10719ms step_avg:94.03ms
step:125/1770 train_time:10813ms step_avg:94.03ms
step:125/1770 val_loss:4.6505 train_time:10905ms step_avg:94.83ms
step:126/1770 train_time:10928ms step_avg:94.21ms
step:127/1770 train_time:11012ms step_avg:94.12ms
step:128/1770 train_time:11111ms step_avg:94.16ms
step:129/1770 train_time:11206ms step_avg:94.17ms
step:130/1770 train_time:11300ms step_avg:94.17ms
step:131/1770 train_time:11394ms step_avg:94.17ms
step:132/1770 train_time:11488ms step_avg:94.16ms
step:133/1770 train_time:11581ms step_avg:94.16ms
step:134/1770 train_time:11676ms step_avg:94.16ms
step:135/1770 train_time:11770ms step_avg:94.16ms
step:136/1770 train_time:11864ms step_avg:94.16ms
step:137/1770 train_time:11958ms step_avg:94.16ms
step:138/1770 train_time:12053ms step_avg:94.17ms
step:139/1770 train_time:12150ms step_avg:94.19ms
step:140/1770 train_time:12244ms step_avg:94.18ms
step:141/1770 train_time:12338ms step_avg:94.19ms
step:142/1770 train_time:12433ms step_avg:94.19ms
step:143/1770 train_time:12527ms step_avg:94.19ms
step:144/1770 train_time:12621ms step_avg:94.19ms
step:145/1770 train_time:12716ms step_avg:94.19ms
step:146/1770 train_time:12811ms step_avg:94.20ms
step:147/1770 train_time:12906ms step_avg:94.20ms
step:148/1770 train_time:13000ms step_avg:94.20ms
step:149/1770 train_time:13095ms step_avg:94.21ms
step:150/1770 train_time:13190ms step_avg:94.21ms
step:151/1770 train_time:13285ms step_avg:94.22ms
step:152/1770 train_time:13379ms step_avg:94.22ms
step:153/1770 train_time:13474ms step_avg:94.22ms
step:154/1770 train_time:13568ms step_avg:94.22ms
step:155/1770 train_time:13662ms step_avg:94.22ms
step:156/1770 train_time:13757ms step_avg:94.22ms
step:157/1770 train_time:13851ms step_avg:94.23ms
step:158/1770 train_time:13946ms step_avg:94.23ms
step:159/1770 train_time:14040ms step_avg:94.23ms
step:160/1770 train_time:14135ms step_avg:94.23ms
step:161/1770 train_time:14230ms step_avg:94.24ms
step:162/1770 train_time:14324ms step_avg:94.24ms
step:163/1770 train_time:14419ms step_avg:94.24ms
step:164/1770 train_time:14514ms step_avg:94.24ms
step:165/1770 train_time:14608ms step_avg:94.25ms
step:166/1770 train_time:14702ms step_avg:94.24ms
step:167/1770 train_time:14796ms step_avg:94.24ms
step:168/1770 train_time:14891ms step_avg:94.25ms
step:169/1770 train_time:14986ms step_avg:94.25ms
step:170/1770 train_time:15081ms step_avg:94.25ms
step:171/1770 train_time:15175ms step_avg:94.26ms
step:172/1770 train_time:15271ms step_avg:94.26ms
step:173/1770 train_time:15366ms step_avg:94.27ms
step:174/1770 train_time:15459ms step_avg:94.26ms
step:175/1770 train_time:15554ms step_avg:94.27ms
step:176/1770 train_time:15649ms step_avg:94.27ms
step:177/1770 train_time:15743ms step_avg:94.27ms
step:178/1770 train_time:15837ms step_avg:94.27ms
step:179/1770 train_time:15932ms step_avg:94.27ms
step:180/1770 train_time:16027ms step_avg:94.28ms
step:181/1770 train_time:16121ms step_avg:94.27ms
step:182/1770 train_time:16215ms step_avg:94.28ms
step:183/1770 train_time:16310ms step_avg:94.28ms
step:184/1770 train_time:16405ms step_avg:94.28ms
step:185/1770 train_time:16499ms step_avg:94.28ms
step:186/1770 train_time:16595ms step_avg:94.29ms
step:187/1770 train_time:16689ms step_avg:94.29ms
step:188/1770 train_time:16783ms step_avg:94.29ms
step:189/1770 train_time:16878ms step_avg:94.29ms
step:190/1770 train_time:16973ms step_avg:94.29ms
step:191/1770 train_time:17068ms step_avg:94.30ms
step:192/1770 train_time:17162ms step_avg:94.30ms
step:193/1770 train_time:17257ms step_avg:94.30ms
step:194/1770 train_time:17352ms step_avg:94.30ms
step:195/1770 train_time:17447ms step_avg:94.31ms
step:196/1770 train_time:17541ms step_avg:94.31ms
step:197/1770 train_time:17636ms step_avg:94.31ms
step:198/1770 train_time:17730ms step_avg:94.31ms
step:199/1770 train_time:17825ms step_avg:94.31ms
step:200/1770 train_time:17919ms step_avg:94.31ms
step:201/1770 train_time:18014ms step_avg:94.31ms
step:202/1770 train_time:18109ms step_avg:94.32ms
step:203/1770 train_time:18203ms step_avg:94.32ms
step:204/1770 train_time:18298ms step_avg:94.32ms
step:205/1770 train_time:18392ms step_avg:94.32ms
step:206/1770 train_time:18487ms step_avg:94.32ms
step:207/1770 train_time:18582ms step_avg:94.32ms
step:208/1770 train_time:18676ms step_avg:94.32ms
step:209/1770 train_time:18772ms step_avg:94.33ms
step:210/1770 train_time:18866ms step_avg:94.33ms
step:211/1770 train_time:18960ms step_avg:94.33ms
step:212/1770 train_time:19055ms step_avg:94.33ms
step:213/1770 train_time:19150ms step_avg:94.34ms
step:214/1770 train_time:19245ms step_avg:94.34ms
step:215/1770 train_time:19340ms step_avg:94.34ms
step:216/1770 train_time:19435ms step_avg:94.34ms
step:217/1770 train_time:19529ms step_avg:94.35ms
step:218/1770 train_time:19623ms step_avg:94.34ms
step:219/1770 train_time:19718ms step_avg:94.35ms
step:220/1770 train_time:19814ms step_avg:94.35ms
step:221/1770 train_time:19909ms step_avg:94.35ms
step:222/1770 train_time:20003ms step_avg:94.35ms
step:223/1770 train_time:20098ms step_avg:94.36ms
step:224/1770 train_time:20193ms step_avg:94.36ms
step:225/1770 train_time:20288ms step_avg:94.36ms
step:226/1770 train_time:20382ms step_avg:94.36ms
step:227/1770 train_time:20477ms step_avg:94.36ms
step:228/1770 train_time:20572ms step_avg:94.37ms
step:229/1770 train_time:20667ms step_avg:94.37ms
step:230/1770 train_time:20761ms step_avg:94.37ms
step:231/1770 train_time:20855ms step_avg:94.37ms
step:232/1770 train_time:20950ms step_avg:94.37ms
step:233/1770 train_time:21044ms step_avg:94.37ms
step:234/1770 train_time:21139ms step_avg:94.37ms
step:235/1770 train_time:21234ms step_avg:94.37ms
step:236/1770 train_time:21329ms step_avg:94.38ms
step:237/1770 train_time:21424ms step_avg:94.38ms
step:238/1770 train_time:21518ms step_avg:94.38ms
step:239/1770 train_time:21614ms step_avg:94.38ms
step:240/1770 train_time:21709ms step_avg:94.39ms
step:241/1770 train_time:21803ms step_avg:94.38ms
step:242/1770 train_time:21897ms step_avg:94.38ms
step:243/1770 train_time:21992ms step_avg:94.38ms
step:244/1770 train_time:22086ms step_avg:94.38ms
step:245/1770 train_time:22180ms step_avg:94.38ms
step:246/1770 train_time:22275ms step_avg:94.39ms
step:247/1770 train_time:22370ms step_avg:94.39ms
step:248/1770 train_time:22465ms step_avg:94.39ms
step:249/1770 train_time:22559ms step_avg:94.39ms
step:250/1770 train_time:22654ms step_avg:94.39ms
step:250/1770 val_loss:4.1142 train_time:22748ms step_avg:94.78ms
step:251/1770 train_time:22772ms step_avg:94.49ms
step:252/1770 train_time:22854ms step_avg:94.44ms
step:253/1770 train_time:22955ms step_avg:94.47ms
step:254/1770 train_time:23051ms step_avg:94.47ms
step:255/1770 train_time:23145ms step_avg:94.47ms
step:256/1770 train_time:23239ms step_avg:94.47ms
step:257/1770 train_time:23334ms step_avg:94.47ms
step:258/1770 train_time:23428ms step_avg:94.47ms
step:259/1770 train_time:23524ms step_avg:94.47ms
step:260/1770 train_time:23617ms step_avg:94.47ms
step:261/1770 train_time:23711ms step_avg:94.47ms
step:262/1770 train_time:23805ms step_avg:94.47ms
step:263/1770 train_time:23900ms step_avg:94.47ms
step:264/1770 train_time:23996ms step_avg:94.47ms
step:265/1770 train_time:24093ms step_avg:94.48ms
step:266/1770 train_time:24188ms step_avg:94.49ms
step:267/1770 train_time:24283ms step_avg:94.49ms
step:268/1770 train_time:24378ms step_avg:94.49ms
step:269/1770 train_time:24473ms step_avg:94.49ms
step:270/1770 train_time:24568ms step_avg:94.49ms
step:271/1770 train_time:24663ms step_avg:94.49ms
step:272/1770 train_time:24759ms step_avg:94.50ms
step:273/1770 train_time:24855ms step_avg:94.51ms
step:274/1770 train_time:24950ms step_avg:94.51ms
step:275/1770 train_time:25045ms step_avg:94.51ms
step:276/1770 train_time:25141ms step_avg:94.51ms
step:277/1770 train_time:25237ms step_avg:94.52ms
step:278/1770 train_time:25333ms step_avg:94.52ms
step:279/1770 train_time:25427ms step_avg:94.53ms
step:280/1770 train_time:25522ms step_avg:94.52ms
step:281/1770 train_time:25617ms step_avg:94.53ms
step:282/1770 train_time:25712ms step_avg:94.53ms
step:283/1770 train_time:25806ms step_avg:94.53ms
step:284/1770 train_time:25901ms step_avg:94.53ms
step:285/1770 train_time:25997ms step_avg:94.53ms
step:286/1770 train_time:26093ms step_avg:94.54ms
step:287/1770 train_time:26188ms step_avg:94.54ms
step:288/1770 train_time:26283ms step_avg:94.54ms
step:289/1770 train_time:26378ms step_avg:94.55ms
step:290/1770 train_time:26474ms step_avg:94.55ms
step:291/1770 train_time:26569ms step_avg:94.55ms
step:292/1770 train_time:26664ms step_avg:94.55ms
step:293/1770 train_time:26759ms step_avg:94.55ms
step:294/1770 train_time:26854ms step_avg:94.56ms
step:295/1770 train_time:26948ms step_avg:94.56ms
step:296/1770 train_time:27043ms step_avg:94.56ms
step:297/1770 train_time:27139ms step_avg:94.56ms
step:298/1770 train_time:27235ms step_avg:94.57ms
step:299/1770 train_time:27330ms step_avg:94.57ms
step:300/1770 train_time:27425ms step_avg:94.57ms
step:301/1770 train_time:27520ms step_avg:94.57ms
step:302/1770 train_time:27616ms step_avg:94.58ms
step:303/1770 train_time:27711ms step_avg:94.58ms
step:304/1770 train_time:27807ms step_avg:94.58ms
step:305/1770 train_time:27901ms step_avg:94.58ms
step:306/1770 train_time:27997ms step_avg:94.58ms
step:307/1770 train_time:28092ms step_avg:94.59ms
step:308/1770 train_time:28187ms step_avg:94.59ms
step:309/1770 train_time:28282ms step_avg:94.59ms
step:310/1770 train_time:28377ms step_avg:94.59ms
step:311/1770 train_time:28473ms step_avg:94.59ms
step:312/1770 train_time:28567ms step_avg:94.59ms
step:313/1770 train_time:28662ms step_avg:94.59ms
step:314/1770 train_time:28758ms step_avg:94.60ms
step:315/1770 train_time:28853ms step_avg:94.60ms
step:316/1770 train_time:28948ms step_avg:94.60ms
step:317/1770 train_time:29042ms step_avg:94.60ms
step:318/1770 train_time:29138ms step_avg:94.60ms
step:319/1770 train_time:29234ms step_avg:94.61ms
step:320/1770 train_time:29329ms step_avg:94.61ms
step:321/1770 train_time:29424ms step_avg:94.61ms
step:322/1770 train_time:29519ms step_avg:94.61ms
step:323/1770 train_time:29615ms step_avg:94.62ms
step:324/1770 train_time:29710ms step_avg:94.62ms
step:325/1770 train_time:29806ms step_avg:94.62ms
step:326/1770 train_time:29901ms step_avg:94.62ms
step:327/1770 train_time:29996ms step_avg:94.62ms
step:328/1770 train_time:30091ms step_avg:94.63ms
step:329/1770 train_time:30186ms step_avg:94.63ms
step:330/1770 train_time:30281ms step_avg:94.63ms
step:331/1770 train_time:30377ms step_avg:94.63ms
step:332/1770 train_time:30473ms step_avg:94.64ms
step:333/1770 train_time:30568ms step_avg:94.64ms
step:334/1770 train_time:30662ms step_avg:94.64ms
step:335/1770 train_time:30758ms step_avg:94.64ms
step:336/1770 train_time:30854ms step_avg:94.64ms
step:337/1770 train_time:30948ms step_avg:94.64ms
step:338/1770 train_time:31043ms step_avg:94.64ms
step:339/1770 train_time:31138ms step_avg:94.64ms
step:340/1770 train_time:31234ms step_avg:94.65ms
step:341/1770 train_time:31329ms step_avg:94.65ms
step:342/1770 train_time:31423ms step_avg:94.65ms
step:343/1770 train_time:31519ms step_avg:94.65ms
step:344/1770 train_time:31614ms step_avg:94.65ms
step:345/1770 train_time:31710ms step_avg:94.66ms
step:346/1770 train_time:31804ms step_avg:94.65ms
step:347/1770 train_time:31899ms step_avg:94.66ms
step:348/1770 train_time:31995ms step_avg:94.66ms
step:349/1770 train_time:32090ms step_avg:94.66ms
step:350/1770 train_time:32184ms step_avg:94.66ms
step:351/1770 train_time:32279ms step_avg:94.66ms
step:352/1770 train_time:32375ms step_avg:94.66ms
step:353/1770 train_time:32470ms step_avg:94.66ms
step:354/1770 train_time:32565ms step_avg:94.67ms
step:355/1770 train_time:32661ms step_avg:94.67ms
step:356/1770 train_time:32756ms step_avg:94.67ms
step:357/1770 train_time:32851ms step_avg:94.67ms
step:358/1770 train_time:32946ms step_avg:94.67ms
step:359/1770 train_time:33041ms step_avg:94.67ms
step:360/1770 train_time:33137ms step_avg:94.68ms
step:361/1770 train_time:33232ms step_avg:94.68ms
step:362/1770 train_time:33327ms step_avg:94.68ms
step:363/1770 train_time:33422ms step_avg:94.68ms
step:364/1770 train_time:33517ms step_avg:94.68ms
step:365/1770 train_time:33612ms step_avg:94.68ms
step:366/1770 train_time:33707ms step_avg:94.68ms
step:367/1770 train_time:33802ms step_avg:94.68ms
step:368/1770 train_time:33898ms step_avg:94.69ms
step:369/1770 train_time:33993ms step_avg:94.69ms
step:370/1770 train_time:34087ms step_avg:94.69ms
step:371/1770 train_time:34182ms step_avg:94.69ms
step:372/1770 train_time:34278ms step_avg:94.69ms
step:373/1770 train_time:34373ms step_avg:94.69ms
step:374/1770 train_time:34467ms step_avg:94.69ms
step:375/1770 train_time:34562ms step_avg:94.69ms
step:375/1770 val_loss:3.9033 train_time:34656ms step_avg:94.95ms
step:376/1770 train_time:34679ms step_avg:94.75ms
step:377/1770 train_time:34765ms step_avg:94.73ms
step:378/1770 train_time:34862ms step_avg:94.73ms
step:379/1770 train_time:34957ms step_avg:94.73ms
step:380/1770 train_time:35052ms step_avg:94.73ms
step:381/1770 train_time:35147ms step_avg:94.74ms
step:382/1770 train_time:35242ms step_avg:94.74ms
step:383/1770 train_time:35336ms step_avg:94.73ms
step:384/1770 train_time:35431ms step_avg:94.74ms
step:385/1770 train_time:35526ms step_avg:94.74ms
step:386/1770 train_time:35621ms step_avg:94.74ms
step:387/1770 train_time:35717ms step_avg:94.74ms
step:388/1770 train_time:35813ms step_avg:94.74ms
step:389/1770 train_time:35910ms step_avg:94.75ms
step:390/1770 train_time:36005ms step_avg:94.75ms
step:391/1770 train_time:36100ms step_avg:94.75ms
step:392/1770 train_time:36196ms step_avg:94.75ms
step:393/1770 train_time:36290ms step_avg:94.75ms
step:394/1770 train_time:36385ms step_avg:94.75ms
step:395/1770 train_time:36479ms step_avg:94.75ms
step:396/1770 train_time:36576ms step_avg:94.76ms
step:397/1770 train_time:36672ms step_avg:94.76ms
step:398/1770 train_time:36770ms step_avg:94.77ms
step:399/1770 train_time:36868ms step_avg:94.78ms
step:400/1770 train_time:36965ms step_avg:94.78ms
step:401/1770 train_time:37062ms step_avg:94.79ms
step:402/1770 train_time:37159ms step_avg:94.79ms
step:403/1770 train_time:37255ms step_avg:94.80ms
step:404/1770 train_time:37352ms step_avg:94.80ms
step:405/1770 train_time:37449ms step_avg:94.81ms
step:406/1770 train_time:37547ms step_avg:94.81ms
step:407/1770 train_time:37643ms step_avg:94.82ms
step:408/1770 train_time:37740ms step_avg:94.82ms
step:409/1770 train_time:37837ms step_avg:94.83ms
step:410/1770 train_time:37933ms step_avg:94.83ms
step:411/1770 train_time:38030ms step_avg:94.84ms
step:412/1770 train_time:38127ms step_avg:94.84ms
step:413/1770 train_time:38225ms step_avg:94.85ms
step:414/1770 train_time:38322ms step_avg:94.86ms
step:415/1770 train_time:38419ms step_avg:94.86ms
step:416/1770 train_time:38516ms step_avg:94.87ms
step:417/1770 train_time:38612ms step_avg:94.87ms
step:418/1770 train_time:38709ms step_avg:94.88ms
step:419/1770 train_time:38807ms step_avg:94.88ms
step:420/1770 train_time:38904ms step_avg:94.89ms
step:421/1770 train_time:39001ms step_avg:94.89ms
step:422/1770 train_time:39097ms step_avg:94.90ms
step:423/1770 train_time:39194ms step_avg:94.90ms
step:424/1770 train_time:39291ms step_avg:94.91ms
step:425/1770 train_time:39389ms step_avg:94.91ms
step:426/1770 train_time:39486ms step_avg:94.92ms
step:427/1770 train_time:39583ms step_avg:94.92ms
step:428/1770 train_time:39680ms step_avg:94.93ms
step:429/1770 train_time:39776ms step_avg:94.93ms
step:430/1770 train_time:39873ms step_avg:94.94ms
step:431/1770 train_time:39970ms step_avg:94.94ms
step:432/1770 train_time:40067ms step_avg:94.95ms
step:433/1770 train_time:40165ms step_avg:94.95ms
step:434/1770 train_time:40261ms step_avg:94.96ms
step:435/1770 train_time:40358ms step_avg:94.96ms
step:436/1770 train_time:40454ms step_avg:94.96ms
step:437/1770 train_time:40551ms step_avg:94.97ms
step:438/1770 train_time:40649ms step_avg:94.97ms
step:439/1770 train_time:40746ms step_avg:94.98ms
step:440/1770 train_time:40843ms step_avg:94.98ms
step:441/1770 train_time:40940ms step_avg:94.99ms
step:442/1770 train_time:41036ms step_avg:94.99ms
step:443/1770 train_time:41133ms step_avg:95.00ms
step:444/1770 train_time:41230ms step_avg:95.00ms
step:445/1770 train_time:41327ms step_avg:95.00ms
step:446/1770 train_time:41424ms step_avg:95.01ms
step:447/1770 train_time:41521ms step_avg:95.01ms
step:448/1770 train_time:41618ms step_avg:95.02ms
step:449/1770 train_time:41714ms step_avg:95.02ms
step:450/1770 train_time:41811ms step_avg:95.02ms
step:451/1770 train_time:41908ms step_avg:95.03ms
step:452/1770 train_time:42005ms step_avg:95.03ms
step:453/1770 train_time:42102ms step_avg:95.04ms
step:454/1770 train_time:42200ms step_avg:95.04ms
step:455/1770 train_time:42295ms step_avg:95.05ms
step:456/1770 train_time:42393ms step_avg:95.05ms
step:457/1770 train_time:42490ms step_avg:95.06ms
step:458/1770 train_time:42588ms step_avg:95.06ms
step:459/1770 train_time:42685ms step_avg:95.07ms
step:460/1770 train_time:42782ms step_avg:95.07ms
step:461/1770 train_time:42878ms step_avg:95.07ms
step:462/1770 train_time:42975ms step_avg:95.08ms
step:463/1770 train_time:43072ms step_avg:95.08ms
step:464/1770 train_time:43169ms step_avg:95.09ms
step:465/1770 train_time:43266ms step_avg:95.09ms
step:466/1770 train_time:43363ms step_avg:95.10ms
step:467/1770 train_time:43460ms step_avg:95.10ms
step:468/1770 train_time:43557ms step_avg:95.10ms
step:469/1770 train_time:43654ms step_avg:95.11ms
step:470/1770 train_time:43751ms step_avg:95.11ms
step:471/1770 train_time:43849ms step_avg:95.12ms
step:472/1770 train_time:43946ms step_avg:95.12ms
step:473/1770 train_time:44043ms step_avg:95.12ms
step:474/1770 train_time:44140ms step_avg:95.13ms
step:475/1770 train_time:44236ms step_avg:95.13ms
step:476/1770 train_time:44333ms step_avg:95.14ms
step:477/1770 train_time:44430ms step_avg:95.14ms
step:478/1770 train_time:44528ms step_avg:95.14ms
step:479/1770 train_time:44625ms step_avg:95.15ms
step:480/1770 train_time:44722ms step_avg:95.15ms
step:481/1770 train_time:44819ms step_avg:95.16ms
step:482/1770 train_time:44915ms step_avg:95.16ms
step:483/1770 train_time:45013ms step_avg:95.16ms
step:484/1770 train_time:45110ms step_avg:95.17ms
step:485/1770 train_time:45207ms step_avg:95.17ms
step:486/1770 train_time:45304ms step_avg:95.18ms
step:487/1770 train_time:45401ms step_avg:95.18ms
step:488/1770 train_time:45498ms step_avg:95.18ms
step:489/1770 train_time:45594ms step_avg:95.19ms
step:490/1770 train_time:45692ms step_avg:95.19ms
step:491/1770 train_time:45790ms step_avg:95.20ms
step:492/1770 train_time:45887ms step_avg:95.20ms
step:493/1770 train_time:45983ms step_avg:95.20ms
step:494/1770 train_time:46080ms step_avg:95.21ms
step:495/1770 train_time:46176ms step_avg:95.21ms
step:496/1770 train_time:46273ms step_avg:95.21ms
step:497/1770 train_time:46371ms step_avg:95.22ms
step:498/1770 train_time:46469ms step_avg:95.22ms
step:499/1770 train_time:46566ms step_avg:95.23ms
step:500/1770 train_time:46662ms step_avg:95.23ms
step:500/1770 val_loss:3.7505 train_time:46758ms step_avg:95.42ms
step:501/1770 train_time:46779ms step_avg:95.27ms
step:502/1770 train_time:46866ms step_avg:95.26ms
step:503/1770 train_time:46965ms step_avg:95.26ms
step:504/1770 train_time:47062ms step_avg:95.27ms
step:505/1770 train_time:47158ms step_avg:95.27ms
step:506/1770 train_time:47255ms step_avg:95.27ms
step:507/1770 train_time:47352ms step_avg:95.28ms
step:508/1770 train_time:47449ms step_avg:95.28ms
step:509/1770 train_time:47546ms step_avg:95.28ms
step:510/1770 train_time:47642ms step_avg:95.28ms
step:511/1770 train_time:47739ms step_avg:95.29ms
step:512/1770 train_time:47836ms step_avg:95.29ms
step:513/1770 train_time:47935ms step_avg:95.30ms
step:514/1770 train_time:48033ms step_avg:95.30ms
step:515/1770 train_time:48130ms step_avg:95.31ms
step:516/1770 train_time:48227ms step_avg:95.31ms
step:517/1770 train_time:48323ms step_avg:95.31ms
step:518/1770 train_time:48420ms step_avg:95.31ms
step:519/1770 train_time:48517ms step_avg:95.32ms
step:520/1770 train_time:48614ms step_avg:95.32ms
step:521/1770 train_time:48711ms step_avg:95.33ms
step:522/1770 train_time:48808ms step_avg:95.33ms
step:523/1770 train_time:48904ms step_avg:95.33ms
step:524/1770 train_time:49001ms step_avg:95.33ms
step:525/1770 train_time:49098ms step_avg:95.34ms
step:526/1770 train_time:49196ms step_avg:95.34ms
step:527/1770 train_time:49294ms step_avg:95.35ms
step:528/1770 train_time:49391ms step_avg:95.35ms
step:529/1770 train_time:49488ms step_avg:95.35ms
step:530/1770 train_time:49585ms step_avg:95.36ms
step:531/1770 train_time:49682ms step_avg:95.36ms
step:532/1770 train_time:49779ms step_avg:95.36ms
step:533/1770 train_time:49877ms step_avg:95.37ms
step:534/1770 train_time:49974ms step_avg:95.37ms
step:535/1770 train_time:50073ms step_avg:95.38ms
step:536/1770 train_time:50170ms step_avg:95.38ms
step:537/1770 train_time:50268ms step_avg:95.38ms
step:538/1770 train_time:50364ms step_avg:95.39ms
step:539/1770 train_time:50461ms step_avg:95.39ms
step:540/1770 train_time:50559ms step_avg:95.39ms
step:541/1770 train_time:50657ms step_avg:95.40ms
step:542/1770 train_time:50754ms step_avg:95.40ms
step:543/1770 train_time:50852ms step_avg:95.41ms
step:544/1770 train_time:50949ms step_avg:95.41ms
step:545/1770 train_time:51046ms step_avg:95.41ms
step:546/1770 train_time:51143ms step_avg:95.42ms
step:547/1770 train_time:51240ms step_avg:95.42ms
step:548/1770 train_time:51338ms step_avg:95.42ms
step:549/1770 train_time:51435ms step_avg:95.43ms
step:550/1770 train_time:51533ms step_avg:95.43ms
step:551/1770 train_time:51631ms step_avg:95.44ms
step:552/1770 train_time:51728ms step_avg:95.44ms
step:553/1770 train_time:51825ms step_avg:95.44ms
step:554/1770 train_time:51922ms step_avg:95.44ms
step:555/1770 train_time:52019ms step_avg:95.45ms
step:556/1770 train_time:52116ms step_avg:95.45ms
step:557/1770 train_time:52214ms step_avg:95.45ms
step:558/1770 train_time:52311ms step_avg:95.46ms
step:559/1770 train_time:52408ms step_avg:95.46ms
step:560/1770 train_time:52505ms step_avg:95.46ms
step:561/1770 train_time:52602ms step_avg:95.47ms
step:562/1770 train_time:52699ms step_avg:95.47ms
step:563/1770 train_time:52797ms step_avg:95.47ms
step:564/1770 train_time:52895ms step_avg:95.48ms
step:565/1770 train_time:52992ms step_avg:95.48ms
step:566/1770 train_time:53090ms step_avg:95.49ms
step:567/1770 train_time:53187ms step_avg:95.49ms
step:568/1770 train_time:53285ms step_avg:95.49ms
step:569/1770 train_time:53381ms step_avg:95.49ms
step:570/1770 train_time:53479ms step_avg:95.50ms
step:571/1770 train_time:53576ms step_avg:95.50ms
step:572/1770 train_time:53674ms step_avg:95.51ms
step:573/1770 train_time:53771ms step_avg:95.51ms
step:574/1770 train_time:53869ms step_avg:95.51ms
step:575/1770 train_time:53966ms step_avg:95.51ms
step:576/1770 train_time:54063ms step_avg:95.52ms
step:577/1770 train_time:54160ms step_avg:95.52ms
step:578/1770 train_time:54258ms step_avg:95.52ms
step:579/1770 train_time:54356ms step_avg:95.53ms
step:580/1770 train_time:54454ms step_avg:95.53ms
step:581/1770 train_time:54551ms step_avg:95.54ms
step:582/1770 train_time:54648ms step_avg:95.54ms
step:583/1770 train_time:54745ms step_avg:95.54ms
step:584/1770 train_time:54842ms step_avg:95.54ms
step:585/1770 train_time:54940ms step_avg:95.55ms
step:586/1770 train_time:55038ms step_avg:95.55ms
step:587/1770 train_time:55136ms step_avg:95.56ms
step:588/1770 train_time:55234ms step_avg:95.56ms
step:589/1770 train_time:55331ms step_avg:95.56ms
step:590/1770 train_time:55429ms step_avg:95.57ms
step:591/1770 train_time:55526ms step_avg:95.57ms
step:592/1770 train_time:55623ms step_avg:95.57ms
step:593/1770 train_time:55721ms step_avg:95.58ms
step:594/1770 train_time:55818ms step_avg:95.58ms
step:595/1770 train_time:55916ms step_avg:95.58ms
step:596/1770 train_time:56013ms step_avg:95.59ms
step:597/1770 train_time:56111ms step_avg:95.59ms
step:598/1770 train_time:56209ms step_avg:95.59ms
step:599/1770 train_time:56306ms step_avg:95.60ms
step:600/1770 train_time:56402ms step_avg:95.60ms
step:601/1770 train_time:56499ms step_avg:95.60ms
step:602/1770 train_time:56597ms step_avg:95.60ms
step:603/1770 train_time:56695ms step_avg:95.61ms
step:604/1770 train_time:56792ms step_avg:95.61ms
step:605/1770 train_time:56889ms step_avg:95.61ms
step:606/1770 train_time:56986ms step_avg:95.61ms
step:607/1770 train_time:57083ms step_avg:95.62ms
step:608/1770 train_time:57180ms step_avg:95.62ms
step:609/1770 train_time:57277ms step_avg:95.62ms
step:610/1770 train_time:57375ms step_avg:95.63ms
step:611/1770 train_time:57473ms step_avg:95.63ms
step:612/1770 train_time:57570ms step_avg:95.63ms
step:613/1770 train_time:57668ms step_avg:95.63ms
step:614/1770 train_time:57765ms step_avg:95.64ms
step:615/1770 train_time:57861ms step_avg:95.64ms
step:616/1770 train_time:57958ms step_avg:95.64ms
step:617/1770 train_time:58056ms step_avg:95.64ms
step:618/1770 train_time:58154ms step_avg:95.65ms
step:619/1770 train_time:58252ms step_avg:95.65ms
step:620/1770 train_time:58348ms step_avg:95.65ms
step:621/1770 train_time:58446ms step_avg:95.66ms
step:622/1770 train_time:58542ms step_avg:95.66ms
step:623/1770 train_time:58639ms step_avg:95.66ms
step:624/1770 train_time:58738ms step_avg:95.66ms
step:625/1770 train_time:58836ms step_avg:95.67ms
step:625/1770 val_loss:3.6648 train_time:58933ms step_avg:95.83ms
step:626/1770 train_time:58954ms step_avg:95.71ms
step:627/1770 train_time:59041ms step_avg:95.69ms
step:628/1770 train_time:59142ms step_avg:95.70ms
step:629/1770 train_time:59239ms step_avg:95.70ms
step:630/1770 train_time:59336ms step_avg:95.70ms
step:631/1770 train_time:59433ms step_avg:95.71ms
step:632/1770 train_time:59530ms step_avg:95.71ms
step:633/1770 train_time:59627ms step_avg:95.71ms
step:634/1770 train_time:59724ms step_avg:95.71ms
step:635/1770 train_time:59820ms step_avg:95.71ms
step:636/1770 train_time:59917ms step_avg:95.71ms
step:637/1770 train_time:60014ms step_avg:95.72ms
step:638/1770 train_time:60113ms step_avg:95.72ms
step:639/1770 train_time:60211ms step_avg:95.73ms
step:640/1770 train_time:60309ms step_avg:95.73ms
step:641/1770 train_time:60407ms step_avg:95.73ms
step:642/1770 train_time:60504ms step_avg:95.73ms
step:643/1770 train_time:60601ms step_avg:95.74ms
step:644/1770 train_time:60698ms step_avg:95.74ms
step:645/1770 train_time:60795ms step_avg:95.74ms
step:646/1770 train_time:60892ms step_avg:95.74ms
step:647/1770 train_time:60990ms step_avg:95.75ms
step:648/1770 train_time:61087ms step_avg:95.75ms
step:649/1770 train_time:61185ms step_avg:95.75ms
step:650/1770 train_time:61282ms step_avg:95.75ms
step:651/1770 train_time:61379ms step_avg:95.76ms
step:652/1770 train_time:61476ms step_avg:95.76ms
step:653/1770 train_time:61575ms step_avg:95.76ms
step:654/1770 train_time:61673ms step_avg:95.77ms
step:655/1770 train_time:61770ms step_avg:95.77ms
step:656/1770 train_time:61867ms step_avg:95.77ms
step:657/1770 train_time:61964ms step_avg:95.77ms
step:658/1770 train_time:62062ms step_avg:95.78ms
step:659/1770 train_time:62161ms step_avg:95.78ms
step:660/1770 train_time:62260ms step_avg:95.78ms
step:661/1770 train_time:62359ms step_avg:95.79ms
step:662/1770 train_time:62458ms step_avg:95.79ms
step:663/1770 train_time:62557ms step_avg:95.80ms
step:664/1770 train_time:62656ms step_avg:95.80ms
step:665/1770 train_time:62756ms step_avg:95.81ms
step:666/1770 train_time:62854ms step_avg:95.81ms
step:667/1770 train_time:62954ms step_avg:95.82ms
step:668/1770 train_time:63055ms step_avg:95.83ms
step:669/1770 train_time:63155ms step_avg:95.83ms
step:670/1770 train_time:63255ms step_avg:95.84ms
step:671/1770 train_time:63355ms step_avg:95.85ms
step:672/1770 train_time:63455ms step_avg:95.85ms
step:673/1770 train_time:63554ms step_avg:95.86ms
step:674/1770 train_time:63653ms step_avg:95.86ms
step:675/1770 train_time:63752ms step_avg:95.87ms
step:676/1770 train_time:63851ms step_avg:95.87ms
step:677/1770 train_time:63951ms step_avg:95.88ms
step:678/1770 train_time:64050ms step_avg:95.88ms
step:679/1770 train_time:64149ms step_avg:95.89ms
step:680/1770 train_time:64248ms step_avg:95.89ms
step:681/1770 train_time:64347ms step_avg:95.90ms
step:682/1770 train_time:64446ms step_avg:95.90ms
step:683/1770 train_time:64545ms step_avg:95.91ms
step:684/1770 train_time:64644ms step_avg:95.91ms
step:685/1770 train_time:64743ms step_avg:95.92ms
step:686/1770 train_time:64841ms step_avg:95.92ms
step:687/1770 train_time:64940ms step_avg:95.92ms
step:688/1770 train_time:65038ms step_avg:95.93ms
step:689/1770 train_time:65137ms step_avg:95.93ms
step:690/1770 train_time:65236ms step_avg:95.94ms
step:691/1770 train_time:65336ms step_avg:95.94ms
step:692/1770 train_time:65436ms step_avg:95.95ms
step:693/1770 train_time:65536ms step_avg:95.95ms
step:694/1770 train_time:65636ms step_avg:95.96ms
step:695/1770 train_time:65735ms step_avg:95.96ms
step:696/1770 train_time:65834ms step_avg:95.97ms
step:697/1770 train_time:65934ms step_avg:95.97ms
step:698/1770 train_time:66034ms step_avg:95.98ms
step:699/1770 train_time:66133ms step_avg:95.98ms
step:700/1770 train_time:66232ms step_avg:95.99ms
step:701/1770 train_time:66331ms step_avg:95.99ms
step:702/1770 train_time:66430ms step_avg:96.00ms
step:703/1770 train_time:66530ms step_avg:96.00ms
step:704/1770 train_time:66630ms step_avg:96.01ms
step:705/1770 train_time:66729ms step_avg:96.01ms
step:706/1770 train_time:66829ms step_avg:96.02ms
step:707/1770 train_time:66928ms step_avg:96.02ms
step:708/1770 train_time:67028ms step_avg:96.03ms
step:709/1770 train_time:67127ms step_avg:96.03ms
step:710/1770 train_time:67227ms step_avg:96.04ms
step:711/1770 train_time:67326ms step_avg:96.04ms
step:712/1770 train_time:67425ms step_avg:96.05ms
step:713/1770 train_time:67524ms step_avg:96.05ms
step:714/1770 train_time:67622ms step_avg:96.05ms
step:715/1770 train_time:67721ms step_avg:96.06ms
step:716/1770 train_time:67819ms step_avg:96.06ms
step:717/1770 train_time:67918ms step_avg:96.07ms
step:718/1770 train_time:68018ms step_avg:96.07ms
step:719/1770 train_time:68117ms step_avg:96.07ms
step:720/1770 train_time:68216ms step_avg:96.08ms
step:721/1770 train_time:68315ms step_avg:96.08ms
step:722/1770 train_time:68414ms step_avg:96.09ms
step:723/1770 train_time:68513ms step_avg:96.09ms
step:724/1770 train_time:68614ms step_avg:96.10ms
step:725/1770 train_time:68714ms step_avg:96.10ms
step:726/1770 train_time:68813ms step_avg:96.11ms
step:727/1770 train_time:68913ms step_avg:96.11ms
step:728/1770 train_time:69012ms step_avg:96.12ms
step:729/1770 train_time:69111ms step_avg:96.12ms
step:730/1770 train_time:69211ms step_avg:96.13ms
step:731/1770 train_time:69310ms step_avg:96.13ms
step:732/1770 train_time:69409ms step_avg:96.13ms
step:733/1770 train_time:69509ms step_avg:96.14ms
step:734/1770 train_time:69608ms step_avg:96.14ms
step:735/1770 train_time:69707ms step_avg:96.15ms
step:736/1770 train_time:69806ms step_avg:96.15ms
step:737/1770 train_time:69905ms step_avg:96.16ms
step:738/1770 train_time:70004ms step_avg:96.16ms
step:739/1770 train_time:70102ms step_avg:96.16ms
step:740/1770 train_time:70201ms step_avg:96.17ms
step:741/1770 train_time:70300ms step_avg:96.17ms
step:742/1770 train_time:70398ms step_avg:96.17ms
step:743/1770 train_time:70497ms step_avg:96.18ms
step:744/1770 train_time:70596ms step_avg:96.18ms
step:745/1770 train_time:70695ms step_avg:96.18ms
step:746/1770 train_time:70795ms step_avg:96.19ms
step:747/1770 train_time:70895ms step_avg:96.19ms
step:748/1770 train_time:70996ms step_avg:96.20ms
step:749/1770 train_time:71095ms step_avg:96.20ms
step:750/1770 train_time:71195ms step_avg:96.21ms
step:750/1770 val_loss:3.6007 train_time:71293ms step_avg:96.34ms
step:751/1770 train_time:71317ms step_avg:96.24ms
step:752/1770 train_time:71407ms step_avg:96.24ms
step:753/1770 train_time:71507ms step_avg:96.24ms
step:754/1770 train_time:71605ms step_avg:96.24ms
step:755/1770 train_time:71704ms step_avg:96.25ms
step:756/1770 train_time:71803ms step_avg:96.25ms
step:757/1770 train_time:71902ms step_avg:96.25ms
step:758/1770 train_time:72001ms step_avg:96.26ms
step:759/1770 train_time:72100ms step_avg:96.26ms
step:760/1770 train_time:72200ms step_avg:96.27ms
step:761/1770 train_time:72299ms step_avg:96.27ms
step:762/1770 train_time:72399ms step_avg:96.28ms
step:763/1770 train_time:72501ms step_avg:96.28ms
step:764/1770 train_time:72603ms step_avg:96.29ms
step:765/1770 train_time:72703ms step_avg:96.30ms
step:766/1770 train_time:72802ms step_avg:96.30ms
step:767/1770 train_time:72902ms step_avg:96.30ms
step:768/1770 train_time:73001ms step_avg:96.31ms
step:769/1770 train_time:73100ms step_avg:96.31ms
step:770/1770 train_time:73199ms step_avg:96.31ms
step:771/1770 train_time:73299ms step_avg:96.32ms
step:772/1770 train_time:73398ms step_avg:96.32ms
step:773/1770 train_time:73498ms step_avg:96.33ms
step:774/1770 train_time:73598ms step_avg:96.33ms
step:775/1770 train_time:73697ms step_avg:96.34ms
step:776/1770 train_time:73797ms step_avg:96.34ms
step:777/1770 train_time:73897ms step_avg:96.34ms
step:778/1770 train_time:73996ms step_avg:96.35ms
step:779/1770 train_time:74095ms step_avg:96.35ms
step:780/1770 train_time:74195ms step_avg:96.36ms
step:781/1770 train_time:74294ms step_avg:96.36ms
step:782/1770 train_time:74393ms step_avg:96.36ms
step:783/1770 train_time:74492ms step_avg:96.37ms
step:784/1770 train_time:74590ms step_avg:96.37ms
step:785/1770 train_time:74689ms step_avg:96.37ms
step:786/1770 train_time:74788ms step_avg:96.38ms
step:787/1770 train_time:74887ms step_avg:96.38ms
step:788/1770 train_time:74986ms step_avg:96.38ms
step:789/1770 train_time:75086ms step_avg:96.39ms
step:790/1770 train_time:75185ms step_avg:96.39ms
step:791/1770 train_time:75285ms step_avg:96.40ms
step:792/1770 train_time:75384ms step_avg:96.40ms
step:793/1770 train_time:75483ms step_avg:96.40ms
step:794/1770 train_time:75583ms step_avg:96.41ms
step:795/1770 train_time:75683ms step_avg:96.41ms
step:796/1770 train_time:75782ms step_avg:96.42ms
step:797/1770 train_time:75882ms step_avg:96.42ms
step:798/1770 train_time:75983ms step_avg:96.42ms
step:799/1770 train_time:76082ms step_avg:96.43ms
step:800/1770 train_time:76182ms step_avg:96.43ms
step:801/1770 train_time:76282ms step_avg:96.44ms
step:802/1770 train_time:76381ms step_avg:96.44ms
step:803/1770 train_time:76481ms step_avg:96.44ms
step:804/1770 train_time:76580ms step_avg:96.45ms
step:805/1770 train_time:76680ms step_avg:96.45ms
step:806/1770 train_time:76780ms step_avg:96.46ms
step:807/1770 train_time:76879ms step_avg:96.46ms
step:808/1770 train_time:76980ms step_avg:96.47ms
step:809/1770 train_time:77080ms step_avg:96.47ms
step:810/1770 train_time:77180ms step_avg:96.47ms
step:811/1770 train_time:77279ms step_avg:96.48ms
step:812/1770 train_time:77379ms step_avg:96.48ms
step:813/1770 train_time:77479ms step_avg:96.49ms
step:814/1770 train_time:77579ms step_avg:96.49ms
step:815/1770 train_time:77678ms step_avg:96.49ms
step:816/1770 train_time:77778ms step_avg:96.50ms
step:817/1770 train_time:77878ms step_avg:96.50ms
step:818/1770 train_time:77978ms step_avg:96.51ms
step:819/1770 train_time:78078ms step_avg:96.51ms
step:820/1770 train_time:78178ms step_avg:96.52ms
step:821/1770 train_time:78277ms step_avg:96.52ms
step:822/1770 train_time:78377ms step_avg:96.52ms
step:823/1770 train_time:78477ms step_avg:96.53ms
step:824/1770 train_time:78577ms step_avg:96.53ms
step:825/1770 train_time:78676ms step_avg:96.53ms
step:826/1770 train_time:78775ms step_avg:96.54ms
step:827/1770 train_time:78875ms step_avg:96.54ms
step:828/1770 train_time:78975ms step_avg:96.55ms
step:829/1770 train_time:79074ms step_avg:96.55ms
step:830/1770 train_time:79173ms step_avg:96.55ms
step:831/1770 train_time:79273ms step_avg:96.56ms
step:832/1770 train_time:79372ms step_avg:96.56ms
step:833/1770 train_time:79471ms step_avg:96.56ms
step:834/1770 train_time:79571ms step_avg:96.57ms
step:835/1770 train_time:79670ms step_avg:96.57ms
step:836/1770 train_time:79769ms step_avg:96.57ms
step:837/1770 train_time:79867ms step_avg:96.57ms
step:838/1770 train_time:79966ms step_avg:96.58ms
step:839/1770 train_time:80064ms step_avg:96.58ms
step:840/1770 train_time:80163ms step_avg:96.58ms
step:841/1770 train_time:80263ms step_avg:96.59ms
step:842/1770 train_time:80362ms step_avg:96.59ms
step:843/1770 train_time:80462ms step_avg:96.59ms
step:844/1770 train_time:80562ms step_avg:96.60ms
step:845/1770 train_time:80662ms step_avg:96.60ms
step:846/1770 train_time:80762ms step_avg:96.61ms
step:847/1770 train_time:80862ms step_avg:96.61ms
step:848/1770 train_time:80961ms step_avg:96.61ms
step:849/1770 train_time:81061ms step_avg:96.62ms
step:850/1770 train_time:81162ms step_avg:96.62ms
step:851/1770 train_time:81262ms step_avg:96.63ms
step:852/1770 train_time:81362ms step_avg:96.63ms
step:853/1770 train_time:81461ms step_avg:96.63ms
step:854/1770 train_time:81561ms step_avg:96.64ms
step:855/1770 train_time:81662ms step_avg:96.64ms
step:856/1770 train_time:81762ms step_avg:96.65ms
step:857/1770 train_time:81862ms step_avg:96.65ms
step:858/1770 train_time:81962ms step_avg:96.65ms
step:859/1770 train_time:82062ms step_avg:96.66ms
step:860/1770 train_time:82162ms step_avg:96.66ms
step:861/1770 train_time:82262ms step_avg:96.66ms
step:862/1770 train_time:82362ms step_avg:96.67ms
step:863/1770 train_time:82461ms step_avg:96.67ms
step:864/1770 train_time:82560ms step_avg:96.67ms
step:865/1770 train_time:82660ms step_avg:96.68ms
step:866/1770 train_time:82761ms step_avg:96.68ms
step:867/1770 train_time:82861ms step_avg:96.69ms
step:868/1770 train_time:82961ms step_avg:96.69ms
step:869/1770 train_time:83061ms step_avg:96.70ms
step:870/1770 train_time:83161ms step_avg:96.70ms
step:871/1770 train_time:83261ms step_avg:96.70ms
step:872/1770 train_time:83361ms step_avg:96.71ms
step:873/1770 train_time:83461ms step_avg:96.71ms
step:874/1770 train_time:83561ms step_avg:96.71ms
step:875/1770 train_time:83660ms step_avg:96.72ms
step:875/1770 val_loss:3.5534 train_time:83759ms step_avg:96.83ms
step:876/1770 train_time:83781ms step_avg:96.75ms
step:877/1770 train_time:83870ms step_avg:96.74ms
step:878/1770 train_time:83970ms step_avg:96.74ms
step:879/1770 train_time:84069ms step_avg:96.74ms
step:880/1770 train_time:84168ms step_avg:96.74ms
step:881/1770 train_time:84267ms step_avg:96.75ms
step:882/1770 train_time:84366ms step_avg:96.75ms
step:883/1770 train_time:84465ms step_avg:96.75ms
step:884/1770 train_time:84564ms step_avg:96.76ms
step:885/1770 train_time:84663ms step_avg:96.76ms
step:886/1770 train_time:84762ms step_avg:96.76ms
step:887/1770 train_time:84863ms step_avg:96.76ms
step:888/1770 train_time:84962ms step_avg:96.77ms
step:889/1770 train_time:85063ms step_avg:96.77ms
step:890/1770 train_time:85163ms step_avg:96.78ms
step:891/1770 train_time:85264ms step_avg:96.78ms
step:892/1770 train_time:85364ms step_avg:96.78ms
step:893/1770 train_time:85464ms step_avg:96.79ms
step:894/1770 train_time:85563ms step_avg:96.79ms
step:895/1770 train_time:85663ms step_avg:96.79ms
step:896/1770 train_time:85762ms step_avg:96.80ms
step:897/1770 train_time:85861ms step_avg:96.80ms
step:898/1770 train_time:85961ms step_avg:96.80ms
step:899/1770 train_time:86061ms step_avg:96.81ms
step:900/1770 train_time:86161ms step_avg:96.81ms
step:901/1770 train_time:86262ms step_avg:96.81ms
step:902/1770 train_time:86362ms step_avg:96.82ms
step:903/1770 train_time:86462ms step_avg:96.82ms
step:904/1770 train_time:86562ms step_avg:96.83ms
step:905/1770 train_time:86661ms step_avg:96.83ms
step:906/1770 train_time:86761ms step_avg:96.83ms
step:907/1770 train_time:86861ms step_avg:96.84ms
step:908/1770 train_time:86961ms step_avg:96.84ms
step:909/1770 train_time:87061ms step_avg:96.84ms
step:910/1770 train_time:87162ms step_avg:96.85ms
step:911/1770 train_time:87263ms step_avg:96.85ms
step:912/1770 train_time:87363ms step_avg:96.85ms
step:913/1770 train_time:87463ms step_avg:96.86ms
step:914/1770 train_time:87563ms step_avg:96.86ms
step:915/1770 train_time:87663ms step_avg:96.86ms
step:916/1770 train_time:87762ms step_avg:96.87ms
step:917/1770 train_time:87861ms step_avg:96.87ms
step:918/1770 train_time:87960ms step_avg:96.87ms
step:919/1770 train_time:88061ms step_avg:96.88ms
step:920/1770 train_time:88163ms step_avg:96.88ms
step:921/1770 train_time:88265ms step_avg:96.89ms
step:922/1770 train_time:88367ms step_avg:96.89ms
step:923/1770 train_time:88467ms step_avg:96.90ms
step:924/1770 train_time:88567ms step_avg:96.90ms
step:925/1770 train_time:88668ms step_avg:96.91ms
step:926/1770 train_time:88769ms step_avg:96.91ms
step:927/1770 train_time:88868ms step_avg:96.91ms
step:928/1770 train_time:88967ms step_avg:96.91ms
step:929/1770 train_time:89067ms step_avg:96.92ms
step:930/1770 train_time:89167ms step_avg:96.92ms
step:931/1770 train_time:89267ms step_avg:96.92ms
step:932/1770 train_time:89368ms step_avg:96.93ms
step:933/1770 train_time:89467ms step_avg:96.93ms
step:934/1770 train_time:89567ms step_avg:96.93ms
step:935/1770 train_time:89667ms step_avg:96.94ms
step:936/1770 train_time:89768ms step_avg:96.94ms
step:937/1770 train_time:89868ms step_avg:96.94ms
step:938/1770 train_time:89968ms step_avg:96.95ms
step:939/1770 train_time:90068ms step_avg:96.95ms
step:940/1770 train_time:90168ms step_avg:96.95ms
step:941/1770 train_time:90268ms step_avg:96.96ms
step:942/1770 train_time:90368ms step_avg:96.96ms
step:943/1770 train_time:90469ms step_avg:96.97ms
step:944/1770 train_time:90569ms step_avg:96.97ms
step:945/1770 train_time:90669ms step_avg:96.97ms
step:946/1770 train_time:90769ms step_avg:96.98ms
step:947/1770 train_time:90869ms step_avg:96.98ms
step:948/1770 train_time:90968ms step_avg:96.98ms
step:949/1770 train_time:91068ms step_avg:96.98ms
step:950/1770 train_time:91168ms step_avg:96.99ms
step:951/1770 train_time:91269ms step_avg:96.99ms
step:952/1770 train_time:91369ms step_avg:96.99ms
step:953/1770 train_time:91470ms step_avg:97.00ms
step:954/1770 train_time:91569ms step_avg:97.00ms
step:955/1770 train_time:91669ms step_avg:97.00ms
step:956/1770 train_time:91769ms step_avg:97.01ms
step:957/1770 train_time:91869ms step_avg:97.01ms
step:958/1770 train_time:91969ms step_avg:97.01ms
step:959/1770 train_time:92070ms step_avg:97.02ms
step:960/1770 train_time:92170ms step_avg:97.02ms
step:961/1770 train_time:92271ms step_avg:97.02ms
step:962/1770 train_time:92372ms step_avg:97.03ms
step:963/1770 train_time:92472ms step_avg:97.03ms
step:964/1770 train_time:92573ms step_avg:97.04ms
step:965/1770 train_time:92674ms step_avg:97.04ms
step:966/1770 train_time:92774ms step_avg:97.04ms
step:967/1770 train_time:92875ms step_avg:97.05ms
step:968/1770 train_time:92976ms step_avg:97.05ms
step:969/1770 train_time:93077ms step_avg:97.06ms
step:970/1770 train_time:93178ms step_avg:97.06ms
step:971/1770 train_time:93280ms step_avg:97.07ms
step:972/1770 train_time:93381ms step_avg:97.07ms
step:973/1770 train_time:93482ms step_avg:97.07ms
step:974/1770 train_time:93584ms step_avg:97.08ms
step:975/1770 train_time:93684ms step_avg:97.08ms
step:976/1770 train_time:93785ms step_avg:97.09ms
step:977/1770 train_time:93886ms step_avg:97.09ms
step:978/1770 train_time:93986ms step_avg:97.09ms
step:979/1770 train_time:94086ms step_avg:97.10ms
step:980/1770 train_time:94186ms step_avg:97.10ms
step:981/1770 train_time:94287ms step_avg:97.10ms
step:982/1770 train_time:94387ms step_avg:97.11ms
step:983/1770 train_time:94487ms step_avg:97.11ms
step:984/1770 train_time:94589ms step_avg:97.11ms
step:985/1770 train_time:94688ms step_avg:97.12ms
step:986/1770 train_time:94788ms step_avg:97.12ms
step:987/1770 train_time:94888ms step_avg:97.12ms
step:988/1770 train_time:94988ms step_avg:97.12ms
step:989/1770 train_time:95090ms step_avg:97.13ms
step:990/1770 train_time:95190ms step_avg:97.13ms
step:991/1770 train_time:95290ms step_avg:97.14ms
step:992/1770 train_time:95390ms step_avg:97.14ms
step:993/1770 train_time:95490ms step_avg:97.14ms
step:994/1770 train_time:95590ms step_avg:97.14ms
step:995/1770 train_time:95690ms step_avg:97.15ms
step:996/1770 train_time:95790ms step_avg:97.15ms
step:997/1770 train_time:95891ms step_avg:97.15ms
step:998/1770 train_time:95990ms step_avg:97.16ms
step:999/1770 train_time:96090ms step_avg:97.16ms
step:1000/1770 train_time:96191ms step_avg:97.16ms
step:1000/1770 val_loss:3.5133 train_time:96289ms step_avg:97.26ms
step:1001/1770 train_time:96311ms step_avg:97.19ms
step:1002/1770 train_time:96401ms step_avg:97.18ms
step:1003/1770 train_time:96503ms step_avg:97.18ms
step:1004/1770 train_time:96603ms step_avg:97.19ms
step:1005/1770 train_time:96702ms step_avg:97.19ms
step:1006/1770 train_time:96802ms step_avg:97.19ms
step:1007/1770 train_time:96902ms step_avg:97.19ms
step:1008/1770 train_time:97002ms step_avg:97.20ms
step:1009/1770 train_time:97101ms step_avg:97.20ms
step:1010/1770 train_time:97201ms step_avg:97.20ms
step:1011/1770 train_time:97304ms step_avg:97.21ms
step:1012/1770 train_time:97406ms step_avg:97.21ms
step:1013/1770 train_time:97509ms step_avg:97.22ms
step:1014/1770 train_time:97610ms step_avg:97.22ms
step:1015/1770 train_time:97711ms step_avg:97.22ms
step:1016/1770 train_time:97811ms step_avg:97.23ms
step:1017/1770 train_time:97912ms step_avg:97.23ms
step:1018/1770 train_time:98013ms step_avg:97.23ms
step:1019/1770 train_time:98114ms step_avg:97.24ms
step:1020/1770 train_time:98215ms step_avg:97.24ms
step:1021/1770 train_time:98317ms step_avg:97.25ms
step:1022/1770 train_time:98417ms step_avg:97.25ms
step:1023/1770 train_time:98518ms step_avg:97.25ms
step:1024/1770 train_time:98618ms step_avg:97.26ms
step:1025/1770 train_time:98718ms step_avg:97.26ms
step:1026/1770 train_time:98818ms step_avg:97.26ms
step:1027/1770 train_time:98918ms step_avg:97.26ms
step:1028/1770 train_time:99019ms step_avg:97.27ms
step:1029/1770 train_time:99119ms step_avg:97.27ms
step:1030/1770 train_time:99219ms step_avg:97.27ms
step:1031/1770 train_time:99320ms step_avg:97.28ms
step:1032/1770 train_time:99420ms step_avg:97.28ms
step:1033/1770 train_time:99520ms step_avg:97.28ms
step:1034/1770 train_time:99620ms step_avg:97.28ms
step:1035/1770 train_time:99720ms step_avg:97.29ms
step:1036/1770 train_time:99820ms step_avg:97.29ms
step:1037/1770 train_time:99920ms step_avg:97.29ms
step:1038/1770 train_time:100020ms step_avg:97.30ms
step:1039/1770 train_time:100120ms step_avg:97.30ms
step:1040/1770 train_time:100220ms step_avg:97.30ms
step:1041/1770 train_time:100320ms step_avg:97.30ms
step:1042/1770 train_time:100421ms step_avg:97.31ms
step:1043/1770 train_time:100521ms step_avg:97.31ms
step:1044/1770 train_time:100621ms step_avg:97.31ms
step:1045/1770 train_time:100722ms step_avg:97.32ms
step:1046/1770 train_time:100822ms step_avg:97.32ms
step:1047/1770 train_time:100922ms step_avg:97.32ms
step:1048/1770 train_time:101023ms step_avg:97.32ms
step:1049/1770 train_time:101123ms step_avg:97.33ms
step:1050/1770 train_time:101224ms step_avg:97.33ms
step:1051/1770 train_time:101325ms step_avg:97.33ms
step:1052/1770 train_time:101426ms step_avg:97.34ms
step:1053/1770 train_time:101526ms step_avg:97.34ms
step:1054/1770 train_time:101628ms step_avg:97.34ms
step:1055/1770 train_time:101729ms step_avg:97.35ms
step:1056/1770 train_time:101830ms step_avg:97.35ms
step:1057/1770 train_time:101932ms step_avg:97.36ms
step:1058/1770 train_time:102034ms step_avg:97.36ms
step:1059/1770 train_time:102136ms step_avg:97.37ms
step:1060/1770 train_time:102238ms step_avg:97.37ms
step:1061/1770 train_time:102338ms step_avg:97.37ms
step:1062/1770 train_time:102439ms step_avg:97.38ms
step:1063/1770 train_time:102540ms step_avg:97.38ms
step:1064/1770 train_time:102641ms step_avg:97.38ms
step:1065/1770 train_time:102741ms step_avg:97.38ms
step:1066/1770 train_time:102841ms step_avg:97.39ms
step:1067/1770 train_time:102942ms step_avg:97.39ms
step:1068/1770 train_time:103043ms step_avg:97.39ms
step:1069/1770 train_time:103144ms step_avg:97.40ms
step:1070/1770 train_time:103245ms step_avg:97.40ms
step:1071/1770 train_time:103347ms step_avg:97.41ms
step:1072/1770 train_time:103449ms step_avg:97.41ms
step:1073/1770 train_time:103550ms step_avg:97.41ms
step:1074/1770 train_time:103651ms step_avg:97.42ms
step:1075/1770 train_time:103752ms step_avg:97.42ms
step:1076/1770 train_time:103855ms step_avg:97.42ms
step:1077/1770 train_time:103956ms step_avg:97.43ms
step:1078/1770 train_time:104057ms step_avg:97.43ms
step:1079/1770 train_time:104158ms step_avg:97.44ms
step:1080/1770 train_time:104258ms step_avg:97.44ms
step:1081/1770 train_time:104358ms step_avg:97.44ms
step:1082/1770 train_time:104458ms step_avg:97.44ms
step:1083/1770 train_time:104558ms step_avg:97.44ms
step:1084/1770 train_time:104658ms step_avg:97.45ms
step:1085/1770 train_time:104758ms step_avg:97.45ms
step:1086/1770 train_time:104859ms step_avg:97.45ms
step:1087/1770 train_time:104959ms step_avg:97.45ms
step:1088/1770 train_time:105059ms step_avg:97.46ms
step:1089/1770 train_time:105159ms step_avg:97.46ms
step:1090/1770 train_time:105260ms step_avg:97.46ms
step:1091/1770 train_time:105359ms step_avg:97.46ms
step:1092/1770 train_time:105460ms step_avg:97.47ms
step:1093/1770 train_time:105560ms step_avg:97.47ms
step:1094/1770 train_time:105660ms step_avg:97.47ms
step:1095/1770 train_time:105760ms step_avg:97.47ms
step:1096/1770 train_time:105860ms step_avg:97.48ms
step:1097/1770 train_time:105960ms step_avg:97.48ms
step:1098/1770 train_time:106060ms step_avg:97.48ms
step:1099/1770 train_time:106160ms step_avg:97.48ms
step:1100/1770 train_time:106261ms step_avg:97.49ms
step:1101/1770 train_time:106361ms step_avg:97.49ms
step:1102/1770 train_time:106461ms step_avg:97.49ms
step:1103/1770 train_time:106561ms step_avg:97.49ms
step:1104/1770 train_time:106662ms step_avg:97.50ms
step:1105/1770 train_time:106762ms step_avg:97.50ms
step:1106/1770 train_time:106863ms step_avg:97.50ms
step:1107/1770 train_time:106963ms step_avg:97.51ms
step:1108/1770 train_time:107065ms step_avg:97.51ms
step:1109/1770 train_time:107165ms step_avg:97.51ms
step:1110/1770 train_time:107268ms step_avg:97.52ms
step:1111/1770 train_time:107369ms step_avg:97.52ms
step:1112/1770 train_time:107471ms step_avg:97.52ms
step:1113/1770 train_time:107573ms step_avg:97.53ms
step:1114/1770 train_time:107674ms step_avg:97.53ms
step:1115/1770 train_time:107776ms step_avg:97.53ms
step:1116/1770 train_time:107878ms step_avg:97.54ms
step:1117/1770 train_time:107978ms step_avg:97.54ms
step:1118/1770 train_time:108078ms step_avg:97.54ms
step:1119/1770 train_time:108178ms step_avg:97.55ms
step:1120/1770 train_time:108278ms step_avg:97.55ms
step:1121/1770 train_time:108378ms step_avg:97.55ms
step:1122/1770 train_time:108478ms step_avg:97.55ms
step:1123/1770 train_time:108578ms step_avg:97.55ms
step:1124/1770 train_time:108678ms step_avg:97.56ms
step:1125/1770 train_time:108778ms step_avg:97.56ms
step:1125/1770 val_loss:3.4732 train_time:108877ms step_avg:97.65ms
step:1126/1770 train_time:108900ms step_avg:97.58ms
step:1127/1770 train_time:108990ms step_avg:97.57ms
step:1128/1770 train_time:109091ms step_avg:97.58ms
step:1129/1770 train_time:109191ms step_avg:97.58ms
step:1130/1770 train_time:109291ms step_avg:97.58ms
step:1131/1770 train_time:109391ms step_avg:97.58ms
step:1132/1770 train_time:109492ms step_avg:97.59ms
step:1133/1770 train_time:109592ms step_avg:97.59ms
step:1134/1770 train_time:109692ms step_avg:97.59ms
step:1135/1770 train_time:109792ms step_avg:97.59ms
step:1136/1770 train_time:109893ms step_avg:97.60ms
step:1137/1770 train_time:109995ms step_avg:97.60ms
step:1138/1770 train_time:110095ms step_avg:97.60ms
step:1139/1770 train_time:110196ms step_avg:97.61ms
step:1140/1770 train_time:110297ms step_avg:97.61ms
step:1141/1770 train_time:110398ms step_avg:97.61ms
step:1142/1770 train_time:110498ms step_avg:97.61ms
step:1143/1770 train_time:110599ms step_avg:97.62ms
step:1144/1770 train_time:110701ms step_avg:97.62ms
step:1145/1770 train_time:110801ms step_avg:97.62ms
step:1146/1770 train_time:110903ms step_avg:97.63ms
step:1147/1770 train_time:111007ms step_avg:97.63ms
step:1148/1770 train_time:111109ms step_avg:97.63ms
step:1149/1770 train_time:111210ms step_avg:97.64ms
step:1150/1770 train_time:111310ms step_avg:97.64ms
step:1151/1770 train_time:111411ms step_avg:97.64ms
step:1152/1770 train_time:111512ms step_avg:97.65ms
step:1153/1770 train_time:111612ms step_avg:97.65ms
step:1154/1770 train_time:111713ms step_avg:97.65ms
step:1155/1770 train_time:111813ms step_avg:97.65ms
step:1156/1770 train_time:111913ms step_avg:97.66ms
step:1157/1770 train_time:112015ms step_avg:97.66ms
step:1158/1770 train_time:112117ms step_avg:97.66ms
step:1159/1770 train_time:112218ms step_avg:97.67ms
step:1160/1770 train_time:112319ms step_avg:97.67ms
step:1161/1770 train_time:112421ms step_avg:97.67ms
step:1162/1770 train_time:112523ms step_avg:97.68ms
step:1163/1770 train_time:112624ms step_avg:97.68ms
step:1164/1770 train_time:112725ms step_avg:97.68ms
step:1165/1770 train_time:112826ms step_avg:97.68ms
step:1166/1770 train_time:112927ms step_avg:97.69ms
step:1167/1770 train_time:113028ms step_avg:97.69ms
step:1168/1770 train_time:113129ms step_avg:97.69ms
step:1169/1770 train_time:113230ms step_avg:97.70ms
step:1170/1770 train_time:113330ms step_avg:97.70ms
step:1171/1770 train_time:113431ms step_avg:97.70ms
step:1172/1770 train_time:113531ms step_avg:97.70ms
step:1173/1770 train_time:113631ms step_avg:97.70ms
step:1174/1770 train_time:113731ms step_avg:97.71ms
step:1175/1770 train_time:113831ms step_avg:97.71ms
step:1176/1770 train_time:113931ms step_avg:97.71ms
step:1177/1770 train_time:114032ms step_avg:97.71ms
step:1178/1770 train_time:114133ms step_avg:97.72ms
step:1179/1770 train_time:114234ms step_avg:97.72ms
step:1180/1770 train_time:114335ms step_avg:97.72ms
step:1181/1770 train_time:114435ms step_avg:97.72ms
step:1182/1770 train_time:114537ms step_avg:97.73ms
step:1183/1770 train_time:114639ms step_avg:97.73ms
step:1184/1770 train_time:114744ms step_avg:97.74ms
step:1185/1770 train_time:114846ms step_avg:97.74ms
step:1186/1770 train_time:114949ms step_avg:97.75ms
step:1187/1770 train_time:115054ms step_avg:97.75ms
step:1188/1770 train_time:115155ms step_avg:97.75ms
step:1189/1770 train_time:115257ms step_avg:97.76ms
step:1190/1770 train_time:115359ms step_avg:97.76ms
step:1191/1770 train_time:115462ms step_avg:97.77ms
step:1192/1770 train_time:115565ms step_avg:97.77ms
step:1193/1770 train_time:115668ms step_avg:97.77ms
step:1194/1770 train_time:115770ms step_avg:97.78ms
step:1195/1770 train_time:115872ms step_avg:97.78ms
step:1196/1770 train_time:115975ms step_avg:97.79ms
step:1197/1770 train_time:116076ms step_avg:97.79ms
step:1198/1770 train_time:116178ms step_avg:97.79ms
step:1199/1770 train_time:116279ms step_avg:97.80ms
step:1200/1770 train_time:116381ms step_avg:97.80ms
step:1201/1770 train_time:116484ms step_avg:97.80ms
step:1202/1770 train_time:116586ms step_avg:97.81ms
step:1203/1770 train_time:116689ms step_avg:97.81ms
step:1204/1770 train_time:116791ms step_avg:97.81ms
step:1205/1770 train_time:116892ms step_avg:97.82ms
step:1206/1770 train_time:116995ms step_avg:97.82ms
step:1207/1770 train_time:117096ms step_avg:97.82ms
step:1208/1770 train_time:117197ms step_avg:97.83ms
step:1209/1770 train_time:117299ms step_avg:97.83ms
step:1210/1770 train_time:117401ms step_avg:97.83ms
step:1211/1770 train_time:117504ms step_avg:97.84ms
step:1212/1770 train_time:117608ms step_avg:97.84ms
step:1213/1770 train_time:117710ms step_avg:97.85ms
step:1214/1770 train_time:117811ms step_avg:97.85ms
step:1215/1770 train_time:117913ms step_avg:97.85ms
step:1216/1770 train_time:118018ms step_avg:97.86ms
step:1217/1770 train_time:118120ms step_avg:97.86ms
step:1218/1770 train_time:118222ms step_avg:97.87ms
step:1219/1770 train_time:118324ms step_avg:97.87ms
step:1220/1770 train_time:118427ms step_avg:97.87ms
step:1221/1770 train_time:118528ms step_avg:97.88ms
step:1222/1770 train_time:118632ms step_avg:97.88ms
step:1223/1770 train_time:118733ms step_avg:97.88ms
step:1224/1770 train_time:118836ms step_avg:97.89ms
step:1225/1770 train_time:118939ms step_avg:97.89ms
step:1226/1770 train_time:119041ms step_avg:97.90ms
step:1227/1770 train_time:119145ms step_avg:97.90ms
step:1228/1770 train_time:119249ms step_avg:97.91ms
step:1229/1770 train_time:119351ms step_avg:97.91ms
step:1230/1770 train_time:119453ms step_avg:97.91ms
step:1231/1770 train_time:119555ms step_avg:97.92ms
step:1232/1770 train_time:119656ms step_avg:97.92ms
step:1233/1770 train_time:119758ms step_avg:97.92ms
step:1234/1770 train_time:119861ms step_avg:97.93ms
step:1235/1770 train_time:119964ms step_avg:97.93ms
step:1236/1770 train_time:120066ms step_avg:97.93ms
step:1237/1770 train_time:120169ms step_avg:97.94ms
step:1238/1770 train_time:120271ms step_avg:97.94ms
step:1239/1770 train_time:120373ms step_avg:97.94ms
step:1240/1770 train_time:120475ms step_avg:97.95ms
step:1241/1770 train_time:120577ms step_avg:97.95ms
step:1242/1770 train_time:120679ms step_avg:97.95ms
step:1243/1770 train_time:120781ms step_avg:97.96ms
step:1244/1770 train_time:120884ms step_avg:97.96ms
step:1245/1770 train_time:120986ms step_avg:97.96ms
step:1246/1770 train_time:121089ms step_avg:97.97ms
step:1247/1770 train_time:121192ms step_avg:97.97ms
step:1248/1770 train_time:121294ms step_avg:97.98ms
step:1249/1770 train_time:121395ms step_avg:97.98ms
step:1250/1770 train_time:121497ms step_avg:97.98ms
step:1250/1770 val_loss:3.4259 train_time:121598ms step_avg:98.06ms
step:1251/1770 train_time:121620ms step_avg:98.00ms
step:1252/1770 train_time:121714ms step_avg:98.00ms
step:1253/1770 train_time:121816ms step_avg:98.00ms
step:1254/1770 train_time:121918ms step_avg:98.00ms
step:1255/1770 train_time:122022ms step_avg:98.01ms
step:1256/1770 train_time:122124ms step_avg:98.01ms
step:1257/1770 train_time:122225ms step_avg:98.02ms
step:1258/1770 train_time:122328ms step_avg:98.02ms
step:1259/1770 train_time:122429ms step_avg:98.02ms
step:1260/1770 train_time:122531ms step_avg:98.02ms
step:1261/1770 train_time:122635ms step_avg:98.03ms
step:1262/1770 train_time:122738ms step_avg:98.03ms
step:1263/1770 train_time:122839ms step_avg:98.04ms
step:1264/1770 train_time:122942ms step_avg:98.04ms
step:1265/1770 train_time:123044ms step_avg:98.04ms
step:1266/1770 train_time:123148ms step_avg:98.05ms
step:1267/1770 train_time:123250ms step_avg:98.05ms
step:1268/1770 train_time:123352ms step_avg:98.05ms
step:1269/1770 train_time:123453ms step_avg:98.06ms
step:1270/1770 train_time:123556ms step_avg:98.06ms
step:1271/1770 train_time:123657ms step_avg:98.06ms
step:1272/1770 train_time:123759ms step_avg:98.07ms
step:1273/1770 train_time:123863ms step_avg:98.07ms
step:1274/1770 train_time:123966ms step_avg:98.07ms
step:1275/1770 train_time:124069ms step_avg:98.08ms
step:1276/1770 train_time:124171ms step_avg:98.08ms
step:1277/1770 train_time:124271ms step_avg:98.08ms
step:1278/1770 train_time:124374ms step_avg:98.09ms
step:1279/1770 train_time:124477ms step_avg:98.09ms
step:1280/1770 train_time:124580ms step_avg:98.09ms
step:1281/1770 train_time:124682ms step_avg:98.10ms
step:1282/1770 train_time:124785ms step_avg:98.10ms
step:1283/1770 train_time:124887ms step_avg:98.10ms
step:1284/1770 train_time:124989ms step_avg:98.11ms
step:1285/1770 train_time:125092ms step_avg:98.11ms
step:1286/1770 train_time:125195ms step_avg:98.12ms
step:1287/1770 train_time:125299ms step_avg:98.12ms
step:1288/1770 train_time:125402ms step_avg:98.12ms
step:1289/1770 train_time:125505ms step_avg:98.13ms
step:1290/1770 train_time:125607ms step_avg:98.13ms
step:1291/1770 train_time:125710ms step_avg:98.13ms
step:1292/1770 train_time:125811ms step_avg:98.14ms
step:1293/1770 train_time:125913ms step_avg:98.14ms
step:1294/1770 train_time:126014ms step_avg:98.14ms
step:1295/1770 train_time:126116ms step_avg:98.14ms
step:1296/1770 train_time:126218ms step_avg:98.15ms
step:1297/1770 train_time:126320ms step_avg:98.15ms
step:1298/1770 train_time:126423ms step_avg:98.15ms
step:1299/1770 train_time:126525ms step_avg:98.16ms
step:1300/1770 train_time:126628ms step_avg:98.16ms
step:1301/1770 train_time:126730ms step_avg:98.16ms
step:1302/1770 train_time:126832ms step_avg:98.17ms
step:1303/1770 train_time:126933ms step_avg:98.17ms
step:1304/1770 train_time:127035ms step_avg:98.17ms
step:1305/1770 train_time:127137ms step_avg:98.18ms
step:1306/1770 train_time:127238ms step_avg:98.18ms
step:1307/1770 train_time:127340ms step_avg:98.18ms
step:1308/1770 train_time:127443ms step_avg:98.18ms
step:1309/1770 train_time:127547ms step_avg:98.19ms
step:1310/1770 train_time:127649ms step_avg:98.19ms
step:1311/1770 train_time:127750ms step_avg:98.19ms
step:1312/1770 train_time:127852ms step_avg:98.20ms
step:1313/1770 train_time:127952ms step_avg:98.20ms
step:1314/1770 train_time:128054ms step_avg:98.20ms
step:1315/1770 train_time:128155ms step_avg:98.20ms
step:1316/1770 train_time:128257ms step_avg:98.21ms
step:1317/1770 train_time:128359ms step_avg:98.21ms
step:1318/1770 train_time:128465ms step_avg:98.21ms
step:1319/1770 train_time:128568ms step_avg:98.22ms
step:1320/1770 train_time:128670ms step_avg:98.22ms
step:1321/1770 train_time:128771ms step_avg:98.22ms
step:1322/1770 train_time:128874ms step_avg:98.23ms
step:1323/1770 train_time:128977ms step_avg:98.23ms
step:1324/1770 train_time:129079ms step_avg:98.23ms
step:1325/1770 train_time:129183ms step_avg:98.24ms
step:1326/1770 train_time:129284ms step_avg:98.24ms
step:1327/1770 train_time:129390ms step_avg:98.25ms
step:1328/1770 train_time:129491ms step_avg:98.25ms
step:1329/1770 train_time:129593ms step_avg:98.25ms
step:1330/1770 train_time:129694ms step_avg:98.25ms
step:1331/1770 train_time:129796ms step_avg:98.26ms
step:1332/1770 train_time:129898ms step_avg:98.26ms
step:1333/1770 train_time:130000ms step_avg:98.26ms
step:1334/1770 train_time:130101ms step_avg:98.26ms
step:1335/1770 train_time:130204ms step_avg:98.27ms
step:1336/1770 train_time:130306ms step_avg:98.27ms
step:1337/1770 train_time:130408ms step_avg:98.27ms
step:1338/1770 train_time:130509ms step_avg:98.28ms
step:1339/1770 train_time:130612ms step_avg:98.28ms
step:1340/1770 train_time:130715ms step_avg:98.28ms
step:1341/1770 train_time:130816ms step_avg:98.28ms
step:1342/1770 train_time:130918ms step_avg:98.29ms
step:1343/1770 train_time:131021ms step_avg:98.29ms
step:1344/1770 train_time:131124ms step_avg:98.29ms
step:1345/1770 train_time:131227ms step_avg:98.30ms
step:1346/1770 train_time:131329ms step_avg:98.30ms
step:1347/1770 train_time:131431ms step_avg:98.30ms
step:1348/1770 train_time:131535ms step_avg:98.31ms
step:1349/1770 train_time:131637ms step_avg:98.31ms
step:1350/1770 train_time:131740ms step_avg:98.31ms
step:1351/1770 train_time:131841ms step_avg:98.32ms
step:1352/1770 train_time:131943ms step_avg:98.32ms
step:1353/1770 train_time:132046ms step_avg:98.32ms
step:1354/1770 train_time:132148ms step_avg:98.32ms
step:1355/1770 train_time:132250ms step_avg:98.33ms
step:1356/1770 train_time:132352ms step_avg:98.33ms
step:1357/1770 train_time:132454ms step_avg:98.33ms
step:1358/1770 train_time:132556ms step_avg:98.34ms
step:1359/1770 train_time:132658ms step_avg:98.34ms
step:1360/1770 train_time:132760ms step_avg:98.34ms
step:1361/1770 train_time:132863ms step_avg:98.34ms
step:1362/1770 train_time:132966ms step_avg:98.35ms
step:1363/1770 train_time:133069ms step_avg:98.35ms
step:1364/1770 train_time:133171ms step_avg:98.35ms
step:1365/1770 train_time:133272ms step_avg:98.36ms
step:1366/1770 train_time:133374ms step_avg:98.36ms
step:1367/1770 train_time:133477ms step_avg:98.36ms
step:1368/1770 train_time:133579ms step_avg:98.36ms
step:1369/1770 train_time:133681ms step_avg:98.37ms
step:1370/1770 train_time:133784ms step_avg:98.37ms
step:1371/1770 train_time:133886ms step_avg:98.37ms
step:1372/1770 train_time:133988ms step_avg:98.38ms
step:1373/1770 train_time:134090ms step_avg:98.38ms
step:1374/1770 train_time:134194ms step_avg:98.38ms
step:1375/1770 train_time:134295ms step_avg:98.38ms
step:1375/1770 val_loss:3.3818 train_time:134396ms step_avg:98.46ms
step:1376/1770 train_time:134418ms step_avg:98.40ms
step:1377/1770 train_time:134508ms step_avg:98.40ms
step:1378/1770 train_time:134610ms step_avg:98.40ms
step:1379/1770 train_time:134712ms step_avg:98.40ms
step:1380/1770 train_time:134813ms step_avg:98.40ms
step:1381/1770 train_time:134916ms step_avg:98.41ms
step:1382/1770 train_time:135017ms step_avg:98.41ms
step:1383/1770 train_time:135120ms step_avg:98.41ms
step:1384/1770 train_time:135222ms step_avg:98.42ms
step:1385/1770 train_time:135325ms step_avg:98.42ms
step:1386/1770 train_time:135428ms step_avg:98.42ms
step:1387/1770 train_time:135531ms step_avg:98.43ms
step:1388/1770 train_time:135634ms step_avg:98.43ms
step:1389/1770 train_time:135736ms step_avg:98.43ms
step:1390/1770 train_time:135838ms step_avg:98.43ms
step:1391/1770 train_time:135940ms step_avg:98.44ms
step:1392/1770 train_time:136043ms step_avg:98.44ms
step:1393/1770 train_time:136144ms step_avg:98.44ms
step:1394/1770 train_time:136246ms step_avg:98.44ms
step:1395/1770 train_time:136349ms step_avg:98.45ms
step:1396/1770 train_time:136453ms step_avg:98.45ms
step:1397/1770 train_time:136554ms step_avg:98.45ms
step:1398/1770 train_time:136657ms step_avg:98.46ms
step:1399/1770 train_time:136759ms step_avg:98.46ms
step:1400/1770 train_time:136863ms step_avg:98.46ms
step:1401/1770 train_time:136965ms step_avg:98.47ms
step:1402/1770 train_time:137067ms step_avg:98.47ms
step:1403/1770 train_time:137169ms step_avg:98.47ms
step:1404/1770 train_time:137272ms step_avg:98.47ms
step:1405/1770 train_time:137374ms step_avg:98.48ms
step:1406/1770 train_time:137476ms step_avg:98.48ms
step:1407/1770 train_time:137578ms step_avg:98.48ms
step:1408/1770 train_time:137680ms step_avg:98.48ms
step:1409/1770 train_time:137783ms step_avg:98.49ms
step:1410/1770 train_time:137885ms step_avg:98.49ms
step:1411/1770 train_time:137988ms step_avg:98.49ms
step:1412/1770 train_time:138089ms step_avg:98.49ms
step:1413/1770 train_time:138191ms step_avg:98.50ms
step:1414/1770 train_time:138294ms step_avg:98.50ms
step:1415/1770 train_time:138396ms step_avg:98.50ms
step:1416/1770 train_time:138499ms step_avg:98.51ms
step:1417/1770 train_time:138602ms step_avg:98.51ms
step:1418/1770 train_time:138704ms step_avg:98.51ms
step:1419/1770 train_time:138807ms step_avg:98.51ms
step:1420/1770 train_time:138908ms step_avg:98.52ms
step:1421/1770 train_time:139010ms step_avg:98.52ms
step:1422/1770 train_time:139112ms step_avg:98.52ms
step:1423/1770 train_time:139214ms step_avg:98.52ms
step:1424/1770 train_time:139316ms step_avg:98.53ms
step:1425/1770 train_time:139418ms step_avg:98.53ms
step:1426/1770 train_time:139520ms step_avg:98.53ms
step:1427/1770 train_time:139623ms step_avg:98.53ms
step:1428/1770 train_time:139727ms step_avg:98.54ms
step:1429/1770 train_time:139829ms step_avg:98.54ms
step:1430/1770 train_time:139930ms step_avg:98.54ms
step:1431/1770 train_time:140033ms step_avg:98.55ms
step:1432/1770 train_time:140134ms step_avg:98.55ms
step:1433/1770 train_time:140236ms step_avg:98.55ms
step:1434/1770 train_time:140337ms step_avg:98.55ms
step:1435/1770 train_time:140440ms step_avg:98.55ms
step:1436/1770 train_time:140544ms step_avg:98.56ms
step:1437/1770 train_time:140646ms step_avg:98.56ms
step:1438/1770 train_time:140747ms step_avg:98.56ms
step:1439/1770 train_time:140849ms step_avg:98.56ms
step:1440/1770 train_time:140950ms step_avg:98.57ms
step:1441/1770 train_time:141056ms step_avg:98.57ms
step:1442/1770 train_time:141157ms step_avg:98.57ms
step:1443/1770 train_time:141260ms step_avg:98.58ms
step:1444/1770 train_time:141363ms step_avg:98.58ms
step:1445/1770 train_time:141466ms step_avg:98.58ms
step:1446/1770 train_time:141568ms step_avg:98.59ms
step:1447/1770 train_time:141672ms step_avg:98.59ms
step:1448/1770 train_time:141776ms step_avg:98.59ms
step:1449/1770 train_time:141881ms step_avg:98.60ms
step:1450/1770 train_time:141984ms step_avg:98.60ms
step:1451/1770 train_time:142088ms step_avg:98.60ms
step:1452/1770 train_time:142190ms step_avg:98.61ms
step:1453/1770 train_time:142293ms step_avg:98.61ms
step:1454/1770 train_time:142396ms step_avg:98.61ms
step:1455/1770 train_time:142501ms step_avg:98.62ms
step:1456/1770 train_time:142605ms step_avg:98.62ms
step:1457/1770 train_time:142709ms step_avg:98.62ms
step:1458/1770 train_time:142812ms step_avg:98.63ms
step:1459/1770 train_time:142916ms step_avg:98.63ms
step:1460/1770 train_time:143021ms step_avg:98.64ms
step:1461/1770 train_time:143126ms step_avg:98.64ms
step:1462/1770 train_time:143229ms step_avg:98.64ms
step:1463/1770 train_time:143332ms step_avg:98.65ms
step:1464/1770 train_time:143437ms step_avg:98.65ms
step:1465/1770 train_time:143541ms step_avg:98.65ms
step:1466/1770 train_time:143645ms step_avg:98.66ms
step:1467/1770 train_time:143750ms step_avg:98.66ms
step:1468/1770 train_time:143853ms step_avg:98.66ms
step:1469/1770 train_time:143956ms step_avg:98.67ms
step:1470/1770 train_time:144059ms step_avg:98.67ms
step:1471/1770 train_time:144162ms step_avg:98.67ms
step:1472/1770 train_time:144265ms step_avg:98.68ms
step:1473/1770 train_time:144369ms step_avg:98.68ms
step:1474/1770 train_time:144473ms step_avg:98.68ms
step:1475/1770 train_time:144576ms step_avg:98.69ms
step:1476/1770 train_time:144679ms step_avg:98.69ms
step:1477/1770 train_time:144786ms step_avg:98.70ms
step:1478/1770 train_time:144890ms step_avg:98.70ms
step:1479/1770 train_time:144992ms step_avg:98.70ms
step:1480/1770 train_time:145095ms step_avg:98.70ms
step:1481/1770 train_time:145203ms step_avg:98.71ms
step:1482/1770 train_time:145305ms step_avg:98.71ms
step:1483/1770 train_time:145409ms step_avg:98.72ms
step:1484/1770 train_time:145511ms step_avg:98.72ms
step:1485/1770 train_time:145613ms step_avg:98.72ms
step:1486/1770 train_time:145716ms step_avg:98.72ms
step:1487/1770 train_time:145820ms step_avg:98.73ms
step:1488/1770 train_time:145925ms step_avg:98.73ms
step:1489/1770 train_time:146029ms step_avg:98.73ms
step:1490/1770 train_time:146132ms step_avg:98.74ms
step:1491/1770 train_time:146235ms step_avg:98.74ms
step:1492/1770 train_time:146340ms step_avg:98.75ms
step:1493/1770 train_time:146446ms step_avg:98.75ms
step:1494/1770 train_time:146552ms step_avg:98.75ms
step:1495/1770 train_time:146655ms step_avg:98.76ms
step:1496/1770 train_time:146758ms step_avg:98.76ms
step:1497/1770 train_time:146862ms step_avg:98.76ms
step:1498/1770 train_time:146965ms step_avg:98.77ms
step:1499/1770 train_time:147069ms step_avg:98.77ms
step:1500/1770 train_time:147171ms step_avg:98.77ms
step:1500/1770 val_loss:3.3444 train_time:147272ms step_avg:98.84ms
step:1501/1770 train_time:147293ms step_avg:98.79ms
step:1502/1770 train_time:147384ms step_avg:98.78ms
step:1503/1770 train_time:147487ms step_avg:98.79ms
step:1504/1770 train_time:147591ms step_avg:98.79ms
step:1505/1770 train_time:147696ms step_avg:98.79ms
step:1506/1770 train_time:147799ms step_avg:98.80ms
step:1507/1770 train_time:147902ms step_avg:98.80ms
step:1508/1770 train_time:148007ms step_avg:98.80ms
step:1509/1770 train_time:148111ms step_avg:98.81ms
step:1510/1770 train_time:148212ms step_avg:98.81ms
step:1511/1770 train_time:148317ms step_avg:98.81ms
step:1512/1770 train_time:148422ms step_avg:98.82ms
step:1513/1770 train_time:148527ms step_avg:98.82ms
step:1514/1770 train_time:148630ms step_avg:98.82ms
step:1515/1770 train_time:148733ms step_avg:98.83ms
step:1516/1770 train_time:148836ms step_avg:98.83ms
step:1517/1770 train_time:148939ms step_avg:98.83ms
step:1518/1770 train_time:149044ms step_avg:98.84ms
step:1519/1770 train_time:149147ms step_avg:98.84ms
step:1520/1770 train_time:149252ms step_avg:98.84ms
step:1521/1770 train_time:149356ms step_avg:98.85ms
step:1522/1770 train_time:149459ms step_avg:98.85ms
step:1523/1770 train_time:149564ms step_avg:98.85ms
step:1524/1770 train_time:149667ms step_avg:98.86ms
step:1525/1770 train_time:149772ms step_avg:98.86ms
step:1526/1770 train_time:149874ms step_avg:98.86ms
step:1527/1770 train_time:149976ms step_avg:98.86ms
step:1528/1770 train_time:150082ms step_avg:98.87ms
step:1529/1770 train_time:150185ms step_avg:98.87ms
step:1530/1770 train_time:150287ms step_avg:98.87ms
step:1531/1770 train_time:150390ms step_avg:98.88ms
step:1532/1770 train_time:150494ms step_avg:98.88ms
step:1533/1770 train_time:150598ms step_avg:98.88ms
step:1534/1770 train_time:150701ms step_avg:98.89ms
step:1535/1770 train_time:150805ms step_avg:98.89ms
step:1536/1770 train_time:150909ms step_avg:98.89ms
step:1537/1770 train_time:151012ms step_avg:98.89ms
step:1538/1770 train_time:151117ms step_avg:98.90ms
step:1539/1770 train_time:151219ms step_avg:98.90ms
step:1540/1770 train_time:151325ms step_avg:98.91ms
step:1541/1770 train_time:151430ms step_avg:98.91ms
step:1542/1770 train_time:151533ms step_avg:98.91ms
step:1543/1770 train_time:151634ms step_avg:98.91ms
step:1544/1770 train_time:151740ms step_avg:98.92ms
step:1545/1770 train_time:151844ms step_avg:98.92ms
step:1546/1770 train_time:151948ms step_avg:98.92ms
step:1547/1770 train_time:152051ms step_avg:98.93ms
step:1548/1770 train_time:152154ms step_avg:98.93ms
step:1549/1770 train_time:152257ms step_avg:98.93ms
step:1550/1770 train_time:152361ms step_avg:98.94ms
step:1551/1770 train_time:152464ms step_avg:98.94ms
step:1552/1770 train_time:152570ms step_avg:98.94ms
step:1553/1770 train_time:152673ms step_avg:98.95ms
step:1554/1770 train_time:152775ms step_avg:98.95ms
step:1555/1770 train_time:152879ms step_avg:98.95ms
step:1556/1770 train_time:152982ms step_avg:98.95ms
step:1557/1770 train_time:153085ms step_avg:98.96ms
step:1558/1770 train_time:153189ms step_avg:98.96ms
step:1559/1770 train_time:153293ms step_avg:98.96ms
step:1560/1770 train_time:153396ms step_avg:98.97ms
step:1561/1770 train_time:153502ms step_avg:98.97ms
step:1562/1770 train_time:153605ms step_avg:98.97ms
step:1563/1770 train_time:153709ms step_avg:98.98ms
step:1564/1770 train_time:153811ms step_avg:98.98ms
step:1565/1770 train_time:153914ms step_avg:98.98ms
step:1566/1770 train_time:154017ms step_avg:98.98ms
step:1567/1770 train_time:154121ms step_avg:98.99ms
step:1568/1770 train_time:154224ms step_avg:98.99ms
step:1569/1770 train_time:154332ms step_avg:98.99ms
step:1570/1770 train_time:154435ms step_avg:99.00ms
step:1571/1770 train_time:154537ms step_avg:99.00ms
step:1572/1770 train_time:154642ms step_avg:99.00ms
step:1573/1770 train_time:154748ms step_avg:99.01ms
step:1574/1770 train_time:154851ms step_avg:99.01ms
step:1575/1770 train_time:154952ms step_avg:99.01ms
step:1576/1770 train_time:155055ms step_avg:99.01ms
step:1577/1770 train_time:155159ms step_avg:99.02ms
step:1578/1770 train_time:155265ms step_avg:99.02ms
step:1579/1770 train_time:155368ms step_avg:99.02ms
step:1580/1770 train_time:155471ms step_avg:99.03ms
step:1581/1770 train_time:155576ms step_avg:99.03ms
step:1582/1770 train_time:155681ms step_avg:99.03ms
step:1583/1770 train_time:155785ms step_avg:99.04ms
step:1584/1770 train_time:155891ms step_avg:99.04ms
step:1585/1770 train_time:155994ms step_avg:99.04ms
step:1586/1770 train_time:156100ms step_avg:99.05ms
step:1587/1770 train_time:156204ms step_avg:99.05ms
step:1588/1770 train_time:156308ms step_avg:99.05ms
step:1589/1770 train_time:156413ms step_avg:99.06ms
step:1590/1770 train_time:156516ms step_avg:99.06ms
step:1591/1770 train_time:156618ms step_avg:99.06ms
step:1592/1770 train_time:156722ms step_avg:99.07ms
step:1593/1770 train_time:156826ms step_avg:99.07ms
step:1594/1770 train_time:156929ms step_avg:99.07ms
step:1595/1770 train_time:157032ms step_avg:99.07ms
step:1596/1770 train_time:157136ms step_avg:99.08ms
step:1597/1770 train_time:157239ms step_avg:99.08ms
step:1598/1770 train_time:157342ms step_avg:99.08ms
step:1599/1770 train_time:157447ms step_avg:99.09ms
step:1600/1770 train_time:157553ms step_avg:99.09ms
step:1601/1770 train_time:157657ms step_avg:99.09ms
step:1602/1770 train_time:157761ms step_avg:99.10ms
step:1603/1770 train_time:157865ms step_avg:99.10ms
step:1604/1770 train_time:157968ms step_avg:99.10ms
step:1605/1770 train_time:158072ms step_avg:99.10ms
step:1606/1770 train_time:158175ms step_avg:99.11ms
step:1607/1770 train_time:158281ms step_avg:99.11ms
step:1608/1770 train_time:158384ms step_avg:99.11ms
step:1609/1770 train_time:158488ms step_avg:99.12ms
step:1610/1770 train_time:158594ms step_avg:99.12ms
step:1611/1770 train_time:158699ms step_avg:99.13ms
step:1612/1770 train_time:158803ms step_avg:99.13ms
step:1613/1770 train_time:158908ms step_avg:99.13ms
step:1614/1770 train_time:159012ms step_avg:99.13ms
step:1615/1770 train_time:159115ms step_avg:99.14ms
step:1616/1770 train_time:159218ms step_avg:99.14ms
step:1617/1770 train_time:159323ms step_avg:99.14ms
step:1618/1770 train_time:159428ms step_avg:99.15ms
step:1619/1770 train_time:159532ms step_avg:99.15ms
step:1620/1770 train_time:159635ms step_avg:99.15ms
step:1621/1770 train_time:159738ms step_avg:99.15ms
step:1622/1770 train_time:159842ms step_avg:99.16ms
step:1623/1770 train_time:159949ms step_avg:99.16ms
step:1624/1770 train_time:160052ms step_avg:99.17ms
step:1625/1770 train_time:160155ms step_avg:99.17ms
step:1625/1770 val_loss:3.3099 train_time:160256ms step_avg:99.23ms
step:1626/1770 train_time:160278ms step_avg:99.18ms
step:1627/1770 train_time:160366ms step_avg:99.18ms
step:1628/1770 train_time:160469ms step_avg:99.18ms
step:1629/1770 train_time:160573ms step_avg:99.18ms
step:1630/1770 train_time:160676ms step_avg:99.18ms
step:1631/1770 train_time:160779ms step_avg:99.18ms
step:1632/1770 train_time:160881ms step_avg:99.19ms
step:1633/1770 train_time:160984ms step_avg:99.19ms
step:1634/1770 train_time:161087ms step_avg:99.19ms
step:1635/1770 train_time:161191ms step_avg:99.19ms
step:1636/1770 train_time:161295ms step_avg:99.20ms
step:1637/1770 train_time:161400ms step_avg:99.20ms
step:1638/1770 train_time:161503ms step_avg:99.20ms
step:1639/1770 train_time:161607ms step_avg:99.21ms
step:1640/1770 train_time:161712ms step_avg:99.21ms
step:1641/1770 train_time:161815ms step_avg:99.21ms
step:1642/1770 train_time:161917ms step_avg:99.21ms
step:1643/1770 train_time:162020ms step_avg:99.22ms
step:1644/1770 train_time:162125ms step_avg:99.22ms
step:1645/1770 train_time:162229ms step_avg:99.22ms
step:1646/1770 train_time:162334ms step_avg:99.23ms
step:1647/1770 train_time:162438ms step_avg:99.23ms
step:1648/1770 train_time:162540ms step_avg:99.23ms
step:1649/1770 train_time:162643ms step_avg:99.23ms
step:1650/1770 train_time:162747ms step_avg:99.24ms
step:1651/1770 train_time:162851ms step_avg:99.24ms
step:1652/1770 train_time:162955ms step_avg:99.24ms
step:1653/1770 train_time:163058ms step_avg:99.24ms
step:1654/1770 train_time:163164ms step_avg:99.25ms
step:1655/1770 train_time:163271ms step_avg:99.25ms
step:1656/1770 train_time:163374ms step_avg:99.26ms
step:1657/1770 train_time:163480ms step_avg:99.26ms
step:1658/1770 train_time:163583ms step_avg:99.26ms
step:1659/1770 train_time:163688ms step_avg:99.26ms
step:1660/1770 train_time:163792ms step_avg:99.27ms
step:1661/1770 train_time:163897ms step_avg:99.27ms
step:1662/1770 train_time:164000ms step_avg:99.27ms
step:1663/1770 train_time:164102ms step_avg:99.28ms
step:1664/1770 train_time:164205ms step_avg:99.28ms
step:1665/1770 train_time:164309ms step_avg:99.28ms
step:1666/1770 train_time:164414ms step_avg:99.28ms
step:1667/1770 train_time:164517ms step_avg:99.29ms
step:1668/1770 train_time:164620ms step_avg:99.29ms
step:1669/1770 train_time:164722ms step_avg:99.29ms
step:1670/1770 train_time:164826ms step_avg:99.29ms
step:1671/1770 train_time:164930ms step_avg:99.30ms
step:1672/1770 train_time:165035ms step_avg:99.30ms
step:1673/1770 train_time:165139ms step_avg:99.30ms
step:1674/1770 train_time:165242ms step_avg:99.30ms
step:1675/1770 train_time:165344ms step_avg:99.31ms
step:1676/1770 train_time:165449ms step_avg:99.31ms
step:1677/1770 train_time:165557ms step_avg:99.31ms
step:1678/1770 train_time:165659ms step_avg:99.32ms
step:1679/1770 train_time:165763ms step_avg:99.32ms
step:1680/1770 train_time:165866ms step_avg:99.32ms
step:1681/1770 train_time:165970ms step_avg:99.32ms
step:1682/1770 train_time:166076ms step_avg:99.33ms
step:1683/1770 train_time:166179ms step_avg:99.33ms
step:1684/1770 train_time:166281ms step_avg:99.33ms
step:1685/1770 train_time:166384ms step_avg:99.33ms
step:1686/1770 train_time:166489ms step_avg:99.34ms
step:1687/1770 train_time:166594ms step_avg:99.34ms
step:1688/1770 train_time:166697ms step_avg:99.34ms
step:1689/1770 train_time:166801ms step_avg:99.35ms
step:1690/1770 train_time:166904ms step_avg:99.35ms
step:1691/1770 train_time:167008ms step_avg:99.35ms
step:1692/1770 train_time:167112ms step_avg:99.35ms
step:1693/1770 train_time:167217ms step_avg:99.36ms
step:1694/1770 train_time:167319ms step_avg:99.36ms
step:1695/1770 train_time:167423ms step_avg:99.36ms
step:1696/1770 train_time:167528ms step_avg:99.36ms
step:1697/1770 train_time:167634ms step_avg:99.37ms
step:1698/1770 train_time:167737ms step_avg:99.37ms
step:1699/1770 train_time:167840ms step_avg:99.37ms
step:1700/1770 train_time:167943ms step_avg:99.37ms
step:1701/1770 train_time:168046ms step_avg:99.38ms
step:1702/1770 train_time:168150ms step_avg:99.38ms
step:1703/1770 train_time:168254ms step_avg:99.38ms
step:1704/1770 train_time:168357ms step_avg:99.38ms
step:1705/1770 train_time:168459ms step_avg:99.39ms
step:1706/1770 train_time:168562ms step_avg:99.39ms
step:1707/1770 train_time:168666ms step_avg:99.39ms
step:1708/1770 train_time:168771ms step_avg:99.39ms
step:1709/1770 train_time:168876ms step_avg:99.40ms
step:1710/1770 train_time:168983ms step_avg:99.40ms
step:1711/1770 train_time:169090ms step_avg:99.41ms
step:1712/1770 train_time:169194ms step_avg:99.41ms
step:1713/1770 train_time:169297ms step_avg:99.41ms
step:1714/1770 train_time:169401ms step_avg:99.41ms
step:1715/1770 train_time:169505ms step_avg:99.42ms
step:1716/1770 train_time:169609ms step_avg:99.42ms
step:1717/1770 train_time:169715ms step_avg:99.42ms
step:1718/1770 train_time:169819ms step_avg:99.43ms
step:1719/1770 train_time:169924ms step_avg:99.43ms
step:1720/1770 train_time:170030ms step_avg:99.43ms
step:1721/1770 train_time:170133ms step_avg:99.44ms
step:1722/1770 train_time:170240ms step_avg:99.44ms
step:1723/1770 train_time:170345ms step_avg:99.44ms
step:1724/1770 train_time:170452ms step_avg:99.45ms
step:1725/1770 train_time:170558ms step_avg:99.45ms
step:1726/1770 train_time:170664ms step_avg:99.45ms
step:1727/1770 train_time:170768ms step_avg:99.46ms
step:1728/1770 train_time:170874ms step_avg:99.46ms
step:1729/1770 train_time:170978ms step_avg:99.46ms
step:1730/1770 train_time:171083ms step_avg:99.47ms
step:1731/1770 train_time:171189ms step_avg:99.47ms
step:1732/1770 train_time:171293ms step_avg:99.47ms
step:1733/1770 train_time:171399ms step_avg:99.48ms
step:1734/1770 train_time:171503ms step_avg:99.48ms
step:1735/1770 train_time:171608ms step_avg:99.48ms
step:1736/1770 train_time:171711ms step_avg:99.48ms
step:1737/1770 train_time:171816ms step_avg:99.49ms
step:1738/1770 train_time:171919ms step_avg:99.49ms
step:1739/1770 train_time:172023ms step_avg:99.49ms
step:1740/1770 train_time:172127ms step_avg:99.50ms
step:1741/1770 train_time:172235ms step_avg:99.50ms
step:1742/1770 train_time:172342ms step_avg:99.50ms
step:1743/1770 train_time:172447ms step_avg:99.51ms
step:1744/1770 train_time:172552ms step_avg:99.51ms
step:1745/1770 train_time:172656ms step_avg:99.51ms
step:1746/1770 train_time:172763ms step_avg:99.52ms
step:1747/1770 train_time:172866ms step_avg:99.52ms
step:1748/1770 train_time:172972ms step_avg:99.52ms
step:1749/1770 train_time:173077ms step_avg:99.53ms
step:1750/1770 train_time:173180ms step_avg:99.53ms
step:1750/1770 val_loss:3.2834 train_time:173283ms step_avg:99.59ms
step:1751/1770 train_time:173304ms step_avg:99.54ms
step:1752/1770 train_time:173394ms step_avg:99.54ms
step:1753/1770 train_time:173497ms step_avg:99.54ms
step:1754/1770 train_time:173602ms step_avg:99.54ms
step:1755/1770 train_time:173705ms step_avg:99.54ms
step:1756/1770 train_time:173810ms step_avg:99.55ms
step:1757/1770 train_time:173914ms step_avg:99.55ms
step:1758/1770 train_time:174017ms step_avg:99.55ms
step:1759/1770 train_time:174122ms step_avg:99.56ms
step:1760/1770 train_time:174226ms step_avg:99.56ms
step:1761/1770 train_time:174334ms step_avg:99.56ms
step:1762/1770 train_time:174443ms step_avg:99.57ms
step:1763/1770 train_time:174546ms step_avg:99.57ms
step:1764/1770 train_time:174651ms step_avg:99.57ms
step:1765/1770 train_time:174755ms step_avg:99.58ms
step:1766/1770 train_time:174863ms step_avg:99.58ms
step:1767/1770 train_time:174966ms step_avg:99.58ms
step:1768/1770 train_time:175070ms step_avg:99.59ms
step:1769/1770 train_time:175173ms step_avg:99.59ms
step:1770/1770 train_time:175277ms step_avg:99.59ms
step:1770/1770 val_loss:3.2801 train_time:175381ms step_avg:99.65ms
peak memory allocated: 28840 MiB reserved: 32192 MiB
