import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:33:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24358ms step_avg:nanms
step:2/1770 train_time:24884ms step_avg:nanms
step:3/1770 train_time:24981ms step_avg:nanms
step:4/1770 train_time:25074ms step_avg:nanms
step:5/1770 train_time:25168ms step_avg:nanms
step:6/1770 train_time:25262ms step_avg:nanms
step:7/1770 train_time:25356ms step_avg:nanms
step:8/1770 train_time:25449ms step_avg:nanms
step:9/1770 train_time:25543ms step_avg:nanms
step:10/1770 train_time:25636ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.73ms
step:14/1770 train_time:376ms step_avg:94.10ms
step:15/1770 train_time:471ms step_avg:94.21ms
step:16/1770 train_time:565ms step_avg:94.21ms
step:17/1770 train_time:660ms step_avg:94.26ms
step:18/1770 train_time:753ms step_avg:94.19ms
step:19/1770 train_time:848ms step_avg:94.19ms
step:20/1770 train_time:942ms step_avg:94.16ms
step:21/1770 train_time:1035ms step_avg:94.12ms
step:22/1770 train_time:1129ms step_avg:94.11ms
step:23/1770 train_time:1223ms step_avg:94.11ms
step:24/1770 train_time:1318ms step_avg:94.12ms
step:25/1770 train_time:1412ms step_avg:94.13ms
step:26/1770 train_time:1507ms step_avg:94.18ms
step:27/1770 train_time:1600ms step_avg:94.12ms
step:28/1770 train_time:1694ms step_avg:94.10ms
step:29/1770 train_time:1788ms step_avg:94.11ms
step:30/1770 train_time:1882ms step_avg:94.10ms
step:31/1770 train_time:1976ms step_avg:94.09ms
step:32/1770 train_time:2070ms step_avg:94.10ms
step:33/1770 train_time:2164ms step_avg:94.09ms
step:34/1770 train_time:2258ms step_avg:94.07ms
step:35/1770 train_time:2351ms step_avg:94.06ms
step:36/1770 train_time:2446ms step_avg:94.07ms
step:37/1770 train_time:2540ms step_avg:94.09ms
step:38/1770 train_time:2634ms step_avg:94.07ms
step:39/1770 train_time:2728ms step_avg:94.07ms
step:40/1770 train_time:2822ms step_avg:94.06ms
step:41/1770 train_time:2916ms step_avg:94.06ms
step:42/1770 train_time:3010ms step_avg:94.06ms
step:43/1770 train_time:3104ms step_avg:94.05ms
step:44/1770 train_time:3197ms step_avg:94.04ms
step:45/1770 train_time:3291ms step_avg:94.03ms
step:46/1770 train_time:3385ms step_avg:94.03ms
step:47/1770 train_time:3479ms step_avg:94.02ms
step:48/1770 train_time:3573ms step_avg:94.02ms
step:49/1770 train_time:3667ms step_avg:94.02ms
step:50/1770 train_time:3761ms step_avg:94.02ms
step:51/1770 train_time:3854ms step_avg:94.01ms
step:52/1770 train_time:3948ms step_avg:94.00ms
step:53/1770 train_time:4042ms step_avg:94.00ms
step:54/1770 train_time:4136ms step_avg:94.00ms
step:55/1770 train_time:4230ms step_avg:94.00ms
step:56/1770 train_time:4324ms step_avg:94.00ms
step:57/1770 train_time:4418ms step_avg:94.01ms
step:58/1770 train_time:4512ms step_avg:94.01ms
step:59/1770 train_time:4606ms step_avg:94.01ms
step:60/1770 train_time:4701ms step_avg:94.01ms
step:61/1770 train_time:4794ms step_avg:94.01ms
step:62/1770 train_time:4888ms step_avg:94.01ms
step:63/1770 train_time:4982ms step_avg:94.01ms
step:64/1770 train_time:5076ms step_avg:94.00ms
step:65/1770 train_time:5170ms step_avg:94.00ms
step:66/1770 train_time:5264ms step_avg:94.00ms
step:67/1770 train_time:5358ms step_avg:94.00ms
step:68/1770 train_time:5452ms step_avg:93.99ms
step:69/1770 train_time:5546ms step_avg:93.99ms
step:70/1770 train_time:5640ms step_avg:93.99ms
step:71/1770 train_time:5733ms step_avg:93.99ms
step:72/1770 train_time:5827ms step_avg:93.98ms
step:73/1770 train_time:5921ms step_avg:93.98ms
step:74/1770 train_time:6015ms step_avg:93.98ms
step:75/1770 train_time:6109ms step_avg:93.98ms
step:76/1770 train_time:6203ms step_avg:93.98ms
step:77/1770 train_time:6297ms step_avg:93.98ms
step:78/1770 train_time:6390ms step_avg:93.98ms
step:79/1770 train_time:6485ms step_avg:93.98ms
step:80/1770 train_time:6579ms step_avg:93.98ms
step:81/1770 train_time:6672ms step_avg:93.98ms
step:82/1770 train_time:6766ms step_avg:93.97ms
step:83/1770 train_time:6860ms step_avg:93.97ms
step:84/1770 train_time:6953ms step_avg:93.96ms
step:85/1770 train_time:7047ms step_avg:93.96ms
step:86/1770 train_time:7141ms step_avg:93.96ms
step:87/1770 train_time:7235ms step_avg:93.96ms
step:88/1770 train_time:7328ms step_avg:93.95ms
step:89/1770 train_time:7422ms step_avg:93.95ms
step:90/1770 train_time:7516ms step_avg:93.95ms
step:91/1770 train_time:7610ms step_avg:93.95ms
step:92/1770 train_time:7704ms step_avg:93.95ms
step:93/1770 train_time:7798ms step_avg:93.95ms
step:94/1770 train_time:7892ms step_avg:93.95ms
step:95/1770 train_time:7986ms step_avg:93.95ms
step:96/1770 train_time:8080ms step_avg:93.95ms
step:97/1770 train_time:8174ms step_avg:93.96ms
step:98/1770 train_time:8268ms step_avg:93.96ms
step:99/1770 train_time:8362ms step_avg:93.96ms
step:100/1770 train_time:8456ms step_avg:93.96ms
step:101/1770 train_time:8550ms step_avg:93.96ms
step:102/1770 train_time:8644ms step_avg:93.96ms
step:103/1770 train_time:8738ms step_avg:93.96ms
step:104/1770 train_time:8832ms step_avg:93.96ms
step:105/1770 train_time:8927ms step_avg:93.97ms
step:106/1770 train_time:9021ms step_avg:93.97ms
step:107/1770 train_time:9114ms step_avg:93.96ms
step:108/1770 train_time:9208ms step_avg:93.96ms
step:109/1770 train_time:9303ms step_avg:93.97ms
step:110/1770 train_time:9396ms step_avg:93.96ms
step:111/1770 train_time:9490ms step_avg:93.96ms
step:112/1770 train_time:9583ms step_avg:93.95ms
step:113/1770 train_time:9677ms step_avg:93.95ms
step:114/1770 train_time:9771ms step_avg:93.96ms
step:115/1770 train_time:9865ms step_avg:93.96ms
step:116/1770 train_time:9960ms step_avg:93.96ms
step:117/1770 train_time:10053ms step_avg:93.96ms
step:118/1770 train_time:10147ms step_avg:93.96ms
step:119/1770 train_time:10241ms step_avg:93.96ms
step:120/1770 train_time:10335ms step_avg:93.96ms
step:121/1770 train_time:10429ms step_avg:93.95ms
step:122/1770 train_time:10523ms step_avg:93.95ms
step:123/1770 train_time:10616ms step_avg:93.95ms
step:124/1770 train_time:10709ms step_avg:93.94ms
step:125/1770 train_time:10803ms step_avg:93.94ms
step:125/1770 val_loss:4.6440 train_time:10895ms step_avg:94.74ms
step:126/1770 train_time:10917ms step_avg:94.11ms
step:127/1770 train_time:11001ms step_avg:94.02ms
step:128/1770 train_time:11099ms step_avg:94.06ms
step:129/1770 train_time:11194ms step_avg:94.07ms
step:130/1770 train_time:11288ms step_avg:94.07ms
step:131/1770 train_time:11382ms step_avg:94.07ms
step:132/1770 train_time:11475ms step_avg:94.06ms
step:133/1770 train_time:11569ms step_avg:94.06ms
step:134/1770 train_time:11664ms step_avg:94.06ms
step:135/1770 train_time:11758ms step_avg:94.06ms
step:136/1770 train_time:11852ms step_avg:94.06ms
step:137/1770 train_time:11947ms step_avg:94.07ms
step:138/1770 train_time:12042ms step_avg:94.08ms
step:139/1770 train_time:12137ms step_avg:94.08ms
step:140/1770 train_time:12232ms step_avg:94.09ms
step:141/1770 train_time:12326ms step_avg:94.09ms
step:142/1770 train_time:12421ms step_avg:94.10ms
step:143/1770 train_time:12515ms step_avg:94.10ms
step:144/1770 train_time:12609ms step_avg:94.10ms
step:145/1770 train_time:12704ms step_avg:94.10ms
step:146/1770 train_time:12798ms step_avg:94.10ms
step:147/1770 train_time:12892ms step_avg:94.11ms
step:148/1770 train_time:12987ms step_avg:94.11ms
step:149/1770 train_time:13082ms step_avg:94.11ms
step:150/1770 train_time:13176ms step_avg:94.12ms
step:151/1770 train_time:13270ms step_avg:94.12ms
step:152/1770 train_time:13365ms step_avg:94.12ms
step:153/1770 train_time:13460ms step_avg:94.13ms
step:154/1770 train_time:13554ms step_avg:94.13ms
step:155/1770 train_time:13649ms step_avg:94.13ms
step:156/1770 train_time:13743ms step_avg:94.13ms
step:157/1770 train_time:13838ms step_avg:94.13ms
step:158/1770 train_time:13931ms step_avg:94.13ms
step:159/1770 train_time:14026ms step_avg:94.13ms
step:160/1770 train_time:14121ms step_avg:94.14ms
step:161/1770 train_time:14215ms step_avg:94.14ms
step:162/1770 train_time:14309ms step_avg:94.14ms
step:163/1770 train_time:14404ms step_avg:94.14ms
step:164/1770 train_time:14499ms step_avg:94.15ms
step:165/1770 train_time:14594ms step_avg:94.15ms
step:166/1770 train_time:14688ms step_avg:94.16ms
step:167/1770 train_time:14783ms step_avg:94.16ms
step:168/1770 train_time:14878ms step_avg:94.16ms
step:169/1770 train_time:14972ms step_avg:94.17ms
step:170/1770 train_time:15067ms step_avg:94.17ms
step:171/1770 train_time:15161ms step_avg:94.17ms
step:172/1770 train_time:15256ms step_avg:94.17ms
step:173/1770 train_time:15350ms step_avg:94.17ms
step:174/1770 train_time:15445ms step_avg:94.18ms
step:175/1770 train_time:15540ms step_avg:94.18ms
step:176/1770 train_time:15634ms step_avg:94.18ms
step:177/1770 train_time:15728ms step_avg:94.18ms
step:178/1770 train_time:15824ms step_avg:94.19ms
step:179/1770 train_time:15918ms step_avg:94.19ms
step:180/1770 train_time:16013ms step_avg:94.19ms
step:181/1770 train_time:16107ms step_avg:94.19ms
step:182/1770 train_time:16202ms step_avg:94.20ms
step:183/1770 train_time:16296ms step_avg:94.19ms
step:184/1770 train_time:16390ms step_avg:94.20ms
step:185/1770 train_time:16485ms step_avg:94.20ms
step:186/1770 train_time:16580ms step_avg:94.21ms
step:187/1770 train_time:16675ms step_avg:94.21ms
step:188/1770 train_time:16769ms step_avg:94.21ms
step:189/1770 train_time:16864ms step_avg:94.21ms
step:190/1770 train_time:16959ms step_avg:94.22ms
step:191/1770 train_time:17053ms step_avg:94.22ms
step:192/1770 train_time:17147ms step_avg:94.22ms
step:193/1770 train_time:17242ms step_avg:94.22ms
step:194/1770 train_time:17336ms step_avg:94.22ms
step:195/1770 train_time:17430ms step_avg:94.22ms
step:196/1770 train_time:17525ms step_avg:94.22ms
step:197/1770 train_time:17620ms step_avg:94.23ms
step:198/1770 train_time:17714ms step_avg:94.23ms
step:199/1770 train_time:17808ms step_avg:94.22ms
step:200/1770 train_time:17904ms step_avg:94.23ms
step:201/1770 train_time:17998ms step_avg:94.23ms
step:202/1770 train_time:18092ms step_avg:94.23ms
step:203/1770 train_time:18187ms step_avg:94.23ms
step:204/1770 train_time:18282ms step_avg:94.24ms
step:205/1770 train_time:18377ms step_avg:94.24ms
step:206/1770 train_time:18471ms step_avg:94.24ms
step:207/1770 train_time:18565ms step_avg:94.24ms
step:208/1770 train_time:18660ms step_avg:94.24ms
step:209/1770 train_time:18754ms step_avg:94.24ms
step:210/1770 train_time:18848ms step_avg:94.24ms
step:211/1770 train_time:18942ms step_avg:94.24ms
step:212/1770 train_time:19036ms step_avg:94.24ms
step:213/1770 train_time:19131ms step_avg:94.24ms
step:214/1770 train_time:19226ms step_avg:94.24ms
step:215/1770 train_time:19321ms step_avg:94.25ms
step:216/1770 train_time:19416ms step_avg:94.25ms
step:217/1770 train_time:19510ms step_avg:94.25ms
step:218/1770 train_time:19605ms step_avg:94.26ms
step:219/1770 train_time:19700ms step_avg:94.26ms
step:220/1770 train_time:19794ms step_avg:94.26ms
step:221/1770 train_time:19889ms step_avg:94.26ms
step:222/1770 train_time:19984ms step_avg:94.26ms
step:223/1770 train_time:20078ms step_avg:94.26ms
step:224/1770 train_time:20173ms step_avg:94.27ms
step:225/1770 train_time:20268ms step_avg:94.27ms
step:226/1770 train_time:20362ms step_avg:94.27ms
step:227/1770 train_time:20456ms step_avg:94.27ms
step:228/1770 train_time:20551ms step_avg:94.27ms
step:229/1770 train_time:20645ms step_avg:94.27ms
step:230/1770 train_time:20741ms step_avg:94.28ms
step:231/1770 train_time:20835ms step_avg:94.28ms
step:232/1770 train_time:20929ms step_avg:94.28ms
step:233/1770 train_time:21025ms step_avg:94.28ms
step:234/1770 train_time:21120ms step_avg:94.29ms
step:235/1770 train_time:21214ms step_avg:94.29ms
step:236/1770 train_time:21309ms step_avg:94.29ms
step:237/1770 train_time:21403ms step_avg:94.29ms
step:238/1770 train_time:21497ms step_avg:94.29ms
step:239/1770 train_time:21592ms step_avg:94.29ms
step:240/1770 train_time:21686ms step_avg:94.29ms
step:241/1770 train_time:21781ms step_avg:94.29ms
step:242/1770 train_time:21876ms step_avg:94.29ms
step:243/1770 train_time:21971ms step_avg:94.29ms
step:244/1770 train_time:22065ms step_avg:94.30ms
step:245/1770 train_time:22160ms step_avg:94.30ms
step:246/1770 train_time:22254ms step_avg:94.30ms
step:247/1770 train_time:22349ms step_avg:94.30ms
step:248/1770 train_time:22443ms step_avg:94.30ms
step:249/1770 train_time:22538ms step_avg:94.30ms
step:250/1770 train_time:22632ms step_avg:94.30ms
step:250/1770 val_loss:4.1011 train_time:22725ms step_avg:94.69ms
step:251/1770 train_time:22746ms step_avg:94.38ms
step:252/1770 train_time:22830ms step_avg:94.34ms
step:253/1770 train_time:22928ms step_avg:94.35ms
step:254/1770 train_time:23024ms step_avg:94.36ms
step:255/1770 train_time:23119ms step_avg:94.36ms
step:256/1770 train_time:23213ms step_avg:94.36ms
step:257/1770 train_time:23307ms step_avg:94.36ms
step:258/1770 train_time:23402ms step_avg:94.36ms
step:259/1770 train_time:23499ms step_avg:94.37ms
step:260/1770 train_time:23589ms step_avg:94.36ms
step:261/1770 train_time:23684ms step_avg:94.36ms
step:262/1770 train_time:23779ms step_avg:94.36ms
step:263/1770 train_time:23875ms step_avg:94.37ms
step:264/1770 train_time:23970ms step_avg:94.37ms
step:265/1770 train_time:24065ms step_avg:94.37ms
step:266/1770 train_time:24161ms step_avg:94.38ms
step:267/1770 train_time:24256ms step_avg:94.38ms
step:268/1770 train_time:24350ms step_avg:94.38ms
step:269/1770 train_time:24446ms step_avg:94.38ms
step:270/1770 train_time:24541ms step_avg:94.39ms
step:271/1770 train_time:24635ms step_avg:94.39ms
step:272/1770 train_time:24730ms step_avg:94.39ms
step:273/1770 train_time:24825ms step_avg:94.39ms
step:274/1770 train_time:24921ms step_avg:94.40ms
step:275/1770 train_time:25017ms step_avg:94.40ms
step:276/1770 train_time:25112ms step_avg:94.41ms
step:277/1770 train_time:25207ms step_avg:94.41ms
step:278/1770 train_time:25302ms step_avg:94.41ms
step:279/1770 train_time:25397ms step_avg:94.41ms
step:280/1770 train_time:25492ms step_avg:94.41ms
step:281/1770 train_time:25587ms step_avg:94.42ms
step:282/1770 train_time:25683ms step_avg:94.42ms
step:283/1770 train_time:25778ms step_avg:94.43ms
step:284/1770 train_time:25873ms step_avg:94.43ms
step:285/1770 train_time:25968ms step_avg:94.43ms
step:286/1770 train_time:26063ms step_avg:94.43ms
step:287/1770 train_time:26159ms step_avg:94.44ms
step:288/1770 train_time:26254ms step_avg:94.44ms
step:289/1770 train_time:26349ms step_avg:94.44ms
step:290/1770 train_time:26444ms step_avg:94.44ms
step:291/1770 train_time:26539ms step_avg:94.45ms
step:292/1770 train_time:26634ms step_avg:94.45ms
step:293/1770 train_time:26729ms step_avg:94.45ms
step:294/1770 train_time:26824ms step_avg:94.45ms
step:295/1770 train_time:26920ms step_avg:94.45ms
step:296/1770 train_time:27014ms step_avg:94.45ms
step:297/1770 train_time:27109ms step_avg:94.46ms
step:298/1770 train_time:27205ms step_avg:94.46ms
step:299/1770 train_time:27301ms step_avg:94.47ms
step:300/1770 train_time:27395ms step_avg:94.47ms
step:301/1770 train_time:27490ms step_avg:94.47ms
step:302/1770 train_time:27585ms step_avg:94.47ms
step:303/1770 train_time:27681ms step_avg:94.47ms
step:304/1770 train_time:27775ms step_avg:94.47ms
step:305/1770 train_time:27870ms step_avg:94.47ms
step:306/1770 train_time:27965ms step_avg:94.47ms
step:307/1770 train_time:28060ms step_avg:94.48ms
step:308/1770 train_time:28155ms step_avg:94.48ms
step:309/1770 train_time:28250ms step_avg:94.48ms
step:310/1770 train_time:28345ms step_avg:94.48ms
step:311/1770 train_time:28441ms step_avg:94.49ms
step:312/1770 train_time:28537ms step_avg:94.49ms
step:313/1770 train_time:28632ms step_avg:94.49ms
step:314/1770 train_time:28727ms step_avg:94.50ms
step:315/1770 train_time:28822ms step_avg:94.50ms
step:316/1770 train_time:28917ms step_avg:94.50ms
step:317/1770 train_time:29011ms step_avg:94.50ms
step:318/1770 train_time:29106ms step_avg:94.50ms
step:319/1770 train_time:29202ms step_avg:94.50ms
step:320/1770 train_time:29296ms step_avg:94.50ms
step:321/1770 train_time:29391ms step_avg:94.51ms
step:322/1770 train_time:29486ms step_avg:94.51ms
step:323/1770 train_time:29581ms step_avg:94.51ms
step:324/1770 train_time:29676ms step_avg:94.51ms
step:325/1770 train_time:29771ms step_avg:94.51ms
step:326/1770 train_time:29866ms step_avg:94.51ms
step:327/1770 train_time:29961ms step_avg:94.52ms
step:328/1770 train_time:30056ms step_avg:94.52ms
step:329/1770 train_time:30151ms step_avg:94.52ms
step:330/1770 train_time:30246ms step_avg:94.52ms
step:331/1770 train_time:30342ms step_avg:94.52ms
step:332/1770 train_time:30437ms step_avg:94.53ms
step:333/1770 train_time:30532ms step_avg:94.53ms
step:334/1770 train_time:30627ms step_avg:94.53ms
step:335/1770 train_time:30722ms step_avg:94.53ms
step:336/1770 train_time:30817ms step_avg:94.53ms
step:337/1770 train_time:30912ms step_avg:94.53ms
step:338/1770 train_time:31007ms step_avg:94.53ms
step:339/1770 train_time:31102ms step_avg:94.53ms
step:340/1770 train_time:31197ms step_avg:94.54ms
step:341/1770 train_time:31291ms step_avg:94.54ms
step:342/1770 train_time:31386ms step_avg:94.54ms
step:343/1770 train_time:31482ms step_avg:94.54ms
step:344/1770 train_time:31577ms step_avg:94.54ms
step:345/1770 train_time:31672ms step_avg:94.54ms
step:346/1770 train_time:31767ms step_avg:94.55ms
step:347/1770 train_time:31862ms step_avg:94.55ms
step:348/1770 train_time:31957ms step_avg:94.55ms
step:349/1770 train_time:32052ms step_avg:94.55ms
step:350/1770 train_time:32147ms step_avg:94.55ms
step:351/1770 train_time:32243ms step_avg:94.56ms
step:352/1770 train_time:32338ms step_avg:94.56ms
step:353/1770 train_time:32433ms step_avg:94.56ms
step:354/1770 train_time:32528ms step_avg:94.56ms
step:355/1770 train_time:32624ms step_avg:94.56ms
step:356/1770 train_time:32719ms step_avg:94.56ms
step:357/1770 train_time:32813ms step_avg:94.56ms
step:358/1770 train_time:32908ms step_avg:94.56ms
step:359/1770 train_time:33004ms step_avg:94.57ms
step:360/1770 train_time:33098ms step_avg:94.57ms
step:361/1770 train_time:33193ms step_avg:94.57ms
step:362/1770 train_time:33289ms step_avg:94.57ms
step:363/1770 train_time:33384ms step_avg:94.57ms
step:364/1770 train_time:33479ms step_avg:94.57ms
step:365/1770 train_time:33575ms step_avg:94.58ms
step:366/1770 train_time:33670ms step_avg:94.58ms
step:367/1770 train_time:33765ms step_avg:94.58ms
step:368/1770 train_time:33860ms step_avg:94.58ms
step:369/1770 train_time:33955ms step_avg:94.58ms
step:370/1770 train_time:34050ms step_avg:94.58ms
step:371/1770 train_time:34145ms step_avg:94.58ms
step:372/1770 train_time:34240ms step_avg:94.59ms
step:373/1770 train_time:34335ms step_avg:94.59ms
step:374/1770 train_time:34430ms step_avg:94.59ms
step:375/1770 train_time:34525ms step_avg:94.59ms
step:375/1770 val_loss:3.8973 train_time:34619ms step_avg:94.85ms
step:376/1770 train_time:34642ms step_avg:94.65ms
step:377/1770 train_time:34723ms step_avg:94.61ms
step:378/1770 train_time:34820ms step_avg:94.62ms
step:379/1770 train_time:34916ms step_avg:94.62ms
step:380/1770 train_time:35010ms step_avg:94.62ms
step:381/1770 train_time:35105ms step_avg:94.62ms
step:382/1770 train_time:35199ms step_avg:94.62ms
step:383/1770 train_time:35294ms step_avg:94.62ms
step:384/1770 train_time:35388ms step_avg:94.62ms
step:385/1770 train_time:35482ms step_avg:94.62ms
step:386/1770 train_time:35577ms step_avg:94.62ms
step:387/1770 train_time:35672ms step_avg:94.62ms
step:388/1770 train_time:35767ms step_avg:94.62ms
step:389/1770 train_time:35862ms step_avg:94.62ms
step:390/1770 train_time:35958ms step_avg:94.63ms
step:391/1770 train_time:36053ms step_avg:94.63ms
step:392/1770 train_time:36148ms step_avg:94.63ms
step:393/1770 train_time:36243ms step_avg:94.63ms
step:394/1770 train_time:36338ms step_avg:94.63ms
step:395/1770 train_time:36432ms step_avg:94.63ms
step:396/1770 train_time:36529ms step_avg:94.63ms
step:397/1770 train_time:36625ms step_avg:94.64ms
step:398/1770 train_time:36722ms step_avg:94.64ms
step:399/1770 train_time:36819ms step_avg:94.65ms
step:400/1770 train_time:36916ms step_avg:94.66ms
step:401/1770 train_time:37013ms step_avg:94.66ms
step:402/1770 train_time:37110ms step_avg:94.67ms
step:403/1770 train_time:37206ms step_avg:94.67ms
step:404/1770 train_time:37303ms step_avg:94.68ms
step:405/1770 train_time:37400ms step_avg:94.68ms
step:406/1770 train_time:37496ms step_avg:94.69ms
step:407/1770 train_time:37593ms step_avg:94.69ms
step:408/1770 train_time:37690ms step_avg:94.70ms
step:409/1770 train_time:37787ms step_avg:94.70ms
step:410/1770 train_time:37883ms step_avg:94.71ms
step:411/1770 train_time:37980ms step_avg:94.71ms
step:412/1770 train_time:38077ms step_avg:94.72ms
step:413/1770 train_time:38174ms step_avg:94.72ms
step:414/1770 train_time:38271ms step_avg:94.73ms
step:415/1770 train_time:38368ms step_avg:94.74ms
step:416/1770 train_time:38464ms step_avg:94.74ms
step:417/1770 train_time:38561ms step_avg:94.75ms
step:418/1770 train_time:38659ms step_avg:94.75ms
step:419/1770 train_time:38756ms step_avg:94.76ms
step:420/1770 train_time:38853ms step_avg:94.76ms
step:421/1770 train_time:38949ms step_avg:94.77ms
step:422/1770 train_time:39046ms step_avg:94.77ms
step:423/1770 train_time:39143ms step_avg:94.78ms
step:424/1770 train_time:39240ms step_avg:94.78ms
step:425/1770 train_time:39337ms step_avg:94.79ms
step:426/1770 train_time:39434ms step_avg:94.79ms
step:427/1770 train_time:39532ms step_avg:94.80ms
step:428/1770 train_time:39629ms step_avg:94.81ms
step:429/1770 train_time:39726ms step_avg:94.81ms
step:430/1770 train_time:39822ms step_avg:94.81ms
step:431/1770 train_time:39919ms step_avg:94.82ms
step:432/1770 train_time:40016ms step_avg:94.83ms
step:433/1770 train_time:40113ms step_avg:94.83ms
step:434/1770 train_time:40210ms step_avg:94.84ms
step:435/1770 train_time:40307ms step_avg:94.84ms
step:436/1770 train_time:40404ms step_avg:94.84ms
step:437/1770 train_time:40501ms step_avg:94.85ms
step:438/1770 train_time:40598ms step_avg:94.86ms
step:439/1770 train_time:40695ms step_avg:94.86ms
step:440/1770 train_time:40792ms step_avg:94.87ms
step:441/1770 train_time:40889ms step_avg:94.87ms
step:442/1770 train_time:40985ms step_avg:94.87ms
step:443/1770 train_time:41082ms step_avg:94.88ms
step:444/1770 train_time:41179ms step_avg:94.88ms
step:445/1770 train_time:41276ms step_avg:94.89ms
step:446/1770 train_time:41373ms step_avg:94.89ms
step:447/1770 train_time:41470ms step_avg:94.90ms
step:448/1770 train_time:41566ms step_avg:94.90ms
step:449/1770 train_time:41663ms step_avg:94.90ms
step:450/1770 train_time:41760ms step_avg:94.91ms
step:451/1770 train_time:41856ms step_avg:94.91ms
step:452/1770 train_time:41954ms step_avg:94.92ms
step:453/1770 train_time:42051ms step_avg:94.92ms
step:454/1770 train_time:42148ms step_avg:94.93ms
step:455/1770 train_time:42244ms step_avg:94.93ms
step:456/1770 train_time:42341ms step_avg:94.94ms
step:457/1770 train_time:42438ms step_avg:94.94ms
step:458/1770 train_time:42536ms step_avg:94.95ms
step:459/1770 train_time:42633ms step_avg:94.95ms
step:460/1770 train_time:42730ms step_avg:94.96ms
step:461/1770 train_time:42827ms step_avg:94.96ms
step:462/1770 train_time:42924ms step_avg:94.96ms
step:463/1770 train_time:43021ms step_avg:94.97ms
step:464/1770 train_time:43118ms step_avg:94.97ms
step:465/1770 train_time:43216ms step_avg:94.98ms
step:466/1770 train_time:43313ms step_avg:94.99ms
step:467/1770 train_time:43410ms step_avg:94.99ms
step:468/1770 train_time:43507ms step_avg:94.99ms
step:469/1770 train_time:43603ms step_avg:95.00ms
step:470/1770 train_time:43700ms step_avg:95.00ms
step:471/1770 train_time:43798ms step_avg:95.01ms
step:472/1770 train_time:43896ms step_avg:95.01ms
step:473/1770 train_time:43993ms step_avg:95.02ms
step:474/1770 train_time:44090ms step_avg:95.02ms
step:475/1770 train_time:44187ms step_avg:95.03ms
step:476/1770 train_time:44284ms step_avg:95.03ms
step:477/1770 train_time:44381ms step_avg:95.03ms
step:478/1770 train_time:44477ms step_avg:95.04ms
step:479/1770 train_time:44575ms step_avg:95.04ms
step:480/1770 train_time:44671ms step_avg:95.05ms
step:481/1770 train_time:44768ms step_avg:95.05ms
step:482/1770 train_time:44865ms step_avg:95.05ms
step:483/1770 train_time:44962ms step_avg:95.06ms
step:484/1770 train_time:45059ms step_avg:95.06ms
step:485/1770 train_time:45156ms step_avg:95.07ms
step:486/1770 train_time:45255ms step_avg:95.07ms
step:487/1770 train_time:45351ms step_avg:95.08ms
step:488/1770 train_time:45448ms step_avg:95.08ms
step:489/1770 train_time:45549ms step_avg:95.09ms
step:490/1770 train_time:45645ms step_avg:95.09ms
step:491/1770 train_time:45742ms step_avg:95.10ms
step:492/1770 train_time:45838ms step_avg:95.10ms
step:493/1770 train_time:45935ms step_avg:95.10ms
step:494/1770 train_time:46032ms step_avg:95.11ms
step:495/1770 train_time:46129ms step_avg:95.11ms
step:496/1770 train_time:46226ms step_avg:95.12ms
step:497/1770 train_time:46323ms step_avg:95.12ms
step:498/1770 train_time:46420ms step_avg:95.12ms
step:499/1770 train_time:46517ms step_avg:95.13ms
step:500/1770 train_time:46615ms step_avg:95.13ms
step:500/1770 val_loss:3.7497 train_time:46710ms step_avg:95.33ms
step:501/1770 train_time:46732ms step_avg:95.18ms
step:502/1770 train_time:46820ms step_avg:95.16ms
step:503/1770 train_time:46918ms step_avg:95.17ms
step:504/1770 train_time:47015ms step_avg:95.17ms
step:505/1770 train_time:47111ms step_avg:95.17ms
step:506/1770 train_time:47207ms step_avg:95.18ms
step:507/1770 train_time:47304ms step_avg:95.18ms
step:508/1770 train_time:47400ms step_avg:95.18ms
step:509/1770 train_time:47497ms step_avg:95.19ms
step:510/1770 train_time:47594ms step_avg:95.19ms
step:511/1770 train_time:47690ms step_avg:95.19ms
step:512/1770 train_time:47787ms step_avg:95.19ms
step:513/1770 train_time:47886ms step_avg:95.20ms
step:514/1770 train_time:47983ms step_avg:95.20ms
step:515/1770 train_time:48081ms step_avg:95.21ms
step:516/1770 train_time:48179ms step_avg:95.22ms
step:517/1770 train_time:48276ms step_avg:95.22ms
step:518/1770 train_time:48373ms step_avg:95.22ms
step:519/1770 train_time:48469ms step_avg:95.22ms
step:520/1770 train_time:48566ms step_avg:95.23ms
step:521/1770 train_time:48663ms step_avg:95.23ms
step:522/1770 train_time:48760ms step_avg:95.23ms
step:523/1770 train_time:48858ms step_avg:95.24ms
step:524/1770 train_time:48955ms step_avg:95.24ms
step:525/1770 train_time:49052ms step_avg:95.25ms
step:526/1770 train_time:49149ms step_avg:95.25ms
step:527/1770 train_time:49246ms step_avg:95.25ms
step:528/1770 train_time:49344ms step_avg:95.26ms
step:529/1770 train_time:49441ms step_avg:95.26ms
step:530/1770 train_time:49539ms step_avg:95.27ms
step:531/1770 train_time:49636ms step_avg:95.27ms
step:532/1770 train_time:49733ms step_avg:95.27ms
step:533/1770 train_time:49829ms step_avg:95.28ms
step:534/1770 train_time:49926ms step_avg:95.28ms
step:535/1770 train_time:50023ms step_avg:95.28ms
step:536/1770 train_time:50122ms step_avg:95.29ms
step:537/1770 train_time:50220ms step_avg:95.29ms
step:538/1770 train_time:50318ms step_avg:95.30ms
step:539/1770 train_time:50415ms step_avg:95.30ms
step:540/1770 train_time:50512ms step_avg:95.31ms
step:541/1770 train_time:50609ms step_avg:95.31ms
step:542/1770 train_time:50707ms step_avg:95.31ms
step:543/1770 train_time:50804ms step_avg:95.32ms
step:544/1770 train_time:50901ms step_avg:95.32ms
step:545/1770 train_time:50999ms step_avg:95.32ms
step:546/1770 train_time:51096ms step_avg:95.33ms
step:547/1770 train_time:51193ms step_avg:95.33ms
step:548/1770 train_time:51290ms step_avg:95.33ms
step:549/1770 train_time:51386ms step_avg:95.34ms
step:550/1770 train_time:51484ms step_avg:95.34ms
step:551/1770 train_time:51582ms step_avg:95.35ms
step:552/1770 train_time:51680ms step_avg:95.35ms
step:553/1770 train_time:51778ms step_avg:95.36ms
step:554/1770 train_time:51875ms step_avg:95.36ms
step:555/1770 train_time:51972ms step_avg:95.36ms
step:556/1770 train_time:52069ms step_avg:95.36ms
step:557/1770 train_time:52166ms step_avg:95.37ms
step:558/1770 train_time:52263ms step_avg:95.37ms
step:559/1770 train_time:52360ms step_avg:95.37ms
step:560/1770 train_time:52458ms step_avg:95.38ms
step:561/1770 train_time:52555ms step_avg:95.38ms
step:562/1770 train_time:52652ms step_avg:95.38ms
step:563/1770 train_time:52749ms step_avg:95.39ms
step:564/1770 train_time:52846ms step_avg:95.39ms
step:565/1770 train_time:52943ms step_avg:95.39ms
step:566/1770 train_time:53041ms step_avg:95.40ms
step:567/1770 train_time:53138ms step_avg:95.40ms
step:568/1770 train_time:53235ms step_avg:95.40ms
step:569/1770 train_time:53332ms step_avg:95.41ms
step:570/1770 train_time:53429ms step_avg:95.41ms
step:571/1770 train_time:53527ms step_avg:95.41ms
step:572/1770 train_time:53625ms step_avg:95.42ms
step:573/1770 train_time:53722ms step_avg:95.42ms
step:574/1770 train_time:53820ms step_avg:95.43ms
step:575/1770 train_time:53918ms step_avg:95.43ms
step:576/1770 train_time:54015ms step_avg:95.43ms
step:577/1770 train_time:54112ms step_avg:95.44ms
step:578/1770 train_time:54208ms step_avg:95.44ms
step:579/1770 train_time:54306ms step_avg:95.44ms
step:580/1770 train_time:54403ms step_avg:95.44ms
step:581/1770 train_time:54501ms step_avg:95.45ms
step:582/1770 train_time:54599ms step_avg:95.45ms
step:583/1770 train_time:54697ms step_avg:95.46ms
step:584/1770 train_time:54794ms step_avg:95.46ms
step:585/1770 train_time:54891ms step_avg:95.46ms
step:586/1770 train_time:54988ms step_avg:95.47ms
step:587/1770 train_time:55086ms step_avg:95.47ms
step:588/1770 train_time:55184ms step_avg:95.47ms
step:589/1770 train_time:55282ms step_avg:95.48ms
step:590/1770 train_time:55379ms step_avg:95.48ms
step:591/1770 train_time:55477ms step_avg:95.48ms
step:592/1770 train_time:55574ms step_avg:95.49ms
step:593/1770 train_time:55671ms step_avg:95.49ms
step:594/1770 train_time:55768ms step_avg:95.49ms
step:595/1770 train_time:55865ms step_avg:95.50ms
step:596/1770 train_time:55962ms step_avg:95.50ms
step:597/1770 train_time:56060ms step_avg:95.50ms
step:598/1770 train_time:56159ms step_avg:95.51ms
step:599/1770 train_time:56257ms step_avg:95.51ms
step:600/1770 train_time:56355ms step_avg:95.52ms
step:601/1770 train_time:56452ms step_avg:95.52ms
step:602/1770 train_time:56549ms step_avg:95.52ms
step:603/1770 train_time:56646ms step_avg:95.52ms
step:604/1770 train_time:56743ms step_avg:95.53ms
step:605/1770 train_time:56841ms step_avg:95.53ms
step:606/1770 train_time:56938ms step_avg:95.53ms
step:607/1770 train_time:57035ms step_avg:95.54ms
step:608/1770 train_time:57133ms step_avg:95.54ms
step:609/1770 train_time:57230ms step_avg:95.54ms
step:610/1770 train_time:57327ms step_avg:95.55ms
step:611/1770 train_time:57425ms step_avg:95.55ms
step:612/1770 train_time:57523ms step_avg:95.55ms
step:613/1770 train_time:57620ms step_avg:95.56ms
step:614/1770 train_time:57718ms step_avg:95.56ms
step:615/1770 train_time:57815ms step_avg:95.56ms
step:616/1770 train_time:57912ms step_avg:95.56ms
step:617/1770 train_time:58009ms step_avg:95.57ms
step:618/1770 train_time:58106ms step_avg:95.57ms
step:619/1770 train_time:58204ms step_avg:95.57ms
step:620/1770 train_time:58301ms step_avg:95.58ms
step:621/1770 train_time:58399ms step_avg:95.58ms
step:622/1770 train_time:58496ms step_avg:95.58ms
step:623/1770 train_time:58593ms step_avg:95.58ms
step:624/1770 train_time:58690ms step_avg:95.59ms
step:625/1770 train_time:58787ms step_avg:95.59ms
step:625/1770 val_loss:3.6623 train_time:58882ms step_avg:95.74ms
step:626/1770 train_time:58904ms step_avg:95.62ms
step:627/1770 train_time:58990ms step_avg:95.61ms
step:628/1770 train_time:59090ms step_avg:95.61ms
step:629/1770 train_time:59187ms step_avg:95.62ms
step:630/1770 train_time:59284ms step_avg:95.62ms
step:631/1770 train_time:59381ms step_avg:95.62ms
step:632/1770 train_time:59478ms step_avg:95.62ms
step:633/1770 train_time:59575ms step_avg:95.63ms
step:634/1770 train_time:59672ms step_avg:95.63ms
step:635/1770 train_time:59768ms step_avg:95.63ms
step:636/1770 train_time:59865ms step_avg:95.63ms
step:637/1770 train_time:59963ms step_avg:95.64ms
step:638/1770 train_time:60061ms step_avg:95.64ms
step:639/1770 train_time:60160ms step_avg:95.64ms
step:640/1770 train_time:60258ms step_avg:95.65ms
step:641/1770 train_time:60355ms step_avg:95.65ms
step:642/1770 train_time:60452ms step_avg:95.65ms
step:643/1770 train_time:60549ms step_avg:95.65ms
step:644/1770 train_time:60646ms step_avg:95.66ms
step:645/1770 train_time:60743ms step_avg:95.66ms
step:646/1770 train_time:60840ms step_avg:95.66ms
step:647/1770 train_time:60938ms step_avg:95.66ms
step:648/1770 train_time:61035ms step_avg:95.67ms
step:649/1770 train_time:61133ms step_avg:95.67ms
step:650/1770 train_time:61230ms step_avg:95.67ms
step:651/1770 train_time:61327ms step_avg:95.67ms
step:652/1770 train_time:61425ms step_avg:95.68ms
step:653/1770 train_time:61522ms step_avg:95.68ms
step:654/1770 train_time:61619ms step_avg:95.68ms
step:655/1770 train_time:61716ms step_avg:95.68ms
step:656/1770 train_time:61813ms step_avg:95.69ms
step:657/1770 train_time:61910ms step_avg:95.69ms
step:658/1770 train_time:62009ms step_avg:95.69ms
step:659/1770 train_time:62108ms step_avg:95.70ms
step:660/1770 train_time:62206ms step_avg:95.70ms
step:661/1770 train_time:62306ms step_avg:95.71ms
step:662/1770 train_time:62404ms step_avg:95.71ms
step:663/1770 train_time:62503ms step_avg:95.72ms
step:664/1770 train_time:62602ms step_avg:95.72ms
step:665/1770 train_time:62701ms step_avg:95.73ms
step:666/1770 train_time:62800ms step_avg:95.73ms
step:667/1770 train_time:62899ms step_avg:95.74ms
step:668/1770 train_time:63000ms step_avg:95.74ms
step:669/1770 train_time:63100ms step_avg:95.75ms
step:670/1770 train_time:63200ms step_avg:95.76ms
step:671/1770 train_time:63299ms step_avg:95.76ms
step:672/1770 train_time:63398ms step_avg:95.77ms
step:673/1770 train_time:63497ms step_avg:95.77ms
step:674/1770 train_time:63596ms step_avg:95.78ms
step:675/1770 train_time:63695ms step_avg:95.78ms
step:676/1770 train_time:63794ms step_avg:95.79ms
step:677/1770 train_time:63892ms step_avg:95.79ms
step:678/1770 train_time:63990ms step_avg:95.79ms
step:679/1770 train_time:64089ms step_avg:95.80ms
step:680/1770 train_time:64187ms step_avg:95.80ms
step:681/1770 train_time:64286ms step_avg:95.81ms
step:682/1770 train_time:64385ms step_avg:95.81ms
step:683/1770 train_time:64484ms step_avg:95.82ms
step:684/1770 train_time:64584ms step_avg:95.82ms
step:685/1770 train_time:64684ms step_avg:95.83ms
step:686/1770 train_time:64782ms step_avg:95.83ms
step:687/1770 train_time:64881ms step_avg:95.84ms
step:688/1770 train_time:64981ms step_avg:95.84ms
step:689/1770 train_time:65080ms step_avg:95.85ms
step:690/1770 train_time:65180ms step_avg:95.85ms
step:691/1770 train_time:65279ms step_avg:95.86ms
step:692/1770 train_time:65379ms step_avg:95.86ms
step:693/1770 train_time:65478ms step_avg:95.87ms
step:694/1770 train_time:65577ms step_avg:95.87ms
step:695/1770 train_time:65676ms step_avg:95.88ms
step:696/1770 train_time:65776ms step_avg:95.88ms
step:697/1770 train_time:65875ms step_avg:95.89ms
step:698/1770 train_time:65974ms step_avg:95.89ms
step:699/1770 train_time:66072ms step_avg:95.90ms
step:700/1770 train_time:66171ms step_avg:95.90ms
step:701/1770 train_time:66270ms step_avg:95.90ms
step:702/1770 train_time:66370ms step_avg:95.91ms
step:703/1770 train_time:66468ms step_avg:95.91ms
step:704/1770 train_time:66567ms step_avg:95.92ms
step:705/1770 train_time:66666ms step_avg:95.92ms
step:706/1770 train_time:66765ms step_avg:95.93ms
step:707/1770 train_time:66865ms step_avg:95.93ms
step:708/1770 train_time:66965ms step_avg:95.94ms
step:709/1770 train_time:67065ms step_avg:95.94ms
step:710/1770 train_time:67164ms step_avg:95.95ms
step:711/1770 train_time:67263ms step_avg:95.95ms
step:712/1770 train_time:67363ms step_avg:95.96ms
step:713/1770 train_time:67463ms step_avg:95.96ms
step:714/1770 train_time:67562ms step_avg:95.97ms
step:715/1770 train_time:67662ms step_avg:95.97ms
step:716/1770 train_time:67760ms step_avg:95.98ms
step:717/1770 train_time:67859ms step_avg:95.98ms
step:718/1770 train_time:67959ms step_avg:95.99ms
step:719/1770 train_time:68058ms step_avg:95.99ms
step:720/1770 train_time:68158ms step_avg:96.00ms
step:721/1770 train_time:68257ms step_avg:96.00ms
step:722/1770 train_time:68356ms step_avg:96.01ms
step:723/1770 train_time:68455ms step_avg:96.01ms
step:724/1770 train_time:68555ms step_avg:96.02ms
step:725/1770 train_time:68655ms step_avg:96.02ms
step:726/1770 train_time:68754ms step_avg:96.02ms
step:727/1770 train_time:68853ms step_avg:96.03ms
step:728/1770 train_time:68951ms step_avg:96.03ms
step:729/1770 train_time:69050ms step_avg:96.04ms
step:730/1770 train_time:69149ms step_avg:96.04ms
step:731/1770 train_time:69248ms step_avg:96.04ms
step:732/1770 train_time:69348ms step_avg:96.05ms
step:733/1770 train_time:69447ms step_avg:96.05ms
step:734/1770 train_time:69546ms step_avg:96.06ms
step:735/1770 train_time:69646ms step_avg:96.06ms
step:736/1770 train_time:69745ms step_avg:96.07ms
step:737/1770 train_time:69844ms step_avg:96.07ms
step:738/1770 train_time:69943ms step_avg:96.08ms
step:739/1770 train_time:70043ms step_avg:96.08ms
step:740/1770 train_time:70143ms step_avg:96.09ms
step:741/1770 train_time:70243ms step_avg:96.09ms
step:742/1770 train_time:70343ms step_avg:96.10ms
step:743/1770 train_time:70443ms step_avg:96.10ms
step:744/1770 train_time:70543ms step_avg:96.11ms
step:745/1770 train_time:70641ms step_avg:96.11ms
step:746/1770 train_time:70741ms step_avg:96.12ms
step:747/1770 train_time:70840ms step_avg:96.12ms
step:748/1770 train_time:70940ms step_avg:96.12ms
step:749/1770 train_time:71039ms step_avg:96.13ms
step:750/1770 train_time:71137ms step_avg:96.13ms
step:750/1770 val_loss:3.6005 train_time:71234ms step_avg:96.26ms
step:751/1770 train_time:71256ms step_avg:96.16ms
step:752/1770 train_time:71345ms step_avg:96.15ms
step:753/1770 train_time:71445ms step_avg:96.16ms
step:754/1770 train_time:71545ms step_avg:96.16ms
step:755/1770 train_time:71643ms step_avg:96.17ms
step:756/1770 train_time:71742ms step_avg:96.17ms
step:757/1770 train_time:71841ms step_avg:96.17ms
step:758/1770 train_time:71939ms step_avg:96.18ms
step:759/1770 train_time:72038ms step_avg:96.18ms
step:760/1770 train_time:72136ms step_avg:96.18ms
step:761/1770 train_time:72235ms step_avg:96.19ms
step:762/1770 train_time:72337ms step_avg:96.19ms
step:763/1770 train_time:72437ms step_avg:96.20ms
step:764/1770 train_time:72536ms step_avg:96.20ms
step:765/1770 train_time:72636ms step_avg:96.21ms
step:766/1770 train_time:72735ms step_avg:96.21ms
step:767/1770 train_time:72834ms step_avg:96.21ms
step:768/1770 train_time:72933ms step_avg:96.22ms
step:769/1770 train_time:73032ms step_avg:96.22ms
step:770/1770 train_time:73130ms step_avg:96.22ms
step:771/1770 train_time:73229ms step_avg:96.23ms
step:772/1770 train_time:73329ms step_avg:96.23ms
step:773/1770 train_time:73429ms step_avg:96.24ms
step:774/1770 train_time:73529ms step_avg:96.24ms
step:775/1770 train_time:73629ms step_avg:96.25ms
step:776/1770 train_time:73729ms step_avg:96.25ms
step:777/1770 train_time:73829ms step_avg:96.26ms
step:778/1770 train_time:73928ms step_avg:96.26ms
step:779/1770 train_time:74028ms step_avg:96.26ms
step:780/1770 train_time:74127ms step_avg:96.27ms
step:781/1770 train_time:74226ms step_avg:96.27ms
step:782/1770 train_time:74325ms step_avg:96.28ms
step:783/1770 train_time:74424ms step_avg:96.28ms
step:784/1770 train_time:74523ms step_avg:96.28ms
step:785/1770 train_time:74622ms step_avg:96.29ms
step:786/1770 train_time:74722ms step_avg:96.29ms
step:787/1770 train_time:74821ms step_avg:96.29ms
step:788/1770 train_time:74920ms step_avg:96.30ms
step:789/1770 train_time:75019ms step_avg:96.30ms
step:790/1770 train_time:75118ms step_avg:96.31ms
step:791/1770 train_time:75217ms step_avg:96.31ms
step:792/1770 train_time:75317ms step_avg:96.31ms
step:793/1770 train_time:75416ms step_avg:96.32ms
step:794/1770 train_time:75517ms step_avg:96.32ms
step:795/1770 train_time:75617ms step_avg:96.33ms
step:796/1770 train_time:75717ms step_avg:96.33ms
step:797/1770 train_time:75816ms step_avg:96.34ms
step:798/1770 train_time:75915ms step_avg:96.34ms
step:799/1770 train_time:76014ms step_avg:96.34ms
step:800/1770 train_time:76113ms step_avg:96.35ms
step:801/1770 train_time:76212ms step_avg:96.35ms
step:802/1770 train_time:76312ms step_avg:96.35ms
step:803/1770 train_time:76412ms step_avg:96.36ms
step:804/1770 train_time:76511ms step_avg:96.36ms
step:805/1770 train_time:76610ms step_avg:96.37ms
step:806/1770 train_time:76710ms step_avg:96.37ms
step:807/1770 train_time:76810ms step_avg:96.37ms
step:808/1770 train_time:76910ms step_avg:96.38ms
step:809/1770 train_time:77010ms step_avg:96.38ms
step:810/1770 train_time:77111ms step_avg:96.39ms
step:811/1770 train_time:77211ms step_avg:96.39ms
step:812/1770 train_time:77311ms step_avg:96.40ms
step:813/1770 train_time:77410ms step_avg:96.40ms
step:814/1770 train_time:77509ms step_avg:96.40ms
step:815/1770 train_time:77609ms step_avg:96.41ms
step:816/1770 train_time:77708ms step_avg:96.41ms
step:817/1770 train_time:77808ms step_avg:96.42ms
step:818/1770 train_time:77907ms step_avg:96.42ms
step:819/1770 train_time:78007ms step_avg:96.42ms
step:820/1770 train_time:78107ms step_avg:96.43ms
step:821/1770 train_time:78207ms step_avg:96.43ms
step:822/1770 train_time:78307ms step_avg:96.44ms
step:823/1770 train_time:78407ms step_avg:96.44ms
step:824/1770 train_time:78507ms step_avg:96.45ms
step:825/1770 train_time:78607ms step_avg:96.45ms
step:826/1770 train_time:78706ms step_avg:96.45ms
step:827/1770 train_time:78806ms step_avg:96.46ms
step:828/1770 train_time:78906ms step_avg:96.46ms
step:829/1770 train_time:79006ms step_avg:96.47ms
step:830/1770 train_time:79105ms step_avg:96.47ms
step:831/1770 train_time:79205ms step_avg:96.47ms
step:832/1770 train_time:79304ms step_avg:96.48ms
step:833/1770 train_time:79404ms step_avg:96.48ms
step:834/1770 train_time:79503ms step_avg:96.48ms
step:835/1770 train_time:79603ms step_avg:96.49ms
step:836/1770 train_time:79703ms step_avg:96.49ms
step:837/1770 train_time:79802ms step_avg:96.50ms
step:838/1770 train_time:79902ms step_avg:96.50ms
step:839/1770 train_time:80001ms step_avg:96.50ms
step:840/1770 train_time:80100ms step_avg:96.51ms
step:841/1770 train_time:80200ms step_avg:96.51ms
step:842/1770 train_time:80299ms step_avg:96.51ms
step:843/1770 train_time:80399ms step_avg:96.52ms
step:844/1770 train_time:80497ms step_avg:96.52ms
step:845/1770 train_time:80596ms step_avg:96.52ms
step:846/1770 train_time:80696ms step_avg:96.53ms
step:847/1770 train_time:80796ms step_avg:96.53ms
step:848/1770 train_time:80895ms step_avg:96.53ms
step:849/1770 train_time:80995ms step_avg:96.54ms
step:850/1770 train_time:81095ms step_avg:96.54ms
step:851/1770 train_time:81194ms step_avg:96.55ms
step:852/1770 train_time:81294ms step_avg:96.55ms
step:853/1770 train_time:81394ms step_avg:96.55ms
step:854/1770 train_time:81494ms step_avg:96.56ms
step:855/1770 train_time:81594ms step_avg:96.56ms
step:856/1770 train_time:81693ms step_avg:96.56ms
step:857/1770 train_time:81792ms step_avg:96.57ms
step:858/1770 train_time:81892ms step_avg:96.57ms
step:859/1770 train_time:81991ms step_avg:96.57ms
step:860/1770 train_time:82090ms step_avg:96.58ms
step:861/1770 train_time:82189ms step_avg:96.58ms
step:862/1770 train_time:82289ms step_avg:96.58ms
step:863/1770 train_time:82389ms step_avg:96.59ms
step:864/1770 train_time:82489ms step_avg:96.59ms
step:865/1770 train_time:82589ms step_avg:96.60ms
step:866/1770 train_time:82690ms step_avg:96.60ms
step:867/1770 train_time:82790ms step_avg:96.60ms
step:868/1770 train_time:82889ms step_avg:96.61ms
step:869/1770 train_time:82989ms step_avg:96.61ms
step:870/1770 train_time:83088ms step_avg:96.61ms
step:871/1770 train_time:83188ms step_avg:96.62ms
step:872/1770 train_time:83287ms step_avg:96.62ms
step:873/1770 train_time:83386ms step_avg:96.62ms
step:874/1770 train_time:83486ms step_avg:96.63ms
step:875/1770 train_time:83586ms step_avg:96.63ms
step:875/1770 val_loss:3.5511 train_time:83686ms step_avg:96.75ms
step:876/1770 train_time:83707ms step_avg:96.66ms
step:877/1770 train_time:83797ms step_avg:96.65ms
step:878/1770 train_time:83897ms step_avg:96.66ms
step:879/1770 train_time:83996ms step_avg:96.66ms
step:880/1770 train_time:84096ms step_avg:96.66ms
step:881/1770 train_time:84194ms step_avg:96.66ms
step:882/1770 train_time:84293ms step_avg:96.67ms
step:883/1770 train_time:84392ms step_avg:96.67ms
step:884/1770 train_time:84490ms step_avg:96.67ms
step:885/1770 train_time:84589ms step_avg:96.67ms
step:886/1770 train_time:84688ms step_avg:96.68ms
step:887/1770 train_time:84789ms step_avg:96.68ms
step:888/1770 train_time:84890ms step_avg:96.69ms
step:889/1770 train_time:84989ms step_avg:96.69ms
step:890/1770 train_time:85089ms step_avg:96.69ms
step:891/1770 train_time:85188ms step_avg:96.69ms
step:892/1770 train_time:85288ms step_avg:96.70ms
step:893/1770 train_time:85387ms step_avg:96.70ms
step:894/1770 train_time:85487ms step_avg:96.70ms
step:895/1770 train_time:85586ms step_avg:96.71ms
step:896/1770 train_time:85685ms step_avg:96.71ms
step:897/1770 train_time:85784ms step_avg:96.71ms
step:898/1770 train_time:85883ms step_avg:96.72ms
step:899/1770 train_time:85982ms step_avg:96.72ms
step:900/1770 train_time:86082ms step_avg:96.72ms
step:901/1770 train_time:86181ms step_avg:96.72ms
step:902/1770 train_time:86281ms step_avg:96.73ms
step:903/1770 train_time:86380ms step_avg:96.73ms
step:904/1770 train_time:86480ms step_avg:96.73ms
step:905/1770 train_time:86580ms step_avg:96.74ms
step:906/1770 train_time:86679ms step_avg:96.74ms
step:907/1770 train_time:86778ms step_avg:96.74ms
step:908/1770 train_time:86878ms step_avg:96.75ms
step:909/1770 train_time:86977ms step_avg:96.75ms
step:910/1770 train_time:87077ms step_avg:96.75ms
step:911/1770 train_time:87176ms step_avg:96.75ms
step:912/1770 train_time:87276ms step_avg:96.76ms
step:913/1770 train_time:87376ms step_avg:96.76ms
step:914/1770 train_time:87475ms step_avg:96.76ms
step:915/1770 train_time:87575ms step_avg:96.77ms
step:916/1770 train_time:87675ms step_avg:96.77ms
step:917/1770 train_time:87775ms step_avg:96.77ms
step:918/1770 train_time:87874ms step_avg:96.78ms
step:919/1770 train_time:87974ms step_avg:96.78ms
step:920/1770 train_time:88077ms step_avg:96.79ms
step:921/1770 train_time:88178ms step_avg:96.79ms
step:922/1770 train_time:88279ms step_avg:96.80ms
step:923/1770 train_time:88380ms step_avg:96.80ms
step:924/1770 train_time:88481ms step_avg:96.81ms
step:925/1770 train_time:88581ms step_avg:96.81ms
step:926/1770 train_time:88682ms step_avg:96.81ms
step:927/1770 train_time:88782ms step_avg:96.82ms
step:928/1770 train_time:88882ms step_avg:96.82ms
step:929/1770 train_time:88982ms step_avg:96.83ms
step:930/1770 train_time:89083ms step_avg:96.83ms
step:931/1770 train_time:89184ms step_avg:96.83ms
step:932/1770 train_time:89284ms step_avg:96.84ms
step:933/1770 train_time:89385ms step_avg:96.84ms
step:934/1770 train_time:89486ms step_avg:96.85ms
step:935/1770 train_time:89586ms step_avg:96.85ms
step:936/1770 train_time:89687ms step_avg:96.85ms
step:937/1770 train_time:89787ms step_avg:96.86ms
step:938/1770 train_time:89888ms step_avg:96.86ms
step:939/1770 train_time:89989ms step_avg:96.87ms
step:940/1770 train_time:90090ms step_avg:96.87ms
step:941/1770 train_time:90191ms step_avg:96.88ms
step:942/1770 train_time:90292ms step_avg:96.88ms
step:943/1770 train_time:90395ms step_avg:96.89ms
step:944/1770 train_time:90496ms step_avg:96.89ms
step:945/1770 train_time:90597ms step_avg:96.89ms
step:946/1770 train_time:90699ms step_avg:96.90ms
step:947/1770 train_time:90800ms step_avg:96.90ms
step:948/1770 train_time:90901ms step_avg:96.91ms
step:949/1770 train_time:91003ms step_avg:96.92ms
step:950/1770 train_time:91104ms step_avg:96.92ms
step:951/1770 train_time:91206ms step_avg:96.92ms
step:952/1770 train_time:91306ms step_avg:96.93ms
step:953/1770 train_time:91406ms step_avg:96.93ms
step:954/1770 train_time:91506ms step_avg:96.93ms
step:955/1770 train_time:91607ms step_avg:96.94ms
step:956/1770 train_time:91708ms step_avg:96.94ms
step:957/1770 train_time:91809ms step_avg:96.95ms
step:958/1770 train_time:91910ms step_avg:96.95ms
step:959/1770 train_time:92010ms step_avg:96.95ms
step:960/1770 train_time:92111ms step_avg:96.96ms
step:961/1770 train_time:92212ms step_avg:96.96ms
step:962/1770 train_time:92313ms step_avg:96.97ms
step:963/1770 train_time:92414ms step_avg:96.97ms
step:964/1770 train_time:92516ms step_avg:96.98ms
step:965/1770 train_time:92617ms step_avg:96.98ms
step:966/1770 train_time:92717ms step_avg:96.98ms
step:967/1770 train_time:92818ms step_avg:96.99ms
step:968/1770 train_time:92920ms step_avg:96.99ms
step:969/1770 train_time:93021ms step_avg:97.00ms
step:970/1770 train_time:93123ms step_avg:97.00ms
step:971/1770 train_time:93224ms step_avg:97.01ms
step:972/1770 train_time:93325ms step_avg:97.01ms
step:973/1770 train_time:93425ms step_avg:97.01ms
step:974/1770 train_time:93527ms step_avg:97.02ms
step:975/1770 train_time:93627ms step_avg:97.02ms
step:976/1770 train_time:93727ms step_avg:97.03ms
step:977/1770 train_time:93828ms step_avg:97.03ms
step:978/1770 train_time:93928ms step_avg:97.03ms
step:979/1770 train_time:94031ms step_avg:97.04ms
step:980/1770 train_time:94132ms step_avg:97.04ms
step:981/1770 train_time:94233ms step_avg:97.05ms
step:982/1770 train_time:94334ms step_avg:97.05ms
step:983/1770 train_time:94435ms step_avg:97.06ms
step:984/1770 train_time:94538ms step_avg:97.06ms
step:985/1770 train_time:94638ms step_avg:97.07ms
step:986/1770 train_time:94739ms step_avg:97.07ms
step:987/1770 train_time:94840ms step_avg:97.07ms
step:988/1770 train_time:94942ms step_avg:97.08ms
step:989/1770 train_time:95043ms step_avg:97.08ms
step:990/1770 train_time:95144ms step_avg:97.09ms
step:991/1770 train_time:95244ms step_avg:97.09ms
step:992/1770 train_time:95345ms step_avg:97.09ms
step:993/1770 train_time:95445ms step_avg:97.10ms
step:994/1770 train_time:95547ms step_avg:97.10ms
step:995/1770 train_time:95648ms step_avg:97.10ms
step:996/1770 train_time:95749ms step_avg:97.11ms
step:997/1770 train_time:95850ms step_avg:97.11ms
step:998/1770 train_time:95951ms step_avg:97.12ms
step:999/1770 train_time:96052ms step_avg:97.12ms
step:1000/1770 train_time:96153ms step_avg:97.12ms
step:1000/1770 val_loss:3.5131 train_time:96252ms step_avg:97.22ms
step:1001/1770 train_time:96274ms step_avg:97.15ms
step:1002/1770 train_time:96362ms step_avg:97.14ms
step:1003/1770 train_time:96465ms step_avg:97.15ms
step:1004/1770 train_time:96566ms step_avg:97.15ms
step:1005/1770 train_time:96666ms step_avg:97.15ms
step:1006/1770 train_time:96766ms step_avg:97.15ms
step:1007/1770 train_time:96866ms step_avg:97.16ms
step:1008/1770 train_time:96967ms step_avg:97.16ms
step:1009/1770 train_time:97066ms step_avg:97.16ms
step:1010/1770 train_time:97166ms step_avg:97.17ms
step:1011/1770 train_time:97270ms step_avg:97.17ms
step:1012/1770 train_time:97372ms step_avg:97.18ms
step:1013/1770 train_time:97474ms step_avg:97.18ms
step:1014/1770 train_time:97576ms step_avg:97.19ms
step:1015/1770 train_time:97676ms step_avg:97.19ms
step:1016/1770 train_time:97776ms step_avg:97.19ms
step:1017/1770 train_time:97877ms step_avg:97.20ms
step:1018/1770 train_time:97977ms step_avg:97.20ms
step:1019/1770 train_time:98077ms step_avg:97.20ms
step:1020/1770 train_time:98178ms step_avg:97.21ms
step:1021/1770 train_time:98279ms step_avg:97.21ms
step:1022/1770 train_time:98380ms step_avg:97.21ms
step:1023/1770 train_time:98480ms step_avg:97.22ms
step:1024/1770 train_time:98581ms step_avg:97.22ms
step:1025/1770 train_time:98682ms step_avg:97.22ms
step:1026/1770 train_time:98782ms step_avg:97.23ms
step:1027/1770 train_time:98884ms step_avg:97.23ms
step:1028/1770 train_time:98985ms step_avg:97.23ms
step:1029/1770 train_time:99086ms step_avg:97.24ms
step:1030/1770 train_time:99188ms step_avg:97.24ms
step:1031/1770 train_time:99288ms step_avg:97.25ms
step:1032/1770 train_time:99390ms step_avg:97.25ms
step:1033/1770 train_time:99491ms step_avg:97.25ms
step:1034/1770 train_time:99592ms step_avg:97.26ms
step:1035/1770 train_time:99692ms step_avg:97.26ms
step:1036/1770 train_time:99793ms step_avg:97.26ms
step:1037/1770 train_time:99894ms step_avg:97.27ms
step:1038/1770 train_time:99995ms step_avg:97.27ms
step:1039/1770 train_time:100095ms step_avg:97.27ms
step:1040/1770 train_time:100195ms step_avg:97.28ms
step:1041/1770 train_time:100296ms step_avg:97.28ms
step:1042/1770 train_time:100397ms step_avg:97.28ms
step:1043/1770 train_time:100498ms step_avg:97.29ms
step:1044/1770 train_time:100599ms step_avg:97.29ms
step:1045/1770 train_time:100701ms step_avg:97.30ms
step:1046/1770 train_time:100801ms step_avg:97.30ms
step:1047/1770 train_time:100903ms step_avg:97.30ms
step:1048/1770 train_time:101004ms step_avg:97.31ms
step:1049/1770 train_time:101105ms step_avg:97.31ms
step:1050/1770 train_time:101207ms step_avg:97.31ms
step:1051/1770 train_time:101308ms step_avg:97.32ms
step:1052/1770 train_time:101409ms step_avg:97.32ms
step:1053/1770 train_time:101510ms step_avg:97.33ms
step:1054/1770 train_time:101610ms step_avg:97.33ms
step:1055/1770 train_time:101712ms step_avg:97.33ms
step:1056/1770 train_time:101813ms step_avg:97.34ms
step:1057/1770 train_time:101914ms step_avg:97.34ms
step:1058/1770 train_time:102014ms step_avg:97.34ms
step:1059/1770 train_time:102116ms step_avg:97.35ms
step:1060/1770 train_time:102216ms step_avg:97.35ms
step:1061/1770 train_time:102317ms step_avg:97.35ms
step:1062/1770 train_time:102419ms step_avg:97.36ms
step:1063/1770 train_time:102522ms step_avg:97.36ms
step:1064/1770 train_time:102624ms step_avg:97.37ms
step:1065/1770 train_time:102725ms step_avg:97.37ms
step:1066/1770 train_time:102826ms step_avg:97.37ms
step:1067/1770 train_time:102928ms step_avg:97.38ms
step:1068/1770 train_time:103029ms step_avg:97.38ms
step:1069/1770 train_time:103131ms step_avg:97.38ms
step:1070/1770 train_time:103232ms step_avg:97.39ms
step:1071/1770 train_time:103332ms step_avg:97.39ms
step:1072/1770 train_time:103433ms step_avg:97.39ms
step:1073/1770 train_time:103534ms step_avg:97.40ms
step:1074/1770 train_time:103636ms step_avg:97.40ms
step:1075/1770 train_time:103736ms step_avg:97.41ms
step:1076/1770 train_time:103838ms step_avg:97.41ms
step:1077/1770 train_time:103938ms step_avg:97.41ms
step:1078/1770 train_time:104039ms step_avg:97.41ms
step:1079/1770 train_time:104139ms step_avg:97.42ms
step:1080/1770 train_time:104240ms step_avg:97.42ms
step:1081/1770 train_time:104340ms step_avg:97.42ms
step:1082/1770 train_time:104442ms step_avg:97.43ms
step:1083/1770 train_time:104543ms step_avg:97.43ms
step:1084/1770 train_time:104644ms step_avg:97.43ms
step:1085/1770 train_time:104745ms step_avg:97.44ms
step:1086/1770 train_time:104847ms step_avg:97.44ms
step:1087/1770 train_time:104947ms step_avg:97.44ms
step:1088/1770 train_time:105048ms step_avg:97.45ms
step:1089/1770 train_time:105149ms step_avg:97.45ms
step:1090/1770 train_time:105250ms step_avg:97.45ms
step:1091/1770 train_time:105351ms step_avg:97.46ms
step:1092/1770 train_time:105453ms step_avg:97.46ms
step:1093/1770 train_time:105554ms step_avg:97.46ms
step:1094/1770 train_time:105654ms step_avg:97.47ms
step:1095/1770 train_time:105755ms step_avg:97.47ms
step:1096/1770 train_time:105855ms step_avg:97.47ms
step:1097/1770 train_time:105956ms step_avg:97.48ms
step:1098/1770 train_time:106057ms step_avg:97.48ms
step:1099/1770 train_time:106157ms step_avg:97.48ms
step:1100/1770 train_time:106258ms step_avg:97.48ms
step:1101/1770 train_time:106359ms step_avg:97.49ms
step:1102/1770 train_time:106460ms step_avg:97.49ms
step:1103/1770 train_time:106561ms step_avg:97.49ms
step:1104/1770 train_time:106663ms step_avg:97.50ms
step:1105/1770 train_time:106764ms step_avg:97.50ms
step:1106/1770 train_time:106866ms step_avg:97.51ms
step:1107/1770 train_time:106967ms step_avg:97.51ms
step:1108/1770 train_time:107068ms step_avg:97.51ms
step:1109/1770 train_time:107169ms step_avg:97.52ms
step:1110/1770 train_time:107270ms step_avg:97.52ms
step:1111/1770 train_time:107371ms step_avg:97.52ms
step:1112/1770 train_time:107472ms step_avg:97.52ms
step:1113/1770 train_time:107573ms step_avg:97.53ms
step:1114/1770 train_time:107674ms step_avg:97.53ms
step:1115/1770 train_time:107775ms step_avg:97.53ms
step:1116/1770 train_time:107877ms step_avg:97.54ms
step:1117/1770 train_time:107977ms step_avg:97.54ms
step:1118/1770 train_time:108077ms step_avg:97.54ms
step:1119/1770 train_time:108178ms step_avg:97.55ms
step:1120/1770 train_time:108278ms step_avg:97.55ms
step:1121/1770 train_time:108379ms step_avg:97.55ms
step:1122/1770 train_time:108479ms step_avg:97.55ms
step:1123/1770 train_time:108580ms step_avg:97.56ms
step:1124/1770 train_time:108680ms step_avg:97.56ms
step:1125/1770 train_time:108782ms step_avg:97.56ms
step:1125/1770 val_loss:3.4732 train_time:108882ms step_avg:97.65ms
step:1126/1770 train_time:108903ms step_avg:97.58ms
step:1127/1770 train_time:108994ms step_avg:97.58ms
step:1128/1770 train_time:109096ms step_avg:97.58ms
step:1129/1770 train_time:109195ms step_avg:97.58ms
step:1130/1770 train_time:109297ms step_avg:97.59ms
step:1131/1770 train_time:109398ms step_avg:97.59ms
step:1132/1770 train_time:109499ms step_avg:97.59ms
step:1133/1770 train_time:109599ms step_avg:97.59ms
step:1134/1770 train_time:109700ms step_avg:97.60ms
step:1135/1770 train_time:109801ms step_avg:97.60ms
step:1136/1770 train_time:109903ms step_avg:97.60ms
step:1137/1770 train_time:110006ms step_avg:97.61ms
step:1138/1770 train_time:110108ms step_avg:97.61ms
step:1139/1770 train_time:110208ms step_avg:97.62ms
step:1140/1770 train_time:110308ms step_avg:97.62ms
step:1141/1770 train_time:110408ms step_avg:97.62ms
step:1142/1770 train_time:110509ms step_avg:97.62ms
step:1143/1770 train_time:110609ms step_avg:97.63ms
step:1144/1770 train_time:110710ms step_avg:97.63ms
step:1145/1770 train_time:110810ms step_avg:97.63ms
step:1146/1770 train_time:110912ms step_avg:97.63ms
step:1147/1770 train_time:111013ms step_avg:97.64ms
step:1148/1770 train_time:111115ms step_avg:97.64ms
step:1149/1770 train_time:111216ms step_avg:97.64ms
step:1150/1770 train_time:111317ms step_avg:97.65ms
step:1151/1770 train_time:111420ms step_avg:97.65ms
step:1152/1770 train_time:111521ms step_avg:97.65ms
step:1153/1770 train_time:111622ms step_avg:97.66ms
step:1154/1770 train_time:111724ms step_avg:97.66ms
step:1155/1770 train_time:111825ms step_avg:97.66ms
step:1156/1770 train_time:111926ms step_avg:97.67ms
step:1157/1770 train_time:112028ms step_avg:97.67ms
step:1158/1770 train_time:112130ms step_avg:97.67ms
step:1159/1770 train_time:112230ms step_avg:97.68ms
step:1160/1770 train_time:112331ms step_avg:97.68ms
step:1161/1770 train_time:112431ms step_avg:97.68ms
step:1162/1770 train_time:112532ms step_avg:97.68ms
step:1163/1770 train_time:112633ms step_avg:97.69ms
step:1164/1770 train_time:112734ms step_avg:97.69ms
step:1165/1770 train_time:112835ms step_avg:97.69ms
step:1166/1770 train_time:112936ms step_avg:97.70ms
step:1167/1770 train_time:113038ms step_avg:97.70ms
step:1168/1770 train_time:113139ms step_avg:97.70ms
step:1169/1770 train_time:113240ms step_avg:97.71ms
step:1170/1770 train_time:113340ms step_avg:97.71ms
step:1171/1770 train_time:113442ms step_avg:97.71ms
step:1172/1770 train_time:113543ms step_avg:97.71ms
step:1173/1770 train_time:113645ms step_avg:97.72ms
step:1174/1770 train_time:113747ms step_avg:97.72ms
step:1175/1770 train_time:113849ms step_avg:97.72ms
step:1176/1770 train_time:113950ms step_avg:97.73ms
step:1177/1770 train_time:114051ms step_avg:97.73ms
step:1178/1770 train_time:114153ms step_avg:97.73ms
step:1179/1770 train_time:114254ms step_avg:97.74ms
step:1180/1770 train_time:114354ms step_avg:97.74ms
step:1181/1770 train_time:114455ms step_avg:97.74ms
step:1182/1770 train_time:114556ms step_avg:97.74ms
step:1183/1770 train_time:114659ms step_avg:97.75ms
step:1184/1770 train_time:114762ms step_avg:97.75ms
step:1185/1770 train_time:114864ms step_avg:97.76ms
step:1186/1770 train_time:114967ms step_avg:97.76ms
step:1187/1770 train_time:115070ms step_avg:97.77ms
step:1188/1770 train_time:115172ms step_avg:97.77ms
step:1189/1770 train_time:115274ms step_avg:97.77ms
step:1190/1770 train_time:115375ms step_avg:97.78ms
step:1191/1770 train_time:115477ms step_avg:97.78ms
step:1192/1770 train_time:115580ms step_avg:97.78ms
step:1193/1770 train_time:115682ms step_avg:97.79ms
step:1194/1770 train_time:115784ms step_avg:97.79ms
step:1195/1770 train_time:115888ms step_avg:97.80ms
step:1196/1770 train_time:115991ms step_avg:97.80ms
step:1197/1770 train_time:116092ms step_avg:97.80ms
step:1198/1770 train_time:116194ms step_avg:97.81ms
step:1199/1770 train_time:116296ms step_avg:97.81ms
step:1200/1770 train_time:116398ms step_avg:97.81ms
step:1201/1770 train_time:116501ms step_avg:97.82ms
step:1202/1770 train_time:116602ms step_avg:97.82ms
step:1203/1770 train_time:116704ms step_avg:97.82ms
step:1204/1770 train_time:116806ms step_avg:97.83ms
step:1205/1770 train_time:116908ms step_avg:97.83ms
step:1206/1770 train_time:117010ms step_avg:97.83ms
step:1207/1770 train_time:117111ms step_avg:97.84ms
step:1208/1770 train_time:117213ms step_avg:97.84ms
step:1209/1770 train_time:117315ms step_avg:97.84ms
step:1210/1770 train_time:117417ms step_avg:97.85ms
step:1211/1770 train_time:117520ms step_avg:97.85ms
step:1212/1770 train_time:117623ms step_avg:97.86ms
step:1213/1770 train_time:117725ms step_avg:97.86ms
step:1214/1770 train_time:117827ms step_avg:97.86ms
step:1215/1770 train_time:117929ms step_avg:97.87ms
step:1216/1770 train_time:118034ms step_avg:97.87ms
step:1217/1770 train_time:118137ms step_avg:97.88ms
step:1218/1770 train_time:118238ms step_avg:97.88ms
step:1219/1770 train_time:118341ms step_avg:97.88ms
step:1220/1770 train_time:118444ms step_avg:97.89ms
step:1221/1770 train_time:118545ms step_avg:97.89ms
step:1222/1770 train_time:118649ms step_avg:97.90ms
step:1223/1770 train_time:118750ms step_avg:97.90ms
step:1224/1770 train_time:118853ms step_avg:97.90ms
step:1225/1770 train_time:118955ms step_avg:97.91ms
step:1226/1770 train_time:119059ms step_avg:97.91ms
step:1227/1770 train_time:119164ms step_avg:97.92ms
step:1228/1770 train_time:119268ms step_avg:97.92ms
step:1229/1770 train_time:119369ms step_avg:97.92ms
step:1230/1770 train_time:119470ms step_avg:97.93ms
step:1231/1770 train_time:119573ms step_avg:97.93ms
step:1232/1770 train_time:119674ms step_avg:97.93ms
step:1233/1770 train_time:119776ms step_avg:97.94ms
step:1234/1770 train_time:119877ms step_avg:97.94ms
step:1235/1770 train_time:119980ms step_avg:97.94ms
step:1236/1770 train_time:120082ms step_avg:97.95ms
step:1237/1770 train_time:120185ms step_avg:97.95ms
step:1238/1770 train_time:120288ms step_avg:97.95ms
step:1239/1770 train_time:120390ms step_avg:97.96ms
step:1240/1770 train_time:120491ms step_avg:97.96ms
step:1241/1770 train_time:120593ms step_avg:97.96ms
step:1242/1770 train_time:120694ms step_avg:97.97ms
step:1243/1770 train_time:120797ms step_avg:97.97ms
step:1244/1770 train_time:120899ms step_avg:97.97ms
step:1245/1770 train_time:121001ms step_avg:97.98ms
step:1246/1770 train_time:121103ms step_avg:97.98ms
step:1247/1770 train_time:121205ms step_avg:97.98ms
step:1248/1770 train_time:121307ms step_avg:97.99ms
step:1249/1770 train_time:121410ms step_avg:97.99ms
step:1250/1770 train_time:121512ms step_avg:97.99ms
step:1250/1770 val_loss:3.4255 train_time:121613ms step_avg:98.07ms
step:1251/1770 train_time:121634ms step_avg:98.01ms
step:1252/1770 train_time:121725ms step_avg:98.01ms
step:1253/1770 train_time:121827ms step_avg:98.01ms
step:1254/1770 train_time:121929ms step_avg:98.01ms
step:1255/1770 train_time:122033ms step_avg:98.02ms
step:1256/1770 train_time:122134ms step_avg:98.02ms
step:1257/1770 train_time:122235ms step_avg:98.02ms
step:1258/1770 train_time:122337ms step_avg:98.03ms
step:1259/1770 train_time:122440ms step_avg:98.03ms
step:1260/1770 train_time:122542ms step_avg:98.03ms
step:1261/1770 train_time:122645ms step_avg:98.04ms
step:1262/1770 train_time:122748ms step_avg:98.04ms
step:1263/1770 train_time:122851ms step_avg:98.05ms
step:1264/1770 train_time:122954ms step_avg:98.05ms
step:1265/1770 train_time:123055ms step_avg:98.05ms
step:1266/1770 train_time:123158ms step_avg:98.06ms
step:1267/1770 train_time:123261ms step_avg:98.06ms
step:1268/1770 train_time:123363ms step_avg:98.06ms
step:1269/1770 train_time:123464ms step_avg:98.07ms
step:1270/1770 train_time:123567ms step_avg:98.07ms
step:1271/1770 train_time:123669ms step_avg:98.07ms
step:1272/1770 train_time:123771ms step_avg:98.08ms
step:1273/1770 train_time:123875ms step_avg:98.08ms
step:1274/1770 train_time:123977ms step_avg:98.08ms
step:1275/1770 train_time:124079ms step_avg:98.09ms
step:1276/1770 train_time:124182ms step_avg:98.09ms
step:1277/1770 train_time:124284ms step_avg:98.09ms
step:1278/1770 train_time:124386ms step_avg:98.10ms
step:1279/1770 train_time:124489ms step_avg:98.10ms
step:1280/1770 train_time:124592ms step_avg:98.10ms
step:1281/1770 train_time:124694ms step_avg:98.11ms
step:1282/1770 train_time:124797ms step_avg:98.11ms
step:1283/1770 train_time:124899ms step_avg:98.11ms
step:1284/1770 train_time:125002ms step_avg:98.12ms
step:1285/1770 train_time:125104ms step_avg:98.12ms
step:1286/1770 train_time:125207ms step_avg:98.12ms
step:1287/1770 train_time:125311ms step_avg:98.13ms
step:1288/1770 train_time:125413ms step_avg:98.13ms
step:1289/1770 train_time:125515ms step_avg:98.14ms
step:1290/1770 train_time:125616ms step_avg:98.14ms
step:1291/1770 train_time:125719ms step_avg:98.14ms
step:1292/1770 train_time:125821ms step_avg:98.14ms
step:1293/1770 train_time:125923ms step_avg:98.15ms
step:1294/1770 train_time:126024ms step_avg:98.15ms
step:1295/1770 train_time:126127ms step_avg:98.15ms
step:1296/1770 train_time:126228ms step_avg:98.16ms
step:1297/1770 train_time:126331ms step_avg:98.16ms
step:1298/1770 train_time:126433ms step_avg:98.16ms
step:1299/1770 train_time:126536ms step_avg:98.17ms
step:1300/1770 train_time:126637ms step_avg:98.17ms
step:1301/1770 train_time:126740ms step_avg:98.17ms
step:1302/1770 train_time:126842ms step_avg:98.18ms
step:1303/1770 train_time:126944ms step_avg:98.18ms
step:1304/1770 train_time:127046ms step_avg:98.18ms
step:1305/1770 train_time:127149ms step_avg:98.18ms
step:1306/1770 train_time:127251ms step_avg:98.19ms
step:1307/1770 train_time:127353ms step_avg:98.19ms
step:1308/1770 train_time:127455ms step_avg:98.19ms
step:1309/1770 train_time:127557ms step_avg:98.20ms
step:1310/1770 train_time:127659ms step_avg:98.20ms
step:1311/1770 train_time:127761ms step_avg:98.20ms
step:1312/1770 train_time:127863ms step_avg:98.21ms
step:1313/1770 train_time:127964ms step_avg:98.21ms
step:1314/1770 train_time:128066ms step_avg:98.21ms
step:1315/1770 train_time:128169ms step_avg:98.21ms
step:1316/1770 train_time:128271ms step_avg:98.22ms
step:1317/1770 train_time:128374ms step_avg:98.22ms
step:1318/1770 train_time:128480ms step_avg:98.23ms
step:1319/1770 train_time:128582ms step_avg:98.23ms
step:1320/1770 train_time:128684ms step_avg:98.23ms
step:1321/1770 train_time:128787ms step_avg:98.24ms
step:1322/1770 train_time:128889ms step_avg:98.24ms
step:1323/1770 train_time:128992ms step_avg:98.24ms
step:1324/1770 train_time:129094ms step_avg:98.25ms
step:1325/1770 train_time:129197ms step_avg:98.25ms
step:1326/1770 train_time:129299ms step_avg:98.25ms
step:1327/1770 train_time:129404ms step_avg:98.26ms
step:1328/1770 train_time:129506ms step_avg:98.26ms
step:1329/1770 train_time:129608ms step_avg:98.26ms
step:1330/1770 train_time:129709ms step_avg:98.26ms
step:1331/1770 train_time:129810ms step_avg:98.27ms
step:1332/1770 train_time:129912ms step_avg:98.27ms
step:1333/1770 train_time:130013ms step_avg:98.27ms
step:1334/1770 train_time:130114ms step_avg:98.27ms
step:1335/1770 train_time:130216ms step_avg:98.28ms
step:1336/1770 train_time:130318ms step_avg:98.28ms
step:1337/1770 train_time:130421ms step_avg:98.28ms
step:1338/1770 train_time:130524ms step_avg:98.29ms
step:1339/1770 train_time:130627ms step_avg:98.29ms
step:1340/1770 train_time:130730ms step_avg:98.29ms
step:1341/1770 train_time:130832ms step_avg:98.30ms
step:1342/1770 train_time:130934ms step_avg:98.30ms
step:1343/1770 train_time:131037ms step_avg:98.30ms
step:1344/1770 train_time:131140ms step_avg:98.31ms
step:1345/1770 train_time:131242ms step_avg:98.31ms
step:1346/1770 train_time:131344ms step_avg:98.31ms
step:1347/1770 train_time:131447ms step_avg:98.31ms
step:1348/1770 train_time:131551ms step_avg:98.32ms
step:1349/1770 train_time:131654ms step_avg:98.32ms
step:1350/1770 train_time:131756ms step_avg:98.33ms
step:1351/1770 train_time:131858ms step_avg:98.33ms
step:1352/1770 train_time:131960ms step_avg:98.33ms
step:1353/1770 train_time:132063ms step_avg:98.33ms
step:1354/1770 train_time:132165ms step_avg:98.34ms
step:1355/1770 train_time:132267ms step_avg:98.34ms
step:1356/1770 train_time:132369ms step_avg:98.34ms
step:1357/1770 train_time:132470ms step_avg:98.34ms
step:1358/1770 train_time:132573ms step_avg:98.35ms
step:1359/1770 train_time:132676ms step_avg:98.35ms
step:1360/1770 train_time:132778ms step_avg:98.35ms
step:1361/1770 train_time:132881ms step_avg:98.36ms
step:1362/1770 train_time:132983ms step_avg:98.36ms
step:1363/1770 train_time:133085ms step_avg:98.36ms
step:1364/1770 train_time:133188ms step_avg:98.37ms
step:1365/1770 train_time:133290ms step_avg:98.37ms
step:1366/1770 train_time:133391ms step_avg:98.37ms
step:1367/1770 train_time:133494ms step_avg:98.37ms
step:1368/1770 train_time:133596ms step_avg:98.38ms
step:1369/1770 train_time:133699ms step_avg:98.38ms
step:1370/1770 train_time:133802ms step_avg:98.38ms
step:1371/1770 train_time:133903ms step_avg:98.39ms
step:1372/1770 train_time:134004ms step_avg:98.39ms
step:1373/1770 train_time:134106ms step_avg:98.39ms
step:1374/1770 train_time:134208ms step_avg:98.39ms
step:1375/1770 train_time:134311ms step_avg:98.40ms
step:1375/1770 val_loss:3.3822 train_time:134412ms step_avg:98.47ms
step:1376/1770 train_time:134433ms step_avg:98.41ms
step:1377/1770 train_time:134524ms step_avg:98.41ms
step:1378/1770 train_time:134626ms step_avg:98.41ms
step:1379/1770 train_time:134727ms step_avg:98.41ms
step:1380/1770 train_time:134829ms step_avg:98.42ms
step:1381/1770 train_time:134931ms step_avg:98.42ms
step:1382/1770 train_time:135032ms step_avg:98.42ms
step:1383/1770 train_time:135135ms step_avg:98.42ms
step:1384/1770 train_time:135237ms step_avg:98.43ms
step:1385/1770 train_time:135340ms step_avg:98.43ms
step:1386/1770 train_time:135443ms step_avg:98.43ms
step:1387/1770 train_time:135547ms step_avg:98.44ms
step:1388/1770 train_time:135649ms step_avg:98.44ms
step:1389/1770 train_time:135751ms step_avg:98.44ms
step:1390/1770 train_time:135853ms step_avg:98.44ms
step:1391/1770 train_time:135956ms step_avg:98.45ms
step:1392/1770 train_time:136059ms step_avg:98.45ms
step:1393/1770 train_time:136160ms step_avg:98.45ms
step:1394/1770 train_time:136262ms step_avg:98.46ms
step:1395/1770 train_time:136365ms step_avg:98.46ms
step:1396/1770 train_time:136468ms step_avg:98.46ms
step:1397/1770 train_time:136570ms step_avg:98.46ms
step:1398/1770 train_time:136672ms step_avg:98.47ms
step:1399/1770 train_time:136773ms step_avg:98.47ms
step:1400/1770 train_time:136876ms step_avg:98.47ms
step:1401/1770 train_time:136978ms step_avg:98.47ms
step:1402/1770 train_time:137081ms step_avg:98.48ms
step:1403/1770 train_time:137182ms step_avg:98.48ms
step:1404/1770 train_time:137285ms step_avg:98.48ms
step:1405/1770 train_time:137387ms step_avg:98.49ms
step:1406/1770 train_time:137489ms step_avg:98.49ms
step:1407/1770 train_time:137591ms step_avg:98.49ms
step:1408/1770 train_time:137693ms step_avg:98.49ms
step:1409/1770 train_time:137795ms step_avg:98.50ms
step:1410/1770 train_time:137898ms step_avg:98.50ms
step:1411/1770 train_time:138001ms step_avg:98.50ms
step:1412/1770 train_time:138102ms step_avg:98.50ms
step:1413/1770 train_time:138203ms step_avg:98.51ms
step:1414/1770 train_time:138306ms step_avg:98.51ms
step:1415/1770 train_time:138409ms step_avg:98.51ms
step:1416/1770 train_time:138512ms step_avg:98.51ms
step:1417/1770 train_time:138614ms step_avg:98.52ms
step:1418/1770 train_time:138717ms step_avg:98.52ms
step:1419/1770 train_time:138820ms step_avg:98.52ms
step:1420/1770 train_time:138922ms step_avg:98.53ms
step:1421/1770 train_time:139024ms step_avg:98.53ms
step:1422/1770 train_time:139125ms step_avg:98.53ms
step:1423/1770 train_time:139227ms step_avg:98.53ms
step:1424/1770 train_time:139329ms step_avg:98.54ms
step:1425/1770 train_time:139431ms step_avg:98.54ms
step:1426/1770 train_time:139534ms step_avg:98.54ms
step:1427/1770 train_time:139636ms step_avg:98.54ms
step:1428/1770 train_time:139740ms step_avg:98.55ms
step:1429/1770 train_time:139843ms step_avg:98.55ms
step:1430/1770 train_time:139944ms step_avg:98.55ms
step:1431/1770 train_time:140048ms step_avg:98.56ms
step:1432/1770 train_time:140150ms step_avg:98.56ms
step:1433/1770 train_time:140252ms step_avg:98.56ms
step:1434/1770 train_time:140353ms step_avg:98.56ms
step:1435/1770 train_time:140455ms step_avg:98.56ms
step:1436/1770 train_time:140558ms step_avg:98.57ms
step:1437/1770 train_time:140660ms step_avg:98.57ms
step:1438/1770 train_time:140762ms step_avg:98.57ms
step:1439/1770 train_time:140865ms step_avg:98.58ms
step:1440/1770 train_time:140967ms step_avg:98.58ms
step:1441/1770 train_time:141073ms step_avg:98.58ms
step:1442/1770 train_time:141175ms step_avg:98.59ms
step:1443/1770 train_time:141279ms step_avg:98.59ms
step:1444/1770 train_time:141382ms step_avg:98.59ms
step:1445/1770 train_time:141484ms step_avg:98.60ms
step:1446/1770 train_time:141587ms step_avg:98.60ms
step:1447/1770 train_time:141690ms step_avg:98.60ms
step:1448/1770 train_time:141793ms step_avg:98.60ms
step:1449/1770 train_time:141897ms step_avg:98.61ms
step:1450/1770 train_time:141999ms step_avg:98.61ms
step:1451/1770 train_time:142103ms step_avg:98.61ms
step:1452/1770 train_time:142206ms step_avg:98.62ms
step:1453/1770 train_time:142308ms step_avg:98.62ms
step:1454/1770 train_time:142412ms step_avg:98.62ms
step:1455/1770 train_time:142517ms step_avg:98.63ms
step:1456/1770 train_time:142621ms step_avg:98.63ms
step:1457/1770 train_time:142724ms step_avg:98.63ms
step:1458/1770 train_time:142828ms step_avg:98.64ms
step:1459/1770 train_time:142932ms step_avg:98.64ms
step:1460/1770 train_time:143035ms step_avg:98.65ms
step:1461/1770 train_time:143139ms step_avg:98.65ms
step:1462/1770 train_time:143241ms step_avg:98.65ms
step:1463/1770 train_time:143345ms step_avg:98.65ms
step:1464/1770 train_time:143449ms step_avg:98.66ms
step:1465/1770 train_time:143552ms step_avg:98.66ms
step:1466/1770 train_time:143657ms step_avg:98.67ms
step:1467/1770 train_time:143762ms step_avg:98.67ms
step:1468/1770 train_time:143865ms step_avg:98.67ms
step:1469/1770 train_time:143968ms step_avg:98.68ms
step:1470/1770 train_time:144071ms step_avg:98.68ms
step:1471/1770 train_time:144173ms step_avg:98.68ms
step:1472/1770 train_time:144277ms step_avg:98.68ms
step:1473/1770 train_time:144380ms step_avg:98.69ms
step:1474/1770 train_time:144485ms step_avg:98.69ms
step:1475/1770 train_time:144588ms step_avg:98.70ms
step:1476/1770 train_time:144691ms step_avg:98.70ms
step:1477/1770 train_time:144796ms step_avg:98.70ms
step:1478/1770 train_time:144899ms step_avg:98.71ms
step:1479/1770 train_time:145002ms step_avg:98.71ms
step:1480/1770 train_time:145106ms step_avg:98.71ms
step:1481/1770 train_time:145214ms step_avg:98.72ms
step:1482/1770 train_time:145316ms step_avg:98.72ms
step:1483/1770 train_time:145420ms step_avg:98.72ms
step:1484/1770 train_time:145523ms step_avg:98.73ms
step:1485/1770 train_time:145626ms step_avg:98.73ms
step:1486/1770 train_time:145729ms step_avg:98.73ms
step:1487/1770 train_time:145832ms step_avg:98.74ms
step:1488/1770 train_time:145935ms step_avg:98.74ms
step:1489/1770 train_time:146040ms step_avg:98.74ms
step:1490/1770 train_time:146143ms step_avg:98.75ms
step:1491/1770 train_time:146246ms step_avg:98.75ms
step:1492/1770 train_time:146351ms step_avg:98.75ms
step:1493/1770 train_time:146457ms step_avg:98.76ms
step:1494/1770 train_time:146563ms step_avg:98.76ms
step:1495/1770 train_time:146667ms step_avg:98.77ms
step:1496/1770 train_time:146769ms step_avg:98.77ms
step:1497/1770 train_time:146873ms step_avg:98.77ms
step:1498/1770 train_time:146975ms step_avg:98.77ms
step:1499/1770 train_time:147078ms step_avg:98.78ms
step:1500/1770 train_time:147181ms step_avg:98.78ms
step:1500/1770 val_loss:3.3443 train_time:147282ms step_avg:98.85ms
step:1501/1770 train_time:147303ms step_avg:98.80ms
step:1502/1770 train_time:147397ms step_avg:98.79ms
step:1503/1770 train_time:147500ms step_avg:98.79ms
step:1504/1770 train_time:147602ms step_avg:98.80ms
step:1505/1770 train_time:147708ms step_avg:98.80ms
step:1506/1770 train_time:147811ms step_avg:98.80ms
step:1507/1770 train_time:147914ms step_avg:98.81ms
step:1508/1770 train_time:148018ms step_avg:98.81ms
step:1509/1770 train_time:148122ms step_avg:98.81ms
step:1510/1770 train_time:148224ms step_avg:98.82ms
step:1511/1770 train_time:148329ms step_avg:98.82ms
step:1512/1770 train_time:148433ms step_avg:98.82ms
step:1513/1770 train_time:148538ms step_avg:98.83ms
step:1514/1770 train_time:148641ms step_avg:98.83ms
step:1515/1770 train_time:148744ms step_avg:98.83ms
step:1516/1770 train_time:148849ms step_avg:98.84ms
step:1517/1770 train_time:148950ms step_avg:98.84ms
step:1518/1770 train_time:149056ms step_avg:98.84ms
step:1519/1770 train_time:149157ms step_avg:98.85ms
step:1520/1770 train_time:149262ms step_avg:98.85ms
step:1521/1770 train_time:149365ms step_avg:98.85ms
step:1522/1770 train_time:149469ms step_avg:98.86ms
step:1523/1770 train_time:149573ms step_avg:98.86ms
step:1524/1770 train_time:149675ms step_avg:98.86ms
step:1525/1770 train_time:149778ms step_avg:98.86ms
step:1526/1770 train_time:149880ms step_avg:98.87ms
step:1527/1770 train_time:149984ms step_avg:98.87ms
step:1528/1770 train_time:150090ms step_avg:98.87ms
step:1529/1770 train_time:150193ms step_avg:98.88ms
step:1530/1770 train_time:150296ms step_avg:98.88ms
step:1531/1770 train_time:150399ms step_avg:98.88ms
step:1532/1770 train_time:150503ms step_avg:98.89ms
step:1533/1770 train_time:150607ms step_avg:98.89ms
step:1534/1770 train_time:150712ms step_avg:98.89ms
step:1535/1770 train_time:150815ms step_avg:98.89ms
step:1536/1770 train_time:150917ms step_avg:98.90ms
step:1537/1770 train_time:151021ms step_avg:98.90ms
step:1538/1770 train_time:151125ms step_avg:98.90ms
step:1539/1770 train_time:151228ms step_avg:98.91ms
step:1540/1770 train_time:151335ms step_avg:98.91ms
step:1541/1770 train_time:151439ms step_avg:98.91ms
step:1542/1770 train_time:151543ms step_avg:98.92ms
step:1543/1770 train_time:151645ms step_avg:98.92ms
step:1544/1770 train_time:151750ms step_avg:98.92ms
step:1545/1770 train_time:151853ms step_avg:98.93ms
step:1546/1770 train_time:151957ms step_avg:98.93ms
step:1547/1770 train_time:152059ms step_avg:98.93ms
step:1548/1770 train_time:152162ms step_avg:98.94ms
step:1549/1770 train_time:152265ms step_avg:98.94ms
step:1550/1770 train_time:152369ms step_avg:98.94ms
step:1551/1770 train_time:152472ms step_avg:98.94ms
step:1552/1770 train_time:152577ms step_avg:98.95ms
step:1553/1770 train_time:152680ms step_avg:98.95ms
step:1554/1770 train_time:152783ms step_avg:98.95ms
step:1555/1770 train_time:152887ms step_avg:98.96ms
step:1556/1770 train_time:152989ms step_avg:98.96ms
step:1557/1770 train_time:153092ms step_avg:98.96ms
step:1558/1770 train_time:153195ms step_avg:98.96ms
step:1559/1770 train_time:153298ms step_avg:98.97ms
step:1560/1770 train_time:153400ms step_avg:98.97ms
step:1561/1770 train_time:153506ms step_avg:98.97ms
step:1562/1770 train_time:153609ms step_avg:98.97ms
step:1563/1770 train_time:153712ms step_avg:98.98ms
step:1564/1770 train_time:153815ms step_avg:98.98ms
step:1565/1770 train_time:153919ms step_avg:98.98ms
step:1566/1770 train_time:154022ms step_avg:98.99ms
step:1567/1770 train_time:154125ms step_avg:98.99ms
step:1568/1770 train_time:154228ms step_avg:98.99ms
step:1569/1770 train_time:154335ms step_avg:99.00ms
step:1570/1770 train_time:154438ms step_avg:99.00ms
step:1571/1770 train_time:154540ms step_avg:99.00ms
step:1572/1770 train_time:154644ms step_avg:99.00ms
step:1573/1770 train_time:154750ms step_avg:99.01ms
step:1574/1770 train_time:154853ms step_avg:99.01ms
step:1575/1770 train_time:154955ms step_avg:99.01ms
step:1576/1770 train_time:155058ms step_avg:99.02ms
step:1577/1770 train_time:155164ms step_avg:99.02ms
step:1578/1770 train_time:155269ms step_avg:99.02ms
step:1579/1770 train_time:155372ms step_avg:99.03ms
step:1580/1770 train_time:155476ms step_avg:99.03ms
step:1581/1770 train_time:155582ms step_avg:99.03ms
step:1582/1770 train_time:155686ms step_avg:99.04ms
step:1583/1770 train_time:155789ms step_avg:99.04ms
step:1584/1770 train_time:155892ms step_avg:99.04ms
step:1585/1770 train_time:155996ms step_avg:99.05ms
step:1586/1770 train_time:156103ms step_avg:99.05ms
step:1587/1770 train_time:156207ms step_avg:99.05ms
step:1588/1770 train_time:156311ms step_avg:99.06ms
step:1589/1770 train_time:156416ms step_avg:99.06ms
step:1590/1770 train_time:156519ms step_avg:99.06ms
step:1591/1770 train_time:156621ms step_avg:99.06ms
step:1592/1770 train_time:156725ms step_avg:99.07ms
step:1593/1770 train_time:156828ms step_avg:99.07ms
step:1594/1770 train_time:156932ms step_avg:99.07ms
step:1595/1770 train_time:157035ms step_avg:99.08ms
step:1596/1770 train_time:157139ms step_avg:99.08ms
step:1597/1770 train_time:157242ms step_avg:99.08ms
step:1598/1770 train_time:157346ms step_avg:99.08ms
step:1599/1770 train_time:157451ms step_avg:99.09ms
step:1600/1770 train_time:157556ms step_avg:99.09ms
step:1601/1770 train_time:157660ms step_avg:99.10ms
step:1602/1770 train_time:157764ms step_avg:99.10ms
step:1603/1770 train_time:157868ms step_avg:99.10ms
step:1604/1770 train_time:157970ms step_avg:99.10ms
step:1605/1770 train_time:158074ms step_avg:99.11ms
step:1606/1770 train_time:158178ms step_avg:99.11ms
step:1607/1770 train_time:158285ms step_avg:99.11ms
step:1608/1770 train_time:158388ms step_avg:99.12ms
step:1609/1770 train_time:158491ms step_avg:99.12ms
step:1610/1770 train_time:158596ms step_avg:99.12ms
step:1611/1770 train_time:158702ms step_avg:99.13ms
step:1612/1770 train_time:158806ms step_avg:99.13ms
step:1613/1770 train_time:158909ms step_avg:99.13ms
step:1614/1770 train_time:159012ms step_avg:99.13ms
step:1615/1770 train_time:159115ms step_avg:99.14ms
step:1616/1770 train_time:159219ms step_avg:99.14ms
step:1617/1770 train_time:159324ms step_avg:99.14ms
step:1618/1770 train_time:159428ms step_avg:99.15ms
step:1619/1770 train_time:159531ms step_avg:99.15ms
step:1620/1770 train_time:159635ms step_avg:99.15ms
step:1621/1770 train_time:159738ms step_avg:99.15ms
step:1622/1770 train_time:159843ms step_avg:99.16ms
step:1623/1770 train_time:159949ms step_avg:99.16ms
step:1624/1770 train_time:160051ms step_avg:99.16ms
step:1625/1770 train_time:160154ms step_avg:99.17ms
step:1625/1770 val_loss:3.3105 train_time:160257ms step_avg:99.23ms
step:1626/1770 train_time:160279ms step_avg:99.18ms
step:1627/1770 train_time:160370ms step_avg:99.18ms
step:1628/1770 train_time:160473ms step_avg:99.18ms
step:1629/1770 train_time:160575ms step_avg:99.18ms
step:1630/1770 train_time:160678ms step_avg:99.18ms
step:1631/1770 train_time:160781ms step_avg:99.19ms
step:1632/1770 train_time:160884ms step_avg:99.19ms
step:1633/1770 train_time:160987ms step_avg:99.19ms
step:1634/1770 train_time:161090ms step_avg:99.19ms
step:1635/1770 train_time:161194ms step_avg:99.20ms
step:1636/1770 train_time:161299ms step_avg:99.20ms
step:1637/1770 train_time:161404ms step_avg:99.20ms
step:1638/1770 train_time:161508ms step_avg:99.21ms
step:1639/1770 train_time:161611ms step_avg:99.21ms
step:1640/1770 train_time:161715ms step_avg:99.21ms
step:1641/1770 train_time:161818ms step_avg:99.21ms
step:1642/1770 train_time:161921ms step_avg:99.22ms
step:1643/1770 train_time:162025ms step_avg:99.22ms
step:1644/1770 train_time:162130ms step_avg:99.22ms
step:1645/1770 train_time:162233ms step_avg:99.22ms
step:1646/1770 train_time:162339ms step_avg:99.23ms
step:1647/1770 train_time:162443ms step_avg:99.23ms
step:1648/1770 train_time:162546ms step_avg:99.23ms
step:1649/1770 train_time:162650ms step_avg:99.24ms
step:1650/1770 train_time:162753ms step_avg:99.24ms
step:1651/1770 train_time:162856ms step_avg:99.24ms
step:1652/1770 train_time:162960ms step_avg:99.24ms
step:1653/1770 train_time:163063ms step_avg:99.25ms
step:1654/1770 train_time:163170ms step_avg:99.25ms
step:1655/1770 train_time:163275ms step_avg:99.26ms
step:1656/1770 train_time:163379ms step_avg:99.26ms
step:1657/1770 train_time:163485ms step_avg:99.26ms
step:1658/1770 train_time:163588ms step_avg:99.26ms
step:1659/1770 train_time:163694ms step_avg:99.27ms
step:1660/1770 train_time:163797ms step_avg:99.27ms
step:1661/1770 train_time:163901ms step_avg:99.27ms
step:1662/1770 train_time:164004ms step_avg:99.28ms
step:1663/1770 train_time:164107ms step_avg:99.28ms
step:1664/1770 train_time:164210ms step_avg:99.28ms
step:1665/1770 train_time:164312ms step_avg:99.28ms
step:1666/1770 train_time:164417ms step_avg:99.29ms
step:1667/1770 train_time:164519ms step_avg:99.29ms
step:1668/1770 train_time:164623ms step_avg:99.29ms
step:1669/1770 train_time:164725ms step_avg:99.29ms
step:1670/1770 train_time:164829ms step_avg:99.29ms
step:1671/1770 train_time:164932ms step_avg:99.30ms
step:1672/1770 train_time:165036ms step_avg:99.30ms
step:1673/1770 train_time:165141ms step_avg:99.30ms
step:1674/1770 train_time:165243ms step_avg:99.30ms
step:1675/1770 train_time:165346ms step_avg:99.31ms
step:1676/1770 train_time:165451ms step_avg:99.31ms
step:1677/1770 train_time:165558ms step_avg:99.31ms
step:1678/1770 train_time:165660ms step_avg:99.32ms
step:1679/1770 train_time:165764ms step_avg:99.32ms
step:1680/1770 train_time:165867ms step_avg:99.32ms
step:1681/1770 train_time:165971ms step_avg:99.32ms
step:1682/1770 train_time:166076ms step_avg:99.33ms
step:1683/1770 train_time:166178ms step_avg:99.33ms
step:1684/1770 train_time:166282ms step_avg:99.33ms
step:1685/1770 train_time:166386ms step_avg:99.33ms
step:1686/1770 train_time:166490ms step_avg:99.34ms
step:1687/1770 train_time:166594ms step_avg:99.34ms
step:1688/1770 train_time:166698ms step_avg:99.34ms
step:1689/1770 train_time:166801ms step_avg:99.35ms
step:1690/1770 train_time:166904ms step_avg:99.35ms
step:1691/1770 train_time:167007ms step_avg:99.35ms
step:1692/1770 train_time:167111ms step_avg:99.35ms
step:1693/1770 train_time:167215ms step_avg:99.36ms
step:1694/1770 train_time:167320ms step_avg:99.36ms
step:1695/1770 train_time:167424ms step_avg:99.36ms
step:1696/1770 train_time:167529ms step_avg:99.36ms
step:1697/1770 train_time:167634ms step_avg:99.37ms
step:1698/1770 train_time:167738ms step_avg:99.37ms
step:1699/1770 train_time:167841ms step_avg:99.37ms
step:1700/1770 train_time:167944ms step_avg:99.38ms
step:1701/1770 train_time:168047ms step_avg:99.38ms
step:1702/1770 train_time:168150ms step_avg:99.38ms
step:1703/1770 train_time:168253ms step_avg:99.38ms
step:1704/1770 train_time:168356ms step_avg:99.38ms
step:1705/1770 train_time:168460ms step_avg:99.39ms
step:1706/1770 train_time:168563ms step_avg:99.39ms
step:1707/1770 train_time:168668ms step_avg:99.39ms
step:1708/1770 train_time:168771ms step_avg:99.39ms
step:1709/1770 train_time:168876ms step_avg:99.40ms
step:1710/1770 train_time:168984ms step_avg:99.40ms
step:1711/1770 train_time:169090ms step_avg:99.41ms
step:1712/1770 train_time:169194ms step_avg:99.41ms
step:1713/1770 train_time:169297ms step_avg:99.41ms
step:1714/1770 train_time:169401ms step_avg:99.41ms
step:1715/1770 train_time:169505ms step_avg:99.42ms
step:1716/1770 train_time:169609ms step_avg:99.42ms
step:1717/1770 train_time:169713ms step_avg:99.42ms
step:1718/1770 train_time:169819ms step_avg:99.43ms
step:1719/1770 train_time:169923ms step_avg:99.43ms
step:1720/1770 train_time:170028ms step_avg:99.43ms
step:1721/1770 train_time:170131ms step_avg:99.43ms
step:1722/1770 train_time:170238ms step_avg:99.44ms
step:1723/1770 train_time:170347ms step_avg:99.44ms
step:1724/1770 train_time:170453ms step_avg:99.45ms
step:1725/1770 train_time:170560ms step_avg:99.45ms
step:1726/1770 train_time:170666ms step_avg:99.46ms
step:1727/1770 train_time:170769ms step_avg:99.46ms
step:1728/1770 train_time:170876ms step_avg:99.46ms
step:1729/1770 train_time:170980ms step_avg:99.46ms
step:1730/1770 train_time:171085ms step_avg:99.47ms
step:1731/1770 train_time:171190ms step_avg:99.47ms
step:1732/1770 train_time:171294ms step_avg:99.47ms
step:1733/1770 train_time:171400ms step_avg:99.48ms
step:1734/1770 train_time:171505ms step_avg:99.48ms
step:1735/1770 train_time:171611ms step_avg:99.48ms
step:1736/1770 train_time:171715ms step_avg:99.49ms
step:1737/1770 train_time:171819ms step_avg:99.49ms
step:1738/1770 train_time:171924ms step_avg:99.49ms
step:1739/1770 train_time:172028ms step_avg:99.50ms
step:1740/1770 train_time:172132ms step_avg:99.50ms
step:1741/1770 train_time:172238ms step_avg:99.50ms
step:1742/1770 train_time:172344ms step_avg:99.51ms
step:1743/1770 train_time:172450ms step_avg:99.51ms
step:1744/1770 train_time:172554ms step_avg:99.51ms
step:1745/1770 train_time:172658ms step_avg:99.51ms
step:1746/1770 train_time:172764ms step_avg:99.52ms
step:1747/1770 train_time:172867ms step_avg:99.52ms
step:1748/1770 train_time:172974ms step_avg:99.52ms
step:1749/1770 train_time:173080ms step_avg:99.53ms
step:1750/1770 train_time:173184ms step_avg:99.53ms
step:1750/1770 val_loss:3.2838 train_time:173287ms step_avg:99.59ms
step:1751/1770 train_time:173308ms step_avg:99.55ms
step:1752/1770 train_time:173399ms step_avg:99.54ms
step:1753/1770 train_time:173504ms step_avg:99.54ms
step:1754/1770 train_time:173609ms step_avg:99.55ms
step:1755/1770 train_time:173712ms step_avg:99.55ms
step:1756/1770 train_time:173817ms step_avg:99.55ms
step:1757/1770 train_time:173921ms step_avg:99.55ms
step:1758/1770 train_time:174026ms step_avg:99.56ms
step:1759/1770 train_time:174130ms step_avg:99.56ms
step:1760/1770 train_time:174234ms step_avg:99.56ms
step:1761/1770 train_time:174341ms step_avg:99.57ms
step:1762/1770 train_time:174449ms step_avg:99.57ms
step:1763/1770 train_time:174552ms step_avg:99.57ms
step:1764/1770 train_time:174656ms step_avg:99.58ms
step:1765/1770 train_time:174760ms step_avg:99.58ms
step:1766/1770 train_time:174869ms step_avg:99.58ms
step:1767/1770 train_time:174972ms step_avg:99.59ms
step:1768/1770 train_time:175076ms step_avg:99.59ms
step:1769/1770 train_time:175179ms step_avg:99.59ms
step:1770/1770 train_time:175282ms step_avg:99.59ms
step:1770/1770 val_loss:3.2809 train_time:175387ms step_avg:99.65ms
peak memory allocated: 28840 MiB reserved: 32192 MiB
