import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:05:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24793ms step_avg:nanms
step:2/1770 train_time:25348ms step_avg:nanms
step:3/1770 train_time:25446ms step_avg:nanms
step:4/1770 train_time:25539ms step_avg:nanms
step:5/1770 train_time:25633ms step_avg:nanms
step:6/1770 train_time:25726ms step_avg:nanms
step:7/1770 train_time:25820ms step_avg:nanms
step:8/1770 train_time:25914ms step_avg:nanms
step:9/1770 train_time:26007ms step_avg:nanms
step:10/1770 train_time:26101ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.78ms
step:14/1770 train_time:375ms step_avg:93.85ms
step:15/1770 train_time:469ms step_avg:93.80ms
step:16/1770 train_time:563ms step_avg:93.86ms
step:17/1770 train_time:657ms step_avg:93.87ms
step:18/1770 train_time:751ms step_avg:93.82ms
step:19/1770 train_time:845ms step_avg:93.87ms
step:20/1770 train_time:939ms step_avg:93.91ms
step:21/1770 train_time:1033ms step_avg:93.95ms
step:22/1770 train_time:1127ms step_avg:93.95ms
step:23/1770 train_time:1222ms step_avg:93.97ms
step:24/1770 train_time:1316ms step_avg:93.98ms
step:25/1770 train_time:1409ms step_avg:93.94ms
step:26/1770 train_time:1503ms step_avg:93.94ms
step:27/1770 train_time:1597ms step_avg:93.97ms
step:28/1770 train_time:1691ms step_avg:93.97ms
step:29/1770 train_time:1785ms step_avg:93.96ms
step:30/1770 train_time:1879ms step_avg:93.97ms
step:31/1770 train_time:1974ms step_avg:94.01ms
step:32/1770 train_time:2068ms step_avg:93.99ms
step:33/1770 train_time:2162ms step_avg:94.00ms
step:34/1770 train_time:2256ms step_avg:94.00ms
step:35/1770 train_time:2349ms step_avg:93.98ms
step:36/1770 train_time:2444ms step_avg:93.99ms
step:37/1770 train_time:2538ms step_avg:94.00ms
step:38/1770 train_time:2632ms step_avg:94.02ms
step:39/1770 train_time:2726ms step_avg:94.02ms
step:40/1770 train_time:2820ms step_avg:94.02ms
step:41/1770 train_time:2915ms step_avg:94.02ms
step:42/1770 train_time:3008ms step_avg:94.02ms
step:43/1770 train_time:3102ms step_avg:94.01ms
step:44/1770 train_time:3197ms step_avg:94.02ms
step:45/1770 train_time:3291ms step_avg:94.02ms
step:46/1770 train_time:3385ms step_avg:94.02ms
step:47/1770 train_time:3479ms step_avg:94.03ms
step:48/1770 train_time:3573ms step_avg:94.03ms
step:49/1770 train_time:3667ms step_avg:94.02ms
step:50/1770 train_time:3761ms step_avg:94.02ms
step:51/1770 train_time:3855ms step_avg:94.03ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4043ms step_avg:94.02ms
step:54/1770 train_time:4137ms step_avg:94.02ms
step:55/1770 train_time:4231ms step_avg:94.02ms
step:56/1770 train_time:4325ms step_avg:94.01ms
step:57/1770 train_time:4419ms step_avg:94.03ms
step:58/1770 train_time:4514ms step_avg:94.04ms
step:59/1770 train_time:4608ms step_avg:94.05ms
step:60/1770 train_time:4702ms step_avg:94.05ms
step:61/1770 train_time:4796ms step_avg:94.05ms
step:62/1770 train_time:4890ms step_avg:94.04ms
step:63/1770 train_time:4984ms step_avg:94.04ms
step:64/1770 train_time:5078ms step_avg:94.04ms
step:65/1770 train_time:5172ms step_avg:94.03ms
step:66/1770 train_time:5266ms step_avg:94.03ms
step:67/1770 train_time:5360ms step_avg:94.03ms
step:68/1770 train_time:5454ms step_avg:94.04ms
step:69/1770 train_time:5548ms step_avg:94.04ms
step:70/1770 train_time:5642ms step_avg:94.04ms
step:71/1770 train_time:5737ms step_avg:94.04ms
step:72/1770 train_time:5831ms step_avg:94.04ms
step:73/1770 train_time:5924ms step_avg:94.03ms
step:74/1770 train_time:6018ms step_avg:94.03ms
step:75/1770 train_time:6112ms step_avg:94.03ms
step:76/1770 train_time:6206ms step_avg:94.03ms
step:77/1770 train_time:6300ms step_avg:94.03ms
step:78/1770 train_time:6394ms step_avg:94.03ms
step:79/1770 train_time:6487ms step_avg:94.02ms
step:80/1770 train_time:6581ms step_avg:94.02ms
step:81/1770 train_time:6675ms step_avg:94.02ms
step:82/1770 train_time:6769ms step_avg:94.01ms
step:83/1770 train_time:6863ms step_avg:94.01ms
step:84/1770 train_time:6958ms step_avg:94.02ms
step:85/1770 train_time:7052ms step_avg:94.03ms
step:86/1770 train_time:7146ms step_avg:94.03ms
step:87/1770 train_time:7240ms step_avg:94.03ms
step:88/1770 train_time:7334ms step_avg:94.03ms
step:89/1770 train_time:7428ms step_avg:94.03ms
step:90/1770 train_time:7523ms step_avg:94.04ms
step:91/1770 train_time:7617ms step_avg:94.04ms
step:92/1770 train_time:7711ms step_avg:94.04ms
step:93/1770 train_time:7805ms step_avg:94.03ms
step:94/1770 train_time:7899ms step_avg:94.03ms
step:95/1770 train_time:7992ms step_avg:94.03ms
step:96/1770 train_time:8086ms step_avg:94.03ms
step:97/1770 train_time:8180ms step_avg:94.03ms
step:98/1770 train_time:8275ms step_avg:94.03ms
step:99/1770 train_time:8368ms step_avg:94.03ms
step:100/1770 train_time:8463ms step_avg:94.03ms
step:101/1770 train_time:8557ms step_avg:94.03ms
step:102/1770 train_time:8651ms step_avg:94.03ms
step:103/1770 train_time:8744ms step_avg:94.02ms
step:104/1770 train_time:8839ms step_avg:94.03ms
step:105/1770 train_time:8933ms step_avg:94.03ms
step:106/1770 train_time:9027ms step_avg:94.03ms
step:107/1770 train_time:9121ms step_avg:94.03ms
step:108/1770 train_time:9215ms step_avg:94.03ms
step:109/1770 train_time:9309ms step_avg:94.03ms
step:110/1770 train_time:9403ms step_avg:94.03ms
step:111/1770 train_time:9497ms step_avg:94.03ms
step:112/1770 train_time:9591ms step_avg:94.03ms
step:113/1770 train_time:9685ms step_avg:94.03ms
step:114/1770 train_time:9779ms step_avg:94.03ms
step:115/1770 train_time:9873ms step_avg:94.03ms
step:116/1770 train_time:9966ms step_avg:94.02ms
step:117/1770 train_time:10061ms step_avg:94.02ms
step:118/1770 train_time:10155ms step_avg:94.03ms
step:119/1770 train_time:10249ms step_avg:94.03ms
step:120/1770 train_time:10343ms step_avg:94.03ms
step:121/1770 train_time:10437ms step_avg:94.03ms
step:122/1770 train_time:10531ms step_avg:94.03ms
step:123/1770 train_time:10625ms step_avg:94.02ms
step:124/1770 train_time:10718ms step_avg:94.02ms
step:125/1770 train_time:10813ms step_avg:94.02ms
step:125/1770 val_loss:4.6476 train_time:10905ms step_avg:94.83ms
step:126/1770 train_time:10927ms step_avg:94.20ms
step:127/1770 train_time:11012ms step_avg:94.12ms
step:128/1770 train_time:11112ms step_avg:94.17ms
step:129/1770 train_time:11207ms step_avg:94.18ms
step:130/1770 train_time:11301ms step_avg:94.17ms
step:131/1770 train_time:11394ms step_avg:94.17ms
step:132/1770 train_time:11488ms step_avg:94.16ms
step:133/1770 train_time:11581ms step_avg:94.16ms
step:134/1770 train_time:11676ms step_avg:94.16ms
step:135/1770 train_time:11770ms step_avg:94.16ms
step:136/1770 train_time:11863ms step_avg:94.15ms
step:137/1770 train_time:11957ms step_avg:94.15ms
step:138/1770 train_time:12053ms step_avg:94.16ms
step:139/1770 train_time:12148ms step_avg:94.17ms
step:140/1770 train_time:12243ms step_avg:94.18ms
step:141/1770 train_time:12338ms step_avg:94.18ms
step:142/1770 train_time:12433ms step_avg:94.19ms
step:143/1770 train_time:12527ms step_avg:94.19ms
step:144/1770 train_time:12622ms step_avg:94.19ms
step:145/1770 train_time:12716ms step_avg:94.19ms
step:146/1770 train_time:12811ms step_avg:94.20ms
step:147/1770 train_time:12905ms step_avg:94.20ms
step:148/1770 train_time:12999ms step_avg:94.20ms
step:149/1770 train_time:13094ms step_avg:94.20ms
step:150/1770 train_time:13189ms step_avg:94.21ms
step:151/1770 train_time:13283ms step_avg:94.21ms
step:152/1770 train_time:13378ms step_avg:94.21ms
step:153/1770 train_time:13473ms step_avg:94.22ms
step:154/1770 train_time:13568ms step_avg:94.22ms
step:155/1770 train_time:13662ms step_avg:94.22ms
step:156/1770 train_time:13756ms step_avg:94.22ms
step:157/1770 train_time:13851ms step_avg:94.23ms
step:158/1770 train_time:13946ms step_avg:94.23ms
step:159/1770 train_time:14040ms step_avg:94.23ms
step:160/1770 train_time:14135ms step_avg:94.23ms
step:161/1770 train_time:14230ms step_avg:94.24ms
step:162/1770 train_time:14324ms step_avg:94.24ms
step:163/1770 train_time:14420ms step_avg:94.25ms
step:164/1770 train_time:14514ms step_avg:94.24ms
step:165/1770 train_time:14608ms step_avg:94.25ms
step:166/1770 train_time:14702ms step_avg:94.25ms
step:167/1770 train_time:14797ms step_avg:94.25ms
step:168/1770 train_time:14892ms step_avg:94.25ms
step:169/1770 train_time:14986ms step_avg:94.25ms
step:170/1770 train_time:15080ms step_avg:94.25ms
step:171/1770 train_time:15175ms step_avg:94.26ms
step:172/1770 train_time:15270ms step_avg:94.26ms
step:173/1770 train_time:15364ms step_avg:94.26ms
step:174/1770 train_time:15459ms step_avg:94.26ms
step:175/1770 train_time:15554ms step_avg:94.26ms
step:176/1770 train_time:15649ms step_avg:94.27ms
step:177/1770 train_time:15743ms step_avg:94.27ms
step:178/1770 train_time:15837ms step_avg:94.27ms
step:179/1770 train_time:15932ms step_avg:94.27ms
step:180/1770 train_time:16027ms step_avg:94.28ms
step:181/1770 train_time:16121ms step_avg:94.27ms
step:182/1770 train_time:16215ms step_avg:94.27ms
step:183/1770 train_time:16310ms step_avg:94.28ms
step:184/1770 train_time:16404ms step_avg:94.28ms
step:185/1770 train_time:16498ms step_avg:94.28ms
step:186/1770 train_time:16594ms step_avg:94.28ms
step:187/1770 train_time:16688ms step_avg:94.28ms
step:188/1770 train_time:16783ms step_avg:94.28ms
step:189/1770 train_time:16877ms step_avg:94.29ms
step:190/1770 train_time:16972ms step_avg:94.29ms
step:191/1770 train_time:17067ms step_avg:94.29ms
step:192/1770 train_time:17161ms step_avg:94.29ms
step:193/1770 train_time:17256ms step_avg:94.30ms
step:194/1770 train_time:17350ms step_avg:94.30ms
step:195/1770 train_time:17445ms step_avg:94.30ms
step:196/1770 train_time:17540ms step_avg:94.30ms
step:197/1770 train_time:17635ms step_avg:94.30ms
step:198/1770 train_time:17729ms step_avg:94.30ms
step:199/1770 train_time:17824ms step_avg:94.31ms
step:200/1770 train_time:17918ms step_avg:94.30ms
step:201/1770 train_time:18013ms step_avg:94.31ms
step:202/1770 train_time:18107ms step_avg:94.31ms
step:203/1770 train_time:18201ms step_avg:94.30ms
step:204/1770 train_time:18296ms step_avg:94.31ms
step:205/1770 train_time:18391ms step_avg:94.31ms
step:206/1770 train_time:18485ms step_avg:94.31ms
step:207/1770 train_time:18579ms step_avg:94.31ms
step:208/1770 train_time:18673ms step_avg:94.31ms
step:209/1770 train_time:18769ms step_avg:94.31ms
step:210/1770 train_time:18863ms step_avg:94.31ms
step:211/1770 train_time:18957ms step_avg:94.31ms
step:212/1770 train_time:19051ms step_avg:94.31ms
step:213/1770 train_time:19146ms step_avg:94.32ms
step:214/1770 train_time:19240ms step_avg:94.31ms
step:215/1770 train_time:19335ms step_avg:94.32ms
step:216/1770 train_time:19430ms step_avg:94.32ms
step:217/1770 train_time:19524ms step_avg:94.32ms
step:218/1770 train_time:19618ms step_avg:94.32ms
step:219/1770 train_time:19713ms step_avg:94.32ms
step:220/1770 train_time:19808ms step_avg:94.32ms
step:221/1770 train_time:19902ms step_avg:94.32ms
step:222/1770 train_time:19996ms step_avg:94.32ms
step:223/1770 train_time:20091ms step_avg:94.32ms
step:224/1770 train_time:20186ms step_avg:94.32ms
step:225/1770 train_time:20279ms step_avg:94.32ms
step:226/1770 train_time:20374ms step_avg:94.32ms
step:227/1770 train_time:20469ms step_avg:94.33ms
step:228/1770 train_time:20563ms step_avg:94.33ms
step:229/1770 train_time:20657ms step_avg:94.33ms
step:230/1770 train_time:20752ms step_avg:94.33ms
step:231/1770 train_time:20848ms step_avg:94.33ms
step:232/1770 train_time:20942ms step_avg:94.33ms
step:233/1770 train_time:21036ms step_avg:94.33ms
step:234/1770 train_time:21131ms step_avg:94.34ms
step:235/1770 train_time:21226ms step_avg:94.34ms
step:236/1770 train_time:21319ms step_avg:94.33ms
step:237/1770 train_time:21414ms step_avg:94.34ms
step:238/1770 train_time:21510ms step_avg:94.34ms
step:239/1770 train_time:21604ms step_avg:94.34ms
step:240/1770 train_time:21699ms step_avg:94.34ms
step:241/1770 train_time:21794ms step_avg:94.35ms
step:242/1770 train_time:21889ms step_avg:94.35ms
step:243/1770 train_time:21984ms step_avg:94.35ms
step:244/1770 train_time:22078ms step_avg:94.35ms
step:245/1770 train_time:22173ms step_avg:94.35ms
step:246/1770 train_time:22268ms step_avg:94.35ms
step:247/1770 train_time:22362ms step_avg:94.35ms
step:248/1770 train_time:22456ms step_avg:94.35ms
step:249/1770 train_time:22551ms step_avg:94.36ms
step:250/1770 train_time:22646ms step_avg:94.36ms
step:250/1770 val_loss:4.1110 train_time:22738ms step_avg:94.74ms
step:251/1770 train_time:22760ms step_avg:94.44ms
step:252/1770 train_time:22847ms step_avg:94.41ms
step:253/1770 train_time:22946ms step_avg:94.43ms
step:254/1770 train_time:23041ms step_avg:94.43ms
step:255/1770 train_time:23136ms step_avg:94.43ms
step:256/1770 train_time:23230ms step_avg:94.43ms
step:257/1770 train_time:23325ms step_avg:94.43ms
step:258/1770 train_time:23419ms step_avg:94.43ms
step:259/1770 train_time:23513ms step_avg:94.43ms
step:260/1770 train_time:23607ms step_avg:94.43ms
step:261/1770 train_time:23702ms step_avg:94.43ms
step:262/1770 train_time:23796ms step_avg:94.43ms
step:263/1770 train_time:23892ms step_avg:94.43ms
step:264/1770 train_time:23988ms step_avg:94.44ms
step:265/1770 train_time:24084ms step_avg:94.45ms
step:266/1770 train_time:24178ms step_avg:94.44ms
step:267/1770 train_time:24273ms step_avg:94.45ms
step:268/1770 train_time:24368ms step_avg:94.45ms
step:269/1770 train_time:24462ms step_avg:94.45ms
step:270/1770 train_time:24557ms step_avg:94.45ms
step:271/1770 train_time:24652ms step_avg:94.45ms
step:272/1770 train_time:24747ms step_avg:94.46ms
step:273/1770 train_time:24842ms step_avg:94.46ms
step:274/1770 train_time:24936ms step_avg:94.46ms
step:275/1770 train_time:25032ms step_avg:94.46ms
step:276/1770 train_time:25128ms step_avg:94.46ms
step:277/1770 train_time:25222ms step_avg:94.47ms
step:278/1770 train_time:25317ms step_avg:94.47ms
step:279/1770 train_time:25413ms step_avg:94.47ms
step:280/1770 train_time:25508ms step_avg:94.47ms
step:281/1770 train_time:25602ms step_avg:94.47ms
step:282/1770 train_time:25697ms step_avg:94.47ms
step:283/1770 train_time:25792ms step_avg:94.47ms
step:284/1770 train_time:25887ms step_avg:94.48ms
step:285/1770 train_time:25981ms step_avg:94.48ms
step:286/1770 train_time:26076ms step_avg:94.48ms
step:287/1770 train_time:26172ms step_avg:94.48ms
step:288/1770 train_time:26267ms step_avg:94.49ms
step:289/1770 train_time:26362ms step_avg:94.49ms
step:290/1770 train_time:26456ms step_avg:94.49ms
step:291/1770 train_time:26552ms step_avg:94.49ms
step:292/1770 train_time:26647ms step_avg:94.49ms
step:293/1770 train_time:26741ms step_avg:94.49ms
step:294/1770 train_time:26836ms step_avg:94.49ms
step:295/1770 train_time:26932ms step_avg:94.50ms
step:296/1770 train_time:27027ms step_avg:94.50ms
step:297/1770 train_time:27121ms step_avg:94.50ms
step:298/1770 train_time:27216ms step_avg:94.50ms
step:299/1770 train_time:27312ms step_avg:94.50ms
step:300/1770 train_time:27407ms step_avg:94.51ms
step:301/1770 train_time:27502ms step_avg:94.51ms
step:302/1770 train_time:27597ms step_avg:94.51ms
step:303/1770 train_time:27692ms step_avg:94.51ms
step:304/1770 train_time:27788ms step_avg:94.52ms
step:305/1770 train_time:27884ms step_avg:94.52ms
step:306/1770 train_time:27978ms step_avg:94.52ms
step:307/1770 train_time:28074ms step_avg:94.52ms
step:308/1770 train_time:28169ms step_avg:94.53ms
step:309/1770 train_time:28264ms step_avg:94.53ms
step:310/1770 train_time:28358ms step_avg:94.53ms
step:311/1770 train_time:28453ms step_avg:94.53ms
step:312/1770 train_time:28549ms step_avg:94.53ms
step:313/1770 train_time:28644ms step_avg:94.53ms
step:314/1770 train_time:28738ms step_avg:94.53ms
step:315/1770 train_time:28834ms step_avg:94.54ms
step:316/1770 train_time:28930ms step_avg:94.54ms
step:317/1770 train_time:29025ms step_avg:94.54ms
step:318/1770 train_time:29120ms step_avg:94.54ms
step:319/1770 train_time:29215ms step_avg:94.55ms
step:320/1770 train_time:29310ms step_avg:94.55ms
step:321/1770 train_time:29405ms step_avg:94.55ms
step:322/1770 train_time:29500ms step_avg:94.55ms
step:323/1770 train_time:29595ms step_avg:94.55ms
step:324/1770 train_time:29690ms step_avg:94.55ms
step:325/1770 train_time:29785ms step_avg:94.56ms
step:326/1770 train_time:29880ms step_avg:94.56ms
step:327/1770 train_time:29975ms step_avg:94.56ms
step:328/1770 train_time:30071ms step_avg:94.56ms
step:329/1770 train_time:30167ms step_avg:94.57ms
step:330/1770 train_time:30262ms step_avg:94.57ms
step:331/1770 train_time:30356ms step_avg:94.57ms
step:332/1770 train_time:30452ms step_avg:94.57ms
step:333/1770 train_time:30548ms step_avg:94.58ms
step:334/1770 train_time:30643ms step_avg:94.58ms
step:335/1770 train_time:30738ms step_avg:94.58ms
step:336/1770 train_time:30833ms step_avg:94.58ms
step:337/1770 train_time:30929ms step_avg:94.58ms
step:338/1770 train_time:31024ms step_avg:94.58ms
step:339/1770 train_time:31119ms step_avg:94.59ms
step:340/1770 train_time:31214ms step_avg:94.59ms
step:341/1770 train_time:31309ms step_avg:94.59ms
step:342/1770 train_time:31404ms step_avg:94.59ms
step:343/1770 train_time:31499ms step_avg:94.59ms
step:344/1770 train_time:31594ms step_avg:94.59ms
step:345/1770 train_time:31689ms step_avg:94.59ms
step:346/1770 train_time:31784ms step_avg:94.60ms
step:347/1770 train_time:31878ms step_avg:94.59ms
step:348/1770 train_time:31973ms step_avg:94.60ms
step:349/1770 train_time:32069ms step_avg:94.60ms
step:350/1770 train_time:32164ms step_avg:94.60ms
step:351/1770 train_time:32258ms step_avg:94.60ms
step:352/1770 train_time:32353ms step_avg:94.60ms
step:353/1770 train_time:32449ms step_avg:94.60ms
step:354/1770 train_time:32544ms step_avg:94.60ms
step:355/1770 train_time:32638ms step_avg:94.60ms
step:356/1770 train_time:32733ms step_avg:94.60ms
step:357/1770 train_time:32829ms step_avg:94.61ms
step:358/1770 train_time:32924ms step_avg:94.61ms
step:359/1770 train_time:33018ms step_avg:94.61ms
step:360/1770 train_time:33114ms step_avg:94.61ms
step:361/1770 train_time:33210ms step_avg:94.61ms
step:362/1770 train_time:33305ms step_avg:94.62ms
step:363/1770 train_time:33400ms step_avg:94.62ms
step:364/1770 train_time:33495ms step_avg:94.62ms
step:365/1770 train_time:33590ms step_avg:94.62ms
step:366/1770 train_time:33685ms step_avg:94.62ms
step:367/1770 train_time:33779ms step_avg:94.62ms
step:368/1770 train_time:33874ms step_avg:94.62ms
step:369/1770 train_time:33970ms step_avg:94.62ms
step:370/1770 train_time:34065ms step_avg:94.62ms
step:371/1770 train_time:34159ms step_avg:94.62ms
step:372/1770 train_time:34254ms step_avg:94.62ms
step:373/1770 train_time:34349ms step_avg:94.63ms
step:374/1770 train_time:34444ms step_avg:94.63ms
step:375/1770 train_time:34538ms step_avg:94.63ms
step:375/1770 val_loss:3.9091 train_time:34632ms step_avg:94.88ms
step:376/1770 train_time:34654ms step_avg:94.68ms
step:377/1770 train_time:34736ms step_avg:94.65ms
step:378/1770 train_time:34834ms step_avg:94.66ms
step:379/1770 train_time:34929ms step_avg:94.66ms
step:380/1770 train_time:35024ms step_avg:94.66ms
step:381/1770 train_time:35119ms step_avg:94.66ms
step:382/1770 train_time:35213ms step_avg:94.66ms
step:383/1770 train_time:35308ms step_avg:94.66ms
step:384/1770 train_time:35403ms step_avg:94.66ms
step:385/1770 train_time:35497ms step_avg:94.66ms
step:386/1770 train_time:35592ms step_avg:94.66ms
step:387/1770 train_time:35687ms step_avg:94.66ms
step:388/1770 train_time:35783ms step_avg:94.66ms
step:389/1770 train_time:35878ms step_avg:94.67ms
step:390/1770 train_time:35973ms step_avg:94.67ms
step:391/1770 train_time:36069ms step_avg:94.67ms
step:392/1770 train_time:36164ms step_avg:94.67ms
step:393/1770 train_time:36259ms step_avg:94.67ms
step:394/1770 train_time:36353ms step_avg:94.67ms
step:395/1770 train_time:36448ms step_avg:94.67ms
step:396/1770 train_time:36544ms step_avg:94.67ms
step:397/1770 train_time:36641ms step_avg:94.68ms
step:398/1770 train_time:36737ms step_avg:94.68ms
step:399/1770 train_time:36834ms step_avg:94.69ms
step:400/1770 train_time:36932ms step_avg:94.70ms
step:401/1770 train_time:37029ms step_avg:94.70ms
step:402/1770 train_time:37127ms step_avg:94.71ms
step:403/1770 train_time:37224ms step_avg:94.72ms
step:404/1770 train_time:37322ms step_avg:94.72ms
step:405/1770 train_time:37418ms step_avg:94.73ms
step:406/1770 train_time:37515ms step_avg:94.73ms
step:407/1770 train_time:37612ms step_avg:94.74ms
step:408/1770 train_time:37709ms step_avg:94.75ms
step:409/1770 train_time:37807ms step_avg:94.75ms
step:410/1770 train_time:37904ms step_avg:94.76ms
step:411/1770 train_time:38001ms step_avg:94.76ms
step:412/1770 train_time:38097ms step_avg:94.77ms
step:413/1770 train_time:38194ms step_avg:94.77ms
step:414/1770 train_time:38292ms step_avg:94.78ms
step:415/1770 train_time:38389ms step_avg:94.79ms
step:416/1770 train_time:38487ms step_avg:94.80ms
step:417/1770 train_time:38584ms step_avg:94.80ms
step:418/1770 train_time:38680ms step_avg:94.80ms
step:419/1770 train_time:38777ms step_avg:94.81ms
step:420/1770 train_time:38874ms step_avg:94.81ms
step:421/1770 train_time:38971ms step_avg:94.82ms
step:422/1770 train_time:39068ms step_avg:94.83ms
step:423/1770 train_time:39166ms step_avg:94.83ms
step:424/1770 train_time:39263ms step_avg:94.84ms
step:425/1770 train_time:39360ms step_avg:94.84ms
step:426/1770 train_time:39457ms step_avg:94.85ms
step:427/1770 train_time:39554ms step_avg:94.85ms
step:428/1770 train_time:39652ms step_avg:94.86ms
step:429/1770 train_time:39749ms step_avg:94.87ms
step:430/1770 train_time:39846ms step_avg:94.87ms
step:431/1770 train_time:39943ms step_avg:94.88ms
step:432/1770 train_time:40040ms step_avg:94.88ms
step:433/1770 train_time:40137ms step_avg:94.89ms
step:434/1770 train_time:40234ms step_avg:94.89ms
step:435/1770 train_time:40331ms step_avg:94.90ms
step:436/1770 train_time:40428ms step_avg:94.90ms
step:437/1770 train_time:40526ms step_avg:94.91ms
step:438/1770 train_time:40622ms step_avg:94.91ms
step:439/1770 train_time:40719ms step_avg:94.92ms
step:440/1770 train_time:40815ms step_avg:94.92ms
step:441/1770 train_time:40912ms step_avg:94.92ms
step:442/1770 train_time:41009ms step_avg:94.93ms
step:443/1770 train_time:41107ms step_avg:94.93ms
step:444/1770 train_time:41204ms step_avg:94.94ms
step:445/1770 train_time:41301ms step_avg:94.94ms
step:446/1770 train_time:41397ms step_avg:94.95ms
step:447/1770 train_time:41494ms step_avg:94.95ms
step:448/1770 train_time:41592ms step_avg:94.96ms
step:449/1770 train_time:41689ms step_avg:94.96ms
step:450/1770 train_time:41787ms step_avg:94.97ms
step:451/1770 train_time:41883ms step_avg:94.97ms
step:452/1770 train_time:41980ms step_avg:94.98ms
step:453/1770 train_time:42076ms step_avg:94.98ms
step:454/1770 train_time:42173ms step_avg:94.99ms
step:455/1770 train_time:42271ms step_avg:94.99ms
step:456/1770 train_time:42369ms step_avg:95.00ms
step:457/1770 train_time:42466ms step_avg:95.00ms
step:458/1770 train_time:42563ms step_avg:95.01ms
step:459/1770 train_time:42659ms step_avg:95.01ms
step:460/1770 train_time:42756ms step_avg:95.01ms
step:461/1770 train_time:42852ms step_avg:95.02ms
step:462/1770 train_time:42949ms step_avg:95.02ms
step:463/1770 train_time:43047ms step_avg:95.03ms
step:464/1770 train_time:43144ms step_avg:95.03ms
step:465/1770 train_time:43240ms step_avg:95.03ms
step:466/1770 train_time:43337ms step_avg:95.04ms
step:467/1770 train_time:43434ms step_avg:95.04ms
step:468/1770 train_time:43531ms step_avg:95.05ms
step:469/1770 train_time:43629ms step_avg:95.05ms
step:470/1770 train_time:43726ms step_avg:95.06ms
step:471/1770 train_time:43824ms step_avg:95.06ms
step:472/1770 train_time:43920ms step_avg:95.07ms
step:473/1770 train_time:44017ms step_avg:95.07ms
step:474/1770 train_time:44114ms step_avg:95.07ms
step:475/1770 train_time:44210ms step_avg:95.08ms
step:476/1770 train_time:44307ms step_avg:95.08ms
step:477/1770 train_time:44404ms step_avg:95.08ms
step:478/1770 train_time:44501ms step_avg:95.09ms
step:479/1770 train_time:44597ms step_avg:95.09ms
step:480/1770 train_time:44694ms step_avg:95.09ms
step:481/1770 train_time:44791ms step_avg:95.10ms
step:482/1770 train_time:44889ms step_avg:95.10ms
step:483/1770 train_time:44986ms step_avg:95.11ms
step:484/1770 train_time:45084ms step_avg:95.11ms
step:485/1770 train_time:45181ms step_avg:95.12ms
step:486/1770 train_time:45277ms step_avg:95.12ms
step:487/1770 train_time:45374ms step_avg:95.12ms
step:488/1770 train_time:45471ms step_avg:95.13ms
step:489/1770 train_time:45569ms step_avg:95.13ms
step:490/1770 train_time:45666ms step_avg:95.14ms
step:491/1770 train_time:45763ms step_avg:95.14ms
step:492/1770 train_time:45859ms step_avg:95.14ms
step:493/1770 train_time:45956ms step_avg:95.15ms
step:494/1770 train_time:46053ms step_avg:95.15ms
step:495/1770 train_time:46150ms step_avg:95.15ms
step:496/1770 train_time:46248ms step_avg:95.16ms
step:497/1770 train_time:46346ms step_avg:95.17ms
step:498/1770 train_time:46442ms step_avg:95.17ms
step:499/1770 train_time:46539ms step_avg:95.17ms
step:500/1770 train_time:46635ms step_avg:95.17ms
step:500/1770 val_loss:3.7561 train_time:46730ms step_avg:95.37ms
step:501/1770 train_time:46754ms step_avg:95.22ms
step:502/1770 train_time:46837ms step_avg:95.20ms
step:503/1770 train_time:46936ms step_avg:95.21ms
step:504/1770 train_time:47034ms step_avg:95.21ms
step:505/1770 train_time:47131ms step_avg:95.22ms
step:506/1770 train_time:47229ms step_avg:95.22ms
step:507/1770 train_time:47325ms step_avg:95.22ms
step:508/1770 train_time:47422ms step_avg:95.22ms
step:509/1770 train_time:47518ms step_avg:95.23ms
step:510/1770 train_time:47614ms step_avg:95.23ms
step:511/1770 train_time:47711ms step_avg:95.23ms
step:512/1770 train_time:47809ms step_avg:95.24ms
step:513/1770 train_time:47907ms step_avg:95.24ms
step:514/1770 train_time:48005ms step_avg:95.25ms
step:515/1770 train_time:48102ms step_avg:95.25ms
step:516/1770 train_time:48199ms step_avg:95.25ms
step:517/1770 train_time:48296ms step_avg:95.26ms
step:518/1770 train_time:48393ms step_avg:95.26ms
step:519/1770 train_time:48490ms step_avg:95.27ms
step:520/1770 train_time:48588ms step_avg:95.27ms
step:521/1770 train_time:48685ms step_avg:95.27ms
step:522/1770 train_time:48782ms step_avg:95.28ms
step:523/1770 train_time:48878ms step_avg:95.28ms
step:524/1770 train_time:48976ms step_avg:95.28ms
step:525/1770 train_time:49074ms step_avg:95.29ms
step:526/1770 train_time:49172ms step_avg:95.29ms
step:527/1770 train_time:49269ms step_avg:95.30ms
step:528/1770 train_time:49367ms step_avg:95.30ms
step:529/1770 train_time:49464ms step_avg:95.31ms
step:530/1770 train_time:49561ms step_avg:95.31ms
step:531/1770 train_time:49658ms step_avg:95.31ms
step:532/1770 train_time:49755ms step_avg:95.32ms
step:533/1770 train_time:49853ms step_avg:95.32ms
step:534/1770 train_time:49950ms step_avg:95.33ms
step:535/1770 train_time:50048ms step_avg:95.33ms
step:536/1770 train_time:50145ms step_avg:95.33ms
step:537/1770 train_time:50242ms step_avg:95.34ms
step:538/1770 train_time:50339ms step_avg:95.34ms
step:539/1770 train_time:50436ms step_avg:95.34ms
step:540/1770 train_time:50533ms step_avg:95.35ms
step:541/1770 train_time:50631ms step_avg:95.35ms
step:542/1770 train_time:50729ms step_avg:95.35ms
step:543/1770 train_time:50826ms step_avg:95.36ms
step:544/1770 train_time:50924ms step_avg:95.36ms
step:545/1770 train_time:51021ms step_avg:95.37ms
step:546/1770 train_time:51117ms step_avg:95.37ms
step:547/1770 train_time:51215ms step_avg:95.37ms
step:548/1770 train_time:51312ms step_avg:95.38ms
step:549/1770 train_time:51410ms step_avg:95.38ms
step:550/1770 train_time:51508ms step_avg:95.38ms
step:551/1770 train_time:51605ms step_avg:95.39ms
step:552/1770 train_time:51701ms step_avg:95.39ms
step:553/1770 train_time:51798ms step_avg:95.39ms
step:554/1770 train_time:51895ms step_avg:95.40ms
step:555/1770 train_time:51993ms step_avg:95.40ms
step:556/1770 train_time:52090ms step_avg:95.40ms
step:557/1770 train_time:52188ms step_avg:95.41ms
step:558/1770 train_time:52286ms step_avg:95.41ms
step:559/1770 train_time:52383ms step_avg:95.41ms
step:560/1770 train_time:52480ms step_avg:95.42ms
step:561/1770 train_time:52577ms step_avg:95.42ms
step:562/1770 train_time:52674ms step_avg:95.42ms
step:563/1770 train_time:52773ms step_avg:95.43ms
step:564/1770 train_time:52871ms step_avg:95.43ms
step:565/1770 train_time:52968ms step_avg:95.44ms
step:566/1770 train_time:53066ms step_avg:95.44ms
step:567/1770 train_time:53163ms step_avg:95.45ms
step:568/1770 train_time:53261ms step_avg:95.45ms
step:569/1770 train_time:53357ms step_avg:95.45ms
step:570/1770 train_time:53454ms step_avg:95.45ms
step:571/1770 train_time:53552ms step_avg:95.46ms
step:572/1770 train_time:53649ms step_avg:95.46ms
step:573/1770 train_time:53747ms step_avg:95.47ms
step:574/1770 train_time:53844ms step_avg:95.47ms
step:575/1770 train_time:53941ms step_avg:95.47ms
step:576/1770 train_time:54038ms step_avg:95.47ms
step:577/1770 train_time:54135ms step_avg:95.48ms
step:578/1770 train_time:54233ms step_avg:95.48ms
step:579/1770 train_time:54330ms step_avg:95.48ms
step:580/1770 train_time:54428ms step_avg:95.49ms
step:581/1770 train_time:54526ms step_avg:95.49ms
step:582/1770 train_time:54623ms step_avg:95.49ms
step:583/1770 train_time:54720ms step_avg:95.50ms
step:584/1770 train_time:54817ms step_avg:95.50ms
step:585/1770 train_time:54915ms step_avg:95.50ms
step:586/1770 train_time:55013ms step_avg:95.51ms
step:587/1770 train_time:55111ms step_avg:95.51ms
step:588/1770 train_time:55209ms step_avg:95.52ms
step:589/1770 train_time:55306ms step_avg:95.52ms
step:590/1770 train_time:55403ms step_avg:95.52ms
step:591/1770 train_time:55500ms step_avg:95.53ms
step:592/1770 train_time:55597ms step_avg:95.53ms
step:593/1770 train_time:55694ms step_avg:95.53ms
step:594/1770 train_time:55792ms step_avg:95.53ms
step:595/1770 train_time:55890ms step_avg:95.54ms
step:596/1770 train_time:55988ms step_avg:95.54ms
step:597/1770 train_time:56086ms step_avg:95.55ms
step:598/1770 train_time:56183ms step_avg:95.55ms
step:599/1770 train_time:56280ms step_avg:95.55ms
step:600/1770 train_time:56377ms step_avg:95.55ms
step:601/1770 train_time:56474ms step_avg:95.56ms
step:602/1770 train_time:56572ms step_avg:95.56ms
step:603/1770 train_time:56670ms step_avg:95.56ms
step:604/1770 train_time:56767ms step_avg:95.57ms
step:605/1770 train_time:56864ms step_avg:95.57ms
step:606/1770 train_time:56961ms step_avg:95.57ms
step:607/1770 train_time:57058ms step_avg:95.58ms
step:608/1770 train_time:57155ms step_avg:95.58ms
step:609/1770 train_time:57253ms step_avg:95.58ms
step:610/1770 train_time:57351ms step_avg:95.59ms
step:611/1770 train_time:57449ms step_avg:95.59ms
step:612/1770 train_time:57546ms step_avg:95.59ms
step:613/1770 train_time:57644ms step_avg:95.60ms
step:614/1770 train_time:57741ms step_avg:95.60ms
step:615/1770 train_time:57837ms step_avg:95.60ms
step:616/1770 train_time:57934ms step_avg:95.60ms
step:617/1770 train_time:58031ms step_avg:95.60ms
step:618/1770 train_time:58129ms step_avg:95.61ms
step:619/1770 train_time:58227ms step_avg:95.61ms
step:620/1770 train_time:58324ms step_avg:95.61ms
step:621/1770 train_time:58422ms step_avg:95.62ms
step:622/1770 train_time:58519ms step_avg:95.62ms
step:623/1770 train_time:58615ms step_avg:95.62ms
step:624/1770 train_time:58713ms step_avg:95.62ms
step:625/1770 train_time:58811ms step_avg:95.63ms
step:625/1770 val_loss:3.6681 train_time:58907ms step_avg:95.78ms
step:626/1770 train_time:58929ms step_avg:95.66ms
step:627/1770 train_time:59016ms step_avg:95.65ms
step:628/1770 train_time:59116ms step_avg:95.66ms
step:629/1770 train_time:59213ms step_avg:95.66ms
step:630/1770 train_time:59310ms step_avg:95.66ms
step:631/1770 train_time:59407ms step_avg:95.66ms
step:632/1770 train_time:59504ms step_avg:95.67ms
step:633/1770 train_time:59601ms step_avg:95.67ms
step:634/1770 train_time:59698ms step_avg:95.67ms
step:635/1770 train_time:59795ms step_avg:95.67ms
step:636/1770 train_time:59892ms step_avg:95.67ms
step:637/1770 train_time:59990ms step_avg:95.68ms
step:638/1770 train_time:60089ms step_avg:95.68ms
step:639/1770 train_time:60187ms step_avg:95.69ms
step:640/1770 train_time:60285ms step_avg:95.69ms
step:641/1770 train_time:60383ms step_avg:95.69ms
step:642/1770 train_time:60479ms step_avg:95.69ms
step:643/1770 train_time:60576ms step_avg:95.70ms
step:644/1770 train_time:60673ms step_avg:95.70ms
step:645/1770 train_time:60770ms step_avg:95.70ms
step:646/1770 train_time:60867ms step_avg:95.70ms
step:647/1770 train_time:60965ms step_avg:95.71ms
step:648/1770 train_time:61063ms step_avg:95.71ms
step:649/1770 train_time:61161ms step_avg:95.71ms
step:650/1770 train_time:61258ms step_avg:95.72ms
step:651/1770 train_time:61355ms step_avg:95.72ms
step:652/1770 train_time:61452ms step_avg:95.72ms
step:653/1770 train_time:61550ms step_avg:95.72ms
step:654/1770 train_time:61648ms step_avg:95.73ms
step:655/1770 train_time:61745ms step_avg:95.73ms
step:656/1770 train_time:61842ms step_avg:95.73ms
step:657/1770 train_time:61939ms step_avg:95.73ms
step:658/1770 train_time:62038ms step_avg:95.74ms
step:659/1770 train_time:62137ms step_avg:95.74ms
step:660/1770 train_time:62236ms step_avg:95.75ms
step:661/1770 train_time:62334ms step_avg:95.75ms
step:662/1770 train_time:62434ms step_avg:95.76ms
step:663/1770 train_time:62532ms step_avg:95.76ms
step:664/1770 train_time:62631ms step_avg:95.77ms
step:665/1770 train_time:62730ms step_avg:95.77ms
step:666/1770 train_time:62829ms step_avg:95.78ms
step:667/1770 train_time:62929ms step_avg:95.78ms
step:668/1770 train_time:63029ms step_avg:95.79ms
step:669/1770 train_time:63129ms step_avg:95.80ms
step:670/1770 train_time:63229ms step_avg:95.80ms
step:671/1770 train_time:63329ms step_avg:95.81ms
step:672/1770 train_time:63429ms step_avg:95.81ms
step:673/1770 train_time:63531ms step_avg:95.82ms
step:674/1770 train_time:63630ms step_avg:95.83ms
step:675/1770 train_time:63731ms step_avg:95.84ms
step:676/1770 train_time:63830ms step_avg:95.84ms
step:677/1770 train_time:63929ms step_avg:95.85ms
step:678/1770 train_time:64028ms step_avg:95.85ms
step:679/1770 train_time:64129ms step_avg:95.86ms
step:680/1770 train_time:64229ms step_avg:95.86ms
step:681/1770 train_time:64328ms step_avg:95.87ms
step:682/1770 train_time:64428ms step_avg:95.88ms
step:683/1770 train_time:64528ms step_avg:95.88ms
step:684/1770 train_time:64628ms step_avg:95.89ms
step:685/1770 train_time:64728ms step_avg:95.89ms
step:686/1770 train_time:64828ms step_avg:95.90ms
step:687/1770 train_time:64927ms step_avg:95.90ms
step:688/1770 train_time:65027ms step_avg:95.91ms
step:689/1770 train_time:65126ms step_avg:95.92ms
step:690/1770 train_time:65226ms step_avg:95.92ms
step:691/1770 train_time:65326ms step_avg:95.93ms
step:692/1770 train_time:65426ms step_avg:95.93ms
step:693/1770 train_time:65525ms step_avg:95.94ms
step:694/1770 train_time:65624ms step_avg:95.94ms
step:695/1770 train_time:65723ms step_avg:95.95ms
step:696/1770 train_time:65822ms step_avg:95.95ms
step:697/1770 train_time:65921ms step_avg:95.96ms
step:698/1770 train_time:66020ms step_avg:95.96ms
step:699/1770 train_time:66119ms step_avg:95.96ms
step:700/1770 train_time:66217ms step_avg:95.97ms
step:701/1770 train_time:66316ms step_avg:95.97ms
step:702/1770 train_time:66414ms step_avg:95.97ms
step:703/1770 train_time:66513ms step_avg:95.98ms
step:704/1770 train_time:66612ms step_avg:95.98ms
step:705/1770 train_time:66711ms step_avg:95.99ms
step:706/1770 train_time:66809ms step_avg:95.99ms
step:707/1770 train_time:66909ms step_avg:96.00ms
step:708/1770 train_time:67008ms step_avg:96.00ms
step:709/1770 train_time:67109ms step_avg:96.01ms
step:710/1770 train_time:67210ms step_avg:96.01ms
step:711/1770 train_time:67310ms step_avg:96.02ms
step:712/1770 train_time:67412ms step_avg:96.03ms
step:713/1770 train_time:67511ms step_avg:96.03ms
step:714/1770 train_time:67610ms step_avg:96.04ms
step:715/1770 train_time:67709ms step_avg:96.04ms
step:716/1770 train_time:67807ms step_avg:96.04ms
step:717/1770 train_time:67907ms step_avg:96.05ms
step:718/1770 train_time:68006ms step_avg:96.05ms
step:719/1770 train_time:68105ms step_avg:96.06ms
step:720/1770 train_time:68205ms step_avg:96.06ms
step:721/1770 train_time:68306ms step_avg:96.07ms
step:722/1770 train_time:68405ms step_avg:96.07ms
step:723/1770 train_time:68505ms step_avg:96.08ms
step:724/1770 train_time:68605ms step_avg:96.09ms
step:725/1770 train_time:68705ms step_avg:96.09ms
step:726/1770 train_time:68804ms step_avg:96.09ms
step:727/1770 train_time:68903ms step_avg:96.10ms
step:728/1770 train_time:69002ms step_avg:96.10ms
step:729/1770 train_time:69100ms step_avg:96.11ms
step:730/1770 train_time:69199ms step_avg:96.11ms
step:731/1770 train_time:69298ms step_avg:96.11ms
step:732/1770 train_time:69396ms step_avg:96.12ms
step:733/1770 train_time:69495ms step_avg:96.12ms
step:734/1770 train_time:69594ms step_avg:96.12ms
step:735/1770 train_time:69692ms step_avg:96.13ms
step:736/1770 train_time:69791ms step_avg:96.13ms
step:737/1770 train_time:69891ms step_avg:96.14ms
step:738/1770 train_time:69990ms step_avg:96.14ms
step:739/1770 train_time:70089ms step_avg:96.14ms
step:740/1770 train_time:70188ms step_avg:96.15ms
step:741/1770 train_time:70288ms step_avg:96.15ms
step:742/1770 train_time:70388ms step_avg:96.16ms
step:743/1770 train_time:70487ms step_avg:96.16ms
step:744/1770 train_time:70586ms step_avg:96.17ms
step:745/1770 train_time:70686ms step_avg:96.17ms
step:746/1770 train_time:70785ms step_avg:96.17ms
step:747/1770 train_time:70885ms step_avg:96.18ms
step:748/1770 train_time:70985ms step_avg:96.19ms
step:749/1770 train_time:71084ms step_avg:96.19ms
step:750/1770 train_time:71183ms step_avg:96.19ms
step:750/1770 val_loss:3.6043 train_time:71280ms step_avg:96.32ms
step:751/1770 train_time:71302ms step_avg:96.22ms
step:752/1770 train_time:71391ms step_avg:96.21ms
step:753/1770 train_time:71492ms step_avg:96.22ms
step:754/1770 train_time:71592ms step_avg:96.23ms
step:755/1770 train_time:71690ms step_avg:96.23ms
step:756/1770 train_time:71789ms step_avg:96.23ms
step:757/1770 train_time:71887ms step_avg:96.23ms
step:758/1770 train_time:71986ms step_avg:96.24ms
step:759/1770 train_time:72084ms step_avg:96.24ms
step:760/1770 train_time:72182ms step_avg:96.24ms
step:761/1770 train_time:72281ms step_avg:96.25ms
step:762/1770 train_time:72380ms step_avg:96.25ms
step:763/1770 train_time:72479ms step_avg:96.25ms
step:764/1770 train_time:72578ms step_avg:96.26ms
step:765/1770 train_time:72677ms step_avg:96.26ms
step:766/1770 train_time:72776ms step_avg:96.26ms
step:767/1770 train_time:72877ms step_avg:96.27ms
step:768/1770 train_time:72976ms step_avg:96.27ms
step:769/1770 train_time:73075ms step_avg:96.28ms
step:770/1770 train_time:73174ms step_avg:96.28ms
step:771/1770 train_time:73274ms step_avg:96.29ms
step:772/1770 train_time:73374ms step_avg:96.29ms
step:773/1770 train_time:73474ms step_avg:96.30ms
step:774/1770 train_time:73573ms step_avg:96.30ms
step:775/1770 train_time:73673ms step_avg:96.30ms
step:776/1770 train_time:73773ms step_avg:96.31ms
step:777/1770 train_time:73872ms step_avg:96.31ms
step:778/1770 train_time:73971ms step_avg:96.32ms
step:779/1770 train_time:74071ms step_avg:96.32ms
step:780/1770 train_time:74170ms step_avg:96.32ms
step:781/1770 train_time:74269ms step_avg:96.33ms
step:782/1770 train_time:74368ms step_avg:96.33ms
step:783/1770 train_time:74467ms step_avg:96.33ms
step:784/1770 train_time:74566ms step_avg:96.34ms
step:785/1770 train_time:74666ms step_avg:96.34ms
step:786/1770 train_time:74765ms step_avg:96.35ms
step:787/1770 train_time:74863ms step_avg:96.35ms
step:788/1770 train_time:74961ms step_avg:96.35ms
step:789/1770 train_time:75061ms step_avg:96.36ms
step:790/1770 train_time:75160ms step_avg:96.36ms
step:791/1770 train_time:75259ms step_avg:96.36ms
step:792/1770 train_time:75358ms step_avg:96.37ms
step:793/1770 train_time:75457ms step_avg:96.37ms
step:794/1770 train_time:75557ms step_avg:96.37ms
step:795/1770 train_time:75657ms step_avg:96.38ms
step:796/1770 train_time:75757ms step_avg:96.38ms
step:797/1770 train_time:75857ms step_avg:96.39ms
step:798/1770 train_time:75958ms step_avg:96.39ms
step:799/1770 train_time:76058ms step_avg:96.40ms
step:800/1770 train_time:76157ms step_avg:96.40ms
step:801/1770 train_time:76257ms step_avg:96.41ms
step:802/1770 train_time:76357ms step_avg:96.41ms
step:803/1770 train_time:76456ms step_avg:96.41ms
step:804/1770 train_time:76556ms step_avg:96.42ms
step:805/1770 train_time:76656ms step_avg:96.42ms
step:806/1770 train_time:76757ms step_avg:96.43ms
step:807/1770 train_time:76857ms step_avg:96.43ms
step:808/1770 train_time:76957ms step_avg:96.44ms
step:809/1770 train_time:77057ms step_avg:96.44ms
step:810/1770 train_time:77156ms step_avg:96.45ms
step:811/1770 train_time:77256ms step_avg:96.45ms
step:812/1770 train_time:77356ms step_avg:96.45ms
step:813/1770 train_time:77456ms step_avg:96.46ms
step:814/1770 train_time:77556ms step_avg:96.46ms
step:815/1770 train_time:77656ms step_avg:96.47ms
step:816/1770 train_time:77756ms step_avg:96.47ms
step:817/1770 train_time:77856ms step_avg:96.48ms
step:818/1770 train_time:77956ms step_avg:96.48ms
step:819/1770 train_time:78056ms step_avg:96.48ms
step:820/1770 train_time:78156ms step_avg:96.49ms
step:821/1770 train_time:78256ms step_avg:96.49ms
step:822/1770 train_time:78356ms step_avg:96.50ms
step:823/1770 train_time:78456ms step_avg:96.50ms
step:824/1770 train_time:78556ms step_avg:96.51ms
step:825/1770 train_time:78656ms step_avg:96.51ms
step:826/1770 train_time:78756ms step_avg:96.51ms
step:827/1770 train_time:78856ms step_avg:96.52ms
step:828/1770 train_time:78956ms step_avg:96.52ms
step:829/1770 train_time:79056ms step_avg:96.53ms
step:830/1770 train_time:79156ms step_avg:96.53ms
step:831/1770 train_time:79255ms step_avg:96.54ms
step:832/1770 train_time:79355ms step_avg:96.54ms
step:833/1770 train_time:79455ms step_avg:96.54ms
step:834/1770 train_time:79555ms step_avg:96.55ms
step:835/1770 train_time:79655ms step_avg:96.55ms
step:836/1770 train_time:79755ms step_avg:96.56ms
step:837/1770 train_time:79855ms step_avg:96.56ms
step:838/1770 train_time:79955ms step_avg:96.56ms
step:839/1770 train_time:80056ms step_avg:96.57ms
step:840/1770 train_time:80157ms step_avg:96.57ms
step:841/1770 train_time:80257ms step_avg:96.58ms
step:842/1770 train_time:80357ms step_avg:96.58ms
step:843/1770 train_time:80457ms step_avg:96.59ms
step:844/1770 train_time:80557ms step_avg:96.59ms
step:845/1770 train_time:80656ms step_avg:96.59ms
step:846/1770 train_time:80755ms step_avg:96.60ms
step:847/1770 train_time:80855ms step_avg:96.60ms
step:848/1770 train_time:80955ms step_avg:96.60ms
step:849/1770 train_time:81055ms step_avg:96.61ms
step:850/1770 train_time:81155ms step_avg:96.61ms
step:851/1770 train_time:81255ms step_avg:96.62ms
step:852/1770 train_time:81356ms step_avg:96.62ms
step:853/1770 train_time:81455ms step_avg:96.63ms
step:854/1770 train_time:81555ms step_avg:96.63ms
step:855/1770 train_time:81654ms step_avg:96.63ms
step:856/1770 train_time:81755ms step_avg:96.64ms
step:857/1770 train_time:81854ms step_avg:96.64ms
step:858/1770 train_time:81953ms step_avg:96.64ms
step:859/1770 train_time:82053ms step_avg:96.65ms
step:860/1770 train_time:82154ms step_avg:96.65ms
step:861/1770 train_time:82254ms step_avg:96.66ms
step:862/1770 train_time:82353ms step_avg:96.66ms
step:863/1770 train_time:82453ms step_avg:96.66ms
step:864/1770 train_time:82553ms step_avg:96.67ms
step:865/1770 train_time:82652ms step_avg:96.67ms
step:866/1770 train_time:82753ms step_avg:96.67ms
step:867/1770 train_time:82852ms step_avg:96.68ms
step:868/1770 train_time:82952ms step_avg:96.68ms
step:869/1770 train_time:83052ms step_avg:96.68ms
step:870/1770 train_time:83152ms step_avg:96.69ms
step:871/1770 train_time:83252ms step_avg:96.69ms
step:872/1770 train_time:83352ms step_avg:96.70ms
step:873/1770 train_time:83451ms step_avg:96.70ms
step:874/1770 train_time:83551ms step_avg:96.70ms
step:875/1770 train_time:83651ms step_avg:96.71ms
step:875/1770 val_loss:3.5555 train_time:83749ms step_avg:96.82ms
step:876/1770 train_time:83770ms step_avg:96.73ms
step:877/1770 train_time:83858ms step_avg:96.72ms
step:878/1770 train_time:83958ms step_avg:96.73ms
step:879/1770 train_time:84057ms step_avg:96.73ms
step:880/1770 train_time:84156ms step_avg:96.73ms
step:881/1770 train_time:84256ms step_avg:96.73ms
step:882/1770 train_time:84355ms step_avg:96.74ms
step:883/1770 train_time:84454ms step_avg:96.74ms
step:884/1770 train_time:84553ms step_avg:96.74ms
step:885/1770 train_time:84652ms step_avg:96.75ms
step:886/1770 train_time:84753ms step_avg:96.75ms
step:887/1770 train_time:84855ms step_avg:96.76ms
step:888/1770 train_time:84956ms step_avg:96.76ms
step:889/1770 train_time:85056ms step_avg:96.76ms
step:890/1770 train_time:85155ms step_avg:96.77ms
step:891/1770 train_time:85255ms step_avg:96.77ms
step:892/1770 train_time:85354ms step_avg:96.77ms
step:893/1770 train_time:85454ms step_avg:96.78ms
step:894/1770 train_time:85553ms step_avg:96.78ms
step:895/1770 train_time:85652ms step_avg:96.78ms
step:896/1770 train_time:85751ms step_avg:96.78ms
step:897/1770 train_time:85853ms step_avg:96.79ms
step:898/1770 train_time:85952ms step_avg:96.79ms
step:899/1770 train_time:86052ms step_avg:96.80ms
step:900/1770 train_time:86152ms step_avg:96.80ms
step:901/1770 train_time:86251ms step_avg:96.80ms
step:902/1770 train_time:86351ms step_avg:96.81ms
step:903/1770 train_time:86451ms step_avg:96.81ms
step:904/1770 train_time:86551ms step_avg:96.81ms
step:905/1770 train_time:86650ms step_avg:96.82ms
step:906/1770 train_time:86749ms step_avg:96.82ms
step:907/1770 train_time:86849ms step_avg:96.82ms
step:908/1770 train_time:86948ms step_avg:96.82ms
step:909/1770 train_time:87049ms step_avg:96.83ms
step:910/1770 train_time:87149ms step_avg:96.83ms
step:911/1770 train_time:87249ms step_avg:96.84ms
step:912/1770 train_time:87349ms step_avg:96.84ms
step:913/1770 train_time:87448ms step_avg:96.84ms
step:914/1770 train_time:87548ms step_avg:96.85ms
step:915/1770 train_time:87648ms step_avg:96.85ms
step:916/1770 train_time:87747ms step_avg:96.85ms
step:917/1770 train_time:87846ms step_avg:96.85ms
step:918/1770 train_time:87945ms step_avg:96.86ms
step:919/1770 train_time:88044ms step_avg:96.86ms
step:920/1770 train_time:88146ms step_avg:96.86ms
step:921/1770 train_time:88247ms step_avg:96.87ms
step:922/1770 train_time:88347ms step_avg:96.87ms
step:923/1770 train_time:88447ms step_avg:96.88ms
step:924/1770 train_time:88548ms step_avg:96.88ms
step:925/1770 train_time:88649ms step_avg:96.88ms
step:926/1770 train_time:88749ms step_avg:96.89ms
step:927/1770 train_time:88850ms step_avg:96.89ms
step:928/1770 train_time:88952ms step_avg:96.90ms
step:929/1770 train_time:89053ms step_avg:96.90ms
step:930/1770 train_time:89154ms step_avg:96.91ms
step:931/1770 train_time:89256ms step_avg:96.91ms
step:932/1770 train_time:89356ms step_avg:96.92ms
step:933/1770 train_time:89458ms step_avg:96.92ms
step:934/1770 train_time:89558ms step_avg:96.92ms
step:935/1770 train_time:89658ms step_avg:96.93ms
step:936/1770 train_time:89758ms step_avg:96.93ms
step:937/1770 train_time:89859ms step_avg:96.93ms
step:938/1770 train_time:89959ms step_avg:96.94ms
step:939/1770 train_time:90059ms step_avg:96.94ms
step:940/1770 train_time:90160ms step_avg:96.95ms
step:941/1770 train_time:90261ms step_avg:96.95ms
step:942/1770 train_time:90361ms step_avg:96.95ms
step:943/1770 train_time:90462ms step_avg:96.96ms
step:944/1770 train_time:90561ms step_avg:96.96ms
step:945/1770 train_time:90661ms step_avg:96.96ms
step:946/1770 train_time:90762ms step_avg:96.97ms
step:947/1770 train_time:90861ms step_avg:96.97ms
step:948/1770 train_time:90961ms step_avg:96.97ms
step:949/1770 train_time:91061ms step_avg:96.98ms
step:950/1770 train_time:91161ms step_avg:96.98ms
step:951/1770 train_time:91262ms step_avg:96.98ms
step:952/1770 train_time:91362ms step_avg:96.99ms
step:953/1770 train_time:91463ms step_avg:96.99ms
step:954/1770 train_time:91562ms step_avg:96.99ms
step:955/1770 train_time:91663ms step_avg:97.00ms
step:956/1770 train_time:91763ms step_avg:97.00ms
step:957/1770 train_time:91864ms step_avg:97.01ms
step:958/1770 train_time:91965ms step_avg:97.01ms
step:959/1770 train_time:92065ms step_avg:97.01ms
step:960/1770 train_time:92165ms step_avg:97.02ms
step:961/1770 train_time:92265ms step_avg:97.02ms
step:962/1770 train_time:92367ms step_avg:97.02ms
step:963/1770 train_time:92468ms step_avg:97.03ms
step:964/1770 train_time:92568ms step_avg:97.03ms
step:965/1770 train_time:92669ms step_avg:97.04ms
step:966/1770 train_time:92771ms step_avg:97.04ms
step:967/1770 train_time:92873ms step_avg:97.05ms
step:968/1770 train_time:92975ms step_avg:97.05ms
step:969/1770 train_time:93075ms step_avg:97.05ms
step:970/1770 train_time:93176ms step_avg:97.06ms
step:971/1770 train_time:93276ms step_avg:97.06ms
step:972/1770 train_time:93377ms step_avg:97.07ms
step:973/1770 train_time:93478ms step_avg:97.07ms
step:974/1770 train_time:93579ms step_avg:97.07ms
step:975/1770 train_time:93680ms step_avg:97.08ms
step:976/1770 train_time:93781ms step_avg:97.08ms
step:977/1770 train_time:93880ms step_avg:97.08ms
step:978/1770 train_time:93980ms step_avg:97.09ms
step:979/1770 train_time:94080ms step_avg:97.09ms
step:980/1770 train_time:94181ms step_avg:97.09ms
step:981/1770 train_time:94281ms step_avg:97.10ms
step:982/1770 train_time:94381ms step_avg:97.10ms
step:983/1770 train_time:94481ms step_avg:97.10ms
step:984/1770 train_time:94582ms step_avg:97.11ms
step:985/1770 train_time:94682ms step_avg:97.11ms
step:986/1770 train_time:94782ms step_avg:97.11ms
step:987/1770 train_time:94882ms step_avg:97.12ms
step:988/1770 train_time:94982ms step_avg:97.12ms
step:989/1770 train_time:95083ms step_avg:97.12ms
step:990/1770 train_time:95183ms step_avg:97.13ms
step:991/1770 train_time:95283ms step_avg:97.13ms
step:992/1770 train_time:95383ms step_avg:97.13ms
step:993/1770 train_time:95483ms step_avg:97.13ms
step:994/1770 train_time:95584ms step_avg:97.14ms
step:995/1770 train_time:95684ms step_avg:97.14ms
step:996/1770 train_time:95784ms step_avg:97.14ms
step:997/1770 train_time:95884ms step_avg:97.15ms
step:998/1770 train_time:95983ms step_avg:97.15ms
step:999/1770 train_time:96084ms step_avg:97.15ms
step:1000/1770 train_time:96184ms step_avg:97.16ms
step:1000/1770 val_loss:3.5162 train_time:96283ms step_avg:97.26ms
step:1001/1770 train_time:96304ms step_avg:97.18ms
step:1002/1770 train_time:96392ms step_avg:97.17ms
step:1003/1770 train_time:96494ms step_avg:97.17ms
step:1004/1770 train_time:96594ms step_avg:97.18ms
step:1005/1770 train_time:96694ms step_avg:97.18ms
step:1006/1770 train_time:96794ms step_avg:97.18ms
step:1007/1770 train_time:96893ms step_avg:97.18ms
step:1008/1770 train_time:96993ms step_avg:97.19ms
step:1009/1770 train_time:97093ms step_avg:97.19ms
step:1010/1770 train_time:97192ms step_avg:97.19ms
step:1011/1770 train_time:97294ms step_avg:97.20ms
step:1012/1770 train_time:97395ms step_avg:97.20ms
step:1013/1770 train_time:97496ms step_avg:97.20ms
step:1014/1770 train_time:97596ms step_avg:97.21ms
step:1015/1770 train_time:97696ms step_avg:97.21ms
step:1016/1770 train_time:97797ms step_avg:97.21ms
step:1017/1770 train_time:97897ms step_avg:97.22ms
step:1018/1770 train_time:97998ms step_avg:97.22ms
step:1019/1770 train_time:98098ms step_avg:97.22ms
step:1020/1770 train_time:98200ms step_avg:97.23ms
step:1021/1770 train_time:98302ms step_avg:97.23ms
step:1022/1770 train_time:98403ms step_avg:97.24ms
step:1023/1770 train_time:98504ms step_avg:97.24ms
step:1024/1770 train_time:98606ms step_avg:97.24ms
step:1025/1770 train_time:98708ms step_avg:97.25ms
step:1026/1770 train_time:98810ms step_avg:97.25ms
step:1027/1770 train_time:98910ms step_avg:97.26ms
step:1028/1770 train_time:99011ms step_avg:97.26ms
step:1029/1770 train_time:99110ms step_avg:97.26ms
step:1030/1770 train_time:99211ms step_avg:97.27ms
step:1031/1770 train_time:99312ms step_avg:97.27ms
step:1032/1770 train_time:99412ms step_avg:97.27ms
step:1033/1770 train_time:99512ms step_avg:97.27ms
step:1034/1770 train_time:99613ms step_avg:97.28ms
step:1035/1770 train_time:99713ms step_avg:97.28ms
step:1036/1770 train_time:99813ms step_avg:97.28ms
step:1037/1770 train_time:99913ms step_avg:97.29ms
step:1038/1770 train_time:100013ms step_avg:97.29ms
step:1039/1770 train_time:100113ms step_avg:97.29ms
step:1040/1770 train_time:100214ms step_avg:97.29ms
step:1041/1770 train_time:100314ms step_avg:97.30ms
step:1042/1770 train_time:100415ms step_avg:97.30ms
step:1043/1770 train_time:100516ms step_avg:97.30ms
step:1044/1770 train_time:100616ms step_avg:97.31ms
step:1045/1770 train_time:100717ms step_avg:97.31ms
step:1046/1770 train_time:100818ms step_avg:97.31ms
step:1047/1770 train_time:100918ms step_avg:97.32ms
step:1048/1770 train_time:101019ms step_avg:97.32ms
step:1049/1770 train_time:101120ms step_avg:97.32ms
step:1050/1770 train_time:101221ms step_avg:97.33ms
step:1051/1770 train_time:101322ms step_avg:97.33ms
step:1052/1770 train_time:101423ms step_avg:97.34ms
step:1053/1770 train_time:101524ms step_avg:97.34ms
step:1054/1770 train_time:101626ms step_avg:97.34ms
step:1055/1770 train_time:101727ms step_avg:97.35ms
step:1056/1770 train_time:101829ms step_avg:97.35ms
step:1057/1770 train_time:101931ms step_avg:97.36ms
step:1058/1770 train_time:102032ms step_avg:97.36ms
step:1059/1770 train_time:102133ms step_avg:97.36ms
step:1060/1770 train_time:102233ms step_avg:97.36ms
step:1061/1770 train_time:102333ms step_avg:97.37ms
step:1062/1770 train_time:102434ms step_avg:97.37ms
step:1063/1770 train_time:102536ms step_avg:97.38ms
step:1064/1770 train_time:102638ms step_avg:97.38ms
step:1065/1770 train_time:102738ms step_avg:97.38ms
step:1066/1770 train_time:102840ms step_avg:97.39ms
step:1067/1770 train_time:102941ms step_avg:97.39ms
step:1068/1770 train_time:103045ms step_avg:97.40ms
step:1069/1770 train_time:103145ms step_avg:97.40ms
step:1070/1770 train_time:103247ms step_avg:97.40ms
step:1071/1770 train_time:103348ms step_avg:97.41ms
step:1072/1770 train_time:103449ms step_avg:97.41ms
step:1073/1770 train_time:103550ms step_avg:97.41ms
step:1074/1770 train_time:103651ms step_avg:97.42ms
step:1075/1770 train_time:103752ms step_avg:97.42ms
step:1076/1770 train_time:103853ms step_avg:97.42ms
step:1077/1770 train_time:103953ms step_avg:97.43ms
step:1078/1770 train_time:104052ms step_avg:97.43ms
step:1079/1770 train_time:104152ms step_avg:97.43ms
step:1080/1770 train_time:104252ms step_avg:97.43ms
step:1081/1770 train_time:104352ms step_avg:97.43ms
step:1082/1770 train_time:104453ms step_avg:97.44ms
step:1083/1770 train_time:104553ms step_avg:97.44ms
step:1084/1770 train_time:104653ms step_avg:97.44ms
step:1085/1770 train_time:104754ms step_avg:97.45ms
step:1086/1770 train_time:104854ms step_avg:97.45ms
step:1087/1770 train_time:104954ms step_avg:97.45ms
step:1088/1770 train_time:105054ms step_avg:97.45ms
step:1089/1770 train_time:105154ms step_avg:97.46ms
step:1090/1770 train_time:105256ms step_avg:97.46ms
step:1091/1770 train_time:105356ms step_avg:97.46ms
step:1092/1770 train_time:105458ms step_avg:97.47ms
step:1093/1770 train_time:105558ms step_avg:97.47ms
step:1094/1770 train_time:105659ms step_avg:97.47ms
step:1095/1770 train_time:105760ms step_avg:97.47ms
step:1096/1770 train_time:105861ms step_avg:97.48ms
step:1097/1770 train_time:105963ms step_avg:97.48ms
step:1098/1770 train_time:106064ms step_avg:97.49ms
step:1099/1770 train_time:106165ms step_avg:97.49ms
step:1100/1770 train_time:106267ms step_avg:97.49ms
step:1101/1770 train_time:106369ms step_avg:97.50ms
step:1102/1770 train_time:106469ms step_avg:97.50ms
step:1103/1770 train_time:106570ms step_avg:97.50ms
step:1104/1770 train_time:106671ms step_avg:97.51ms
step:1105/1770 train_time:106771ms step_avg:97.51ms
step:1106/1770 train_time:106872ms step_avg:97.51ms
step:1107/1770 train_time:106972ms step_avg:97.51ms
step:1108/1770 train_time:107073ms step_avg:97.52ms
step:1109/1770 train_time:107173ms step_avg:97.52ms
step:1110/1770 train_time:107273ms step_avg:97.52ms
step:1111/1770 train_time:107374ms step_avg:97.52ms
step:1112/1770 train_time:107475ms step_avg:97.53ms
step:1113/1770 train_time:107576ms step_avg:97.53ms
step:1114/1770 train_time:107677ms step_avg:97.53ms
step:1115/1770 train_time:107777ms step_avg:97.54ms
step:1116/1770 train_time:107878ms step_avg:97.54ms
step:1117/1770 train_time:107980ms step_avg:97.54ms
step:1118/1770 train_time:108081ms step_avg:97.55ms
step:1119/1770 train_time:108183ms step_avg:97.55ms
step:1120/1770 train_time:108284ms step_avg:97.55ms
step:1121/1770 train_time:108386ms step_avg:97.56ms
step:1122/1770 train_time:108488ms step_avg:97.56ms
step:1123/1770 train_time:108589ms step_avg:97.56ms
step:1124/1770 train_time:108690ms step_avg:97.57ms
step:1125/1770 train_time:108792ms step_avg:97.57ms
step:1125/1770 val_loss:3.4759 train_time:108891ms step_avg:97.66ms
step:1126/1770 train_time:108912ms step_avg:97.59ms
step:1127/1770 train_time:109001ms step_avg:97.58ms
step:1128/1770 train_time:109102ms step_avg:97.59ms
step:1129/1770 train_time:109202ms step_avg:97.59ms
step:1130/1770 train_time:109303ms step_avg:97.59ms
step:1131/1770 train_time:109403ms step_avg:97.59ms
step:1132/1770 train_time:109505ms step_avg:97.60ms
step:1133/1770 train_time:109605ms step_avg:97.60ms
step:1134/1770 train_time:109706ms step_avg:97.60ms
step:1135/1770 train_time:109806ms step_avg:97.61ms
step:1136/1770 train_time:109909ms step_avg:97.61ms
step:1137/1770 train_time:110013ms step_avg:97.62ms
step:1138/1770 train_time:110115ms step_avg:97.62ms
step:1139/1770 train_time:110216ms step_avg:97.62ms
step:1140/1770 train_time:110316ms step_avg:97.63ms
step:1141/1770 train_time:110416ms step_avg:97.63ms
step:1142/1770 train_time:110516ms step_avg:97.63ms
step:1143/1770 train_time:110616ms step_avg:97.63ms
step:1144/1770 train_time:110716ms step_avg:97.63ms
step:1145/1770 train_time:110816ms step_avg:97.64ms
step:1146/1770 train_time:110917ms step_avg:97.64ms
step:1147/1770 train_time:111019ms step_avg:97.64ms
step:1148/1770 train_time:111119ms step_avg:97.64ms
step:1149/1770 train_time:111219ms step_avg:97.65ms
step:1150/1770 train_time:111319ms step_avg:97.65ms
step:1151/1770 train_time:111420ms step_avg:97.65ms
step:1152/1770 train_time:111522ms step_avg:97.66ms
step:1153/1770 train_time:111622ms step_avg:97.66ms
step:1154/1770 train_time:111722ms step_avg:97.66ms
step:1155/1770 train_time:111823ms step_avg:97.66ms
step:1156/1770 train_time:111923ms step_avg:97.66ms
step:1157/1770 train_time:112025ms step_avg:97.67ms
step:1158/1770 train_time:112127ms step_avg:97.67ms
step:1159/1770 train_time:112228ms step_avg:97.67ms
step:1160/1770 train_time:112329ms step_avg:97.68ms
step:1161/1770 train_time:112431ms step_avg:97.68ms
step:1162/1770 train_time:112533ms step_avg:97.69ms
step:1163/1770 train_time:112634ms step_avg:97.69ms
step:1164/1770 train_time:112735ms step_avg:97.69ms
step:1165/1770 train_time:112835ms step_avg:97.69ms
step:1166/1770 train_time:112938ms step_avg:97.70ms
step:1167/1770 train_time:113038ms step_avg:97.70ms
step:1168/1770 train_time:113139ms step_avg:97.70ms
step:1169/1770 train_time:113238ms step_avg:97.70ms
step:1170/1770 train_time:113338ms step_avg:97.71ms
step:1171/1770 train_time:113439ms step_avg:97.71ms
step:1172/1770 train_time:113539ms step_avg:97.71ms
step:1173/1770 train_time:113639ms step_avg:97.71ms
step:1174/1770 train_time:113740ms step_avg:97.71ms
step:1175/1770 train_time:113841ms step_avg:97.72ms
step:1176/1770 train_time:113941ms step_avg:97.72ms
step:1177/1770 train_time:114042ms step_avg:97.72ms
step:1178/1770 train_time:114143ms step_avg:97.73ms
step:1179/1770 train_time:114243ms step_avg:97.73ms
step:1180/1770 train_time:114344ms step_avg:97.73ms
step:1181/1770 train_time:114444ms step_avg:97.73ms
step:1182/1770 train_time:114545ms step_avg:97.73ms
step:1183/1770 train_time:114648ms step_avg:97.74ms
step:1184/1770 train_time:114753ms step_avg:97.74ms
step:1185/1770 train_time:114855ms step_avg:97.75ms
step:1186/1770 train_time:114958ms step_avg:97.75ms
step:1187/1770 train_time:115062ms step_avg:97.76ms
step:1188/1770 train_time:115163ms step_avg:97.76ms
step:1189/1770 train_time:115264ms step_avg:97.76ms
step:1190/1770 train_time:115365ms step_avg:97.77ms
step:1191/1770 train_time:115467ms step_avg:97.77ms
step:1192/1770 train_time:115569ms step_avg:97.77ms
step:1193/1770 train_time:115672ms step_avg:97.78ms
step:1194/1770 train_time:115774ms step_avg:97.78ms
step:1195/1770 train_time:115878ms step_avg:97.79ms
step:1196/1770 train_time:115981ms step_avg:97.79ms
step:1197/1770 train_time:116084ms step_avg:97.80ms
step:1198/1770 train_time:116186ms step_avg:97.80ms
step:1199/1770 train_time:116288ms step_avg:97.80ms
step:1200/1770 train_time:116390ms step_avg:97.81ms
step:1201/1770 train_time:116493ms step_avg:97.81ms
step:1202/1770 train_time:116595ms step_avg:97.81ms
step:1203/1770 train_time:116696ms step_avg:97.82ms
step:1204/1770 train_time:116799ms step_avg:97.82ms
step:1205/1770 train_time:116901ms step_avg:97.83ms
step:1206/1770 train_time:117004ms step_avg:97.83ms
step:1207/1770 train_time:117105ms step_avg:97.83ms
step:1208/1770 train_time:117207ms step_avg:97.84ms
step:1209/1770 train_time:117310ms step_avg:97.84ms
step:1210/1770 train_time:117411ms step_avg:97.84ms
step:1211/1770 train_time:117514ms step_avg:97.85ms
step:1212/1770 train_time:117618ms step_avg:97.85ms
step:1213/1770 train_time:117720ms step_avg:97.86ms
step:1214/1770 train_time:117821ms step_avg:97.86ms
step:1215/1770 train_time:117924ms step_avg:97.86ms
step:1216/1770 train_time:118028ms step_avg:97.87ms
step:1217/1770 train_time:118130ms step_avg:97.87ms
step:1218/1770 train_time:118232ms step_avg:97.87ms
step:1219/1770 train_time:118334ms step_avg:97.88ms
step:1220/1770 train_time:118437ms step_avg:97.88ms
step:1221/1770 train_time:118538ms step_avg:97.88ms
step:1222/1770 train_time:118642ms step_avg:97.89ms
step:1223/1770 train_time:118743ms step_avg:97.89ms
step:1224/1770 train_time:118845ms step_avg:97.90ms
step:1225/1770 train_time:118947ms step_avg:97.90ms
step:1226/1770 train_time:119048ms step_avg:97.90ms
step:1227/1770 train_time:119153ms step_avg:97.91ms
step:1228/1770 train_time:119257ms step_avg:97.91ms
step:1229/1770 train_time:119359ms step_avg:97.92ms
step:1230/1770 train_time:119460ms step_avg:97.92ms
step:1231/1770 train_time:119562ms step_avg:97.92ms
step:1232/1770 train_time:119664ms step_avg:97.92ms
step:1233/1770 train_time:119766ms step_avg:97.93ms
step:1234/1770 train_time:119868ms step_avg:97.93ms
step:1235/1770 train_time:119970ms step_avg:97.93ms
step:1236/1770 train_time:120073ms step_avg:97.94ms
step:1237/1770 train_time:120175ms step_avg:97.94ms
step:1238/1770 train_time:120278ms step_avg:97.95ms
step:1239/1770 train_time:120379ms step_avg:97.95ms
step:1240/1770 train_time:120481ms step_avg:97.95ms
step:1241/1770 train_time:120583ms step_avg:97.96ms
step:1242/1770 train_time:120685ms step_avg:97.96ms
step:1243/1770 train_time:120787ms step_avg:97.96ms
step:1244/1770 train_time:120889ms step_avg:97.97ms
step:1245/1770 train_time:120991ms step_avg:97.97ms
step:1246/1770 train_time:121093ms step_avg:97.97ms
step:1247/1770 train_time:121195ms step_avg:97.98ms
step:1248/1770 train_time:121298ms step_avg:97.98ms
step:1249/1770 train_time:121400ms step_avg:97.98ms
step:1250/1770 train_time:121502ms step_avg:97.99ms
step:1250/1770 val_loss:3.4283 train_time:121603ms step_avg:98.07ms
step:1251/1770 train_time:121625ms step_avg:98.01ms
step:1252/1770 train_time:121717ms step_avg:98.00ms
step:1253/1770 train_time:121819ms step_avg:98.00ms
step:1254/1770 train_time:121921ms step_avg:98.01ms
step:1255/1770 train_time:122025ms step_avg:98.01ms
step:1256/1770 train_time:122126ms step_avg:98.01ms
step:1257/1770 train_time:122227ms step_avg:98.02ms
step:1258/1770 train_time:122330ms step_avg:98.02ms
step:1259/1770 train_time:122433ms step_avg:98.02ms
step:1260/1770 train_time:122534ms step_avg:98.03ms
step:1261/1770 train_time:122638ms step_avg:98.03ms
step:1262/1770 train_time:122742ms step_avg:98.04ms
step:1263/1770 train_time:122844ms step_avg:98.04ms
step:1264/1770 train_time:122949ms step_avg:98.05ms
step:1265/1770 train_time:123050ms step_avg:98.05ms
step:1266/1770 train_time:123153ms step_avg:98.05ms
step:1267/1770 train_time:123255ms step_avg:98.06ms
step:1268/1770 train_time:123358ms step_avg:98.06ms
step:1269/1770 train_time:123459ms step_avg:98.06ms
step:1270/1770 train_time:123561ms step_avg:98.06ms
step:1271/1770 train_time:123664ms step_avg:98.07ms
step:1272/1770 train_time:123766ms step_avg:98.07ms
step:1273/1770 train_time:123870ms step_avg:98.08ms
step:1274/1770 train_time:123973ms step_avg:98.08ms
step:1275/1770 train_time:124074ms step_avg:98.08ms
step:1276/1770 train_time:124176ms step_avg:98.09ms
step:1277/1770 train_time:124278ms step_avg:98.09ms
step:1278/1770 train_time:124381ms step_avg:98.09ms
step:1279/1770 train_time:124483ms step_avg:98.10ms
step:1280/1770 train_time:124587ms step_avg:98.10ms
step:1281/1770 train_time:124688ms step_avg:98.10ms
step:1282/1770 train_time:124792ms step_avg:98.11ms
step:1283/1770 train_time:124894ms step_avg:98.11ms
step:1284/1770 train_time:124996ms step_avg:98.11ms
step:1285/1770 train_time:125099ms step_avg:98.12ms
step:1286/1770 train_time:125202ms step_avg:98.12ms
step:1287/1770 train_time:125305ms step_avg:98.12ms
step:1288/1770 train_time:125408ms step_avg:98.13ms
step:1289/1770 train_time:125511ms step_avg:98.13ms
step:1290/1770 train_time:125612ms step_avg:98.13ms
step:1291/1770 train_time:125714ms step_avg:98.14ms
step:1292/1770 train_time:125815ms step_avg:98.14ms
step:1293/1770 train_time:125918ms step_avg:98.14ms
step:1294/1770 train_time:126019ms step_avg:98.15ms
step:1295/1770 train_time:126121ms step_avg:98.15ms
step:1296/1770 train_time:126222ms step_avg:98.15ms
step:1297/1770 train_time:126324ms step_avg:98.15ms
step:1298/1770 train_time:126427ms step_avg:98.16ms
step:1299/1770 train_time:126529ms step_avg:98.16ms
step:1300/1770 train_time:126632ms step_avg:98.16ms
step:1301/1770 train_time:126734ms step_avg:98.17ms
step:1302/1770 train_time:126836ms step_avg:98.17ms
step:1303/1770 train_time:126938ms step_avg:98.17ms
step:1304/1770 train_time:127039ms step_avg:98.18ms
step:1305/1770 train_time:127141ms step_avg:98.18ms
step:1306/1770 train_time:127244ms step_avg:98.18ms
step:1307/1770 train_time:127345ms step_avg:98.18ms
step:1308/1770 train_time:127448ms step_avg:98.19ms
step:1309/1770 train_time:127550ms step_avg:98.19ms
step:1310/1770 train_time:127653ms step_avg:98.19ms
step:1311/1770 train_time:127754ms step_avg:98.20ms
step:1312/1770 train_time:127856ms step_avg:98.20ms
step:1313/1770 train_time:127956ms step_avg:98.20ms
step:1314/1770 train_time:128058ms step_avg:98.20ms
step:1315/1770 train_time:128160ms step_avg:98.21ms
step:1316/1770 train_time:128262ms step_avg:98.21ms
step:1317/1770 train_time:128364ms step_avg:98.21ms
step:1318/1770 train_time:128469ms step_avg:98.22ms
step:1319/1770 train_time:128572ms step_avg:98.22ms
step:1320/1770 train_time:128674ms step_avg:98.22ms
step:1321/1770 train_time:128776ms step_avg:98.23ms
step:1322/1770 train_time:128879ms step_avg:98.23ms
step:1323/1770 train_time:128981ms step_avg:98.23ms
step:1324/1770 train_time:129084ms step_avg:98.24ms
step:1325/1770 train_time:129187ms step_avg:98.24ms
step:1326/1770 train_time:129289ms step_avg:98.24ms
step:1327/1770 train_time:129394ms step_avg:98.25ms
step:1328/1770 train_time:129495ms step_avg:98.25ms
step:1329/1770 train_time:129597ms step_avg:98.25ms
step:1330/1770 train_time:129698ms step_avg:98.26ms
step:1331/1770 train_time:129800ms step_avg:98.26ms
step:1332/1770 train_time:129902ms step_avg:98.26ms
step:1333/1770 train_time:130004ms step_avg:98.26ms
step:1334/1770 train_time:130106ms step_avg:98.27ms
step:1335/1770 train_time:130208ms step_avg:98.27ms
step:1336/1770 train_time:130310ms step_avg:98.27ms
step:1337/1770 train_time:130412ms step_avg:98.28ms
step:1338/1770 train_time:130514ms step_avg:98.28ms
step:1339/1770 train_time:130616ms step_avg:98.28ms
step:1340/1770 train_time:130719ms step_avg:98.29ms
step:1341/1770 train_time:130820ms step_avg:98.29ms
step:1342/1770 train_time:130923ms step_avg:98.29ms
step:1343/1770 train_time:131027ms step_avg:98.29ms
step:1344/1770 train_time:131130ms step_avg:98.30ms
step:1345/1770 train_time:131232ms step_avg:98.30ms
step:1346/1770 train_time:131334ms step_avg:98.30ms
step:1347/1770 train_time:131436ms step_avg:98.31ms
step:1348/1770 train_time:131540ms step_avg:98.31ms
step:1349/1770 train_time:131643ms step_avg:98.31ms
step:1350/1770 train_time:131745ms step_avg:98.32ms
step:1351/1770 train_time:131847ms step_avg:98.32ms
step:1352/1770 train_time:131950ms step_avg:98.32ms
step:1353/1770 train_time:132053ms step_avg:98.33ms
step:1354/1770 train_time:132155ms step_avg:98.33ms
step:1355/1770 train_time:132256ms step_avg:98.33ms
step:1356/1770 train_time:132358ms step_avg:98.33ms
step:1357/1770 train_time:132460ms step_avg:98.34ms
step:1358/1770 train_time:132562ms step_avg:98.34ms
step:1359/1770 train_time:132664ms step_avg:98.34ms
step:1360/1770 train_time:132766ms step_avg:98.35ms
step:1361/1770 train_time:132869ms step_avg:98.35ms
step:1362/1770 train_time:132971ms step_avg:98.35ms
step:1363/1770 train_time:133074ms step_avg:98.35ms
step:1364/1770 train_time:133176ms step_avg:98.36ms
step:1365/1770 train_time:133277ms step_avg:98.36ms
step:1366/1770 train_time:133378ms step_avg:98.36ms
step:1367/1770 train_time:133481ms step_avg:98.36ms
step:1368/1770 train_time:133583ms step_avg:98.37ms
step:1369/1770 train_time:133686ms step_avg:98.37ms
step:1370/1770 train_time:133789ms step_avg:98.37ms
step:1371/1770 train_time:133891ms step_avg:98.38ms
step:1372/1770 train_time:133994ms step_avg:98.38ms
step:1373/1770 train_time:134096ms step_avg:98.38ms
step:1374/1770 train_time:134198ms step_avg:98.39ms
step:1375/1770 train_time:134301ms step_avg:98.39ms
step:1375/1770 val_loss:3.3844 train_time:134402ms step_avg:98.46ms
step:1376/1770 train_time:134423ms step_avg:98.41ms
step:1377/1770 train_time:134513ms step_avg:98.40ms
step:1378/1770 train_time:134614ms step_avg:98.40ms
step:1379/1770 train_time:134715ms step_avg:98.40ms
step:1380/1770 train_time:134817ms step_avg:98.41ms
step:1381/1770 train_time:134920ms step_avg:98.41ms
step:1382/1770 train_time:135022ms step_avg:98.41ms
step:1383/1770 train_time:135124ms step_avg:98.42ms
step:1384/1770 train_time:135226ms step_avg:98.42ms
step:1385/1770 train_time:135329ms step_avg:98.42ms
step:1386/1770 train_time:135431ms step_avg:98.42ms
step:1387/1770 train_time:135534ms step_avg:98.43ms
step:1388/1770 train_time:135636ms step_avg:98.43ms
step:1389/1770 train_time:135738ms step_avg:98.43ms
step:1390/1770 train_time:135840ms step_avg:98.43ms
step:1391/1770 train_time:135942ms step_avg:98.44ms
step:1392/1770 train_time:136045ms step_avg:98.44ms
step:1393/1770 train_time:136147ms step_avg:98.44ms
step:1394/1770 train_time:136248ms step_avg:98.45ms
step:1395/1770 train_time:136351ms step_avg:98.45ms
step:1396/1770 train_time:136454ms step_avg:98.45ms
step:1397/1770 train_time:136556ms step_avg:98.45ms
step:1398/1770 train_time:136658ms step_avg:98.46ms
step:1399/1770 train_time:136760ms step_avg:98.46ms
step:1400/1770 train_time:136863ms step_avg:98.46ms
step:1401/1770 train_time:136966ms step_avg:98.47ms
step:1402/1770 train_time:137069ms step_avg:98.47ms
step:1403/1770 train_time:137171ms step_avg:98.47ms
step:1404/1770 train_time:137274ms step_avg:98.47ms
step:1405/1770 train_time:137376ms step_avg:98.48ms
step:1406/1770 train_time:137478ms step_avg:98.48ms
step:1407/1770 train_time:137580ms step_avg:98.48ms
step:1408/1770 train_time:137683ms step_avg:98.49ms
step:1409/1770 train_time:137785ms step_avg:98.49ms
step:1410/1770 train_time:137887ms step_avg:98.49ms
step:1411/1770 train_time:137991ms step_avg:98.49ms
step:1412/1770 train_time:138092ms step_avg:98.50ms
step:1413/1770 train_time:138194ms step_avg:98.50ms
step:1414/1770 train_time:138296ms step_avg:98.50ms
step:1415/1770 train_time:138399ms step_avg:98.50ms
step:1416/1770 train_time:138502ms step_avg:98.51ms
step:1417/1770 train_time:138604ms step_avg:98.51ms
step:1418/1770 train_time:138706ms step_avg:98.51ms
step:1419/1770 train_time:138810ms step_avg:98.52ms
step:1420/1770 train_time:138911ms step_avg:98.52ms
step:1421/1770 train_time:139013ms step_avg:98.52ms
step:1422/1770 train_time:139114ms step_avg:98.52ms
step:1423/1770 train_time:139216ms step_avg:98.53ms
step:1424/1770 train_time:139318ms step_avg:98.53ms
step:1425/1770 train_time:139420ms step_avg:98.53ms
step:1426/1770 train_time:139523ms step_avg:98.53ms
step:1427/1770 train_time:139625ms step_avg:98.54ms
step:1428/1770 train_time:139729ms step_avg:98.54ms
step:1429/1770 train_time:139831ms step_avg:98.54ms
step:1430/1770 train_time:139933ms step_avg:98.54ms
step:1431/1770 train_time:140036ms step_avg:98.55ms
step:1432/1770 train_time:140137ms step_avg:98.55ms
step:1433/1770 train_time:140239ms step_avg:98.55ms
step:1434/1770 train_time:140340ms step_avg:98.55ms
step:1435/1770 train_time:140442ms step_avg:98.56ms
step:1436/1770 train_time:140546ms step_avg:98.56ms
step:1437/1770 train_time:140649ms step_avg:98.56ms
step:1438/1770 train_time:140750ms step_avg:98.56ms
step:1439/1770 train_time:140852ms step_avg:98.57ms
step:1440/1770 train_time:140954ms step_avg:98.57ms
step:1441/1770 train_time:141059ms step_avg:98.57ms
step:1442/1770 train_time:141160ms step_avg:98.58ms
step:1443/1770 train_time:141263ms step_avg:98.58ms
step:1444/1770 train_time:141366ms step_avg:98.58ms
step:1445/1770 train_time:141468ms step_avg:98.58ms
step:1446/1770 train_time:141571ms step_avg:98.59ms
step:1447/1770 train_time:141674ms step_avg:98.59ms
step:1448/1770 train_time:141776ms step_avg:98.59ms
step:1449/1770 train_time:141880ms step_avg:98.60ms
step:1450/1770 train_time:141984ms step_avg:98.60ms
step:1451/1770 train_time:142088ms step_avg:98.60ms
step:1452/1770 train_time:142191ms step_avg:98.61ms
step:1453/1770 train_time:142294ms step_avg:98.61ms
step:1454/1770 train_time:142396ms step_avg:98.61ms
step:1455/1770 train_time:142501ms step_avg:98.62ms
step:1456/1770 train_time:142605ms step_avg:98.62ms
step:1457/1770 train_time:142709ms step_avg:98.62ms
step:1458/1770 train_time:142813ms step_avg:98.63ms
step:1459/1770 train_time:142917ms step_avg:98.63ms
step:1460/1770 train_time:143020ms step_avg:98.63ms
step:1461/1770 train_time:143125ms step_avg:98.64ms
step:1462/1770 train_time:143228ms step_avg:98.64ms
step:1463/1770 train_time:143331ms step_avg:98.64ms
step:1464/1770 train_time:143436ms step_avg:98.65ms
step:1465/1770 train_time:143539ms step_avg:98.65ms
step:1466/1770 train_time:143643ms step_avg:98.66ms
step:1467/1770 train_time:143748ms step_avg:98.66ms
step:1468/1770 train_time:143851ms step_avg:98.66ms
step:1469/1770 train_time:143953ms step_avg:98.67ms
step:1470/1770 train_time:144056ms step_avg:98.67ms
step:1471/1770 train_time:144159ms step_avg:98.67ms
step:1472/1770 train_time:144263ms step_avg:98.67ms
step:1473/1770 train_time:144367ms step_avg:98.68ms
step:1474/1770 train_time:144472ms step_avg:98.68ms
step:1475/1770 train_time:144574ms step_avg:98.69ms
step:1476/1770 train_time:144677ms step_avg:98.69ms
step:1477/1770 train_time:144782ms step_avg:98.69ms
step:1478/1770 train_time:144886ms step_avg:98.70ms
step:1479/1770 train_time:144989ms step_avg:98.70ms
step:1480/1770 train_time:145091ms step_avg:98.70ms
step:1481/1770 train_time:145198ms step_avg:98.71ms
step:1482/1770 train_time:145301ms step_avg:98.71ms
step:1483/1770 train_time:145405ms step_avg:98.71ms
step:1484/1770 train_time:145508ms step_avg:98.72ms
step:1485/1770 train_time:145611ms step_avg:98.72ms
step:1486/1770 train_time:145713ms step_avg:98.72ms
step:1487/1770 train_time:145816ms step_avg:98.72ms
step:1488/1770 train_time:145920ms step_avg:98.73ms
step:1489/1770 train_time:146026ms step_avg:98.73ms
step:1490/1770 train_time:146130ms step_avg:98.74ms
step:1491/1770 train_time:146232ms step_avg:98.74ms
step:1492/1770 train_time:146336ms step_avg:98.74ms
step:1493/1770 train_time:146441ms step_avg:98.75ms
step:1494/1770 train_time:146548ms step_avg:98.75ms
step:1495/1770 train_time:146650ms step_avg:98.75ms
step:1496/1770 train_time:146753ms step_avg:98.76ms
step:1497/1770 train_time:146856ms step_avg:98.76ms
step:1498/1770 train_time:146958ms step_avg:98.76ms
step:1499/1770 train_time:147061ms step_avg:98.76ms
step:1500/1770 train_time:147164ms step_avg:98.77ms
step:1500/1770 val_loss:3.3464 train_time:147266ms step_avg:98.84ms
step:1501/1770 train_time:147287ms step_avg:98.78ms
step:1502/1770 train_time:147380ms step_avg:98.78ms
step:1503/1770 train_time:147482ms step_avg:98.78ms
step:1504/1770 train_time:147585ms step_avg:98.79ms
step:1505/1770 train_time:147690ms step_avg:98.79ms
step:1506/1770 train_time:147793ms step_avg:98.79ms
step:1507/1770 train_time:147897ms step_avg:98.80ms
step:1508/1770 train_time:148002ms step_avg:98.80ms
step:1509/1770 train_time:148105ms step_avg:98.80ms
step:1510/1770 train_time:148207ms step_avg:98.80ms
step:1511/1770 train_time:148313ms step_avg:98.81ms
step:1512/1770 train_time:148418ms step_avg:98.81ms
step:1513/1770 train_time:148522ms step_avg:98.82ms
step:1514/1770 train_time:148625ms step_avg:98.82ms
step:1515/1770 train_time:148728ms step_avg:98.82ms
step:1516/1770 train_time:148831ms step_avg:98.83ms
step:1517/1770 train_time:148935ms step_avg:98.83ms
step:1518/1770 train_time:149041ms step_avg:98.83ms
step:1519/1770 train_time:149142ms step_avg:98.84ms
step:1520/1770 train_time:149247ms step_avg:98.84ms
step:1521/1770 train_time:149349ms step_avg:98.84ms
step:1522/1770 train_time:149453ms step_avg:98.84ms
step:1523/1770 train_time:149558ms step_avg:98.85ms
step:1524/1770 train_time:149661ms step_avg:98.85ms
step:1525/1770 train_time:149763ms step_avg:98.85ms
step:1526/1770 train_time:149866ms step_avg:98.86ms
step:1527/1770 train_time:149969ms step_avg:98.86ms
step:1528/1770 train_time:150075ms step_avg:98.86ms
step:1529/1770 train_time:150177ms step_avg:98.87ms
step:1530/1770 train_time:150280ms step_avg:98.87ms
step:1531/1770 train_time:150383ms step_avg:98.87ms
step:1532/1770 train_time:150487ms step_avg:98.87ms
step:1533/1770 train_time:150590ms step_avg:98.88ms
step:1534/1770 train_time:150694ms step_avg:98.88ms
step:1535/1770 train_time:150798ms step_avg:98.88ms
step:1536/1770 train_time:150900ms step_avg:98.89ms
step:1537/1770 train_time:151004ms step_avg:98.89ms
step:1538/1770 train_time:151108ms step_avg:98.89ms
step:1539/1770 train_time:151210ms step_avg:98.89ms
step:1540/1770 train_time:151317ms step_avg:98.90ms
step:1541/1770 train_time:151421ms step_avg:98.90ms
step:1542/1770 train_time:151524ms step_avg:98.91ms
step:1543/1770 train_time:151626ms step_avg:98.91ms
step:1544/1770 train_time:151732ms step_avg:98.91ms
step:1545/1770 train_time:151835ms step_avg:98.92ms
step:1546/1770 train_time:151939ms step_avg:98.92ms
step:1547/1770 train_time:152041ms step_avg:98.92ms
step:1548/1770 train_time:152144ms step_avg:98.92ms
step:1549/1770 train_time:152246ms step_avg:98.93ms
step:1550/1770 train_time:152350ms step_avg:98.93ms
step:1551/1770 train_time:152454ms step_avg:98.93ms
step:1552/1770 train_time:152559ms step_avg:98.94ms
step:1553/1770 train_time:152663ms step_avg:98.94ms
step:1554/1770 train_time:152766ms step_avg:98.94ms
step:1555/1770 train_time:152870ms step_avg:98.94ms
step:1556/1770 train_time:152973ms step_avg:98.95ms
step:1557/1770 train_time:153076ms step_avg:98.95ms
step:1558/1770 train_time:153180ms step_avg:98.95ms
step:1559/1770 train_time:153284ms step_avg:98.96ms
step:1560/1770 train_time:153386ms step_avg:98.96ms
step:1561/1770 train_time:153491ms step_avg:98.96ms
step:1562/1770 train_time:153594ms step_avg:98.97ms
step:1563/1770 train_time:153697ms step_avg:98.97ms
step:1564/1770 train_time:153800ms step_avg:98.97ms
step:1565/1770 train_time:153904ms step_avg:98.97ms
step:1566/1770 train_time:154006ms step_avg:98.98ms
step:1567/1770 train_time:154110ms step_avg:98.98ms
step:1568/1770 train_time:154213ms step_avg:98.98ms
step:1569/1770 train_time:154320ms step_avg:98.99ms
step:1570/1770 train_time:154423ms step_avg:98.99ms
step:1571/1770 train_time:154525ms step_avg:98.99ms
step:1572/1770 train_time:154629ms step_avg:98.99ms
step:1573/1770 train_time:154734ms step_avg:99.00ms
step:1574/1770 train_time:154838ms step_avg:99.00ms
step:1575/1770 train_time:154940ms step_avg:99.00ms
step:1576/1770 train_time:155043ms step_avg:99.01ms
step:1577/1770 train_time:155147ms step_avg:99.01ms
step:1578/1770 train_time:155252ms step_avg:99.01ms
step:1579/1770 train_time:155356ms step_avg:99.02ms
step:1580/1770 train_time:155459ms step_avg:99.02ms
step:1581/1770 train_time:155565ms step_avg:99.02ms
step:1582/1770 train_time:155669ms step_avg:99.03ms
step:1583/1770 train_time:155773ms step_avg:99.03ms
step:1584/1770 train_time:155878ms step_avg:99.03ms
step:1585/1770 train_time:155982ms step_avg:99.04ms
step:1586/1770 train_time:156088ms step_avg:99.04ms
step:1587/1770 train_time:156192ms step_avg:99.04ms
step:1588/1770 train_time:156296ms step_avg:99.05ms
step:1589/1770 train_time:156402ms step_avg:99.05ms
step:1590/1770 train_time:156506ms step_avg:99.05ms
step:1591/1770 train_time:156608ms step_avg:99.06ms
step:1592/1770 train_time:156712ms step_avg:99.06ms
step:1593/1770 train_time:156816ms step_avg:99.06ms
step:1594/1770 train_time:156920ms step_avg:99.07ms
step:1595/1770 train_time:157024ms step_avg:99.07ms
step:1596/1770 train_time:157128ms step_avg:99.07ms
step:1597/1770 train_time:157230ms step_avg:99.07ms
step:1598/1770 train_time:157334ms step_avg:99.08ms
step:1599/1770 train_time:157440ms step_avg:99.08ms
step:1600/1770 train_time:157545ms step_avg:99.08ms
step:1601/1770 train_time:157649ms step_avg:99.09ms
step:1602/1770 train_time:157753ms step_avg:99.09ms
step:1603/1770 train_time:157856ms step_avg:99.09ms
step:1604/1770 train_time:157959ms step_avg:99.10ms
step:1605/1770 train_time:158062ms step_avg:99.10ms
step:1606/1770 train_time:158165ms step_avg:99.10ms
step:1607/1770 train_time:158272ms step_avg:99.11ms
step:1608/1770 train_time:158375ms step_avg:99.11ms
step:1609/1770 train_time:158479ms step_avg:99.11ms
step:1610/1770 train_time:158583ms step_avg:99.11ms
step:1611/1770 train_time:158689ms step_avg:99.12ms
step:1612/1770 train_time:158793ms step_avg:99.12ms
step:1613/1770 train_time:158896ms step_avg:99.12ms
step:1614/1770 train_time:159001ms step_avg:99.13ms
step:1615/1770 train_time:159104ms step_avg:99.13ms
step:1616/1770 train_time:159207ms step_avg:99.13ms
step:1617/1770 train_time:159312ms step_avg:99.14ms
step:1618/1770 train_time:159416ms step_avg:99.14ms
step:1619/1770 train_time:159520ms step_avg:99.14ms
step:1620/1770 train_time:159624ms step_avg:99.15ms
step:1621/1770 train_time:159728ms step_avg:99.15ms
step:1622/1770 train_time:159832ms step_avg:99.15ms
step:1623/1770 train_time:159938ms step_avg:99.16ms
step:1624/1770 train_time:160041ms step_avg:99.16ms
step:1625/1770 train_time:160144ms step_avg:99.16ms
step:1625/1770 val_loss:3.3125 train_time:160245ms step_avg:99.22ms
step:1626/1770 train_time:160267ms step_avg:99.17ms
step:1627/1770 train_time:160357ms step_avg:99.17ms
step:1628/1770 train_time:160461ms step_avg:99.17ms
step:1629/1770 train_time:160564ms step_avg:99.17ms
step:1630/1770 train_time:160667ms step_avg:99.18ms
step:1631/1770 train_time:160770ms step_avg:99.18ms
step:1632/1770 train_time:160872ms step_avg:99.18ms
step:1633/1770 train_time:160975ms step_avg:99.18ms
step:1634/1770 train_time:161077ms step_avg:99.19ms
step:1635/1770 train_time:161180ms step_avg:99.19ms
step:1636/1770 train_time:161285ms step_avg:99.19ms
step:1637/1770 train_time:161390ms step_avg:99.19ms
step:1638/1770 train_time:161493ms step_avg:99.20ms
step:1639/1770 train_time:161596ms step_avg:99.20ms
step:1640/1770 train_time:161701ms step_avg:99.20ms
step:1641/1770 train_time:161805ms step_avg:99.21ms
step:1642/1770 train_time:161908ms step_avg:99.21ms
step:1643/1770 train_time:162011ms step_avg:99.21ms
step:1644/1770 train_time:162115ms step_avg:99.21ms
step:1645/1770 train_time:162218ms step_avg:99.22ms
step:1646/1770 train_time:162323ms step_avg:99.22ms
step:1647/1770 train_time:162428ms step_avg:99.22ms
step:1648/1770 train_time:162531ms step_avg:99.22ms
step:1649/1770 train_time:162633ms step_avg:99.23ms
step:1650/1770 train_time:162736ms step_avg:99.23ms
step:1651/1770 train_time:162840ms step_avg:99.23ms
step:1652/1770 train_time:162944ms step_avg:99.24ms
step:1653/1770 train_time:163048ms step_avg:99.24ms
step:1654/1770 train_time:163155ms step_avg:99.24ms
step:1655/1770 train_time:163260ms step_avg:99.25ms
step:1656/1770 train_time:163363ms step_avg:99.25ms
step:1657/1770 train_time:163469ms step_avg:99.25ms
step:1658/1770 train_time:163572ms step_avg:99.26ms
step:1659/1770 train_time:163677ms step_avg:99.26ms
step:1660/1770 train_time:163780ms step_avg:99.26ms
step:1661/1770 train_time:163885ms step_avg:99.26ms
step:1662/1770 train_time:163989ms step_avg:99.27ms
step:1663/1770 train_time:164091ms step_avg:99.27ms
step:1664/1770 train_time:164194ms step_avg:99.27ms
step:1665/1770 train_time:164297ms step_avg:99.27ms
step:1666/1770 train_time:164401ms step_avg:99.28ms
step:1667/1770 train_time:164504ms step_avg:99.28ms
step:1668/1770 train_time:164608ms step_avg:99.28ms
step:1669/1770 train_time:164711ms step_avg:99.28ms
step:1670/1770 train_time:164814ms step_avg:99.29ms
step:1671/1770 train_time:164918ms step_avg:99.29ms
step:1672/1770 train_time:165023ms step_avg:99.29ms
step:1673/1770 train_time:165129ms step_avg:99.30ms
step:1674/1770 train_time:165232ms step_avg:99.30ms
step:1675/1770 train_time:165334ms step_avg:99.30ms
step:1676/1770 train_time:165438ms step_avg:99.30ms
step:1677/1770 train_time:165546ms step_avg:99.31ms
step:1678/1770 train_time:165649ms step_avg:99.31ms
step:1679/1770 train_time:165752ms step_avg:99.31ms
step:1680/1770 train_time:165854ms step_avg:99.31ms
step:1681/1770 train_time:165958ms step_avg:99.32ms
step:1682/1770 train_time:166064ms step_avg:99.32ms
step:1683/1770 train_time:166167ms step_avg:99.32ms
step:1684/1770 train_time:166269ms step_avg:99.32ms
step:1685/1770 train_time:166373ms step_avg:99.33ms
step:1686/1770 train_time:166476ms step_avg:99.33ms
step:1687/1770 train_time:166582ms step_avg:99.33ms
step:1688/1770 train_time:166685ms step_avg:99.34ms
step:1689/1770 train_time:166788ms step_avg:99.34ms
step:1690/1770 train_time:166891ms step_avg:99.34ms
step:1691/1770 train_time:166994ms step_avg:99.34ms
step:1692/1770 train_time:167097ms step_avg:99.34ms
step:1693/1770 train_time:167203ms step_avg:99.35ms
step:1694/1770 train_time:167306ms step_avg:99.35ms
step:1695/1770 train_time:167409ms step_avg:99.35ms
step:1696/1770 train_time:167514ms step_avg:99.36ms
step:1697/1770 train_time:167620ms step_avg:99.36ms
step:1698/1770 train_time:167724ms step_avg:99.36ms
step:1699/1770 train_time:167827ms step_avg:99.36ms
step:1700/1770 train_time:167931ms step_avg:99.37ms
step:1701/1770 train_time:168034ms step_avg:99.37ms
step:1702/1770 train_time:168137ms step_avg:99.37ms
step:1703/1770 train_time:168239ms step_avg:99.37ms
step:1704/1770 train_time:168343ms step_avg:99.38ms
step:1705/1770 train_time:168446ms step_avg:99.38ms
step:1706/1770 train_time:168549ms step_avg:99.38ms
step:1707/1770 train_time:168653ms step_avg:99.38ms
step:1708/1770 train_time:168756ms step_avg:99.39ms
step:1709/1770 train_time:168862ms step_avg:99.39ms
step:1710/1770 train_time:168970ms step_avg:99.39ms
step:1711/1770 train_time:169075ms step_avg:99.40ms
step:1712/1770 train_time:169179ms step_avg:99.40ms
step:1713/1770 train_time:169283ms step_avg:99.40ms
step:1714/1770 train_time:169387ms step_avg:99.41ms
step:1715/1770 train_time:169490ms step_avg:99.41ms
step:1716/1770 train_time:169594ms step_avg:99.41ms
step:1717/1770 train_time:169697ms step_avg:99.41ms
step:1718/1770 train_time:169803ms step_avg:99.42ms
step:1719/1770 train_time:169908ms step_avg:99.42ms
step:1720/1770 train_time:170013ms step_avg:99.42ms
step:1721/1770 train_time:170117ms step_avg:99.43ms
step:1722/1770 train_time:170225ms step_avg:99.43ms
step:1723/1770 train_time:170330ms step_avg:99.43ms
step:1724/1770 train_time:170435ms step_avg:99.44ms
step:1725/1770 train_time:170542ms step_avg:99.44ms
step:1726/1770 train_time:170648ms step_avg:99.45ms
step:1727/1770 train_time:170752ms step_avg:99.45ms
step:1728/1770 train_time:170857ms step_avg:99.45ms
step:1729/1770 train_time:170961ms step_avg:99.45ms
step:1730/1770 train_time:171066ms step_avg:99.46ms
step:1731/1770 train_time:171172ms step_avg:99.46ms
step:1732/1770 train_time:171277ms step_avg:99.46ms
step:1733/1770 train_time:171382ms step_avg:99.47ms
step:1734/1770 train_time:171486ms step_avg:99.47ms
step:1735/1770 train_time:171591ms step_avg:99.47ms
step:1736/1770 train_time:171695ms step_avg:99.48ms
step:1737/1770 train_time:171799ms step_avg:99.48ms
step:1738/1770 train_time:171903ms step_avg:99.48ms
step:1739/1770 train_time:172008ms step_avg:99.48ms
step:1740/1770 train_time:172111ms step_avg:99.49ms
step:1741/1770 train_time:172217ms step_avg:99.49ms
step:1742/1770 train_time:172325ms step_avg:99.49ms
step:1743/1770 train_time:172430ms step_avg:99.50ms
step:1744/1770 train_time:172534ms step_avg:99.50ms
step:1745/1770 train_time:172637ms step_avg:99.50ms
step:1746/1770 train_time:172745ms step_avg:99.51ms
step:1747/1770 train_time:172848ms step_avg:99.51ms
step:1748/1770 train_time:172954ms step_avg:99.51ms
step:1749/1770 train_time:173059ms step_avg:99.52ms
step:1750/1770 train_time:173163ms step_avg:99.52ms
step:1750/1770 val_loss:3.2857 train_time:173267ms step_avg:99.58ms
step:1751/1770 train_time:173288ms step_avg:99.53ms
step:1752/1770 train_time:173379ms step_avg:99.53ms
step:1753/1770 train_time:173484ms step_avg:99.53ms
step:1754/1770 train_time:173588ms step_avg:99.53ms
step:1755/1770 train_time:173692ms step_avg:99.54ms
step:1756/1770 train_time:173796ms step_avg:99.54ms
step:1757/1770 train_time:173900ms step_avg:99.54ms
step:1758/1770 train_time:174004ms step_avg:99.54ms
step:1759/1770 train_time:174108ms step_avg:99.55ms
step:1760/1770 train_time:174213ms step_avg:99.55ms
step:1761/1770 train_time:174319ms step_avg:99.55ms
step:1762/1770 train_time:174428ms step_avg:99.56ms
step:1763/1770 train_time:174531ms step_avg:99.56ms
step:1764/1770 train_time:174635ms step_avg:99.56ms
step:1765/1770 train_time:174740ms step_avg:99.57ms
step:1766/1770 train_time:174848ms step_avg:99.57ms
step:1767/1770 train_time:174951ms step_avg:99.57ms
step:1768/1770 train_time:175055ms step_avg:99.58ms
step:1769/1770 train_time:175158ms step_avg:99.58ms
step:1770/1770 train_time:175262ms step_avg:99.58ms
step:1770/1770 val_loss:3.2826 train_time:175367ms step_avg:99.64ms
peak memory allocated: 28840 MiB reserved: 32292 MiB
