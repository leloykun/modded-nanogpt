import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:24:53 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24546ms step_avg:nanms
step:2/1770 train_time:25074ms step_avg:nanms
step:3/1770 train_time:25170ms step_avg:nanms
step:4/1770 train_time:25263ms step_avg:nanms
step:5/1770 train_time:25357ms step_avg:nanms
step:6/1770 train_time:25450ms step_avg:nanms
step:7/1770 train_time:25545ms step_avg:nanms
step:8/1770 train_time:25638ms step_avg:nanms
step:9/1770 train_time:25732ms step_avg:nanms
step:10/1770 train_time:25826ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.42ms
step:14/1770 train_time:378ms step_avg:94.59ms
step:15/1770 train_time:473ms step_avg:94.50ms
step:16/1770 train_time:566ms step_avg:94.40ms
step:17/1770 train_time:661ms step_avg:94.36ms
step:18/1770 train_time:755ms step_avg:94.34ms
step:19/1770 train_time:849ms step_avg:94.30ms
step:20/1770 train_time:943ms step_avg:94.30ms
step:21/1770 train_time:1036ms step_avg:94.22ms
step:22/1770 train_time:1131ms step_avg:94.21ms
step:23/1770 train_time:1224ms step_avg:94.18ms
step:24/1770 train_time:1319ms step_avg:94.18ms
step:25/1770 train_time:1413ms step_avg:94.21ms
step:26/1770 train_time:1507ms step_avg:94.17ms
step:27/1770 train_time:1601ms step_avg:94.17ms
step:28/1770 train_time:1695ms step_avg:94.17ms
step:29/1770 train_time:1789ms step_avg:94.15ms
step:30/1770 train_time:1883ms step_avg:94.14ms
step:31/1770 train_time:1978ms step_avg:94.17ms
step:32/1770 train_time:2071ms step_avg:94.16ms
step:33/1770 train_time:2166ms step_avg:94.15ms
step:34/1770 train_time:2260ms step_avg:94.16ms
step:35/1770 train_time:2354ms step_avg:94.17ms
step:36/1770 train_time:2448ms step_avg:94.17ms
step:37/1770 train_time:2542ms step_avg:94.14ms
step:38/1770 train_time:2637ms step_avg:94.16ms
step:39/1770 train_time:2731ms step_avg:94.17ms
step:40/1770 train_time:2825ms step_avg:94.16ms
step:41/1770 train_time:2919ms step_avg:94.15ms
step:42/1770 train_time:3013ms step_avg:94.16ms
step:43/1770 train_time:3108ms step_avg:94.17ms
step:44/1770 train_time:3202ms step_avg:94.19ms
step:45/1770 train_time:3296ms step_avg:94.18ms
step:46/1770 train_time:3390ms step_avg:94.17ms
step:47/1770 train_time:3484ms step_avg:94.16ms
step:48/1770 train_time:3578ms step_avg:94.16ms
step:49/1770 train_time:3673ms step_avg:94.17ms
step:50/1770 train_time:3767ms step_avg:94.16ms
step:51/1770 train_time:3861ms step_avg:94.17ms
step:52/1770 train_time:3956ms step_avg:94.18ms
step:53/1770 train_time:4049ms step_avg:94.17ms
step:54/1770 train_time:4143ms step_avg:94.16ms
step:55/1770 train_time:4237ms step_avg:94.17ms
step:56/1770 train_time:4332ms step_avg:94.18ms
step:57/1770 train_time:4426ms step_avg:94.18ms
step:58/1770 train_time:4520ms step_avg:94.17ms
step:59/1770 train_time:4614ms step_avg:94.17ms
step:60/1770 train_time:4709ms step_avg:94.17ms
step:61/1770 train_time:4802ms step_avg:94.17ms
step:62/1770 train_time:4897ms step_avg:94.17ms
step:63/1770 train_time:4991ms step_avg:94.17ms
step:64/1770 train_time:5085ms step_avg:94.17ms
step:65/1770 train_time:5180ms step_avg:94.17ms
step:66/1770 train_time:5274ms step_avg:94.18ms
step:67/1770 train_time:5368ms step_avg:94.18ms
step:68/1770 train_time:5463ms step_avg:94.19ms
step:69/1770 train_time:5557ms step_avg:94.19ms
step:70/1770 train_time:5651ms step_avg:94.19ms
step:71/1770 train_time:5745ms step_avg:94.18ms
step:72/1770 train_time:5839ms step_avg:94.18ms
step:73/1770 train_time:5934ms step_avg:94.19ms
step:74/1770 train_time:6027ms step_avg:94.18ms
step:75/1770 train_time:6121ms step_avg:94.17ms
step:76/1770 train_time:6216ms step_avg:94.18ms
step:77/1770 train_time:6310ms step_avg:94.18ms
step:78/1770 train_time:6404ms step_avg:94.18ms
step:79/1770 train_time:6499ms step_avg:94.18ms
step:80/1770 train_time:6593ms step_avg:94.18ms
step:81/1770 train_time:6686ms step_avg:94.18ms
step:82/1770 train_time:6780ms step_avg:94.17ms
step:83/1770 train_time:6875ms step_avg:94.18ms
step:84/1770 train_time:6970ms step_avg:94.18ms
step:85/1770 train_time:7063ms step_avg:94.17ms
step:86/1770 train_time:7157ms step_avg:94.17ms
step:87/1770 train_time:7251ms step_avg:94.17ms
step:88/1770 train_time:7345ms step_avg:94.16ms
step:89/1770 train_time:7440ms step_avg:94.18ms
step:90/1770 train_time:7533ms step_avg:94.16ms
step:91/1770 train_time:7627ms step_avg:94.16ms
step:92/1770 train_time:7721ms step_avg:94.16ms
step:93/1770 train_time:7815ms step_avg:94.16ms
step:94/1770 train_time:7909ms step_avg:94.15ms
step:95/1770 train_time:8002ms step_avg:94.14ms
step:96/1770 train_time:8096ms step_avg:94.14ms
step:97/1770 train_time:8191ms step_avg:94.14ms
step:98/1770 train_time:8284ms step_avg:94.14ms
step:99/1770 train_time:8378ms step_avg:94.14ms
step:100/1770 train_time:8472ms step_avg:94.14ms
step:101/1770 train_time:8566ms step_avg:94.13ms
step:102/1770 train_time:8660ms step_avg:94.13ms
step:103/1770 train_time:8754ms step_avg:94.13ms
step:104/1770 train_time:8848ms step_avg:94.13ms
step:105/1770 train_time:8943ms step_avg:94.14ms
step:106/1770 train_time:9036ms step_avg:94.13ms
step:107/1770 train_time:9130ms step_avg:94.13ms
step:108/1770 train_time:9224ms step_avg:94.12ms
step:109/1770 train_time:9318ms step_avg:94.12ms
step:110/1770 train_time:9413ms step_avg:94.13ms
step:111/1770 train_time:9506ms step_avg:94.12ms
step:112/1770 train_time:9600ms step_avg:94.12ms
step:113/1770 train_time:9695ms step_avg:94.12ms
step:114/1770 train_time:9788ms step_avg:94.12ms
step:115/1770 train_time:9882ms step_avg:94.12ms
step:116/1770 train_time:9976ms step_avg:94.11ms
step:117/1770 train_time:10070ms step_avg:94.11ms
step:118/1770 train_time:10164ms step_avg:94.11ms
step:119/1770 train_time:10258ms step_avg:94.11ms
step:120/1770 train_time:10352ms step_avg:94.11ms
step:121/1770 train_time:10446ms step_avg:94.11ms
step:122/1770 train_time:10540ms step_avg:94.11ms
step:123/1770 train_time:10637ms step_avg:94.13ms
step:124/1770 train_time:10728ms step_avg:94.10ms
step:125/1770 train_time:10822ms step_avg:94.10ms
step:125/1770 val_loss:4.6735 train_time:10915ms step_avg:94.91ms
step:126/1770 train_time:10937ms step_avg:94.29ms
step:127/1770 train_time:11022ms step_avg:94.20ms
step:128/1770 train_time:11122ms step_avg:94.25ms
step:129/1770 train_time:11216ms step_avg:94.26ms
step:130/1770 train_time:11310ms step_avg:94.25ms
step:131/1770 train_time:11404ms step_avg:94.25ms
step:132/1770 train_time:11498ms step_avg:94.24ms
step:133/1770 train_time:11592ms step_avg:94.24ms
step:134/1770 train_time:11687ms step_avg:94.25ms
step:135/1770 train_time:11780ms step_avg:94.24ms
step:136/1770 train_time:11875ms step_avg:94.25ms
step:137/1770 train_time:11970ms step_avg:94.25ms
step:138/1770 train_time:12064ms step_avg:94.25ms
step:139/1770 train_time:12160ms step_avg:94.26ms
step:140/1770 train_time:12255ms step_avg:94.27ms
step:141/1770 train_time:12349ms step_avg:94.27ms
step:142/1770 train_time:12444ms step_avg:94.27ms
step:143/1770 train_time:12538ms step_avg:94.27ms
step:144/1770 train_time:12633ms step_avg:94.28ms
step:145/1770 train_time:12728ms step_avg:94.28ms
step:146/1770 train_time:12822ms step_avg:94.28ms
step:147/1770 train_time:12916ms step_avg:94.28ms
step:148/1770 train_time:13011ms step_avg:94.29ms
step:149/1770 train_time:13106ms step_avg:94.29ms
step:150/1770 train_time:13201ms step_avg:94.29ms
step:151/1770 train_time:13296ms step_avg:94.29ms
step:152/1770 train_time:13391ms step_avg:94.30ms
step:153/1770 train_time:13485ms step_avg:94.30ms
step:154/1770 train_time:13580ms step_avg:94.30ms
step:155/1770 train_time:13674ms step_avg:94.31ms
step:156/1770 train_time:13769ms step_avg:94.31ms
step:157/1770 train_time:13863ms step_avg:94.31ms
step:158/1770 train_time:13958ms step_avg:94.31ms
step:159/1770 train_time:14053ms step_avg:94.31ms
step:160/1770 train_time:14147ms step_avg:94.32ms
step:161/1770 train_time:14242ms step_avg:94.32ms
step:162/1770 train_time:14336ms step_avg:94.32ms
step:163/1770 train_time:14432ms step_avg:94.32ms
step:164/1770 train_time:14525ms step_avg:94.32ms
step:165/1770 train_time:14620ms step_avg:94.32ms
step:166/1770 train_time:14715ms step_avg:94.33ms
step:167/1770 train_time:14810ms step_avg:94.33ms
step:168/1770 train_time:14905ms step_avg:94.33ms
step:169/1770 train_time:14999ms step_avg:94.34ms
step:170/1770 train_time:15094ms step_avg:94.34ms
step:171/1770 train_time:15189ms step_avg:94.34ms
step:172/1770 train_time:15283ms step_avg:94.34ms
step:173/1770 train_time:15378ms step_avg:94.34ms
step:174/1770 train_time:15473ms step_avg:94.35ms
step:175/1770 train_time:15568ms step_avg:94.35ms
step:176/1770 train_time:15662ms step_avg:94.35ms
step:177/1770 train_time:15756ms step_avg:94.35ms
step:178/1770 train_time:15852ms step_avg:94.36ms
step:179/1770 train_time:15946ms step_avg:94.36ms
step:180/1770 train_time:16041ms step_avg:94.36ms
step:181/1770 train_time:16135ms step_avg:94.36ms
step:182/1770 train_time:16231ms step_avg:94.37ms
step:183/1770 train_time:16326ms step_avg:94.37ms
step:184/1770 train_time:16421ms step_avg:94.37ms
step:185/1770 train_time:16516ms step_avg:94.38ms
step:186/1770 train_time:16611ms step_avg:94.38ms
step:187/1770 train_time:16706ms step_avg:94.38ms
step:188/1770 train_time:16801ms step_avg:94.39ms
step:189/1770 train_time:16895ms step_avg:94.39ms
step:190/1770 train_time:16990ms step_avg:94.39ms
step:191/1770 train_time:17085ms step_avg:94.39ms
step:192/1770 train_time:17180ms step_avg:94.40ms
step:193/1770 train_time:17275ms step_avg:94.40ms
step:194/1770 train_time:17370ms step_avg:94.40ms
step:195/1770 train_time:17464ms step_avg:94.40ms
step:196/1770 train_time:17559ms step_avg:94.40ms
step:197/1770 train_time:17654ms step_avg:94.40ms
step:198/1770 train_time:17748ms step_avg:94.41ms
step:199/1770 train_time:17843ms step_avg:94.41ms
step:200/1770 train_time:17937ms step_avg:94.41ms
step:201/1770 train_time:18033ms step_avg:94.41ms
step:202/1770 train_time:18128ms step_avg:94.41ms
step:203/1770 train_time:18222ms step_avg:94.41ms
step:204/1770 train_time:18317ms step_avg:94.42ms
step:205/1770 train_time:18412ms step_avg:94.42ms
step:206/1770 train_time:18506ms step_avg:94.42ms
step:207/1770 train_time:18601ms step_avg:94.42ms
step:208/1770 train_time:18695ms step_avg:94.42ms
step:209/1770 train_time:18790ms step_avg:94.42ms
step:210/1770 train_time:18884ms step_avg:94.42ms
step:211/1770 train_time:18979ms step_avg:94.42ms
step:212/1770 train_time:19074ms step_avg:94.42ms
step:213/1770 train_time:19169ms step_avg:94.43ms
step:214/1770 train_time:19264ms step_avg:94.43ms
step:215/1770 train_time:19358ms step_avg:94.43ms
step:216/1770 train_time:19453ms step_avg:94.43ms
step:217/1770 train_time:19548ms step_avg:94.43ms
step:218/1770 train_time:19642ms step_avg:94.43ms
step:219/1770 train_time:19736ms step_avg:94.43ms
step:220/1770 train_time:19832ms step_avg:94.44ms
step:221/1770 train_time:19926ms step_avg:94.44ms
step:222/1770 train_time:20021ms step_avg:94.44ms
step:223/1770 train_time:20116ms step_avg:94.44ms
step:224/1770 train_time:20211ms step_avg:94.44ms
step:225/1770 train_time:20306ms step_avg:94.44ms
step:226/1770 train_time:20400ms step_avg:94.44ms
step:227/1770 train_time:20495ms step_avg:94.45ms
step:228/1770 train_time:20590ms step_avg:94.45ms
step:229/1770 train_time:20684ms step_avg:94.45ms
step:230/1770 train_time:20779ms step_avg:94.45ms
step:231/1770 train_time:20874ms step_avg:94.45ms
step:232/1770 train_time:20968ms step_avg:94.45ms
step:233/1770 train_time:21063ms step_avg:94.45ms
step:234/1770 train_time:21157ms step_avg:94.45ms
step:235/1770 train_time:21252ms step_avg:94.46ms
step:236/1770 train_time:21347ms step_avg:94.46ms
step:237/1770 train_time:21442ms step_avg:94.46ms
step:238/1770 train_time:21537ms step_avg:94.46ms
step:239/1770 train_time:21632ms step_avg:94.46ms
step:240/1770 train_time:21727ms step_avg:94.46ms
step:241/1770 train_time:21821ms step_avg:94.46ms
step:242/1770 train_time:21916ms step_avg:94.47ms
step:243/1770 train_time:22011ms step_avg:94.47ms
step:244/1770 train_time:22105ms step_avg:94.47ms
step:245/1770 train_time:22200ms step_avg:94.47ms
step:246/1770 train_time:22295ms step_avg:94.47ms
step:247/1770 train_time:22391ms step_avg:94.48ms
step:248/1770 train_time:22485ms step_avg:94.47ms
step:249/1770 train_time:22579ms step_avg:94.47ms
step:250/1770 train_time:22675ms step_avg:94.48ms
step:250/1770 val_loss:4.1097 train_time:22768ms step_avg:94.87ms
step:251/1770 train_time:22790ms step_avg:94.56ms
step:252/1770 train_time:22876ms step_avg:94.53ms
step:253/1770 train_time:22975ms step_avg:94.55ms
step:254/1770 train_time:23070ms step_avg:94.55ms
step:255/1770 train_time:23165ms step_avg:94.55ms
step:256/1770 train_time:23260ms step_avg:94.55ms
step:257/1770 train_time:23354ms step_avg:94.55ms
step:258/1770 train_time:23449ms step_avg:94.55ms
step:259/1770 train_time:23543ms step_avg:94.55ms
step:260/1770 train_time:23638ms step_avg:94.55ms
step:261/1770 train_time:23732ms step_avg:94.55ms
step:262/1770 train_time:23827ms step_avg:94.55ms
step:263/1770 train_time:23924ms step_avg:94.56ms
step:264/1770 train_time:24020ms step_avg:94.57ms
step:265/1770 train_time:24115ms step_avg:94.57ms
step:266/1770 train_time:24210ms step_avg:94.57ms
step:267/1770 train_time:24306ms step_avg:94.58ms
step:268/1770 train_time:24401ms step_avg:94.58ms
step:269/1770 train_time:24496ms step_avg:94.58ms
step:270/1770 train_time:24591ms step_avg:94.58ms
step:271/1770 train_time:24686ms step_avg:94.58ms
step:272/1770 train_time:24781ms step_avg:94.58ms
step:273/1770 train_time:24876ms step_avg:94.59ms
step:274/1770 train_time:24971ms step_avg:94.59ms
step:275/1770 train_time:25067ms step_avg:94.59ms
step:276/1770 train_time:25163ms step_avg:94.60ms
step:277/1770 train_time:25259ms step_avg:94.60ms
step:278/1770 train_time:25354ms step_avg:94.60ms
step:279/1770 train_time:25449ms step_avg:94.61ms
step:280/1770 train_time:25545ms step_avg:94.61ms
step:281/1770 train_time:25640ms step_avg:94.61ms
step:282/1770 train_time:25735ms step_avg:94.61ms
step:283/1770 train_time:25829ms step_avg:94.61ms
step:284/1770 train_time:25925ms step_avg:94.62ms
step:285/1770 train_time:26020ms step_avg:94.62ms
step:286/1770 train_time:26115ms step_avg:94.62ms
step:287/1770 train_time:26210ms step_avg:94.62ms
step:288/1770 train_time:26306ms step_avg:94.63ms
step:289/1770 train_time:26402ms step_avg:94.63ms
step:290/1770 train_time:26497ms step_avg:94.63ms
step:291/1770 train_time:26592ms step_avg:94.63ms
step:292/1770 train_time:26688ms step_avg:94.64ms
step:293/1770 train_time:26784ms step_avg:94.64ms
step:294/1770 train_time:26879ms step_avg:94.65ms
step:295/1770 train_time:26974ms step_avg:94.65ms
step:296/1770 train_time:27070ms step_avg:94.65ms
step:297/1770 train_time:27166ms step_avg:94.66ms
step:298/1770 train_time:27262ms step_avg:94.66ms
step:299/1770 train_time:27357ms step_avg:94.66ms
step:300/1770 train_time:27452ms step_avg:94.66ms
step:301/1770 train_time:27547ms step_avg:94.66ms
step:302/1770 train_time:27643ms step_avg:94.67ms
step:303/1770 train_time:27738ms step_avg:94.67ms
step:304/1770 train_time:27833ms step_avg:94.67ms
step:305/1770 train_time:27928ms step_avg:94.67ms
step:306/1770 train_time:28023ms step_avg:94.67ms
step:307/1770 train_time:28118ms step_avg:94.67ms
step:308/1770 train_time:28214ms step_avg:94.68ms
step:309/1770 train_time:28309ms step_avg:94.68ms
step:310/1770 train_time:28405ms step_avg:94.68ms
step:311/1770 train_time:28501ms step_avg:94.69ms
step:312/1770 train_time:28595ms step_avg:94.69ms
step:313/1770 train_time:28691ms step_avg:94.69ms
step:314/1770 train_time:28786ms step_avg:94.69ms
step:315/1770 train_time:28882ms step_avg:94.69ms
step:316/1770 train_time:28976ms step_avg:94.69ms
step:317/1770 train_time:29071ms step_avg:94.69ms
step:318/1770 train_time:29166ms step_avg:94.70ms
step:319/1770 train_time:29262ms step_avg:94.70ms
step:320/1770 train_time:29357ms step_avg:94.70ms
step:321/1770 train_time:29452ms step_avg:94.70ms
step:322/1770 train_time:29547ms step_avg:94.70ms
step:323/1770 train_time:29643ms step_avg:94.71ms
step:324/1770 train_time:29739ms step_avg:94.71ms
step:325/1770 train_time:29834ms step_avg:94.71ms
step:326/1770 train_time:29929ms step_avg:94.71ms
step:327/1770 train_time:30025ms step_avg:94.72ms
step:328/1770 train_time:30120ms step_avg:94.72ms
step:329/1770 train_time:30215ms step_avg:94.72ms
step:330/1770 train_time:30310ms step_avg:94.72ms
step:331/1770 train_time:30405ms step_avg:94.72ms
step:332/1770 train_time:30500ms step_avg:94.72ms
step:333/1770 train_time:30595ms step_avg:94.72ms
step:334/1770 train_time:30690ms step_avg:94.72ms
step:335/1770 train_time:30787ms step_avg:94.73ms
step:336/1770 train_time:30883ms step_avg:94.73ms
step:337/1770 train_time:30978ms step_avg:94.73ms
step:338/1770 train_time:31073ms step_avg:94.73ms
step:339/1770 train_time:31168ms step_avg:94.74ms
step:340/1770 train_time:31263ms step_avg:94.74ms
step:341/1770 train_time:31358ms step_avg:94.74ms
step:342/1770 train_time:31454ms step_avg:94.74ms
step:343/1770 train_time:31549ms step_avg:94.74ms
step:344/1770 train_time:31644ms step_avg:94.74ms
step:345/1770 train_time:31740ms step_avg:94.75ms
step:346/1770 train_time:31835ms step_avg:94.75ms
step:347/1770 train_time:31931ms step_avg:94.75ms
step:348/1770 train_time:32027ms step_avg:94.75ms
step:349/1770 train_time:32122ms step_avg:94.76ms
step:350/1770 train_time:32217ms step_avg:94.76ms
step:351/1770 train_time:32312ms step_avg:94.76ms
step:352/1770 train_time:32408ms step_avg:94.76ms
step:353/1770 train_time:32503ms step_avg:94.76ms
step:354/1770 train_time:32598ms step_avg:94.76ms
step:355/1770 train_time:32692ms step_avg:94.76ms
step:356/1770 train_time:32788ms step_avg:94.76ms
step:357/1770 train_time:32883ms step_avg:94.76ms
step:358/1770 train_time:32978ms step_avg:94.76ms
step:359/1770 train_time:33074ms step_avg:94.77ms
step:360/1770 train_time:33169ms step_avg:94.77ms
step:361/1770 train_time:33265ms step_avg:94.77ms
step:362/1770 train_time:33360ms step_avg:94.77ms
step:363/1770 train_time:33454ms step_avg:94.77ms
step:364/1770 train_time:33549ms step_avg:94.77ms
step:365/1770 train_time:33645ms step_avg:94.77ms
step:366/1770 train_time:33740ms step_avg:94.77ms
step:367/1770 train_time:33835ms step_avg:94.77ms
step:368/1770 train_time:33930ms step_avg:94.78ms
step:369/1770 train_time:34026ms step_avg:94.78ms
step:370/1770 train_time:34121ms step_avg:94.78ms
step:371/1770 train_time:34216ms step_avg:94.78ms
step:372/1770 train_time:34311ms step_avg:94.78ms
step:373/1770 train_time:34407ms step_avg:94.78ms
step:374/1770 train_time:34502ms step_avg:94.79ms
step:375/1770 train_time:34597ms step_avg:94.79ms
step:375/1770 val_loss:3.9039 train_time:34690ms step_avg:95.04ms
step:376/1770 train_time:34711ms step_avg:94.84ms
step:377/1770 train_time:34798ms step_avg:94.82ms
step:378/1770 train_time:34897ms step_avg:94.83ms
step:379/1770 train_time:34993ms step_avg:94.83ms
step:380/1770 train_time:35088ms step_avg:94.83ms
step:381/1770 train_time:35183ms step_avg:94.83ms
step:382/1770 train_time:35277ms step_avg:94.83ms
step:383/1770 train_time:35372ms step_avg:94.83ms
step:384/1770 train_time:35467ms step_avg:94.83ms
step:385/1770 train_time:35561ms step_avg:94.83ms
step:386/1770 train_time:35656ms step_avg:94.83ms
step:387/1770 train_time:35751ms step_avg:94.83ms
step:388/1770 train_time:35846ms step_avg:94.83ms
step:389/1770 train_time:35942ms step_avg:94.83ms
step:390/1770 train_time:36039ms step_avg:94.84ms
step:391/1770 train_time:36134ms step_avg:94.84ms
step:392/1770 train_time:36229ms step_avg:94.84ms
step:393/1770 train_time:36324ms step_avg:94.84ms
step:394/1770 train_time:36418ms step_avg:94.84ms
step:395/1770 train_time:36514ms step_avg:94.84ms
step:396/1770 train_time:36611ms step_avg:94.85ms
step:397/1770 train_time:36707ms step_avg:94.85ms
step:398/1770 train_time:36804ms step_avg:94.86ms
step:399/1770 train_time:36901ms step_avg:94.86ms
step:400/1770 train_time:36999ms step_avg:94.87ms
step:401/1770 train_time:37096ms step_avg:94.87ms
step:402/1770 train_time:37193ms step_avg:94.88ms
step:403/1770 train_time:37290ms step_avg:94.89ms
step:404/1770 train_time:37387ms step_avg:94.89ms
step:405/1770 train_time:37483ms step_avg:94.89ms
step:406/1770 train_time:37580ms step_avg:94.90ms
step:407/1770 train_time:37676ms step_avg:94.90ms
step:408/1770 train_time:37774ms step_avg:94.91ms
step:409/1770 train_time:37871ms step_avg:94.92ms
step:410/1770 train_time:37968ms step_avg:94.92ms
step:411/1770 train_time:38065ms step_avg:94.92ms
step:412/1770 train_time:38162ms step_avg:94.93ms
step:413/1770 train_time:38260ms step_avg:94.94ms
step:414/1770 train_time:38357ms step_avg:94.94ms
step:415/1770 train_time:38454ms step_avg:94.95ms
step:416/1770 train_time:38551ms step_avg:94.95ms
step:417/1770 train_time:38648ms step_avg:94.96ms
step:418/1770 train_time:38744ms step_avg:94.96ms
step:419/1770 train_time:38842ms step_avg:94.97ms
step:420/1770 train_time:38939ms step_avg:94.97ms
step:421/1770 train_time:39036ms step_avg:94.98ms
step:422/1770 train_time:39134ms step_avg:94.99ms
step:423/1770 train_time:39232ms step_avg:94.99ms
step:424/1770 train_time:39330ms step_avg:95.00ms
step:425/1770 train_time:39427ms step_avg:95.00ms
step:426/1770 train_time:39524ms step_avg:95.01ms
step:427/1770 train_time:39620ms step_avg:95.01ms
step:428/1770 train_time:39717ms step_avg:95.02ms
step:429/1770 train_time:39814ms step_avg:95.02ms
step:430/1770 train_time:39911ms step_avg:95.03ms
step:431/1770 train_time:40008ms step_avg:95.03ms
step:432/1770 train_time:40105ms step_avg:95.03ms
step:433/1770 train_time:40201ms step_avg:95.04ms
step:434/1770 train_time:40298ms step_avg:95.04ms
step:435/1770 train_time:40396ms step_avg:95.05ms
step:436/1770 train_time:40495ms step_avg:95.06ms
step:437/1770 train_time:40593ms step_avg:95.07ms
step:438/1770 train_time:40691ms step_avg:95.07ms
step:439/1770 train_time:40788ms step_avg:95.08ms
step:440/1770 train_time:40884ms step_avg:95.08ms
step:441/1770 train_time:40981ms step_avg:95.08ms
step:442/1770 train_time:41078ms step_avg:95.09ms
step:443/1770 train_time:41176ms step_avg:95.09ms
step:444/1770 train_time:41273ms step_avg:95.10ms
step:445/1770 train_time:41370ms step_avg:95.10ms
step:446/1770 train_time:41468ms step_avg:95.11ms
step:447/1770 train_time:41564ms step_avg:95.11ms
step:448/1770 train_time:41661ms step_avg:95.12ms
step:449/1770 train_time:41758ms step_avg:95.12ms
step:450/1770 train_time:41856ms step_avg:95.13ms
step:451/1770 train_time:41954ms step_avg:95.13ms
step:452/1770 train_time:42051ms step_avg:95.14ms
step:453/1770 train_time:42149ms step_avg:95.14ms
step:454/1770 train_time:42246ms step_avg:95.15ms
step:455/1770 train_time:42342ms step_avg:95.15ms
step:456/1770 train_time:42439ms step_avg:95.15ms
step:457/1770 train_time:42536ms step_avg:95.16ms
step:458/1770 train_time:42633ms step_avg:95.16ms
step:459/1770 train_time:42731ms step_avg:95.17ms
step:460/1770 train_time:42828ms step_avg:95.17ms
step:461/1770 train_time:42924ms step_avg:95.18ms
step:462/1770 train_time:43021ms step_avg:95.18ms
step:463/1770 train_time:43118ms step_avg:95.18ms
step:464/1770 train_time:43215ms step_avg:95.19ms
step:465/1770 train_time:43313ms step_avg:95.19ms
step:466/1770 train_time:43410ms step_avg:95.20ms
step:467/1770 train_time:43508ms step_avg:95.20ms
step:468/1770 train_time:43604ms step_avg:95.21ms
step:469/1770 train_time:43700ms step_avg:95.21ms
step:470/1770 train_time:43798ms step_avg:95.21ms
step:471/1770 train_time:43895ms step_avg:95.22ms
step:472/1770 train_time:43993ms step_avg:95.22ms
step:473/1770 train_time:44091ms step_avg:95.23ms
step:474/1770 train_time:44188ms step_avg:95.23ms
step:475/1770 train_time:44285ms step_avg:95.24ms
step:476/1770 train_time:44383ms step_avg:95.24ms
step:477/1770 train_time:44480ms step_avg:95.25ms
step:478/1770 train_time:44577ms step_avg:95.25ms
step:479/1770 train_time:44675ms step_avg:95.26ms
step:480/1770 train_time:44773ms step_avg:95.26ms
step:481/1770 train_time:44871ms step_avg:95.27ms
step:482/1770 train_time:44966ms step_avg:95.27ms
step:483/1770 train_time:45062ms step_avg:95.27ms
step:484/1770 train_time:45159ms step_avg:95.27ms
step:485/1770 train_time:45257ms step_avg:95.28ms
step:486/1770 train_time:45355ms step_avg:95.28ms
step:487/1770 train_time:45453ms step_avg:95.29ms
step:488/1770 train_time:45550ms step_avg:95.29ms
step:489/1770 train_time:45647ms step_avg:95.30ms
step:490/1770 train_time:45743ms step_avg:95.30ms
step:491/1770 train_time:45840ms step_avg:95.30ms
step:492/1770 train_time:45938ms step_avg:95.31ms
step:493/1770 train_time:46035ms step_avg:95.31ms
step:494/1770 train_time:46133ms step_avg:95.32ms
step:495/1770 train_time:46230ms step_avg:95.32ms
step:496/1770 train_time:46327ms step_avg:95.32ms
step:497/1770 train_time:46424ms step_avg:95.33ms
step:498/1770 train_time:46521ms step_avg:95.33ms
step:499/1770 train_time:46618ms step_avg:95.33ms
step:500/1770 train_time:46715ms step_avg:95.34ms
step:500/1770 val_loss:3.7603 train_time:46812ms step_avg:95.53ms
step:501/1770 train_time:46832ms step_avg:95.38ms
step:502/1770 train_time:46921ms step_avg:95.37ms
step:503/1770 train_time:47020ms step_avg:95.37ms
step:504/1770 train_time:47117ms step_avg:95.38ms
step:505/1770 train_time:47213ms step_avg:95.38ms
step:506/1770 train_time:47310ms step_avg:95.38ms
step:507/1770 train_time:47407ms step_avg:95.39ms
step:508/1770 train_time:47503ms step_avg:95.39ms
step:509/1770 train_time:47600ms step_avg:95.39ms
step:510/1770 train_time:47696ms step_avg:95.39ms
step:511/1770 train_time:47793ms step_avg:95.40ms
step:512/1770 train_time:47891ms step_avg:95.40ms
step:513/1770 train_time:47989ms step_avg:95.40ms
step:514/1770 train_time:48087ms step_avg:95.41ms
step:515/1770 train_time:48185ms step_avg:95.42ms
step:516/1770 train_time:48283ms step_avg:95.42ms
step:517/1770 train_time:48380ms step_avg:95.42ms
step:518/1770 train_time:48477ms step_avg:95.43ms
step:519/1770 train_time:48573ms step_avg:95.43ms
step:520/1770 train_time:48670ms step_avg:95.43ms
step:521/1770 train_time:48767ms step_avg:95.43ms
step:522/1770 train_time:48864ms step_avg:95.44ms
step:523/1770 train_time:48962ms step_avg:95.44ms
step:524/1770 train_time:49059ms step_avg:95.45ms
step:525/1770 train_time:49156ms step_avg:95.45ms
step:526/1770 train_time:49252ms step_avg:95.45ms
step:527/1770 train_time:49351ms step_avg:95.46ms
step:528/1770 train_time:49449ms step_avg:95.46ms
step:529/1770 train_time:49546ms step_avg:95.47ms
step:530/1770 train_time:49644ms step_avg:95.47ms
step:531/1770 train_time:49742ms step_avg:95.47ms
step:532/1770 train_time:49839ms step_avg:95.48ms
step:533/1770 train_time:49936ms step_avg:95.48ms
step:534/1770 train_time:50033ms step_avg:95.48ms
step:535/1770 train_time:50131ms step_avg:95.49ms
step:536/1770 train_time:50228ms step_avg:95.49ms
step:537/1770 train_time:50327ms step_avg:95.50ms
step:538/1770 train_time:50424ms step_avg:95.50ms
step:539/1770 train_time:50522ms step_avg:95.50ms
step:540/1770 train_time:50619ms step_avg:95.51ms
step:541/1770 train_time:50716ms step_avg:95.51ms
step:542/1770 train_time:50813ms step_avg:95.51ms
step:543/1770 train_time:50910ms step_avg:95.52ms
step:544/1770 train_time:51007ms step_avg:95.52ms
step:545/1770 train_time:51106ms step_avg:95.52ms
step:546/1770 train_time:51204ms step_avg:95.53ms
step:547/1770 train_time:51302ms step_avg:95.53ms
step:548/1770 train_time:51399ms step_avg:95.54ms
step:549/1770 train_time:51496ms step_avg:95.54ms
step:550/1770 train_time:51593ms step_avg:95.54ms
step:551/1770 train_time:51691ms step_avg:95.55ms
step:552/1770 train_time:51788ms step_avg:95.55ms
step:553/1770 train_time:51886ms step_avg:95.55ms
step:554/1770 train_time:51983ms step_avg:95.56ms
step:555/1770 train_time:52081ms step_avg:95.56ms
step:556/1770 train_time:52178ms step_avg:95.56ms
step:557/1770 train_time:52275ms step_avg:95.57ms
step:558/1770 train_time:52372ms step_avg:95.57ms
step:559/1770 train_time:52470ms step_avg:95.57ms
step:560/1770 train_time:52568ms step_avg:95.58ms
step:561/1770 train_time:52666ms step_avg:95.58ms
step:562/1770 train_time:52763ms step_avg:95.59ms
step:563/1770 train_time:52860ms step_avg:95.59ms
step:564/1770 train_time:52957ms step_avg:95.59ms
step:565/1770 train_time:53054ms step_avg:95.59ms
step:566/1770 train_time:53151ms step_avg:95.60ms
step:567/1770 train_time:53248ms step_avg:95.60ms
step:568/1770 train_time:53346ms step_avg:95.60ms
step:569/1770 train_time:53444ms step_avg:95.61ms
step:570/1770 train_time:53542ms step_avg:95.61ms
step:571/1770 train_time:53642ms step_avg:95.62ms
step:572/1770 train_time:53738ms step_avg:95.62ms
step:573/1770 train_time:53835ms step_avg:95.62ms
step:574/1770 train_time:53932ms step_avg:95.62ms
step:575/1770 train_time:54030ms step_avg:95.63ms
step:576/1770 train_time:54128ms step_avg:95.63ms
step:577/1770 train_time:54225ms step_avg:95.64ms
step:578/1770 train_time:54323ms step_avg:95.64ms
step:579/1770 train_time:54420ms step_avg:95.64ms
step:580/1770 train_time:54517ms step_avg:95.64ms
step:581/1770 train_time:54615ms step_avg:95.65ms
step:582/1770 train_time:54712ms step_avg:95.65ms
step:583/1770 train_time:54810ms step_avg:95.65ms
step:584/1770 train_time:54908ms step_avg:95.66ms
step:585/1770 train_time:55005ms step_avg:95.66ms
step:586/1770 train_time:55103ms step_avg:95.67ms
step:587/1770 train_time:55201ms step_avg:95.67ms
step:588/1770 train_time:55298ms step_avg:95.67ms
step:589/1770 train_time:55395ms step_avg:95.67ms
step:590/1770 train_time:55492ms step_avg:95.68ms
step:591/1770 train_time:55590ms step_avg:95.68ms
step:592/1770 train_time:55688ms step_avg:95.68ms
step:593/1770 train_time:55786ms step_avg:95.69ms
step:594/1770 train_time:55884ms step_avg:95.69ms
step:595/1770 train_time:55981ms step_avg:95.69ms
step:596/1770 train_time:56078ms step_avg:95.70ms
step:597/1770 train_time:56176ms step_avg:95.70ms
step:598/1770 train_time:56273ms step_avg:95.70ms
step:599/1770 train_time:56371ms step_avg:95.71ms
step:600/1770 train_time:56468ms step_avg:95.71ms
step:601/1770 train_time:56566ms step_avg:95.71ms
step:602/1770 train_time:56664ms step_avg:95.72ms
step:603/1770 train_time:56761ms step_avg:95.72ms
step:604/1770 train_time:56860ms step_avg:95.72ms
step:605/1770 train_time:56956ms step_avg:95.72ms
step:606/1770 train_time:57053ms step_avg:95.73ms
step:607/1770 train_time:57151ms step_avg:95.73ms
step:608/1770 train_time:57248ms step_avg:95.73ms
step:609/1770 train_time:57347ms step_avg:95.74ms
step:610/1770 train_time:57445ms step_avg:95.74ms
step:611/1770 train_time:57543ms step_avg:95.75ms
step:612/1770 train_time:57640ms step_avg:95.75ms
step:613/1770 train_time:57737ms step_avg:95.75ms
step:614/1770 train_time:57834ms step_avg:95.75ms
step:615/1770 train_time:57931ms step_avg:95.75ms
step:616/1770 train_time:58029ms step_avg:95.76ms
step:617/1770 train_time:58126ms step_avg:95.76ms
step:618/1770 train_time:58224ms step_avg:95.76ms
step:619/1770 train_time:58321ms step_avg:95.77ms
step:620/1770 train_time:58419ms step_avg:95.77ms
step:621/1770 train_time:58516ms step_avg:95.77ms
step:622/1770 train_time:58613ms step_avg:95.77ms
step:623/1770 train_time:58710ms step_avg:95.77ms
step:624/1770 train_time:58808ms step_avg:95.78ms
step:625/1770 train_time:58906ms step_avg:95.78ms
step:625/1770 val_loss:3.6674 train_time:59003ms step_avg:95.94ms
step:626/1770 train_time:59024ms step_avg:95.82ms
step:627/1770 train_time:59109ms step_avg:95.80ms
step:628/1770 train_time:59210ms step_avg:95.81ms
step:629/1770 train_time:59309ms step_avg:95.81ms
step:630/1770 train_time:59405ms step_avg:95.82ms
step:631/1770 train_time:59502ms step_avg:95.82ms
step:632/1770 train_time:59599ms step_avg:95.82ms
step:633/1770 train_time:59696ms step_avg:95.82ms
step:634/1770 train_time:59794ms step_avg:95.82ms
step:635/1770 train_time:59892ms step_avg:95.83ms
step:636/1770 train_time:59988ms step_avg:95.83ms
step:637/1770 train_time:60085ms step_avg:95.83ms
step:638/1770 train_time:60182ms step_avg:95.83ms
step:639/1770 train_time:60280ms step_avg:95.84ms
step:640/1770 train_time:60378ms step_avg:95.84ms
step:641/1770 train_time:60476ms step_avg:95.84ms
step:642/1770 train_time:60574ms step_avg:95.84ms
step:643/1770 train_time:60672ms step_avg:95.85ms
step:644/1770 train_time:60769ms step_avg:95.85ms
step:645/1770 train_time:60866ms step_avg:95.85ms
step:646/1770 train_time:60963ms step_avg:95.85ms
step:647/1770 train_time:61060ms step_avg:95.86ms
step:648/1770 train_time:61158ms step_avg:95.86ms
step:649/1770 train_time:61256ms step_avg:95.86ms
step:650/1770 train_time:61354ms step_avg:95.87ms
step:651/1770 train_time:61452ms step_avg:95.87ms
step:652/1770 train_time:61550ms step_avg:95.87ms
step:653/1770 train_time:61647ms step_avg:95.87ms
step:654/1770 train_time:61744ms step_avg:95.88ms
step:655/1770 train_time:61841ms step_avg:95.88ms
step:656/1770 train_time:61938ms step_avg:95.88ms
step:657/1770 train_time:62040ms step_avg:95.89ms
step:658/1770 train_time:62135ms step_avg:95.89ms
step:659/1770 train_time:62235ms step_avg:95.89ms
step:660/1770 train_time:62335ms step_avg:95.90ms
step:661/1770 train_time:62435ms step_avg:95.91ms
step:662/1770 train_time:62536ms step_avg:95.91ms
step:663/1770 train_time:62636ms step_avg:95.92ms
step:664/1770 train_time:62737ms step_avg:95.93ms
step:665/1770 train_time:62836ms step_avg:95.93ms
step:666/1770 train_time:62936ms step_avg:95.94ms
step:667/1770 train_time:63036ms step_avg:95.95ms
step:668/1770 train_time:63136ms step_avg:95.95ms
step:669/1770 train_time:63236ms step_avg:95.96ms
step:670/1770 train_time:63336ms step_avg:95.96ms
step:671/1770 train_time:63436ms step_avg:95.97ms
step:672/1770 train_time:63536ms step_avg:95.98ms
step:673/1770 train_time:63636ms step_avg:95.98ms
step:674/1770 train_time:63736ms step_avg:95.99ms
step:675/1770 train_time:63836ms step_avg:95.99ms
step:676/1770 train_time:63935ms step_avg:96.00ms
step:677/1770 train_time:64040ms step_avg:96.01ms
step:678/1770 train_time:64135ms step_avg:96.01ms
step:679/1770 train_time:64235ms step_avg:96.02ms
step:680/1770 train_time:64334ms step_avg:96.02ms
step:681/1770 train_time:64434ms step_avg:96.03ms
step:682/1770 train_time:64534ms step_avg:96.03ms
step:683/1770 train_time:64634ms step_avg:96.04ms
step:684/1770 train_time:64734ms step_avg:96.04ms
step:685/1770 train_time:64834ms step_avg:96.05ms
step:686/1770 train_time:64933ms step_avg:96.05ms
step:687/1770 train_time:65033ms step_avg:96.06ms
step:688/1770 train_time:65132ms step_avg:96.07ms
step:689/1770 train_time:65232ms step_avg:96.07ms
step:690/1770 train_time:65332ms step_avg:96.08ms
step:691/1770 train_time:65432ms step_avg:96.08ms
step:692/1770 train_time:65531ms step_avg:96.09ms
step:693/1770 train_time:65630ms step_avg:96.09ms
step:694/1770 train_time:65729ms step_avg:96.10ms
step:695/1770 train_time:65828ms step_avg:96.10ms
step:696/1770 train_time:65927ms step_avg:96.10ms
step:697/1770 train_time:66026ms step_avg:96.11ms
step:698/1770 train_time:66125ms step_avg:96.11ms
step:699/1770 train_time:66225ms step_avg:96.12ms
step:700/1770 train_time:66325ms step_avg:96.12ms
step:701/1770 train_time:66424ms step_avg:96.13ms
step:702/1770 train_time:66523ms step_avg:96.13ms
step:703/1770 train_time:66622ms step_avg:96.14ms
step:704/1770 train_time:66722ms step_avg:96.14ms
step:705/1770 train_time:66821ms step_avg:96.14ms
step:706/1770 train_time:66919ms step_avg:96.15ms
step:707/1770 train_time:67018ms step_avg:96.15ms
step:708/1770 train_time:67118ms step_avg:96.16ms
step:709/1770 train_time:67217ms step_avg:96.16ms
step:710/1770 train_time:67317ms step_avg:96.17ms
step:711/1770 train_time:67416ms step_avg:96.17ms
step:712/1770 train_time:67516ms step_avg:96.18ms
step:713/1770 train_time:67616ms step_avg:96.18ms
step:714/1770 train_time:67716ms step_avg:96.19ms
step:715/1770 train_time:67815ms step_avg:96.19ms
step:716/1770 train_time:67915ms step_avg:96.20ms
step:717/1770 train_time:68015ms step_avg:96.20ms
step:718/1770 train_time:68115ms step_avg:96.21ms
step:719/1770 train_time:68215ms step_avg:96.21ms
step:720/1770 train_time:68315ms step_avg:96.22ms
step:721/1770 train_time:68415ms step_avg:96.22ms
step:722/1770 train_time:68515ms step_avg:96.23ms
step:723/1770 train_time:68615ms step_avg:96.23ms
step:724/1770 train_time:68714ms step_avg:96.24ms
step:725/1770 train_time:68814ms step_avg:96.24ms
step:726/1770 train_time:68913ms step_avg:96.25ms
step:727/1770 train_time:69013ms step_avg:96.25ms
step:728/1770 train_time:69113ms step_avg:96.26ms
step:729/1770 train_time:69212ms step_avg:96.26ms
step:730/1770 train_time:69311ms step_avg:96.27ms
step:731/1770 train_time:69411ms step_avg:96.27ms
step:732/1770 train_time:69510ms step_avg:96.27ms
step:733/1770 train_time:69609ms step_avg:96.28ms
step:734/1770 train_time:69708ms step_avg:96.28ms
step:735/1770 train_time:69807ms step_avg:96.29ms
step:736/1770 train_time:69905ms step_avg:96.29ms
step:737/1770 train_time:70004ms step_avg:96.29ms
step:738/1770 train_time:70103ms step_avg:96.30ms
step:739/1770 train_time:70203ms step_avg:96.30ms
step:740/1770 train_time:70302ms step_avg:96.30ms
step:741/1770 train_time:70401ms step_avg:96.31ms
step:742/1770 train_time:70500ms step_avg:96.31ms
step:743/1770 train_time:70600ms step_avg:96.32ms
step:744/1770 train_time:70698ms step_avg:96.32ms
step:745/1770 train_time:70797ms step_avg:96.32ms
step:746/1770 train_time:70897ms step_avg:96.33ms
step:747/1770 train_time:70996ms step_avg:96.33ms
step:748/1770 train_time:71096ms step_avg:96.34ms
step:749/1770 train_time:71195ms step_avg:96.34ms
step:750/1770 train_time:71295ms step_avg:96.34ms
step:750/1770 val_loss:3.6035 train_time:71393ms step_avg:96.48ms
step:751/1770 train_time:71415ms step_avg:96.38ms
step:752/1770 train_time:71504ms step_avg:96.37ms
step:753/1770 train_time:71604ms step_avg:96.37ms
step:754/1770 train_time:71703ms step_avg:96.37ms
step:755/1770 train_time:71802ms step_avg:96.38ms
step:756/1770 train_time:71901ms step_avg:96.38ms
step:757/1770 train_time:72000ms step_avg:96.39ms
step:758/1770 train_time:72099ms step_avg:96.39ms
step:759/1770 train_time:72198ms step_avg:96.39ms
step:760/1770 train_time:72298ms step_avg:96.40ms
step:761/1770 train_time:72397ms step_avg:96.40ms
step:762/1770 train_time:72498ms step_avg:96.41ms
step:763/1770 train_time:72599ms step_avg:96.41ms
step:764/1770 train_time:72699ms step_avg:96.42ms
step:765/1770 train_time:72800ms step_avg:96.42ms
step:766/1770 train_time:72899ms step_avg:96.43ms
step:767/1770 train_time:72998ms step_avg:96.43ms
step:768/1770 train_time:73098ms step_avg:96.43ms
step:769/1770 train_time:73197ms step_avg:96.44ms
step:770/1770 train_time:73296ms step_avg:96.44ms
step:771/1770 train_time:73396ms step_avg:96.45ms
step:772/1770 train_time:73497ms step_avg:96.45ms
step:773/1770 train_time:73597ms step_avg:96.46ms
step:774/1770 train_time:73698ms step_avg:96.46ms
step:775/1770 train_time:73798ms step_avg:96.47ms
step:776/1770 train_time:73898ms step_avg:96.47ms
step:777/1770 train_time:73997ms step_avg:96.48ms
step:778/1770 train_time:74096ms step_avg:96.48ms
step:779/1770 train_time:74195ms step_avg:96.48ms
step:780/1770 train_time:74294ms step_avg:96.49ms
step:781/1770 train_time:74393ms step_avg:96.49ms
step:782/1770 train_time:74493ms step_avg:96.49ms
step:783/1770 train_time:74593ms step_avg:96.50ms
step:784/1770 train_time:74692ms step_avg:96.50ms
step:785/1770 train_time:74791ms step_avg:96.50ms
step:786/1770 train_time:74891ms step_avg:96.51ms
step:787/1770 train_time:74990ms step_avg:96.51ms
step:788/1770 train_time:75089ms step_avg:96.52ms
step:789/1770 train_time:75189ms step_avg:96.52ms
step:790/1770 train_time:75288ms step_avg:96.52ms
step:791/1770 train_time:75387ms step_avg:96.53ms
step:792/1770 train_time:75486ms step_avg:96.53ms
step:793/1770 train_time:75585ms step_avg:96.53ms
step:794/1770 train_time:75685ms step_avg:96.54ms
step:795/1770 train_time:75785ms step_avg:96.54ms
step:796/1770 train_time:75885ms step_avg:96.55ms
step:797/1770 train_time:75984ms step_avg:96.55ms
step:798/1770 train_time:76083ms step_avg:96.55ms
step:799/1770 train_time:76183ms step_avg:96.56ms
step:800/1770 train_time:76282ms step_avg:96.56ms
step:801/1770 train_time:76381ms step_avg:96.56ms
step:802/1770 train_time:76480ms step_avg:96.57ms
step:803/1770 train_time:76579ms step_avg:96.57ms
step:804/1770 train_time:76679ms step_avg:96.57ms
step:805/1770 train_time:76778ms step_avg:96.58ms
step:806/1770 train_time:76878ms step_avg:96.58ms
step:807/1770 train_time:76977ms step_avg:96.58ms
step:808/1770 train_time:77077ms step_avg:96.59ms
step:809/1770 train_time:77177ms step_avg:96.59ms
step:810/1770 train_time:77279ms step_avg:96.60ms
step:811/1770 train_time:77379ms step_avg:96.60ms
step:812/1770 train_time:77479ms step_avg:96.61ms
step:813/1770 train_time:77579ms step_avg:96.61ms
step:814/1770 train_time:77679ms step_avg:96.62ms
step:815/1770 train_time:77779ms step_avg:96.62ms
step:816/1770 train_time:77878ms step_avg:96.62ms
step:817/1770 train_time:77977ms step_avg:96.63ms
step:818/1770 train_time:78077ms step_avg:96.63ms
step:819/1770 train_time:78177ms step_avg:96.63ms
step:820/1770 train_time:78277ms step_avg:96.64ms
step:821/1770 train_time:78377ms step_avg:96.64ms
step:822/1770 train_time:78477ms step_avg:96.65ms
step:823/1770 train_time:78577ms step_avg:96.65ms
step:824/1770 train_time:78678ms step_avg:96.66ms
step:825/1770 train_time:78778ms step_avg:96.66ms
step:826/1770 train_time:78878ms step_avg:96.66ms
step:827/1770 train_time:78978ms step_avg:96.67ms
step:828/1770 train_time:79078ms step_avg:96.67ms
step:829/1770 train_time:79178ms step_avg:96.68ms
step:830/1770 train_time:79278ms step_avg:96.68ms
step:831/1770 train_time:79378ms step_avg:96.68ms
step:832/1770 train_time:79478ms step_avg:96.69ms
step:833/1770 train_time:79578ms step_avg:96.69ms
step:834/1770 train_time:79678ms step_avg:96.70ms
step:835/1770 train_time:79778ms step_avg:96.70ms
step:836/1770 train_time:79879ms step_avg:96.71ms
step:837/1770 train_time:79978ms step_avg:96.71ms
step:838/1770 train_time:80078ms step_avg:96.71ms
step:839/1770 train_time:80178ms step_avg:96.72ms
step:840/1770 train_time:80279ms step_avg:96.72ms
step:841/1770 train_time:80378ms step_avg:96.72ms
step:842/1770 train_time:80477ms step_avg:96.73ms
step:843/1770 train_time:80577ms step_avg:96.73ms
step:844/1770 train_time:80677ms step_avg:96.74ms
step:845/1770 train_time:80777ms step_avg:96.74ms
step:846/1770 train_time:80877ms step_avg:96.74ms
step:847/1770 train_time:80976ms step_avg:96.75ms
step:848/1770 train_time:81075ms step_avg:96.75ms
step:849/1770 train_time:81175ms step_avg:96.75ms
step:850/1770 train_time:81274ms step_avg:96.76ms
step:851/1770 train_time:81374ms step_avg:96.76ms
step:852/1770 train_time:81474ms step_avg:96.76ms
step:853/1770 train_time:81574ms step_avg:96.77ms
step:854/1770 train_time:81673ms step_avg:96.77ms
step:855/1770 train_time:81773ms step_avg:96.77ms
step:856/1770 train_time:81873ms step_avg:96.78ms
step:857/1770 train_time:81973ms step_avg:96.78ms
step:858/1770 train_time:82072ms step_avg:96.78ms
step:859/1770 train_time:82172ms step_avg:96.79ms
step:860/1770 train_time:82272ms step_avg:96.79ms
step:861/1770 train_time:82371ms step_avg:96.79ms
step:862/1770 train_time:82470ms step_avg:96.80ms
step:863/1770 train_time:82569ms step_avg:96.80ms
step:864/1770 train_time:82668ms step_avg:96.80ms
step:865/1770 train_time:82767ms step_avg:96.80ms
step:866/1770 train_time:82867ms step_avg:96.81ms
step:867/1770 train_time:82966ms step_avg:96.81ms
step:868/1770 train_time:83065ms step_avg:96.81ms
step:869/1770 train_time:83164ms step_avg:96.82ms
step:870/1770 train_time:83264ms step_avg:96.82ms
step:871/1770 train_time:83364ms step_avg:96.82ms
step:872/1770 train_time:83464ms step_avg:96.83ms
step:873/1770 train_time:83564ms step_avg:96.83ms
step:874/1770 train_time:83664ms step_avg:96.83ms
step:875/1770 train_time:83763ms step_avg:96.84ms
step:875/1770 val_loss:3.5536 train_time:83861ms step_avg:96.95ms
step:876/1770 train_time:83881ms step_avg:96.86ms
step:877/1770 train_time:83972ms step_avg:96.85ms
step:878/1770 train_time:84073ms step_avg:96.86ms
step:879/1770 train_time:84173ms step_avg:96.86ms
step:880/1770 train_time:84272ms step_avg:96.86ms
step:881/1770 train_time:84371ms step_avg:96.87ms
step:882/1770 train_time:84471ms step_avg:96.87ms
step:883/1770 train_time:84570ms step_avg:96.87ms
step:884/1770 train_time:84669ms step_avg:96.88ms
step:885/1770 train_time:84768ms step_avg:96.88ms
step:886/1770 train_time:84868ms step_avg:96.88ms
step:887/1770 train_time:84971ms step_avg:96.89ms
step:888/1770 train_time:85072ms step_avg:96.89ms
step:889/1770 train_time:85173ms step_avg:96.90ms
step:890/1770 train_time:85273ms step_avg:96.90ms
step:891/1770 train_time:85373ms step_avg:96.90ms
step:892/1770 train_time:85473ms step_avg:96.91ms
step:893/1770 train_time:85572ms step_avg:96.91ms
step:894/1770 train_time:85671ms step_avg:96.91ms
step:895/1770 train_time:85771ms step_avg:96.92ms
step:896/1770 train_time:85871ms step_avg:96.92ms
step:897/1770 train_time:85971ms step_avg:96.92ms
step:898/1770 train_time:86071ms step_avg:96.93ms
step:899/1770 train_time:86171ms step_avg:96.93ms
step:900/1770 train_time:86270ms step_avg:96.93ms
step:901/1770 train_time:86370ms step_avg:96.94ms
step:902/1770 train_time:86470ms step_avg:96.94ms
step:903/1770 train_time:86569ms step_avg:96.94ms
step:904/1770 train_time:86669ms step_avg:96.94ms
step:905/1770 train_time:86768ms step_avg:96.95ms
step:906/1770 train_time:86867ms step_avg:96.95ms
step:907/1770 train_time:86967ms step_avg:96.95ms
step:908/1770 train_time:87066ms step_avg:96.96ms
step:909/1770 train_time:87165ms step_avg:96.96ms
step:910/1770 train_time:87265ms step_avg:96.96ms
step:911/1770 train_time:87364ms step_avg:96.96ms
step:912/1770 train_time:87464ms step_avg:96.97ms
step:913/1770 train_time:87563ms step_avg:96.97ms
step:914/1770 train_time:87663ms step_avg:96.97ms
step:915/1770 train_time:87763ms step_avg:96.98ms
step:916/1770 train_time:87863ms step_avg:96.98ms
step:917/1770 train_time:87963ms step_avg:96.98ms
step:918/1770 train_time:88062ms step_avg:96.99ms
step:919/1770 train_time:88162ms step_avg:96.99ms
step:920/1770 train_time:88264ms step_avg:96.99ms
step:921/1770 train_time:88366ms step_avg:97.00ms
step:922/1770 train_time:88466ms step_avg:97.00ms
step:923/1770 train_time:88566ms step_avg:97.01ms
step:924/1770 train_time:88667ms step_avg:97.01ms
step:925/1770 train_time:88767ms step_avg:97.01ms
step:926/1770 train_time:88867ms step_avg:97.02ms
step:927/1770 train_time:88968ms step_avg:97.02ms
step:928/1770 train_time:89068ms step_avg:97.02ms
step:929/1770 train_time:89168ms step_avg:97.03ms
step:930/1770 train_time:89269ms step_avg:97.03ms
step:931/1770 train_time:89370ms step_avg:97.04ms
step:932/1770 train_time:89471ms step_avg:97.04ms
step:933/1770 train_time:89571ms step_avg:97.04ms
step:934/1770 train_time:89673ms step_avg:97.05ms
step:935/1770 train_time:89774ms step_avg:97.05ms
step:936/1770 train_time:89876ms step_avg:97.06ms
step:937/1770 train_time:89977ms step_avg:97.06ms
step:938/1770 train_time:90077ms step_avg:97.07ms
step:939/1770 train_time:90178ms step_avg:97.07ms
step:940/1770 train_time:90279ms step_avg:97.07ms
step:941/1770 train_time:90380ms step_avg:97.08ms
step:942/1770 train_time:90482ms step_avg:97.08ms
step:943/1770 train_time:90583ms step_avg:97.09ms
step:944/1770 train_time:90684ms step_avg:97.09ms
step:945/1770 train_time:90785ms step_avg:97.10ms
step:946/1770 train_time:90885ms step_avg:97.10ms
step:947/1770 train_time:90986ms step_avg:97.10ms
step:948/1770 train_time:91087ms step_avg:97.11ms
step:949/1770 train_time:91188ms step_avg:97.11ms
step:950/1770 train_time:91288ms step_avg:97.12ms
step:951/1770 train_time:91389ms step_avg:97.12ms
step:952/1770 train_time:91489ms step_avg:97.12ms
step:953/1770 train_time:91590ms step_avg:97.13ms
step:954/1770 train_time:91691ms step_avg:97.13ms
step:955/1770 train_time:91793ms step_avg:97.14ms
step:956/1770 train_time:91895ms step_avg:97.14ms
step:957/1770 train_time:91997ms step_avg:97.15ms
step:958/1770 train_time:92100ms step_avg:97.15ms
step:959/1770 train_time:92201ms step_avg:97.16ms
step:960/1770 train_time:92301ms step_avg:97.16ms
step:961/1770 train_time:92401ms step_avg:97.16ms
step:962/1770 train_time:92503ms step_avg:97.17ms
step:963/1770 train_time:92604ms step_avg:97.17ms
step:964/1770 train_time:92705ms step_avg:97.17ms
step:965/1770 train_time:92805ms step_avg:97.18ms
step:966/1770 train_time:92906ms step_avg:97.18ms
step:967/1770 train_time:93006ms step_avg:97.19ms
step:968/1770 train_time:93108ms step_avg:97.19ms
step:969/1770 train_time:93209ms step_avg:97.19ms
step:970/1770 train_time:93309ms step_avg:97.20ms
step:971/1770 train_time:93410ms step_avg:97.20ms
step:972/1770 train_time:93515ms step_avg:97.21ms
step:973/1770 train_time:93614ms step_avg:97.21ms
step:974/1770 train_time:93715ms step_avg:97.22ms
step:975/1770 train_time:93818ms step_avg:97.22ms
step:976/1770 train_time:93918ms step_avg:97.22ms
step:977/1770 train_time:94020ms step_avg:97.23ms
step:978/1770 train_time:94120ms step_avg:97.23ms
step:979/1770 train_time:94221ms step_avg:97.24ms
step:980/1770 train_time:94322ms step_avg:97.24ms
step:981/1770 train_time:94423ms step_avg:97.24ms
step:982/1770 train_time:94525ms step_avg:97.25ms
step:983/1770 train_time:94626ms step_avg:97.25ms
step:984/1770 train_time:94727ms step_avg:97.26ms
step:985/1770 train_time:94829ms step_avg:97.26ms
step:986/1770 train_time:94928ms step_avg:97.26ms
step:987/1770 train_time:95029ms step_avg:97.27ms
step:988/1770 train_time:95129ms step_avg:97.27ms
step:989/1770 train_time:95231ms step_avg:97.27ms
step:990/1770 train_time:95332ms step_avg:97.28ms
step:991/1770 train_time:95433ms step_avg:97.28ms
step:992/1770 train_time:95535ms step_avg:97.29ms
step:993/1770 train_time:95637ms step_avg:97.29ms
step:994/1770 train_time:95739ms step_avg:97.30ms
step:995/1770 train_time:95840ms step_avg:97.30ms
step:996/1770 train_time:95941ms step_avg:97.30ms
step:997/1770 train_time:96043ms step_avg:97.31ms
step:998/1770 train_time:96145ms step_avg:97.31ms
step:999/1770 train_time:96246ms step_avg:97.32ms
step:1000/1770 train_time:96347ms step_avg:97.32ms
step:1000/1770 val_loss:3.5162 train_time:96445ms step_avg:97.42ms
step:1001/1770 train_time:96465ms step_avg:97.34ms
step:1002/1770 train_time:96556ms step_avg:97.33ms
step:1003/1770 train_time:96661ms step_avg:97.34ms
step:1004/1770 train_time:96763ms step_avg:97.35ms
step:1005/1770 train_time:96862ms step_avg:97.35ms
step:1006/1770 train_time:96962ms step_avg:97.35ms
step:1007/1770 train_time:97062ms step_avg:97.35ms
step:1008/1770 train_time:97163ms step_avg:97.36ms
step:1009/1770 train_time:97263ms step_avg:97.36ms
step:1010/1770 train_time:97363ms step_avg:97.36ms
step:1011/1770 train_time:97466ms step_avg:97.37ms
step:1012/1770 train_time:97569ms step_avg:97.37ms
step:1013/1770 train_time:97671ms step_avg:97.38ms
step:1014/1770 train_time:97771ms step_avg:97.38ms
step:1015/1770 train_time:97871ms step_avg:97.38ms
step:1016/1770 train_time:97971ms step_avg:97.39ms
step:1017/1770 train_time:98072ms step_avg:97.39ms
step:1018/1770 train_time:98172ms step_avg:97.39ms
step:1019/1770 train_time:98272ms step_avg:97.40ms
step:1020/1770 train_time:98373ms step_avg:97.40ms
step:1021/1770 train_time:98476ms step_avg:97.40ms
step:1022/1770 train_time:98578ms step_avg:97.41ms
step:1023/1770 train_time:98680ms step_avg:97.41ms
step:1024/1770 train_time:98780ms step_avg:97.42ms
step:1025/1770 train_time:98881ms step_avg:97.42ms
step:1026/1770 train_time:98982ms step_avg:97.42ms
step:1027/1770 train_time:99084ms step_avg:97.43ms
step:1028/1770 train_time:99186ms step_avg:97.43ms
step:1029/1770 train_time:99286ms step_avg:97.44ms
step:1030/1770 train_time:99387ms step_avg:97.44ms
step:1031/1770 train_time:99488ms step_avg:97.44ms
step:1032/1770 train_time:99588ms step_avg:97.44ms
step:1033/1770 train_time:99689ms step_avg:97.45ms
step:1034/1770 train_time:99789ms step_avg:97.45ms
step:1035/1770 train_time:99890ms step_avg:97.45ms
step:1036/1770 train_time:99990ms step_avg:97.46ms
step:1037/1770 train_time:100091ms step_avg:97.46ms
step:1038/1770 train_time:100192ms step_avg:97.46ms
step:1039/1770 train_time:100293ms step_avg:97.47ms
step:1040/1770 train_time:100394ms step_avg:97.47ms
step:1041/1770 train_time:100494ms step_avg:97.47ms
step:1042/1770 train_time:100596ms step_avg:97.48ms
step:1043/1770 train_time:100697ms step_avg:97.48ms
step:1044/1770 train_time:100798ms step_avg:97.48ms
step:1045/1770 train_time:100899ms step_avg:97.49ms
step:1046/1770 train_time:101000ms step_avg:97.49ms
step:1047/1770 train_time:101100ms step_avg:97.49ms
step:1048/1770 train_time:101201ms step_avg:97.50ms
step:1049/1770 train_time:101302ms step_avg:97.50ms
step:1050/1770 train_time:101403ms step_avg:97.50ms
step:1051/1770 train_time:101506ms step_avg:97.51ms
step:1052/1770 train_time:101608ms step_avg:97.51ms
step:1053/1770 train_time:101709ms step_avg:97.52ms
step:1054/1770 train_time:101809ms step_avg:97.52ms
step:1055/1770 train_time:101910ms step_avg:97.52ms
step:1056/1770 train_time:102010ms step_avg:97.52ms
step:1057/1770 train_time:102111ms step_avg:97.53ms
step:1058/1770 train_time:102212ms step_avg:97.53ms
step:1059/1770 train_time:102313ms step_avg:97.53ms
step:1060/1770 train_time:102416ms step_avg:97.54ms
step:1061/1770 train_time:102517ms step_avg:97.54ms
step:1062/1770 train_time:102620ms step_avg:97.55ms
step:1063/1770 train_time:102722ms step_avg:97.55ms
step:1064/1770 train_time:102824ms step_avg:97.56ms
step:1065/1770 train_time:102925ms step_avg:97.56ms
step:1066/1770 train_time:103025ms step_avg:97.56ms
step:1067/1770 train_time:103126ms step_avg:97.56ms
step:1068/1770 train_time:103228ms step_avg:97.57ms
step:1069/1770 train_time:103329ms step_avg:97.57ms
step:1070/1770 train_time:103429ms step_avg:97.57ms
step:1071/1770 train_time:103531ms step_avg:97.58ms
step:1072/1770 train_time:103631ms step_avg:97.58ms
step:1073/1770 train_time:103732ms step_avg:97.58ms
step:1074/1770 train_time:103832ms step_avg:97.59ms
step:1075/1770 train_time:103935ms step_avg:97.59ms
step:1076/1770 train_time:104036ms step_avg:97.59ms
step:1077/1770 train_time:104138ms step_avg:97.60ms
step:1078/1770 train_time:104240ms step_avg:97.60ms
step:1079/1770 train_time:104341ms step_avg:97.61ms
step:1080/1770 train_time:104442ms step_avg:97.61ms
step:1081/1770 train_time:104542ms step_avg:97.61ms
step:1082/1770 train_time:104643ms step_avg:97.62ms
step:1083/1770 train_time:104746ms step_avg:97.62ms
step:1084/1770 train_time:104847ms step_avg:97.62ms
step:1085/1770 train_time:104948ms step_avg:97.63ms
step:1086/1770 train_time:105049ms step_avg:97.63ms
step:1087/1770 train_time:105150ms step_avg:97.63ms
step:1088/1770 train_time:105251ms step_avg:97.64ms
step:1089/1770 train_time:105351ms step_avg:97.64ms
step:1090/1770 train_time:105453ms step_avg:97.64ms
step:1091/1770 train_time:105553ms step_avg:97.64ms
step:1092/1770 train_time:105654ms step_avg:97.65ms
step:1093/1770 train_time:105756ms step_avg:97.65ms
step:1094/1770 train_time:105859ms step_avg:97.66ms
step:1095/1770 train_time:105960ms step_avg:97.66ms
step:1096/1770 train_time:106062ms step_avg:97.66ms
step:1097/1770 train_time:106163ms step_avg:97.67ms
step:1098/1770 train_time:106263ms step_avg:97.67ms
step:1099/1770 train_time:106364ms step_avg:97.67ms
step:1100/1770 train_time:106465ms step_avg:97.67ms
step:1101/1770 train_time:106566ms step_avg:97.68ms
step:1102/1770 train_time:106668ms step_avg:97.68ms
step:1103/1770 train_time:106769ms step_avg:97.68ms
step:1104/1770 train_time:106871ms step_avg:97.69ms
step:1105/1770 train_time:106971ms step_avg:97.69ms
step:1106/1770 train_time:107071ms step_avg:97.69ms
step:1107/1770 train_time:107171ms step_avg:97.69ms
step:1108/1770 train_time:107273ms step_avg:97.70ms
step:1109/1770 train_time:107377ms step_avg:97.70ms
step:1110/1770 train_time:107476ms step_avg:97.71ms
step:1111/1770 train_time:107577ms step_avg:97.71ms
step:1112/1770 train_time:107680ms step_avg:97.71ms
step:1113/1770 train_time:107781ms step_avg:97.72ms
step:1114/1770 train_time:107882ms step_avg:97.72ms
step:1115/1770 train_time:107983ms step_avg:97.72ms
step:1116/1770 train_time:108084ms step_avg:97.73ms
step:1117/1770 train_time:108185ms step_avg:97.73ms
step:1118/1770 train_time:108286ms step_avg:97.73ms
step:1119/1770 train_time:108387ms step_avg:97.73ms
step:1120/1770 train_time:108489ms step_avg:97.74ms
step:1121/1770 train_time:108589ms step_avg:97.74ms
step:1122/1770 train_time:108690ms step_avg:97.74ms
step:1123/1770 train_time:108790ms step_avg:97.75ms
step:1124/1770 train_time:108891ms step_avg:97.75ms
step:1125/1770 train_time:108992ms step_avg:97.75ms
step:1125/1770 val_loss:3.4747 train_time:109091ms step_avg:97.84ms
step:1126/1770 train_time:109112ms step_avg:97.77ms
step:1127/1770 train_time:109203ms step_avg:97.76ms
step:1128/1770 train_time:109306ms step_avg:97.77ms
step:1129/1770 train_time:109407ms step_avg:97.77ms
step:1130/1770 train_time:109508ms step_avg:97.77ms
step:1131/1770 train_time:109608ms step_avg:97.78ms
step:1132/1770 train_time:109709ms step_avg:97.78ms
step:1133/1770 train_time:109810ms step_avg:97.78ms
step:1134/1770 train_time:109911ms step_avg:97.79ms
step:1135/1770 train_time:110011ms step_avg:97.79ms
step:1136/1770 train_time:110114ms step_avg:97.79ms
step:1137/1770 train_time:110217ms step_avg:97.80ms
step:1138/1770 train_time:110318ms step_avg:97.80ms
step:1139/1770 train_time:110418ms step_avg:97.80ms
step:1140/1770 train_time:110518ms step_avg:97.80ms
step:1141/1770 train_time:110619ms step_avg:97.81ms
step:1142/1770 train_time:110720ms step_avg:97.81ms
step:1143/1770 train_time:110821ms step_avg:97.81ms
step:1144/1770 train_time:110924ms step_avg:97.82ms
step:1145/1770 train_time:111026ms step_avg:97.82ms
step:1146/1770 train_time:111129ms step_avg:97.82ms
step:1147/1770 train_time:111231ms step_avg:97.83ms
step:1148/1770 train_time:111332ms step_avg:97.83ms
step:1149/1770 train_time:111433ms step_avg:97.83ms
step:1150/1770 train_time:111534ms step_avg:97.84ms
step:1151/1770 train_time:111635ms step_avg:97.84ms
step:1152/1770 train_time:111741ms step_avg:97.85ms
step:1153/1770 train_time:111837ms step_avg:97.85ms
step:1154/1770 train_time:111938ms step_avg:97.85ms
step:1155/1770 train_time:112039ms step_avg:97.85ms
step:1156/1770 train_time:112139ms step_avg:97.85ms
step:1157/1770 train_time:112243ms step_avg:97.86ms
step:1158/1770 train_time:112345ms step_avg:97.86ms
step:1159/1770 train_time:112446ms step_avg:97.86ms
step:1160/1770 train_time:112548ms step_avg:97.87ms
step:1161/1770 train_time:112649ms step_avg:97.87ms
step:1162/1770 train_time:112750ms step_avg:97.87ms
step:1163/1770 train_time:112852ms step_avg:97.88ms
step:1164/1770 train_time:112953ms step_avg:97.88ms
step:1165/1770 train_time:113055ms step_avg:97.88ms
step:1166/1770 train_time:113156ms step_avg:97.89ms
step:1167/1770 train_time:113257ms step_avg:97.89ms
step:1168/1770 train_time:113358ms step_avg:97.89ms
step:1169/1770 train_time:113458ms step_avg:97.89ms
step:1170/1770 train_time:113558ms step_avg:97.90ms
step:1171/1770 train_time:113659ms step_avg:97.90ms
step:1172/1770 train_time:113761ms step_avg:97.90ms
step:1173/1770 train_time:113862ms step_avg:97.90ms
step:1174/1770 train_time:113964ms step_avg:97.91ms
step:1175/1770 train_time:114067ms step_avg:97.91ms
step:1176/1770 train_time:114169ms step_avg:97.92ms
step:1177/1770 train_time:114270ms step_avg:97.92ms
step:1178/1770 train_time:114371ms step_avg:97.92ms
step:1179/1770 train_time:114471ms step_avg:97.92ms
step:1180/1770 train_time:114573ms step_avg:97.93ms
step:1181/1770 train_time:114674ms step_avg:97.93ms
step:1182/1770 train_time:114775ms step_avg:97.93ms
step:1183/1770 train_time:114876ms step_avg:97.93ms
step:1184/1770 train_time:114979ms step_avg:97.94ms
step:1185/1770 train_time:115081ms step_avg:97.94ms
step:1186/1770 train_time:115183ms step_avg:97.94ms
step:1187/1770 train_time:115288ms step_avg:97.95ms
step:1188/1770 train_time:115390ms step_avg:97.95ms
step:1189/1770 train_time:115491ms step_avg:97.96ms
step:1190/1770 train_time:115593ms step_avg:97.96ms
step:1191/1770 train_time:115695ms step_avg:97.96ms
step:1192/1770 train_time:115798ms step_avg:97.97ms
step:1193/1770 train_time:115900ms step_avg:97.97ms
step:1194/1770 train_time:116002ms step_avg:97.97ms
step:1195/1770 train_time:116105ms step_avg:97.98ms
step:1196/1770 train_time:116208ms step_avg:97.98ms
step:1197/1770 train_time:116310ms step_avg:97.99ms
step:1198/1770 train_time:116412ms step_avg:97.99ms
step:1199/1770 train_time:116514ms step_avg:97.99ms
step:1200/1770 train_time:116616ms step_avg:98.00ms
step:1201/1770 train_time:116721ms step_avg:98.00ms
step:1202/1770 train_time:116822ms step_avg:98.00ms
step:1203/1770 train_time:116923ms step_avg:98.01ms
step:1204/1770 train_time:117026ms step_avg:98.01ms
step:1205/1770 train_time:117127ms step_avg:98.01ms
step:1206/1770 train_time:117230ms step_avg:98.02ms
step:1207/1770 train_time:117332ms step_avg:98.02ms
step:1208/1770 train_time:117434ms step_avg:98.02ms
step:1209/1770 train_time:117536ms step_avg:98.03ms
step:1210/1770 train_time:117638ms step_avg:98.03ms
step:1211/1770 train_time:117740ms step_avg:98.04ms
step:1212/1770 train_time:117844ms step_avg:98.04ms
step:1213/1770 train_time:117946ms step_avg:98.04ms
step:1214/1770 train_time:118048ms step_avg:98.05ms
step:1215/1770 train_time:118151ms step_avg:98.05ms
step:1216/1770 train_time:118256ms step_avg:98.06ms
step:1217/1770 train_time:118358ms step_avg:98.06ms
step:1218/1770 train_time:118460ms step_avg:98.06ms
step:1219/1770 train_time:118562ms step_avg:98.07ms
step:1220/1770 train_time:118665ms step_avg:98.07ms
step:1221/1770 train_time:118767ms step_avg:98.07ms
step:1222/1770 train_time:118870ms step_avg:98.08ms
step:1223/1770 train_time:118971ms step_avg:98.08ms
step:1224/1770 train_time:119074ms step_avg:98.08ms
step:1225/1770 train_time:119177ms step_avg:98.09ms
step:1226/1770 train_time:119279ms step_avg:98.09ms
step:1227/1770 train_time:119383ms step_avg:98.10ms
step:1228/1770 train_time:119487ms step_avg:98.10ms
step:1229/1770 train_time:119589ms step_avg:98.10ms
step:1230/1770 train_time:119692ms step_avg:98.11ms
step:1231/1770 train_time:119794ms step_avg:98.11ms
step:1232/1770 train_time:119895ms step_avg:98.11ms
step:1233/1770 train_time:119997ms step_avg:98.12ms
step:1234/1770 train_time:120100ms step_avg:98.12ms
step:1235/1770 train_time:120201ms step_avg:98.12ms
step:1236/1770 train_time:120304ms step_avg:98.13ms
step:1237/1770 train_time:120406ms step_avg:98.13ms
step:1238/1770 train_time:120510ms step_avg:98.14ms
step:1239/1770 train_time:120612ms step_avg:98.14ms
step:1240/1770 train_time:120714ms step_avg:98.14ms
step:1241/1770 train_time:120817ms step_avg:98.15ms
step:1242/1770 train_time:120918ms step_avg:98.15ms
step:1243/1770 train_time:121020ms step_avg:98.15ms
step:1244/1770 train_time:121122ms step_avg:98.15ms
step:1245/1770 train_time:121225ms step_avg:98.16ms
step:1246/1770 train_time:121328ms step_avg:98.16ms
step:1247/1770 train_time:121430ms step_avg:98.17ms
step:1248/1770 train_time:121533ms step_avg:98.17ms
step:1249/1770 train_time:121634ms step_avg:98.17ms
step:1250/1770 train_time:121736ms step_avg:98.17ms
step:1250/1770 val_loss:3.4271 train_time:121838ms step_avg:98.26ms
step:1251/1770 train_time:121859ms step_avg:98.19ms
step:1252/1770 train_time:121951ms step_avg:98.19ms
step:1253/1770 train_time:122054ms step_avg:98.19ms
step:1254/1770 train_time:122156ms step_avg:98.20ms
step:1255/1770 train_time:122261ms step_avg:98.20ms
step:1256/1770 train_time:122362ms step_avg:98.20ms
step:1257/1770 train_time:122463ms step_avg:98.21ms
step:1258/1770 train_time:122565ms step_avg:98.21ms
step:1259/1770 train_time:122668ms step_avg:98.21ms
step:1260/1770 train_time:122769ms step_avg:98.22ms
step:1261/1770 train_time:122872ms step_avg:98.22ms
step:1262/1770 train_time:122975ms step_avg:98.22ms
step:1263/1770 train_time:123077ms step_avg:98.23ms
step:1264/1770 train_time:123179ms step_avg:98.23ms
step:1265/1770 train_time:123281ms step_avg:98.23ms
step:1266/1770 train_time:123384ms step_avg:98.24ms
step:1267/1770 train_time:123489ms step_avg:98.24ms
step:1268/1770 train_time:123588ms step_avg:98.24ms
step:1269/1770 train_time:123690ms step_avg:98.25ms
step:1270/1770 train_time:123793ms step_avg:98.25ms
step:1271/1770 train_time:123896ms step_avg:98.25ms
step:1272/1770 train_time:123998ms step_avg:98.26ms
step:1273/1770 train_time:124101ms step_avg:98.26ms
step:1274/1770 train_time:124204ms step_avg:98.26ms
step:1275/1770 train_time:124305ms step_avg:98.26ms
step:1276/1770 train_time:124408ms step_avg:98.27ms
step:1277/1770 train_time:124510ms step_avg:98.27ms
step:1278/1770 train_time:124613ms step_avg:98.28ms
step:1279/1770 train_time:124716ms step_avg:98.28ms
step:1280/1770 train_time:124818ms step_avg:98.28ms
step:1281/1770 train_time:124920ms step_avg:98.28ms
step:1282/1770 train_time:125022ms step_avg:98.29ms
step:1283/1770 train_time:125125ms step_avg:98.29ms
step:1284/1770 train_time:125228ms step_avg:98.29ms
step:1285/1770 train_time:125330ms step_avg:98.30ms
step:1286/1770 train_time:125433ms step_avg:98.30ms
step:1287/1770 train_time:125536ms step_avg:98.31ms
step:1288/1770 train_time:125639ms step_avg:98.31ms
step:1289/1770 train_time:125741ms step_avg:98.31ms
step:1290/1770 train_time:125843ms step_avg:98.31ms
step:1291/1770 train_time:125945ms step_avg:98.32ms
step:1292/1770 train_time:126047ms step_avg:98.32ms
step:1293/1770 train_time:126150ms step_avg:98.32ms
step:1294/1770 train_time:126251ms step_avg:98.33ms
step:1295/1770 train_time:126354ms step_avg:98.33ms
step:1296/1770 train_time:126457ms step_avg:98.33ms
step:1297/1770 train_time:126559ms step_avg:98.34ms
step:1298/1770 train_time:126662ms step_avg:98.34ms
step:1299/1770 train_time:126764ms step_avg:98.34ms
step:1300/1770 train_time:126865ms step_avg:98.35ms
step:1301/1770 train_time:126968ms step_avg:98.35ms
step:1302/1770 train_time:127069ms step_avg:98.35ms
step:1303/1770 train_time:127172ms step_avg:98.35ms
step:1304/1770 train_time:127274ms step_avg:98.36ms
step:1305/1770 train_time:127377ms step_avg:98.36ms
step:1306/1770 train_time:127480ms step_avg:98.36ms
step:1307/1770 train_time:127582ms step_avg:98.37ms
step:1308/1770 train_time:127685ms step_avg:98.37ms
step:1309/1770 train_time:127787ms step_avg:98.37ms
step:1310/1770 train_time:127889ms step_avg:98.38ms
step:1311/1770 train_time:127990ms step_avg:98.38ms
step:1312/1770 train_time:128092ms step_avg:98.38ms
step:1313/1770 train_time:128193ms step_avg:98.38ms
step:1314/1770 train_time:128296ms step_avg:98.39ms
step:1315/1770 train_time:128398ms step_avg:98.39ms
step:1316/1770 train_time:128500ms step_avg:98.39ms
step:1317/1770 train_time:128603ms step_avg:98.40ms
step:1318/1770 train_time:128709ms step_avg:98.40ms
step:1319/1770 train_time:128812ms step_avg:98.40ms
step:1320/1770 train_time:128914ms step_avg:98.41ms
step:1321/1770 train_time:129017ms step_avg:98.41ms
step:1322/1770 train_time:129119ms step_avg:98.41ms
step:1323/1770 train_time:129221ms step_avg:98.42ms
step:1324/1770 train_time:129324ms step_avg:98.42ms
step:1325/1770 train_time:129427ms step_avg:98.42ms
step:1326/1770 train_time:129529ms step_avg:98.43ms
step:1327/1770 train_time:129634ms step_avg:98.43ms
step:1328/1770 train_time:129736ms step_avg:98.43ms
step:1329/1770 train_time:129839ms step_avg:98.44ms
step:1330/1770 train_time:129941ms step_avg:98.44ms
step:1331/1770 train_time:130042ms step_avg:98.44ms
step:1332/1770 train_time:130143ms step_avg:98.44ms
step:1333/1770 train_time:130245ms step_avg:98.45ms
step:1334/1770 train_time:130346ms step_avg:98.45ms
step:1335/1770 train_time:130448ms step_avg:98.45ms
step:1336/1770 train_time:130550ms step_avg:98.45ms
step:1337/1770 train_time:130651ms step_avg:98.46ms
step:1338/1770 train_time:130754ms step_avg:98.46ms
step:1339/1770 train_time:130858ms step_avg:98.46ms
step:1340/1770 train_time:130962ms step_avg:98.47ms
step:1341/1770 train_time:131064ms step_avg:98.47ms
step:1342/1770 train_time:131167ms step_avg:98.47ms
step:1343/1770 train_time:131269ms step_avg:98.48ms
step:1344/1770 train_time:131373ms step_avg:98.48ms
step:1345/1770 train_time:131474ms step_avg:98.48ms
step:1346/1770 train_time:131577ms step_avg:98.49ms
step:1347/1770 train_time:131679ms step_avg:98.49ms
step:1348/1770 train_time:131784ms step_avg:98.49ms
step:1349/1770 train_time:131888ms step_avg:98.50ms
step:1350/1770 train_time:131991ms step_avg:98.50ms
step:1351/1770 train_time:132093ms step_avg:98.50ms
step:1352/1770 train_time:132196ms step_avg:98.51ms
step:1353/1770 train_time:132299ms step_avg:98.51ms
step:1354/1770 train_time:132401ms step_avg:98.51ms
step:1355/1770 train_time:132503ms step_avg:98.52ms
step:1356/1770 train_time:132605ms step_avg:98.52ms
step:1357/1770 train_time:132707ms step_avg:98.52ms
step:1358/1770 train_time:132809ms step_avg:98.52ms
step:1359/1770 train_time:132913ms step_avg:98.53ms
step:1360/1770 train_time:133014ms step_avg:98.53ms
step:1361/1770 train_time:133116ms step_avg:98.53ms
step:1362/1770 train_time:133218ms step_avg:98.53ms
step:1363/1770 train_time:133321ms step_avg:98.54ms
step:1364/1770 train_time:133425ms step_avg:98.54ms
step:1365/1770 train_time:133527ms step_avg:98.54ms
step:1366/1770 train_time:133628ms step_avg:98.55ms
step:1367/1770 train_time:133731ms step_avg:98.55ms
step:1368/1770 train_time:133832ms step_avg:98.55ms
step:1369/1770 train_time:133935ms step_avg:98.55ms
step:1370/1770 train_time:134038ms step_avg:98.56ms
step:1371/1770 train_time:134140ms step_avg:98.56ms
step:1372/1770 train_time:134242ms step_avg:98.56ms
step:1373/1770 train_time:134344ms step_avg:98.57ms
step:1374/1770 train_time:134447ms step_avg:98.57ms
step:1375/1770 train_time:134550ms step_avg:98.57ms
step:1375/1770 val_loss:3.3835 train_time:134651ms step_avg:98.65ms
step:1376/1770 train_time:134671ms step_avg:98.59ms
step:1377/1770 train_time:134764ms step_avg:98.58ms
step:1378/1770 train_time:134866ms step_avg:98.59ms
step:1379/1770 train_time:134967ms step_avg:98.59ms
step:1380/1770 train_time:135069ms step_avg:98.59ms
step:1381/1770 train_time:135171ms step_avg:98.59ms
step:1382/1770 train_time:135273ms step_avg:98.60ms
step:1383/1770 train_time:135376ms step_avg:98.60ms
step:1384/1770 train_time:135478ms step_avg:98.60ms
step:1385/1770 train_time:135580ms step_avg:98.60ms
step:1386/1770 train_time:135683ms step_avg:98.61ms
step:1387/1770 train_time:135786ms step_avg:98.61ms
step:1388/1770 train_time:135887ms step_avg:98.61ms
step:1389/1770 train_time:135989ms step_avg:98.61ms
step:1390/1770 train_time:136092ms step_avg:98.62ms
step:1391/1770 train_time:136194ms step_avg:98.62ms
step:1392/1770 train_time:136296ms step_avg:98.62ms
step:1393/1770 train_time:136398ms step_avg:98.62ms
step:1394/1770 train_time:136500ms step_avg:98.63ms
step:1395/1770 train_time:136603ms step_avg:98.63ms
step:1396/1770 train_time:136707ms step_avg:98.63ms
step:1397/1770 train_time:136809ms step_avg:98.64ms
step:1398/1770 train_time:136912ms step_avg:98.64ms
step:1399/1770 train_time:137014ms step_avg:98.64ms
step:1400/1770 train_time:137117ms step_avg:98.65ms
step:1401/1770 train_time:137220ms step_avg:98.65ms
step:1402/1770 train_time:137323ms step_avg:98.65ms
step:1403/1770 train_time:137425ms step_avg:98.65ms
step:1404/1770 train_time:137528ms step_avg:98.66ms
step:1405/1770 train_time:137630ms step_avg:98.66ms
step:1406/1770 train_time:137733ms step_avg:98.66ms
step:1407/1770 train_time:137835ms step_avg:98.66ms
step:1408/1770 train_time:137938ms step_avg:98.67ms
step:1409/1770 train_time:138042ms step_avg:98.67ms
step:1410/1770 train_time:138144ms step_avg:98.67ms
step:1411/1770 train_time:138246ms step_avg:98.68ms
step:1412/1770 train_time:138348ms step_avg:98.68ms
step:1413/1770 train_time:138449ms step_avg:98.68ms
step:1414/1770 train_time:138552ms step_avg:98.68ms
step:1415/1770 train_time:138654ms step_avg:98.69ms
step:1416/1770 train_time:138757ms step_avg:98.69ms
step:1417/1770 train_time:138860ms step_avg:98.69ms
step:1418/1770 train_time:138962ms step_avg:98.69ms
step:1419/1770 train_time:139065ms step_avg:98.70ms
step:1420/1770 train_time:139166ms step_avg:98.70ms
step:1421/1770 train_time:139268ms step_avg:98.70ms
step:1422/1770 train_time:139370ms step_avg:98.70ms
step:1423/1770 train_time:139472ms step_avg:98.71ms
step:1424/1770 train_time:139574ms step_avg:98.71ms
step:1425/1770 train_time:139676ms step_avg:98.71ms
step:1426/1770 train_time:139779ms step_avg:98.71ms
step:1427/1770 train_time:139881ms step_avg:98.72ms
step:1428/1770 train_time:139984ms step_avg:98.72ms
step:1429/1770 train_time:140087ms step_avg:98.72ms
step:1430/1770 train_time:140188ms step_avg:98.72ms
step:1431/1770 train_time:140293ms step_avg:98.73ms
step:1432/1770 train_time:140394ms step_avg:98.73ms
step:1433/1770 train_time:140496ms step_avg:98.73ms
step:1434/1770 train_time:140597ms step_avg:98.73ms
step:1435/1770 train_time:140699ms step_avg:98.74ms
step:1436/1770 train_time:140803ms step_avg:98.74ms
step:1437/1770 train_time:140906ms step_avg:98.74ms
step:1438/1770 train_time:141007ms step_avg:98.74ms
step:1439/1770 train_time:141109ms step_avg:98.75ms
step:1440/1770 train_time:141212ms step_avg:98.75ms
step:1441/1770 train_time:141317ms step_avg:98.75ms
step:1442/1770 train_time:141418ms step_avg:98.76ms
step:1443/1770 train_time:141520ms step_avg:98.76ms
step:1444/1770 train_time:141623ms step_avg:98.76ms
step:1445/1770 train_time:141726ms step_avg:98.76ms
step:1446/1770 train_time:141829ms step_avg:98.77ms
step:1447/1770 train_time:141933ms step_avg:98.77ms
step:1448/1770 train_time:142036ms step_avg:98.77ms
step:1449/1770 train_time:142140ms step_avg:98.78ms
step:1450/1770 train_time:142244ms step_avg:98.78ms
step:1451/1770 train_time:142348ms step_avg:98.78ms
step:1452/1770 train_time:142451ms step_avg:98.79ms
step:1453/1770 train_time:142553ms step_avg:98.79ms
step:1454/1770 train_time:142657ms step_avg:98.79ms
step:1455/1770 train_time:142762ms step_avg:98.80ms
step:1456/1770 train_time:142867ms step_avg:98.80ms
step:1457/1770 train_time:142970ms step_avg:98.80ms
step:1458/1770 train_time:143073ms step_avg:98.81ms
step:1459/1770 train_time:143178ms step_avg:98.81ms
step:1460/1770 train_time:143281ms step_avg:98.81ms
step:1461/1770 train_time:143386ms step_avg:98.82ms
step:1462/1770 train_time:143489ms step_avg:98.82ms
step:1463/1770 train_time:143593ms step_avg:98.83ms
step:1464/1770 train_time:143698ms step_avg:98.83ms
step:1465/1770 train_time:143801ms step_avg:98.83ms
step:1466/1770 train_time:143905ms step_avg:98.84ms
step:1467/1770 train_time:144009ms step_avg:98.84ms
step:1468/1770 train_time:144113ms step_avg:98.84ms
step:1469/1770 train_time:144216ms step_avg:98.85ms
step:1470/1770 train_time:144319ms step_avg:98.85ms
step:1471/1770 train_time:144422ms step_avg:98.85ms
step:1472/1770 train_time:144525ms step_avg:98.85ms
step:1473/1770 train_time:144629ms step_avg:98.86ms
step:1474/1770 train_time:144733ms step_avg:98.86ms
step:1475/1770 train_time:144836ms step_avg:98.86ms
step:1476/1770 train_time:144939ms step_avg:98.87ms
step:1477/1770 train_time:145045ms step_avg:98.87ms
step:1478/1770 train_time:145149ms step_avg:98.88ms
step:1479/1770 train_time:145253ms step_avg:98.88ms
step:1480/1770 train_time:145354ms step_avg:98.88ms
step:1481/1770 train_time:145460ms step_avg:98.89ms
step:1482/1770 train_time:145564ms step_avg:98.89ms
step:1483/1770 train_time:145668ms step_avg:98.89ms
step:1484/1770 train_time:145771ms step_avg:98.89ms
step:1485/1770 train_time:145874ms step_avg:98.90ms
step:1486/1770 train_time:145976ms step_avg:98.90ms
step:1487/1770 train_time:146081ms step_avg:98.90ms
step:1488/1770 train_time:146187ms step_avg:98.91ms
step:1489/1770 train_time:146291ms step_avg:98.91ms
step:1490/1770 train_time:146394ms step_avg:98.92ms
step:1491/1770 train_time:146498ms step_avg:98.92ms
step:1492/1770 train_time:146602ms step_avg:98.92ms
step:1493/1770 train_time:146708ms step_avg:98.93ms
step:1494/1770 train_time:146815ms step_avg:98.93ms
step:1495/1770 train_time:146917ms step_avg:98.93ms
step:1496/1770 train_time:147020ms step_avg:98.94ms
step:1497/1770 train_time:147125ms step_avg:98.94ms
step:1498/1770 train_time:147228ms step_avg:98.94ms
step:1499/1770 train_time:147330ms step_avg:98.95ms
step:1500/1770 train_time:147433ms step_avg:98.95ms
step:1500/1770 val_loss:3.3459 train_time:147534ms step_avg:99.02ms
step:1501/1770 train_time:147554ms step_avg:98.96ms
step:1502/1770 train_time:147647ms step_avg:98.96ms
step:1503/1770 train_time:147750ms step_avg:98.96ms
step:1504/1770 train_time:147853ms step_avg:98.96ms
step:1505/1770 train_time:147958ms step_avg:98.97ms
step:1506/1770 train_time:148060ms step_avg:98.97ms
step:1507/1770 train_time:148164ms step_avg:98.97ms
step:1508/1770 train_time:148269ms step_avg:98.98ms
step:1509/1770 train_time:148372ms step_avg:98.98ms
step:1510/1770 train_time:148474ms step_avg:98.98ms
step:1511/1770 train_time:148578ms step_avg:98.99ms
step:1512/1770 train_time:148683ms step_avg:98.99ms
step:1513/1770 train_time:148788ms step_avg:98.99ms
step:1514/1770 train_time:148891ms step_avg:99.00ms
step:1515/1770 train_time:148994ms step_avg:99.00ms
step:1516/1770 train_time:149098ms step_avg:99.00ms
step:1517/1770 train_time:149201ms step_avg:99.01ms
step:1518/1770 train_time:149307ms step_avg:99.01ms
step:1519/1770 train_time:149408ms step_avg:99.01ms
step:1520/1770 train_time:149513ms step_avg:99.01ms
step:1521/1770 train_time:149616ms step_avg:99.02ms
step:1522/1770 train_time:149720ms step_avg:99.02ms
step:1523/1770 train_time:149824ms step_avg:99.02ms
step:1524/1770 train_time:149927ms step_avg:99.03ms
step:1525/1770 train_time:150030ms step_avg:99.03ms
step:1526/1770 train_time:150133ms step_avg:99.03ms
step:1527/1770 train_time:150236ms step_avg:99.03ms
step:1528/1770 train_time:150341ms step_avg:99.04ms
step:1529/1770 train_time:150444ms step_avg:99.04ms
step:1530/1770 train_time:150548ms step_avg:99.04ms
step:1531/1770 train_time:150651ms step_avg:99.05ms
step:1532/1770 train_time:150755ms step_avg:99.05ms
step:1533/1770 train_time:150859ms step_avg:99.05ms
step:1534/1770 train_time:150964ms step_avg:99.06ms
step:1535/1770 train_time:151066ms step_avg:99.06ms
step:1536/1770 train_time:151169ms step_avg:99.06ms
step:1537/1770 train_time:151272ms step_avg:99.06ms
step:1538/1770 train_time:151378ms step_avg:99.07ms
step:1539/1770 train_time:151482ms step_avg:99.07ms
step:1540/1770 train_time:151588ms step_avg:99.08ms
step:1541/1770 train_time:151692ms step_avg:99.08ms
step:1542/1770 train_time:151796ms step_avg:99.08ms
step:1543/1770 train_time:151899ms step_avg:99.09ms
step:1544/1770 train_time:152004ms step_avg:99.09ms
step:1545/1770 train_time:152107ms step_avg:99.09ms
step:1546/1770 train_time:152211ms step_avg:99.10ms
step:1547/1770 train_time:152314ms step_avg:99.10ms
step:1548/1770 train_time:152417ms step_avg:99.10ms
step:1549/1770 train_time:152521ms step_avg:99.10ms
step:1550/1770 train_time:152625ms step_avg:99.11ms
step:1551/1770 train_time:152729ms step_avg:99.11ms
step:1552/1770 train_time:152835ms step_avg:99.11ms
step:1553/1770 train_time:152938ms step_avg:99.12ms
step:1554/1770 train_time:153041ms step_avg:99.12ms
step:1555/1770 train_time:153145ms step_avg:99.12ms
step:1556/1770 train_time:153248ms step_avg:99.13ms
step:1557/1770 train_time:153351ms step_avg:99.13ms
step:1558/1770 train_time:153454ms step_avg:99.13ms
step:1559/1770 train_time:153558ms step_avg:99.13ms
step:1560/1770 train_time:153661ms step_avg:99.14ms
step:1561/1770 train_time:153766ms step_avg:99.14ms
step:1562/1770 train_time:153869ms step_avg:99.14ms
step:1563/1770 train_time:153973ms step_avg:99.15ms
step:1564/1770 train_time:154077ms step_avg:99.15ms
step:1565/1770 train_time:154181ms step_avg:99.15ms
step:1566/1770 train_time:154284ms step_avg:99.15ms
step:1567/1770 train_time:154389ms step_avg:99.16ms
step:1568/1770 train_time:154491ms step_avg:99.16ms
step:1569/1770 train_time:154598ms step_avg:99.16ms
step:1570/1770 train_time:154701ms step_avg:99.17ms
step:1571/1770 train_time:154804ms step_avg:99.17ms
step:1572/1770 train_time:154909ms step_avg:99.17ms
step:1573/1770 train_time:155014ms step_avg:99.18ms
step:1574/1770 train_time:155118ms step_avg:99.18ms
step:1575/1770 train_time:155220ms step_avg:99.18ms
step:1576/1770 train_time:155322ms step_avg:99.18ms
step:1577/1770 train_time:155427ms step_avg:99.19ms
step:1578/1770 train_time:155532ms step_avg:99.19ms
step:1579/1770 train_time:155634ms step_avg:99.19ms
step:1580/1770 train_time:155738ms step_avg:99.20ms
step:1581/1770 train_time:155845ms step_avg:99.20ms
step:1582/1770 train_time:155949ms step_avg:99.20ms
step:1583/1770 train_time:156053ms step_avg:99.21ms
step:1584/1770 train_time:156157ms step_avg:99.21ms
step:1585/1770 train_time:156260ms step_avg:99.21ms
step:1586/1770 train_time:156367ms step_avg:99.22ms
step:1587/1770 train_time:156470ms step_avg:99.22ms
step:1588/1770 train_time:156578ms step_avg:99.23ms
step:1589/1770 train_time:156679ms step_avg:99.23ms
step:1590/1770 train_time:156782ms step_avg:99.23ms
step:1591/1770 train_time:156885ms step_avg:99.23ms
step:1592/1770 train_time:156990ms step_avg:99.24ms
step:1593/1770 train_time:157094ms step_avg:99.24ms
step:1594/1770 train_time:157197ms step_avg:99.24ms
step:1595/1770 train_time:157301ms step_avg:99.24ms
step:1596/1770 train_time:157405ms step_avg:99.25ms
step:1597/1770 train_time:157508ms step_avg:99.25ms
step:1598/1770 train_time:157611ms step_avg:99.25ms
step:1599/1770 train_time:157716ms step_avg:99.25ms
step:1600/1770 train_time:157822ms step_avg:99.26ms
step:1601/1770 train_time:157925ms step_avg:99.26ms
step:1602/1770 train_time:158031ms step_avg:99.27ms
step:1603/1770 train_time:158135ms step_avg:99.27ms
step:1604/1770 train_time:158237ms step_avg:99.27ms
step:1605/1770 train_time:158340ms step_avg:99.27ms
step:1606/1770 train_time:158444ms step_avg:99.28ms
step:1607/1770 train_time:158551ms step_avg:99.28ms
step:1608/1770 train_time:158654ms step_avg:99.28ms
step:1609/1770 train_time:158758ms step_avg:99.29ms
step:1610/1770 train_time:158863ms step_avg:99.29ms
step:1611/1770 train_time:158969ms step_avg:99.29ms
step:1612/1770 train_time:159074ms step_avg:99.30ms
step:1613/1770 train_time:159176ms step_avg:99.30ms
step:1614/1770 train_time:159281ms step_avg:99.30ms
step:1615/1770 train_time:159385ms step_avg:99.31ms
step:1616/1770 train_time:159488ms step_avg:99.31ms
step:1617/1770 train_time:159593ms step_avg:99.31ms
step:1618/1770 train_time:159698ms step_avg:99.31ms
step:1619/1770 train_time:159802ms step_avg:99.32ms
step:1620/1770 train_time:159906ms step_avg:99.32ms
step:1621/1770 train_time:160009ms step_avg:99.32ms
step:1622/1770 train_time:160113ms step_avg:99.33ms
step:1623/1770 train_time:160219ms step_avg:99.33ms
step:1624/1770 train_time:160322ms step_avg:99.33ms
step:1625/1770 train_time:160424ms step_avg:99.33ms
step:1625/1770 val_loss:3.3110 train_time:160527ms step_avg:99.40ms
step:1626/1770 train_time:160548ms step_avg:99.35ms
step:1627/1770 train_time:160637ms step_avg:99.34ms
step:1628/1770 train_time:160741ms step_avg:99.35ms
step:1629/1770 train_time:160844ms step_avg:99.35ms
step:1630/1770 train_time:160947ms step_avg:99.35ms
step:1631/1770 train_time:161050ms step_avg:99.35ms
step:1632/1770 train_time:161153ms step_avg:99.35ms
step:1633/1770 train_time:161256ms step_avg:99.36ms
step:1634/1770 train_time:161359ms step_avg:99.36ms
step:1635/1770 train_time:161462ms step_avg:99.36ms
step:1636/1770 train_time:161566ms step_avg:99.36ms
step:1637/1770 train_time:161671ms step_avg:99.37ms
step:1638/1770 train_time:161774ms step_avg:99.37ms
step:1639/1770 train_time:161878ms step_avg:99.37ms
step:1640/1770 train_time:161982ms step_avg:99.38ms
step:1641/1770 train_time:162086ms step_avg:99.38ms
step:1642/1770 train_time:162189ms step_avg:99.38ms
step:1643/1770 train_time:162292ms step_avg:99.38ms
step:1644/1770 train_time:162397ms step_avg:99.39ms
step:1645/1770 train_time:162501ms step_avg:99.39ms
step:1646/1770 train_time:162606ms step_avg:99.39ms
step:1647/1770 train_time:162712ms step_avg:99.40ms
step:1648/1770 train_time:162814ms step_avg:99.40ms
step:1649/1770 train_time:162917ms step_avg:99.40ms
step:1650/1770 train_time:163021ms step_avg:99.40ms
step:1651/1770 train_time:163123ms step_avg:99.40ms
step:1652/1770 train_time:163227ms step_avg:99.41ms
step:1653/1770 train_time:163330ms step_avg:99.41ms
step:1654/1770 train_time:163438ms step_avg:99.41ms
step:1655/1770 train_time:163544ms step_avg:99.42ms
step:1656/1770 train_time:163648ms step_avg:99.42ms
step:1657/1770 train_time:163753ms step_avg:99.42ms
step:1658/1770 train_time:163856ms step_avg:99.43ms
step:1659/1770 train_time:163961ms step_avg:99.43ms
step:1660/1770 train_time:164064ms step_avg:99.43ms
step:1661/1770 train_time:164169ms step_avg:99.44ms
step:1662/1770 train_time:164272ms step_avg:99.44ms
step:1663/1770 train_time:164375ms step_avg:99.44ms
step:1664/1770 train_time:164478ms step_avg:99.44ms
step:1665/1770 train_time:164582ms step_avg:99.45ms
step:1666/1770 train_time:164686ms step_avg:99.45ms
step:1667/1770 train_time:164789ms step_avg:99.45ms
step:1668/1770 train_time:164892ms step_avg:99.45ms
step:1669/1770 train_time:164994ms step_avg:99.45ms
step:1670/1770 train_time:165098ms step_avg:99.46ms
step:1671/1770 train_time:165203ms step_avg:99.46ms
step:1672/1770 train_time:165306ms step_avg:99.46ms
step:1673/1770 train_time:165411ms step_avg:99.47ms
step:1674/1770 train_time:165514ms step_avg:99.47ms
step:1675/1770 train_time:165617ms step_avg:99.47ms
step:1676/1770 train_time:165722ms step_avg:99.47ms
step:1677/1770 train_time:165829ms step_avg:99.48ms
step:1678/1770 train_time:165932ms step_avg:99.48ms
step:1679/1770 train_time:166035ms step_avg:99.48ms
step:1680/1770 train_time:166139ms step_avg:99.48ms
step:1681/1770 train_time:166244ms step_avg:99.49ms
step:1682/1770 train_time:166349ms step_avg:99.49ms
step:1683/1770 train_time:166452ms step_avg:99.49ms
step:1684/1770 train_time:166555ms step_avg:99.50ms
step:1685/1770 train_time:166660ms step_avg:99.50ms
step:1686/1770 train_time:166765ms step_avg:99.50ms
step:1687/1770 train_time:166870ms step_avg:99.50ms
step:1688/1770 train_time:166972ms step_avg:99.51ms
step:1689/1770 train_time:167076ms step_avg:99.51ms
step:1690/1770 train_time:167180ms step_avg:99.51ms
step:1691/1770 train_time:167284ms step_avg:99.51ms
step:1692/1770 train_time:167387ms step_avg:99.52ms
step:1693/1770 train_time:167492ms step_avg:99.52ms
step:1694/1770 train_time:167596ms step_avg:99.52ms
step:1695/1770 train_time:167700ms step_avg:99.53ms
step:1696/1770 train_time:167805ms step_avg:99.53ms
step:1697/1770 train_time:167910ms step_avg:99.53ms
step:1698/1770 train_time:168014ms step_avg:99.53ms
step:1699/1770 train_time:168117ms step_avg:99.54ms
step:1700/1770 train_time:168220ms step_avg:99.54ms
step:1701/1770 train_time:168323ms step_avg:99.54ms
step:1702/1770 train_time:168427ms step_avg:99.54ms
step:1703/1770 train_time:168531ms step_avg:99.55ms
step:1704/1770 train_time:168633ms step_avg:99.55ms
step:1705/1770 train_time:168736ms step_avg:99.55ms
step:1706/1770 train_time:168840ms step_avg:99.55ms
step:1707/1770 train_time:168944ms step_avg:99.55ms
step:1708/1770 train_time:169050ms step_avg:99.56ms
step:1709/1770 train_time:169154ms step_avg:99.56ms
step:1710/1770 train_time:169262ms step_avg:99.57ms
step:1711/1770 train_time:169368ms step_avg:99.57ms
step:1712/1770 train_time:169472ms step_avg:99.57ms
step:1713/1770 train_time:169575ms step_avg:99.57ms
step:1714/1770 train_time:169679ms step_avg:99.58ms
step:1715/1770 train_time:169781ms step_avg:99.58ms
step:1716/1770 train_time:169886ms step_avg:99.58ms
step:1717/1770 train_time:169991ms step_avg:99.58ms
step:1718/1770 train_time:170095ms step_avg:99.59ms
step:1719/1770 train_time:170200ms step_avg:99.59ms
step:1720/1770 train_time:170305ms step_avg:99.59ms
step:1721/1770 train_time:170413ms step_avg:99.60ms
step:1722/1770 train_time:170516ms step_avg:99.60ms
step:1723/1770 train_time:170622ms step_avg:99.60ms
step:1724/1770 train_time:170727ms step_avg:99.61ms
step:1725/1770 train_time:170834ms step_avg:99.61ms
step:1726/1770 train_time:170940ms step_avg:99.62ms
step:1727/1770 train_time:171044ms step_avg:99.62ms
step:1728/1770 train_time:171149ms step_avg:99.62ms
step:1729/1770 train_time:171252ms step_avg:99.62ms
step:1730/1770 train_time:171358ms step_avg:99.63ms
step:1731/1770 train_time:171463ms step_avg:99.63ms
step:1732/1770 train_time:171568ms step_avg:99.63ms
step:1733/1770 train_time:171674ms step_avg:99.64ms
step:1734/1770 train_time:171778ms step_avg:99.64ms
step:1735/1770 train_time:171884ms step_avg:99.64ms
step:1736/1770 train_time:171987ms step_avg:99.64ms
step:1737/1770 train_time:172091ms step_avg:99.65ms
step:1738/1770 train_time:172195ms step_avg:99.65ms
step:1739/1770 train_time:172299ms step_avg:99.65ms
step:1740/1770 train_time:172404ms step_avg:99.66ms
step:1741/1770 train_time:172511ms step_avg:99.66ms
step:1742/1770 train_time:172619ms step_avg:99.66ms
step:1743/1770 train_time:172725ms step_avg:99.67ms
step:1744/1770 train_time:172829ms step_avg:99.67ms
step:1745/1770 train_time:172933ms step_avg:99.67ms
step:1746/1770 train_time:173041ms step_avg:99.68ms
step:1747/1770 train_time:173143ms step_avg:99.68ms
step:1748/1770 train_time:173249ms step_avg:99.68ms
step:1749/1770 train_time:173354ms step_avg:99.69ms
step:1750/1770 train_time:173458ms step_avg:99.69ms
step:1750/1770 val_loss:3.2842 train_time:173561ms step_avg:99.75ms
step:1751/1770 train_time:173582ms step_avg:99.70ms
step:1752/1770 train_time:173674ms step_avg:99.70ms
step:1753/1770 train_time:173778ms step_avg:99.70ms
step:1754/1770 train_time:173883ms step_avg:99.70ms
step:1755/1770 train_time:173986ms step_avg:99.71ms
step:1756/1770 train_time:174091ms step_avg:99.71ms
step:1757/1770 train_time:174195ms step_avg:99.71ms
step:1758/1770 train_time:174299ms step_avg:99.71ms
step:1759/1770 train_time:174404ms step_avg:99.72ms
step:1760/1770 train_time:174509ms step_avg:99.72ms
step:1761/1770 train_time:174616ms step_avg:99.72ms
step:1762/1770 train_time:174724ms step_avg:99.73ms
step:1763/1770 train_time:174827ms step_avg:99.73ms
step:1764/1770 train_time:174932ms step_avg:99.73ms
step:1765/1770 train_time:175036ms step_avg:99.74ms
step:1766/1770 train_time:175145ms step_avg:99.74ms
step:1767/1770 train_time:175247ms step_avg:99.74ms
step:1768/1770 train_time:175352ms step_avg:99.75ms
step:1769/1770 train_time:175456ms step_avg:99.75ms
step:1770/1770 train_time:175560ms step_avg:99.75ms
step:1770/1770 val_loss:3.2812 train_time:175665ms step_avg:99.81ms
peak memory allocated: 28840 MiB reserved: 32292 MiB
