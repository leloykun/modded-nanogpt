import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:58:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24487ms step_avg:nanms
step:2/1770 train_time:25027ms step_avg:nanms
step:3/1770 train_time:25125ms step_avg:nanms
step:4/1770 train_time:25217ms step_avg:nanms
step:5/1770 train_time:25311ms step_avg:nanms
step:6/1770 train_time:25404ms step_avg:nanms
step:7/1770 train_time:25498ms step_avg:nanms
step:8/1770 train_time:25591ms step_avg:nanms
step:9/1770 train_time:25685ms step_avg:nanms
step:10/1770 train_time:25779ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.95ms
step:14/1770 train_time:377ms step_avg:94.17ms
step:15/1770 train_time:471ms step_avg:94.26ms
step:16/1770 train_time:566ms step_avg:94.32ms
step:17/1770 train_time:660ms step_avg:94.27ms
step:18/1770 train_time:755ms step_avg:94.32ms
step:19/1770 train_time:848ms step_avg:94.27ms
step:20/1770 train_time:942ms step_avg:94.24ms
step:21/1770 train_time:1036ms step_avg:94.18ms
step:22/1770 train_time:1130ms step_avg:94.14ms
step:23/1770 train_time:1224ms step_avg:94.17ms
step:24/1770 train_time:1318ms step_avg:94.13ms
step:25/1770 train_time:1412ms step_avg:94.14ms
step:26/1770 train_time:1506ms step_avg:94.13ms
step:27/1770 train_time:1600ms step_avg:94.12ms
step:28/1770 train_time:1694ms step_avg:94.11ms
step:29/1770 train_time:1788ms step_avg:94.12ms
step:30/1770 train_time:1882ms step_avg:94.11ms
step:31/1770 train_time:1976ms step_avg:94.09ms
step:32/1770 train_time:2070ms step_avg:94.08ms
step:33/1770 train_time:2164ms step_avg:94.09ms
step:34/1770 train_time:2258ms step_avg:94.08ms
step:35/1770 train_time:2352ms step_avg:94.08ms
step:36/1770 train_time:2447ms step_avg:94.10ms
step:37/1770 train_time:2541ms step_avg:94.10ms
step:38/1770 train_time:2635ms step_avg:94.09ms
step:39/1770 train_time:2729ms step_avg:94.09ms
step:40/1770 train_time:2822ms step_avg:94.07ms
step:41/1770 train_time:2917ms step_avg:94.09ms
step:42/1770 train_time:3011ms step_avg:94.10ms
step:43/1770 train_time:3105ms step_avg:94.09ms
step:44/1770 train_time:3198ms step_avg:94.07ms
step:45/1770 train_time:3292ms step_avg:94.07ms
step:46/1770 train_time:3387ms step_avg:94.07ms
step:47/1770 train_time:3481ms step_avg:94.08ms
step:48/1770 train_time:3575ms step_avg:94.08ms
step:49/1770 train_time:3669ms step_avg:94.08ms
step:50/1770 train_time:3764ms step_avg:94.09ms
step:51/1770 train_time:3858ms step_avg:94.09ms
step:52/1770 train_time:3951ms step_avg:94.08ms
step:53/1770 train_time:4045ms step_avg:94.08ms
step:54/1770 train_time:4139ms step_avg:94.08ms
step:55/1770 train_time:4233ms step_avg:94.07ms
step:56/1770 train_time:4327ms step_avg:94.07ms
step:57/1770 train_time:4421ms step_avg:94.06ms
step:58/1770 train_time:4515ms step_avg:94.05ms
step:59/1770 train_time:4608ms step_avg:94.05ms
step:60/1770 train_time:4702ms step_avg:94.03ms
step:61/1770 train_time:4795ms step_avg:94.03ms
step:62/1770 train_time:4890ms step_avg:94.05ms
step:63/1770 train_time:4985ms step_avg:94.05ms
step:64/1770 train_time:5079ms step_avg:94.05ms
step:65/1770 train_time:5173ms step_avg:94.05ms
step:66/1770 train_time:5268ms step_avg:94.06ms
step:67/1770 train_time:5361ms step_avg:94.05ms
step:68/1770 train_time:5455ms step_avg:94.04ms
step:69/1770 train_time:5549ms step_avg:94.04ms
step:70/1770 train_time:5643ms step_avg:94.05ms
step:71/1770 train_time:5736ms step_avg:94.04ms
step:72/1770 train_time:5830ms step_avg:94.04ms
step:73/1770 train_time:5925ms step_avg:94.04ms
step:74/1770 train_time:6018ms step_avg:94.04ms
step:75/1770 train_time:6112ms step_avg:94.03ms
step:76/1770 train_time:6206ms step_avg:94.04ms
step:77/1770 train_time:6301ms step_avg:94.04ms
step:78/1770 train_time:6395ms step_avg:94.04ms
step:79/1770 train_time:6489ms step_avg:94.04ms
step:80/1770 train_time:6583ms step_avg:94.05ms
step:81/1770 train_time:6677ms step_avg:94.05ms
step:82/1770 train_time:6771ms step_avg:94.04ms
step:83/1770 train_time:6865ms step_avg:94.04ms
step:84/1770 train_time:6958ms step_avg:94.03ms
step:85/1770 train_time:7052ms step_avg:94.03ms
step:86/1770 train_time:7146ms step_avg:94.02ms
step:87/1770 train_time:7239ms step_avg:94.02ms
step:88/1770 train_time:7334ms step_avg:94.02ms
step:89/1770 train_time:7428ms step_avg:94.02ms
step:90/1770 train_time:7522ms step_avg:94.02ms
step:91/1770 train_time:7715ms step_avg:95.25ms
step:92/1770 train_time:7764ms step_avg:94.69ms
step:93/1770 train_time:7857ms step_avg:94.67ms
step:94/1770 train_time:7951ms step_avg:94.65ms
step:95/1770 train_time:8045ms step_avg:94.64ms
step:96/1770 train_time:8138ms step_avg:94.63ms
step:97/1770 train_time:8231ms step_avg:94.61ms
step:98/1770 train_time:8325ms step_avg:94.60ms
step:99/1770 train_time:8418ms step_avg:94.59ms
step:100/1770 train_time:8512ms step_avg:94.58ms
step:101/1770 train_time:8606ms step_avg:94.57ms
step:102/1770 train_time:8701ms step_avg:94.58ms
step:103/1770 train_time:8796ms step_avg:94.58ms
step:104/1770 train_time:8890ms step_avg:94.58ms
step:105/1770 train_time:8985ms step_avg:94.57ms
step:106/1770 train_time:9078ms step_avg:94.56ms
step:107/1770 train_time:9171ms step_avg:94.55ms
step:108/1770 train_time:9265ms step_avg:94.54ms
step:109/1770 train_time:9359ms step_avg:94.53ms
step:110/1770 train_time:9452ms step_avg:94.52ms
step:111/1770 train_time:9546ms step_avg:94.52ms
step:112/1770 train_time:9641ms step_avg:94.52ms
step:113/1770 train_time:9735ms step_avg:94.51ms
step:114/1770 train_time:9829ms step_avg:94.50ms
step:115/1770 train_time:9923ms step_avg:94.50ms
step:116/1770 train_time:10017ms step_avg:94.50ms
step:117/1770 train_time:10111ms step_avg:94.49ms
step:118/1770 train_time:10204ms step_avg:94.49ms
step:119/1770 train_time:10298ms step_avg:94.48ms
step:120/1770 train_time:10392ms step_avg:94.47ms
step:121/1770 train_time:10485ms step_avg:94.46ms
step:122/1770 train_time:10579ms step_avg:94.46ms
step:123/1770 train_time:10674ms step_avg:94.46ms
step:124/1770 train_time:10768ms step_avg:94.46ms
step:125/1770 train_time:10862ms step_avg:94.45ms
step:125/1770 val_loss:4.6443 train_time:10955ms step_avg:95.26ms
step:126/1770 train_time:10977ms step_avg:94.63ms
step:127/1770 train_time:11062ms step_avg:94.54ms
step:128/1770 train_time:11160ms step_avg:94.58ms
step:129/1770 train_time:11255ms step_avg:94.58ms
step:130/1770 train_time:11349ms step_avg:94.57ms
step:131/1770 train_time:11442ms step_avg:94.56ms
step:132/1770 train_time:11536ms step_avg:94.55ms
step:133/1770 train_time:11629ms step_avg:94.54ms
step:134/1770 train_time:11723ms step_avg:94.54ms
step:135/1770 train_time:11817ms step_avg:94.54ms
step:136/1770 train_time:11911ms step_avg:94.53ms
step:137/1770 train_time:12005ms step_avg:94.53ms
step:138/1770 train_time:12100ms step_avg:94.53ms
step:139/1770 train_time:12194ms step_avg:94.53ms
step:140/1770 train_time:12289ms step_avg:94.53ms
step:141/1770 train_time:12383ms step_avg:94.53ms
step:142/1770 train_time:12478ms step_avg:94.53ms
step:143/1770 train_time:12572ms step_avg:94.52ms
step:144/1770 train_time:12666ms step_avg:94.53ms
step:145/1770 train_time:12761ms step_avg:94.52ms
step:146/1770 train_time:12855ms step_avg:94.52ms
step:147/1770 train_time:12950ms step_avg:94.52ms
step:148/1770 train_time:13045ms step_avg:94.53ms
step:149/1770 train_time:13140ms step_avg:94.53ms
step:150/1770 train_time:13234ms step_avg:94.53ms
step:151/1770 train_time:13328ms step_avg:94.53ms
step:152/1770 train_time:13423ms step_avg:94.53ms
step:153/1770 train_time:13517ms step_avg:94.52ms
step:154/1770 train_time:13611ms step_avg:94.52ms
step:155/1770 train_time:13706ms step_avg:94.52ms
step:156/1770 train_time:13801ms step_avg:94.52ms
step:157/1770 train_time:13895ms step_avg:94.52ms
step:158/1770 train_time:13990ms step_avg:94.53ms
step:159/1770 train_time:14084ms step_avg:94.53ms
step:160/1770 train_time:14179ms step_avg:94.53ms
step:161/1770 train_time:14275ms step_avg:94.53ms
step:162/1770 train_time:14369ms step_avg:94.53ms
step:163/1770 train_time:14464ms step_avg:94.53ms
step:164/1770 train_time:14559ms step_avg:94.54ms
step:165/1770 train_time:14654ms step_avg:94.54ms
step:166/1770 train_time:14749ms step_avg:94.54ms
step:167/1770 train_time:14843ms step_avg:94.54ms
step:168/1770 train_time:14938ms step_avg:94.54ms
step:169/1770 train_time:15033ms step_avg:94.54ms
step:170/1770 train_time:15127ms step_avg:94.54ms
step:171/1770 train_time:15222ms step_avg:94.54ms
step:172/1770 train_time:15317ms step_avg:94.55ms
step:173/1770 train_time:15411ms step_avg:94.55ms
step:174/1770 train_time:15506ms step_avg:94.55ms
step:175/1770 train_time:15600ms step_avg:94.55ms
step:176/1770 train_time:15695ms step_avg:94.55ms
step:177/1770 train_time:15789ms step_avg:94.54ms
step:178/1770 train_time:15883ms step_avg:94.54ms
step:179/1770 train_time:15978ms step_avg:94.54ms
step:180/1770 train_time:16072ms step_avg:94.54ms
step:181/1770 train_time:16167ms step_avg:94.54ms
step:182/1770 train_time:16262ms step_avg:94.55ms
step:183/1770 train_time:16357ms step_avg:94.55ms
step:184/1770 train_time:16452ms step_avg:94.55ms
step:185/1770 train_time:16547ms step_avg:94.55ms
step:186/1770 train_time:16641ms step_avg:94.55ms
step:187/1770 train_time:16736ms step_avg:94.55ms
step:188/1770 train_time:16830ms step_avg:94.55ms
step:189/1770 train_time:16925ms step_avg:94.55ms
step:190/1770 train_time:17019ms step_avg:94.55ms
step:191/1770 train_time:17114ms step_avg:94.55ms
step:192/1770 train_time:17209ms step_avg:94.55ms
step:193/1770 train_time:17304ms step_avg:94.56ms
step:194/1770 train_time:17399ms step_avg:94.56ms
step:195/1770 train_time:17493ms step_avg:94.56ms
step:196/1770 train_time:17588ms step_avg:94.56ms
step:197/1770 train_time:17683ms step_avg:94.56ms
step:198/1770 train_time:17776ms step_avg:94.56ms
step:199/1770 train_time:17871ms step_avg:94.56ms
step:200/1770 train_time:17966ms step_avg:94.56ms
step:201/1770 train_time:18060ms step_avg:94.56ms
step:202/1770 train_time:18156ms step_avg:94.56ms
step:203/1770 train_time:18250ms step_avg:94.56ms
step:204/1770 train_time:18345ms step_avg:94.56ms
step:205/1770 train_time:18440ms step_avg:94.56ms
step:206/1770 train_time:18535ms step_avg:94.56ms
step:207/1770 train_time:18629ms step_avg:94.57ms
step:208/1770 train_time:18725ms step_avg:94.57ms
step:209/1770 train_time:18819ms step_avg:94.57ms
step:210/1770 train_time:18914ms step_avg:94.57ms
step:211/1770 train_time:19008ms step_avg:94.57ms
step:212/1770 train_time:19103ms step_avg:94.57ms
step:213/1770 train_time:19197ms step_avg:94.57ms
step:214/1770 train_time:19292ms step_avg:94.57ms
step:215/1770 train_time:19387ms step_avg:94.57ms
step:216/1770 train_time:19482ms step_avg:94.57ms
step:217/1770 train_time:19577ms step_avg:94.57ms
step:218/1770 train_time:19672ms step_avg:94.58ms
step:219/1770 train_time:19767ms step_avg:94.58ms
step:220/1770 train_time:19862ms step_avg:94.58ms
step:221/1770 train_time:19957ms step_avg:94.58ms
step:222/1770 train_time:20051ms step_avg:94.58ms
step:223/1770 train_time:20146ms step_avg:94.58ms
step:224/1770 train_time:20240ms step_avg:94.58ms
step:225/1770 train_time:20335ms step_avg:94.58ms
step:226/1770 train_time:20430ms step_avg:94.58ms
step:227/1770 train_time:20525ms step_avg:94.59ms
step:228/1770 train_time:20620ms step_avg:94.59ms
step:229/1770 train_time:20715ms step_avg:94.59ms
step:230/1770 train_time:20810ms step_avg:94.59ms
step:231/1770 train_time:20905ms step_avg:94.59ms
step:232/1770 train_time:21000ms step_avg:94.60ms
step:233/1770 train_time:21095ms step_avg:94.60ms
step:234/1770 train_time:21189ms step_avg:94.60ms
step:235/1770 train_time:21284ms step_avg:94.59ms
step:236/1770 train_time:21378ms step_avg:94.59ms
step:237/1770 train_time:21472ms step_avg:94.59ms
step:238/1770 train_time:21567ms step_avg:94.59ms
step:239/1770 train_time:21662ms step_avg:94.59ms
step:240/1770 train_time:21757ms step_avg:94.60ms
step:241/1770 train_time:21851ms step_avg:94.59ms
step:242/1770 train_time:21946ms step_avg:94.60ms
step:243/1770 train_time:22041ms step_avg:94.59ms
step:244/1770 train_time:22135ms step_avg:94.60ms
step:245/1770 train_time:22230ms step_avg:94.59ms
step:246/1770 train_time:22324ms step_avg:94.59ms
step:247/1770 train_time:22418ms step_avg:94.59ms
step:248/1770 train_time:22513ms step_avg:94.59ms
step:249/1770 train_time:22608ms step_avg:94.60ms
step:250/1770 train_time:22703ms step_avg:94.60ms
step:250/1770 val_loss:4.1073 train_time:22796ms step_avg:94.98ms
step:251/1770 train_time:22819ms step_avg:94.68ms
step:252/1770 train_time:22903ms step_avg:94.64ms
step:253/1770 train_time:23002ms step_avg:94.66ms
step:254/1770 train_time:23097ms step_avg:94.66ms
step:255/1770 train_time:23191ms step_avg:94.66ms
step:256/1770 train_time:23285ms step_avg:94.66ms
step:257/1770 train_time:23379ms step_avg:94.65ms
step:258/1770 train_time:23473ms step_avg:94.65ms
step:259/1770 train_time:23568ms step_avg:94.65ms
step:260/1770 train_time:23662ms step_avg:94.65ms
step:261/1770 train_time:23756ms step_avg:94.65ms
step:262/1770 train_time:23851ms step_avg:94.65ms
step:263/1770 train_time:23946ms step_avg:94.65ms
step:264/1770 train_time:24042ms step_avg:94.65ms
step:265/1770 train_time:24138ms step_avg:94.66ms
step:266/1770 train_time:24233ms step_avg:94.66ms
step:267/1770 train_time:24328ms step_avg:94.66ms
step:268/1770 train_time:24423ms step_avg:94.66ms
step:269/1770 train_time:24518ms step_avg:94.66ms
step:270/1770 train_time:24613ms step_avg:94.66ms
step:271/1770 train_time:24707ms step_avg:94.66ms
step:272/1770 train_time:24803ms step_avg:94.67ms
step:273/1770 train_time:24898ms step_avg:94.67ms
step:274/1770 train_time:24993ms step_avg:94.67ms
step:275/1770 train_time:25088ms step_avg:94.67ms
step:276/1770 train_time:25184ms step_avg:94.68ms
step:277/1770 train_time:25280ms step_avg:94.68ms
step:278/1770 train_time:25375ms step_avg:94.68ms
step:279/1770 train_time:25470ms step_avg:94.68ms
step:280/1770 train_time:25566ms step_avg:94.69ms
step:281/1770 train_time:25661ms step_avg:94.69ms
step:282/1770 train_time:25756ms step_avg:94.69ms
step:283/1770 train_time:25851ms step_avg:94.69ms
step:284/1770 train_time:25946ms step_avg:94.69ms
step:285/1770 train_time:26041ms step_avg:94.70ms
step:286/1770 train_time:26137ms step_avg:94.70ms
step:287/1770 train_time:26233ms step_avg:94.70ms
step:288/1770 train_time:26327ms step_avg:94.70ms
step:289/1770 train_time:26422ms step_avg:94.70ms
step:290/1770 train_time:26517ms step_avg:94.70ms
step:291/1770 train_time:26612ms step_avg:94.71ms
step:292/1770 train_time:26708ms step_avg:94.71ms
step:293/1770 train_time:26802ms step_avg:94.71ms
step:294/1770 train_time:26898ms step_avg:94.71ms
step:295/1770 train_time:26992ms step_avg:94.71ms
step:296/1770 train_time:27088ms step_avg:94.71ms
step:297/1770 train_time:27183ms step_avg:94.71ms
step:298/1770 train_time:27278ms step_avg:94.72ms
step:299/1770 train_time:27373ms step_avg:94.72ms
step:300/1770 train_time:27468ms step_avg:94.72ms
step:301/1770 train_time:27564ms step_avg:94.72ms
step:302/1770 train_time:27659ms step_avg:94.72ms
step:303/1770 train_time:27754ms step_avg:94.72ms
step:304/1770 train_time:27849ms step_avg:94.73ms
step:305/1770 train_time:27945ms step_avg:94.73ms
step:306/1770 train_time:28040ms step_avg:94.73ms
step:307/1770 train_time:28134ms step_avg:94.73ms
step:308/1770 train_time:28230ms step_avg:94.73ms
step:309/1770 train_time:28325ms step_avg:94.73ms
step:310/1770 train_time:28420ms step_avg:94.73ms
step:311/1770 train_time:28515ms step_avg:94.74ms
step:312/1770 train_time:28611ms step_avg:94.74ms
step:313/1770 train_time:28706ms step_avg:94.74ms
step:314/1770 train_time:28801ms step_avg:94.74ms
step:315/1770 train_time:28895ms step_avg:94.74ms
step:316/1770 train_time:28991ms step_avg:94.74ms
step:317/1770 train_time:29086ms step_avg:94.74ms
step:318/1770 train_time:29180ms step_avg:94.74ms
step:319/1770 train_time:29276ms step_avg:94.74ms
step:320/1770 train_time:29371ms step_avg:94.75ms
step:321/1770 train_time:29467ms step_avg:94.75ms
step:322/1770 train_time:29563ms step_avg:94.75ms
step:323/1770 train_time:29658ms step_avg:94.75ms
step:324/1770 train_time:29753ms step_avg:94.75ms
step:325/1770 train_time:29848ms step_avg:94.75ms
step:326/1770 train_time:29943ms step_avg:94.75ms
step:327/1770 train_time:30037ms step_avg:94.76ms
step:328/1770 train_time:30132ms step_avg:94.75ms
step:329/1770 train_time:30227ms step_avg:94.76ms
step:330/1770 train_time:30322ms step_avg:94.76ms
step:331/1770 train_time:30417ms step_avg:94.76ms
step:332/1770 train_time:30512ms step_avg:94.76ms
step:333/1770 train_time:30607ms step_avg:94.76ms
step:334/1770 train_time:30702ms step_avg:94.76ms
step:335/1770 train_time:30797ms step_avg:94.76ms
step:336/1770 train_time:30891ms step_avg:94.76ms
step:337/1770 train_time:30987ms step_avg:94.76ms
step:338/1770 train_time:31082ms step_avg:94.76ms
step:339/1770 train_time:31177ms step_avg:94.76ms
step:340/1770 train_time:31272ms step_avg:94.76ms
step:341/1770 train_time:31367ms step_avg:94.77ms
step:342/1770 train_time:31463ms step_avg:94.77ms
step:343/1770 train_time:31558ms step_avg:94.77ms
step:344/1770 train_time:31653ms step_avg:94.77ms
step:345/1770 train_time:31748ms step_avg:94.77ms
step:346/1770 train_time:31843ms step_avg:94.77ms
step:347/1770 train_time:31938ms step_avg:94.77ms
step:348/1770 train_time:32032ms step_avg:94.77ms
step:349/1770 train_time:32127ms step_avg:94.77ms
step:350/1770 train_time:32222ms step_avg:94.77ms
step:351/1770 train_time:32317ms step_avg:94.77ms
step:352/1770 train_time:32412ms step_avg:94.77ms
step:353/1770 train_time:32507ms step_avg:94.77ms
step:354/1770 train_time:32603ms step_avg:94.78ms
step:355/1770 train_time:32698ms step_avg:94.78ms
step:356/1770 train_time:32792ms step_avg:94.78ms
step:357/1770 train_time:32888ms step_avg:94.78ms
step:358/1770 train_time:32983ms step_avg:94.78ms
step:359/1770 train_time:33078ms step_avg:94.78ms
step:360/1770 train_time:33173ms step_avg:94.78ms
step:361/1770 train_time:33268ms step_avg:94.78ms
step:362/1770 train_time:33364ms step_avg:94.78ms
step:363/1770 train_time:33459ms step_avg:94.79ms
step:364/1770 train_time:33554ms step_avg:94.79ms
step:365/1770 train_time:33649ms step_avg:94.79ms
step:366/1770 train_time:33744ms step_avg:94.79ms
step:367/1770 train_time:33839ms step_avg:94.79ms
step:368/1770 train_time:33933ms step_avg:94.79ms
step:369/1770 train_time:34029ms step_avg:94.79ms
step:370/1770 train_time:34124ms step_avg:94.79ms
step:371/1770 train_time:34219ms step_avg:94.79ms
step:372/1770 train_time:34313ms step_avg:94.79ms
step:373/1770 train_time:34409ms step_avg:94.79ms
step:374/1770 train_time:34504ms step_avg:94.79ms
step:375/1770 train_time:34598ms step_avg:94.79ms
step:375/1770 val_loss:3.9094 train_time:34691ms step_avg:95.04ms
step:376/1770 train_time:34713ms step_avg:94.84ms
step:377/1770 train_time:34798ms step_avg:94.82ms
step:378/1770 train_time:34898ms step_avg:94.83ms
step:379/1770 train_time:34995ms step_avg:94.84ms
step:380/1770 train_time:35089ms step_avg:94.84ms
step:381/1770 train_time:35184ms step_avg:94.83ms
step:382/1770 train_time:35278ms step_avg:94.83ms
step:383/1770 train_time:35372ms step_avg:94.83ms
step:384/1770 train_time:35467ms step_avg:94.83ms
step:385/1770 train_time:35561ms step_avg:94.83ms
step:386/1770 train_time:35656ms step_avg:94.83ms
step:387/1770 train_time:35751ms step_avg:94.83ms
step:388/1770 train_time:35847ms step_avg:94.83ms
step:389/1770 train_time:35943ms step_avg:94.84ms
step:390/1770 train_time:36038ms step_avg:94.84ms
step:391/1770 train_time:36134ms step_avg:94.84ms
step:392/1770 train_time:36230ms step_avg:94.84ms
step:393/1770 train_time:36324ms step_avg:94.84ms
step:394/1770 train_time:36419ms step_avg:94.84ms
step:395/1770 train_time:36514ms step_avg:94.84ms
step:396/1770 train_time:36611ms step_avg:94.85ms
step:397/1770 train_time:36707ms step_avg:94.85ms
step:398/1770 train_time:36804ms step_avg:94.86ms
step:399/1770 train_time:36901ms step_avg:94.86ms
step:400/1770 train_time:36998ms step_avg:94.87ms
step:401/1770 train_time:37095ms step_avg:94.87ms
step:402/1770 train_time:37193ms step_avg:94.88ms
step:403/1770 train_time:37290ms step_avg:94.89ms
step:404/1770 train_time:37387ms step_avg:94.89ms
step:405/1770 train_time:37483ms step_avg:94.89ms
step:406/1770 train_time:37580ms step_avg:94.90ms
step:407/1770 train_time:37677ms step_avg:94.91ms
step:408/1770 train_time:37775ms step_avg:94.91ms
step:409/1770 train_time:37872ms step_avg:94.92ms
step:410/1770 train_time:37968ms step_avg:94.92ms
step:411/1770 train_time:38066ms step_avg:94.93ms
step:412/1770 train_time:38163ms step_avg:94.93ms
step:413/1770 train_time:38260ms step_avg:94.94ms
step:414/1770 train_time:38357ms step_avg:94.94ms
step:415/1770 train_time:38454ms step_avg:94.95ms
step:416/1770 train_time:38551ms step_avg:94.95ms
step:417/1770 train_time:38647ms step_avg:94.96ms
step:418/1770 train_time:38744ms step_avg:94.96ms
step:419/1770 train_time:38841ms step_avg:94.96ms
step:420/1770 train_time:38937ms step_avg:94.97ms
step:421/1770 train_time:39035ms step_avg:94.98ms
step:422/1770 train_time:39132ms step_avg:94.98ms
step:423/1770 train_time:39228ms step_avg:94.98ms
step:424/1770 train_time:39325ms step_avg:94.99ms
step:425/1770 train_time:39423ms step_avg:94.99ms
step:426/1770 train_time:39520ms step_avg:95.00ms
step:427/1770 train_time:39616ms step_avg:95.00ms
step:428/1770 train_time:39713ms step_avg:95.01ms
step:429/1770 train_time:39810ms step_avg:95.01ms
step:430/1770 train_time:39906ms step_avg:95.02ms
step:431/1770 train_time:40004ms step_avg:95.02ms
step:432/1770 train_time:40101ms step_avg:95.03ms
step:433/1770 train_time:40198ms step_avg:95.03ms
step:434/1770 train_time:40295ms step_avg:95.03ms
step:435/1770 train_time:40391ms step_avg:95.04ms
step:436/1770 train_time:40488ms step_avg:95.04ms
step:437/1770 train_time:40585ms step_avg:95.05ms
step:438/1770 train_time:40683ms step_avg:95.05ms
step:439/1770 train_time:40780ms step_avg:95.06ms
step:440/1770 train_time:40877ms step_avg:95.06ms
step:441/1770 train_time:40974ms step_avg:95.07ms
step:442/1770 train_time:41071ms step_avg:95.07ms
step:443/1770 train_time:41167ms step_avg:95.07ms
step:444/1770 train_time:41265ms step_avg:95.08ms
step:445/1770 train_time:41361ms step_avg:95.08ms
step:446/1770 train_time:41458ms step_avg:95.09ms
step:447/1770 train_time:41556ms step_avg:95.09ms
step:448/1770 train_time:41653ms step_avg:95.10ms
step:449/1770 train_time:41750ms step_avg:95.10ms
step:450/1770 train_time:41847ms step_avg:95.11ms
step:451/1770 train_time:41944ms step_avg:95.11ms
step:452/1770 train_time:42041ms step_avg:95.12ms
step:453/1770 train_time:42138ms step_avg:95.12ms
step:454/1770 train_time:42235ms step_avg:95.12ms
step:455/1770 train_time:42332ms step_avg:95.13ms
step:456/1770 train_time:42429ms step_avg:95.13ms
step:457/1770 train_time:42526ms step_avg:95.14ms
step:458/1770 train_time:42624ms step_avg:95.14ms
step:459/1770 train_time:42721ms step_avg:95.15ms
step:460/1770 train_time:42818ms step_avg:95.15ms
step:461/1770 train_time:42915ms step_avg:95.16ms
step:462/1770 train_time:43012ms step_avg:95.16ms
step:463/1770 train_time:43109ms step_avg:95.16ms
step:464/1770 train_time:43207ms step_avg:95.17ms
step:465/1770 train_time:43305ms step_avg:95.18ms
step:466/1770 train_time:43402ms step_avg:95.18ms
step:467/1770 train_time:43499ms step_avg:95.18ms
step:468/1770 train_time:43597ms step_avg:95.19ms
step:469/1770 train_time:43693ms step_avg:95.19ms
step:470/1770 train_time:43790ms step_avg:95.20ms
step:471/1770 train_time:43887ms step_avg:95.20ms
step:472/1770 train_time:43984ms step_avg:95.20ms
step:473/1770 train_time:44081ms step_avg:95.21ms
step:474/1770 train_time:44178ms step_avg:95.21ms
step:475/1770 train_time:44275ms step_avg:95.22ms
step:476/1770 train_time:44372ms step_avg:95.22ms
step:477/1770 train_time:44469ms step_avg:95.22ms
step:478/1770 train_time:44567ms step_avg:95.23ms
step:479/1770 train_time:44664ms step_avg:95.23ms
step:480/1770 train_time:44761ms step_avg:95.24ms
step:481/1770 train_time:44858ms step_avg:95.24ms
step:482/1770 train_time:44955ms step_avg:95.24ms
step:483/1770 train_time:45052ms step_avg:95.25ms
step:484/1770 train_time:45149ms step_avg:95.25ms
step:485/1770 train_time:45246ms step_avg:95.25ms
step:486/1770 train_time:45343ms step_avg:95.26ms
step:487/1770 train_time:45440ms step_avg:95.26ms
step:488/1770 train_time:45537ms step_avg:95.27ms
step:489/1770 train_time:45634ms step_avg:95.27ms
step:490/1770 train_time:45730ms step_avg:95.27ms
step:491/1770 train_time:45827ms step_avg:95.28ms
step:492/1770 train_time:45925ms step_avg:95.28ms
step:493/1770 train_time:46023ms step_avg:95.28ms
step:494/1770 train_time:46120ms step_avg:95.29ms
step:495/1770 train_time:46217ms step_avg:95.29ms
step:496/1770 train_time:46313ms step_avg:95.29ms
step:497/1770 train_time:46410ms step_avg:95.30ms
step:498/1770 train_time:46507ms step_avg:95.30ms
step:499/1770 train_time:46604ms step_avg:95.30ms
step:500/1770 train_time:46701ms step_avg:95.31ms
step:500/1770 val_loss:3.7567 train_time:46796ms step_avg:95.50ms
step:501/1770 train_time:46817ms step_avg:95.35ms
step:502/1770 train_time:46905ms step_avg:95.33ms
step:503/1770 train_time:47005ms step_avg:95.35ms
step:504/1770 train_time:47102ms step_avg:95.35ms
step:505/1770 train_time:47199ms step_avg:95.35ms
step:506/1770 train_time:47295ms step_avg:95.35ms
step:507/1770 train_time:47392ms step_avg:95.36ms
step:508/1770 train_time:47489ms step_avg:95.36ms
step:509/1770 train_time:47586ms step_avg:95.36ms
step:510/1770 train_time:47683ms step_avg:95.37ms
step:511/1770 train_time:47779ms step_avg:95.37ms
step:512/1770 train_time:47877ms step_avg:95.37ms
step:513/1770 train_time:47976ms step_avg:95.38ms
step:514/1770 train_time:48073ms step_avg:95.38ms
step:515/1770 train_time:48170ms step_avg:95.39ms
step:516/1770 train_time:48267ms step_avg:95.39ms
step:517/1770 train_time:48364ms step_avg:95.39ms
step:518/1770 train_time:48462ms step_avg:95.40ms
step:519/1770 train_time:48559ms step_avg:95.40ms
step:520/1770 train_time:48655ms step_avg:95.40ms
step:521/1770 train_time:48752ms step_avg:95.41ms
step:522/1770 train_time:48849ms step_avg:95.41ms
step:523/1770 train_time:48947ms step_avg:95.41ms
step:524/1770 train_time:49044ms step_avg:95.42ms
step:525/1770 train_time:49141ms step_avg:95.42ms
step:526/1770 train_time:49238ms step_avg:95.42ms
step:527/1770 train_time:49335ms step_avg:95.43ms
step:528/1770 train_time:49433ms step_avg:95.43ms
step:529/1770 train_time:49529ms step_avg:95.43ms
step:530/1770 train_time:49627ms step_avg:95.44ms
step:531/1770 train_time:49725ms step_avg:95.44ms
step:532/1770 train_time:49823ms step_avg:95.45ms
step:533/1770 train_time:49920ms step_avg:95.45ms
step:534/1770 train_time:50017ms step_avg:95.45ms
step:535/1770 train_time:50115ms step_avg:95.46ms
step:536/1770 train_time:50212ms step_avg:95.46ms
step:537/1770 train_time:50309ms step_avg:95.46ms
step:538/1770 train_time:50406ms step_avg:95.47ms
step:539/1770 train_time:50504ms step_avg:95.47ms
step:540/1770 train_time:50601ms step_avg:95.47ms
step:541/1770 train_time:50698ms step_avg:95.48ms
step:542/1770 train_time:50796ms step_avg:95.48ms
step:543/1770 train_time:50894ms step_avg:95.49ms
step:544/1770 train_time:50991ms step_avg:95.49ms
step:545/1770 train_time:51089ms step_avg:95.49ms
step:546/1770 train_time:51187ms step_avg:95.50ms
step:547/1770 train_time:51285ms step_avg:95.50ms
step:548/1770 train_time:51382ms step_avg:95.51ms
step:549/1770 train_time:51479ms step_avg:95.51ms
step:550/1770 train_time:51576ms step_avg:95.51ms
step:551/1770 train_time:51674ms step_avg:95.52ms
step:552/1770 train_time:51771ms step_avg:95.52ms
step:553/1770 train_time:51869ms step_avg:95.52ms
step:554/1770 train_time:51966ms step_avg:95.53ms
step:555/1770 train_time:52064ms step_avg:95.53ms
step:556/1770 train_time:52161ms step_avg:95.53ms
step:557/1770 train_time:52259ms step_avg:95.54ms
step:558/1770 train_time:52357ms step_avg:95.54ms
step:559/1770 train_time:52455ms step_avg:95.55ms
step:560/1770 train_time:52553ms step_avg:95.55ms
step:561/1770 train_time:52650ms step_avg:95.55ms
step:562/1770 train_time:52748ms step_avg:95.56ms
step:563/1770 train_time:52845ms step_avg:95.56ms
step:564/1770 train_time:52942ms step_avg:95.56ms
step:565/1770 train_time:53039ms step_avg:95.57ms
step:566/1770 train_time:53136ms step_avg:95.57ms
step:567/1770 train_time:53233ms step_avg:95.57ms
step:568/1770 train_time:53331ms step_avg:95.58ms
step:569/1770 train_time:53429ms step_avg:95.58ms
step:570/1770 train_time:53527ms step_avg:95.58ms
step:571/1770 train_time:53625ms step_avg:95.59ms
step:572/1770 train_time:53722ms step_avg:95.59ms
step:573/1770 train_time:53819ms step_avg:95.59ms
step:574/1770 train_time:53917ms step_avg:95.60ms
step:575/1770 train_time:54014ms step_avg:95.60ms
step:576/1770 train_time:54111ms step_avg:95.60ms
step:577/1770 train_time:54209ms step_avg:95.61ms
step:578/1770 train_time:54306ms step_avg:95.61ms
step:579/1770 train_time:54404ms step_avg:95.61ms
step:580/1770 train_time:54501ms step_avg:95.62ms
step:581/1770 train_time:54599ms step_avg:95.62ms
step:582/1770 train_time:54696ms step_avg:95.62ms
step:583/1770 train_time:54794ms step_avg:95.63ms
step:584/1770 train_time:54891ms step_avg:95.63ms
step:585/1770 train_time:54988ms step_avg:95.63ms
step:586/1770 train_time:55085ms step_avg:95.63ms
step:587/1770 train_time:55182ms step_avg:95.64ms
step:588/1770 train_time:55279ms step_avg:95.64ms
step:589/1770 train_time:55376ms step_avg:95.64ms
step:590/1770 train_time:55474ms step_avg:95.64ms
step:591/1770 train_time:55572ms step_avg:95.65ms
step:592/1770 train_time:55669ms step_avg:95.65ms
step:593/1770 train_time:55767ms step_avg:95.66ms
step:594/1770 train_time:55865ms step_avg:95.66ms
step:595/1770 train_time:55962ms step_avg:95.66ms
step:596/1770 train_time:56059ms step_avg:95.66ms
step:597/1770 train_time:56157ms step_avg:95.67ms
step:598/1770 train_time:56254ms step_avg:95.67ms
step:599/1770 train_time:56352ms step_avg:95.67ms
step:600/1770 train_time:56448ms step_avg:95.68ms
step:601/1770 train_time:56546ms step_avg:95.68ms
step:602/1770 train_time:56643ms step_avg:95.68ms
step:603/1770 train_time:56740ms step_avg:95.68ms
step:604/1770 train_time:56838ms step_avg:95.69ms
step:605/1770 train_time:56936ms step_avg:95.69ms
step:606/1770 train_time:57033ms step_avg:95.69ms
step:607/1770 train_time:57131ms step_avg:95.70ms
step:608/1770 train_time:57228ms step_avg:95.70ms
step:609/1770 train_time:57325ms step_avg:95.70ms
step:610/1770 train_time:57422ms step_avg:95.70ms
step:611/1770 train_time:57519ms step_avg:95.71ms
step:612/1770 train_time:57617ms step_avg:95.71ms
step:613/1770 train_time:57714ms step_avg:95.71ms
step:614/1770 train_time:57812ms step_avg:95.71ms
step:615/1770 train_time:57909ms step_avg:95.72ms
step:616/1770 train_time:58007ms step_avg:95.72ms
step:617/1770 train_time:58106ms step_avg:95.73ms
step:618/1770 train_time:58203ms step_avg:95.73ms
step:619/1770 train_time:58300ms step_avg:95.73ms
step:620/1770 train_time:58397ms step_avg:95.73ms
step:621/1770 train_time:58495ms step_avg:95.74ms
step:622/1770 train_time:58592ms step_avg:95.74ms
step:623/1770 train_time:58689ms step_avg:95.74ms
step:624/1770 train_time:58788ms step_avg:95.75ms
step:625/1770 train_time:58887ms step_avg:95.75ms
step:625/1770 val_loss:3.6716 train_time:58983ms step_avg:95.91ms
step:626/1770 train_time:59004ms step_avg:95.79ms
step:627/1770 train_time:59089ms step_avg:95.77ms
step:628/1770 train_time:59190ms step_avg:95.78ms
step:629/1770 train_time:59287ms step_avg:95.78ms
step:630/1770 train_time:59385ms step_avg:95.78ms
step:631/1770 train_time:59482ms step_avg:95.78ms
step:632/1770 train_time:59579ms step_avg:95.79ms
step:633/1770 train_time:59676ms step_avg:95.79ms
step:634/1770 train_time:59773ms step_avg:95.79ms
step:635/1770 train_time:59870ms step_avg:95.79ms
step:636/1770 train_time:59967ms step_avg:95.79ms
step:637/1770 train_time:60065ms step_avg:95.80ms
step:638/1770 train_time:60164ms step_avg:95.80ms
step:639/1770 train_time:60263ms step_avg:95.81ms
step:640/1770 train_time:60361ms step_avg:95.81ms
step:641/1770 train_time:60458ms step_avg:95.81ms
step:642/1770 train_time:60555ms step_avg:95.82ms
step:643/1770 train_time:60652ms step_avg:95.82ms
step:644/1770 train_time:60749ms step_avg:95.82ms
step:645/1770 train_time:60846ms step_avg:95.82ms
step:646/1770 train_time:60943ms step_avg:95.82ms
step:647/1770 train_time:61040ms step_avg:95.82ms
step:648/1770 train_time:61138ms step_avg:95.83ms
step:649/1770 train_time:61236ms step_avg:95.83ms
step:650/1770 train_time:61333ms step_avg:95.83ms
step:651/1770 train_time:61431ms step_avg:95.84ms
step:652/1770 train_time:61529ms step_avg:95.84ms
step:653/1770 train_time:61626ms step_avg:95.84ms
step:654/1770 train_time:61723ms step_avg:95.84ms
step:655/1770 train_time:61821ms step_avg:95.85ms
step:656/1770 train_time:61919ms step_avg:95.85ms
step:657/1770 train_time:62016ms step_avg:95.85ms
step:658/1770 train_time:62114ms step_avg:95.86ms
step:659/1770 train_time:62214ms step_avg:95.86ms
step:660/1770 train_time:62314ms step_avg:95.87ms
step:661/1770 train_time:62413ms step_avg:95.87ms
step:662/1770 train_time:62513ms step_avg:95.88ms
step:663/1770 train_time:62612ms step_avg:95.88ms
step:664/1770 train_time:62711ms step_avg:95.89ms
step:665/1770 train_time:62810ms step_avg:95.89ms
step:666/1770 train_time:62910ms step_avg:95.90ms
step:667/1770 train_time:63008ms step_avg:95.90ms
step:668/1770 train_time:63107ms step_avg:95.91ms
step:669/1770 train_time:63206ms step_avg:95.91ms
step:670/1770 train_time:63306ms step_avg:95.92ms
step:671/1770 train_time:63405ms step_avg:95.92ms
step:672/1770 train_time:63505ms step_avg:95.93ms
step:673/1770 train_time:63604ms step_avg:95.93ms
step:674/1770 train_time:63703ms step_avg:95.94ms
step:675/1770 train_time:63803ms step_avg:95.94ms
step:676/1770 train_time:63904ms step_avg:95.95ms
step:677/1770 train_time:64004ms step_avg:95.96ms
step:678/1770 train_time:64104ms step_avg:95.96ms
step:679/1770 train_time:64204ms step_avg:95.97ms
step:680/1770 train_time:64303ms step_avg:95.97ms
step:681/1770 train_time:64403ms step_avg:95.98ms
step:682/1770 train_time:64502ms step_avg:95.99ms
step:683/1770 train_time:64602ms step_avg:95.99ms
step:684/1770 train_time:64701ms step_avg:96.00ms
step:685/1770 train_time:64801ms step_avg:96.00ms
step:686/1770 train_time:64901ms step_avg:96.01ms
step:687/1770 train_time:65001ms step_avg:96.01ms
step:688/1770 train_time:65101ms step_avg:96.02ms
step:689/1770 train_time:65201ms step_avg:96.02ms
step:690/1770 train_time:65301ms step_avg:96.03ms
step:691/1770 train_time:65401ms step_avg:96.04ms
step:692/1770 train_time:65500ms step_avg:96.04ms
step:693/1770 train_time:65600ms step_avg:96.05ms
step:694/1770 train_time:65699ms step_avg:96.05ms
step:695/1770 train_time:65797ms step_avg:96.05ms
step:696/1770 train_time:65896ms step_avg:96.06ms
step:697/1770 train_time:65995ms step_avg:96.06ms
step:698/1770 train_time:66094ms step_avg:96.07ms
step:699/1770 train_time:66193ms step_avg:96.07ms
step:700/1770 train_time:66292ms step_avg:96.08ms
step:701/1770 train_time:66391ms step_avg:96.08ms
step:702/1770 train_time:66490ms step_avg:96.08ms
step:703/1770 train_time:66590ms step_avg:96.09ms
step:704/1770 train_time:66690ms step_avg:96.09ms
step:705/1770 train_time:66789ms step_avg:96.10ms
step:706/1770 train_time:66888ms step_avg:96.10ms
step:707/1770 train_time:66987ms step_avg:96.11ms
step:708/1770 train_time:67086ms step_avg:96.11ms
step:709/1770 train_time:67185ms step_avg:96.12ms
step:710/1770 train_time:67284ms step_avg:96.12ms
step:711/1770 train_time:67384ms step_avg:96.13ms
step:712/1770 train_time:67484ms step_avg:96.13ms
step:713/1770 train_time:67584ms step_avg:96.14ms
step:714/1770 train_time:67684ms step_avg:96.14ms
step:715/1770 train_time:67784ms step_avg:96.15ms
step:716/1770 train_time:67883ms step_avg:96.15ms
step:717/1770 train_time:67983ms step_avg:96.16ms
step:718/1770 train_time:68082ms step_avg:96.16ms
step:719/1770 train_time:68182ms step_avg:96.17ms
step:720/1770 train_time:68281ms step_avg:96.17ms
step:721/1770 train_time:68381ms step_avg:96.18ms
step:722/1770 train_time:68480ms step_avg:96.18ms
step:723/1770 train_time:68579ms step_avg:96.18ms
step:724/1770 train_time:68679ms step_avg:96.19ms
step:725/1770 train_time:68778ms step_avg:96.19ms
step:726/1770 train_time:68877ms step_avg:96.20ms
step:727/1770 train_time:68976ms step_avg:96.20ms
step:728/1770 train_time:69077ms step_avg:96.21ms
step:729/1770 train_time:69175ms step_avg:96.21ms
step:730/1770 train_time:69274ms step_avg:96.21ms
step:731/1770 train_time:69374ms step_avg:96.22ms
step:732/1770 train_time:69473ms step_avg:96.22ms
step:733/1770 train_time:69572ms step_avg:96.23ms
step:734/1770 train_time:69671ms step_avg:96.23ms
step:735/1770 train_time:69770ms step_avg:96.23ms
step:736/1770 train_time:69870ms step_avg:96.24ms
step:737/1770 train_time:69970ms step_avg:96.24ms
step:738/1770 train_time:70069ms step_avg:96.25ms
step:739/1770 train_time:70169ms step_avg:96.25ms
step:740/1770 train_time:70268ms step_avg:96.26ms
step:741/1770 train_time:70367ms step_avg:96.26ms
step:742/1770 train_time:70465ms step_avg:96.26ms
step:743/1770 train_time:70565ms step_avg:96.27ms
step:744/1770 train_time:70665ms step_avg:96.27ms
step:745/1770 train_time:70764ms step_avg:96.28ms
step:746/1770 train_time:70864ms step_avg:96.28ms
step:747/1770 train_time:70964ms step_avg:96.29ms
step:748/1770 train_time:71064ms step_avg:96.29ms
step:749/1770 train_time:71164ms step_avg:96.30ms
step:750/1770 train_time:71263ms step_avg:96.30ms
step:750/1770 val_loss:3.6081 train_time:71361ms step_avg:96.43ms
step:751/1770 train_time:71383ms step_avg:96.33ms
step:752/1770 train_time:71472ms step_avg:96.32ms
step:753/1770 train_time:71573ms step_avg:96.33ms
step:754/1770 train_time:71672ms step_avg:96.33ms
step:755/1770 train_time:71771ms step_avg:96.34ms
step:756/1770 train_time:71869ms step_avg:96.34ms
step:757/1770 train_time:71968ms step_avg:96.34ms
step:758/1770 train_time:72067ms step_avg:96.35ms
step:759/1770 train_time:72166ms step_avg:96.35ms
step:760/1770 train_time:72265ms step_avg:96.35ms
step:761/1770 train_time:72364ms step_avg:96.36ms
step:762/1770 train_time:72463ms step_avg:96.36ms
step:763/1770 train_time:72564ms step_avg:96.37ms
step:764/1770 train_time:72664ms step_avg:96.37ms
step:765/1770 train_time:72765ms step_avg:96.38ms
step:766/1770 train_time:72865ms step_avg:96.38ms
step:767/1770 train_time:72965ms step_avg:96.39ms
step:768/1770 train_time:73064ms step_avg:96.39ms
step:769/1770 train_time:73164ms step_avg:96.39ms
step:770/1770 train_time:73263ms step_avg:96.40ms
step:771/1770 train_time:73361ms step_avg:96.40ms
step:772/1770 train_time:73461ms step_avg:96.41ms
step:773/1770 train_time:73559ms step_avg:96.41ms
step:774/1770 train_time:73659ms step_avg:96.41ms
step:775/1770 train_time:73759ms step_avg:96.42ms
step:776/1770 train_time:73858ms step_avg:96.42ms
step:777/1770 train_time:73957ms step_avg:96.42ms
step:778/1770 train_time:74056ms step_avg:96.43ms
step:779/1770 train_time:74155ms step_avg:96.43ms
step:780/1770 train_time:74254ms step_avg:96.43ms
step:781/1770 train_time:74353ms step_avg:96.44ms
step:782/1770 train_time:74452ms step_avg:96.44ms
step:783/1770 train_time:74551ms step_avg:96.44ms
step:784/1770 train_time:74650ms step_avg:96.45ms
step:785/1770 train_time:74750ms step_avg:96.45ms
step:786/1770 train_time:74848ms step_avg:96.45ms
step:787/1770 train_time:74947ms step_avg:96.46ms
step:788/1770 train_time:75046ms step_avg:96.46ms
step:789/1770 train_time:75146ms step_avg:96.46ms
step:790/1770 train_time:75245ms step_avg:96.47ms
step:791/1770 train_time:75345ms step_avg:96.47ms
step:792/1770 train_time:75445ms step_avg:96.48ms
step:793/1770 train_time:75545ms step_avg:96.48ms
step:794/1770 train_time:75645ms step_avg:96.49ms
step:795/1770 train_time:75744ms step_avg:96.49ms
step:796/1770 train_time:75844ms step_avg:96.49ms
step:797/1770 train_time:75944ms step_avg:96.50ms
step:798/1770 train_time:76043ms step_avg:96.50ms
step:799/1770 train_time:76143ms step_avg:96.51ms
step:800/1770 train_time:76242ms step_avg:96.51ms
step:801/1770 train_time:76341ms step_avg:96.51ms
step:802/1770 train_time:76440ms step_avg:96.52ms
step:803/1770 train_time:76540ms step_avg:96.52ms
step:804/1770 train_time:76640ms step_avg:96.52ms
step:805/1770 train_time:76739ms step_avg:96.53ms
step:806/1770 train_time:76839ms step_avg:96.53ms
step:807/1770 train_time:76939ms step_avg:96.54ms
step:808/1770 train_time:77039ms step_avg:96.54ms
step:809/1770 train_time:77139ms step_avg:96.54ms
step:810/1770 train_time:77238ms step_avg:96.55ms
step:811/1770 train_time:77337ms step_avg:96.55ms
step:812/1770 train_time:77436ms step_avg:96.55ms
step:813/1770 train_time:77536ms step_avg:96.56ms
step:814/1770 train_time:77635ms step_avg:96.56ms
step:815/1770 train_time:77735ms step_avg:96.57ms
step:816/1770 train_time:77834ms step_avg:96.57ms
step:817/1770 train_time:77934ms step_avg:96.57ms
step:818/1770 train_time:78034ms step_avg:96.58ms
step:819/1770 train_time:78135ms step_avg:96.58ms
step:820/1770 train_time:78235ms step_avg:96.59ms
step:821/1770 train_time:78335ms step_avg:96.59ms
step:822/1770 train_time:78434ms step_avg:96.59ms
step:823/1770 train_time:78533ms step_avg:96.60ms
step:824/1770 train_time:78632ms step_avg:96.60ms
step:825/1770 train_time:78731ms step_avg:96.60ms
step:826/1770 train_time:78830ms step_avg:96.61ms
step:827/1770 train_time:78929ms step_avg:96.61ms
step:828/1770 train_time:79029ms step_avg:96.61ms
step:829/1770 train_time:79129ms step_avg:96.62ms
step:830/1770 train_time:79229ms step_avg:96.62ms
step:831/1770 train_time:79329ms step_avg:96.63ms
step:832/1770 train_time:79429ms step_avg:96.63ms
step:833/1770 train_time:79529ms step_avg:96.63ms
step:834/1770 train_time:79628ms step_avg:96.64ms
step:835/1770 train_time:79727ms step_avg:96.64ms
step:836/1770 train_time:79827ms step_avg:96.64ms
step:837/1770 train_time:79926ms step_avg:96.65ms
step:838/1770 train_time:80025ms step_avg:96.65ms
step:839/1770 train_time:80125ms step_avg:96.65ms
step:840/1770 train_time:80225ms step_avg:96.66ms
step:841/1770 train_time:80325ms step_avg:96.66ms
step:842/1770 train_time:80425ms step_avg:96.67ms
step:843/1770 train_time:80525ms step_avg:96.67ms
step:844/1770 train_time:80624ms step_avg:96.67ms
step:845/1770 train_time:80723ms step_avg:96.67ms
step:846/1770 train_time:80822ms step_avg:96.68ms
step:847/1770 train_time:80921ms step_avg:96.68ms
step:848/1770 train_time:81020ms step_avg:96.68ms
step:849/1770 train_time:81119ms step_avg:96.69ms
step:850/1770 train_time:81218ms step_avg:96.69ms
step:851/1770 train_time:81317ms step_avg:96.69ms
step:852/1770 train_time:81417ms step_avg:96.69ms
step:853/1770 train_time:81516ms step_avg:96.70ms
step:854/1770 train_time:81616ms step_avg:96.70ms
step:855/1770 train_time:81715ms step_avg:96.70ms
step:856/1770 train_time:81815ms step_avg:96.71ms
step:857/1770 train_time:81915ms step_avg:96.71ms
step:858/1770 train_time:82015ms step_avg:96.72ms
step:859/1770 train_time:82115ms step_avg:96.72ms
step:860/1770 train_time:82215ms step_avg:96.72ms
step:861/1770 train_time:82314ms step_avg:96.73ms
step:862/1770 train_time:82414ms step_avg:96.73ms
step:863/1770 train_time:82512ms step_avg:96.73ms
step:864/1770 train_time:82613ms step_avg:96.74ms
step:865/1770 train_time:82712ms step_avg:96.74ms
step:866/1770 train_time:82812ms step_avg:96.74ms
step:867/1770 train_time:82912ms step_avg:96.75ms
step:868/1770 train_time:83011ms step_avg:96.75ms
step:869/1770 train_time:83110ms step_avg:96.75ms
step:870/1770 train_time:83209ms step_avg:96.75ms
step:871/1770 train_time:83308ms step_avg:96.76ms
step:872/1770 train_time:83408ms step_avg:96.76ms
step:873/1770 train_time:83507ms step_avg:96.76ms
step:874/1770 train_time:83607ms step_avg:96.77ms
step:875/1770 train_time:83707ms step_avg:96.77ms
step:875/1770 val_loss:3.5568 train_time:83805ms step_avg:96.88ms
step:876/1770 train_time:83826ms step_avg:96.80ms
step:877/1770 train_time:83917ms step_avg:96.79ms
step:878/1770 train_time:84017ms step_avg:96.79ms
step:879/1770 train_time:84116ms step_avg:96.80ms
step:880/1770 train_time:84215ms step_avg:96.80ms
step:881/1770 train_time:84314ms step_avg:96.80ms
step:882/1770 train_time:84413ms step_avg:96.80ms
step:883/1770 train_time:84511ms step_avg:96.81ms
step:884/1770 train_time:84610ms step_avg:96.81ms
step:885/1770 train_time:84709ms step_avg:96.81ms
step:886/1770 train_time:84809ms step_avg:96.81ms
step:887/1770 train_time:84910ms step_avg:96.82ms
step:888/1770 train_time:85010ms step_avg:96.82ms
step:889/1770 train_time:85110ms step_avg:96.83ms
step:890/1770 train_time:85209ms step_avg:96.83ms
step:891/1770 train_time:85309ms step_avg:96.83ms
step:892/1770 train_time:85408ms step_avg:96.83ms
step:893/1770 train_time:85507ms step_avg:96.84ms
step:894/1770 train_time:85606ms step_avg:96.84ms
step:895/1770 train_time:85706ms step_avg:96.84ms
step:896/1770 train_time:85806ms step_avg:96.85ms
step:897/1770 train_time:85906ms step_avg:96.85ms
step:898/1770 train_time:86006ms step_avg:96.85ms
step:899/1770 train_time:86105ms step_avg:96.86ms
step:900/1770 train_time:86204ms step_avg:96.86ms
step:901/1770 train_time:86305ms step_avg:96.86ms
step:902/1770 train_time:86406ms step_avg:96.87ms
step:903/1770 train_time:86505ms step_avg:96.87ms
step:904/1770 train_time:86605ms step_avg:96.87ms
step:905/1770 train_time:86705ms step_avg:96.88ms
step:906/1770 train_time:86804ms step_avg:96.88ms
step:907/1770 train_time:86904ms step_avg:96.88ms
step:908/1770 train_time:87003ms step_avg:96.89ms
step:909/1770 train_time:87102ms step_avg:96.89ms
step:910/1770 train_time:87202ms step_avg:96.89ms
step:911/1770 train_time:87302ms step_avg:96.89ms
step:912/1770 train_time:87401ms step_avg:96.90ms
step:913/1770 train_time:87500ms step_avg:96.90ms
step:914/1770 train_time:87599ms step_avg:96.90ms
step:915/1770 train_time:87699ms step_avg:96.90ms
step:916/1770 train_time:87798ms step_avg:96.91ms
step:917/1770 train_time:87897ms step_avg:96.91ms
step:918/1770 train_time:87997ms step_avg:96.91ms
step:919/1770 train_time:88096ms step_avg:96.91ms
step:920/1770 train_time:88198ms step_avg:96.92ms
step:921/1770 train_time:88299ms step_avg:96.92ms
step:922/1770 train_time:88399ms step_avg:96.93ms
step:923/1770 train_time:88500ms step_avg:96.93ms
step:924/1770 train_time:88600ms step_avg:96.94ms
step:925/1770 train_time:88701ms step_avg:96.94ms
step:926/1770 train_time:88802ms step_avg:96.94ms
step:927/1770 train_time:88903ms step_avg:96.95ms
step:928/1770 train_time:89004ms step_avg:96.95ms
step:929/1770 train_time:89104ms step_avg:96.96ms
step:930/1770 train_time:89206ms step_avg:96.96ms
step:931/1770 train_time:89306ms step_avg:96.97ms
step:932/1770 train_time:89407ms step_avg:96.97ms
step:933/1770 train_time:89508ms step_avg:96.97ms
step:934/1770 train_time:89609ms step_avg:96.98ms
step:935/1770 train_time:89711ms step_avg:96.98ms
step:936/1770 train_time:89812ms step_avg:96.99ms
step:937/1770 train_time:89912ms step_avg:96.99ms
step:938/1770 train_time:90014ms step_avg:97.00ms
step:939/1770 train_time:90115ms step_avg:97.00ms
step:940/1770 train_time:90217ms step_avg:97.01ms
step:941/1770 train_time:90319ms step_avg:97.01ms
step:942/1770 train_time:90420ms step_avg:97.02ms
step:943/1770 train_time:90522ms step_avg:97.02ms
step:944/1770 train_time:90622ms step_avg:97.03ms
step:945/1770 train_time:90722ms step_avg:97.03ms
step:946/1770 train_time:90824ms step_avg:97.03ms
step:947/1770 train_time:90925ms step_avg:97.04ms
step:948/1770 train_time:91026ms step_avg:97.04ms
step:949/1770 train_time:91128ms step_avg:97.05ms
step:950/1770 train_time:91230ms step_avg:97.05ms
step:951/1770 train_time:91331ms step_avg:97.06ms
step:952/1770 train_time:91431ms step_avg:97.06ms
step:953/1770 train_time:91532ms step_avg:97.06ms
step:954/1770 train_time:91633ms step_avg:97.07ms
step:955/1770 train_time:91734ms step_avg:97.07ms
step:956/1770 train_time:91835ms step_avg:97.08ms
step:957/1770 train_time:91937ms step_avg:97.08ms
step:958/1770 train_time:92038ms step_avg:97.09ms
step:959/1770 train_time:92139ms step_avg:97.09ms
step:960/1770 train_time:92239ms step_avg:97.09ms
step:961/1770 train_time:92340ms step_avg:97.10ms
step:962/1770 train_time:92441ms step_avg:97.10ms
step:963/1770 train_time:92541ms step_avg:97.11ms
step:964/1770 train_time:92642ms step_avg:97.11ms
step:965/1770 train_time:92744ms step_avg:97.11ms
step:966/1770 train_time:92846ms step_avg:97.12ms
step:967/1770 train_time:92948ms step_avg:97.12ms
step:968/1770 train_time:93049ms step_avg:97.13ms
step:969/1770 train_time:93151ms step_avg:97.13ms
step:970/1770 train_time:93251ms step_avg:97.14ms
step:971/1770 train_time:93352ms step_avg:97.14ms
step:972/1770 train_time:93452ms step_avg:97.14ms
step:973/1770 train_time:93553ms step_avg:97.15ms
step:974/1770 train_time:93654ms step_avg:97.15ms
step:975/1770 train_time:93755ms step_avg:97.16ms
step:976/1770 train_time:93857ms step_avg:97.16ms
step:977/1770 train_time:93959ms step_avg:97.17ms
step:978/1770 train_time:94060ms step_avg:97.17ms
step:979/1770 train_time:94161ms step_avg:97.17ms
step:980/1770 train_time:94261ms step_avg:97.18ms
step:981/1770 train_time:94361ms step_avg:97.18ms
step:982/1770 train_time:94462ms step_avg:97.18ms
step:983/1770 train_time:94562ms step_avg:97.19ms
step:984/1770 train_time:94664ms step_avg:97.19ms
step:985/1770 train_time:94765ms step_avg:97.19ms
step:986/1770 train_time:94867ms step_avg:97.20ms
step:987/1770 train_time:94969ms step_avg:97.21ms
step:988/1770 train_time:95070ms step_avg:97.21ms
step:989/1770 train_time:95173ms step_avg:97.21ms
step:990/1770 train_time:95273ms step_avg:97.22ms
step:991/1770 train_time:95374ms step_avg:97.22ms
step:992/1770 train_time:95475ms step_avg:97.23ms
step:993/1770 train_time:95576ms step_avg:97.23ms
step:994/1770 train_time:95677ms step_avg:97.23ms
step:995/1770 train_time:95779ms step_avg:97.24ms
step:996/1770 train_time:95881ms step_avg:97.24ms
step:997/1770 train_time:95981ms step_avg:97.25ms
step:998/1770 train_time:96081ms step_avg:97.25ms
step:999/1770 train_time:96181ms step_avg:97.25ms
step:1000/1770 train_time:96282ms step_avg:97.25ms
step:1000/1770 val_loss:3.5183 train_time:96382ms step_avg:97.36ms
step:1001/1770 train_time:96403ms step_avg:97.28ms
step:1002/1770 train_time:96496ms step_avg:97.27ms
step:1003/1770 train_time:96598ms step_avg:97.28ms
step:1004/1770 train_time:96699ms step_avg:97.28ms
step:1005/1770 train_time:96799ms step_avg:97.29ms
step:1006/1770 train_time:96899ms step_avg:97.29ms
step:1007/1770 train_time:96999ms step_avg:97.29ms
step:1008/1770 train_time:97099ms step_avg:97.29ms
step:1009/1770 train_time:97199ms step_avg:97.30ms
step:1010/1770 train_time:97299ms step_avg:97.30ms
step:1011/1770 train_time:97401ms step_avg:97.30ms
step:1012/1770 train_time:97503ms step_avg:97.31ms
step:1013/1770 train_time:97604ms step_avg:97.31ms
step:1014/1770 train_time:97705ms step_avg:97.32ms
step:1015/1770 train_time:97806ms step_avg:97.32ms
step:1016/1770 train_time:97906ms step_avg:97.32ms
step:1017/1770 train_time:98007ms step_avg:97.33ms
step:1018/1770 train_time:98108ms step_avg:97.33ms
step:1019/1770 train_time:98209ms step_avg:97.33ms
step:1020/1770 train_time:98310ms step_avg:97.34ms
step:1021/1770 train_time:98411ms step_avg:97.34ms
step:1022/1770 train_time:98511ms step_avg:97.34ms
step:1023/1770 train_time:98613ms step_avg:97.35ms
step:1024/1770 train_time:98714ms step_avg:97.35ms
step:1025/1770 train_time:98816ms step_avg:97.36ms
step:1026/1770 train_time:98918ms step_avg:97.36ms
step:1027/1770 train_time:99019ms step_avg:97.36ms
step:1028/1770 train_time:99120ms step_avg:97.37ms
step:1029/1770 train_time:99221ms step_avg:97.37ms
step:1030/1770 train_time:99322ms step_avg:97.37ms
step:1031/1770 train_time:99422ms step_avg:97.38ms
step:1032/1770 train_time:99523ms step_avg:97.38ms
step:1033/1770 train_time:99625ms step_avg:97.39ms
step:1034/1770 train_time:99726ms step_avg:97.39ms
step:1035/1770 train_time:99827ms step_avg:97.39ms
step:1036/1770 train_time:99928ms step_avg:97.40ms
step:1037/1770 train_time:100029ms step_avg:97.40ms
step:1038/1770 train_time:100130ms step_avg:97.40ms
step:1039/1770 train_time:100231ms step_avg:97.41ms
step:1040/1770 train_time:100331ms step_avg:97.41ms
step:1041/1770 train_time:100432ms step_avg:97.41ms
step:1042/1770 train_time:100533ms step_avg:97.42ms
step:1043/1770 train_time:100633ms step_avg:97.42ms
step:1044/1770 train_time:100734ms step_avg:97.42ms
step:1045/1770 train_time:100835ms step_avg:97.43ms
step:1046/1770 train_time:100936ms step_avg:97.43ms
step:1047/1770 train_time:101036ms step_avg:97.43ms
step:1048/1770 train_time:101137ms step_avg:97.43ms
step:1049/1770 train_time:101238ms step_avg:97.44ms
step:1050/1770 train_time:101339ms step_avg:97.44ms
step:1051/1770 train_time:101441ms step_avg:97.45ms
step:1052/1770 train_time:101542ms step_avg:97.45ms
step:1053/1770 train_time:101643ms step_avg:97.45ms
step:1054/1770 train_time:101744ms step_avg:97.46ms
step:1055/1770 train_time:101844ms step_avg:97.46ms
step:1056/1770 train_time:101944ms step_avg:97.46ms
step:1057/1770 train_time:102045ms step_avg:97.46ms
step:1058/1770 train_time:102146ms step_avg:97.47ms
step:1059/1770 train_time:102248ms step_avg:97.47ms
step:1060/1770 train_time:102349ms step_avg:97.47ms
step:1061/1770 train_time:102450ms step_avg:97.48ms
step:1062/1770 train_time:102551ms step_avg:97.48ms
step:1063/1770 train_time:102654ms step_avg:97.49ms
step:1064/1770 train_time:102755ms step_avg:97.49ms
step:1065/1770 train_time:102856ms step_avg:97.49ms
step:1066/1770 train_time:102957ms step_avg:97.50ms
step:1067/1770 train_time:103059ms step_avg:97.50ms
step:1068/1770 train_time:103160ms step_avg:97.50ms
step:1069/1770 train_time:103261ms step_avg:97.51ms
step:1070/1770 train_time:103361ms step_avg:97.51ms
step:1071/1770 train_time:103462ms step_avg:97.51ms
step:1072/1770 train_time:103563ms step_avg:97.52ms
step:1073/1770 train_time:103664ms step_avg:97.52ms
step:1074/1770 train_time:103766ms step_avg:97.52ms
step:1075/1770 train_time:103867ms step_avg:97.53ms
step:1076/1770 train_time:103970ms step_avg:97.53ms
step:1077/1770 train_time:104072ms step_avg:97.54ms
step:1078/1770 train_time:104172ms step_avg:97.54ms
step:1079/1770 train_time:104272ms step_avg:97.54ms
step:1080/1770 train_time:104373ms step_avg:97.54ms
step:1081/1770 train_time:104473ms step_avg:97.55ms
step:1082/1770 train_time:104574ms step_avg:97.55ms
step:1083/1770 train_time:104676ms step_avg:97.55ms
step:1084/1770 train_time:104777ms step_avg:97.56ms
step:1085/1770 train_time:104878ms step_avg:97.56ms
step:1086/1770 train_time:104979ms step_avg:97.56ms
step:1087/1770 train_time:105080ms step_avg:97.57ms
step:1088/1770 train_time:105181ms step_avg:97.57ms
step:1089/1770 train_time:105281ms step_avg:97.57ms
step:1090/1770 train_time:105383ms step_avg:97.58ms
step:1091/1770 train_time:105484ms step_avg:97.58ms
step:1092/1770 train_time:105586ms step_avg:97.58ms
step:1093/1770 train_time:105687ms step_avg:97.59ms
step:1094/1770 train_time:105789ms step_avg:97.59ms
step:1095/1770 train_time:105890ms step_avg:97.59ms
step:1096/1770 train_time:105991ms step_avg:97.60ms
step:1097/1770 train_time:106092ms step_avg:97.60ms
step:1098/1770 train_time:106191ms step_avg:97.60ms
step:1099/1770 train_time:106293ms step_avg:97.61ms
step:1100/1770 train_time:106394ms step_avg:97.61ms
step:1101/1770 train_time:106496ms step_avg:97.61ms
step:1102/1770 train_time:106597ms step_avg:97.62ms
step:1103/1770 train_time:106698ms step_avg:97.62ms
step:1104/1770 train_time:106800ms step_avg:97.62ms
step:1105/1770 train_time:106901ms step_avg:97.63ms
step:1106/1770 train_time:107002ms step_avg:97.63ms
step:1107/1770 train_time:107102ms step_avg:97.63ms
step:1108/1770 train_time:107203ms step_avg:97.63ms
step:1109/1770 train_time:107304ms step_avg:97.64ms
step:1110/1770 train_time:107406ms step_avg:97.64ms
step:1111/1770 train_time:107508ms step_avg:97.65ms
step:1112/1770 train_time:107609ms step_avg:97.65ms
step:1113/1770 train_time:107710ms step_avg:97.65ms
step:1114/1770 train_time:107811ms step_avg:97.65ms
step:1115/1770 train_time:107912ms step_avg:97.66ms
step:1116/1770 train_time:108013ms step_avg:97.66ms
step:1117/1770 train_time:108114ms step_avg:97.66ms
step:1118/1770 train_time:108215ms step_avg:97.67ms
step:1119/1770 train_time:108318ms step_avg:97.67ms
step:1120/1770 train_time:108420ms step_avg:97.68ms
step:1121/1770 train_time:108520ms step_avg:97.68ms
step:1122/1770 train_time:108622ms step_avg:97.68ms
step:1123/1770 train_time:108722ms step_avg:97.68ms
step:1124/1770 train_time:108823ms step_avg:97.69ms
step:1125/1770 train_time:108925ms step_avg:97.69ms
step:1125/1770 val_loss:3.4770 train_time:109025ms step_avg:97.78ms
step:1126/1770 train_time:109047ms step_avg:97.71ms
step:1127/1770 train_time:109138ms step_avg:97.71ms
step:1128/1770 train_time:109240ms step_avg:97.71ms
step:1129/1770 train_time:109341ms step_avg:97.71ms
step:1130/1770 train_time:109442ms step_avg:97.72ms
step:1131/1770 train_time:109542ms step_avg:97.72ms
step:1132/1770 train_time:109643ms step_avg:97.72ms
step:1133/1770 train_time:109743ms step_avg:97.72ms
step:1134/1770 train_time:109844ms step_avg:97.73ms
step:1135/1770 train_time:109944ms step_avg:97.73ms
step:1136/1770 train_time:110045ms step_avg:97.73ms
step:1137/1770 train_time:110148ms step_avg:97.74ms
step:1138/1770 train_time:110249ms step_avg:97.74ms
step:1139/1770 train_time:110350ms step_avg:97.74ms
step:1140/1770 train_time:110452ms step_avg:97.74ms
step:1141/1770 train_time:110553ms step_avg:97.75ms
step:1142/1770 train_time:110653ms step_avg:97.75ms
step:1143/1770 train_time:110753ms step_avg:97.75ms
step:1144/1770 train_time:110854ms step_avg:97.76ms
step:1145/1770 train_time:110955ms step_avg:97.76ms
step:1146/1770 train_time:111056ms step_avg:97.76ms
step:1147/1770 train_time:111157ms step_avg:97.76ms
step:1148/1770 train_time:111259ms step_avg:97.77ms
step:1149/1770 train_time:111359ms step_avg:97.77ms
step:1150/1770 train_time:111460ms step_avg:97.77ms
step:1151/1770 train_time:111561ms step_avg:97.77ms
step:1152/1770 train_time:111662ms step_avg:97.78ms
step:1153/1770 train_time:111763ms step_avg:97.78ms
step:1154/1770 train_time:111865ms step_avg:97.78ms
step:1155/1770 train_time:111967ms step_avg:97.79ms
step:1156/1770 train_time:112069ms step_avg:97.79ms
step:1157/1770 train_time:112171ms step_avg:97.80ms
step:1158/1770 train_time:112272ms step_avg:97.80ms
step:1159/1770 train_time:112373ms step_avg:97.80ms
step:1160/1770 train_time:112473ms step_avg:97.80ms
step:1161/1770 train_time:112573ms step_avg:97.80ms
step:1162/1770 train_time:112674ms step_avg:97.81ms
step:1163/1770 train_time:112776ms step_avg:97.81ms
step:1164/1770 train_time:112877ms step_avg:97.81ms
step:1165/1770 train_time:112979ms step_avg:97.82ms
step:1166/1770 train_time:113081ms step_avg:97.82ms
step:1167/1770 train_time:113182ms step_avg:97.82ms
step:1168/1770 train_time:113284ms step_avg:97.83ms
step:1169/1770 train_time:113384ms step_avg:97.83ms
step:1170/1770 train_time:113485ms step_avg:97.83ms
step:1171/1770 train_time:113586ms step_avg:97.83ms
step:1172/1770 train_time:113688ms step_avg:97.84ms
step:1173/1770 train_time:113790ms step_avg:97.84ms
step:1174/1770 train_time:113891ms step_avg:97.84ms
step:1175/1770 train_time:113992ms step_avg:97.85ms
step:1176/1770 train_time:114093ms step_avg:97.85ms
step:1177/1770 train_time:114194ms step_avg:97.85ms
step:1178/1770 train_time:114296ms step_avg:97.86ms
step:1179/1770 train_time:114397ms step_avg:97.86ms
step:1180/1770 train_time:114498ms step_avg:97.86ms
step:1181/1770 train_time:114601ms step_avg:97.87ms
step:1182/1770 train_time:114702ms step_avg:97.87ms
step:1183/1770 train_time:114803ms step_avg:97.87ms
step:1184/1770 train_time:114906ms step_avg:97.88ms
step:1185/1770 train_time:115008ms step_avg:97.88ms
step:1186/1770 train_time:115111ms step_avg:97.88ms
step:1187/1770 train_time:115215ms step_avg:97.89ms
step:1188/1770 train_time:115317ms step_avg:97.89ms
step:1189/1770 train_time:115419ms step_avg:97.90ms
step:1190/1770 train_time:115521ms step_avg:97.90ms
step:1191/1770 train_time:115623ms step_avg:97.90ms
step:1192/1770 train_time:115726ms step_avg:97.91ms
step:1193/1770 train_time:115828ms step_avg:97.91ms
step:1194/1770 train_time:115929ms step_avg:97.91ms
step:1195/1770 train_time:116032ms step_avg:97.92ms
step:1196/1770 train_time:116135ms step_avg:97.92ms
step:1197/1770 train_time:116237ms step_avg:97.92ms
step:1198/1770 train_time:116338ms step_avg:97.93ms
step:1199/1770 train_time:116441ms step_avg:97.93ms
step:1200/1770 train_time:116543ms step_avg:97.94ms
step:1201/1770 train_time:116646ms step_avg:97.94ms
step:1202/1770 train_time:116748ms step_avg:97.94ms
step:1203/1770 train_time:116849ms step_avg:97.95ms
step:1204/1770 train_time:116952ms step_avg:97.95ms
step:1205/1770 train_time:117053ms step_avg:97.95ms
step:1206/1770 train_time:117156ms step_avg:97.96ms
step:1207/1770 train_time:117257ms step_avg:97.96ms
step:1208/1770 train_time:117359ms step_avg:97.96ms
step:1209/1770 train_time:117460ms step_avg:97.97ms
step:1210/1770 train_time:117562ms step_avg:97.97ms
step:1211/1770 train_time:117664ms step_avg:97.97ms
step:1212/1770 train_time:117768ms step_avg:97.98ms
step:1213/1770 train_time:117870ms step_avg:97.98ms
step:1214/1770 train_time:117971ms step_avg:97.98ms
step:1215/1770 train_time:118074ms step_avg:97.99ms
step:1216/1770 train_time:118177ms step_avg:97.99ms
step:1217/1770 train_time:118279ms step_avg:97.99ms
step:1218/1770 train_time:118380ms step_avg:98.00ms
step:1219/1770 train_time:118483ms step_avg:98.00ms
step:1220/1770 train_time:118586ms step_avg:98.01ms
step:1221/1770 train_time:118688ms step_avg:98.01ms
step:1222/1770 train_time:118792ms step_avg:98.01ms
step:1223/1770 train_time:118893ms step_avg:98.02ms
step:1224/1770 train_time:118996ms step_avg:98.02ms
step:1225/1770 train_time:119098ms step_avg:98.02ms
step:1226/1770 train_time:119200ms step_avg:98.03ms
step:1227/1770 train_time:119305ms step_avg:98.03ms
step:1228/1770 train_time:119409ms step_avg:98.04ms
step:1229/1770 train_time:119511ms step_avg:98.04ms
step:1230/1770 train_time:119613ms step_avg:98.04ms
step:1231/1770 train_time:119716ms step_avg:98.05ms
step:1232/1770 train_time:119818ms step_avg:98.05ms
step:1233/1770 train_time:119920ms step_avg:98.05ms
step:1234/1770 train_time:120022ms step_avg:98.06ms
step:1235/1770 train_time:120124ms step_avg:98.06ms
step:1236/1770 train_time:120226ms step_avg:98.06ms
step:1237/1770 train_time:120328ms step_avg:98.07ms
step:1238/1770 train_time:120431ms step_avg:98.07ms
step:1239/1770 train_time:120533ms step_avg:98.07ms
step:1240/1770 train_time:120635ms step_avg:98.08ms
step:1241/1770 train_time:120737ms step_avg:98.08ms
step:1242/1770 train_time:120839ms step_avg:98.08ms
step:1243/1770 train_time:120941ms step_avg:98.09ms
step:1244/1770 train_time:121042ms step_avg:98.09ms
step:1245/1770 train_time:121144ms step_avg:98.09ms
step:1246/1770 train_time:121247ms step_avg:98.10ms
step:1247/1770 train_time:121349ms step_avg:98.10ms
step:1248/1770 train_time:121451ms step_avg:98.10ms
step:1249/1770 train_time:121553ms step_avg:98.11ms
step:1250/1770 train_time:121655ms step_avg:98.11ms
step:1250/1770 val_loss:3.4289 train_time:121757ms step_avg:98.19ms
step:1251/1770 train_time:121778ms step_avg:98.13ms
step:1252/1770 train_time:121866ms step_avg:98.12ms
step:1253/1770 train_time:121969ms step_avg:98.12ms
step:1254/1770 train_time:122071ms step_avg:98.13ms
step:1255/1770 train_time:122175ms step_avg:98.13ms
step:1256/1770 train_time:122276ms step_avg:98.13ms
step:1257/1770 train_time:122378ms step_avg:98.14ms
step:1258/1770 train_time:122480ms step_avg:98.14ms
step:1259/1770 train_time:122582ms step_avg:98.14ms
step:1260/1770 train_time:122684ms step_avg:98.15ms
step:1261/1770 train_time:122788ms step_avg:98.15ms
step:1262/1770 train_time:122891ms step_avg:98.16ms
step:1263/1770 train_time:122992ms step_avg:98.16ms
step:1264/1770 train_time:123095ms step_avg:98.16ms
step:1265/1770 train_time:123197ms step_avg:98.16ms
step:1266/1770 train_time:123299ms step_avg:98.17ms
step:1267/1770 train_time:123401ms step_avg:98.17ms
step:1268/1770 train_time:123504ms step_avg:98.17ms
step:1269/1770 train_time:123606ms step_avg:98.18ms
step:1270/1770 train_time:123709ms step_avg:98.18ms
step:1271/1770 train_time:123812ms step_avg:98.19ms
step:1272/1770 train_time:123913ms step_avg:98.19ms
step:1273/1770 train_time:124015ms step_avg:98.19ms
step:1274/1770 train_time:124118ms step_avg:98.19ms
step:1275/1770 train_time:124220ms step_avg:98.20ms
step:1276/1770 train_time:124321ms step_avg:98.20ms
step:1277/1770 train_time:124423ms step_avg:98.20ms
step:1278/1770 train_time:124527ms step_avg:98.21ms
step:1279/1770 train_time:124630ms step_avg:98.21ms
step:1280/1770 train_time:124733ms step_avg:98.21ms
step:1281/1770 train_time:124834ms step_avg:98.22ms
step:1282/1770 train_time:124937ms step_avg:98.22ms
step:1283/1770 train_time:125040ms step_avg:98.23ms
step:1284/1770 train_time:125143ms step_avg:98.23ms
step:1285/1770 train_time:125245ms step_avg:98.23ms
step:1286/1770 train_time:125348ms step_avg:98.23ms
step:1287/1770 train_time:125452ms step_avg:98.24ms
step:1288/1770 train_time:125554ms step_avg:98.24ms
step:1289/1770 train_time:125656ms step_avg:98.25ms
step:1290/1770 train_time:125758ms step_avg:98.25ms
step:1291/1770 train_time:125860ms step_avg:98.25ms
step:1292/1770 train_time:125962ms step_avg:98.25ms
step:1293/1770 train_time:126064ms step_avg:98.26ms
step:1294/1770 train_time:126166ms step_avg:98.26ms
step:1295/1770 train_time:126268ms step_avg:98.26ms
step:1296/1770 train_time:126370ms step_avg:98.27ms
step:1297/1770 train_time:126473ms step_avg:98.27ms
step:1298/1770 train_time:126575ms step_avg:98.27ms
step:1299/1770 train_time:126677ms step_avg:98.28ms
step:1300/1770 train_time:126780ms step_avg:98.28ms
step:1301/1770 train_time:126882ms step_avg:98.28ms
step:1302/1770 train_time:126983ms step_avg:98.28ms
step:1303/1770 train_time:127084ms step_avg:98.29ms
step:1304/1770 train_time:127186ms step_avg:98.29ms
step:1305/1770 train_time:127289ms step_avg:98.29ms
step:1306/1770 train_time:127391ms step_avg:98.30ms
step:1307/1770 train_time:127493ms step_avg:98.30ms
step:1308/1770 train_time:127596ms step_avg:98.30ms
step:1309/1770 train_time:127698ms step_avg:98.30ms
step:1310/1770 train_time:127800ms step_avg:98.31ms
step:1311/1770 train_time:127901ms step_avg:98.31ms
step:1312/1770 train_time:128003ms step_avg:98.31ms
step:1313/1770 train_time:128105ms step_avg:98.32ms
step:1314/1770 train_time:128207ms step_avg:98.32ms
step:1315/1770 train_time:128310ms step_avg:98.32ms
step:1316/1770 train_time:128412ms step_avg:98.32ms
step:1317/1770 train_time:128515ms step_avg:98.33ms
step:1318/1770 train_time:128619ms step_avg:98.33ms
step:1319/1770 train_time:128722ms step_avg:98.34ms
step:1320/1770 train_time:128823ms step_avg:98.34ms
step:1321/1770 train_time:128926ms step_avg:98.34ms
step:1322/1770 train_time:129028ms step_avg:98.34ms
step:1323/1770 train_time:129131ms step_avg:98.35ms
step:1324/1770 train_time:129234ms step_avg:98.35ms
step:1325/1770 train_time:129337ms step_avg:98.35ms
step:1326/1770 train_time:129438ms step_avg:98.36ms
step:1327/1770 train_time:129544ms step_avg:98.36ms
step:1328/1770 train_time:129645ms step_avg:98.36ms
step:1329/1770 train_time:129747ms step_avg:98.37ms
step:1330/1770 train_time:129848ms step_avg:98.37ms
step:1331/1770 train_time:129950ms step_avg:98.37ms
step:1332/1770 train_time:130052ms step_avg:98.38ms
step:1333/1770 train_time:130153ms step_avg:98.38ms
step:1334/1770 train_time:130256ms step_avg:98.38ms
step:1335/1770 train_time:130358ms step_avg:98.38ms
step:1336/1770 train_time:130460ms step_avg:98.39ms
step:1337/1770 train_time:130562ms step_avg:98.39ms
step:1338/1770 train_time:130663ms step_avg:98.39ms
step:1339/1770 train_time:130767ms step_avg:98.40ms
step:1340/1770 train_time:130871ms step_avg:98.40ms
step:1341/1770 train_time:130972ms step_avg:98.40ms
step:1342/1770 train_time:131075ms step_avg:98.40ms
step:1343/1770 train_time:131179ms step_avg:98.41ms
step:1344/1770 train_time:131282ms step_avg:98.41ms
step:1345/1770 train_time:131384ms step_avg:98.41ms
step:1346/1770 train_time:131486ms step_avg:98.42ms
step:1347/1770 train_time:131588ms step_avg:98.42ms
step:1348/1770 train_time:131692ms step_avg:98.42ms
step:1349/1770 train_time:131794ms step_avg:98.43ms
step:1350/1770 train_time:131897ms step_avg:98.43ms
step:1351/1770 train_time:131999ms step_avg:98.43ms
step:1352/1770 train_time:132101ms step_avg:98.44ms
step:1353/1770 train_time:132205ms step_avg:98.44ms
step:1354/1770 train_time:132307ms step_avg:98.44ms
step:1355/1770 train_time:132410ms step_avg:98.45ms
step:1356/1770 train_time:132512ms step_avg:98.45ms
step:1357/1770 train_time:132614ms step_avg:98.45ms
step:1358/1770 train_time:132717ms step_avg:98.45ms
step:1359/1770 train_time:132819ms step_avg:98.46ms
step:1360/1770 train_time:132922ms step_avg:98.46ms
step:1361/1770 train_time:133024ms step_avg:98.46ms
step:1362/1770 train_time:133127ms step_avg:98.47ms
step:1363/1770 train_time:133230ms step_avg:98.47ms
step:1364/1770 train_time:133333ms step_avg:98.47ms
step:1365/1770 train_time:133435ms step_avg:98.48ms
step:1366/1770 train_time:133537ms step_avg:98.48ms
step:1367/1770 train_time:133639ms step_avg:98.48ms
step:1368/1770 train_time:133740ms step_avg:98.48ms
step:1369/1770 train_time:133844ms step_avg:98.49ms
step:1370/1770 train_time:133946ms step_avg:98.49ms
step:1371/1770 train_time:134048ms step_avg:98.49ms
step:1372/1770 train_time:134149ms step_avg:98.49ms
step:1373/1770 train_time:134251ms step_avg:98.50ms
step:1374/1770 train_time:134354ms step_avg:98.50ms
step:1375/1770 train_time:134456ms step_avg:98.50ms
step:1375/1770 val_loss:3.3848 train_time:134557ms step_avg:98.58ms
step:1376/1770 train_time:134578ms step_avg:98.52ms
step:1377/1770 train_time:134669ms step_avg:98.51ms
step:1378/1770 train_time:134772ms step_avg:98.52ms
step:1379/1770 train_time:134873ms step_avg:98.52ms
step:1380/1770 train_time:134975ms step_avg:98.52ms
step:1381/1770 train_time:135077ms step_avg:98.52ms
step:1382/1770 train_time:135178ms step_avg:98.53ms
step:1383/1770 train_time:135281ms step_avg:98.53ms
step:1384/1770 train_time:135383ms step_avg:98.53ms
step:1385/1770 train_time:135485ms step_avg:98.53ms
step:1386/1770 train_time:135588ms step_avg:98.54ms
step:1387/1770 train_time:135692ms step_avg:98.54ms
step:1388/1770 train_time:135795ms step_avg:98.54ms
step:1389/1770 train_time:135897ms step_avg:98.55ms
step:1390/1770 train_time:135999ms step_avg:98.55ms
step:1391/1770 train_time:136101ms step_avg:98.55ms
step:1392/1770 train_time:136203ms step_avg:98.55ms
step:1393/1770 train_time:136305ms step_avg:98.56ms
step:1394/1770 train_time:136406ms step_avg:98.56ms
step:1395/1770 train_time:136509ms step_avg:98.56ms
step:1396/1770 train_time:136612ms step_avg:98.57ms
step:1397/1770 train_time:136715ms step_avg:98.57ms
step:1398/1770 train_time:136817ms step_avg:98.57ms
step:1399/1770 train_time:136920ms step_avg:98.57ms
step:1400/1770 train_time:137022ms step_avg:98.58ms
step:1401/1770 train_time:137124ms step_avg:98.58ms
step:1402/1770 train_time:137226ms step_avg:98.58ms
step:1403/1770 train_time:137328ms step_avg:98.58ms
step:1404/1770 train_time:137430ms step_avg:98.59ms
step:1405/1770 train_time:137533ms step_avg:98.59ms
step:1406/1770 train_time:137635ms step_avg:98.59ms
step:1407/1770 train_time:137737ms step_avg:98.59ms
step:1408/1770 train_time:137839ms step_avg:98.60ms
step:1409/1770 train_time:137942ms step_avg:98.60ms
step:1410/1770 train_time:138044ms step_avg:98.60ms
step:1411/1770 train_time:138145ms step_avg:98.60ms
step:1412/1770 train_time:138247ms step_avg:98.61ms
step:1413/1770 train_time:138348ms step_avg:98.61ms
step:1414/1770 train_time:138451ms step_avg:98.61ms
step:1415/1770 train_time:138553ms step_avg:98.61ms
step:1416/1770 train_time:138657ms step_avg:98.62ms
step:1417/1770 train_time:138758ms step_avg:98.62ms
step:1418/1770 train_time:138861ms step_avg:98.62ms
step:1419/1770 train_time:138964ms step_avg:98.63ms
step:1420/1770 train_time:139066ms step_avg:98.63ms
step:1421/1770 train_time:139168ms step_avg:98.63ms
step:1422/1770 train_time:139270ms step_avg:98.63ms
step:1423/1770 train_time:139372ms step_avg:98.64ms
step:1424/1770 train_time:139475ms step_avg:98.64ms
step:1425/1770 train_time:139577ms step_avg:98.64ms
step:1426/1770 train_time:139679ms step_avg:98.64ms
step:1427/1770 train_time:139781ms step_avg:98.65ms
step:1428/1770 train_time:139884ms step_avg:98.65ms
step:1429/1770 train_time:139986ms step_avg:98.65ms
step:1430/1770 train_time:140088ms step_avg:98.65ms
step:1431/1770 train_time:140192ms step_avg:98.66ms
step:1432/1770 train_time:140293ms step_avg:98.66ms
step:1433/1770 train_time:140395ms step_avg:98.66ms
step:1434/1770 train_time:140498ms step_avg:98.66ms
step:1435/1770 train_time:140599ms step_avg:98.67ms
step:1436/1770 train_time:140703ms step_avg:98.67ms
step:1437/1770 train_time:140806ms step_avg:98.67ms
step:1438/1770 train_time:140908ms step_avg:98.68ms
step:1439/1770 train_time:141010ms step_avg:98.68ms
step:1440/1770 train_time:141112ms step_avg:98.68ms
step:1441/1770 train_time:141218ms step_avg:98.68ms
step:1442/1770 train_time:141320ms step_avg:98.69ms
step:1443/1770 train_time:141422ms step_avg:98.69ms
step:1444/1770 train_time:141524ms step_avg:98.69ms
step:1445/1770 train_time:141627ms step_avg:98.69ms
step:1446/1770 train_time:141730ms step_avg:98.70ms
step:1447/1770 train_time:141834ms step_avg:98.70ms
step:1448/1770 train_time:141937ms step_avg:98.70ms
step:1449/1770 train_time:142041ms step_avg:98.71ms
step:1450/1770 train_time:142143ms step_avg:98.71ms
step:1451/1770 train_time:142247ms step_avg:98.71ms
step:1452/1770 train_time:142350ms step_avg:98.72ms
step:1453/1770 train_time:142453ms step_avg:98.72ms
step:1454/1770 train_time:142556ms step_avg:98.72ms
step:1455/1770 train_time:142661ms step_avg:98.73ms
step:1456/1770 train_time:142765ms step_avg:98.73ms
step:1457/1770 train_time:142869ms step_avg:98.73ms
step:1458/1770 train_time:142972ms step_avg:98.74ms
step:1459/1770 train_time:143076ms step_avg:98.74ms
step:1460/1770 train_time:143180ms step_avg:98.75ms
step:1461/1770 train_time:143284ms step_avg:98.75ms
step:1462/1770 train_time:143387ms step_avg:98.75ms
step:1463/1770 train_time:143491ms step_avg:98.75ms
step:1464/1770 train_time:143595ms step_avg:98.76ms
step:1465/1770 train_time:143698ms step_avg:98.76ms
step:1466/1770 train_time:143802ms step_avg:98.77ms
step:1467/1770 train_time:143907ms step_avg:98.77ms
step:1468/1770 train_time:144010ms step_avg:98.77ms
step:1469/1770 train_time:144113ms step_avg:98.78ms
step:1470/1770 train_time:144217ms step_avg:98.78ms
step:1471/1770 train_time:144320ms step_avg:98.78ms
step:1472/1770 train_time:144423ms step_avg:98.78ms
step:1473/1770 train_time:144527ms step_avg:98.79ms
step:1474/1770 train_time:144631ms step_avg:98.79ms
step:1475/1770 train_time:144734ms step_avg:98.79ms
step:1476/1770 train_time:144836ms step_avg:98.80ms
step:1477/1770 train_time:144941ms step_avg:98.80ms
step:1478/1770 train_time:145045ms step_avg:98.80ms
step:1479/1770 train_time:145148ms step_avg:98.81ms
step:1480/1770 train_time:145252ms step_avg:98.81ms
step:1481/1770 train_time:145359ms step_avg:98.82ms
step:1482/1770 train_time:145462ms step_avg:98.82ms
step:1483/1770 train_time:145566ms step_avg:98.82ms
step:1484/1770 train_time:145669ms step_avg:98.83ms
step:1485/1770 train_time:145772ms step_avg:98.83ms
step:1486/1770 train_time:145875ms step_avg:98.83ms
step:1487/1770 train_time:145978ms step_avg:98.83ms
step:1488/1770 train_time:146083ms step_avg:98.84ms
step:1489/1770 train_time:146187ms step_avg:98.84ms
step:1490/1770 train_time:146291ms step_avg:98.84ms
step:1491/1770 train_time:146393ms step_avg:98.85ms
step:1492/1770 train_time:146497ms step_avg:98.85ms
step:1493/1770 train_time:146603ms step_avg:98.86ms
step:1494/1770 train_time:146709ms step_avg:98.86ms
step:1495/1770 train_time:146812ms step_avg:98.86ms
step:1496/1770 train_time:146915ms step_avg:98.87ms
step:1497/1770 train_time:147019ms step_avg:98.87ms
step:1498/1770 train_time:147121ms step_avg:98.87ms
step:1499/1770 train_time:147224ms step_avg:98.87ms
step:1500/1770 train_time:147326ms step_avg:98.88ms
step:1500/1770 val_loss:3.3469 train_time:147427ms step_avg:98.94ms
step:1501/1770 train_time:147448ms step_avg:98.89ms
step:1502/1770 train_time:147542ms step_avg:98.89ms
step:1503/1770 train_time:147644ms step_avg:98.89ms
step:1504/1770 train_time:147748ms step_avg:98.89ms
step:1505/1770 train_time:147853ms step_avg:98.90ms
step:1506/1770 train_time:147956ms step_avg:98.90ms
step:1507/1770 train_time:148060ms step_avg:98.90ms
step:1508/1770 train_time:148166ms step_avg:98.91ms
step:1509/1770 train_time:148269ms step_avg:98.91ms
step:1510/1770 train_time:148371ms step_avg:98.91ms
step:1511/1770 train_time:148476ms step_avg:98.92ms
step:1512/1770 train_time:148581ms step_avg:98.92ms
step:1513/1770 train_time:148685ms step_avg:98.93ms
step:1514/1770 train_time:148789ms step_avg:98.93ms
step:1515/1770 train_time:148891ms step_avg:98.93ms
step:1516/1770 train_time:148995ms step_avg:98.93ms
step:1517/1770 train_time:149099ms step_avg:98.94ms
step:1518/1770 train_time:149204ms step_avg:98.94ms
step:1519/1770 train_time:149306ms step_avg:98.94ms
step:1520/1770 train_time:149410ms step_avg:98.95ms
step:1521/1770 train_time:149514ms step_avg:98.95ms
step:1522/1770 train_time:149618ms step_avg:98.95ms
step:1523/1770 train_time:149721ms step_avg:98.96ms
step:1524/1770 train_time:149824ms step_avg:98.96ms
step:1525/1770 train_time:149927ms step_avg:98.96ms
step:1526/1770 train_time:150029ms step_avg:98.96ms
step:1527/1770 train_time:150133ms step_avg:98.97ms
step:1528/1770 train_time:150239ms step_avg:98.97ms
step:1529/1770 train_time:150342ms step_avg:98.97ms
step:1530/1770 train_time:150445ms step_avg:98.98ms
step:1531/1770 train_time:150548ms step_avg:98.98ms
step:1532/1770 train_time:150652ms step_avg:98.98ms
step:1533/1770 train_time:150756ms step_avg:98.99ms
step:1534/1770 train_time:150860ms step_avg:98.99ms
step:1535/1770 train_time:150963ms step_avg:98.99ms
step:1536/1770 train_time:151066ms step_avg:98.99ms
step:1537/1770 train_time:151169ms step_avg:99.00ms
step:1538/1770 train_time:151275ms step_avg:99.00ms
step:1539/1770 train_time:151378ms step_avg:99.00ms
step:1540/1770 train_time:151483ms step_avg:99.01ms
step:1541/1770 train_time:151588ms step_avg:99.01ms
step:1542/1770 train_time:151691ms step_avg:99.02ms
step:1543/1770 train_time:151794ms step_avg:99.02ms
step:1544/1770 train_time:151899ms step_avg:99.02ms
step:1545/1770 train_time:152003ms step_avg:99.02ms
step:1546/1770 train_time:152106ms step_avg:99.03ms
step:1547/1770 train_time:152210ms step_avg:99.03ms
step:1548/1770 train_time:152313ms step_avg:99.03ms
step:1549/1770 train_time:152417ms step_avg:99.04ms
step:1550/1770 train_time:152522ms step_avg:99.04ms
step:1551/1770 train_time:152624ms step_avg:99.04ms
step:1552/1770 train_time:152730ms step_avg:99.05ms
step:1553/1770 train_time:152833ms step_avg:99.05ms
step:1554/1770 train_time:152936ms step_avg:99.05ms
step:1555/1770 train_time:153040ms step_avg:99.05ms
step:1556/1770 train_time:153143ms step_avg:99.06ms
step:1557/1770 train_time:153246ms step_avg:99.06ms
step:1558/1770 train_time:153351ms step_avg:99.06ms
step:1559/1770 train_time:153455ms step_avg:99.07ms
step:1560/1770 train_time:153557ms step_avg:99.07ms
step:1561/1770 train_time:153663ms step_avg:99.07ms
step:1562/1770 train_time:153766ms step_avg:99.08ms
step:1563/1770 train_time:153870ms step_avg:99.08ms
step:1564/1770 train_time:153973ms step_avg:99.08ms
step:1565/1770 train_time:154075ms step_avg:99.08ms
step:1566/1770 train_time:154178ms step_avg:99.09ms
step:1567/1770 train_time:154283ms step_avg:99.09ms
step:1568/1770 train_time:154385ms step_avg:99.09ms
step:1569/1770 train_time:154493ms step_avg:99.10ms
step:1570/1770 train_time:154596ms step_avg:99.10ms
step:1571/1770 train_time:154700ms step_avg:99.10ms
step:1572/1770 train_time:154805ms step_avg:99.11ms
step:1573/1770 train_time:154910ms step_avg:99.11ms
step:1574/1770 train_time:155012ms step_avg:99.11ms
step:1575/1770 train_time:155114ms step_avg:99.11ms
step:1576/1770 train_time:155217ms step_avg:99.12ms
step:1577/1770 train_time:155322ms step_avg:99.12ms
step:1578/1770 train_time:155427ms step_avg:99.12ms
step:1579/1770 train_time:155529ms step_avg:99.13ms
step:1580/1770 train_time:155633ms step_avg:99.13ms
step:1581/1770 train_time:155739ms step_avg:99.13ms
step:1582/1770 train_time:155843ms step_avg:99.14ms
step:1583/1770 train_time:155947ms step_avg:99.14ms
step:1584/1770 train_time:156052ms step_avg:99.14ms
step:1585/1770 train_time:156155ms step_avg:99.15ms
step:1586/1770 train_time:156264ms step_avg:99.15ms
step:1587/1770 train_time:156367ms step_avg:99.15ms
step:1588/1770 train_time:156470ms step_avg:99.16ms
step:1589/1770 train_time:156575ms step_avg:99.16ms
step:1590/1770 train_time:156679ms step_avg:99.16ms
step:1591/1770 train_time:156783ms step_avg:99.17ms
step:1592/1770 train_time:156887ms step_avg:99.17ms
step:1593/1770 train_time:156990ms step_avg:99.17ms
step:1594/1770 train_time:157093ms step_avg:99.17ms
step:1595/1770 train_time:157196ms step_avg:99.18ms
step:1596/1770 train_time:157300ms step_avg:99.18ms
step:1597/1770 train_time:157404ms step_avg:99.18ms
step:1598/1770 train_time:157507ms step_avg:99.19ms
step:1599/1770 train_time:157611ms step_avg:99.19ms
step:1600/1770 train_time:157718ms step_avg:99.19ms
step:1601/1770 train_time:157822ms step_avg:99.20ms
step:1602/1770 train_time:157926ms step_avg:99.20ms
step:1603/1770 train_time:158029ms step_avg:99.20ms
step:1604/1770 train_time:158132ms step_avg:99.20ms
step:1605/1770 train_time:158235ms step_avg:99.21ms
step:1606/1770 train_time:158339ms step_avg:99.21ms
step:1607/1770 train_time:158446ms step_avg:99.21ms
step:1608/1770 train_time:158549ms step_avg:99.22ms
step:1609/1770 train_time:158652ms step_avg:99.22ms
step:1610/1770 train_time:158757ms step_avg:99.22ms
step:1611/1770 train_time:158863ms step_avg:99.23ms
step:1612/1770 train_time:158968ms step_avg:99.23ms
step:1613/1770 train_time:159071ms step_avg:99.23ms
step:1614/1770 train_time:159176ms step_avg:99.24ms
step:1615/1770 train_time:159280ms step_avg:99.24ms
step:1616/1770 train_time:159382ms step_avg:99.24ms
step:1617/1770 train_time:159489ms step_avg:99.25ms
step:1618/1770 train_time:159593ms step_avg:99.25ms
step:1619/1770 train_time:159697ms step_avg:99.25ms
step:1620/1770 train_time:159800ms step_avg:99.25ms
step:1621/1770 train_time:159903ms step_avg:99.26ms
step:1622/1770 train_time:160007ms step_avg:99.26ms
step:1623/1770 train_time:160115ms step_avg:99.27ms
step:1624/1770 train_time:160217ms step_avg:99.27ms
step:1625/1770 train_time:160320ms step_avg:99.27ms
step:1625/1770 val_loss:3.3119 train_time:160421ms step_avg:99.33ms
step:1626/1770 train_time:160443ms step_avg:99.28ms
step:1627/1770 train_time:160532ms step_avg:99.28ms
step:1628/1770 train_time:160636ms step_avg:99.28ms
step:1629/1770 train_time:160738ms step_avg:99.28ms
step:1630/1770 train_time:160842ms step_avg:99.28ms
step:1631/1770 train_time:160944ms step_avg:99.29ms
step:1632/1770 train_time:161047ms step_avg:99.29ms
step:1633/1770 train_time:161150ms step_avg:99.29ms
step:1634/1770 train_time:161253ms step_avg:99.29ms
step:1635/1770 train_time:161356ms step_avg:99.30ms
step:1636/1770 train_time:161460ms step_avg:99.30ms
step:1637/1770 train_time:161566ms step_avg:99.30ms
step:1638/1770 train_time:161670ms step_avg:99.31ms
step:1639/1770 train_time:161773ms step_avg:99.31ms
step:1640/1770 train_time:161876ms step_avg:99.31ms
step:1641/1770 train_time:161979ms step_avg:99.31ms
step:1642/1770 train_time:162081ms step_avg:99.31ms
step:1643/1770 train_time:162184ms step_avg:99.32ms
step:1644/1770 train_time:162289ms step_avg:99.32ms
step:1645/1770 train_time:162392ms step_avg:99.32ms
step:1646/1770 train_time:162498ms step_avg:99.33ms
step:1647/1770 train_time:162602ms step_avg:99.33ms
step:1648/1770 train_time:162706ms step_avg:99.33ms
step:1649/1770 train_time:162809ms step_avg:99.33ms
step:1650/1770 train_time:162913ms step_avg:99.34ms
step:1651/1770 train_time:163016ms step_avg:99.34ms
step:1652/1770 train_time:163120ms step_avg:99.34ms
step:1653/1770 train_time:163223ms step_avg:99.34ms
step:1654/1770 train_time:163330ms step_avg:99.35ms
step:1655/1770 train_time:163435ms step_avg:99.35ms
step:1656/1770 train_time:163538ms step_avg:99.35ms
step:1657/1770 train_time:163643ms step_avg:99.36ms
step:1658/1770 train_time:163746ms step_avg:99.36ms
step:1659/1770 train_time:163851ms step_avg:99.36ms
step:1660/1770 train_time:163954ms step_avg:99.37ms
step:1661/1770 train_time:164059ms step_avg:99.37ms
step:1662/1770 train_time:164162ms step_avg:99.37ms
step:1663/1770 train_time:164265ms step_avg:99.37ms
step:1664/1770 train_time:164369ms step_avg:99.38ms
step:1665/1770 train_time:164471ms step_avg:99.38ms
step:1666/1770 train_time:164575ms step_avg:99.38ms
step:1667/1770 train_time:164677ms step_avg:99.38ms
step:1668/1770 train_time:164780ms step_avg:99.39ms
step:1669/1770 train_time:164883ms step_avg:99.39ms
step:1670/1770 train_time:164987ms step_avg:99.39ms
step:1671/1770 train_time:165091ms step_avg:99.39ms
step:1672/1770 train_time:165194ms step_avg:99.39ms
step:1673/1770 train_time:165299ms step_avg:99.40ms
step:1674/1770 train_time:165402ms step_avg:99.40ms
step:1675/1770 train_time:165505ms step_avg:99.40ms
step:1676/1770 train_time:165609ms step_avg:99.41ms
step:1677/1770 train_time:165716ms step_avg:99.41ms
step:1678/1770 train_time:165819ms step_avg:99.41ms
step:1679/1770 train_time:165922ms step_avg:99.41ms
step:1680/1770 train_time:166026ms step_avg:99.42ms
step:1681/1770 train_time:166131ms step_avg:99.42ms
step:1682/1770 train_time:166235ms step_avg:99.42ms
step:1683/1770 train_time:166338ms step_avg:99.43ms
step:1684/1770 train_time:166442ms step_avg:99.43ms
step:1685/1770 train_time:166546ms step_avg:99.43ms
step:1686/1770 train_time:166650ms step_avg:99.43ms
step:1687/1770 train_time:166756ms step_avg:99.44ms
step:1688/1770 train_time:166858ms step_avg:99.44ms
step:1689/1770 train_time:166962ms step_avg:99.44ms
step:1690/1770 train_time:167065ms step_avg:99.44ms
step:1691/1770 train_time:167169ms step_avg:99.45ms
step:1692/1770 train_time:167272ms step_avg:99.45ms
step:1693/1770 train_time:167376ms step_avg:99.45ms
step:1694/1770 train_time:167479ms step_avg:99.45ms
step:1695/1770 train_time:167582ms step_avg:99.46ms
step:1696/1770 train_time:167687ms step_avg:99.46ms
step:1697/1770 train_time:167792ms step_avg:99.46ms
step:1698/1770 train_time:167897ms step_avg:99.47ms
step:1699/1770 train_time:168000ms step_avg:99.47ms
step:1700/1770 train_time:168103ms step_avg:99.47ms
step:1701/1770 train_time:168207ms step_avg:99.47ms
step:1702/1770 train_time:168311ms step_avg:99.47ms
step:1703/1770 train_time:168414ms step_avg:99.48ms
step:1704/1770 train_time:168518ms step_avg:99.48ms
step:1705/1770 train_time:168621ms step_avg:99.48ms
step:1706/1770 train_time:168723ms step_avg:99.48ms
step:1707/1770 train_time:168828ms step_avg:99.49ms
step:1708/1770 train_time:168932ms step_avg:99.49ms
step:1709/1770 train_time:169036ms step_avg:99.49ms
step:1710/1770 train_time:169143ms step_avg:99.50ms
step:1711/1770 train_time:169249ms step_avg:99.50ms
step:1712/1770 train_time:169354ms step_avg:99.50ms
step:1713/1770 train_time:169457ms step_avg:99.50ms
step:1714/1770 train_time:169560ms step_avg:99.51ms
step:1715/1770 train_time:169664ms step_avg:99.51ms
step:1716/1770 train_time:169769ms step_avg:99.51ms
step:1717/1770 train_time:169873ms step_avg:99.52ms
step:1718/1770 train_time:169977ms step_avg:99.52ms
step:1719/1770 train_time:170082ms step_avg:99.52ms
step:1720/1770 train_time:170188ms step_avg:99.53ms
step:1721/1770 train_time:170291ms step_avg:99.53ms
step:1722/1770 train_time:170398ms step_avg:99.53ms
step:1723/1770 train_time:170503ms step_avg:99.53ms
step:1724/1770 train_time:170609ms step_avg:99.54ms
step:1725/1770 train_time:170715ms step_avg:99.54ms
step:1726/1770 train_time:170822ms step_avg:99.55ms
step:1727/1770 train_time:170925ms step_avg:99.55ms
step:1728/1770 train_time:171031ms step_avg:99.55ms
step:1729/1770 train_time:171135ms step_avg:99.55ms
step:1730/1770 train_time:171240ms step_avg:99.56ms
step:1731/1770 train_time:171345ms step_avg:99.56ms
step:1732/1770 train_time:171449ms step_avg:99.56ms
step:1733/1770 train_time:171554ms step_avg:99.57ms
step:1734/1770 train_time:171658ms step_avg:99.57ms
step:1735/1770 train_time:171762ms step_avg:99.57ms
step:1736/1770 train_time:171866ms step_avg:99.57ms
step:1737/1770 train_time:171970ms step_avg:99.58ms
step:1738/1770 train_time:172074ms step_avg:99.58ms
step:1739/1770 train_time:172179ms step_avg:99.58ms
step:1740/1770 train_time:172283ms step_avg:99.59ms
step:1741/1770 train_time:172389ms step_avg:99.59ms
step:1742/1770 train_time:172496ms step_avg:99.59ms
step:1743/1770 train_time:172601ms step_avg:99.60ms
step:1744/1770 train_time:172706ms step_avg:99.60ms
step:1745/1770 train_time:172809ms step_avg:99.60ms
step:1746/1770 train_time:172917ms step_avg:99.61ms
step:1747/1770 train_time:173019ms step_avg:99.61ms
step:1748/1770 train_time:173125ms step_avg:99.61ms
step:1749/1770 train_time:173230ms step_avg:99.61ms
step:1750/1770 train_time:173334ms step_avg:99.62ms
step:1750/1770 val_loss:3.2851 train_time:173436ms step_avg:99.68ms
step:1751/1770 train_time:173458ms step_avg:99.63ms
step:1752/1770 train_time:173548ms step_avg:99.63ms
step:1753/1770 train_time:173653ms step_avg:99.63ms
step:1754/1770 train_time:173759ms step_avg:99.63ms
step:1755/1770 train_time:173862ms step_avg:99.63ms
step:1756/1770 train_time:173966ms step_avg:99.64ms
step:1757/1770 train_time:174071ms step_avg:99.64ms
step:1758/1770 train_time:174175ms step_avg:99.64ms
step:1759/1770 train_time:174279ms step_avg:99.65ms
step:1760/1770 train_time:174383ms step_avg:99.65ms
step:1761/1770 train_time:174490ms step_avg:99.65ms
step:1762/1770 train_time:174598ms step_avg:99.66ms
step:1763/1770 train_time:174701ms step_avg:99.66ms
step:1764/1770 train_time:174806ms step_avg:99.66ms
step:1765/1770 train_time:174910ms step_avg:99.66ms
step:1766/1770 train_time:175018ms step_avg:99.67ms
step:1767/1770 train_time:175121ms step_avg:99.67ms
step:1768/1770 train_time:175226ms step_avg:99.67ms
step:1769/1770 train_time:175330ms step_avg:99.68ms
step:1770/1770 train_time:175432ms step_avg:99.68ms
step:1770/1770 val_loss:3.2822 train_time:175536ms step_avg:99.74ms
peak memory allocated: 28840 MiB reserved: 32192 MiB
