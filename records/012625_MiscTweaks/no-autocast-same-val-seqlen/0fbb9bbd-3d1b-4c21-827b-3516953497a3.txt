import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 14:47:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:25735ms step_avg:nanms
step:2/1770 train_time:26247ms step_avg:nanms
step:3/1770 train_time:26342ms step_avg:nanms
step:4/1770 train_time:26434ms step_avg:nanms
step:5/1770 train_time:26528ms step_avg:nanms
step:6/1770 train_time:26622ms step_avg:nanms
step:7/1770 train_time:26717ms step_avg:nanms
step:8/1770 train_time:26810ms step_avg:nanms
step:9/1770 train_time:26904ms step_avg:nanms
step:10/1770 train_time:26998ms step_avg:nanms
step:11/1770 train_time:97ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.84ms
step:14/1770 train_time:375ms step_avg:93.79ms
step:15/1770 train_time:470ms step_avg:93.90ms
step:16/1770 train_time:564ms step_avg:93.93ms
step:17/1770 train_time:658ms step_avg:93.94ms
step:18/1770 train_time:751ms step_avg:93.90ms
step:19/1770 train_time:845ms step_avg:93.91ms
step:20/1770 train_time:940ms step_avg:93.96ms
step:21/1770 train_time:1033ms step_avg:93.95ms
step:22/1770 train_time:1127ms step_avg:93.93ms
step:23/1770 train_time:1221ms step_avg:93.94ms
step:24/1770 train_time:1315ms step_avg:93.95ms
step:25/1770 train_time:1409ms step_avg:93.96ms
step:26/1770 train_time:1504ms step_avg:93.97ms
step:27/1770 train_time:1598ms step_avg:93.99ms
step:28/1770 train_time:1692ms step_avg:93.98ms
step:29/1770 train_time:1789ms step_avg:94.13ms
step:30/1770 train_time:1879ms step_avg:93.96ms
step:31/1770 train_time:1973ms step_avg:93.95ms
step:32/1770 train_time:2067ms step_avg:93.93ms
step:33/1770 train_time:2161ms step_avg:93.95ms
step:34/1770 train_time:2255ms step_avg:93.94ms
step:35/1770 train_time:2348ms step_avg:93.93ms
step:36/1770 train_time:2442ms step_avg:93.92ms
step:37/1770 train_time:2537ms step_avg:93.95ms
step:38/1770 train_time:2630ms step_avg:93.94ms
step:39/1770 train_time:2724ms step_avg:93.95ms
step:40/1770 train_time:2819ms step_avg:93.96ms
step:41/1770 train_time:2913ms step_avg:93.96ms
step:42/1770 train_time:3007ms step_avg:93.96ms
step:43/1770 train_time:3101ms step_avg:93.97ms
step:44/1770 train_time:3195ms step_avg:93.97ms
step:45/1770 train_time:3289ms step_avg:93.96ms
step:46/1770 train_time:3383ms step_avg:93.96ms
step:47/1770 train_time:3476ms step_avg:93.96ms
step:48/1770 train_time:3570ms step_avg:93.95ms
step:49/1770 train_time:3664ms step_avg:93.95ms
step:50/1770 train_time:3758ms step_avg:93.95ms
step:51/1770 train_time:3852ms step_avg:93.95ms
step:52/1770 train_time:3946ms step_avg:93.95ms
step:53/1770 train_time:4040ms step_avg:93.94ms
step:54/1770 train_time:4133ms step_avg:93.93ms
step:55/1770 train_time:4227ms step_avg:93.93ms
step:56/1770 train_time:4321ms step_avg:93.94ms
step:57/1770 train_time:4415ms step_avg:93.94ms
step:58/1770 train_time:4509ms step_avg:93.94ms
step:59/1770 train_time:4603ms step_avg:93.94ms
step:60/1770 train_time:4697ms step_avg:93.93ms
step:61/1770 train_time:4791ms step_avg:93.93ms
step:62/1770 train_time:4885ms step_avg:93.94ms
step:63/1770 train_time:4979ms step_avg:93.94ms
step:64/1770 train_time:5073ms step_avg:93.94ms
step:65/1770 train_time:5167ms step_avg:93.94ms
step:66/1770 train_time:5261ms step_avg:93.94ms
step:67/1770 train_time:5354ms step_avg:93.94ms
step:68/1770 train_time:5448ms step_avg:93.94ms
step:69/1770 train_time:5542ms step_avg:93.94ms
step:70/1770 train_time:5636ms step_avg:93.94ms
step:71/1770 train_time:5730ms step_avg:93.94ms
step:72/1770 train_time:5824ms step_avg:93.94ms
step:73/1770 train_time:5918ms step_avg:93.94ms
step:74/1770 train_time:6012ms step_avg:93.93ms
step:75/1770 train_time:6106ms step_avg:93.94ms
step:76/1770 train_time:6200ms step_avg:93.93ms
step:77/1770 train_time:6293ms step_avg:93.93ms
step:78/1770 train_time:6387ms step_avg:93.93ms
step:79/1770 train_time:6482ms step_avg:93.94ms
step:80/1770 train_time:6576ms step_avg:93.94ms
step:81/1770 train_time:6670ms step_avg:93.94ms
step:82/1770 train_time:6764ms step_avg:93.94ms
step:83/1770 train_time:6858ms step_avg:93.94ms
step:84/1770 train_time:6952ms step_avg:93.95ms
step:85/1770 train_time:7047ms step_avg:93.96ms
step:86/1770 train_time:7141ms step_avg:93.96ms
step:87/1770 train_time:7235ms step_avg:93.96ms
step:88/1770 train_time:7328ms step_avg:93.95ms
step:89/1770 train_time:7422ms step_avg:93.95ms
step:90/1770 train_time:7517ms step_avg:93.96ms
step:91/1770 train_time:7610ms step_avg:93.95ms
step:92/1770 train_time:7705ms step_avg:93.97ms
step:93/1770 train_time:7798ms step_avg:93.95ms
step:94/1770 train_time:7892ms step_avg:93.95ms
step:95/1770 train_time:7986ms step_avg:93.95ms
step:96/1770 train_time:8080ms step_avg:93.95ms
step:97/1770 train_time:8173ms step_avg:93.94ms
step:98/1770 train_time:8267ms step_avg:93.94ms
step:99/1770 train_time:8361ms step_avg:93.94ms
step:100/1770 train_time:8455ms step_avg:93.94ms
step:101/1770 train_time:8549ms step_avg:93.94ms
step:102/1770 train_time:8643ms step_avg:93.94ms
step:103/1770 train_time:8738ms step_avg:93.96ms
step:104/1770 train_time:8832ms step_avg:93.95ms
step:105/1770 train_time:8926ms step_avg:93.95ms
step:106/1770 train_time:9020ms step_avg:93.95ms
step:107/1770 train_time:9113ms step_avg:93.95ms
step:108/1770 train_time:9207ms step_avg:93.95ms
step:109/1770 train_time:9301ms step_avg:93.95ms
step:110/1770 train_time:9395ms step_avg:93.95ms
step:111/1770 train_time:9494ms step_avg:94.00ms
step:112/1770 train_time:9583ms step_avg:93.95ms
step:113/1770 train_time:9677ms step_avg:93.96ms
step:114/1770 train_time:9771ms step_avg:93.95ms
step:115/1770 train_time:9865ms step_avg:93.95ms
step:116/1770 train_time:9959ms step_avg:93.95ms
step:117/1770 train_time:10053ms step_avg:93.95ms
step:118/1770 train_time:10146ms step_avg:93.95ms
step:119/1770 train_time:10241ms step_avg:93.95ms
step:120/1770 train_time:10334ms step_avg:93.95ms
step:121/1770 train_time:10428ms step_avg:93.95ms
step:122/1770 train_time:10522ms step_avg:93.95ms
step:123/1770 train_time:10616ms step_avg:93.95ms
step:124/1770 train_time:10710ms step_avg:93.95ms
step:125/1770 train_time:10804ms step_avg:93.95ms
step:125/1770 val_loss:4.6431 train_time:10897ms step_avg:94.75ms
step:126/1770 train_time:10920ms step_avg:94.14ms
step:127/1770 train_time:11061ms step_avg:94.54ms
step:128/1770 train_time:11110ms step_avg:94.15ms
step:129/1770 train_time:11203ms step_avg:94.14ms
step:130/1770 train_time:11297ms step_avg:94.14ms
step:131/1770 train_time:11391ms step_avg:94.14ms
step:132/1770 train_time:11485ms step_avg:94.14ms
step:133/1770 train_time:11578ms step_avg:94.13ms
step:134/1770 train_time:11672ms step_avg:94.13ms
step:135/1770 train_time:11766ms step_avg:94.13ms
step:136/1770 train_time:11860ms step_avg:94.13ms
step:137/1770 train_time:11956ms step_avg:94.14ms
step:138/1770 train_time:12051ms step_avg:94.15ms
step:139/1770 train_time:12146ms step_avg:94.15ms
step:140/1770 train_time:12240ms step_avg:94.15ms
step:141/1770 train_time:12334ms step_avg:94.16ms
step:142/1770 train_time:12429ms step_avg:94.16ms
step:143/1770 train_time:12523ms step_avg:94.16ms
step:144/1770 train_time:12617ms step_avg:94.16ms
step:145/1770 train_time:12711ms step_avg:94.16ms
step:146/1770 train_time:12805ms step_avg:94.16ms
step:147/1770 train_time:12899ms step_avg:94.16ms
step:148/1770 train_time:12994ms step_avg:94.16ms
step:149/1770 train_time:13089ms step_avg:94.17ms
step:150/1770 train_time:13183ms step_avg:94.17ms
step:151/1770 train_time:13278ms step_avg:94.17ms
step:152/1770 train_time:13373ms step_avg:94.17ms
step:153/1770 train_time:13467ms step_avg:94.18ms
step:154/1770 train_time:13561ms step_avg:94.17ms
step:155/1770 train_time:13656ms step_avg:94.18ms
step:156/1770 train_time:13750ms step_avg:94.18ms
step:157/1770 train_time:13844ms step_avg:94.18ms
step:158/1770 train_time:13938ms step_avg:94.18ms
step:159/1770 train_time:14033ms step_avg:94.18ms
step:160/1770 train_time:14128ms step_avg:94.19ms
step:161/1770 train_time:14222ms step_avg:94.18ms
step:162/1770 train_time:14316ms step_avg:94.19ms
step:163/1770 train_time:14412ms step_avg:94.19ms
step:164/1770 train_time:14506ms step_avg:94.20ms
step:165/1770 train_time:14600ms step_avg:94.20ms
step:166/1770 train_time:14695ms step_avg:94.20ms
step:167/1770 train_time:14789ms step_avg:94.20ms
step:168/1770 train_time:14884ms step_avg:94.20ms
step:169/1770 train_time:14978ms step_avg:94.20ms
step:170/1770 train_time:15073ms step_avg:94.21ms
step:171/1770 train_time:15169ms step_avg:94.22ms
step:172/1770 train_time:15262ms step_avg:94.21ms
step:173/1770 train_time:15356ms step_avg:94.21ms
step:174/1770 train_time:15452ms step_avg:94.22ms
step:175/1770 train_time:15546ms step_avg:94.22ms
step:176/1770 train_time:15641ms step_avg:94.22ms
step:177/1770 train_time:15736ms step_avg:94.23ms
step:178/1770 train_time:15831ms step_avg:94.23ms
step:179/1770 train_time:15925ms step_avg:94.23ms
step:180/1770 train_time:16019ms step_avg:94.23ms
step:181/1770 train_time:16114ms step_avg:94.23ms
step:182/1770 train_time:16209ms step_avg:94.24ms
step:183/1770 train_time:16303ms step_avg:94.24ms
step:184/1770 train_time:16397ms step_avg:94.24ms
step:185/1770 train_time:16492ms step_avg:94.24ms
step:186/1770 train_time:16587ms step_avg:94.24ms
step:187/1770 train_time:16682ms step_avg:94.25ms
step:188/1770 train_time:16777ms step_avg:94.25ms
step:189/1770 train_time:16872ms step_avg:94.26ms
step:190/1770 train_time:16967ms step_avg:94.26ms
step:191/1770 train_time:17061ms step_avg:94.26ms
step:192/1770 train_time:17156ms step_avg:94.26ms
step:193/1770 train_time:17254ms step_avg:94.28ms
step:194/1770 train_time:17345ms step_avg:94.27ms
step:195/1770 train_time:17439ms step_avg:94.26ms
step:196/1770 train_time:17534ms step_avg:94.27ms
step:197/1770 train_time:17628ms step_avg:94.27ms
step:198/1770 train_time:17722ms step_avg:94.27ms
step:199/1770 train_time:17818ms step_avg:94.27ms
step:200/1770 train_time:17913ms step_avg:94.28ms
step:201/1770 train_time:18008ms step_avg:94.28ms
step:202/1770 train_time:18102ms step_avg:94.28ms
step:203/1770 train_time:18196ms step_avg:94.28ms
step:204/1770 train_time:18292ms step_avg:94.29ms
step:205/1770 train_time:18385ms step_avg:94.28ms
step:206/1770 train_time:18480ms step_avg:94.28ms
step:207/1770 train_time:18574ms step_avg:94.29ms
step:208/1770 train_time:18669ms step_avg:94.29ms
step:209/1770 train_time:18763ms step_avg:94.28ms
step:210/1770 train_time:18857ms step_avg:94.29ms
step:211/1770 train_time:18952ms step_avg:94.29ms
step:212/1770 train_time:19047ms step_avg:94.29ms
step:213/1770 train_time:19141ms step_avg:94.29ms
step:214/1770 train_time:19236ms step_avg:94.30ms
step:215/1770 train_time:19331ms step_avg:94.30ms
step:216/1770 train_time:19426ms step_avg:94.30ms
step:217/1770 train_time:19520ms step_avg:94.30ms
step:218/1770 train_time:19615ms step_avg:94.30ms
step:219/1770 train_time:19710ms step_avg:94.31ms
step:220/1770 train_time:19804ms step_avg:94.30ms
step:221/1770 train_time:19899ms step_avg:94.31ms
step:222/1770 train_time:19994ms step_avg:94.31ms
step:223/1770 train_time:20088ms step_avg:94.31ms
step:224/1770 train_time:20182ms step_avg:94.31ms
step:225/1770 train_time:20277ms step_avg:94.31ms
step:226/1770 train_time:20372ms step_avg:94.31ms
step:227/1770 train_time:20466ms step_avg:94.31ms
step:228/1770 train_time:20560ms step_avg:94.31ms
step:229/1770 train_time:20655ms step_avg:94.31ms
step:230/1770 train_time:20749ms step_avg:94.32ms
step:231/1770 train_time:20844ms step_avg:94.32ms
step:232/1770 train_time:20938ms step_avg:94.32ms
step:233/1770 train_time:21033ms step_avg:94.32ms
step:234/1770 train_time:21128ms step_avg:94.32ms
step:235/1770 train_time:21222ms step_avg:94.32ms
step:236/1770 train_time:21317ms step_avg:94.32ms
step:237/1770 train_time:21413ms step_avg:94.33ms
step:238/1770 train_time:21507ms step_avg:94.33ms
step:239/1770 train_time:21601ms step_avg:94.33ms
step:240/1770 train_time:21697ms step_avg:94.33ms
step:241/1770 train_time:21792ms step_avg:94.34ms
step:242/1770 train_time:21886ms step_avg:94.34ms
step:243/1770 train_time:21981ms step_avg:94.34ms
step:244/1770 train_time:22076ms step_avg:94.34ms
step:245/1770 train_time:22171ms step_avg:94.34ms
step:246/1770 train_time:22265ms step_avg:94.34ms
step:247/1770 train_time:22360ms step_avg:94.34ms
step:248/1770 train_time:22455ms step_avg:94.35ms
step:249/1770 train_time:22550ms step_avg:94.35ms
step:250/1770 train_time:22644ms step_avg:94.35ms
step:250/1770 val_loss:4.1125 train_time:22737ms step_avg:94.74ms
step:251/1770 train_time:22758ms step_avg:94.43ms
step:252/1770 train_time:22843ms step_avg:94.39ms
step:253/1770 train_time:22941ms step_avg:94.41ms
step:254/1770 train_time:23037ms step_avg:94.42ms
step:255/1770 train_time:23132ms step_avg:94.42ms
step:256/1770 train_time:23226ms step_avg:94.41ms
step:257/1770 train_time:23320ms step_avg:94.41ms
step:258/1770 train_time:23414ms step_avg:94.41ms
step:259/1770 train_time:23508ms step_avg:94.41ms
step:260/1770 train_time:23602ms step_avg:94.41ms
step:261/1770 train_time:23696ms step_avg:94.41ms
step:262/1770 train_time:23791ms step_avg:94.41ms
step:263/1770 train_time:23886ms step_avg:94.41ms
step:264/1770 train_time:23981ms step_avg:94.41ms
step:265/1770 train_time:24077ms step_avg:94.42ms
step:266/1770 train_time:24172ms step_avg:94.42ms
step:267/1770 train_time:24266ms step_avg:94.42ms
step:268/1770 train_time:24361ms step_avg:94.42ms
step:269/1770 train_time:24457ms step_avg:94.43ms
step:270/1770 train_time:24552ms step_avg:94.43ms
step:271/1770 train_time:24646ms step_avg:94.43ms
step:272/1770 train_time:24741ms step_avg:94.43ms
step:273/1770 train_time:24836ms step_avg:94.43ms
step:274/1770 train_time:24931ms step_avg:94.44ms
step:275/1770 train_time:25026ms step_avg:94.44ms
step:276/1770 train_time:25121ms step_avg:94.44ms
step:277/1770 train_time:25216ms step_avg:94.44ms
step:278/1770 train_time:25312ms step_avg:94.45ms
step:279/1770 train_time:25407ms step_avg:94.45ms
step:280/1770 train_time:25502ms step_avg:94.45ms
step:281/1770 train_time:25597ms step_avg:94.45ms
step:282/1770 train_time:25691ms step_avg:94.45ms
step:283/1770 train_time:25786ms step_avg:94.45ms
step:284/1770 train_time:25881ms step_avg:94.45ms
step:285/1770 train_time:25976ms step_avg:94.46ms
step:286/1770 train_time:26071ms step_avg:94.46ms
step:287/1770 train_time:26166ms step_avg:94.46ms
step:288/1770 train_time:26261ms step_avg:94.46ms
step:289/1770 train_time:26356ms step_avg:94.47ms
step:290/1770 train_time:26452ms step_avg:94.47ms
step:291/1770 train_time:26547ms step_avg:94.47ms
step:292/1770 train_time:26642ms step_avg:94.47ms
step:293/1770 train_time:26737ms step_avg:94.48ms
step:294/1770 train_time:26831ms step_avg:94.48ms
step:295/1770 train_time:26926ms step_avg:94.48ms
step:296/1770 train_time:27021ms step_avg:94.48ms
step:297/1770 train_time:27117ms step_avg:94.48ms
step:298/1770 train_time:27212ms step_avg:94.49ms
step:299/1770 train_time:27307ms step_avg:94.49ms
step:300/1770 train_time:27402ms step_avg:94.49ms
step:301/1770 train_time:27498ms step_avg:94.49ms
step:302/1770 train_time:27593ms step_avg:94.50ms
step:303/1770 train_time:27687ms step_avg:94.50ms
step:304/1770 train_time:27782ms step_avg:94.50ms
step:305/1770 train_time:27877ms step_avg:94.50ms
step:306/1770 train_time:27972ms step_avg:94.50ms
step:307/1770 train_time:28067ms step_avg:94.50ms
step:308/1770 train_time:28162ms step_avg:94.50ms
step:309/1770 train_time:28257ms step_avg:94.51ms
step:310/1770 train_time:28352ms step_avg:94.51ms
step:311/1770 train_time:28447ms step_avg:94.51ms
step:312/1770 train_time:28541ms step_avg:94.51ms
step:313/1770 train_time:28637ms step_avg:94.51ms
step:314/1770 train_time:28732ms step_avg:94.51ms
step:315/1770 train_time:28827ms step_avg:94.51ms
step:316/1770 train_time:28921ms step_avg:94.51ms
step:317/1770 train_time:29016ms step_avg:94.52ms
step:318/1770 train_time:29111ms step_avg:94.52ms
step:319/1770 train_time:29206ms step_avg:94.52ms
step:320/1770 train_time:29301ms step_avg:94.52ms
step:321/1770 train_time:29397ms step_avg:94.52ms
step:322/1770 train_time:29491ms step_avg:94.52ms
step:323/1770 train_time:29586ms step_avg:94.52ms
step:324/1770 train_time:29681ms step_avg:94.53ms
step:325/1770 train_time:29777ms step_avg:94.53ms
step:326/1770 train_time:29872ms step_avg:94.53ms
step:327/1770 train_time:29966ms step_avg:94.53ms
step:328/1770 train_time:30061ms step_avg:94.53ms
step:329/1770 train_time:30157ms step_avg:94.53ms
step:330/1770 train_time:30251ms step_avg:94.54ms
step:331/1770 train_time:30346ms step_avg:94.54ms
step:332/1770 train_time:30441ms step_avg:94.54ms
step:333/1770 train_time:30536ms step_avg:94.54ms
step:334/1770 train_time:30631ms step_avg:94.54ms
step:335/1770 train_time:30725ms step_avg:94.54ms
step:336/1770 train_time:30820ms step_avg:94.54ms
step:337/1770 train_time:30915ms step_avg:94.54ms
step:338/1770 train_time:31010ms step_avg:94.54ms
step:339/1770 train_time:31105ms step_avg:94.54ms
step:340/1770 train_time:31200ms step_avg:94.54ms
step:341/1770 train_time:31295ms step_avg:94.55ms
step:342/1770 train_time:31389ms step_avg:94.55ms
step:343/1770 train_time:31484ms step_avg:94.55ms
step:344/1770 train_time:31579ms step_avg:94.55ms
step:345/1770 train_time:31675ms step_avg:94.55ms
step:346/1770 train_time:31769ms step_avg:94.55ms
step:347/1770 train_time:31863ms step_avg:94.55ms
step:348/1770 train_time:31963ms step_avg:94.56ms
step:349/1770 train_time:32054ms step_avg:94.55ms
step:350/1770 train_time:32148ms step_avg:94.55ms
step:351/1770 train_time:32244ms step_avg:94.56ms
step:352/1770 train_time:32339ms step_avg:94.56ms
step:353/1770 train_time:32434ms step_avg:94.56ms
step:354/1770 train_time:32529ms step_avg:94.56ms
step:355/1770 train_time:32624ms step_avg:94.56ms
step:356/1770 train_time:32719ms step_avg:94.56ms
step:357/1770 train_time:32815ms step_avg:94.57ms
step:358/1770 train_time:32909ms step_avg:94.57ms
step:359/1770 train_time:33004ms step_avg:94.57ms
step:360/1770 train_time:33099ms step_avg:94.57ms
step:361/1770 train_time:33195ms step_avg:94.57ms
step:362/1770 train_time:33290ms step_avg:94.57ms
step:363/1770 train_time:33385ms step_avg:94.57ms
step:364/1770 train_time:33479ms step_avg:94.57ms
step:365/1770 train_time:33574ms step_avg:94.58ms
step:366/1770 train_time:33669ms step_avg:94.58ms
step:367/1770 train_time:33764ms step_avg:94.58ms
step:368/1770 train_time:33858ms step_avg:94.58ms
step:369/1770 train_time:33954ms step_avg:94.58ms
step:370/1770 train_time:34049ms step_avg:94.58ms
step:371/1770 train_time:34144ms step_avg:94.58ms
step:372/1770 train_time:34239ms step_avg:94.58ms
step:373/1770 train_time:34334ms step_avg:94.58ms
step:374/1770 train_time:34429ms step_avg:94.59ms
step:375/1770 train_time:34524ms step_avg:94.59ms
step:375/1770 val_loss:3.9113 train_time:34617ms step_avg:94.84ms
step:376/1770 train_time:34641ms step_avg:94.65ms
step:377/1770 train_time:34726ms step_avg:94.62ms
step:378/1770 train_time:34825ms step_avg:94.63ms
step:379/1770 train_time:34921ms step_avg:94.64ms
step:380/1770 train_time:35016ms step_avg:94.64ms
step:381/1770 train_time:35111ms step_avg:94.64ms
step:382/1770 train_time:35205ms step_avg:94.64ms
step:383/1770 train_time:35300ms step_avg:94.64ms
step:384/1770 train_time:35394ms step_avg:94.64ms
step:385/1770 train_time:35488ms step_avg:94.64ms
step:386/1770 train_time:35583ms step_avg:94.64ms
step:387/1770 train_time:35679ms step_avg:94.64ms
step:388/1770 train_time:35775ms step_avg:94.64ms
step:389/1770 train_time:35874ms step_avg:94.65ms
step:390/1770 train_time:35965ms step_avg:94.65ms
step:391/1770 train_time:36060ms step_avg:94.65ms
step:392/1770 train_time:36156ms step_avg:94.65ms
step:393/1770 train_time:36251ms step_avg:94.65ms
step:394/1770 train_time:36345ms step_avg:94.65ms
step:395/1770 train_time:36441ms step_avg:94.65ms
step:396/1770 train_time:36538ms step_avg:94.66ms
step:397/1770 train_time:36634ms step_avg:94.66ms
step:398/1770 train_time:36731ms step_avg:94.67ms
step:399/1770 train_time:36828ms step_avg:94.67ms
step:400/1770 train_time:36925ms step_avg:94.68ms
step:401/1770 train_time:37022ms step_avg:94.69ms
step:402/1770 train_time:37119ms step_avg:94.69ms
step:403/1770 train_time:37216ms step_avg:94.70ms
step:404/1770 train_time:37313ms step_avg:94.70ms
step:405/1770 train_time:37409ms step_avg:94.71ms
step:406/1770 train_time:37505ms step_avg:94.71ms
step:407/1770 train_time:37602ms step_avg:94.71ms
step:408/1770 train_time:37699ms step_avg:94.72ms
step:409/1770 train_time:37797ms step_avg:94.73ms
step:410/1770 train_time:37894ms step_avg:94.73ms
step:411/1770 train_time:37990ms step_avg:94.74ms
step:412/1770 train_time:38086ms step_avg:94.74ms
step:413/1770 train_time:38183ms step_avg:94.75ms
step:414/1770 train_time:38280ms step_avg:94.75ms
step:415/1770 train_time:38377ms step_avg:94.76ms
step:416/1770 train_time:38474ms step_avg:94.76ms
step:417/1770 train_time:38571ms step_avg:94.77ms
step:418/1770 train_time:38668ms step_avg:94.77ms
step:419/1770 train_time:38765ms step_avg:94.78ms
step:420/1770 train_time:38862ms step_avg:94.79ms
step:421/1770 train_time:38960ms step_avg:94.79ms
step:422/1770 train_time:39057ms step_avg:94.80ms
step:423/1770 train_time:39153ms step_avg:94.80ms
step:424/1770 train_time:39250ms step_avg:94.81ms
step:425/1770 train_time:39346ms step_avg:94.81ms
step:426/1770 train_time:39443ms step_avg:94.82ms
step:427/1770 train_time:39541ms step_avg:94.82ms
step:428/1770 train_time:39638ms step_avg:94.83ms
step:429/1770 train_time:39736ms step_avg:94.83ms
step:430/1770 train_time:39832ms step_avg:94.84ms
step:431/1770 train_time:39929ms step_avg:94.84ms
step:432/1770 train_time:40026ms step_avg:94.85ms
step:433/1770 train_time:40123ms step_avg:94.85ms
step:434/1770 train_time:40220ms step_avg:94.86ms
step:435/1770 train_time:40317ms step_avg:94.86ms
step:436/1770 train_time:40414ms step_avg:94.87ms
step:437/1770 train_time:40510ms step_avg:94.87ms
step:438/1770 train_time:40607ms step_avg:94.88ms
step:439/1770 train_time:40704ms step_avg:94.88ms
step:440/1770 train_time:40801ms step_avg:94.89ms
step:441/1770 train_time:40899ms step_avg:94.89ms
step:442/1770 train_time:40996ms step_avg:94.90ms
step:443/1770 train_time:41094ms step_avg:94.90ms
step:444/1770 train_time:41190ms step_avg:94.91ms
step:445/1770 train_time:41286ms step_avg:94.91ms
step:446/1770 train_time:41383ms step_avg:94.92ms
step:447/1770 train_time:41481ms step_avg:94.92ms
step:448/1770 train_time:41578ms step_avg:94.93ms
step:449/1770 train_time:41675ms step_avg:94.93ms
step:450/1770 train_time:41776ms step_avg:94.95ms
step:451/1770 train_time:41867ms step_avg:94.94ms
step:452/1770 train_time:41964ms step_avg:94.94ms
step:453/1770 train_time:42061ms step_avg:94.95ms
step:454/1770 train_time:42159ms step_avg:94.95ms
step:455/1770 train_time:42256ms step_avg:94.96ms
step:456/1770 train_time:42353ms step_avg:94.96ms
step:457/1770 train_time:42449ms step_avg:94.96ms
step:458/1770 train_time:42546ms step_avg:94.97ms
step:459/1770 train_time:42643ms step_avg:94.97ms
step:460/1770 train_time:42740ms step_avg:94.98ms
step:461/1770 train_time:42837ms step_avg:94.98ms
step:462/1770 train_time:42934ms step_avg:94.99ms
step:463/1770 train_time:43030ms step_avg:94.99ms
step:464/1770 train_time:43126ms step_avg:94.99ms
step:465/1770 train_time:43223ms step_avg:95.00ms
step:466/1770 train_time:43320ms step_avg:95.00ms
step:467/1770 train_time:43418ms step_avg:95.01ms
step:468/1770 train_time:43515ms step_avg:95.01ms
step:469/1770 train_time:43612ms step_avg:95.01ms
step:470/1770 train_time:43708ms step_avg:95.02ms
step:471/1770 train_time:43804ms step_avg:95.02ms
step:472/1770 train_time:43902ms step_avg:95.03ms
step:473/1770 train_time:43999ms step_avg:95.03ms
step:474/1770 train_time:44096ms step_avg:95.03ms
step:475/1770 train_time:44193ms step_avg:95.04ms
step:476/1770 train_time:44290ms step_avg:95.04ms
step:477/1770 train_time:44386ms step_avg:95.05ms
step:478/1770 train_time:44483ms step_avg:95.05ms
step:479/1770 train_time:44580ms step_avg:95.05ms
step:480/1770 train_time:44677ms step_avg:95.06ms
step:481/1770 train_time:44775ms step_avg:95.06ms
step:482/1770 train_time:44871ms step_avg:95.07ms
step:483/1770 train_time:44972ms step_avg:95.08ms
step:484/1770 train_time:45064ms step_avg:95.07ms
step:485/1770 train_time:45161ms step_avg:95.08ms
step:486/1770 train_time:45258ms step_avg:95.08ms
step:487/1770 train_time:45355ms step_avg:95.08ms
step:488/1770 train_time:45452ms step_avg:95.09ms
step:489/1770 train_time:45549ms step_avg:95.09ms
step:490/1770 train_time:45646ms step_avg:95.10ms
step:491/1770 train_time:45743ms step_avg:95.10ms
step:492/1770 train_time:45840ms step_avg:95.10ms
step:493/1770 train_time:45938ms step_avg:95.11ms
step:494/1770 train_time:46035ms step_avg:95.11ms
step:495/1770 train_time:46132ms step_avg:95.12ms
step:496/1770 train_time:46228ms step_avg:95.12ms
step:497/1770 train_time:46325ms step_avg:95.12ms
step:498/1770 train_time:46422ms step_avg:95.13ms
step:499/1770 train_time:46520ms step_avg:95.13ms
step:500/1770 train_time:46616ms step_avg:95.14ms
step:500/1770 val_loss:3.7627 train_time:46711ms step_avg:95.33ms
step:501/1770 train_time:46733ms step_avg:95.18ms
step:502/1770 train_time:46822ms step_avg:95.17ms
step:503/1770 train_time:46922ms step_avg:95.18ms
step:504/1770 train_time:47019ms step_avg:95.18ms
step:505/1770 train_time:47117ms step_avg:95.18ms
step:506/1770 train_time:47213ms step_avg:95.19ms
step:507/1770 train_time:47310ms step_avg:95.19ms
step:508/1770 train_time:47407ms step_avg:95.19ms
step:509/1770 train_time:47503ms step_avg:95.20ms
step:510/1770 train_time:47600ms step_avg:95.20ms
step:511/1770 train_time:47697ms step_avg:95.20ms
step:512/1770 train_time:47794ms step_avg:95.21ms
step:513/1770 train_time:47892ms step_avg:95.21ms
step:514/1770 train_time:47990ms step_avg:95.22ms
step:515/1770 train_time:48087ms step_avg:95.22ms
step:516/1770 train_time:48184ms step_avg:95.23ms
step:517/1770 train_time:48281ms step_avg:95.23ms
step:518/1770 train_time:48377ms step_avg:95.23ms
step:519/1770 train_time:48475ms step_avg:95.24ms
step:520/1770 train_time:48572ms step_avg:95.24ms
step:521/1770 train_time:48669ms step_avg:95.24ms
step:522/1770 train_time:48765ms step_avg:95.25ms
step:523/1770 train_time:48862ms step_avg:95.25ms
step:524/1770 train_time:48960ms step_avg:95.25ms
step:525/1770 train_time:49056ms step_avg:95.25ms
step:526/1770 train_time:49154ms step_avg:95.26ms
step:527/1770 train_time:49252ms step_avg:95.27ms
step:528/1770 train_time:49349ms step_avg:95.27ms
step:529/1770 train_time:49447ms step_avg:95.27ms
step:530/1770 train_time:49543ms step_avg:95.28ms
step:531/1770 train_time:49640ms step_avg:95.28ms
step:532/1770 train_time:49737ms step_avg:95.28ms
step:533/1770 train_time:49834ms step_avg:95.29ms
step:534/1770 train_time:49932ms step_avg:95.29ms
step:535/1770 train_time:50029ms step_avg:95.29ms
step:536/1770 train_time:50126ms step_avg:95.30ms
step:537/1770 train_time:50224ms step_avg:95.30ms
step:538/1770 train_time:50321ms step_avg:95.30ms
step:539/1770 train_time:50418ms step_avg:95.31ms
step:540/1770 train_time:50516ms step_avg:95.31ms
step:541/1770 train_time:50613ms step_avg:95.32ms
step:542/1770 train_time:50711ms step_avg:95.32ms
step:543/1770 train_time:50808ms step_avg:95.33ms
step:544/1770 train_time:50905ms step_avg:95.33ms
step:545/1770 train_time:51003ms step_avg:95.33ms
step:546/1770 train_time:51100ms step_avg:95.34ms
step:547/1770 train_time:51197ms step_avg:95.34ms
step:548/1770 train_time:51294ms step_avg:95.34ms
step:549/1770 train_time:51392ms step_avg:95.35ms
step:550/1770 train_time:51489ms step_avg:95.35ms
step:551/1770 train_time:51586ms step_avg:95.35ms
step:552/1770 train_time:51683ms step_avg:95.36ms
step:553/1770 train_time:51780ms step_avg:95.36ms
step:554/1770 train_time:51878ms step_avg:95.36ms
step:555/1770 train_time:51975ms step_avg:95.37ms
step:556/1770 train_time:52073ms step_avg:95.37ms
step:557/1770 train_time:52172ms step_avg:95.38ms
step:558/1770 train_time:52269ms step_avg:95.38ms
step:559/1770 train_time:52367ms step_avg:95.39ms
step:560/1770 train_time:52464ms step_avg:95.39ms
step:561/1770 train_time:52561ms step_avg:95.39ms
step:562/1770 train_time:52658ms step_avg:95.39ms
step:563/1770 train_time:52755ms step_avg:95.40ms
step:564/1770 train_time:52852ms step_avg:95.40ms
step:565/1770 train_time:52950ms step_avg:95.41ms
step:566/1770 train_time:53047ms step_avg:95.41ms
step:567/1770 train_time:53143ms step_avg:95.41ms
step:568/1770 train_time:53241ms step_avg:95.41ms
step:569/1770 train_time:53338ms step_avg:95.42ms
step:570/1770 train_time:53435ms step_avg:95.42ms
step:571/1770 train_time:53533ms step_avg:95.42ms
step:572/1770 train_time:53630ms step_avg:95.43ms
step:573/1770 train_time:53728ms step_avg:95.43ms
step:574/1770 train_time:53825ms step_avg:95.43ms
step:575/1770 train_time:53922ms step_avg:95.44ms
step:576/1770 train_time:54019ms step_avg:95.44ms
step:577/1770 train_time:54116ms step_avg:95.44ms
step:578/1770 train_time:54214ms step_avg:95.45ms
step:579/1770 train_time:54312ms step_avg:95.45ms
step:580/1770 train_time:54409ms step_avg:95.45ms
step:581/1770 train_time:54507ms step_avg:95.46ms
step:582/1770 train_time:54604ms step_avg:95.46ms
step:583/1770 train_time:54701ms step_avg:95.46ms
step:584/1770 train_time:54798ms step_avg:95.47ms
step:585/1770 train_time:54895ms step_avg:95.47ms
step:586/1770 train_time:54993ms step_avg:95.47ms
step:587/1770 train_time:55090ms step_avg:95.48ms
step:588/1770 train_time:55187ms step_avg:95.48ms
step:589/1770 train_time:55284ms step_avg:95.48ms
step:590/1770 train_time:55381ms step_avg:95.48ms
step:591/1770 train_time:55481ms step_avg:95.49ms
step:592/1770 train_time:55575ms step_avg:95.49ms
step:593/1770 train_time:55673ms step_avg:95.49ms
step:594/1770 train_time:55771ms step_avg:95.50ms
step:595/1770 train_time:55869ms step_avg:95.50ms
step:596/1770 train_time:55967ms step_avg:95.51ms
step:597/1770 train_time:56064ms step_avg:95.51ms
step:598/1770 train_time:56161ms step_avg:95.51ms
step:599/1770 train_time:56258ms step_avg:95.51ms
step:600/1770 train_time:56356ms step_avg:95.52ms
step:601/1770 train_time:56453ms step_avg:95.52ms
step:602/1770 train_time:56551ms step_avg:95.52ms
step:603/1770 train_time:56648ms step_avg:95.53ms
step:604/1770 train_time:56745ms step_avg:95.53ms
step:605/1770 train_time:56842ms step_avg:95.53ms
step:606/1770 train_time:56939ms step_avg:95.53ms
step:607/1770 train_time:57035ms step_avg:95.54ms
step:608/1770 train_time:57133ms step_avg:95.54ms
step:609/1770 train_time:57231ms step_avg:95.54ms
step:610/1770 train_time:57329ms step_avg:95.55ms
step:611/1770 train_time:57426ms step_avg:95.55ms
step:612/1770 train_time:57523ms step_avg:95.55ms
step:613/1770 train_time:57620ms step_avg:95.55ms
step:614/1770 train_time:57716ms step_avg:95.56ms
step:615/1770 train_time:57814ms step_avg:95.56ms
step:616/1770 train_time:57912ms step_avg:95.56ms
step:617/1770 train_time:58009ms step_avg:95.57ms
step:618/1770 train_time:58106ms step_avg:95.57ms
step:619/1770 train_time:58204ms step_avg:95.57ms
step:620/1770 train_time:58301ms step_avg:95.58ms
step:621/1770 train_time:58398ms step_avg:95.58ms
step:622/1770 train_time:58496ms step_avg:95.58ms
step:623/1770 train_time:58594ms step_avg:95.59ms
step:624/1770 train_time:58691ms step_avg:95.59ms
step:625/1770 train_time:58788ms step_avg:95.59ms
step:625/1770 val_loss:3.6734 train_time:58884ms step_avg:95.75ms
step:626/1770 train_time:58906ms step_avg:95.63ms
step:627/1770 train_time:58995ms step_avg:95.62ms
step:628/1770 train_time:59094ms step_avg:95.62ms
step:629/1770 train_time:59192ms step_avg:95.63ms
step:630/1770 train_time:59288ms step_avg:95.63ms
step:631/1770 train_time:59385ms step_avg:95.63ms
step:632/1770 train_time:59482ms step_avg:95.63ms
step:633/1770 train_time:59579ms step_avg:95.63ms
step:634/1770 train_time:59676ms step_avg:95.64ms
step:635/1770 train_time:59773ms step_avg:95.64ms
step:636/1770 train_time:59870ms step_avg:95.64ms
step:637/1770 train_time:59967ms step_avg:95.64ms
step:638/1770 train_time:60066ms step_avg:95.65ms
step:639/1770 train_time:60164ms step_avg:95.65ms
step:640/1770 train_time:60262ms step_avg:95.65ms
step:641/1770 train_time:60360ms step_avg:95.66ms
step:642/1770 train_time:60457ms step_avg:95.66ms
step:643/1770 train_time:60554ms step_avg:95.66ms
step:644/1770 train_time:60651ms step_avg:95.66ms
step:645/1770 train_time:60748ms step_avg:95.67ms
step:646/1770 train_time:60845ms step_avg:95.67ms
step:647/1770 train_time:60943ms step_avg:95.67ms
step:648/1770 train_time:61040ms step_avg:95.67ms
step:649/1770 train_time:61138ms step_avg:95.68ms
step:650/1770 train_time:61236ms step_avg:95.68ms
step:651/1770 train_time:61334ms step_avg:95.68ms
step:652/1770 train_time:61430ms step_avg:95.69ms
step:653/1770 train_time:61527ms step_avg:95.69ms
step:654/1770 train_time:61624ms step_avg:95.69ms
step:655/1770 train_time:61721ms step_avg:95.69ms
step:656/1770 train_time:61819ms step_avg:95.70ms
step:657/1770 train_time:61916ms step_avg:95.70ms
step:658/1770 train_time:62015ms step_avg:95.70ms
step:659/1770 train_time:62114ms step_avg:95.71ms
step:660/1770 train_time:62214ms step_avg:95.71ms
step:661/1770 train_time:62312ms step_avg:95.72ms
step:662/1770 train_time:62411ms step_avg:95.72ms
step:663/1770 train_time:62510ms step_avg:95.73ms
step:664/1770 train_time:62609ms step_avg:95.73ms
step:665/1770 train_time:62708ms step_avg:95.74ms
step:666/1770 train_time:62807ms step_avg:95.74ms
step:667/1770 train_time:62906ms step_avg:95.75ms
step:668/1770 train_time:63005ms step_avg:95.75ms
step:669/1770 train_time:63103ms step_avg:95.76ms
step:670/1770 train_time:63203ms step_avg:95.76ms
step:671/1770 train_time:63303ms step_avg:95.77ms
step:672/1770 train_time:63402ms step_avg:95.77ms
step:673/1770 train_time:63502ms step_avg:95.78ms
step:674/1770 train_time:63602ms step_avg:95.79ms
step:675/1770 train_time:63702ms step_avg:95.79ms
step:676/1770 train_time:63802ms step_avg:95.80ms
step:677/1770 train_time:63902ms step_avg:95.81ms
step:678/1770 train_time:64002ms step_avg:95.81ms
step:679/1770 train_time:64102ms step_avg:95.82ms
step:680/1770 train_time:64201ms step_avg:95.82ms
step:681/1770 train_time:64300ms step_avg:95.83ms
step:682/1770 train_time:64400ms step_avg:95.83ms
step:683/1770 train_time:64500ms step_avg:95.84ms
step:684/1770 train_time:64600ms step_avg:95.85ms
step:685/1770 train_time:64699ms step_avg:95.85ms
step:686/1770 train_time:64799ms step_avg:95.86ms
step:687/1770 train_time:64898ms step_avg:95.86ms
step:688/1770 train_time:64998ms step_avg:95.87ms
step:689/1770 train_time:65097ms step_avg:95.87ms
step:690/1770 train_time:65196ms step_avg:95.88ms
step:691/1770 train_time:65295ms step_avg:95.88ms
step:692/1770 train_time:65393ms step_avg:95.88ms
step:693/1770 train_time:65492ms step_avg:95.89ms
step:694/1770 train_time:65592ms step_avg:95.89ms
step:695/1770 train_time:65690ms step_avg:95.90ms
step:696/1770 train_time:65789ms step_avg:95.90ms
step:697/1770 train_time:65887ms step_avg:95.91ms
step:698/1770 train_time:65986ms step_avg:95.91ms
step:699/1770 train_time:66085ms step_avg:95.91ms
step:700/1770 train_time:66184ms step_avg:95.92ms
step:701/1770 train_time:66283ms step_avg:95.92ms
step:702/1770 train_time:66383ms step_avg:95.93ms
step:703/1770 train_time:66482ms step_avg:95.93ms
step:704/1770 train_time:66582ms step_avg:95.94ms
step:705/1770 train_time:66682ms step_avg:95.95ms
step:706/1770 train_time:66782ms step_avg:95.95ms
step:707/1770 train_time:66882ms step_avg:95.96ms
step:708/1770 train_time:66981ms step_avg:95.96ms
step:709/1770 train_time:67081ms step_avg:95.97ms
step:710/1770 train_time:67180ms step_avg:95.97ms
step:711/1770 train_time:67279ms step_avg:95.98ms
step:712/1770 train_time:67379ms step_avg:95.98ms
step:713/1770 train_time:67479ms step_avg:95.99ms
step:714/1770 train_time:67578ms step_avg:95.99ms
step:715/1770 train_time:67678ms step_avg:96.00ms
step:716/1770 train_time:67778ms step_avg:96.00ms
step:717/1770 train_time:67878ms step_avg:96.01ms
step:718/1770 train_time:67977ms step_avg:96.01ms
step:719/1770 train_time:68076ms step_avg:96.02ms
step:720/1770 train_time:68175ms step_avg:96.02ms
step:721/1770 train_time:68275ms step_avg:96.03ms
step:722/1770 train_time:68375ms step_avg:96.03ms
step:723/1770 train_time:68474ms step_avg:96.04ms
step:724/1770 train_time:68572ms step_avg:96.04ms
step:725/1770 train_time:68671ms step_avg:96.04ms
step:726/1770 train_time:68770ms step_avg:96.05ms
step:727/1770 train_time:68869ms step_avg:96.05ms
step:728/1770 train_time:68967ms step_avg:96.05ms
step:729/1770 train_time:69066ms step_avg:96.06ms
step:730/1770 train_time:69164ms step_avg:96.06ms
step:731/1770 train_time:69263ms step_avg:96.07ms
step:732/1770 train_time:69363ms step_avg:96.07ms
step:733/1770 train_time:69463ms step_avg:96.08ms
step:734/1770 train_time:69563ms step_avg:96.08ms
step:735/1770 train_time:69667ms step_avg:96.09ms
step:736/1770 train_time:69764ms step_avg:96.09ms
step:737/1770 train_time:69863ms step_avg:96.10ms
step:738/1770 train_time:69963ms step_avg:96.10ms
step:739/1770 train_time:70063ms step_avg:96.11ms
step:740/1770 train_time:70162ms step_avg:96.11ms
step:741/1770 train_time:70261ms step_avg:96.12ms
step:742/1770 train_time:70360ms step_avg:96.12ms
step:743/1770 train_time:70460ms step_avg:96.13ms
step:744/1770 train_time:70560ms step_avg:96.13ms
step:745/1770 train_time:70660ms step_avg:96.14ms
step:746/1770 train_time:70761ms step_avg:96.14ms
step:747/1770 train_time:70860ms step_avg:96.15ms
step:748/1770 train_time:70959ms step_avg:96.15ms
step:749/1770 train_time:71059ms step_avg:96.16ms
step:750/1770 train_time:71158ms step_avg:96.16ms
step:750/1770 val_loss:3.6081 train_time:71255ms step_avg:96.29ms
step:751/1770 train_time:71277ms step_avg:96.19ms
step:752/1770 train_time:71367ms step_avg:96.18ms
step:753/1770 train_time:71470ms step_avg:96.19ms
step:754/1770 train_time:71569ms step_avg:96.19ms
step:755/1770 train_time:71666ms step_avg:96.20ms
step:756/1770 train_time:71764ms step_avg:96.20ms
step:757/1770 train_time:71863ms step_avg:96.20ms
step:758/1770 train_time:71962ms step_avg:96.21ms
step:759/1770 train_time:72060ms step_avg:96.21ms
step:760/1770 train_time:72159ms step_avg:96.21ms
step:761/1770 train_time:72258ms step_avg:96.22ms
step:762/1770 train_time:72357ms step_avg:96.22ms
step:763/1770 train_time:72458ms step_avg:96.23ms
step:764/1770 train_time:72558ms step_avg:96.23ms
step:765/1770 train_time:72659ms step_avg:96.24ms
step:766/1770 train_time:72759ms step_avg:96.24ms
step:767/1770 train_time:72859ms step_avg:96.25ms
step:768/1770 train_time:72959ms step_avg:96.25ms
step:769/1770 train_time:73058ms step_avg:96.26ms
step:770/1770 train_time:73157ms step_avg:96.26ms
step:771/1770 train_time:73255ms step_avg:96.26ms
step:772/1770 train_time:73354ms step_avg:96.27ms
step:773/1770 train_time:73453ms step_avg:96.27ms
step:774/1770 train_time:73553ms step_avg:96.27ms
step:775/1770 train_time:73652ms step_avg:96.28ms
step:776/1770 train_time:73751ms step_avg:96.28ms
step:777/1770 train_time:73851ms step_avg:96.29ms
step:778/1770 train_time:73950ms step_avg:96.29ms
step:779/1770 train_time:74050ms step_avg:96.29ms
step:780/1770 train_time:74148ms step_avg:96.30ms
step:781/1770 train_time:74247ms step_avg:96.30ms
step:782/1770 train_time:74345ms step_avg:96.30ms
step:783/1770 train_time:74444ms step_avg:96.30ms
step:784/1770 train_time:74542ms step_avg:96.31ms
step:785/1770 train_time:74641ms step_avg:96.31ms
step:786/1770 train_time:74740ms step_avg:96.31ms
step:787/1770 train_time:74839ms step_avg:96.32ms
step:788/1770 train_time:74939ms step_avg:96.32ms
step:789/1770 train_time:75042ms step_avg:96.33ms
step:790/1770 train_time:75137ms step_avg:96.33ms
step:791/1770 train_time:75237ms step_avg:96.33ms
step:792/1770 train_time:75336ms step_avg:96.34ms
step:793/1770 train_time:75436ms step_avg:96.34ms
step:794/1770 train_time:75535ms step_avg:96.35ms
step:795/1770 train_time:75636ms step_avg:96.35ms
step:796/1770 train_time:75736ms step_avg:96.36ms
step:797/1770 train_time:75836ms step_avg:96.36ms
step:798/1770 train_time:75936ms step_avg:96.37ms
step:799/1770 train_time:76036ms step_avg:96.37ms
step:800/1770 train_time:76136ms step_avg:96.37ms
step:801/1770 train_time:76236ms step_avg:96.38ms
step:802/1770 train_time:76335ms step_avg:96.38ms
step:803/1770 train_time:76434ms step_avg:96.39ms
step:804/1770 train_time:76533ms step_avg:96.39ms
step:805/1770 train_time:76632ms step_avg:96.39ms
step:806/1770 train_time:76732ms step_avg:96.40ms
step:807/1770 train_time:76832ms step_avg:96.40ms
step:808/1770 train_time:76936ms step_avg:96.41ms
step:809/1770 train_time:77031ms step_avg:96.41ms
step:810/1770 train_time:77131ms step_avg:96.41ms
step:811/1770 train_time:77230ms step_avg:96.42ms
step:812/1770 train_time:77329ms step_avg:96.42ms
step:813/1770 train_time:77429ms step_avg:96.42ms
step:814/1770 train_time:77528ms step_avg:96.43ms
step:815/1770 train_time:77626ms step_avg:96.43ms
step:816/1770 train_time:77725ms step_avg:96.43ms
step:817/1770 train_time:77823ms step_avg:96.44ms
step:818/1770 train_time:77922ms step_avg:96.44ms
step:819/1770 train_time:78021ms step_avg:96.44ms
step:820/1770 train_time:78121ms step_avg:96.45ms
step:821/1770 train_time:78220ms step_avg:96.45ms
step:822/1770 train_time:78319ms step_avg:96.45ms
step:823/1770 train_time:78419ms step_avg:96.46ms
step:824/1770 train_time:78520ms step_avg:96.46ms
step:825/1770 train_time:78620ms step_avg:96.47ms
step:826/1770 train_time:78720ms step_avg:96.47ms
step:827/1770 train_time:78820ms step_avg:96.47ms
step:828/1770 train_time:78920ms step_avg:96.48ms
step:829/1770 train_time:79020ms step_avg:96.48ms
step:830/1770 train_time:79120ms step_avg:96.49ms
step:831/1770 train_time:79220ms step_avg:96.49ms
step:832/1770 train_time:79319ms step_avg:96.50ms
step:833/1770 train_time:79419ms step_avg:96.50ms
step:834/1770 train_time:79519ms step_avg:96.50ms
step:835/1770 train_time:79619ms step_avg:96.51ms
step:836/1770 train_time:79719ms step_avg:96.51ms
step:837/1770 train_time:79819ms step_avg:96.52ms
step:838/1770 train_time:79919ms step_avg:96.52ms
step:839/1770 train_time:80019ms step_avg:96.52ms
step:840/1770 train_time:80119ms step_avg:96.53ms
step:841/1770 train_time:80219ms step_avg:96.53ms
step:842/1770 train_time:80318ms step_avg:96.54ms
step:843/1770 train_time:80418ms step_avg:96.54ms
step:844/1770 train_time:80517ms step_avg:96.54ms
step:845/1770 train_time:80617ms step_avg:96.55ms
step:846/1770 train_time:80717ms step_avg:96.55ms
step:847/1770 train_time:80817ms step_avg:96.56ms
step:848/1770 train_time:80916ms step_avg:96.56ms
step:849/1770 train_time:81016ms step_avg:96.56ms
step:850/1770 train_time:81116ms step_avg:96.57ms
step:851/1770 train_time:81215ms step_avg:96.57ms
step:852/1770 train_time:81315ms step_avg:96.57ms
step:853/1770 train_time:81414ms step_avg:96.58ms
step:854/1770 train_time:81514ms step_avg:96.58ms
step:855/1770 train_time:81613ms step_avg:96.58ms
step:856/1770 train_time:81713ms step_avg:96.59ms
step:857/1770 train_time:81813ms step_avg:96.59ms
step:858/1770 train_time:81912ms step_avg:96.59ms
step:859/1770 train_time:82012ms step_avg:96.60ms
step:860/1770 train_time:82111ms step_avg:96.60ms
step:861/1770 train_time:82210ms step_avg:96.60ms
step:862/1770 train_time:82309ms step_avg:96.61ms
step:863/1770 train_time:82408ms step_avg:96.61ms
step:864/1770 train_time:82507ms step_avg:96.61ms
step:865/1770 train_time:82606ms step_avg:96.62ms
step:866/1770 train_time:82705ms step_avg:96.62ms
step:867/1770 train_time:82804ms step_avg:96.62ms
step:868/1770 train_time:82903ms step_avg:96.62ms
step:869/1770 train_time:83002ms step_avg:96.63ms
step:870/1770 train_time:83101ms step_avg:96.63ms
step:871/1770 train_time:83200ms step_avg:96.63ms
step:872/1770 train_time:83300ms step_avg:96.64ms
step:873/1770 train_time:83400ms step_avg:96.64ms
step:874/1770 train_time:83500ms step_avg:96.64ms
step:875/1770 train_time:83599ms step_avg:96.65ms
step:875/1770 val_loss:3.5589 train_time:83698ms step_avg:96.76ms
step:876/1770 train_time:83719ms step_avg:96.67ms
step:877/1770 train_time:83812ms step_avg:96.67ms
step:878/1770 train_time:83913ms step_avg:96.67ms
step:879/1770 train_time:84012ms step_avg:96.68ms
step:880/1770 train_time:84111ms step_avg:96.68ms
step:881/1770 train_time:84210ms step_avg:96.68ms
step:882/1770 train_time:84309ms step_avg:96.69ms
step:883/1770 train_time:84408ms step_avg:96.69ms
step:884/1770 train_time:84508ms step_avg:96.69ms
step:885/1770 train_time:84607ms step_avg:96.69ms
step:886/1770 train_time:84706ms step_avg:96.70ms
step:887/1770 train_time:84807ms step_avg:96.70ms
step:888/1770 train_time:84908ms step_avg:96.71ms
step:889/1770 train_time:85008ms step_avg:96.71ms
step:890/1770 train_time:85108ms step_avg:96.71ms
step:891/1770 train_time:85208ms step_avg:96.72ms
step:892/1770 train_time:85307ms step_avg:96.72ms
step:893/1770 train_time:85407ms step_avg:96.72ms
step:894/1770 train_time:85506ms step_avg:96.73ms
step:895/1770 train_time:85605ms step_avg:96.73ms
step:896/1770 train_time:85705ms step_avg:96.73ms
step:897/1770 train_time:85805ms step_avg:96.74ms
step:898/1770 train_time:85906ms step_avg:96.74ms
step:899/1770 train_time:86006ms step_avg:96.74ms
step:900/1770 train_time:86106ms step_avg:96.75ms
step:901/1770 train_time:86206ms step_avg:96.75ms
step:902/1770 train_time:86306ms step_avg:96.76ms
step:903/1770 train_time:86405ms step_avg:96.76ms
step:904/1770 train_time:86505ms step_avg:96.76ms
step:905/1770 train_time:86604ms step_avg:96.76ms
step:906/1770 train_time:86704ms step_avg:96.77ms
step:907/1770 train_time:86804ms step_avg:96.77ms
step:908/1770 train_time:86905ms step_avg:96.78ms
step:909/1770 train_time:87010ms step_avg:96.79ms
step:910/1770 train_time:87106ms step_avg:96.78ms
step:911/1770 train_time:87206ms step_avg:96.79ms
step:912/1770 train_time:87305ms step_avg:96.79ms
step:913/1770 train_time:87406ms step_avg:96.80ms
step:914/1770 train_time:87506ms step_avg:96.80ms
step:915/1770 train_time:87606ms step_avg:96.80ms
step:916/1770 train_time:87706ms step_avg:96.81ms
step:917/1770 train_time:87805ms step_avg:96.81ms
step:918/1770 train_time:87905ms step_avg:96.81ms
step:919/1770 train_time:88006ms step_avg:96.82ms
step:920/1770 train_time:88108ms step_avg:96.82ms
step:921/1770 train_time:88210ms step_avg:96.83ms
step:922/1770 train_time:88310ms step_avg:96.83ms
step:923/1770 train_time:88410ms step_avg:96.83ms
step:924/1770 train_time:88511ms step_avg:96.84ms
step:925/1770 train_time:88611ms step_avg:96.84ms
step:926/1770 train_time:88712ms step_avg:96.85ms
step:927/1770 train_time:88812ms step_avg:96.85ms
step:928/1770 train_time:88913ms step_avg:96.85ms
step:929/1770 train_time:89013ms step_avg:96.86ms
step:930/1770 train_time:89114ms step_avg:96.86ms
step:931/1770 train_time:89214ms step_avg:96.87ms
step:932/1770 train_time:89314ms step_avg:96.87ms
step:933/1770 train_time:89414ms step_avg:96.87ms
step:934/1770 train_time:89514ms step_avg:96.88ms
step:935/1770 train_time:89614ms step_avg:96.88ms
step:936/1770 train_time:89714ms step_avg:96.88ms
step:937/1770 train_time:89813ms step_avg:96.89ms
step:938/1770 train_time:89914ms step_avg:96.89ms
step:939/1770 train_time:90014ms step_avg:96.89ms
step:940/1770 train_time:90115ms step_avg:96.90ms
step:941/1770 train_time:90216ms step_avg:96.90ms
step:942/1770 train_time:90318ms step_avg:96.91ms
step:943/1770 train_time:90417ms step_avg:96.91ms
step:944/1770 train_time:90517ms step_avg:96.91ms
step:945/1770 train_time:90617ms step_avg:96.92ms
step:946/1770 train_time:90718ms step_avg:96.92ms
step:947/1770 train_time:90818ms step_avg:96.92ms
step:948/1770 train_time:90919ms step_avg:96.93ms
step:949/1770 train_time:91020ms step_avg:96.93ms
step:950/1770 train_time:91121ms step_avg:96.94ms
step:951/1770 train_time:91222ms step_avg:96.94ms
step:952/1770 train_time:91323ms step_avg:96.95ms
step:953/1770 train_time:91424ms step_avg:96.95ms
step:954/1770 train_time:91524ms step_avg:96.95ms
step:955/1770 train_time:91625ms step_avg:96.96ms
step:956/1770 train_time:91727ms step_avg:96.96ms
step:957/1770 train_time:91828ms step_avg:96.97ms
step:958/1770 train_time:91929ms step_avg:96.97ms
step:959/1770 train_time:92032ms step_avg:96.98ms
step:960/1770 train_time:92132ms step_avg:96.98ms
step:961/1770 train_time:92233ms step_avg:96.99ms
step:962/1770 train_time:92334ms step_avg:96.99ms
step:963/1770 train_time:92434ms step_avg:96.99ms
step:964/1770 train_time:92534ms step_avg:97.00ms
step:965/1770 train_time:92634ms step_avg:97.00ms
step:966/1770 train_time:92734ms step_avg:97.00ms
step:967/1770 train_time:92835ms step_avg:97.01ms
step:968/1770 train_time:92936ms step_avg:97.01ms
step:969/1770 train_time:93036ms step_avg:97.01ms
step:970/1770 train_time:93136ms step_avg:97.02ms
step:971/1770 train_time:93236ms step_avg:97.02ms
step:972/1770 train_time:93336ms step_avg:97.02ms
step:973/1770 train_time:93436ms step_avg:97.03ms
step:974/1770 train_time:93536ms step_avg:97.03ms
step:975/1770 train_time:93637ms step_avg:97.03ms
step:976/1770 train_time:93736ms step_avg:97.04ms
step:977/1770 train_time:93836ms step_avg:97.04ms
step:978/1770 train_time:93937ms step_avg:97.04ms
step:979/1770 train_time:94037ms step_avg:97.05ms
step:980/1770 train_time:94138ms step_avg:97.05ms
step:981/1770 train_time:94238ms step_avg:97.05ms
step:982/1770 train_time:94338ms step_avg:97.06ms
step:983/1770 train_time:94439ms step_avg:97.06ms
step:984/1770 train_time:94540ms step_avg:97.06ms
step:985/1770 train_time:94641ms step_avg:97.07ms
step:986/1770 train_time:94742ms step_avg:97.07ms
step:987/1770 train_time:94842ms step_avg:97.07ms
step:988/1770 train_time:94942ms step_avg:97.08ms
step:989/1770 train_time:95044ms step_avg:97.08ms
step:990/1770 train_time:95145ms step_avg:97.09ms
step:991/1770 train_time:95247ms step_avg:97.09ms
step:992/1770 train_time:95350ms step_avg:97.10ms
step:993/1770 train_time:95450ms step_avg:97.10ms
step:994/1770 train_time:95551ms step_avg:97.10ms
step:995/1770 train_time:95652ms step_avg:97.11ms
step:996/1770 train_time:95752ms step_avg:97.11ms
step:997/1770 train_time:95853ms step_avg:97.12ms
step:998/1770 train_time:95953ms step_avg:97.12ms
step:999/1770 train_time:96054ms step_avg:97.12ms
step:1000/1770 train_time:96156ms step_avg:97.13ms
step:1000/1770 val_loss:3.5180 train_time:96255ms step_avg:97.23ms
step:1001/1770 train_time:96276ms step_avg:97.15ms
step:1002/1770 train_time:96370ms step_avg:97.15ms
step:1003/1770 train_time:96473ms step_avg:97.15ms
step:1004/1770 train_time:96574ms step_avg:97.16ms
step:1005/1770 train_time:96675ms step_avg:97.16ms
step:1006/1770 train_time:96774ms step_avg:97.16ms
step:1007/1770 train_time:96874ms step_avg:97.17ms
step:1008/1770 train_time:96974ms step_avg:97.17ms
step:1009/1770 train_time:97077ms step_avg:97.17ms
step:1010/1770 train_time:97174ms step_avg:97.17ms
step:1011/1770 train_time:97275ms step_avg:97.18ms
step:1012/1770 train_time:97377ms step_avg:97.18ms
step:1013/1770 train_time:97478ms step_avg:97.19ms
step:1014/1770 train_time:97578ms step_avg:97.19ms
step:1015/1770 train_time:97678ms step_avg:97.19ms
step:1016/1770 train_time:97777ms step_avg:97.19ms
step:1017/1770 train_time:97877ms step_avg:97.20ms
step:1018/1770 train_time:97976ms step_avg:97.20ms
step:1019/1770 train_time:98077ms step_avg:97.20ms
step:1020/1770 train_time:98177ms step_avg:97.20ms
step:1021/1770 train_time:98277ms step_avg:97.21ms
step:1022/1770 train_time:98377ms step_avg:97.21ms
step:1023/1770 train_time:98477ms step_avg:97.21ms
step:1024/1770 train_time:98577ms step_avg:97.22ms
step:1025/1770 train_time:98677ms step_avg:97.22ms
step:1026/1770 train_time:98778ms step_avg:97.22ms
step:1027/1770 train_time:98879ms step_avg:97.23ms
step:1028/1770 train_time:98978ms step_avg:97.23ms
step:1029/1770 train_time:99078ms step_avg:97.23ms
step:1030/1770 train_time:99178ms step_avg:97.23ms
step:1031/1770 train_time:99278ms step_avg:97.24ms
step:1032/1770 train_time:99378ms step_avg:97.24ms
step:1033/1770 train_time:99479ms step_avg:97.24ms
step:1034/1770 train_time:99578ms step_avg:97.24ms
step:1035/1770 train_time:99679ms step_avg:97.25ms
step:1036/1770 train_time:99778ms step_avg:97.25ms
step:1037/1770 train_time:99878ms step_avg:97.25ms
step:1038/1770 train_time:99978ms step_avg:97.26ms
step:1039/1770 train_time:100078ms step_avg:97.26ms
step:1040/1770 train_time:100178ms step_avg:97.26ms
step:1041/1770 train_time:100279ms step_avg:97.26ms
step:1042/1770 train_time:100379ms step_avg:97.27ms
step:1043/1770 train_time:100480ms step_avg:97.27ms
step:1044/1770 train_time:100580ms step_avg:97.27ms
step:1045/1770 train_time:100680ms step_avg:97.28ms
step:1046/1770 train_time:100780ms step_avg:97.28ms
step:1047/1770 train_time:100880ms step_avg:97.28ms
step:1048/1770 train_time:100980ms step_avg:97.28ms
step:1049/1770 train_time:101080ms step_avg:97.29ms
step:1050/1770 train_time:101180ms step_avg:97.29ms
step:1051/1770 train_time:101281ms step_avg:97.29ms
step:1052/1770 train_time:101382ms step_avg:97.30ms
step:1053/1770 train_time:101482ms step_avg:97.30ms
step:1054/1770 train_time:101583ms step_avg:97.30ms
step:1055/1770 train_time:101684ms step_avg:97.31ms
step:1056/1770 train_time:101785ms step_avg:97.31ms
step:1057/1770 train_time:101886ms step_avg:97.31ms
step:1058/1770 train_time:101987ms step_avg:97.32ms
step:1059/1770 train_time:102089ms step_avg:97.32ms
step:1060/1770 train_time:102193ms step_avg:97.33ms
step:1061/1770 train_time:102294ms step_avg:97.33ms
step:1062/1770 train_time:102396ms step_avg:97.33ms
step:1063/1770 train_time:102498ms step_avg:97.34ms
step:1064/1770 train_time:102599ms step_avg:97.34ms
step:1065/1770 train_time:102698ms step_avg:97.34ms
step:1066/1770 train_time:102798ms step_avg:97.35ms
step:1067/1770 train_time:102899ms step_avg:97.35ms
step:1068/1770 train_time:103000ms step_avg:97.35ms
step:1069/1770 train_time:103101ms step_avg:97.36ms
step:1070/1770 train_time:103203ms step_avg:97.36ms
step:1071/1770 train_time:103305ms step_avg:97.37ms
step:1072/1770 train_time:103406ms step_avg:97.37ms
step:1073/1770 train_time:103508ms step_avg:97.37ms
step:1074/1770 train_time:103609ms step_avg:97.38ms
step:1075/1770 train_time:103710ms step_avg:97.38ms
step:1076/1770 train_time:103813ms step_avg:97.39ms
step:1077/1770 train_time:103914ms step_avg:97.39ms
step:1078/1770 train_time:104015ms step_avg:97.39ms
step:1079/1770 train_time:104115ms step_avg:97.39ms
step:1080/1770 train_time:104216ms step_avg:97.40ms
step:1081/1770 train_time:104317ms step_avg:97.40ms
step:1082/1770 train_time:104418ms step_avg:97.40ms
step:1083/1770 train_time:104518ms step_avg:97.41ms
step:1084/1770 train_time:104619ms step_avg:97.41ms
step:1085/1770 train_time:104719ms step_avg:97.41ms
step:1086/1770 train_time:104819ms step_avg:97.42ms
step:1087/1770 train_time:104919ms step_avg:97.42ms
step:1088/1770 train_time:105019ms step_avg:97.42ms
step:1089/1770 train_time:105120ms step_avg:97.42ms
step:1090/1770 train_time:105221ms step_avg:97.43ms
step:1091/1770 train_time:105321ms step_avg:97.43ms
step:1092/1770 train_time:105422ms step_avg:97.43ms
step:1093/1770 train_time:105523ms step_avg:97.44ms
step:1094/1770 train_time:105625ms step_avg:97.44ms
step:1095/1770 train_time:105725ms step_avg:97.44ms
step:1096/1770 train_time:105826ms step_avg:97.45ms
step:1097/1770 train_time:105927ms step_avg:97.45ms
step:1098/1770 train_time:106028ms step_avg:97.45ms
step:1099/1770 train_time:106130ms step_avg:97.46ms
step:1100/1770 train_time:106232ms step_avg:97.46ms
step:1101/1770 train_time:106334ms step_avg:97.46ms
step:1102/1770 train_time:106434ms step_avg:97.47ms
step:1103/1770 train_time:106535ms step_avg:97.47ms
step:1104/1770 train_time:106636ms step_avg:97.47ms
step:1105/1770 train_time:106737ms step_avg:97.48ms
step:1106/1770 train_time:106838ms step_avg:97.48ms
step:1107/1770 train_time:106938ms step_avg:97.48ms
step:1108/1770 train_time:107039ms step_avg:97.49ms
step:1109/1770 train_time:107139ms step_avg:97.49ms
step:1110/1770 train_time:107240ms step_avg:97.49ms
step:1111/1770 train_time:107341ms step_avg:97.49ms
step:1112/1770 train_time:107442ms step_avg:97.50ms
step:1113/1770 train_time:107543ms step_avg:97.50ms
step:1114/1770 train_time:107644ms step_avg:97.50ms
step:1115/1770 train_time:107745ms step_avg:97.51ms
step:1116/1770 train_time:107846ms step_avg:97.51ms
step:1117/1770 train_time:107948ms step_avg:97.51ms
step:1118/1770 train_time:108049ms step_avg:97.52ms
step:1119/1770 train_time:108150ms step_avg:97.52ms
step:1120/1770 train_time:108252ms step_avg:97.52ms
step:1121/1770 train_time:108353ms step_avg:97.53ms
step:1122/1770 train_time:108454ms step_avg:97.53ms
step:1123/1770 train_time:108555ms step_avg:97.53ms
step:1124/1770 train_time:108656ms step_avg:97.54ms
step:1125/1770 train_time:108757ms step_avg:97.54ms
step:1125/1770 val_loss:3.4763 train_time:108856ms step_avg:97.63ms
step:1126/1770 train_time:108877ms step_avg:97.56ms
step:1127/1770 train_time:108970ms step_avg:97.56ms
step:1128/1770 train_time:109073ms step_avg:97.56ms
step:1129/1770 train_time:109173ms step_avg:97.56ms
step:1130/1770 train_time:109274ms step_avg:97.57ms
step:1131/1770 train_time:109375ms step_avg:97.57ms
step:1132/1770 train_time:109475ms step_avg:97.57ms
step:1133/1770 train_time:109575ms step_avg:97.57ms
step:1134/1770 train_time:109676ms step_avg:97.58ms
step:1135/1770 train_time:109776ms step_avg:97.58ms
step:1136/1770 train_time:109876ms step_avg:97.58ms
step:1137/1770 train_time:109978ms step_avg:97.58ms
step:1138/1770 train_time:110078ms step_avg:97.59ms
step:1139/1770 train_time:110178ms step_avg:97.59ms
step:1140/1770 train_time:110279ms step_avg:97.59ms
step:1141/1770 train_time:110379ms step_avg:97.59ms
step:1142/1770 train_time:110479ms step_avg:97.60ms
step:1143/1770 train_time:110580ms step_avg:97.60ms
step:1144/1770 train_time:110681ms step_avg:97.60ms
step:1145/1770 train_time:110781ms step_avg:97.60ms
step:1146/1770 train_time:110882ms step_avg:97.61ms
step:1147/1770 train_time:110984ms step_avg:97.61ms
step:1148/1770 train_time:111085ms step_avg:97.61ms
step:1149/1770 train_time:111186ms step_avg:97.62ms
step:1150/1770 train_time:111287ms step_avg:97.62ms
step:1151/1770 train_time:111389ms step_avg:97.62ms
step:1152/1770 train_time:111492ms step_avg:97.63ms
step:1153/1770 train_time:111593ms step_avg:97.63ms
step:1154/1770 train_time:111694ms step_avg:97.63ms
step:1155/1770 train_time:111795ms step_avg:97.64ms
step:1156/1770 train_time:111895ms step_avg:97.64ms
step:1157/1770 train_time:111997ms step_avg:97.64ms
step:1158/1770 train_time:112098ms step_avg:97.65ms
step:1159/1770 train_time:112197ms step_avg:97.65ms
step:1160/1770 train_time:112297ms step_avg:97.65ms
step:1161/1770 train_time:112397ms step_avg:97.65ms
step:1162/1770 train_time:112499ms step_avg:97.66ms
step:1163/1770 train_time:112600ms step_avg:97.66ms
step:1164/1770 train_time:112700ms step_avg:97.66ms
step:1165/1770 train_time:112801ms step_avg:97.66ms
step:1166/1770 train_time:112903ms step_avg:97.67ms
step:1167/1770 train_time:113004ms step_avg:97.67ms
step:1168/1770 train_time:113105ms step_avg:97.67ms
step:1169/1770 train_time:113207ms step_avg:97.68ms
step:1170/1770 train_time:113308ms step_avg:97.68ms
step:1171/1770 train_time:113410ms step_avg:97.68ms
step:1172/1770 train_time:113512ms step_avg:97.69ms
step:1173/1770 train_time:113613ms step_avg:97.69ms
step:1174/1770 train_time:113714ms step_avg:97.69ms
step:1175/1770 train_time:113816ms step_avg:97.70ms
step:1176/1770 train_time:113917ms step_avg:97.70ms
step:1177/1770 train_time:114016ms step_avg:97.70ms
step:1178/1770 train_time:114116ms step_avg:97.70ms
step:1179/1770 train_time:114217ms step_avg:97.70ms
step:1180/1770 train_time:114317ms step_avg:97.71ms
step:1181/1770 train_time:114418ms step_avg:97.71ms
step:1182/1770 train_time:114519ms step_avg:97.71ms
step:1183/1770 train_time:114621ms step_avg:97.72ms
step:1184/1770 train_time:114725ms step_avg:97.72ms
step:1185/1770 train_time:114826ms step_avg:97.72ms
step:1186/1770 train_time:114929ms step_avg:97.73ms
step:1187/1770 train_time:115038ms step_avg:97.74ms
step:1188/1770 train_time:115136ms step_avg:97.74ms
step:1189/1770 train_time:115236ms step_avg:97.74ms
step:1190/1770 train_time:115337ms step_avg:97.74ms
step:1191/1770 train_time:115439ms step_avg:97.75ms
step:1192/1770 train_time:115541ms step_avg:97.75ms
step:1193/1770 train_time:115643ms step_avg:97.75ms
step:1194/1770 train_time:115745ms step_avg:97.76ms
step:1195/1770 train_time:115848ms step_avg:97.76ms
step:1196/1770 train_time:115952ms step_avg:97.77ms
step:1197/1770 train_time:116055ms step_avg:97.77ms
step:1198/1770 train_time:116157ms step_avg:97.77ms
step:1199/1770 train_time:116259ms step_avg:97.78ms
step:1200/1770 train_time:116362ms step_avg:97.78ms
step:1201/1770 train_time:116464ms step_avg:97.79ms
step:1202/1770 train_time:116565ms step_avg:97.79ms
step:1203/1770 train_time:116667ms step_avg:97.79ms
step:1204/1770 train_time:116769ms step_avg:97.80ms
step:1205/1770 train_time:116871ms step_avg:97.80ms
step:1206/1770 train_time:116974ms step_avg:97.80ms
step:1207/1770 train_time:117076ms step_avg:97.81ms
step:1208/1770 train_time:117177ms step_avg:97.81ms
step:1209/1770 train_time:117279ms step_avg:97.81ms
step:1210/1770 train_time:117380ms step_avg:97.82ms
step:1211/1770 train_time:117482ms step_avg:97.82ms
step:1212/1770 train_time:117586ms step_avg:97.83ms
step:1213/1770 train_time:117688ms step_avg:97.83ms
step:1214/1770 train_time:117790ms step_avg:97.83ms
step:1215/1770 train_time:117892ms step_avg:97.84ms
step:1216/1770 train_time:117997ms step_avg:97.84ms
step:1217/1770 train_time:118098ms step_avg:97.84ms
step:1218/1770 train_time:118200ms step_avg:97.85ms
step:1219/1770 train_time:118302ms step_avg:97.85ms
step:1220/1770 train_time:118405ms step_avg:97.86ms
step:1221/1770 train_time:118506ms step_avg:97.86ms
step:1222/1770 train_time:118611ms step_avg:97.86ms
step:1223/1770 train_time:118713ms step_avg:97.87ms
step:1224/1770 train_time:118816ms step_avg:97.87ms
step:1225/1770 train_time:118918ms step_avg:97.87ms
step:1226/1770 train_time:119019ms step_avg:97.88ms
step:1227/1770 train_time:119124ms step_avg:97.88ms
step:1228/1770 train_time:119228ms step_avg:97.89ms
step:1229/1770 train_time:119330ms step_avg:97.89ms
step:1230/1770 train_time:119432ms step_avg:97.90ms
step:1231/1770 train_time:119535ms step_avg:97.90ms
step:1232/1770 train_time:119636ms step_avg:97.90ms
step:1233/1770 train_time:119738ms step_avg:97.90ms
step:1234/1770 train_time:119840ms step_avg:97.91ms
step:1235/1770 train_time:119941ms step_avg:97.91ms
step:1236/1770 train_time:120043ms step_avg:97.91ms
step:1237/1770 train_time:120145ms step_avg:97.92ms
step:1238/1770 train_time:120248ms step_avg:97.92ms
step:1239/1770 train_time:120351ms step_avg:97.93ms
step:1240/1770 train_time:120454ms step_avg:97.93ms
step:1241/1770 train_time:120556ms step_avg:97.93ms
step:1242/1770 train_time:120658ms step_avg:97.94ms
step:1243/1770 train_time:120760ms step_avg:97.94ms
step:1244/1770 train_time:120861ms step_avg:97.94ms
step:1245/1770 train_time:120964ms step_avg:97.95ms
step:1246/1770 train_time:121065ms step_avg:97.95ms
step:1247/1770 train_time:121168ms step_avg:97.95ms
step:1248/1770 train_time:121270ms step_avg:97.96ms
step:1249/1770 train_time:121372ms step_avg:97.96ms
step:1250/1770 train_time:121474ms step_avg:97.96ms
step:1250/1770 val_loss:3.4291 train_time:121576ms step_avg:98.05ms
step:1251/1770 train_time:121597ms step_avg:97.98ms
step:1252/1770 train_time:121686ms step_avg:97.98ms
step:1253/1770 train_time:121789ms step_avg:97.98ms
step:1254/1770 train_time:121893ms step_avg:97.98ms
step:1255/1770 train_time:121996ms step_avg:97.99ms
step:1256/1770 train_time:122097ms step_avg:97.99ms
step:1257/1770 train_time:122199ms step_avg:97.99ms
step:1258/1770 train_time:122301ms step_avg:98.00ms
step:1259/1770 train_time:122403ms step_avg:98.00ms
step:1260/1770 train_time:122504ms step_avg:98.00ms
step:1261/1770 train_time:122608ms step_avg:98.01ms
step:1262/1770 train_time:122711ms step_avg:98.01ms
step:1263/1770 train_time:122813ms step_avg:98.01ms
step:1264/1770 train_time:122916ms step_avg:98.02ms
step:1265/1770 train_time:123017ms step_avg:98.02ms
step:1266/1770 train_time:123120ms step_avg:98.03ms
step:1267/1770 train_time:123223ms step_avg:98.03ms
step:1268/1770 train_time:123325ms step_avg:98.03ms
step:1269/1770 train_time:123427ms step_avg:98.04ms
step:1270/1770 train_time:123529ms step_avg:98.04ms
step:1271/1770 train_time:123631ms step_avg:98.04ms
step:1272/1770 train_time:123733ms step_avg:98.05ms
step:1273/1770 train_time:123837ms step_avg:98.05ms
step:1274/1770 train_time:123940ms step_avg:98.05ms
step:1275/1770 train_time:124042ms step_avg:98.06ms
step:1276/1770 train_time:124144ms step_avg:98.06ms
step:1277/1770 train_time:124245ms step_avg:98.06ms
step:1278/1770 train_time:124349ms step_avg:98.07ms
step:1279/1770 train_time:124451ms step_avg:98.07ms
step:1280/1770 train_time:124555ms step_avg:98.07ms
step:1281/1770 train_time:124657ms step_avg:98.08ms
step:1282/1770 train_time:124760ms step_avg:98.08ms
step:1283/1770 train_time:124863ms step_avg:98.09ms
step:1284/1770 train_time:124965ms step_avg:98.09ms
step:1285/1770 train_time:125067ms step_avg:98.09ms
step:1286/1770 train_time:125170ms step_avg:98.10ms
step:1287/1770 train_time:125274ms step_avg:98.10ms
step:1288/1770 train_time:125377ms step_avg:98.10ms
step:1289/1770 train_time:125479ms step_avg:98.11ms
step:1290/1770 train_time:125581ms step_avg:98.11ms
step:1291/1770 train_time:125683ms step_avg:98.11ms
step:1292/1770 train_time:125784ms step_avg:98.12ms
step:1293/1770 train_time:125887ms step_avg:98.12ms
step:1294/1770 train_time:125988ms step_avg:98.12ms
step:1295/1770 train_time:126090ms step_avg:98.12ms
step:1296/1770 train_time:126193ms step_avg:98.13ms
step:1297/1770 train_time:126294ms step_avg:98.13ms
step:1298/1770 train_time:126397ms step_avg:98.13ms
step:1299/1770 train_time:126499ms step_avg:98.14ms
step:1300/1770 train_time:126601ms step_avg:98.14ms
step:1301/1770 train_time:126703ms step_avg:98.14ms
step:1302/1770 train_time:126805ms step_avg:98.15ms
step:1303/1770 train_time:126907ms step_avg:98.15ms
step:1304/1770 train_time:127008ms step_avg:98.15ms
step:1305/1770 train_time:127110ms step_avg:98.15ms
step:1306/1770 train_time:127212ms step_avg:98.16ms
step:1307/1770 train_time:127315ms step_avg:98.16ms
step:1308/1770 train_time:127418ms step_avg:98.16ms
step:1309/1770 train_time:127521ms step_avg:98.17ms
step:1310/1770 train_time:127622ms step_avg:98.17ms
step:1311/1770 train_time:127724ms step_avg:98.17ms
step:1312/1770 train_time:127825ms step_avg:98.18ms
step:1313/1770 train_time:127926ms step_avg:98.18ms
step:1314/1770 train_time:128028ms step_avg:98.18ms
step:1315/1770 train_time:128130ms step_avg:98.18ms
step:1316/1770 train_time:128232ms step_avg:98.19ms
step:1317/1770 train_time:128335ms step_avg:98.19ms
step:1318/1770 train_time:128441ms step_avg:98.20ms
step:1319/1770 train_time:128544ms step_avg:98.20ms
step:1320/1770 train_time:128646ms step_avg:98.20ms
step:1321/1770 train_time:128749ms step_avg:98.21ms
step:1322/1770 train_time:128851ms step_avg:98.21ms
step:1323/1770 train_time:128954ms step_avg:98.21ms
step:1324/1770 train_time:129057ms step_avg:98.22ms
step:1325/1770 train_time:129160ms step_avg:98.22ms
step:1326/1770 train_time:129261ms step_avg:98.22ms
step:1327/1770 train_time:129367ms step_avg:98.23ms
step:1328/1770 train_time:129468ms step_avg:98.23ms
step:1329/1770 train_time:129569ms step_avg:98.23ms
step:1330/1770 train_time:129671ms step_avg:98.24ms
step:1331/1770 train_time:129773ms step_avg:98.24ms
step:1332/1770 train_time:129875ms step_avg:98.24ms
step:1333/1770 train_time:129977ms step_avg:98.24ms
step:1334/1770 train_time:130080ms step_avg:98.25ms
step:1335/1770 train_time:130182ms step_avg:98.25ms
step:1336/1770 train_time:130283ms step_avg:98.25ms
step:1337/1770 train_time:130386ms step_avg:98.26ms
step:1338/1770 train_time:130487ms step_avg:98.26ms
step:1339/1770 train_time:130592ms step_avg:98.26ms
step:1340/1770 train_time:130693ms step_avg:98.27ms
step:1341/1770 train_time:130794ms step_avg:98.27ms
step:1342/1770 train_time:130898ms step_avg:98.27ms
step:1343/1770 train_time:131001ms step_avg:98.28ms
step:1344/1770 train_time:131104ms step_avg:98.28ms
step:1345/1770 train_time:131205ms step_avg:98.28ms
step:1346/1770 train_time:131307ms step_avg:98.28ms
step:1347/1770 train_time:131409ms step_avg:98.29ms
step:1348/1770 train_time:131515ms step_avg:98.29ms
step:1349/1770 train_time:131617ms step_avg:98.30ms
step:1350/1770 train_time:131719ms step_avg:98.30ms
step:1351/1770 train_time:131821ms step_avg:98.30ms
step:1352/1770 train_time:131923ms step_avg:98.30ms
step:1353/1770 train_time:132025ms step_avg:98.31ms
step:1354/1770 train_time:132127ms step_avg:98.31ms
step:1355/1770 train_time:132229ms step_avg:98.31ms
step:1356/1770 train_time:132331ms step_avg:98.31ms
step:1357/1770 train_time:132434ms step_avg:98.32ms
step:1358/1770 train_time:132537ms step_avg:98.32ms
step:1359/1770 train_time:132640ms step_avg:98.32ms
step:1360/1770 train_time:132742ms step_avg:98.33ms
step:1361/1770 train_time:132845ms step_avg:98.33ms
step:1362/1770 train_time:132946ms step_avg:98.33ms
step:1363/1770 train_time:133048ms step_avg:98.34ms
step:1364/1770 train_time:133150ms step_avg:98.34ms
step:1365/1770 train_time:133252ms step_avg:98.34ms
step:1366/1770 train_time:133354ms step_avg:98.34ms
step:1367/1770 train_time:133457ms step_avg:98.35ms
step:1368/1770 train_time:133560ms step_avg:98.35ms
step:1369/1770 train_time:133662ms step_avg:98.35ms
step:1370/1770 train_time:133765ms step_avg:98.36ms
step:1371/1770 train_time:133867ms step_avg:98.36ms
step:1372/1770 train_time:133968ms step_avg:98.36ms
step:1373/1770 train_time:134070ms step_avg:98.36ms
step:1374/1770 train_time:134173ms step_avg:98.37ms
step:1375/1770 train_time:134276ms step_avg:98.37ms
step:1375/1770 val_loss:3.3844 train_time:134376ms step_avg:98.44ms
step:1376/1770 train_time:134400ms step_avg:98.39ms
step:1377/1770 train_time:134488ms step_avg:98.38ms
step:1378/1770 train_time:134590ms step_avg:98.38ms
step:1379/1770 train_time:134692ms step_avg:98.39ms
step:1380/1770 train_time:134794ms step_avg:98.39ms
step:1381/1770 train_time:134896ms step_avg:98.39ms
step:1382/1770 train_time:134998ms step_avg:98.39ms
step:1383/1770 train_time:135101ms step_avg:98.40ms
step:1384/1770 train_time:135202ms step_avg:98.40ms
step:1385/1770 train_time:135304ms step_avg:98.40ms
step:1386/1770 train_time:135406ms step_avg:98.41ms
step:1387/1770 train_time:135509ms step_avg:98.41ms
step:1388/1770 train_time:135610ms step_avg:98.41ms
step:1389/1770 train_time:135713ms step_avg:98.41ms
step:1390/1770 train_time:135815ms step_avg:98.42ms
step:1391/1770 train_time:135917ms step_avg:98.42ms
step:1392/1770 train_time:136020ms step_avg:98.42ms
step:1393/1770 train_time:136122ms step_avg:98.43ms
step:1394/1770 train_time:136230ms step_avg:98.43ms
step:1395/1770 train_time:136327ms step_avg:98.43ms
step:1396/1770 train_time:136431ms step_avg:98.43ms
step:1397/1770 train_time:136533ms step_avg:98.44ms
step:1398/1770 train_time:136635ms step_avg:98.44ms
step:1399/1770 train_time:136739ms step_avg:98.44ms
step:1400/1770 train_time:136841ms step_avg:98.45ms
step:1401/1770 train_time:136943ms step_avg:98.45ms
step:1402/1770 train_time:137046ms step_avg:98.45ms
step:1403/1770 train_time:137148ms step_avg:98.45ms
step:1404/1770 train_time:137250ms step_avg:98.46ms
step:1405/1770 train_time:137352ms step_avg:98.46ms
step:1406/1770 train_time:137454ms step_avg:98.46ms
step:1407/1770 train_time:137556ms step_avg:98.47ms
step:1408/1770 train_time:137658ms step_avg:98.47ms
step:1409/1770 train_time:137761ms step_avg:98.47ms
step:1410/1770 train_time:137863ms step_avg:98.47ms
step:1411/1770 train_time:137965ms step_avg:98.48ms
step:1412/1770 train_time:138067ms step_avg:98.48ms
step:1413/1770 train_time:138168ms step_avg:98.48ms
step:1414/1770 train_time:138271ms step_avg:98.48ms
step:1415/1770 train_time:138375ms step_avg:98.49ms
step:1416/1770 train_time:138478ms step_avg:98.49ms
step:1417/1770 train_time:138580ms step_avg:98.49ms
step:1418/1770 train_time:138682ms step_avg:98.50ms
step:1419/1770 train_time:138785ms step_avg:98.50ms
step:1420/1770 train_time:138887ms step_avg:98.50ms
step:1421/1770 train_time:138988ms step_avg:98.50ms
step:1422/1770 train_time:139090ms step_avg:98.51ms
step:1423/1770 train_time:139192ms step_avg:98.51ms
step:1424/1770 train_time:139294ms step_avg:98.51ms
step:1425/1770 train_time:139396ms step_avg:98.51ms
step:1426/1770 train_time:139499ms step_avg:98.52ms
step:1427/1770 train_time:139601ms step_avg:98.52ms
step:1428/1770 train_time:139705ms step_avg:98.52ms
step:1429/1770 train_time:139807ms step_avg:98.53ms
step:1430/1770 train_time:139909ms step_avg:98.53ms
step:1431/1770 train_time:140012ms step_avg:98.53ms
step:1432/1770 train_time:140114ms step_avg:98.53ms
step:1433/1770 train_time:140215ms step_avg:98.54ms
step:1434/1770 train_time:140317ms step_avg:98.54ms
step:1435/1770 train_time:140419ms step_avg:98.54ms
step:1436/1770 train_time:140522ms step_avg:98.54ms
step:1437/1770 train_time:140625ms step_avg:98.55ms
step:1438/1770 train_time:140726ms step_avg:98.55ms
step:1439/1770 train_time:140829ms step_avg:98.55ms
step:1440/1770 train_time:140930ms step_avg:98.55ms
step:1441/1770 train_time:141035ms step_avg:98.56ms
step:1442/1770 train_time:141137ms step_avg:98.56ms
step:1443/1770 train_time:141240ms step_avg:98.56ms
step:1444/1770 train_time:141342ms step_avg:98.57ms
step:1445/1770 train_time:141445ms step_avg:98.57ms
step:1446/1770 train_time:141547ms step_avg:98.57ms
step:1447/1770 train_time:141651ms step_avg:98.57ms
step:1448/1770 train_time:141755ms step_avg:98.58ms
step:1449/1770 train_time:141858ms step_avg:98.58ms
step:1450/1770 train_time:141961ms step_avg:98.58ms
step:1451/1770 train_time:142064ms step_avg:98.59ms
step:1452/1770 train_time:142167ms step_avg:98.59ms
step:1453/1770 train_time:142269ms step_avg:98.59ms
step:1454/1770 train_time:142372ms step_avg:98.60ms
step:1455/1770 train_time:142477ms step_avg:98.60ms
step:1456/1770 train_time:142581ms step_avg:98.60ms
step:1457/1770 train_time:142684ms step_avg:98.61ms
step:1458/1770 train_time:142787ms step_avg:98.61ms
step:1459/1770 train_time:142892ms step_avg:98.61ms
step:1460/1770 train_time:142996ms step_avg:98.62ms
step:1461/1770 train_time:143099ms step_avg:98.62ms
step:1462/1770 train_time:143202ms step_avg:98.62ms
step:1463/1770 train_time:143305ms step_avg:98.63ms
step:1464/1770 train_time:143409ms step_avg:98.63ms
step:1465/1770 train_time:143512ms step_avg:98.63ms
step:1466/1770 train_time:143617ms step_avg:98.64ms
step:1467/1770 train_time:143721ms step_avg:98.64ms
step:1468/1770 train_time:143824ms step_avg:98.64ms
step:1469/1770 train_time:143927ms step_avg:98.65ms
step:1470/1770 train_time:144030ms step_avg:98.65ms
step:1471/1770 train_time:144133ms step_avg:98.65ms
step:1472/1770 train_time:144236ms step_avg:98.66ms
step:1473/1770 train_time:144340ms step_avg:98.66ms
step:1474/1770 train_time:144445ms step_avg:98.66ms
step:1475/1770 train_time:144548ms step_avg:98.67ms
step:1476/1770 train_time:144651ms step_avg:98.67ms
step:1477/1770 train_time:144756ms step_avg:98.68ms
step:1478/1770 train_time:144860ms step_avg:98.68ms
step:1479/1770 train_time:144964ms step_avg:98.68ms
step:1480/1770 train_time:145066ms step_avg:98.68ms
step:1481/1770 train_time:145173ms step_avg:98.69ms
step:1482/1770 train_time:145276ms step_avg:98.69ms
step:1483/1770 train_time:145380ms step_avg:98.70ms
step:1484/1770 train_time:145483ms step_avg:98.70ms
step:1485/1770 train_time:145586ms step_avg:98.70ms
step:1486/1770 train_time:145688ms step_avg:98.70ms
step:1487/1770 train_time:145791ms step_avg:98.71ms
step:1488/1770 train_time:145895ms step_avg:98.71ms
step:1489/1770 train_time:146000ms step_avg:98.72ms
step:1490/1770 train_time:146103ms step_avg:98.72ms
step:1491/1770 train_time:146207ms step_avg:98.72ms
step:1492/1770 train_time:146311ms step_avg:98.73ms
step:1493/1770 train_time:146418ms step_avg:98.73ms
step:1494/1770 train_time:146524ms step_avg:98.74ms
step:1495/1770 train_time:146626ms step_avg:98.74ms
step:1496/1770 train_time:146729ms step_avg:98.74ms
step:1497/1770 train_time:146832ms step_avg:98.74ms
step:1498/1770 train_time:146935ms step_avg:98.75ms
step:1499/1770 train_time:147042ms step_avg:98.75ms
step:1500/1770 train_time:147142ms step_avg:98.75ms
step:1500/1770 val_loss:3.3460 train_time:147243ms step_avg:98.82ms
step:1501/1770 train_time:147265ms step_avg:98.77ms
step:1502/1770 train_time:147358ms step_avg:98.77ms
step:1503/1770 train_time:147461ms step_avg:98.77ms
step:1504/1770 train_time:147564ms step_avg:98.77ms
step:1505/1770 train_time:147669ms step_avg:98.78ms
step:1506/1770 train_time:147773ms step_avg:98.78ms
step:1507/1770 train_time:147876ms step_avg:98.78ms
step:1508/1770 train_time:147981ms step_avg:98.79ms
step:1509/1770 train_time:148084ms step_avg:98.79ms
step:1510/1770 train_time:148186ms step_avg:98.79ms
step:1511/1770 train_time:148290ms step_avg:98.79ms
step:1512/1770 train_time:148395ms step_avg:98.80ms
step:1513/1770 train_time:148500ms step_avg:98.80ms
step:1514/1770 train_time:148603ms step_avg:98.81ms
step:1515/1770 train_time:148706ms step_avg:98.81ms
step:1516/1770 train_time:148809ms step_avg:98.81ms
step:1517/1770 train_time:148912ms step_avg:98.81ms
step:1518/1770 train_time:149019ms step_avg:98.82ms
step:1519/1770 train_time:149121ms step_avg:98.82ms
step:1520/1770 train_time:149225ms step_avg:98.82ms
step:1521/1770 train_time:149328ms step_avg:98.83ms
step:1522/1770 train_time:149433ms step_avg:98.83ms
step:1523/1770 train_time:149537ms step_avg:98.83ms
step:1524/1770 train_time:149640ms step_avg:98.84ms
step:1525/1770 train_time:149742ms step_avg:98.84ms
step:1526/1770 train_time:149845ms step_avg:98.84ms
step:1527/1770 train_time:149949ms step_avg:98.85ms
step:1528/1770 train_time:150054ms step_avg:98.85ms
step:1529/1770 train_time:150157ms step_avg:98.85ms
step:1530/1770 train_time:150260ms step_avg:98.86ms
step:1531/1770 train_time:150362ms step_avg:98.86ms
step:1532/1770 train_time:150466ms step_avg:98.86ms
step:1533/1770 train_time:150570ms step_avg:98.86ms
step:1534/1770 train_time:150674ms step_avg:98.87ms
step:1535/1770 train_time:150778ms step_avg:98.87ms
step:1536/1770 train_time:150881ms step_avg:98.87ms
step:1537/1770 train_time:150984ms step_avg:98.88ms
step:1538/1770 train_time:151089ms step_avg:98.88ms
step:1539/1770 train_time:151192ms step_avg:98.88ms
step:1540/1770 train_time:151301ms step_avg:98.89ms
step:1541/1770 train_time:151405ms step_avg:98.89ms
step:1542/1770 train_time:151508ms step_avg:98.90ms
step:1543/1770 train_time:151611ms step_avg:98.90ms
step:1544/1770 train_time:151716ms step_avg:98.90ms
step:1545/1770 train_time:151819ms step_avg:98.90ms
step:1546/1770 train_time:151922ms step_avg:98.91ms
step:1547/1770 train_time:152024ms step_avg:98.91ms
step:1548/1770 train_time:152127ms step_avg:98.91ms
step:1549/1770 train_time:152231ms step_avg:98.92ms
step:1550/1770 train_time:152335ms step_avg:98.92ms
step:1551/1770 train_time:152439ms step_avg:98.92ms
step:1552/1770 train_time:152543ms step_avg:98.93ms
step:1553/1770 train_time:152646ms step_avg:98.93ms
step:1554/1770 train_time:152748ms step_avg:98.93ms
step:1555/1770 train_time:152853ms step_avg:98.93ms
step:1556/1770 train_time:152956ms step_avg:98.94ms
step:1557/1770 train_time:153059ms step_avg:98.94ms
step:1558/1770 train_time:153162ms step_avg:98.94ms
step:1559/1770 train_time:153265ms step_avg:98.94ms
step:1560/1770 train_time:153367ms step_avg:98.95ms
step:1561/1770 train_time:153473ms step_avg:98.95ms
step:1562/1770 train_time:153577ms step_avg:98.95ms
step:1563/1770 train_time:153680ms step_avg:98.96ms
step:1564/1770 train_time:153782ms step_avg:98.96ms
step:1565/1770 train_time:153885ms step_avg:98.96ms
step:1566/1770 train_time:153988ms step_avg:98.96ms
step:1567/1770 train_time:154092ms step_avg:98.97ms
step:1568/1770 train_time:154194ms step_avg:98.97ms
step:1569/1770 train_time:154301ms step_avg:98.97ms
step:1570/1770 train_time:154404ms step_avg:98.98ms
step:1571/1770 train_time:154507ms step_avg:98.98ms
step:1572/1770 train_time:154612ms step_avg:98.98ms
step:1573/1770 train_time:154717ms step_avg:98.99ms
step:1574/1770 train_time:154820ms step_avg:98.99ms
step:1575/1770 train_time:154922ms step_avg:98.99ms
step:1576/1770 train_time:155025ms step_avg:98.99ms
step:1577/1770 train_time:155129ms step_avg:99.00ms
step:1578/1770 train_time:155234ms step_avg:99.00ms
step:1579/1770 train_time:155338ms step_avg:99.00ms
step:1580/1770 train_time:155441ms step_avg:99.01ms
step:1581/1770 train_time:155547ms step_avg:99.01ms
step:1582/1770 train_time:155651ms step_avg:99.01ms
step:1583/1770 train_time:155755ms step_avg:99.02ms
step:1584/1770 train_time:155860ms step_avg:99.02ms
step:1585/1770 train_time:155963ms step_avg:99.02ms
step:1586/1770 train_time:156069ms step_avg:99.03ms
step:1587/1770 train_time:156173ms step_avg:99.03ms
step:1588/1770 train_time:156276ms step_avg:99.03ms
step:1589/1770 train_time:156382ms step_avg:99.04ms
step:1590/1770 train_time:156484ms step_avg:99.04ms
step:1591/1770 train_time:156587ms step_avg:99.04ms
step:1592/1770 train_time:156691ms step_avg:99.05ms
step:1593/1770 train_time:156794ms step_avg:99.05ms
step:1594/1770 train_time:156897ms step_avg:99.05ms
step:1595/1770 train_time:157002ms step_avg:99.05ms
step:1596/1770 train_time:157106ms step_avg:99.06ms
step:1597/1770 train_time:157209ms step_avg:99.06ms
step:1598/1770 train_time:157313ms step_avg:99.06ms
step:1599/1770 train_time:157419ms step_avg:99.07ms
step:1600/1770 train_time:157525ms step_avg:99.07ms
step:1601/1770 train_time:157628ms step_avg:99.07ms
step:1602/1770 train_time:157732ms step_avg:99.08ms
step:1603/1770 train_time:157836ms step_avg:99.08ms
step:1604/1770 train_time:157939ms step_avg:99.08ms
step:1605/1770 train_time:158042ms step_avg:99.09ms
step:1606/1770 train_time:158145ms step_avg:99.09ms
step:1607/1770 train_time:158252ms step_avg:99.09ms
step:1608/1770 train_time:158355ms step_avg:99.10ms
step:1609/1770 train_time:158458ms step_avg:99.10ms
step:1610/1770 train_time:158562ms step_avg:99.10ms
step:1611/1770 train_time:158668ms step_avg:99.11ms
step:1612/1770 train_time:158771ms step_avg:99.11ms
step:1613/1770 train_time:158875ms step_avg:99.11ms
step:1614/1770 train_time:158979ms step_avg:99.11ms
step:1615/1770 train_time:159083ms step_avg:99.12ms
step:1616/1770 train_time:159186ms step_avg:99.12ms
step:1617/1770 train_time:159291ms step_avg:99.12ms
step:1618/1770 train_time:159396ms step_avg:99.13ms
step:1619/1770 train_time:159500ms step_avg:99.13ms
step:1620/1770 train_time:159603ms step_avg:99.13ms
step:1621/1770 train_time:159706ms step_avg:99.13ms
step:1622/1770 train_time:159810ms step_avg:99.14ms
step:1623/1770 train_time:159918ms step_avg:99.14ms
step:1624/1770 train_time:160020ms step_avg:99.15ms
step:1625/1770 train_time:160123ms step_avg:99.15ms
step:1625/1770 val_loss:3.3118 train_time:160224ms step_avg:99.21ms
step:1626/1770 train_time:160246ms step_avg:99.16ms
step:1627/1770 train_time:160337ms step_avg:99.16ms
step:1628/1770 train_time:160439ms step_avg:99.16ms
step:1629/1770 train_time:160542ms step_avg:99.16ms
step:1630/1770 train_time:160644ms step_avg:99.16ms
step:1631/1770 train_time:160747ms step_avg:99.17ms
step:1632/1770 train_time:160850ms step_avg:99.17ms
step:1633/1770 train_time:160953ms step_avg:99.17ms
step:1634/1770 train_time:161056ms step_avg:99.17ms
step:1635/1770 train_time:161160ms step_avg:99.18ms
step:1636/1770 train_time:161263ms step_avg:99.18ms
step:1637/1770 train_time:161369ms step_avg:99.18ms
step:1638/1770 train_time:161472ms step_avg:99.18ms
step:1639/1770 train_time:161576ms step_avg:99.19ms
step:1640/1770 train_time:161679ms step_avg:99.19ms
step:1641/1770 train_time:161782ms step_avg:99.19ms
step:1642/1770 train_time:161883ms step_avg:99.19ms
step:1643/1770 train_time:161987ms step_avg:99.20ms
step:1644/1770 train_time:162092ms step_avg:99.20ms
step:1645/1770 train_time:162196ms step_avg:99.20ms
step:1646/1770 train_time:162301ms step_avg:99.21ms
step:1647/1770 train_time:162406ms step_avg:99.21ms
step:1648/1770 train_time:162510ms step_avg:99.21ms
step:1649/1770 train_time:162614ms step_avg:99.22ms
step:1650/1770 train_time:162717ms step_avg:99.22ms
step:1651/1770 train_time:162822ms step_avg:99.22ms
step:1652/1770 train_time:162922ms step_avg:99.22ms
step:1653/1770 train_time:163026ms step_avg:99.22ms
step:1654/1770 train_time:163133ms step_avg:99.23ms
step:1655/1770 train_time:163239ms step_avg:99.23ms
step:1656/1770 train_time:163342ms step_avg:99.24ms
step:1657/1770 train_time:163447ms step_avg:99.24ms
step:1658/1770 train_time:163551ms step_avg:99.24ms
step:1659/1770 train_time:163657ms step_avg:99.25ms
step:1660/1770 train_time:163761ms step_avg:99.25ms
step:1661/1770 train_time:163864ms step_avg:99.25ms
step:1662/1770 train_time:163968ms step_avg:99.25ms
step:1663/1770 train_time:164071ms step_avg:99.26ms
step:1664/1770 train_time:164174ms step_avg:99.26ms
step:1665/1770 train_time:164277ms step_avg:99.26ms
step:1666/1770 train_time:164381ms step_avg:99.26ms
step:1667/1770 train_time:164485ms step_avg:99.27ms
step:1668/1770 train_time:164589ms step_avg:99.27ms
step:1669/1770 train_time:164692ms step_avg:99.27ms
step:1670/1770 train_time:164795ms step_avg:99.27ms
step:1671/1770 train_time:164899ms step_avg:99.28ms
step:1672/1770 train_time:165003ms step_avg:99.28ms
step:1673/1770 train_time:165107ms step_avg:99.28ms
step:1674/1770 train_time:165211ms step_avg:99.29ms
step:1675/1770 train_time:165314ms step_avg:99.29ms
step:1676/1770 train_time:165419ms step_avg:99.29ms
step:1677/1770 train_time:165526ms step_avg:99.30ms
step:1678/1770 train_time:165629ms step_avg:99.30ms
step:1679/1770 train_time:165733ms step_avg:99.30ms
step:1680/1770 train_time:165836ms step_avg:99.30ms
step:1681/1770 train_time:165940ms step_avg:99.31ms
step:1682/1770 train_time:166044ms step_avg:99.31ms
step:1683/1770 train_time:166147ms step_avg:99.31ms
step:1684/1770 train_time:166250ms step_avg:99.31ms
step:1685/1770 train_time:166354ms step_avg:99.32ms
step:1686/1770 train_time:166458ms step_avg:99.32ms
step:1687/1770 train_time:166562ms step_avg:99.32ms
step:1688/1770 train_time:166667ms step_avg:99.32ms
step:1689/1770 train_time:166770ms step_avg:99.33ms
step:1690/1770 train_time:166873ms step_avg:99.33ms
step:1691/1770 train_time:166977ms step_avg:99.33ms
step:1692/1770 train_time:167079ms step_avg:99.33ms
step:1693/1770 train_time:167184ms step_avg:99.34ms
step:1694/1770 train_time:167287ms step_avg:99.34ms
step:1695/1770 train_time:167391ms step_avg:99.34ms
step:1696/1770 train_time:167495ms step_avg:99.34ms
step:1697/1770 train_time:167601ms step_avg:99.35ms
step:1698/1770 train_time:167705ms step_avg:99.35ms
step:1699/1770 train_time:167808ms step_avg:99.35ms
step:1700/1770 train_time:167912ms step_avg:99.36ms
step:1701/1770 train_time:168018ms step_avg:99.36ms
step:1702/1770 train_time:168118ms step_avg:99.36ms
step:1703/1770 train_time:168221ms step_avg:99.36ms
step:1704/1770 train_time:168324ms step_avg:99.36ms
step:1705/1770 train_time:168427ms step_avg:99.37ms
step:1706/1770 train_time:168530ms step_avg:99.37ms
step:1707/1770 train_time:168635ms step_avg:99.37ms
step:1708/1770 train_time:168738ms step_avg:99.37ms
step:1709/1770 train_time:168844ms step_avg:99.38ms
step:1710/1770 train_time:168950ms step_avg:99.38ms
step:1711/1770 train_time:169056ms step_avg:99.39ms
step:1712/1770 train_time:169161ms step_avg:99.39ms
step:1713/1770 train_time:169264ms step_avg:99.39ms
step:1714/1770 train_time:169368ms step_avg:99.39ms
step:1715/1770 train_time:169472ms step_avg:99.40ms
step:1716/1770 train_time:169576ms step_avg:99.40ms
step:1717/1770 train_time:169680ms step_avg:99.40ms
step:1718/1770 train_time:169785ms step_avg:99.41ms
step:1719/1770 train_time:169890ms step_avg:99.41ms
step:1720/1770 train_time:169995ms step_avg:99.41ms
step:1721/1770 train_time:170098ms step_avg:99.41ms
step:1722/1770 train_time:170205ms step_avg:99.42ms
step:1723/1770 train_time:170310ms step_avg:99.42ms
step:1724/1770 train_time:170416ms step_avg:99.43ms
step:1725/1770 train_time:170522ms step_avg:99.43ms
step:1726/1770 train_time:170627ms step_avg:99.43ms
step:1727/1770 train_time:170731ms step_avg:99.44ms
step:1728/1770 train_time:170838ms step_avg:99.44ms
step:1729/1770 train_time:170941ms step_avg:99.44ms
step:1730/1770 train_time:171047ms step_avg:99.45ms
step:1731/1770 train_time:171152ms step_avg:99.45ms
step:1732/1770 train_time:171256ms step_avg:99.45ms
step:1733/1770 train_time:171362ms step_avg:99.46ms
step:1734/1770 train_time:171465ms step_avg:99.46ms
step:1735/1770 train_time:171571ms step_avg:99.46ms
step:1736/1770 train_time:171674ms step_avg:99.46ms
step:1737/1770 train_time:171780ms step_avg:99.47ms
step:1738/1770 train_time:171884ms step_avg:99.47ms
step:1739/1770 train_time:171988ms step_avg:99.47ms
step:1740/1770 train_time:172092ms step_avg:99.48ms
step:1741/1770 train_time:172198ms step_avg:99.48ms
step:1742/1770 train_time:172305ms step_avg:99.48ms
step:1743/1770 train_time:172411ms step_avg:99.49ms
step:1744/1770 train_time:172515ms step_avg:99.49ms
step:1745/1770 train_time:172619ms step_avg:99.49ms
step:1746/1770 train_time:172726ms step_avg:99.50ms
step:1747/1770 train_time:172829ms step_avg:99.50ms
step:1748/1770 train_time:172935ms step_avg:99.50ms
step:1749/1770 train_time:173039ms step_avg:99.51ms
step:1750/1770 train_time:173143ms step_avg:99.51ms
step:1750/1770 val_loss:3.2849 train_time:173246ms step_avg:99.57ms
step:1751/1770 train_time:173267ms step_avg:99.52ms
step:1752/1770 train_time:173359ms step_avg:99.52ms
step:1753/1770 train_time:173463ms step_avg:99.52ms
step:1754/1770 train_time:173568ms step_avg:99.52ms
step:1755/1770 train_time:173672ms step_avg:99.53ms
step:1756/1770 train_time:173776ms step_avg:99.53ms
step:1757/1770 train_time:173881ms step_avg:99.53ms
step:1758/1770 train_time:173985ms step_avg:99.53ms
step:1759/1770 train_time:174089ms step_avg:99.54ms
step:1760/1770 train_time:174194ms step_avg:99.54ms
step:1761/1770 train_time:174301ms step_avg:99.54ms
step:1762/1770 train_time:174408ms step_avg:99.55ms
step:1763/1770 train_time:174511ms step_avg:99.55ms
step:1764/1770 train_time:174616ms step_avg:99.55ms
step:1765/1770 train_time:174721ms step_avg:99.56ms
step:1766/1770 train_time:174829ms step_avg:99.56ms
step:1767/1770 train_time:174932ms step_avg:99.56ms
step:1768/1770 train_time:175037ms step_avg:99.57ms
step:1769/1770 train_time:175140ms step_avg:99.57ms
step:1770/1770 train_time:175244ms step_avg:99.57ms
step:1770/1770 val_loss:3.2822 train_time:175347ms step_avg:99.63ms
peak memory allocated: 28840 MiB reserved: 32232 MiB
