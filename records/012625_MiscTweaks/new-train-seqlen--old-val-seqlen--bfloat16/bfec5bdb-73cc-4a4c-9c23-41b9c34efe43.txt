import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:29:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23158ms step_avg:nanms
step:2/1770 train_time:23692ms step_avg:nanms
step:3/1770 train_time:23791ms step_avg:nanms
step:4/1770 train_time:23888ms step_avg:nanms
step:5/1770 train_time:23987ms step_avg:nanms
step:6/1770 train_time:24086ms step_avg:nanms
step:7/1770 train_time:24184ms step_avg:nanms
step:8/1770 train_time:24283ms step_avg:nanms
step:9/1770 train_time:24382ms step_avg:nanms
step:10/1770 train_time:24481ms step_avg:nanms
step:11/1770 train_time:99ms step_avg:nanms
step:12/1770 train_time:197ms step_avg:nanms
step:13/1770 train_time:296ms step_avg:98.69ms
step:14/1770 train_time:395ms step_avg:98.72ms
step:15/1770 train_time:494ms step_avg:98.86ms
step:16/1770 train_time:594ms step_avg:99.02ms
step:17/1770 train_time:694ms step_avg:99.10ms
step:18/1770 train_time:793ms step_avg:99.12ms
step:19/1770 train_time:893ms step_avg:99.22ms
step:20/1770 train_time:993ms step_avg:99.29ms
step:21/1770 train_time:1093ms step_avg:99.32ms
step:22/1770 train_time:1194ms step_avg:99.50ms
step:23/1770 train_time:1292ms step_avg:99.41ms
step:24/1770 train_time:1392ms step_avg:99.44ms
step:25/1770 train_time:1492ms step_avg:99.45ms
step:26/1770 train_time:1592ms step_avg:99.48ms
step:27/1770 train_time:1692ms step_avg:99.51ms
step:28/1770 train_time:1792ms step_avg:99.54ms
step:29/1770 train_time:1892ms step_avg:99.59ms
step:30/1770 train_time:1992ms step_avg:99.58ms
step:31/1770 train_time:2092ms step_avg:99.62ms
step:32/1770 train_time:2191ms step_avg:99.59ms
step:33/1770 train_time:2291ms step_avg:99.61ms
step:34/1770 train_time:2392ms step_avg:99.66ms
step:35/1770 train_time:2493ms step_avg:99.70ms
step:36/1770 train_time:2592ms step_avg:99.69ms
step:37/1770 train_time:2692ms step_avg:99.72ms
step:38/1770 train_time:2793ms step_avg:99.76ms
step:39/1770 train_time:2892ms step_avg:99.73ms
step:40/1770 train_time:2992ms step_avg:99.74ms
step:41/1770 train_time:3092ms step_avg:99.73ms
step:42/1770 train_time:3192ms step_avg:99.75ms
step:43/1770 train_time:3291ms step_avg:99.74ms
step:44/1770 train_time:3392ms step_avg:99.76ms
step:45/1770 train_time:3494ms step_avg:99.83ms
step:46/1770 train_time:3593ms step_avg:99.81ms
step:47/1770 train_time:3693ms step_avg:99.80ms
step:48/1770 train_time:3793ms step_avg:99.82ms
step:49/1770 train_time:3893ms step_avg:99.82ms
step:50/1770 train_time:3993ms step_avg:99.82ms
step:51/1770 train_time:4093ms step_avg:99.83ms
step:52/1770 train_time:4192ms step_avg:99.82ms
step:53/1770 train_time:4292ms step_avg:99.82ms
step:54/1770 train_time:4392ms step_avg:99.81ms
step:55/1770 train_time:4492ms step_avg:99.83ms
step:56/1770 train_time:4592ms step_avg:99.83ms
step:57/1770 train_time:4692ms step_avg:99.83ms
step:58/1770 train_time:4791ms step_avg:99.82ms
step:59/1770 train_time:4891ms step_avg:99.81ms
step:60/1770 train_time:4990ms step_avg:99.79ms
step:61/1770 train_time:5089ms step_avg:99.78ms
step:62/1770 train_time:5188ms step_avg:99.78ms
step:63/1770 train_time:5288ms step_avg:99.77ms
step:64/1770 train_time:5387ms step_avg:99.77ms
step:65/1770 train_time:5487ms step_avg:99.77ms
step:66/1770 train_time:5588ms step_avg:99.78ms
step:67/1770 train_time:5687ms step_avg:99.77ms
step:68/1770 train_time:5787ms step_avg:99.77ms
step:69/1770 train_time:5886ms step_avg:99.77ms
step:70/1770 train_time:5987ms step_avg:99.78ms
step:71/1770 train_time:6087ms step_avg:99.78ms
step:72/1770 train_time:6187ms step_avg:99.80ms
step:73/1770 train_time:6287ms step_avg:99.80ms
step:74/1770 train_time:6387ms step_avg:99.80ms
step:75/1770 train_time:6488ms step_avg:99.81ms
step:76/1770 train_time:6587ms step_avg:99.81ms
step:77/1770 train_time:6687ms step_avg:99.81ms
step:78/1770 train_time:6787ms step_avg:99.81ms
step:79/1770 train_time:6887ms step_avg:99.81ms
step:80/1770 train_time:6987ms step_avg:99.81ms
step:81/1770 train_time:7086ms step_avg:99.81ms
step:82/1770 train_time:7186ms step_avg:99.81ms
step:83/1770 train_time:7285ms step_avg:99.80ms
step:84/1770 train_time:7385ms step_avg:99.80ms
step:85/1770 train_time:7485ms step_avg:99.79ms
step:86/1770 train_time:7584ms step_avg:99.79ms
step:87/1770 train_time:7683ms step_avg:99.78ms
step:88/1770 train_time:7782ms step_avg:99.77ms
step:89/1770 train_time:7881ms step_avg:99.76ms
step:90/1770 train_time:7981ms step_avg:99.76ms
step:91/1770 train_time:8080ms step_avg:99.75ms
step:92/1770 train_time:8179ms step_avg:99.74ms
step:93/1770 train_time:8277ms step_avg:99.72ms
step:94/1770 train_time:8376ms step_avg:99.71ms
step:95/1770 train_time:8475ms step_avg:99.70ms
step:96/1770 train_time:8574ms step_avg:99.70ms
step:97/1770 train_time:8674ms step_avg:99.70ms
step:98/1770 train_time:8773ms step_avg:99.70ms
step:99/1770 train_time:8873ms step_avg:99.69ms
step:100/1770 train_time:8972ms step_avg:99.69ms
step:101/1770 train_time:9072ms step_avg:99.69ms
step:102/1770 train_time:9171ms step_avg:99.69ms
step:103/1770 train_time:9272ms step_avg:99.70ms
step:104/1770 train_time:9373ms step_avg:99.71ms
step:105/1770 train_time:9472ms step_avg:99.71ms
step:106/1770 train_time:9573ms step_avg:99.71ms
step:107/1770 train_time:9672ms step_avg:99.71ms
step:108/1770 train_time:9772ms step_avg:99.72ms
step:109/1770 train_time:9872ms step_avg:99.72ms
step:110/1770 train_time:9972ms step_avg:99.72ms
step:111/1770 train_time:10072ms step_avg:99.72ms
step:112/1770 train_time:10171ms step_avg:99.71ms
step:113/1770 train_time:10270ms step_avg:99.71ms
step:114/1770 train_time:10370ms step_avg:99.71ms
step:115/1770 train_time:10469ms step_avg:99.71ms
step:116/1770 train_time:10569ms step_avg:99.71ms
step:117/1770 train_time:10669ms step_avg:99.71ms
step:118/1770 train_time:10768ms step_avg:99.70ms
step:119/1770 train_time:10867ms step_avg:99.70ms
step:120/1770 train_time:10967ms step_avg:99.70ms
step:121/1770 train_time:11066ms step_avg:99.69ms
step:122/1770 train_time:11166ms step_avg:99.70ms
step:123/1770 train_time:11265ms step_avg:99.69ms
step:124/1770 train_time:11364ms step_avg:99.69ms
step:125/1770 train_time:11463ms step_avg:99.68ms
step:125/1770 val_loss:4.6451 train_time:11561ms step_avg:100.53ms
step:126/1770 train_time:11580ms step_avg:99.82ms
step:127/1770 train_time:11675ms step_avg:99.79ms
step:128/1770 train_time:11781ms step_avg:99.84ms
step:129/1770 train_time:11881ms step_avg:99.84ms
step:130/1770 train_time:11981ms step_avg:99.84ms
step:131/1770 train_time:12080ms step_avg:99.83ms
step:132/1770 train_time:12179ms step_avg:99.83ms
step:133/1770 train_time:12278ms step_avg:99.82ms
step:134/1770 train_time:12379ms step_avg:99.83ms
step:135/1770 train_time:12478ms step_avg:99.83ms
step:136/1770 train_time:12578ms step_avg:99.83ms
step:137/1770 train_time:12679ms step_avg:99.83ms
step:138/1770 train_time:12779ms step_avg:99.84ms
step:139/1770 train_time:12880ms step_avg:99.84ms
step:140/1770 train_time:12980ms step_avg:99.85ms
step:141/1770 train_time:13080ms step_avg:99.85ms
step:142/1770 train_time:13181ms step_avg:99.85ms
step:143/1770 train_time:13280ms step_avg:99.85ms
step:144/1770 train_time:13381ms step_avg:99.86ms
step:145/1770 train_time:13482ms step_avg:99.87ms
step:146/1770 train_time:13582ms step_avg:99.87ms
step:147/1770 train_time:13681ms step_avg:99.86ms
step:148/1770 train_time:13782ms step_avg:99.87ms
step:149/1770 train_time:13882ms step_avg:99.87ms
step:150/1770 train_time:13983ms step_avg:99.88ms
step:151/1770 train_time:14084ms step_avg:99.89ms
step:152/1770 train_time:14184ms step_avg:99.89ms
step:153/1770 train_time:14284ms step_avg:99.89ms
step:154/1770 train_time:14383ms step_avg:99.88ms
step:155/1770 train_time:14484ms step_avg:99.89ms
step:156/1770 train_time:14583ms step_avg:99.88ms
step:157/1770 train_time:14683ms step_avg:99.89ms
step:158/1770 train_time:14783ms step_avg:99.88ms
step:159/1770 train_time:14883ms step_avg:99.89ms
step:160/1770 train_time:14984ms step_avg:99.89ms
step:161/1770 train_time:15083ms step_avg:99.88ms
step:162/1770 train_time:15182ms step_avg:99.88ms
step:163/1770 train_time:15282ms step_avg:99.88ms
step:164/1770 train_time:15382ms step_avg:99.88ms
step:165/1770 train_time:15482ms step_avg:99.88ms
step:166/1770 train_time:15582ms step_avg:99.88ms
step:167/1770 train_time:15681ms step_avg:99.88ms
step:168/1770 train_time:15781ms step_avg:99.88ms
step:169/1770 train_time:15881ms step_avg:99.88ms
step:170/1770 train_time:15982ms step_avg:99.89ms
step:171/1770 train_time:16081ms step_avg:99.88ms
step:172/1770 train_time:16182ms step_avg:99.89ms
step:173/1770 train_time:16282ms step_avg:99.89ms
step:174/1770 train_time:16382ms step_avg:99.89ms
step:175/1770 train_time:16481ms step_avg:99.89ms
step:176/1770 train_time:16581ms step_avg:99.89ms
step:177/1770 train_time:16681ms step_avg:99.89ms
step:178/1770 train_time:16781ms step_avg:99.89ms
step:179/1770 train_time:16881ms step_avg:99.89ms
step:180/1770 train_time:16982ms step_avg:99.89ms
step:181/1770 train_time:17082ms step_avg:99.89ms
step:182/1770 train_time:17182ms step_avg:99.89ms
step:183/1770 train_time:17282ms step_avg:99.90ms
step:184/1770 train_time:17381ms step_avg:99.89ms
step:185/1770 train_time:17481ms step_avg:99.89ms
step:186/1770 train_time:17581ms step_avg:99.89ms
step:187/1770 train_time:17681ms step_avg:99.89ms
step:188/1770 train_time:17781ms step_avg:99.89ms
step:189/1770 train_time:17882ms step_avg:99.90ms
step:190/1770 train_time:17982ms step_avg:99.90ms
step:191/1770 train_time:18082ms step_avg:99.90ms
step:192/1770 train_time:18182ms step_avg:99.90ms
step:193/1770 train_time:18282ms step_avg:99.90ms
step:194/1770 train_time:18382ms step_avg:99.90ms
step:195/1770 train_time:18481ms step_avg:99.90ms
step:196/1770 train_time:18581ms step_avg:99.90ms
step:197/1770 train_time:18681ms step_avg:99.90ms
step:198/1770 train_time:18781ms step_avg:99.90ms
step:199/1770 train_time:18881ms step_avg:99.90ms
step:200/1770 train_time:18982ms step_avg:99.91ms
step:201/1770 train_time:19082ms step_avg:99.90ms
step:202/1770 train_time:19182ms step_avg:99.90ms
step:203/1770 train_time:19281ms step_avg:99.90ms
step:204/1770 train_time:19381ms step_avg:99.90ms
step:205/1770 train_time:19481ms step_avg:99.90ms
step:206/1770 train_time:19580ms step_avg:99.90ms
step:207/1770 train_time:19681ms step_avg:99.90ms
step:208/1770 train_time:19781ms step_avg:99.90ms
step:209/1770 train_time:19880ms step_avg:99.90ms
step:210/1770 train_time:19981ms step_avg:99.91ms
step:211/1770 train_time:20082ms step_avg:99.91ms
step:212/1770 train_time:20182ms step_avg:99.91ms
step:213/1770 train_time:20283ms step_avg:99.92ms
step:214/1770 train_time:20383ms step_avg:99.91ms
step:215/1770 train_time:20482ms step_avg:99.91ms
step:216/1770 train_time:20582ms step_avg:99.91ms
step:217/1770 train_time:20682ms step_avg:99.91ms
step:218/1770 train_time:20782ms step_avg:99.91ms
step:219/1770 train_time:20882ms step_avg:99.91ms
step:220/1770 train_time:20982ms step_avg:99.91ms
step:221/1770 train_time:21083ms step_avg:99.92ms
step:222/1770 train_time:21182ms step_avg:99.91ms
step:223/1770 train_time:21282ms step_avg:99.92ms
step:224/1770 train_time:21383ms step_avg:99.92ms
step:225/1770 train_time:21483ms step_avg:99.92ms
step:226/1770 train_time:21583ms step_avg:99.92ms
step:227/1770 train_time:21683ms step_avg:99.92ms
step:228/1770 train_time:21784ms step_avg:99.93ms
step:229/1770 train_time:21883ms step_avg:99.92ms
step:230/1770 train_time:21984ms step_avg:99.93ms
step:231/1770 train_time:22083ms step_avg:99.92ms
step:232/1770 train_time:22183ms step_avg:99.92ms
step:233/1770 train_time:22282ms step_avg:99.92ms
step:234/1770 train_time:22383ms step_avg:99.92ms
step:235/1770 train_time:22485ms step_avg:99.94ms
step:236/1770 train_time:22584ms step_avg:99.93ms
step:237/1770 train_time:22684ms step_avg:99.93ms
step:238/1770 train_time:22784ms step_avg:99.93ms
step:239/1770 train_time:22884ms step_avg:99.93ms
step:240/1770 train_time:22983ms step_avg:99.93ms
step:241/1770 train_time:23083ms step_avg:99.93ms
step:242/1770 train_time:23183ms step_avg:99.93ms
step:243/1770 train_time:23283ms step_avg:99.93ms
step:244/1770 train_time:23382ms step_avg:99.92ms
step:245/1770 train_time:23482ms step_avg:99.92ms
step:246/1770 train_time:23582ms step_avg:99.92ms
step:247/1770 train_time:23682ms step_avg:99.92ms
step:248/1770 train_time:23782ms step_avg:99.92ms
step:249/1770 train_time:23883ms step_avg:99.93ms
step:250/1770 train_time:23985ms step_avg:99.94ms
step:250/1770 val_loss:4.1001 train_time:24083ms step_avg:100.35ms
step:251/1770 train_time:24103ms step_avg:100.01ms
step:252/1770 train_time:24198ms step_avg:99.99ms
step:253/1770 train_time:24301ms step_avg:100.01ms
step:254/1770 train_time:24404ms step_avg:100.02ms
step:255/1770 train_time:24506ms step_avg:100.02ms
step:256/1770 train_time:24605ms step_avg:100.02ms
step:257/1770 train_time:24705ms step_avg:100.02ms
step:258/1770 train_time:24805ms step_avg:100.02ms
step:259/1770 train_time:24904ms step_avg:100.02ms
step:260/1770 train_time:25004ms step_avg:100.02ms
step:261/1770 train_time:25104ms step_avg:100.02ms
step:262/1770 train_time:25205ms step_avg:100.02ms
step:263/1770 train_time:25304ms step_avg:100.02ms
step:264/1770 train_time:25405ms step_avg:100.02ms
step:265/1770 train_time:25506ms step_avg:100.02ms
step:266/1770 train_time:25606ms step_avg:100.02ms
step:267/1770 train_time:25706ms step_avg:100.02ms
step:268/1770 train_time:25806ms step_avg:100.02ms
step:269/1770 train_time:25906ms step_avg:100.02ms
step:270/1770 train_time:26006ms step_avg:100.02ms
step:271/1770 train_time:26106ms step_avg:100.02ms
step:272/1770 train_time:26206ms step_avg:100.02ms
step:273/1770 train_time:26306ms step_avg:100.02ms
step:274/1770 train_time:26406ms step_avg:100.02ms
step:275/1770 train_time:26506ms step_avg:100.02ms
step:276/1770 train_time:26606ms step_avg:100.02ms
step:277/1770 train_time:26706ms step_avg:100.02ms
step:278/1770 train_time:26807ms step_avg:100.03ms
step:279/1770 train_time:26907ms step_avg:100.02ms
step:280/1770 train_time:27006ms step_avg:100.02ms
step:281/1770 train_time:27106ms step_avg:100.02ms
step:282/1770 train_time:27206ms step_avg:100.02ms
step:283/1770 train_time:27306ms step_avg:100.02ms
step:284/1770 train_time:27406ms step_avg:100.02ms
step:285/1770 train_time:27506ms step_avg:100.02ms
step:286/1770 train_time:27606ms step_avg:100.02ms
step:287/1770 train_time:27706ms step_avg:100.02ms
step:288/1770 train_time:27807ms step_avg:100.02ms
step:289/1770 train_time:27907ms step_avg:100.02ms
step:290/1770 train_time:28007ms step_avg:100.02ms
step:291/1770 train_time:28107ms step_avg:100.02ms
step:292/1770 train_time:28207ms step_avg:100.02ms
step:293/1770 train_time:28307ms step_avg:100.02ms
step:294/1770 train_time:28407ms step_avg:100.02ms
step:295/1770 train_time:28507ms step_avg:100.02ms
step:296/1770 train_time:28607ms step_avg:100.02ms
step:297/1770 train_time:28707ms step_avg:100.02ms
step:298/1770 train_time:28807ms step_avg:100.02ms
step:299/1770 train_time:28906ms step_avg:100.02ms
step:300/1770 train_time:29006ms step_avg:100.02ms
step:301/1770 train_time:29107ms step_avg:100.02ms
step:302/1770 train_time:29207ms step_avg:100.02ms
step:303/1770 train_time:29306ms step_avg:100.02ms
step:304/1770 train_time:29406ms step_avg:100.02ms
step:305/1770 train_time:29506ms step_avg:100.02ms
step:306/1770 train_time:29607ms step_avg:100.02ms
step:307/1770 train_time:29707ms step_avg:100.02ms
step:308/1770 train_time:29807ms step_avg:100.02ms
step:309/1770 train_time:29907ms step_avg:100.02ms
step:310/1770 train_time:30007ms step_avg:100.02ms
step:311/1770 train_time:30107ms step_avg:100.02ms
step:312/1770 train_time:30207ms step_avg:100.02ms
step:313/1770 train_time:30306ms step_avg:100.02ms
step:314/1770 train_time:30406ms step_avg:100.02ms
step:315/1770 train_time:30506ms step_avg:100.02ms
step:316/1770 train_time:30606ms step_avg:100.02ms
step:317/1770 train_time:30706ms step_avg:100.02ms
step:318/1770 train_time:30807ms step_avg:100.02ms
step:319/1770 train_time:30907ms step_avg:100.02ms
step:320/1770 train_time:31007ms step_avg:100.02ms
step:321/1770 train_time:31107ms step_avg:100.02ms
step:322/1770 train_time:31207ms step_avg:100.02ms
step:323/1770 train_time:31307ms step_avg:100.02ms
step:324/1770 train_time:31407ms step_avg:100.02ms
step:325/1770 train_time:31506ms step_avg:100.02ms
step:326/1770 train_time:31606ms step_avg:100.02ms
step:327/1770 train_time:31706ms step_avg:100.02ms
step:328/1770 train_time:31806ms step_avg:100.02ms
step:329/1770 train_time:31906ms step_avg:100.02ms
step:330/1770 train_time:32006ms step_avg:100.02ms
step:331/1770 train_time:32106ms step_avg:100.02ms
step:332/1770 train_time:32207ms step_avg:100.02ms
step:333/1770 train_time:32307ms step_avg:100.02ms
step:334/1770 train_time:32407ms step_avg:100.02ms
step:335/1770 train_time:32506ms step_avg:100.02ms
step:336/1770 train_time:32606ms step_avg:100.02ms
step:337/1770 train_time:32706ms step_avg:100.02ms
step:338/1770 train_time:32806ms step_avg:100.02ms
step:339/1770 train_time:32906ms step_avg:100.02ms
step:340/1770 train_time:33007ms step_avg:100.02ms
step:341/1770 train_time:33106ms step_avg:100.02ms
step:342/1770 train_time:33206ms step_avg:100.02ms
step:343/1770 train_time:33306ms step_avg:100.02ms
step:344/1770 train_time:33406ms step_avg:100.02ms
step:345/1770 train_time:33506ms step_avg:100.02ms
step:346/1770 train_time:33606ms step_avg:100.02ms
step:347/1770 train_time:33706ms step_avg:100.02ms
step:348/1770 train_time:33806ms step_avg:100.02ms
step:349/1770 train_time:33906ms step_avg:100.02ms
step:350/1770 train_time:34006ms step_avg:100.02ms
step:351/1770 train_time:34106ms step_avg:100.02ms
step:352/1770 train_time:34206ms step_avg:100.02ms
step:353/1770 train_time:34306ms step_avg:100.02ms
step:354/1770 train_time:34406ms step_avg:100.02ms
step:355/1770 train_time:34506ms step_avg:100.02ms
step:356/1770 train_time:34606ms step_avg:100.02ms
step:357/1770 train_time:34706ms step_avg:100.02ms
step:358/1770 train_time:34806ms step_avg:100.02ms
step:359/1770 train_time:34905ms step_avg:100.02ms
step:360/1770 train_time:35006ms step_avg:100.02ms
step:361/1770 train_time:35106ms step_avg:100.02ms
step:362/1770 train_time:35205ms step_avg:100.02ms
step:363/1770 train_time:35305ms step_avg:100.01ms
step:364/1770 train_time:35405ms step_avg:100.01ms
step:365/1770 train_time:35505ms step_avg:100.01ms
step:366/1770 train_time:35605ms step_avg:100.01ms
step:367/1770 train_time:35705ms step_avg:100.02ms
step:368/1770 train_time:35805ms step_avg:100.01ms
step:369/1770 train_time:35905ms step_avg:100.01ms
step:370/1770 train_time:36006ms step_avg:100.02ms
step:371/1770 train_time:36106ms step_avg:100.02ms
step:372/1770 train_time:36206ms step_avg:100.02ms
step:373/1770 train_time:36305ms step_avg:100.01ms
step:374/1770 train_time:36405ms step_avg:100.01ms
step:375/1770 train_time:36505ms step_avg:100.01ms
step:375/1770 val_loss:3.8931 train_time:36604ms step_avg:100.28ms
step:376/1770 train_time:36623ms step_avg:100.06ms
step:377/1770 train_time:36716ms step_avg:100.04ms
step:378/1770 train_time:36821ms step_avg:100.06ms
step:379/1770 train_time:36925ms step_avg:100.07ms
step:380/1770 train_time:37026ms step_avg:100.07ms
step:381/1770 train_time:37126ms step_avg:100.07ms
step:382/1770 train_time:37226ms step_avg:100.07ms
step:383/1770 train_time:37326ms step_avg:100.07ms
step:384/1770 train_time:37426ms step_avg:100.07ms
step:385/1770 train_time:37526ms step_avg:100.07ms
step:386/1770 train_time:37626ms step_avg:100.07ms
step:387/1770 train_time:37726ms step_avg:100.07ms
step:388/1770 train_time:37826ms step_avg:100.07ms
step:389/1770 train_time:37926ms step_avg:100.07ms
step:390/1770 train_time:38027ms step_avg:100.07ms
step:391/1770 train_time:38127ms step_avg:100.07ms
step:392/1770 train_time:38227ms step_avg:100.07ms
step:393/1770 train_time:38327ms step_avg:100.07ms
step:394/1770 train_time:38426ms step_avg:100.07ms
step:395/1770 train_time:38526ms step_avg:100.07ms
step:396/1770 train_time:38628ms step_avg:100.07ms
step:397/1770 train_time:38730ms step_avg:100.08ms
step:398/1770 train_time:38832ms step_avg:100.08ms
step:399/1770 train_time:38934ms step_avg:100.09ms
step:400/1770 train_time:39036ms step_avg:100.09ms
step:401/1770 train_time:39139ms step_avg:100.10ms
step:402/1770 train_time:39242ms step_avg:100.11ms
step:403/1770 train_time:39344ms step_avg:100.11ms
step:404/1770 train_time:39446ms step_avg:100.12ms
step:405/1770 train_time:39548ms step_avg:100.12ms
step:406/1770 train_time:39650ms step_avg:100.13ms
step:407/1770 train_time:39753ms step_avg:100.13ms
step:408/1770 train_time:39855ms step_avg:100.14ms
step:409/1770 train_time:39958ms step_avg:100.14ms
step:410/1770 train_time:40060ms step_avg:100.15ms
step:411/1770 train_time:40163ms step_avg:100.16ms
step:412/1770 train_time:40266ms step_avg:100.16ms
step:413/1770 train_time:40368ms step_avg:100.17ms
step:414/1770 train_time:40469ms step_avg:100.17ms
step:415/1770 train_time:40571ms step_avg:100.18ms
step:416/1770 train_time:40674ms step_avg:100.18ms
step:417/1770 train_time:40776ms step_avg:100.19ms
step:418/1770 train_time:40879ms step_avg:100.19ms
step:419/1770 train_time:40981ms step_avg:100.20ms
step:420/1770 train_time:41083ms step_avg:100.20ms
step:421/1770 train_time:41185ms step_avg:100.21ms
step:422/1770 train_time:41287ms step_avg:100.21ms
step:423/1770 train_time:41389ms step_avg:100.22ms
step:424/1770 train_time:41491ms step_avg:100.22ms
step:425/1770 train_time:41594ms step_avg:100.23ms
step:426/1770 train_time:41696ms step_avg:100.23ms
step:427/1770 train_time:41799ms step_avg:100.24ms
step:428/1770 train_time:41902ms step_avg:100.24ms
step:429/1770 train_time:42005ms step_avg:100.25ms
step:430/1770 train_time:42107ms step_avg:100.25ms
step:431/1770 train_time:42209ms step_avg:100.26ms
step:432/1770 train_time:42311ms step_avg:100.26ms
step:433/1770 train_time:42413ms step_avg:100.27ms
step:434/1770 train_time:42515ms step_avg:100.27ms
step:435/1770 train_time:42618ms step_avg:100.28ms
step:436/1770 train_time:42720ms step_avg:100.28ms
step:437/1770 train_time:42823ms step_avg:100.29ms
step:438/1770 train_time:42926ms step_avg:100.29ms
step:439/1770 train_time:43028ms step_avg:100.30ms
step:440/1770 train_time:43129ms step_avg:100.30ms
step:441/1770 train_time:43232ms step_avg:100.31ms
step:442/1770 train_time:43334ms step_avg:100.31ms
step:443/1770 train_time:43436ms step_avg:100.31ms
step:444/1770 train_time:43539ms step_avg:100.32ms
step:445/1770 train_time:43642ms step_avg:100.33ms
step:446/1770 train_time:43744ms step_avg:100.33ms
step:447/1770 train_time:43847ms step_avg:100.34ms
step:448/1770 train_time:43948ms step_avg:100.34ms
step:449/1770 train_time:44051ms step_avg:100.34ms
step:450/1770 train_time:44154ms step_avg:100.35ms
step:451/1770 train_time:44256ms step_avg:100.35ms
step:452/1770 train_time:44358ms step_avg:100.36ms
step:453/1770 train_time:44461ms step_avg:100.36ms
step:454/1770 train_time:44564ms step_avg:100.37ms
step:455/1770 train_time:44666ms step_avg:100.37ms
step:456/1770 train_time:44768ms step_avg:100.38ms
step:457/1770 train_time:44870ms step_avg:100.38ms
step:458/1770 train_time:44971ms step_avg:100.38ms
step:459/1770 train_time:45074ms step_avg:100.39ms
step:460/1770 train_time:45176ms step_avg:100.39ms
step:461/1770 train_time:45279ms step_avg:100.40ms
step:462/1770 train_time:45381ms step_avg:100.40ms
step:463/1770 train_time:45483ms step_avg:100.40ms
step:464/1770 train_time:45586ms step_avg:100.41ms
step:465/1770 train_time:45688ms step_avg:100.41ms
step:466/1770 train_time:45790ms step_avg:100.42ms
step:467/1770 train_time:45892ms step_avg:100.42ms
step:468/1770 train_time:45994ms step_avg:100.42ms
step:469/1770 train_time:46096ms step_avg:100.43ms
step:470/1770 train_time:46199ms step_avg:100.43ms
step:471/1770 train_time:46301ms step_avg:100.44ms
step:472/1770 train_time:46404ms step_avg:100.44ms
step:473/1770 train_time:46507ms step_avg:100.45ms
step:474/1770 train_time:46608ms step_avg:100.45ms
step:475/1770 train_time:46710ms step_avg:100.45ms
step:476/1770 train_time:46813ms step_avg:100.46ms
step:477/1770 train_time:46915ms step_avg:100.46ms
step:478/1770 train_time:47017ms step_avg:100.46ms
step:479/1770 train_time:47119ms step_avg:100.47ms
step:480/1770 train_time:47222ms step_avg:100.47ms
step:481/1770 train_time:47326ms step_avg:100.48ms
step:482/1770 train_time:47427ms step_avg:100.48ms
step:483/1770 train_time:47529ms step_avg:100.48ms
step:484/1770 train_time:47630ms step_avg:100.49ms
step:485/1770 train_time:47733ms step_avg:100.49ms
step:486/1770 train_time:47835ms step_avg:100.49ms
step:487/1770 train_time:47938ms step_avg:100.50ms
step:488/1770 train_time:48041ms step_avg:100.50ms
step:489/1770 train_time:48144ms step_avg:100.51ms
step:490/1770 train_time:48246ms step_avg:100.51ms
step:491/1770 train_time:48348ms step_avg:100.52ms
step:492/1770 train_time:48451ms step_avg:100.52ms
step:493/1770 train_time:48553ms step_avg:100.52ms
step:494/1770 train_time:48656ms step_avg:100.53ms
step:495/1770 train_time:48758ms step_avg:100.53ms
step:496/1770 train_time:48860ms step_avg:100.54ms
step:497/1770 train_time:48963ms step_avg:100.54ms
step:498/1770 train_time:49066ms step_avg:100.55ms
step:499/1770 train_time:49168ms step_avg:100.55ms
step:500/1770 train_time:49270ms step_avg:100.55ms
step:500/1770 val_loss:3.7487 train_time:49371ms step_avg:100.76ms
step:501/1770 train_time:49389ms step_avg:100.59ms
step:502/1770 train_time:49486ms step_avg:100.58ms
step:503/1770 train_time:49593ms step_avg:100.59ms
step:504/1770 train_time:49696ms step_avg:100.60ms
step:505/1770 train_time:49798ms step_avg:100.60ms
step:506/1770 train_time:49900ms step_avg:100.61ms
step:507/1770 train_time:50003ms step_avg:100.61ms
step:508/1770 train_time:50106ms step_avg:100.62ms
step:509/1770 train_time:50209ms step_avg:100.62ms
step:510/1770 train_time:50311ms step_avg:100.62ms
step:511/1770 train_time:50413ms step_avg:100.62ms
step:512/1770 train_time:50515ms step_avg:100.63ms
step:513/1770 train_time:50616ms step_avg:100.63ms
step:514/1770 train_time:50718ms step_avg:100.63ms
step:515/1770 train_time:50820ms step_avg:100.63ms
step:516/1770 train_time:50923ms step_avg:100.64ms
step:517/1770 train_time:51026ms step_avg:100.64ms
step:518/1770 train_time:51129ms step_avg:100.65ms
step:519/1770 train_time:51231ms step_avg:100.65ms
step:520/1770 train_time:51333ms step_avg:100.65ms
step:521/1770 train_time:51436ms step_avg:100.66ms
step:522/1770 train_time:51538ms step_avg:100.66ms
step:523/1770 train_time:51640ms step_avg:100.66ms
step:524/1770 train_time:51742ms step_avg:100.67ms
step:525/1770 train_time:51845ms step_avg:100.67ms
step:526/1770 train_time:51947ms step_avg:100.67ms
step:527/1770 train_time:52050ms step_avg:100.68ms
step:528/1770 train_time:52152ms step_avg:100.68ms
step:529/1770 train_time:52255ms step_avg:100.68ms
step:530/1770 train_time:52358ms step_avg:100.69ms
step:531/1770 train_time:52460ms step_avg:100.69ms
step:532/1770 train_time:52562ms step_avg:100.69ms
step:533/1770 train_time:52665ms step_avg:100.70ms
step:534/1770 train_time:52769ms step_avg:100.70ms
step:535/1770 train_time:52872ms step_avg:100.71ms
step:536/1770 train_time:52974ms step_avg:100.71ms
step:537/1770 train_time:53077ms step_avg:100.71ms
step:538/1770 train_time:53179ms step_avg:100.72ms
step:539/1770 train_time:53281ms step_avg:100.72ms
step:540/1770 train_time:53385ms step_avg:100.73ms
step:541/1770 train_time:53488ms step_avg:100.73ms
step:542/1770 train_time:53591ms step_avg:100.73ms
step:543/1770 train_time:53693ms step_avg:100.74ms
step:544/1770 train_time:53795ms step_avg:100.74ms
step:545/1770 train_time:53898ms step_avg:100.74ms
step:546/1770 train_time:54001ms step_avg:100.75ms
step:547/1770 train_time:54103ms step_avg:100.75ms
step:548/1770 train_time:54206ms step_avg:100.75ms
step:549/1770 train_time:54309ms step_avg:100.76ms
step:550/1770 train_time:54412ms step_avg:100.76ms
step:551/1770 train_time:54514ms step_avg:100.76ms
step:552/1770 train_time:54616ms step_avg:100.77ms
step:553/1770 train_time:54718ms step_avg:100.77ms
step:554/1770 train_time:54820ms step_avg:100.77ms
step:555/1770 train_time:54924ms step_avg:100.78ms
step:556/1770 train_time:55027ms step_avg:100.78ms
step:557/1770 train_time:55129ms step_avg:100.78ms
step:558/1770 train_time:55231ms step_avg:100.79ms
step:559/1770 train_time:55334ms step_avg:100.79ms
step:560/1770 train_time:55437ms step_avg:100.79ms
step:561/1770 train_time:55539ms step_avg:100.80ms
step:562/1770 train_time:55642ms step_avg:100.80ms
step:563/1770 train_time:55745ms step_avg:100.81ms
step:564/1770 train_time:55848ms step_avg:100.81ms
step:565/1770 train_time:55951ms step_avg:100.81ms
step:566/1770 train_time:56053ms step_avg:100.81ms
step:567/1770 train_time:56155ms step_avg:100.82ms
step:568/1770 train_time:56258ms step_avg:100.82ms
step:569/1770 train_time:56361ms step_avg:100.82ms
step:570/1770 train_time:56464ms step_avg:100.83ms
step:571/1770 train_time:56567ms step_avg:100.83ms
step:572/1770 train_time:56670ms step_avg:100.84ms
step:573/1770 train_time:56772ms step_avg:100.84ms
step:574/1770 train_time:56875ms step_avg:100.84ms
step:575/1770 train_time:56978ms step_avg:100.85ms
step:576/1770 train_time:57081ms step_avg:100.85ms
step:577/1770 train_time:57183ms step_avg:100.85ms
step:578/1770 train_time:57287ms step_avg:100.86ms
step:579/1770 train_time:57390ms step_avg:100.86ms
step:580/1770 train_time:57493ms step_avg:100.86ms
step:581/1770 train_time:57595ms step_avg:100.87ms
step:582/1770 train_time:57698ms step_avg:100.87ms
step:583/1770 train_time:57800ms step_avg:100.87ms
step:584/1770 train_time:57903ms step_avg:100.88ms
step:585/1770 train_time:58006ms step_avg:100.88ms
step:586/1770 train_time:58109ms step_avg:100.88ms
step:587/1770 train_time:58211ms step_avg:100.89ms
step:588/1770 train_time:58313ms step_avg:100.89ms
step:589/1770 train_time:58416ms step_avg:100.89ms
step:590/1770 train_time:58519ms step_avg:100.89ms
step:591/1770 train_time:58621ms step_avg:100.90ms
step:592/1770 train_time:58725ms step_avg:100.90ms
step:593/1770 train_time:58828ms step_avg:100.91ms
step:594/1770 train_time:58931ms step_avg:100.91ms
step:595/1770 train_time:59033ms step_avg:100.91ms
step:596/1770 train_time:59136ms step_avg:100.91ms
step:597/1770 train_time:59239ms step_avg:100.92ms
step:598/1770 train_time:59341ms step_avg:100.92ms
step:599/1770 train_time:59444ms step_avg:100.92ms
step:600/1770 train_time:59547ms step_avg:100.93ms
step:601/1770 train_time:59650ms step_avg:100.93ms
step:602/1770 train_time:59753ms step_avg:100.93ms
step:603/1770 train_time:59856ms step_avg:100.94ms
step:604/1770 train_time:59958ms step_avg:100.94ms
step:605/1770 train_time:60060ms step_avg:100.94ms
step:606/1770 train_time:60163ms step_avg:100.95ms
step:607/1770 train_time:60267ms step_avg:100.95ms
step:608/1770 train_time:60370ms step_avg:100.95ms
step:609/1770 train_time:60472ms step_avg:100.96ms
step:610/1770 train_time:60575ms step_avg:100.96ms
step:611/1770 train_time:60678ms step_avg:100.96ms
step:612/1770 train_time:60780ms step_avg:100.96ms
step:613/1770 train_time:60884ms step_avg:100.97ms
step:614/1770 train_time:60987ms step_avg:100.97ms
step:615/1770 train_time:61089ms step_avg:100.97ms
step:616/1770 train_time:61192ms step_avg:100.98ms
step:617/1770 train_time:61294ms step_avg:100.98ms
step:618/1770 train_time:61396ms step_avg:100.98ms
step:619/1770 train_time:61499ms step_avg:100.98ms
step:620/1770 train_time:61601ms step_avg:100.98ms
step:621/1770 train_time:61704ms step_avg:100.99ms
step:622/1770 train_time:61807ms step_avg:100.99ms
step:623/1770 train_time:61910ms step_avg:100.99ms
step:624/1770 train_time:62012ms step_avg:101.00ms
step:625/1770 train_time:62114ms step_avg:101.00ms
step:625/1770 val_loss:3.6614 train_time:62216ms step_avg:101.16ms
step:626/1770 train_time:62234ms step_avg:101.03ms
step:627/1770 train_time:62334ms step_avg:101.03ms
step:628/1770 train_time:62440ms step_avg:101.04ms
step:629/1770 train_time:62543ms step_avg:101.04ms
step:630/1770 train_time:62646ms step_avg:101.04ms
step:631/1770 train_time:62750ms step_avg:101.05ms
step:632/1770 train_time:62853ms step_avg:101.05ms
step:633/1770 train_time:62956ms step_avg:101.05ms
step:634/1770 train_time:63058ms step_avg:101.05ms
step:635/1770 train_time:63160ms step_avg:101.06ms
step:636/1770 train_time:63262ms step_avg:101.06ms
step:637/1770 train_time:63365ms step_avg:101.06ms
step:638/1770 train_time:63468ms step_avg:101.06ms
step:639/1770 train_time:63572ms step_avg:101.07ms
step:640/1770 train_time:63675ms step_avg:101.07ms
step:641/1770 train_time:63777ms step_avg:101.07ms
step:642/1770 train_time:63879ms step_avg:101.07ms
step:643/1770 train_time:63982ms step_avg:101.08ms
step:644/1770 train_time:64085ms step_avg:101.08ms
step:645/1770 train_time:64188ms step_avg:101.08ms
step:646/1770 train_time:64291ms step_avg:101.09ms
step:647/1770 train_time:64394ms step_avg:101.09ms
step:648/1770 train_time:64497ms step_avg:101.09ms
step:649/1770 train_time:64598ms step_avg:101.09ms
step:650/1770 train_time:64701ms step_avg:101.09ms
step:651/1770 train_time:64803ms step_avg:101.10ms
step:652/1770 train_time:64906ms step_avg:101.10ms
step:653/1770 train_time:65009ms step_avg:101.10ms
step:654/1770 train_time:65112ms step_avg:101.11ms
step:655/1770 train_time:65215ms step_avg:101.11ms
step:656/1770 train_time:65317ms step_avg:101.11ms
step:657/1770 train_time:65420ms step_avg:101.11ms
step:658/1770 train_time:65525ms step_avg:101.12ms
step:659/1770 train_time:65629ms step_avg:101.12ms
step:660/1770 train_time:65733ms step_avg:101.13ms
step:661/1770 train_time:65838ms step_avg:101.13ms
step:662/1770 train_time:65942ms step_avg:101.14ms
step:663/1770 train_time:66046ms step_avg:101.14ms
step:664/1770 train_time:66152ms step_avg:101.15ms
step:665/1770 train_time:66256ms step_avg:101.15ms
step:666/1770 train_time:66360ms step_avg:101.16ms
step:667/1770 train_time:66465ms step_avg:101.16ms
step:668/1770 train_time:66569ms step_avg:101.17ms
step:669/1770 train_time:66674ms step_avg:101.17ms
step:670/1770 train_time:66779ms step_avg:101.18ms
step:671/1770 train_time:66883ms step_avg:101.18ms
step:672/1770 train_time:66987ms step_avg:101.19ms
step:673/1770 train_time:67092ms step_avg:101.19ms
step:674/1770 train_time:67196ms step_avg:101.20ms
step:675/1770 train_time:67300ms step_avg:101.20ms
step:676/1770 train_time:67404ms step_avg:101.21ms
step:677/1770 train_time:67509ms step_avg:101.21ms
step:678/1770 train_time:67613ms step_avg:101.22ms
step:679/1770 train_time:67718ms step_avg:101.22ms
step:680/1770 train_time:67822ms step_avg:101.23ms
step:681/1770 train_time:67926ms step_avg:101.23ms
step:682/1770 train_time:68031ms step_avg:101.24ms
step:683/1770 train_time:68136ms step_avg:101.24ms
step:684/1770 train_time:68240ms step_avg:101.25ms
step:685/1770 train_time:68344ms step_avg:101.25ms
step:686/1770 train_time:68449ms step_avg:101.26ms
step:687/1770 train_time:68553ms step_avg:101.26ms
step:688/1770 train_time:68657ms step_avg:101.26ms
step:689/1770 train_time:68761ms step_avg:101.27ms
step:690/1770 train_time:68866ms step_avg:101.27ms
step:691/1770 train_time:68971ms step_avg:101.28ms
step:692/1770 train_time:69076ms step_avg:101.28ms
step:693/1770 train_time:69180ms step_avg:101.29ms
step:694/1770 train_time:69285ms step_avg:101.29ms
step:695/1770 train_time:69389ms step_avg:101.30ms
step:696/1770 train_time:69494ms step_avg:101.30ms
step:697/1770 train_time:69598ms step_avg:101.31ms
step:698/1770 train_time:69702ms step_avg:101.31ms
step:699/1770 train_time:69806ms step_avg:101.31ms
step:700/1770 train_time:69911ms step_avg:101.32ms
step:701/1770 train_time:70016ms step_avg:101.33ms
step:702/1770 train_time:70120ms step_avg:101.33ms
step:703/1770 train_time:70224ms step_avg:101.33ms
step:704/1770 train_time:70329ms step_avg:101.34ms
step:705/1770 train_time:70434ms step_avg:101.34ms
step:706/1770 train_time:70538ms step_avg:101.35ms
step:707/1770 train_time:70642ms step_avg:101.35ms
step:708/1770 train_time:70746ms step_avg:101.36ms
step:709/1770 train_time:70851ms step_avg:101.36ms
step:710/1770 train_time:70956ms step_avg:101.37ms
step:711/1770 train_time:71059ms step_avg:101.37ms
step:712/1770 train_time:71164ms step_avg:101.37ms
step:713/1770 train_time:71268ms step_avg:101.38ms
step:714/1770 train_time:71373ms step_avg:101.38ms
step:715/1770 train_time:71478ms step_avg:101.39ms
step:716/1770 train_time:71583ms step_avg:101.39ms
step:717/1770 train_time:71687ms step_avg:101.40ms
step:718/1770 train_time:71792ms step_avg:101.40ms
step:719/1770 train_time:71897ms step_avg:101.41ms
step:720/1770 train_time:72000ms step_avg:101.41ms
step:721/1770 train_time:72105ms step_avg:101.41ms
step:722/1770 train_time:72208ms step_avg:101.42ms
step:723/1770 train_time:72313ms step_avg:101.42ms
step:724/1770 train_time:72418ms step_avg:101.43ms
step:725/1770 train_time:72522ms step_avg:101.43ms
step:726/1770 train_time:72626ms step_avg:101.43ms
step:727/1770 train_time:72731ms step_avg:101.44ms
step:728/1770 train_time:72836ms step_avg:101.44ms
step:729/1770 train_time:72940ms step_avg:101.45ms
step:730/1770 train_time:73044ms step_avg:101.45ms
step:731/1770 train_time:73148ms step_avg:101.45ms
step:732/1770 train_time:73253ms step_avg:101.46ms
step:733/1770 train_time:73358ms step_avg:101.46ms
step:734/1770 train_time:73462ms step_avg:101.47ms
step:735/1770 train_time:73566ms step_avg:101.47ms
step:736/1770 train_time:73671ms step_avg:101.48ms
step:737/1770 train_time:73776ms step_avg:101.48ms
step:738/1770 train_time:73880ms step_avg:101.48ms
step:739/1770 train_time:73984ms step_avg:101.49ms
step:740/1770 train_time:74088ms step_avg:101.49ms
step:741/1770 train_time:74193ms step_avg:101.50ms
step:742/1770 train_time:74298ms step_avg:101.50ms
step:743/1770 train_time:74401ms step_avg:101.50ms
step:744/1770 train_time:74506ms step_avg:101.51ms
step:745/1770 train_time:74610ms step_avg:101.51ms
step:746/1770 train_time:74715ms step_avg:101.51ms
step:747/1770 train_time:74819ms step_avg:101.52ms
step:748/1770 train_time:74923ms step_avg:101.52ms
step:749/1770 train_time:75027ms step_avg:101.53ms
step:750/1770 train_time:75132ms step_avg:101.53ms
step:750/1770 val_loss:3.5994 train_time:75235ms step_avg:101.67ms
step:751/1770 train_time:75253ms step_avg:101.56ms
step:752/1770 train_time:75353ms step_avg:101.55ms
step:753/1770 train_time:75461ms step_avg:101.56ms
step:754/1770 train_time:75566ms step_avg:101.57ms
step:755/1770 train_time:75671ms step_avg:101.57ms
step:756/1770 train_time:75775ms step_avg:101.58ms
step:757/1770 train_time:75880ms step_avg:101.58ms
step:758/1770 train_time:75985ms step_avg:101.58ms
step:759/1770 train_time:76088ms step_avg:101.59ms
step:760/1770 train_time:76193ms step_avg:101.59ms
step:761/1770 train_time:76297ms step_avg:101.59ms
step:762/1770 train_time:76402ms step_avg:101.60ms
step:763/1770 train_time:76506ms step_avg:101.60ms
step:764/1770 train_time:76611ms step_avg:101.61ms
step:765/1770 train_time:76715ms step_avg:101.61ms
step:766/1770 train_time:76821ms step_avg:101.61ms
step:767/1770 train_time:76925ms step_avg:101.62ms
step:768/1770 train_time:77030ms step_avg:101.62ms
step:769/1770 train_time:77135ms step_avg:101.63ms
step:770/1770 train_time:77240ms step_avg:101.63ms
step:771/1770 train_time:77345ms step_avg:101.64ms
step:772/1770 train_time:77448ms step_avg:101.64ms
step:773/1770 train_time:77553ms step_avg:101.64ms
step:774/1770 train_time:77658ms step_avg:101.65ms
step:775/1770 train_time:77763ms step_avg:101.65ms
step:776/1770 train_time:77867ms step_avg:101.65ms
step:777/1770 train_time:77972ms step_avg:101.66ms
step:778/1770 train_time:78075ms step_avg:101.66ms
step:779/1770 train_time:78180ms step_avg:101.66ms
step:780/1770 train_time:78285ms step_avg:101.67ms
step:781/1770 train_time:78389ms step_avg:101.67ms
step:782/1770 train_time:78493ms step_avg:101.67ms
step:783/1770 train_time:78597ms step_avg:101.68ms
step:784/1770 train_time:78702ms step_avg:101.68ms
step:785/1770 train_time:78806ms step_avg:101.68ms
step:786/1770 train_time:78910ms step_avg:101.69ms
step:787/1770 train_time:79013ms step_avg:101.69ms
step:788/1770 train_time:79118ms step_avg:101.69ms
step:789/1770 train_time:79223ms step_avg:101.70ms
step:790/1770 train_time:79327ms step_avg:101.70ms
step:791/1770 train_time:79432ms step_avg:101.71ms
step:792/1770 train_time:79536ms step_avg:101.71ms
step:793/1770 train_time:79641ms step_avg:101.71ms
step:794/1770 train_time:79747ms step_avg:101.72ms
step:795/1770 train_time:79851ms step_avg:101.72ms
step:796/1770 train_time:79955ms step_avg:101.72ms
step:797/1770 train_time:80061ms step_avg:101.73ms
step:798/1770 train_time:80165ms step_avg:101.73ms
step:799/1770 train_time:80269ms step_avg:101.74ms
step:800/1770 train_time:80374ms step_avg:101.74ms
step:801/1770 train_time:80479ms step_avg:101.74ms
step:802/1770 train_time:80584ms step_avg:101.75ms
step:803/1770 train_time:80688ms step_avg:101.75ms
step:804/1770 train_time:80793ms step_avg:101.75ms
step:805/1770 train_time:80898ms step_avg:101.76ms
step:806/1770 train_time:81003ms step_avg:101.76ms
step:807/1770 train_time:81108ms step_avg:101.77ms
step:808/1770 train_time:81213ms step_avg:101.77ms
step:809/1770 train_time:81318ms step_avg:101.77ms
step:810/1770 train_time:81423ms step_avg:101.78ms
step:811/1770 train_time:81527ms step_avg:101.78ms
step:812/1770 train_time:81631ms step_avg:101.78ms
step:813/1770 train_time:81736ms step_avg:101.79ms
step:814/1770 train_time:81840ms step_avg:101.79ms
step:815/1770 train_time:81945ms step_avg:101.79ms
step:816/1770 train_time:82049ms step_avg:101.80ms
step:817/1770 train_time:82154ms step_avg:101.80ms
step:818/1770 train_time:82258ms step_avg:101.81ms
step:819/1770 train_time:82364ms step_avg:101.81ms
step:820/1770 train_time:82468ms step_avg:101.81ms
step:821/1770 train_time:82572ms step_avg:101.82ms
step:822/1770 train_time:82677ms step_avg:101.82ms
step:823/1770 train_time:82783ms step_avg:101.82ms
step:824/1770 train_time:82888ms step_avg:101.83ms
step:825/1770 train_time:82992ms step_avg:101.83ms
step:826/1770 train_time:83097ms step_avg:101.83ms
step:827/1770 train_time:83203ms step_avg:101.84ms
step:828/1770 train_time:83307ms step_avg:101.84ms
step:829/1770 train_time:83412ms step_avg:101.85ms
step:830/1770 train_time:83516ms step_avg:101.85ms
step:831/1770 train_time:83621ms step_avg:101.85ms
step:832/1770 train_time:83726ms step_avg:101.86ms
step:833/1770 train_time:83831ms step_avg:101.86ms
step:834/1770 train_time:83935ms step_avg:101.86ms
step:835/1770 train_time:84040ms step_avg:101.87ms
step:836/1770 train_time:84146ms step_avg:101.87ms
step:837/1770 train_time:84250ms step_avg:101.87ms
step:838/1770 train_time:84355ms step_avg:101.88ms
step:839/1770 train_time:84460ms step_avg:101.88ms
step:840/1770 train_time:84565ms step_avg:101.89ms
step:841/1770 train_time:84669ms step_avg:101.89ms
step:842/1770 train_time:84773ms step_avg:101.89ms
step:843/1770 train_time:84879ms step_avg:101.90ms
step:844/1770 train_time:84984ms step_avg:101.90ms
step:845/1770 train_time:85088ms step_avg:101.90ms
step:846/1770 train_time:85192ms step_avg:101.90ms
step:847/1770 train_time:85297ms step_avg:101.91ms
step:848/1770 train_time:85402ms step_avg:101.91ms
step:849/1770 train_time:85506ms step_avg:101.91ms
step:850/1770 train_time:85611ms step_avg:101.92ms
step:851/1770 train_time:85715ms step_avg:101.92ms
step:852/1770 train_time:85820ms step_avg:101.92ms
step:853/1770 train_time:85924ms step_avg:101.93ms
step:854/1770 train_time:86028ms step_avg:101.93ms
step:855/1770 train_time:86133ms step_avg:101.93ms
step:856/1770 train_time:86238ms step_avg:101.94ms
step:857/1770 train_time:86343ms step_avg:101.94ms
step:858/1770 train_time:86447ms step_avg:101.94ms
step:859/1770 train_time:86552ms step_avg:101.95ms
step:860/1770 train_time:86657ms step_avg:101.95ms
step:861/1770 train_time:86762ms step_avg:101.95ms
step:862/1770 train_time:86866ms step_avg:101.96ms
step:863/1770 train_time:86970ms step_avg:101.96ms
step:864/1770 train_time:87074ms step_avg:101.96ms
step:865/1770 train_time:87179ms step_avg:101.96ms
step:866/1770 train_time:87284ms step_avg:101.97ms
step:867/1770 train_time:87389ms step_avg:101.97ms
step:868/1770 train_time:87493ms step_avg:101.97ms
step:869/1770 train_time:87598ms step_avg:101.98ms
step:870/1770 train_time:87703ms step_avg:101.98ms
step:871/1770 train_time:87807ms step_avg:101.98ms
step:872/1770 train_time:87911ms step_avg:101.99ms
step:873/1770 train_time:88015ms step_avg:101.99ms
step:874/1770 train_time:88120ms step_avg:101.99ms
step:875/1770 train_time:88225ms step_avg:101.99ms
step:875/1770 val_loss:3.5517 train_time:88328ms step_avg:102.11ms
step:876/1770 train_time:88346ms step_avg:102.02ms
step:877/1770 train_time:88447ms step_avg:102.02ms
step:878/1770 train_time:88553ms step_avg:102.02ms
step:879/1770 train_time:88659ms step_avg:102.02ms
step:880/1770 train_time:88764ms step_avg:102.03ms
step:881/1770 train_time:88869ms step_avg:102.03ms
step:882/1770 train_time:88973ms step_avg:102.03ms
step:883/1770 train_time:89078ms step_avg:102.04ms
step:884/1770 train_time:89182ms step_avg:102.04ms
step:885/1770 train_time:89286ms step_avg:102.04ms
step:886/1770 train_time:89391ms step_avg:102.04ms
step:887/1770 train_time:89495ms step_avg:102.05ms
step:888/1770 train_time:89599ms step_avg:102.05ms
step:889/1770 train_time:89705ms step_avg:102.05ms
step:890/1770 train_time:89810ms step_avg:102.06ms
step:891/1770 train_time:89914ms step_avg:102.06ms
step:892/1770 train_time:90019ms step_avg:102.06ms
step:893/1770 train_time:90123ms step_avg:102.06ms
step:894/1770 train_time:90228ms step_avg:102.07ms
step:895/1770 train_time:90332ms step_avg:102.07ms
step:896/1770 train_time:90437ms step_avg:102.07ms
step:897/1770 train_time:90541ms step_avg:102.08ms
step:898/1770 train_time:90647ms step_avg:102.08ms
step:899/1770 train_time:90751ms step_avg:102.08ms
step:900/1770 train_time:90856ms step_avg:102.08ms
step:901/1770 train_time:90960ms step_avg:102.09ms
step:902/1770 train_time:91066ms step_avg:102.09ms
step:903/1770 train_time:91171ms step_avg:102.09ms
step:904/1770 train_time:91275ms step_avg:102.10ms
step:905/1770 train_time:91379ms step_avg:102.10ms
step:906/1770 train_time:91484ms step_avg:102.10ms
step:907/1770 train_time:91589ms step_avg:102.11ms
step:908/1770 train_time:91693ms step_avg:102.11ms
step:909/1770 train_time:91799ms step_avg:102.11ms
step:910/1770 train_time:91903ms step_avg:102.11ms
step:911/1770 train_time:92008ms step_avg:102.12ms
step:912/1770 train_time:92112ms step_avg:102.12ms
step:913/1770 train_time:92217ms step_avg:102.12ms
step:914/1770 train_time:92322ms step_avg:102.13ms
step:915/1770 train_time:92427ms step_avg:102.13ms
step:916/1770 train_time:92532ms step_avg:102.13ms
step:917/1770 train_time:92636ms step_avg:102.13ms
step:918/1770 train_time:92740ms step_avg:102.14ms
step:919/1770 train_time:92845ms step_avg:102.14ms
step:920/1770 train_time:92951ms step_avg:102.14ms
step:921/1770 train_time:93059ms step_avg:102.15ms
step:922/1770 train_time:93164ms step_avg:102.15ms
step:923/1770 train_time:93271ms step_avg:102.16ms
step:924/1770 train_time:93376ms step_avg:102.16ms
step:925/1770 train_time:93482ms step_avg:102.17ms
step:926/1770 train_time:93588ms step_avg:102.17ms
step:927/1770 train_time:93694ms step_avg:102.17ms
step:928/1770 train_time:93800ms step_avg:102.18ms
step:929/1770 train_time:93906ms step_avg:102.18ms
step:930/1770 train_time:94012ms step_avg:102.19ms
step:931/1770 train_time:94119ms step_avg:102.19ms
step:932/1770 train_time:94225ms step_avg:102.20ms
step:933/1770 train_time:94331ms step_avg:102.20ms
step:934/1770 train_time:94437ms step_avg:102.20ms
step:935/1770 train_time:94543ms step_avg:102.21ms
step:936/1770 train_time:94649ms step_avg:102.21ms
step:937/1770 train_time:94755ms step_avg:102.22ms
step:938/1770 train_time:94860ms step_avg:102.22ms
step:939/1770 train_time:94967ms step_avg:102.22ms
step:940/1770 train_time:95073ms step_avg:102.23ms
step:941/1770 train_time:95179ms step_avg:102.23ms
step:942/1770 train_time:95285ms step_avg:102.24ms
step:943/1770 train_time:95392ms step_avg:102.24ms
step:944/1770 train_time:95497ms step_avg:102.25ms
step:945/1770 train_time:95604ms step_avg:102.25ms
step:946/1770 train_time:95710ms step_avg:102.25ms
step:947/1770 train_time:95816ms step_avg:102.26ms
step:948/1770 train_time:95921ms step_avg:102.26ms
step:949/1770 train_time:96028ms step_avg:102.27ms
step:950/1770 train_time:96133ms step_avg:102.27ms
step:951/1770 train_time:96240ms step_avg:102.27ms
step:952/1770 train_time:96346ms step_avg:102.28ms
step:953/1770 train_time:96452ms step_avg:102.28ms
step:954/1770 train_time:96558ms step_avg:102.29ms
step:955/1770 train_time:96664ms step_avg:102.29ms
step:956/1770 train_time:96771ms step_avg:102.29ms
step:957/1770 train_time:96876ms step_avg:102.30ms
step:958/1770 train_time:96981ms step_avg:102.30ms
step:959/1770 train_time:97088ms step_avg:102.31ms
step:960/1770 train_time:97193ms step_avg:102.31ms
step:961/1770 train_time:97299ms step_avg:102.31ms
step:962/1770 train_time:97406ms step_avg:102.32ms
step:963/1770 train_time:97512ms step_avg:102.32ms
step:964/1770 train_time:97618ms step_avg:102.33ms
step:965/1770 train_time:97725ms step_avg:102.33ms
step:966/1770 train_time:97831ms step_avg:102.33ms
step:967/1770 train_time:97937ms step_avg:102.34ms
step:968/1770 train_time:98043ms step_avg:102.34ms
step:969/1770 train_time:98150ms step_avg:102.35ms
step:970/1770 train_time:98255ms step_avg:102.35ms
step:971/1770 train_time:98362ms step_avg:102.35ms
step:972/1770 train_time:98468ms step_avg:102.36ms
step:973/1770 train_time:98574ms step_avg:102.36ms
step:974/1770 train_time:98679ms step_avg:102.36ms
step:975/1770 train_time:98787ms step_avg:102.37ms
step:976/1770 train_time:98892ms step_avg:102.37ms
step:977/1770 train_time:98999ms step_avg:102.38ms
step:978/1770 train_time:99106ms step_avg:102.38ms
step:979/1770 train_time:99212ms step_avg:102.39ms
step:980/1770 train_time:99317ms step_avg:102.39ms
step:981/1770 train_time:99423ms step_avg:102.39ms
step:982/1770 train_time:99530ms step_avg:102.40ms
step:983/1770 train_time:99636ms step_avg:102.40ms
step:984/1770 train_time:99742ms step_avg:102.40ms
step:985/1770 train_time:99848ms step_avg:102.41ms
step:986/1770 train_time:99953ms step_avg:102.41ms
step:987/1770 train_time:100060ms step_avg:102.42ms
step:988/1770 train_time:100166ms step_avg:102.42ms
step:989/1770 train_time:100273ms step_avg:102.42ms
step:990/1770 train_time:100380ms step_avg:102.43ms
step:991/1770 train_time:100486ms step_avg:102.43ms
step:992/1770 train_time:100593ms step_avg:102.44ms
step:993/1770 train_time:100699ms step_avg:102.44ms
step:994/1770 train_time:100806ms step_avg:102.45ms
step:995/1770 train_time:100912ms step_avg:102.45ms
step:996/1770 train_time:101018ms step_avg:102.45ms
step:997/1770 train_time:101124ms step_avg:102.46ms
step:998/1770 train_time:101230ms step_avg:102.46ms
step:999/1770 train_time:101336ms step_avg:102.46ms
step:1000/1770 train_time:101442ms step_avg:102.47ms
step:1000/1770 val_loss:3.5127 train_time:101547ms step_avg:102.57ms
step:1001/1770 train_time:101564ms step_avg:102.49ms
step:1002/1770 train_time:101667ms step_avg:102.49ms
step:1003/1770 train_time:101776ms step_avg:102.49ms
step:1004/1770 train_time:101883ms step_avg:102.50ms
step:1005/1770 train_time:101988ms step_avg:102.50ms
step:1006/1770 train_time:102094ms step_avg:102.50ms
step:1007/1770 train_time:102200ms step_avg:102.51ms
step:1008/1770 train_time:102306ms step_avg:102.51ms
step:1009/1770 train_time:102412ms step_avg:102.51ms
step:1010/1770 train_time:102518ms step_avg:102.52ms
step:1011/1770 train_time:102625ms step_avg:102.52ms
step:1012/1770 train_time:102731ms step_avg:102.53ms
step:1013/1770 train_time:102838ms step_avg:102.53ms
step:1014/1770 train_time:102944ms step_avg:102.53ms
step:1015/1770 train_time:103050ms step_avg:102.54ms
step:1016/1770 train_time:103156ms step_avg:102.54ms
step:1017/1770 train_time:103262ms step_avg:102.54ms
step:1018/1770 train_time:103368ms step_avg:102.55ms
step:1019/1770 train_time:103474ms step_avg:102.55ms
step:1020/1770 train_time:103580ms step_avg:102.55ms
step:1021/1770 train_time:103686ms step_avg:102.56ms
step:1022/1770 train_time:103791ms step_avg:102.56ms
step:1023/1770 train_time:103898ms step_avg:102.56ms
step:1024/1770 train_time:104003ms step_avg:102.57ms
step:1025/1770 train_time:104109ms step_avg:102.57ms
step:1026/1770 train_time:104216ms step_avg:102.58ms
step:1027/1770 train_time:104323ms step_avg:102.58ms
step:1028/1770 train_time:104430ms step_avg:102.58ms
step:1029/1770 train_time:104536ms step_avg:102.59ms
step:1030/1770 train_time:104643ms step_avg:102.59ms
step:1031/1770 train_time:104748ms step_avg:102.59ms
step:1032/1770 train_time:104853ms step_avg:102.60ms
step:1033/1770 train_time:104960ms step_avg:102.60ms
step:1034/1770 train_time:105065ms step_avg:102.60ms
step:1035/1770 train_time:105172ms step_avg:102.61ms
step:1036/1770 train_time:105278ms step_avg:102.61ms
step:1037/1770 train_time:105384ms step_avg:102.61ms
step:1038/1770 train_time:105489ms step_avg:102.62ms
step:1039/1770 train_time:105597ms step_avg:102.62ms
step:1040/1770 train_time:105702ms step_avg:102.62ms
step:1041/1770 train_time:105809ms step_avg:102.63ms
step:1042/1770 train_time:105915ms step_avg:102.63ms
step:1043/1770 train_time:106021ms step_avg:102.63ms
step:1044/1770 train_time:106127ms step_avg:102.64ms
step:1045/1770 train_time:106232ms step_avg:102.64ms
step:1046/1770 train_time:106339ms step_avg:102.64ms
step:1047/1770 train_time:106445ms step_avg:102.65ms
step:1048/1770 train_time:106551ms step_avg:102.65ms
step:1049/1770 train_time:106657ms step_avg:102.65ms
step:1050/1770 train_time:106763ms step_avg:102.66ms
step:1051/1770 train_time:106869ms step_avg:102.66ms
step:1052/1770 train_time:106975ms step_avg:102.66ms
step:1053/1770 train_time:107081ms step_avg:102.67ms
step:1054/1770 train_time:107187ms step_avg:102.67ms
step:1055/1770 train_time:107293ms step_avg:102.67ms
step:1056/1770 train_time:107399ms step_avg:102.68ms
step:1057/1770 train_time:107505ms step_avg:102.68ms
step:1058/1770 train_time:107612ms step_avg:102.68ms
step:1059/1770 train_time:107719ms step_avg:102.69ms
step:1060/1770 train_time:107825ms step_avg:102.69ms
step:1061/1770 train_time:107931ms step_avg:102.69ms
step:1062/1770 train_time:108038ms step_avg:102.70ms
step:1063/1770 train_time:108145ms step_avg:102.70ms
step:1064/1770 train_time:108251ms step_avg:102.70ms
step:1065/1770 train_time:108357ms step_avg:102.71ms
step:1066/1770 train_time:108463ms step_avg:102.71ms
step:1067/1770 train_time:108570ms step_avg:102.72ms
step:1068/1770 train_time:108678ms step_avg:102.72ms
step:1069/1770 train_time:108784ms step_avg:102.72ms
step:1070/1770 train_time:108890ms step_avg:102.73ms
step:1071/1770 train_time:108998ms step_avg:102.73ms
step:1072/1770 train_time:109104ms step_avg:102.73ms
step:1073/1770 train_time:109210ms step_avg:102.74ms
step:1074/1770 train_time:109316ms step_avg:102.74ms
step:1075/1770 train_time:109422ms step_avg:102.74ms
step:1076/1770 train_time:109529ms step_avg:102.75ms
step:1077/1770 train_time:109636ms step_avg:102.75ms
step:1078/1770 train_time:109742ms step_avg:102.76ms
step:1079/1770 train_time:109848ms step_avg:102.76ms
step:1080/1770 train_time:109954ms step_avg:102.76ms
step:1081/1770 train_time:110061ms step_avg:102.76ms
step:1082/1770 train_time:110167ms step_avg:102.77ms
step:1083/1770 train_time:110273ms step_avg:102.77ms
step:1084/1770 train_time:110380ms step_avg:102.77ms
step:1085/1770 train_time:110486ms step_avg:102.78ms
step:1086/1770 train_time:110592ms step_avg:102.78ms
step:1087/1770 train_time:110698ms step_avg:102.78ms
step:1088/1770 train_time:110805ms step_avg:102.79ms
step:1089/1770 train_time:110911ms step_avg:102.79ms
step:1090/1770 train_time:111019ms step_avg:102.80ms
step:1091/1770 train_time:111124ms step_avg:102.80ms
step:1092/1770 train_time:111231ms step_avg:102.80ms
step:1093/1770 train_time:111337ms step_avg:102.80ms
step:1094/1770 train_time:111444ms step_avg:102.81ms
step:1095/1770 train_time:111551ms step_avg:102.81ms
step:1096/1770 train_time:111658ms step_avg:102.82ms
step:1097/1770 train_time:111763ms step_avg:102.82ms
step:1098/1770 train_time:111870ms step_avg:102.82ms
step:1099/1770 train_time:111975ms step_avg:102.82ms
step:1100/1770 train_time:112082ms step_avg:102.83ms
step:1101/1770 train_time:112188ms step_avg:102.83ms
step:1102/1770 train_time:112295ms step_avg:102.83ms
step:1103/1770 train_time:112402ms step_avg:102.84ms
step:1104/1770 train_time:112508ms step_avg:102.84ms
step:1105/1770 train_time:112614ms step_avg:102.84ms
step:1106/1770 train_time:112721ms step_avg:102.85ms
step:1107/1770 train_time:112827ms step_avg:102.85ms
step:1108/1770 train_time:112934ms step_avg:102.85ms
step:1109/1770 train_time:113041ms step_avg:102.86ms
step:1110/1770 train_time:113146ms step_avg:102.86ms
step:1111/1770 train_time:113253ms step_avg:102.86ms
step:1112/1770 train_time:113359ms step_avg:102.87ms
step:1113/1770 train_time:113465ms step_avg:102.87ms
step:1114/1770 train_time:113572ms step_avg:102.87ms
step:1115/1770 train_time:113679ms step_avg:102.88ms
step:1116/1770 train_time:113785ms step_avg:102.88ms
step:1117/1770 train_time:113892ms step_avg:102.88ms
step:1118/1770 train_time:113999ms step_avg:102.89ms
step:1119/1770 train_time:114104ms step_avg:102.89ms
step:1120/1770 train_time:114210ms step_avg:102.89ms
step:1121/1770 train_time:114317ms step_avg:102.90ms
step:1122/1770 train_time:114423ms step_avg:102.90ms
step:1123/1770 train_time:114529ms step_avg:102.90ms
step:1124/1770 train_time:114637ms step_avg:102.91ms
step:1125/1770 train_time:114744ms step_avg:102.91ms
step:1125/1770 val_loss:3.4718 train_time:114848ms step_avg:103.00ms
step:1126/1770 train_time:114866ms step_avg:102.93ms
step:1127/1770 train_time:114968ms step_avg:102.93ms
step:1128/1770 train_time:115075ms step_avg:102.93ms
step:1129/1770 train_time:115181ms step_avg:102.93ms
step:1130/1770 train_time:115288ms step_avg:102.94ms
step:1131/1770 train_time:115394ms step_avg:102.94ms
step:1132/1770 train_time:115500ms step_avg:102.94ms
step:1133/1770 train_time:115607ms step_avg:102.94ms
step:1134/1770 train_time:115713ms step_avg:102.95ms
step:1135/1770 train_time:115820ms step_avg:102.95ms
step:1136/1770 train_time:115927ms step_avg:102.95ms
step:1137/1770 train_time:116034ms step_avg:102.96ms
step:1138/1770 train_time:116140ms step_avg:102.96ms
step:1139/1770 train_time:116246ms step_avg:102.96ms
step:1140/1770 train_time:116351ms step_avg:102.97ms
step:1141/1770 train_time:116458ms step_avg:102.97ms
step:1142/1770 train_time:116564ms step_avg:102.97ms
step:1143/1770 train_time:116670ms step_avg:102.97ms
step:1144/1770 train_time:116776ms step_avg:102.98ms
step:1145/1770 train_time:116882ms step_avg:102.98ms
step:1146/1770 train_time:116989ms step_avg:102.98ms
step:1147/1770 train_time:117096ms step_avg:102.99ms
step:1148/1770 train_time:117202ms step_avg:102.99ms
step:1149/1770 train_time:117308ms step_avg:102.99ms
step:1150/1770 train_time:117414ms step_avg:102.99ms
step:1151/1770 train_time:117520ms step_avg:103.00ms
step:1152/1770 train_time:117627ms step_avg:103.00ms
step:1153/1770 train_time:117733ms step_avg:103.00ms
step:1154/1770 train_time:117839ms step_avg:103.01ms
step:1155/1770 train_time:117946ms step_avg:103.01ms
step:1156/1770 train_time:118052ms step_avg:103.01ms
step:1157/1770 train_time:118160ms step_avg:103.02ms
step:1158/1770 train_time:118267ms step_avg:103.02ms
step:1159/1770 train_time:118373ms step_avg:103.02ms
step:1160/1770 train_time:118479ms step_avg:103.03ms
step:1161/1770 train_time:118586ms step_avg:103.03ms
step:1162/1770 train_time:118693ms step_avg:103.03ms
step:1163/1770 train_time:118799ms step_avg:103.03ms
step:1164/1770 train_time:118906ms step_avg:103.04ms
step:1165/1770 train_time:119012ms step_avg:103.04ms
step:1166/1770 train_time:119118ms step_avg:103.04ms
step:1167/1770 train_time:119224ms step_avg:103.05ms
step:1168/1770 train_time:119331ms step_avg:103.05ms
step:1169/1770 train_time:119437ms step_avg:103.05ms
step:1170/1770 train_time:119543ms step_avg:103.05ms
step:1171/1770 train_time:119650ms step_avg:103.06ms
step:1172/1770 train_time:119756ms step_avg:103.06ms
step:1173/1770 train_time:119862ms step_avg:103.06ms
step:1174/1770 train_time:119968ms step_avg:103.07ms
step:1175/1770 train_time:120074ms step_avg:103.07ms
step:1176/1770 train_time:120180ms step_avg:103.07ms
step:1177/1770 train_time:120287ms step_avg:103.07ms
step:1178/1770 train_time:120393ms step_avg:103.08ms
step:1179/1770 train_time:120499ms step_avg:103.08ms
step:1180/1770 train_time:120606ms step_avg:103.08ms
step:1181/1770 train_time:120712ms step_avg:103.08ms
step:1182/1770 train_time:120819ms step_avg:103.09ms
step:1183/1770 train_time:120928ms step_avg:103.09ms
step:1184/1770 train_time:121036ms step_avg:103.10ms
step:1185/1770 train_time:121143ms step_avg:103.10ms
step:1186/1770 train_time:121251ms step_avg:103.10ms
step:1187/1770 train_time:121361ms step_avg:103.11ms
step:1188/1770 train_time:121469ms step_avg:103.11ms
step:1189/1770 train_time:121575ms step_avg:103.12ms
step:1190/1770 train_time:121682ms step_avg:103.12ms
step:1191/1770 train_time:121791ms step_avg:103.12ms
step:1192/1770 train_time:121898ms step_avg:103.13ms
step:1193/1770 train_time:122006ms step_avg:103.13ms
step:1194/1770 train_time:122115ms step_avg:103.14ms
step:1195/1770 train_time:122222ms step_avg:103.14ms
step:1196/1770 train_time:122331ms step_avg:103.15ms
step:1197/1770 train_time:122438ms step_avg:103.15ms
step:1198/1770 train_time:122546ms step_avg:103.15ms
step:1199/1770 train_time:122654ms step_avg:103.16ms
step:1200/1770 train_time:122762ms step_avg:103.16ms
step:1201/1770 train_time:122871ms step_avg:103.17ms
step:1202/1770 train_time:122978ms step_avg:103.17ms
step:1203/1770 train_time:123086ms step_avg:103.17ms
step:1204/1770 train_time:123194ms step_avg:103.18ms
step:1205/1770 train_time:123301ms step_avg:103.18ms
step:1206/1770 train_time:123409ms step_avg:103.19ms
step:1207/1770 train_time:123516ms step_avg:103.19ms
step:1208/1770 train_time:123624ms step_avg:103.19ms
step:1209/1770 train_time:123732ms step_avg:103.20ms
step:1210/1770 train_time:123839ms step_avg:103.20ms
step:1211/1770 train_time:123946ms step_avg:103.20ms
step:1212/1770 train_time:124056ms step_avg:103.21ms
step:1213/1770 train_time:124163ms step_avg:103.21ms
step:1214/1770 train_time:124270ms step_avg:103.21ms
step:1215/1770 train_time:124379ms step_avg:103.22ms
step:1216/1770 train_time:124489ms step_avg:103.22ms
step:1217/1770 train_time:124596ms step_avg:103.23ms
step:1218/1770 train_time:124703ms step_avg:103.23ms
step:1219/1770 train_time:124811ms step_avg:103.23ms
step:1220/1770 train_time:124918ms step_avg:103.24ms
step:1221/1770 train_time:125025ms step_avg:103.24ms
step:1222/1770 train_time:125135ms step_avg:103.25ms
step:1223/1770 train_time:125242ms step_avg:103.25ms
step:1224/1770 train_time:125352ms step_avg:103.26ms
step:1225/1770 train_time:125461ms step_avg:103.26ms
step:1226/1770 train_time:125570ms step_avg:103.26ms
step:1227/1770 train_time:125679ms step_avg:103.27ms
step:1228/1770 train_time:125787ms step_avg:103.27ms
step:1229/1770 train_time:125894ms step_avg:103.28ms
step:1230/1770 train_time:126002ms step_avg:103.28ms
step:1231/1770 train_time:126110ms step_avg:103.28ms
step:1232/1770 train_time:126217ms step_avg:103.29ms
step:1233/1770 train_time:126324ms step_avg:103.29ms
step:1234/1770 train_time:126431ms step_avg:103.29ms
step:1235/1770 train_time:126539ms step_avg:103.30ms
step:1236/1770 train_time:126647ms step_avg:103.30ms
step:1237/1770 train_time:126754ms step_avg:103.30ms
step:1238/1770 train_time:126862ms step_avg:103.31ms
step:1239/1770 train_time:126970ms step_avg:103.31ms
step:1240/1770 train_time:127077ms step_avg:103.31ms
step:1241/1770 train_time:127185ms step_avg:103.32ms
step:1242/1770 train_time:127293ms step_avg:103.32ms
step:1243/1770 train_time:127401ms step_avg:103.33ms
step:1244/1770 train_time:127509ms step_avg:103.33ms
step:1245/1770 train_time:127616ms step_avg:103.33ms
step:1246/1770 train_time:127724ms step_avg:103.34ms
step:1247/1770 train_time:127833ms step_avg:103.34ms
step:1248/1770 train_time:127941ms step_avg:103.35ms
step:1249/1770 train_time:128050ms step_avg:103.35ms
step:1250/1770 train_time:128156ms step_avg:103.35ms
step:1250/1770 val_loss:3.4239 train_time:128264ms step_avg:103.44ms
step:1251/1770 train_time:128281ms step_avg:103.37ms
step:1252/1770 train_time:128382ms step_avg:103.37ms
step:1253/1770 train_time:128494ms step_avg:103.37ms
step:1254/1770 train_time:128601ms step_avg:103.38ms
step:1255/1770 train_time:128711ms step_avg:103.38ms
step:1256/1770 train_time:128818ms step_avg:103.39ms
step:1257/1770 train_time:128925ms step_avg:103.39ms
step:1258/1770 train_time:129034ms step_avg:103.39ms
step:1259/1770 train_time:129142ms step_avg:103.40ms
step:1260/1770 train_time:129249ms step_avg:103.40ms
step:1261/1770 train_time:129357ms step_avg:103.40ms
step:1262/1770 train_time:129465ms step_avg:103.41ms
step:1263/1770 train_time:129573ms step_avg:103.41ms
step:1264/1770 train_time:129681ms step_avg:103.41ms
step:1265/1770 train_time:129789ms step_avg:103.42ms
step:1266/1770 train_time:129897ms step_avg:103.42ms
step:1267/1770 train_time:130004ms step_avg:103.42ms
step:1268/1770 train_time:130112ms step_avg:103.43ms
step:1269/1770 train_time:130219ms step_avg:103.43ms
step:1270/1770 train_time:130327ms step_avg:103.43ms
step:1271/1770 train_time:130434ms step_avg:103.44ms
step:1272/1770 train_time:130541ms step_avg:103.44ms
step:1273/1770 train_time:130650ms step_avg:103.44ms
step:1274/1770 train_time:130758ms step_avg:103.45ms
step:1275/1770 train_time:130865ms step_avg:103.45ms
step:1276/1770 train_time:130973ms step_avg:103.45ms
step:1277/1770 train_time:131080ms step_avg:103.46ms
step:1278/1770 train_time:131188ms step_avg:103.46ms
step:1279/1770 train_time:131296ms step_avg:103.46ms
step:1280/1770 train_time:131404ms step_avg:103.47ms
step:1281/1770 train_time:131511ms step_avg:103.47ms
step:1282/1770 train_time:131620ms step_avg:103.47ms
step:1283/1770 train_time:131727ms step_avg:103.48ms
step:1284/1770 train_time:131835ms step_avg:103.48ms
step:1285/1770 train_time:131943ms step_avg:103.48ms
step:1286/1770 train_time:132052ms step_avg:103.49ms
step:1287/1770 train_time:132161ms step_avg:103.49ms
step:1288/1770 train_time:132270ms step_avg:103.50ms
step:1289/1770 train_time:132377ms step_avg:103.50ms
step:1290/1770 train_time:132484ms step_avg:103.50ms
step:1291/1770 train_time:132591ms step_avg:103.51ms
step:1292/1770 train_time:132699ms step_avg:103.51ms
step:1293/1770 train_time:132807ms step_avg:103.51ms
step:1294/1770 train_time:132914ms step_avg:103.52ms
step:1295/1770 train_time:133022ms step_avg:103.52ms
step:1296/1770 train_time:133130ms step_avg:103.52ms
step:1297/1770 train_time:133237ms step_avg:103.53ms
step:1298/1770 train_time:133344ms step_avg:103.53ms
step:1299/1770 train_time:133452ms step_avg:103.53ms
step:1300/1770 train_time:133559ms step_avg:103.53ms
step:1301/1770 train_time:133669ms step_avg:103.54ms
step:1302/1770 train_time:133776ms step_avg:103.54ms
step:1303/1770 train_time:133883ms step_avg:103.54ms
step:1304/1770 train_time:133991ms step_avg:103.55ms
step:1305/1770 train_time:134098ms step_avg:103.55ms
step:1306/1770 train_time:134206ms step_avg:103.55ms
step:1307/1770 train_time:134314ms step_avg:103.56ms
step:1308/1770 train_time:134421ms step_avg:103.56ms
step:1309/1770 train_time:134529ms step_avg:103.56ms
step:1310/1770 train_time:134636ms step_avg:103.57ms
step:1311/1770 train_time:134743ms step_avg:103.57ms
step:1312/1770 train_time:134851ms step_avg:103.57ms
step:1313/1770 train_time:134958ms step_avg:103.57ms
step:1314/1770 train_time:135066ms step_avg:103.58ms
step:1315/1770 train_time:135174ms step_avg:103.58ms
step:1316/1770 train_time:135282ms step_avg:103.58ms
step:1317/1770 train_time:135390ms step_avg:103.59ms
step:1318/1770 train_time:135500ms step_avg:103.59ms
step:1319/1770 train_time:135609ms step_avg:103.60ms
step:1320/1770 train_time:135716ms step_avg:103.60ms
step:1321/1770 train_time:135824ms step_avg:103.60ms
step:1322/1770 train_time:135932ms step_avg:103.61ms
step:1323/1770 train_time:136040ms step_avg:103.61ms
step:1324/1770 train_time:136149ms step_avg:103.61ms
step:1325/1770 train_time:136258ms step_avg:103.62ms
step:1326/1770 train_time:136366ms step_avg:103.62ms
step:1327/1770 train_time:136477ms step_avg:103.63ms
step:1328/1770 train_time:136584ms step_avg:103.63ms
step:1329/1770 train_time:136692ms step_avg:103.63ms
step:1330/1770 train_time:136800ms step_avg:103.64ms
step:1331/1770 train_time:136907ms step_avg:103.64ms
step:1332/1770 train_time:137015ms step_avg:103.64ms
step:1333/1770 train_time:137122ms step_avg:103.64ms
step:1334/1770 train_time:137229ms step_avg:103.65ms
step:1335/1770 train_time:137336ms step_avg:103.65ms
step:1336/1770 train_time:137444ms step_avg:103.65ms
step:1337/1770 train_time:137554ms step_avg:103.66ms
step:1338/1770 train_time:137661ms step_avg:103.66ms
step:1339/1770 train_time:137770ms step_avg:103.66ms
step:1340/1770 train_time:137879ms step_avg:103.67ms
step:1341/1770 train_time:137986ms step_avg:103.67ms
step:1342/1770 train_time:138095ms step_avg:103.68ms
step:1343/1770 train_time:138204ms step_avg:103.68ms
step:1344/1770 train_time:138313ms step_avg:103.68ms
step:1345/1770 train_time:138420ms step_avg:103.69ms
step:1346/1770 train_time:138528ms step_avg:103.69ms
step:1347/1770 train_time:138636ms step_avg:103.69ms
step:1348/1770 train_time:138746ms step_avg:103.70ms
step:1349/1770 train_time:138854ms step_avg:103.70ms
step:1350/1770 train_time:138962ms step_avg:103.70ms
step:1351/1770 train_time:139070ms step_avg:103.71ms
step:1352/1770 train_time:139177ms step_avg:103.71ms
step:1353/1770 train_time:139286ms step_avg:103.71ms
step:1354/1770 train_time:139393ms step_avg:103.72ms
step:1355/1770 train_time:139500ms step_avg:103.72ms
step:1356/1770 train_time:139608ms step_avg:103.72ms
step:1357/1770 train_time:139716ms step_avg:103.72ms
step:1358/1770 train_time:139823ms step_avg:103.73ms
step:1359/1770 train_time:139931ms step_avg:103.73ms
step:1360/1770 train_time:140041ms step_avg:103.73ms
step:1361/1770 train_time:140149ms step_avg:103.74ms
step:1362/1770 train_time:140256ms step_avg:103.74ms
step:1363/1770 train_time:140364ms step_avg:103.74ms
step:1364/1770 train_time:140472ms step_avg:103.75ms
step:1365/1770 train_time:140579ms step_avg:103.75ms
step:1366/1770 train_time:140687ms step_avg:103.75ms
step:1367/1770 train_time:140795ms step_avg:103.75ms
step:1368/1770 train_time:140902ms step_avg:103.76ms
step:1369/1770 train_time:141011ms step_avg:103.76ms
step:1370/1770 train_time:141119ms step_avg:103.76ms
step:1371/1770 train_time:141226ms step_avg:103.77ms
step:1372/1770 train_time:141333ms step_avg:103.77ms
step:1373/1770 train_time:141441ms step_avg:103.77ms
step:1374/1770 train_time:141550ms step_avg:103.78ms
step:1375/1770 train_time:141657ms step_avg:103.78ms
step:1375/1770 val_loss:3.3810 train_time:141765ms step_avg:103.86ms
step:1376/1770 train_time:141783ms step_avg:103.79ms
step:1377/1770 train_time:141882ms step_avg:103.79ms
step:1378/1770 train_time:141992ms step_avg:103.80ms
step:1379/1770 train_time:142100ms step_avg:103.80ms
step:1380/1770 train_time:142208ms step_avg:103.80ms
step:1381/1770 train_time:142316ms step_avg:103.80ms
step:1382/1770 train_time:142423ms step_avg:103.81ms
step:1383/1770 train_time:142532ms step_avg:103.81ms
step:1384/1770 train_time:142640ms step_avg:103.81ms
step:1385/1770 train_time:142748ms step_avg:103.82ms
step:1386/1770 train_time:142856ms step_avg:103.82ms
step:1387/1770 train_time:142964ms step_avg:103.82ms
step:1388/1770 train_time:143071ms step_avg:103.83ms
step:1389/1770 train_time:143179ms step_avg:103.83ms
step:1390/1770 train_time:143288ms step_avg:103.83ms
step:1391/1770 train_time:143394ms step_avg:103.83ms
step:1392/1770 train_time:143502ms step_avg:103.84ms
step:1393/1770 train_time:143609ms step_avg:103.84ms
step:1394/1770 train_time:143717ms step_avg:103.84ms
step:1395/1770 train_time:143825ms step_avg:103.84ms
step:1396/1770 train_time:143934ms step_avg:103.85ms
step:1397/1770 train_time:144042ms step_avg:103.85ms
step:1398/1770 train_time:144150ms step_avg:103.85ms
step:1399/1770 train_time:144257ms step_avg:103.86ms
step:1400/1770 train_time:144365ms step_avg:103.86ms
step:1401/1770 train_time:144472ms step_avg:103.86ms
step:1402/1770 train_time:144579ms step_avg:103.86ms
step:1403/1770 train_time:144687ms step_avg:103.87ms
step:1404/1770 train_time:144795ms step_avg:103.87ms
step:1405/1770 train_time:144903ms step_avg:103.87ms
step:1406/1770 train_time:145011ms step_avg:103.88ms
step:1407/1770 train_time:145118ms step_avg:103.88ms
step:1408/1770 train_time:145226ms step_avg:103.88ms
step:1409/1770 train_time:145334ms step_avg:103.88ms
step:1410/1770 train_time:145442ms step_avg:103.89ms
step:1411/1770 train_time:145549ms step_avg:103.89ms
step:1412/1770 train_time:145656ms step_avg:103.89ms
step:1413/1770 train_time:145763ms step_avg:103.89ms
step:1414/1770 train_time:145871ms step_avg:103.90ms
step:1415/1770 train_time:145979ms step_avg:103.90ms
step:1416/1770 train_time:146089ms step_avg:103.90ms
step:1417/1770 train_time:146196ms step_avg:103.91ms
step:1418/1770 train_time:146303ms step_avg:103.91ms
step:1419/1770 train_time:146411ms step_avg:103.91ms
step:1420/1770 train_time:146518ms step_avg:103.91ms
step:1421/1770 train_time:146625ms step_avg:103.92ms
step:1422/1770 train_time:146733ms step_avg:103.92ms
step:1423/1770 train_time:146840ms step_avg:103.92ms
step:1424/1770 train_time:146949ms step_avg:103.92ms
step:1425/1770 train_time:147056ms step_avg:103.93ms
step:1426/1770 train_time:147164ms step_avg:103.93ms
step:1427/1770 train_time:147272ms step_avg:103.93ms
step:1428/1770 train_time:147382ms step_avg:103.94ms
step:1429/1770 train_time:147490ms step_avg:103.94ms
step:1430/1770 train_time:147597ms step_avg:103.94ms
step:1431/1770 train_time:147707ms step_avg:103.95ms
step:1432/1770 train_time:147814ms step_avg:103.95ms
step:1433/1770 train_time:147921ms step_avg:103.95ms
step:1434/1770 train_time:148029ms step_avg:103.95ms
step:1435/1770 train_time:148136ms step_avg:103.95ms
step:1436/1770 train_time:148245ms step_avg:103.96ms
step:1437/1770 train_time:148354ms step_avg:103.96ms
step:1438/1770 train_time:148461ms step_avg:103.96ms
step:1439/1770 train_time:148569ms step_avg:103.97ms
step:1440/1770 train_time:148676ms step_avg:103.97ms
step:1441/1770 train_time:148786ms step_avg:103.97ms
step:1442/1770 train_time:148893ms step_avg:103.98ms
step:1443/1770 train_time:149001ms step_avg:103.98ms
step:1444/1770 train_time:149109ms step_avg:103.98ms
step:1445/1770 train_time:149217ms step_avg:103.98ms
step:1446/1770 train_time:149325ms step_avg:103.99ms
step:1447/1770 train_time:149434ms step_avg:103.99ms
step:1448/1770 train_time:149544ms step_avg:103.99ms
step:1449/1770 train_time:149654ms step_avg:104.00ms
step:1450/1770 train_time:149762ms step_avg:104.00ms
step:1451/1770 train_time:149872ms step_avg:104.01ms
step:1452/1770 train_time:149980ms step_avg:104.01ms
step:1453/1770 train_time:150089ms step_avg:104.01ms
step:1454/1770 train_time:150198ms step_avg:104.02ms
step:1455/1770 train_time:150308ms step_avg:104.02ms
step:1456/1770 train_time:150418ms step_avg:104.02ms
step:1457/1770 train_time:150529ms step_avg:104.03ms
step:1458/1770 train_time:150637ms step_avg:104.03ms
step:1459/1770 train_time:150747ms step_avg:104.04ms
step:1460/1770 train_time:150855ms step_avg:104.04ms
step:1461/1770 train_time:150964ms step_avg:104.04ms
step:1462/1770 train_time:151073ms step_avg:104.04ms
step:1463/1770 train_time:151181ms step_avg:104.05ms
step:1464/1770 train_time:151293ms step_avg:104.05ms
step:1465/1770 train_time:151402ms step_avg:104.06ms
step:1466/1770 train_time:151512ms step_avg:104.06ms
step:1467/1770 train_time:151623ms step_avg:104.06ms
step:1468/1770 train_time:151731ms step_avg:104.07ms
step:1469/1770 train_time:151840ms step_avg:104.07ms
step:1470/1770 train_time:151949ms step_avg:104.07ms
step:1471/1770 train_time:152058ms step_avg:104.08ms
step:1472/1770 train_time:152168ms step_avg:104.08ms
step:1473/1770 train_time:152278ms step_avg:104.09ms
step:1474/1770 train_time:152388ms step_avg:104.09ms
step:1475/1770 train_time:152496ms step_avg:104.09ms
step:1476/1770 train_time:152604ms step_avg:104.10ms
step:1477/1770 train_time:152714ms step_avg:104.10ms
step:1478/1770 train_time:152824ms step_avg:104.10ms
step:1479/1770 train_time:152932ms step_avg:104.11ms
step:1480/1770 train_time:153041ms step_avg:104.11ms
step:1481/1770 train_time:153154ms step_avg:104.12ms
step:1482/1770 train_time:153261ms step_avg:104.12ms
step:1483/1770 train_time:153370ms step_avg:104.12ms
step:1484/1770 train_time:153479ms step_avg:104.12ms
step:1485/1770 train_time:153587ms step_avg:104.13ms
step:1486/1770 train_time:153696ms step_avg:104.13ms
step:1487/1770 train_time:153805ms step_avg:104.13ms
step:1488/1770 train_time:153914ms step_avg:104.14ms
step:1489/1770 train_time:154025ms step_avg:104.14ms
step:1490/1770 train_time:154135ms step_avg:104.14ms
step:1491/1770 train_time:154243ms step_avg:104.15ms
step:1492/1770 train_time:154352ms step_avg:104.15ms
step:1493/1770 train_time:154464ms step_avg:104.16ms
step:1494/1770 train_time:154576ms step_avg:104.16ms
step:1495/1770 train_time:154684ms step_avg:104.16ms
step:1496/1770 train_time:154793ms step_avg:104.17ms
step:1497/1770 train_time:154902ms step_avg:104.17ms
step:1498/1770 train_time:155011ms step_avg:104.17ms
step:1499/1770 train_time:155119ms step_avg:104.18ms
step:1500/1770 train_time:155227ms step_avg:104.18ms
step:1500/1770 val_loss:3.3432 train_time:155334ms step_avg:104.25ms
step:1501/1770 train_time:155352ms step_avg:104.19ms
step:1502/1770 train_time:155454ms step_avg:104.19ms
step:1503/1770 train_time:155564ms step_avg:104.20ms
step:1504/1770 train_time:155673ms step_avg:104.20ms
step:1505/1770 train_time:155784ms step_avg:104.20ms
step:1506/1770 train_time:155893ms step_avg:104.21ms
step:1507/1770 train_time:156002ms step_avg:104.21ms
step:1508/1770 train_time:156112ms step_avg:104.21ms
step:1509/1770 train_time:156222ms step_avg:104.22ms
step:1510/1770 train_time:156330ms step_avg:104.22ms
step:1511/1770 train_time:156440ms step_avg:104.22ms
step:1512/1770 train_time:156549ms step_avg:104.23ms
step:1513/1770 train_time:156659ms step_avg:104.23ms
step:1514/1770 train_time:156767ms step_avg:104.23ms
step:1515/1770 train_time:156877ms step_avg:104.24ms
step:1516/1770 train_time:156986ms step_avg:104.24ms
step:1517/1770 train_time:157095ms step_avg:104.24ms
step:1518/1770 train_time:157206ms step_avg:104.25ms
step:1519/1770 train_time:157314ms step_avg:104.25ms
step:1520/1770 train_time:157424ms step_avg:104.25ms
step:1521/1770 train_time:157532ms step_avg:104.26ms
step:1522/1770 train_time:157641ms step_avg:104.26ms
step:1523/1770 train_time:157751ms step_avg:104.26ms
step:1524/1770 train_time:157861ms step_avg:104.27ms
step:1525/1770 train_time:157969ms step_avg:104.27ms
step:1526/1770 train_time:158079ms step_avg:104.27ms
step:1527/1770 train_time:158188ms step_avg:104.28ms
step:1528/1770 train_time:158300ms step_avg:104.28ms
step:1529/1770 train_time:158409ms step_avg:104.29ms
step:1530/1770 train_time:158518ms step_avg:104.29ms
step:1531/1770 train_time:158627ms step_avg:104.29ms
step:1532/1770 train_time:158736ms step_avg:104.29ms
step:1533/1770 train_time:158846ms step_avg:104.30ms
step:1534/1770 train_time:158955ms step_avg:104.30ms
step:1535/1770 train_time:159062ms step_avg:104.30ms
step:1536/1770 train_time:159171ms step_avg:104.31ms
step:1537/1770 train_time:159280ms step_avg:104.31ms
step:1538/1770 train_time:159390ms step_avg:104.31ms
step:1539/1770 train_time:159498ms step_avg:104.32ms
step:1540/1770 train_time:159610ms step_avg:104.32ms
step:1541/1770 train_time:159721ms step_avg:104.32ms
step:1542/1770 train_time:159830ms step_avg:104.33ms
step:1543/1770 train_time:159938ms step_avg:104.33ms
step:1544/1770 train_time:160050ms step_avg:104.34ms
step:1545/1770 train_time:160159ms step_avg:104.34ms
step:1546/1770 train_time:160268ms step_avg:104.34ms
step:1547/1770 train_time:160376ms step_avg:104.34ms
step:1548/1770 train_time:160484ms step_avg:104.35ms
step:1549/1770 train_time:160593ms step_avg:104.35ms
step:1550/1770 train_time:160703ms step_avg:104.35ms
step:1551/1770 train_time:160811ms step_avg:104.35ms
step:1552/1770 train_time:160923ms step_avg:104.36ms
step:1553/1770 train_time:161032ms step_avg:104.36ms
step:1554/1770 train_time:161140ms step_avg:104.37ms
step:1555/1770 train_time:161250ms step_avg:104.37ms
step:1556/1770 train_time:161358ms step_avg:104.37ms
step:1557/1770 train_time:161467ms step_avg:104.37ms
step:1558/1770 train_time:161577ms step_avg:104.38ms
step:1559/1770 train_time:161685ms step_avg:104.38ms
step:1560/1770 train_time:161793ms step_avg:104.38ms
step:1561/1770 train_time:161904ms step_avg:104.39ms
step:1562/1770 train_time:162012ms step_avg:104.39ms
step:1563/1770 train_time:162121ms step_avg:104.39ms
step:1564/1770 train_time:162230ms step_avg:104.39ms
step:1565/1770 train_time:162339ms step_avg:104.40ms
step:1566/1770 train_time:162447ms step_avg:104.40ms
step:1567/1770 train_time:162558ms step_avg:104.40ms
step:1568/1770 train_time:162666ms step_avg:104.41ms
step:1569/1770 train_time:162780ms step_avg:104.41ms
step:1570/1770 train_time:162889ms step_avg:104.42ms
step:1571/1770 train_time:162998ms step_avg:104.42ms
step:1572/1770 train_time:163107ms step_avg:104.42ms
step:1573/1770 train_time:163218ms step_avg:104.43ms
step:1574/1770 train_time:163327ms step_avg:104.43ms
step:1575/1770 train_time:163435ms step_avg:104.43ms
step:1576/1770 train_time:163543ms step_avg:104.43ms
step:1577/1770 train_time:163654ms step_avg:104.44ms
step:1578/1770 train_time:163764ms step_avg:104.44ms
step:1579/1770 train_time:163872ms step_avg:104.44ms
step:1580/1770 train_time:163982ms step_avg:104.45ms
step:1581/1770 train_time:164093ms step_avg:104.45ms
step:1582/1770 train_time:164204ms step_avg:104.46ms
step:1583/1770 train_time:164313ms step_avg:104.46ms
step:1584/1770 train_time:164423ms step_avg:104.46ms
step:1585/1770 train_time:164532ms step_avg:104.46ms
step:1586/1770 train_time:164646ms step_avg:104.47ms
step:1587/1770 train_time:164755ms step_avg:104.47ms
step:1588/1770 train_time:164863ms step_avg:104.48ms
step:1589/1770 train_time:164974ms step_avg:104.48ms
step:1590/1770 train_time:165083ms step_avg:104.48ms
step:1591/1770 train_time:165191ms step_avg:104.49ms
step:1592/1770 train_time:165302ms step_avg:104.49ms
step:1593/1770 train_time:165411ms step_avg:104.49ms
step:1594/1770 train_time:165520ms step_avg:104.50ms
step:1595/1770 train_time:165630ms step_avg:104.50ms
step:1596/1770 train_time:165739ms step_avg:104.50ms
step:1597/1770 train_time:165847ms step_avg:104.50ms
step:1598/1770 train_time:165956ms step_avg:104.51ms
step:1599/1770 train_time:166066ms step_avg:104.51ms
step:1600/1770 train_time:166177ms step_avg:104.51ms
step:1601/1770 train_time:166286ms step_avg:104.52ms
step:1602/1770 train_time:166397ms step_avg:104.52ms
step:1603/1770 train_time:166506ms step_avg:104.52ms
step:1604/1770 train_time:166614ms step_avg:104.53ms
step:1605/1770 train_time:166722ms step_avg:104.53ms
step:1606/1770 train_time:166832ms step_avg:104.53ms
step:1607/1770 train_time:166945ms step_avg:104.54ms
step:1608/1770 train_time:167054ms step_avg:104.54ms
step:1609/1770 train_time:167162ms step_avg:104.54ms
step:1610/1770 train_time:167271ms step_avg:104.54ms
step:1611/1770 train_time:167382ms step_avg:104.55ms
step:1612/1770 train_time:167492ms step_avg:104.55ms
step:1613/1770 train_time:167600ms step_avg:104.55ms
step:1614/1770 train_time:167709ms step_avg:104.56ms
step:1615/1770 train_time:167819ms step_avg:104.56ms
step:1616/1770 train_time:167928ms step_avg:104.56ms
step:1617/1770 train_time:168039ms step_avg:104.57ms
step:1618/1770 train_time:168149ms step_avg:104.57ms
step:1619/1770 train_time:168258ms step_avg:104.57ms
step:1620/1770 train_time:168368ms step_avg:104.58ms
step:1621/1770 train_time:168478ms step_avg:104.58ms
step:1622/1770 train_time:168588ms step_avg:104.58ms
step:1623/1770 train_time:168700ms step_avg:104.59ms
step:1624/1770 train_time:168809ms step_avg:104.59ms
step:1625/1770 train_time:168917ms step_avg:104.59ms
step:1625/1770 val_loss:3.3084 train_time:169025ms step_avg:104.66ms
step:1626/1770 train_time:169043ms step_avg:104.61ms
step:1627/1770 train_time:169143ms step_avg:104.60ms
step:1628/1770 train_time:169252ms step_avg:104.61ms
step:1629/1770 train_time:169361ms step_avg:104.61ms
step:1630/1770 train_time:169470ms step_avg:104.61ms
step:1631/1770 train_time:169579ms step_avg:104.61ms
step:1632/1770 train_time:169688ms step_avg:104.62ms
step:1633/1770 train_time:169798ms step_avg:104.62ms
step:1634/1770 train_time:169907ms step_avg:104.62ms
step:1635/1770 train_time:170016ms step_avg:104.63ms
step:1636/1770 train_time:170128ms step_avg:104.63ms
step:1637/1770 train_time:170238ms step_avg:104.63ms
step:1638/1770 train_time:170346ms step_avg:104.64ms
step:1639/1770 train_time:170455ms step_avg:104.64ms
step:1640/1770 train_time:170565ms step_avg:104.64ms
step:1641/1770 train_time:170674ms step_avg:104.64ms
step:1642/1770 train_time:170783ms step_avg:104.65ms
step:1643/1770 train_time:170892ms step_avg:104.65ms
step:1644/1770 train_time:171002ms step_avg:104.65ms
step:1645/1770 train_time:171110ms step_avg:104.65ms
step:1646/1770 train_time:171222ms step_avg:104.66ms
step:1647/1770 train_time:171331ms step_avg:104.66ms
step:1648/1770 train_time:171439ms step_avg:104.66ms
step:1649/1770 train_time:171548ms step_avg:104.67ms
step:1650/1770 train_time:171657ms step_avg:104.67ms
step:1651/1770 train_time:171766ms step_avg:104.67ms
step:1652/1770 train_time:171875ms step_avg:104.67ms
step:1653/1770 train_time:171984ms step_avg:104.68ms
step:1654/1770 train_time:172096ms step_avg:104.68ms
step:1655/1770 train_time:172209ms step_avg:104.69ms
step:1656/1770 train_time:172317ms step_avg:104.69ms
step:1657/1770 train_time:172428ms step_avg:104.69ms
step:1658/1770 train_time:172537ms step_avg:104.69ms
step:1659/1770 train_time:172649ms step_avg:104.70ms
step:1660/1770 train_time:172757ms step_avg:104.70ms
step:1661/1770 train_time:172867ms step_avg:104.70ms
step:1662/1770 train_time:172976ms step_avg:104.71ms
step:1663/1770 train_time:173086ms step_avg:104.71ms
step:1664/1770 train_time:173194ms step_avg:104.71ms
step:1665/1770 train_time:173303ms step_avg:104.71ms
step:1666/1770 train_time:173411ms step_avg:104.72ms
step:1667/1770 train_time:173520ms step_avg:104.72ms
step:1668/1770 train_time:173628ms step_avg:104.72ms
step:1669/1770 train_time:173736ms step_avg:104.72ms
step:1670/1770 train_time:173845ms step_avg:104.73ms
step:1671/1770 train_time:173955ms step_avg:104.73ms
step:1672/1770 train_time:174064ms step_avg:104.73ms
step:1673/1770 train_time:174174ms step_avg:104.73ms
step:1674/1770 train_time:174282ms step_avg:104.74ms
step:1675/1770 train_time:174391ms step_avg:104.74ms
step:1676/1770 train_time:174501ms step_avg:104.74ms
step:1677/1770 train_time:174614ms step_avg:104.75ms
step:1678/1770 train_time:174721ms step_avg:104.75ms
step:1679/1770 train_time:174831ms step_avg:104.75ms
step:1680/1770 train_time:174940ms step_avg:104.75ms
step:1681/1770 train_time:175049ms step_avg:104.76ms
step:1682/1770 train_time:175161ms step_avg:104.76ms
step:1683/1770 train_time:175269ms step_avg:104.76ms
step:1684/1770 train_time:175378ms step_avg:104.77ms
step:1685/1770 train_time:175486ms step_avg:104.77ms
step:1686/1770 train_time:175596ms step_avg:104.77ms
step:1687/1770 train_time:175707ms step_avg:104.77ms
step:1688/1770 train_time:175816ms step_avg:104.78ms
step:1689/1770 train_time:175924ms step_avg:104.78ms
step:1690/1770 train_time:176033ms step_avg:104.78ms
step:1691/1770 train_time:176142ms step_avg:104.78ms
step:1692/1770 train_time:176251ms step_avg:104.79ms
step:1693/1770 train_time:176361ms step_avg:104.79ms
step:1694/1770 train_time:176471ms step_avg:104.79ms
step:1695/1770 train_time:176579ms step_avg:104.79ms
step:1696/1770 train_time:176691ms step_avg:104.80ms
step:1697/1770 train_time:176799ms step_avg:104.80ms
step:1698/1770 train_time:176910ms step_avg:104.80ms
step:1699/1770 train_time:177019ms step_avg:104.81ms
step:1700/1770 train_time:177128ms step_avg:104.81ms
step:1701/1770 train_time:177238ms step_avg:104.81ms
step:1702/1770 train_time:177347ms step_avg:104.82ms
step:1703/1770 train_time:177455ms step_avg:104.82ms
step:1704/1770 train_time:177564ms step_avg:104.82ms
step:1705/1770 train_time:177672ms step_avg:104.82ms
step:1706/1770 train_time:177781ms step_avg:104.82ms
step:1707/1770 train_time:177891ms step_avg:104.83ms
step:1708/1770 train_time:178000ms step_avg:104.83ms
step:1709/1770 train_time:178111ms step_avg:104.83ms
step:1710/1770 train_time:178224ms step_avg:104.84ms
step:1711/1770 train_time:178336ms step_avg:104.84ms
step:1712/1770 train_time:178446ms step_avg:104.84ms
step:1713/1770 train_time:178554ms step_avg:104.85ms
step:1714/1770 train_time:178664ms step_avg:104.85ms
step:1715/1770 train_time:178773ms step_avg:104.85ms
step:1716/1770 train_time:178883ms step_avg:104.86ms
step:1717/1770 train_time:178992ms step_avg:104.86ms
step:1718/1770 train_time:179103ms step_avg:104.86ms
step:1719/1770 train_time:179213ms step_avg:104.86ms
step:1720/1770 train_time:179324ms step_avg:104.87ms
step:1721/1770 train_time:179433ms step_avg:104.87ms
step:1722/1770 train_time:179545ms step_avg:104.87ms
step:1723/1770 train_time:179656ms step_avg:104.88ms
step:1724/1770 train_time:179768ms step_avg:104.88ms
step:1725/1770 train_time:179881ms step_avg:104.89ms
step:1726/1770 train_time:179993ms step_avg:104.89ms
step:1727/1770 train_time:180102ms step_avg:104.89ms
step:1728/1770 train_time:180213ms step_avg:104.90ms
step:1729/1770 train_time:180322ms step_avg:104.90ms
step:1730/1770 train_time:180433ms step_avg:104.90ms
step:1731/1770 train_time:180545ms step_avg:104.91ms
step:1732/1770 train_time:180654ms step_avg:104.91ms
step:1733/1770 train_time:180766ms step_avg:104.91ms
step:1734/1770 train_time:180875ms step_avg:104.92ms
step:1735/1770 train_time:180986ms step_avg:104.92ms
step:1736/1770 train_time:181096ms step_avg:104.92ms
step:1737/1770 train_time:181206ms step_avg:104.93ms
step:1738/1770 train_time:181316ms step_avg:104.93ms
step:1739/1770 train_time:181426ms step_avg:104.93ms
step:1740/1770 train_time:181535ms step_avg:104.93ms
step:1741/1770 train_time:181649ms step_avg:104.94ms
step:1742/1770 train_time:181763ms step_avg:104.94ms
step:1743/1770 train_time:181874ms step_avg:104.95ms
step:1744/1770 train_time:181984ms step_avg:104.95ms
step:1745/1770 train_time:182093ms step_avg:104.95ms
step:1746/1770 train_time:182206ms step_avg:104.96ms
step:1747/1770 train_time:182314ms step_avg:104.96ms
step:1748/1770 train_time:182427ms step_avg:104.96ms
step:1749/1770 train_time:182537ms step_avg:104.97ms
step:1750/1770 train_time:182647ms step_avg:104.97ms
step:1750/1770 val_loss:3.2814 train_time:182755ms step_avg:105.03ms
step:1751/1770 train_time:182773ms step_avg:104.98ms
step:1752/1770 train_time:182877ms step_avg:104.98ms
step:1753/1770 train_time:182988ms step_avg:104.98ms
step:1754/1770 train_time:183098ms step_avg:104.99ms
step:1755/1770 train_time:183208ms step_avg:104.99ms
step:1756/1770 train_time:183319ms step_avg:104.99ms
step:1757/1770 train_time:183429ms step_avg:105.00ms
step:1758/1770 train_time:183539ms step_avg:105.00ms
step:1759/1770 train_time:183649ms step_avg:105.00ms
step:1760/1770 train_time:183759ms step_avg:105.01ms
step:1761/1770 train_time:183872ms step_avg:105.01ms
step:1762/1770 train_time:183986ms step_avg:105.01ms
step:1763/1770 train_time:184095ms step_avg:105.02ms
step:1764/1770 train_time:184206ms step_avg:105.02ms
step:1765/1770 train_time:184317ms step_avg:105.02ms
step:1766/1770 train_time:184430ms step_avg:105.03ms
step:1767/1770 train_time:184538ms step_avg:105.03ms
step:1768/1770 train_time:184648ms step_avg:105.03ms
step:1769/1770 train_time:184757ms step_avg:105.04ms
step:1770/1770 train_time:184866ms step_avg:105.04ms
step:1770/1770 val_loss:3.2784 train_time:184975ms step_avg:105.10ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
