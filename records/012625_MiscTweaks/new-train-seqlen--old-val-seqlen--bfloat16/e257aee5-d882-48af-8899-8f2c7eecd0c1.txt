import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:14:59 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:100977ms step_avg:nanms
step:2/1770 train_time:102145ms step_avg:nanms
step:3/1770 train_time:102244ms step_avg:nanms
step:4/1770 train_time:102341ms step_avg:nanms
step:5/1770 train_time:102440ms step_avg:nanms
step:6/1770 train_time:102540ms step_avg:nanms
step:7/1770 train_time:102639ms step_avg:nanms
step:8/1770 train_time:102737ms step_avg:nanms
step:9/1770 train_time:102836ms step_avg:nanms
step:10/1770 train_time:102935ms step_avg:nanms
step:11/1770 train_time:98ms step_avg:nanms
step:12/1770 train_time:197ms step_avg:nanms
step:13/1770 train_time:297ms step_avg:98.97ms
step:14/1770 train_time:396ms step_avg:98.92ms
step:15/1770 train_time:495ms step_avg:98.90ms
step:16/1770 train_time:594ms step_avg:98.92ms
step:17/1770 train_time:693ms step_avg:98.93ms
step:18/1770 train_time:791ms step_avg:98.88ms
step:19/1770 train_time:890ms step_avg:98.91ms
step:20/1770 train_time:990ms step_avg:98.97ms
step:21/1770 train_time:1089ms step_avg:98.96ms
step:22/1770 train_time:1187ms step_avg:98.94ms
step:23/1770 train_time:1286ms step_avg:98.96ms
step:24/1770 train_time:1386ms step_avg:99.01ms
step:25/1770 train_time:1486ms step_avg:99.05ms
step:26/1770 train_time:1585ms step_avg:99.09ms
step:27/1770 train_time:1684ms step_avg:99.04ms
step:28/1770 train_time:1783ms step_avg:99.04ms
step:29/1770 train_time:1882ms step_avg:99.07ms
step:30/1770 train_time:1982ms step_avg:99.11ms
step:31/1770 train_time:2083ms step_avg:99.18ms
step:32/1770 train_time:2183ms step_avg:99.23ms
step:33/1770 train_time:2283ms step_avg:99.27ms
step:34/1770 train_time:2384ms step_avg:99.33ms
step:35/1770 train_time:2485ms step_avg:99.39ms
step:36/1770 train_time:2585ms step_avg:99.42ms
step:37/1770 train_time:2684ms step_avg:99.41ms
step:38/1770 train_time:2784ms step_avg:99.42ms
step:39/1770 train_time:2883ms step_avg:99.43ms
step:40/1770 train_time:2982ms step_avg:99.41ms
step:41/1770 train_time:3082ms step_avg:99.42ms
step:42/1770 train_time:3181ms step_avg:99.40ms
step:43/1770 train_time:3280ms step_avg:99.41ms
step:44/1770 train_time:3380ms step_avg:99.42ms
step:45/1770 train_time:3481ms step_avg:99.45ms
step:46/1770 train_time:3580ms step_avg:99.45ms
step:47/1770 train_time:3680ms step_avg:99.46ms
step:48/1770 train_time:3780ms step_avg:99.47ms
step:49/1770 train_time:3880ms step_avg:99.48ms
step:50/1770 train_time:3979ms step_avg:99.49ms
step:51/1770 train_time:4079ms step_avg:99.48ms
step:52/1770 train_time:4178ms step_avg:99.47ms
step:53/1770 train_time:4277ms step_avg:99.47ms
step:54/1770 train_time:4376ms step_avg:99.46ms
step:55/1770 train_time:4476ms step_avg:99.46ms
step:56/1770 train_time:4575ms step_avg:99.46ms
step:57/1770 train_time:4674ms step_avg:99.45ms
step:58/1770 train_time:4773ms step_avg:99.43ms
step:59/1770 train_time:4872ms step_avg:99.42ms
step:60/1770 train_time:4971ms step_avg:99.42ms
step:61/1770 train_time:5070ms step_avg:99.40ms
step:62/1770 train_time:5168ms step_avg:99.39ms
step:63/1770 train_time:5266ms step_avg:99.37ms
step:64/1770 train_time:5365ms step_avg:99.36ms
step:65/1770 train_time:5464ms step_avg:99.35ms
step:66/1770 train_time:5563ms step_avg:99.33ms
step:67/1770 train_time:5661ms step_avg:99.32ms
step:68/1770 train_time:5760ms step_avg:99.31ms
step:69/1770 train_time:5859ms step_avg:99.31ms
step:70/1770 train_time:5958ms step_avg:99.30ms
step:71/1770 train_time:6057ms step_avg:99.30ms
step:72/1770 train_time:6156ms step_avg:99.30ms
step:73/1770 train_time:6256ms step_avg:99.30ms
step:74/1770 train_time:6355ms step_avg:99.29ms
step:75/1770 train_time:6454ms step_avg:99.29ms
step:76/1770 train_time:6553ms step_avg:99.29ms
step:77/1770 train_time:6651ms step_avg:99.27ms
step:78/1770 train_time:6751ms step_avg:99.27ms
step:79/1770 train_time:6849ms step_avg:99.26ms
step:80/1770 train_time:6948ms step_avg:99.26ms
step:81/1770 train_time:7047ms step_avg:99.25ms
step:82/1770 train_time:7147ms step_avg:99.26ms
step:83/1770 train_time:7245ms step_avg:99.25ms
step:84/1770 train_time:7344ms step_avg:99.25ms
step:85/1770 train_time:7443ms step_avg:99.24ms
step:86/1770 train_time:7541ms step_avg:99.23ms
step:87/1770 train_time:7640ms step_avg:99.22ms
step:88/1770 train_time:7739ms step_avg:99.21ms
step:89/1770 train_time:7838ms step_avg:99.22ms
step:90/1770 train_time:7937ms step_avg:99.21ms
step:91/1770 train_time:8037ms step_avg:99.22ms
step:92/1770 train_time:8136ms step_avg:99.22ms
step:93/1770 train_time:8235ms step_avg:99.21ms
step:94/1770 train_time:8334ms step_avg:99.21ms
step:95/1770 train_time:8432ms step_avg:99.20ms
step:96/1770 train_time:8531ms step_avg:99.20ms
step:97/1770 train_time:8631ms step_avg:99.20ms
step:98/1770 train_time:8730ms step_avg:99.21ms
step:99/1770 train_time:8829ms step_avg:99.20ms
step:100/1770 train_time:8928ms step_avg:99.20ms
step:101/1770 train_time:9027ms step_avg:99.20ms
step:102/1770 train_time:9126ms step_avg:99.20ms
step:103/1770 train_time:9225ms step_avg:99.19ms
step:104/1770 train_time:9324ms step_avg:99.20ms
step:105/1770 train_time:9424ms step_avg:99.20ms
step:106/1770 train_time:9524ms step_avg:99.21ms
step:107/1770 train_time:9623ms step_avg:99.20ms
step:108/1770 train_time:9722ms step_avg:99.20ms
step:109/1770 train_time:9822ms step_avg:99.21ms
step:110/1770 train_time:9920ms step_avg:99.20ms
step:111/1770 train_time:10019ms step_avg:99.20ms
step:112/1770 train_time:10118ms step_avg:99.20ms
step:113/1770 train_time:10217ms step_avg:99.20ms
step:114/1770 train_time:10317ms step_avg:99.20ms
step:115/1770 train_time:10416ms step_avg:99.20ms
step:116/1770 train_time:10516ms step_avg:99.21ms
step:117/1770 train_time:10615ms step_avg:99.21ms
step:118/1770 train_time:10714ms step_avg:99.21ms
step:119/1770 train_time:10813ms step_avg:99.21ms
step:120/1770 train_time:10913ms step_avg:99.20ms
step:121/1770 train_time:11012ms step_avg:99.21ms
step:122/1770 train_time:11111ms step_avg:99.21ms
step:123/1770 train_time:11210ms step_avg:99.21ms
step:124/1770 train_time:11309ms step_avg:99.20ms
step:125/1770 train_time:11407ms step_avg:99.19ms
step:125/1770 val_loss:4.6409 train_time:11505ms step_avg:100.04ms
step:126/1770 train_time:11526ms step_avg:99.36ms
step:127/1770 train_time:11618ms step_avg:99.30ms
step:128/1770 train_time:11723ms step_avg:99.35ms
step:129/1770 train_time:11822ms step_avg:99.35ms
step:130/1770 train_time:11921ms step_avg:99.34ms
step:131/1770 train_time:12021ms step_avg:99.35ms
step:132/1770 train_time:12120ms step_avg:99.35ms
step:133/1770 train_time:12219ms step_avg:99.34ms
step:134/1770 train_time:12319ms step_avg:99.35ms
step:135/1770 train_time:12418ms step_avg:99.35ms
step:136/1770 train_time:12518ms step_avg:99.35ms
step:137/1770 train_time:12617ms step_avg:99.35ms
step:138/1770 train_time:12717ms step_avg:99.36ms
step:139/1770 train_time:12818ms step_avg:99.37ms
step:140/1770 train_time:12918ms step_avg:99.37ms
step:141/1770 train_time:13018ms step_avg:99.38ms
step:142/1770 train_time:13118ms step_avg:99.38ms
step:143/1770 train_time:13218ms step_avg:99.38ms
step:144/1770 train_time:13317ms step_avg:99.38ms
step:145/1770 train_time:13416ms step_avg:99.38ms
step:146/1770 train_time:13515ms step_avg:99.38ms
step:147/1770 train_time:13614ms step_avg:99.37ms
step:148/1770 train_time:13714ms step_avg:99.37ms
step:149/1770 train_time:13814ms step_avg:99.38ms
step:150/1770 train_time:13913ms step_avg:99.38ms
step:151/1770 train_time:14012ms step_avg:99.38ms
step:152/1770 train_time:14111ms step_avg:99.38ms
step:153/1770 train_time:14211ms step_avg:99.38ms
step:154/1770 train_time:14311ms step_avg:99.38ms
step:155/1770 train_time:14411ms step_avg:99.39ms
step:156/1770 train_time:14510ms step_avg:99.38ms
step:157/1770 train_time:14609ms step_avg:99.38ms
step:158/1770 train_time:14709ms step_avg:99.39ms
step:159/1770 train_time:14809ms step_avg:99.39ms
step:160/1770 train_time:14909ms step_avg:99.39ms
step:161/1770 train_time:15009ms step_avg:99.40ms
step:162/1770 train_time:15110ms step_avg:99.41ms
step:163/1770 train_time:15209ms step_avg:99.41ms
step:164/1770 train_time:15309ms step_avg:99.41ms
step:165/1770 train_time:15410ms step_avg:99.42ms
step:166/1770 train_time:15509ms step_avg:99.42ms
step:167/1770 train_time:15609ms step_avg:99.42ms
step:168/1770 train_time:15709ms step_avg:99.42ms
step:169/1770 train_time:15809ms step_avg:99.43ms
step:170/1770 train_time:15908ms step_avg:99.43ms
step:171/1770 train_time:16008ms step_avg:99.43ms
step:172/1770 train_time:16108ms step_avg:99.43ms
step:173/1770 train_time:16208ms step_avg:99.44ms
step:174/1770 train_time:16309ms step_avg:99.45ms
step:175/1770 train_time:16410ms step_avg:99.45ms
step:176/1770 train_time:16511ms step_avg:99.46ms
step:177/1770 train_time:16609ms step_avg:99.46ms
step:178/1770 train_time:16709ms step_avg:99.46ms
step:179/1770 train_time:16810ms step_avg:99.47ms
step:180/1770 train_time:16910ms step_avg:99.47ms
step:181/1770 train_time:17010ms step_avg:99.47ms
step:182/1770 train_time:17110ms step_avg:99.48ms
step:183/1770 train_time:17211ms step_avg:99.48ms
step:184/1770 train_time:17310ms step_avg:99.48ms
step:185/1770 train_time:17410ms step_avg:99.49ms
step:186/1770 train_time:17511ms step_avg:99.49ms
step:187/1770 train_time:17611ms step_avg:99.50ms
step:188/1770 train_time:17710ms step_avg:99.50ms
step:189/1770 train_time:17810ms step_avg:99.50ms
step:190/1770 train_time:17911ms step_avg:99.50ms
step:191/1770 train_time:18009ms step_avg:99.50ms
step:192/1770 train_time:18110ms step_avg:99.51ms
step:193/1770 train_time:18210ms step_avg:99.51ms
step:194/1770 train_time:18310ms step_avg:99.51ms
step:195/1770 train_time:18410ms step_avg:99.51ms
step:196/1770 train_time:18510ms step_avg:99.52ms
step:197/1770 train_time:18612ms step_avg:99.53ms
step:198/1770 train_time:18709ms step_avg:99.52ms
step:199/1770 train_time:18809ms step_avg:99.52ms
step:200/1770 train_time:18909ms step_avg:99.52ms
step:201/1770 train_time:19009ms step_avg:99.52ms
step:202/1770 train_time:19108ms step_avg:99.52ms
step:203/1770 train_time:19208ms step_avg:99.52ms
step:204/1770 train_time:19310ms step_avg:99.54ms
step:205/1770 train_time:19409ms step_avg:99.53ms
step:206/1770 train_time:19509ms step_avg:99.54ms
step:207/1770 train_time:19610ms step_avg:99.54ms
step:208/1770 train_time:19711ms step_avg:99.55ms
step:209/1770 train_time:19810ms step_avg:99.55ms
step:210/1770 train_time:19910ms step_avg:99.55ms
step:211/1770 train_time:20011ms step_avg:99.56ms
step:212/1770 train_time:20110ms step_avg:99.55ms
step:213/1770 train_time:20210ms step_avg:99.55ms
step:214/1770 train_time:20309ms step_avg:99.56ms
step:215/1770 train_time:20409ms step_avg:99.56ms
step:216/1770 train_time:20509ms step_avg:99.56ms
step:217/1770 train_time:20609ms step_avg:99.56ms
step:218/1770 train_time:20709ms step_avg:99.56ms
step:219/1770 train_time:20811ms step_avg:99.57ms
step:220/1770 train_time:20911ms step_avg:99.57ms
step:221/1770 train_time:21011ms step_avg:99.58ms
step:222/1770 train_time:21110ms step_avg:99.58ms
step:223/1770 train_time:21210ms step_avg:99.58ms
step:224/1770 train_time:21310ms step_avg:99.58ms
step:225/1770 train_time:21409ms step_avg:99.58ms
step:226/1770 train_time:21508ms step_avg:99.58ms
step:227/1770 train_time:21607ms step_avg:99.57ms
step:228/1770 train_time:21708ms step_avg:99.58ms
step:229/1770 train_time:21810ms step_avg:99.59ms
step:230/1770 train_time:21912ms step_avg:99.60ms
step:231/1770 train_time:22011ms step_avg:99.60ms
step:232/1770 train_time:22111ms step_avg:99.60ms
step:233/1770 train_time:22211ms step_avg:99.60ms
step:234/1770 train_time:22311ms step_avg:99.60ms
step:235/1770 train_time:22411ms step_avg:99.60ms
step:236/1770 train_time:22511ms step_avg:99.61ms
step:237/1770 train_time:22611ms step_avg:99.61ms
step:238/1770 train_time:22710ms step_avg:99.61ms
step:239/1770 train_time:22810ms step_avg:99.61ms
step:240/1770 train_time:22909ms step_avg:99.60ms
step:241/1770 train_time:23009ms step_avg:99.61ms
step:242/1770 train_time:23110ms step_avg:99.61ms
step:243/1770 train_time:23210ms step_avg:99.62ms
step:244/1770 train_time:23311ms step_avg:99.62ms
step:245/1770 train_time:23411ms step_avg:99.62ms
step:246/1770 train_time:23511ms step_avg:99.62ms
step:247/1770 train_time:23610ms step_avg:99.62ms
step:248/1770 train_time:23711ms step_avg:99.63ms
step:249/1770 train_time:23811ms step_avg:99.63ms
step:250/1770 train_time:23910ms step_avg:99.63ms
step:250/1770 val_loss:4.0992 train_time:24009ms step_avg:100.04ms
step:251/1770 train_time:24027ms step_avg:99.70ms
step:252/1770 train_time:24121ms step_avg:99.68ms
step:253/1770 train_time:24224ms step_avg:99.69ms
step:254/1770 train_time:24325ms step_avg:99.69ms
step:255/1770 train_time:24424ms step_avg:99.69ms
step:256/1770 train_time:24523ms step_avg:99.69ms
step:257/1770 train_time:24622ms step_avg:99.69ms
step:258/1770 train_time:24722ms step_avg:99.68ms
step:259/1770 train_time:24820ms step_avg:99.68ms
step:260/1770 train_time:24919ms step_avg:99.68ms
step:261/1770 train_time:25019ms step_avg:99.68ms
step:262/1770 train_time:25119ms step_avg:99.68ms
step:263/1770 train_time:25219ms step_avg:99.68ms
step:264/1770 train_time:25319ms step_avg:99.68ms
step:265/1770 train_time:25421ms step_avg:99.69ms
step:266/1770 train_time:25522ms step_avg:99.69ms
step:267/1770 train_time:25622ms step_avg:99.70ms
step:268/1770 train_time:25721ms step_avg:99.70ms
step:269/1770 train_time:25821ms step_avg:99.70ms
step:270/1770 train_time:25921ms step_avg:99.70ms
step:271/1770 train_time:26021ms step_avg:99.70ms
step:272/1770 train_time:26121ms step_avg:99.70ms
step:273/1770 train_time:26221ms step_avg:99.70ms
step:274/1770 train_time:26321ms step_avg:99.70ms
step:275/1770 train_time:26420ms step_avg:99.70ms
step:276/1770 train_time:26521ms step_avg:99.70ms
step:277/1770 train_time:26620ms step_avg:99.70ms
step:278/1770 train_time:26720ms step_avg:99.70ms
step:279/1770 train_time:26821ms step_avg:99.71ms
step:280/1770 train_time:26920ms step_avg:99.70ms
step:281/1770 train_time:27021ms step_avg:99.71ms
step:282/1770 train_time:27121ms step_avg:99.71ms
step:283/1770 train_time:27220ms step_avg:99.71ms
step:284/1770 train_time:27321ms step_avg:99.71ms
step:285/1770 train_time:27421ms step_avg:99.71ms
step:286/1770 train_time:27522ms step_avg:99.72ms
step:287/1770 train_time:27620ms step_avg:99.71ms
step:288/1770 train_time:27720ms step_avg:99.71ms
step:289/1770 train_time:27821ms step_avg:99.72ms
step:290/1770 train_time:27920ms step_avg:99.71ms
step:291/1770 train_time:28020ms step_avg:99.72ms
step:292/1770 train_time:28120ms step_avg:99.72ms
step:293/1770 train_time:28221ms step_avg:99.72ms
step:294/1770 train_time:28320ms step_avg:99.72ms
step:295/1770 train_time:28420ms step_avg:99.72ms
step:296/1770 train_time:28519ms step_avg:99.72ms
step:297/1770 train_time:28619ms step_avg:99.72ms
step:298/1770 train_time:28720ms step_avg:99.72ms
step:299/1770 train_time:28820ms step_avg:99.72ms
step:300/1770 train_time:28921ms step_avg:99.73ms
step:301/1770 train_time:29020ms step_avg:99.72ms
step:302/1770 train_time:29120ms step_avg:99.73ms
step:303/1770 train_time:29220ms step_avg:99.73ms
step:304/1770 train_time:29320ms step_avg:99.73ms
step:305/1770 train_time:29420ms step_avg:99.73ms
step:306/1770 train_time:29520ms step_avg:99.73ms
step:307/1770 train_time:29620ms step_avg:99.73ms
step:308/1770 train_time:29719ms step_avg:99.73ms
step:309/1770 train_time:29820ms step_avg:99.73ms
step:310/1770 train_time:29920ms step_avg:99.73ms
step:311/1770 train_time:30020ms step_avg:99.73ms
step:312/1770 train_time:30120ms step_avg:99.74ms
step:313/1770 train_time:30220ms step_avg:99.74ms
step:314/1770 train_time:30321ms step_avg:99.74ms
step:315/1770 train_time:30420ms step_avg:99.74ms
step:316/1770 train_time:30520ms step_avg:99.74ms
step:317/1770 train_time:30620ms step_avg:99.74ms
step:318/1770 train_time:30720ms step_avg:99.74ms
step:319/1770 train_time:30820ms step_avg:99.74ms
step:320/1770 train_time:30920ms step_avg:99.74ms
step:321/1770 train_time:31021ms step_avg:99.75ms
step:322/1770 train_time:31121ms step_avg:99.75ms
step:323/1770 train_time:31221ms step_avg:99.75ms
step:324/1770 train_time:31320ms step_avg:99.75ms
step:325/1770 train_time:31420ms step_avg:99.75ms
step:326/1770 train_time:31520ms step_avg:99.75ms
step:327/1770 train_time:31620ms step_avg:99.75ms
step:328/1770 train_time:31720ms step_avg:99.75ms
step:329/1770 train_time:31820ms step_avg:99.75ms
step:330/1770 train_time:31920ms step_avg:99.75ms
step:331/1770 train_time:32020ms step_avg:99.75ms
step:332/1770 train_time:32120ms step_avg:99.75ms
step:333/1770 train_time:32220ms step_avg:99.75ms
step:334/1770 train_time:32320ms step_avg:99.75ms
step:335/1770 train_time:32421ms step_avg:99.76ms
step:336/1770 train_time:32520ms step_avg:99.75ms
step:337/1770 train_time:32620ms step_avg:99.76ms
step:338/1770 train_time:32720ms step_avg:99.76ms
step:339/1770 train_time:32820ms step_avg:99.76ms
step:340/1770 train_time:32920ms step_avg:99.76ms
step:341/1770 train_time:33020ms step_avg:99.76ms
step:342/1770 train_time:33120ms step_avg:99.76ms
step:343/1770 train_time:33219ms step_avg:99.76ms
step:344/1770 train_time:33320ms step_avg:99.76ms
step:345/1770 train_time:33420ms step_avg:99.76ms
step:346/1770 train_time:33520ms step_avg:99.76ms
step:347/1770 train_time:33620ms step_avg:99.76ms
step:348/1770 train_time:33720ms step_avg:99.76ms
step:349/1770 train_time:33821ms step_avg:99.77ms
step:350/1770 train_time:33920ms step_avg:99.76ms
step:351/1770 train_time:34020ms step_avg:99.77ms
step:352/1770 train_time:34121ms step_avg:99.77ms
step:353/1770 train_time:34220ms step_avg:99.77ms
step:354/1770 train_time:34321ms step_avg:99.77ms
step:355/1770 train_time:34421ms step_avg:99.77ms
step:356/1770 train_time:34521ms step_avg:99.77ms
step:357/1770 train_time:34620ms step_avg:99.77ms
step:358/1770 train_time:34720ms step_avg:99.77ms
step:359/1770 train_time:34821ms step_avg:99.77ms
step:360/1770 train_time:34921ms step_avg:99.77ms
step:361/1770 train_time:35020ms step_avg:99.77ms
step:362/1770 train_time:35120ms step_avg:99.77ms
step:363/1770 train_time:35221ms step_avg:99.78ms
step:364/1770 train_time:35320ms step_avg:99.77ms
step:365/1770 train_time:35420ms step_avg:99.78ms
step:366/1770 train_time:35521ms step_avg:99.78ms
step:367/1770 train_time:35621ms step_avg:99.78ms
step:368/1770 train_time:35721ms step_avg:99.78ms
step:369/1770 train_time:35820ms step_avg:99.78ms
step:370/1770 train_time:35921ms step_avg:99.78ms
step:371/1770 train_time:36021ms step_avg:99.78ms
step:372/1770 train_time:36121ms step_avg:99.78ms
step:373/1770 train_time:36221ms step_avg:99.78ms
step:374/1770 train_time:36321ms step_avg:99.78ms
step:375/1770 train_time:36422ms step_avg:99.79ms
step:375/1770 val_loss:3.8994 train_time:36520ms step_avg:100.05ms
step:376/1770 train_time:36539ms step_avg:99.83ms
step:377/1770 train_time:36634ms step_avg:99.82ms
step:378/1770 train_time:36739ms step_avg:99.83ms
step:379/1770 train_time:36839ms step_avg:99.83ms
step:380/1770 train_time:36939ms step_avg:99.83ms
step:381/1770 train_time:37039ms step_avg:99.83ms
step:382/1770 train_time:37139ms step_avg:99.84ms
step:383/1770 train_time:37238ms step_avg:99.84ms
step:384/1770 train_time:37338ms step_avg:99.83ms
step:385/1770 train_time:37438ms step_avg:99.83ms
step:386/1770 train_time:37538ms step_avg:99.84ms
step:387/1770 train_time:37638ms step_avg:99.83ms
step:388/1770 train_time:37738ms step_avg:99.84ms
step:389/1770 train_time:37838ms step_avg:99.84ms
step:390/1770 train_time:37938ms step_avg:99.84ms
step:391/1770 train_time:38038ms step_avg:99.84ms
step:392/1770 train_time:38138ms step_avg:99.84ms
step:393/1770 train_time:38238ms step_avg:99.84ms
step:394/1770 train_time:38338ms step_avg:99.84ms
step:395/1770 train_time:38438ms step_avg:99.84ms
step:396/1770 train_time:38540ms step_avg:99.84ms
step:397/1770 train_time:38642ms step_avg:99.85ms
step:398/1770 train_time:38743ms step_avg:99.85ms
step:399/1770 train_time:38845ms step_avg:99.86ms
step:400/1770 train_time:38947ms step_avg:99.86ms
step:401/1770 train_time:39050ms step_avg:99.87ms
step:402/1770 train_time:39152ms step_avg:99.88ms
step:403/1770 train_time:39254ms step_avg:99.88ms
step:404/1770 train_time:39357ms step_avg:99.89ms
step:405/1770 train_time:39459ms step_avg:99.90ms
step:406/1770 train_time:39561ms step_avg:99.90ms
step:407/1770 train_time:39663ms step_avg:99.91ms
step:408/1770 train_time:39766ms step_avg:99.91ms
step:409/1770 train_time:39867ms step_avg:99.92ms
step:410/1770 train_time:39970ms step_avg:99.92ms
step:411/1770 train_time:40073ms step_avg:99.93ms
step:412/1770 train_time:40175ms step_avg:99.94ms
step:413/1770 train_time:40278ms step_avg:99.94ms
step:414/1770 train_time:40380ms step_avg:99.95ms
step:415/1770 train_time:40482ms step_avg:99.95ms
step:416/1770 train_time:40584ms step_avg:99.96ms
step:417/1770 train_time:40685ms step_avg:99.96ms
step:418/1770 train_time:40788ms step_avg:99.97ms
step:419/1770 train_time:40891ms step_avg:99.98ms
step:420/1770 train_time:40993ms step_avg:99.98ms
step:421/1770 train_time:41095ms step_avg:99.99ms
step:422/1770 train_time:41197ms step_avg:99.99ms
step:423/1770 train_time:41298ms step_avg:100.00ms
step:424/1770 train_time:41400ms step_avg:100.00ms
step:425/1770 train_time:41502ms step_avg:100.00ms
step:426/1770 train_time:41604ms step_avg:100.01ms
step:427/1770 train_time:41706ms step_avg:100.01ms
step:428/1770 train_time:41808ms step_avg:100.02ms
step:429/1770 train_time:41911ms step_avg:100.03ms
step:430/1770 train_time:42014ms step_avg:100.03ms
step:431/1770 train_time:42117ms step_avg:100.04ms
step:432/1770 train_time:42219ms step_avg:100.05ms
step:433/1770 train_time:42322ms step_avg:100.05ms
step:434/1770 train_time:42424ms step_avg:100.06ms
step:435/1770 train_time:42526ms step_avg:100.06ms
step:436/1770 train_time:42628ms step_avg:100.07ms
step:437/1770 train_time:42731ms step_avg:100.07ms
step:438/1770 train_time:42833ms step_avg:100.08ms
step:439/1770 train_time:42935ms step_avg:100.08ms
step:440/1770 train_time:43037ms step_avg:100.09ms
step:441/1770 train_time:43139ms step_avg:100.09ms
step:442/1770 train_time:43241ms step_avg:100.09ms
step:443/1770 train_time:43343ms step_avg:100.10ms
step:444/1770 train_time:43444ms step_avg:100.10ms
step:445/1770 train_time:43546ms step_avg:100.11ms
step:446/1770 train_time:43649ms step_avg:100.11ms
step:447/1770 train_time:43752ms step_avg:100.12ms
step:448/1770 train_time:43853ms step_avg:100.12ms
step:449/1770 train_time:43956ms step_avg:100.13ms
step:450/1770 train_time:44058ms step_avg:100.13ms
step:451/1770 train_time:44160ms step_avg:100.14ms
step:452/1770 train_time:44263ms step_avg:100.14ms
step:453/1770 train_time:44365ms step_avg:100.15ms
step:454/1770 train_time:44467ms step_avg:100.15ms
step:455/1770 train_time:44570ms step_avg:100.16ms
step:456/1770 train_time:44672ms step_avg:100.16ms
step:457/1770 train_time:44774ms step_avg:100.17ms
step:458/1770 train_time:44877ms step_avg:100.17ms
step:459/1770 train_time:44979ms step_avg:100.18ms
step:460/1770 train_time:45081ms step_avg:100.18ms
step:461/1770 train_time:45183ms step_avg:100.18ms
step:462/1770 train_time:45285ms step_avg:100.19ms
step:463/1770 train_time:45387ms step_avg:100.19ms
step:464/1770 train_time:45491ms step_avg:100.20ms
step:465/1770 train_time:45592ms step_avg:100.20ms
step:466/1770 train_time:45694ms step_avg:100.21ms
step:467/1770 train_time:45797ms step_avg:100.21ms
step:468/1770 train_time:45899ms step_avg:100.22ms
step:469/1770 train_time:46001ms step_avg:100.22ms
step:470/1770 train_time:46102ms step_avg:100.22ms
step:471/1770 train_time:46204ms step_avg:100.23ms
step:472/1770 train_time:46306ms step_avg:100.23ms
step:473/1770 train_time:46409ms step_avg:100.23ms
step:474/1770 train_time:46511ms step_avg:100.24ms
step:475/1770 train_time:46614ms step_avg:100.24ms
step:476/1770 train_time:46716ms step_avg:100.25ms
step:477/1770 train_time:46819ms step_avg:100.25ms
step:478/1770 train_time:46921ms step_avg:100.26ms
step:479/1770 train_time:47023ms step_avg:100.26ms
step:480/1770 train_time:47125ms step_avg:100.27ms
step:481/1770 train_time:47228ms step_avg:100.27ms
step:482/1770 train_time:47330ms step_avg:100.28ms
step:483/1770 train_time:47432ms step_avg:100.28ms
step:484/1770 train_time:47535ms step_avg:100.28ms
step:485/1770 train_time:47638ms step_avg:100.29ms
step:486/1770 train_time:47740ms step_avg:100.29ms
step:487/1770 train_time:47842ms step_avg:100.30ms
step:488/1770 train_time:47944ms step_avg:100.30ms
step:489/1770 train_time:48045ms step_avg:100.30ms
step:490/1770 train_time:48147ms step_avg:100.31ms
step:491/1770 train_time:48249ms step_avg:100.31ms
step:492/1770 train_time:48352ms step_avg:100.32ms
step:493/1770 train_time:48454ms step_avg:100.32ms
step:494/1770 train_time:48556ms step_avg:100.32ms
step:495/1770 train_time:48659ms step_avg:100.33ms
step:496/1770 train_time:48761ms step_avg:100.33ms
step:497/1770 train_time:48863ms step_avg:100.34ms
step:498/1770 train_time:48966ms step_avg:100.34ms
step:499/1770 train_time:49068ms step_avg:100.34ms
step:500/1770 train_time:49171ms step_avg:100.35ms
step:500/1770 val_loss:3.7508 train_time:49271ms step_avg:100.55ms
step:501/1770 train_time:49290ms step_avg:100.39ms
step:502/1770 train_time:49383ms step_avg:100.37ms
step:503/1770 train_time:49491ms step_avg:100.39ms
step:504/1770 train_time:49594ms step_avg:100.39ms
step:505/1770 train_time:49696ms step_avg:100.40ms
step:506/1770 train_time:49798ms step_avg:100.40ms
step:507/1770 train_time:49900ms step_avg:100.40ms
step:508/1770 train_time:50002ms step_avg:100.41ms
step:509/1770 train_time:50105ms step_avg:100.41ms
step:510/1770 train_time:50207ms step_avg:100.41ms
step:511/1770 train_time:50309ms step_avg:100.42ms
step:512/1770 train_time:50412ms step_avg:100.42ms
step:513/1770 train_time:50514ms step_avg:100.43ms
step:514/1770 train_time:50616ms step_avg:100.43ms
step:515/1770 train_time:50719ms step_avg:100.43ms
step:516/1770 train_time:50822ms step_avg:100.44ms
step:517/1770 train_time:50924ms step_avg:100.44ms
step:518/1770 train_time:51026ms step_avg:100.44ms
step:519/1770 train_time:51128ms step_avg:100.45ms
step:520/1770 train_time:51230ms step_avg:100.45ms
step:521/1770 train_time:51333ms step_avg:100.45ms
step:522/1770 train_time:51435ms step_avg:100.46ms
step:523/1770 train_time:51537ms step_avg:100.46ms
step:524/1770 train_time:51640ms step_avg:100.47ms
step:525/1770 train_time:51742ms step_avg:100.47ms
step:526/1770 train_time:51843ms step_avg:100.47ms
step:527/1770 train_time:51945ms step_avg:100.47ms
step:528/1770 train_time:52047ms step_avg:100.48ms
step:529/1770 train_time:52150ms step_avg:100.48ms
step:530/1770 train_time:52252ms step_avg:100.49ms
step:531/1770 train_time:52355ms step_avg:100.49ms
step:532/1770 train_time:52458ms step_avg:100.49ms
step:533/1770 train_time:52560ms step_avg:100.50ms
step:534/1770 train_time:52663ms step_avg:100.50ms
step:535/1770 train_time:52765ms step_avg:100.51ms
step:536/1770 train_time:52868ms step_avg:100.51ms
step:537/1770 train_time:52970ms step_avg:100.51ms
step:538/1770 train_time:53073ms step_avg:100.52ms
step:539/1770 train_time:53176ms step_avg:100.52ms
step:540/1770 train_time:53279ms step_avg:100.53ms
step:541/1770 train_time:53381ms step_avg:100.53ms
step:542/1770 train_time:53483ms step_avg:100.53ms
step:543/1770 train_time:53586ms step_avg:100.54ms
step:544/1770 train_time:53689ms step_avg:100.54ms
step:545/1770 train_time:53792ms step_avg:100.54ms
step:546/1770 train_time:53894ms step_avg:100.55ms
step:547/1770 train_time:53997ms step_avg:100.55ms
step:548/1770 train_time:54099ms step_avg:100.56ms
step:549/1770 train_time:54202ms step_avg:100.56ms
step:550/1770 train_time:54304ms step_avg:100.56ms
step:551/1770 train_time:54406ms step_avg:100.57ms
step:552/1770 train_time:54509ms step_avg:100.57ms
step:553/1770 train_time:54612ms step_avg:100.57ms
step:554/1770 train_time:54715ms step_avg:100.58ms
step:555/1770 train_time:54817ms step_avg:100.58ms
step:556/1770 train_time:54920ms step_avg:100.59ms
step:557/1770 train_time:55023ms step_avg:100.59ms
step:558/1770 train_time:55126ms step_avg:100.59ms
step:559/1770 train_time:55228ms step_avg:100.60ms
step:560/1770 train_time:55330ms step_avg:100.60ms
step:561/1770 train_time:55433ms step_avg:100.61ms
step:562/1770 train_time:55536ms step_avg:100.61ms
step:563/1770 train_time:55639ms step_avg:100.61ms
step:564/1770 train_time:55742ms step_avg:100.62ms
step:565/1770 train_time:55844ms step_avg:100.62ms
step:566/1770 train_time:55946ms step_avg:100.62ms
step:567/1770 train_time:56048ms step_avg:100.62ms
step:568/1770 train_time:56151ms step_avg:100.63ms
step:569/1770 train_time:56254ms step_avg:100.63ms
step:570/1770 train_time:56357ms step_avg:100.64ms
step:571/1770 train_time:56459ms step_avg:100.64ms
step:572/1770 train_time:56562ms step_avg:100.64ms
step:573/1770 train_time:56664ms step_avg:100.65ms
step:574/1770 train_time:56767ms step_avg:100.65ms
step:575/1770 train_time:56869ms step_avg:100.65ms
step:576/1770 train_time:56972ms step_avg:100.66ms
step:577/1770 train_time:57074ms step_avg:100.66ms
step:578/1770 train_time:57177ms step_avg:100.66ms
step:579/1770 train_time:57280ms step_avg:100.67ms
step:580/1770 train_time:57383ms step_avg:100.67ms
step:581/1770 train_time:57485ms step_avg:100.67ms
step:582/1770 train_time:57587ms step_avg:100.68ms
step:583/1770 train_time:57690ms step_avg:100.68ms
step:584/1770 train_time:57793ms step_avg:100.68ms
step:585/1770 train_time:57895ms step_avg:100.69ms
step:586/1770 train_time:57998ms step_avg:100.69ms
step:587/1770 train_time:58101ms step_avg:100.69ms
step:588/1770 train_time:58203ms step_avg:100.70ms
step:589/1770 train_time:58305ms step_avg:100.70ms
step:590/1770 train_time:58408ms step_avg:100.70ms
step:591/1770 train_time:58510ms step_avg:100.71ms
step:592/1770 train_time:58613ms step_avg:100.71ms
step:593/1770 train_time:58716ms step_avg:100.71ms
step:594/1770 train_time:58819ms step_avg:100.72ms
step:595/1770 train_time:58922ms step_avg:100.72ms
step:596/1770 train_time:59024ms step_avg:100.72ms
step:597/1770 train_time:59127ms step_avg:100.73ms
step:598/1770 train_time:59230ms step_avg:100.73ms
step:599/1770 train_time:59333ms step_avg:100.74ms
step:600/1770 train_time:59435ms step_avg:100.74ms
step:601/1770 train_time:59538ms step_avg:100.74ms
step:602/1770 train_time:59640ms step_avg:100.74ms
step:603/1770 train_time:59743ms step_avg:100.75ms
step:604/1770 train_time:59844ms step_avg:100.75ms
step:605/1770 train_time:59946ms step_avg:100.75ms
step:606/1770 train_time:60049ms step_avg:100.75ms
step:607/1770 train_time:60152ms step_avg:100.76ms
step:608/1770 train_time:60255ms step_avg:100.76ms
step:609/1770 train_time:60357ms step_avg:100.76ms
step:610/1770 train_time:60460ms step_avg:100.77ms
step:611/1770 train_time:60562ms step_avg:100.77ms
step:612/1770 train_time:60665ms step_avg:100.77ms
step:613/1770 train_time:60768ms step_avg:100.78ms
step:614/1770 train_time:60871ms step_avg:100.78ms
step:615/1770 train_time:60974ms step_avg:100.78ms
step:616/1770 train_time:61077ms step_avg:100.79ms
step:617/1770 train_time:61179ms step_avg:100.79ms
step:618/1770 train_time:61283ms step_avg:100.79ms
step:619/1770 train_time:61385ms step_avg:100.80ms
step:620/1770 train_time:61488ms step_avg:100.80ms
step:621/1770 train_time:61591ms step_avg:100.80ms
step:622/1770 train_time:61694ms step_avg:100.81ms
step:623/1770 train_time:61796ms step_avg:100.81ms
step:624/1770 train_time:61899ms step_avg:100.81ms
step:625/1770 train_time:62002ms step_avg:100.82ms
step:625/1770 val_loss:3.6618 train_time:62103ms step_avg:100.98ms
step:626/1770 train_time:62121ms step_avg:100.85ms
step:627/1770 train_time:62219ms step_avg:100.84ms
step:628/1770 train_time:62324ms step_avg:100.85ms
step:629/1770 train_time:62428ms step_avg:100.85ms
step:630/1770 train_time:62532ms step_avg:100.86ms
step:631/1770 train_time:62635ms step_avg:100.86ms
step:632/1770 train_time:62737ms step_avg:100.86ms
step:633/1770 train_time:62839ms step_avg:100.87ms
step:634/1770 train_time:62943ms step_avg:100.87ms
step:635/1770 train_time:63046ms step_avg:100.87ms
step:636/1770 train_time:63148ms step_avg:100.88ms
step:637/1770 train_time:63251ms step_avg:100.88ms
step:638/1770 train_time:63353ms step_avg:100.88ms
step:639/1770 train_time:63456ms step_avg:100.88ms
step:640/1770 train_time:63559ms step_avg:100.89ms
step:641/1770 train_time:63661ms step_avg:100.89ms
step:642/1770 train_time:63764ms step_avg:100.89ms
step:643/1770 train_time:63866ms step_avg:100.89ms
step:644/1770 train_time:63969ms step_avg:100.90ms
step:645/1770 train_time:64072ms step_avg:100.90ms
step:646/1770 train_time:64174ms step_avg:100.90ms
step:647/1770 train_time:64276ms step_avg:100.90ms
step:648/1770 train_time:64379ms step_avg:100.91ms
step:649/1770 train_time:64481ms step_avg:100.91ms
step:650/1770 train_time:64584ms step_avg:100.91ms
step:651/1770 train_time:64687ms step_avg:100.92ms
step:652/1770 train_time:64790ms step_avg:100.92ms
step:653/1770 train_time:64892ms step_avg:100.92ms
step:654/1770 train_time:64994ms step_avg:100.92ms
step:655/1770 train_time:65096ms step_avg:100.92ms
step:656/1770 train_time:65198ms step_avg:100.93ms
step:657/1770 train_time:65301ms step_avg:100.93ms
step:658/1770 train_time:65406ms step_avg:100.93ms
step:659/1770 train_time:65510ms step_avg:100.94ms
step:660/1770 train_time:65614ms step_avg:100.95ms
step:661/1770 train_time:65719ms step_avg:100.95ms
step:662/1770 train_time:65824ms step_avg:100.96ms
step:663/1770 train_time:65928ms step_avg:100.96ms
step:664/1770 train_time:66032ms step_avg:100.97ms
step:665/1770 train_time:66136ms step_avg:100.97ms
step:666/1770 train_time:66240ms step_avg:100.98ms
step:667/1770 train_time:66344ms step_avg:100.98ms
step:668/1770 train_time:66449ms step_avg:100.99ms
step:669/1770 train_time:66553ms step_avg:100.99ms
step:670/1770 train_time:66657ms step_avg:101.00ms
step:671/1770 train_time:66761ms step_avg:101.00ms
step:672/1770 train_time:66866ms step_avg:101.01ms
step:673/1770 train_time:66970ms step_avg:101.01ms
step:674/1770 train_time:67074ms step_avg:101.01ms
step:675/1770 train_time:67178ms step_avg:101.02ms
step:676/1770 train_time:67282ms step_avg:101.02ms
step:677/1770 train_time:67386ms step_avg:101.03ms
step:678/1770 train_time:67490ms step_avg:101.03ms
step:679/1770 train_time:67594ms step_avg:101.04ms
step:680/1770 train_time:67698ms step_avg:101.04ms
step:681/1770 train_time:67804ms step_avg:101.05ms
step:682/1770 train_time:67907ms step_avg:101.05ms
step:683/1770 train_time:68013ms step_avg:101.06ms
step:684/1770 train_time:68116ms step_avg:101.06ms
step:685/1770 train_time:68220ms step_avg:101.07ms
step:686/1770 train_time:68325ms step_avg:101.07ms
step:687/1770 train_time:68429ms step_avg:101.08ms
step:688/1770 train_time:68533ms step_avg:101.08ms
step:689/1770 train_time:68637ms step_avg:101.09ms
step:690/1770 train_time:68741ms step_avg:101.09ms
step:691/1770 train_time:68846ms step_avg:101.10ms
step:692/1770 train_time:68950ms step_avg:101.10ms
step:693/1770 train_time:69054ms step_avg:101.10ms
step:694/1770 train_time:69158ms step_avg:101.11ms
step:695/1770 train_time:69263ms step_avg:101.11ms
step:696/1770 train_time:69367ms step_avg:101.12ms
step:697/1770 train_time:69472ms step_avg:101.12ms
step:698/1770 train_time:69576ms step_avg:101.13ms
step:699/1770 train_time:69680ms step_avg:101.13ms
step:700/1770 train_time:69784ms step_avg:101.14ms
step:701/1770 train_time:69888ms step_avg:101.14ms
step:702/1770 train_time:69992ms step_avg:101.14ms
step:703/1770 train_time:70096ms step_avg:101.15ms
step:704/1770 train_time:70200ms step_avg:101.15ms
step:705/1770 train_time:70305ms step_avg:101.16ms
step:706/1770 train_time:70410ms step_avg:101.16ms
step:707/1770 train_time:70514ms step_avg:101.17ms
step:708/1770 train_time:70618ms step_avg:101.17ms
step:709/1770 train_time:70722ms step_avg:101.18ms
step:710/1770 train_time:70827ms step_avg:101.18ms
step:711/1770 train_time:70931ms step_avg:101.19ms
step:712/1770 train_time:71035ms step_avg:101.19ms
step:713/1770 train_time:71139ms step_avg:101.19ms
step:714/1770 train_time:71244ms step_avg:101.20ms
step:715/1770 train_time:71349ms step_avg:101.20ms
step:716/1770 train_time:71453ms step_avg:101.21ms
step:717/1770 train_time:71557ms step_avg:101.21ms
step:718/1770 train_time:71661ms step_avg:101.22ms
step:719/1770 train_time:71765ms step_avg:101.22ms
step:720/1770 train_time:71870ms step_avg:101.23ms
step:721/1770 train_time:71974ms step_avg:101.23ms
step:722/1770 train_time:72078ms step_avg:101.23ms
step:723/1770 train_time:72183ms step_avg:101.24ms
step:724/1770 train_time:72288ms step_avg:101.24ms
step:725/1770 train_time:72391ms step_avg:101.25ms
step:726/1770 train_time:72495ms step_avg:101.25ms
step:727/1770 train_time:72599ms step_avg:101.25ms
step:728/1770 train_time:72703ms step_avg:101.26ms
step:729/1770 train_time:72807ms step_avg:101.26ms
step:730/1770 train_time:72912ms step_avg:101.27ms
step:731/1770 train_time:73016ms step_avg:101.27ms
step:732/1770 train_time:73120ms step_avg:101.27ms
step:733/1770 train_time:73225ms step_avg:101.28ms
step:734/1770 train_time:73329ms step_avg:101.28ms
step:735/1770 train_time:73433ms step_avg:101.29ms
step:736/1770 train_time:73537ms step_avg:101.29ms
step:737/1770 train_time:73641ms step_avg:101.29ms
step:738/1770 train_time:73745ms step_avg:101.30ms
step:739/1770 train_time:73850ms step_avg:101.30ms
step:740/1770 train_time:73954ms step_avg:101.31ms
step:741/1770 train_time:74058ms step_avg:101.31ms
step:742/1770 train_time:74163ms step_avg:101.31ms
step:743/1770 train_time:74267ms step_avg:101.32ms
step:744/1770 train_time:74371ms step_avg:101.32ms
step:745/1770 train_time:74475ms step_avg:101.33ms
step:746/1770 train_time:74579ms step_avg:101.33ms
step:747/1770 train_time:74683ms step_avg:101.33ms
step:748/1770 train_time:74787ms step_avg:101.34ms
step:749/1770 train_time:74892ms step_avg:101.34ms
step:750/1770 train_time:74995ms step_avg:101.34ms
step:750/1770 val_loss:3.5995 train_time:75097ms step_avg:101.48ms
step:751/1770 train_time:75116ms step_avg:101.37ms
step:752/1770 train_time:75216ms step_avg:101.37ms
step:753/1770 train_time:75324ms step_avg:101.38ms
step:754/1770 train_time:75428ms step_avg:101.38ms
step:755/1770 train_time:75533ms step_avg:101.39ms
step:756/1770 train_time:75637ms step_avg:101.39ms
step:757/1770 train_time:75741ms step_avg:101.39ms
step:758/1770 train_time:75845ms step_avg:101.40ms
step:759/1770 train_time:75950ms step_avg:101.40ms
step:760/1770 train_time:76055ms step_avg:101.41ms
step:761/1770 train_time:76159ms step_avg:101.41ms
step:762/1770 train_time:76264ms step_avg:101.41ms
step:763/1770 train_time:76368ms step_avg:101.42ms
step:764/1770 train_time:76472ms step_avg:101.42ms
step:765/1770 train_time:76576ms step_avg:101.43ms
step:766/1770 train_time:76681ms step_avg:101.43ms
step:767/1770 train_time:76786ms step_avg:101.43ms
step:768/1770 train_time:76891ms step_avg:101.44ms
step:769/1770 train_time:76995ms step_avg:101.44ms
step:770/1770 train_time:77099ms step_avg:101.45ms
step:771/1770 train_time:77204ms step_avg:101.45ms
step:772/1770 train_time:77308ms step_avg:101.45ms
step:773/1770 train_time:77412ms step_avg:101.46ms
step:774/1770 train_time:77515ms step_avg:101.46ms
step:775/1770 train_time:77619ms step_avg:101.46ms
step:776/1770 train_time:77724ms step_avg:101.47ms
step:777/1770 train_time:77828ms step_avg:101.47ms
step:778/1770 train_time:77932ms step_avg:101.47ms
step:779/1770 train_time:78037ms step_avg:101.48ms
step:780/1770 train_time:78141ms step_avg:101.48ms
step:781/1770 train_time:78245ms step_avg:101.49ms
step:782/1770 train_time:78349ms step_avg:101.49ms
step:783/1770 train_time:78454ms step_avg:101.49ms
step:784/1770 train_time:78558ms step_avg:101.50ms
step:785/1770 train_time:78662ms step_avg:101.50ms
step:786/1770 train_time:78766ms step_avg:101.50ms
step:787/1770 train_time:78870ms step_avg:101.51ms
step:788/1770 train_time:78975ms step_avg:101.51ms
step:789/1770 train_time:79079ms step_avg:101.51ms
step:790/1770 train_time:79184ms step_avg:101.52ms
step:791/1770 train_time:79289ms step_avg:101.52ms
step:792/1770 train_time:79393ms step_avg:101.53ms
step:793/1770 train_time:79498ms step_avg:101.53ms
step:794/1770 train_time:79603ms step_avg:101.53ms
step:795/1770 train_time:79708ms step_avg:101.54ms
step:796/1770 train_time:79812ms step_avg:101.54ms
step:797/1770 train_time:79917ms step_avg:101.55ms
step:798/1770 train_time:80022ms step_avg:101.55ms
step:799/1770 train_time:80126ms step_avg:101.55ms
step:800/1770 train_time:80231ms step_avg:101.56ms
step:801/1770 train_time:80335ms step_avg:101.56ms
step:802/1770 train_time:80440ms step_avg:101.57ms
step:803/1770 train_time:80545ms step_avg:101.57ms
step:804/1770 train_time:80650ms step_avg:101.57ms
step:805/1770 train_time:80754ms step_avg:101.58ms
step:806/1770 train_time:80859ms step_avg:101.58ms
step:807/1770 train_time:80964ms step_avg:101.59ms
step:808/1770 train_time:81068ms step_avg:101.59ms
step:809/1770 train_time:81172ms step_avg:101.59ms
step:810/1770 train_time:81277ms step_avg:101.60ms
step:811/1770 train_time:81381ms step_avg:101.60ms
step:812/1770 train_time:81485ms step_avg:101.60ms
step:813/1770 train_time:81590ms step_avg:101.61ms
step:814/1770 train_time:81694ms step_avg:101.61ms
step:815/1770 train_time:81799ms step_avg:101.61ms
step:816/1770 train_time:81904ms step_avg:101.62ms
step:817/1770 train_time:82008ms step_avg:101.62ms
step:818/1770 train_time:82112ms step_avg:101.62ms
step:819/1770 train_time:82217ms step_avg:101.63ms
step:820/1770 train_time:82322ms step_avg:101.63ms
step:821/1770 train_time:82427ms step_avg:101.64ms
step:822/1770 train_time:82531ms step_avg:101.64ms
step:823/1770 train_time:82636ms step_avg:101.64ms
step:824/1770 train_time:82740ms step_avg:101.65ms
step:825/1770 train_time:82845ms step_avg:101.65ms
step:826/1770 train_time:82950ms step_avg:101.65ms
step:827/1770 train_time:83054ms step_avg:101.66ms
step:828/1770 train_time:83158ms step_avg:101.66ms
step:829/1770 train_time:83263ms step_avg:101.66ms
step:830/1770 train_time:83369ms step_avg:101.67ms
step:831/1770 train_time:83472ms step_avg:101.67ms
step:832/1770 train_time:83576ms step_avg:101.67ms
step:833/1770 train_time:83682ms step_avg:101.68ms
step:834/1770 train_time:83787ms step_avg:101.68ms
step:835/1770 train_time:83891ms step_avg:101.69ms
step:836/1770 train_time:83996ms step_avg:101.69ms
step:837/1770 train_time:84101ms step_avg:101.69ms
step:838/1770 train_time:84206ms step_avg:101.70ms
step:839/1770 train_time:84311ms step_avg:101.70ms
step:840/1770 train_time:84414ms step_avg:101.70ms
step:841/1770 train_time:84519ms step_avg:101.71ms
step:842/1770 train_time:84624ms step_avg:101.71ms
step:843/1770 train_time:84728ms step_avg:101.71ms
step:844/1770 train_time:84832ms step_avg:101.72ms
step:845/1770 train_time:84937ms step_avg:101.72ms
step:846/1770 train_time:85042ms step_avg:101.72ms
step:847/1770 train_time:85146ms step_avg:101.73ms
step:848/1770 train_time:85250ms step_avg:101.73ms
step:849/1770 train_time:85354ms step_avg:101.73ms
step:850/1770 train_time:85459ms step_avg:101.74ms
step:851/1770 train_time:85564ms step_avg:101.74ms
step:852/1770 train_time:85668ms step_avg:101.74ms
step:853/1770 train_time:85772ms step_avg:101.75ms
step:854/1770 train_time:85876ms step_avg:101.75ms
step:855/1770 train_time:85981ms step_avg:101.75ms
step:856/1770 train_time:86085ms step_avg:101.76ms
step:857/1770 train_time:86190ms step_avg:101.76ms
step:858/1770 train_time:86294ms step_avg:101.76ms
step:859/1770 train_time:86398ms step_avg:101.76ms
step:860/1770 train_time:86503ms step_avg:101.77ms
step:861/1770 train_time:86607ms step_avg:101.77ms
step:862/1770 train_time:86712ms step_avg:101.77ms
step:863/1770 train_time:86816ms step_avg:101.78ms
step:864/1770 train_time:86921ms step_avg:101.78ms
step:865/1770 train_time:87026ms step_avg:101.78ms
step:866/1770 train_time:87130ms step_avg:101.79ms
step:867/1770 train_time:87235ms step_avg:101.79ms
step:868/1770 train_time:87339ms step_avg:101.79ms
step:869/1770 train_time:87445ms step_avg:101.80ms
step:870/1770 train_time:87549ms step_avg:101.80ms
step:871/1770 train_time:87653ms step_avg:101.80ms
step:872/1770 train_time:87758ms step_avg:101.81ms
step:873/1770 train_time:87862ms step_avg:101.81ms
step:874/1770 train_time:87967ms step_avg:101.81ms
step:875/1770 train_time:88072ms step_avg:101.82ms
step:875/1770 val_loss:3.5502 train_time:88175ms step_avg:101.94ms
step:876/1770 train_time:88193ms step_avg:101.84ms
step:877/1770 train_time:88294ms step_avg:101.84ms
step:878/1770 train_time:88400ms step_avg:101.84ms
step:879/1770 train_time:88506ms step_avg:101.85ms
step:880/1770 train_time:88610ms step_avg:101.85ms
step:881/1770 train_time:88715ms step_avg:101.85ms
step:882/1770 train_time:88819ms step_avg:101.86ms
step:883/1770 train_time:88924ms step_avg:101.86ms
step:884/1770 train_time:89028ms step_avg:101.86ms
step:885/1770 train_time:89134ms step_avg:101.87ms
step:886/1770 train_time:89238ms step_avg:101.87ms
step:887/1770 train_time:89342ms step_avg:101.87ms
step:888/1770 train_time:89447ms step_avg:101.88ms
step:889/1770 train_time:89552ms step_avg:101.88ms
step:890/1770 train_time:89656ms step_avg:101.88ms
step:891/1770 train_time:89761ms step_avg:101.89ms
step:892/1770 train_time:89866ms step_avg:101.89ms
step:893/1770 train_time:89971ms step_avg:101.89ms
step:894/1770 train_time:90076ms step_avg:101.90ms
step:895/1770 train_time:90181ms step_avg:101.90ms
step:896/1770 train_time:90285ms step_avg:101.90ms
step:897/1770 train_time:90390ms step_avg:101.91ms
step:898/1770 train_time:90495ms step_avg:101.91ms
step:899/1770 train_time:90599ms step_avg:101.91ms
step:900/1770 train_time:90703ms step_avg:101.91ms
step:901/1770 train_time:90809ms step_avg:101.92ms
step:902/1770 train_time:90914ms step_avg:101.92ms
step:903/1770 train_time:91018ms step_avg:101.92ms
step:904/1770 train_time:91123ms step_avg:101.93ms
step:905/1770 train_time:91228ms step_avg:101.93ms
step:906/1770 train_time:91333ms step_avg:101.93ms
step:907/1770 train_time:91437ms step_avg:101.94ms
step:908/1770 train_time:91541ms step_avg:101.94ms
step:909/1770 train_time:91646ms step_avg:101.94ms
step:910/1770 train_time:91751ms step_avg:101.95ms
step:911/1770 train_time:91856ms step_avg:101.95ms
step:912/1770 train_time:91960ms step_avg:101.95ms
step:913/1770 train_time:92064ms step_avg:101.95ms
step:914/1770 train_time:92170ms step_avg:101.96ms
step:915/1770 train_time:92274ms step_avg:101.96ms
step:916/1770 train_time:92378ms step_avg:101.96ms
step:917/1770 train_time:92483ms step_avg:101.97ms
step:918/1770 train_time:92587ms step_avg:101.97ms
step:919/1770 train_time:92692ms step_avg:101.97ms
step:920/1770 train_time:92799ms step_avg:101.98ms
step:921/1770 train_time:92905ms step_avg:101.98ms
step:922/1770 train_time:93011ms step_avg:101.99ms
step:923/1770 train_time:93117ms step_avg:101.99ms
step:924/1770 train_time:93223ms step_avg:101.99ms
step:925/1770 train_time:93329ms step_avg:102.00ms
step:926/1770 train_time:93436ms step_avg:102.00ms
step:927/1770 train_time:93542ms step_avg:102.01ms
step:928/1770 train_time:93647ms step_avg:102.01ms
step:929/1770 train_time:93753ms step_avg:102.02ms
step:930/1770 train_time:93858ms step_avg:102.02ms
step:931/1770 train_time:93965ms step_avg:102.02ms
step:932/1770 train_time:94070ms step_avg:102.03ms
step:933/1770 train_time:94177ms step_avg:102.03ms
step:934/1770 train_time:94283ms step_avg:102.04ms
step:935/1770 train_time:94389ms step_avg:102.04ms
step:936/1770 train_time:94495ms step_avg:102.05ms
step:937/1770 train_time:94600ms step_avg:102.05ms
step:938/1770 train_time:94706ms step_avg:102.05ms
step:939/1770 train_time:94812ms step_avg:102.06ms
step:940/1770 train_time:94918ms step_avg:102.06ms
step:941/1770 train_time:95024ms step_avg:102.07ms
step:942/1770 train_time:95131ms step_avg:102.07ms
step:943/1770 train_time:95236ms step_avg:102.08ms
step:944/1770 train_time:95342ms step_avg:102.08ms
step:945/1770 train_time:95448ms step_avg:102.08ms
step:946/1770 train_time:95555ms step_avg:102.09ms
step:947/1770 train_time:95660ms step_avg:102.09ms
step:948/1770 train_time:95766ms step_avg:102.10ms
step:949/1770 train_time:95873ms step_avg:102.10ms
step:950/1770 train_time:95979ms step_avg:102.10ms
step:951/1770 train_time:96084ms step_avg:102.11ms
step:952/1770 train_time:96190ms step_avg:102.11ms
step:953/1770 train_time:96297ms step_avg:102.12ms
step:954/1770 train_time:96402ms step_avg:102.12ms
step:955/1770 train_time:96508ms step_avg:102.13ms
step:956/1770 train_time:96614ms step_avg:102.13ms
step:957/1770 train_time:96720ms step_avg:102.13ms
step:958/1770 train_time:96826ms step_avg:102.14ms
step:959/1770 train_time:96931ms step_avg:102.14ms
step:960/1770 train_time:97037ms step_avg:102.14ms
step:961/1770 train_time:97143ms step_avg:102.15ms
step:962/1770 train_time:97250ms step_avg:102.15ms
step:963/1770 train_time:97355ms step_avg:102.16ms
step:964/1770 train_time:97461ms step_avg:102.16ms
step:965/1770 train_time:97568ms step_avg:102.17ms
step:966/1770 train_time:97674ms step_avg:102.17ms
step:967/1770 train_time:97779ms step_avg:102.17ms
step:968/1770 train_time:97886ms step_avg:102.18ms
step:969/1770 train_time:97992ms step_avg:102.18ms
step:970/1770 train_time:98097ms step_avg:102.18ms
step:971/1770 train_time:98202ms step_avg:102.19ms
step:972/1770 train_time:98309ms step_avg:102.19ms
step:973/1770 train_time:98415ms step_avg:102.20ms
step:974/1770 train_time:98521ms step_avg:102.20ms
step:975/1770 train_time:98628ms step_avg:102.20ms
step:976/1770 train_time:98734ms step_avg:102.21ms
step:977/1770 train_time:98839ms step_avg:102.21ms
step:978/1770 train_time:98945ms step_avg:102.22ms
step:979/1770 train_time:99051ms step_avg:102.22ms
step:980/1770 train_time:99157ms step_avg:102.22ms
step:981/1770 train_time:99263ms step_avg:102.23ms
step:982/1770 train_time:99370ms step_avg:102.23ms
step:983/1770 train_time:99476ms step_avg:102.24ms
step:984/1770 train_time:99581ms step_avg:102.24ms
step:985/1770 train_time:99688ms step_avg:102.24ms
step:986/1770 train_time:99794ms step_avg:102.25ms
step:987/1770 train_time:99899ms step_avg:102.25ms
step:988/1770 train_time:100005ms step_avg:102.25ms
step:989/1770 train_time:100112ms step_avg:102.26ms
step:990/1770 train_time:100217ms step_avg:102.26ms
step:991/1770 train_time:100324ms step_avg:102.27ms
step:992/1770 train_time:100430ms step_avg:102.27ms
step:993/1770 train_time:100536ms step_avg:102.27ms
step:994/1770 train_time:100642ms step_avg:102.28ms
step:995/1770 train_time:100748ms step_avg:102.28ms
step:996/1770 train_time:100854ms step_avg:102.29ms
step:997/1770 train_time:100959ms step_avg:102.29ms
step:998/1770 train_time:101065ms step_avg:102.29ms
step:999/1770 train_time:101171ms step_avg:102.30ms
step:1000/1770 train_time:101277ms step_avg:102.30ms
step:1000/1770 val_loss:3.5127 train_time:101381ms step_avg:102.40ms
step:1001/1770 train_time:101399ms step_avg:102.32ms
step:1002/1770 train_time:101501ms step_avg:102.32ms
step:1003/1770 train_time:101609ms step_avg:102.32ms
step:1004/1770 train_time:101715ms step_avg:102.33ms
step:1005/1770 train_time:101821ms step_avg:102.33ms
step:1006/1770 train_time:101928ms step_avg:102.34ms
step:1007/1770 train_time:102034ms step_avg:102.34ms
step:1008/1770 train_time:102141ms step_avg:102.35ms
step:1009/1770 train_time:102246ms step_avg:102.35ms
step:1010/1770 train_time:102352ms step_avg:102.35ms
step:1011/1770 train_time:102459ms step_avg:102.36ms
step:1012/1770 train_time:102564ms step_avg:102.36ms
step:1013/1770 train_time:102671ms step_avg:102.36ms
step:1014/1770 train_time:102778ms step_avg:102.37ms
step:1015/1770 train_time:102883ms step_avg:102.37ms
step:1016/1770 train_time:102989ms step_avg:102.37ms
step:1017/1770 train_time:103096ms step_avg:102.38ms
step:1018/1770 train_time:103202ms step_avg:102.38ms
step:1019/1770 train_time:103307ms step_avg:102.39ms
step:1020/1770 train_time:103413ms step_avg:102.39ms
step:1021/1770 train_time:103519ms step_avg:102.39ms
step:1022/1770 train_time:103625ms step_avg:102.40ms
step:1023/1770 train_time:103731ms step_avg:102.40ms
step:1024/1770 train_time:103837ms step_avg:102.40ms
step:1025/1770 train_time:103943ms step_avg:102.41ms
step:1026/1770 train_time:104049ms step_avg:102.41ms
step:1027/1770 train_time:104155ms step_avg:102.41ms
step:1028/1770 train_time:104261ms step_avg:102.42ms
step:1029/1770 train_time:104366ms step_avg:102.42ms
step:1030/1770 train_time:104473ms step_avg:102.42ms
step:1031/1770 train_time:104579ms step_avg:102.43ms
step:1032/1770 train_time:104684ms step_avg:102.43ms
step:1033/1770 train_time:104791ms step_avg:102.43ms
step:1034/1770 train_time:104897ms step_avg:102.44ms
step:1035/1770 train_time:105002ms step_avg:102.44ms
step:1036/1770 train_time:105108ms step_avg:102.44ms
step:1037/1770 train_time:105214ms step_avg:102.45ms
step:1038/1770 train_time:105320ms step_avg:102.45ms
step:1039/1770 train_time:105426ms step_avg:102.45ms
step:1040/1770 train_time:105532ms step_avg:102.46ms
step:1041/1770 train_time:105638ms step_avg:102.46ms
step:1042/1770 train_time:105745ms step_avg:102.47ms
step:1043/1770 train_time:105851ms step_avg:102.47ms
step:1044/1770 train_time:105957ms step_avg:102.47ms
step:1045/1770 train_time:106063ms step_avg:102.48ms
step:1046/1770 train_time:106169ms step_avg:102.48ms
step:1047/1770 train_time:106275ms step_avg:102.48ms
step:1048/1770 train_time:106381ms step_avg:102.49ms
step:1049/1770 train_time:106487ms step_avg:102.49ms
step:1050/1770 train_time:106593ms step_avg:102.49ms
step:1051/1770 train_time:106700ms step_avg:102.50ms
step:1052/1770 train_time:106806ms step_avg:102.50ms
step:1053/1770 train_time:106913ms step_avg:102.51ms
step:1054/1770 train_time:107018ms step_avg:102.51ms
step:1055/1770 train_time:107124ms step_avg:102.51ms
step:1056/1770 train_time:107230ms step_avg:102.51ms
step:1057/1770 train_time:107336ms step_avg:102.52ms
step:1058/1770 train_time:107442ms step_avg:102.52ms
step:1059/1770 train_time:107548ms step_avg:102.52ms
step:1060/1770 train_time:107655ms step_avg:102.53ms
step:1061/1770 train_time:107761ms step_avg:102.53ms
step:1062/1770 train_time:107868ms step_avg:102.54ms
step:1063/1770 train_time:107975ms step_avg:102.54ms
step:1064/1770 train_time:108083ms step_avg:102.55ms
step:1065/1770 train_time:108188ms step_avg:102.55ms
step:1066/1770 train_time:108295ms step_avg:102.55ms
step:1067/1770 train_time:108401ms step_avg:102.56ms
step:1068/1770 train_time:108507ms step_avg:102.56ms
step:1069/1770 train_time:108613ms step_avg:102.56ms
step:1070/1770 train_time:108719ms step_avg:102.56ms
step:1071/1770 train_time:108826ms step_avg:102.57ms
step:1072/1770 train_time:108933ms step_avg:102.57ms
step:1073/1770 train_time:109039ms step_avg:102.58ms
step:1074/1770 train_time:109145ms step_avg:102.58ms
step:1075/1770 train_time:109252ms step_avg:102.58ms
step:1076/1770 train_time:109359ms step_avg:102.59ms
step:1077/1770 train_time:109464ms step_avg:102.59ms
step:1078/1770 train_time:109572ms step_avg:102.60ms
step:1079/1770 train_time:109678ms step_avg:102.60ms
step:1080/1770 train_time:109783ms step_avg:102.60ms
step:1081/1770 train_time:109889ms step_avg:102.60ms
step:1082/1770 train_time:109996ms step_avg:102.61ms
step:1083/1770 train_time:110102ms step_avg:102.61ms
step:1084/1770 train_time:110208ms step_avg:102.61ms
step:1085/1770 train_time:110314ms step_avg:102.62ms
step:1086/1770 train_time:110421ms step_avg:102.62ms
step:1087/1770 train_time:110526ms step_avg:102.62ms
step:1088/1770 train_time:110633ms step_avg:102.63ms
step:1089/1770 train_time:110739ms step_avg:102.63ms
step:1090/1770 train_time:110846ms step_avg:102.63ms
step:1091/1770 train_time:110952ms step_avg:102.64ms
step:1092/1770 train_time:111058ms step_avg:102.64ms
step:1093/1770 train_time:111164ms step_avg:102.64ms
step:1094/1770 train_time:111271ms step_avg:102.65ms
step:1095/1770 train_time:111377ms step_avg:102.65ms
step:1096/1770 train_time:111483ms step_avg:102.65ms
step:1097/1770 train_time:111589ms step_avg:102.66ms
step:1098/1770 train_time:111694ms step_avg:102.66ms
step:1099/1770 train_time:111801ms step_avg:102.66ms
step:1100/1770 train_time:111908ms step_avg:102.67ms
step:1101/1770 train_time:112014ms step_avg:102.67ms
step:1102/1770 train_time:112121ms step_avg:102.67ms
step:1103/1770 train_time:112227ms step_avg:102.68ms
step:1104/1770 train_time:112333ms step_avg:102.68ms
step:1105/1770 train_time:112440ms step_avg:102.68ms
step:1106/1770 train_time:112546ms step_avg:102.69ms
step:1107/1770 train_time:112652ms step_avg:102.69ms
step:1108/1770 train_time:112758ms step_avg:102.69ms
step:1109/1770 train_time:112864ms step_avg:102.70ms
step:1110/1770 train_time:112970ms step_avg:102.70ms
step:1111/1770 train_time:113076ms step_avg:102.70ms
step:1112/1770 train_time:113183ms step_avg:102.71ms
step:1113/1770 train_time:113289ms step_avg:102.71ms
step:1114/1770 train_time:113395ms step_avg:102.71ms
step:1115/1770 train_time:113502ms step_avg:102.72ms
step:1116/1770 train_time:113608ms step_avg:102.72ms
step:1117/1770 train_time:113715ms step_avg:102.72ms
step:1118/1770 train_time:113821ms step_avg:102.73ms
step:1119/1770 train_time:113927ms step_avg:102.73ms
step:1120/1770 train_time:114033ms step_avg:102.73ms
step:1121/1770 train_time:114140ms step_avg:102.74ms
step:1122/1770 train_time:114245ms step_avg:102.74ms
step:1123/1770 train_time:114351ms step_avg:102.74ms
step:1124/1770 train_time:114458ms step_avg:102.74ms
step:1125/1770 train_time:114564ms step_avg:102.75ms
step:1125/1770 val_loss:3.4704 train_time:114668ms step_avg:102.84ms
step:1126/1770 train_time:114686ms step_avg:102.77ms
step:1127/1770 train_time:114789ms step_avg:102.77ms
step:1128/1770 train_time:114898ms step_avg:102.77ms
step:1129/1770 train_time:115004ms step_avg:102.77ms
step:1130/1770 train_time:115110ms step_avg:102.78ms
step:1131/1770 train_time:115217ms step_avg:102.78ms
step:1132/1770 train_time:115324ms step_avg:102.78ms
step:1133/1770 train_time:115430ms step_avg:102.79ms
step:1134/1770 train_time:115536ms step_avg:102.79ms
step:1135/1770 train_time:115643ms step_avg:102.79ms
step:1136/1770 train_time:115749ms step_avg:102.80ms
step:1137/1770 train_time:115857ms step_avg:102.80ms
step:1138/1770 train_time:115963ms step_avg:102.80ms
step:1139/1770 train_time:116069ms step_avg:102.81ms
step:1140/1770 train_time:116176ms step_avg:102.81ms
step:1141/1770 train_time:116281ms step_avg:102.81ms
step:1142/1770 train_time:116388ms step_avg:102.82ms
step:1143/1770 train_time:116493ms step_avg:102.82ms
step:1144/1770 train_time:116599ms step_avg:102.82ms
step:1145/1770 train_time:116705ms step_avg:102.82ms
step:1146/1770 train_time:116811ms step_avg:102.83ms
step:1147/1770 train_time:116918ms step_avg:102.83ms
step:1148/1770 train_time:117024ms step_avg:102.83ms
step:1149/1770 train_time:117130ms step_avg:102.84ms
step:1150/1770 train_time:117236ms step_avg:102.84ms
step:1151/1770 train_time:117343ms step_avg:102.84ms
step:1152/1770 train_time:117450ms step_avg:102.85ms
step:1153/1770 train_time:117556ms step_avg:102.85ms
step:1154/1770 train_time:117662ms step_avg:102.85ms
step:1155/1770 train_time:117767ms step_avg:102.85ms
step:1156/1770 train_time:117873ms step_avg:102.86ms
step:1157/1770 train_time:117980ms step_avg:102.86ms
step:1158/1770 train_time:118086ms step_avg:102.86ms
step:1159/1770 train_time:118191ms step_avg:102.86ms
step:1160/1770 train_time:118298ms step_avg:102.87ms
step:1161/1770 train_time:118404ms step_avg:102.87ms
step:1162/1770 train_time:118511ms step_avg:102.87ms
step:1163/1770 train_time:118617ms step_avg:102.88ms
step:1164/1770 train_time:118723ms step_avg:102.88ms
step:1165/1770 train_time:118829ms step_avg:102.88ms
step:1166/1770 train_time:118936ms step_avg:102.89ms
step:1167/1770 train_time:119042ms step_avg:102.89ms
step:1168/1770 train_time:119149ms step_avg:102.89ms
step:1169/1770 train_time:119256ms step_avg:102.90ms
step:1170/1770 train_time:119363ms step_avg:102.90ms
step:1171/1770 train_time:119469ms step_avg:102.90ms
step:1172/1770 train_time:119576ms step_avg:102.91ms
step:1173/1770 train_time:119682ms step_avg:102.91ms
step:1174/1770 train_time:119788ms step_avg:102.91ms
step:1175/1770 train_time:119895ms step_avg:102.91ms
step:1176/1770 train_time:120002ms step_avg:102.92ms
step:1177/1770 train_time:120109ms step_avg:102.92ms
step:1178/1770 train_time:120215ms step_avg:102.92ms
step:1179/1770 train_time:120321ms step_avg:102.93ms
step:1180/1770 train_time:120426ms step_avg:102.93ms
step:1181/1770 train_time:120533ms step_avg:102.93ms
step:1182/1770 train_time:120639ms step_avg:102.93ms
step:1183/1770 train_time:120746ms step_avg:102.94ms
step:1184/1770 train_time:120855ms step_avg:102.94ms
step:1185/1770 train_time:120962ms step_avg:102.95ms
step:1186/1770 train_time:121069ms step_avg:102.95ms
step:1187/1770 train_time:121179ms step_avg:102.96ms
step:1188/1770 train_time:121286ms step_avg:102.96ms
step:1189/1770 train_time:121393ms step_avg:102.96ms
step:1190/1770 train_time:121500ms step_avg:102.97ms
step:1191/1770 train_time:121607ms step_avg:102.97ms
step:1192/1770 train_time:121714ms step_avg:102.97ms
step:1193/1770 train_time:121822ms step_avg:102.98ms
step:1194/1770 train_time:121929ms step_avg:102.98ms
step:1195/1770 train_time:122037ms step_avg:102.98ms
step:1196/1770 train_time:122145ms step_avg:102.99ms
step:1197/1770 train_time:122252ms step_avg:102.99ms
step:1198/1770 train_time:122360ms step_avg:103.00ms
step:1199/1770 train_time:122468ms step_avg:103.00ms
step:1200/1770 train_time:122577ms step_avg:103.01ms
step:1201/1770 train_time:122684ms step_avg:103.01ms
step:1202/1770 train_time:122792ms step_avg:103.01ms
step:1203/1770 train_time:122899ms step_avg:103.02ms
step:1204/1770 train_time:123007ms step_avg:103.02ms
step:1205/1770 train_time:123114ms step_avg:103.02ms
step:1206/1770 train_time:123222ms step_avg:103.03ms
step:1207/1770 train_time:123329ms step_avg:103.03ms
step:1208/1770 train_time:123437ms step_avg:103.04ms
step:1209/1770 train_time:123545ms step_avg:103.04ms
step:1210/1770 train_time:123651ms step_avg:103.04ms
step:1211/1770 train_time:123759ms step_avg:103.05ms
step:1212/1770 train_time:123868ms step_avg:103.05ms
step:1213/1770 train_time:123974ms step_avg:103.05ms
step:1214/1770 train_time:124081ms step_avg:103.06ms
step:1215/1770 train_time:124189ms step_avg:103.06ms
step:1216/1770 train_time:124299ms step_avg:103.07ms
step:1217/1770 train_time:124406ms step_avg:103.07ms
step:1218/1770 train_time:124514ms step_avg:103.07ms
step:1219/1770 train_time:124622ms step_avg:103.08ms
step:1220/1770 train_time:124729ms step_avg:103.08ms
step:1221/1770 train_time:124836ms step_avg:103.08ms
step:1222/1770 train_time:124946ms step_avg:103.09ms
step:1223/1770 train_time:125053ms step_avg:103.09ms
step:1224/1770 train_time:125162ms step_avg:103.10ms
step:1225/1770 train_time:125271ms step_avg:103.10ms
step:1226/1770 train_time:125378ms step_avg:103.11ms
step:1227/1770 train_time:125487ms step_avg:103.11ms
step:1228/1770 train_time:125596ms step_avg:103.12ms
step:1229/1770 train_time:125704ms step_avg:103.12ms
step:1230/1770 train_time:125811ms step_avg:103.12ms
step:1231/1770 train_time:125919ms step_avg:103.13ms
step:1232/1770 train_time:126025ms step_avg:103.13ms
step:1233/1770 train_time:126132ms step_avg:103.13ms
step:1234/1770 train_time:126240ms step_avg:103.14ms
step:1235/1770 train_time:126347ms step_avg:103.14ms
step:1236/1770 train_time:126456ms step_avg:103.14ms
step:1237/1770 train_time:126564ms step_avg:103.15ms
step:1238/1770 train_time:126672ms step_avg:103.15ms
step:1239/1770 train_time:126780ms step_avg:103.16ms
step:1240/1770 train_time:126887ms step_avg:103.16ms
step:1241/1770 train_time:126995ms step_avg:103.16ms
step:1242/1770 train_time:127103ms step_avg:103.17ms
step:1243/1770 train_time:127211ms step_avg:103.17ms
step:1244/1770 train_time:127318ms step_avg:103.18ms
step:1245/1770 train_time:127425ms step_avg:103.18ms
step:1246/1770 train_time:127533ms step_avg:103.18ms
step:1247/1770 train_time:127641ms step_avg:103.19ms
step:1248/1770 train_time:127749ms step_avg:103.19ms
step:1249/1770 train_time:127857ms step_avg:103.19ms
step:1250/1770 train_time:127964ms step_avg:103.20ms
step:1250/1770 val_loss:3.4239 train_time:128071ms step_avg:103.28ms
step:1251/1770 train_time:128091ms step_avg:103.22ms
step:1252/1770 train_time:128189ms step_avg:103.21ms
step:1253/1770 train_time:128297ms step_avg:103.22ms
step:1254/1770 train_time:128405ms step_avg:103.22ms
step:1255/1770 train_time:128515ms step_avg:103.23ms
step:1256/1770 train_time:128622ms step_avg:103.23ms
step:1257/1770 train_time:128729ms step_avg:103.23ms
step:1258/1770 train_time:128836ms step_avg:103.23ms
step:1259/1770 train_time:128944ms step_avg:103.24ms
step:1260/1770 train_time:129051ms step_avg:103.24ms
step:1261/1770 train_time:129159ms step_avg:103.24ms
step:1262/1770 train_time:129268ms step_avg:103.25ms
step:1263/1770 train_time:129375ms step_avg:103.25ms
step:1264/1770 train_time:129483ms step_avg:103.26ms
step:1265/1770 train_time:129590ms step_avg:103.26ms
step:1266/1770 train_time:129698ms step_avg:103.26ms
step:1267/1770 train_time:129806ms step_avg:103.27ms
step:1268/1770 train_time:129913ms step_avg:103.27ms
step:1269/1770 train_time:130020ms step_avg:103.27ms
step:1270/1770 train_time:130129ms step_avg:103.28ms
step:1271/1770 train_time:130235ms step_avg:103.28ms
step:1272/1770 train_time:130342ms step_avg:103.28ms
step:1273/1770 train_time:130451ms step_avg:103.29ms
step:1274/1770 train_time:130558ms step_avg:103.29ms
step:1275/1770 train_time:130666ms step_avg:103.29ms
step:1276/1770 train_time:130775ms step_avg:103.30ms
step:1277/1770 train_time:130882ms step_avg:103.30ms
step:1278/1770 train_time:130990ms step_avg:103.30ms
step:1279/1770 train_time:131097ms step_avg:103.31ms
step:1280/1770 train_time:131206ms step_avg:103.31ms
step:1281/1770 train_time:131313ms step_avg:103.31ms
step:1282/1770 train_time:131422ms step_avg:103.32ms
step:1283/1770 train_time:131530ms step_avg:103.32ms
step:1284/1770 train_time:131637ms step_avg:103.33ms
step:1285/1770 train_time:131746ms step_avg:103.33ms
step:1286/1770 train_time:131855ms step_avg:103.33ms
step:1287/1770 train_time:131965ms step_avg:103.34ms
step:1288/1770 train_time:132072ms step_avg:103.34ms
step:1289/1770 train_time:132180ms step_avg:103.35ms
step:1290/1770 train_time:132288ms step_avg:103.35ms
step:1291/1770 train_time:132396ms step_avg:103.35ms
step:1292/1770 train_time:132502ms step_avg:103.36ms
step:1293/1770 train_time:132610ms step_avg:103.36ms
step:1294/1770 train_time:132717ms step_avg:103.36ms
step:1295/1770 train_time:132825ms step_avg:103.37ms
step:1296/1770 train_time:132932ms step_avg:103.37ms
step:1297/1770 train_time:133039ms step_avg:103.37ms
step:1298/1770 train_time:133147ms step_avg:103.38ms
step:1299/1770 train_time:133255ms step_avg:103.38ms
step:1300/1770 train_time:133363ms step_avg:103.38ms
step:1301/1770 train_time:133471ms step_avg:103.39ms
step:1302/1770 train_time:133578ms step_avg:103.39ms
step:1303/1770 train_time:133686ms step_avg:103.39ms
step:1304/1770 train_time:133793ms step_avg:103.40ms
step:1305/1770 train_time:133901ms step_avg:103.40ms
step:1306/1770 train_time:134009ms step_avg:103.40ms
step:1307/1770 train_time:134116ms step_avg:103.41ms
step:1308/1770 train_time:134223ms step_avg:103.41ms
step:1309/1770 train_time:134331ms step_avg:103.41ms
step:1310/1770 train_time:134438ms step_avg:103.41ms
step:1311/1770 train_time:134546ms step_avg:103.42ms
step:1312/1770 train_time:134653ms step_avg:103.42ms
step:1313/1770 train_time:134758ms step_avg:103.42ms
step:1314/1770 train_time:134866ms step_avg:103.43ms
step:1315/1770 train_time:134974ms step_avg:103.43ms
step:1316/1770 train_time:135082ms step_avg:103.43ms
step:1317/1770 train_time:135190ms step_avg:103.44ms
step:1318/1770 train_time:135300ms step_avg:103.44ms
step:1319/1770 train_time:135409ms step_avg:103.44ms
step:1320/1770 train_time:135516ms step_avg:103.45ms
step:1321/1770 train_time:135624ms step_avg:103.45ms
step:1322/1770 train_time:135732ms step_avg:103.45ms
step:1323/1770 train_time:135840ms step_avg:103.46ms
step:1324/1770 train_time:135949ms step_avg:103.46ms
step:1325/1770 train_time:136059ms step_avg:103.47ms
step:1326/1770 train_time:136167ms step_avg:103.47ms
step:1327/1770 train_time:136278ms step_avg:103.48ms
step:1328/1770 train_time:136386ms step_avg:103.48ms
step:1329/1770 train_time:136494ms step_avg:103.48ms
step:1330/1770 train_time:136601ms step_avg:103.49ms
step:1331/1770 train_time:136708ms step_avg:103.49ms
step:1332/1770 train_time:136816ms step_avg:103.49ms
step:1333/1770 train_time:136923ms step_avg:103.49ms
step:1334/1770 train_time:137031ms step_avg:103.50ms
step:1335/1770 train_time:137138ms step_avg:103.50ms
step:1336/1770 train_time:137245ms step_avg:103.50ms
step:1337/1770 train_time:137353ms step_avg:103.51ms
step:1338/1770 train_time:137460ms step_avg:103.51ms
step:1339/1770 train_time:137568ms step_avg:103.51ms
step:1340/1770 train_time:137676ms step_avg:103.52ms
step:1341/1770 train_time:137784ms step_avg:103.52ms
step:1342/1770 train_time:137892ms step_avg:103.52ms
step:1343/1770 train_time:138001ms step_avg:103.53ms
step:1344/1770 train_time:138109ms step_avg:103.53ms
step:1345/1770 train_time:138216ms step_avg:103.53ms
step:1346/1770 train_time:138323ms step_avg:103.54ms
step:1347/1770 train_time:138430ms step_avg:103.54ms
step:1348/1770 train_time:138541ms step_avg:103.54ms
step:1349/1770 train_time:138648ms step_avg:103.55ms
step:1350/1770 train_time:138755ms step_avg:103.55ms
step:1351/1770 train_time:138863ms step_avg:103.55ms
step:1352/1770 train_time:138971ms step_avg:103.56ms
step:1353/1770 train_time:139080ms step_avg:103.56ms
step:1354/1770 train_time:139188ms step_avg:103.56ms
step:1355/1770 train_time:139295ms step_avg:103.57ms
step:1356/1770 train_time:139404ms step_avg:103.57ms
step:1357/1770 train_time:139511ms step_avg:103.57ms
step:1358/1770 train_time:139619ms step_avg:103.57ms
step:1359/1770 train_time:139727ms step_avg:103.58ms
step:1360/1770 train_time:139835ms step_avg:103.58ms
step:1361/1770 train_time:139943ms step_avg:103.58ms
step:1362/1770 train_time:140051ms step_avg:103.59ms
step:1363/1770 train_time:140159ms step_avg:103.59ms
step:1364/1770 train_time:140266ms step_avg:103.59ms
step:1365/1770 train_time:140374ms step_avg:103.60ms
step:1366/1770 train_time:140480ms step_avg:103.60ms
step:1367/1770 train_time:140590ms step_avg:103.60ms
step:1368/1770 train_time:140697ms step_avg:103.61ms
step:1369/1770 train_time:140806ms step_avg:103.61ms
step:1370/1770 train_time:140915ms step_avg:103.61ms
step:1371/1770 train_time:141022ms step_avg:103.62ms
step:1372/1770 train_time:141129ms step_avg:103.62ms
step:1373/1770 train_time:141237ms step_avg:103.62ms
step:1374/1770 train_time:141346ms step_avg:103.63ms
step:1375/1770 train_time:141453ms step_avg:103.63ms
step:1375/1770 val_loss:3.3800 train_time:141560ms step_avg:103.71ms
step:1376/1770 train_time:141578ms step_avg:103.64ms
step:1377/1770 train_time:141679ms step_avg:103.64ms
step:1378/1770 train_time:141788ms step_avg:103.65ms
step:1379/1770 train_time:141895ms step_avg:103.65ms
step:1380/1770 train_time:142003ms step_avg:103.65ms
step:1381/1770 train_time:142112ms step_avg:103.66ms
step:1382/1770 train_time:142219ms step_avg:103.66ms
step:1383/1770 train_time:142328ms step_avg:103.66ms
step:1384/1770 train_time:142435ms step_avg:103.66ms
step:1385/1770 train_time:142543ms step_avg:103.67ms
step:1386/1770 train_time:142651ms step_avg:103.67ms
step:1387/1770 train_time:142758ms step_avg:103.67ms
step:1388/1770 train_time:142866ms step_avg:103.68ms
step:1389/1770 train_time:142973ms step_avg:103.68ms
step:1390/1770 train_time:143082ms step_avg:103.68ms
step:1391/1770 train_time:143188ms step_avg:103.68ms
step:1392/1770 train_time:143295ms step_avg:103.69ms
step:1393/1770 train_time:143403ms step_avg:103.69ms
step:1394/1770 train_time:143510ms step_avg:103.69ms
step:1395/1770 train_time:143619ms step_avg:103.70ms
step:1396/1770 train_time:143728ms step_avg:103.70ms
step:1397/1770 train_time:143835ms step_avg:103.70ms
step:1398/1770 train_time:143943ms step_avg:103.71ms
step:1399/1770 train_time:144050ms step_avg:103.71ms
step:1400/1770 train_time:144157ms step_avg:103.71ms
step:1401/1770 train_time:144265ms step_avg:103.71ms
step:1402/1770 train_time:144372ms step_avg:103.72ms
step:1403/1770 train_time:144480ms step_avg:103.72ms
step:1404/1770 train_time:144588ms step_avg:103.72ms
step:1405/1770 train_time:144695ms step_avg:103.72ms
step:1406/1770 train_time:144802ms step_avg:103.73ms
step:1407/1770 train_time:144910ms step_avg:103.73ms
step:1408/1770 train_time:145017ms step_avg:103.73ms
step:1409/1770 train_time:145126ms step_avg:103.74ms
step:1410/1770 train_time:145234ms step_avg:103.74ms
step:1411/1770 train_time:145341ms step_avg:103.74ms
step:1412/1770 train_time:145450ms step_avg:103.74ms
step:1413/1770 train_time:145557ms step_avg:103.75ms
step:1414/1770 train_time:145665ms step_avg:103.75ms
step:1415/1770 train_time:145773ms step_avg:103.75ms
step:1416/1770 train_time:145882ms step_avg:103.76ms
step:1417/1770 train_time:145989ms step_avg:103.76ms
step:1418/1770 train_time:146097ms step_avg:103.76ms
step:1419/1770 train_time:146205ms step_avg:103.76ms
step:1420/1770 train_time:146312ms step_avg:103.77ms
step:1421/1770 train_time:146419ms step_avg:103.77ms
step:1422/1770 train_time:146527ms step_avg:103.77ms
step:1423/1770 train_time:146634ms step_avg:103.77ms
step:1424/1770 train_time:146742ms step_avg:103.78ms
step:1425/1770 train_time:146849ms step_avg:103.78ms
step:1426/1770 train_time:146957ms step_avg:103.78ms
step:1427/1770 train_time:147064ms step_avg:103.79ms
step:1428/1770 train_time:147172ms step_avg:103.79ms
step:1429/1770 train_time:147279ms step_avg:103.79ms
step:1430/1770 train_time:147386ms step_avg:103.79ms
step:1431/1770 train_time:147495ms step_avg:103.80ms
step:1432/1770 train_time:147603ms step_avg:103.80ms
step:1433/1770 train_time:147711ms step_avg:103.80ms
step:1434/1770 train_time:147817ms step_avg:103.80ms
step:1435/1770 train_time:147925ms step_avg:103.81ms
step:1436/1770 train_time:148034ms step_avg:103.81ms
step:1437/1770 train_time:148143ms step_avg:103.81ms
step:1438/1770 train_time:148250ms step_avg:103.82ms
step:1439/1770 train_time:148358ms step_avg:103.82ms
step:1440/1770 train_time:148465ms step_avg:103.82ms
step:1441/1770 train_time:148575ms step_avg:103.83ms
step:1442/1770 train_time:148682ms step_avg:103.83ms
step:1443/1770 train_time:148789ms step_avg:103.83ms
step:1444/1770 train_time:148897ms step_avg:103.83ms
step:1445/1770 train_time:149007ms step_avg:103.84ms
step:1446/1770 train_time:149115ms step_avg:103.84ms
step:1447/1770 train_time:149225ms step_avg:103.84ms
step:1448/1770 train_time:149333ms step_avg:103.85ms
step:1449/1770 train_time:149444ms step_avg:103.85ms
step:1450/1770 train_time:149552ms step_avg:103.86ms
step:1451/1770 train_time:149661ms step_avg:103.86ms
step:1452/1770 train_time:149771ms step_avg:103.86ms
step:1453/1770 train_time:149880ms step_avg:103.87ms
step:1454/1770 train_time:149989ms step_avg:103.87ms
step:1455/1770 train_time:150098ms step_avg:103.87ms
step:1456/1770 train_time:150209ms step_avg:103.88ms
step:1457/1770 train_time:150318ms step_avg:103.88ms
step:1458/1770 train_time:150427ms step_avg:103.89ms
step:1459/1770 train_time:150537ms step_avg:103.89ms
step:1460/1770 train_time:150646ms step_avg:103.89ms
step:1461/1770 train_time:150754ms step_avg:103.90ms
step:1462/1770 train_time:150863ms step_avg:103.90ms
step:1463/1770 train_time:150972ms step_avg:103.90ms
step:1464/1770 train_time:151081ms step_avg:103.91ms
step:1465/1770 train_time:151190ms step_avg:103.91ms
step:1466/1770 train_time:151301ms step_avg:103.92ms
step:1467/1770 train_time:151411ms step_avg:103.92ms
step:1468/1770 train_time:151521ms step_avg:103.92ms
step:1469/1770 train_time:151629ms step_avg:103.93ms
step:1470/1770 train_time:151737ms step_avg:103.93ms
step:1471/1770 train_time:151846ms step_avg:103.93ms
step:1472/1770 train_time:151954ms step_avg:103.94ms
step:1473/1770 train_time:152064ms step_avg:103.94ms
step:1474/1770 train_time:152173ms step_avg:103.94ms
step:1475/1770 train_time:152282ms step_avg:103.95ms
step:1476/1770 train_time:152390ms step_avg:103.95ms
step:1477/1770 train_time:152501ms step_avg:103.95ms
step:1478/1770 train_time:152612ms step_avg:103.96ms
step:1479/1770 train_time:152721ms step_avg:103.96ms
step:1480/1770 train_time:152829ms step_avg:103.97ms
step:1481/1770 train_time:152941ms step_avg:103.97ms
step:1482/1770 train_time:153050ms step_avg:103.97ms
step:1483/1770 train_time:153159ms step_avg:103.98ms
step:1484/1770 train_time:153268ms step_avg:103.98ms
step:1485/1770 train_time:153376ms step_avg:103.98ms
step:1486/1770 train_time:153485ms step_avg:103.99ms
step:1487/1770 train_time:153593ms step_avg:103.99ms
step:1488/1770 train_time:153703ms step_avg:103.99ms
step:1489/1770 train_time:153812ms step_avg:104.00ms
step:1490/1770 train_time:153921ms step_avg:104.00ms
step:1491/1770 train_time:154030ms step_avg:104.00ms
step:1492/1770 train_time:154140ms step_avg:104.01ms
step:1493/1770 train_time:154252ms step_avg:104.01ms
step:1494/1770 train_time:154366ms step_avg:104.02ms
step:1495/1770 train_time:154473ms step_avg:104.02ms
step:1496/1770 train_time:154581ms step_avg:104.02ms
step:1497/1770 train_time:154690ms step_avg:104.03ms
step:1498/1770 train_time:154798ms step_avg:104.03ms
step:1499/1770 train_time:154907ms step_avg:104.03ms
step:1500/1770 train_time:155014ms step_avg:104.04ms
step:1500/1770 val_loss:3.3421 train_time:155121ms step_avg:104.11ms
step:1501/1770 train_time:155139ms step_avg:104.05ms
step:1502/1770 train_time:155240ms step_avg:104.05ms
step:1503/1770 train_time:155349ms step_avg:104.05ms
step:1504/1770 train_time:155458ms step_avg:104.05ms
step:1505/1770 train_time:155568ms step_avg:104.06ms
step:1506/1770 train_time:155678ms step_avg:104.06ms
step:1507/1770 train_time:155787ms step_avg:104.07ms
step:1508/1770 train_time:155897ms step_avg:104.07ms
step:1509/1770 train_time:156006ms step_avg:104.07ms
step:1510/1770 train_time:156114ms step_avg:104.08ms
step:1511/1770 train_time:156224ms step_avg:104.08ms
step:1512/1770 train_time:156332ms step_avg:104.08ms
step:1513/1770 train_time:156443ms step_avg:104.09ms
step:1514/1770 train_time:156552ms step_avg:104.09ms
step:1515/1770 train_time:156661ms step_avg:104.09ms
step:1516/1770 train_time:156770ms step_avg:104.10ms
step:1517/1770 train_time:156879ms step_avg:104.10ms
step:1518/1770 train_time:156989ms step_avg:104.10ms
step:1519/1770 train_time:157097ms step_avg:104.11ms
step:1520/1770 train_time:157207ms step_avg:104.11ms
step:1521/1770 train_time:157315ms step_avg:104.11ms
step:1522/1770 train_time:157424ms step_avg:104.12ms
step:1523/1770 train_time:157534ms step_avg:104.12ms
step:1524/1770 train_time:157643ms step_avg:104.12ms
step:1525/1770 train_time:157751ms step_avg:104.13ms
step:1526/1770 train_time:157861ms step_avg:104.13ms
step:1527/1770 train_time:157969ms step_avg:104.13ms
step:1528/1770 train_time:158079ms step_avg:104.14ms
step:1529/1770 train_time:158187ms step_avg:104.14ms
step:1530/1770 train_time:158296ms step_avg:104.14ms
step:1531/1770 train_time:158405ms step_avg:104.15ms
step:1532/1770 train_time:158515ms step_avg:104.15ms
step:1533/1770 train_time:158624ms step_avg:104.15ms
step:1534/1770 train_time:158733ms step_avg:104.16ms
step:1535/1770 train_time:158842ms step_avg:104.16ms
step:1536/1770 train_time:158951ms step_avg:104.16ms
step:1537/1770 train_time:159061ms step_avg:104.17ms
step:1538/1770 train_time:159171ms step_avg:104.17ms
step:1539/1770 train_time:159280ms step_avg:104.17ms
step:1540/1770 train_time:159392ms step_avg:104.18ms
step:1541/1770 train_time:159501ms step_avg:104.18ms
step:1542/1770 train_time:159610ms step_avg:104.18ms
step:1543/1770 train_time:159719ms step_avg:104.19ms
step:1544/1770 train_time:159830ms step_avg:104.19ms
step:1545/1770 train_time:159939ms step_avg:104.19ms
step:1546/1770 train_time:160049ms step_avg:104.20ms
step:1547/1770 train_time:160157ms step_avg:104.20ms
step:1548/1770 train_time:160266ms step_avg:104.20ms
step:1549/1770 train_time:160373ms step_avg:104.21ms
step:1550/1770 train_time:160482ms step_avg:104.21ms
step:1551/1770 train_time:160591ms step_avg:104.21ms
step:1552/1770 train_time:160701ms step_avg:104.22ms
step:1553/1770 train_time:160810ms step_avg:104.22ms
step:1554/1770 train_time:160918ms step_avg:104.22ms
step:1555/1770 train_time:161027ms step_avg:104.22ms
step:1556/1770 train_time:161135ms step_avg:104.23ms
step:1557/1770 train_time:161244ms step_avg:104.23ms
step:1558/1770 train_time:161353ms step_avg:104.23ms
step:1559/1770 train_time:161463ms step_avg:104.24ms
step:1560/1770 train_time:161570ms step_avg:104.24ms
step:1561/1770 train_time:161681ms step_avg:104.24ms
step:1562/1770 train_time:161790ms step_avg:104.25ms
step:1563/1770 train_time:161899ms step_avg:104.25ms
step:1564/1770 train_time:162008ms step_avg:104.25ms
step:1565/1770 train_time:162116ms step_avg:104.25ms
step:1566/1770 train_time:162225ms step_avg:104.26ms
step:1567/1770 train_time:162334ms step_avg:104.26ms
step:1568/1770 train_time:162442ms step_avg:104.26ms
step:1569/1770 train_time:162555ms step_avg:104.27ms
step:1570/1770 train_time:162663ms step_avg:104.27ms
step:1571/1770 train_time:162771ms step_avg:104.27ms
step:1572/1770 train_time:162881ms step_avg:104.28ms
step:1573/1770 train_time:162992ms step_avg:104.28ms
step:1574/1770 train_time:163102ms step_avg:104.28ms
step:1575/1770 train_time:163209ms step_avg:104.29ms
step:1576/1770 train_time:163318ms step_avg:104.29ms
step:1577/1770 train_time:163428ms step_avg:104.29ms
step:1578/1770 train_time:163537ms step_avg:104.30ms
step:1579/1770 train_time:163646ms step_avg:104.30ms
step:1580/1770 train_time:163754ms step_avg:104.30ms
step:1581/1770 train_time:163866ms step_avg:104.31ms
step:1582/1770 train_time:163976ms step_avg:104.31ms
step:1583/1770 train_time:164085ms step_avg:104.31ms
step:1584/1770 train_time:164194ms step_avg:104.32ms
step:1585/1770 train_time:164303ms step_avg:104.32ms
step:1586/1770 train_time:164415ms step_avg:104.32ms
step:1587/1770 train_time:164524ms step_avg:104.33ms
step:1588/1770 train_time:164633ms step_avg:104.33ms
step:1589/1770 train_time:164745ms step_avg:104.33ms
step:1590/1770 train_time:164853ms step_avg:104.34ms
step:1591/1770 train_time:164963ms step_avg:104.34ms
step:1592/1770 train_time:165073ms step_avg:104.34ms
step:1593/1770 train_time:165182ms step_avg:104.35ms
step:1594/1770 train_time:165290ms step_avg:104.35ms
step:1595/1770 train_time:165400ms step_avg:104.35ms
step:1596/1770 train_time:165509ms step_avg:104.36ms
step:1597/1770 train_time:165617ms step_avg:104.36ms
step:1598/1770 train_time:165726ms step_avg:104.36ms
step:1599/1770 train_time:165836ms step_avg:104.37ms
step:1600/1770 train_time:165947ms step_avg:104.37ms
step:1601/1770 train_time:166056ms step_avg:104.37ms
step:1602/1770 train_time:166167ms step_avg:104.38ms
step:1603/1770 train_time:166275ms step_avg:104.38ms
step:1604/1770 train_time:166383ms step_avg:104.38ms
step:1605/1770 train_time:166491ms step_avg:104.38ms
step:1606/1770 train_time:166600ms step_avg:104.39ms
step:1607/1770 train_time:166712ms step_avg:104.39ms
step:1608/1770 train_time:166821ms step_avg:104.39ms
step:1609/1770 train_time:166929ms step_avg:104.40ms
step:1610/1770 train_time:167040ms step_avg:104.40ms
step:1611/1770 train_time:167151ms step_avg:104.40ms
step:1612/1770 train_time:167260ms step_avg:104.41ms
step:1613/1770 train_time:167369ms step_avg:104.41ms
step:1614/1770 train_time:167478ms step_avg:104.41ms
step:1615/1770 train_time:167587ms step_avg:104.42ms
step:1616/1770 train_time:167696ms step_avg:104.42ms
step:1617/1770 train_time:167807ms step_avg:104.42ms
step:1618/1770 train_time:167916ms step_avg:104.43ms
step:1619/1770 train_time:168026ms step_avg:104.43ms
step:1620/1770 train_time:168135ms step_avg:104.43ms
step:1621/1770 train_time:168244ms step_avg:104.43ms
step:1622/1770 train_time:168354ms step_avg:104.44ms
step:1623/1770 train_time:168466ms step_avg:104.44ms
step:1624/1770 train_time:168574ms step_avg:104.45ms
step:1625/1770 train_time:168682ms step_avg:104.45ms
step:1625/1770 val_loss:3.3074 train_time:168789ms step_avg:104.51ms
step:1626/1770 train_time:168808ms step_avg:104.46ms
step:1627/1770 train_time:168908ms step_avg:104.46ms
step:1628/1770 train_time:169018ms step_avg:104.46ms
step:1629/1770 train_time:169127ms step_avg:104.46ms
step:1630/1770 train_time:169236ms step_avg:104.47ms
step:1631/1770 train_time:169344ms step_avg:104.47ms
step:1632/1770 train_time:169453ms step_avg:104.47ms
step:1633/1770 train_time:169561ms step_avg:104.47ms
step:1634/1770 train_time:169670ms step_avg:104.48ms
step:1635/1770 train_time:169779ms step_avg:104.48ms
step:1636/1770 train_time:169888ms step_avg:104.48ms
step:1637/1770 train_time:169998ms step_avg:104.49ms
step:1638/1770 train_time:170106ms step_avg:104.49ms
step:1639/1770 train_time:170216ms step_avg:104.49ms
step:1640/1770 train_time:170326ms step_avg:104.49ms
step:1641/1770 train_time:170434ms step_avg:104.50ms
step:1642/1770 train_time:170543ms step_avg:104.50ms
step:1643/1770 train_time:170651ms step_avg:104.50ms
step:1644/1770 train_time:170762ms step_avg:104.51ms
step:1645/1770 train_time:170870ms step_avg:104.51ms
step:1646/1770 train_time:170982ms step_avg:104.51ms
step:1647/1770 train_time:171091ms step_avg:104.51ms
step:1648/1770 train_time:171199ms step_avg:104.52ms
step:1649/1770 train_time:171307ms step_avg:104.52ms
step:1650/1770 train_time:171417ms step_avg:104.52ms
step:1651/1770 train_time:171524ms step_avg:104.52ms
step:1652/1770 train_time:171633ms step_avg:104.53ms
step:1653/1770 train_time:171742ms step_avg:104.53ms
step:1654/1770 train_time:171856ms step_avg:104.54ms
step:1655/1770 train_time:171967ms step_avg:104.54ms
step:1656/1770 train_time:172076ms step_avg:104.54ms
step:1657/1770 train_time:172187ms step_avg:104.55ms
step:1658/1770 train_time:172296ms step_avg:104.55ms
step:1659/1770 train_time:172406ms step_avg:104.55ms
step:1660/1770 train_time:172515ms step_avg:104.55ms
step:1661/1770 train_time:172625ms step_avg:104.56ms
step:1662/1770 train_time:172734ms step_avg:104.56ms
step:1663/1770 train_time:172842ms step_avg:104.56ms
step:1664/1770 train_time:172951ms step_avg:104.57ms
step:1665/1770 train_time:173060ms step_avg:104.57ms
step:1666/1770 train_time:173170ms step_avg:104.57ms
step:1667/1770 train_time:173279ms step_avg:104.57ms
step:1668/1770 train_time:173387ms step_avg:104.58ms
step:1669/1770 train_time:173495ms step_avg:104.58ms
step:1670/1770 train_time:173603ms step_avg:104.58ms
step:1671/1770 train_time:173713ms step_avg:104.58ms
step:1672/1770 train_time:173822ms step_avg:104.59ms
step:1673/1770 train_time:173931ms step_avg:104.59ms
step:1674/1770 train_time:174040ms step_avg:104.59ms
step:1675/1770 train_time:174148ms step_avg:104.59ms
step:1676/1770 train_time:174259ms step_avg:104.60ms
step:1677/1770 train_time:174371ms step_avg:104.60ms
step:1678/1770 train_time:174479ms step_avg:104.60ms
step:1679/1770 train_time:174589ms step_avg:104.61ms
step:1680/1770 train_time:174698ms step_avg:104.61ms
step:1681/1770 train_time:174807ms step_avg:104.61ms
step:1682/1770 train_time:174918ms step_avg:104.62ms
step:1683/1770 train_time:175026ms step_avg:104.62ms
step:1684/1770 train_time:175135ms step_avg:104.62ms
step:1685/1770 train_time:175244ms step_avg:104.62ms
step:1686/1770 train_time:175354ms step_avg:104.63ms
step:1687/1770 train_time:175465ms step_avg:104.63ms
step:1688/1770 train_time:175574ms step_avg:104.63ms
step:1689/1770 train_time:175683ms step_avg:104.64ms
step:1690/1770 train_time:175792ms step_avg:104.64ms
step:1691/1770 train_time:175900ms step_avg:104.64ms
step:1692/1770 train_time:176009ms step_avg:104.64ms
step:1693/1770 train_time:176120ms step_avg:104.65ms
step:1694/1770 train_time:176229ms step_avg:104.65ms
step:1695/1770 train_time:176339ms step_avg:104.65ms
step:1696/1770 train_time:176450ms step_avg:104.66ms
step:1697/1770 train_time:176560ms step_avg:104.66ms
step:1698/1770 train_time:176669ms step_avg:104.66ms
step:1699/1770 train_time:176778ms step_avg:104.66ms
step:1700/1770 train_time:176886ms step_avg:104.67ms
step:1701/1770 train_time:176996ms step_avg:104.67ms
step:1702/1770 train_time:177105ms step_avg:104.67ms
step:1703/1770 train_time:177213ms step_avg:104.67ms
step:1704/1770 train_time:177323ms step_avg:104.68ms
step:1705/1770 train_time:177431ms step_avg:104.68ms
step:1706/1770 train_time:177539ms step_avg:104.68ms
step:1707/1770 train_time:177648ms step_avg:104.68ms
step:1708/1770 train_time:177758ms step_avg:104.69ms
step:1709/1770 train_time:177869ms step_avg:104.69ms
step:1710/1770 train_time:177981ms step_avg:104.69ms
step:1711/1770 train_time:178092ms step_avg:104.70ms
step:1712/1770 train_time:178203ms step_avg:104.70ms
step:1713/1770 train_time:178311ms step_avg:104.70ms
step:1714/1770 train_time:178421ms step_avg:104.71ms
step:1715/1770 train_time:178530ms step_avg:104.71ms
step:1716/1770 train_time:178641ms step_avg:104.71ms
step:1717/1770 train_time:178749ms step_avg:104.72ms
step:1718/1770 train_time:178861ms step_avg:104.72ms
step:1719/1770 train_time:178972ms step_avg:104.72ms
step:1720/1770 train_time:179083ms step_avg:104.73ms
step:1721/1770 train_time:179193ms step_avg:104.73ms
step:1722/1770 train_time:179305ms step_avg:104.73ms
step:1723/1770 train_time:179416ms step_avg:104.74ms
step:1724/1770 train_time:179528ms step_avg:104.74ms
step:1725/1770 train_time:179641ms step_avg:104.75ms
step:1726/1770 train_time:179752ms step_avg:104.75ms
step:1727/1770 train_time:179861ms step_avg:104.75ms
step:1728/1770 train_time:179973ms step_avg:104.76ms
step:1729/1770 train_time:180083ms step_avg:104.76ms
step:1730/1770 train_time:180193ms step_avg:104.76ms
step:1731/1770 train_time:180305ms step_avg:104.77ms
step:1732/1770 train_time:180415ms step_avg:104.77ms
step:1733/1770 train_time:180526ms step_avg:104.77ms
step:1734/1770 train_time:180635ms step_avg:104.78ms
step:1735/1770 train_time:180745ms step_avg:104.78ms
step:1736/1770 train_time:180855ms step_avg:104.78ms
step:1737/1770 train_time:180966ms step_avg:104.79ms
step:1738/1770 train_time:181076ms step_avg:104.79ms
step:1739/1770 train_time:181186ms step_avg:104.79ms
step:1740/1770 train_time:181295ms step_avg:104.79ms
step:1741/1770 train_time:181407ms step_avg:104.80ms
step:1742/1770 train_time:181521ms step_avg:104.80ms
step:1743/1770 train_time:181630ms step_avg:104.81ms
step:1744/1770 train_time:181741ms step_avg:104.81ms
step:1745/1770 train_time:181850ms step_avg:104.81ms
step:1746/1770 train_time:181963ms step_avg:104.82ms
step:1747/1770 train_time:182071ms step_avg:104.82ms
step:1748/1770 train_time:182184ms step_avg:104.82ms
step:1749/1770 train_time:182296ms step_avg:104.83ms
step:1750/1770 train_time:182405ms step_avg:104.83ms
step:1750/1770 val_loss:3.2803 train_time:182514ms step_avg:104.89ms
step:1751/1770 train_time:182532ms step_avg:104.84ms
step:1752/1770 train_time:182634ms step_avg:104.84ms
step:1753/1770 train_time:182745ms step_avg:104.85ms
step:1754/1770 train_time:182855ms step_avg:104.85ms
step:1755/1770 train_time:182963ms step_avg:104.85ms
step:1756/1770 train_time:183074ms step_avg:104.85ms
step:1757/1770 train_time:183185ms step_avg:104.86ms
step:1758/1770 train_time:183295ms step_avg:104.86ms
step:1759/1770 train_time:183406ms step_avg:104.86ms
step:1760/1770 train_time:183517ms step_avg:104.87ms
step:1761/1770 train_time:183630ms step_avg:104.87ms
step:1762/1770 train_time:183742ms step_avg:104.88ms
step:1763/1770 train_time:183851ms step_avg:104.88ms
step:1764/1770 train_time:183961ms step_avg:104.88ms
step:1765/1770 train_time:184071ms step_avg:104.88ms
step:1766/1770 train_time:184184ms step_avg:104.89ms
step:1767/1770 train_time:184293ms step_avg:104.89ms
step:1768/1770 train_time:184403ms step_avg:104.89ms
step:1769/1770 train_time:184512ms step_avg:104.90ms
step:1770/1770 train_time:184622ms step_avg:104.90ms
step:1770/1770 val_loss:3.2774 train_time:184732ms step_avg:104.96ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
