import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:20:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23374ms step_avg:nanms
step:2/1770 train_time:23812ms step_avg:nanms
step:3/1770 train_time:23911ms step_avg:nanms
step:4/1770 train_time:24008ms step_avg:nanms
step:5/1770 train_time:24107ms step_avg:nanms
step:6/1770 train_time:24205ms step_avg:nanms
step:7/1770 train_time:24304ms step_avg:nanms
step:8/1770 train_time:24403ms step_avg:nanms
step:9/1770 train_time:24501ms step_avg:nanms
step:10/1770 train_time:24600ms step_avg:nanms
step:11/1770 train_time:99ms step_avg:nanms
step:12/1770 train_time:198ms step_avg:nanms
step:13/1770 train_time:297ms step_avg:99.04ms
step:14/1770 train_time:396ms step_avg:99.07ms
step:15/1770 train_time:495ms step_avg:99.04ms
step:16/1770 train_time:594ms step_avg:99.08ms
step:17/1770 train_time:693ms step_avg:99.02ms
step:18/1770 train_time:792ms step_avg:99.04ms
step:19/1770 train_time:891ms step_avg:99.03ms
step:20/1770 train_time:990ms step_avg:99.01ms
step:21/1770 train_time:1089ms step_avg:99.04ms
step:22/1770 train_time:1188ms step_avg:99.01ms
step:23/1770 train_time:1288ms step_avg:99.05ms
step:24/1770 train_time:1386ms step_avg:99.03ms
step:25/1770 train_time:1485ms step_avg:99.02ms
step:26/1770 train_time:1584ms step_avg:99.02ms
step:27/1770 train_time:1684ms step_avg:99.03ms
step:28/1770 train_time:1783ms step_avg:99.03ms
step:29/1770 train_time:1882ms step_avg:99.06ms
step:30/1770 train_time:1983ms step_avg:99.15ms
step:31/1770 train_time:2081ms step_avg:99.12ms
step:32/1770 train_time:2181ms step_avg:99.12ms
step:33/1770 train_time:2280ms step_avg:99.15ms
step:34/1770 train_time:2380ms step_avg:99.15ms
step:35/1770 train_time:2479ms step_avg:99.15ms
step:36/1770 train_time:2578ms step_avg:99.16ms
step:37/1770 train_time:2678ms step_avg:99.20ms
step:38/1770 train_time:2778ms step_avg:99.20ms
step:39/1770 train_time:2877ms step_avg:99.21ms
step:40/1770 train_time:2976ms step_avg:99.20ms
step:41/1770 train_time:3076ms step_avg:99.22ms
step:42/1770 train_time:3175ms step_avg:99.23ms
step:43/1770 train_time:3275ms step_avg:99.24ms
step:44/1770 train_time:3374ms step_avg:99.24ms
step:45/1770 train_time:3473ms step_avg:99.24ms
step:46/1770 train_time:3573ms step_avg:99.24ms
step:47/1770 train_time:3672ms step_avg:99.25ms
step:48/1770 train_time:3771ms step_avg:99.24ms
step:49/1770 train_time:3873ms step_avg:99.31ms
step:50/1770 train_time:3970ms step_avg:99.26ms
step:51/1770 train_time:4070ms step_avg:99.27ms
step:52/1770 train_time:4169ms step_avg:99.26ms
step:53/1770 train_time:4268ms step_avg:99.26ms
step:54/1770 train_time:4367ms step_avg:99.26ms
step:55/1770 train_time:4467ms step_avg:99.26ms
step:56/1770 train_time:4566ms step_avg:99.26ms
step:57/1770 train_time:4665ms step_avg:99.25ms
step:58/1770 train_time:4764ms step_avg:99.25ms
step:59/1770 train_time:4863ms step_avg:99.25ms
step:60/1770 train_time:4962ms step_avg:99.24ms
step:61/1770 train_time:5061ms step_avg:99.23ms
step:62/1770 train_time:5161ms step_avg:99.25ms
step:63/1770 train_time:5261ms step_avg:99.26ms
step:64/1770 train_time:5360ms step_avg:99.26ms
step:65/1770 train_time:5460ms step_avg:99.28ms
step:66/1770 train_time:5561ms step_avg:99.30ms
step:67/1770 train_time:5660ms step_avg:99.30ms
step:68/1770 train_time:5759ms step_avg:99.30ms
step:69/1770 train_time:5860ms step_avg:99.31ms
step:70/1770 train_time:5960ms step_avg:99.33ms
step:71/1770 train_time:6061ms step_avg:99.35ms
step:72/1770 train_time:6161ms step_avg:99.37ms
step:73/1770 train_time:6261ms step_avg:99.39ms
step:74/1770 train_time:6361ms step_avg:99.39ms
step:75/1770 train_time:6460ms step_avg:99.39ms
step:76/1770 train_time:6560ms step_avg:99.39ms
step:77/1770 train_time:6660ms step_avg:99.40ms
step:78/1770 train_time:6759ms step_avg:99.40ms
step:79/1770 train_time:6859ms step_avg:99.40ms
step:80/1770 train_time:6958ms step_avg:99.41ms
step:81/1770 train_time:7058ms step_avg:99.41ms
step:82/1770 train_time:7157ms step_avg:99.40ms
step:83/1770 train_time:7256ms step_avg:99.40ms
step:84/1770 train_time:7355ms step_avg:99.40ms
step:85/1770 train_time:7455ms step_avg:99.40ms
step:86/1770 train_time:7554ms step_avg:99.40ms
step:87/1770 train_time:7653ms step_avg:99.40ms
step:88/1770 train_time:7752ms step_avg:99.39ms
step:89/1770 train_time:7852ms step_avg:99.39ms
step:90/1770 train_time:7950ms step_avg:99.38ms
step:91/1770 train_time:8049ms step_avg:99.37ms
step:92/1770 train_time:8148ms step_avg:99.36ms
step:93/1770 train_time:8248ms step_avg:99.37ms
step:94/1770 train_time:8346ms step_avg:99.36ms
step:95/1770 train_time:8444ms step_avg:99.35ms
step:96/1770 train_time:8543ms step_avg:99.34ms
step:97/1770 train_time:8643ms step_avg:99.34ms
step:98/1770 train_time:8742ms step_avg:99.34ms
step:99/1770 train_time:8842ms step_avg:99.35ms
step:100/1770 train_time:8942ms step_avg:99.35ms
step:101/1770 train_time:9042ms step_avg:99.36ms
step:102/1770 train_time:9142ms step_avg:99.37ms
step:103/1770 train_time:9242ms step_avg:99.37ms
step:104/1770 train_time:9342ms step_avg:99.38ms
step:105/1770 train_time:9442ms step_avg:99.39ms
step:106/1770 train_time:9542ms step_avg:99.40ms
step:107/1770 train_time:9641ms step_avg:99.40ms
step:108/1770 train_time:9740ms step_avg:99.39ms
step:109/1770 train_time:9840ms step_avg:99.40ms
step:110/1770 train_time:9940ms step_avg:99.40ms
step:111/1770 train_time:10040ms step_avg:99.41ms
step:112/1770 train_time:10140ms step_avg:99.41ms
step:113/1770 train_time:10240ms step_avg:99.42ms
step:114/1770 train_time:10340ms step_avg:99.42ms
step:115/1770 train_time:10440ms step_avg:99.42ms
step:116/1770 train_time:10540ms step_avg:99.44ms
step:117/1770 train_time:10641ms step_avg:99.45ms
step:118/1770 train_time:10740ms step_avg:99.45ms
step:119/1770 train_time:10840ms step_avg:99.45ms
step:120/1770 train_time:10940ms step_avg:99.45ms
step:121/1770 train_time:11040ms step_avg:99.46ms
step:122/1770 train_time:11139ms step_avg:99.46ms
step:123/1770 train_time:11238ms step_avg:99.45ms
step:124/1770 train_time:11337ms step_avg:99.45ms
step:125/1770 train_time:11437ms step_avg:99.45ms
step:125/1770 val_loss:4.6452 train_time:11535ms step_avg:100.30ms
step:126/1770 train_time:11555ms step_avg:99.61ms
step:127/1770 train_time:11647ms step_avg:99.55ms
step:128/1770 train_time:11751ms step_avg:99.59ms
step:129/1770 train_time:11852ms step_avg:99.59ms
step:130/1770 train_time:11951ms step_avg:99.59ms
step:131/1770 train_time:12050ms step_avg:99.58ms
step:132/1770 train_time:12148ms step_avg:99.58ms
step:133/1770 train_time:12247ms step_avg:99.57ms
step:134/1770 train_time:12347ms step_avg:99.57ms
step:135/1770 train_time:12446ms step_avg:99.57ms
step:136/1770 train_time:12546ms step_avg:99.57ms
step:137/1770 train_time:12645ms step_avg:99.57ms
step:138/1770 train_time:12749ms step_avg:99.60ms
step:139/1770 train_time:12845ms step_avg:99.57ms
step:140/1770 train_time:12945ms step_avg:99.58ms
step:141/1770 train_time:13045ms step_avg:99.58ms
step:142/1770 train_time:13144ms step_avg:99.58ms
step:143/1770 train_time:13245ms step_avg:99.59ms
step:144/1770 train_time:13344ms step_avg:99.58ms
step:145/1770 train_time:13444ms step_avg:99.59ms
step:146/1770 train_time:13545ms step_avg:99.59ms
step:147/1770 train_time:13644ms step_avg:99.59ms
step:148/1770 train_time:13745ms step_avg:99.60ms
step:149/1770 train_time:13845ms step_avg:99.61ms
step:150/1770 train_time:13947ms step_avg:99.62ms
step:151/1770 train_time:14046ms step_avg:99.61ms
step:152/1770 train_time:14146ms step_avg:99.62ms
step:153/1770 train_time:14246ms step_avg:99.62ms
step:154/1770 train_time:14345ms step_avg:99.62ms
step:155/1770 train_time:14445ms step_avg:99.62ms
step:156/1770 train_time:14545ms step_avg:99.62ms
step:157/1770 train_time:14649ms step_avg:99.65ms
step:158/1770 train_time:14745ms step_avg:99.63ms
step:159/1770 train_time:14845ms step_avg:99.63ms
step:160/1770 train_time:14944ms step_avg:99.63ms
step:161/1770 train_time:15045ms step_avg:99.63ms
step:162/1770 train_time:15144ms step_avg:99.63ms
step:163/1770 train_time:15244ms step_avg:99.64ms
step:164/1770 train_time:15345ms step_avg:99.64ms
step:165/1770 train_time:15445ms step_avg:99.64ms
step:166/1770 train_time:15544ms step_avg:99.64ms
step:167/1770 train_time:15644ms step_avg:99.65ms
step:168/1770 train_time:15744ms step_avg:99.64ms
step:169/1770 train_time:15844ms step_avg:99.65ms
step:170/1770 train_time:15944ms step_avg:99.65ms
step:171/1770 train_time:16044ms step_avg:99.65ms
step:172/1770 train_time:16144ms step_avg:99.65ms
step:173/1770 train_time:16244ms step_avg:99.66ms
step:174/1770 train_time:16343ms step_avg:99.66ms
step:175/1770 train_time:16444ms step_avg:99.66ms
step:176/1770 train_time:16544ms step_avg:99.66ms
step:177/1770 train_time:16644ms step_avg:99.66ms
step:178/1770 train_time:16744ms step_avg:99.67ms
step:179/1770 train_time:16843ms step_avg:99.66ms
step:180/1770 train_time:16943ms step_avg:99.66ms
step:181/1770 train_time:17042ms step_avg:99.66ms
step:182/1770 train_time:17143ms step_avg:99.67ms
step:183/1770 train_time:17243ms step_avg:99.67ms
step:184/1770 train_time:17342ms step_avg:99.67ms
step:185/1770 train_time:17443ms step_avg:99.67ms
step:186/1770 train_time:17544ms step_avg:99.68ms
step:187/1770 train_time:17643ms step_avg:99.68ms
step:188/1770 train_time:17744ms step_avg:99.69ms
step:189/1770 train_time:17844ms step_avg:99.69ms
step:190/1770 train_time:17945ms step_avg:99.69ms
step:191/1770 train_time:18044ms step_avg:99.69ms
step:192/1770 train_time:18144ms step_avg:99.69ms
step:193/1770 train_time:18244ms step_avg:99.70ms
step:194/1770 train_time:18344ms step_avg:99.70ms
step:195/1770 train_time:18444ms step_avg:99.70ms
step:196/1770 train_time:18546ms step_avg:99.71ms
step:197/1770 train_time:18643ms step_avg:99.70ms
step:198/1770 train_time:18744ms step_avg:99.70ms
step:199/1770 train_time:18843ms step_avg:99.70ms
step:200/1770 train_time:18943ms step_avg:99.70ms
step:201/1770 train_time:19043ms step_avg:99.70ms
step:202/1770 train_time:19143ms step_avg:99.70ms
step:203/1770 train_time:19242ms step_avg:99.70ms
step:204/1770 train_time:19343ms step_avg:99.70ms
step:205/1770 train_time:19443ms step_avg:99.71ms
step:206/1770 train_time:19543ms step_avg:99.71ms
step:207/1770 train_time:19644ms step_avg:99.71ms
step:208/1770 train_time:19745ms step_avg:99.72ms
step:209/1770 train_time:19846ms step_avg:99.73ms
step:210/1770 train_time:19945ms step_avg:99.73ms
step:211/1770 train_time:20046ms step_avg:99.73ms
step:212/1770 train_time:20146ms step_avg:99.73ms
step:213/1770 train_time:20245ms step_avg:99.73ms
step:214/1770 train_time:20346ms step_avg:99.73ms
step:215/1770 train_time:20446ms step_avg:99.74ms
step:216/1770 train_time:20545ms step_avg:99.73ms
step:217/1770 train_time:20645ms step_avg:99.74ms
step:218/1770 train_time:20745ms step_avg:99.74ms
step:219/1770 train_time:20845ms step_avg:99.74ms
step:220/1770 train_time:20945ms step_avg:99.74ms
step:221/1770 train_time:21045ms step_avg:99.74ms
step:222/1770 train_time:21145ms step_avg:99.74ms
step:223/1770 train_time:21245ms step_avg:99.74ms
step:224/1770 train_time:21344ms step_avg:99.74ms
step:225/1770 train_time:21444ms step_avg:99.74ms
step:226/1770 train_time:21544ms step_avg:99.74ms
step:227/1770 train_time:21646ms step_avg:99.75ms
step:228/1770 train_time:21744ms step_avg:99.74ms
step:229/1770 train_time:21845ms step_avg:99.75ms
step:230/1770 train_time:21944ms step_avg:99.75ms
step:231/1770 train_time:22044ms step_avg:99.75ms
step:232/1770 train_time:22144ms step_avg:99.75ms
step:233/1770 train_time:22245ms step_avg:99.75ms
step:234/1770 train_time:22344ms step_avg:99.75ms
step:235/1770 train_time:22444ms step_avg:99.75ms
step:236/1770 train_time:22544ms step_avg:99.75ms
step:237/1770 train_time:22644ms step_avg:99.76ms
step:238/1770 train_time:22745ms step_avg:99.76ms
step:239/1770 train_time:22845ms step_avg:99.76ms
step:240/1770 train_time:22945ms step_avg:99.76ms
step:241/1770 train_time:23045ms step_avg:99.76ms
step:242/1770 train_time:23145ms step_avg:99.76ms
step:243/1770 train_time:23244ms step_avg:99.76ms
step:244/1770 train_time:23344ms step_avg:99.76ms
step:245/1770 train_time:23444ms step_avg:99.76ms
step:246/1770 train_time:23544ms step_avg:99.76ms
step:247/1770 train_time:23644ms step_avg:99.76ms
step:248/1770 train_time:23743ms step_avg:99.76ms
step:249/1770 train_time:23843ms step_avg:99.76ms
step:250/1770 train_time:23943ms step_avg:99.76ms
step:250/1770 val_loss:4.1026 train_time:24041ms step_avg:100.17ms
step:251/1770 train_time:24061ms step_avg:99.84ms
step:252/1770 train_time:24158ms step_avg:99.83ms
step:253/1770 train_time:24263ms step_avg:99.85ms
step:254/1770 train_time:24363ms step_avg:99.85ms
step:255/1770 train_time:24463ms step_avg:99.85ms
step:256/1770 train_time:24562ms step_avg:99.85ms
step:257/1770 train_time:24662ms step_avg:99.85ms
step:258/1770 train_time:24762ms step_avg:99.84ms
step:259/1770 train_time:24862ms step_avg:99.85ms
step:260/1770 train_time:24961ms step_avg:99.85ms
step:261/1770 train_time:25061ms step_avg:99.84ms
step:262/1770 train_time:25161ms step_avg:99.84ms
step:263/1770 train_time:25261ms step_avg:99.85ms
step:264/1770 train_time:25362ms step_avg:99.85ms
step:265/1770 train_time:25462ms step_avg:99.85ms
step:266/1770 train_time:25562ms step_avg:99.85ms
step:267/1770 train_time:25663ms step_avg:99.85ms
step:268/1770 train_time:25763ms step_avg:99.86ms
step:269/1770 train_time:25863ms step_avg:99.86ms
step:270/1770 train_time:25963ms step_avg:99.86ms
step:271/1770 train_time:26063ms step_avg:99.86ms
step:272/1770 train_time:26163ms step_avg:99.86ms
step:273/1770 train_time:26263ms step_avg:99.86ms
step:274/1770 train_time:26364ms step_avg:99.86ms
step:275/1770 train_time:26464ms step_avg:99.86ms
step:276/1770 train_time:26567ms step_avg:99.88ms
step:277/1770 train_time:26664ms step_avg:99.87ms
step:278/1770 train_time:26764ms step_avg:99.87ms
step:279/1770 train_time:26864ms step_avg:99.87ms
step:280/1770 train_time:26964ms step_avg:99.87ms
step:281/1770 train_time:27065ms step_avg:99.87ms
step:282/1770 train_time:27165ms step_avg:99.87ms
step:283/1770 train_time:27265ms step_avg:99.87ms
step:284/1770 train_time:27366ms step_avg:99.88ms
step:285/1770 train_time:27467ms step_avg:99.88ms
step:286/1770 train_time:27568ms step_avg:99.88ms
step:287/1770 train_time:27669ms step_avg:99.89ms
step:288/1770 train_time:27770ms step_avg:99.89ms
step:289/1770 train_time:27870ms step_avg:99.89ms
step:290/1770 train_time:27970ms step_avg:99.89ms
step:291/1770 train_time:28071ms step_avg:99.90ms
step:292/1770 train_time:28170ms step_avg:99.89ms
step:293/1770 train_time:28271ms step_avg:99.90ms
step:294/1770 train_time:28371ms step_avg:99.90ms
step:295/1770 train_time:28472ms step_avg:99.90ms
step:296/1770 train_time:28573ms step_avg:99.91ms
step:297/1770 train_time:28676ms step_avg:99.92ms
step:298/1770 train_time:28775ms step_avg:99.91ms
step:299/1770 train_time:28877ms step_avg:99.92ms
step:300/1770 train_time:28977ms step_avg:99.92ms
step:301/1770 train_time:29076ms step_avg:99.92ms
step:302/1770 train_time:29176ms step_avg:99.92ms
step:303/1770 train_time:29276ms step_avg:99.92ms
step:304/1770 train_time:29377ms step_avg:99.92ms
step:305/1770 train_time:29477ms step_avg:99.92ms
step:306/1770 train_time:29577ms step_avg:99.92ms
step:307/1770 train_time:29677ms step_avg:99.92ms
step:308/1770 train_time:29777ms step_avg:99.92ms
step:309/1770 train_time:29877ms step_avg:99.92ms
step:310/1770 train_time:29976ms step_avg:99.92ms
step:311/1770 train_time:30076ms step_avg:99.92ms
step:312/1770 train_time:30177ms step_avg:99.93ms
step:313/1770 train_time:30277ms step_avg:99.92ms
step:314/1770 train_time:30376ms step_avg:99.92ms
step:315/1770 train_time:30476ms step_avg:99.92ms
step:316/1770 train_time:30576ms step_avg:99.92ms
step:317/1770 train_time:30676ms step_avg:99.92ms
step:318/1770 train_time:30777ms step_avg:99.92ms
step:319/1770 train_time:30876ms step_avg:99.92ms
step:320/1770 train_time:30976ms step_avg:99.92ms
step:321/1770 train_time:31076ms step_avg:99.92ms
step:322/1770 train_time:31176ms step_avg:99.92ms
step:323/1770 train_time:31276ms step_avg:99.92ms
step:324/1770 train_time:31376ms step_avg:99.92ms
step:325/1770 train_time:31476ms step_avg:99.92ms
step:326/1770 train_time:31576ms step_avg:99.93ms
step:327/1770 train_time:31679ms step_avg:99.93ms
step:328/1770 train_time:31777ms step_avg:99.93ms
step:329/1770 train_time:31877ms step_avg:99.93ms
step:330/1770 train_time:31977ms step_avg:99.93ms
step:331/1770 train_time:32077ms step_avg:99.93ms
step:332/1770 train_time:32177ms step_avg:99.93ms
step:333/1770 train_time:32277ms step_avg:99.93ms
step:334/1770 train_time:32377ms step_avg:99.93ms
step:335/1770 train_time:32476ms step_avg:99.93ms
step:336/1770 train_time:32576ms step_avg:99.93ms
step:337/1770 train_time:32677ms step_avg:99.93ms
step:338/1770 train_time:32776ms step_avg:99.93ms
step:339/1770 train_time:32877ms step_avg:99.93ms
step:340/1770 train_time:32977ms step_avg:99.93ms
step:341/1770 train_time:33077ms step_avg:99.93ms
step:342/1770 train_time:33177ms step_avg:99.93ms
step:343/1770 train_time:33277ms step_avg:99.93ms
step:344/1770 train_time:33377ms step_avg:99.93ms
step:345/1770 train_time:33477ms step_avg:99.93ms
step:346/1770 train_time:33577ms step_avg:99.93ms
step:347/1770 train_time:33679ms step_avg:99.94ms
step:348/1770 train_time:33777ms step_avg:99.93ms
step:349/1770 train_time:33877ms step_avg:99.93ms
step:350/1770 train_time:33977ms step_avg:99.93ms
step:351/1770 train_time:34077ms step_avg:99.93ms
step:352/1770 train_time:34177ms step_avg:99.93ms
step:353/1770 train_time:34277ms step_avg:99.93ms
step:354/1770 train_time:34377ms step_avg:99.93ms
step:355/1770 train_time:34477ms step_avg:99.93ms
step:356/1770 train_time:34577ms step_avg:99.93ms
step:357/1770 train_time:34676ms step_avg:99.93ms
step:358/1770 train_time:34776ms step_avg:99.93ms
step:359/1770 train_time:34877ms step_avg:99.93ms
step:360/1770 train_time:34977ms step_avg:99.93ms
step:361/1770 train_time:35077ms step_avg:99.93ms
step:362/1770 train_time:35177ms step_avg:99.93ms
step:363/1770 train_time:35277ms step_avg:99.94ms
step:364/1770 train_time:35377ms step_avg:99.93ms
step:365/1770 train_time:35477ms step_avg:99.93ms
step:366/1770 train_time:35578ms step_avg:99.94ms
step:367/1770 train_time:35676ms step_avg:99.93ms
step:368/1770 train_time:35776ms step_avg:99.93ms
step:369/1770 train_time:35876ms step_avg:99.93ms
step:370/1770 train_time:35976ms step_avg:99.93ms
step:371/1770 train_time:36076ms step_avg:99.93ms
step:372/1770 train_time:36177ms step_avg:99.94ms
step:373/1770 train_time:36276ms step_avg:99.93ms
step:374/1770 train_time:36376ms step_avg:99.93ms
step:375/1770 train_time:36476ms step_avg:99.93ms
step:375/1770 val_loss:3.8974 train_time:36575ms step_avg:100.21ms
step:376/1770 train_time:36594ms step_avg:99.98ms
step:377/1770 train_time:36687ms step_avg:99.96ms
step:378/1770 train_time:36793ms step_avg:99.98ms
step:379/1770 train_time:36894ms step_avg:99.99ms
step:380/1770 train_time:36994ms step_avg:99.98ms
step:381/1770 train_time:37094ms step_avg:99.98ms
step:382/1770 train_time:37194ms step_avg:99.98ms
step:383/1770 train_time:37294ms step_avg:99.98ms
step:384/1770 train_time:37394ms step_avg:99.98ms
step:385/1770 train_time:37494ms step_avg:99.98ms
step:386/1770 train_time:37594ms step_avg:99.98ms
step:387/1770 train_time:37694ms step_avg:99.98ms
step:388/1770 train_time:37794ms step_avg:99.98ms
step:389/1770 train_time:37894ms step_avg:99.98ms
step:390/1770 train_time:37994ms step_avg:99.98ms
step:391/1770 train_time:38094ms step_avg:99.99ms
step:392/1770 train_time:38196ms step_avg:99.99ms
step:393/1770 train_time:38295ms step_avg:99.99ms
step:394/1770 train_time:38395ms step_avg:99.99ms
step:395/1770 train_time:38495ms step_avg:99.99ms
step:396/1770 train_time:38597ms step_avg:99.99ms
step:397/1770 train_time:38699ms step_avg:100.00ms
step:398/1770 train_time:38801ms step_avg:100.00ms
step:399/1770 train_time:38904ms step_avg:100.01ms
step:400/1770 train_time:39006ms step_avg:100.02ms
step:401/1770 train_time:39108ms step_avg:100.02ms
step:402/1770 train_time:39211ms step_avg:100.03ms
step:403/1770 train_time:39314ms step_avg:100.04ms
step:404/1770 train_time:39417ms step_avg:100.04ms
step:405/1770 train_time:39519ms step_avg:100.05ms
step:406/1770 train_time:39621ms step_avg:100.05ms
step:407/1770 train_time:39723ms step_avg:100.06ms
step:408/1770 train_time:39825ms step_avg:100.06ms
step:409/1770 train_time:39927ms step_avg:100.07ms
step:410/1770 train_time:40030ms step_avg:100.07ms
step:411/1770 train_time:40132ms step_avg:100.08ms
step:412/1770 train_time:40234ms step_avg:100.09ms
step:413/1770 train_time:40337ms step_avg:100.09ms
step:414/1770 train_time:40439ms step_avg:100.10ms
step:415/1770 train_time:40540ms step_avg:100.10ms
step:416/1770 train_time:40641ms step_avg:100.10ms
step:417/1770 train_time:40743ms step_avg:100.11ms
step:418/1770 train_time:40845ms step_avg:100.11ms
step:419/1770 train_time:40947ms step_avg:100.12ms
step:420/1770 train_time:41049ms step_avg:100.12ms
step:421/1770 train_time:41152ms step_avg:100.13ms
step:422/1770 train_time:41254ms step_avg:100.13ms
step:423/1770 train_time:41356ms step_avg:100.14ms
step:424/1770 train_time:41458ms step_avg:100.14ms
step:425/1770 train_time:41560ms step_avg:100.14ms
step:426/1770 train_time:41663ms step_avg:100.15ms
step:427/1770 train_time:41769ms step_avg:100.17ms
step:428/1770 train_time:41867ms step_avg:100.16ms
step:429/1770 train_time:41969ms step_avg:100.17ms
step:430/1770 train_time:42072ms step_avg:100.17ms
step:431/1770 train_time:42174ms step_avg:100.18ms
step:432/1770 train_time:42276ms step_avg:100.18ms
step:433/1770 train_time:42378ms step_avg:100.19ms
step:434/1770 train_time:42481ms step_avg:100.19ms
step:435/1770 train_time:42583ms step_avg:100.20ms
step:436/1770 train_time:42685ms step_avg:100.20ms
step:437/1770 train_time:42788ms step_avg:100.21ms
step:438/1770 train_time:42890ms step_avg:100.21ms
step:439/1770 train_time:42992ms step_avg:100.21ms
step:440/1770 train_time:43095ms step_avg:100.22ms
step:441/1770 train_time:43197ms step_avg:100.22ms
step:442/1770 train_time:43298ms step_avg:100.23ms
step:443/1770 train_time:43400ms step_avg:100.23ms
step:444/1770 train_time:43502ms step_avg:100.24ms
step:445/1770 train_time:43604ms step_avg:100.24ms
step:446/1770 train_time:43706ms step_avg:100.24ms
step:447/1770 train_time:43809ms step_avg:100.25ms
step:448/1770 train_time:43912ms step_avg:100.25ms
step:449/1770 train_time:44015ms step_avg:100.26ms
step:450/1770 train_time:44117ms step_avg:100.27ms
step:451/1770 train_time:44219ms step_avg:100.27ms
step:452/1770 train_time:44320ms step_avg:100.27ms
step:453/1770 train_time:44422ms step_avg:100.28ms
step:454/1770 train_time:44525ms step_avg:100.28ms
step:455/1770 train_time:44627ms step_avg:100.29ms
step:456/1770 train_time:44729ms step_avg:100.29ms
step:457/1770 train_time:44832ms step_avg:100.29ms
step:458/1770 train_time:44934ms step_avg:100.30ms
step:459/1770 train_time:45036ms step_avg:100.30ms
step:460/1770 train_time:45138ms step_avg:100.31ms
step:461/1770 train_time:45240ms step_avg:100.31ms
step:462/1770 train_time:45342ms step_avg:100.31ms
step:463/1770 train_time:45444ms step_avg:100.32ms
step:464/1770 train_time:45546ms step_avg:100.32ms
step:465/1770 train_time:45647ms step_avg:100.32ms
step:466/1770 train_time:45750ms step_avg:100.33ms
step:467/1770 train_time:45853ms step_avg:100.33ms
step:468/1770 train_time:45955ms step_avg:100.34ms
step:469/1770 train_time:46057ms step_avg:100.34ms
step:470/1770 train_time:46159ms step_avg:100.35ms
step:471/1770 train_time:46260ms step_avg:100.35ms
step:472/1770 train_time:46363ms step_avg:100.35ms
step:473/1770 train_time:46465ms step_avg:100.36ms
step:474/1770 train_time:46567ms step_avg:100.36ms
step:475/1770 train_time:46670ms step_avg:100.36ms
step:476/1770 train_time:46772ms step_avg:100.37ms
step:477/1770 train_time:46875ms step_avg:100.37ms
step:478/1770 train_time:46978ms step_avg:100.38ms
step:479/1770 train_time:47079ms step_avg:100.38ms
step:480/1770 train_time:47180ms step_avg:100.38ms
step:481/1770 train_time:47282ms step_avg:100.39ms
step:482/1770 train_time:47384ms step_avg:100.39ms
step:483/1770 train_time:47486ms step_avg:100.39ms
step:484/1770 train_time:47589ms step_avg:100.40ms
step:485/1770 train_time:47691ms step_avg:100.40ms
step:486/1770 train_time:47794ms step_avg:100.41ms
step:487/1770 train_time:47896ms step_avg:100.41ms
step:488/1770 train_time:47998ms step_avg:100.41ms
step:489/1770 train_time:48101ms step_avg:100.42ms
step:490/1770 train_time:48202ms step_avg:100.42ms
step:491/1770 train_time:48304ms step_avg:100.42ms
step:492/1770 train_time:48407ms step_avg:100.43ms
step:493/1770 train_time:48510ms step_avg:100.43ms
step:494/1770 train_time:48612ms step_avg:100.44ms
step:495/1770 train_time:48715ms step_avg:100.44ms
step:496/1770 train_time:48817ms step_avg:100.45ms
step:497/1770 train_time:48919ms step_avg:100.45ms
step:498/1770 train_time:49020ms step_avg:100.45ms
step:499/1770 train_time:49123ms step_avg:100.46ms
step:500/1770 train_time:49225ms step_avg:100.46ms
step:500/1770 val_loss:3.7484 train_time:49326ms step_avg:100.67ms
step:501/1770 train_time:49344ms step_avg:100.50ms
step:502/1770 train_time:49444ms step_avg:100.50ms
step:503/1770 train_time:49550ms step_avg:100.51ms
step:504/1770 train_time:49654ms step_avg:100.51ms
step:505/1770 train_time:49756ms step_avg:100.52ms
step:506/1770 train_time:49859ms step_avg:100.52ms
step:507/1770 train_time:49961ms step_avg:100.53ms
step:508/1770 train_time:50064ms step_avg:100.53ms
step:509/1770 train_time:50167ms step_avg:100.54ms
step:510/1770 train_time:50269ms step_avg:100.54ms
step:511/1770 train_time:50371ms step_avg:100.54ms
step:512/1770 train_time:50473ms step_avg:100.54ms
step:513/1770 train_time:50575ms step_avg:100.55ms
step:514/1770 train_time:50678ms step_avg:100.55ms
step:515/1770 train_time:50781ms step_avg:100.56ms
step:516/1770 train_time:50883ms step_avg:100.56ms
step:517/1770 train_time:50985ms step_avg:100.56ms
step:518/1770 train_time:51086ms step_avg:100.56ms
step:519/1770 train_time:51188ms step_avg:100.57ms
step:520/1770 train_time:51290ms step_avg:100.57ms
step:521/1770 train_time:51392ms step_avg:100.57ms
step:522/1770 train_time:51494ms step_avg:100.58ms
step:523/1770 train_time:51597ms step_avg:100.58ms
step:524/1770 train_time:51700ms step_avg:100.58ms
step:525/1770 train_time:51803ms step_avg:100.59ms
step:526/1770 train_time:51905ms step_avg:100.59ms
step:527/1770 train_time:52008ms step_avg:100.60ms
step:528/1770 train_time:52110ms step_avg:100.60ms
step:529/1770 train_time:52212ms step_avg:100.60ms
step:530/1770 train_time:52315ms step_avg:100.61ms
step:531/1770 train_time:52418ms step_avg:100.61ms
step:532/1770 train_time:52521ms step_avg:100.61ms
step:533/1770 train_time:52623ms step_avg:100.62ms
step:534/1770 train_time:52726ms step_avg:100.62ms
step:535/1770 train_time:52828ms step_avg:100.63ms
step:536/1770 train_time:52931ms step_avg:100.63ms
step:537/1770 train_time:53033ms step_avg:100.63ms
step:538/1770 train_time:53136ms step_avg:100.64ms
step:539/1770 train_time:53239ms step_avg:100.64ms
step:540/1770 train_time:53341ms step_avg:100.64ms
step:541/1770 train_time:53444ms step_avg:100.65ms
step:542/1770 train_time:53547ms step_avg:100.65ms
step:543/1770 train_time:53649ms step_avg:100.65ms
step:544/1770 train_time:53751ms step_avg:100.66ms
step:545/1770 train_time:53853ms step_avg:100.66ms
step:546/1770 train_time:53961ms step_avg:100.67ms
step:547/1770 train_time:54059ms step_avg:100.67ms
step:548/1770 train_time:54162ms step_avg:100.67ms
step:549/1770 train_time:54264ms step_avg:100.68ms
step:550/1770 train_time:54367ms step_avg:100.68ms
step:551/1770 train_time:54469ms step_avg:100.68ms
step:552/1770 train_time:54572ms step_avg:100.69ms
step:553/1770 train_time:54675ms step_avg:100.69ms
step:554/1770 train_time:54777ms step_avg:100.69ms
step:555/1770 train_time:54880ms step_avg:100.70ms
step:556/1770 train_time:54983ms step_avg:100.70ms
step:557/1770 train_time:55085ms step_avg:100.70ms
step:558/1770 train_time:55187ms step_avg:100.71ms
step:559/1770 train_time:55289ms step_avg:100.71ms
step:560/1770 train_time:55392ms step_avg:100.71ms
step:561/1770 train_time:55494ms step_avg:100.72ms
step:562/1770 train_time:55597ms step_avg:100.72ms
step:563/1770 train_time:55700ms step_avg:100.72ms
step:564/1770 train_time:55803ms step_avg:100.73ms
step:565/1770 train_time:55905ms step_avg:100.73ms
step:566/1770 train_time:56008ms step_avg:100.73ms
step:567/1770 train_time:56110ms step_avg:100.74ms
step:568/1770 train_time:56213ms step_avg:100.74ms
step:569/1770 train_time:56316ms step_avg:100.74ms
step:570/1770 train_time:56419ms step_avg:100.75ms
step:571/1770 train_time:56522ms step_avg:100.75ms
step:572/1770 train_time:56624ms step_avg:100.76ms
step:573/1770 train_time:56727ms step_avg:100.76ms
step:574/1770 train_time:56829ms step_avg:100.76ms
step:575/1770 train_time:56931ms step_avg:100.76ms
step:576/1770 train_time:57033ms step_avg:100.77ms
step:577/1770 train_time:57136ms step_avg:100.77ms
step:578/1770 train_time:57239ms step_avg:100.77ms
step:579/1770 train_time:57342ms step_avg:100.78ms
step:580/1770 train_time:57444ms step_avg:100.78ms
step:581/1770 train_time:57547ms step_avg:100.78ms
step:582/1770 train_time:57650ms step_avg:100.79ms
step:583/1770 train_time:57755ms step_avg:100.79ms
step:584/1770 train_time:57855ms step_avg:100.79ms
step:585/1770 train_time:57958ms step_avg:100.80ms
step:586/1770 train_time:58060ms step_avg:100.80ms
step:587/1770 train_time:58164ms step_avg:100.80ms
step:588/1770 train_time:58266ms step_avg:100.81ms
step:589/1770 train_time:58368ms step_avg:100.81ms
step:590/1770 train_time:58471ms step_avg:100.81ms
step:591/1770 train_time:58573ms step_avg:100.81ms
step:592/1770 train_time:58676ms step_avg:100.82ms
step:593/1770 train_time:58779ms step_avg:100.82ms
step:594/1770 train_time:58881ms step_avg:100.82ms
step:595/1770 train_time:58984ms step_avg:100.83ms
step:596/1770 train_time:59086ms step_avg:100.83ms
step:597/1770 train_time:59188ms step_avg:100.83ms
step:598/1770 train_time:59290ms step_avg:100.83ms
step:599/1770 train_time:59392ms step_avg:100.84ms
step:600/1770 train_time:59494ms step_avg:100.84ms
step:601/1770 train_time:59598ms step_avg:100.84ms
step:602/1770 train_time:59700ms step_avg:100.85ms
step:603/1770 train_time:59803ms step_avg:100.85ms
step:604/1770 train_time:59906ms step_avg:100.85ms
step:605/1770 train_time:60008ms step_avg:100.85ms
step:606/1770 train_time:60111ms step_avg:100.86ms
step:607/1770 train_time:60213ms step_avg:100.86ms
step:608/1770 train_time:60316ms step_avg:100.86ms
step:609/1770 train_time:60419ms step_avg:100.87ms
step:610/1770 train_time:60522ms step_avg:100.87ms
step:611/1770 train_time:60625ms step_avg:100.87ms
step:612/1770 train_time:60727ms step_avg:100.88ms
step:613/1770 train_time:60830ms step_avg:100.88ms
step:614/1770 train_time:60931ms step_avg:100.88ms
step:615/1770 train_time:61034ms step_avg:100.88ms
step:616/1770 train_time:61137ms step_avg:100.89ms
step:617/1770 train_time:61240ms step_avg:100.89ms
step:618/1770 train_time:61343ms step_avg:100.89ms
step:619/1770 train_time:61445ms step_avg:100.90ms
step:620/1770 train_time:61548ms step_avg:100.90ms
step:621/1770 train_time:61651ms step_avg:100.90ms
step:622/1770 train_time:61753ms step_avg:100.90ms
step:623/1770 train_time:61855ms step_avg:100.91ms
step:624/1770 train_time:61958ms step_avg:100.91ms
step:625/1770 train_time:62060ms step_avg:100.91ms
step:625/1770 val_loss:3.6602 train_time:62162ms step_avg:101.08ms
step:626/1770 train_time:62182ms step_avg:100.95ms
step:627/1770 train_time:62277ms step_avg:100.94ms
step:628/1770 train_time:62382ms step_avg:100.94ms
step:629/1770 train_time:62485ms step_avg:100.95ms
step:630/1770 train_time:62589ms step_avg:100.95ms
step:631/1770 train_time:62691ms step_avg:100.95ms
step:632/1770 train_time:62793ms step_avg:100.95ms
step:633/1770 train_time:62896ms step_avg:100.96ms
step:634/1770 train_time:62998ms step_avg:100.96ms
step:635/1770 train_time:63101ms step_avg:100.96ms
step:636/1770 train_time:63204ms step_avg:100.97ms
step:637/1770 train_time:63307ms step_avg:100.97ms
step:638/1770 train_time:63410ms step_avg:100.97ms
step:639/1770 train_time:63512ms step_avg:100.97ms
step:640/1770 train_time:63614ms step_avg:100.97ms
step:641/1770 train_time:63716ms step_avg:100.98ms
step:642/1770 train_time:63819ms step_avg:100.98ms
step:643/1770 train_time:63922ms step_avg:100.98ms
step:644/1770 train_time:64025ms step_avg:100.99ms
step:645/1770 train_time:64128ms step_avg:100.99ms
step:646/1770 train_time:64231ms step_avg:100.99ms
step:647/1770 train_time:64334ms step_avg:101.00ms
step:648/1770 train_time:64437ms step_avg:101.00ms
step:649/1770 train_time:64540ms step_avg:101.00ms
step:650/1770 train_time:64643ms step_avg:101.00ms
step:651/1770 train_time:64745ms step_avg:101.01ms
step:652/1770 train_time:64848ms step_avg:101.01ms
step:653/1770 train_time:64950ms step_avg:101.01ms
step:654/1770 train_time:65053ms step_avg:101.01ms
step:655/1770 train_time:65155ms step_avg:101.02ms
step:656/1770 train_time:65258ms step_avg:101.02ms
step:657/1770 train_time:65361ms step_avg:101.02ms
step:658/1770 train_time:65465ms step_avg:101.03ms
step:659/1770 train_time:65569ms step_avg:101.03ms
step:660/1770 train_time:65673ms step_avg:101.04ms
step:661/1770 train_time:65777ms step_avg:101.04ms
step:662/1770 train_time:65883ms step_avg:101.05ms
step:663/1770 train_time:65989ms step_avg:101.05ms
step:664/1770 train_time:66092ms step_avg:101.06ms
step:665/1770 train_time:66196ms step_avg:101.06ms
step:666/1770 train_time:66301ms step_avg:101.07ms
step:667/1770 train_time:66405ms step_avg:101.07ms
step:668/1770 train_time:66510ms step_avg:101.08ms
step:669/1770 train_time:66614ms step_avg:101.08ms
step:670/1770 train_time:66717ms step_avg:101.09ms
step:671/1770 train_time:66822ms step_avg:101.09ms
step:672/1770 train_time:66927ms step_avg:101.10ms
step:673/1770 train_time:67031ms step_avg:101.10ms
step:674/1770 train_time:67135ms step_avg:101.11ms
step:675/1770 train_time:67239ms step_avg:101.11ms
step:676/1770 train_time:67343ms step_avg:101.12ms
step:677/1770 train_time:67448ms step_avg:101.12ms
step:678/1770 train_time:67552ms step_avg:101.13ms
step:679/1770 train_time:67656ms step_avg:101.13ms
step:680/1770 train_time:67760ms step_avg:101.13ms
step:681/1770 train_time:67864ms step_avg:101.14ms
step:682/1770 train_time:67969ms step_avg:101.14ms
step:683/1770 train_time:68073ms step_avg:101.15ms
step:684/1770 train_time:68177ms step_avg:101.15ms
step:685/1770 train_time:68282ms step_avg:101.16ms
step:686/1770 train_time:68386ms step_avg:101.16ms
step:687/1770 train_time:68491ms step_avg:101.17ms
step:688/1770 train_time:68594ms step_avg:101.17ms
step:689/1770 train_time:68699ms step_avg:101.18ms
step:690/1770 train_time:68803ms step_avg:101.18ms
step:691/1770 train_time:68908ms step_avg:101.19ms
step:692/1770 train_time:69012ms step_avg:101.19ms
step:693/1770 train_time:69116ms step_avg:101.19ms
step:694/1770 train_time:69221ms step_avg:101.20ms
step:695/1770 train_time:69325ms step_avg:101.20ms
step:696/1770 train_time:69429ms step_avg:101.21ms
step:697/1770 train_time:69533ms step_avg:101.21ms
step:698/1770 train_time:69636ms step_avg:101.22ms
step:699/1770 train_time:69741ms step_avg:101.22ms
step:700/1770 train_time:69845ms step_avg:101.22ms
step:701/1770 train_time:69950ms step_avg:101.23ms
step:702/1770 train_time:70054ms step_avg:101.23ms
step:703/1770 train_time:70157ms step_avg:101.24ms
step:704/1770 train_time:70262ms step_avg:101.24ms
step:705/1770 train_time:70367ms step_avg:101.25ms
step:706/1770 train_time:70471ms step_avg:101.25ms
step:707/1770 train_time:70574ms step_avg:101.25ms
step:708/1770 train_time:70678ms step_avg:101.26ms
step:709/1770 train_time:70783ms step_avg:101.26ms
step:710/1770 train_time:70888ms step_avg:101.27ms
step:711/1770 train_time:70992ms step_avg:101.27ms
step:712/1770 train_time:71096ms step_avg:101.28ms
step:713/1770 train_time:71201ms step_avg:101.28ms
step:714/1770 train_time:71305ms step_avg:101.29ms
step:715/1770 train_time:71410ms step_avg:101.29ms
step:716/1770 train_time:71514ms step_avg:101.29ms
step:717/1770 train_time:71617ms step_avg:101.30ms
step:718/1770 train_time:71722ms step_avg:101.30ms
step:719/1770 train_time:71827ms step_avg:101.31ms
step:720/1770 train_time:71931ms step_avg:101.31ms
step:721/1770 train_time:72036ms step_avg:101.32ms
step:722/1770 train_time:72139ms step_avg:101.32ms
step:723/1770 train_time:72244ms step_avg:101.32ms
step:724/1770 train_time:72349ms step_avg:101.33ms
step:725/1770 train_time:72452ms step_avg:101.33ms
step:726/1770 train_time:72556ms step_avg:101.34ms
step:727/1770 train_time:72661ms step_avg:101.34ms
step:728/1770 train_time:72765ms step_avg:101.34ms
step:729/1770 train_time:72869ms step_avg:101.35ms
step:730/1770 train_time:72973ms step_avg:101.35ms
step:731/1770 train_time:73076ms step_avg:101.35ms
step:732/1770 train_time:73181ms step_avg:101.36ms
step:733/1770 train_time:73286ms step_avg:101.36ms
step:734/1770 train_time:73390ms step_avg:101.37ms
step:735/1770 train_time:73494ms step_avg:101.37ms
step:736/1770 train_time:73598ms step_avg:101.37ms
step:737/1770 train_time:73703ms step_avg:101.38ms
step:738/1770 train_time:73808ms step_avg:101.38ms
step:739/1770 train_time:73911ms step_avg:101.39ms
step:740/1770 train_time:74015ms step_avg:101.39ms
step:741/1770 train_time:74119ms step_avg:101.39ms
step:742/1770 train_time:74224ms step_avg:101.40ms
step:743/1770 train_time:74328ms step_avg:101.40ms
step:744/1770 train_time:74432ms step_avg:101.41ms
step:745/1770 train_time:74536ms step_avg:101.41ms
step:746/1770 train_time:74640ms step_avg:101.41ms
step:747/1770 train_time:74745ms step_avg:101.42ms
step:748/1770 train_time:74853ms step_avg:101.43ms
step:749/1770 train_time:74953ms step_avg:101.42ms
step:750/1770 train_time:75056ms step_avg:101.43ms
step:750/1770 val_loss:3.5986 train_time:75160ms step_avg:101.57ms
step:751/1770 train_time:75178ms step_avg:101.45ms
step:752/1770 train_time:75277ms step_avg:101.45ms
step:753/1770 train_time:75383ms step_avg:101.46ms
step:754/1770 train_time:75487ms step_avg:101.46ms
step:755/1770 train_time:75592ms step_avg:101.47ms
step:756/1770 train_time:75697ms step_avg:101.47ms
step:757/1770 train_time:75800ms step_avg:101.47ms
step:758/1770 train_time:75905ms step_avg:101.48ms
step:759/1770 train_time:76009ms step_avg:101.48ms
step:760/1770 train_time:76113ms step_avg:101.48ms
step:761/1770 train_time:76217ms step_avg:101.49ms
step:762/1770 train_time:76322ms step_avg:101.49ms
step:763/1770 train_time:76426ms step_avg:101.50ms
step:764/1770 train_time:76530ms step_avg:101.50ms
step:765/1770 train_time:76635ms step_avg:101.50ms
step:766/1770 train_time:76739ms step_avg:101.51ms
step:767/1770 train_time:76843ms step_avg:101.51ms
step:768/1770 train_time:76947ms step_avg:101.51ms
step:769/1770 train_time:77052ms step_avg:101.52ms
step:770/1770 train_time:77156ms step_avg:101.52ms
step:771/1770 train_time:77261ms step_avg:101.53ms
step:772/1770 train_time:77365ms step_avg:101.53ms
step:773/1770 train_time:77469ms step_avg:101.53ms
step:774/1770 train_time:77574ms step_avg:101.54ms
step:775/1770 train_time:77679ms step_avg:101.54ms
step:776/1770 train_time:77783ms step_avg:101.54ms
step:777/1770 train_time:77887ms step_avg:101.55ms
step:778/1770 train_time:77992ms step_avg:101.55ms
step:779/1770 train_time:78096ms step_avg:101.55ms
step:780/1770 train_time:78200ms step_avg:101.56ms
step:781/1770 train_time:78304ms step_avg:101.56ms
step:782/1770 train_time:78408ms step_avg:101.56ms
step:783/1770 train_time:78512ms step_avg:101.57ms
step:784/1770 train_time:78616ms step_avg:101.57ms
step:785/1770 train_time:78720ms step_avg:101.57ms
step:786/1770 train_time:78824ms step_avg:101.58ms
step:787/1770 train_time:78928ms step_avg:101.58ms
step:788/1770 train_time:79033ms step_avg:101.58ms
step:789/1770 train_time:79137ms step_avg:101.59ms
step:790/1770 train_time:79241ms step_avg:101.59ms
step:791/1770 train_time:79345ms step_avg:101.59ms
step:792/1770 train_time:79450ms step_avg:101.60ms
step:793/1770 train_time:79555ms step_avg:101.60ms
step:794/1770 train_time:79661ms step_avg:101.61ms
step:795/1770 train_time:79765ms step_avg:101.61ms
step:796/1770 train_time:79869ms step_avg:101.62ms
step:797/1770 train_time:79975ms step_avg:101.62ms
step:798/1770 train_time:80080ms step_avg:101.62ms
step:799/1770 train_time:80184ms step_avg:101.63ms
step:800/1770 train_time:80288ms step_avg:101.63ms
step:801/1770 train_time:80393ms step_avg:101.64ms
step:802/1770 train_time:80498ms step_avg:101.64ms
step:803/1770 train_time:80602ms step_avg:101.64ms
step:804/1770 train_time:80707ms step_avg:101.65ms
step:805/1770 train_time:80812ms step_avg:101.65ms
step:806/1770 train_time:80917ms step_avg:101.65ms
step:807/1770 train_time:81021ms step_avg:101.66ms
step:808/1770 train_time:81125ms step_avg:101.66ms
step:809/1770 train_time:81229ms step_avg:101.66ms
step:810/1770 train_time:81334ms step_avg:101.67ms
step:811/1770 train_time:81438ms step_avg:101.67ms
step:812/1770 train_time:81543ms step_avg:101.67ms
step:813/1770 train_time:81647ms step_avg:101.68ms
step:814/1770 train_time:81751ms step_avg:101.68ms
step:815/1770 train_time:81856ms step_avg:101.68ms
step:816/1770 train_time:81960ms step_avg:101.69ms
step:817/1770 train_time:82064ms step_avg:101.69ms
step:818/1770 train_time:82169ms step_avg:101.69ms
step:819/1770 train_time:82274ms step_avg:101.70ms
step:820/1770 train_time:82379ms step_avg:101.70ms
step:821/1770 train_time:82483ms step_avg:101.71ms
step:822/1770 train_time:82587ms step_avg:101.71ms
step:823/1770 train_time:82692ms step_avg:101.71ms
step:824/1770 train_time:82797ms step_avg:101.72ms
step:825/1770 train_time:82902ms step_avg:101.72ms
step:826/1770 train_time:83006ms step_avg:101.72ms
step:827/1770 train_time:83111ms step_avg:101.73ms
step:828/1770 train_time:83216ms step_avg:101.73ms
step:829/1770 train_time:83321ms step_avg:101.74ms
step:830/1770 train_time:83425ms step_avg:101.74ms
step:831/1770 train_time:83530ms step_avg:101.74ms
step:832/1770 train_time:83635ms step_avg:101.75ms
step:833/1770 train_time:83740ms step_avg:101.75ms
step:834/1770 train_time:83844ms step_avg:101.75ms
step:835/1770 train_time:83948ms step_avg:101.75ms
step:836/1770 train_time:84053ms step_avg:101.76ms
step:837/1770 train_time:84158ms step_avg:101.76ms
step:838/1770 train_time:84262ms step_avg:101.77ms
step:839/1770 train_time:84366ms step_avg:101.77ms
step:840/1770 train_time:84471ms step_avg:101.77ms
step:841/1770 train_time:84575ms step_avg:101.78ms
step:842/1770 train_time:84681ms step_avg:101.78ms
step:843/1770 train_time:84785ms step_avg:101.78ms
step:844/1770 train_time:84889ms step_avg:101.79ms
step:845/1770 train_time:84994ms step_avg:101.79ms
step:846/1770 train_time:85099ms step_avg:101.79ms
step:847/1770 train_time:85203ms step_avg:101.80ms
step:848/1770 train_time:85306ms step_avg:101.80ms
step:849/1770 train_time:85412ms step_avg:101.80ms
step:850/1770 train_time:85516ms step_avg:101.80ms
step:851/1770 train_time:85621ms step_avg:101.81ms
step:852/1770 train_time:85725ms step_avg:101.81ms
step:853/1770 train_time:85829ms step_avg:101.81ms
step:854/1770 train_time:85934ms step_avg:101.82ms
step:855/1770 train_time:86038ms step_avg:101.82ms
step:856/1770 train_time:86142ms step_avg:101.82ms
step:857/1770 train_time:86246ms step_avg:101.83ms
step:858/1770 train_time:86351ms step_avg:101.83ms
step:859/1770 train_time:86456ms step_avg:101.83ms
step:860/1770 train_time:86561ms step_avg:101.84ms
step:861/1770 train_time:86665ms step_avg:101.84ms
step:862/1770 train_time:86769ms step_avg:101.84ms
step:863/1770 train_time:86875ms step_avg:101.85ms
step:864/1770 train_time:86979ms step_avg:101.85ms
step:865/1770 train_time:87084ms step_avg:101.85ms
step:866/1770 train_time:87188ms step_avg:101.86ms
step:867/1770 train_time:87293ms step_avg:101.86ms
step:868/1770 train_time:87398ms step_avg:101.86ms
step:869/1770 train_time:87503ms step_avg:101.87ms
step:870/1770 train_time:87607ms step_avg:101.87ms
step:871/1770 train_time:87712ms step_avg:101.87ms
step:872/1770 train_time:87817ms step_avg:101.88ms
step:873/1770 train_time:87921ms step_avg:101.88ms
step:874/1770 train_time:88025ms step_avg:101.88ms
step:875/1770 train_time:88130ms step_avg:101.88ms
step:875/1770 val_loss:3.5485 train_time:88233ms step_avg:102.00ms
step:876/1770 train_time:88251ms step_avg:101.91ms
step:877/1770 train_time:88353ms step_avg:101.91ms
step:878/1770 train_time:88461ms step_avg:101.91ms
step:879/1770 train_time:88566ms step_avg:101.92ms
step:880/1770 train_time:88670ms step_avg:101.92ms
step:881/1770 train_time:88775ms step_avg:101.92ms
step:882/1770 train_time:88880ms step_avg:101.93ms
step:883/1770 train_time:88984ms step_avg:101.93ms
step:884/1770 train_time:89089ms step_avg:101.93ms
step:885/1770 train_time:89193ms step_avg:101.93ms
step:886/1770 train_time:89298ms step_avg:101.94ms
step:887/1770 train_time:89403ms step_avg:101.94ms
step:888/1770 train_time:89508ms step_avg:101.95ms
step:889/1770 train_time:89612ms step_avg:101.95ms
step:890/1770 train_time:89717ms step_avg:101.95ms
step:891/1770 train_time:89822ms step_avg:101.95ms
step:892/1770 train_time:89927ms step_avg:101.96ms
step:893/1770 train_time:90031ms step_avg:101.96ms
step:894/1770 train_time:90136ms step_avg:101.96ms
step:895/1770 train_time:90241ms step_avg:101.97ms
step:896/1770 train_time:90346ms step_avg:101.97ms
step:897/1770 train_time:90450ms step_avg:101.97ms
step:898/1770 train_time:90555ms step_avg:101.98ms
step:899/1770 train_time:90659ms step_avg:101.98ms
step:900/1770 train_time:90764ms step_avg:101.98ms
step:901/1770 train_time:90868ms step_avg:101.98ms
step:902/1770 train_time:90974ms step_avg:101.99ms
step:903/1770 train_time:91078ms step_avg:101.99ms
step:904/1770 train_time:91184ms step_avg:101.99ms
step:905/1770 train_time:91288ms step_avg:102.00ms
step:906/1770 train_time:91392ms step_avg:102.00ms
step:907/1770 train_time:91497ms step_avg:102.00ms
step:908/1770 train_time:91602ms step_avg:102.01ms
step:909/1770 train_time:91706ms step_avg:102.01ms
step:910/1770 train_time:91810ms step_avg:102.01ms
step:911/1770 train_time:91915ms step_avg:102.01ms
step:912/1770 train_time:92020ms step_avg:102.02ms
step:913/1770 train_time:92124ms step_avg:102.02ms
step:914/1770 train_time:92229ms step_avg:102.02ms
step:915/1770 train_time:92333ms step_avg:102.03ms
step:916/1770 train_time:92438ms step_avg:102.03ms
step:917/1770 train_time:92543ms step_avg:102.03ms
step:918/1770 train_time:92647ms step_avg:102.03ms
step:919/1770 train_time:92751ms step_avg:102.04ms
step:920/1770 train_time:92857ms step_avg:102.04ms
step:921/1770 train_time:92964ms step_avg:102.05ms
step:922/1770 train_time:93070ms step_avg:102.05ms
step:923/1770 train_time:93176ms step_avg:102.05ms
step:924/1770 train_time:93282ms step_avg:102.06ms
step:925/1770 train_time:93388ms step_avg:102.06ms
step:926/1770 train_time:93494ms step_avg:102.07ms
step:927/1770 train_time:93600ms step_avg:102.07ms
step:928/1770 train_time:93706ms step_avg:102.08ms
step:929/1770 train_time:93811ms step_avg:102.08ms
step:930/1770 train_time:93917ms step_avg:102.08ms
step:931/1770 train_time:94023ms step_avg:102.09ms
step:932/1770 train_time:94129ms step_avg:102.09ms
step:933/1770 train_time:94234ms step_avg:102.10ms
step:934/1770 train_time:94341ms step_avg:102.10ms
step:935/1770 train_time:94448ms step_avg:102.11ms
step:936/1770 train_time:94553ms step_avg:102.11ms
step:937/1770 train_time:94658ms step_avg:102.11ms
step:938/1770 train_time:94765ms step_avg:102.12ms
step:939/1770 train_time:94870ms step_avg:102.12ms
step:940/1770 train_time:94976ms step_avg:102.13ms
step:941/1770 train_time:95083ms step_avg:102.13ms
step:942/1770 train_time:95189ms step_avg:102.13ms
step:943/1770 train_time:95295ms step_avg:102.14ms
step:944/1770 train_time:95400ms step_avg:102.14ms
step:945/1770 train_time:95506ms step_avg:102.15ms
step:946/1770 train_time:95612ms step_avg:102.15ms
step:947/1770 train_time:95719ms step_avg:102.15ms
step:948/1770 train_time:95825ms step_avg:102.16ms
step:949/1770 train_time:95931ms step_avg:102.16ms
step:950/1770 train_time:96038ms step_avg:102.17ms
step:951/1770 train_time:96144ms step_avg:102.17ms
step:952/1770 train_time:96250ms step_avg:102.18ms
step:953/1770 train_time:96356ms step_avg:102.18ms
step:954/1770 train_time:96462ms step_avg:102.18ms
step:955/1770 train_time:96568ms step_avg:102.19ms
step:956/1770 train_time:96673ms step_avg:102.19ms
step:957/1770 train_time:96779ms step_avg:102.20ms
step:958/1770 train_time:96885ms step_avg:102.20ms
step:959/1770 train_time:96991ms step_avg:102.20ms
step:960/1770 train_time:97097ms step_avg:102.21ms
step:961/1770 train_time:97202ms step_avg:102.21ms
step:962/1770 train_time:97309ms step_avg:102.22ms
step:963/1770 train_time:97415ms step_avg:102.22ms
step:964/1770 train_time:97521ms step_avg:102.22ms
step:965/1770 train_time:97627ms step_avg:102.23ms
step:966/1770 train_time:97733ms step_avg:102.23ms
step:967/1770 train_time:97839ms step_avg:102.24ms
step:968/1770 train_time:97946ms step_avg:102.24ms
step:969/1770 train_time:98051ms step_avg:102.24ms
step:970/1770 train_time:98157ms step_avg:102.25ms
step:971/1770 train_time:98264ms step_avg:102.25ms
step:972/1770 train_time:98370ms step_avg:102.26ms
step:973/1770 train_time:98476ms step_avg:102.26ms
step:974/1770 train_time:98582ms step_avg:102.26ms
step:975/1770 train_time:98688ms step_avg:102.27ms
step:976/1770 train_time:98794ms step_avg:102.27ms
step:977/1770 train_time:98901ms step_avg:102.28ms
step:978/1770 train_time:99007ms step_avg:102.28ms
step:979/1770 train_time:99113ms step_avg:102.28ms
step:980/1770 train_time:99218ms step_avg:102.29ms
step:981/1770 train_time:99324ms step_avg:102.29ms
step:982/1770 train_time:99430ms step_avg:102.29ms
step:983/1770 train_time:99536ms step_avg:102.30ms
step:984/1770 train_time:99642ms step_avg:102.30ms
step:985/1770 train_time:99748ms step_avg:102.31ms
step:986/1770 train_time:99853ms step_avg:102.31ms
step:987/1770 train_time:99959ms step_avg:102.31ms
step:988/1770 train_time:100065ms step_avg:102.32ms
step:989/1770 train_time:100172ms step_avg:102.32ms
step:990/1770 train_time:100277ms step_avg:102.32ms
step:991/1770 train_time:100383ms step_avg:102.33ms
step:992/1770 train_time:100489ms step_avg:102.33ms
step:993/1770 train_time:100595ms step_avg:102.33ms
step:994/1770 train_time:100702ms step_avg:102.34ms
step:995/1770 train_time:100807ms step_avg:102.34ms
step:996/1770 train_time:100912ms step_avg:102.34ms
step:997/1770 train_time:101018ms step_avg:102.35ms
step:998/1770 train_time:101124ms step_avg:102.35ms
step:999/1770 train_time:101230ms step_avg:102.36ms
step:1000/1770 train_time:101336ms step_avg:102.36ms
step:1000/1770 val_loss:3.5109 train_time:101441ms step_avg:102.47ms
step:1001/1770 train_time:101459ms step_avg:102.38ms
step:1002/1770 train_time:101563ms step_avg:102.38ms
step:1003/1770 train_time:101673ms step_avg:102.39ms
step:1004/1770 train_time:101780ms step_avg:102.39ms
step:1005/1770 train_time:101887ms step_avg:102.40ms
step:1006/1770 train_time:101992ms step_avg:102.40ms
step:1007/1770 train_time:102099ms step_avg:102.41ms
step:1008/1770 train_time:102205ms step_avg:102.41ms
step:1009/1770 train_time:102310ms step_avg:102.41ms
step:1010/1770 train_time:102416ms step_avg:102.42ms
step:1011/1770 train_time:102524ms step_avg:102.42ms
step:1012/1770 train_time:102629ms step_avg:102.42ms
step:1013/1770 train_time:102735ms step_avg:102.43ms
step:1014/1770 train_time:102842ms step_avg:102.43ms
step:1015/1770 train_time:102947ms step_avg:102.43ms
step:1016/1770 train_time:103052ms step_avg:102.44ms
step:1017/1770 train_time:103159ms step_avg:102.44ms
step:1018/1770 train_time:103264ms step_avg:102.44ms
step:1019/1770 train_time:103369ms step_avg:102.45ms
step:1020/1770 train_time:103475ms step_avg:102.45ms
step:1021/1770 train_time:103581ms step_avg:102.45ms
step:1022/1770 train_time:103688ms step_avg:102.46ms
step:1023/1770 train_time:103794ms step_avg:102.46ms
step:1024/1770 train_time:103900ms step_avg:102.47ms
step:1025/1770 train_time:104006ms step_avg:102.47ms
step:1026/1770 train_time:104112ms step_avg:102.47ms
step:1027/1770 train_time:104219ms step_avg:102.48ms
step:1028/1770 train_time:104325ms step_avg:102.48ms
step:1029/1770 train_time:104432ms step_avg:102.48ms
step:1030/1770 train_time:104538ms step_avg:102.49ms
step:1031/1770 train_time:104644ms step_avg:102.49ms
step:1032/1770 train_time:104750ms step_avg:102.49ms
step:1033/1770 train_time:104856ms step_avg:102.50ms
step:1034/1770 train_time:104962ms step_avg:102.50ms
step:1035/1770 train_time:105067ms step_avg:102.50ms
step:1036/1770 train_time:105173ms step_avg:102.51ms
step:1037/1770 train_time:105279ms step_avg:102.51ms
step:1038/1770 train_time:105386ms step_avg:102.52ms
step:1039/1770 train_time:105491ms step_avg:102.52ms
step:1040/1770 train_time:105597ms step_avg:102.52ms
step:1041/1770 train_time:105702ms step_avg:102.52ms
step:1042/1770 train_time:105809ms step_avg:102.53ms
step:1043/1770 train_time:105915ms step_avg:102.53ms
step:1044/1770 train_time:106021ms step_avg:102.53ms
step:1045/1770 train_time:106128ms step_avg:102.54ms
step:1046/1770 train_time:106233ms step_avg:102.54ms
step:1047/1770 train_time:106339ms step_avg:102.55ms
step:1048/1770 train_time:106445ms step_avg:102.55ms
step:1049/1770 train_time:106552ms step_avg:102.55ms
step:1050/1770 train_time:106657ms step_avg:102.56ms
step:1051/1770 train_time:106764ms step_avg:102.56ms
step:1052/1770 train_time:106869ms step_avg:102.56ms
step:1053/1770 train_time:106976ms step_avg:102.57ms
step:1054/1770 train_time:107082ms step_avg:102.57ms
step:1055/1770 train_time:107188ms step_avg:102.57ms
step:1056/1770 train_time:107293ms step_avg:102.57ms
step:1057/1770 train_time:107400ms step_avg:102.58ms
step:1058/1770 train_time:107506ms step_avg:102.58ms
step:1059/1770 train_time:107612ms step_avg:102.58ms
step:1060/1770 train_time:107719ms step_avg:102.59ms
step:1061/1770 train_time:107825ms step_avg:102.59ms
step:1062/1770 train_time:107932ms step_avg:102.60ms
step:1063/1770 train_time:108040ms step_avg:102.60ms
step:1064/1770 train_time:108148ms step_avg:102.61ms
step:1065/1770 train_time:108253ms step_avg:102.61ms
step:1066/1770 train_time:108361ms step_avg:102.61ms
step:1067/1770 train_time:108466ms step_avg:102.62ms
step:1068/1770 train_time:108573ms step_avg:102.62ms
step:1069/1770 train_time:108679ms step_avg:102.62ms
step:1070/1770 train_time:108785ms step_avg:102.63ms
step:1071/1770 train_time:108893ms step_avg:102.63ms
step:1072/1770 train_time:108999ms step_avg:102.64ms
step:1073/1770 train_time:109105ms step_avg:102.64ms
step:1074/1770 train_time:109210ms step_avg:102.64ms
step:1075/1770 train_time:109317ms step_avg:102.64ms
step:1076/1770 train_time:109424ms step_avg:102.65ms
step:1077/1770 train_time:109530ms step_avg:102.65ms
step:1078/1770 train_time:109636ms step_avg:102.66ms
step:1079/1770 train_time:109742ms step_avg:102.66ms
step:1080/1770 train_time:109848ms step_avg:102.66ms
step:1081/1770 train_time:109954ms step_avg:102.66ms
step:1082/1770 train_time:110061ms step_avg:102.67ms
step:1083/1770 train_time:110167ms step_avg:102.67ms
step:1084/1770 train_time:110273ms step_avg:102.68ms
step:1085/1770 train_time:110380ms step_avg:102.68ms
step:1086/1770 train_time:110487ms step_avg:102.68ms
step:1087/1770 train_time:110593ms step_avg:102.69ms
step:1088/1770 train_time:110699ms step_avg:102.69ms
step:1089/1770 train_time:110806ms step_avg:102.69ms
step:1090/1770 train_time:110913ms step_avg:102.70ms
step:1091/1770 train_time:111019ms step_avg:102.70ms
step:1092/1770 train_time:111125ms step_avg:102.70ms
step:1093/1770 train_time:111231ms step_avg:102.71ms
step:1094/1770 train_time:111338ms step_avg:102.71ms
step:1095/1770 train_time:111445ms step_avg:102.71ms
step:1096/1770 train_time:111550ms step_avg:102.72ms
step:1097/1770 train_time:111657ms step_avg:102.72ms
step:1098/1770 train_time:111763ms step_avg:102.72ms
step:1099/1770 train_time:111869ms step_avg:102.73ms
step:1100/1770 train_time:111975ms step_avg:102.73ms
step:1101/1770 train_time:112081ms step_avg:102.73ms
step:1102/1770 train_time:112188ms step_avg:102.74ms
step:1103/1770 train_time:112294ms step_avg:102.74ms
step:1104/1770 train_time:112400ms step_avg:102.74ms
step:1105/1770 train_time:112506ms step_avg:102.75ms
step:1106/1770 train_time:112612ms step_avg:102.75ms
step:1107/1770 train_time:112719ms step_avg:102.75ms
step:1108/1770 train_time:112825ms step_avg:102.75ms
step:1109/1770 train_time:112931ms step_avg:102.76ms
step:1110/1770 train_time:113037ms step_avg:102.76ms
step:1111/1770 train_time:113143ms step_avg:102.76ms
step:1112/1770 train_time:113249ms step_avg:102.77ms
step:1113/1770 train_time:113355ms step_avg:102.77ms
step:1114/1770 train_time:113462ms step_avg:102.77ms
step:1115/1770 train_time:113568ms step_avg:102.78ms
step:1116/1770 train_time:113674ms step_avg:102.78ms
step:1117/1770 train_time:113781ms step_avg:102.78ms
step:1118/1770 train_time:113887ms step_avg:102.79ms
step:1119/1770 train_time:113993ms step_avg:102.79ms
step:1120/1770 train_time:114099ms step_avg:102.79ms
step:1121/1770 train_time:114206ms step_avg:102.80ms
step:1122/1770 train_time:114311ms step_avg:102.80ms
step:1123/1770 train_time:114417ms step_avg:102.80ms
step:1124/1770 train_time:114524ms step_avg:102.80ms
step:1125/1770 train_time:114630ms step_avg:102.81ms
step:1125/1770 val_loss:3.4689 train_time:114734ms step_avg:102.90ms
step:1126/1770 train_time:114752ms step_avg:102.82ms
step:1127/1770 train_time:114855ms step_avg:102.82ms
step:1128/1770 train_time:114963ms step_avg:102.83ms
step:1129/1770 train_time:115069ms step_avg:102.83ms
step:1130/1770 train_time:115175ms step_avg:102.84ms
step:1131/1770 train_time:115281ms step_avg:102.84ms
step:1132/1770 train_time:115388ms step_avg:102.84ms
step:1133/1770 train_time:115494ms step_avg:102.84ms
step:1134/1770 train_time:115601ms step_avg:102.85ms
step:1135/1770 train_time:115707ms step_avg:102.85ms
step:1136/1770 train_time:115813ms step_avg:102.85ms
step:1137/1770 train_time:115921ms step_avg:102.86ms
step:1138/1770 train_time:116026ms step_avg:102.86ms
step:1139/1770 train_time:116132ms step_avg:102.86ms
step:1140/1770 train_time:116239ms step_avg:102.87ms
step:1141/1770 train_time:116346ms step_avg:102.87ms
step:1142/1770 train_time:116452ms step_avg:102.87ms
step:1143/1770 train_time:116558ms step_avg:102.88ms
step:1144/1770 train_time:116665ms step_avg:102.88ms
step:1145/1770 train_time:116770ms step_avg:102.88ms
step:1146/1770 train_time:116876ms step_avg:102.88ms
step:1147/1770 train_time:116983ms step_avg:102.89ms
step:1148/1770 train_time:117089ms step_avg:102.89ms
step:1149/1770 train_time:117194ms step_avg:102.89ms
step:1150/1770 train_time:117300ms step_avg:102.90ms
step:1151/1770 train_time:117407ms step_avg:102.90ms
step:1152/1770 train_time:117515ms step_avg:102.90ms
step:1153/1770 train_time:117621ms step_avg:102.91ms
step:1154/1770 train_time:117727ms step_avg:102.91ms
step:1155/1770 train_time:117833ms step_avg:102.91ms
step:1156/1770 train_time:117939ms step_avg:102.91ms
step:1157/1770 train_time:118048ms step_avg:102.92ms
step:1158/1770 train_time:118154ms step_avg:102.92ms
step:1159/1770 train_time:118260ms step_avg:102.92ms
step:1160/1770 train_time:118367ms step_avg:102.93ms
step:1161/1770 train_time:118472ms step_avg:102.93ms
step:1162/1770 train_time:118581ms step_avg:102.93ms
step:1163/1770 train_time:118687ms step_avg:102.94ms
step:1164/1770 train_time:118793ms step_avg:102.94ms
step:1165/1770 train_time:118900ms step_avg:102.94ms
step:1166/1770 train_time:119007ms step_avg:102.95ms
step:1167/1770 train_time:119113ms step_avg:102.95ms
step:1168/1770 train_time:119220ms step_avg:102.95ms
step:1169/1770 train_time:119326ms step_avg:102.96ms
step:1170/1770 train_time:119432ms step_avg:102.96ms
step:1171/1770 train_time:119539ms step_avg:102.96ms
step:1172/1770 train_time:119645ms step_avg:102.96ms
step:1173/1770 train_time:119751ms step_avg:102.97ms
step:1174/1770 train_time:119858ms step_avg:102.97ms
step:1175/1770 train_time:119964ms step_avg:102.97ms
step:1176/1770 train_time:120070ms step_avg:102.98ms
step:1177/1770 train_time:120175ms step_avg:102.98ms
step:1178/1770 train_time:120282ms step_avg:102.98ms
step:1179/1770 train_time:120387ms step_avg:102.98ms
step:1180/1770 train_time:120494ms step_avg:102.99ms
step:1181/1770 train_time:120600ms step_avg:102.99ms
step:1182/1770 train_time:120707ms step_avg:102.99ms
step:1183/1770 train_time:120814ms step_avg:103.00ms
step:1184/1770 train_time:120923ms step_avg:103.00ms
step:1185/1770 train_time:121030ms step_avg:103.00ms
step:1186/1770 train_time:121139ms step_avg:103.01ms
step:1187/1770 train_time:121249ms step_avg:103.02ms
step:1188/1770 train_time:121356ms step_avg:103.02ms
step:1189/1770 train_time:121464ms step_avg:103.02ms
step:1190/1770 train_time:121570ms step_avg:103.03ms
step:1191/1770 train_time:121678ms step_avg:103.03ms
step:1192/1770 train_time:121785ms step_avg:103.03ms
step:1193/1770 train_time:121892ms step_avg:103.04ms
step:1194/1770 train_time:122000ms step_avg:103.04ms
step:1195/1770 train_time:122108ms step_avg:103.04ms
step:1196/1770 train_time:122216ms step_avg:103.05ms
step:1197/1770 train_time:122323ms step_avg:103.05ms
step:1198/1770 train_time:122429ms step_avg:103.05ms
step:1199/1770 train_time:122537ms step_avg:103.06ms
step:1200/1770 train_time:122644ms step_avg:103.06ms
step:1201/1770 train_time:122753ms step_avg:103.07ms
step:1202/1770 train_time:122860ms step_avg:103.07ms
step:1203/1770 train_time:122968ms step_avg:103.07ms
step:1204/1770 train_time:123076ms step_avg:103.08ms
step:1205/1770 train_time:123183ms step_avg:103.08ms
step:1206/1770 train_time:123290ms step_avg:103.09ms
step:1207/1770 train_time:123398ms step_avg:103.09ms
step:1208/1770 train_time:123507ms step_avg:103.09ms
step:1209/1770 train_time:123613ms step_avg:103.10ms
step:1210/1770 train_time:123719ms step_avg:103.10ms
step:1211/1770 train_time:123827ms step_avg:103.10ms
step:1212/1770 train_time:123937ms step_avg:103.11ms
step:1213/1770 train_time:124044ms step_avg:103.11ms
step:1214/1770 train_time:124151ms step_avg:103.12ms
step:1215/1770 train_time:124259ms step_avg:103.12ms
step:1216/1770 train_time:124368ms step_avg:103.12ms
step:1217/1770 train_time:124475ms step_avg:103.13ms
step:1218/1770 train_time:124583ms step_avg:103.13ms
step:1219/1770 train_time:124690ms step_avg:103.13ms
step:1220/1770 train_time:124798ms step_avg:103.14ms
step:1221/1770 train_time:124906ms step_avg:103.14ms
step:1222/1770 train_time:125014ms step_avg:103.15ms
step:1223/1770 train_time:125122ms step_avg:103.15ms
step:1224/1770 train_time:125230ms step_avg:103.15ms
step:1225/1770 train_time:125338ms step_avg:103.16ms
step:1226/1770 train_time:125445ms step_avg:103.16ms
step:1227/1770 train_time:125554ms step_avg:103.17ms
step:1228/1770 train_time:125664ms step_avg:103.17ms
step:1229/1770 train_time:125771ms step_avg:103.18ms
step:1230/1770 train_time:125879ms step_avg:103.18ms
step:1231/1770 train_time:125987ms step_avg:103.18ms
step:1232/1770 train_time:126094ms step_avg:103.19ms
step:1233/1770 train_time:126201ms step_avg:103.19ms
step:1234/1770 train_time:126309ms step_avg:103.19ms
step:1235/1770 train_time:126416ms step_avg:103.20ms
step:1236/1770 train_time:126523ms step_avg:103.20ms
step:1237/1770 train_time:126631ms step_avg:103.20ms
step:1238/1770 train_time:126739ms step_avg:103.21ms
step:1239/1770 train_time:126847ms step_avg:103.21ms
step:1240/1770 train_time:126954ms step_avg:103.21ms
step:1241/1770 train_time:127062ms step_avg:103.22ms
step:1242/1770 train_time:127170ms step_avg:103.22ms
step:1243/1770 train_time:127277ms step_avg:103.23ms
step:1244/1770 train_time:127385ms step_avg:103.23ms
step:1245/1770 train_time:127493ms step_avg:103.23ms
step:1246/1770 train_time:127601ms step_avg:103.24ms
step:1247/1770 train_time:127709ms step_avg:103.24ms
step:1248/1770 train_time:127818ms step_avg:103.25ms
step:1249/1770 train_time:127926ms step_avg:103.25ms
step:1250/1770 train_time:128032ms step_avg:103.25ms
step:1250/1770 val_loss:3.4220 train_time:128140ms step_avg:103.34ms
step:1251/1770 train_time:128159ms step_avg:103.27ms
step:1252/1770 train_time:128257ms step_avg:103.27ms
step:1253/1770 train_time:128366ms step_avg:103.27ms
step:1254/1770 train_time:128474ms step_avg:103.27ms
step:1255/1770 train_time:128585ms step_avg:103.28ms
step:1256/1770 train_time:128690ms step_avg:103.28ms
step:1257/1770 train_time:128797ms step_avg:103.29ms
step:1258/1770 train_time:128905ms step_avg:103.29ms
step:1259/1770 train_time:129013ms step_avg:103.29ms
step:1260/1770 train_time:129120ms step_avg:103.30ms
step:1261/1770 train_time:129229ms step_avg:103.30ms
step:1262/1770 train_time:129337ms step_avg:103.30ms
step:1263/1770 train_time:129445ms step_avg:103.31ms
step:1264/1770 train_time:129555ms step_avg:103.31ms
step:1265/1770 train_time:129662ms step_avg:103.32ms
step:1266/1770 train_time:129771ms step_avg:103.32ms
step:1267/1770 train_time:129879ms step_avg:103.32ms
step:1268/1770 train_time:129987ms step_avg:103.33ms
step:1269/1770 train_time:130095ms step_avg:103.33ms
step:1270/1770 train_time:130203ms step_avg:103.34ms
step:1271/1770 train_time:130311ms step_avg:103.34ms
step:1272/1770 train_time:130418ms step_avg:103.34ms
step:1273/1770 train_time:130527ms step_avg:103.35ms
step:1274/1770 train_time:130634ms step_avg:103.35ms
step:1275/1770 train_time:130741ms step_avg:103.35ms
step:1276/1770 train_time:130850ms step_avg:103.36ms
step:1277/1770 train_time:130957ms step_avg:103.36ms
step:1278/1770 train_time:131065ms step_avg:103.36ms
step:1279/1770 train_time:131172ms step_avg:103.37ms
step:1280/1770 train_time:131281ms step_avg:103.37ms
step:1281/1770 train_time:131388ms step_avg:103.37ms
step:1282/1770 train_time:131496ms step_avg:103.38ms
step:1283/1770 train_time:131604ms step_avg:103.38ms
step:1284/1770 train_time:131712ms step_avg:103.38ms
step:1285/1770 train_time:131820ms step_avg:103.39ms
step:1286/1770 train_time:131929ms step_avg:103.39ms
step:1287/1770 train_time:132039ms step_avg:103.40ms
step:1288/1770 train_time:132146ms step_avg:103.40ms
step:1289/1770 train_time:132253ms step_avg:103.40ms
step:1290/1770 train_time:132360ms step_avg:103.41ms
step:1291/1770 train_time:132468ms step_avg:103.41ms
step:1292/1770 train_time:132575ms step_avg:103.41ms
step:1293/1770 train_time:132683ms step_avg:103.42ms
step:1294/1770 train_time:132790ms step_avg:103.42ms
step:1295/1770 train_time:132898ms step_avg:103.42ms
step:1296/1770 train_time:133006ms step_avg:103.43ms
step:1297/1770 train_time:133113ms step_avg:103.43ms
step:1298/1770 train_time:133221ms step_avg:103.43ms
step:1299/1770 train_time:133328ms step_avg:103.44ms
step:1300/1770 train_time:133435ms step_avg:103.44ms
step:1301/1770 train_time:133544ms step_avg:103.44ms
step:1302/1770 train_time:133653ms step_avg:103.45ms
step:1303/1770 train_time:133760ms step_avg:103.45ms
step:1304/1770 train_time:133867ms step_avg:103.45ms
step:1305/1770 train_time:133975ms step_avg:103.46ms
step:1306/1770 train_time:134082ms step_avg:103.46ms
step:1307/1770 train_time:134190ms step_avg:103.46ms
step:1308/1770 train_time:134297ms step_avg:103.46ms
step:1309/1770 train_time:134405ms step_avg:103.47ms
step:1310/1770 train_time:134511ms step_avg:103.47ms
step:1311/1770 train_time:134619ms step_avg:103.47ms
step:1312/1770 train_time:134726ms step_avg:103.48ms
step:1313/1770 train_time:134833ms step_avg:103.48ms
step:1314/1770 train_time:134940ms step_avg:103.48ms
step:1315/1770 train_time:135048ms step_avg:103.49ms
step:1316/1770 train_time:135156ms step_avg:103.49ms
step:1317/1770 train_time:135264ms step_avg:103.49ms
step:1318/1770 train_time:135374ms step_avg:103.50ms
step:1319/1770 train_time:135482ms step_avg:103.50ms
step:1320/1770 train_time:135590ms step_avg:103.50ms
step:1321/1770 train_time:135697ms step_avg:103.51ms
step:1322/1770 train_time:135806ms step_avg:103.51ms
step:1323/1770 train_time:135913ms step_avg:103.51ms
step:1324/1770 train_time:136021ms step_avg:103.52ms
step:1325/1770 train_time:136130ms step_avg:103.52ms
step:1326/1770 train_time:136237ms step_avg:103.52ms
step:1327/1770 train_time:136348ms step_avg:103.53ms
step:1328/1770 train_time:136455ms step_avg:103.53ms
step:1329/1770 train_time:136563ms step_avg:103.54ms
step:1330/1770 train_time:136670ms step_avg:103.54ms
step:1331/1770 train_time:136778ms step_avg:103.54ms
step:1332/1770 train_time:136886ms step_avg:103.54ms
step:1333/1770 train_time:136993ms step_avg:103.55ms
step:1334/1770 train_time:137099ms step_avg:103.55ms
step:1335/1770 train_time:137207ms step_avg:103.55ms
step:1336/1770 train_time:137313ms step_avg:103.55ms
step:1337/1770 train_time:137421ms step_avg:103.56ms
step:1338/1770 train_time:137528ms step_avg:103.56ms
step:1339/1770 train_time:137636ms step_avg:103.56ms
step:1340/1770 train_time:137745ms step_avg:103.57ms
step:1341/1770 train_time:137852ms step_avg:103.57ms
step:1342/1770 train_time:137961ms step_avg:103.57ms
step:1343/1770 train_time:138069ms step_avg:103.58ms
step:1344/1770 train_time:138176ms step_avg:103.58ms
step:1345/1770 train_time:138283ms step_avg:103.58ms
step:1346/1770 train_time:138392ms step_avg:103.59ms
step:1347/1770 train_time:138499ms step_avg:103.59ms
step:1348/1770 train_time:138608ms step_avg:103.59ms
step:1349/1770 train_time:138715ms step_avg:103.60ms
step:1350/1770 train_time:138824ms step_avg:103.60ms
step:1351/1770 train_time:138932ms step_avg:103.60ms
step:1352/1770 train_time:139038ms step_avg:103.61ms
step:1353/1770 train_time:139148ms step_avg:103.61ms
step:1354/1770 train_time:139254ms step_avg:103.61ms
step:1355/1770 train_time:139361ms step_avg:103.61ms
step:1356/1770 train_time:139468ms step_avg:103.62ms
step:1357/1770 train_time:139575ms step_avg:103.62ms
step:1358/1770 train_time:139683ms step_avg:103.62ms
step:1359/1770 train_time:139791ms step_avg:103.63ms
step:1360/1770 train_time:139899ms step_avg:103.63ms
step:1361/1770 train_time:140007ms step_avg:103.63ms
step:1362/1770 train_time:140114ms step_avg:103.63ms
step:1363/1770 train_time:140223ms step_avg:103.64ms
step:1364/1770 train_time:140330ms step_avg:103.64ms
step:1365/1770 train_time:140438ms step_avg:103.64ms
step:1366/1770 train_time:140545ms step_avg:103.65ms
step:1367/1770 train_time:140653ms step_avg:103.65ms
step:1368/1770 train_time:140761ms step_avg:103.65ms
step:1369/1770 train_time:140870ms step_avg:103.66ms
step:1370/1770 train_time:140978ms step_avg:103.66ms
step:1371/1770 train_time:141087ms step_avg:103.66ms
step:1372/1770 train_time:141193ms step_avg:103.67ms
step:1373/1770 train_time:141300ms step_avg:103.67ms
step:1374/1770 train_time:141408ms step_avg:103.67ms
step:1375/1770 train_time:141516ms step_avg:103.67ms
step:1375/1770 val_loss:3.3787 train_time:141624ms step_avg:103.75ms
step:1376/1770 train_time:141642ms step_avg:103.69ms
step:1377/1770 train_time:141741ms step_avg:103.69ms
step:1378/1770 train_time:141850ms step_avg:103.69ms
step:1379/1770 train_time:141957ms step_avg:103.69ms
step:1380/1770 train_time:142065ms step_avg:103.70ms
step:1381/1770 train_time:142173ms step_avg:103.70ms
step:1382/1770 train_time:142280ms step_avg:103.70ms
step:1383/1770 train_time:142388ms step_avg:103.71ms
step:1384/1770 train_time:142496ms step_avg:103.71ms
step:1385/1770 train_time:142604ms step_avg:103.71ms
step:1386/1770 train_time:142712ms step_avg:103.71ms
step:1387/1770 train_time:142820ms step_avg:103.72ms
step:1388/1770 train_time:142928ms step_avg:103.72ms
step:1389/1770 train_time:143035ms step_avg:103.72ms
step:1390/1770 train_time:143143ms step_avg:103.73ms
step:1391/1770 train_time:143251ms step_avg:103.73ms
step:1392/1770 train_time:143357ms step_avg:103.73ms
step:1393/1770 train_time:143465ms step_avg:103.73ms
step:1394/1770 train_time:143573ms step_avg:103.74ms
step:1395/1770 train_time:143682ms step_avg:103.74ms
step:1396/1770 train_time:143791ms step_avg:103.75ms
step:1397/1770 train_time:143899ms step_avg:103.75ms
step:1398/1770 train_time:144006ms step_avg:103.75ms
step:1399/1770 train_time:144114ms step_avg:103.75ms
step:1400/1770 train_time:144221ms step_avg:103.76ms
step:1401/1770 train_time:144328ms step_avg:103.76ms
step:1402/1770 train_time:144436ms step_avg:103.76ms
step:1403/1770 train_time:144544ms step_avg:103.76ms
step:1404/1770 train_time:144652ms step_avg:103.77ms
step:1405/1770 train_time:144759ms step_avg:103.77ms
step:1406/1770 train_time:144866ms step_avg:103.77ms
step:1407/1770 train_time:144973ms step_avg:103.77ms
step:1408/1770 train_time:145081ms step_avg:103.78ms
step:1409/1770 train_time:145189ms step_avg:103.78ms
step:1410/1770 train_time:145296ms step_avg:103.78ms
step:1411/1770 train_time:145403ms step_avg:103.79ms
step:1412/1770 train_time:145511ms step_avg:103.79ms
step:1413/1770 train_time:145618ms step_avg:103.79ms
step:1414/1770 train_time:145727ms step_avg:103.79ms
step:1415/1770 train_time:145835ms step_avg:103.80ms
step:1416/1770 train_time:145943ms step_avg:103.80ms
step:1417/1770 train_time:146051ms step_avg:103.80ms
step:1418/1770 train_time:146158ms step_avg:103.81ms
step:1419/1770 train_time:146267ms step_avg:103.81ms
step:1420/1770 train_time:146375ms step_avg:103.81ms
step:1421/1770 train_time:146482ms step_avg:103.81ms
step:1422/1770 train_time:146590ms step_avg:103.82ms
step:1423/1770 train_time:146697ms step_avg:103.82ms
step:1424/1770 train_time:146805ms step_avg:103.82ms
step:1425/1770 train_time:146912ms step_avg:103.82ms
step:1426/1770 train_time:147020ms step_avg:103.83ms
step:1427/1770 train_time:147128ms step_avg:103.83ms
step:1428/1770 train_time:147237ms step_avg:103.83ms
step:1429/1770 train_time:147344ms step_avg:103.84ms
step:1430/1770 train_time:147451ms step_avg:103.84ms
step:1431/1770 train_time:147559ms step_avg:103.84ms
step:1432/1770 train_time:147667ms step_avg:103.84ms
step:1433/1770 train_time:147775ms step_avg:103.85ms
step:1434/1770 train_time:147882ms step_avg:103.85ms
step:1435/1770 train_time:147990ms step_avg:103.85ms
step:1436/1770 train_time:148099ms step_avg:103.86ms
step:1437/1770 train_time:148207ms step_avg:103.86ms
step:1438/1770 train_time:148314ms step_avg:103.86ms
step:1439/1770 train_time:148421ms step_avg:103.86ms
step:1440/1770 train_time:148529ms step_avg:103.87ms
step:1441/1770 train_time:148639ms step_avg:103.87ms
step:1442/1770 train_time:148746ms step_avg:103.87ms
step:1443/1770 train_time:148853ms step_avg:103.88ms
step:1444/1770 train_time:148962ms step_avg:103.88ms
step:1445/1770 train_time:149070ms step_avg:103.88ms
step:1446/1770 train_time:149179ms step_avg:103.89ms
step:1447/1770 train_time:149289ms step_avg:103.89ms
step:1448/1770 train_time:149397ms step_avg:103.89ms
step:1449/1770 train_time:149507ms step_avg:103.90ms
step:1450/1770 train_time:149615ms step_avg:103.90ms
step:1451/1770 train_time:149723ms step_avg:103.90ms
step:1452/1770 train_time:149834ms step_avg:103.91ms
step:1453/1770 train_time:149943ms step_avg:103.91ms
step:1454/1770 train_time:150052ms step_avg:103.91ms
step:1455/1770 train_time:150162ms step_avg:103.92ms
step:1456/1770 train_time:150272ms step_avg:103.92ms
step:1457/1770 train_time:150382ms step_avg:103.93ms
step:1458/1770 train_time:150491ms step_avg:103.93ms
step:1459/1770 train_time:150600ms step_avg:103.93ms
step:1460/1770 train_time:150710ms step_avg:103.94ms
step:1461/1770 train_time:150818ms step_avg:103.94ms
step:1462/1770 train_time:150927ms step_avg:103.94ms
step:1463/1770 train_time:151036ms step_avg:103.95ms
step:1464/1770 train_time:151146ms step_avg:103.95ms
step:1465/1770 train_time:151254ms step_avg:103.95ms
step:1466/1770 train_time:151363ms step_avg:103.96ms
step:1467/1770 train_time:151473ms step_avg:103.96ms
step:1468/1770 train_time:151583ms step_avg:103.97ms
step:1469/1770 train_time:151691ms step_avg:103.97ms
step:1470/1770 train_time:151799ms step_avg:103.97ms
step:1471/1770 train_time:151907ms step_avg:103.97ms
step:1472/1770 train_time:152016ms step_avg:103.98ms
step:1473/1770 train_time:152126ms step_avg:103.98ms
step:1474/1770 train_time:152236ms step_avg:103.99ms
step:1475/1770 train_time:152345ms step_avg:103.99ms
step:1476/1770 train_time:152453ms step_avg:103.99ms
step:1477/1770 train_time:152564ms step_avg:104.00ms
step:1478/1770 train_time:152673ms step_avg:104.00ms
step:1479/1770 train_time:152782ms step_avg:104.00ms
step:1480/1770 train_time:152891ms step_avg:104.01ms
step:1481/1770 train_time:153003ms step_avg:104.01ms
step:1482/1770 train_time:153111ms step_avg:104.02ms
step:1483/1770 train_time:153220ms step_avg:104.02ms
step:1484/1770 train_time:153330ms step_avg:104.02ms
step:1485/1770 train_time:153438ms step_avg:104.03ms
step:1486/1770 train_time:153547ms step_avg:104.03ms
step:1487/1770 train_time:153656ms step_avg:104.03ms
step:1488/1770 train_time:153764ms step_avg:104.04ms
step:1489/1770 train_time:153875ms step_avg:104.04ms
step:1490/1770 train_time:153984ms step_avg:104.04ms
step:1491/1770 train_time:154093ms step_avg:104.05ms
step:1492/1770 train_time:154203ms step_avg:104.05ms
step:1493/1770 train_time:154314ms step_avg:104.06ms
step:1494/1770 train_time:154428ms step_avg:104.06ms
step:1495/1770 train_time:154535ms step_avg:104.06ms
step:1496/1770 train_time:154645ms step_avg:104.07ms
step:1497/1770 train_time:154754ms step_avg:104.07ms
step:1498/1770 train_time:154862ms step_avg:104.07ms
step:1499/1770 train_time:154970ms step_avg:104.08ms
step:1500/1770 train_time:155078ms step_avg:104.08ms
step:1500/1770 val_loss:3.3406 train_time:155186ms step_avg:104.15ms
step:1501/1770 train_time:155204ms step_avg:104.09ms
step:1502/1770 train_time:155307ms step_avg:104.09ms
step:1503/1770 train_time:155415ms step_avg:104.10ms
step:1504/1770 train_time:155525ms step_avg:104.10ms
step:1505/1770 train_time:155636ms step_avg:104.10ms
step:1506/1770 train_time:155744ms step_avg:104.11ms
step:1507/1770 train_time:155852ms step_avg:104.11ms
step:1508/1770 train_time:155965ms step_avg:104.12ms
step:1509/1770 train_time:156071ms step_avg:104.12ms
step:1510/1770 train_time:156179ms step_avg:104.12ms
step:1511/1770 train_time:156289ms step_avg:104.12ms
step:1512/1770 train_time:156397ms step_avg:104.13ms
step:1513/1770 train_time:156506ms step_avg:104.13ms
step:1514/1770 train_time:156615ms step_avg:104.13ms
step:1515/1770 train_time:156724ms step_avg:104.14ms
step:1516/1770 train_time:156834ms step_avg:104.14ms
step:1517/1770 train_time:156942ms step_avg:104.14ms
step:1518/1770 train_time:157053ms step_avg:104.15ms
step:1519/1770 train_time:157161ms step_avg:104.15ms
step:1520/1770 train_time:157271ms step_avg:104.15ms
step:1521/1770 train_time:157379ms step_avg:104.16ms
step:1522/1770 train_time:157487ms step_avg:104.16ms
step:1523/1770 train_time:157597ms step_avg:104.16ms
step:1524/1770 train_time:157706ms step_avg:104.16ms
step:1525/1770 train_time:157815ms step_avg:104.17ms
step:1526/1770 train_time:157923ms step_avg:104.17ms
step:1527/1770 train_time:158031ms step_avg:104.17ms
step:1528/1770 train_time:158143ms step_avg:104.18ms
step:1529/1770 train_time:158252ms step_avg:104.18ms
step:1530/1770 train_time:158361ms step_avg:104.18ms
step:1531/1770 train_time:158469ms step_avg:104.19ms
step:1532/1770 train_time:158578ms step_avg:104.19ms
step:1533/1770 train_time:158688ms step_avg:104.19ms
step:1534/1770 train_time:158797ms step_avg:104.20ms
step:1535/1770 train_time:158905ms step_avg:104.20ms
step:1536/1770 train_time:159013ms step_avg:104.20ms
step:1537/1770 train_time:159123ms step_avg:104.21ms
step:1538/1770 train_time:159233ms step_avg:104.21ms
step:1539/1770 train_time:159342ms step_avg:104.21ms
step:1540/1770 train_time:159453ms step_avg:104.22ms
step:1541/1770 train_time:159564ms step_avg:104.22ms
step:1542/1770 train_time:159672ms step_avg:104.22ms
step:1543/1770 train_time:159781ms step_avg:104.23ms
step:1544/1770 train_time:159891ms step_avg:104.23ms
step:1545/1770 train_time:160001ms step_avg:104.24ms
step:1546/1770 train_time:160110ms step_avg:104.24ms
step:1547/1770 train_time:160219ms step_avg:104.24ms
step:1548/1770 train_time:160327ms step_avg:104.24ms
step:1549/1770 train_time:160435ms step_avg:104.25ms
step:1550/1770 train_time:160544ms step_avg:104.25ms
step:1551/1770 train_time:160652ms step_avg:104.25ms
step:1552/1770 train_time:160763ms step_avg:104.26ms
step:1553/1770 train_time:160871ms step_avg:104.26ms
step:1554/1770 train_time:160980ms step_avg:104.26ms
step:1555/1770 train_time:161089ms step_avg:104.26ms
step:1556/1770 train_time:161197ms step_avg:104.27ms
step:1557/1770 train_time:161305ms step_avg:104.27ms
step:1558/1770 train_time:161414ms step_avg:104.27ms
step:1559/1770 train_time:161524ms step_avg:104.28ms
step:1560/1770 train_time:161633ms step_avg:104.28ms
step:1561/1770 train_time:161743ms step_avg:104.28ms
step:1562/1770 train_time:161851ms step_avg:104.29ms
step:1563/1770 train_time:161961ms step_avg:104.29ms
step:1564/1770 train_time:162070ms step_avg:104.29ms
step:1565/1770 train_time:162177ms step_avg:104.29ms
step:1566/1770 train_time:162286ms step_avg:104.30ms
step:1567/1770 train_time:162394ms step_avg:104.30ms
step:1568/1770 train_time:162503ms step_avg:104.30ms
step:1569/1770 train_time:162615ms step_avg:104.31ms
step:1570/1770 train_time:162723ms step_avg:104.31ms
step:1571/1770 train_time:162832ms step_avg:104.31ms
step:1572/1770 train_time:162941ms step_avg:104.32ms
step:1573/1770 train_time:163051ms step_avg:104.32ms
step:1574/1770 train_time:163159ms step_avg:104.32ms
step:1575/1770 train_time:163268ms step_avg:104.32ms
step:1576/1770 train_time:163377ms step_avg:104.33ms
step:1577/1770 train_time:163487ms step_avg:104.33ms
step:1578/1770 train_time:163597ms step_avg:104.33ms
step:1579/1770 train_time:163705ms step_avg:104.34ms
step:1580/1770 train_time:163814ms step_avg:104.34ms
step:1581/1770 train_time:163926ms step_avg:104.34ms
step:1582/1770 train_time:164035ms step_avg:104.35ms
step:1583/1770 train_time:164144ms step_avg:104.35ms
step:1584/1770 train_time:164254ms step_avg:104.35ms
step:1585/1770 train_time:164363ms step_avg:104.36ms
step:1586/1770 train_time:164476ms step_avg:104.36ms
step:1587/1770 train_time:164584ms step_avg:104.37ms
step:1588/1770 train_time:164693ms step_avg:104.37ms
step:1589/1770 train_time:164805ms step_avg:104.37ms
step:1590/1770 train_time:164913ms step_avg:104.38ms
step:1591/1770 train_time:165022ms step_avg:104.38ms
step:1592/1770 train_time:165131ms step_avg:104.38ms
step:1593/1770 train_time:165241ms step_avg:104.38ms
step:1594/1770 train_time:165349ms step_avg:104.39ms
step:1595/1770 train_time:165458ms step_avg:104.39ms
step:1596/1770 train_time:165569ms step_avg:104.39ms
step:1597/1770 train_time:165676ms step_avg:104.40ms
step:1598/1770 train_time:165786ms step_avg:104.40ms
step:1599/1770 train_time:165895ms step_avg:104.40ms
step:1600/1770 train_time:166006ms step_avg:104.41ms
step:1601/1770 train_time:166116ms step_avg:104.41ms
step:1602/1770 train_time:166226ms step_avg:104.41ms
step:1603/1770 train_time:166334ms step_avg:104.42ms
step:1604/1770 train_time:166443ms step_avg:104.42ms
step:1605/1770 train_time:166550ms step_avg:104.42ms
step:1606/1770 train_time:166659ms step_avg:104.42ms
step:1607/1770 train_time:166772ms step_avg:104.43ms
step:1608/1770 train_time:166880ms step_avg:104.43ms
step:1609/1770 train_time:166989ms step_avg:104.43ms
step:1610/1770 train_time:167099ms step_avg:104.44ms
step:1611/1770 train_time:167210ms step_avg:104.44ms
step:1612/1770 train_time:167319ms step_avg:104.44ms
step:1613/1770 train_time:167428ms step_avg:104.45ms
step:1614/1770 train_time:167536ms step_avg:104.45ms
step:1615/1770 train_time:167645ms step_avg:104.45ms
step:1616/1770 train_time:167753ms step_avg:104.45ms
step:1617/1770 train_time:167865ms step_avg:104.46ms
step:1618/1770 train_time:167974ms step_avg:104.46ms
step:1619/1770 train_time:168084ms step_avg:104.46ms
step:1620/1770 train_time:168193ms step_avg:104.47ms
step:1621/1770 train_time:168303ms step_avg:104.47ms
step:1622/1770 train_time:168412ms step_avg:104.47ms
step:1623/1770 train_time:168525ms step_avg:104.48ms
step:1624/1770 train_time:168633ms step_avg:104.48ms
step:1625/1770 train_time:168741ms step_avg:104.48ms
step:1625/1770 val_loss:3.3058 train_time:168847ms step_avg:104.55ms
step:1626/1770 train_time:168866ms step_avg:104.50ms
step:1627/1770 train_time:168968ms step_avg:104.49ms
step:1628/1770 train_time:169077ms step_avg:104.50ms
step:1629/1770 train_time:169186ms step_avg:104.50ms
step:1630/1770 train_time:169295ms step_avg:104.50ms
step:1631/1770 train_time:169404ms step_avg:104.51ms
step:1632/1770 train_time:169513ms step_avg:104.51ms
step:1633/1770 train_time:169621ms step_avg:104.51ms
step:1634/1770 train_time:169730ms step_avg:104.51ms
step:1635/1770 train_time:169840ms step_avg:104.52ms
step:1636/1770 train_time:169949ms step_avg:104.52ms
step:1637/1770 train_time:170059ms step_avg:104.52ms
step:1638/1770 train_time:170167ms step_avg:104.53ms
step:1639/1770 train_time:170276ms step_avg:104.53ms
step:1640/1770 train_time:170386ms step_avg:104.53ms
step:1641/1770 train_time:170495ms step_avg:104.53ms
step:1642/1770 train_time:170604ms step_avg:104.54ms
step:1643/1770 train_time:170712ms step_avg:104.54ms
step:1644/1770 train_time:170823ms step_avg:104.54ms
step:1645/1770 train_time:170931ms step_avg:104.54ms
step:1646/1770 train_time:171042ms step_avg:104.55ms
step:1647/1770 train_time:171152ms step_avg:104.55ms
step:1648/1770 train_time:171260ms step_avg:104.55ms
step:1649/1770 train_time:171368ms step_avg:104.56ms
step:1650/1770 train_time:171478ms step_avg:104.56ms
step:1651/1770 train_time:171586ms step_avg:104.56ms
step:1652/1770 train_time:171695ms step_avg:104.56ms
step:1653/1770 train_time:171804ms step_avg:104.57ms
step:1654/1770 train_time:171917ms step_avg:104.57ms
step:1655/1770 train_time:172028ms step_avg:104.58ms
step:1656/1770 train_time:172138ms step_avg:104.58ms
step:1657/1770 train_time:172247ms step_avg:104.58ms
step:1658/1770 train_time:172356ms step_avg:104.58ms
step:1659/1770 train_time:172466ms step_avg:104.59ms
step:1660/1770 train_time:172574ms step_avg:104.59ms
step:1661/1770 train_time:172684ms step_avg:104.59ms
step:1662/1770 train_time:172793ms step_avg:104.60ms
step:1663/1770 train_time:172902ms step_avg:104.60ms
step:1664/1770 train_time:173010ms step_avg:104.60ms
step:1665/1770 train_time:173118ms step_avg:104.60ms
step:1666/1770 train_time:173227ms step_avg:104.61ms
step:1667/1770 train_time:173336ms step_avg:104.61ms
step:1668/1770 train_time:173444ms step_avg:104.61ms
step:1669/1770 train_time:173552ms step_avg:104.61ms
step:1670/1770 train_time:173660ms step_avg:104.61ms
step:1671/1770 train_time:173770ms step_avg:104.62ms
step:1672/1770 train_time:173880ms step_avg:104.62ms
step:1673/1770 train_time:173990ms step_avg:104.62ms
step:1674/1770 train_time:174099ms step_avg:104.63ms
step:1675/1770 train_time:174209ms step_avg:104.63ms
step:1676/1770 train_time:174318ms step_avg:104.63ms
step:1677/1770 train_time:174430ms step_avg:104.64ms
step:1678/1770 train_time:174539ms step_avg:104.64ms
step:1679/1770 train_time:174647ms step_avg:104.64ms
step:1680/1770 train_time:174756ms step_avg:104.64ms
step:1681/1770 train_time:174865ms step_avg:104.65ms
step:1682/1770 train_time:174976ms step_avg:104.65ms
step:1683/1770 train_time:175085ms step_avg:104.65ms
step:1684/1770 train_time:175193ms step_avg:104.66ms
step:1685/1770 train_time:175302ms step_avg:104.66ms
step:1686/1770 train_time:175412ms step_avg:104.66ms
step:1687/1770 train_time:175522ms step_avg:104.66ms
step:1688/1770 train_time:175630ms step_avg:104.67ms
step:1689/1770 train_time:175739ms step_avg:104.67ms
step:1690/1770 train_time:175847ms step_avg:104.67ms
step:1691/1770 train_time:175956ms step_avg:104.67ms
step:1692/1770 train_time:176065ms step_avg:104.68ms
step:1693/1770 train_time:176176ms step_avg:104.68ms
step:1694/1770 train_time:176286ms step_avg:104.68ms
step:1695/1770 train_time:176394ms step_avg:104.69ms
step:1696/1770 train_time:176506ms step_avg:104.69ms
step:1697/1770 train_time:176617ms step_avg:104.69ms
step:1698/1770 train_time:176727ms step_avg:104.70ms
step:1699/1770 train_time:176835ms step_avg:104.70ms
step:1700/1770 train_time:176944ms step_avg:104.70ms
step:1701/1770 train_time:177053ms step_avg:104.70ms
step:1702/1770 train_time:177162ms step_avg:104.71ms
step:1703/1770 train_time:177270ms step_avg:104.71ms
step:1704/1770 train_time:177379ms step_avg:104.71ms
step:1705/1770 train_time:177487ms step_avg:104.71ms
step:1706/1770 train_time:177595ms step_avg:104.71ms
step:1707/1770 train_time:177705ms step_avg:104.72ms
step:1708/1770 train_time:177815ms step_avg:104.72ms
step:1709/1770 train_time:177926ms step_avg:104.72ms
step:1710/1770 train_time:178039ms step_avg:104.73ms
step:1711/1770 train_time:178151ms step_avg:104.73ms
step:1712/1770 train_time:178261ms step_avg:104.74ms
step:1713/1770 train_time:178369ms step_avg:104.74ms
step:1714/1770 train_time:178479ms step_avg:104.74ms
step:1715/1770 train_time:178587ms step_avg:104.74ms
step:1716/1770 train_time:178698ms step_avg:104.75ms
step:1717/1770 train_time:178807ms step_avg:104.75ms
step:1718/1770 train_time:178918ms step_avg:104.75ms
step:1719/1770 train_time:179027ms step_avg:104.76ms
step:1720/1770 train_time:179139ms step_avg:104.76ms
step:1721/1770 train_time:179249ms step_avg:104.76ms
step:1722/1770 train_time:179361ms step_avg:104.77ms
step:1723/1770 train_time:179472ms step_avg:104.77ms
step:1724/1770 train_time:179584ms step_avg:104.77ms
step:1725/1770 train_time:179698ms step_avg:104.78ms
step:1726/1770 train_time:179809ms step_avg:104.78ms
step:1727/1770 train_time:179919ms step_avg:104.79ms
step:1728/1770 train_time:180031ms step_avg:104.79ms
step:1729/1770 train_time:180141ms step_avg:104.79ms
step:1730/1770 train_time:180251ms step_avg:104.80ms
step:1731/1770 train_time:180363ms step_avg:104.80ms
step:1732/1770 train_time:180472ms step_avg:104.80ms
step:1733/1770 train_time:180584ms step_avg:104.81ms
step:1734/1770 train_time:180692ms step_avg:104.81ms
step:1735/1770 train_time:180803ms step_avg:104.81ms
step:1736/1770 train_time:180915ms step_avg:104.82ms
step:1737/1770 train_time:181022ms step_avg:104.82ms
step:1738/1770 train_time:181132ms step_avg:104.82ms
step:1739/1770 train_time:181241ms step_avg:104.82ms
step:1740/1770 train_time:181350ms step_avg:104.83ms
step:1741/1770 train_time:181463ms step_avg:104.83ms
step:1742/1770 train_time:181576ms step_avg:104.84ms
step:1743/1770 train_time:181687ms step_avg:104.84ms
step:1744/1770 train_time:181796ms step_avg:104.84ms
step:1745/1770 train_time:181905ms step_avg:104.84ms
step:1746/1770 train_time:182018ms step_avg:104.85ms
step:1747/1770 train_time:182126ms step_avg:104.85ms
step:1748/1770 train_time:182240ms step_avg:104.86ms
step:1749/1770 train_time:182350ms step_avg:104.86ms
step:1750/1770 train_time:182459ms step_avg:104.86ms
step:1750/1770 val_loss:3.2789 train_time:182567ms step_avg:104.92ms
step:1751/1770 train_time:182585ms step_avg:104.87ms
step:1752/1770 train_time:182685ms step_avg:104.87ms
step:1753/1770 train_time:182796ms step_avg:104.87ms
step:1754/1770 train_time:182907ms step_avg:104.88ms
step:1755/1770 train_time:183016ms step_avg:104.88ms
step:1756/1770 train_time:183127ms step_avg:104.88ms
step:1757/1770 train_time:183237ms step_avg:104.89ms
step:1758/1770 train_time:183347ms step_avg:104.89ms
step:1759/1770 train_time:183457ms step_avg:104.89ms
step:1760/1770 train_time:183566ms step_avg:104.89ms
step:1761/1770 train_time:183679ms step_avg:104.90ms
step:1762/1770 train_time:183792ms step_avg:104.90ms
step:1763/1770 train_time:183901ms step_avg:104.91ms
step:1764/1770 train_time:184012ms step_avg:104.91ms
step:1765/1770 train_time:184122ms step_avg:104.91ms
step:1766/1770 train_time:184237ms step_avg:104.92ms
step:1767/1770 train_time:184346ms step_avg:104.92ms
step:1768/1770 train_time:184455ms step_avg:104.92ms
step:1769/1770 train_time:184564ms step_avg:104.93ms
step:1770/1770 train_time:184673ms step_avg:104.93ms
step:1770/1770 val_loss:3.2758 train_time:184783ms step_avg:104.99ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
