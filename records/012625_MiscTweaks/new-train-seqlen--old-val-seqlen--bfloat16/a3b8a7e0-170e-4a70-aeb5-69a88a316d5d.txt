import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:25:23 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:26738ms step_avg:nanms
step:2/1770 train_time:27228ms step_avg:nanms
step:3/1770 train_time:27326ms step_avg:nanms
step:4/1770 train_time:27424ms step_avg:nanms
step:5/1770 train_time:27523ms step_avg:nanms
step:6/1770 train_time:27621ms step_avg:nanms
step:7/1770 train_time:27721ms step_avg:nanms
step:8/1770 train_time:27820ms step_avg:nanms
step:9/1770 train_time:27919ms step_avg:nanms
step:10/1770 train_time:28018ms step_avg:nanms
step:11/1770 train_time:98ms step_avg:nanms
step:12/1770 train_time:198ms step_avg:nanms
step:13/1770 train_time:301ms step_avg:100.19ms
step:14/1770 train_time:399ms step_avg:99.76ms
step:15/1770 train_time:498ms step_avg:99.58ms
step:16/1770 train_time:597ms step_avg:99.42ms
step:17/1770 train_time:696ms step_avg:99.41ms
step:18/1770 train_time:795ms step_avg:99.33ms
step:19/1770 train_time:894ms step_avg:99.31ms
step:20/1770 train_time:993ms step_avg:99.29ms
step:21/1770 train_time:1092ms step_avg:99.25ms
step:22/1770 train_time:1191ms step_avg:99.24ms
step:23/1770 train_time:1291ms step_avg:99.31ms
step:24/1770 train_time:1390ms step_avg:99.31ms
step:25/1770 train_time:1490ms step_avg:99.36ms
step:26/1770 train_time:1590ms step_avg:99.39ms
step:27/1770 train_time:1690ms step_avg:99.39ms
step:28/1770 train_time:1789ms step_avg:99.40ms
step:29/1770 train_time:1890ms step_avg:99.45ms
step:30/1770 train_time:1990ms step_avg:99.50ms
step:31/1770 train_time:2089ms step_avg:99.46ms
step:32/1770 train_time:2188ms step_avg:99.46ms
step:33/1770 train_time:2288ms step_avg:99.48ms
step:34/1770 train_time:2388ms step_avg:99.50ms
step:35/1770 train_time:2488ms step_avg:99.51ms
step:36/1770 train_time:2587ms step_avg:99.51ms
step:37/1770 train_time:2688ms step_avg:99.55ms
step:38/1770 train_time:2789ms step_avg:99.62ms
step:39/1770 train_time:2888ms step_avg:99.58ms
step:40/1770 train_time:2989ms step_avg:99.64ms
step:41/1770 train_time:3090ms step_avg:99.66ms
step:42/1770 train_time:3190ms step_avg:99.69ms
step:43/1770 train_time:3289ms step_avg:99.67ms
step:44/1770 train_time:3389ms step_avg:99.67ms
step:45/1770 train_time:3488ms step_avg:99.64ms
step:46/1770 train_time:3586ms step_avg:99.62ms
step:47/1770 train_time:3686ms step_avg:99.62ms
step:48/1770 train_time:3785ms step_avg:99.60ms
step:49/1770 train_time:3884ms step_avg:99.59ms
step:50/1770 train_time:3984ms step_avg:99.59ms
step:51/1770 train_time:4083ms step_avg:99.58ms
step:52/1770 train_time:4182ms step_avg:99.57ms
step:53/1770 train_time:4281ms step_avg:99.56ms
step:54/1770 train_time:4380ms step_avg:99.55ms
step:55/1770 train_time:4483ms step_avg:99.62ms
step:56/1770 train_time:4579ms step_avg:99.54ms
step:57/1770 train_time:4678ms step_avg:99.54ms
step:58/1770 train_time:4777ms step_avg:99.53ms
step:59/1770 train_time:4876ms step_avg:99.51ms
step:60/1770 train_time:4975ms step_avg:99.50ms
step:61/1770 train_time:5074ms step_avg:99.49ms
step:62/1770 train_time:5173ms step_avg:99.48ms
step:63/1770 train_time:5272ms step_avg:99.47ms
step:64/1770 train_time:5371ms step_avg:99.47ms
step:65/1770 train_time:5470ms step_avg:99.46ms
step:66/1770 train_time:5569ms step_avg:99.45ms
step:67/1770 train_time:5669ms step_avg:99.45ms
step:68/1770 train_time:5768ms step_avg:99.45ms
step:69/1770 train_time:5868ms step_avg:99.46ms
step:70/1770 train_time:5968ms step_avg:99.46ms
step:71/1770 train_time:6068ms step_avg:99.47ms
step:72/1770 train_time:6166ms step_avg:99.46ms
step:73/1770 train_time:6266ms step_avg:99.46ms
step:74/1770 train_time:6366ms step_avg:99.47ms
step:75/1770 train_time:6466ms step_avg:99.48ms
step:76/1770 train_time:6566ms step_avg:99.48ms
step:77/1770 train_time:6665ms step_avg:99.48ms
step:78/1770 train_time:6764ms step_avg:99.47ms
step:79/1770 train_time:6864ms step_avg:99.47ms
step:80/1770 train_time:6963ms step_avg:99.47ms
step:81/1770 train_time:7062ms step_avg:99.47ms
step:82/1770 train_time:7163ms step_avg:99.49ms
step:83/1770 train_time:7263ms step_avg:99.49ms
step:84/1770 train_time:7363ms step_avg:99.50ms
step:85/1770 train_time:7463ms step_avg:99.51ms
step:86/1770 train_time:7563ms step_avg:99.51ms
step:87/1770 train_time:7663ms step_avg:99.52ms
step:88/1770 train_time:7762ms step_avg:99.51ms
step:89/1770 train_time:7861ms step_avg:99.50ms
step:90/1770 train_time:7960ms step_avg:99.49ms
step:91/1770 train_time:8058ms step_avg:99.48ms
step:92/1770 train_time:8157ms step_avg:99.47ms
step:93/1770 train_time:8255ms step_avg:99.46ms
step:94/1770 train_time:8354ms step_avg:99.45ms
step:95/1770 train_time:8453ms step_avg:99.44ms
step:96/1770 train_time:8552ms step_avg:99.44ms
step:97/1770 train_time:8651ms step_avg:99.44ms
step:98/1770 train_time:8751ms step_avg:99.44ms
step:99/1770 train_time:8851ms step_avg:99.45ms
step:100/1770 train_time:8950ms step_avg:99.44ms
step:101/1770 train_time:9049ms step_avg:99.44ms
step:102/1770 train_time:9148ms step_avg:99.44ms
step:103/1770 train_time:9248ms step_avg:99.44ms
step:104/1770 train_time:9350ms step_avg:99.47ms
step:105/1770 train_time:9451ms step_avg:99.48ms
step:106/1770 train_time:9551ms step_avg:99.49ms
step:107/1770 train_time:9650ms step_avg:99.48ms
step:108/1770 train_time:9749ms step_avg:99.48ms
step:109/1770 train_time:9850ms step_avg:99.49ms
step:110/1770 train_time:9950ms step_avg:99.50ms
step:111/1770 train_time:10050ms step_avg:99.51ms
step:112/1770 train_time:10150ms step_avg:99.51ms
step:113/1770 train_time:10249ms step_avg:99.50ms
step:114/1770 train_time:10348ms step_avg:99.50ms
step:115/1770 train_time:10447ms step_avg:99.50ms
step:116/1770 train_time:10549ms step_avg:99.52ms
step:117/1770 train_time:10650ms step_avg:99.53ms
step:118/1770 train_time:10751ms step_avg:99.55ms
step:119/1770 train_time:10851ms step_avg:99.55ms
step:120/1770 train_time:10952ms step_avg:99.56ms
step:121/1770 train_time:11050ms step_avg:99.55ms
step:122/1770 train_time:11150ms step_avg:99.56ms
step:123/1770 train_time:11251ms step_avg:99.56ms
step:124/1770 train_time:11350ms step_avg:99.57ms
step:125/1770 train_time:11452ms step_avg:99.58ms
step:125/1770 val_loss:4.6513 train_time:11551ms step_avg:100.44ms
step:126/1770 train_time:11569ms step_avg:99.73ms
step:127/1770 train_time:11665ms step_avg:99.70ms
step:128/1770 train_time:11770ms step_avg:99.75ms
step:129/1770 train_time:11870ms step_avg:99.75ms
step:130/1770 train_time:11969ms step_avg:99.74ms
step:131/1770 train_time:12068ms step_avg:99.74ms
step:132/1770 train_time:12171ms step_avg:99.76ms
step:133/1770 train_time:12265ms step_avg:99.72ms
step:134/1770 train_time:12364ms step_avg:99.71ms
step:135/1770 train_time:12464ms step_avg:99.71ms
step:136/1770 train_time:12563ms step_avg:99.71ms
step:137/1770 train_time:12663ms step_avg:99.71ms
step:138/1770 train_time:12763ms step_avg:99.71ms
step:139/1770 train_time:12863ms step_avg:99.71ms
step:140/1770 train_time:12963ms step_avg:99.71ms
step:141/1770 train_time:13062ms step_avg:99.71ms
step:142/1770 train_time:13162ms step_avg:99.71ms
step:143/1770 train_time:13262ms step_avg:99.72ms
step:144/1770 train_time:13362ms step_avg:99.72ms
step:145/1770 train_time:13462ms step_avg:99.71ms
step:146/1770 train_time:13562ms step_avg:99.72ms
step:147/1770 train_time:13662ms step_avg:99.72ms
step:148/1770 train_time:13762ms step_avg:99.72ms
step:149/1770 train_time:13862ms step_avg:99.73ms
step:150/1770 train_time:13962ms step_avg:99.73ms
step:151/1770 train_time:14062ms step_avg:99.73ms
step:152/1770 train_time:14161ms step_avg:99.73ms
step:153/1770 train_time:14261ms step_avg:99.73ms
step:154/1770 train_time:14361ms step_avg:99.73ms
step:155/1770 train_time:14461ms step_avg:99.73ms
step:156/1770 train_time:14561ms step_avg:99.73ms
step:157/1770 train_time:14661ms step_avg:99.73ms
step:158/1770 train_time:14760ms step_avg:99.73ms
step:159/1770 train_time:14861ms step_avg:99.74ms
step:160/1770 train_time:14961ms step_avg:99.74ms
step:161/1770 train_time:15061ms step_avg:99.74ms
step:162/1770 train_time:15161ms step_avg:99.74ms
step:163/1770 train_time:15261ms step_avg:99.74ms
step:164/1770 train_time:15360ms step_avg:99.74ms
step:165/1770 train_time:15460ms step_avg:99.74ms
step:166/1770 train_time:15560ms step_avg:99.74ms
step:167/1770 train_time:15660ms step_avg:99.74ms
step:168/1770 train_time:15760ms step_avg:99.75ms
step:169/1770 train_time:15860ms step_avg:99.75ms
step:170/1770 train_time:15961ms step_avg:99.75ms
step:171/1770 train_time:16060ms step_avg:99.75ms
step:172/1770 train_time:16160ms step_avg:99.75ms
step:173/1770 train_time:16261ms step_avg:99.76ms
step:174/1770 train_time:16361ms step_avg:99.76ms
step:175/1770 train_time:16462ms step_avg:99.77ms
step:176/1770 train_time:16561ms step_avg:99.77ms
step:177/1770 train_time:16661ms step_avg:99.77ms
step:178/1770 train_time:16761ms step_avg:99.77ms
step:179/1770 train_time:16861ms step_avg:99.77ms
step:180/1770 train_time:16961ms step_avg:99.77ms
step:181/1770 train_time:17061ms step_avg:99.77ms
step:182/1770 train_time:17161ms step_avg:99.77ms
step:183/1770 train_time:17261ms step_avg:99.78ms
step:184/1770 train_time:17361ms step_avg:99.77ms
step:185/1770 train_time:17459ms step_avg:99.77ms
step:186/1770 train_time:17560ms step_avg:99.77ms
step:187/1770 train_time:17661ms step_avg:99.78ms
step:188/1770 train_time:17761ms step_avg:99.78ms
step:189/1770 train_time:17861ms step_avg:99.78ms
step:190/1770 train_time:17961ms step_avg:99.79ms
step:191/1770 train_time:18061ms step_avg:99.78ms
step:192/1770 train_time:18161ms step_avg:99.79ms
step:193/1770 train_time:18261ms step_avg:99.79ms
step:194/1770 train_time:18361ms step_avg:99.79ms
step:195/1770 train_time:18461ms step_avg:99.79ms
step:196/1770 train_time:18561ms step_avg:99.79ms
step:197/1770 train_time:18661ms step_avg:99.79ms
step:198/1770 train_time:18761ms step_avg:99.79ms
step:199/1770 train_time:18859ms step_avg:99.79ms
step:200/1770 train_time:18959ms step_avg:99.79ms
step:201/1770 train_time:19060ms step_avg:99.79ms
step:202/1770 train_time:19161ms step_avg:99.80ms
step:203/1770 train_time:19262ms step_avg:99.80ms
step:204/1770 train_time:19362ms step_avg:99.80ms
step:205/1770 train_time:19462ms step_avg:99.81ms
step:206/1770 train_time:19561ms step_avg:99.80ms
step:207/1770 train_time:19661ms step_avg:99.80ms
step:208/1770 train_time:19761ms step_avg:99.80ms
step:209/1770 train_time:19861ms step_avg:99.80ms
step:210/1770 train_time:19961ms step_avg:99.81ms
step:211/1770 train_time:20061ms step_avg:99.81ms
step:212/1770 train_time:20162ms step_avg:99.81ms
step:213/1770 train_time:20261ms step_avg:99.81ms
step:214/1770 train_time:20362ms step_avg:99.81ms
step:215/1770 train_time:20462ms step_avg:99.81ms
step:216/1770 train_time:20562ms step_avg:99.82ms
step:217/1770 train_time:20662ms step_avg:99.82ms
step:218/1770 train_time:20762ms step_avg:99.82ms
step:219/1770 train_time:20862ms step_avg:99.82ms
step:220/1770 train_time:20961ms step_avg:99.81ms
step:221/1770 train_time:21061ms step_avg:99.81ms
step:222/1770 train_time:21161ms step_avg:99.81ms
step:223/1770 train_time:21260ms step_avg:99.81ms
step:224/1770 train_time:21360ms step_avg:99.81ms
step:225/1770 train_time:21461ms step_avg:99.82ms
step:226/1770 train_time:21562ms step_avg:99.82ms
step:227/1770 train_time:21661ms step_avg:99.82ms
step:228/1770 train_time:21761ms step_avg:99.82ms
step:229/1770 train_time:21862ms step_avg:99.83ms
step:230/1770 train_time:21961ms step_avg:99.82ms
step:231/1770 train_time:22062ms step_avg:99.83ms
step:232/1770 train_time:22162ms step_avg:99.83ms
step:233/1770 train_time:22263ms step_avg:99.83ms
step:234/1770 train_time:22362ms step_avg:99.83ms
step:235/1770 train_time:22462ms step_avg:99.83ms
step:236/1770 train_time:22564ms step_avg:99.84ms
step:237/1770 train_time:22662ms step_avg:99.83ms
step:238/1770 train_time:22762ms step_avg:99.83ms
step:239/1770 train_time:22863ms step_avg:99.84ms
step:240/1770 train_time:22963ms step_avg:99.84ms
step:241/1770 train_time:23061ms step_avg:99.83ms
step:242/1770 train_time:23161ms step_avg:99.83ms
step:243/1770 train_time:23261ms step_avg:99.83ms
step:244/1770 train_time:23361ms step_avg:99.83ms
step:245/1770 train_time:23461ms step_avg:99.83ms
step:246/1770 train_time:23561ms step_avg:99.84ms
step:247/1770 train_time:23661ms step_avg:99.83ms
step:248/1770 train_time:23760ms step_avg:99.83ms
step:249/1770 train_time:23860ms step_avg:99.83ms
step:250/1770 train_time:23961ms step_avg:99.84ms
step:250/1770 val_loss:4.1076 train_time:24059ms step_avg:100.25ms
step:251/1770 train_time:24076ms step_avg:99.90ms
step:252/1770 train_time:24174ms step_avg:99.89ms
step:253/1770 train_time:24278ms step_avg:99.91ms
step:254/1770 train_time:24379ms step_avg:99.91ms
step:255/1770 train_time:24479ms step_avg:99.92ms
step:256/1770 train_time:24579ms step_avg:99.91ms
step:257/1770 train_time:24679ms step_avg:99.91ms
step:258/1770 train_time:24779ms step_avg:99.91ms
step:259/1770 train_time:24879ms step_avg:99.91ms
step:260/1770 train_time:24979ms step_avg:99.91ms
step:261/1770 train_time:25078ms step_avg:99.91ms
step:262/1770 train_time:25177ms step_avg:99.91ms
step:263/1770 train_time:25283ms step_avg:99.93ms
step:264/1770 train_time:25377ms step_avg:99.91ms
step:265/1770 train_time:25476ms step_avg:99.91ms
step:266/1770 train_time:25577ms step_avg:99.91ms
step:267/1770 train_time:25677ms step_avg:99.91ms
step:268/1770 train_time:25777ms step_avg:99.91ms
step:269/1770 train_time:25877ms step_avg:99.91ms
step:270/1770 train_time:25977ms step_avg:99.91ms
step:271/1770 train_time:26077ms step_avg:99.91ms
step:272/1770 train_time:26177ms step_avg:99.91ms
step:273/1770 train_time:26277ms step_avg:99.91ms
step:274/1770 train_time:26377ms step_avg:99.91ms
step:275/1770 train_time:26476ms step_avg:99.91ms
step:276/1770 train_time:26576ms step_avg:99.91ms
step:277/1770 train_time:26676ms step_avg:99.91ms
step:278/1770 train_time:26777ms step_avg:99.91ms
step:279/1770 train_time:26877ms step_avg:99.91ms
step:280/1770 train_time:26977ms step_avg:99.92ms
step:281/1770 train_time:27077ms step_avg:99.92ms
step:282/1770 train_time:27181ms step_avg:99.93ms
step:283/1770 train_time:27277ms step_avg:99.92ms
step:284/1770 train_time:27377ms step_avg:99.92ms
step:285/1770 train_time:27477ms step_avg:99.91ms
step:286/1770 train_time:27577ms step_avg:99.92ms
step:287/1770 train_time:27679ms step_avg:99.92ms
step:288/1770 train_time:27780ms step_avg:99.93ms
step:289/1770 train_time:27881ms step_avg:99.93ms
step:290/1770 train_time:27981ms step_avg:99.93ms
step:291/1770 train_time:28081ms step_avg:99.93ms
step:292/1770 train_time:28182ms step_avg:99.93ms
step:293/1770 train_time:28281ms step_avg:99.93ms
step:294/1770 train_time:28382ms step_avg:99.94ms
step:295/1770 train_time:28482ms step_avg:99.94ms
step:296/1770 train_time:28582ms step_avg:99.94ms
step:297/1770 train_time:28682ms step_avg:99.94ms
step:298/1770 train_time:28783ms step_avg:99.94ms
step:299/1770 train_time:28884ms step_avg:99.94ms
step:300/1770 train_time:28984ms step_avg:99.94ms
step:301/1770 train_time:29085ms step_avg:99.95ms
step:302/1770 train_time:29186ms step_avg:99.95ms
step:303/1770 train_time:29287ms step_avg:99.96ms
step:304/1770 train_time:29388ms step_avg:99.96ms
step:305/1770 train_time:29489ms step_avg:99.96ms
step:306/1770 train_time:29589ms step_avg:99.96ms
step:307/1770 train_time:29689ms step_avg:99.96ms
step:308/1770 train_time:29789ms step_avg:99.96ms
step:309/1770 train_time:29890ms step_avg:99.97ms
step:310/1770 train_time:29993ms step_avg:99.98ms
step:311/1770 train_time:30095ms step_avg:99.98ms
step:312/1770 train_time:30196ms step_avg:99.99ms
step:313/1770 train_time:30296ms step_avg:99.99ms
step:314/1770 train_time:30397ms step_avg:99.99ms
step:315/1770 train_time:30496ms step_avg:99.99ms
step:316/1770 train_time:30596ms step_avg:99.99ms
step:317/1770 train_time:30696ms step_avg:99.99ms
step:318/1770 train_time:30796ms step_avg:99.99ms
step:319/1770 train_time:30897ms step_avg:99.99ms
step:320/1770 train_time:30996ms step_avg:99.99ms
step:321/1770 train_time:31096ms step_avg:99.99ms
step:322/1770 train_time:31196ms step_avg:99.99ms
step:323/1770 train_time:31296ms step_avg:99.99ms
step:324/1770 train_time:31396ms step_avg:99.99ms
step:325/1770 train_time:31496ms step_avg:99.99ms
step:326/1770 train_time:31597ms step_avg:99.99ms
step:327/1770 train_time:31696ms step_avg:99.99ms
step:328/1770 train_time:31797ms step_avg:99.99ms
step:329/1770 train_time:31897ms step_avg:99.99ms
step:330/1770 train_time:31997ms step_avg:99.99ms
step:331/1770 train_time:32097ms step_avg:99.99ms
step:332/1770 train_time:32197ms step_avg:99.99ms
step:333/1770 train_time:32298ms step_avg:99.99ms
step:334/1770 train_time:32398ms step_avg:99.99ms
step:335/1770 train_time:32498ms step_avg:99.99ms
step:336/1770 train_time:32600ms step_avg:100.00ms
step:337/1770 train_time:32700ms step_avg:100.00ms
step:338/1770 train_time:32801ms step_avg:100.00ms
step:339/1770 train_time:32901ms step_avg:100.00ms
step:340/1770 train_time:33001ms step_avg:100.00ms
step:341/1770 train_time:33101ms step_avg:100.00ms
step:342/1770 train_time:33202ms step_avg:100.00ms
step:343/1770 train_time:33301ms step_avg:100.00ms
step:344/1770 train_time:33401ms step_avg:100.00ms
step:345/1770 train_time:33502ms step_avg:100.00ms
step:346/1770 train_time:33602ms step_avg:100.01ms
step:347/1770 train_time:33703ms step_avg:100.01ms
step:348/1770 train_time:33803ms step_avg:100.01ms
step:349/1770 train_time:33903ms step_avg:100.01ms
step:350/1770 train_time:34003ms step_avg:100.01ms
step:351/1770 train_time:34104ms step_avg:100.01ms
step:352/1770 train_time:34205ms step_avg:100.02ms
step:353/1770 train_time:34306ms step_avg:100.02ms
step:354/1770 train_time:34409ms step_avg:100.02ms
step:355/1770 train_time:34509ms step_avg:100.03ms
step:356/1770 train_time:34610ms step_avg:100.03ms
step:357/1770 train_time:34710ms step_avg:100.03ms
step:358/1770 train_time:34811ms step_avg:100.03ms
step:359/1770 train_time:34912ms step_avg:100.03ms
step:360/1770 train_time:35013ms step_avg:100.04ms
step:361/1770 train_time:35113ms step_avg:100.04ms
step:362/1770 train_time:35215ms step_avg:100.04ms
step:363/1770 train_time:35315ms step_avg:100.04ms
step:364/1770 train_time:35415ms step_avg:100.04ms
step:365/1770 train_time:35514ms step_avg:100.04ms
step:366/1770 train_time:35615ms step_avg:100.04ms
step:367/1770 train_time:35714ms step_avg:100.04ms
step:368/1770 train_time:35814ms step_avg:100.04ms
step:369/1770 train_time:35914ms step_avg:100.04ms
step:370/1770 train_time:36014ms step_avg:100.04ms
step:371/1770 train_time:36113ms step_avg:100.04ms
step:372/1770 train_time:36213ms step_avg:100.04ms
step:373/1770 train_time:36313ms step_avg:100.04ms
step:374/1770 train_time:36414ms step_avg:100.04ms
step:375/1770 train_time:36515ms step_avg:100.04ms
step:375/1770 val_loss:3.9046 train_time:36614ms step_avg:100.31ms
step:376/1770 train_time:36631ms step_avg:100.09ms
step:377/1770 train_time:36727ms step_avg:100.07ms
step:378/1770 train_time:36834ms step_avg:100.09ms
step:379/1770 train_time:36934ms step_avg:100.09ms
step:380/1770 train_time:37035ms step_avg:100.09ms
step:381/1770 train_time:37136ms step_avg:100.10ms
step:382/1770 train_time:37236ms step_avg:100.10ms
step:383/1770 train_time:37335ms step_avg:100.09ms
step:384/1770 train_time:37436ms step_avg:100.09ms
step:385/1770 train_time:37535ms step_avg:100.09ms
step:386/1770 train_time:37635ms step_avg:100.09ms
step:387/1770 train_time:37735ms step_avg:100.09ms
step:388/1770 train_time:37835ms step_avg:100.09ms
step:389/1770 train_time:37935ms step_avg:100.09ms
step:390/1770 train_time:38035ms step_avg:100.09ms
step:391/1770 train_time:38135ms step_avg:100.09ms
step:392/1770 train_time:38236ms step_avg:100.09ms
step:393/1770 train_time:38336ms step_avg:100.09ms
step:394/1770 train_time:38436ms step_avg:100.09ms
step:395/1770 train_time:38536ms step_avg:100.09ms
step:396/1770 train_time:38638ms step_avg:100.10ms
step:397/1770 train_time:38739ms step_avg:100.10ms
step:398/1770 train_time:38841ms step_avg:100.11ms
step:399/1770 train_time:38943ms step_avg:100.11ms
step:400/1770 train_time:39045ms step_avg:100.12ms
step:401/1770 train_time:39148ms step_avg:100.12ms
step:402/1770 train_time:39250ms step_avg:100.13ms
step:403/1770 train_time:39352ms step_avg:100.13ms
step:404/1770 train_time:39455ms step_avg:100.14ms
step:405/1770 train_time:39557ms step_avg:100.14ms
step:406/1770 train_time:39659ms step_avg:100.15ms
step:407/1770 train_time:39761ms step_avg:100.15ms
step:408/1770 train_time:39862ms step_avg:100.16ms
step:409/1770 train_time:39964ms step_avg:100.16ms
step:410/1770 train_time:40067ms step_avg:100.17ms
step:411/1770 train_time:40169ms step_avg:100.17ms
step:412/1770 train_time:40271ms step_avg:100.18ms
step:413/1770 train_time:40373ms step_avg:100.18ms
step:414/1770 train_time:40475ms step_avg:100.19ms
step:415/1770 train_time:40577ms step_avg:100.19ms
step:416/1770 train_time:40679ms step_avg:100.19ms
step:417/1770 train_time:40781ms step_avg:100.20ms
step:418/1770 train_time:40883ms step_avg:100.20ms
step:419/1770 train_time:40986ms step_avg:100.21ms
step:420/1770 train_time:41088ms step_avg:100.21ms
step:421/1770 train_time:41190ms step_avg:100.22ms
step:422/1770 train_time:41293ms step_avg:100.22ms
step:423/1770 train_time:41395ms step_avg:100.23ms
step:424/1770 train_time:41498ms step_avg:100.24ms
step:425/1770 train_time:41599ms step_avg:100.24ms
step:426/1770 train_time:41701ms step_avg:100.24ms
step:427/1770 train_time:41803ms step_avg:100.25ms
step:428/1770 train_time:41905ms step_avg:100.25ms
step:429/1770 train_time:42007ms step_avg:100.26ms
step:430/1770 train_time:42110ms step_avg:100.26ms
step:431/1770 train_time:42212ms step_avg:100.27ms
step:432/1770 train_time:42314ms step_avg:100.27ms
step:433/1770 train_time:42416ms step_avg:100.27ms
step:434/1770 train_time:42518ms step_avg:100.28ms
step:435/1770 train_time:42619ms step_avg:100.28ms
step:436/1770 train_time:42721ms step_avg:100.28ms
step:437/1770 train_time:42823ms step_avg:100.29ms
step:438/1770 train_time:42926ms step_avg:100.29ms
step:439/1770 train_time:43028ms step_avg:100.30ms
step:440/1770 train_time:43131ms step_avg:100.30ms
step:441/1770 train_time:43234ms step_avg:100.31ms
step:442/1770 train_time:43336ms step_avg:100.32ms
step:443/1770 train_time:43438ms step_avg:100.32ms
step:444/1770 train_time:43539ms step_avg:100.32ms
step:445/1770 train_time:43642ms step_avg:100.33ms
step:446/1770 train_time:43744ms step_avg:100.33ms
step:447/1770 train_time:43847ms step_avg:100.34ms
step:448/1770 train_time:43949ms step_avg:100.34ms
step:449/1770 train_time:44052ms step_avg:100.35ms
step:450/1770 train_time:44154ms step_avg:100.35ms
step:451/1770 train_time:44257ms step_avg:100.36ms
step:452/1770 train_time:44358ms step_avg:100.36ms
step:453/1770 train_time:44460ms step_avg:100.36ms
step:454/1770 train_time:44562ms step_avg:100.36ms
step:455/1770 train_time:44664ms step_avg:100.37ms
step:456/1770 train_time:44766ms step_avg:100.37ms
step:457/1770 train_time:44868ms step_avg:100.38ms
step:458/1770 train_time:44971ms step_avg:100.38ms
step:459/1770 train_time:45073ms step_avg:100.39ms
step:460/1770 train_time:45175ms step_avg:100.39ms
step:461/1770 train_time:45278ms step_avg:100.39ms
step:462/1770 train_time:45380ms step_avg:100.40ms
step:463/1770 train_time:45486ms step_avg:100.41ms
step:464/1770 train_time:45585ms step_avg:100.41ms
step:465/1770 train_time:45687ms step_avg:100.41ms
step:466/1770 train_time:45790ms step_avg:100.42ms
step:467/1770 train_time:45892ms step_avg:100.42ms
step:468/1770 train_time:45995ms step_avg:100.43ms
step:469/1770 train_time:46097ms step_avg:100.43ms
step:470/1770 train_time:46199ms step_avg:100.43ms
step:471/1770 train_time:46300ms step_avg:100.43ms
step:472/1770 train_time:46404ms step_avg:100.44ms
step:473/1770 train_time:46504ms step_avg:100.44ms
step:474/1770 train_time:46606ms step_avg:100.44ms
step:475/1770 train_time:46709ms step_avg:100.45ms
step:476/1770 train_time:46811ms step_avg:100.45ms
step:477/1770 train_time:46913ms step_avg:100.46ms
step:478/1770 train_time:47015ms step_avg:100.46ms
step:479/1770 train_time:47117ms step_avg:100.46ms
step:480/1770 train_time:47219ms step_avg:100.47ms
step:481/1770 train_time:47321ms step_avg:100.47ms
step:482/1770 train_time:47423ms step_avg:100.47ms
step:483/1770 train_time:47525ms step_avg:100.48ms
step:484/1770 train_time:47628ms step_avg:100.48ms
step:485/1770 train_time:47731ms step_avg:100.49ms
step:486/1770 train_time:47833ms step_avg:100.49ms
step:487/1770 train_time:47936ms step_avg:100.49ms
step:488/1770 train_time:48038ms step_avg:100.50ms
step:489/1770 train_time:48139ms step_avg:100.50ms
step:490/1770 train_time:48241ms step_avg:100.50ms
step:491/1770 train_time:48343ms step_avg:100.51ms
step:492/1770 train_time:48446ms step_avg:100.51ms
step:493/1770 train_time:48548ms step_avg:100.51ms
step:494/1770 train_time:48651ms step_avg:100.52ms
step:495/1770 train_time:48754ms step_avg:100.52ms
step:496/1770 train_time:48855ms step_avg:100.53ms
step:497/1770 train_time:48958ms step_avg:100.53ms
step:498/1770 train_time:49060ms step_avg:100.53ms
step:499/1770 train_time:49162ms step_avg:100.53ms
step:500/1770 train_time:49263ms step_avg:100.54ms
step:500/1770 val_loss:3.7508 train_time:49364ms step_avg:100.74ms
step:501/1770 train_time:49382ms step_avg:100.57ms
step:502/1770 train_time:49481ms step_avg:100.57ms
step:503/1770 train_time:49587ms step_avg:100.58ms
step:504/1770 train_time:49690ms step_avg:100.59ms
step:505/1770 train_time:49793ms step_avg:100.59ms
step:506/1770 train_time:49895ms step_avg:100.59ms
step:507/1770 train_time:49998ms step_avg:100.60ms
step:508/1770 train_time:50100ms step_avg:100.60ms
step:509/1770 train_time:50202ms step_avg:100.61ms
step:510/1770 train_time:50304ms step_avg:100.61ms
step:511/1770 train_time:50406ms step_avg:100.61ms
step:512/1770 train_time:50507ms step_avg:100.61ms
step:513/1770 train_time:50609ms step_avg:100.62ms
step:514/1770 train_time:50712ms step_avg:100.62ms
step:515/1770 train_time:50814ms step_avg:100.62ms
step:516/1770 train_time:50916ms step_avg:100.63ms
step:517/1770 train_time:51019ms step_avg:100.63ms
step:518/1770 train_time:51122ms step_avg:100.63ms
step:519/1770 train_time:51225ms step_avg:100.64ms
step:520/1770 train_time:51327ms step_avg:100.64ms
step:521/1770 train_time:51429ms step_avg:100.64ms
step:522/1770 train_time:51531ms step_avg:100.65ms
step:523/1770 train_time:51634ms step_avg:100.65ms
step:524/1770 train_time:51737ms step_avg:100.66ms
step:525/1770 train_time:51839ms step_avg:100.66ms
step:526/1770 train_time:51943ms step_avg:100.66ms
step:527/1770 train_time:52045ms step_avg:100.67ms
step:528/1770 train_time:52148ms step_avg:100.67ms
step:529/1770 train_time:52250ms step_avg:100.67ms
step:530/1770 train_time:52353ms step_avg:100.68ms
step:531/1770 train_time:52455ms step_avg:100.68ms
step:532/1770 train_time:52558ms step_avg:100.69ms
step:533/1770 train_time:52661ms step_avg:100.69ms
step:534/1770 train_time:52766ms step_avg:100.70ms
step:535/1770 train_time:52866ms step_avg:100.70ms
step:536/1770 train_time:52968ms step_avg:100.70ms
step:537/1770 train_time:53070ms step_avg:100.70ms
step:538/1770 train_time:53172ms step_avg:100.71ms
step:539/1770 train_time:53275ms step_avg:100.71ms
step:540/1770 train_time:53378ms step_avg:100.71ms
step:541/1770 train_time:53480ms step_avg:100.72ms
step:542/1770 train_time:53583ms step_avg:100.72ms
step:543/1770 train_time:53686ms step_avg:100.72ms
step:544/1770 train_time:53787ms step_avg:100.73ms
step:545/1770 train_time:53890ms step_avg:100.73ms
step:546/1770 train_time:53992ms step_avg:100.73ms
step:547/1770 train_time:54095ms step_avg:100.74ms
step:548/1770 train_time:54198ms step_avg:100.74ms
step:549/1770 train_time:54301ms step_avg:100.74ms
step:550/1770 train_time:54403ms step_avg:100.75ms
step:551/1770 train_time:54506ms step_avg:100.75ms
step:552/1770 train_time:54607ms step_avg:100.75ms
step:553/1770 train_time:54710ms step_avg:100.75ms
step:554/1770 train_time:54813ms step_avg:100.76ms
step:555/1770 train_time:54916ms step_avg:100.76ms
step:556/1770 train_time:55018ms step_avg:100.77ms
step:557/1770 train_time:55121ms step_avg:100.77ms
step:558/1770 train_time:55224ms step_avg:100.77ms
step:559/1770 train_time:55327ms step_avg:100.78ms
step:560/1770 train_time:55428ms step_avg:100.78ms
step:561/1770 train_time:55530ms step_avg:100.78ms
step:562/1770 train_time:55632ms step_avg:100.78ms
step:563/1770 train_time:55735ms step_avg:100.79ms
step:564/1770 train_time:55838ms step_avg:100.79ms
step:565/1770 train_time:55941ms step_avg:100.79ms
step:566/1770 train_time:56044ms step_avg:100.80ms
step:567/1770 train_time:56146ms step_avg:100.80ms
step:568/1770 train_time:56248ms step_avg:100.80ms
step:569/1770 train_time:56351ms step_avg:100.81ms
step:570/1770 train_time:56453ms step_avg:100.81ms
step:571/1770 train_time:56556ms step_avg:100.81ms
step:572/1770 train_time:56659ms step_avg:100.82ms
step:573/1770 train_time:56762ms step_avg:100.82ms
step:574/1770 train_time:56865ms step_avg:100.83ms
step:575/1770 train_time:56968ms step_avg:100.83ms
step:576/1770 train_time:57071ms step_avg:100.83ms
step:577/1770 train_time:57173ms step_avg:100.83ms
step:578/1770 train_time:57276ms step_avg:100.84ms
step:579/1770 train_time:57379ms step_avg:100.84ms
step:580/1770 train_time:57482ms step_avg:100.85ms
step:581/1770 train_time:57585ms step_avg:100.85ms
step:582/1770 train_time:57687ms step_avg:100.85ms
step:583/1770 train_time:57789ms step_avg:100.85ms
step:584/1770 train_time:57892ms step_avg:100.86ms
step:585/1770 train_time:57994ms step_avg:100.86ms
step:586/1770 train_time:58097ms step_avg:100.86ms
step:587/1770 train_time:58200ms step_avg:100.87ms
step:588/1770 train_time:58302ms step_avg:100.87ms
step:589/1770 train_time:58405ms step_avg:100.87ms
step:590/1770 train_time:58507ms step_avg:100.87ms
step:591/1770 train_time:58609ms step_avg:100.88ms
step:592/1770 train_time:58711ms step_avg:100.88ms
step:593/1770 train_time:58814ms step_avg:100.88ms
step:594/1770 train_time:58917ms step_avg:100.89ms
step:595/1770 train_time:59020ms step_avg:100.89ms
step:596/1770 train_time:59123ms step_avg:100.89ms
step:597/1770 train_time:59226ms step_avg:100.90ms
step:598/1770 train_time:59328ms step_avg:100.90ms
step:599/1770 train_time:59430ms step_avg:100.90ms
step:600/1770 train_time:59533ms step_avg:100.90ms
step:601/1770 train_time:59636ms step_avg:100.91ms
step:602/1770 train_time:59739ms step_avg:100.91ms
step:603/1770 train_time:59842ms step_avg:100.91ms
step:604/1770 train_time:59945ms step_avg:100.92ms
step:605/1770 train_time:60047ms step_avg:100.92ms
step:606/1770 train_time:60149ms step_avg:100.92ms
step:607/1770 train_time:60251ms step_avg:100.92ms
step:608/1770 train_time:60354ms step_avg:100.93ms
step:609/1770 train_time:60457ms step_avg:100.93ms
step:610/1770 train_time:60560ms step_avg:100.93ms
step:611/1770 train_time:60663ms step_avg:100.94ms
step:612/1770 train_time:60766ms step_avg:100.94ms
step:613/1770 train_time:60868ms step_avg:100.94ms
step:614/1770 train_time:60971ms step_avg:100.95ms
step:615/1770 train_time:61073ms step_avg:100.95ms
step:616/1770 train_time:61177ms step_avg:100.95ms
step:617/1770 train_time:61280ms step_avg:100.96ms
step:618/1770 train_time:61383ms step_avg:100.96ms
step:619/1770 train_time:61485ms step_avg:100.96ms
step:620/1770 train_time:61587ms step_avg:100.96ms
step:621/1770 train_time:61690ms step_avg:100.97ms
step:622/1770 train_time:61792ms step_avg:100.97ms
step:623/1770 train_time:61894ms step_avg:100.97ms
step:624/1770 train_time:61997ms step_avg:100.97ms
step:625/1770 train_time:62100ms step_avg:100.98ms
step:625/1770 val_loss:3.6634 train_time:62201ms step_avg:101.14ms
step:626/1770 train_time:62218ms step_avg:101.00ms
step:627/1770 train_time:62318ms step_avg:101.00ms
step:628/1770 train_time:62424ms step_avg:101.01ms
step:629/1770 train_time:62529ms step_avg:101.02ms
step:630/1770 train_time:62632ms step_avg:101.02ms
step:631/1770 train_time:62735ms step_avg:101.02ms
step:632/1770 train_time:62836ms step_avg:101.02ms
step:633/1770 train_time:62938ms step_avg:101.02ms
step:634/1770 train_time:63040ms step_avg:101.03ms
step:635/1770 train_time:63143ms step_avg:101.03ms
step:636/1770 train_time:63246ms step_avg:101.03ms
step:637/1770 train_time:63348ms step_avg:101.03ms
step:638/1770 train_time:63451ms step_avg:101.04ms
step:639/1770 train_time:63554ms step_avg:101.04ms
step:640/1770 train_time:63656ms step_avg:101.04ms
step:641/1770 train_time:63759ms step_avg:101.04ms
step:642/1770 train_time:63862ms step_avg:101.05ms
step:643/1770 train_time:63964ms step_avg:101.05ms
step:644/1770 train_time:64068ms step_avg:101.05ms
step:645/1770 train_time:64170ms step_avg:101.06ms
step:646/1770 train_time:64273ms step_avg:101.06ms
step:647/1770 train_time:64376ms step_avg:101.06ms
step:648/1770 train_time:64478ms step_avg:101.06ms
step:649/1770 train_time:64581ms step_avg:101.07ms
step:650/1770 train_time:64683ms step_avg:101.07ms
step:651/1770 train_time:64786ms step_avg:101.07ms
step:652/1770 train_time:64889ms step_avg:101.07ms
step:653/1770 train_time:64992ms step_avg:101.08ms
step:654/1770 train_time:65094ms step_avg:101.08ms
step:655/1770 train_time:65196ms step_avg:101.08ms
step:656/1770 train_time:65299ms step_avg:101.08ms
step:657/1770 train_time:65401ms step_avg:101.08ms
step:658/1770 train_time:65505ms step_avg:101.09ms
step:659/1770 train_time:65610ms step_avg:101.09ms
step:660/1770 train_time:65715ms step_avg:101.10ms
step:661/1770 train_time:65818ms step_avg:101.10ms
step:662/1770 train_time:65923ms step_avg:101.11ms
step:663/1770 train_time:66026ms step_avg:101.11ms
step:664/1770 train_time:66131ms step_avg:101.12ms
step:665/1770 train_time:66235ms step_avg:101.12ms
step:666/1770 train_time:66339ms step_avg:101.13ms
step:667/1770 train_time:66443ms step_avg:101.13ms
step:668/1770 train_time:66547ms step_avg:101.14ms
step:669/1770 train_time:66651ms step_avg:101.14ms
step:670/1770 train_time:66756ms step_avg:101.15ms
step:671/1770 train_time:66860ms step_avg:101.15ms
step:672/1770 train_time:66964ms step_avg:101.15ms
step:673/1770 train_time:67069ms step_avg:101.16ms
step:674/1770 train_time:67174ms step_avg:101.16ms
step:675/1770 train_time:67277ms step_avg:101.17ms
step:676/1770 train_time:67381ms step_avg:101.17ms
step:677/1770 train_time:67485ms step_avg:101.18ms
step:678/1770 train_time:67590ms step_avg:101.18ms
step:679/1770 train_time:67694ms step_avg:101.19ms
step:680/1770 train_time:67798ms step_avg:101.19ms
step:681/1770 train_time:67901ms step_avg:101.19ms
step:682/1770 train_time:68006ms step_avg:101.20ms
step:683/1770 train_time:68110ms step_avg:101.20ms
step:684/1770 train_time:68215ms step_avg:101.21ms
step:685/1770 train_time:68319ms step_avg:101.21ms
step:686/1770 train_time:68423ms step_avg:101.22ms
step:687/1770 train_time:68528ms step_avg:101.22ms
step:688/1770 train_time:68634ms step_avg:101.23ms
step:689/1770 train_time:68738ms step_avg:101.23ms
step:690/1770 train_time:68842ms step_avg:101.24ms
step:691/1770 train_time:68946ms step_avg:101.24ms
step:692/1770 train_time:69051ms step_avg:101.25ms
step:693/1770 train_time:69155ms step_avg:101.25ms
step:694/1770 train_time:69259ms step_avg:101.26ms
step:695/1770 train_time:69363ms step_avg:101.26ms
step:696/1770 train_time:69467ms step_avg:101.26ms
step:697/1770 train_time:69572ms step_avg:101.27ms
step:698/1770 train_time:69676ms step_avg:101.27ms
step:699/1770 train_time:69780ms step_avg:101.28ms
step:700/1770 train_time:69884ms step_avg:101.28ms
step:701/1770 train_time:69989ms step_avg:101.29ms
step:702/1770 train_time:70094ms step_avg:101.29ms
step:703/1770 train_time:70198ms step_avg:101.30ms
step:704/1770 train_time:70302ms step_avg:101.30ms
step:705/1770 train_time:70406ms step_avg:101.30ms
step:706/1770 train_time:70511ms step_avg:101.31ms
step:707/1770 train_time:70616ms step_avg:101.31ms
step:708/1770 train_time:70720ms step_avg:101.32ms
step:709/1770 train_time:70825ms step_avg:101.32ms
step:710/1770 train_time:70930ms step_avg:101.33ms
step:711/1770 train_time:71035ms step_avg:101.33ms
step:712/1770 train_time:71139ms step_avg:101.34ms
step:713/1770 train_time:71242ms step_avg:101.34ms
step:714/1770 train_time:71347ms step_avg:101.35ms
step:715/1770 train_time:71453ms step_avg:101.35ms
step:716/1770 train_time:71556ms step_avg:101.35ms
step:717/1770 train_time:71660ms step_avg:101.36ms
step:718/1770 train_time:71764ms step_avg:101.36ms
step:719/1770 train_time:71868ms step_avg:101.37ms
step:720/1770 train_time:71974ms step_avg:101.37ms
step:721/1770 train_time:72078ms step_avg:101.38ms
step:722/1770 train_time:72182ms step_avg:101.38ms
step:723/1770 train_time:72287ms step_avg:101.38ms
step:724/1770 train_time:72392ms step_avg:101.39ms
step:725/1770 train_time:72496ms step_avg:101.39ms
step:726/1770 train_time:72599ms step_avg:101.40ms
step:727/1770 train_time:72704ms step_avg:101.40ms
step:728/1770 train_time:72808ms step_avg:101.40ms
step:729/1770 train_time:72913ms step_avg:101.41ms
step:730/1770 train_time:73017ms step_avg:101.41ms
step:731/1770 train_time:73122ms step_avg:101.42ms
step:732/1770 train_time:73226ms step_avg:101.42ms
step:733/1770 train_time:73331ms step_avg:101.43ms
step:734/1770 train_time:73435ms step_avg:101.43ms
step:735/1770 train_time:73539ms step_avg:101.43ms
step:736/1770 train_time:73642ms step_avg:101.44ms
step:737/1770 train_time:73746ms step_avg:101.44ms
step:738/1770 train_time:73851ms step_avg:101.44ms
step:739/1770 train_time:73955ms step_avg:101.45ms
step:740/1770 train_time:74059ms step_avg:101.45ms
step:741/1770 train_time:74163ms step_avg:101.45ms
step:742/1770 train_time:74268ms step_avg:101.46ms
step:743/1770 train_time:74373ms step_avg:101.46ms
step:744/1770 train_time:74477ms step_avg:101.47ms
step:745/1770 train_time:74581ms step_avg:101.47ms
step:746/1770 train_time:74685ms step_avg:101.47ms
step:747/1770 train_time:74789ms step_avg:101.48ms
step:748/1770 train_time:74894ms step_avg:101.48ms
step:749/1770 train_time:74997ms step_avg:101.48ms
step:750/1770 train_time:75101ms step_avg:101.49ms
step:750/1770 val_loss:3.6000 train_time:75203ms step_avg:101.63ms
step:751/1770 train_time:75221ms step_avg:101.51ms
step:752/1770 train_time:75321ms step_avg:101.51ms
step:753/1770 train_time:75428ms step_avg:101.52ms
step:754/1770 train_time:75538ms step_avg:101.53ms
step:755/1770 train_time:75636ms step_avg:101.53ms
step:756/1770 train_time:75741ms step_avg:101.53ms
step:757/1770 train_time:75845ms step_avg:101.53ms
step:758/1770 train_time:75950ms step_avg:101.54ms
step:759/1770 train_time:76055ms step_avg:101.54ms
step:760/1770 train_time:76158ms step_avg:101.54ms
step:761/1770 train_time:76263ms step_avg:101.55ms
step:762/1770 train_time:76367ms step_avg:101.55ms
step:763/1770 train_time:76470ms step_avg:101.55ms
step:764/1770 train_time:76574ms step_avg:101.56ms
step:765/1770 train_time:76678ms step_avg:101.56ms
step:766/1770 train_time:76783ms step_avg:101.56ms
step:767/1770 train_time:76888ms step_avg:101.57ms
step:768/1770 train_time:76992ms step_avg:101.57ms
step:769/1770 train_time:77096ms step_avg:101.58ms
step:770/1770 train_time:77200ms step_avg:101.58ms
step:771/1770 train_time:77305ms step_avg:101.58ms
step:772/1770 train_time:77409ms step_avg:101.59ms
step:773/1770 train_time:77513ms step_avg:101.59ms
step:774/1770 train_time:77617ms step_avg:101.59ms
step:775/1770 train_time:77722ms step_avg:101.60ms
step:776/1770 train_time:77826ms step_avg:101.60ms
step:777/1770 train_time:77931ms step_avg:101.60ms
step:778/1770 train_time:78035ms step_avg:101.61ms
step:779/1770 train_time:78139ms step_avg:101.61ms
step:780/1770 train_time:78244ms step_avg:101.62ms
step:781/1770 train_time:78349ms step_avg:101.62ms
step:782/1770 train_time:78452ms step_avg:101.62ms
step:783/1770 train_time:78556ms step_avg:101.62ms
step:784/1770 train_time:78660ms step_avg:101.63ms
step:785/1770 train_time:78765ms step_avg:101.63ms
step:786/1770 train_time:78869ms step_avg:101.64ms
step:787/1770 train_time:78973ms step_avg:101.64ms
step:788/1770 train_time:79077ms step_avg:101.64ms
step:789/1770 train_time:79182ms step_avg:101.65ms
step:790/1770 train_time:79287ms step_avg:101.65ms
step:791/1770 train_time:79392ms step_avg:101.65ms
step:792/1770 train_time:79496ms step_avg:101.66ms
step:793/1770 train_time:79601ms step_avg:101.66ms
step:794/1770 train_time:79705ms step_avg:101.67ms
step:795/1770 train_time:79811ms step_avg:101.67ms
step:796/1770 train_time:79915ms step_avg:101.67ms
step:797/1770 train_time:80019ms step_avg:101.68ms
step:798/1770 train_time:80124ms step_avg:101.68ms
step:799/1770 train_time:80229ms step_avg:101.68ms
step:800/1770 train_time:80333ms step_avg:101.69ms
step:801/1770 train_time:80438ms step_avg:101.69ms
step:802/1770 train_time:80543ms step_avg:101.70ms
step:803/1770 train_time:80648ms step_avg:101.70ms
step:804/1770 train_time:80752ms step_avg:101.70ms
step:805/1770 train_time:80857ms step_avg:101.71ms
step:806/1770 train_time:80962ms step_avg:101.71ms
step:807/1770 train_time:81068ms step_avg:101.72ms
step:808/1770 train_time:81172ms step_avg:101.72ms
step:809/1770 train_time:81276ms step_avg:101.72ms
step:810/1770 train_time:81380ms step_avg:101.73ms
step:811/1770 train_time:81485ms step_avg:101.73ms
step:812/1770 train_time:81590ms step_avg:101.73ms
step:813/1770 train_time:81694ms step_avg:101.74ms
step:814/1770 train_time:81799ms step_avg:101.74ms
step:815/1770 train_time:81904ms step_avg:101.74ms
step:816/1770 train_time:82009ms step_avg:101.75ms
step:817/1770 train_time:82113ms step_avg:101.75ms
step:818/1770 train_time:82217ms step_avg:101.75ms
step:819/1770 train_time:82323ms step_avg:101.76ms
step:820/1770 train_time:82428ms step_avg:101.76ms
step:821/1770 train_time:82532ms step_avg:101.77ms
step:822/1770 train_time:82636ms step_avg:101.77ms
step:823/1770 train_time:82740ms step_avg:101.77ms
step:824/1770 train_time:82845ms step_avg:101.77ms
step:825/1770 train_time:82950ms step_avg:101.78ms
step:826/1770 train_time:83054ms step_avg:101.78ms
step:827/1770 train_time:83159ms step_avg:101.79ms
step:828/1770 train_time:83263ms step_avg:101.79ms
step:829/1770 train_time:83368ms step_avg:101.79ms
step:830/1770 train_time:83472ms step_avg:101.79ms
step:831/1770 train_time:83576ms step_avg:101.80ms
step:832/1770 train_time:83681ms step_avg:101.80ms
step:833/1770 train_time:83785ms step_avg:101.80ms
step:834/1770 train_time:83890ms step_avg:101.81ms
step:835/1770 train_time:83993ms step_avg:101.81ms
step:836/1770 train_time:84098ms step_avg:101.81ms
step:837/1770 train_time:84203ms step_avg:101.82ms
step:838/1770 train_time:84308ms step_avg:101.82ms
step:839/1770 train_time:84412ms step_avg:101.82ms
step:840/1770 train_time:84517ms step_avg:101.83ms
step:841/1770 train_time:84621ms step_avg:101.83ms
step:842/1770 train_time:84726ms step_avg:101.83ms
step:843/1770 train_time:84830ms step_avg:101.84ms
step:844/1770 train_time:84934ms step_avg:101.84ms
step:845/1770 train_time:85039ms step_avg:101.84ms
step:846/1770 train_time:85143ms step_avg:101.85ms
step:847/1770 train_time:85248ms step_avg:101.85ms
step:848/1770 train_time:85352ms step_avg:101.85ms
step:849/1770 train_time:85457ms step_avg:101.86ms
step:850/1770 train_time:85562ms step_avg:101.86ms
step:851/1770 train_time:85667ms step_avg:101.86ms
step:852/1770 train_time:85771ms step_avg:101.87ms
step:853/1770 train_time:85876ms step_avg:101.87ms
step:854/1770 train_time:85980ms step_avg:101.87ms
step:855/1770 train_time:86085ms step_avg:101.88ms
step:856/1770 train_time:86190ms step_avg:101.88ms
step:857/1770 train_time:86293ms step_avg:101.88ms
step:858/1770 train_time:86397ms step_avg:101.88ms
step:859/1770 train_time:86502ms step_avg:101.89ms
step:860/1770 train_time:86607ms step_avg:101.89ms
step:861/1770 train_time:86711ms step_avg:101.89ms
step:862/1770 train_time:86815ms step_avg:101.90ms
step:863/1770 train_time:86919ms step_avg:101.90ms
step:864/1770 train_time:87024ms step_avg:101.90ms
step:865/1770 train_time:87129ms step_avg:101.91ms
step:866/1770 train_time:87234ms step_avg:101.91ms
step:867/1770 train_time:87338ms step_avg:101.91ms
step:868/1770 train_time:87443ms step_avg:101.92ms
step:869/1770 train_time:87549ms step_avg:101.92ms
step:870/1770 train_time:87653ms step_avg:101.92ms
step:871/1770 train_time:87757ms step_avg:101.92ms
step:872/1770 train_time:87863ms step_avg:101.93ms
step:873/1770 train_time:87967ms step_avg:101.93ms
step:874/1770 train_time:88071ms step_avg:101.93ms
step:875/1770 train_time:88175ms step_avg:101.94ms
step:875/1770 val_loss:3.5515 train_time:88278ms step_avg:102.06ms
step:876/1770 train_time:88296ms step_avg:101.96ms
step:877/1770 train_time:88396ms step_avg:101.96ms
step:878/1770 train_time:88505ms step_avg:101.96ms
step:879/1770 train_time:88610ms step_avg:101.97ms
step:880/1770 train_time:88715ms step_avg:101.97ms
step:881/1770 train_time:88820ms step_avg:101.97ms
step:882/1770 train_time:88924ms step_avg:101.98ms
step:883/1770 train_time:89029ms step_avg:101.98ms
step:884/1770 train_time:89133ms step_avg:101.98ms
step:885/1770 train_time:89238ms step_avg:101.99ms
step:886/1770 train_time:89341ms step_avg:101.99ms
step:887/1770 train_time:89445ms step_avg:101.99ms
step:888/1770 train_time:89550ms step_avg:101.99ms
step:889/1770 train_time:89654ms step_avg:102.00ms
step:890/1770 train_time:89759ms step_avg:102.00ms
step:891/1770 train_time:89864ms step_avg:102.00ms
step:892/1770 train_time:89968ms step_avg:102.00ms
step:893/1770 train_time:90073ms step_avg:102.01ms
step:894/1770 train_time:90177ms step_avg:102.01ms
step:895/1770 train_time:90282ms step_avg:102.01ms
step:896/1770 train_time:90386ms step_avg:102.02ms
step:897/1770 train_time:90491ms step_avg:102.02ms
step:898/1770 train_time:90597ms step_avg:102.02ms
step:899/1770 train_time:90701ms step_avg:102.03ms
step:900/1770 train_time:90806ms step_avg:102.03ms
step:901/1770 train_time:90910ms step_avg:102.03ms
step:902/1770 train_time:91015ms step_avg:102.03ms
step:903/1770 train_time:91120ms step_avg:102.04ms
step:904/1770 train_time:91224ms step_avg:102.04ms
step:905/1770 train_time:91328ms step_avg:102.04ms
step:906/1770 train_time:91434ms step_avg:102.05ms
step:907/1770 train_time:91539ms step_avg:102.05ms
step:908/1770 train_time:91643ms step_avg:102.05ms
step:909/1770 train_time:91747ms step_avg:102.05ms
step:910/1770 train_time:91852ms step_avg:102.06ms
step:911/1770 train_time:91956ms step_avg:102.06ms
step:912/1770 train_time:92061ms step_avg:102.06ms
step:913/1770 train_time:92165ms step_avg:102.07ms
step:914/1770 train_time:92270ms step_avg:102.07ms
step:915/1770 train_time:92375ms step_avg:102.07ms
step:916/1770 train_time:92480ms step_avg:102.07ms
step:917/1770 train_time:92584ms step_avg:102.08ms
step:918/1770 train_time:92688ms step_avg:102.08ms
step:919/1770 train_time:92792ms step_avg:102.08ms
step:920/1770 train_time:92899ms step_avg:102.09ms
step:921/1770 train_time:93005ms step_avg:102.09ms
step:922/1770 train_time:93115ms step_avg:102.10ms
step:923/1770 train_time:93217ms step_avg:102.10ms
step:924/1770 train_time:93323ms step_avg:102.10ms
step:925/1770 train_time:93428ms step_avg:102.11ms
step:926/1770 train_time:93535ms step_avg:102.11ms
step:927/1770 train_time:93640ms step_avg:102.12ms
step:928/1770 train_time:93746ms step_avg:102.12ms
step:929/1770 train_time:93851ms step_avg:102.12ms
step:930/1770 train_time:93957ms step_avg:102.13ms
step:931/1770 train_time:94063ms step_avg:102.13ms
step:932/1770 train_time:94169ms step_avg:102.14ms
step:933/1770 train_time:94276ms step_avg:102.14ms
step:934/1770 train_time:94383ms step_avg:102.15ms
step:935/1770 train_time:94488ms step_avg:102.15ms
step:936/1770 train_time:94594ms step_avg:102.15ms
step:937/1770 train_time:94700ms step_avg:102.16ms
step:938/1770 train_time:94806ms step_avg:102.16ms
step:939/1770 train_time:94912ms step_avg:102.17ms
step:940/1770 train_time:95018ms step_avg:102.17ms
step:941/1770 train_time:95123ms step_avg:102.17ms
step:942/1770 train_time:95230ms step_avg:102.18ms
step:943/1770 train_time:95336ms step_avg:102.18ms
step:944/1770 train_time:95442ms step_avg:102.19ms
step:945/1770 train_time:95547ms step_avg:102.19ms
step:946/1770 train_time:95654ms step_avg:102.19ms
step:947/1770 train_time:95760ms step_avg:102.20ms
step:948/1770 train_time:95866ms step_avg:102.20ms
step:949/1770 train_time:95971ms step_avg:102.21ms
step:950/1770 train_time:96078ms step_avg:102.21ms
step:951/1770 train_time:96184ms step_avg:102.21ms
step:952/1770 train_time:96290ms step_avg:102.22ms
step:953/1770 train_time:96396ms step_avg:102.22ms
step:954/1770 train_time:96502ms step_avg:102.23ms
step:955/1770 train_time:96608ms step_avg:102.23ms
step:956/1770 train_time:96714ms step_avg:102.23ms
step:957/1770 train_time:96820ms step_avg:102.24ms
step:958/1770 train_time:96926ms step_avg:102.24ms
step:959/1770 train_time:97032ms step_avg:102.25ms
step:960/1770 train_time:97137ms step_avg:102.25ms
step:961/1770 train_time:97243ms step_avg:102.25ms
step:962/1770 train_time:97350ms step_avg:102.26ms
step:963/1770 train_time:97456ms step_avg:102.26ms
step:964/1770 train_time:97563ms step_avg:102.27ms
step:965/1770 train_time:97668ms step_avg:102.27ms
step:966/1770 train_time:97774ms step_avg:102.27ms
step:967/1770 train_time:97881ms step_avg:102.28ms
step:968/1770 train_time:97987ms step_avg:102.28ms
step:969/1770 train_time:98093ms step_avg:102.29ms
step:970/1770 train_time:98199ms step_avg:102.29ms
step:971/1770 train_time:98305ms step_avg:102.29ms
step:972/1770 train_time:98410ms step_avg:102.30ms
step:973/1770 train_time:98516ms step_avg:102.30ms
step:974/1770 train_time:98623ms step_avg:102.31ms
step:975/1770 train_time:98729ms step_avg:102.31ms
step:976/1770 train_time:98834ms step_avg:102.31ms
step:977/1770 train_time:98941ms step_avg:102.32ms
step:978/1770 train_time:99047ms step_avg:102.32ms
step:979/1770 train_time:99153ms step_avg:102.33ms
step:980/1770 train_time:99260ms step_avg:102.33ms
step:981/1770 train_time:99365ms step_avg:102.33ms
step:982/1770 train_time:99471ms step_avg:102.34ms
step:983/1770 train_time:99577ms step_avg:102.34ms
step:984/1770 train_time:99683ms step_avg:102.34ms
step:985/1770 train_time:99789ms step_avg:102.35ms
step:986/1770 train_time:99894ms step_avg:102.35ms
step:987/1770 train_time:100001ms step_avg:102.36ms
step:988/1770 train_time:100106ms step_avg:102.36ms
step:989/1770 train_time:100213ms step_avg:102.36ms
step:990/1770 train_time:100320ms step_avg:102.37ms
step:991/1770 train_time:100425ms step_avg:102.37ms
step:992/1770 train_time:100530ms step_avg:102.37ms
step:993/1770 train_time:100637ms step_avg:102.38ms
step:994/1770 train_time:100743ms step_avg:102.38ms
step:995/1770 train_time:100849ms step_avg:102.38ms
step:996/1770 train_time:100954ms step_avg:102.39ms
step:997/1770 train_time:101061ms step_avg:102.39ms
step:998/1770 train_time:101166ms step_avg:102.39ms
step:999/1770 train_time:101272ms step_avg:102.40ms
step:1000/1770 train_time:101378ms step_avg:102.40ms
step:1000/1770 val_loss:3.5133 train_time:101482ms step_avg:102.51ms
step:1001/1770 train_time:101500ms step_avg:102.42ms
step:1002/1770 train_time:101598ms step_avg:102.42ms
step:1003/1770 train_time:101707ms step_avg:102.42ms
step:1004/1770 train_time:101813ms step_avg:102.43ms
step:1005/1770 train_time:101919ms step_avg:102.43ms
step:1006/1770 train_time:102025ms step_avg:102.43ms
step:1007/1770 train_time:102131ms step_avg:102.44ms
step:1008/1770 train_time:102236ms step_avg:102.44ms
step:1009/1770 train_time:102342ms step_avg:102.44ms
step:1010/1770 train_time:102448ms step_avg:102.45ms
step:1011/1770 train_time:102554ms step_avg:102.45ms
step:1012/1770 train_time:102660ms step_avg:102.46ms
step:1013/1770 train_time:102767ms step_avg:102.46ms
step:1014/1770 train_time:102873ms step_avg:102.46ms
step:1015/1770 train_time:102978ms step_avg:102.47ms
step:1016/1770 train_time:103085ms step_avg:102.47ms
step:1017/1770 train_time:103190ms step_avg:102.47ms
step:1018/1770 train_time:103296ms step_avg:102.48ms
step:1019/1770 train_time:103402ms step_avg:102.48ms
step:1020/1770 train_time:103509ms step_avg:102.48ms
step:1021/1770 train_time:103615ms step_avg:102.49ms
step:1022/1770 train_time:103721ms step_avg:102.49ms
step:1023/1770 train_time:103827ms step_avg:102.49ms
step:1024/1770 train_time:103933ms step_avg:102.50ms
step:1025/1770 train_time:104039ms step_avg:102.50ms
step:1026/1770 train_time:104146ms step_avg:102.51ms
step:1027/1770 train_time:104253ms step_avg:102.51ms
step:1028/1770 train_time:104359ms step_avg:102.51ms
step:1029/1770 train_time:104465ms step_avg:102.52ms
step:1030/1770 train_time:104572ms step_avg:102.52ms
step:1031/1770 train_time:104677ms step_avg:102.52ms
step:1032/1770 train_time:104782ms step_avg:102.53ms
step:1033/1770 train_time:104888ms step_avg:102.53ms
step:1034/1770 train_time:104994ms step_avg:102.53ms
step:1035/1770 train_time:105101ms step_avg:102.54ms
step:1036/1770 train_time:105207ms step_avg:102.54ms
step:1037/1770 train_time:105313ms step_avg:102.54ms
step:1038/1770 train_time:105419ms step_avg:102.55ms
step:1039/1770 train_time:105526ms step_avg:102.55ms
step:1040/1770 train_time:105632ms step_avg:102.55ms
step:1041/1770 train_time:105737ms step_avg:102.56ms
step:1042/1770 train_time:105843ms step_avg:102.56ms
step:1043/1770 train_time:105950ms step_avg:102.57ms
step:1044/1770 train_time:106056ms step_avg:102.57ms
step:1045/1770 train_time:106162ms step_avg:102.57ms
step:1046/1770 train_time:106268ms step_avg:102.58ms
step:1047/1770 train_time:106374ms step_avg:102.58ms
step:1048/1770 train_time:106480ms step_avg:102.58ms
step:1049/1770 train_time:106585ms step_avg:102.58ms
step:1050/1770 train_time:106691ms step_avg:102.59ms
step:1051/1770 train_time:106797ms step_avg:102.59ms
step:1052/1770 train_time:106902ms step_avg:102.59ms
step:1053/1770 train_time:107009ms step_avg:102.60ms
step:1054/1770 train_time:107114ms step_avg:102.60ms
step:1055/1770 train_time:107222ms step_avg:102.60ms
step:1056/1770 train_time:107327ms step_avg:102.61ms
step:1057/1770 train_time:107433ms step_avg:102.61ms
step:1058/1770 train_time:107539ms step_avg:102.61ms
step:1059/1770 train_time:107645ms step_avg:102.62ms
step:1060/1770 train_time:107752ms step_avg:102.62ms
step:1061/1770 train_time:107858ms step_avg:102.62ms
step:1062/1770 train_time:107964ms step_avg:102.63ms
step:1063/1770 train_time:108071ms step_avg:102.63ms
step:1064/1770 train_time:108177ms step_avg:102.64ms
step:1065/1770 train_time:108284ms step_avg:102.64ms
step:1066/1770 train_time:108390ms step_avg:102.64ms
step:1067/1770 train_time:108497ms step_avg:102.65ms
step:1068/1770 train_time:108604ms step_avg:102.65ms
step:1069/1770 train_time:108711ms step_avg:102.65ms
step:1070/1770 train_time:108816ms step_avg:102.66ms
step:1071/1770 train_time:108923ms step_avg:102.66ms
step:1072/1770 train_time:109029ms step_avg:102.66ms
step:1073/1770 train_time:109135ms step_avg:102.67ms
step:1074/1770 train_time:109241ms step_avg:102.67ms
step:1075/1770 train_time:109347ms step_avg:102.67ms
step:1076/1770 train_time:109454ms step_avg:102.68ms
step:1077/1770 train_time:109559ms step_avg:102.68ms
step:1078/1770 train_time:109666ms step_avg:102.68ms
step:1079/1770 train_time:109773ms step_avg:102.69ms
step:1080/1770 train_time:109880ms step_avg:102.69ms
step:1081/1770 train_time:109986ms step_avg:102.69ms
step:1082/1770 train_time:110093ms step_avg:102.70ms
step:1083/1770 train_time:110198ms step_avg:102.70ms
step:1084/1770 train_time:110305ms step_avg:102.70ms
step:1085/1770 train_time:110412ms step_avg:102.71ms
step:1086/1770 train_time:110517ms step_avg:102.71ms
step:1087/1770 train_time:110623ms step_avg:102.71ms
step:1088/1770 train_time:110730ms step_avg:102.72ms
step:1089/1770 train_time:110836ms step_avg:102.72ms
step:1090/1770 train_time:110943ms step_avg:102.73ms
step:1091/1770 train_time:111049ms step_avg:102.73ms
step:1092/1770 train_time:111155ms step_avg:102.73ms
step:1093/1770 train_time:111265ms step_avg:102.74ms
step:1094/1770 train_time:111369ms step_avg:102.74ms
step:1095/1770 train_time:111475ms step_avg:102.74ms
step:1096/1770 train_time:111581ms step_avg:102.74ms
step:1097/1770 train_time:111687ms step_avg:102.75ms
step:1098/1770 train_time:111793ms step_avg:102.75ms
step:1099/1770 train_time:111899ms step_avg:102.75ms
step:1100/1770 train_time:112006ms step_avg:102.76ms
step:1101/1770 train_time:112112ms step_avg:102.76ms
step:1102/1770 train_time:112218ms step_avg:102.76ms
step:1103/1770 train_time:112325ms step_avg:102.77ms
step:1104/1770 train_time:112432ms step_avg:102.77ms
step:1105/1770 train_time:112538ms step_avg:102.77ms
step:1106/1770 train_time:112645ms step_avg:102.78ms
step:1107/1770 train_time:112751ms step_avg:102.78ms
step:1108/1770 train_time:112857ms step_avg:102.78ms
step:1109/1770 train_time:112969ms step_avg:102.79ms
step:1110/1770 train_time:113070ms step_avg:102.79ms
step:1111/1770 train_time:113176ms step_avg:102.79ms
step:1112/1770 train_time:113283ms step_avg:102.80ms
step:1113/1770 train_time:113388ms step_avg:102.80ms
step:1114/1770 train_time:113494ms step_avg:102.80ms
step:1115/1770 train_time:113600ms step_avg:102.81ms
step:1116/1770 train_time:113707ms step_avg:102.81ms
step:1117/1770 train_time:113813ms step_avg:102.81ms
step:1118/1770 train_time:113919ms step_avg:102.81ms
step:1119/1770 train_time:114026ms step_avg:102.82ms
step:1120/1770 train_time:114132ms step_avg:102.82ms
step:1121/1770 train_time:114238ms step_avg:102.82ms
step:1122/1770 train_time:114345ms step_avg:102.83ms
step:1123/1770 train_time:114451ms step_avg:102.83ms
step:1124/1770 train_time:114557ms step_avg:102.83ms
step:1125/1770 train_time:114663ms step_avg:102.84ms
step:1125/1770 val_loss:3.4709 train_time:114767ms step_avg:102.93ms
step:1126/1770 train_time:114785ms step_avg:102.85ms
step:1127/1770 train_time:114887ms step_avg:102.85ms
step:1128/1770 train_time:114994ms step_avg:102.86ms
step:1129/1770 train_time:115100ms step_avg:102.86ms
step:1130/1770 train_time:115207ms step_avg:102.86ms
step:1131/1770 train_time:115313ms step_avg:102.87ms
step:1132/1770 train_time:115420ms step_avg:102.87ms
step:1133/1770 train_time:115526ms step_avg:102.87ms
step:1134/1770 train_time:115632ms step_avg:102.88ms
step:1135/1770 train_time:115738ms step_avg:102.88ms
step:1136/1770 train_time:115844ms step_avg:102.88ms
step:1137/1770 train_time:115951ms step_avg:102.88ms
step:1138/1770 train_time:116057ms step_avg:102.89ms
step:1139/1770 train_time:116163ms step_avg:102.89ms
step:1140/1770 train_time:116269ms step_avg:102.89ms
step:1141/1770 train_time:116374ms step_avg:102.89ms
step:1142/1770 train_time:116480ms step_avg:102.90ms
step:1143/1770 train_time:116587ms step_avg:102.90ms
step:1144/1770 train_time:116693ms step_avg:102.90ms
step:1145/1770 train_time:116799ms step_avg:102.91ms
step:1146/1770 train_time:116906ms step_avg:102.91ms
step:1147/1770 train_time:117012ms step_avg:102.91ms
step:1148/1770 train_time:117119ms step_avg:102.92ms
step:1149/1770 train_time:117225ms step_avg:102.92ms
step:1150/1770 train_time:117330ms step_avg:102.92ms
step:1151/1770 train_time:117436ms step_avg:102.92ms
step:1152/1770 train_time:117543ms step_avg:102.93ms
step:1153/1770 train_time:117648ms step_avg:102.93ms
step:1154/1770 train_time:117754ms step_avg:102.93ms
step:1155/1770 train_time:117860ms step_avg:102.93ms
step:1156/1770 train_time:117966ms step_avg:102.94ms
step:1157/1770 train_time:118074ms step_avg:102.94ms
step:1158/1770 train_time:118181ms step_avg:102.94ms
step:1159/1770 train_time:118287ms step_avg:102.95ms
step:1160/1770 train_time:118392ms step_avg:102.95ms
step:1161/1770 train_time:118499ms step_avg:102.95ms
step:1162/1770 train_time:118606ms step_avg:102.96ms
step:1163/1770 train_time:118712ms step_avg:102.96ms
step:1164/1770 train_time:118817ms step_avg:102.96ms
step:1165/1770 train_time:118924ms step_avg:102.96ms
step:1166/1770 train_time:119031ms step_avg:102.97ms
step:1167/1770 train_time:119137ms step_avg:102.97ms
step:1168/1770 train_time:119242ms step_avg:102.97ms
step:1169/1770 train_time:119348ms step_avg:102.97ms
step:1170/1770 train_time:119454ms step_avg:102.98ms
step:1171/1770 train_time:119561ms step_avg:102.98ms
step:1172/1770 train_time:119667ms step_avg:102.98ms
step:1173/1770 train_time:119773ms step_avg:102.99ms
step:1174/1770 train_time:119879ms step_avg:102.99ms
step:1175/1770 train_time:119986ms step_avg:102.99ms
step:1176/1770 train_time:120091ms step_avg:102.99ms
step:1177/1770 train_time:120197ms step_avg:103.00ms
step:1178/1770 train_time:120304ms step_avg:103.00ms
step:1179/1770 train_time:120410ms step_avg:103.00ms
step:1180/1770 train_time:120516ms step_avg:103.01ms
step:1181/1770 train_time:120623ms step_avg:103.01ms
step:1182/1770 train_time:120729ms step_avg:103.01ms
step:1183/1770 train_time:120836ms step_avg:103.01ms
step:1184/1770 train_time:120946ms step_avg:103.02ms
step:1185/1770 train_time:121052ms step_avg:103.02ms
step:1186/1770 train_time:121161ms step_avg:103.03ms
step:1187/1770 train_time:121270ms step_avg:103.03ms
step:1188/1770 train_time:121377ms step_avg:103.04ms
step:1189/1770 train_time:121484ms step_avg:103.04ms
step:1190/1770 train_time:121590ms step_avg:103.04ms
step:1191/1770 train_time:121698ms step_avg:103.05ms
step:1192/1770 train_time:121806ms step_avg:103.05ms
step:1193/1770 train_time:121913ms step_avg:103.05ms
step:1194/1770 train_time:122021ms step_avg:103.06ms
step:1195/1770 train_time:122133ms step_avg:103.07ms
step:1196/1770 train_time:122237ms step_avg:103.07ms
step:1197/1770 train_time:122344ms step_avg:103.07ms
step:1198/1770 train_time:122451ms step_avg:103.07ms
step:1199/1770 train_time:122559ms step_avg:103.08ms
step:1200/1770 train_time:122667ms step_avg:103.08ms
step:1201/1770 train_time:122774ms step_avg:103.09ms
step:1202/1770 train_time:122882ms step_avg:103.09ms
step:1203/1770 train_time:122989ms step_avg:103.09ms
step:1204/1770 train_time:123097ms step_avg:103.10ms
step:1205/1770 train_time:123204ms step_avg:103.10ms
step:1206/1770 train_time:123312ms step_avg:103.10ms
step:1207/1770 train_time:123419ms step_avg:103.11ms
step:1208/1770 train_time:123528ms step_avg:103.11ms
step:1209/1770 train_time:123637ms step_avg:103.12ms
step:1210/1770 train_time:123742ms step_avg:103.12ms
step:1211/1770 train_time:123848ms step_avg:103.12ms
step:1212/1770 train_time:123958ms step_avg:103.13ms
step:1213/1770 train_time:124065ms step_avg:103.13ms
step:1214/1770 train_time:124173ms step_avg:103.13ms
step:1215/1770 train_time:124280ms step_avg:103.14ms
step:1216/1770 train_time:124390ms step_avg:103.14ms
step:1217/1770 train_time:124497ms step_avg:103.15ms
step:1218/1770 train_time:124604ms step_avg:103.15ms
step:1219/1770 train_time:124713ms step_avg:103.15ms
step:1220/1770 train_time:124821ms step_avg:103.16ms
step:1221/1770 train_time:124928ms step_avg:103.16ms
step:1222/1770 train_time:125038ms step_avg:103.17ms
step:1223/1770 train_time:125143ms step_avg:103.17ms
step:1224/1770 train_time:125251ms step_avg:103.17ms
step:1225/1770 train_time:125359ms step_avg:103.18ms
step:1226/1770 train_time:125467ms step_avg:103.18ms
step:1227/1770 train_time:125576ms step_avg:103.18ms
step:1228/1770 train_time:125685ms step_avg:103.19ms
step:1229/1770 train_time:125792ms step_avg:103.19ms
step:1230/1770 train_time:125899ms step_avg:103.20ms
step:1231/1770 train_time:126008ms step_avg:103.20ms
step:1232/1770 train_time:126115ms step_avg:103.20ms
step:1233/1770 train_time:126223ms step_avg:103.21ms
step:1234/1770 train_time:126332ms step_avg:103.21ms
step:1235/1770 train_time:126436ms step_avg:103.21ms
step:1236/1770 train_time:126544ms step_avg:103.22ms
step:1237/1770 train_time:126651ms step_avg:103.22ms
step:1238/1770 train_time:126758ms step_avg:103.22ms
step:1239/1770 train_time:126867ms step_avg:103.23ms
step:1240/1770 train_time:126974ms step_avg:103.23ms
step:1241/1770 train_time:127082ms step_avg:103.23ms
step:1242/1770 train_time:127189ms step_avg:103.24ms
step:1243/1770 train_time:127296ms step_avg:103.24ms
step:1244/1770 train_time:127403ms step_avg:103.24ms
step:1245/1770 train_time:127511ms step_avg:103.25ms
step:1246/1770 train_time:127620ms step_avg:103.25ms
step:1247/1770 train_time:127728ms step_avg:103.26ms
step:1248/1770 train_time:127837ms step_avg:103.26ms
step:1249/1770 train_time:127944ms step_avg:103.26ms
step:1250/1770 train_time:128051ms step_avg:103.27ms
step:1250/1770 val_loss:3.4234 train_time:128158ms step_avg:103.35ms
step:1251/1770 train_time:128176ms step_avg:103.28ms
step:1252/1770 train_time:128279ms step_avg:103.28ms
step:1253/1770 train_time:128389ms step_avg:103.29ms
step:1254/1770 train_time:128497ms step_avg:103.29ms
step:1255/1770 train_time:128606ms step_avg:103.30ms
step:1256/1770 train_time:128713ms step_avg:103.30ms
step:1257/1770 train_time:128820ms step_avg:103.30ms
step:1258/1770 train_time:128927ms step_avg:103.31ms
step:1259/1770 train_time:129036ms step_avg:103.31ms
step:1260/1770 train_time:129142ms step_avg:103.31ms
step:1261/1770 train_time:129252ms step_avg:103.32ms
step:1262/1770 train_time:129360ms step_avg:103.32ms
step:1263/1770 train_time:129467ms step_avg:103.33ms
step:1264/1770 train_time:129577ms step_avg:103.33ms
step:1265/1770 train_time:129684ms step_avg:103.33ms
step:1266/1770 train_time:129792ms step_avg:103.34ms
step:1267/1770 train_time:129899ms step_avg:103.34ms
step:1268/1770 train_time:130007ms step_avg:103.34ms
step:1269/1770 train_time:130115ms step_avg:103.35ms
step:1270/1770 train_time:130224ms step_avg:103.35ms
step:1271/1770 train_time:130332ms step_avg:103.36ms
step:1272/1770 train_time:130439ms step_avg:103.36ms
step:1273/1770 train_time:130547ms step_avg:103.36ms
step:1274/1770 train_time:130656ms step_avg:103.37ms
step:1275/1770 train_time:130764ms step_avg:103.37ms
step:1276/1770 train_time:130871ms step_avg:103.37ms
step:1277/1770 train_time:130978ms step_avg:103.38ms
step:1278/1770 train_time:131085ms step_avg:103.38ms
step:1279/1770 train_time:131194ms step_avg:103.38ms
step:1280/1770 train_time:131302ms step_avg:103.39ms
step:1281/1770 train_time:131409ms step_avg:103.39ms
step:1282/1770 train_time:131518ms step_avg:103.39ms
step:1283/1770 train_time:131625ms step_avg:103.40ms
step:1284/1770 train_time:131733ms step_avg:103.40ms
step:1285/1770 train_time:131840ms step_avg:103.40ms
step:1286/1770 train_time:131949ms step_avg:103.41ms
step:1287/1770 train_time:132058ms step_avg:103.41ms
step:1288/1770 train_time:132166ms step_avg:103.42ms
step:1289/1770 train_time:132278ms step_avg:103.42ms
step:1290/1770 train_time:132380ms step_avg:103.42ms
step:1291/1770 train_time:132488ms step_avg:103.43ms
step:1292/1770 train_time:132596ms step_avg:103.43ms
step:1293/1770 train_time:132703ms step_avg:103.43ms
step:1294/1770 train_time:132811ms step_avg:103.44ms
step:1295/1770 train_time:132919ms step_avg:103.44ms
step:1296/1770 train_time:133026ms step_avg:103.44ms
step:1297/1770 train_time:133133ms step_avg:103.44ms
step:1298/1770 train_time:133241ms step_avg:103.45ms
step:1299/1770 train_time:133348ms step_avg:103.45ms
step:1300/1770 train_time:133455ms step_avg:103.45ms
step:1301/1770 train_time:133563ms step_avg:103.46ms
step:1302/1770 train_time:133671ms step_avg:103.46ms
step:1303/1770 train_time:133779ms step_avg:103.46ms
step:1304/1770 train_time:133886ms step_avg:103.47ms
step:1305/1770 train_time:133993ms step_avg:103.47ms
step:1306/1770 train_time:134101ms step_avg:103.47ms
step:1307/1770 train_time:134208ms step_avg:103.48ms
step:1308/1770 train_time:134316ms step_avg:103.48ms
step:1309/1770 train_time:134423ms step_avg:103.48ms
step:1310/1770 train_time:134531ms step_avg:103.49ms
step:1311/1770 train_time:134638ms step_avg:103.49ms
step:1312/1770 train_time:134745ms step_avg:103.49ms
step:1313/1770 train_time:134852ms step_avg:103.49ms
step:1314/1770 train_time:134960ms step_avg:103.50ms
step:1315/1770 train_time:135067ms step_avg:103.50ms
step:1316/1770 train_time:135175ms step_avg:103.50ms
step:1317/1770 train_time:135282ms step_avg:103.51ms
step:1318/1770 train_time:135392ms step_avg:103.51ms
step:1319/1770 train_time:135500ms step_avg:103.51ms
step:1320/1770 train_time:135607ms step_avg:103.52ms
step:1321/1770 train_time:135715ms step_avg:103.52ms
step:1322/1770 train_time:135822ms step_avg:103.52ms
step:1323/1770 train_time:135930ms step_avg:103.53ms
step:1324/1770 train_time:136039ms step_avg:103.53ms
step:1325/1770 train_time:136147ms step_avg:103.53ms
step:1326/1770 train_time:136255ms step_avg:103.54ms
step:1327/1770 train_time:136365ms step_avg:103.54ms
step:1328/1770 train_time:136472ms step_avg:103.54ms
step:1329/1770 train_time:136579ms step_avg:103.55ms
step:1330/1770 train_time:136687ms step_avg:103.55ms
step:1331/1770 train_time:136794ms step_avg:103.55ms
step:1332/1770 train_time:136901ms step_avg:103.56ms
step:1333/1770 train_time:137008ms step_avg:103.56ms
step:1334/1770 train_time:137116ms step_avg:103.56ms
step:1335/1770 train_time:137223ms step_avg:103.56ms
step:1336/1770 train_time:137330ms step_avg:103.57ms
step:1337/1770 train_time:137439ms step_avg:103.57ms
step:1338/1770 train_time:137546ms step_avg:103.57ms
step:1339/1770 train_time:137654ms step_avg:103.58ms
step:1340/1770 train_time:137764ms step_avg:103.58ms
step:1341/1770 train_time:137871ms step_avg:103.58ms
step:1342/1770 train_time:137980ms step_avg:103.59ms
step:1343/1770 train_time:138089ms step_avg:103.59ms
step:1344/1770 train_time:138197ms step_avg:103.60ms
step:1345/1770 train_time:138305ms step_avg:103.60ms
step:1346/1770 train_time:138413ms step_avg:103.60ms
step:1347/1770 train_time:138520ms step_avg:103.61ms
step:1348/1770 train_time:138630ms step_avg:103.61ms
step:1349/1770 train_time:138738ms step_avg:103.61ms
step:1350/1770 train_time:138845ms step_avg:103.62ms
step:1351/1770 train_time:138953ms step_avg:103.62ms
step:1352/1770 train_time:139060ms step_avg:103.62ms
step:1353/1770 train_time:139169ms step_avg:103.63ms
step:1354/1770 train_time:139276ms step_avg:103.63ms
step:1355/1770 train_time:139383ms step_avg:103.63ms
step:1356/1770 train_time:139491ms step_avg:103.63ms
step:1357/1770 train_time:139599ms step_avg:103.64ms
step:1358/1770 train_time:139706ms step_avg:103.64ms
step:1359/1770 train_time:139814ms step_avg:103.64ms
step:1360/1770 train_time:139922ms step_avg:103.65ms
step:1361/1770 train_time:140031ms step_avg:103.65ms
step:1362/1770 train_time:140138ms step_avg:103.65ms
step:1363/1770 train_time:140247ms step_avg:103.66ms
step:1364/1770 train_time:140355ms step_avg:103.66ms
step:1365/1770 train_time:140462ms step_avg:103.66ms
step:1366/1770 train_time:140569ms step_avg:103.66ms
step:1367/1770 train_time:140678ms step_avg:103.67ms
step:1368/1770 train_time:140786ms step_avg:103.67ms
step:1369/1770 train_time:140894ms step_avg:103.68ms
step:1370/1770 train_time:141003ms step_avg:103.68ms
step:1371/1770 train_time:141110ms step_avg:103.68ms
step:1372/1770 train_time:141217ms step_avg:103.68ms
step:1373/1770 train_time:141325ms step_avg:103.69ms
step:1374/1770 train_time:141434ms step_avg:103.69ms
step:1375/1770 train_time:141541ms step_avg:103.69ms
step:1375/1770 val_loss:3.3796 train_time:141649ms step_avg:103.77ms
step:1376/1770 train_time:141666ms step_avg:103.71ms
step:1377/1770 train_time:141769ms step_avg:103.71ms
step:1378/1770 train_time:141878ms step_avg:103.71ms
step:1379/1770 train_time:141985ms step_avg:103.71ms
step:1380/1770 train_time:142093ms step_avg:103.72ms
step:1381/1770 train_time:142202ms step_avg:103.72ms
step:1382/1770 train_time:142309ms step_avg:103.72ms
step:1383/1770 train_time:142417ms step_avg:103.73ms
step:1384/1770 train_time:142525ms step_avg:103.73ms
step:1385/1770 train_time:142632ms step_avg:103.73ms
step:1386/1770 train_time:142741ms step_avg:103.74ms
step:1387/1770 train_time:142849ms step_avg:103.74ms
step:1388/1770 train_time:142957ms step_avg:103.74ms
step:1389/1770 train_time:143065ms step_avg:103.75ms
step:1390/1770 train_time:143172ms step_avg:103.75ms
step:1391/1770 train_time:143279ms step_avg:103.75ms
step:1392/1770 train_time:143387ms step_avg:103.75ms
step:1393/1770 train_time:143494ms step_avg:103.76ms
step:1394/1770 train_time:143601ms step_avg:103.76ms
step:1395/1770 train_time:143711ms step_avg:103.76ms
step:1396/1770 train_time:143819ms step_avg:103.77ms
step:1397/1770 train_time:143932ms step_avg:103.77ms
step:1398/1770 train_time:144034ms step_avg:103.77ms
step:1399/1770 train_time:144141ms step_avg:103.77ms
step:1400/1770 train_time:144249ms step_avg:103.78ms
step:1401/1770 train_time:144356ms step_avg:103.78ms
step:1402/1770 train_time:144464ms step_avg:103.78ms
step:1403/1770 train_time:144572ms step_avg:103.78ms
step:1404/1770 train_time:144680ms step_avg:103.79ms
step:1405/1770 train_time:144787ms step_avg:103.79ms
step:1406/1770 train_time:144895ms step_avg:103.79ms
step:1407/1770 train_time:145002ms step_avg:103.80ms
step:1408/1770 train_time:145110ms step_avg:103.80ms
step:1409/1770 train_time:145217ms step_avg:103.80ms
step:1410/1770 train_time:145325ms step_avg:103.80ms
step:1411/1770 train_time:145432ms step_avg:103.81ms
step:1412/1770 train_time:145541ms step_avg:103.81ms
step:1413/1770 train_time:145646ms step_avg:103.81ms
step:1414/1770 train_time:145754ms step_avg:103.81ms
step:1415/1770 train_time:145863ms step_avg:103.82ms
step:1416/1770 train_time:145971ms step_avg:103.82ms
step:1417/1770 train_time:146078ms step_avg:103.82ms
step:1418/1770 train_time:146185ms step_avg:103.82ms
step:1419/1770 train_time:146293ms step_avg:103.83ms
step:1420/1770 train_time:146400ms step_avg:103.83ms
step:1421/1770 train_time:146507ms step_avg:103.83ms
step:1422/1770 train_time:146616ms step_avg:103.84ms
step:1423/1770 train_time:146724ms step_avg:103.84ms
step:1424/1770 train_time:146832ms step_avg:103.84ms
step:1425/1770 train_time:146939ms step_avg:103.84ms
step:1426/1770 train_time:147048ms step_avg:103.85ms
step:1427/1770 train_time:147154ms step_avg:103.85ms
step:1428/1770 train_time:147263ms step_avg:103.85ms
step:1429/1770 train_time:147370ms step_avg:103.86ms
step:1430/1770 train_time:147479ms step_avg:103.86ms
step:1431/1770 train_time:147587ms step_avg:103.86ms
step:1432/1770 train_time:147694ms step_avg:103.86ms
step:1433/1770 train_time:147802ms step_avg:103.87ms
step:1434/1770 train_time:147908ms step_avg:103.87ms
step:1435/1770 train_time:148017ms step_avg:103.87ms
step:1436/1770 train_time:148126ms step_avg:103.88ms
step:1437/1770 train_time:148234ms step_avg:103.88ms
step:1438/1770 train_time:148341ms step_avg:103.88ms
step:1439/1770 train_time:148448ms step_avg:103.88ms
step:1440/1770 train_time:148556ms step_avg:103.89ms
step:1441/1770 train_time:148667ms step_avg:103.89ms
step:1442/1770 train_time:148774ms step_avg:103.89ms
step:1443/1770 train_time:148882ms step_avg:103.90ms
step:1444/1770 train_time:148989ms step_avg:103.90ms
step:1445/1770 train_time:149098ms step_avg:103.90ms
step:1446/1770 train_time:149207ms step_avg:103.90ms
step:1447/1770 train_time:149315ms step_avg:103.91ms
step:1448/1770 train_time:149424ms step_avg:103.91ms
step:1449/1770 train_time:149534ms step_avg:103.91ms
step:1450/1770 train_time:149642ms step_avg:103.92ms
step:1451/1770 train_time:149750ms step_avg:103.92ms
step:1452/1770 train_time:149860ms step_avg:103.93ms
step:1453/1770 train_time:149968ms step_avg:103.93ms
step:1454/1770 train_time:150077ms step_avg:103.93ms
step:1455/1770 train_time:150186ms step_avg:103.94ms
step:1456/1770 train_time:150296ms step_avg:103.94ms
step:1457/1770 train_time:150405ms step_avg:103.94ms
step:1458/1770 train_time:150514ms step_avg:103.95ms
step:1459/1770 train_time:150625ms step_avg:103.95ms
step:1460/1770 train_time:150734ms step_avg:103.95ms
step:1461/1770 train_time:150844ms step_avg:103.96ms
step:1462/1770 train_time:150952ms step_avg:103.96ms
step:1463/1770 train_time:151062ms step_avg:103.97ms
step:1464/1770 train_time:151172ms step_avg:103.97ms
step:1465/1770 train_time:151280ms step_avg:103.97ms
step:1466/1770 train_time:151389ms step_avg:103.98ms
step:1467/1770 train_time:151501ms step_avg:103.98ms
step:1468/1770 train_time:151610ms step_avg:103.99ms
step:1469/1770 train_time:151719ms step_avg:103.99ms
step:1470/1770 train_time:151827ms step_avg:103.99ms
step:1471/1770 train_time:151936ms step_avg:103.99ms
step:1472/1770 train_time:152045ms step_avg:104.00ms
step:1473/1770 train_time:152155ms step_avg:104.00ms
step:1474/1770 train_time:152266ms step_avg:104.01ms
step:1475/1770 train_time:152375ms step_avg:104.01ms
step:1476/1770 train_time:152484ms step_avg:104.01ms
step:1477/1770 train_time:152594ms step_avg:104.02ms
step:1478/1770 train_time:152703ms step_avg:104.02ms
step:1479/1770 train_time:152812ms step_avg:104.02ms
step:1480/1770 train_time:152921ms step_avg:104.03ms
step:1481/1770 train_time:153036ms step_avg:104.04ms
step:1482/1770 train_time:153142ms step_avg:104.04ms
step:1483/1770 train_time:153252ms step_avg:104.04ms
step:1484/1770 train_time:153362ms step_avg:104.04ms
step:1485/1770 train_time:153471ms step_avg:104.05ms
step:1486/1770 train_time:153579ms step_avg:104.05ms
step:1487/1770 train_time:153687ms step_avg:104.05ms
step:1488/1770 train_time:153796ms step_avg:104.06ms
step:1489/1770 train_time:153907ms step_avg:104.06ms
step:1490/1770 train_time:154016ms step_avg:104.06ms
step:1491/1770 train_time:154124ms step_avg:104.07ms
step:1492/1770 train_time:154235ms step_avg:104.07ms
step:1493/1770 train_time:154346ms step_avg:104.08ms
step:1494/1770 train_time:154460ms step_avg:104.08ms
step:1495/1770 train_time:154567ms step_avg:104.09ms
step:1496/1770 train_time:154676ms step_avg:104.09ms
step:1497/1770 train_time:154784ms step_avg:104.09ms
step:1498/1770 train_time:154892ms step_avg:104.09ms
step:1499/1770 train_time:155001ms step_avg:104.10ms
step:1500/1770 train_time:155109ms step_avg:104.10ms
step:1500/1770 val_loss:3.3420 train_time:155215ms step_avg:104.17ms
step:1501/1770 train_time:155232ms step_avg:104.11ms
step:1502/1770 train_time:155336ms step_avg:104.11ms
step:1503/1770 train_time:155445ms step_avg:104.12ms
step:1504/1770 train_time:155554ms step_avg:104.12ms
step:1505/1770 train_time:155665ms step_avg:104.12ms
step:1506/1770 train_time:155774ms step_avg:104.13ms
step:1507/1770 train_time:155883ms step_avg:104.13ms
step:1508/1770 train_time:155993ms step_avg:104.13ms
step:1509/1770 train_time:156102ms step_avg:104.14ms
step:1510/1770 train_time:156211ms step_avg:104.14ms
step:1511/1770 train_time:156320ms step_avg:104.14ms
step:1512/1770 train_time:156430ms step_avg:104.15ms
step:1513/1770 train_time:156538ms step_avg:104.15ms
step:1514/1770 train_time:156647ms step_avg:104.15ms
step:1515/1770 train_time:156756ms step_avg:104.16ms
step:1516/1770 train_time:156865ms step_avg:104.16ms
step:1517/1770 train_time:156975ms step_avg:104.16ms
step:1518/1770 train_time:157083ms step_avg:104.17ms
step:1519/1770 train_time:157192ms step_avg:104.17ms
step:1520/1770 train_time:157302ms step_avg:104.17ms
step:1521/1770 train_time:157410ms step_avg:104.18ms
step:1522/1770 train_time:157518ms step_avg:104.18ms
step:1523/1770 train_time:157628ms step_avg:104.18ms
step:1524/1770 train_time:157736ms step_avg:104.19ms
step:1525/1770 train_time:157845ms step_avg:104.19ms
step:1526/1770 train_time:157954ms step_avg:104.19ms
step:1527/1770 train_time:158062ms step_avg:104.19ms
step:1528/1770 train_time:158174ms step_avg:104.20ms
step:1529/1770 train_time:158282ms step_avg:104.20ms
step:1530/1770 train_time:158391ms step_avg:104.20ms
step:1531/1770 train_time:158499ms step_avg:104.21ms
step:1532/1770 train_time:158609ms step_avg:104.21ms
step:1533/1770 train_time:158719ms step_avg:104.21ms
step:1534/1770 train_time:158828ms step_avg:104.22ms
step:1535/1770 train_time:158936ms step_avg:104.22ms
step:1536/1770 train_time:159044ms step_avg:104.22ms
step:1537/1770 train_time:159153ms step_avg:104.23ms
step:1538/1770 train_time:159263ms step_avg:104.23ms
step:1539/1770 train_time:159371ms step_avg:104.23ms
step:1540/1770 train_time:159483ms step_avg:104.24ms
step:1541/1770 train_time:159593ms step_avg:104.24ms
step:1542/1770 train_time:159702ms step_avg:104.24ms
step:1543/1770 train_time:159811ms step_avg:104.25ms
step:1544/1770 train_time:159922ms step_avg:104.25ms
step:1545/1770 train_time:160031ms step_avg:104.25ms
step:1546/1770 train_time:160140ms step_avg:104.26ms
step:1547/1770 train_time:160248ms step_avg:104.26ms
step:1548/1770 train_time:160357ms step_avg:104.26ms
step:1549/1770 train_time:160465ms step_avg:104.27ms
step:1550/1770 train_time:160575ms step_avg:104.27ms
step:1551/1770 train_time:160683ms step_avg:104.27ms
step:1552/1770 train_time:160796ms step_avg:104.28ms
step:1553/1770 train_time:160905ms step_avg:104.28ms
step:1554/1770 train_time:161014ms step_avg:104.28ms
step:1555/1770 train_time:161124ms step_avg:104.29ms
step:1556/1770 train_time:161232ms step_avg:104.29ms
step:1557/1770 train_time:161340ms step_avg:104.29ms
step:1558/1770 train_time:161450ms step_avg:104.30ms
step:1559/1770 train_time:161558ms step_avg:104.30ms
step:1560/1770 train_time:161666ms step_avg:104.30ms
step:1561/1770 train_time:161777ms step_avg:104.30ms
step:1562/1770 train_time:161885ms step_avg:104.31ms
step:1563/1770 train_time:161994ms step_avg:104.31ms
step:1564/1770 train_time:162102ms step_avg:104.31ms
step:1565/1770 train_time:162211ms step_avg:104.32ms
step:1566/1770 train_time:162320ms step_avg:104.32ms
step:1567/1770 train_time:162430ms step_avg:104.32ms
step:1568/1770 train_time:162539ms step_avg:104.33ms
step:1569/1770 train_time:162652ms step_avg:104.33ms
step:1570/1770 train_time:162761ms step_avg:104.33ms
step:1571/1770 train_time:162872ms step_avg:104.34ms
step:1572/1770 train_time:162979ms step_avg:104.34ms
step:1573/1770 train_time:163091ms step_avg:104.34ms
step:1574/1770 train_time:163199ms step_avg:104.35ms
step:1575/1770 train_time:163307ms step_avg:104.35ms
step:1576/1770 train_time:163415ms step_avg:104.35ms
step:1577/1770 train_time:163526ms step_avg:104.36ms
step:1578/1770 train_time:163637ms step_avg:104.36ms
step:1579/1770 train_time:163746ms step_avg:104.36ms
step:1580/1770 train_time:163855ms step_avg:104.37ms
step:1581/1770 train_time:163966ms step_avg:104.37ms
step:1582/1770 train_time:164078ms step_avg:104.38ms
step:1583/1770 train_time:164187ms step_avg:104.38ms
step:1584/1770 train_time:164297ms step_avg:104.38ms
step:1585/1770 train_time:164405ms step_avg:104.38ms
step:1586/1770 train_time:164519ms step_avg:104.39ms
step:1587/1770 train_time:164629ms step_avg:104.39ms
step:1588/1770 train_time:164738ms step_avg:104.40ms
step:1589/1770 train_time:164848ms step_avg:104.40ms
step:1590/1770 train_time:164957ms step_avg:104.40ms
step:1591/1770 train_time:165066ms step_avg:104.41ms
step:1592/1770 train_time:165177ms step_avg:104.41ms
step:1593/1770 train_time:165286ms step_avg:104.41ms
step:1594/1770 train_time:165395ms step_avg:104.42ms
step:1595/1770 train_time:165504ms step_avg:104.42ms
step:1596/1770 train_time:165614ms step_avg:104.42ms
step:1597/1770 train_time:165721ms step_avg:104.42ms
step:1598/1770 train_time:165831ms step_avg:104.43ms
step:1599/1770 train_time:165941ms step_avg:104.43ms
step:1600/1770 train_time:166052ms step_avg:104.44ms
step:1601/1770 train_time:166161ms step_avg:104.44ms
step:1602/1770 train_time:166272ms step_avg:104.44ms
step:1603/1770 train_time:166381ms step_avg:104.44ms
step:1604/1770 train_time:166489ms step_avg:104.45ms
step:1605/1770 train_time:166598ms step_avg:104.45ms
step:1606/1770 train_time:166707ms step_avg:104.45ms
step:1607/1770 train_time:166821ms step_avg:104.46ms
step:1608/1770 train_time:166930ms step_avg:104.46ms
step:1609/1770 train_time:167038ms step_avg:104.46ms
step:1610/1770 train_time:167148ms step_avg:104.47ms
step:1611/1770 train_time:167258ms step_avg:104.47ms
step:1612/1770 train_time:167367ms step_avg:104.47ms
step:1613/1770 train_time:167476ms step_avg:104.48ms
step:1614/1770 train_time:167585ms step_avg:104.48ms
step:1615/1770 train_time:167695ms step_avg:104.48ms
step:1616/1770 train_time:167804ms step_avg:104.49ms
step:1617/1770 train_time:167915ms step_avg:104.49ms
step:1618/1770 train_time:168024ms step_avg:104.49ms
step:1619/1770 train_time:168133ms step_avg:104.50ms
step:1620/1770 train_time:168242ms step_avg:104.50ms
step:1621/1770 train_time:168352ms step_avg:104.50ms
step:1622/1770 train_time:168461ms step_avg:104.50ms
step:1623/1770 train_time:168574ms step_avg:104.51ms
step:1624/1770 train_time:168683ms step_avg:104.51ms
step:1625/1770 train_time:168791ms step_avg:104.51ms
step:1625/1770 val_loss:3.3068 train_time:168899ms step_avg:104.58ms
step:1626/1770 train_time:168916ms step_avg:104.53ms
step:1627/1770 train_time:169016ms step_avg:104.52ms
step:1628/1770 train_time:169126ms step_avg:104.53ms
step:1629/1770 train_time:169233ms step_avg:104.53ms
step:1630/1770 train_time:169342ms step_avg:104.53ms
step:1631/1770 train_time:169450ms step_avg:104.53ms
step:1632/1770 train_time:169560ms step_avg:104.54ms
step:1633/1770 train_time:169669ms step_avg:104.54ms
step:1634/1770 train_time:169778ms step_avg:104.54ms
step:1635/1770 train_time:169888ms step_avg:104.55ms
step:1636/1770 train_time:169997ms step_avg:104.55ms
step:1637/1770 train_time:170106ms step_avg:104.55ms
step:1638/1770 train_time:170214ms step_avg:104.55ms
step:1639/1770 train_time:170324ms step_avg:104.56ms
step:1640/1770 train_time:170433ms step_avg:104.56ms
step:1641/1770 train_time:170541ms step_avg:104.56ms
step:1642/1770 train_time:170649ms step_avg:104.56ms
step:1643/1770 train_time:170758ms step_avg:104.57ms
step:1644/1770 train_time:170870ms step_avg:104.57ms
step:1645/1770 train_time:170978ms step_avg:104.57ms
step:1646/1770 train_time:171089ms step_avg:104.58ms
step:1647/1770 train_time:171199ms step_avg:104.58ms
step:1648/1770 train_time:171306ms step_avg:104.58ms
step:1649/1770 train_time:171415ms step_avg:104.59ms
step:1650/1770 train_time:171525ms step_avg:104.59ms
step:1651/1770 train_time:171632ms step_avg:104.59ms
step:1652/1770 train_time:171741ms step_avg:104.59ms
step:1653/1770 train_time:171850ms step_avg:104.60ms
step:1654/1770 train_time:171963ms step_avg:104.60ms
step:1655/1770 train_time:172074ms step_avg:104.60ms
step:1656/1770 train_time:172183ms step_avg:104.61ms
step:1657/1770 train_time:172293ms step_avg:104.61ms
step:1658/1770 train_time:172402ms step_avg:104.61ms
step:1659/1770 train_time:172512ms step_avg:104.62ms
step:1660/1770 train_time:172621ms step_avg:104.62ms
step:1661/1770 train_time:172730ms step_avg:104.62ms
step:1662/1770 train_time:172840ms step_avg:104.62ms
step:1663/1770 train_time:172948ms step_avg:104.63ms
step:1664/1770 train_time:173057ms step_avg:104.63ms
step:1665/1770 train_time:173166ms step_avg:104.63ms
step:1666/1770 train_time:173276ms step_avg:104.64ms
step:1667/1770 train_time:173386ms step_avg:104.64ms
step:1668/1770 train_time:173493ms step_avg:104.64ms
step:1669/1770 train_time:173602ms step_avg:104.64ms
step:1670/1770 train_time:173709ms step_avg:104.64ms
step:1671/1770 train_time:173818ms step_avg:104.65ms
step:1672/1770 train_time:173928ms step_avg:104.65ms
step:1673/1770 train_time:174037ms step_avg:104.65ms
step:1674/1770 train_time:174147ms step_avg:104.66ms
step:1675/1770 train_time:174254ms step_avg:104.66ms
step:1676/1770 train_time:174364ms step_avg:104.66ms
step:1677/1770 train_time:174477ms step_avg:104.67ms
step:1678/1770 train_time:174585ms step_avg:104.67ms
step:1679/1770 train_time:174694ms step_avg:104.67ms
step:1680/1770 train_time:174804ms step_avg:104.67ms
step:1681/1770 train_time:174913ms step_avg:104.68ms
step:1682/1770 train_time:175023ms step_avg:104.68ms
step:1683/1770 train_time:175131ms step_avg:104.68ms
step:1684/1770 train_time:175239ms step_avg:104.68ms
step:1685/1770 train_time:175349ms step_avg:104.69ms
step:1686/1770 train_time:175459ms step_avg:104.69ms
step:1687/1770 train_time:175569ms step_avg:104.69ms
step:1688/1770 train_time:175678ms step_avg:104.69ms
step:1689/1770 train_time:175787ms step_avg:104.70ms
step:1690/1770 train_time:175896ms step_avg:104.70ms
step:1691/1770 train_time:176005ms step_avg:104.70ms
step:1692/1770 train_time:176115ms step_avg:104.71ms
step:1693/1770 train_time:176226ms step_avg:104.71ms
step:1694/1770 train_time:176334ms step_avg:104.71ms
step:1695/1770 train_time:176444ms step_avg:104.71ms
step:1696/1770 train_time:176555ms step_avg:104.72ms
step:1697/1770 train_time:176665ms step_avg:104.72ms
step:1698/1770 train_time:176775ms step_avg:104.72ms
step:1699/1770 train_time:176883ms step_avg:104.73ms
step:1700/1770 train_time:176991ms step_avg:104.73ms
step:1701/1770 train_time:177100ms step_avg:104.73ms
step:1702/1770 train_time:177210ms step_avg:104.73ms
step:1703/1770 train_time:177319ms step_avg:104.74ms
step:1704/1770 train_time:177428ms step_avg:104.74ms
step:1705/1770 train_time:177536ms step_avg:104.74ms
step:1706/1770 train_time:177645ms step_avg:104.74ms
step:1707/1770 train_time:177755ms step_avg:104.75ms
step:1708/1770 train_time:177864ms step_avg:104.75ms
step:1709/1770 train_time:177975ms step_avg:104.75ms
step:1710/1770 train_time:178088ms step_avg:104.76ms
step:1711/1770 train_time:178202ms step_avg:104.76ms
step:1712/1770 train_time:178310ms step_avg:104.77ms
step:1713/1770 train_time:178420ms step_avg:104.77ms
step:1714/1770 train_time:178530ms step_avg:104.77ms
step:1715/1770 train_time:178639ms step_avg:104.77ms
step:1716/1770 train_time:178750ms step_avg:104.78ms
step:1717/1770 train_time:178859ms step_avg:104.78ms
step:1718/1770 train_time:178970ms step_avg:104.78ms
step:1719/1770 train_time:179080ms step_avg:104.79ms
step:1720/1770 train_time:179190ms step_avg:104.79ms
step:1721/1770 train_time:179300ms step_avg:104.79ms
step:1722/1770 train_time:179411ms step_avg:104.80ms
step:1723/1770 train_time:179522ms step_avg:104.80ms
step:1724/1770 train_time:179635ms step_avg:104.80ms
step:1725/1770 train_time:179747ms step_avg:104.81ms
step:1726/1770 train_time:179858ms step_avg:104.81ms
step:1727/1770 train_time:179968ms step_avg:104.82ms
step:1728/1770 train_time:180079ms step_avg:104.82ms
step:1729/1770 train_time:180189ms step_avg:104.82ms
step:1730/1770 train_time:180300ms step_avg:104.83ms
step:1731/1770 train_time:180413ms step_avg:104.83ms
step:1732/1770 train_time:180523ms step_avg:104.83ms
step:1733/1770 train_time:180634ms step_avg:104.84ms
step:1734/1770 train_time:180744ms step_avg:104.84ms
step:1735/1770 train_time:180854ms step_avg:104.84ms
step:1736/1770 train_time:180964ms step_avg:104.85ms
step:1737/1770 train_time:181074ms step_avg:104.85ms
step:1738/1770 train_time:181184ms step_avg:104.85ms
step:1739/1770 train_time:181294ms step_avg:104.85ms
step:1740/1770 train_time:181403ms step_avg:104.86ms
step:1741/1770 train_time:181516ms step_avg:104.86ms
step:1742/1770 train_time:181630ms step_avg:104.87ms
step:1743/1770 train_time:181741ms step_avg:104.87ms
step:1744/1770 train_time:181850ms step_avg:104.87ms
step:1745/1770 train_time:181959ms step_avg:104.88ms
step:1746/1770 train_time:182072ms step_avg:104.88ms
step:1747/1770 train_time:182181ms step_avg:104.88ms
step:1748/1770 train_time:182292ms step_avg:104.89ms
step:1749/1770 train_time:182404ms step_avg:104.89ms
step:1750/1770 train_time:182513ms step_avg:104.89ms
step:1750/1770 val_loss:3.2795 train_time:182621ms step_avg:104.95ms
step:1751/1770 train_time:182639ms step_avg:104.90ms
step:1752/1770 train_time:182739ms step_avg:104.90ms
step:1753/1770 train_time:182852ms step_avg:104.91ms
step:1754/1770 train_time:182962ms step_avg:104.91ms
step:1755/1770 train_time:183071ms step_avg:104.91ms
step:1756/1770 train_time:183182ms step_avg:104.92ms
step:1757/1770 train_time:183291ms step_avg:104.92ms
step:1758/1770 train_time:183400ms step_avg:104.92ms
step:1759/1770 train_time:183512ms step_avg:104.92ms
step:1760/1770 train_time:183621ms step_avg:104.93ms
step:1761/1770 train_time:183733ms step_avg:104.93ms
step:1762/1770 train_time:183846ms step_avg:104.93ms
step:1763/1770 train_time:183955ms step_avg:104.94ms
step:1764/1770 train_time:184065ms step_avg:104.94ms
step:1765/1770 train_time:184175ms step_avg:104.94ms
step:1766/1770 train_time:184289ms step_avg:104.95ms
step:1767/1770 train_time:184397ms step_avg:104.95ms
step:1768/1770 train_time:184508ms step_avg:104.95ms
step:1769/1770 train_time:184617ms step_avg:104.96ms
step:1770/1770 train_time:184726ms step_avg:104.96ms
step:1770/1770 val_loss:3.2765 train_time:184836ms step_avg:105.02ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
