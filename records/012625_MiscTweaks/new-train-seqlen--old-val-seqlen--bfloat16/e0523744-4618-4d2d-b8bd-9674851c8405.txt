import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:34:29 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   39C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:25969ms step_avg:nanms
step:2/1770 train_time:26382ms step_avg:nanms
step:3/1770 train_time:26510ms step_avg:nanms
step:4/1770 train_time:26607ms step_avg:nanms
step:5/1770 train_time:26706ms step_avg:nanms
step:6/1770 train_time:26805ms step_avg:nanms
step:7/1770 train_time:26904ms step_avg:nanms
step:8/1770 train_time:27004ms step_avg:nanms
step:9/1770 train_time:27103ms step_avg:nanms
step:10/1770 train_time:27202ms step_avg:nanms
step:11/1770 train_time:99ms step_avg:nanms
step:12/1770 train_time:198ms step_avg:nanms
step:13/1770 train_time:298ms step_avg:99.45ms
step:14/1770 train_time:397ms step_avg:99.31ms
step:15/1770 train_time:496ms step_avg:99.29ms
step:16/1770 train_time:597ms step_avg:99.57ms
step:17/1770 train_time:698ms step_avg:99.71ms
step:18/1770 train_time:798ms step_avg:99.70ms
step:19/1770 train_time:898ms step_avg:99.75ms
step:20/1770 train_time:998ms step_avg:99.81ms
step:21/1770 train_time:1098ms step_avg:99.80ms
step:22/1770 train_time:1198ms step_avg:99.86ms
step:23/1770 train_time:1298ms step_avg:99.87ms
step:24/1770 train_time:1398ms step_avg:99.87ms
step:25/1770 train_time:1499ms step_avg:99.95ms
step:26/1770 train_time:1599ms step_avg:99.95ms
step:27/1770 train_time:1699ms step_avg:99.97ms
step:28/1770 train_time:1798ms step_avg:99.91ms
step:29/1770 train_time:1899ms step_avg:99.93ms
step:30/1770 train_time:1999ms step_avg:99.93ms
step:31/1770 train_time:2099ms step_avg:99.94ms
step:32/1770 train_time:2200ms step_avg:99.99ms
step:33/1770 train_time:2300ms step_avg:100.00ms
step:34/1770 train_time:2400ms step_avg:99.99ms
step:35/1770 train_time:2499ms step_avg:99.98ms
step:36/1770 train_time:2600ms step_avg:99.98ms
step:37/1770 train_time:2700ms step_avg:100.00ms
step:38/1770 train_time:2801ms step_avg:100.02ms
step:39/1770 train_time:2900ms step_avg:100.01ms
step:40/1770 train_time:3001ms step_avg:100.04ms
step:41/1770 train_time:3102ms step_avg:100.05ms
step:42/1770 train_time:3200ms step_avg:100.01ms
step:43/1770 train_time:3300ms step_avg:100.01ms
step:44/1770 train_time:3400ms step_avg:100.00ms
step:45/1770 train_time:3500ms step_avg:99.99ms
step:46/1770 train_time:3600ms step_avg:99.99ms
step:47/1770 train_time:3700ms step_avg:99.99ms
step:48/1770 train_time:3800ms step_avg:100.01ms
step:49/1770 train_time:3899ms step_avg:99.98ms
step:50/1770 train_time:3999ms step_avg:99.98ms
step:51/1770 train_time:4100ms step_avg:99.99ms
step:52/1770 train_time:4200ms step_avg:100.00ms
step:53/1770 train_time:4300ms step_avg:100.00ms
step:54/1770 train_time:4400ms step_avg:100.00ms
step:55/1770 train_time:4501ms step_avg:100.01ms
step:56/1770 train_time:4599ms step_avg:99.98ms
step:57/1770 train_time:4698ms step_avg:99.97ms
step:58/1770 train_time:4798ms step_avg:99.96ms
step:59/1770 train_time:4898ms step_avg:99.96ms
step:60/1770 train_time:4999ms step_avg:99.97ms
step:61/1770 train_time:5098ms step_avg:99.97ms
step:62/1770 train_time:5200ms step_avg:99.99ms
step:63/1770 train_time:5299ms step_avg:99.97ms
step:64/1770 train_time:5398ms step_avg:99.96ms
step:65/1770 train_time:5498ms step_avg:99.97ms
step:66/1770 train_time:5599ms step_avg:99.98ms
step:67/1770 train_time:5700ms step_avg:99.99ms
step:68/1770 train_time:5799ms step_avg:99.99ms
step:69/1770 train_time:5900ms step_avg:99.99ms
step:70/1770 train_time:5998ms step_avg:99.97ms
step:71/1770 train_time:6099ms step_avg:99.98ms
step:72/1770 train_time:6200ms step_avg:100.00ms
step:73/1770 train_time:6300ms step_avg:100.00ms
step:74/1770 train_time:6400ms step_avg:100.01ms
step:75/1770 train_time:6501ms step_avg:100.01ms
step:76/1770 train_time:6601ms step_avg:100.01ms
step:77/1770 train_time:6699ms step_avg:99.99ms
step:78/1770 train_time:6799ms step_avg:99.99ms
step:79/1770 train_time:6899ms step_avg:99.98ms
step:80/1770 train_time:6998ms step_avg:99.97ms
step:81/1770 train_time:7099ms step_avg:99.98ms
step:82/1770 train_time:7199ms step_avg:99.98ms
step:83/1770 train_time:7299ms step_avg:99.99ms
step:84/1770 train_time:7398ms step_avg:99.97ms
step:85/1770 train_time:7498ms step_avg:99.98ms
step:86/1770 train_time:7599ms step_avg:99.98ms
step:87/1770 train_time:7698ms step_avg:99.98ms
step:88/1770 train_time:7798ms step_avg:99.98ms
step:89/1770 train_time:7898ms step_avg:99.98ms
step:90/1770 train_time:7999ms step_avg:99.99ms
step:91/1770 train_time:8098ms step_avg:99.97ms
step:92/1770 train_time:8198ms step_avg:99.98ms
step:93/1770 train_time:8299ms step_avg:99.99ms
step:94/1770 train_time:8399ms step_avg:99.99ms
step:95/1770 train_time:8500ms step_avg:100.00ms
step:96/1770 train_time:8600ms step_avg:100.00ms
step:97/1770 train_time:8700ms step_avg:100.00ms
step:98/1770 train_time:8799ms step_avg:99.99ms
step:99/1770 train_time:8899ms step_avg:99.99ms
step:100/1770 train_time:9000ms step_avg:100.00ms
step:101/1770 train_time:9100ms step_avg:100.00ms
step:102/1770 train_time:9200ms step_avg:100.00ms
step:103/1770 train_time:9300ms step_avg:100.00ms
step:104/1770 train_time:9400ms step_avg:100.00ms
step:105/1770 train_time:9499ms step_avg:99.99ms
step:106/1770 train_time:9600ms step_avg:100.00ms
step:107/1770 train_time:9700ms step_avg:100.00ms
step:108/1770 train_time:9800ms step_avg:100.01ms
step:109/1770 train_time:9901ms step_avg:100.01ms
step:110/1770 train_time:10001ms step_avg:100.01ms
step:111/1770 train_time:10101ms step_avg:100.01ms
step:112/1770 train_time:10199ms step_avg:99.99ms
step:113/1770 train_time:10299ms step_avg:99.99ms
step:114/1770 train_time:10399ms step_avg:99.99ms
step:115/1770 train_time:10498ms step_avg:99.98ms
step:116/1770 train_time:10599ms step_avg:99.99ms
step:117/1770 train_time:10699ms step_avg:99.99ms
step:118/1770 train_time:10800ms step_avg:100.00ms
step:119/1770 train_time:10898ms step_avg:99.98ms
step:120/1770 train_time:10998ms step_avg:99.98ms
step:121/1770 train_time:11099ms step_avg:99.99ms
step:122/1770 train_time:11198ms step_avg:99.98ms
step:123/1770 train_time:11298ms step_avg:99.98ms
step:124/1770 train_time:11397ms step_avg:99.97ms
step:125/1770 train_time:11497ms step_avg:99.98ms
step:125/1770 val_loss:4.6568 train_time:11595ms step_avg:100.83ms
step:126/1770 train_time:11613ms step_avg:100.11ms
step:127/1770 train_time:11706ms step_avg:100.05ms
step:128/1770 train_time:11812ms step_avg:100.10ms
step:129/1770 train_time:11912ms step_avg:100.10ms
step:130/1770 train_time:12011ms step_avg:100.09ms
step:131/1770 train_time:12110ms step_avg:100.08ms
step:132/1770 train_time:12209ms step_avg:100.07ms
step:133/1770 train_time:12309ms step_avg:100.08ms
step:134/1770 train_time:12410ms step_avg:100.08ms
step:135/1770 train_time:12510ms step_avg:100.08ms
step:136/1770 train_time:12610ms step_avg:100.08ms
step:137/1770 train_time:12710ms step_avg:100.08ms
step:138/1770 train_time:12811ms step_avg:100.08ms
step:139/1770 train_time:12912ms step_avg:100.09ms
step:140/1770 train_time:13014ms step_avg:100.10ms
step:141/1770 train_time:13114ms step_avg:100.11ms
step:142/1770 train_time:13214ms step_avg:100.11ms
step:143/1770 train_time:13314ms step_avg:100.10ms
step:144/1770 train_time:13413ms step_avg:100.10ms
step:145/1770 train_time:13513ms step_avg:100.10ms
step:146/1770 train_time:13613ms step_avg:100.10ms
step:147/1770 train_time:13713ms step_avg:100.10ms
step:148/1770 train_time:13813ms step_avg:100.10ms
step:149/1770 train_time:13913ms step_avg:100.10ms
step:150/1770 train_time:14014ms step_avg:100.10ms
step:151/1770 train_time:14113ms step_avg:100.10ms
step:152/1770 train_time:14213ms step_avg:100.09ms
step:153/1770 train_time:14313ms step_avg:100.09ms
step:154/1770 train_time:14413ms step_avg:100.09ms
step:155/1770 train_time:14513ms step_avg:100.09ms
step:156/1770 train_time:14613ms step_avg:100.09ms
step:157/1770 train_time:14712ms step_avg:100.08ms
step:158/1770 train_time:14813ms step_avg:100.09ms
step:159/1770 train_time:14913ms step_avg:100.09ms
step:160/1770 train_time:15013ms step_avg:100.09ms
step:161/1770 train_time:15112ms step_avg:100.08ms
step:162/1770 train_time:15212ms step_avg:100.08ms
step:163/1770 train_time:15312ms step_avg:100.08ms
step:164/1770 train_time:15412ms step_avg:100.08ms
step:165/1770 train_time:15512ms step_avg:100.08ms
step:166/1770 train_time:15612ms step_avg:100.08ms
step:167/1770 train_time:15712ms step_avg:100.07ms
step:168/1770 train_time:15812ms step_avg:100.07ms
step:169/1770 train_time:15912ms step_avg:100.08ms
step:170/1770 train_time:16012ms step_avg:100.08ms
step:171/1770 train_time:16112ms step_avg:100.08ms
step:172/1770 train_time:16212ms step_avg:100.08ms
step:173/1770 train_time:16312ms step_avg:100.08ms
step:174/1770 train_time:16413ms step_avg:100.08ms
step:175/1770 train_time:16514ms step_avg:100.08ms
step:176/1770 train_time:16613ms step_avg:100.08ms
step:177/1770 train_time:16713ms step_avg:100.08ms
step:178/1770 train_time:16813ms step_avg:100.08ms
step:179/1770 train_time:16914ms step_avg:100.08ms
step:180/1770 train_time:17013ms step_avg:100.08ms
step:181/1770 train_time:17113ms step_avg:100.08ms
step:182/1770 train_time:17213ms step_avg:100.07ms
step:183/1770 train_time:17313ms step_avg:100.07ms
step:184/1770 train_time:17413ms step_avg:100.08ms
step:185/1770 train_time:17513ms step_avg:100.07ms
step:186/1770 train_time:17613ms step_avg:100.07ms
step:187/1770 train_time:17713ms step_avg:100.07ms
step:188/1770 train_time:17813ms step_avg:100.07ms
step:189/1770 train_time:17913ms step_avg:100.07ms
step:190/1770 train_time:18013ms step_avg:100.07ms
step:191/1770 train_time:18112ms step_avg:100.07ms
step:192/1770 train_time:18213ms step_avg:100.07ms
step:193/1770 train_time:18313ms step_avg:100.07ms
step:194/1770 train_time:18412ms step_avg:100.07ms
step:195/1770 train_time:18512ms step_avg:100.07ms
step:196/1770 train_time:18612ms step_avg:100.07ms
step:197/1770 train_time:18712ms step_avg:100.06ms
step:198/1770 train_time:18812ms step_avg:100.06ms
step:199/1770 train_time:18913ms step_avg:100.07ms
step:200/1770 train_time:19013ms step_avg:100.07ms
step:201/1770 train_time:19113ms step_avg:100.07ms
step:202/1770 train_time:19213ms step_avg:100.07ms
step:203/1770 train_time:19313ms step_avg:100.07ms
step:204/1770 train_time:19413ms step_avg:100.07ms
step:205/1770 train_time:19513ms step_avg:100.06ms
step:206/1770 train_time:19613ms step_avg:100.07ms
step:207/1770 train_time:19713ms step_avg:100.07ms
step:208/1770 train_time:19813ms step_avg:100.07ms
step:209/1770 train_time:19913ms step_avg:100.07ms
step:210/1770 train_time:20013ms step_avg:100.07ms
step:211/1770 train_time:20113ms step_avg:100.06ms
step:212/1770 train_time:20213ms step_avg:100.06ms
step:213/1770 train_time:20313ms step_avg:100.06ms
step:214/1770 train_time:20413ms step_avg:100.06ms
step:215/1770 train_time:20513ms step_avg:100.06ms
step:216/1770 train_time:20613ms step_avg:100.06ms
step:217/1770 train_time:20713ms step_avg:100.06ms
step:218/1770 train_time:20812ms step_avg:100.06ms
step:219/1770 train_time:20912ms step_avg:100.06ms
step:220/1770 train_time:21012ms step_avg:100.06ms
step:221/1770 train_time:21113ms step_avg:100.06ms
step:222/1770 train_time:21212ms step_avg:100.06ms
step:223/1770 train_time:21312ms step_avg:100.06ms
step:224/1770 train_time:21412ms step_avg:100.06ms
step:225/1770 train_time:21513ms step_avg:100.06ms
step:226/1770 train_time:21612ms step_avg:100.06ms
step:227/1770 train_time:21712ms step_avg:100.06ms
step:228/1770 train_time:21812ms step_avg:100.06ms
step:229/1770 train_time:21912ms step_avg:100.05ms
step:230/1770 train_time:22012ms step_avg:100.05ms
step:231/1770 train_time:22112ms step_avg:100.05ms
step:232/1770 train_time:22212ms step_avg:100.05ms
step:233/1770 train_time:22312ms step_avg:100.05ms
step:234/1770 train_time:22412ms step_avg:100.05ms
step:235/1770 train_time:22512ms step_avg:100.05ms
step:236/1770 train_time:22613ms step_avg:100.06ms
step:237/1770 train_time:22713ms step_avg:100.06ms
step:238/1770 train_time:22813ms step_avg:100.06ms
step:239/1770 train_time:22913ms step_avg:100.05ms
step:240/1770 train_time:23013ms step_avg:100.06ms
step:241/1770 train_time:23113ms step_avg:100.06ms
step:242/1770 train_time:23213ms step_avg:100.06ms
step:243/1770 train_time:23313ms step_avg:100.06ms
step:244/1770 train_time:23413ms step_avg:100.06ms
step:245/1770 train_time:23513ms step_avg:100.06ms
step:246/1770 train_time:23613ms step_avg:100.05ms
step:247/1770 train_time:23713ms step_avg:100.06ms
step:248/1770 train_time:23814ms step_avg:100.06ms
step:249/1770 train_time:23913ms step_avg:100.06ms
step:250/1770 train_time:24014ms step_avg:100.06ms
step:250/1770 val_loss:4.1138 train_time:24112ms step_avg:100.47ms
step:251/1770 train_time:24133ms step_avg:100.14ms
step:252/1770 train_time:24226ms step_avg:100.11ms
step:253/1770 train_time:24330ms step_avg:100.12ms
step:254/1770 train_time:24430ms step_avg:100.12ms
step:255/1770 train_time:24530ms step_avg:100.12ms
step:256/1770 train_time:24629ms step_avg:100.12ms
step:257/1770 train_time:24728ms step_avg:100.12ms
step:258/1770 train_time:24828ms step_avg:100.11ms
step:259/1770 train_time:24927ms step_avg:100.11ms
step:260/1770 train_time:25027ms step_avg:100.11ms
step:261/1770 train_time:25127ms step_avg:100.11ms
step:262/1770 train_time:25226ms step_avg:100.10ms
step:263/1770 train_time:25326ms step_avg:100.10ms
step:264/1770 train_time:25426ms step_avg:100.10ms
step:265/1770 train_time:25526ms step_avg:100.10ms
step:266/1770 train_time:25626ms step_avg:100.10ms
step:267/1770 train_time:25727ms step_avg:100.10ms
step:268/1770 train_time:25827ms step_avg:100.10ms
step:269/1770 train_time:25927ms step_avg:100.11ms
step:270/1770 train_time:26027ms step_avg:100.11ms
step:271/1770 train_time:26127ms step_avg:100.10ms
step:272/1770 train_time:26228ms step_avg:100.11ms
step:273/1770 train_time:26328ms step_avg:100.11ms
step:274/1770 train_time:26428ms step_avg:100.11ms
step:275/1770 train_time:26528ms step_avg:100.11ms
step:276/1770 train_time:26628ms step_avg:100.11ms
step:277/1770 train_time:26728ms step_avg:100.11ms
step:278/1770 train_time:26829ms step_avg:100.11ms
step:279/1770 train_time:26929ms step_avg:100.11ms
step:280/1770 train_time:27029ms step_avg:100.11ms
step:281/1770 train_time:27129ms step_avg:100.11ms
step:282/1770 train_time:27229ms step_avg:100.11ms
step:283/1770 train_time:27329ms step_avg:100.11ms
step:284/1770 train_time:27429ms step_avg:100.11ms
step:285/1770 train_time:27529ms step_avg:100.11ms
step:286/1770 train_time:27630ms step_avg:100.11ms
step:287/1770 train_time:27730ms step_avg:100.11ms
step:288/1770 train_time:27830ms step_avg:100.11ms
step:289/1770 train_time:27930ms step_avg:100.11ms
step:290/1770 train_time:28030ms step_avg:100.11ms
step:291/1770 train_time:28130ms step_avg:100.11ms
step:292/1770 train_time:28230ms step_avg:100.11ms
step:293/1770 train_time:28332ms step_avg:100.11ms
step:294/1770 train_time:28431ms step_avg:100.11ms
step:295/1770 train_time:28531ms step_avg:100.11ms
step:296/1770 train_time:28632ms step_avg:100.11ms
step:297/1770 train_time:28732ms step_avg:100.11ms
step:298/1770 train_time:28833ms step_avg:100.11ms
step:299/1770 train_time:28933ms step_avg:100.11ms
step:300/1770 train_time:29033ms step_avg:100.11ms
step:301/1770 train_time:29133ms step_avg:100.12ms
step:302/1770 train_time:29234ms step_avg:100.12ms
step:303/1770 train_time:29336ms step_avg:100.12ms
step:304/1770 train_time:29437ms step_avg:100.13ms
step:305/1770 train_time:29538ms step_avg:100.13ms
step:306/1770 train_time:29639ms step_avg:100.13ms
step:307/1770 train_time:29740ms step_avg:100.13ms
step:308/1770 train_time:29840ms step_avg:100.13ms
step:309/1770 train_time:29940ms step_avg:100.13ms
step:310/1770 train_time:30040ms step_avg:100.13ms
step:311/1770 train_time:30139ms step_avg:100.13ms
step:312/1770 train_time:30239ms step_avg:100.13ms
step:313/1770 train_time:30340ms step_avg:100.13ms
step:314/1770 train_time:30439ms step_avg:100.13ms
step:315/1770 train_time:30540ms step_avg:100.13ms
step:316/1770 train_time:30639ms step_avg:100.13ms
step:317/1770 train_time:30740ms step_avg:100.13ms
step:318/1770 train_time:30839ms step_avg:100.13ms
step:319/1770 train_time:30939ms step_avg:100.13ms
step:320/1770 train_time:31039ms step_avg:100.13ms
step:321/1770 train_time:31140ms step_avg:100.13ms
step:322/1770 train_time:31239ms step_avg:100.13ms
step:323/1770 train_time:31339ms step_avg:100.13ms
step:324/1770 train_time:31440ms step_avg:100.13ms
step:325/1770 train_time:31539ms step_avg:100.13ms
step:326/1770 train_time:31639ms step_avg:100.12ms
step:327/1770 train_time:31739ms step_avg:100.12ms
step:328/1770 train_time:31840ms step_avg:100.13ms
step:329/1770 train_time:31940ms step_avg:100.13ms
step:330/1770 train_time:32040ms step_avg:100.13ms
step:331/1770 train_time:32140ms step_avg:100.12ms
step:332/1770 train_time:32240ms step_avg:100.12ms
step:333/1770 train_time:32341ms step_avg:100.13ms
step:334/1770 train_time:32440ms step_avg:100.12ms
step:335/1770 train_time:32540ms step_avg:100.12ms
step:336/1770 train_time:32640ms step_avg:100.12ms
step:337/1770 train_time:32740ms step_avg:100.12ms
step:338/1770 train_time:32840ms step_avg:100.12ms
step:339/1770 train_time:32940ms step_avg:100.12ms
step:340/1770 train_time:33040ms step_avg:100.12ms
step:341/1770 train_time:33140ms step_avg:100.12ms
step:342/1770 train_time:33240ms step_avg:100.12ms
step:343/1770 train_time:33340ms step_avg:100.12ms
step:344/1770 train_time:33439ms step_avg:100.12ms
step:345/1770 train_time:33540ms step_avg:100.12ms
step:346/1770 train_time:33639ms step_avg:100.12ms
step:347/1770 train_time:33739ms step_avg:100.12ms
step:348/1770 train_time:33839ms step_avg:100.12ms
step:349/1770 train_time:33939ms step_avg:100.11ms
step:350/1770 train_time:34039ms step_avg:100.11ms
step:351/1770 train_time:34139ms step_avg:100.12ms
step:352/1770 train_time:34239ms step_avg:100.11ms
step:353/1770 train_time:34339ms step_avg:100.11ms
step:354/1770 train_time:34439ms step_avg:100.11ms
step:355/1770 train_time:34539ms step_avg:100.11ms
step:356/1770 train_time:34639ms step_avg:100.11ms
step:357/1770 train_time:34739ms step_avg:100.11ms
step:358/1770 train_time:34839ms step_avg:100.11ms
step:359/1770 train_time:34940ms step_avg:100.11ms
step:360/1770 train_time:35039ms step_avg:100.11ms
step:361/1770 train_time:35139ms step_avg:100.11ms
step:362/1770 train_time:35239ms step_avg:100.11ms
step:363/1770 train_time:35339ms step_avg:100.11ms
step:364/1770 train_time:35440ms step_avg:100.11ms
step:365/1770 train_time:35540ms step_avg:100.11ms
step:366/1770 train_time:35640ms step_avg:100.11ms
step:367/1770 train_time:35740ms step_avg:100.11ms
step:368/1770 train_time:35840ms step_avg:100.11ms
step:369/1770 train_time:35940ms step_avg:100.11ms
step:370/1770 train_time:36040ms step_avg:100.11ms
step:371/1770 train_time:36140ms step_avg:100.11ms
step:372/1770 train_time:36240ms step_avg:100.11ms
step:373/1770 train_time:36340ms step_avg:100.11ms
step:374/1770 train_time:36440ms step_avg:100.11ms
step:375/1770 train_time:36540ms step_avg:100.11ms
step:375/1770 val_loss:3.9086 train_time:36639ms step_avg:100.38ms
step:376/1770 train_time:36659ms step_avg:100.16ms
step:377/1770 train_time:36752ms step_avg:100.14ms
step:378/1770 train_time:36857ms step_avg:100.16ms
step:379/1770 train_time:36959ms step_avg:100.16ms
step:380/1770 train_time:37059ms step_avg:100.16ms
step:381/1770 train_time:37159ms step_avg:100.16ms
step:382/1770 train_time:37260ms step_avg:100.16ms
step:383/1770 train_time:37360ms step_avg:100.16ms
step:384/1770 train_time:37461ms step_avg:100.16ms
step:385/1770 train_time:37561ms step_avg:100.16ms
step:386/1770 train_time:37662ms step_avg:100.16ms
step:387/1770 train_time:37762ms step_avg:100.16ms
step:388/1770 train_time:37862ms step_avg:100.16ms
step:389/1770 train_time:37964ms step_avg:100.17ms
step:390/1770 train_time:38065ms step_avg:100.17ms
step:391/1770 train_time:38165ms step_avg:100.17ms
step:392/1770 train_time:38265ms step_avg:100.17ms
step:393/1770 train_time:38365ms step_avg:100.17ms
step:394/1770 train_time:38465ms step_avg:100.17ms
step:395/1770 train_time:38566ms step_avg:100.17ms
step:396/1770 train_time:38667ms step_avg:100.17ms
step:397/1770 train_time:38769ms step_avg:100.18ms
step:398/1770 train_time:38871ms step_avg:100.18ms
step:399/1770 train_time:38973ms step_avg:100.19ms
step:400/1770 train_time:39075ms step_avg:100.19ms
step:401/1770 train_time:39178ms step_avg:100.20ms
step:402/1770 train_time:39280ms step_avg:100.20ms
step:403/1770 train_time:39383ms step_avg:100.21ms
step:404/1770 train_time:39486ms step_avg:100.22ms
step:405/1770 train_time:39588ms step_avg:100.22ms
step:406/1770 train_time:39689ms step_avg:100.23ms
step:407/1770 train_time:39791ms step_avg:100.23ms
step:408/1770 train_time:39893ms step_avg:100.23ms
step:409/1770 train_time:39995ms step_avg:100.24ms
step:410/1770 train_time:40097ms step_avg:100.24ms
step:411/1770 train_time:40199ms step_avg:100.25ms
step:412/1770 train_time:40302ms step_avg:100.25ms
step:413/1770 train_time:40405ms step_avg:100.26ms
step:414/1770 train_time:40508ms step_avg:100.27ms
step:415/1770 train_time:40609ms step_avg:100.27ms
step:416/1770 train_time:40712ms step_avg:100.28ms
step:417/1770 train_time:40815ms step_avg:100.28ms
step:418/1770 train_time:40917ms step_avg:100.29ms
step:419/1770 train_time:41019ms step_avg:100.29ms
step:420/1770 train_time:41122ms step_avg:100.30ms
step:421/1770 train_time:41224ms step_avg:100.30ms
step:422/1770 train_time:41327ms step_avg:100.31ms
step:423/1770 train_time:41429ms step_avg:100.31ms
step:424/1770 train_time:41531ms step_avg:100.32ms
step:425/1770 train_time:41633ms step_avg:100.32ms
step:426/1770 train_time:41735ms step_avg:100.33ms
step:427/1770 train_time:41837ms step_avg:100.33ms
step:428/1770 train_time:41940ms step_avg:100.33ms
step:429/1770 train_time:42042ms step_avg:100.34ms
step:430/1770 train_time:42144ms step_avg:100.34ms
step:431/1770 train_time:42247ms step_avg:100.35ms
step:432/1770 train_time:42348ms step_avg:100.35ms
step:433/1770 train_time:42450ms step_avg:100.35ms
step:434/1770 train_time:42552ms step_avg:100.36ms
step:435/1770 train_time:42654ms step_avg:100.36ms
step:436/1770 train_time:42756ms step_avg:100.37ms
step:437/1770 train_time:42858ms step_avg:100.37ms
step:438/1770 train_time:42961ms step_avg:100.38ms
step:439/1770 train_time:43063ms step_avg:100.38ms
step:440/1770 train_time:43166ms step_avg:100.39ms
step:441/1770 train_time:43268ms step_avg:100.39ms
step:442/1770 train_time:43370ms step_avg:100.39ms
step:443/1770 train_time:43473ms step_avg:100.40ms
step:444/1770 train_time:43575ms step_avg:100.40ms
step:445/1770 train_time:43677ms step_avg:100.41ms
step:446/1770 train_time:43779ms step_avg:100.41ms
step:447/1770 train_time:43882ms step_avg:100.42ms
step:448/1770 train_time:43984ms step_avg:100.42ms
step:449/1770 train_time:44087ms step_avg:100.43ms
step:450/1770 train_time:44189ms step_avg:100.43ms
step:451/1770 train_time:44291ms step_avg:100.43ms
step:452/1770 train_time:44394ms step_avg:100.44ms
step:453/1770 train_time:44496ms step_avg:100.44ms
step:454/1770 train_time:44597ms step_avg:100.44ms
step:455/1770 train_time:44700ms step_avg:100.45ms
step:456/1770 train_time:44803ms step_avg:100.45ms
step:457/1770 train_time:44906ms step_avg:100.46ms
step:458/1770 train_time:45007ms step_avg:100.46ms
step:459/1770 train_time:45109ms step_avg:100.47ms
step:460/1770 train_time:45211ms step_avg:100.47ms
step:461/1770 train_time:45313ms step_avg:100.47ms
step:462/1770 train_time:45415ms step_avg:100.48ms
step:463/1770 train_time:45517ms step_avg:100.48ms
step:464/1770 train_time:45619ms step_avg:100.48ms
step:465/1770 train_time:45721ms step_avg:100.49ms
step:466/1770 train_time:45824ms step_avg:100.49ms
step:467/1770 train_time:45926ms step_avg:100.50ms
step:468/1770 train_time:46028ms step_avg:100.50ms
step:469/1770 train_time:46130ms step_avg:100.50ms
step:470/1770 train_time:46232ms step_avg:100.51ms
step:471/1770 train_time:46335ms step_avg:100.51ms
step:472/1770 train_time:46436ms step_avg:100.51ms
step:473/1770 train_time:46539ms step_avg:100.52ms
step:474/1770 train_time:46641ms step_avg:100.52ms
step:475/1770 train_time:46744ms step_avg:100.52ms
step:476/1770 train_time:46847ms step_avg:100.53ms
step:477/1770 train_time:46949ms step_avg:100.53ms
step:478/1770 train_time:47051ms step_avg:100.54ms
step:479/1770 train_time:47153ms step_avg:100.54ms
step:480/1770 train_time:47255ms step_avg:100.54ms
step:481/1770 train_time:47357ms step_avg:100.55ms
step:482/1770 train_time:47459ms step_avg:100.55ms
step:483/1770 train_time:47561ms step_avg:100.55ms
step:484/1770 train_time:47664ms step_avg:100.56ms
step:485/1770 train_time:47766ms step_avg:100.56ms
step:486/1770 train_time:47868ms step_avg:100.56ms
step:487/1770 train_time:47969ms step_avg:100.56ms
step:488/1770 train_time:48071ms step_avg:100.57ms
step:489/1770 train_time:48174ms step_avg:100.57ms
step:490/1770 train_time:48275ms step_avg:100.57ms
step:491/1770 train_time:48377ms step_avg:100.58ms
step:492/1770 train_time:48480ms step_avg:100.58ms
step:493/1770 train_time:48582ms step_avg:100.58ms
step:494/1770 train_time:48685ms step_avg:100.59ms
step:495/1770 train_time:48787ms step_avg:100.59ms
step:496/1770 train_time:48889ms step_avg:100.59ms
step:497/1770 train_time:48991ms step_avg:100.60ms
step:498/1770 train_time:49093ms step_avg:100.60ms
step:499/1770 train_time:49196ms step_avg:100.60ms
step:500/1770 train_time:49297ms step_avg:100.61ms
step:500/1770 val_loss:3.7550 train_time:49399ms step_avg:100.81ms
step:501/1770 train_time:49417ms step_avg:100.64ms
step:502/1770 train_time:49513ms step_avg:100.64ms
step:503/1770 train_time:49620ms step_avg:100.65ms
step:504/1770 train_time:49723ms step_avg:100.65ms
step:505/1770 train_time:49825ms step_avg:100.66ms
step:506/1770 train_time:49927ms step_avg:100.66ms
step:507/1770 train_time:50030ms step_avg:100.66ms
step:508/1770 train_time:50132ms step_avg:100.67ms
step:509/1770 train_time:50234ms step_avg:100.67ms
step:510/1770 train_time:50336ms step_avg:100.67ms
step:511/1770 train_time:50439ms step_avg:100.68ms
step:512/1770 train_time:50541ms step_avg:100.68ms
step:513/1770 train_time:50643ms step_avg:100.68ms
step:514/1770 train_time:50745ms step_avg:100.68ms
step:515/1770 train_time:50848ms step_avg:100.69ms
step:516/1770 train_time:50950ms step_avg:100.69ms
step:517/1770 train_time:51052ms step_avg:100.69ms
step:518/1770 train_time:51154ms step_avg:100.70ms
step:519/1770 train_time:51257ms step_avg:100.70ms
step:520/1770 train_time:51359ms step_avg:100.70ms
step:521/1770 train_time:51461ms step_avg:100.71ms
step:522/1770 train_time:51563ms step_avg:100.71ms
step:523/1770 train_time:51666ms step_avg:100.71ms
step:524/1770 train_time:51769ms step_avg:100.72ms
step:525/1770 train_time:51872ms step_avg:100.72ms
step:526/1770 train_time:51974ms step_avg:100.72ms
step:527/1770 train_time:52076ms step_avg:100.73ms
step:528/1770 train_time:52178ms step_avg:100.73ms
step:529/1770 train_time:52281ms step_avg:100.73ms
step:530/1770 train_time:52384ms step_avg:100.74ms
step:531/1770 train_time:52487ms step_avg:100.74ms
step:532/1770 train_time:52590ms step_avg:100.75ms
step:533/1770 train_time:52693ms step_avg:100.75ms
step:534/1770 train_time:52795ms step_avg:100.75ms
step:535/1770 train_time:52898ms step_avg:100.76ms
step:536/1770 train_time:53000ms step_avg:100.76ms
step:537/1770 train_time:53103ms step_avg:100.76ms
step:538/1770 train_time:53205ms step_avg:100.77ms
step:539/1770 train_time:53308ms step_avg:100.77ms
step:540/1770 train_time:53412ms step_avg:100.78ms
step:541/1770 train_time:53514ms step_avg:100.78ms
step:542/1770 train_time:53615ms step_avg:100.78ms
step:543/1770 train_time:53718ms step_avg:100.79ms
step:544/1770 train_time:53821ms step_avg:100.79ms
step:545/1770 train_time:53924ms step_avg:100.79ms
step:546/1770 train_time:54027ms step_avg:100.80ms
step:547/1770 train_time:54130ms step_avg:100.80ms
step:548/1770 train_time:54233ms step_avg:100.80ms
step:549/1770 train_time:54335ms step_avg:100.81ms
step:550/1770 train_time:54437ms step_avg:100.81ms
step:551/1770 train_time:54539ms step_avg:100.81ms
step:552/1770 train_time:54642ms step_avg:100.81ms
step:553/1770 train_time:54744ms step_avg:100.82ms
step:554/1770 train_time:54847ms step_avg:100.82ms
step:555/1770 train_time:54949ms step_avg:100.82ms
step:556/1770 train_time:55052ms step_avg:100.83ms
step:557/1770 train_time:55155ms step_avg:100.83ms
step:558/1770 train_time:55257ms step_avg:100.83ms
step:559/1770 train_time:55360ms step_avg:100.84ms
step:560/1770 train_time:55462ms step_avg:100.84ms
step:561/1770 train_time:55564ms step_avg:100.84ms
step:562/1770 train_time:55667ms step_avg:100.85ms
step:563/1770 train_time:55771ms step_avg:100.85ms
step:564/1770 train_time:55873ms step_avg:100.85ms
step:565/1770 train_time:55976ms step_avg:100.86ms
step:566/1770 train_time:56078ms step_avg:100.86ms
step:567/1770 train_time:56181ms step_avg:100.86ms
step:568/1770 train_time:56283ms step_avg:100.87ms
step:569/1770 train_time:56386ms step_avg:100.87ms
step:570/1770 train_time:56489ms step_avg:100.87ms
step:571/1770 train_time:56592ms step_avg:100.88ms
step:572/1770 train_time:56694ms step_avg:100.88ms
step:573/1770 train_time:56796ms step_avg:100.88ms
step:574/1770 train_time:56899ms step_avg:100.88ms
step:575/1770 train_time:57001ms step_avg:100.89ms
step:576/1770 train_time:57103ms step_avg:100.89ms
step:577/1770 train_time:57207ms step_avg:100.89ms
step:578/1770 train_time:57309ms step_avg:100.90ms
step:579/1770 train_time:57412ms step_avg:100.90ms
step:580/1770 train_time:57514ms step_avg:100.90ms
step:581/1770 train_time:57617ms step_avg:100.91ms
step:582/1770 train_time:57720ms step_avg:100.91ms
step:583/1770 train_time:57822ms step_avg:100.91ms
step:584/1770 train_time:57924ms step_avg:100.91ms
step:585/1770 train_time:58027ms step_avg:100.92ms
step:586/1770 train_time:58130ms step_avg:100.92ms
step:587/1770 train_time:58233ms step_avg:100.92ms
step:588/1770 train_time:58335ms step_avg:100.92ms
step:589/1770 train_time:58436ms step_avg:100.93ms
step:590/1770 train_time:58539ms step_avg:100.93ms
step:591/1770 train_time:58641ms step_avg:100.93ms
step:592/1770 train_time:58743ms step_avg:100.93ms
step:593/1770 train_time:58846ms step_avg:100.94ms
step:594/1770 train_time:58949ms step_avg:100.94ms
step:595/1770 train_time:59052ms step_avg:100.94ms
step:596/1770 train_time:59155ms step_avg:100.95ms
step:597/1770 train_time:59258ms step_avg:100.95ms
step:598/1770 train_time:59360ms step_avg:100.95ms
step:599/1770 train_time:59463ms step_avg:100.96ms
step:600/1770 train_time:59565ms step_avg:100.96ms
step:601/1770 train_time:59668ms step_avg:100.96ms
step:602/1770 train_time:59771ms step_avg:100.96ms
step:603/1770 train_time:59874ms step_avg:100.97ms
step:604/1770 train_time:59976ms step_avg:100.97ms
step:605/1770 train_time:60079ms step_avg:100.97ms
step:606/1770 train_time:60181ms step_avg:100.97ms
step:607/1770 train_time:60283ms step_avg:100.98ms
step:608/1770 train_time:60386ms step_avg:100.98ms
step:609/1770 train_time:60489ms step_avg:100.98ms
step:610/1770 train_time:60592ms step_avg:100.99ms
step:611/1770 train_time:60694ms step_avg:100.99ms
step:612/1770 train_time:60796ms step_avg:100.99ms
step:613/1770 train_time:60899ms step_avg:100.99ms
step:614/1770 train_time:61001ms step_avg:101.00ms
step:615/1770 train_time:61103ms step_avg:101.00ms
step:616/1770 train_time:61206ms step_avg:101.00ms
step:617/1770 train_time:61309ms step_avg:101.00ms
step:618/1770 train_time:61413ms step_avg:101.01ms
step:619/1770 train_time:61516ms step_avg:101.01ms
step:620/1770 train_time:61618ms step_avg:101.01ms
step:621/1770 train_time:61721ms step_avg:101.02ms
step:622/1770 train_time:61823ms step_avg:101.02ms
step:623/1770 train_time:61926ms step_avg:101.02ms
step:624/1770 train_time:62029ms step_avg:101.02ms
step:625/1770 train_time:62131ms step_avg:101.03ms
step:625/1770 val_loss:3.6666 train_time:62232ms step_avg:101.19ms
step:626/1770 train_time:62250ms step_avg:101.05ms
step:627/1770 train_time:62347ms step_avg:101.05ms
step:628/1770 train_time:62454ms step_avg:101.06ms
step:629/1770 train_time:62557ms step_avg:101.06ms
step:630/1770 train_time:62660ms step_avg:101.06ms
step:631/1770 train_time:62762ms step_avg:101.07ms
step:632/1770 train_time:62864ms step_avg:101.07ms
step:633/1770 train_time:62967ms step_avg:101.07ms
step:634/1770 train_time:63070ms step_avg:101.07ms
step:635/1770 train_time:63173ms step_avg:101.08ms
step:636/1770 train_time:63276ms step_avg:101.08ms
step:637/1770 train_time:63378ms step_avg:101.08ms
step:638/1770 train_time:63481ms step_avg:101.08ms
step:639/1770 train_time:63583ms step_avg:101.09ms
step:640/1770 train_time:63686ms step_avg:101.09ms
step:641/1770 train_time:63789ms step_avg:101.09ms
step:642/1770 train_time:63892ms step_avg:101.09ms
step:643/1770 train_time:63994ms step_avg:101.10ms
step:644/1770 train_time:64097ms step_avg:101.10ms
step:645/1770 train_time:64200ms step_avg:101.10ms
step:646/1770 train_time:64302ms step_avg:101.10ms
step:647/1770 train_time:64405ms step_avg:101.11ms
step:648/1770 train_time:64507ms step_avg:101.11ms
step:649/1770 train_time:64610ms step_avg:101.11ms
step:650/1770 train_time:64713ms step_avg:101.11ms
step:651/1770 train_time:64816ms step_avg:101.12ms
step:652/1770 train_time:64918ms step_avg:101.12ms
step:653/1770 train_time:65020ms step_avg:101.12ms
step:654/1770 train_time:65122ms step_avg:101.12ms
step:655/1770 train_time:65225ms step_avg:101.12ms
step:656/1770 train_time:65327ms step_avg:101.13ms
step:657/1770 train_time:65430ms step_avg:101.13ms
step:658/1770 train_time:65534ms step_avg:101.13ms
step:659/1770 train_time:65638ms step_avg:101.14ms
step:660/1770 train_time:65743ms step_avg:101.14ms
step:661/1770 train_time:65847ms step_avg:101.15ms
step:662/1770 train_time:65952ms step_avg:101.15ms
step:663/1770 train_time:66056ms step_avg:101.16ms
step:664/1770 train_time:66160ms step_avg:101.16ms
step:665/1770 train_time:66264ms step_avg:101.17ms
step:666/1770 train_time:66367ms step_avg:101.17ms
step:667/1770 train_time:66472ms step_avg:101.18ms
step:668/1770 train_time:66577ms step_avg:101.18ms
step:669/1770 train_time:66681ms step_avg:101.18ms
step:670/1770 train_time:66785ms step_avg:101.19ms
step:671/1770 train_time:66889ms step_avg:101.19ms
step:672/1770 train_time:66993ms step_avg:101.20ms
step:673/1770 train_time:67098ms step_avg:101.20ms
step:674/1770 train_time:67202ms step_avg:101.21ms
step:675/1770 train_time:67306ms step_avg:101.21ms
step:676/1770 train_time:67411ms step_avg:101.22ms
step:677/1770 train_time:67516ms step_avg:101.22ms
step:678/1770 train_time:67620ms step_avg:101.23ms
step:679/1770 train_time:67724ms step_avg:101.23ms
step:680/1770 train_time:67828ms step_avg:101.24ms
step:681/1770 train_time:67933ms step_avg:101.24ms
step:682/1770 train_time:68037ms step_avg:101.25ms
step:683/1770 train_time:68141ms step_avg:101.25ms
step:684/1770 train_time:68245ms step_avg:101.25ms
step:685/1770 train_time:68349ms step_avg:101.26ms
step:686/1770 train_time:68455ms step_avg:101.26ms
step:687/1770 train_time:68559ms step_avg:101.27ms
step:688/1770 train_time:68664ms step_avg:101.27ms
step:689/1770 train_time:68769ms step_avg:101.28ms
step:690/1770 train_time:68873ms step_avg:101.28ms
step:691/1770 train_time:68978ms step_avg:101.29ms
step:692/1770 train_time:69081ms step_avg:101.29ms
step:693/1770 train_time:69185ms step_avg:101.30ms
step:694/1770 train_time:69288ms step_avg:101.30ms
step:695/1770 train_time:69393ms step_avg:101.30ms
step:696/1770 train_time:69498ms step_avg:101.31ms
step:697/1770 train_time:69602ms step_avg:101.31ms
step:698/1770 train_time:69706ms step_avg:101.32ms
step:699/1770 train_time:69810ms step_avg:101.32ms
step:700/1770 train_time:69915ms step_avg:101.33ms
step:701/1770 train_time:70019ms step_avg:101.33ms
step:702/1770 train_time:70124ms step_avg:101.34ms
step:703/1770 train_time:70228ms step_avg:101.34ms
step:704/1770 train_time:70333ms step_avg:101.34ms
step:705/1770 train_time:70437ms step_avg:101.35ms
step:706/1770 train_time:70541ms step_avg:101.35ms
step:707/1770 train_time:70645ms step_avg:101.36ms
step:708/1770 train_time:70750ms step_avg:101.36ms
step:709/1770 train_time:70855ms step_avg:101.37ms
step:710/1770 train_time:70959ms step_avg:101.37ms
step:711/1770 train_time:71063ms step_avg:101.37ms
step:712/1770 train_time:71167ms step_avg:101.38ms
step:713/1770 train_time:71272ms step_avg:101.38ms
step:714/1770 train_time:71376ms step_avg:101.39ms
step:715/1770 train_time:71480ms step_avg:101.39ms
step:716/1770 train_time:71584ms step_avg:101.39ms
step:717/1770 train_time:71687ms step_avg:101.40ms
step:718/1770 train_time:71792ms step_avg:101.40ms
step:719/1770 train_time:71897ms step_avg:101.41ms
step:720/1770 train_time:72001ms step_avg:101.41ms
step:721/1770 train_time:72106ms step_avg:101.41ms
step:722/1770 train_time:72210ms step_avg:101.42ms
step:723/1770 train_time:72315ms step_avg:101.42ms
step:724/1770 train_time:72419ms step_avg:101.43ms
step:725/1770 train_time:72524ms step_avg:101.43ms
step:726/1770 train_time:72628ms step_avg:101.44ms
step:727/1770 train_time:72733ms step_avg:101.44ms
step:728/1770 train_time:72837ms step_avg:101.44ms
step:729/1770 train_time:72942ms step_avg:101.45ms
step:730/1770 train_time:73046ms step_avg:101.45ms
step:731/1770 train_time:73150ms step_avg:101.46ms
step:732/1770 train_time:73255ms step_avg:101.46ms
step:733/1770 train_time:73359ms step_avg:101.47ms
step:734/1770 train_time:73463ms step_avg:101.47ms
step:735/1770 train_time:73567ms step_avg:101.47ms
step:736/1770 train_time:73673ms step_avg:101.48ms
step:737/1770 train_time:73777ms step_avg:101.48ms
step:738/1770 train_time:73881ms step_avg:101.48ms
step:739/1770 train_time:73985ms step_avg:101.49ms
step:740/1770 train_time:74089ms step_avg:101.49ms
step:741/1770 train_time:74194ms step_avg:101.50ms
step:742/1770 train_time:74298ms step_avg:101.50ms
step:743/1770 train_time:74402ms step_avg:101.50ms
step:744/1770 train_time:74506ms step_avg:101.51ms
step:745/1770 train_time:74610ms step_avg:101.51ms
step:746/1770 train_time:74714ms step_avg:101.51ms
step:747/1770 train_time:74818ms step_avg:101.52ms
step:748/1770 train_time:74922ms step_avg:101.52ms
step:749/1770 train_time:75026ms step_avg:101.52ms
step:750/1770 train_time:75131ms step_avg:101.53ms
step:750/1770 val_loss:3.6013 train_time:75234ms step_avg:101.67ms
step:751/1770 train_time:75251ms step_avg:101.55ms
step:752/1770 train_time:75355ms step_avg:101.56ms
step:753/1770 train_time:75461ms step_avg:101.56ms
step:754/1770 train_time:75566ms step_avg:101.57ms
step:755/1770 train_time:75671ms step_avg:101.57ms
step:756/1770 train_time:75775ms step_avg:101.58ms
step:757/1770 train_time:75879ms step_avg:101.58ms
step:758/1770 train_time:75983ms step_avg:101.58ms
step:759/1770 train_time:76088ms step_avg:101.59ms
step:760/1770 train_time:76192ms step_avg:101.59ms
step:761/1770 train_time:76296ms step_avg:101.59ms
step:762/1770 train_time:76400ms step_avg:101.60ms
step:763/1770 train_time:76505ms step_avg:101.60ms
step:764/1770 train_time:76609ms step_avg:101.60ms
step:765/1770 train_time:76713ms step_avg:101.61ms
step:766/1770 train_time:76817ms step_avg:101.61ms
step:767/1770 train_time:76922ms step_avg:101.61ms
step:768/1770 train_time:77026ms step_avg:101.62ms
step:769/1770 train_time:77130ms step_avg:101.62ms
step:770/1770 train_time:77234ms step_avg:101.62ms
step:771/1770 train_time:77338ms step_avg:101.63ms
step:772/1770 train_time:77443ms step_avg:101.63ms
step:773/1770 train_time:77547ms step_avg:101.63ms
step:774/1770 train_time:77651ms step_avg:101.64ms
step:775/1770 train_time:77756ms step_avg:101.64ms
step:776/1770 train_time:77860ms step_avg:101.64ms
step:777/1770 train_time:77964ms step_avg:101.65ms
step:778/1770 train_time:78069ms step_avg:101.65ms
step:779/1770 train_time:78173ms step_avg:101.66ms
step:780/1770 train_time:78278ms step_avg:101.66ms
step:781/1770 train_time:78382ms step_avg:101.66ms
step:782/1770 train_time:78486ms step_avg:101.67ms
step:783/1770 train_time:78590ms step_avg:101.67ms
step:784/1770 train_time:78694ms step_avg:101.67ms
step:785/1770 train_time:78799ms step_avg:101.68ms
step:786/1770 train_time:78903ms step_avg:101.68ms
step:787/1770 train_time:79007ms step_avg:101.68ms
step:788/1770 train_time:79111ms step_avg:101.69ms
step:789/1770 train_time:79217ms step_avg:101.69ms
step:790/1770 train_time:79321ms step_avg:101.69ms
step:791/1770 train_time:79426ms step_avg:101.70ms
step:792/1770 train_time:79530ms step_avg:101.70ms
step:793/1770 train_time:79634ms step_avg:101.70ms
step:794/1770 train_time:79738ms step_avg:101.71ms
step:795/1770 train_time:79843ms step_avg:101.71ms
step:796/1770 train_time:79948ms step_avg:101.72ms
step:797/1770 train_time:80053ms step_avg:101.72ms
step:798/1770 train_time:80156ms step_avg:101.72ms
step:799/1770 train_time:80261ms step_avg:101.73ms
step:800/1770 train_time:80366ms step_avg:101.73ms
step:801/1770 train_time:80470ms step_avg:101.73ms
step:802/1770 train_time:80575ms step_avg:101.74ms
step:803/1770 train_time:80680ms step_avg:101.74ms
step:804/1770 train_time:80784ms step_avg:101.74ms
step:805/1770 train_time:80889ms step_avg:101.75ms
step:806/1770 train_time:80993ms step_avg:101.75ms
step:807/1770 train_time:81098ms step_avg:101.75ms
step:808/1770 train_time:81202ms step_avg:101.76ms
step:809/1770 train_time:81307ms step_avg:101.76ms
step:810/1770 train_time:81412ms step_avg:101.77ms
step:811/1770 train_time:81517ms step_avg:101.77ms
step:812/1770 train_time:81621ms step_avg:101.77ms
step:813/1770 train_time:81726ms step_avg:101.78ms
step:814/1770 train_time:81831ms step_avg:101.78ms
step:815/1770 train_time:81935ms step_avg:101.78ms
step:816/1770 train_time:82039ms step_avg:101.79ms
step:817/1770 train_time:82145ms step_avg:101.79ms
step:818/1770 train_time:82249ms step_avg:101.79ms
step:819/1770 train_time:82353ms step_avg:101.80ms
step:820/1770 train_time:82457ms step_avg:101.80ms
step:821/1770 train_time:82561ms step_avg:101.80ms
step:822/1770 train_time:82667ms step_avg:101.81ms
step:823/1770 train_time:82771ms step_avg:101.81ms
step:824/1770 train_time:82876ms step_avg:101.81ms
step:825/1770 train_time:82980ms step_avg:101.82ms
step:826/1770 train_time:83084ms step_avg:101.82ms
step:827/1770 train_time:83190ms step_avg:101.82ms
step:828/1770 train_time:83294ms step_avg:101.83ms
step:829/1770 train_time:83399ms step_avg:101.83ms
step:830/1770 train_time:83503ms step_avg:101.83ms
step:831/1770 train_time:83608ms step_avg:101.84ms
step:832/1770 train_time:83712ms step_avg:101.84ms
step:833/1770 train_time:83817ms step_avg:101.84ms
step:834/1770 train_time:83922ms step_avg:101.85ms
step:835/1770 train_time:84027ms step_avg:101.85ms
step:836/1770 train_time:84131ms step_avg:101.85ms
step:837/1770 train_time:84236ms step_avg:101.86ms
step:838/1770 train_time:84340ms step_avg:101.86ms
step:839/1770 train_time:84445ms step_avg:101.86ms
step:840/1770 train_time:84550ms step_avg:101.87ms
step:841/1770 train_time:84654ms step_avg:101.87ms
step:842/1770 train_time:84758ms step_avg:101.87ms
step:843/1770 train_time:84863ms step_avg:101.88ms
step:844/1770 train_time:84968ms step_avg:101.88ms
step:845/1770 train_time:85072ms step_avg:101.88ms
step:846/1770 train_time:85176ms step_avg:101.89ms
step:847/1770 train_time:85281ms step_avg:101.89ms
step:848/1770 train_time:85385ms step_avg:101.89ms
step:849/1770 train_time:85490ms step_avg:101.89ms
step:850/1770 train_time:85594ms step_avg:101.90ms
step:851/1770 train_time:85698ms step_avg:101.90ms
step:852/1770 train_time:85803ms step_avg:101.90ms
step:853/1770 train_time:85907ms step_avg:101.91ms
step:854/1770 train_time:86012ms step_avg:101.91ms
step:855/1770 train_time:86116ms step_avg:101.91ms
step:856/1770 train_time:86220ms step_avg:101.92ms
step:857/1770 train_time:86326ms step_avg:101.92ms
step:858/1770 train_time:86430ms step_avg:101.92ms
step:859/1770 train_time:86535ms step_avg:101.93ms
step:860/1770 train_time:86639ms step_avg:101.93ms
step:861/1770 train_time:86744ms step_avg:101.93ms
step:862/1770 train_time:86848ms step_avg:101.93ms
step:863/1770 train_time:86952ms step_avg:101.94ms
step:864/1770 train_time:87057ms step_avg:101.94ms
step:865/1770 train_time:87161ms step_avg:101.94ms
step:866/1770 train_time:87267ms step_avg:101.95ms
step:867/1770 train_time:87371ms step_avg:101.95ms
step:868/1770 train_time:87476ms step_avg:101.95ms
step:869/1770 train_time:87580ms step_avg:101.96ms
step:870/1770 train_time:87685ms step_avg:101.96ms
step:871/1770 train_time:87789ms step_avg:101.96ms
step:872/1770 train_time:87893ms step_avg:101.96ms
step:873/1770 train_time:87997ms step_avg:101.97ms
step:874/1770 train_time:88102ms step_avg:101.97ms
step:875/1770 train_time:88206ms step_avg:101.97ms
step:875/1770 val_loss:3.5524 train_time:88309ms step_avg:102.09ms
step:876/1770 train_time:88326ms step_avg:101.99ms
step:877/1770 train_time:88429ms step_avg:101.99ms
step:878/1770 train_time:88536ms step_avg:102.00ms
step:879/1770 train_time:88642ms step_avg:102.00ms
step:880/1770 train_time:88746ms step_avg:102.01ms
step:881/1770 train_time:88851ms step_avg:102.01ms
step:882/1770 train_time:88955ms step_avg:102.01ms
step:883/1770 train_time:89060ms step_avg:102.02ms
step:884/1770 train_time:89165ms step_avg:102.02ms
step:885/1770 train_time:89269ms step_avg:102.02ms
step:886/1770 train_time:89373ms step_avg:102.02ms
step:887/1770 train_time:89478ms step_avg:102.03ms
step:888/1770 train_time:89582ms step_avg:102.03ms
step:889/1770 train_time:89687ms step_avg:102.03ms
step:890/1770 train_time:89791ms step_avg:102.04ms
step:891/1770 train_time:89895ms step_avg:102.04ms
step:892/1770 train_time:90001ms step_avg:102.04ms
step:893/1770 train_time:90105ms step_avg:102.04ms
step:894/1770 train_time:90211ms step_avg:102.05ms
step:895/1770 train_time:90315ms step_avg:102.05ms
step:896/1770 train_time:90421ms step_avg:102.05ms
step:897/1770 train_time:90525ms step_avg:102.06ms
step:898/1770 train_time:90630ms step_avg:102.06ms
step:899/1770 train_time:90733ms step_avg:102.06ms
step:900/1770 train_time:90839ms step_avg:102.07ms
step:901/1770 train_time:90944ms step_avg:102.07ms
step:902/1770 train_time:91049ms step_avg:102.07ms
step:903/1770 train_time:91153ms step_avg:102.07ms
step:904/1770 train_time:91258ms step_avg:102.08ms
step:905/1770 train_time:91363ms step_avg:102.08ms
step:906/1770 train_time:91467ms step_avg:102.08ms
step:907/1770 train_time:91571ms step_avg:102.09ms
step:908/1770 train_time:91676ms step_avg:102.09ms
step:909/1770 train_time:91780ms step_avg:102.09ms
step:910/1770 train_time:91885ms step_avg:102.09ms
step:911/1770 train_time:91990ms step_avg:102.10ms
step:912/1770 train_time:92094ms step_avg:102.10ms
step:913/1770 train_time:92199ms step_avg:102.10ms
step:914/1770 train_time:92304ms step_avg:102.11ms
step:915/1770 train_time:92408ms step_avg:102.11ms
step:916/1770 train_time:92512ms step_avg:102.11ms
step:917/1770 train_time:92617ms step_avg:102.11ms
step:918/1770 train_time:92722ms step_avg:102.12ms
step:919/1770 train_time:92826ms step_avg:102.12ms
step:920/1770 train_time:92931ms step_avg:102.12ms
step:921/1770 train_time:93038ms step_avg:102.13ms
step:922/1770 train_time:93145ms step_avg:102.13ms
step:923/1770 train_time:93251ms step_avg:102.14ms
step:924/1770 train_time:93356ms step_avg:102.14ms
step:925/1770 train_time:93462ms step_avg:102.14ms
step:926/1770 train_time:93568ms step_avg:102.15ms
step:927/1770 train_time:93674ms step_avg:102.15ms
step:928/1770 train_time:93780ms step_avg:102.16ms
step:929/1770 train_time:93885ms step_avg:102.16ms
step:930/1770 train_time:93991ms step_avg:102.16ms
step:931/1770 train_time:94097ms step_avg:102.17ms
step:932/1770 train_time:94203ms step_avg:102.17ms
step:933/1770 train_time:94309ms step_avg:102.18ms
step:934/1770 train_time:94414ms step_avg:102.18ms
step:935/1770 train_time:94520ms step_avg:102.18ms
step:936/1770 train_time:94626ms step_avg:102.19ms
step:937/1770 train_time:94732ms step_avg:102.19ms
step:938/1770 train_time:94839ms step_avg:102.20ms
step:939/1770 train_time:94944ms step_avg:102.20ms
step:940/1770 train_time:95051ms step_avg:102.20ms
step:941/1770 train_time:95157ms step_avg:102.21ms
step:942/1770 train_time:95263ms step_avg:102.21ms
step:943/1770 train_time:95369ms step_avg:102.22ms
step:944/1770 train_time:95474ms step_avg:102.22ms
step:945/1770 train_time:95581ms step_avg:102.23ms
step:946/1770 train_time:95687ms step_avg:102.23ms
step:947/1770 train_time:95793ms step_avg:102.23ms
step:948/1770 train_time:95899ms step_avg:102.24ms
step:949/1770 train_time:96005ms step_avg:102.24ms
step:950/1770 train_time:96111ms step_avg:102.25ms
step:951/1770 train_time:96219ms step_avg:102.25ms
step:952/1770 train_time:96324ms step_avg:102.26ms
step:953/1770 train_time:96430ms step_avg:102.26ms
step:954/1770 train_time:96536ms step_avg:102.26ms
step:955/1770 train_time:96643ms step_avg:102.27ms
step:956/1770 train_time:96748ms step_avg:102.27ms
step:957/1770 train_time:96854ms step_avg:102.27ms
step:958/1770 train_time:96960ms step_avg:102.28ms
step:959/1770 train_time:97065ms step_avg:102.28ms
step:960/1770 train_time:97171ms step_avg:102.28ms
step:961/1770 train_time:97276ms step_avg:102.29ms
step:962/1770 train_time:97382ms step_avg:102.29ms
step:963/1770 train_time:97487ms step_avg:102.29ms
step:964/1770 train_time:97593ms step_avg:102.30ms
step:965/1770 train_time:97700ms step_avg:102.30ms
step:966/1770 train_time:97805ms step_avg:102.31ms
step:967/1770 train_time:97912ms step_avg:102.31ms
step:968/1770 train_time:98018ms step_avg:102.31ms
step:969/1770 train_time:98124ms step_avg:102.32ms
step:970/1770 train_time:98229ms step_avg:102.32ms
step:971/1770 train_time:98335ms step_avg:102.33ms
step:972/1770 train_time:98442ms step_avg:102.33ms
step:973/1770 train_time:98547ms step_avg:102.33ms
step:974/1770 train_time:98652ms step_avg:102.34ms
step:975/1770 train_time:98759ms step_avg:102.34ms
step:976/1770 train_time:98864ms step_avg:102.34ms
step:977/1770 train_time:98970ms step_avg:102.35ms
step:978/1770 train_time:99076ms step_avg:102.35ms
step:979/1770 train_time:99181ms step_avg:102.35ms
step:980/1770 train_time:99287ms step_avg:102.36ms
step:981/1770 train_time:99393ms step_avg:102.36ms
step:982/1770 train_time:99500ms step_avg:102.37ms
step:983/1770 train_time:99606ms step_avg:102.37ms
step:984/1770 train_time:99712ms step_avg:102.37ms
step:985/1770 train_time:99818ms step_avg:102.38ms
step:986/1770 train_time:99924ms step_avg:102.38ms
step:987/1770 train_time:100030ms step_avg:102.38ms
step:988/1770 train_time:100135ms step_avg:102.39ms
step:989/1770 train_time:100243ms step_avg:102.39ms
step:990/1770 train_time:100349ms step_avg:102.40ms
step:991/1770 train_time:100455ms step_avg:102.40ms
step:992/1770 train_time:100562ms step_avg:102.41ms
step:993/1770 train_time:100668ms step_avg:102.41ms
step:994/1770 train_time:100774ms step_avg:102.41ms
step:995/1770 train_time:100880ms step_avg:102.42ms
step:996/1770 train_time:100986ms step_avg:102.42ms
step:997/1770 train_time:101092ms step_avg:102.42ms
step:998/1770 train_time:101198ms step_avg:102.43ms
step:999/1770 train_time:101304ms step_avg:102.43ms
step:1000/1770 train_time:101410ms step_avg:102.43ms
step:1000/1770 val_loss:3.5123 train_time:101514ms step_avg:102.54ms
step:1001/1770 train_time:101532ms step_avg:102.45ms
step:1002/1770 train_time:101631ms step_avg:102.45ms
step:1003/1770 train_time:101740ms step_avg:102.46ms
step:1004/1770 train_time:101846ms step_avg:102.46ms
step:1005/1770 train_time:101952ms step_avg:102.46ms
step:1006/1770 train_time:102058ms step_avg:102.47ms
step:1007/1770 train_time:102164ms step_avg:102.47ms
step:1008/1770 train_time:102270ms step_avg:102.47ms
step:1009/1770 train_time:102376ms step_avg:102.48ms
step:1010/1770 train_time:102482ms step_avg:102.48ms
step:1011/1770 train_time:102588ms step_avg:102.49ms
step:1012/1770 train_time:102695ms step_avg:102.49ms
step:1013/1770 train_time:102801ms step_avg:102.49ms
step:1014/1770 train_time:102907ms step_avg:102.50ms
step:1015/1770 train_time:103012ms step_avg:102.50ms
step:1016/1770 train_time:103119ms step_avg:102.50ms
step:1017/1770 train_time:103224ms step_avg:102.51ms
step:1018/1770 train_time:103330ms step_avg:102.51ms
step:1019/1770 train_time:103435ms step_avg:102.51ms
step:1020/1770 train_time:103542ms step_avg:102.52ms
step:1021/1770 train_time:103648ms step_avg:102.52ms
step:1022/1770 train_time:103754ms step_avg:102.52ms
step:1023/1770 train_time:103860ms step_avg:102.53ms
step:1024/1770 train_time:103966ms step_avg:102.53ms
step:1025/1770 train_time:104072ms step_avg:102.53ms
step:1026/1770 train_time:104179ms step_avg:102.54ms
step:1027/1770 train_time:104286ms step_avg:102.54ms
step:1028/1770 train_time:104392ms step_avg:102.55ms
step:1029/1770 train_time:104498ms step_avg:102.55ms
step:1030/1770 train_time:104604ms step_avg:102.55ms
step:1031/1770 train_time:104710ms step_avg:102.56ms
step:1032/1770 train_time:104815ms step_avg:102.56ms
step:1033/1770 train_time:104921ms step_avg:102.56ms
step:1034/1770 train_time:105026ms step_avg:102.56ms
step:1035/1770 train_time:105133ms step_avg:102.57ms
step:1036/1770 train_time:105239ms step_avg:102.57ms
step:1037/1770 train_time:105344ms step_avg:102.57ms
step:1038/1770 train_time:105450ms step_avg:102.58ms
step:1039/1770 train_time:105556ms step_avg:102.58ms
step:1040/1770 train_time:105663ms step_avg:102.59ms
step:1041/1770 train_time:105768ms step_avg:102.59ms
step:1042/1770 train_time:105874ms step_avg:102.59ms
step:1043/1770 train_time:105980ms step_avg:102.59ms
step:1044/1770 train_time:106086ms step_avg:102.60ms
step:1045/1770 train_time:106192ms step_avg:102.60ms
step:1046/1770 train_time:106299ms step_avg:102.60ms
step:1047/1770 train_time:106404ms step_avg:102.61ms
step:1048/1770 train_time:106511ms step_avg:102.61ms
step:1049/1770 train_time:106617ms step_avg:102.62ms
step:1050/1770 train_time:106724ms step_avg:102.62ms
step:1051/1770 train_time:106830ms step_avg:102.62ms
step:1052/1770 train_time:106936ms step_avg:102.63ms
step:1053/1770 train_time:107042ms step_avg:102.63ms
step:1054/1770 train_time:107149ms step_avg:102.63ms
step:1055/1770 train_time:107255ms step_avg:102.64ms
step:1056/1770 train_time:107361ms step_avg:102.64ms
step:1057/1770 train_time:107468ms step_avg:102.64ms
step:1058/1770 train_time:107574ms step_avg:102.65ms
step:1059/1770 train_time:107680ms step_avg:102.65ms
step:1060/1770 train_time:107786ms step_avg:102.65ms
step:1061/1770 train_time:107892ms step_avg:102.66ms
step:1062/1770 train_time:108000ms step_avg:102.66ms
step:1063/1770 train_time:108108ms step_avg:102.67ms
step:1064/1770 train_time:108214ms step_avg:102.67ms
step:1065/1770 train_time:108321ms step_avg:102.67ms
step:1066/1770 train_time:108427ms step_avg:102.68ms
step:1067/1770 train_time:108533ms step_avg:102.68ms
step:1068/1770 train_time:108640ms step_avg:102.68ms
step:1069/1770 train_time:108746ms step_avg:102.69ms
step:1070/1770 train_time:108851ms step_avg:102.69ms
step:1071/1770 train_time:108959ms step_avg:102.69ms
step:1072/1770 train_time:109065ms step_avg:102.70ms
step:1073/1770 train_time:109170ms step_avg:102.70ms
step:1074/1770 train_time:109277ms step_avg:102.70ms
step:1075/1770 train_time:109384ms step_avg:102.71ms
step:1076/1770 train_time:109490ms step_avg:102.71ms
step:1077/1770 train_time:109597ms step_avg:102.71ms
step:1078/1770 train_time:109703ms step_avg:102.72ms
step:1079/1770 train_time:109810ms step_avg:102.72ms
step:1080/1770 train_time:109916ms step_avg:102.73ms
step:1081/1770 train_time:110022ms step_avg:102.73ms
step:1082/1770 train_time:110128ms step_avg:102.73ms
step:1083/1770 train_time:110235ms step_avg:102.73ms
step:1084/1770 train_time:110341ms step_avg:102.74ms
step:1085/1770 train_time:110447ms step_avg:102.74ms
step:1086/1770 train_time:110552ms step_avg:102.74ms
step:1087/1770 train_time:110659ms step_avg:102.75ms
step:1088/1770 train_time:110765ms step_avg:102.75ms
step:1089/1770 train_time:110870ms step_avg:102.75ms
step:1090/1770 train_time:110979ms step_avg:102.76ms
step:1091/1770 train_time:111085ms step_avg:102.76ms
step:1092/1770 train_time:111190ms step_avg:102.76ms
step:1093/1770 train_time:111297ms step_avg:102.77ms
step:1094/1770 train_time:111404ms step_avg:102.77ms
step:1095/1770 train_time:111510ms step_avg:102.77ms
step:1096/1770 train_time:111617ms step_avg:102.78ms
step:1097/1770 train_time:111723ms step_avg:102.78ms
step:1098/1770 train_time:111828ms step_avg:102.78ms
step:1099/1770 train_time:111934ms step_avg:102.79ms
step:1100/1770 train_time:112040ms step_avg:102.79ms
step:1101/1770 train_time:112146ms step_avg:102.79ms
step:1102/1770 train_time:112252ms step_avg:102.79ms
step:1103/1770 train_time:112359ms step_avg:102.80ms
step:1104/1770 train_time:112465ms step_avg:102.80ms
step:1105/1770 train_time:112571ms step_avg:102.80ms
step:1106/1770 train_time:112678ms step_avg:102.81ms
step:1107/1770 train_time:112784ms step_avg:102.81ms
step:1108/1770 train_time:112890ms step_avg:102.81ms
step:1109/1770 train_time:112997ms step_avg:102.82ms
step:1110/1770 train_time:113103ms step_avg:102.82ms
step:1111/1770 train_time:113211ms step_avg:102.83ms
step:1112/1770 train_time:113317ms step_avg:102.83ms
step:1113/1770 train_time:113424ms step_avg:102.83ms
step:1114/1770 train_time:113531ms step_avg:102.84ms
step:1115/1770 train_time:113638ms step_avg:102.84ms
step:1116/1770 train_time:113744ms step_avg:102.84ms
step:1117/1770 train_time:113850ms step_avg:102.85ms
step:1118/1770 train_time:113957ms step_avg:102.85ms
step:1119/1770 train_time:114063ms step_avg:102.85ms
step:1120/1770 train_time:114169ms step_avg:102.86ms
step:1121/1770 train_time:114274ms step_avg:102.86ms
step:1122/1770 train_time:114381ms step_avg:102.86ms
step:1123/1770 train_time:114486ms step_avg:102.86ms
step:1124/1770 train_time:114593ms step_avg:102.87ms
step:1125/1770 train_time:114699ms step_avg:102.87ms
step:1125/1770 val_loss:3.4716 train_time:114803ms step_avg:102.96ms
step:1126/1770 train_time:114821ms step_avg:102.89ms
step:1127/1770 train_time:114924ms step_avg:102.89ms
step:1128/1770 train_time:115033ms step_avg:102.89ms
step:1129/1770 train_time:115139ms step_avg:102.89ms
step:1130/1770 train_time:115245ms step_avg:102.90ms
step:1131/1770 train_time:115352ms step_avg:102.90ms
step:1132/1770 train_time:115459ms step_avg:102.90ms
step:1133/1770 train_time:115565ms step_avg:102.91ms
step:1134/1770 train_time:115672ms step_avg:102.91ms
step:1135/1770 train_time:115778ms step_avg:102.91ms
step:1136/1770 train_time:115883ms step_avg:102.92ms
step:1137/1770 train_time:115991ms step_avg:102.92ms
step:1138/1770 train_time:116097ms step_avg:102.92ms
step:1139/1770 train_time:116203ms step_avg:102.93ms
step:1140/1770 train_time:116309ms step_avg:102.93ms
step:1141/1770 train_time:116416ms step_avg:102.93ms
step:1142/1770 train_time:116523ms step_avg:102.94ms
step:1143/1770 train_time:116629ms step_avg:102.94ms
step:1144/1770 train_time:116736ms step_avg:102.94ms
step:1145/1770 train_time:116842ms step_avg:102.94ms
step:1146/1770 train_time:116948ms step_avg:102.95ms
step:1147/1770 train_time:117055ms step_avg:102.95ms
step:1148/1770 train_time:117161ms step_avg:102.95ms
step:1149/1770 train_time:117267ms step_avg:102.96ms
step:1150/1770 train_time:117374ms step_avg:102.96ms
step:1151/1770 train_time:117480ms step_avg:102.96ms
step:1152/1770 train_time:117587ms step_avg:102.97ms
step:1153/1770 train_time:117693ms step_avg:102.97ms
step:1154/1770 train_time:117799ms step_avg:102.97ms
step:1155/1770 train_time:117906ms step_avg:102.97ms
step:1156/1770 train_time:118012ms step_avg:102.98ms
step:1157/1770 train_time:118120ms step_avg:102.98ms
step:1158/1770 train_time:118226ms step_avg:102.98ms
step:1159/1770 train_time:118331ms step_avg:102.99ms
step:1160/1770 train_time:118438ms step_avg:102.99ms
step:1161/1770 train_time:118544ms step_avg:102.99ms
step:1162/1770 train_time:118651ms step_avg:103.00ms
step:1163/1770 train_time:118758ms step_avg:103.00ms
step:1164/1770 train_time:118864ms step_avg:103.00ms
step:1165/1770 train_time:118971ms step_avg:103.00ms
step:1166/1770 train_time:119077ms step_avg:103.01ms
step:1167/1770 train_time:119184ms step_avg:103.01ms
step:1168/1770 train_time:119291ms step_avg:103.01ms
step:1169/1770 train_time:119397ms step_avg:103.02ms
step:1170/1770 train_time:119502ms step_avg:103.02ms
step:1171/1770 train_time:119609ms step_avg:103.02ms
step:1172/1770 train_time:119716ms step_avg:103.03ms
step:1173/1770 train_time:119822ms step_avg:103.03ms
step:1174/1770 train_time:119928ms step_avg:103.03ms
step:1175/1770 train_time:120034ms step_avg:103.03ms
step:1176/1770 train_time:120140ms step_avg:103.04ms
step:1177/1770 train_time:120246ms step_avg:103.04ms
step:1178/1770 train_time:120353ms step_avg:103.04ms
step:1179/1770 train_time:120458ms step_avg:103.04ms
step:1180/1770 train_time:120565ms step_avg:103.05ms
step:1181/1770 train_time:120671ms step_avg:103.05ms
step:1182/1770 train_time:120778ms step_avg:103.05ms
step:1183/1770 train_time:120885ms step_avg:103.06ms
step:1184/1770 train_time:120995ms step_avg:103.06ms
step:1185/1770 train_time:121101ms step_avg:103.06ms
step:1186/1770 train_time:121210ms step_avg:103.07ms
step:1187/1770 train_time:121319ms step_avg:103.08ms
step:1188/1770 train_time:121426ms step_avg:103.08ms
step:1189/1770 train_time:121534ms step_avg:103.08ms
step:1190/1770 train_time:121641ms step_avg:103.09ms
step:1191/1770 train_time:121748ms step_avg:103.09ms
step:1192/1770 train_time:121856ms step_avg:103.09ms
step:1193/1770 train_time:121963ms step_avg:103.10ms
step:1194/1770 train_time:122069ms step_avg:103.10ms
step:1195/1770 train_time:122178ms step_avg:103.10ms
step:1196/1770 train_time:122286ms step_avg:103.11ms
step:1197/1770 train_time:122393ms step_avg:103.11ms
step:1198/1770 train_time:122500ms step_avg:103.11ms
step:1199/1770 train_time:122608ms step_avg:103.12ms
step:1200/1770 train_time:122716ms step_avg:103.12ms
step:1201/1770 train_time:122825ms step_avg:103.13ms
step:1202/1770 train_time:122932ms step_avg:103.13ms
step:1203/1770 train_time:123040ms step_avg:103.13ms
step:1204/1770 train_time:123147ms step_avg:103.14ms
step:1205/1770 train_time:123254ms step_avg:103.14ms
step:1206/1770 train_time:123363ms step_avg:103.15ms
step:1207/1770 train_time:123469ms step_avg:103.15ms
step:1208/1770 train_time:123578ms step_avg:103.15ms
step:1209/1770 train_time:123684ms step_avg:103.16ms
step:1210/1770 train_time:123791ms step_avg:103.16ms
step:1211/1770 train_time:123899ms step_avg:103.16ms
step:1212/1770 train_time:124009ms step_avg:103.17ms
step:1213/1770 train_time:124116ms step_avg:103.17ms
step:1214/1770 train_time:124224ms step_avg:103.18ms
step:1215/1770 train_time:124331ms step_avg:103.18ms
step:1216/1770 train_time:124441ms step_avg:103.18ms
step:1217/1770 train_time:124548ms step_avg:103.19ms
step:1218/1770 train_time:124655ms step_avg:103.19ms
step:1219/1770 train_time:124763ms step_avg:103.20ms
step:1220/1770 train_time:124870ms step_avg:103.20ms
step:1221/1770 train_time:124978ms step_avg:103.20ms
step:1222/1770 train_time:125086ms step_avg:103.21ms
step:1223/1770 train_time:125194ms step_avg:103.21ms
step:1224/1770 train_time:125302ms step_avg:103.21ms
step:1225/1770 train_time:125411ms step_avg:103.22ms
step:1226/1770 train_time:125519ms step_avg:103.22ms
step:1227/1770 train_time:125628ms step_avg:103.23ms
step:1228/1770 train_time:125739ms step_avg:103.23ms
step:1229/1770 train_time:125846ms step_avg:103.24ms
step:1230/1770 train_time:125954ms step_avg:103.24ms
step:1231/1770 train_time:126062ms step_avg:103.24ms
step:1232/1770 train_time:126169ms step_avg:103.25ms
step:1233/1770 train_time:126276ms step_avg:103.25ms
step:1234/1770 train_time:126384ms step_avg:103.26ms
step:1235/1770 train_time:126491ms step_avg:103.26ms
step:1236/1770 train_time:126599ms step_avg:103.26ms
step:1237/1770 train_time:126706ms step_avg:103.26ms
step:1238/1770 train_time:126814ms step_avg:103.27ms
step:1239/1770 train_time:126922ms step_avg:103.27ms
step:1240/1770 train_time:127029ms step_avg:103.28ms
step:1241/1770 train_time:127138ms step_avg:103.28ms
step:1242/1770 train_time:127245ms step_avg:103.28ms
step:1243/1770 train_time:127353ms step_avg:103.29ms
step:1244/1770 train_time:127459ms step_avg:103.29ms
step:1245/1770 train_time:127567ms step_avg:103.29ms
step:1246/1770 train_time:127676ms step_avg:103.30ms
step:1247/1770 train_time:127783ms step_avg:103.30ms
step:1248/1770 train_time:127891ms step_avg:103.30ms
step:1249/1770 train_time:127998ms step_avg:103.31ms
step:1250/1770 train_time:128106ms step_avg:103.31ms
step:1250/1770 val_loss:3.4241 train_time:128214ms step_avg:103.40ms
step:1251/1770 train_time:128233ms step_avg:103.33ms
step:1252/1770 train_time:128332ms step_avg:103.33ms
step:1253/1770 train_time:128442ms step_avg:103.33ms
step:1254/1770 train_time:128549ms step_avg:103.34ms
step:1255/1770 train_time:128658ms step_avg:103.34ms
step:1256/1770 train_time:128765ms step_avg:103.34ms
step:1257/1770 train_time:128873ms step_avg:103.35ms
step:1258/1770 train_time:128979ms step_avg:103.35ms
step:1259/1770 train_time:129088ms step_avg:103.35ms
step:1260/1770 train_time:129194ms step_avg:103.36ms
step:1261/1770 train_time:129304ms step_avg:103.36ms
step:1262/1770 train_time:129412ms step_avg:103.36ms
step:1263/1770 train_time:129519ms step_avg:103.37ms
step:1264/1770 train_time:129628ms step_avg:103.37ms
step:1265/1770 train_time:129735ms step_avg:103.37ms
step:1266/1770 train_time:129842ms step_avg:103.38ms
step:1267/1770 train_time:129950ms step_avg:103.38ms
step:1268/1770 train_time:130058ms step_avg:103.38ms
step:1269/1770 train_time:130166ms step_avg:103.39ms
step:1270/1770 train_time:130275ms step_avg:103.39ms
step:1271/1770 train_time:130383ms step_avg:103.40ms
step:1272/1770 train_time:130490ms step_avg:103.40ms
step:1273/1770 train_time:130599ms step_avg:103.40ms
step:1274/1770 train_time:130706ms step_avg:103.41ms
step:1275/1770 train_time:130814ms step_avg:103.41ms
step:1276/1770 train_time:130921ms step_avg:103.41ms
step:1277/1770 train_time:131028ms step_avg:103.42ms
step:1278/1770 train_time:131136ms step_avg:103.42ms
step:1279/1770 train_time:131243ms step_avg:103.42ms
step:1280/1770 train_time:131352ms step_avg:103.43ms
step:1281/1770 train_time:131459ms step_avg:103.43ms
step:1282/1770 train_time:131567ms step_avg:103.43ms
step:1283/1770 train_time:131675ms step_avg:103.44ms
step:1284/1770 train_time:131783ms step_avg:103.44ms
step:1285/1770 train_time:131891ms step_avg:103.44ms
step:1286/1770 train_time:132000ms step_avg:103.45ms
step:1287/1770 train_time:132110ms step_avg:103.45ms
step:1288/1770 train_time:132218ms step_avg:103.46ms
step:1289/1770 train_time:132326ms step_avg:103.46ms
step:1290/1770 train_time:132433ms step_avg:103.46ms
step:1291/1770 train_time:132541ms step_avg:103.47ms
step:1292/1770 train_time:132648ms step_avg:103.47ms
step:1293/1770 train_time:132755ms step_avg:103.47ms
step:1294/1770 train_time:132863ms step_avg:103.48ms
step:1295/1770 train_time:132970ms step_avg:103.48ms
step:1296/1770 train_time:133077ms step_avg:103.48ms
step:1297/1770 train_time:133184ms step_avg:103.48ms
step:1298/1770 train_time:133292ms step_avg:103.49ms
step:1299/1770 train_time:133399ms step_avg:103.49ms
step:1300/1770 train_time:133506ms step_avg:103.49ms
step:1301/1770 train_time:133615ms step_avg:103.50ms
step:1302/1770 train_time:133723ms step_avg:103.50ms
step:1303/1770 train_time:133830ms step_avg:103.50ms
step:1304/1770 train_time:133937ms step_avg:103.51ms
step:1305/1770 train_time:134045ms step_avg:103.51ms
step:1306/1770 train_time:134152ms step_avg:103.51ms
step:1307/1770 train_time:134259ms step_avg:103.52ms
step:1308/1770 train_time:134366ms step_avg:103.52ms
step:1309/1770 train_time:134474ms step_avg:103.52ms
step:1310/1770 train_time:134582ms step_avg:103.52ms
step:1311/1770 train_time:134689ms step_avg:103.53ms
step:1312/1770 train_time:134795ms step_avg:103.53ms
step:1313/1770 train_time:134902ms step_avg:103.53ms
step:1314/1770 train_time:135010ms step_avg:103.54ms
step:1315/1770 train_time:135117ms step_avg:103.54ms
step:1316/1770 train_time:135225ms step_avg:103.54ms
step:1317/1770 train_time:135333ms step_avg:103.54ms
step:1318/1770 train_time:135443ms step_avg:103.55ms
step:1319/1770 train_time:135550ms step_avg:103.55ms
step:1320/1770 train_time:135657ms step_avg:103.56ms
step:1321/1770 train_time:135766ms step_avg:103.56ms
step:1322/1770 train_time:135872ms step_avg:103.56ms
step:1323/1770 train_time:135981ms step_avg:103.57ms
step:1324/1770 train_time:136089ms step_avg:103.57ms
step:1325/1770 train_time:136198ms step_avg:103.57ms
step:1326/1770 train_time:136305ms step_avg:103.58ms
step:1327/1770 train_time:136415ms step_avg:103.58ms
step:1328/1770 train_time:136523ms step_avg:103.58ms
step:1329/1770 train_time:136630ms step_avg:103.59ms
step:1330/1770 train_time:136737ms step_avg:103.59ms
step:1331/1770 train_time:136844ms step_avg:103.59ms
step:1332/1770 train_time:136951ms step_avg:103.59ms
step:1333/1770 train_time:137057ms step_avg:103.60ms
step:1334/1770 train_time:137166ms step_avg:103.60ms
step:1335/1770 train_time:137273ms step_avg:103.60ms
step:1336/1770 train_time:137380ms step_avg:103.61ms
step:1337/1770 train_time:137489ms step_avg:103.61ms
step:1338/1770 train_time:137595ms step_avg:103.61ms
step:1339/1770 train_time:137703ms step_avg:103.61ms
step:1340/1770 train_time:137813ms step_avg:103.62ms
step:1341/1770 train_time:137921ms step_avg:103.62ms
step:1342/1770 train_time:138029ms step_avg:103.63ms
step:1343/1770 train_time:138138ms step_avg:103.63ms
step:1344/1770 train_time:138247ms step_avg:103.63ms
step:1345/1770 train_time:138354ms step_avg:103.64ms
step:1346/1770 train_time:138462ms step_avg:103.64ms
step:1347/1770 train_time:138569ms step_avg:103.64ms
step:1348/1770 train_time:138679ms step_avg:103.65ms
step:1349/1770 train_time:138787ms step_avg:103.65ms
step:1350/1770 train_time:138895ms step_avg:103.65ms
step:1351/1770 train_time:139004ms step_avg:103.66ms
step:1352/1770 train_time:139111ms step_avg:103.66ms
step:1353/1770 train_time:139221ms step_avg:103.66ms
step:1354/1770 train_time:139328ms step_avg:103.67ms
step:1355/1770 train_time:139435ms step_avg:103.67ms
step:1356/1770 train_time:139543ms step_avg:103.67ms
step:1357/1770 train_time:139650ms step_avg:103.67ms
step:1358/1770 train_time:139758ms step_avg:103.68ms
step:1359/1770 train_time:139866ms step_avg:103.68ms
step:1360/1770 train_time:139974ms step_avg:103.68ms
step:1361/1770 train_time:140083ms step_avg:103.69ms
step:1362/1770 train_time:140190ms step_avg:103.69ms
step:1363/1770 train_time:140298ms step_avg:103.69ms
step:1364/1770 train_time:140406ms step_avg:103.70ms
step:1365/1770 train_time:140514ms step_avg:103.70ms
step:1366/1770 train_time:140621ms step_avg:103.70ms
step:1367/1770 train_time:140730ms step_avg:103.71ms
step:1368/1770 train_time:140836ms step_avg:103.71ms
step:1369/1770 train_time:140945ms step_avg:103.71ms
step:1370/1770 train_time:141054ms step_avg:103.72ms
step:1371/1770 train_time:141163ms step_avg:103.72ms
step:1372/1770 train_time:141269ms step_avg:103.72ms
step:1373/1770 train_time:141377ms step_avg:103.72ms
step:1374/1770 train_time:141486ms step_avg:103.73ms
step:1375/1770 train_time:141594ms step_avg:103.73ms
step:1375/1770 val_loss:3.3798 train_time:141702ms step_avg:103.81ms
step:1376/1770 train_time:141720ms step_avg:103.75ms
step:1377/1770 train_time:141820ms step_avg:103.75ms
step:1378/1770 train_time:141929ms step_avg:103.75ms
step:1379/1770 train_time:142037ms step_avg:103.75ms
step:1380/1770 train_time:142144ms step_avg:103.75ms
step:1381/1770 train_time:142253ms step_avg:103.76ms
step:1382/1770 train_time:142360ms step_avg:103.76ms
step:1383/1770 train_time:142468ms step_avg:103.76ms
step:1384/1770 train_time:142576ms step_avg:103.77ms
step:1385/1770 train_time:142683ms step_avg:103.77ms
step:1386/1770 train_time:142792ms step_avg:103.77ms
step:1387/1770 train_time:142900ms step_avg:103.78ms
step:1388/1770 train_time:143008ms step_avg:103.78ms
step:1389/1770 train_time:143116ms step_avg:103.78ms
step:1390/1770 train_time:143223ms step_avg:103.78ms
step:1391/1770 train_time:143330ms step_avg:103.79ms
step:1392/1770 train_time:143437ms step_avg:103.79ms
step:1393/1770 train_time:143545ms step_avg:103.79ms
step:1394/1770 train_time:143653ms step_avg:103.80ms
step:1395/1770 train_time:143762ms step_avg:103.80ms
step:1396/1770 train_time:143871ms step_avg:103.80ms
step:1397/1770 train_time:143979ms step_avg:103.81ms
step:1398/1770 train_time:144086ms step_avg:103.81ms
step:1399/1770 train_time:144194ms step_avg:103.81ms
step:1400/1770 train_time:144302ms step_avg:103.81ms
step:1401/1770 train_time:144410ms step_avg:103.82ms
step:1402/1770 train_time:144518ms step_avg:103.82ms
step:1403/1770 train_time:144624ms step_avg:103.82ms
step:1404/1770 train_time:144733ms step_avg:103.83ms
step:1405/1770 train_time:144840ms step_avg:103.83ms
step:1406/1770 train_time:144948ms step_avg:103.83ms
step:1407/1770 train_time:145055ms step_avg:103.83ms
step:1408/1770 train_time:145163ms step_avg:103.84ms
step:1409/1770 train_time:145271ms step_avg:103.84ms
step:1410/1770 train_time:145378ms step_avg:103.84ms
step:1411/1770 train_time:145484ms step_avg:103.84ms
step:1412/1770 train_time:145592ms step_avg:103.85ms
step:1413/1770 train_time:145699ms step_avg:103.85ms
step:1414/1770 train_time:145807ms step_avg:103.85ms
step:1415/1770 train_time:145916ms step_avg:103.85ms
step:1416/1770 train_time:146024ms step_avg:103.86ms
step:1417/1770 train_time:146132ms step_avg:103.86ms
step:1418/1770 train_time:146239ms step_avg:103.86ms
step:1419/1770 train_time:146348ms step_avg:103.87ms
step:1420/1770 train_time:146456ms step_avg:103.87ms
step:1421/1770 train_time:146564ms step_avg:103.87ms
step:1422/1770 train_time:146671ms step_avg:103.87ms
step:1423/1770 train_time:146779ms step_avg:103.88ms
step:1424/1770 train_time:146886ms step_avg:103.88ms
step:1425/1770 train_time:146994ms step_avg:103.88ms
step:1426/1770 train_time:147101ms step_avg:103.89ms
step:1427/1770 train_time:147209ms step_avg:103.89ms
step:1428/1770 train_time:147318ms step_avg:103.89ms
step:1429/1770 train_time:147425ms step_avg:103.89ms
step:1430/1770 train_time:147533ms step_avg:103.90ms
step:1431/1770 train_time:147642ms step_avg:103.90ms
step:1432/1770 train_time:147749ms step_avg:103.90ms
step:1433/1770 train_time:147856ms step_avg:103.90ms
step:1434/1770 train_time:147963ms step_avg:103.91ms
step:1435/1770 train_time:148071ms step_avg:103.91ms
step:1436/1770 train_time:148181ms step_avg:103.91ms
step:1437/1770 train_time:148289ms step_avg:103.92ms
step:1438/1770 train_time:148396ms step_avg:103.92ms
step:1439/1770 train_time:148504ms step_avg:103.92ms
step:1440/1770 train_time:148612ms step_avg:103.92ms
step:1441/1770 train_time:148722ms step_avg:103.93ms
step:1442/1770 train_time:148829ms step_avg:103.93ms
step:1443/1770 train_time:148936ms step_avg:103.93ms
step:1444/1770 train_time:149044ms step_avg:103.94ms
step:1445/1770 train_time:149152ms step_avg:103.94ms
step:1446/1770 train_time:149261ms step_avg:103.94ms
step:1447/1770 train_time:149371ms step_avg:103.95ms
step:1448/1770 train_time:149479ms step_avg:103.95ms
step:1449/1770 train_time:149589ms step_avg:103.95ms
step:1450/1770 train_time:149698ms step_avg:103.96ms
step:1451/1770 train_time:149808ms step_avg:103.96ms
step:1452/1770 train_time:149917ms step_avg:103.96ms
step:1453/1770 train_time:150025ms step_avg:103.97ms
step:1454/1770 train_time:150135ms step_avg:103.97ms
step:1455/1770 train_time:150244ms step_avg:103.98ms
step:1456/1770 train_time:150354ms step_avg:103.98ms
step:1457/1770 train_time:150463ms step_avg:103.98ms
step:1458/1770 train_time:150573ms step_avg:103.99ms
step:1459/1770 train_time:150681ms step_avg:103.99ms
step:1460/1770 train_time:150791ms step_avg:103.99ms
step:1461/1770 train_time:150900ms step_avg:104.00ms
step:1462/1770 train_time:151008ms step_avg:104.00ms
step:1463/1770 train_time:151117ms step_avg:104.00ms
step:1464/1770 train_time:151228ms step_avg:104.01ms
step:1465/1770 train_time:151336ms step_avg:104.01ms
step:1466/1770 train_time:151446ms step_avg:104.01ms
step:1467/1770 train_time:151556ms step_avg:104.02ms
step:1468/1770 train_time:151665ms step_avg:104.02ms
step:1469/1770 train_time:151774ms step_avg:104.03ms
step:1470/1770 train_time:151881ms step_avg:104.03ms
step:1471/1770 train_time:151990ms step_avg:104.03ms
step:1472/1770 train_time:152098ms step_avg:104.03ms
step:1473/1770 train_time:152208ms step_avg:104.04ms
step:1474/1770 train_time:152318ms step_avg:104.04ms
step:1475/1770 train_time:152426ms step_avg:104.04ms
step:1476/1770 train_time:152535ms step_avg:104.05ms
step:1477/1770 train_time:152645ms step_avg:104.05ms
step:1478/1770 train_time:152754ms step_avg:104.06ms
step:1479/1770 train_time:152863ms step_avg:104.06ms
step:1480/1770 train_time:152971ms step_avg:104.06ms
step:1481/1770 train_time:153084ms step_avg:104.07ms
step:1482/1770 train_time:153192ms step_avg:104.07ms
step:1483/1770 train_time:153302ms step_avg:104.07ms
step:1484/1770 train_time:153411ms step_avg:104.08ms
step:1485/1770 train_time:153520ms step_avg:104.08ms
step:1486/1770 train_time:153628ms step_avg:104.08ms
step:1487/1770 train_time:153736ms step_avg:104.09ms
step:1488/1770 train_time:153845ms step_avg:104.09ms
step:1489/1770 train_time:153956ms step_avg:104.09ms
step:1490/1770 train_time:154065ms step_avg:104.10ms
step:1491/1770 train_time:154174ms step_avg:104.10ms
step:1492/1770 train_time:154285ms step_avg:104.11ms
step:1493/1770 train_time:154397ms step_avg:104.11ms
step:1494/1770 train_time:154510ms step_avg:104.12ms
step:1495/1770 train_time:154619ms step_avg:104.12ms
step:1496/1770 train_time:154727ms step_avg:104.12ms
step:1497/1770 train_time:154836ms step_avg:104.13ms
step:1498/1770 train_time:154944ms step_avg:104.13ms
step:1499/1770 train_time:155052ms step_avg:104.13ms
step:1500/1770 train_time:155161ms step_avg:104.13ms
step:1500/1770 val_loss:3.3417 train_time:155268ms step_avg:104.21ms
step:1501/1770 train_time:155285ms step_avg:104.15ms
step:1502/1770 train_time:155390ms step_avg:104.15ms
step:1503/1770 train_time:155501ms step_avg:104.15ms
step:1504/1770 train_time:155610ms step_avg:104.16ms
step:1505/1770 train_time:155719ms step_avg:104.16ms
step:1506/1770 train_time:155828ms step_avg:104.16ms
step:1507/1770 train_time:155936ms step_avg:104.17ms
step:1508/1770 train_time:156046ms step_avg:104.17ms
step:1509/1770 train_time:156155ms step_avg:104.17ms
step:1510/1770 train_time:156262ms step_avg:104.17ms
step:1511/1770 train_time:156372ms step_avg:104.18ms
step:1512/1770 train_time:156481ms step_avg:104.18ms
step:1513/1770 train_time:156590ms step_avg:104.19ms
step:1514/1770 train_time:156699ms step_avg:104.19ms
step:1515/1770 train_time:156809ms step_avg:104.19ms
step:1516/1770 train_time:156918ms step_avg:104.20ms
step:1517/1770 train_time:157026ms step_avg:104.20ms
step:1518/1770 train_time:157137ms step_avg:104.20ms
step:1519/1770 train_time:157245ms step_avg:104.20ms
step:1520/1770 train_time:157355ms step_avg:104.21ms
step:1521/1770 train_time:157462ms step_avg:104.21ms
step:1522/1770 train_time:157572ms step_avg:104.21ms
step:1523/1770 train_time:157681ms step_avg:104.22ms
step:1524/1770 train_time:157790ms step_avg:104.22ms
step:1525/1770 train_time:157899ms step_avg:104.22ms
step:1526/1770 train_time:158008ms step_avg:104.23ms
step:1527/1770 train_time:158116ms step_avg:104.23ms
step:1528/1770 train_time:158227ms step_avg:104.23ms
step:1529/1770 train_time:158335ms step_avg:104.24ms
step:1530/1770 train_time:158443ms step_avg:104.24ms
step:1531/1770 train_time:158551ms step_avg:104.24ms
step:1532/1770 train_time:158662ms step_avg:104.25ms
step:1533/1770 train_time:158772ms step_avg:104.25ms
step:1534/1770 train_time:158881ms step_avg:104.25ms
step:1535/1770 train_time:158989ms step_avg:104.26ms
step:1536/1770 train_time:159098ms step_avg:104.26ms
step:1537/1770 train_time:159206ms step_avg:104.26ms
step:1538/1770 train_time:159317ms step_avg:104.26ms
step:1539/1770 train_time:159424ms step_avg:104.27ms
step:1540/1770 train_time:159536ms step_avg:104.27ms
step:1541/1770 train_time:159645ms step_avg:104.28ms
step:1542/1770 train_time:159754ms step_avg:104.28ms
step:1543/1770 train_time:159863ms step_avg:104.28ms
step:1544/1770 train_time:159973ms step_avg:104.29ms
step:1545/1770 train_time:160082ms step_avg:104.29ms
step:1546/1770 train_time:160192ms step_avg:104.29ms
step:1547/1770 train_time:160300ms step_avg:104.29ms
step:1548/1770 train_time:160410ms step_avg:104.30ms
step:1549/1770 train_time:160518ms step_avg:104.30ms
step:1550/1770 train_time:160628ms step_avg:104.30ms
step:1551/1770 train_time:160736ms step_avg:104.31ms
step:1552/1770 train_time:160848ms step_avg:104.31ms
step:1553/1770 train_time:160957ms step_avg:104.31ms
step:1554/1770 train_time:161064ms step_avg:104.32ms
step:1555/1770 train_time:161173ms step_avg:104.32ms
step:1556/1770 train_time:161281ms step_avg:104.32ms
step:1557/1770 train_time:161390ms step_avg:104.32ms
step:1558/1770 train_time:161499ms step_avg:104.33ms
step:1559/1770 train_time:161609ms step_avg:104.33ms
step:1560/1770 train_time:161717ms step_avg:104.33ms
step:1561/1770 train_time:161828ms step_avg:104.34ms
step:1562/1770 train_time:161936ms step_avg:104.34ms
step:1563/1770 train_time:162045ms step_avg:104.34ms
step:1564/1770 train_time:162154ms step_avg:104.35ms
step:1565/1770 train_time:162261ms step_avg:104.35ms
step:1566/1770 train_time:162371ms step_avg:104.35ms
step:1567/1770 train_time:162479ms step_avg:104.35ms
step:1568/1770 train_time:162588ms step_avg:104.36ms
step:1569/1770 train_time:162701ms step_avg:104.36ms
step:1570/1770 train_time:162810ms step_avg:104.37ms
step:1571/1770 train_time:162918ms step_avg:104.37ms
step:1572/1770 train_time:163028ms step_avg:104.37ms
step:1573/1770 train_time:163140ms step_avg:104.38ms
step:1574/1770 train_time:163249ms step_avg:104.38ms
step:1575/1770 train_time:163357ms step_avg:104.38ms
step:1576/1770 train_time:163466ms step_avg:104.38ms
step:1577/1770 train_time:163576ms step_avg:104.39ms
step:1578/1770 train_time:163686ms step_avg:104.39ms
step:1579/1770 train_time:163794ms step_avg:104.39ms
step:1580/1770 train_time:163903ms step_avg:104.40ms
step:1581/1770 train_time:164015ms step_avg:104.40ms
step:1582/1770 train_time:164125ms step_avg:104.40ms
step:1583/1770 train_time:164234ms step_avg:104.41ms
step:1584/1770 train_time:164343ms step_avg:104.41ms
step:1585/1770 train_time:164452ms step_avg:104.41ms
step:1586/1770 train_time:164565ms step_avg:104.42ms
step:1587/1770 train_time:164675ms step_avg:104.42ms
step:1588/1770 train_time:164783ms step_avg:104.43ms
step:1589/1770 train_time:164894ms step_avg:104.43ms
step:1590/1770 train_time:165002ms step_avg:104.43ms
step:1591/1770 train_time:165110ms step_avg:104.43ms
step:1592/1770 train_time:165220ms step_avg:104.44ms
step:1593/1770 train_time:165329ms step_avg:104.44ms
step:1594/1770 train_time:165438ms step_avg:104.44ms
step:1595/1770 train_time:165546ms step_avg:104.45ms
step:1596/1770 train_time:165656ms step_avg:104.45ms
step:1597/1770 train_time:165764ms step_avg:104.45ms
step:1598/1770 train_time:165873ms step_avg:104.45ms
step:1599/1770 train_time:165983ms step_avg:104.46ms
step:1600/1770 train_time:166094ms step_avg:104.46ms
step:1601/1770 train_time:166204ms step_avg:104.47ms
step:1602/1770 train_time:166314ms step_avg:104.47ms
step:1603/1770 train_time:166423ms step_avg:104.47ms
step:1604/1770 train_time:166531ms step_avg:104.47ms
step:1605/1770 train_time:166639ms step_avg:104.48ms
step:1606/1770 train_time:166748ms step_avg:104.48ms
step:1607/1770 train_time:166861ms step_avg:104.48ms
step:1608/1770 train_time:166971ms step_avg:104.49ms
step:1609/1770 train_time:167081ms step_avg:104.49ms
step:1610/1770 train_time:167191ms step_avg:104.49ms
step:1611/1770 train_time:167301ms step_avg:104.50ms
step:1612/1770 train_time:167411ms step_avg:104.50ms
step:1613/1770 train_time:167519ms step_avg:104.50ms
step:1614/1770 train_time:167628ms step_avg:104.51ms
step:1615/1770 train_time:167739ms step_avg:104.51ms
step:1616/1770 train_time:167848ms step_avg:104.51ms
step:1617/1770 train_time:167959ms step_avg:104.52ms
step:1618/1770 train_time:168070ms step_avg:104.52ms
step:1619/1770 train_time:168179ms step_avg:104.52ms
step:1620/1770 train_time:168290ms step_avg:104.53ms
step:1621/1770 train_time:168399ms step_avg:104.53ms
step:1622/1770 train_time:168509ms step_avg:104.53ms
step:1623/1770 train_time:168621ms step_avg:104.54ms
step:1624/1770 train_time:168731ms step_avg:104.54ms
step:1625/1770 train_time:168840ms step_avg:104.54ms
step:1625/1770 val_loss:3.3071 train_time:168948ms step_avg:104.61ms
step:1626/1770 train_time:168965ms step_avg:104.56ms
step:1627/1770 train_time:169067ms step_avg:104.56ms
step:1628/1770 train_time:169177ms step_avg:104.56ms
step:1629/1770 train_time:169286ms step_avg:104.56ms
step:1630/1770 train_time:169395ms step_avg:104.56ms
step:1631/1770 train_time:169504ms step_avg:104.57ms
step:1632/1770 train_time:169613ms step_avg:104.57ms
step:1633/1770 train_time:169722ms step_avg:104.57ms
step:1634/1770 train_time:169830ms step_avg:104.58ms
step:1635/1770 train_time:169939ms step_avg:104.58ms
step:1636/1770 train_time:170048ms step_avg:104.58ms
step:1637/1770 train_time:170158ms step_avg:104.58ms
step:1638/1770 train_time:170266ms step_avg:104.59ms
step:1639/1770 train_time:170375ms step_avg:104.59ms
step:1640/1770 train_time:170485ms step_avg:104.59ms
step:1641/1770 train_time:170595ms step_avg:104.60ms
step:1642/1770 train_time:170703ms step_avg:104.60ms
step:1643/1770 train_time:170811ms step_avg:104.60ms
step:1644/1770 train_time:170922ms step_avg:104.60ms
step:1645/1770 train_time:171031ms step_avg:104.61ms
step:1646/1770 train_time:171143ms step_avg:104.61ms
step:1647/1770 train_time:171253ms step_avg:104.61ms
step:1648/1770 train_time:171361ms step_avg:104.62ms
step:1649/1770 train_time:171470ms step_avg:104.62ms
step:1650/1770 train_time:171579ms step_avg:104.62ms
step:1651/1770 train_time:171687ms step_avg:104.62ms
step:1652/1770 train_time:171796ms step_avg:104.63ms
step:1653/1770 train_time:171905ms step_avg:104.63ms
step:1654/1770 train_time:172018ms step_avg:104.63ms
step:1655/1770 train_time:172129ms step_avg:104.64ms
step:1656/1770 train_time:172239ms step_avg:104.64ms
step:1657/1770 train_time:172350ms step_avg:104.64ms
step:1658/1770 train_time:172459ms step_avg:104.65ms
step:1659/1770 train_time:172569ms step_avg:104.65ms
step:1660/1770 train_time:172677ms step_avg:104.65ms
step:1661/1770 train_time:172787ms step_avg:104.66ms
step:1662/1770 train_time:172897ms step_avg:104.66ms
step:1663/1770 train_time:173006ms step_avg:104.66ms
step:1664/1770 train_time:173114ms step_avg:104.66ms
step:1665/1770 train_time:173223ms step_avg:104.67ms
step:1666/1770 train_time:173334ms step_avg:104.67ms
step:1667/1770 train_time:173443ms step_avg:104.67ms
step:1668/1770 train_time:173551ms step_avg:104.67ms
step:1669/1770 train_time:173659ms step_avg:104.68ms
step:1670/1770 train_time:173767ms step_avg:104.68ms
step:1671/1770 train_time:173877ms step_avg:104.68ms
step:1672/1770 train_time:173987ms step_avg:104.69ms
step:1673/1770 train_time:174096ms step_avg:104.69ms
step:1674/1770 train_time:174207ms step_avg:104.69ms
step:1675/1770 train_time:174314ms step_avg:104.69ms
step:1676/1770 train_time:174424ms step_avg:104.70ms
step:1677/1770 train_time:174536ms step_avg:104.70ms
step:1678/1770 train_time:174645ms step_avg:104.70ms
step:1679/1770 train_time:174755ms step_avg:104.71ms
step:1680/1770 train_time:174864ms step_avg:104.71ms
step:1681/1770 train_time:174974ms step_avg:104.71ms
step:1682/1770 train_time:175084ms step_avg:104.72ms
step:1683/1770 train_time:175192ms step_avg:104.72ms
step:1684/1770 train_time:175300ms step_avg:104.72ms
step:1685/1770 train_time:175409ms step_avg:104.72ms
step:1686/1770 train_time:175519ms step_avg:104.72ms
step:1687/1770 train_time:175629ms step_avg:104.73ms
step:1688/1770 train_time:175738ms step_avg:104.73ms
step:1689/1770 train_time:175846ms step_avg:104.73ms
step:1690/1770 train_time:175955ms step_avg:104.74ms
step:1691/1770 train_time:176064ms step_avg:104.74ms
step:1692/1770 train_time:176173ms step_avg:104.74ms
step:1693/1770 train_time:176284ms step_avg:104.74ms
step:1694/1770 train_time:176393ms step_avg:104.75ms
step:1695/1770 train_time:176503ms step_avg:104.75ms
step:1696/1770 train_time:176614ms step_avg:104.75ms
step:1697/1770 train_time:176724ms step_avg:104.76ms
step:1698/1770 train_time:176834ms step_avg:104.76ms
step:1699/1770 train_time:176943ms step_avg:104.76ms
step:1700/1770 train_time:177051ms step_avg:104.76ms
step:1701/1770 train_time:177159ms step_avg:104.77ms
step:1702/1770 train_time:177269ms step_avg:104.77ms
step:1703/1770 train_time:177377ms step_avg:104.77ms
step:1704/1770 train_time:177487ms step_avg:104.77ms
step:1705/1770 train_time:177595ms step_avg:104.78ms
step:1706/1770 train_time:177704ms step_avg:104.78ms
step:1707/1770 train_time:177813ms step_avg:104.78ms
step:1708/1770 train_time:177923ms step_avg:104.78ms
step:1709/1770 train_time:178033ms step_avg:104.79ms
step:1710/1770 train_time:178145ms step_avg:104.79ms
step:1711/1770 train_time:178257ms step_avg:104.80ms
step:1712/1770 train_time:178367ms step_avg:104.80ms
step:1713/1770 train_time:178475ms step_avg:104.80ms
step:1714/1770 train_time:178586ms step_avg:104.80ms
step:1715/1770 train_time:178695ms step_avg:104.81ms
step:1716/1770 train_time:178805ms step_avg:104.81ms
step:1717/1770 train_time:178914ms step_avg:104.81ms
step:1718/1770 train_time:179026ms step_avg:104.82ms
step:1719/1770 train_time:179137ms step_avg:104.82ms
step:1720/1770 train_time:179248ms step_avg:104.82ms
step:1721/1770 train_time:179357ms step_avg:104.83ms
step:1722/1770 train_time:179470ms step_avg:104.83ms
step:1723/1770 train_time:179582ms step_avg:104.83ms
step:1724/1770 train_time:179694ms step_avg:104.84ms
step:1725/1770 train_time:179806ms step_avg:104.84ms
step:1726/1770 train_time:179917ms step_avg:104.85ms
step:1727/1770 train_time:180026ms step_avg:104.85ms
step:1728/1770 train_time:180138ms step_avg:104.85ms
step:1729/1770 train_time:180247ms step_avg:104.86ms
step:1730/1770 train_time:180358ms step_avg:104.86ms
step:1731/1770 train_time:180470ms step_avg:104.86ms
step:1732/1770 train_time:180579ms step_avg:104.87ms
step:1733/1770 train_time:180690ms step_avg:104.87ms
step:1734/1770 train_time:180798ms step_avg:104.87ms
step:1735/1770 train_time:180910ms step_avg:104.88ms
step:1736/1770 train_time:181020ms step_avg:104.88ms
step:1737/1770 train_time:181129ms step_avg:104.88ms
step:1738/1770 train_time:181239ms step_avg:104.88ms
step:1739/1770 train_time:181348ms step_avg:104.89ms
step:1740/1770 train_time:181458ms step_avg:104.89ms
step:1741/1770 train_time:181570ms step_avg:104.89ms
step:1742/1770 train_time:181684ms step_avg:104.90ms
step:1743/1770 train_time:181794ms step_avg:104.90ms
step:1744/1770 train_time:181905ms step_avg:104.90ms
step:1745/1770 train_time:182014ms step_avg:104.91ms
step:1746/1770 train_time:182127ms step_avg:104.91ms
step:1747/1770 train_time:182236ms step_avg:104.91ms
step:1748/1770 train_time:182349ms step_avg:104.92ms
step:1749/1770 train_time:182459ms step_avg:104.92ms
step:1750/1770 train_time:182569ms step_avg:104.92ms
step:1750/1770 val_loss:3.2801 train_time:182676ms step_avg:104.99ms
step:1751/1770 train_time:182695ms step_avg:104.94ms
step:1752/1770 train_time:182798ms step_avg:104.94ms
step:1753/1770 train_time:182910ms step_avg:104.94ms
step:1754/1770 train_time:183020ms step_avg:104.94ms
step:1755/1770 train_time:183131ms step_avg:104.95ms
step:1756/1770 train_time:183241ms step_avg:104.95ms
step:1757/1770 train_time:183351ms step_avg:104.95ms
step:1758/1770 train_time:183460ms step_avg:104.95ms
step:1759/1770 train_time:183572ms step_avg:104.96ms
step:1760/1770 train_time:183681ms step_avg:104.96ms
step:1761/1770 train_time:183793ms step_avg:104.96ms
step:1762/1770 train_time:183906ms step_avg:104.97ms
step:1763/1770 train_time:184015ms step_avg:104.97ms
step:1764/1770 train_time:184124ms step_avg:104.97ms
step:1765/1770 train_time:184235ms step_avg:104.98ms
step:1766/1770 train_time:184349ms step_avg:104.98ms
step:1767/1770 train_time:184457ms step_avg:104.98ms
step:1768/1770 train_time:184567ms step_avg:104.99ms
step:1769/1770 train_time:184676ms step_avg:104.99ms
step:1770/1770 train_time:184785ms step_avg:104.99ms
step:1770/1770 val_loss:3.2771 train_time:184895ms step_avg:105.05ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
