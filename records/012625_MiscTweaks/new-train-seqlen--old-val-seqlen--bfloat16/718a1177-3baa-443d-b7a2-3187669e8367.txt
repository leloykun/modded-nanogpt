import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=False, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:39:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23552ms step_avg:nanms
step:2/1770 train_time:23957ms step_avg:nanms
step:3/1770 train_time:24058ms step_avg:nanms
step:4/1770 train_time:24156ms step_avg:nanms
step:5/1770 train_time:24254ms step_avg:nanms
step:6/1770 train_time:24353ms step_avg:nanms
step:7/1770 train_time:24453ms step_avg:nanms
step:8/1770 train_time:24552ms step_avg:nanms
step:9/1770 train_time:24651ms step_avg:nanms
step:10/1770 train_time:24750ms step_avg:nanms
step:11/1770 train_time:98ms step_avg:nanms
step:12/1770 train_time:197ms step_avg:nanms
step:13/1770 train_time:296ms step_avg:98.73ms
step:14/1770 train_time:396ms step_avg:98.88ms
step:15/1770 train_time:495ms step_avg:98.99ms
step:16/1770 train_time:594ms step_avg:99.06ms
step:17/1770 train_time:693ms step_avg:99.06ms
step:18/1770 train_time:793ms step_avg:99.12ms
step:19/1770 train_time:892ms step_avg:99.11ms
step:20/1770 train_time:992ms step_avg:99.19ms
step:21/1770 train_time:1092ms step_avg:99.23ms
step:22/1770 train_time:1191ms step_avg:99.26ms
step:23/1770 train_time:1291ms step_avg:99.27ms
step:24/1770 train_time:1394ms step_avg:99.59ms
step:25/1770 train_time:1490ms step_avg:99.36ms
step:26/1770 train_time:1589ms step_avg:99.34ms
step:27/1770 train_time:1689ms step_avg:99.34ms
step:28/1770 train_time:1788ms step_avg:99.35ms
step:29/1770 train_time:1887ms step_avg:99.31ms
step:30/1770 train_time:1986ms step_avg:99.30ms
step:31/1770 train_time:2085ms step_avg:99.28ms
step:32/1770 train_time:2184ms step_avg:99.25ms
step:33/1770 train_time:2282ms step_avg:99.23ms
step:34/1770 train_time:2381ms step_avg:99.20ms
step:35/1770 train_time:2480ms step_avg:99.18ms
step:36/1770 train_time:2579ms step_avg:99.18ms
step:37/1770 train_time:2677ms step_avg:99.16ms
step:38/1770 train_time:2776ms step_avg:99.15ms
step:39/1770 train_time:2876ms step_avg:99.17ms
step:40/1770 train_time:2975ms step_avg:99.17ms
step:41/1770 train_time:3075ms step_avg:99.19ms
step:42/1770 train_time:3175ms step_avg:99.21ms
step:43/1770 train_time:3278ms step_avg:99.32ms
step:44/1770 train_time:3376ms step_avg:99.31ms
step:45/1770 train_time:3476ms step_avg:99.32ms
step:46/1770 train_time:3576ms step_avg:99.34ms
step:47/1770 train_time:3676ms step_avg:99.34ms
step:48/1770 train_time:3775ms step_avg:99.35ms
step:49/1770 train_time:3875ms step_avg:99.36ms
step:50/1770 train_time:3975ms step_avg:99.37ms
step:51/1770 train_time:4075ms step_avg:99.38ms
step:52/1770 train_time:4175ms step_avg:99.40ms
step:53/1770 train_time:4276ms step_avg:99.43ms
step:54/1770 train_time:4376ms step_avg:99.45ms
step:55/1770 train_time:4475ms step_avg:99.45ms
step:56/1770 train_time:4574ms step_avg:99.44ms
step:57/1770 train_time:4674ms step_avg:99.45ms
step:58/1770 train_time:4774ms step_avg:99.47ms
step:59/1770 train_time:4874ms step_avg:99.47ms
step:60/1770 train_time:4974ms step_avg:99.47ms
step:61/1770 train_time:5074ms step_avg:99.49ms
step:62/1770 train_time:5173ms step_avg:99.48ms
step:63/1770 train_time:5274ms step_avg:99.50ms
step:64/1770 train_time:5375ms step_avg:99.53ms
step:65/1770 train_time:5475ms step_avg:99.54ms
step:66/1770 train_time:5575ms step_avg:99.56ms
step:67/1770 train_time:5675ms step_avg:99.56ms
step:68/1770 train_time:5775ms step_avg:99.56ms
step:69/1770 train_time:5874ms step_avg:99.57ms
step:70/1770 train_time:5974ms step_avg:99.57ms
step:71/1770 train_time:6074ms step_avg:99.58ms
step:72/1770 train_time:6174ms step_avg:99.58ms
step:73/1770 train_time:6274ms step_avg:99.58ms
step:74/1770 train_time:6374ms step_avg:99.59ms
step:75/1770 train_time:6473ms step_avg:99.59ms
step:76/1770 train_time:6574ms step_avg:99.60ms
step:77/1770 train_time:6673ms step_avg:99.60ms
step:78/1770 train_time:6773ms step_avg:99.60ms
step:79/1770 train_time:6872ms step_avg:99.60ms
step:80/1770 train_time:6972ms step_avg:99.59ms
step:81/1770 train_time:7071ms step_avg:99.59ms
step:82/1770 train_time:7171ms step_avg:99.60ms
step:83/1770 train_time:7271ms step_avg:99.60ms
step:84/1770 train_time:7370ms step_avg:99.60ms
step:85/1770 train_time:7471ms step_avg:99.61ms
step:86/1770 train_time:7571ms step_avg:99.61ms
step:87/1770 train_time:7670ms step_avg:99.61ms
step:88/1770 train_time:7769ms step_avg:99.61ms
step:89/1770 train_time:7868ms step_avg:99.60ms
step:90/1770 train_time:7967ms step_avg:99.59ms
step:91/1770 train_time:8065ms step_avg:99.57ms
step:92/1770 train_time:8164ms step_avg:99.57ms
step:93/1770 train_time:8263ms step_avg:99.56ms
step:94/1770 train_time:8362ms step_avg:99.55ms
step:95/1770 train_time:8461ms step_avg:99.55ms
step:96/1770 train_time:8561ms step_avg:99.54ms
step:97/1770 train_time:8659ms step_avg:99.53ms
step:98/1770 train_time:8758ms step_avg:99.52ms
step:99/1770 train_time:8857ms step_avg:99.51ms
step:100/1770 train_time:8956ms step_avg:99.51ms
step:101/1770 train_time:9055ms step_avg:99.51ms
step:102/1770 train_time:9155ms step_avg:99.51ms
step:103/1770 train_time:9254ms step_avg:99.51ms
step:104/1770 train_time:9354ms step_avg:99.51ms
step:105/1770 train_time:9453ms step_avg:99.51ms
step:106/1770 train_time:9553ms step_avg:99.51ms
step:107/1770 train_time:9653ms step_avg:99.51ms
step:108/1770 train_time:9753ms step_avg:99.52ms
step:109/1770 train_time:9853ms step_avg:99.53ms
step:110/1770 train_time:9953ms step_avg:99.53ms
step:111/1770 train_time:10053ms step_avg:99.53ms
step:112/1770 train_time:10153ms step_avg:99.54ms
step:113/1770 train_time:10253ms step_avg:99.54ms
step:114/1770 train_time:10352ms step_avg:99.54ms
step:115/1770 train_time:10452ms step_avg:99.54ms
step:116/1770 train_time:10551ms step_avg:99.54ms
step:117/1770 train_time:10650ms step_avg:99.54ms
step:118/1770 train_time:10750ms step_avg:99.53ms
step:119/1770 train_time:10849ms step_avg:99.53ms
step:120/1770 train_time:10948ms step_avg:99.53ms
step:121/1770 train_time:11047ms step_avg:99.53ms
step:122/1770 train_time:11147ms step_avg:99.52ms
step:123/1770 train_time:11246ms step_avg:99.52ms
step:124/1770 train_time:11344ms step_avg:99.51ms
step:125/1770 train_time:11443ms step_avg:99.51ms
step:125/1770 val_loss:4.6527 train_time:11541ms step_avg:100.36ms
step:126/1770 train_time:11560ms step_avg:99.65ms
step:127/1770 train_time:11655ms step_avg:99.61ms
step:128/1770 train_time:11757ms step_avg:99.64ms
step:129/1770 train_time:11857ms step_avg:99.64ms
step:130/1770 train_time:11956ms step_avg:99.63ms
step:131/1770 train_time:12055ms step_avg:99.63ms
step:132/1770 train_time:12154ms step_avg:99.62ms
step:133/1770 train_time:12253ms step_avg:99.62ms
step:134/1770 train_time:12352ms step_avg:99.62ms
step:135/1770 train_time:12452ms step_avg:99.62ms
step:136/1770 train_time:12552ms step_avg:99.62ms
step:137/1770 train_time:12652ms step_avg:99.62ms
step:138/1770 train_time:12751ms step_avg:99.62ms
step:139/1770 train_time:12850ms step_avg:99.62ms
step:140/1770 train_time:12951ms step_avg:99.62ms
step:141/1770 train_time:13050ms step_avg:99.62ms
step:142/1770 train_time:13150ms step_avg:99.62ms
step:143/1770 train_time:13250ms step_avg:99.62ms
step:144/1770 train_time:13349ms step_avg:99.62ms
step:145/1770 train_time:13448ms step_avg:99.62ms
step:146/1770 train_time:13548ms step_avg:99.62ms
step:147/1770 train_time:13648ms step_avg:99.62ms
step:148/1770 train_time:13749ms step_avg:99.63ms
step:149/1770 train_time:13848ms step_avg:99.63ms
step:150/1770 train_time:13948ms step_avg:99.63ms
step:151/1770 train_time:14047ms step_avg:99.63ms
step:152/1770 train_time:14147ms step_avg:99.63ms
step:153/1770 train_time:14247ms step_avg:99.63ms
step:154/1770 train_time:14347ms step_avg:99.63ms
step:155/1770 train_time:14447ms step_avg:99.64ms
step:156/1770 train_time:14546ms step_avg:99.63ms
step:157/1770 train_time:14647ms step_avg:99.64ms
step:158/1770 train_time:14747ms step_avg:99.64ms
step:159/1770 train_time:14847ms step_avg:99.64ms
step:160/1770 train_time:14947ms step_avg:99.64ms
step:161/1770 train_time:15047ms step_avg:99.65ms
step:162/1770 train_time:15147ms step_avg:99.65ms
step:163/1770 train_time:15246ms step_avg:99.65ms
step:164/1770 train_time:15346ms step_avg:99.65ms
step:165/1770 train_time:15446ms step_avg:99.65ms
step:166/1770 train_time:15547ms step_avg:99.66ms
step:167/1770 train_time:15647ms step_avg:99.66ms
step:168/1770 train_time:15747ms step_avg:99.66ms
step:169/1770 train_time:15848ms step_avg:99.67ms
step:170/1770 train_time:15947ms step_avg:99.67ms
step:171/1770 train_time:16047ms step_avg:99.67ms
step:172/1770 train_time:16147ms step_avg:99.67ms
step:173/1770 train_time:16247ms step_avg:99.67ms
step:174/1770 train_time:16347ms step_avg:99.67ms
step:175/1770 train_time:16447ms step_avg:99.68ms
step:176/1770 train_time:16547ms step_avg:99.68ms
step:177/1770 train_time:16647ms step_avg:99.68ms
step:178/1770 train_time:16747ms step_avg:99.69ms
step:179/1770 train_time:16847ms step_avg:99.69ms
step:180/1770 train_time:16947ms step_avg:99.69ms
step:181/1770 train_time:17047ms step_avg:99.69ms
step:182/1770 train_time:17147ms step_avg:99.69ms
step:183/1770 train_time:17247ms step_avg:99.70ms
step:184/1770 train_time:17347ms step_avg:99.70ms
step:185/1770 train_time:17447ms step_avg:99.70ms
step:186/1770 train_time:17547ms step_avg:99.70ms
step:187/1770 train_time:17647ms step_avg:99.70ms
step:188/1770 train_time:17747ms step_avg:99.70ms
step:189/1770 train_time:17846ms step_avg:99.70ms
step:190/1770 train_time:17947ms step_avg:99.70ms
step:191/1770 train_time:18047ms step_avg:99.71ms
step:192/1770 train_time:18147ms step_avg:99.71ms
step:193/1770 train_time:18248ms step_avg:99.71ms
step:194/1770 train_time:18348ms step_avg:99.72ms
step:195/1770 train_time:18448ms step_avg:99.72ms
step:196/1770 train_time:18547ms step_avg:99.72ms
step:197/1770 train_time:18647ms step_avg:99.72ms
step:198/1770 train_time:18747ms step_avg:99.72ms
step:199/1770 train_time:18847ms step_avg:99.72ms
step:200/1770 train_time:18947ms step_avg:99.72ms
step:201/1770 train_time:19047ms step_avg:99.72ms
step:202/1770 train_time:19147ms step_avg:99.73ms
step:203/1770 train_time:19247ms step_avg:99.73ms
step:204/1770 train_time:19348ms step_avg:99.73ms
step:205/1770 train_time:19447ms step_avg:99.73ms
step:206/1770 train_time:19548ms step_avg:99.73ms
step:207/1770 train_time:19648ms step_avg:99.73ms
step:208/1770 train_time:19748ms step_avg:99.74ms
step:209/1770 train_time:19847ms step_avg:99.74ms
step:210/1770 train_time:19947ms step_avg:99.74ms
step:211/1770 train_time:20048ms step_avg:99.74ms
step:212/1770 train_time:20147ms step_avg:99.74ms
step:213/1770 train_time:20247ms step_avg:99.74ms
step:214/1770 train_time:20347ms step_avg:99.74ms
step:215/1770 train_time:20447ms step_avg:99.74ms
step:216/1770 train_time:20547ms step_avg:99.74ms
step:217/1770 train_time:20647ms step_avg:99.75ms
step:218/1770 train_time:20749ms step_avg:99.76ms
step:219/1770 train_time:20849ms step_avg:99.76ms
step:220/1770 train_time:20949ms step_avg:99.76ms
step:221/1770 train_time:21049ms step_avg:99.76ms
step:222/1770 train_time:21148ms step_avg:99.76ms
step:223/1770 train_time:21248ms step_avg:99.76ms
step:224/1770 train_time:21348ms step_avg:99.76ms
step:225/1770 train_time:21449ms step_avg:99.76ms
step:226/1770 train_time:21548ms step_avg:99.76ms
step:227/1770 train_time:21648ms step_avg:99.76ms
step:228/1770 train_time:21748ms step_avg:99.76ms
step:229/1770 train_time:21848ms step_avg:99.76ms
step:230/1770 train_time:21948ms step_avg:99.76ms
step:231/1770 train_time:22047ms step_avg:99.76ms
step:232/1770 train_time:22148ms step_avg:99.77ms
step:233/1770 train_time:22247ms step_avg:99.76ms
step:234/1770 train_time:22347ms step_avg:99.76ms
step:235/1770 train_time:22447ms step_avg:99.77ms
step:236/1770 train_time:22548ms step_avg:99.77ms
step:237/1770 train_time:22647ms step_avg:99.77ms
step:238/1770 train_time:22747ms step_avg:99.77ms
step:239/1770 train_time:22848ms step_avg:99.77ms
step:240/1770 train_time:22947ms step_avg:99.77ms
step:241/1770 train_time:23047ms step_avg:99.77ms
step:242/1770 train_time:23147ms step_avg:99.77ms
step:243/1770 train_time:23247ms step_avg:99.77ms
step:244/1770 train_time:23347ms step_avg:99.77ms
step:245/1770 train_time:23447ms step_avg:99.77ms
step:246/1770 train_time:23549ms step_avg:99.78ms
step:247/1770 train_time:23647ms step_avg:99.78ms
step:248/1770 train_time:23748ms step_avg:99.78ms
step:249/1770 train_time:23848ms step_avg:99.78ms
step:250/1770 train_time:23948ms step_avg:99.78ms
step:250/1770 val_loss:4.1059 train_time:24046ms step_avg:100.19ms
step:251/1770 train_time:24065ms step_avg:99.85ms
step:252/1770 train_time:24157ms step_avg:99.82ms
step:253/1770 train_time:24261ms step_avg:99.84ms
step:254/1770 train_time:24362ms step_avg:99.85ms
step:255/1770 train_time:24463ms step_avg:99.85ms
step:256/1770 train_time:24563ms step_avg:99.85ms
step:257/1770 train_time:24663ms step_avg:99.85ms
step:258/1770 train_time:24763ms step_avg:99.85ms
step:259/1770 train_time:24863ms step_avg:99.85ms
step:260/1770 train_time:24963ms step_avg:99.85ms
step:261/1770 train_time:25063ms step_avg:99.85ms
step:262/1770 train_time:25164ms step_avg:99.86ms
step:263/1770 train_time:25265ms step_avg:99.86ms
step:264/1770 train_time:25367ms step_avg:99.87ms
step:265/1770 train_time:25468ms step_avg:99.88ms
step:266/1770 train_time:25568ms step_avg:99.88ms
step:267/1770 train_time:25669ms step_avg:99.88ms
step:268/1770 train_time:25769ms step_avg:99.88ms
step:269/1770 train_time:25869ms step_avg:99.88ms
step:270/1770 train_time:25969ms step_avg:99.88ms
step:271/1770 train_time:26069ms step_avg:99.88ms
step:272/1770 train_time:26169ms step_avg:99.88ms
step:273/1770 train_time:26269ms step_avg:99.88ms
step:274/1770 train_time:26369ms step_avg:99.88ms
step:275/1770 train_time:26470ms step_avg:99.88ms
step:276/1770 train_time:26569ms step_avg:99.88ms
step:277/1770 train_time:26670ms step_avg:99.89ms
step:278/1770 train_time:26770ms step_avg:99.89ms
step:279/1770 train_time:26871ms step_avg:99.89ms
step:280/1770 train_time:26971ms step_avg:99.89ms
step:281/1770 train_time:27071ms step_avg:99.89ms
step:282/1770 train_time:27171ms step_avg:99.89ms
step:283/1770 train_time:27272ms step_avg:99.90ms
step:284/1770 train_time:27372ms step_avg:99.90ms
step:285/1770 train_time:27472ms step_avg:99.90ms
step:286/1770 train_time:27572ms step_avg:99.90ms
step:287/1770 train_time:27673ms step_avg:99.90ms
step:288/1770 train_time:27773ms step_avg:99.90ms
step:289/1770 train_time:27873ms step_avg:99.90ms
step:290/1770 train_time:27974ms step_avg:99.91ms
step:291/1770 train_time:28074ms step_avg:99.91ms
step:292/1770 train_time:28174ms step_avg:99.91ms
step:293/1770 train_time:28274ms step_avg:99.91ms
step:294/1770 train_time:28375ms step_avg:99.91ms
step:295/1770 train_time:28475ms step_avg:99.91ms
step:296/1770 train_time:28576ms step_avg:99.91ms
step:297/1770 train_time:28675ms step_avg:99.91ms
step:298/1770 train_time:28776ms step_avg:99.92ms
step:299/1770 train_time:28875ms step_avg:99.92ms
step:300/1770 train_time:28976ms step_avg:99.92ms
step:301/1770 train_time:29076ms step_avg:99.92ms
step:302/1770 train_time:29175ms step_avg:99.91ms
step:303/1770 train_time:29275ms step_avg:99.92ms
step:304/1770 train_time:29376ms step_avg:99.92ms
step:305/1770 train_time:29476ms step_avg:99.92ms
step:306/1770 train_time:29576ms step_avg:99.92ms
step:307/1770 train_time:29676ms step_avg:99.92ms
step:308/1770 train_time:29776ms step_avg:99.92ms
step:309/1770 train_time:29876ms step_avg:99.92ms
step:310/1770 train_time:29976ms step_avg:99.92ms
step:311/1770 train_time:30076ms step_avg:99.92ms
step:312/1770 train_time:30176ms step_avg:99.92ms
step:313/1770 train_time:30276ms step_avg:99.92ms
step:314/1770 train_time:30376ms step_avg:99.92ms
step:315/1770 train_time:30476ms step_avg:99.92ms
step:316/1770 train_time:30577ms step_avg:99.92ms
step:317/1770 train_time:30676ms step_avg:99.92ms
step:318/1770 train_time:30776ms step_avg:99.92ms
step:319/1770 train_time:30876ms step_avg:99.92ms
step:320/1770 train_time:30976ms step_avg:99.92ms
step:321/1770 train_time:31076ms step_avg:99.92ms
step:322/1770 train_time:31176ms step_avg:99.92ms
step:323/1770 train_time:31276ms step_avg:99.92ms
step:324/1770 train_time:31376ms step_avg:99.92ms
step:325/1770 train_time:31476ms step_avg:99.92ms
step:326/1770 train_time:31576ms step_avg:99.92ms
step:327/1770 train_time:31676ms step_avg:99.92ms
step:328/1770 train_time:31776ms step_avg:99.93ms
step:329/1770 train_time:31876ms step_avg:99.93ms
step:330/1770 train_time:31976ms step_avg:99.93ms
step:331/1770 train_time:32076ms step_avg:99.93ms
step:332/1770 train_time:32176ms step_avg:99.93ms
step:333/1770 train_time:32276ms step_avg:99.93ms
step:334/1770 train_time:32376ms step_avg:99.93ms
step:335/1770 train_time:32476ms step_avg:99.93ms
step:336/1770 train_time:32577ms step_avg:99.93ms
step:337/1770 train_time:32677ms step_avg:99.93ms
step:338/1770 train_time:32777ms step_avg:99.93ms
step:339/1770 train_time:32876ms step_avg:99.93ms
step:340/1770 train_time:32976ms step_avg:99.93ms
step:341/1770 train_time:33076ms step_avg:99.93ms
step:342/1770 train_time:33176ms step_avg:99.93ms
step:343/1770 train_time:33277ms step_avg:99.93ms
step:344/1770 train_time:33376ms step_avg:99.93ms
step:345/1770 train_time:33476ms step_avg:99.93ms
step:346/1770 train_time:33576ms step_avg:99.93ms
step:347/1770 train_time:33676ms step_avg:99.93ms
step:348/1770 train_time:33776ms step_avg:99.93ms
step:349/1770 train_time:33876ms step_avg:99.93ms
step:350/1770 train_time:33976ms step_avg:99.93ms
step:351/1770 train_time:34076ms step_avg:99.93ms
step:352/1770 train_time:34176ms step_avg:99.93ms
step:353/1770 train_time:34276ms step_avg:99.93ms
step:354/1770 train_time:34376ms step_avg:99.93ms
step:355/1770 train_time:34476ms step_avg:99.93ms
step:356/1770 train_time:34576ms step_avg:99.93ms
step:357/1770 train_time:34676ms step_avg:99.93ms
step:358/1770 train_time:34776ms step_avg:99.93ms
step:359/1770 train_time:34876ms step_avg:99.93ms
step:360/1770 train_time:34977ms step_avg:99.93ms
step:361/1770 train_time:35077ms step_avg:99.93ms
step:362/1770 train_time:35177ms step_avg:99.93ms
step:363/1770 train_time:35277ms step_avg:99.93ms
step:364/1770 train_time:35377ms step_avg:99.93ms
step:365/1770 train_time:35477ms step_avg:99.94ms
step:366/1770 train_time:35577ms step_avg:99.94ms
step:367/1770 train_time:35678ms step_avg:99.94ms
step:368/1770 train_time:35778ms step_avg:99.94ms
step:369/1770 train_time:35878ms step_avg:99.94ms
step:370/1770 train_time:35978ms step_avg:99.94ms
step:371/1770 train_time:36078ms step_avg:99.94ms
step:372/1770 train_time:36178ms step_avg:99.94ms
step:373/1770 train_time:36279ms step_avg:99.94ms
step:374/1770 train_time:36379ms step_avg:99.94ms
step:375/1770 train_time:36479ms step_avg:99.94ms
step:375/1770 val_loss:3.9062 train_time:36578ms step_avg:100.21ms
step:376/1770 train_time:36596ms step_avg:99.99ms
step:377/1770 train_time:36689ms step_avg:99.97ms
step:378/1770 train_time:36792ms step_avg:99.98ms
step:379/1770 train_time:36893ms step_avg:99.98ms
step:380/1770 train_time:36993ms step_avg:99.98ms
step:381/1770 train_time:37092ms step_avg:99.98ms
step:382/1770 train_time:37192ms step_avg:99.98ms
step:383/1770 train_time:37291ms step_avg:99.98ms
step:384/1770 train_time:37391ms step_avg:99.98ms
step:385/1770 train_time:37491ms step_avg:99.98ms
step:386/1770 train_time:37591ms step_avg:99.98ms
step:387/1770 train_time:37692ms step_avg:99.98ms
step:388/1770 train_time:37793ms step_avg:99.98ms
step:389/1770 train_time:37893ms step_avg:99.98ms
step:390/1770 train_time:37995ms step_avg:99.99ms
step:391/1770 train_time:38096ms step_avg:99.99ms
step:392/1770 train_time:38196ms step_avg:99.99ms
step:393/1770 train_time:38297ms step_avg:99.99ms
step:394/1770 train_time:38399ms step_avg:100.00ms
step:395/1770 train_time:38500ms step_avg:100.00ms
step:396/1770 train_time:38604ms step_avg:100.01ms
step:397/1770 train_time:38705ms step_avg:100.01ms
step:398/1770 train_time:38807ms step_avg:100.02ms
step:399/1770 train_time:38909ms step_avg:100.02ms
step:400/1770 train_time:39011ms step_avg:100.03ms
step:401/1770 train_time:39113ms step_avg:100.03ms
step:402/1770 train_time:39214ms step_avg:100.04ms
step:403/1770 train_time:39316ms step_avg:100.04ms
step:404/1770 train_time:39418ms step_avg:100.04ms
step:405/1770 train_time:39520ms step_avg:100.05ms
step:406/1770 train_time:39622ms step_avg:100.06ms
step:407/1770 train_time:39724ms step_avg:100.06ms
step:408/1770 train_time:39826ms step_avg:100.07ms
step:409/1770 train_time:39928ms step_avg:100.07ms
step:410/1770 train_time:40030ms step_avg:100.08ms
step:411/1770 train_time:40132ms step_avg:100.08ms
step:412/1770 train_time:40234ms step_avg:100.09ms
step:413/1770 train_time:40336ms step_avg:100.09ms
step:414/1770 train_time:40438ms step_avg:100.09ms
step:415/1770 train_time:40541ms step_avg:100.10ms
step:416/1770 train_time:40643ms step_avg:100.11ms
step:417/1770 train_time:40745ms step_avg:100.11ms
step:418/1770 train_time:40846ms step_avg:100.11ms
step:419/1770 train_time:40949ms step_avg:100.12ms
step:420/1770 train_time:41051ms step_avg:100.12ms
step:421/1770 train_time:41153ms step_avg:100.13ms
step:422/1770 train_time:41254ms step_avg:100.13ms
step:423/1770 train_time:41357ms step_avg:100.14ms
step:424/1770 train_time:41460ms step_avg:100.14ms
step:425/1770 train_time:41562ms step_avg:100.15ms
step:426/1770 train_time:41664ms step_avg:100.15ms
step:427/1770 train_time:41765ms step_avg:100.16ms
step:428/1770 train_time:41868ms step_avg:100.16ms
step:429/1770 train_time:41970ms step_avg:100.17ms
step:430/1770 train_time:42072ms step_avg:100.17ms
step:431/1770 train_time:42173ms step_avg:100.17ms
step:432/1770 train_time:42275ms step_avg:100.18ms
step:433/1770 train_time:42378ms step_avg:100.18ms
step:434/1770 train_time:42481ms step_avg:100.19ms
step:435/1770 train_time:42583ms step_avg:100.20ms
step:436/1770 train_time:42685ms step_avg:100.20ms
step:437/1770 train_time:42787ms step_avg:100.20ms
step:438/1770 train_time:42890ms step_avg:100.21ms
step:439/1770 train_time:42992ms step_avg:100.21ms
step:440/1770 train_time:43094ms step_avg:100.22ms
step:441/1770 train_time:43196ms step_avg:100.22ms
step:442/1770 train_time:43299ms step_avg:100.23ms
step:443/1770 train_time:43401ms step_avg:100.23ms
step:444/1770 train_time:43503ms step_avg:100.24ms
step:445/1770 train_time:43605ms step_avg:100.24ms
step:446/1770 train_time:43707ms step_avg:100.25ms
step:447/1770 train_time:43809ms step_avg:100.25ms
step:448/1770 train_time:43911ms step_avg:100.25ms
step:449/1770 train_time:44012ms step_avg:100.26ms
step:450/1770 train_time:44114ms step_avg:100.26ms
step:451/1770 train_time:44217ms step_avg:100.27ms
step:452/1770 train_time:44320ms step_avg:100.27ms
step:453/1770 train_time:44422ms step_avg:100.28ms
step:454/1770 train_time:44524ms step_avg:100.28ms
step:455/1770 train_time:44626ms step_avg:100.28ms
step:456/1770 train_time:44728ms step_avg:100.29ms
step:457/1770 train_time:44830ms step_avg:100.29ms
step:458/1770 train_time:44933ms step_avg:100.30ms
step:459/1770 train_time:45034ms step_avg:100.30ms
step:460/1770 train_time:45136ms step_avg:100.30ms
step:461/1770 train_time:45239ms step_avg:100.31ms
step:462/1770 train_time:45342ms step_avg:100.31ms
step:463/1770 train_time:45444ms step_avg:100.32ms
step:464/1770 train_time:45546ms step_avg:100.32ms
step:465/1770 train_time:45648ms step_avg:100.32ms
step:466/1770 train_time:45750ms step_avg:100.33ms
step:467/1770 train_time:45852ms step_avg:100.33ms
step:468/1770 train_time:45953ms step_avg:100.33ms
step:469/1770 train_time:46055ms step_avg:100.34ms
step:470/1770 train_time:46159ms step_avg:100.35ms
step:471/1770 train_time:46261ms step_avg:100.35ms
step:472/1770 train_time:46363ms step_avg:100.35ms
step:473/1770 train_time:46465ms step_avg:100.36ms
step:474/1770 train_time:46568ms step_avg:100.36ms
step:475/1770 train_time:46669ms step_avg:100.36ms
step:476/1770 train_time:46771ms step_avg:100.37ms
step:477/1770 train_time:46873ms step_avg:100.37ms
step:478/1770 train_time:46975ms step_avg:100.37ms
step:479/1770 train_time:47077ms step_avg:100.38ms
step:480/1770 train_time:47179ms step_avg:100.38ms
step:481/1770 train_time:47282ms step_avg:100.39ms
step:482/1770 train_time:47384ms step_avg:100.39ms
step:483/1770 train_time:47486ms step_avg:100.39ms
step:484/1770 train_time:47588ms step_avg:100.40ms
step:485/1770 train_time:47690ms step_avg:100.40ms
step:486/1770 train_time:47793ms step_avg:100.40ms
step:487/1770 train_time:47895ms step_avg:100.41ms
step:488/1770 train_time:47997ms step_avg:100.41ms
step:489/1770 train_time:48100ms step_avg:100.42ms
step:490/1770 train_time:48202ms step_avg:100.42ms
step:491/1770 train_time:48305ms step_avg:100.43ms
step:492/1770 train_time:48407ms step_avg:100.43ms
step:493/1770 train_time:48509ms step_avg:100.43ms
step:494/1770 train_time:48611ms step_avg:100.44ms
step:495/1770 train_time:48717ms step_avg:100.45ms
step:496/1770 train_time:48814ms step_avg:100.44ms
step:497/1770 train_time:48916ms step_avg:100.44ms
step:498/1770 train_time:49019ms step_avg:100.45ms
step:499/1770 train_time:49121ms step_avg:100.45ms
step:500/1770 train_time:49223ms step_avg:100.45ms
step:500/1770 val_loss:3.7559 train_time:49323ms step_avg:100.66ms
step:501/1770 train_time:49342ms step_avg:100.49ms
step:502/1770 train_time:49439ms step_avg:100.49ms
step:503/1770 train_time:49544ms step_avg:100.49ms
step:504/1770 train_time:49646ms step_avg:100.50ms
step:505/1770 train_time:49748ms step_avg:100.50ms
step:506/1770 train_time:49851ms step_avg:100.51ms
step:507/1770 train_time:49953ms step_avg:100.51ms
step:508/1770 train_time:50054ms step_avg:100.51ms
step:509/1770 train_time:50157ms step_avg:100.51ms
step:510/1770 train_time:50259ms step_avg:100.52ms
step:511/1770 train_time:50362ms step_avg:100.52ms
step:512/1770 train_time:50464ms step_avg:100.53ms
step:513/1770 train_time:50566ms step_avg:100.53ms
step:514/1770 train_time:50669ms step_avg:100.53ms
step:515/1770 train_time:50771ms step_avg:100.54ms
step:516/1770 train_time:50873ms step_avg:100.54ms
step:517/1770 train_time:50975ms step_avg:100.54ms
step:518/1770 train_time:51077ms step_avg:100.54ms
step:519/1770 train_time:51179ms step_avg:100.55ms
step:520/1770 train_time:51282ms step_avg:100.55ms
step:521/1770 train_time:51385ms step_avg:100.56ms
step:522/1770 train_time:51486ms step_avg:100.56ms
step:523/1770 train_time:51589ms step_avg:100.56ms
step:524/1770 train_time:51691ms step_avg:100.57ms
step:525/1770 train_time:51794ms step_avg:100.57ms
step:526/1770 train_time:51895ms step_avg:100.57ms
step:527/1770 train_time:51998ms step_avg:100.58ms
step:528/1770 train_time:52100ms step_avg:100.58ms
step:529/1770 train_time:52204ms step_avg:100.58ms
step:530/1770 train_time:52306ms step_avg:100.59ms
step:531/1770 train_time:52408ms step_avg:100.59ms
step:532/1770 train_time:52511ms step_avg:100.60ms
step:533/1770 train_time:52613ms step_avg:100.60ms
step:534/1770 train_time:52715ms step_avg:100.60ms
step:535/1770 train_time:52817ms step_avg:100.60ms
step:536/1770 train_time:52920ms step_avg:100.61ms
step:537/1770 train_time:53024ms step_avg:100.61ms
step:538/1770 train_time:53126ms step_avg:100.62ms
step:539/1770 train_time:53229ms step_avg:100.62ms
step:540/1770 train_time:53333ms step_avg:100.63ms
step:541/1770 train_time:53434ms step_avg:100.63ms
step:542/1770 train_time:53537ms step_avg:100.63ms
step:543/1770 train_time:53640ms step_avg:100.64ms
step:544/1770 train_time:53743ms step_avg:100.64ms
step:545/1770 train_time:53846ms step_avg:100.65ms
step:546/1770 train_time:53948ms step_avg:100.65ms
step:547/1770 train_time:54050ms step_avg:100.65ms
step:548/1770 train_time:54152ms step_avg:100.65ms
step:549/1770 train_time:54254ms step_avg:100.66ms
step:550/1770 train_time:54356ms step_avg:100.66ms
step:551/1770 train_time:54460ms step_avg:100.66ms
step:552/1770 train_time:54562ms step_avg:100.67ms
step:553/1770 train_time:54665ms step_avg:100.67ms
step:554/1770 train_time:54768ms step_avg:100.68ms
step:555/1770 train_time:54871ms step_avg:100.68ms
step:556/1770 train_time:54973ms step_avg:100.68ms
step:557/1770 train_time:55076ms step_avg:100.69ms
step:558/1770 train_time:55179ms step_avg:100.69ms
step:559/1770 train_time:55282ms step_avg:100.70ms
step:560/1770 train_time:55384ms step_avg:100.70ms
step:561/1770 train_time:55487ms step_avg:100.70ms
step:562/1770 train_time:55589ms step_avg:100.70ms
step:563/1770 train_time:55691ms step_avg:100.71ms
step:564/1770 train_time:55794ms step_avg:100.71ms
step:565/1770 train_time:55896ms step_avg:100.71ms
step:566/1770 train_time:56000ms step_avg:100.72ms
step:567/1770 train_time:56103ms step_avg:100.72ms
step:568/1770 train_time:56206ms step_avg:100.73ms
step:569/1770 train_time:56308ms step_avg:100.73ms
step:570/1770 train_time:56411ms step_avg:100.73ms
step:571/1770 train_time:56513ms step_avg:100.74ms
step:572/1770 train_time:56616ms step_avg:100.74ms
step:573/1770 train_time:56721ms step_avg:100.75ms
step:574/1770 train_time:56822ms step_avg:100.75ms
step:575/1770 train_time:56925ms step_avg:100.75ms
step:576/1770 train_time:57027ms step_avg:100.75ms
step:577/1770 train_time:57130ms step_avg:100.76ms
step:578/1770 train_time:57233ms step_avg:100.76ms
step:579/1770 train_time:57335ms step_avg:100.76ms
step:580/1770 train_time:57437ms step_avg:100.77ms
step:581/1770 train_time:57540ms step_avg:100.77ms
step:582/1770 train_time:57643ms step_avg:100.77ms
step:583/1770 train_time:57746ms step_avg:100.78ms
step:584/1770 train_time:57848ms step_avg:100.78ms
step:585/1770 train_time:57951ms step_avg:100.78ms
step:586/1770 train_time:58053ms step_avg:100.79ms
step:587/1770 train_time:58155ms step_avg:100.79ms
step:588/1770 train_time:58257ms step_avg:100.79ms
step:589/1770 train_time:58360ms step_avg:100.80ms
step:590/1770 train_time:58463ms step_avg:100.80ms
step:591/1770 train_time:58565ms step_avg:100.80ms
step:592/1770 train_time:58667ms step_avg:100.80ms
step:593/1770 train_time:58770ms step_avg:100.81ms
step:594/1770 train_time:58872ms step_avg:100.81ms
step:595/1770 train_time:58974ms step_avg:100.81ms
step:596/1770 train_time:59077ms step_avg:100.81ms
step:597/1770 train_time:59180ms step_avg:100.82ms
step:598/1770 train_time:59284ms step_avg:100.82ms
step:599/1770 train_time:59386ms step_avg:100.83ms
step:600/1770 train_time:59488ms step_avg:100.83ms
step:601/1770 train_time:59591ms step_avg:100.83ms
step:602/1770 train_time:59693ms step_avg:100.83ms
step:603/1770 train_time:59795ms step_avg:100.84ms
step:604/1770 train_time:59898ms step_avg:100.84ms
step:605/1770 train_time:60001ms step_avg:100.84ms
step:606/1770 train_time:60104ms step_avg:100.85ms
step:607/1770 train_time:60206ms step_avg:100.85ms
step:608/1770 train_time:60308ms step_avg:100.85ms
step:609/1770 train_time:60411ms step_avg:100.85ms
step:610/1770 train_time:60513ms step_avg:100.85ms
step:611/1770 train_time:60615ms step_avg:100.86ms
step:612/1770 train_time:60718ms step_avg:100.86ms
step:613/1770 train_time:60821ms step_avg:100.86ms
step:614/1770 train_time:60924ms step_avg:100.87ms
step:615/1770 train_time:61029ms step_avg:100.87ms
step:616/1770 train_time:61129ms step_avg:100.87ms
step:617/1770 train_time:61232ms step_avg:100.88ms
step:618/1770 train_time:61334ms step_avg:100.88ms
step:619/1770 train_time:61436ms step_avg:100.88ms
step:620/1770 train_time:61539ms step_avg:100.88ms
step:621/1770 train_time:61642ms step_avg:100.89ms
step:622/1770 train_time:61745ms step_avg:100.89ms
step:623/1770 train_time:61847ms step_avg:100.89ms
step:624/1770 train_time:61950ms step_avg:100.90ms
step:625/1770 train_time:62052ms step_avg:100.90ms
step:625/1770 val_loss:3.6662 train_time:62153ms step_avg:101.06ms
step:626/1770 train_time:62172ms step_avg:100.93ms
step:627/1770 train_time:62269ms step_avg:100.92ms
step:628/1770 train_time:62373ms step_avg:100.93ms
step:629/1770 train_time:62477ms step_avg:100.93ms
step:630/1770 train_time:62580ms step_avg:100.93ms
step:631/1770 train_time:62682ms step_avg:100.94ms
step:632/1770 train_time:62786ms step_avg:100.94ms
step:633/1770 train_time:62889ms step_avg:100.94ms
step:634/1770 train_time:62990ms step_avg:100.95ms
step:635/1770 train_time:63092ms step_avg:100.95ms
step:636/1770 train_time:63196ms step_avg:100.95ms
step:637/1770 train_time:63299ms step_avg:100.95ms
step:638/1770 train_time:63401ms step_avg:100.96ms
step:639/1770 train_time:63504ms step_avg:100.96ms
step:640/1770 train_time:63607ms step_avg:100.96ms
step:641/1770 train_time:63708ms step_avg:100.96ms
step:642/1770 train_time:63810ms step_avg:100.97ms
step:643/1770 train_time:63913ms step_avg:100.97ms
step:644/1770 train_time:64016ms step_avg:100.97ms
step:645/1770 train_time:64118ms step_avg:100.97ms
step:646/1770 train_time:64222ms step_avg:100.98ms
step:647/1770 train_time:64324ms step_avg:100.98ms
step:648/1770 train_time:64427ms step_avg:100.98ms
step:649/1770 train_time:64529ms step_avg:100.99ms
step:650/1770 train_time:64631ms step_avg:100.99ms
step:651/1770 train_time:64734ms step_avg:100.99ms
step:652/1770 train_time:64836ms step_avg:100.99ms
step:653/1770 train_time:64939ms step_avg:100.99ms
step:654/1770 train_time:65042ms step_avg:101.00ms
step:655/1770 train_time:65144ms step_avg:101.00ms
step:656/1770 train_time:65247ms step_avg:101.00ms
step:657/1770 train_time:65350ms step_avg:101.00ms
step:658/1770 train_time:65453ms step_avg:101.01ms
step:659/1770 train_time:65559ms step_avg:101.01ms
step:660/1770 train_time:65663ms step_avg:101.02ms
step:661/1770 train_time:65768ms step_avg:101.03ms
step:662/1770 train_time:65871ms step_avg:101.03ms
step:663/1770 train_time:65976ms step_avg:101.04ms
step:664/1770 train_time:66080ms step_avg:101.04ms
step:665/1770 train_time:66186ms step_avg:101.05ms
step:666/1770 train_time:66289ms step_avg:101.05ms
step:667/1770 train_time:66393ms step_avg:101.06ms
step:668/1770 train_time:66498ms step_avg:101.06ms
step:669/1770 train_time:66602ms step_avg:101.07ms
step:670/1770 train_time:66707ms step_avg:101.07ms
step:671/1770 train_time:66810ms step_avg:101.07ms
step:672/1770 train_time:66914ms step_avg:101.08ms
step:673/1770 train_time:67022ms step_avg:101.09ms
step:674/1770 train_time:67123ms step_avg:101.09ms
step:675/1770 train_time:67227ms step_avg:101.09ms
step:676/1770 train_time:67331ms step_avg:101.10ms
step:677/1770 train_time:67435ms step_avg:101.10ms
step:678/1770 train_time:67540ms step_avg:101.11ms
step:679/1770 train_time:67643ms step_avg:101.11ms
step:680/1770 train_time:67748ms step_avg:101.12ms
step:681/1770 train_time:67852ms step_avg:101.12ms
step:682/1770 train_time:67957ms step_avg:101.13ms
step:683/1770 train_time:68061ms step_avg:101.13ms
step:684/1770 train_time:68165ms step_avg:101.14ms
step:685/1770 train_time:68269ms step_avg:101.14ms
step:686/1770 train_time:68373ms step_avg:101.14ms
step:687/1770 train_time:68478ms step_avg:101.15ms
step:688/1770 train_time:68582ms step_avg:101.15ms
step:689/1770 train_time:68687ms step_avg:101.16ms
step:690/1770 train_time:68790ms step_avg:101.16ms
step:691/1770 train_time:68894ms step_avg:101.17ms
step:692/1770 train_time:68999ms step_avg:101.17ms
step:693/1770 train_time:69103ms step_avg:101.18ms
step:694/1770 train_time:69208ms step_avg:101.18ms
step:695/1770 train_time:69311ms step_avg:101.18ms
step:696/1770 train_time:69416ms step_avg:101.19ms
step:697/1770 train_time:69520ms step_avg:101.19ms
step:698/1770 train_time:69625ms step_avg:101.20ms
step:699/1770 train_time:69729ms step_avg:101.20ms
step:700/1770 train_time:69833ms step_avg:101.21ms
step:701/1770 train_time:69938ms step_avg:101.21ms
step:702/1770 train_time:70043ms step_avg:101.22ms
step:703/1770 train_time:70146ms step_avg:101.22ms
step:704/1770 train_time:70250ms step_avg:101.23ms
step:705/1770 train_time:70355ms step_avg:101.23ms
step:706/1770 train_time:70459ms step_avg:101.23ms
step:707/1770 train_time:70564ms step_avg:101.24ms
step:708/1770 train_time:70667ms step_avg:101.24ms
step:709/1770 train_time:70772ms step_avg:101.25ms
step:710/1770 train_time:70877ms step_avg:101.25ms
step:711/1770 train_time:70981ms step_avg:101.26ms
step:712/1770 train_time:71086ms step_avg:101.26ms
step:713/1770 train_time:71190ms step_avg:101.27ms
step:714/1770 train_time:71294ms step_avg:101.27ms
step:715/1770 train_time:71399ms step_avg:101.28ms
step:716/1770 train_time:71503ms step_avg:101.28ms
step:717/1770 train_time:71607ms step_avg:101.28ms
step:718/1770 train_time:71711ms step_avg:101.29ms
step:719/1770 train_time:71816ms step_avg:101.29ms
step:720/1770 train_time:71921ms step_avg:101.30ms
step:721/1770 train_time:72025ms step_avg:101.30ms
step:722/1770 train_time:72129ms step_avg:101.30ms
step:723/1770 train_time:72233ms step_avg:101.31ms
step:724/1770 train_time:72337ms step_avg:101.31ms
step:725/1770 train_time:72441ms step_avg:101.32ms
step:726/1770 train_time:72546ms step_avg:101.32ms
step:727/1770 train_time:72650ms step_avg:101.32ms
step:728/1770 train_time:72754ms step_avg:101.33ms
step:729/1770 train_time:72859ms step_avg:101.33ms
step:730/1770 train_time:72963ms step_avg:101.34ms
step:731/1770 train_time:73067ms step_avg:101.34ms
step:732/1770 train_time:73171ms step_avg:101.35ms
step:733/1770 train_time:73276ms step_avg:101.35ms
step:734/1770 train_time:73381ms step_avg:101.35ms
step:735/1770 train_time:73485ms step_avg:101.36ms
step:736/1770 train_time:73588ms step_avg:101.36ms
step:737/1770 train_time:73692ms step_avg:101.36ms
step:738/1770 train_time:73797ms step_avg:101.37ms
step:739/1770 train_time:73901ms step_avg:101.37ms
step:740/1770 train_time:74004ms step_avg:101.38ms
step:741/1770 train_time:74108ms step_avg:101.38ms
step:742/1770 train_time:74212ms step_avg:101.38ms
step:743/1770 train_time:74317ms step_avg:101.39ms
step:744/1770 train_time:74422ms step_avg:101.39ms
step:745/1770 train_time:74526ms step_avg:101.40ms
step:746/1770 train_time:74630ms step_avg:101.40ms
step:747/1770 train_time:74734ms step_avg:101.40ms
step:748/1770 train_time:74839ms step_avg:101.41ms
step:749/1770 train_time:74943ms step_avg:101.41ms
step:750/1770 train_time:75047ms step_avg:101.41ms
step:750/1770 val_loss:3.6009 train_time:75149ms step_avg:101.55ms
step:751/1770 train_time:75169ms step_avg:101.44ms
step:752/1770 train_time:75268ms step_avg:101.44ms
step:753/1770 train_time:75374ms step_avg:101.45ms
step:754/1770 train_time:75479ms step_avg:101.45ms
step:755/1770 train_time:75583ms step_avg:101.45ms
step:756/1770 train_time:75688ms step_avg:101.46ms
step:757/1770 train_time:75792ms step_avg:101.46ms
step:758/1770 train_time:75896ms step_avg:101.46ms
step:759/1770 train_time:76000ms step_avg:101.47ms
step:760/1770 train_time:76104ms step_avg:101.47ms
step:761/1770 train_time:76208ms step_avg:101.48ms
step:762/1770 train_time:76313ms step_avg:101.48ms
step:763/1770 train_time:76417ms step_avg:101.48ms
step:764/1770 train_time:76522ms step_avg:101.49ms
step:765/1770 train_time:76626ms step_avg:101.49ms
step:766/1770 train_time:76730ms step_avg:101.49ms
step:767/1770 train_time:76834ms step_avg:101.50ms
step:768/1770 train_time:76938ms step_avg:101.50ms
step:769/1770 train_time:77042ms step_avg:101.50ms
step:770/1770 train_time:77147ms step_avg:101.51ms
step:771/1770 train_time:77251ms step_avg:101.51ms
step:772/1770 train_time:77355ms step_avg:101.52ms
step:773/1770 train_time:77459ms step_avg:101.52ms
step:774/1770 train_time:77564ms step_avg:101.52ms
step:775/1770 train_time:77668ms step_avg:101.53ms
step:776/1770 train_time:77772ms step_avg:101.53ms
step:777/1770 train_time:77876ms step_avg:101.53ms
step:778/1770 train_time:77981ms step_avg:101.54ms
step:779/1770 train_time:78085ms step_avg:101.54ms
step:780/1770 train_time:78189ms step_avg:101.54ms
step:781/1770 train_time:78293ms step_avg:101.55ms
step:782/1770 train_time:78397ms step_avg:101.55ms
step:783/1770 train_time:78503ms step_avg:101.56ms
step:784/1770 train_time:78607ms step_avg:101.56ms
step:785/1770 train_time:78711ms step_avg:101.56ms
step:786/1770 train_time:78815ms step_avg:101.57ms
step:787/1770 train_time:78919ms step_avg:101.57ms
step:788/1770 train_time:79024ms step_avg:101.57ms
step:789/1770 train_time:79129ms step_avg:101.58ms
step:790/1770 train_time:79233ms step_avg:101.58ms
step:791/1770 train_time:79337ms step_avg:101.58ms
step:792/1770 train_time:79442ms step_avg:101.59ms
step:793/1770 train_time:79546ms step_avg:101.59ms
step:794/1770 train_time:79651ms step_avg:101.60ms
step:795/1770 train_time:79755ms step_avg:101.60ms
step:796/1770 train_time:79861ms step_avg:101.60ms
step:797/1770 train_time:79966ms step_avg:101.61ms
step:798/1770 train_time:80070ms step_avg:101.61ms
step:799/1770 train_time:80175ms step_avg:101.62ms
step:800/1770 train_time:80280ms step_avg:101.62ms
step:801/1770 train_time:80385ms step_avg:101.62ms
step:802/1770 train_time:80489ms step_avg:101.63ms
step:803/1770 train_time:80593ms step_avg:101.63ms
step:804/1770 train_time:80697ms step_avg:101.63ms
step:805/1770 train_time:80802ms step_avg:101.64ms
step:806/1770 train_time:80907ms step_avg:101.64ms
step:807/1770 train_time:81011ms step_avg:101.65ms
step:808/1770 train_time:81115ms step_avg:101.65ms
step:809/1770 train_time:81221ms step_avg:101.65ms
step:810/1770 train_time:81327ms step_avg:101.66ms
step:811/1770 train_time:81431ms step_avg:101.66ms
step:812/1770 train_time:81535ms step_avg:101.67ms
step:813/1770 train_time:81641ms step_avg:101.67ms
step:814/1770 train_time:81746ms step_avg:101.67ms
step:815/1770 train_time:81850ms step_avg:101.68ms
step:816/1770 train_time:81954ms step_avg:101.68ms
step:817/1770 train_time:82059ms step_avg:101.68ms
step:818/1770 train_time:82164ms step_avg:101.69ms
step:819/1770 train_time:82268ms step_avg:101.69ms
step:820/1770 train_time:82373ms step_avg:101.69ms
step:821/1770 train_time:82476ms step_avg:101.70ms
step:822/1770 train_time:82582ms step_avg:101.70ms
step:823/1770 train_time:82687ms step_avg:101.71ms
step:824/1770 train_time:82791ms step_avg:101.71ms
step:825/1770 train_time:82895ms step_avg:101.71ms
step:826/1770 train_time:82999ms step_avg:101.71ms
step:827/1770 train_time:83104ms step_avg:101.72ms
step:828/1770 train_time:83209ms step_avg:101.72ms
step:829/1770 train_time:83314ms step_avg:101.73ms
step:830/1770 train_time:83418ms step_avg:101.73ms
step:831/1770 train_time:83523ms step_avg:101.73ms
step:832/1770 train_time:83627ms step_avg:101.74ms
step:833/1770 train_time:83732ms step_avg:101.74ms
step:834/1770 train_time:83836ms step_avg:101.74ms
step:835/1770 train_time:83941ms step_avg:101.75ms
step:836/1770 train_time:84046ms step_avg:101.75ms
step:837/1770 train_time:84151ms step_avg:101.75ms
step:838/1770 train_time:84256ms step_avg:101.76ms
step:839/1770 train_time:84361ms step_avg:101.76ms
step:840/1770 train_time:84466ms step_avg:101.77ms
step:841/1770 train_time:84570ms step_avg:101.77ms
step:842/1770 train_time:84674ms step_avg:101.77ms
step:843/1770 train_time:84780ms step_avg:101.78ms
step:844/1770 train_time:84884ms step_avg:101.78ms
step:845/1770 train_time:84989ms step_avg:101.78ms
step:846/1770 train_time:85093ms step_avg:101.79ms
step:847/1770 train_time:85197ms step_avg:101.79ms
step:848/1770 train_time:85301ms step_avg:101.79ms
step:849/1770 train_time:85406ms step_avg:101.79ms
step:850/1770 train_time:85510ms step_avg:101.80ms
step:851/1770 train_time:85615ms step_avg:101.80ms
step:852/1770 train_time:85720ms step_avg:101.80ms
step:853/1770 train_time:85824ms step_avg:101.81ms
step:854/1770 train_time:85928ms step_avg:101.81ms
step:855/1770 train_time:86032ms step_avg:101.81ms
step:856/1770 train_time:86136ms step_avg:101.82ms
step:857/1770 train_time:86241ms step_avg:101.82ms
step:858/1770 train_time:86345ms step_avg:101.82ms
step:859/1770 train_time:86449ms step_avg:101.82ms
step:860/1770 train_time:86554ms step_avg:101.83ms
step:861/1770 train_time:86658ms step_avg:101.83ms
step:862/1770 train_time:86763ms step_avg:101.83ms
step:863/1770 train_time:86868ms step_avg:101.84ms
step:864/1770 train_time:86972ms step_avg:101.84ms
step:865/1770 train_time:87076ms step_avg:101.84ms
step:866/1770 train_time:87182ms step_avg:101.85ms
step:867/1770 train_time:87287ms step_avg:101.85ms
step:868/1770 train_time:87391ms step_avg:101.85ms
step:869/1770 train_time:87495ms step_avg:101.86ms
step:870/1770 train_time:87604ms step_avg:101.86ms
step:871/1770 train_time:87705ms step_avg:101.86ms
step:872/1770 train_time:87809ms step_avg:101.87ms
step:873/1770 train_time:87913ms step_avg:101.87ms
step:874/1770 train_time:88018ms step_avg:101.87ms
step:875/1770 train_time:88123ms step_avg:101.88ms
step:875/1770 val_loss:3.5529 train_time:88226ms step_avg:102.00ms
step:876/1770 train_time:88245ms step_avg:101.90ms
step:877/1770 train_time:88345ms step_avg:101.90ms
step:878/1770 train_time:88451ms step_avg:101.90ms
step:879/1770 train_time:88556ms step_avg:101.91ms
step:880/1770 train_time:88660ms step_avg:101.91ms
step:881/1770 train_time:88766ms step_avg:101.91ms
step:882/1770 train_time:88870ms step_avg:101.92ms
step:883/1770 train_time:88975ms step_avg:101.92ms
step:884/1770 train_time:89078ms step_avg:101.92ms
step:885/1770 train_time:89183ms step_avg:101.92ms
step:886/1770 train_time:89288ms step_avg:101.93ms
step:887/1770 train_time:89392ms step_avg:101.93ms
step:888/1770 train_time:89497ms step_avg:101.93ms
step:889/1770 train_time:89602ms step_avg:101.94ms
step:890/1770 train_time:89707ms step_avg:101.94ms
step:891/1770 train_time:89811ms step_avg:101.94ms
step:892/1770 train_time:89916ms step_avg:101.95ms
step:893/1770 train_time:90020ms step_avg:101.95ms
step:894/1770 train_time:90125ms step_avg:101.95ms
step:895/1770 train_time:90230ms step_avg:101.95ms
step:896/1770 train_time:90334ms step_avg:101.96ms
step:897/1770 train_time:90438ms step_avg:101.96ms
step:898/1770 train_time:90543ms step_avg:101.96ms
step:899/1770 train_time:90648ms step_avg:101.97ms
step:900/1770 train_time:90752ms step_avg:101.97ms
step:901/1770 train_time:90857ms step_avg:101.97ms
step:902/1770 train_time:90961ms step_avg:101.97ms
step:903/1770 train_time:91065ms step_avg:101.98ms
step:904/1770 train_time:91170ms step_avg:101.98ms
step:905/1770 train_time:91274ms step_avg:101.98ms
step:906/1770 train_time:91378ms step_avg:101.98ms
step:907/1770 train_time:91482ms step_avg:101.99ms
step:908/1770 train_time:91588ms step_avg:101.99ms
step:909/1770 train_time:91692ms step_avg:101.99ms
step:910/1770 train_time:91797ms step_avg:102.00ms
step:911/1770 train_time:91901ms step_avg:102.00ms
step:912/1770 train_time:92006ms step_avg:102.00ms
step:913/1770 train_time:92110ms step_avg:102.00ms
step:914/1770 train_time:92214ms step_avg:102.01ms
step:915/1770 train_time:92318ms step_avg:102.01ms
step:916/1770 train_time:92423ms step_avg:102.01ms
step:917/1770 train_time:92528ms step_avg:102.02ms
step:918/1770 train_time:92633ms step_avg:102.02ms
step:919/1770 train_time:92738ms step_avg:102.02ms
step:920/1770 train_time:92845ms step_avg:102.03ms
step:921/1770 train_time:92951ms step_avg:102.03ms
step:922/1770 train_time:93058ms step_avg:102.04ms
step:923/1770 train_time:93164ms step_avg:102.04ms
step:924/1770 train_time:93270ms step_avg:102.05ms
step:925/1770 train_time:93376ms step_avg:102.05ms
step:926/1770 train_time:93482ms step_avg:102.05ms
step:927/1770 train_time:93588ms step_avg:102.06ms
step:928/1770 train_time:93694ms step_avg:102.06ms
step:929/1770 train_time:93799ms step_avg:102.07ms
step:930/1770 train_time:93905ms step_avg:102.07ms
step:931/1770 train_time:94011ms step_avg:102.07ms
step:932/1770 train_time:94117ms step_avg:102.08ms
step:933/1770 train_time:94224ms step_avg:102.08ms
step:934/1770 train_time:94329ms step_avg:102.09ms
step:935/1770 train_time:94435ms step_avg:102.09ms
step:936/1770 train_time:94540ms step_avg:102.10ms
step:937/1770 train_time:94646ms step_avg:102.10ms
step:938/1770 train_time:94751ms step_avg:102.10ms
step:939/1770 train_time:94857ms step_avg:102.11ms
step:940/1770 train_time:94964ms step_avg:102.11ms
step:941/1770 train_time:95069ms step_avg:102.12ms
step:942/1770 train_time:95176ms step_avg:102.12ms
step:943/1770 train_time:95282ms step_avg:102.12ms
step:944/1770 train_time:95388ms step_avg:102.13ms
step:945/1770 train_time:95494ms step_avg:102.13ms
step:946/1770 train_time:95599ms step_avg:102.14ms
step:947/1770 train_time:95706ms step_avg:102.14ms
step:948/1770 train_time:95811ms step_avg:102.14ms
step:949/1770 train_time:95918ms step_avg:102.15ms
step:950/1770 train_time:96024ms step_avg:102.15ms
step:951/1770 train_time:96131ms step_avg:102.16ms
step:952/1770 train_time:96237ms step_avg:102.16ms
step:953/1770 train_time:96344ms step_avg:102.17ms
step:954/1770 train_time:96450ms step_avg:102.17ms
step:955/1770 train_time:96556ms step_avg:102.18ms
step:956/1770 train_time:96662ms step_avg:102.18ms
step:957/1770 train_time:96768ms step_avg:102.18ms
step:958/1770 train_time:96874ms step_avg:102.19ms
step:959/1770 train_time:96979ms step_avg:102.19ms
step:960/1770 train_time:97085ms step_avg:102.19ms
step:961/1770 train_time:97191ms step_avg:102.20ms
step:962/1770 train_time:97298ms step_avg:102.20ms
step:963/1770 train_time:97404ms step_avg:102.21ms
step:964/1770 train_time:97510ms step_avg:102.21ms
step:965/1770 train_time:97615ms step_avg:102.21ms
step:966/1770 train_time:97721ms step_avg:102.22ms
step:967/1770 train_time:97827ms step_avg:102.22ms
step:968/1770 train_time:97935ms step_avg:102.23ms
step:969/1770 train_time:98040ms step_avg:102.23ms
step:970/1770 train_time:98146ms step_avg:102.24ms
step:971/1770 train_time:98252ms step_avg:102.24ms
step:972/1770 train_time:98358ms step_avg:102.24ms
step:973/1770 train_time:98464ms step_avg:102.25ms
step:974/1770 train_time:98570ms step_avg:102.25ms
step:975/1770 train_time:98676ms step_avg:102.26ms
step:976/1770 train_time:98782ms step_avg:102.26ms
step:977/1770 train_time:98888ms step_avg:102.26ms
step:978/1770 train_time:98993ms step_avg:102.27ms
step:979/1770 train_time:99099ms step_avg:102.27ms
step:980/1770 train_time:99206ms step_avg:102.27ms
step:981/1770 train_time:99311ms step_avg:102.28ms
step:982/1770 train_time:99418ms step_avg:102.28ms
step:983/1770 train_time:99524ms step_avg:102.29ms
step:984/1770 train_time:99631ms step_avg:102.29ms
step:985/1770 train_time:99737ms step_avg:102.29ms
step:986/1770 train_time:99843ms step_avg:102.30ms
step:987/1770 train_time:99949ms step_avg:102.30ms
step:988/1770 train_time:100055ms step_avg:102.31ms
step:989/1770 train_time:100162ms step_avg:102.31ms
step:990/1770 train_time:100268ms step_avg:102.31ms
step:991/1770 train_time:100374ms step_avg:102.32ms
step:992/1770 train_time:100480ms step_avg:102.32ms
step:993/1770 train_time:100587ms step_avg:102.33ms
step:994/1770 train_time:100693ms step_avg:102.33ms
step:995/1770 train_time:100800ms step_avg:102.33ms
step:996/1770 train_time:100906ms step_avg:102.34ms
step:997/1770 train_time:101011ms step_avg:102.34ms
step:998/1770 train_time:101116ms step_avg:102.34ms
step:999/1770 train_time:101222ms step_avg:102.35ms
step:1000/1770 train_time:101328ms step_avg:102.35ms
step:1000/1770 val_loss:3.5139 train_time:101432ms step_avg:102.46ms
step:1001/1770 train_time:101451ms step_avg:102.37ms
step:1002/1770 train_time:101551ms step_avg:102.37ms
step:1003/1770 train_time:101659ms step_avg:102.38ms
step:1004/1770 train_time:101765ms step_avg:102.38ms
step:1005/1770 train_time:101871ms step_avg:102.38ms
step:1006/1770 train_time:101978ms step_avg:102.39ms
step:1007/1770 train_time:102084ms step_avg:102.39ms
step:1008/1770 train_time:102189ms step_avg:102.39ms
step:1009/1770 train_time:102296ms step_avg:102.40ms
step:1010/1770 train_time:102401ms step_avg:102.40ms
step:1011/1770 train_time:102508ms step_avg:102.41ms
step:1012/1770 train_time:102615ms step_avg:102.41ms
step:1013/1770 train_time:102721ms step_avg:102.41ms
step:1014/1770 train_time:102828ms step_avg:102.42ms
step:1015/1770 train_time:102934ms step_avg:102.42ms
step:1016/1770 train_time:103039ms step_avg:102.42ms
step:1017/1770 train_time:103146ms step_avg:102.43ms
step:1018/1770 train_time:103252ms step_avg:102.43ms
step:1019/1770 train_time:103357ms step_avg:102.44ms
step:1020/1770 train_time:103463ms step_avg:102.44ms
step:1021/1770 train_time:103569ms step_avg:102.44ms
step:1022/1770 train_time:103675ms step_avg:102.45ms
step:1023/1770 train_time:103781ms step_avg:102.45ms
step:1024/1770 train_time:103887ms step_avg:102.45ms
step:1025/1770 train_time:103993ms step_avg:102.46ms
step:1026/1770 train_time:104099ms step_avg:102.46ms
step:1027/1770 train_time:104205ms step_avg:102.46ms
step:1028/1770 train_time:104311ms step_avg:102.47ms
step:1029/1770 train_time:104417ms step_avg:102.47ms
step:1030/1770 train_time:104522ms step_avg:102.47ms
step:1031/1770 train_time:104627ms step_avg:102.48ms
step:1032/1770 train_time:104734ms step_avg:102.48ms
step:1033/1770 train_time:104840ms step_avg:102.48ms
step:1034/1770 train_time:104945ms step_avg:102.49ms
step:1035/1770 train_time:105051ms step_avg:102.49ms
step:1036/1770 train_time:105157ms step_avg:102.49ms
step:1037/1770 train_time:105263ms step_avg:102.50ms
step:1038/1770 train_time:105368ms step_avg:102.50ms
step:1039/1770 train_time:105474ms step_avg:102.50ms
step:1040/1770 train_time:105580ms step_avg:102.50ms
step:1041/1770 train_time:105685ms step_avg:102.51ms
step:1042/1770 train_time:105791ms step_avg:102.51ms
step:1043/1770 train_time:105897ms step_avg:102.51ms
step:1044/1770 train_time:106003ms step_avg:102.52ms
step:1045/1770 train_time:106109ms step_avg:102.52ms
step:1046/1770 train_time:106215ms step_avg:102.52ms
step:1047/1770 train_time:106321ms step_avg:102.53ms
step:1048/1770 train_time:106427ms step_avg:102.53ms
step:1049/1770 train_time:106533ms step_avg:102.53ms
step:1050/1770 train_time:106640ms step_avg:102.54ms
step:1051/1770 train_time:106745ms step_avg:102.54ms
step:1052/1770 train_time:106851ms step_avg:102.54ms
step:1053/1770 train_time:106958ms step_avg:102.55ms
step:1054/1770 train_time:107064ms step_avg:102.55ms
step:1055/1770 train_time:107170ms step_avg:102.55ms
step:1056/1770 train_time:107276ms step_avg:102.56ms
step:1057/1770 train_time:107382ms step_avg:102.56ms
step:1058/1770 train_time:107487ms step_avg:102.56ms
step:1059/1770 train_time:107593ms step_avg:102.57ms
step:1060/1770 train_time:107700ms step_avg:102.57ms
step:1061/1770 train_time:107805ms step_avg:102.57ms
step:1062/1770 train_time:107913ms step_avg:102.58ms
step:1063/1770 train_time:108021ms step_avg:102.58ms
step:1064/1770 train_time:108127ms step_avg:102.59ms
step:1065/1770 train_time:108233ms step_avg:102.59ms
step:1066/1770 train_time:108340ms step_avg:102.59ms
step:1067/1770 train_time:108446ms step_avg:102.60ms
step:1068/1770 train_time:108553ms step_avg:102.60ms
step:1069/1770 train_time:108660ms step_avg:102.61ms
step:1070/1770 train_time:108766ms step_avg:102.61ms
step:1071/1770 train_time:108872ms step_avg:102.61ms
step:1072/1770 train_time:108978ms step_avg:102.62ms
step:1073/1770 train_time:109083ms step_avg:102.62ms
step:1074/1770 train_time:109189ms step_avg:102.62ms
step:1075/1770 train_time:109295ms step_avg:102.62ms
step:1076/1770 train_time:109402ms step_avg:102.63ms
step:1077/1770 train_time:109507ms step_avg:102.63ms
step:1078/1770 train_time:109615ms step_avg:102.64ms
step:1079/1770 train_time:109720ms step_avg:102.64ms
step:1080/1770 train_time:109826ms step_avg:102.64ms
step:1081/1770 train_time:109933ms step_avg:102.65ms
step:1082/1770 train_time:110039ms step_avg:102.65ms
step:1083/1770 train_time:110145ms step_avg:102.65ms
step:1084/1770 train_time:110251ms step_avg:102.66ms
step:1085/1770 train_time:110357ms step_avg:102.66ms
step:1086/1770 train_time:110463ms step_avg:102.66ms
step:1087/1770 train_time:110569ms step_avg:102.66ms
step:1088/1770 train_time:110676ms step_avg:102.67ms
step:1089/1770 train_time:110782ms step_avg:102.67ms
step:1090/1770 train_time:110889ms step_avg:102.68ms
step:1091/1770 train_time:110995ms step_avg:102.68ms
step:1092/1770 train_time:111101ms step_avg:102.68ms
step:1093/1770 train_time:111207ms step_avg:102.68ms
step:1094/1770 train_time:111315ms step_avg:102.69ms
step:1095/1770 train_time:111421ms step_avg:102.69ms
step:1096/1770 train_time:111526ms step_avg:102.69ms
step:1097/1770 train_time:111633ms step_avg:102.70ms
step:1098/1770 train_time:111739ms step_avg:102.70ms
step:1099/1770 train_time:111844ms step_avg:102.70ms
step:1100/1770 train_time:111950ms step_avg:102.71ms
step:1101/1770 train_time:112056ms step_avg:102.71ms
step:1102/1770 train_time:112162ms step_avg:102.71ms
step:1103/1770 train_time:112268ms step_avg:102.72ms
step:1104/1770 train_time:112376ms step_avg:102.72ms
step:1105/1770 train_time:112482ms step_avg:102.72ms
step:1106/1770 train_time:112587ms step_avg:102.73ms
step:1107/1770 train_time:112693ms step_avg:102.73ms
step:1108/1770 train_time:112800ms step_avg:102.73ms
step:1109/1770 train_time:112905ms step_avg:102.73ms
step:1110/1770 train_time:113012ms step_avg:102.74ms
step:1111/1770 train_time:113118ms step_avg:102.74ms
step:1112/1770 train_time:113225ms step_avg:102.75ms
step:1113/1770 train_time:113332ms step_avg:102.75ms
step:1114/1770 train_time:113438ms step_avg:102.75ms
step:1115/1770 train_time:113544ms step_avg:102.76ms
step:1116/1770 train_time:113651ms step_avg:102.76ms
step:1117/1770 train_time:113758ms step_avg:102.76ms
step:1118/1770 train_time:113863ms step_avg:102.76ms
step:1119/1770 train_time:113969ms step_avg:102.77ms
step:1120/1770 train_time:114076ms step_avg:102.77ms
step:1121/1770 train_time:114182ms step_avg:102.77ms
step:1122/1770 train_time:114288ms step_avg:102.78ms
step:1123/1770 train_time:114394ms step_avg:102.78ms
step:1124/1770 train_time:114500ms step_avg:102.78ms
step:1125/1770 train_time:114606ms step_avg:102.79ms
step:1125/1770 val_loss:3.4721 train_time:114711ms step_avg:102.88ms
step:1126/1770 train_time:114729ms step_avg:102.80ms
step:1127/1770 train_time:114834ms step_avg:102.81ms
step:1128/1770 train_time:114937ms step_avg:102.81ms
step:1129/1770 train_time:115043ms step_avg:102.81ms
step:1130/1770 train_time:115151ms step_avg:102.81ms
step:1131/1770 train_time:115257ms step_avg:102.82ms
step:1132/1770 train_time:115364ms step_avg:102.82ms
step:1133/1770 train_time:115470ms step_avg:102.82ms
step:1134/1770 train_time:115576ms step_avg:102.83ms
step:1135/1770 train_time:115682ms step_avg:102.83ms
step:1136/1770 train_time:115789ms step_avg:102.83ms
step:1137/1770 train_time:115896ms step_avg:102.84ms
step:1138/1770 train_time:116001ms step_avg:102.84ms
step:1139/1770 train_time:116107ms step_avg:102.84ms
step:1140/1770 train_time:116214ms step_avg:102.84ms
step:1141/1770 train_time:116319ms step_avg:102.85ms
step:1142/1770 train_time:116425ms step_avg:102.85ms
step:1143/1770 train_time:116531ms step_avg:102.85ms
step:1144/1770 train_time:116639ms step_avg:102.86ms
step:1145/1770 train_time:116743ms step_avg:102.86ms
step:1146/1770 train_time:116850ms step_avg:102.86ms
step:1147/1770 train_time:116956ms step_avg:102.86ms
step:1148/1770 train_time:117062ms step_avg:102.87ms
step:1149/1770 train_time:117168ms step_avg:102.87ms
step:1150/1770 train_time:117274ms step_avg:102.87ms
step:1151/1770 train_time:117381ms step_avg:102.88ms
step:1152/1770 train_time:117488ms step_avg:102.88ms
step:1153/1770 train_time:117593ms step_avg:102.88ms
step:1154/1770 train_time:117700ms step_avg:102.88ms
step:1155/1770 train_time:117806ms step_avg:102.89ms
step:1156/1770 train_time:117912ms step_avg:102.89ms
step:1157/1770 train_time:118020ms step_avg:102.89ms
step:1158/1770 train_time:118127ms step_avg:102.90ms
step:1159/1770 train_time:118234ms step_avg:102.90ms
step:1160/1770 train_time:118341ms step_avg:102.91ms
step:1161/1770 train_time:118447ms step_avg:102.91ms
step:1162/1770 train_time:118553ms step_avg:102.91ms
step:1163/1770 train_time:118659ms step_avg:102.91ms
step:1164/1770 train_time:118765ms step_avg:102.92ms
step:1165/1770 train_time:118871ms step_avg:102.92ms
step:1166/1770 train_time:118977ms step_avg:102.92ms
step:1167/1770 train_time:119083ms step_avg:102.92ms
step:1168/1770 train_time:119190ms step_avg:102.93ms
step:1169/1770 train_time:119296ms step_avg:102.93ms
step:1170/1770 train_time:119402ms step_avg:102.93ms
step:1171/1770 train_time:119509ms step_avg:102.94ms
step:1172/1770 train_time:119615ms step_avg:102.94ms
step:1173/1770 train_time:119721ms step_avg:102.94ms
step:1174/1770 train_time:119827ms step_avg:102.94ms
step:1175/1770 train_time:119933ms step_avg:102.95ms
step:1176/1770 train_time:120039ms step_avg:102.95ms
step:1177/1770 train_time:120145ms step_avg:102.95ms
step:1178/1770 train_time:120252ms step_avg:102.96ms
step:1179/1770 train_time:120358ms step_avg:102.96ms
step:1180/1770 train_time:120465ms step_avg:102.96ms
step:1181/1770 train_time:120572ms step_avg:102.96ms
step:1182/1770 train_time:120678ms step_avg:102.97ms
step:1183/1770 train_time:120786ms step_avg:102.97ms
step:1184/1770 train_time:120894ms step_avg:102.98ms
step:1185/1770 train_time:121000ms step_avg:102.98ms
step:1186/1770 train_time:121108ms step_avg:102.98ms
step:1187/1770 train_time:121217ms step_avg:102.99ms
step:1188/1770 train_time:121324ms step_avg:102.99ms
step:1189/1770 train_time:121432ms step_avg:103.00ms
step:1190/1770 train_time:121539ms step_avg:103.00ms
step:1191/1770 train_time:121647ms step_avg:103.00ms
step:1192/1770 train_time:121754ms step_avg:103.01ms
step:1193/1770 train_time:121861ms step_avg:103.01ms
step:1194/1770 train_time:121969ms step_avg:103.01ms
step:1195/1770 train_time:122076ms step_avg:103.02ms
step:1196/1770 train_time:122187ms step_avg:103.02ms
step:1197/1770 train_time:122293ms step_avg:103.03ms
step:1198/1770 train_time:122400ms step_avg:103.03ms
step:1199/1770 train_time:122507ms step_avg:103.03ms
step:1200/1770 train_time:122615ms step_avg:103.04ms
step:1201/1770 train_time:122723ms step_avg:103.04ms
step:1202/1770 train_time:122830ms step_avg:103.05ms
step:1203/1770 train_time:122937ms step_avg:103.05ms
step:1204/1770 train_time:123045ms step_avg:103.05ms
step:1205/1770 train_time:123151ms step_avg:103.06ms
step:1206/1770 train_time:123260ms step_avg:103.06ms
step:1207/1770 train_time:123367ms step_avg:103.06ms
step:1208/1770 train_time:123474ms step_avg:103.07ms
step:1209/1770 train_time:123581ms step_avg:103.07ms
step:1210/1770 train_time:123688ms step_avg:103.07ms
step:1211/1770 train_time:123796ms step_avg:103.08ms
step:1212/1770 train_time:123905ms step_avg:103.08ms
step:1213/1770 train_time:124012ms step_avg:103.09ms
step:1214/1770 train_time:124119ms step_avg:103.09ms
step:1215/1770 train_time:124227ms step_avg:103.09ms
step:1216/1770 train_time:124338ms step_avg:103.10ms
step:1217/1770 train_time:124444ms step_avg:103.10ms
step:1218/1770 train_time:124551ms step_avg:103.10ms
step:1219/1770 train_time:124659ms step_avg:103.11ms
step:1220/1770 train_time:124766ms step_avg:103.11ms
step:1221/1770 train_time:124873ms step_avg:103.12ms
step:1222/1770 train_time:124981ms step_avg:103.12ms
step:1223/1770 train_time:125089ms step_avg:103.12ms
step:1224/1770 train_time:125196ms step_avg:103.13ms
step:1225/1770 train_time:125304ms step_avg:103.13ms
step:1226/1770 train_time:125411ms step_avg:103.13ms
step:1227/1770 train_time:125520ms step_avg:103.14ms
step:1228/1770 train_time:125629ms step_avg:103.14ms
step:1229/1770 train_time:125738ms step_avg:103.15ms
step:1230/1770 train_time:125845ms step_avg:103.15ms
step:1231/1770 train_time:125953ms step_avg:103.16ms
step:1232/1770 train_time:126060ms step_avg:103.16ms
step:1233/1770 train_time:126167ms step_avg:103.16ms
step:1234/1770 train_time:126275ms step_avg:103.17ms
step:1235/1770 train_time:126382ms step_avg:103.17ms
step:1236/1770 train_time:126490ms step_avg:103.17ms
step:1237/1770 train_time:126597ms step_avg:103.18ms
step:1238/1770 train_time:126705ms step_avg:103.18ms
step:1239/1770 train_time:126812ms step_avg:103.18ms
step:1240/1770 train_time:126920ms step_avg:103.19ms
step:1241/1770 train_time:127028ms step_avg:103.19ms
step:1242/1770 train_time:127135ms step_avg:103.19ms
step:1243/1770 train_time:127242ms step_avg:103.20ms
step:1244/1770 train_time:127349ms step_avg:103.20ms
step:1245/1770 train_time:127457ms step_avg:103.20ms
step:1246/1770 train_time:127565ms step_avg:103.21ms
step:1247/1770 train_time:127672ms step_avg:103.21ms
step:1248/1770 train_time:127780ms step_avg:103.21ms
step:1249/1770 train_time:127888ms step_avg:103.22ms
step:1250/1770 train_time:127995ms step_avg:103.22ms
step:1250/1770 val_loss:3.4251 train_time:128101ms step_avg:103.31ms
step:1251/1770 train_time:128120ms step_avg:103.24ms
step:1252/1770 train_time:128220ms step_avg:103.24ms
step:1253/1770 train_time:128330ms step_avg:103.24ms
step:1254/1770 train_time:128437ms step_avg:103.25ms
step:1255/1770 train_time:128546ms step_avg:103.25ms
step:1256/1770 train_time:128653ms step_avg:103.25ms
step:1257/1770 train_time:128760ms step_avg:103.26ms
step:1258/1770 train_time:128868ms step_avg:103.26ms
step:1259/1770 train_time:128976ms step_avg:103.26ms
step:1260/1770 train_time:129083ms step_avg:103.27ms
step:1261/1770 train_time:129192ms step_avg:103.27ms
step:1262/1770 train_time:129301ms step_avg:103.28ms
step:1263/1770 train_time:129408ms step_avg:103.28ms
step:1264/1770 train_time:129516ms step_avg:103.28ms
step:1265/1770 train_time:129623ms step_avg:103.29ms
step:1266/1770 train_time:129731ms step_avg:103.29ms
step:1267/1770 train_time:129839ms step_avg:103.29ms
step:1268/1770 train_time:129947ms step_avg:103.30ms
step:1269/1770 train_time:130055ms step_avg:103.30ms
step:1270/1770 train_time:130163ms step_avg:103.30ms
step:1271/1770 train_time:130271ms step_avg:103.31ms
step:1272/1770 train_time:130378ms step_avg:103.31ms
step:1273/1770 train_time:130487ms step_avg:103.32ms
step:1274/1770 train_time:130594ms step_avg:103.32ms
step:1275/1770 train_time:130702ms step_avg:103.32ms
step:1276/1770 train_time:130809ms step_avg:103.32ms
step:1277/1770 train_time:130916ms step_avg:103.33ms
step:1278/1770 train_time:131024ms step_avg:103.33ms
step:1279/1770 train_time:131131ms step_avg:103.33ms
step:1280/1770 train_time:131240ms step_avg:103.34ms
step:1281/1770 train_time:131346ms step_avg:103.34ms
step:1282/1770 train_time:131456ms step_avg:103.35ms
step:1283/1770 train_time:131564ms step_avg:103.35ms
step:1284/1770 train_time:131671ms step_avg:103.35ms
step:1285/1770 train_time:131778ms step_avg:103.36ms
step:1286/1770 train_time:131888ms step_avg:103.36ms
step:1287/1770 train_time:131996ms step_avg:103.36ms
step:1288/1770 train_time:132106ms step_avg:103.37ms
step:1289/1770 train_time:132212ms step_avg:103.37ms
step:1290/1770 train_time:132319ms step_avg:103.37ms
step:1291/1770 train_time:132426ms step_avg:103.38ms
step:1292/1770 train_time:132534ms step_avg:103.38ms
step:1293/1770 train_time:132641ms step_avg:103.38ms
step:1294/1770 train_time:132749ms step_avg:103.39ms
step:1295/1770 train_time:132857ms step_avg:103.39ms
step:1296/1770 train_time:132964ms step_avg:103.39ms
step:1297/1770 train_time:133071ms step_avg:103.40ms
step:1298/1770 train_time:133179ms step_avg:103.40ms
step:1299/1770 train_time:133286ms step_avg:103.40ms
step:1300/1770 train_time:133394ms step_avg:103.41ms
step:1301/1770 train_time:133502ms step_avg:103.41ms
step:1302/1770 train_time:133610ms step_avg:103.41ms
step:1303/1770 train_time:133717ms step_avg:103.42ms
step:1304/1770 train_time:133824ms step_avg:103.42ms
step:1305/1770 train_time:133932ms step_avg:103.42ms
step:1306/1770 train_time:134039ms step_avg:103.42ms
step:1307/1770 train_time:134146ms step_avg:103.43ms
step:1308/1770 train_time:134254ms step_avg:103.43ms
step:1309/1770 train_time:134362ms step_avg:103.43ms
step:1310/1770 train_time:134470ms step_avg:103.44ms
step:1311/1770 train_time:134577ms step_avg:103.44ms
step:1312/1770 train_time:134683ms step_avg:103.44ms
step:1313/1770 train_time:134790ms step_avg:103.45ms
step:1314/1770 train_time:134898ms step_avg:103.45ms
step:1315/1770 train_time:135006ms step_avg:103.45ms
step:1316/1770 train_time:135113ms step_avg:103.46ms
step:1317/1770 train_time:135221ms step_avg:103.46ms
step:1318/1770 train_time:135331ms step_avg:103.46ms
step:1319/1770 train_time:135439ms step_avg:103.47ms
step:1320/1770 train_time:135546ms step_avg:103.47ms
step:1321/1770 train_time:135654ms step_avg:103.47ms
step:1322/1770 train_time:135761ms step_avg:103.48ms
step:1323/1770 train_time:135870ms step_avg:103.48ms
step:1324/1770 train_time:135978ms step_avg:103.48ms
step:1325/1770 train_time:136087ms step_avg:103.49ms
step:1326/1770 train_time:136194ms step_avg:103.49ms
step:1327/1770 train_time:136304ms step_avg:103.50ms
step:1328/1770 train_time:136412ms step_avg:103.50ms
step:1329/1770 train_time:136520ms step_avg:103.50ms
step:1330/1770 train_time:136626ms step_avg:103.50ms
step:1331/1770 train_time:136733ms step_avg:103.51ms
step:1332/1770 train_time:136840ms step_avg:103.51ms
step:1333/1770 train_time:136948ms step_avg:103.51ms
step:1334/1770 train_time:137055ms step_avg:103.52ms
step:1335/1770 train_time:137162ms step_avg:103.52ms
step:1336/1770 train_time:137270ms step_avg:103.52ms
step:1337/1770 train_time:137377ms step_avg:103.52ms
step:1338/1770 train_time:137484ms step_avg:103.53ms
step:1339/1770 train_time:137593ms step_avg:103.53ms
step:1340/1770 train_time:137702ms step_avg:103.54ms
step:1341/1770 train_time:137809ms step_avg:103.54ms
step:1342/1770 train_time:137917ms step_avg:103.54ms
step:1343/1770 train_time:138027ms step_avg:103.55ms
step:1344/1770 train_time:138135ms step_avg:103.55ms
step:1345/1770 train_time:138242ms step_avg:103.55ms
step:1346/1770 train_time:138350ms step_avg:103.56ms
step:1347/1770 train_time:138458ms step_avg:103.56ms
step:1348/1770 train_time:138568ms step_avg:103.56ms
step:1349/1770 train_time:138676ms step_avg:103.57ms
step:1350/1770 train_time:138783ms step_avg:103.57ms
step:1351/1770 train_time:138891ms step_avg:103.57ms
step:1352/1770 train_time:138998ms step_avg:103.58ms
step:1353/1770 train_time:139106ms step_avg:103.58ms
step:1354/1770 train_time:139214ms step_avg:103.58ms
step:1355/1770 train_time:139321ms step_avg:103.58ms
step:1356/1770 train_time:139428ms step_avg:103.59ms
step:1357/1770 train_time:139536ms step_avg:103.59ms
step:1358/1770 train_time:139643ms step_avg:103.59ms
step:1359/1770 train_time:139752ms step_avg:103.60ms
step:1360/1770 train_time:139861ms step_avg:103.60ms
step:1361/1770 train_time:139969ms step_avg:103.60ms
step:1362/1770 train_time:140077ms step_avg:103.61ms
step:1363/1770 train_time:140185ms step_avg:103.61ms
step:1364/1770 train_time:140293ms step_avg:103.61ms
step:1365/1770 train_time:140399ms step_avg:103.62ms
step:1366/1770 train_time:140507ms step_avg:103.62ms
step:1367/1770 train_time:140615ms step_avg:103.62ms
step:1368/1770 train_time:140723ms step_avg:103.62ms
step:1369/1770 train_time:140832ms step_avg:103.63ms
step:1370/1770 train_time:140939ms step_avg:103.63ms
step:1371/1770 train_time:141047ms step_avg:103.64ms
step:1372/1770 train_time:141155ms step_avg:103.64ms
step:1373/1770 train_time:141262ms step_avg:103.64ms
step:1374/1770 train_time:141371ms step_avg:103.64ms
step:1375/1770 train_time:141479ms step_avg:103.65ms
step:1375/1770 val_loss:3.3809 train_time:141586ms step_avg:103.73ms
step:1376/1770 train_time:141605ms step_avg:103.66ms
step:1377/1770 train_time:141704ms step_avg:103.66ms
step:1378/1770 train_time:141814ms step_avg:103.67ms
step:1379/1770 train_time:141922ms step_avg:103.67ms
step:1380/1770 train_time:142030ms step_avg:103.67ms
step:1381/1770 train_time:142137ms step_avg:103.67ms
step:1382/1770 train_time:142244ms step_avg:103.68ms
step:1383/1770 train_time:142352ms step_avg:103.68ms
step:1384/1770 train_time:142461ms step_avg:103.68ms
step:1385/1770 train_time:142569ms step_avg:103.69ms
step:1386/1770 train_time:142677ms step_avg:103.69ms
step:1387/1770 train_time:142785ms step_avg:103.69ms
step:1388/1770 train_time:142892ms step_avg:103.70ms
step:1389/1770 train_time:142999ms step_avg:103.70ms
step:1390/1770 train_time:143107ms step_avg:103.70ms
step:1391/1770 train_time:143214ms step_avg:103.70ms
step:1392/1770 train_time:143321ms step_avg:103.71ms
step:1393/1770 train_time:143429ms step_avg:103.71ms
step:1394/1770 train_time:143536ms step_avg:103.71ms
step:1395/1770 train_time:143645ms step_avg:103.71ms
step:1396/1770 train_time:143754ms step_avg:103.72ms
step:1397/1770 train_time:143863ms step_avg:103.72ms
step:1398/1770 train_time:143969ms step_avg:103.72ms
step:1399/1770 train_time:144075ms step_avg:103.73ms
step:1400/1770 train_time:144183ms step_avg:103.73ms
step:1401/1770 train_time:144290ms step_avg:103.73ms
step:1402/1770 train_time:144397ms step_avg:103.73ms
step:1403/1770 train_time:144504ms step_avg:103.74ms
step:1404/1770 train_time:144613ms step_avg:103.74ms
step:1405/1770 train_time:144721ms step_avg:103.74ms
step:1406/1770 train_time:144829ms step_avg:103.75ms
step:1407/1770 train_time:144936ms step_avg:103.75ms
step:1408/1770 train_time:145043ms step_avg:103.75ms
step:1409/1770 train_time:145151ms step_avg:103.75ms
step:1410/1770 train_time:145259ms step_avg:103.76ms
step:1411/1770 train_time:145366ms step_avg:103.76ms
step:1412/1770 train_time:145474ms step_avg:103.76ms
step:1413/1770 train_time:145581ms step_avg:103.76ms
step:1414/1770 train_time:145689ms step_avg:103.77ms
step:1415/1770 train_time:145797ms step_avg:103.77ms
step:1416/1770 train_time:145906ms step_avg:103.77ms
step:1417/1770 train_time:146014ms step_avg:103.78ms
step:1418/1770 train_time:146121ms step_avg:103.78ms
step:1419/1770 train_time:146229ms step_avg:103.78ms
step:1420/1770 train_time:146337ms step_avg:103.78ms
step:1421/1770 train_time:146444ms step_avg:103.79ms
step:1422/1770 train_time:146551ms step_avg:103.79ms
step:1423/1770 train_time:146658ms step_avg:103.79ms
step:1424/1770 train_time:146767ms step_avg:103.80ms
step:1425/1770 train_time:146874ms step_avg:103.80ms
step:1426/1770 train_time:146982ms step_avg:103.80ms
step:1427/1770 train_time:147089ms step_avg:103.80ms
step:1428/1770 train_time:147198ms step_avg:103.81ms
step:1429/1770 train_time:147306ms step_avg:103.81ms
step:1430/1770 train_time:147413ms step_avg:103.81ms
step:1431/1770 train_time:147523ms step_avg:103.82ms
step:1432/1770 train_time:147630ms step_avg:103.82ms
step:1433/1770 train_time:147737ms step_avg:103.82ms
step:1434/1770 train_time:147843ms step_avg:103.82ms
step:1435/1770 train_time:147957ms step_avg:103.83ms
step:1436/1770 train_time:148062ms step_avg:103.83ms
step:1437/1770 train_time:148170ms step_avg:103.83ms
step:1438/1770 train_time:148277ms step_avg:103.84ms
step:1439/1770 train_time:148384ms step_avg:103.84ms
step:1440/1770 train_time:148492ms step_avg:103.84ms
step:1441/1770 train_time:148601ms step_avg:103.84ms
step:1442/1770 train_time:148709ms step_avg:103.85ms
step:1443/1770 train_time:148817ms step_avg:103.85ms
step:1444/1770 train_time:148925ms step_avg:103.85ms
step:1445/1770 train_time:149033ms step_avg:103.86ms
step:1446/1770 train_time:149143ms step_avg:103.86ms
step:1447/1770 train_time:149251ms step_avg:103.86ms
step:1448/1770 train_time:149360ms step_avg:103.87ms
step:1449/1770 train_time:149470ms step_avg:103.87ms
step:1450/1770 train_time:149578ms step_avg:103.87ms
step:1451/1770 train_time:149687ms step_avg:103.88ms
step:1452/1770 train_time:149796ms step_avg:103.88ms
step:1453/1770 train_time:149904ms step_avg:103.88ms
step:1454/1770 train_time:150013ms step_avg:103.89ms
step:1455/1770 train_time:150123ms step_avg:103.89ms
step:1456/1770 train_time:150234ms step_avg:103.90ms
step:1457/1770 train_time:150343ms step_avg:103.90ms
step:1458/1770 train_time:150453ms step_avg:103.90ms
step:1459/1770 train_time:150562ms step_avg:103.91ms
step:1460/1770 train_time:150672ms step_avg:103.91ms
step:1461/1770 train_time:150780ms step_avg:103.91ms
step:1462/1770 train_time:150890ms step_avg:103.92ms
step:1463/1770 train_time:150998ms step_avg:103.92ms
step:1464/1770 train_time:151110ms step_avg:103.93ms
step:1465/1770 train_time:151219ms step_avg:103.93ms
step:1466/1770 train_time:151330ms step_avg:103.94ms
step:1467/1770 train_time:151440ms step_avg:103.94ms
step:1468/1770 train_time:151549ms step_avg:103.94ms
step:1469/1770 train_time:151658ms step_avg:103.95ms
step:1470/1770 train_time:151770ms step_avg:103.95ms
step:1471/1770 train_time:151875ms step_avg:103.95ms
step:1472/1770 train_time:151983ms step_avg:103.96ms
step:1473/1770 train_time:152093ms step_avg:103.96ms
step:1474/1770 train_time:152203ms step_avg:103.96ms
step:1475/1770 train_time:152311ms step_avg:103.97ms
step:1476/1770 train_time:152419ms step_avg:103.97ms
step:1477/1770 train_time:152531ms step_avg:103.97ms
step:1478/1770 train_time:152641ms step_avg:103.98ms
step:1479/1770 train_time:152749ms step_avg:103.98ms
step:1480/1770 train_time:152858ms step_avg:103.98ms
step:1481/1770 train_time:152971ms step_avg:103.99ms
step:1482/1770 train_time:153078ms step_avg:103.99ms
step:1483/1770 train_time:153187ms step_avg:104.00ms
step:1484/1770 train_time:153296ms step_avg:104.00ms
step:1485/1770 train_time:153404ms step_avg:104.00ms
step:1486/1770 train_time:153513ms step_avg:104.01ms
step:1487/1770 train_time:153621ms step_avg:104.01ms
step:1488/1770 train_time:153731ms step_avg:104.01ms
step:1489/1770 train_time:153841ms step_avg:104.02ms
step:1490/1770 train_time:153950ms step_avg:104.02ms
step:1491/1770 train_time:154060ms step_avg:104.02ms
step:1492/1770 train_time:154169ms step_avg:104.03ms
step:1493/1770 train_time:154281ms step_avg:104.03ms
step:1494/1770 train_time:154394ms step_avg:104.04ms
step:1495/1770 train_time:154502ms step_avg:104.04ms
step:1496/1770 train_time:154611ms step_avg:104.04ms
step:1497/1770 train_time:154720ms step_avg:104.05ms
step:1498/1770 train_time:154828ms step_avg:104.05ms
step:1499/1770 train_time:154936ms step_avg:104.05ms
step:1500/1770 train_time:155045ms step_avg:104.06ms
step:1500/1770 val_loss:3.3428 train_time:155152ms step_avg:104.13ms
step:1501/1770 train_time:155170ms step_avg:104.07ms
step:1502/1770 train_time:155272ms step_avg:104.07ms
step:1503/1770 train_time:155382ms step_avg:104.07ms
step:1504/1770 train_time:155491ms step_avg:104.08ms
step:1505/1770 train_time:155602ms step_avg:104.08ms
step:1506/1770 train_time:155711ms step_avg:104.08ms
step:1507/1770 train_time:155820ms step_avg:104.09ms
step:1508/1770 train_time:155930ms step_avg:104.09ms
step:1509/1770 train_time:156039ms step_avg:104.10ms
step:1510/1770 train_time:156147ms step_avg:104.10ms
step:1511/1770 train_time:156257ms step_avg:104.10ms
step:1512/1770 train_time:156366ms step_avg:104.11ms
step:1513/1770 train_time:156476ms step_avg:104.11ms
step:1514/1770 train_time:156584ms step_avg:104.11ms
step:1515/1770 train_time:156693ms step_avg:104.12ms
step:1516/1770 train_time:156802ms step_avg:104.12ms
step:1517/1770 train_time:156911ms step_avg:104.12ms
step:1518/1770 train_time:157023ms step_avg:104.13ms
step:1519/1770 train_time:157130ms step_avg:104.13ms
step:1520/1770 train_time:157240ms step_avg:104.13ms
step:1521/1770 train_time:157347ms step_avg:104.13ms
step:1522/1770 train_time:157457ms step_avg:104.14ms
step:1523/1770 train_time:157566ms step_avg:104.14ms
step:1524/1770 train_time:157675ms step_avg:104.14ms
step:1525/1770 train_time:157783ms step_avg:104.15ms
step:1526/1770 train_time:157891ms step_avg:104.15ms
step:1527/1770 train_time:158000ms step_avg:104.15ms
step:1528/1770 train_time:158111ms step_avg:104.16ms
step:1529/1770 train_time:158220ms step_avg:104.16ms
step:1530/1770 train_time:158329ms step_avg:104.16ms
step:1531/1770 train_time:158437ms step_avg:104.17ms
step:1532/1770 train_time:158546ms step_avg:104.17ms
step:1533/1770 train_time:158656ms step_avg:104.17ms
step:1534/1770 train_time:158765ms step_avg:104.18ms
step:1535/1770 train_time:158874ms step_avg:104.18ms
step:1536/1770 train_time:158982ms step_avg:104.18ms
step:1537/1770 train_time:159091ms step_avg:104.19ms
step:1538/1770 train_time:159202ms step_avg:104.19ms
step:1539/1770 train_time:159309ms step_avg:104.19ms
step:1540/1770 train_time:159423ms step_avg:104.20ms
step:1541/1770 train_time:159532ms step_avg:104.20ms
step:1542/1770 train_time:159641ms step_avg:104.20ms
step:1543/1770 train_time:159749ms step_avg:104.21ms
step:1544/1770 train_time:159860ms step_avg:104.21ms
step:1545/1770 train_time:159969ms step_avg:104.21ms
step:1546/1770 train_time:160079ms step_avg:104.22ms
step:1547/1770 train_time:160187ms step_avg:104.22ms
step:1548/1770 train_time:160297ms step_avg:104.22ms
step:1549/1770 train_time:160405ms step_avg:104.23ms
step:1550/1770 train_time:160513ms step_avg:104.23ms
step:1551/1770 train_time:160621ms step_avg:104.23ms
step:1552/1770 train_time:160731ms step_avg:104.24ms
step:1553/1770 train_time:160840ms step_avg:104.24ms
step:1554/1770 train_time:160948ms step_avg:104.24ms
step:1555/1770 train_time:161058ms step_avg:104.24ms
step:1556/1770 train_time:161166ms step_avg:104.25ms
step:1557/1770 train_time:161275ms step_avg:104.25ms
step:1558/1770 train_time:161384ms step_avg:104.25ms
step:1559/1770 train_time:161493ms step_avg:104.26ms
step:1560/1770 train_time:161601ms step_avg:104.26ms
step:1561/1770 train_time:161713ms step_avg:104.26ms
step:1562/1770 train_time:161821ms step_avg:104.27ms
step:1563/1770 train_time:161930ms step_avg:104.27ms
step:1564/1770 train_time:162038ms step_avg:104.27ms
step:1565/1770 train_time:162146ms step_avg:104.27ms
step:1566/1770 train_time:162257ms step_avg:104.28ms
step:1567/1770 train_time:162365ms step_avg:104.28ms
step:1568/1770 train_time:162474ms step_avg:104.28ms
step:1569/1770 train_time:162587ms step_avg:104.29ms
step:1570/1770 train_time:162694ms step_avg:104.29ms
step:1571/1770 train_time:162803ms step_avg:104.29ms
step:1572/1770 train_time:162913ms step_avg:104.30ms
step:1573/1770 train_time:163023ms step_avg:104.30ms
step:1574/1770 train_time:163132ms step_avg:104.30ms
step:1575/1770 train_time:163240ms step_avg:104.31ms
step:1576/1770 train_time:163349ms step_avg:104.31ms
step:1577/1770 train_time:163459ms step_avg:104.31ms
step:1578/1770 train_time:163569ms step_avg:104.32ms
step:1579/1770 train_time:163677ms step_avg:104.32ms
step:1580/1770 train_time:163786ms step_avg:104.32ms
step:1581/1770 train_time:163898ms step_avg:104.33ms
step:1582/1770 train_time:164011ms step_avg:104.33ms
step:1583/1770 train_time:164117ms step_avg:104.33ms
step:1584/1770 train_time:164226ms step_avg:104.34ms
step:1585/1770 train_time:164336ms step_avg:104.34ms
step:1586/1770 train_time:164449ms step_avg:104.35ms
step:1587/1770 train_time:164558ms step_avg:104.35ms
step:1588/1770 train_time:164667ms step_avg:104.35ms
step:1589/1770 train_time:164778ms step_avg:104.36ms
step:1590/1770 train_time:164886ms step_avg:104.36ms
step:1591/1770 train_time:164994ms step_avg:104.36ms
step:1592/1770 train_time:165104ms step_avg:104.36ms
step:1593/1770 train_time:165213ms step_avg:104.37ms
step:1594/1770 train_time:165322ms step_avg:104.37ms
step:1595/1770 train_time:165430ms step_avg:104.37ms
step:1596/1770 train_time:165541ms step_avg:104.38ms
step:1597/1770 train_time:165649ms step_avg:104.38ms
step:1598/1770 train_time:165759ms step_avg:104.38ms
step:1599/1770 train_time:165870ms step_avg:104.39ms
step:1600/1770 train_time:165980ms step_avg:104.39ms
step:1601/1770 train_time:166090ms step_avg:104.39ms
step:1602/1770 train_time:166200ms step_avg:104.40ms
step:1603/1770 train_time:166309ms step_avg:104.40ms
step:1604/1770 train_time:166417ms step_avg:104.40ms
step:1605/1770 train_time:166525ms step_avg:104.40ms
step:1606/1770 train_time:166634ms step_avg:104.41ms
step:1607/1770 train_time:166747ms step_avg:104.41ms
step:1608/1770 train_time:166857ms step_avg:104.42ms
step:1609/1770 train_time:166966ms step_avg:104.42ms
step:1610/1770 train_time:167077ms step_avg:104.42ms
step:1611/1770 train_time:167187ms step_avg:104.43ms
step:1612/1770 train_time:167296ms step_avg:104.43ms
step:1613/1770 train_time:167405ms step_avg:104.43ms
step:1614/1770 train_time:167514ms step_avg:104.44ms
step:1615/1770 train_time:167624ms step_avg:104.44ms
step:1616/1770 train_time:167732ms step_avg:104.44ms
step:1617/1770 train_time:167843ms step_avg:104.45ms
step:1618/1770 train_time:167953ms step_avg:104.45ms
step:1619/1770 train_time:168062ms step_avg:104.45ms
step:1620/1770 train_time:168172ms step_avg:104.45ms
step:1621/1770 train_time:168281ms step_avg:104.46ms
step:1622/1770 train_time:168391ms step_avg:104.46ms
step:1623/1770 train_time:168503ms step_avg:104.47ms
step:1624/1770 train_time:168611ms step_avg:104.47ms
step:1625/1770 train_time:168719ms step_avg:104.47ms
step:1625/1770 val_loss:3.3080 train_time:168827ms step_avg:104.54ms
step:1626/1770 train_time:168845ms step_avg:104.48ms
step:1627/1770 train_time:168944ms step_avg:104.48ms
step:1628/1770 train_time:169052ms step_avg:104.48ms
step:1629/1770 train_time:169162ms step_avg:104.49ms
step:1630/1770 train_time:169271ms step_avg:104.49ms
step:1631/1770 train_time:169380ms step_avg:104.49ms
step:1632/1770 train_time:169489ms step_avg:104.49ms
step:1633/1770 train_time:169598ms step_avg:104.50ms
step:1634/1770 train_time:169706ms step_avg:104.50ms
step:1635/1770 train_time:169815ms step_avg:104.50ms
step:1636/1770 train_time:169925ms step_avg:104.50ms
step:1637/1770 train_time:170034ms step_avg:104.51ms
step:1638/1770 train_time:170143ms step_avg:104.51ms
step:1639/1770 train_time:170252ms step_avg:104.51ms
step:1640/1770 train_time:170362ms step_avg:104.52ms
step:1641/1770 train_time:170470ms step_avg:104.52ms
step:1642/1770 train_time:170578ms step_avg:104.52ms
step:1643/1770 train_time:170687ms step_avg:104.52ms
step:1644/1770 train_time:170798ms step_avg:104.53ms
step:1645/1770 train_time:170906ms step_avg:104.53ms
step:1646/1770 train_time:171017ms step_avg:104.53ms
step:1647/1770 train_time:171126ms step_avg:104.54ms
step:1648/1770 train_time:171234ms step_avg:104.54ms
step:1649/1770 train_time:171343ms step_avg:104.54ms
step:1650/1770 train_time:171452ms step_avg:104.54ms
step:1651/1770 train_time:171561ms step_avg:104.55ms
step:1652/1770 train_time:171669ms step_avg:104.55ms
step:1653/1770 train_time:171778ms step_avg:104.55ms
step:1654/1770 train_time:171891ms step_avg:104.56ms
step:1655/1770 train_time:172003ms step_avg:104.56ms
step:1656/1770 train_time:172113ms step_avg:104.56ms
step:1657/1770 train_time:172224ms step_avg:104.57ms
step:1658/1770 train_time:172332ms step_avg:104.57ms
step:1659/1770 train_time:172443ms step_avg:104.57ms
step:1660/1770 train_time:172551ms step_avg:104.58ms
step:1661/1770 train_time:172661ms step_avg:104.58ms
step:1662/1770 train_time:172771ms step_avg:104.58ms
step:1663/1770 train_time:172880ms step_avg:104.59ms
step:1664/1770 train_time:172989ms step_avg:104.59ms
step:1665/1770 train_time:173098ms step_avg:104.59ms
step:1666/1770 train_time:173207ms step_avg:104.59ms
step:1667/1770 train_time:173315ms step_avg:104.60ms
step:1668/1770 train_time:173424ms step_avg:104.60ms
step:1669/1770 train_time:173532ms step_avg:104.60ms
step:1670/1770 train_time:173640ms step_avg:104.60ms
step:1671/1770 train_time:173750ms step_avg:104.61ms
step:1672/1770 train_time:173859ms step_avg:104.61ms
step:1673/1770 train_time:173968ms step_avg:104.61ms
step:1674/1770 train_time:174077ms step_avg:104.61ms
step:1675/1770 train_time:174185ms step_avg:104.62ms
step:1676/1770 train_time:174296ms step_avg:104.62ms
step:1677/1770 train_time:174409ms step_avg:104.62ms
step:1678/1770 train_time:174517ms step_avg:104.63ms
step:1679/1770 train_time:174626ms step_avg:104.63ms
step:1680/1770 train_time:174735ms step_avg:104.63ms
step:1681/1770 train_time:174844ms step_avg:104.63ms
step:1682/1770 train_time:174955ms step_avg:104.64ms
step:1683/1770 train_time:175064ms step_avg:104.64ms
step:1684/1770 train_time:175172ms step_avg:104.64ms
step:1685/1770 train_time:175280ms step_avg:104.64ms
step:1686/1770 train_time:175390ms step_avg:104.65ms
step:1687/1770 train_time:175501ms step_avg:104.65ms
step:1688/1770 train_time:175610ms step_avg:104.65ms
step:1689/1770 train_time:175718ms step_avg:104.66ms
step:1690/1770 train_time:175827ms step_avg:104.66ms
step:1691/1770 train_time:175937ms step_avg:104.66ms
step:1692/1770 train_time:176045ms step_avg:104.66ms
step:1693/1770 train_time:176155ms step_avg:104.67ms
step:1694/1770 train_time:176264ms step_avg:104.67ms
step:1695/1770 train_time:176373ms step_avg:104.67ms
step:1696/1770 train_time:176485ms step_avg:104.68ms
step:1697/1770 train_time:176596ms step_avg:104.68ms
step:1698/1770 train_time:176706ms step_avg:104.68ms
step:1699/1770 train_time:176814ms step_avg:104.69ms
step:1700/1770 train_time:176923ms step_avg:104.69ms
step:1701/1770 train_time:177035ms step_avg:104.69ms
step:1702/1770 train_time:177141ms step_avg:104.69ms
step:1703/1770 train_time:177249ms step_avg:104.70ms
step:1704/1770 train_time:177357ms step_avg:104.70ms
step:1705/1770 train_time:177466ms step_avg:104.70ms
step:1706/1770 train_time:177574ms step_avg:104.70ms
step:1707/1770 train_time:177685ms step_avg:104.71ms
step:1708/1770 train_time:177794ms step_avg:104.71ms
step:1709/1770 train_time:177905ms step_avg:104.71ms
step:1710/1770 train_time:178018ms step_avg:104.72ms
step:1711/1770 train_time:178130ms step_avg:104.72ms
step:1712/1770 train_time:178240ms step_avg:104.72ms
step:1713/1770 train_time:178348ms step_avg:104.73ms
step:1714/1770 train_time:178458ms step_avg:104.73ms
step:1715/1770 train_time:178568ms step_avg:104.73ms
step:1716/1770 train_time:178678ms step_avg:104.74ms
step:1717/1770 train_time:178787ms step_avg:104.74ms
step:1718/1770 train_time:178897ms step_avg:104.74ms
step:1719/1770 train_time:179008ms step_avg:104.74ms
step:1720/1770 train_time:179118ms step_avg:104.75ms
step:1721/1770 train_time:179227ms step_avg:104.75ms
step:1722/1770 train_time:179339ms step_avg:104.75ms
step:1723/1770 train_time:179450ms step_avg:104.76ms
step:1724/1770 train_time:179562ms step_avg:104.76ms
step:1725/1770 train_time:179674ms step_avg:104.77ms
step:1726/1770 train_time:179786ms step_avg:104.77ms
step:1727/1770 train_time:179895ms step_avg:104.77ms
step:1728/1770 train_time:180007ms step_avg:104.78ms
step:1729/1770 train_time:180117ms step_avg:104.78ms
step:1730/1770 train_time:180232ms step_avg:104.79ms
step:1731/1770 train_time:180339ms step_avg:104.79ms
step:1732/1770 train_time:180448ms step_avg:104.79ms
step:1733/1770 train_time:180559ms step_avg:104.79ms
step:1734/1770 train_time:180668ms step_avg:104.80ms
step:1735/1770 train_time:180778ms step_avg:104.80ms
step:1736/1770 train_time:180887ms step_avg:104.80ms
step:1737/1770 train_time:180996ms step_avg:104.80ms
step:1738/1770 train_time:181106ms step_avg:104.81ms
step:1739/1770 train_time:181215ms step_avg:104.81ms
step:1740/1770 train_time:181324ms step_avg:104.81ms
step:1741/1770 train_time:181436ms step_avg:104.82ms
step:1742/1770 train_time:181549ms step_avg:104.82ms
step:1743/1770 train_time:181659ms step_avg:104.82ms
step:1744/1770 train_time:181768ms step_avg:104.83ms
step:1745/1770 train_time:181877ms step_avg:104.83ms
step:1746/1770 train_time:181991ms step_avg:104.83ms
step:1747/1770 train_time:182100ms step_avg:104.84ms
step:1748/1770 train_time:182212ms step_avg:104.84ms
step:1749/1770 train_time:182324ms step_avg:104.84ms
step:1750/1770 train_time:182433ms step_avg:104.85ms
step:1750/1770 val_loss:3.2810 train_time:182542ms step_avg:104.91ms
step:1751/1770 train_time:182560ms step_avg:104.86ms
step:1752/1770 train_time:182662ms step_avg:104.86ms
step:1753/1770 train_time:182771ms step_avg:104.86ms
step:1754/1770 train_time:182881ms step_avg:104.86ms
step:1755/1770 train_time:182991ms step_avg:104.87ms
step:1756/1770 train_time:183101ms step_avg:104.87ms
step:1757/1770 train_time:183212ms step_avg:104.87ms
step:1758/1770 train_time:183322ms step_avg:104.88ms
step:1759/1770 train_time:183432ms step_avg:104.88ms
step:1760/1770 train_time:183542ms step_avg:104.88ms
step:1761/1770 train_time:183655ms step_avg:104.89ms
step:1762/1770 train_time:183769ms step_avg:104.89ms
step:1763/1770 train_time:183877ms step_avg:104.89ms
step:1764/1770 train_time:183987ms step_avg:104.90ms
step:1765/1770 train_time:184097ms step_avg:104.90ms
step:1766/1770 train_time:184211ms step_avg:104.90ms
step:1767/1770 train_time:184320ms step_avg:104.91ms
step:1768/1770 train_time:184431ms step_avg:104.91ms
step:1769/1770 train_time:184540ms step_avg:104.91ms
step:1770/1770 train_time:184649ms step_avg:104.91ms
step:1770/1770 val_loss:3.2779 train_time:184759ms step_avg:104.98ms
peak memory allocated: 24161 MiB reserved: 27952 MiB
