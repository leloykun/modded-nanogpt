import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:17:36 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23193ms step_avg:nanms
step:2/1770 train_time:23711ms step_avg:nanms
step:3/1770 train_time:23806ms step_avg:nanms
step:4/1770 train_time:23899ms step_avg:nanms
step:5/1770 train_time:23992ms step_avg:nanms
step:6/1770 train_time:24086ms step_avg:nanms
step:7/1770 train_time:24179ms step_avg:nanms
step:8/1770 train_time:24273ms step_avg:nanms
step:9/1770 train_time:24367ms step_avg:nanms
step:10/1770 train_time:24461ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.60ms
step:14/1770 train_time:375ms step_avg:93.76ms
step:15/1770 train_time:469ms step_avg:93.83ms
step:16/1770 train_time:563ms step_avg:93.87ms
step:17/1770 train_time:657ms step_avg:93.83ms
step:18/1770 train_time:750ms step_avg:93.80ms
step:19/1770 train_time:844ms step_avg:93.79ms
step:20/1770 train_time:938ms step_avg:93.81ms
step:21/1770 train_time:1032ms step_avg:93.80ms
step:22/1770 train_time:1126ms step_avg:93.80ms
step:23/1770 train_time:1219ms step_avg:93.79ms
step:24/1770 train_time:1313ms step_avg:93.82ms
step:25/1770 train_time:1408ms step_avg:93.84ms
step:26/1770 train_time:1502ms step_avg:93.88ms
step:27/1770 train_time:1596ms step_avg:93.87ms
step:28/1770 train_time:1689ms step_avg:93.85ms
step:29/1770 train_time:1783ms step_avg:93.86ms
step:30/1770 train_time:1877ms step_avg:93.86ms
step:31/1770 train_time:1971ms step_avg:93.85ms
step:32/1770 train_time:2065ms step_avg:93.85ms
step:33/1770 train_time:2159ms step_avg:93.85ms
step:34/1770 train_time:2252ms step_avg:93.85ms
step:35/1770 train_time:2346ms step_avg:93.84ms
step:36/1770 train_time:2440ms step_avg:93.83ms
step:37/1770 train_time:2534ms step_avg:93.84ms
step:38/1770 train_time:2627ms step_avg:93.82ms
step:39/1770 train_time:2721ms step_avg:93.83ms
step:40/1770 train_time:2815ms step_avg:93.83ms
step:41/1770 train_time:2908ms step_avg:93.82ms
step:42/1770 train_time:3002ms step_avg:93.82ms
step:43/1770 train_time:3096ms step_avg:93.83ms
step:44/1770 train_time:3190ms step_avg:93.83ms
step:45/1770 train_time:3285ms step_avg:93.85ms
step:46/1770 train_time:3378ms step_avg:93.85ms
step:47/1770 train_time:3473ms step_avg:93.86ms
step:48/1770 train_time:3566ms step_avg:93.84ms
step:49/1770 train_time:3660ms step_avg:93.84ms
step:50/1770 train_time:3754ms step_avg:93.84ms
step:51/1770 train_time:3847ms step_avg:93.84ms
step:52/1770 train_time:3941ms step_avg:93.82ms
step:53/1770 train_time:4035ms step_avg:93.83ms
step:54/1770 train_time:4128ms step_avg:93.83ms
step:55/1770 train_time:4222ms step_avg:93.82ms
step:56/1770 train_time:4316ms step_avg:93.83ms
step:57/1770 train_time:4410ms step_avg:93.83ms
step:58/1770 train_time:4506ms step_avg:93.88ms
step:59/1770 train_time:4598ms step_avg:93.84ms
step:60/1770 train_time:4692ms step_avg:93.83ms
step:61/1770 train_time:4785ms step_avg:93.83ms
step:62/1770 train_time:4879ms step_avg:93.83ms
step:63/1770 train_time:4973ms step_avg:93.83ms
step:64/1770 train_time:5067ms step_avg:93.84ms
step:65/1770 train_time:5161ms step_avg:93.83ms
step:66/1770 train_time:5254ms step_avg:93.83ms
step:67/1770 train_time:5349ms step_avg:93.84ms
step:68/1770 train_time:5442ms step_avg:93.83ms
step:69/1770 train_time:5536ms step_avg:93.84ms
step:70/1770 train_time:5630ms step_avg:93.84ms
step:71/1770 train_time:5724ms step_avg:93.84ms
step:72/1770 train_time:5819ms step_avg:93.85ms
step:73/1770 train_time:5912ms step_avg:93.85ms
step:74/1770 train_time:6006ms step_avg:93.85ms
step:75/1770 train_time:6100ms step_avg:93.85ms
step:76/1770 train_time:6193ms step_avg:93.84ms
step:77/1770 train_time:6287ms step_avg:93.84ms
step:78/1770 train_time:6381ms step_avg:93.84ms
step:79/1770 train_time:6475ms step_avg:93.84ms
step:80/1770 train_time:6570ms step_avg:93.85ms
step:81/1770 train_time:6664ms step_avg:93.86ms
step:82/1770 train_time:6758ms step_avg:93.86ms
step:83/1770 train_time:6852ms step_avg:93.86ms
step:84/1770 train_time:6945ms step_avg:93.85ms
step:85/1770 train_time:7039ms step_avg:93.85ms
step:86/1770 train_time:7133ms step_avg:93.85ms
step:87/1770 train_time:7227ms step_avg:93.85ms
step:88/1770 train_time:7320ms step_avg:93.84ms
step:89/1770 train_time:7413ms step_avg:93.84ms
step:90/1770 train_time:7507ms step_avg:93.84ms
step:91/1770 train_time:7601ms step_avg:93.83ms
step:92/1770 train_time:7694ms step_avg:93.83ms
step:93/1770 train_time:7788ms step_avg:93.83ms
step:94/1770 train_time:7881ms step_avg:93.82ms
step:95/1770 train_time:7975ms step_avg:93.82ms
step:96/1770 train_time:8069ms step_avg:93.83ms
step:97/1770 train_time:8163ms step_avg:93.83ms
step:98/1770 train_time:8257ms step_avg:93.83ms
step:99/1770 train_time:8351ms step_avg:93.83ms
step:100/1770 train_time:8445ms step_avg:93.83ms
step:101/1770 train_time:8538ms step_avg:93.83ms
step:102/1770 train_time:8632ms step_avg:93.83ms
step:103/1770 train_time:8726ms step_avg:93.83ms
step:104/1770 train_time:8820ms step_avg:93.83ms
step:105/1770 train_time:8914ms step_avg:93.83ms
step:106/1770 train_time:9007ms step_avg:93.83ms
step:107/1770 train_time:9101ms step_avg:93.83ms
step:108/1770 train_time:9195ms step_avg:93.83ms
step:109/1770 train_time:9289ms step_avg:93.83ms
step:110/1770 train_time:9383ms step_avg:93.83ms
step:111/1770 train_time:9477ms step_avg:93.83ms
step:112/1770 train_time:9571ms step_avg:93.83ms
step:113/1770 train_time:9665ms step_avg:93.83ms
step:114/1770 train_time:9759ms step_avg:93.84ms
step:115/1770 train_time:9852ms step_avg:93.83ms
step:116/1770 train_time:9946ms step_avg:93.83ms
step:117/1770 train_time:10041ms step_avg:93.84ms
step:118/1770 train_time:10134ms step_avg:93.83ms
step:119/1770 train_time:10228ms step_avg:93.83ms
step:120/1770 train_time:10321ms step_avg:93.83ms
step:121/1770 train_time:10416ms step_avg:93.84ms
step:122/1770 train_time:10509ms step_avg:93.83ms
step:123/1770 train_time:10602ms step_avg:93.83ms
step:124/1770 train_time:10696ms step_avg:93.82ms
step:125/1770 train_time:10790ms step_avg:93.82ms
step:125/1770 val_loss:4.6424 train_time:10882ms step_avg:94.63ms
step:126/1770 train_time:10903ms step_avg:93.99ms
step:127/1770 train_time:10980ms step_avg:93.84ms
step:128/1770 train_time:11079ms step_avg:93.89ms
step:129/1770 train_time:11180ms step_avg:93.95ms
step:130/1770 train_time:11275ms step_avg:93.96ms
step:131/1770 train_time:11369ms step_avg:93.96ms
step:132/1770 train_time:11463ms step_avg:93.96ms
step:133/1770 train_time:11556ms step_avg:93.95ms
step:134/1770 train_time:11650ms step_avg:93.96ms
step:135/1770 train_time:11744ms step_avg:93.96ms
step:136/1770 train_time:11838ms step_avg:93.96ms
step:137/1770 train_time:11933ms step_avg:93.96ms
step:138/1770 train_time:12027ms step_avg:93.96ms
step:139/1770 train_time:12122ms step_avg:93.97ms
step:140/1770 train_time:12217ms step_avg:93.98ms
step:141/1770 train_time:12312ms step_avg:93.98ms
step:142/1770 train_time:12406ms step_avg:93.99ms
step:143/1770 train_time:12501ms step_avg:93.99ms
step:144/1770 train_time:12595ms step_avg:93.99ms
step:145/1770 train_time:12689ms step_avg:94.00ms
step:146/1770 train_time:12784ms step_avg:94.00ms
step:147/1770 train_time:12879ms step_avg:94.01ms
step:148/1770 train_time:12973ms step_avg:94.01ms
step:149/1770 train_time:13067ms step_avg:94.01ms
step:150/1770 train_time:13161ms step_avg:94.01ms
step:151/1770 train_time:13256ms step_avg:94.01ms
step:152/1770 train_time:13350ms step_avg:94.02ms
step:153/1770 train_time:13445ms step_avg:94.02ms
step:154/1770 train_time:13540ms step_avg:94.03ms
step:155/1770 train_time:13634ms step_avg:94.03ms
step:156/1770 train_time:13729ms step_avg:94.03ms
step:157/1770 train_time:13823ms step_avg:94.03ms
step:158/1770 train_time:13917ms step_avg:94.03ms
step:159/1770 train_time:14012ms step_avg:94.04ms
step:160/1770 train_time:14106ms step_avg:94.04ms
step:161/1770 train_time:14201ms step_avg:94.04ms
step:162/1770 train_time:14296ms step_avg:94.05ms
step:163/1770 train_time:14390ms step_avg:94.05ms
step:164/1770 train_time:14484ms step_avg:94.05ms
step:165/1770 train_time:14579ms step_avg:94.06ms
step:166/1770 train_time:14673ms step_avg:94.06ms
step:167/1770 train_time:14767ms step_avg:94.06ms
step:168/1770 train_time:14862ms step_avg:94.07ms
step:169/1770 train_time:14956ms step_avg:94.06ms
step:170/1770 train_time:15051ms step_avg:94.07ms
step:171/1770 train_time:15146ms step_avg:94.07ms
step:172/1770 train_time:15240ms step_avg:94.08ms
step:173/1770 train_time:15334ms step_avg:94.08ms
step:174/1770 train_time:15429ms step_avg:94.08ms
step:175/1770 train_time:15523ms step_avg:94.08ms
step:176/1770 train_time:15617ms step_avg:94.08ms
step:177/1770 train_time:15711ms step_avg:94.08ms
step:178/1770 train_time:15806ms step_avg:94.08ms
step:179/1770 train_time:15900ms step_avg:94.08ms
step:180/1770 train_time:15995ms step_avg:94.09ms
step:181/1770 train_time:16089ms step_avg:94.09ms
step:182/1770 train_time:16184ms step_avg:94.09ms
step:183/1770 train_time:16278ms step_avg:94.09ms
step:184/1770 train_time:16372ms step_avg:94.09ms
step:185/1770 train_time:16467ms step_avg:94.10ms
step:186/1770 train_time:16562ms step_avg:94.10ms
step:187/1770 train_time:16656ms step_avg:94.10ms
step:188/1770 train_time:16751ms step_avg:94.11ms
step:189/1770 train_time:16845ms step_avg:94.11ms
step:190/1770 train_time:16940ms step_avg:94.11ms
step:191/1770 train_time:17034ms step_avg:94.11ms
step:192/1770 train_time:17130ms step_avg:94.12ms
step:193/1770 train_time:17224ms step_avg:94.12ms
step:194/1770 train_time:17318ms step_avg:94.12ms
step:195/1770 train_time:17413ms step_avg:94.12ms
step:196/1770 train_time:17507ms step_avg:94.13ms
step:197/1770 train_time:17602ms step_avg:94.13ms
step:198/1770 train_time:17697ms step_avg:94.13ms
step:199/1770 train_time:17791ms step_avg:94.13ms
step:200/1770 train_time:17886ms step_avg:94.14ms
step:201/1770 train_time:17981ms step_avg:94.14ms
step:202/1770 train_time:18075ms step_avg:94.14ms
step:203/1770 train_time:18170ms step_avg:94.14ms
step:204/1770 train_time:18264ms step_avg:94.15ms
step:205/1770 train_time:18359ms step_avg:94.15ms
step:206/1770 train_time:18453ms step_avg:94.15ms
step:207/1770 train_time:18548ms step_avg:94.15ms
step:208/1770 train_time:18642ms step_avg:94.15ms
step:209/1770 train_time:18736ms step_avg:94.15ms
step:210/1770 train_time:18831ms step_avg:94.15ms
step:211/1770 train_time:18926ms step_avg:94.16ms
step:212/1770 train_time:19020ms step_avg:94.16ms
step:213/1770 train_time:19115ms step_avg:94.16ms
step:214/1770 train_time:19210ms step_avg:94.16ms
step:215/1770 train_time:19305ms step_avg:94.17ms
step:216/1770 train_time:19399ms step_avg:94.17ms
step:217/1770 train_time:19493ms step_avg:94.17ms
step:218/1770 train_time:19588ms step_avg:94.17ms
step:219/1770 train_time:19683ms step_avg:94.18ms
step:220/1770 train_time:19778ms step_avg:94.18ms
step:221/1770 train_time:19873ms step_avg:94.18ms
step:222/1770 train_time:19967ms step_avg:94.18ms
step:223/1770 train_time:20062ms step_avg:94.19ms
step:224/1770 train_time:20156ms step_avg:94.19ms
step:225/1770 train_time:20250ms step_avg:94.19ms
step:226/1770 train_time:20345ms step_avg:94.19ms
step:227/1770 train_time:20439ms step_avg:94.19ms
step:228/1770 train_time:20534ms step_avg:94.19ms
step:229/1770 train_time:20629ms step_avg:94.19ms
step:230/1770 train_time:20723ms step_avg:94.20ms
step:231/1770 train_time:20818ms step_avg:94.20ms
step:232/1770 train_time:20913ms step_avg:94.20ms
step:233/1770 train_time:21007ms step_avg:94.20ms
step:234/1770 train_time:21102ms step_avg:94.21ms
step:235/1770 train_time:21197ms step_avg:94.21ms
step:236/1770 train_time:21291ms step_avg:94.21ms
step:237/1770 train_time:21386ms step_avg:94.21ms
step:238/1770 train_time:21480ms step_avg:94.21ms
step:239/1770 train_time:21574ms step_avg:94.21ms
step:240/1770 train_time:21669ms step_avg:94.21ms
step:241/1770 train_time:21763ms step_avg:94.21ms
step:242/1770 train_time:21858ms step_avg:94.22ms
step:243/1770 train_time:21952ms step_avg:94.22ms
step:244/1770 train_time:22047ms step_avg:94.22ms
step:245/1770 train_time:22142ms step_avg:94.22ms
step:246/1770 train_time:22236ms step_avg:94.22ms
step:247/1770 train_time:22331ms step_avg:94.22ms
step:248/1770 train_time:22425ms step_avg:94.22ms
step:249/1770 train_time:22519ms step_avg:94.22ms
step:250/1770 train_time:22614ms step_avg:94.22ms
step:250/1770 val_loss:4.1142 train_time:22707ms step_avg:94.61ms
step:251/1770 train_time:22727ms step_avg:94.30ms
step:252/1770 train_time:22809ms step_avg:94.25ms
step:253/1770 train_time:22908ms step_avg:94.27ms
step:254/1770 train_time:23003ms step_avg:94.27ms
step:255/1770 train_time:23097ms step_avg:94.27ms
step:256/1770 train_time:23191ms step_avg:94.27ms
step:257/1770 train_time:23285ms step_avg:94.27ms
step:258/1770 train_time:23379ms step_avg:94.27ms
step:259/1770 train_time:23473ms step_avg:94.27ms
step:260/1770 train_time:23567ms step_avg:94.27ms
step:261/1770 train_time:23662ms step_avg:94.27ms
step:262/1770 train_time:23758ms step_avg:94.28ms
step:263/1770 train_time:23854ms step_avg:94.28ms
step:264/1770 train_time:23949ms step_avg:94.29ms
step:265/1770 train_time:24044ms step_avg:94.29ms
step:266/1770 train_time:24139ms step_avg:94.29ms
step:267/1770 train_time:24234ms step_avg:94.30ms
step:268/1770 train_time:24329ms step_avg:94.30ms
step:269/1770 train_time:24424ms step_avg:94.30ms
step:270/1770 train_time:24519ms step_avg:94.31ms
step:271/1770 train_time:24614ms step_avg:94.31ms
step:272/1770 train_time:24708ms step_avg:94.31ms
step:273/1770 train_time:24804ms step_avg:94.31ms
step:274/1770 train_time:24900ms step_avg:94.32ms
step:275/1770 train_time:24995ms step_avg:94.32ms
step:276/1770 train_time:25090ms step_avg:94.32ms
step:277/1770 train_time:25185ms step_avg:94.33ms
step:278/1770 train_time:25280ms step_avg:94.33ms
step:279/1770 train_time:25375ms step_avg:94.33ms
step:280/1770 train_time:25470ms step_avg:94.33ms
step:281/1770 train_time:25565ms step_avg:94.33ms
step:282/1770 train_time:25660ms step_avg:94.34ms
step:283/1770 train_time:25754ms step_avg:94.34ms
step:284/1770 train_time:25850ms step_avg:94.34ms
step:285/1770 train_time:25944ms step_avg:94.34ms
step:286/1770 train_time:26039ms step_avg:94.35ms
step:287/1770 train_time:26135ms step_avg:94.35ms
step:288/1770 train_time:26230ms step_avg:94.35ms
step:289/1770 train_time:26326ms step_avg:94.36ms
step:290/1770 train_time:26421ms step_avg:94.36ms
step:291/1770 train_time:26516ms step_avg:94.36ms
step:292/1770 train_time:26611ms step_avg:94.36ms
step:293/1770 train_time:26706ms step_avg:94.37ms
step:294/1770 train_time:26801ms step_avg:94.37ms
step:295/1770 train_time:26896ms step_avg:94.37ms
step:296/1770 train_time:26991ms step_avg:94.37ms
step:297/1770 train_time:27086ms step_avg:94.38ms
step:298/1770 train_time:27182ms step_avg:94.38ms
step:299/1770 train_time:27277ms step_avg:94.38ms
step:300/1770 train_time:27372ms step_avg:94.39ms
step:301/1770 train_time:27467ms step_avg:94.39ms
step:302/1770 train_time:27562ms step_avg:94.39ms
step:303/1770 train_time:27657ms step_avg:94.39ms
step:304/1770 train_time:27752ms step_avg:94.39ms
step:305/1770 train_time:27847ms step_avg:94.40ms
step:306/1770 train_time:27942ms step_avg:94.40ms
step:307/1770 train_time:28037ms step_avg:94.40ms
step:308/1770 train_time:28132ms step_avg:94.40ms
step:309/1770 train_time:28227ms step_avg:94.41ms
step:310/1770 train_time:28323ms step_avg:94.41ms
step:311/1770 train_time:28417ms step_avg:94.41ms
step:312/1770 train_time:28512ms step_avg:94.41ms
step:313/1770 train_time:28607ms step_avg:94.41ms
step:314/1770 train_time:28702ms step_avg:94.42ms
step:315/1770 train_time:28797ms step_avg:94.42ms
step:316/1770 train_time:28892ms step_avg:94.42ms
step:317/1770 train_time:28987ms step_avg:94.42ms
step:318/1770 train_time:29082ms step_avg:94.42ms
step:319/1770 train_time:29177ms step_avg:94.43ms
step:320/1770 train_time:29273ms step_avg:94.43ms
step:321/1770 train_time:29373ms step_avg:94.45ms
step:322/1770 train_time:29463ms step_avg:94.43ms
step:323/1770 train_time:29558ms step_avg:94.43ms
step:324/1770 train_time:29653ms step_avg:94.44ms
step:325/1770 train_time:29748ms step_avg:94.44ms
step:326/1770 train_time:29843ms step_avg:94.44ms
step:327/1770 train_time:29938ms step_avg:94.44ms
step:328/1770 train_time:30033ms step_avg:94.44ms
step:329/1770 train_time:30128ms step_avg:94.45ms
step:330/1770 train_time:30224ms step_avg:94.45ms
step:331/1770 train_time:30319ms step_avg:94.45ms
step:332/1770 train_time:30414ms step_avg:94.45ms
step:333/1770 train_time:30510ms step_avg:94.46ms
step:334/1770 train_time:30605ms step_avg:94.46ms
step:335/1770 train_time:30700ms step_avg:94.46ms
step:336/1770 train_time:30795ms step_avg:94.46ms
step:337/1770 train_time:30890ms step_avg:94.46ms
step:338/1770 train_time:30985ms step_avg:94.47ms
step:339/1770 train_time:31080ms step_avg:94.47ms
step:340/1770 train_time:31175ms step_avg:94.47ms
step:341/1770 train_time:31270ms step_avg:94.47ms
step:342/1770 train_time:31370ms step_avg:94.49ms
step:343/1770 train_time:31461ms step_avg:94.48ms
step:344/1770 train_time:31556ms step_avg:94.48ms
step:345/1770 train_time:31650ms step_avg:94.48ms
step:346/1770 train_time:31745ms step_avg:94.48ms
step:347/1770 train_time:31841ms step_avg:94.48ms
step:348/1770 train_time:31936ms step_avg:94.49ms
step:349/1770 train_time:32031ms step_avg:94.49ms
step:350/1770 train_time:32127ms step_avg:94.49ms
step:351/1770 train_time:32222ms step_avg:94.49ms
step:352/1770 train_time:32317ms step_avg:94.49ms
step:353/1770 train_time:32412ms step_avg:94.49ms
step:354/1770 train_time:32507ms step_avg:94.50ms
step:355/1770 train_time:32602ms step_avg:94.50ms
step:356/1770 train_time:32696ms step_avg:94.50ms
step:357/1770 train_time:32791ms step_avg:94.50ms
step:358/1770 train_time:32886ms step_avg:94.50ms
step:359/1770 train_time:32983ms step_avg:94.51ms
step:360/1770 train_time:33078ms step_avg:94.51ms
step:361/1770 train_time:33173ms step_avg:94.51ms
step:362/1770 train_time:33267ms step_avg:94.51ms
step:363/1770 train_time:33367ms step_avg:94.52ms
step:364/1770 train_time:33457ms step_avg:94.51ms
step:365/1770 train_time:33552ms step_avg:94.51ms
step:366/1770 train_time:33647ms step_avg:94.51ms
step:367/1770 train_time:33742ms step_avg:94.52ms
step:368/1770 train_time:33837ms step_avg:94.52ms
step:369/1770 train_time:33932ms step_avg:94.52ms
step:370/1770 train_time:34027ms step_avg:94.52ms
step:371/1770 train_time:34123ms step_avg:94.52ms
step:372/1770 train_time:34218ms step_avg:94.52ms
step:373/1770 train_time:34313ms step_avg:94.53ms
step:374/1770 train_time:34407ms step_avg:94.53ms
step:375/1770 train_time:34502ms step_avg:94.53ms
step:375/1770 val_loss:3.9043 train_time:34596ms step_avg:94.78ms
step:376/1770 train_time:34617ms step_avg:94.58ms
step:377/1770 train_time:34701ms step_avg:94.55ms
step:378/1770 train_time:34801ms step_avg:94.57ms
step:379/1770 train_time:34896ms step_avg:94.57ms
step:380/1770 train_time:34990ms step_avg:94.57ms
step:381/1770 train_time:35085ms step_avg:94.57ms
step:382/1770 train_time:35180ms step_avg:94.57ms
step:383/1770 train_time:35274ms step_avg:94.57ms
step:384/1770 train_time:35368ms step_avg:94.57ms
step:385/1770 train_time:35462ms step_avg:94.57ms
step:386/1770 train_time:35557ms step_avg:94.57ms
step:387/1770 train_time:35652ms step_avg:94.57ms
step:388/1770 train_time:35749ms step_avg:94.58ms
step:389/1770 train_time:35846ms step_avg:94.58ms
step:390/1770 train_time:35941ms step_avg:94.58ms
step:391/1770 train_time:36035ms step_avg:94.58ms
step:392/1770 train_time:36130ms step_avg:94.58ms
step:393/1770 train_time:36225ms step_avg:94.58ms
step:394/1770 train_time:36320ms step_avg:94.58ms
step:395/1770 train_time:36414ms step_avg:94.58ms
step:396/1770 train_time:36510ms step_avg:94.59ms
step:397/1770 train_time:36607ms step_avg:94.59ms
step:398/1770 train_time:36705ms step_avg:94.60ms
step:399/1770 train_time:36802ms step_avg:94.61ms
step:400/1770 train_time:36899ms step_avg:94.61ms
step:401/1770 train_time:36996ms step_avg:94.62ms
step:402/1770 train_time:37092ms step_avg:94.62ms
step:403/1770 train_time:37194ms step_avg:94.64ms
step:404/1770 train_time:37286ms step_avg:94.64ms
step:405/1770 train_time:37383ms step_avg:94.64ms
step:406/1770 train_time:37480ms step_avg:94.65ms
step:407/1770 train_time:37576ms step_avg:94.65ms
step:408/1770 train_time:37673ms step_avg:94.66ms
step:409/1770 train_time:37769ms step_avg:94.66ms
step:410/1770 train_time:37867ms step_avg:94.67ms
step:411/1770 train_time:37964ms step_avg:94.67ms
step:412/1770 train_time:38062ms step_avg:94.68ms
step:413/1770 train_time:38159ms step_avg:94.69ms
step:414/1770 train_time:38256ms step_avg:94.69ms
step:415/1770 train_time:38353ms step_avg:94.70ms
step:416/1770 train_time:38450ms step_avg:94.70ms
step:417/1770 train_time:38546ms step_avg:94.71ms
step:418/1770 train_time:38643ms step_avg:94.71ms
step:419/1770 train_time:38740ms step_avg:94.72ms
step:420/1770 train_time:38837ms step_avg:94.72ms
step:421/1770 train_time:38934ms step_avg:94.73ms
step:422/1770 train_time:39030ms step_avg:94.73ms
step:423/1770 train_time:39127ms step_avg:94.74ms
step:424/1770 train_time:39225ms step_avg:94.75ms
step:425/1770 train_time:39322ms step_avg:94.75ms
step:426/1770 train_time:39419ms step_avg:94.76ms
step:427/1770 train_time:39515ms step_avg:94.76ms
step:428/1770 train_time:39612ms step_avg:94.77ms
step:429/1770 train_time:39708ms step_avg:94.77ms
step:430/1770 train_time:39805ms step_avg:94.77ms
step:431/1770 train_time:39902ms step_avg:94.78ms
step:432/1770 train_time:39999ms step_avg:94.78ms
step:433/1770 train_time:40096ms step_avg:94.79ms
step:434/1770 train_time:40193ms step_avg:94.80ms
step:435/1770 train_time:40290ms step_avg:94.80ms
step:436/1770 train_time:40387ms step_avg:94.81ms
step:437/1770 train_time:40485ms step_avg:94.81ms
step:438/1770 train_time:40582ms step_avg:94.82ms
step:439/1770 train_time:40679ms step_avg:94.82ms
step:440/1770 train_time:40775ms step_avg:94.83ms
step:441/1770 train_time:40872ms step_avg:94.83ms
step:442/1770 train_time:40969ms step_avg:94.83ms
step:443/1770 train_time:41066ms step_avg:94.84ms
step:444/1770 train_time:41164ms step_avg:94.85ms
step:445/1770 train_time:41261ms step_avg:94.85ms
step:446/1770 train_time:41358ms step_avg:94.86ms
step:447/1770 train_time:41455ms step_avg:94.86ms
step:448/1770 train_time:41551ms step_avg:94.87ms
step:449/1770 train_time:41649ms step_avg:94.87ms
step:450/1770 train_time:41746ms step_avg:94.88ms
step:451/1770 train_time:41843ms step_avg:94.88ms
step:452/1770 train_time:41940ms step_avg:94.89ms
step:453/1770 train_time:42036ms step_avg:94.89ms
step:454/1770 train_time:42133ms step_avg:94.90ms
step:455/1770 train_time:42230ms step_avg:94.90ms
step:456/1770 train_time:42328ms step_avg:94.91ms
step:457/1770 train_time:42425ms step_avg:94.91ms
step:458/1770 train_time:42522ms step_avg:94.92ms
step:459/1770 train_time:42619ms step_avg:94.92ms
step:460/1770 train_time:42716ms step_avg:94.93ms
step:461/1770 train_time:42812ms step_avg:94.93ms
step:462/1770 train_time:42909ms step_avg:94.93ms
step:463/1770 train_time:43007ms step_avg:94.94ms
step:464/1770 train_time:43104ms step_avg:94.94ms
step:465/1770 train_time:43201ms step_avg:94.95ms
step:466/1770 train_time:43298ms step_avg:94.95ms
step:467/1770 train_time:43394ms step_avg:94.95ms
step:468/1770 train_time:43491ms step_avg:94.96ms
step:469/1770 train_time:43588ms step_avg:94.96ms
step:470/1770 train_time:43686ms step_avg:94.97ms
step:471/1770 train_time:43783ms step_avg:94.97ms
step:472/1770 train_time:43880ms step_avg:94.98ms
step:473/1770 train_time:43977ms step_avg:94.98ms
step:474/1770 train_time:44074ms step_avg:94.99ms
step:475/1770 train_time:44170ms step_avg:94.99ms
step:476/1770 train_time:44267ms step_avg:94.99ms
step:477/1770 train_time:44364ms step_avg:95.00ms
step:478/1770 train_time:44460ms step_avg:95.00ms
step:479/1770 train_time:44557ms step_avg:95.00ms
step:480/1770 train_time:44654ms step_avg:95.01ms
step:481/1770 train_time:44750ms step_avg:95.01ms
step:482/1770 train_time:44848ms step_avg:95.02ms
step:483/1770 train_time:44945ms step_avg:95.02ms
step:484/1770 train_time:45042ms step_avg:95.03ms
step:485/1770 train_time:45138ms step_avg:95.03ms
step:486/1770 train_time:45235ms step_avg:95.03ms
step:487/1770 train_time:45332ms step_avg:95.04ms
step:488/1770 train_time:45429ms step_avg:95.04ms
step:489/1770 train_time:45527ms step_avg:95.05ms
step:490/1770 train_time:45625ms step_avg:95.05ms
step:491/1770 train_time:45722ms step_avg:95.06ms
step:492/1770 train_time:45818ms step_avg:95.06ms
step:493/1770 train_time:45915ms step_avg:95.06ms
step:494/1770 train_time:46012ms step_avg:95.07ms
step:495/1770 train_time:46110ms step_avg:95.07ms
step:496/1770 train_time:46207ms step_avg:95.08ms
step:497/1770 train_time:46304ms step_avg:95.08ms
step:498/1770 train_time:46401ms step_avg:95.08ms
step:499/1770 train_time:46498ms step_avg:95.09ms
step:500/1770 train_time:46594ms step_avg:95.09ms
step:500/1770 val_loss:3.7519 train_time:46689ms step_avg:95.28ms
step:501/1770 train_time:46710ms step_avg:95.13ms
step:502/1770 train_time:46798ms step_avg:95.12ms
step:503/1770 train_time:46897ms step_avg:95.12ms
step:504/1770 train_time:46994ms step_avg:95.13ms
step:505/1770 train_time:47090ms step_avg:95.13ms
step:506/1770 train_time:47186ms step_avg:95.13ms
step:507/1770 train_time:47283ms step_avg:95.14ms
step:508/1770 train_time:47379ms step_avg:95.14ms
step:509/1770 train_time:47476ms step_avg:95.14ms
step:510/1770 train_time:47572ms step_avg:95.14ms
step:511/1770 train_time:47668ms step_avg:95.15ms
step:512/1770 train_time:47766ms step_avg:95.15ms
step:513/1770 train_time:47863ms step_avg:95.15ms
step:514/1770 train_time:47961ms step_avg:95.16ms
step:515/1770 train_time:48059ms step_avg:95.17ms
step:516/1770 train_time:48155ms step_avg:95.17ms
step:517/1770 train_time:48252ms step_avg:95.17ms
step:518/1770 train_time:48348ms step_avg:95.17ms
step:519/1770 train_time:48444ms step_avg:95.18ms
step:520/1770 train_time:48541ms step_avg:95.18ms
step:521/1770 train_time:48638ms step_avg:95.18ms
step:522/1770 train_time:48735ms step_avg:95.19ms
step:523/1770 train_time:48832ms step_avg:95.19ms
step:524/1770 train_time:48928ms step_avg:95.19ms
step:525/1770 train_time:49025ms step_avg:95.19ms
step:526/1770 train_time:49122ms step_avg:95.20ms
step:527/1770 train_time:49219ms step_avg:95.20ms
step:528/1770 train_time:49316ms step_avg:95.20ms
step:529/1770 train_time:49413ms step_avg:95.21ms
step:530/1770 train_time:49509ms step_avg:95.21ms
step:531/1770 train_time:49610ms step_avg:95.22ms
step:532/1770 train_time:49704ms step_avg:95.22ms
step:533/1770 train_time:49801ms step_avg:95.22ms
step:534/1770 train_time:49898ms step_avg:95.23ms
step:535/1770 train_time:49996ms step_avg:95.23ms
step:536/1770 train_time:50093ms step_avg:95.23ms
step:537/1770 train_time:50192ms step_avg:95.24ms
step:538/1770 train_time:50289ms step_avg:95.24ms
step:539/1770 train_time:50386ms step_avg:95.25ms
step:540/1770 train_time:50483ms step_avg:95.25ms
step:541/1770 train_time:50580ms step_avg:95.25ms
step:542/1770 train_time:50678ms step_avg:95.26ms
step:543/1770 train_time:50775ms step_avg:95.26ms
step:544/1770 train_time:50871ms step_avg:95.26ms
step:545/1770 train_time:50969ms step_avg:95.27ms
step:546/1770 train_time:51066ms step_avg:95.27ms
step:547/1770 train_time:51163ms step_avg:95.28ms
step:548/1770 train_time:51261ms step_avg:95.28ms
step:549/1770 train_time:51358ms step_avg:95.28ms
step:550/1770 train_time:51456ms step_avg:95.29ms
step:551/1770 train_time:51552ms step_avg:95.29ms
step:552/1770 train_time:51650ms step_avg:95.29ms
step:553/1770 train_time:51747ms step_avg:95.30ms
step:554/1770 train_time:51844ms step_avg:95.30ms
step:555/1770 train_time:51941ms step_avg:95.30ms
step:556/1770 train_time:52038ms step_avg:95.31ms
step:557/1770 train_time:52135ms step_avg:95.31ms
step:558/1770 train_time:52233ms step_avg:95.32ms
step:559/1770 train_time:52330ms step_avg:95.32ms
step:560/1770 train_time:52427ms step_avg:95.32ms
step:561/1770 train_time:52524ms step_avg:95.32ms
step:562/1770 train_time:52621ms step_avg:95.33ms
step:563/1770 train_time:52718ms step_avg:95.33ms
step:564/1770 train_time:52815ms step_avg:95.33ms
step:565/1770 train_time:52913ms step_avg:95.34ms
step:566/1770 train_time:53010ms step_avg:95.34ms
step:567/1770 train_time:53107ms step_avg:95.34ms
step:568/1770 train_time:53204ms step_avg:95.35ms
step:569/1770 train_time:53301ms step_avg:95.35ms
step:570/1770 train_time:53399ms step_avg:95.36ms
step:571/1770 train_time:53497ms step_avg:95.36ms
step:572/1770 train_time:53594ms step_avg:95.36ms
step:573/1770 train_time:53691ms step_avg:95.37ms
step:574/1770 train_time:53788ms step_avg:95.37ms
step:575/1770 train_time:53885ms step_avg:95.37ms
step:576/1770 train_time:53982ms step_avg:95.37ms
step:577/1770 train_time:54079ms step_avg:95.38ms
step:578/1770 train_time:54177ms step_avg:95.38ms
step:579/1770 train_time:54274ms step_avg:95.38ms
step:580/1770 train_time:54371ms step_avg:95.39ms
step:581/1770 train_time:54468ms step_avg:95.39ms
step:582/1770 train_time:54565ms step_avg:95.39ms
step:583/1770 train_time:54663ms step_avg:95.40ms
step:584/1770 train_time:54760ms step_avg:95.40ms
step:585/1770 train_time:54857ms step_avg:95.40ms
step:586/1770 train_time:54955ms step_avg:95.41ms
step:587/1770 train_time:55052ms step_avg:95.41ms
step:588/1770 train_time:55148ms step_avg:95.41ms
step:589/1770 train_time:55246ms step_avg:95.42ms
step:590/1770 train_time:55342ms step_avg:95.42ms
step:591/1770 train_time:55440ms step_avg:95.42ms
step:592/1770 train_time:55538ms step_avg:95.43ms
step:593/1770 train_time:55635ms step_avg:95.43ms
step:594/1770 train_time:55732ms step_avg:95.43ms
step:595/1770 train_time:55830ms step_avg:95.44ms
step:596/1770 train_time:55927ms step_avg:95.44ms
step:597/1770 train_time:56024ms step_avg:95.44ms
step:598/1770 train_time:56122ms step_avg:95.44ms
step:599/1770 train_time:56219ms step_avg:95.45ms
step:600/1770 train_time:56317ms step_avg:95.45ms
step:601/1770 train_time:56413ms step_avg:95.45ms
step:602/1770 train_time:56510ms step_avg:95.46ms
step:603/1770 train_time:56607ms step_avg:95.46ms
step:604/1770 train_time:56704ms step_avg:95.46ms
step:605/1770 train_time:56802ms step_avg:95.46ms
step:606/1770 train_time:56899ms step_avg:95.47ms
step:607/1770 train_time:56996ms step_avg:95.47ms
step:608/1770 train_time:57093ms step_avg:95.47ms
step:609/1770 train_time:57191ms step_avg:95.48ms
step:610/1770 train_time:57288ms step_avg:95.48ms
step:611/1770 train_time:57386ms step_avg:95.48ms
step:612/1770 train_time:57483ms step_avg:95.49ms
step:613/1770 train_time:57581ms step_avg:95.49ms
step:614/1770 train_time:57678ms step_avg:95.49ms
step:615/1770 train_time:57775ms step_avg:95.50ms
step:616/1770 train_time:57872ms step_avg:95.50ms
step:617/1770 train_time:57969ms step_avg:95.50ms
step:618/1770 train_time:58066ms step_avg:95.50ms
step:619/1770 train_time:58163ms step_avg:95.51ms
step:620/1770 train_time:58260ms step_avg:95.51ms
step:621/1770 train_time:58358ms step_avg:95.51ms
step:622/1770 train_time:58455ms step_avg:95.51ms
step:623/1770 train_time:58552ms step_avg:95.52ms
step:624/1770 train_time:58649ms step_avg:95.52ms
step:625/1770 train_time:58746ms step_avg:95.52ms
step:625/1770 val_loss:3.6652 train_time:58842ms step_avg:95.68ms
step:626/1770 train_time:58863ms step_avg:95.56ms
step:627/1770 train_time:58953ms step_avg:95.55ms
step:628/1770 train_time:59052ms step_avg:95.55ms
step:629/1770 train_time:59150ms step_avg:95.56ms
step:630/1770 train_time:59247ms step_avg:95.56ms
step:631/1770 train_time:59343ms step_avg:95.56ms
step:632/1770 train_time:59440ms step_avg:95.56ms
step:633/1770 train_time:59537ms step_avg:95.56ms
step:634/1770 train_time:59634ms step_avg:95.57ms
step:635/1770 train_time:59730ms step_avg:95.57ms
step:636/1770 train_time:59827ms step_avg:95.57ms
step:637/1770 train_time:59925ms step_avg:95.57ms
step:638/1770 train_time:60024ms step_avg:95.58ms
step:639/1770 train_time:60125ms step_avg:95.59ms
step:640/1770 train_time:60218ms step_avg:95.58ms
step:641/1770 train_time:60315ms step_avg:95.59ms
step:642/1770 train_time:60412ms step_avg:95.59ms
step:643/1770 train_time:60509ms step_avg:95.59ms
step:644/1770 train_time:60606ms step_avg:95.59ms
step:645/1770 train_time:60703ms step_avg:95.60ms
step:646/1770 train_time:60800ms step_avg:95.60ms
step:647/1770 train_time:60898ms step_avg:95.60ms
step:648/1770 train_time:60995ms step_avg:95.60ms
step:649/1770 train_time:61093ms step_avg:95.61ms
step:650/1770 train_time:61190ms step_avg:95.61ms
step:651/1770 train_time:61288ms step_avg:95.61ms
step:652/1770 train_time:61385ms step_avg:95.62ms
step:653/1770 train_time:61482ms step_avg:95.62ms
step:654/1770 train_time:61579ms step_avg:95.62ms
step:655/1770 train_time:61676ms step_avg:95.62ms
step:656/1770 train_time:61774ms step_avg:95.63ms
step:657/1770 train_time:61871ms step_avg:95.63ms
step:658/1770 train_time:61970ms step_avg:95.63ms
step:659/1770 train_time:62068ms step_avg:95.64ms
step:660/1770 train_time:62167ms step_avg:95.64ms
step:661/1770 train_time:62266ms step_avg:95.65ms
step:662/1770 train_time:62365ms step_avg:95.65ms
step:663/1770 train_time:62464ms step_avg:95.66ms
step:664/1770 train_time:62563ms step_avg:95.66ms
step:665/1770 train_time:62661ms step_avg:95.67ms
step:666/1770 train_time:62761ms step_avg:95.67ms
step:667/1770 train_time:62859ms step_avg:95.68ms
step:668/1770 train_time:62958ms step_avg:95.68ms
step:669/1770 train_time:63057ms step_avg:95.69ms
step:670/1770 train_time:63156ms step_avg:95.69ms
step:671/1770 train_time:63254ms step_avg:95.69ms
step:672/1770 train_time:63353ms step_avg:95.70ms
step:673/1770 train_time:63453ms step_avg:95.71ms
step:674/1770 train_time:63552ms step_avg:95.71ms
step:675/1770 train_time:63652ms step_avg:95.72ms
step:676/1770 train_time:63752ms step_avg:95.72ms
step:677/1770 train_time:63852ms step_avg:95.73ms
step:678/1770 train_time:63952ms step_avg:95.74ms
step:679/1770 train_time:64051ms step_avg:95.74ms
step:680/1770 train_time:64150ms step_avg:95.75ms
step:681/1770 train_time:64249ms step_avg:95.75ms
step:682/1770 train_time:64348ms step_avg:95.76ms
step:683/1770 train_time:64446ms step_avg:95.76ms
step:684/1770 train_time:64544ms step_avg:95.76ms
step:685/1770 train_time:64643ms step_avg:95.77ms
step:686/1770 train_time:64741ms step_avg:95.77ms
step:687/1770 train_time:64840ms step_avg:95.77ms
step:688/1770 train_time:64939ms step_avg:95.78ms
step:689/1770 train_time:65038ms step_avg:95.78ms
step:690/1770 train_time:65136ms step_avg:95.79ms
step:691/1770 train_time:65234ms step_avg:95.79ms
step:692/1770 train_time:65336ms step_avg:95.80ms
step:693/1770 train_time:65432ms step_avg:95.80ms
step:694/1770 train_time:65532ms step_avg:95.81ms
step:695/1770 train_time:65632ms step_avg:95.81ms
step:696/1770 train_time:65731ms step_avg:95.82ms
step:697/1770 train_time:65831ms step_avg:95.82ms
step:698/1770 train_time:65930ms step_avg:95.83ms
step:699/1770 train_time:66030ms step_avg:95.83ms
step:700/1770 train_time:66129ms step_avg:95.84ms
step:701/1770 train_time:66227ms step_avg:95.84ms
step:702/1770 train_time:66326ms step_avg:95.85ms
step:703/1770 train_time:66425ms step_avg:95.85ms
step:704/1770 train_time:66524ms step_avg:95.86ms
step:705/1770 train_time:66623ms step_avg:95.86ms
step:706/1770 train_time:66723ms step_avg:95.87ms
step:707/1770 train_time:66821ms step_avg:95.87ms
step:708/1770 train_time:66921ms step_avg:95.87ms
step:709/1770 train_time:67021ms step_avg:95.88ms
step:710/1770 train_time:67121ms step_avg:95.89ms
step:711/1770 train_time:67222ms step_avg:95.89ms
step:712/1770 train_time:67318ms step_avg:95.90ms
step:713/1770 train_time:67417ms step_avg:95.90ms
step:714/1770 train_time:67515ms step_avg:95.90ms
step:715/1770 train_time:67614ms step_avg:95.91ms
step:716/1770 train_time:67713ms step_avg:95.91ms
step:717/1770 train_time:67812ms step_avg:95.91ms
step:718/1770 train_time:67911ms step_avg:95.92ms
step:719/1770 train_time:68010ms step_avg:95.92ms
step:720/1770 train_time:68110ms step_avg:95.93ms
step:721/1770 train_time:68210ms step_avg:95.94ms
step:722/1770 train_time:68309ms step_avg:95.94ms
step:723/1770 train_time:68408ms step_avg:95.94ms
step:724/1770 train_time:68506ms step_avg:95.95ms
step:725/1770 train_time:68605ms step_avg:95.95ms
step:726/1770 train_time:68704ms step_avg:95.95ms
step:727/1770 train_time:68802ms step_avg:95.96ms
step:728/1770 train_time:68901ms step_avg:95.96ms
step:729/1770 train_time:68999ms step_avg:95.97ms
step:730/1770 train_time:69098ms step_avg:95.97ms
step:731/1770 train_time:69198ms step_avg:95.97ms
step:732/1770 train_time:69297ms step_avg:95.98ms
step:733/1770 train_time:69395ms step_avg:95.98ms
step:734/1770 train_time:69494ms step_avg:95.99ms
step:735/1770 train_time:69594ms step_avg:95.99ms
step:736/1770 train_time:69693ms step_avg:96.00ms
step:737/1770 train_time:69792ms step_avg:96.00ms
step:738/1770 train_time:69892ms step_avg:96.01ms
step:739/1770 train_time:69992ms step_avg:96.01ms
step:740/1770 train_time:70092ms step_avg:96.02ms
step:741/1770 train_time:70192ms step_avg:96.02ms
step:742/1770 train_time:70292ms step_avg:96.03ms
step:743/1770 train_time:70390ms step_avg:96.03ms
step:744/1770 train_time:70489ms step_avg:96.03ms
step:745/1770 train_time:70588ms step_avg:96.04ms
step:746/1770 train_time:70686ms step_avg:96.04ms
step:747/1770 train_time:70785ms step_avg:96.04ms
step:748/1770 train_time:70884ms step_avg:96.05ms
step:749/1770 train_time:70983ms step_avg:96.05ms
step:750/1770 train_time:71083ms step_avg:96.06ms
step:750/1770 val_loss:3.6003 train_time:71180ms step_avg:96.19ms
step:751/1770 train_time:71201ms step_avg:96.09ms
step:752/1770 train_time:71291ms step_avg:96.08ms
step:753/1770 train_time:71391ms step_avg:96.09ms
step:754/1770 train_time:71490ms step_avg:96.09ms
step:755/1770 train_time:71588ms step_avg:96.09ms
step:756/1770 train_time:71687ms step_avg:96.09ms
step:757/1770 train_time:71784ms step_avg:96.10ms
step:758/1770 train_time:71882ms step_avg:96.10ms
step:759/1770 train_time:71981ms step_avg:96.10ms
step:760/1770 train_time:72079ms step_avg:96.10ms
step:761/1770 train_time:72177ms step_avg:96.11ms
step:762/1770 train_time:72277ms step_avg:96.11ms
step:763/1770 train_time:72376ms step_avg:96.12ms
step:764/1770 train_time:72475ms step_avg:96.12ms
step:765/1770 train_time:72574ms step_avg:96.12ms
step:766/1770 train_time:72673ms step_avg:96.13ms
step:767/1770 train_time:72773ms step_avg:96.13ms
step:768/1770 train_time:72872ms step_avg:96.14ms
step:769/1770 train_time:72972ms step_avg:96.14ms
step:770/1770 train_time:73072ms step_avg:96.15ms
step:771/1770 train_time:73171ms step_avg:96.15ms
step:772/1770 train_time:73270ms step_avg:96.15ms
step:773/1770 train_time:73368ms step_avg:96.16ms
step:774/1770 train_time:73467ms step_avg:96.16ms
step:775/1770 train_time:73566ms step_avg:96.16ms
step:776/1770 train_time:73665ms step_avg:96.17ms
step:777/1770 train_time:73764ms step_avg:96.17ms
step:778/1770 train_time:73863ms step_avg:96.18ms
step:779/1770 train_time:73962ms step_avg:96.18ms
step:780/1770 train_time:74061ms step_avg:96.18ms
step:781/1770 train_time:74161ms step_avg:96.19ms
step:782/1770 train_time:74260ms step_avg:96.19ms
step:783/1770 train_time:74360ms step_avg:96.20ms
step:784/1770 train_time:74459ms step_avg:96.20ms
step:785/1770 train_time:74558ms step_avg:96.20ms
step:786/1770 train_time:74656ms step_avg:96.21ms
step:787/1770 train_time:74755ms step_avg:96.21ms
step:788/1770 train_time:74854ms step_avg:96.21ms
step:789/1770 train_time:74954ms step_avg:96.22ms
step:790/1770 train_time:75054ms step_avg:96.22ms
step:791/1770 train_time:75155ms step_avg:96.23ms
step:792/1770 train_time:75255ms step_avg:96.23ms
step:793/1770 train_time:75355ms step_avg:96.24ms
step:794/1770 train_time:75455ms step_avg:96.24ms
step:795/1770 train_time:75554ms step_avg:96.25ms
step:796/1770 train_time:75654ms step_avg:96.25ms
step:797/1770 train_time:75753ms step_avg:96.26ms
step:798/1770 train_time:75852ms step_avg:96.26ms
step:799/1770 train_time:75952ms step_avg:96.26ms
step:800/1770 train_time:76052ms step_avg:96.27ms
step:801/1770 train_time:76150ms step_avg:96.27ms
step:802/1770 train_time:76250ms step_avg:96.28ms
step:803/1770 train_time:76350ms step_avg:96.28ms
step:804/1770 train_time:76449ms step_avg:96.28ms
step:805/1770 train_time:76548ms step_avg:96.29ms
step:806/1770 train_time:76647ms step_avg:96.29ms
step:807/1770 train_time:76746ms step_avg:96.29ms
step:808/1770 train_time:76846ms step_avg:96.30ms
step:809/1770 train_time:76945ms step_avg:96.30ms
step:810/1770 train_time:77045ms step_avg:96.31ms
step:811/1770 train_time:77144ms step_avg:96.31ms
step:812/1770 train_time:77243ms step_avg:96.31ms
step:813/1770 train_time:77343ms step_avg:96.32ms
step:814/1770 train_time:77444ms step_avg:96.32ms
step:815/1770 train_time:77542ms step_avg:96.33ms
step:816/1770 train_time:77641ms step_avg:96.33ms
step:817/1770 train_time:77740ms step_avg:96.33ms
step:818/1770 train_time:77840ms step_avg:96.34ms
step:819/1770 train_time:77939ms step_avg:96.34ms
step:820/1770 train_time:78038ms step_avg:96.34ms
step:821/1770 train_time:78137ms step_avg:96.35ms
step:822/1770 train_time:78235ms step_avg:96.35ms
step:823/1770 train_time:78335ms step_avg:96.35ms
step:824/1770 train_time:78434ms step_avg:96.36ms
step:825/1770 train_time:78534ms step_avg:96.36ms
step:826/1770 train_time:78633ms step_avg:96.36ms
step:827/1770 train_time:78733ms step_avg:96.37ms
step:828/1770 train_time:78833ms step_avg:96.37ms
step:829/1770 train_time:78933ms step_avg:96.38ms
step:830/1770 train_time:79032ms step_avg:96.38ms
step:831/1770 train_time:79135ms step_avg:96.39ms
step:832/1770 train_time:79233ms step_avg:96.39ms
step:833/1770 train_time:79332ms step_avg:96.39ms
step:834/1770 train_time:79431ms step_avg:96.40ms
step:835/1770 train_time:79530ms step_avg:96.40ms
step:836/1770 train_time:79629ms step_avg:96.40ms
step:837/1770 train_time:79729ms step_avg:96.41ms
step:838/1770 train_time:79828ms step_avg:96.41ms
step:839/1770 train_time:79928ms step_avg:96.41ms
step:840/1770 train_time:80026ms step_avg:96.42ms
step:841/1770 train_time:80126ms step_avg:96.42ms
step:842/1770 train_time:80224ms step_avg:96.42ms
step:843/1770 train_time:80324ms step_avg:96.43ms
step:844/1770 train_time:80423ms step_avg:96.43ms
step:845/1770 train_time:80522ms step_avg:96.43ms
step:846/1770 train_time:80622ms step_avg:96.44ms
step:847/1770 train_time:80721ms step_avg:96.44ms
step:848/1770 train_time:80819ms step_avg:96.44ms
step:849/1770 train_time:80919ms step_avg:96.45ms
step:850/1770 train_time:81018ms step_avg:96.45ms
step:851/1770 train_time:81117ms step_avg:96.45ms
step:852/1770 train_time:81216ms step_avg:96.46ms
step:853/1770 train_time:81315ms step_avg:96.46ms
step:854/1770 train_time:81414ms step_avg:96.46ms
step:855/1770 train_time:81514ms step_avg:96.47ms
step:856/1770 train_time:81614ms step_avg:96.47ms
step:857/1770 train_time:81714ms step_avg:96.47ms
step:858/1770 train_time:81813ms step_avg:96.48ms
step:859/1770 train_time:81913ms step_avg:96.48ms
step:860/1770 train_time:82013ms step_avg:96.49ms
step:861/1770 train_time:82113ms step_avg:96.49ms
step:862/1770 train_time:82213ms step_avg:96.49ms
step:863/1770 train_time:82313ms step_avg:96.50ms
step:864/1770 train_time:82412ms step_avg:96.50ms
step:865/1770 train_time:82511ms step_avg:96.50ms
step:866/1770 train_time:82611ms step_avg:96.51ms
step:867/1770 train_time:82710ms step_avg:96.51ms
step:868/1770 train_time:82809ms step_avg:96.51ms
step:869/1770 train_time:82908ms step_avg:96.52ms
step:870/1770 train_time:83008ms step_avg:96.52ms
step:871/1770 train_time:83107ms step_avg:96.52ms
step:872/1770 train_time:83206ms step_avg:96.53ms
step:873/1770 train_time:83305ms step_avg:96.53ms
step:874/1770 train_time:83404ms step_avg:96.53ms
step:875/1770 train_time:83504ms step_avg:96.54ms
step:875/1770 val_loss:3.5519 train_time:83602ms step_avg:96.65ms
step:876/1770 train_time:83623ms step_avg:96.56ms
step:877/1770 train_time:83715ms step_avg:96.56ms
step:878/1770 train_time:83815ms step_avg:96.56ms
step:879/1770 train_time:83915ms step_avg:96.57ms
step:880/1770 train_time:84014ms step_avg:96.57ms
step:881/1770 train_time:84113ms step_avg:96.57ms
step:882/1770 train_time:84211ms step_avg:96.57ms
step:883/1770 train_time:84310ms step_avg:96.58ms
step:884/1770 train_time:84408ms step_avg:96.58ms
step:885/1770 train_time:84507ms step_avg:96.58ms
step:886/1770 train_time:84606ms step_avg:96.58ms
step:887/1770 train_time:84707ms step_avg:96.59ms
step:888/1770 train_time:84807ms step_avg:96.59ms
step:889/1770 train_time:84907ms step_avg:96.60ms
step:890/1770 train_time:85006ms step_avg:96.60ms
step:891/1770 train_time:85105ms step_avg:96.60ms
step:892/1770 train_time:85204ms step_avg:96.60ms
step:893/1770 train_time:85303ms step_avg:96.61ms
step:894/1770 train_time:85402ms step_avg:96.61ms
step:895/1770 train_time:85501ms step_avg:96.61ms
step:896/1770 train_time:85601ms step_avg:96.62ms
step:897/1770 train_time:85701ms step_avg:96.62ms
step:898/1770 train_time:85802ms step_avg:96.62ms
step:899/1770 train_time:85902ms step_avg:96.63ms
step:900/1770 train_time:86002ms step_avg:96.63ms
step:901/1770 train_time:86101ms step_avg:96.63ms
step:902/1770 train_time:86201ms step_avg:96.64ms
step:903/1770 train_time:86300ms step_avg:96.64ms
step:904/1770 train_time:86399ms step_avg:96.64ms
step:905/1770 train_time:86498ms step_avg:96.65ms
step:906/1770 train_time:86597ms step_avg:96.65ms
step:907/1770 train_time:86696ms step_avg:96.65ms
step:908/1770 train_time:86795ms step_avg:96.65ms
step:909/1770 train_time:86894ms step_avg:96.66ms
step:910/1770 train_time:86994ms step_avg:96.66ms
step:911/1770 train_time:87093ms step_avg:96.66ms
step:912/1770 train_time:87192ms step_avg:96.67ms
step:913/1770 train_time:87292ms step_avg:96.67ms
step:914/1770 train_time:87392ms step_avg:96.67ms
step:915/1770 train_time:87492ms step_avg:96.68ms
step:916/1770 train_time:87592ms step_avg:96.68ms
step:917/1770 train_time:87691ms step_avg:96.68ms
step:918/1770 train_time:87790ms step_avg:96.69ms
step:919/1770 train_time:87890ms step_avg:96.69ms
step:920/1770 train_time:87992ms step_avg:96.69ms
step:921/1770 train_time:88094ms step_avg:96.70ms
step:922/1770 train_time:88195ms step_avg:96.70ms
step:923/1770 train_time:88295ms step_avg:96.71ms
step:924/1770 train_time:88394ms step_avg:96.71ms
step:925/1770 train_time:88495ms step_avg:96.72ms
step:926/1770 train_time:88595ms step_avg:96.72ms
step:927/1770 train_time:88695ms step_avg:96.72ms
step:928/1770 train_time:88795ms step_avg:96.73ms
step:929/1770 train_time:88895ms step_avg:96.73ms
step:930/1770 train_time:88996ms step_avg:96.74ms
step:931/1770 train_time:89098ms step_avg:96.74ms
step:932/1770 train_time:89198ms step_avg:96.74ms
step:933/1770 train_time:89298ms step_avg:96.75ms
step:934/1770 train_time:89399ms step_avg:96.75ms
step:935/1770 train_time:89499ms step_avg:96.76ms
step:936/1770 train_time:89600ms step_avg:96.76ms
step:937/1770 train_time:89700ms step_avg:96.76ms
step:938/1770 train_time:89801ms step_avg:96.77ms
step:939/1770 train_time:89902ms step_avg:96.77ms
step:940/1770 train_time:90003ms step_avg:96.78ms
step:941/1770 train_time:90104ms step_avg:96.78ms
step:942/1770 train_time:90205ms step_avg:96.79ms
step:943/1770 train_time:90307ms step_avg:96.79ms
step:944/1770 train_time:90407ms step_avg:96.80ms
step:945/1770 train_time:90507ms step_avg:96.80ms
step:946/1770 train_time:90608ms step_avg:96.80ms
step:947/1770 train_time:90709ms step_avg:96.81ms
step:948/1770 train_time:90810ms step_avg:96.81ms
step:949/1770 train_time:90913ms step_avg:96.82ms
step:950/1770 train_time:91015ms step_avg:96.82ms
step:951/1770 train_time:91117ms step_avg:96.83ms
step:952/1770 train_time:91217ms step_avg:96.83ms
step:953/1770 train_time:91318ms step_avg:96.84ms
step:954/1770 train_time:91417ms step_avg:96.84ms
step:955/1770 train_time:91521ms step_avg:96.85ms
step:956/1770 train_time:91619ms step_avg:96.85ms
step:957/1770 train_time:91720ms step_avg:96.85ms
step:958/1770 train_time:91820ms step_avg:96.86ms
step:959/1770 train_time:91921ms step_avg:96.86ms
step:960/1770 train_time:92021ms step_avg:96.86ms
step:961/1770 train_time:92122ms step_avg:96.87ms
step:962/1770 train_time:92225ms step_avg:96.87ms
step:963/1770 train_time:92325ms step_avg:96.88ms
step:964/1770 train_time:92426ms step_avg:96.88ms
step:965/1770 train_time:92526ms step_avg:96.89ms
step:966/1770 train_time:92626ms step_avg:96.89ms
step:967/1770 train_time:92727ms step_avg:96.89ms
step:968/1770 train_time:92828ms step_avg:96.90ms
step:969/1770 train_time:92929ms step_avg:96.90ms
step:970/1770 train_time:93030ms step_avg:96.91ms
step:971/1770 train_time:93132ms step_avg:96.91ms
step:972/1770 train_time:93236ms step_avg:96.92ms
step:973/1770 train_time:93334ms step_avg:96.92ms
step:974/1770 train_time:93434ms step_avg:96.92ms
step:975/1770 train_time:93535ms step_avg:96.93ms
step:976/1770 train_time:93635ms step_avg:96.93ms
step:977/1770 train_time:93736ms step_avg:96.93ms
step:978/1770 train_time:93836ms step_avg:96.94ms
step:979/1770 train_time:93936ms step_avg:96.94ms
step:980/1770 train_time:94037ms step_avg:96.95ms
step:981/1770 train_time:94138ms step_avg:96.95ms
step:982/1770 train_time:94239ms step_avg:96.95ms
step:983/1770 train_time:94339ms step_avg:96.96ms
step:984/1770 train_time:94440ms step_avg:96.96ms
step:985/1770 train_time:94540ms step_avg:96.96ms
step:986/1770 train_time:94640ms step_avg:96.97ms
step:987/1770 train_time:94742ms step_avg:96.97ms
step:988/1770 train_time:94842ms step_avg:96.98ms
step:989/1770 train_time:94944ms step_avg:96.98ms
step:990/1770 train_time:95044ms step_avg:96.98ms
step:991/1770 train_time:95145ms step_avg:96.99ms
step:992/1770 train_time:95246ms step_avg:96.99ms
step:993/1770 train_time:95347ms step_avg:97.00ms
step:994/1770 train_time:95447ms step_avg:97.00ms
step:995/1770 train_time:95548ms step_avg:97.00ms
step:996/1770 train_time:95649ms step_avg:97.01ms
step:997/1770 train_time:95750ms step_avg:97.01ms
step:998/1770 train_time:95851ms step_avg:97.02ms
step:999/1770 train_time:95952ms step_avg:97.02ms
step:1000/1770 train_time:96054ms step_avg:97.02ms
step:1000/1770 val_loss:3.5142 train_time:96153ms step_avg:97.12ms
step:1001/1770 train_time:96174ms step_avg:97.05ms
step:1002/1770 train_time:96265ms step_avg:97.04ms
step:1003/1770 train_time:96368ms step_avg:97.05ms
step:1004/1770 train_time:96469ms step_avg:97.05ms
step:1005/1770 train_time:96569ms step_avg:97.05ms
step:1006/1770 train_time:96669ms step_avg:97.06ms
step:1007/1770 train_time:96769ms step_avg:97.06ms
step:1008/1770 train_time:96869ms step_avg:97.06ms
step:1009/1770 train_time:96968ms step_avg:97.07ms
step:1010/1770 train_time:97069ms step_avg:97.07ms
step:1011/1770 train_time:97171ms step_avg:97.07ms
step:1012/1770 train_time:97273ms step_avg:97.08ms
step:1013/1770 train_time:97374ms step_avg:97.08ms
step:1014/1770 train_time:97474ms step_avg:97.09ms
step:1015/1770 train_time:97575ms step_avg:97.09ms
step:1016/1770 train_time:97676ms step_avg:97.09ms
step:1017/1770 train_time:97777ms step_avg:97.10ms
step:1018/1770 train_time:97877ms step_avg:97.10ms
step:1019/1770 train_time:97977ms step_avg:97.10ms
step:1020/1770 train_time:98078ms step_avg:97.11ms
step:1021/1770 train_time:98180ms step_avg:97.11ms
step:1022/1770 train_time:98280ms step_avg:97.11ms
step:1023/1770 train_time:98381ms step_avg:97.12ms
step:1024/1770 train_time:98482ms step_avg:97.12ms
step:1025/1770 train_time:98583ms step_avg:97.13ms
step:1026/1770 train_time:98683ms step_avg:97.13ms
step:1027/1770 train_time:98784ms step_avg:97.13ms
step:1028/1770 train_time:98885ms step_avg:97.14ms
step:1029/1770 train_time:98986ms step_avg:97.14ms
step:1030/1770 train_time:99087ms step_avg:97.14ms
step:1031/1770 train_time:99188ms step_avg:97.15ms
step:1032/1770 train_time:99289ms step_avg:97.15ms
step:1033/1770 train_time:99389ms step_avg:97.15ms
step:1034/1770 train_time:99491ms step_avg:97.16ms
step:1035/1770 train_time:99592ms step_avg:97.16ms
step:1036/1770 train_time:99692ms step_avg:97.17ms
step:1037/1770 train_time:99792ms step_avg:97.17ms
step:1038/1770 train_time:99892ms step_avg:97.17ms
step:1039/1770 train_time:99994ms step_avg:97.18ms
step:1040/1770 train_time:100094ms step_avg:97.18ms
step:1041/1770 train_time:100195ms step_avg:97.18ms
step:1042/1770 train_time:100297ms step_avg:97.19ms
step:1043/1770 train_time:100398ms step_avg:97.19ms
step:1044/1770 train_time:100500ms step_avg:97.20ms
step:1045/1770 train_time:100600ms step_avg:97.20ms
step:1046/1770 train_time:100700ms step_avg:97.20ms
step:1047/1770 train_time:100801ms step_avg:97.20ms
step:1048/1770 train_time:100901ms step_avg:97.21ms
step:1049/1770 train_time:101000ms step_avg:97.21ms
step:1050/1770 train_time:101101ms step_avg:97.21ms
step:1051/1770 train_time:101202ms step_avg:97.22ms
step:1052/1770 train_time:101302ms step_avg:97.22ms
step:1053/1770 train_time:101403ms step_avg:97.22ms
step:1054/1770 train_time:101504ms step_avg:97.23ms
step:1055/1770 train_time:101605ms step_avg:97.23ms
step:1056/1770 train_time:101705ms step_avg:97.23ms
step:1057/1770 train_time:101806ms step_avg:97.24ms
step:1058/1770 train_time:101907ms step_avg:97.24ms
step:1059/1770 train_time:102007ms step_avg:97.24ms
step:1060/1770 train_time:102112ms step_avg:97.25ms
step:1061/1770 train_time:102210ms step_avg:97.25ms
step:1062/1770 train_time:102312ms step_avg:97.25ms
step:1063/1770 train_time:102414ms step_avg:97.26ms
step:1064/1770 train_time:102515ms step_avg:97.26ms
step:1065/1770 train_time:102616ms step_avg:97.27ms
step:1066/1770 train_time:102717ms step_avg:97.27ms
step:1067/1770 train_time:102820ms step_avg:97.28ms
step:1068/1770 train_time:102922ms step_avg:97.28ms
step:1069/1770 train_time:103023ms step_avg:97.28ms
step:1070/1770 train_time:103123ms step_avg:97.29ms
step:1071/1770 train_time:103224ms step_avg:97.29ms
step:1072/1770 train_time:103325ms step_avg:97.29ms
step:1073/1770 train_time:103425ms step_avg:97.30ms
step:1074/1770 train_time:103526ms step_avg:97.30ms
step:1075/1770 train_time:103627ms step_avg:97.30ms
step:1076/1770 train_time:103729ms step_avg:97.31ms
step:1077/1770 train_time:103831ms step_avg:97.31ms
step:1078/1770 train_time:103931ms step_avg:97.31ms
step:1079/1770 train_time:104033ms step_avg:97.32ms
step:1080/1770 train_time:104133ms step_avg:97.32ms
step:1081/1770 train_time:104233ms step_avg:97.32ms
step:1082/1770 train_time:104334ms step_avg:97.33ms
step:1083/1770 train_time:104436ms step_avg:97.33ms
step:1084/1770 train_time:104537ms step_avg:97.33ms
step:1085/1770 train_time:104638ms step_avg:97.34ms
step:1086/1770 train_time:104740ms step_avg:97.34ms
step:1087/1770 train_time:104840ms step_avg:97.34ms
step:1088/1770 train_time:104941ms step_avg:97.35ms
step:1089/1770 train_time:105042ms step_avg:97.35ms
step:1090/1770 train_time:105143ms step_avg:97.35ms
step:1091/1770 train_time:105243ms step_avg:97.36ms
step:1092/1770 train_time:105343ms step_avg:97.36ms
step:1093/1770 train_time:105444ms step_avg:97.36ms
step:1094/1770 train_time:105546ms step_avg:97.37ms
step:1095/1770 train_time:105646ms step_avg:97.37ms
step:1096/1770 train_time:105747ms step_avg:97.37ms
step:1097/1770 train_time:105847ms step_avg:97.38ms
step:1098/1770 train_time:105949ms step_avg:97.38ms
step:1099/1770 train_time:106050ms step_avg:97.38ms
step:1100/1770 train_time:106150ms step_avg:97.39ms
step:1101/1770 train_time:106251ms step_avg:97.39ms
step:1102/1770 train_time:106352ms step_avg:97.39ms
step:1103/1770 train_time:106452ms step_avg:97.39ms
step:1104/1770 train_time:106554ms step_avg:97.40ms
step:1105/1770 train_time:106656ms step_avg:97.40ms
step:1106/1770 train_time:106757ms step_avg:97.41ms
step:1107/1770 train_time:106859ms step_avg:97.41ms
step:1108/1770 train_time:106960ms step_avg:97.41ms
step:1109/1770 train_time:107061ms step_avg:97.42ms
step:1110/1770 train_time:107162ms step_avg:97.42ms
step:1111/1770 train_time:107263ms step_avg:97.42ms
step:1112/1770 train_time:107363ms step_avg:97.43ms
step:1113/1770 train_time:107463ms step_avg:97.43ms
step:1114/1770 train_time:107565ms step_avg:97.43ms
step:1115/1770 train_time:107666ms step_avg:97.44ms
step:1116/1770 train_time:107768ms step_avg:97.44ms
step:1117/1770 train_time:107869ms step_avg:97.44ms
step:1118/1770 train_time:107970ms step_avg:97.45ms
step:1119/1770 train_time:108071ms step_avg:97.45ms
step:1120/1770 train_time:108171ms step_avg:97.45ms
step:1121/1770 train_time:108271ms step_avg:97.45ms
step:1122/1770 train_time:108372ms step_avg:97.46ms
step:1123/1770 train_time:108472ms step_avg:97.46ms
step:1124/1770 train_time:108573ms step_avg:97.46ms
step:1125/1770 train_time:108675ms step_avg:97.47ms
step:1125/1770 val_loss:3.4721 train_time:108775ms step_avg:97.56ms
step:1126/1770 train_time:108796ms step_avg:97.49ms
step:1127/1770 train_time:108888ms step_avg:97.48ms
step:1128/1770 train_time:108990ms step_avg:97.49ms
step:1129/1770 train_time:109091ms step_avg:97.49ms
step:1130/1770 train_time:109191ms step_avg:97.49ms
step:1131/1770 train_time:109292ms step_avg:97.49ms
step:1132/1770 train_time:109392ms step_avg:97.50ms
step:1133/1770 train_time:109492ms step_avg:97.50ms
step:1134/1770 train_time:109592ms step_avg:97.50ms
step:1135/1770 train_time:109692ms step_avg:97.50ms
step:1136/1770 train_time:109793ms step_avg:97.51ms
step:1137/1770 train_time:109898ms step_avg:97.51ms
step:1138/1770 train_time:109999ms step_avg:97.52ms
step:1139/1770 train_time:110099ms step_avg:97.52ms
step:1140/1770 train_time:110200ms step_avg:97.52ms
step:1141/1770 train_time:110300ms step_avg:97.52ms
step:1142/1770 train_time:110404ms step_avg:97.53ms
step:1143/1770 train_time:110501ms step_avg:97.53ms
step:1144/1770 train_time:110602ms step_avg:97.53ms
step:1145/1770 train_time:110702ms step_avg:97.54ms
step:1146/1770 train_time:110804ms step_avg:97.54ms
step:1147/1770 train_time:110906ms step_avg:97.54ms
step:1148/1770 train_time:111007ms step_avg:97.55ms
step:1149/1770 train_time:111108ms step_avg:97.55ms
step:1150/1770 train_time:111209ms step_avg:97.55ms
step:1151/1770 train_time:111310ms step_avg:97.55ms
step:1152/1770 train_time:111411ms step_avg:97.56ms
step:1153/1770 train_time:111511ms step_avg:97.56ms
step:1154/1770 train_time:111611ms step_avg:97.56ms
step:1155/1770 train_time:111711ms step_avg:97.56ms
step:1156/1770 train_time:111813ms step_avg:97.57ms
step:1157/1770 train_time:111916ms step_avg:97.57ms
step:1158/1770 train_time:112017ms step_avg:97.58ms
step:1159/1770 train_time:112118ms step_avg:97.58ms
step:1160/1770 train_time:112220ms step_avg:97.58ms
step:1161/1770 train_time:112320ms step_avg:97.58ms
step:1162/1770 train_time:112421ms step_avg:97.59ms
step:1163/1770 train_time:112522ms step_avg:97.59ms
step:1164/1770 train_time:112623ms step_avg:97.59ms
step:1165/1770 train_time:112724ms step_avg:97.60ms
step:1166/1770 train_time:112826ms step_avg:97.60ms
step:1167/1770 train_time:112928ms step_avg:97.60ms
step:1168/1770 train_time:113029ms step_avg:97.61ms
step:1169/1770 train_time:113130ms step_avg:97.61ms
step:1170/1770 train_time:113230ms step_avg:97.61ms
step:1171/1770 train_time:113331ms step_avg:97.61ms
step:1172/1770 train_time:113431ms step_avg:97.62ms
step:1173/1770 train_time:113532ms step_avg:97.62ms
step:1174/1770 train_time:113634ms step_avg:97.62ms
step:1175/1770 train_time:113736ms step_avg:97.63ms
step:1176/1770 train_time:113837ms step_avg:97.63ms
step:1177/1770 train_time:113938ms step_avg:97.63ms
step:1178/1770 train_time:114040ms step_avg:97.64ms
step:1179/1770 train_time:114141ms step_avg:97.64ms
step:1180/1770 train_time:114241ms step_avg:97.64ms
step:1181/1770 train_time:114342ms step_avg:97.65ms
step:1182/1770 train_time:114443ms step_avg:97.65ms
step:1183/1770 train_time:114545ms step_avg:97.65ms
step:1184/1770 train_time:114649ms step_avg:97.66ms
step:1185/1770 train_time:114751ms step_avg:97.66ms
step:1186/1770 train_time:114853ms step_avg:97.66ms
step:1187/1770 train_time:114957ms step_avg:97.67ms
step:1188/1770 train_time:115058ms step_avg:97.67ms
step:1189/1770 train_time:115161ms step_avg:97.68ms
step:1190/1770 train_time:115262ms step_avg:97.68ms
step:1191/1770 train_time:115364ms step_avg:97.68ms
step:1192/1770 train_time:115466ms step_avg:97.69ms
step:1193/1770 train_time:115570ms step_avg:97.69ms
step:1194/1770 train_time:115671ms step_avg:97.70ms
step:1195/1770 train_time:115774ms step_avg:97.70ms
step:1196/1770 train_time:115877ms step_avg:97.70ms
step:1197/1770 train_time:115978ms step_avg:97.71ms
step:1198/1770 train_time:116080ms step_avg:97.71ms
step:1199/1770 train_time:116182ms step_avg:97.71ms
step:1200/1770 train_time:116285ms step_avg:97.72ms
step:1201/1770 train_time:116387ms step_avg:97.72ms
step:1202/1770 train_time:116489ms step_avg:97.73ms
step:1203/1770 train_time:116591ms step_avg:97.73ms
step:1204/1770 train_time:116694ms step_avg:97.73ms
step:1205/1770 train_time:116794ms step_avg:97.74ms
step:1206/1770 train_time:116897ms step_avg:97.74ms
step:1207/1770 train_time:116999ms step_avg:97.74ms
step:1208/1770 train_time:117101ms step_avg:97.75ms
step:1209/1770 train_time:117202ms step_avg:97.75ms
step:1210/1770 train_time:117304ms step_avg:97.75ms
step:1211/1770 train_time:117407ms step_avg:97.76ms
step:1212/1770 train_time:117510ms step_avg:97.76ms
step:1213/1770 train_time:117612ms step_avg:97.77ms
step:1214/1770 train_time:117714ms step_avg:97.77ms
step:1215/1770 train_time:117816ms step_avg:97.77ms
step:1216/1770 train_time:117920ms step_avg:97.78ms
step:1217/1770 train_time:118022ms step_avg:97.78ms
step:1218/1770 train_time:118123ms step_avg:97.78ms
step:1219/1770 train_time:118225ms step_avg:97.79ms
step:1220/1770 train_time:118328ms step_avg:97.79ms
step:1221/1770 train_time:118430ms step_avg:97.80ms
step:1222/1770 train_time:118533ms step_avg:97.80ms
step:1223/1770 train_time:118635ms step_avg:97.80ms
step:1224/1770 train_time:118737ms step_avg:97.81ms
step:1225/1770 train_time:118840ms step_avg:97.81ms
step:1226/1770 train_time:118942ms step_avg:97.81ms
step:1227/1770 train_time:119046ms step_avg:97.82ms
step:1228/1770 train_time:119150ms step_avg:97.82ms
step:1229/1770 train_time:119251ms step_avg:97.83ms
step:1230/1770 train_time:119353ms step_avg:97.83ms
step:1231/1770 train_time:119455ms step_avg:97.83ms
step:1232/1770 train_time:119556ms step_avg:97.84ms
step:1233/1770 train_time:119658ms step_avg:97.84ms
step:1234/1770 train_time:119760ms step_avg:97.84ms
step:1235/1770 train_time:119862ms step_avg:97.85ms
step:1236/1770 train_time:119965ms step_avg:97.85ms
step:1237/1770 train_time:120067ms step_avg:97.85ms
step:1238/1770 train_time:120170ms step_avg:97.86ms
step:1239/1770 train_time:120272ms step_avg:97.86ms
step:1240/1770 train_time:120373ms step_avg:97.86ms
step:1241/1770 train_time:120476ms step_avg:97.87ms
step:1242/1770 train_time:120578ms step_avg:97.87ms
step:1243/1770 train_time:120680ms step_avg:97.87ms
step:1244/1770 train_time:120782ms step_avg:97.88ms
step:1245/1770 train_time:120883ms step_avg:97.88ms
step:1246/1770 train_time:120986ms step_avg:97.88ms
step:1247/1770 train_time:121087ms step_avg:97.89ms
step:1248/1770 train_time:121190ms step_avg:97.89ms
step:1249/1770 train_time:121293ms step_avg:97.90ms
step:1250/1770 train_time:121394ms step_avg:97.90ms
step:1250/1770 val_loss:3.4247 train_time:121496ms step_avg:97.98ms
step:1251/1770 train_time:121517ms step_avg:97.92ms
step:1252/1770 train_time:121610ms step_avg:97.91ms
step:1253/1770 train_time:121713ms step_avg:97.92ms
step:1254/1770 train_time:121814ms step_avg:97.92ms
step:1255/1770 train_time:121918ms step_avg:97.93ms
step:1256/1770 train_time:122020ms step_avg:97.93ms
step:1257/1770 train_time:122121ms step_avg:97.93ms
step:1258/1770 train_time:122223ms step_avg:97.93ms
step:1259/1770 train_time:122325ms step_avg:97.94ms
step:1260/1770 train_time:122426ms step_avg:97.94ms
step:1261/1770 train_time:122529ms step_avg:97.94ms
step:1262/1770 train_time:122633ms step_avg:97.95ms
step:1263/1770 train_time:122734ms step_avg:97.95ms
step:1264/1770 train_time:122838ms step_avg:97.96ms
step:1265/1770 train_time:122940ms step_avg:97.96ms
step:1266/1770 train_time:123042ms step_avg:97.96ms
step:1267/1770 train_time:123145ms step_avg:97.97ms
step:1268/1770 train_time:123247ms step_avg:97.97ms
step:1269/1770 train_time:123349ms step_avg:97.97ms
step:1270/1770 train_time:123451ms step_avg:97.98ms
step:1271/1770 train_time:123553ms step_avg:97.98ms
step:1272/1770 train_time:123654ms step_avg:97.98ms
step:1273/1770 train_time:123757ms step_avg:97.99ms
step:1274/1770 train_time:123860ms step_avg:97.99ms
step:1275/1770 train_time:123961ms step_avg:97.99ms
step:1276/1770 train_time:124065ms step_avg:98.00ms
step:1277/1770 train_time:124167ms step_avg:98.00ms
step:1278/1770 train_time:124270ms step_avg:98.00ms
step:1279/1770 train_time:124372ms step_avg:98.01ms
step:1280/1770 train_time:124475ms step_avg:98.01ms
step:1281/1770 train_time:124577ms step_avg:98.01ms
step:1282/1770 train_time:124680ms step_avg:98.02ms
step:1283/1770 train_time:124782ms step_avg:98.02ms
step:1284/1770 train_time:124884ms step_avg:98.03ms
step:1285/1770 train_time:124986ms step_avg:98.03ms
step:1286/1770 train_time:125090ms step_avg:98.03ms
step:1287/1770 train_time:125193ms step_avg:98.04ms
step:1288/1770 train_time:125296ms step_avg:98.04ms
step:1289/1770 train_time:125398ms step_avg:98.04ms
step:1290/1770 train_time:125500ms step_avg:98.05ms
step:1291/1770 train_time:125602ms step_avg:98.05ms
step:1292/1770 train_time:125703ms step_avg:98.05ms
step:1293/1770 train_time:125806ms step_avg:98.06ms
step:1294/1770 train_time:125908ms step_avg:98.06ms
step:1295/1770 train_time:126011ms step_avg:98.06ms
step:1296/1770 train_time:126113ms step_avg:98.07ms
step:1297/1770 train_time:126214ms step_avg:98.07ms
step:1298/1770 train_time:126316ms step_avg:98.07ms
step:1299/1770 train_time:126418ms step_avg:98.07ms
step:1300/1770 train_time:126520ms step_avg:98.08ms
step:1301/1770 train_time:126622ms step_avg:98.08ms
step:1302/1770 train_time:126724ms step_avg:98.08ms
step:1303/1770 train_time:126826ms step_avg:98.09ms
step:1304/1770 train_time:126929ms step_avg:98.09ms
step:1305/1770 train_time:127031ms step_avg:98.09ms
step:1306/1770 train_time:127133ms step_avg:98.10ms
step:1307/1770 train_time:127234ms step_avg:98.10ms
step:1308/1770 train_time:127336ms step_avg:98.10ms
step:1309/1770 train_time:127438ms step_avg:98.10ms
step:1310/1770 train_time:127539ms step_avg:98.11ms
step:1311/1770 train_time:127642ms step_avg:98.11ms
step:1312/1770 train_time:127743ms step_avg:98.11ms
step:1313/1770 train_time:127845ms step_avg:98.12ms
step:1314/1770 train_time:127947ms step_avg:98.12ms
step:1315/1770 train_time:128049ms step_avg:98.12ms
step:1316/1770 train_time:128152ms step_avg:98.13ms
step:1317/1770 train_time:128254ms step_avg:98.13ms
step:1318/1770 train_time:128359ms step_avg:98.13ms
step:1319/1770 train_time:128462ms step_avg:98.14ms
step:1320/1770 train_time:128564ms step_avg:98.14ms
step:1321/1770 train_time:128666ms step_avg:98.14ms
step:1322/1770 train_time:128768ms step_avg:98.15ms
step:1323/1770 train_time:128871ms step_avg:98.15ms
step:1324/1770 train_time:128974ms step_avg:98.15ms
step:1325/1770 train_time:129077ms step_avg:98.16ms
step:1326/1770 train_time:129179ms step_avg:98.16ms
step:1327/1770 train_time:129286ms step_avg:98.17ms
step:1328/1770 train_time:129385ms step_avg:98.17ms
step:1329/1770 train_time:129487ms step_avg:98.17ms
step:1330/1770 train_time:129588ms step_avg:98.17ms
step:1331/1770 train_time:129690ms step_avg:98.18ms
step:1332/1770 train_time:129791ms step_avg:98.18ms
step:1333/1770 train_time:129892ms step_avg:98.18ms
step:1334/1770 train_time:129994ms step_avg:98.18ms
step:1335/1770 train_time:130096ms step_avg:98.19ms
step:1336/1770 train_time:130197ms step_avg:98.19ms
step:1337/1770 train_time:130301ms step_avg:98.19ms
step:1338/1770 train_time:130403ms step_avg:98.20ms
step:1339/1770 train_time:130506ms step_avg:98.20ms
step:1340/1770 train_time:130610ms step_avg:98.20ms
step:1341/1770 train_time:130711ms step_avg:98.21ms
step:1342/1770 train_time:130814ms step_avg:98.21ms
step:1343/1770 train_time:130917ms step_avg:98.21ms
step:1344/1770 train_time:131019ms step_avg:98.22ms
step:1345/1770 train_time:131121ms step_avg:98.22ms
step:1346/1770 train_time:131224ms step_avg:98.22ms
step:1347/1770 train_time:131326ms step_avg:98.22ms
step:1348/1770 train_time:131430ms step_avg:98.23ms
step:1349/1770 train_time:131532ms step_avg:98.23ms
step:1350/1770 train_time:131634ms step_avg:98.23ms
step:1351/1770 train_time:131737ms step_avg:98.24ms
step:1352/1770 train_time:131838ms step_avg:98.24ms
step:1353/1770 train_time:131942ms step_avg:98.24ms
step:1354/1770 train_time:132044ms step_avg:98.25ms
step:1355/1770 train_time:132146ms step_avg:98.25ms
step:1356/1770 train_time:132248ms step_avg:98.25ms
step:1357/1770 train_time:132350ms step_avg:98.26ms
step:1358/1770 train_time:132453ms step_avg:98.26ms
step:1359/1770 train_time:132554ms step_avg:98.26ms
step:1360/1770 train_time:132657ms step_avg:98.26ms
step:1361/1770 train_time:132759ms step_avg:98.27ms
step:1362/1770 train_time:132862ms step_avg:98.27ms
step:1363/1770 train_time:132965ms step_avg:98.27ms
step:1364/1770 train_time:133067ms step_avg:98.28ms
step:1365/1770 train_time:133169ms step_avg:98.28ms
step:1366/1770 train_time:133272ms step_avg:98.28ms
step:1367/1770 train_time:133375ms step_avg:98.29ms
step:1368/1770 train_time:133476ms step_avg:98.29ms
step:1369/1770 train_time:133579ms step_avg:98.29ms
step:1370/1770 train_time:133682ms step_avg:98.30ms
step:1371/1770 train_time:133784ms step_avg:98.30ms
step:1372/1770 train_time:133885ms step_avg:98.30ms
step:1373/1770 train_time:133989ms step_avg:98.30ms
step:1374/1770 train_time:134092ms step_avg:98.31ms
step:1375/1770 train_time:134194ms step_avg:98.31ms
step:1375/1770 val_loss:3.3807 train_time:134295ms step_avg:98.38ms
step:1376/1770 train_time:134316ms step_avg:98.33ms
step:1377/1770 train_time:134408ms step_avg:98.32ms
step:1378/1770 train_time:134510ms step_avg:98.33ms
step:1379/1770 train_time:134612ms step_avg:98.33ms
step:1380/1770 train_time:134714ms step_avg:98.33ms
step:1381/1770 train_time:134816ms step_avg:98.33ms
step:1382/1770 train_time:134917ms step_avg:98.34ms
step:1383/1770 train_time:135020ms step_avg:98.34ms
step:1384/1770 train_time:135122ms step_avg:98.34ms
step:1385/1770 train_time:135224ms step_avg:98.35ms
step:1386/1770 train_time:135327ms step_avg:98.35ms
step:1387/1770 train_time:135430ms step_avg:98.35ms
step:1388/1770 train_time:135532ms step_avg:98.35ms
step:1389/1770 train_time:135635ms step_avg:98.36ms
step:1390/1770 train_time:135737ms step_avg:98.36ms
step:1391/1770 train_time:135838ms step_avg:98.36ms
step:1392/1770 train_time:135940ms step_avg:98.37ms
step:1393/1770 train_time:136042ms step_avg:98.37ms
step:1394/1770 train_time:136144ms step_avg:98.37ms
step:1395/1770 train_time:136247ms step_avg:98.37ms
step:1396/1770 train_time:136350ms step_avg:98.38ms
step:1397/1770 train_time:136452ms step_avg:98.38ms
step:1398/1770 train_time:136557ms step_avg:98.38ms
step:1399/1770 train_time:136656ms step_avg:98.38ms
step:1400/1770 train_time:136759ms step_avg:98.39ms
step:1401/1770 train_time:136861ms step_avg:98.39ms
step:1402/1770 train_time:136964ms step_avg:98.39ms
step:1403/1770 train_time:137065ms step_avg:98.40ms
step:1404/1770 train_time:137169ms step_avg:98.40ms
step:1405/1770 train_time:137271ms step_avg:98.40ms
step:1406/1770 train_time:137373ms step_avg:98.40ms
step:1407/1770 train_time:137474ms step_avg:98.41ms
step:1408/1770 train_time:137577ms step_avg:98.41ms
step:1409/1770 train_time:137679ms step_avg:98.41ms
step:1410/1770 train_time:137781ms step_avg:98.42ms
step:1411/1770 train_time:137883ms step_avg:98.42ms
step:1412/1770 train_time:137984ms step_avg:98.42ms
step:1413/1770 train_time:138085ms step_avg:98.42ms
step:1414/1770 train_time:138189ms step_avg:98.43ms
step:1415/1770 train_time:138292ms step_avg:98.43ms
step:1416/1770 train_time:138395ms step_avg:98.43ms
step:1417/1770 train_time:138496ms step_avg:98.43ms
step:1418/1770 train_time:138599ms step_avg:98.44ms
step:1419/1770 train_time:138702ms step_avg:98.44ms
step:1420/1770 train_time:138804ms step_avg:98.44ms
step:1421/1770 train_time:138906ms step_avg:98.44ms
step:1422/1770 train_time:139008ms step_avg:98.45ms
step:1423/1770 train_time:139110ms step_avg:98.45ms
step:1424/1770 train_time:139213ms step_avg:98.45ms
step:1425/1770 train_time:139315ms step_avg:98.46ms
step:1426/1770 train_time:139417ms step_avg:98.46ms
step:1427/1770 train_time:139519ms step_avg:98.46ms
step:1428/1770 train_time:139623ms step_avg:98.46ms
step:1429/1770 train_time:139725ms step_avg:98.47ms
step:1430/1770 train_time:139827ms step_avg:98.47ms
step:1431/1770 train_time:139930ms step_avg:98.47ms
step:1432/1770 train_time:140031ms step_avg:98.47ms
step:1433/1770 train_time:140132ms step_avg:98.48ms
step:1434/1770 train_time:140233ms step_avg:98.48ms
step:1435/1770 train_time:140335ms step_avg:98.48ms
step:1436/1770 train_time:140440ms step_avg:98.48ms
step:1437/1770 train_time:140542ms step_avg:98.49ms
step:1438/1770 train_time:140644ms step_avg:98.49ms
step:1439/1770 train_time:140750ms step_avg:98.50ms
step:1440/1770 train_time:140848ms step_avg:98.50ms
step:1441/1770 train_time:140953ms step_avg:98.50ms
step:1442/1770 train_time:141054ms step_avg:98.50ms
step:1443/1770 train_time:141156ms step_avg:98.50ms
step:1444/1770 train_time:141259ms step_avg:98.51ms
step:1445/1770 train_time:141361ms step_avg:98.51ms
step:1446/1770 train_time:141465ms step_avg:98.51ms
step:1447/1770 train_time:141567ms step_avg:98.52ms
step:1448/1770 train_time:141671ms step_avg:98.52ms
step:1449/1770 train_time:141775ms step_avg:98.52ms
step:1450/1770 train_time:141877ms step_avg:98.53ms
step:1451/1770 train_time:141982ms step_avg:98.53ms
step:1452/1770 train_time:142085ms step_avg:98.53ms
step:1453/1770 train_time:142189ms step_avg:98.54ms
step:1454/1770 train_time:142292ms step_avg:98.54ms
step:1455/1770 train_time:142396ms step_avg:98.54ms
step:1456/1770 train_time:142500ms step_avg:98.55ms
step:1457/1770 train_time:142603ms step_avg:98.55ms
step:1458/1770 train_time:142706ms step_avg:98.55ms
step:1459/1770 train_time:142810ms step_avg:98.56ms
step:1460/1770 train_time:142913ms step_avg:98.56ms
step:1461/1770 train_time:143017ms step_avg:98.56ms
step:1462/1770 train_time:143121ms step_avg:98.57ms
step:1463/1770 train_time:143224ms step_avg:98.57ms
step:1464/1770 train_time:143330ms step_avg:98.58ms
step:1465/1770 train_time:143433ms step_avg:98.58ms
step:1466/1770 train_time:143537ms step_avg:98.58ms
step:1467/1770 train_time:143641ms step_avg:98.59ms
step:1468/1770 train_time:143744ms step_avg:98.59ms
step:1469/1770 train_time:143847ms step_avg:98.59ms
step:1470/1770 train_time:143950ms step_avg:98.60ms
step:1471/1770 train_time:144054ms step_avg:98.60ms
step:1472/1770 train_time:144157ms step_avg:98.60ms
step:1473/1770 train_time:144261ms step_avg:98.61ms
step:1474/1770 train_time:144364ms step_avg:98.61ms
step:1475/1770 train_time:144467ms step_avg:98.61ms
step:1476/1770 train_time:144570ms step_avg:98.62ms
step:1477/1770 train_time:144675ms step_avg:98.62ms
step:1478/1770 train_time:144780ms step_avg:98.62ms
step:1479/1770 train_time:144882ms step_avg:98.63ms
step:1480/1770 train_time:144985ms step_avg:98.63ms
step:1481/1770 train_time:145092ms step_avg:98.64ms
step:1482/1770 train_time:145195ms step_avg:98.64ms
step:1483/1770 train_time:145299ms step_avg:98.64ms
step:1484/1770 train_time:145401ms step_avg:98.64ms
step:1485/1770 train_time:145503ms step_avg:98.65ms
step:1486/1770 train_time:145606ms step_avg:98.65ms
step:1487/1770 train_time:145709ms step_avg:98.65ms
step:1488/1770 train_time:145813ms step_avg:98.66ms
step:1489/1770 train_time:145917ms step_avg:98.66ms
step:1490/1770 train_time:146020ms step_avg:98.66ms
step:1491/1770 train_time:146123ms step_avg:98.67ms
step:1492/1770 train_time:146227ms step_avg:98.67ms
step:1493/1770 train_time:146334ms step_avg:98.67ms
step:1494/1770 train_time:146441ms step_avg:98.68ms
step:1495/1770 train_time:146542ms step_avg:98.68ms
step:1496/1770 train_time:146649ms step_avg:98.69ms
step:1497/1770 train_time:146748ms step_avg:98.69ms
step:1498/1770 train_time:146850ms step_avg:98.69ms
step:1499/1770 train_time:146953ms step_avg:98.69ms
step:1500/1770 train_time:147056ms step_avg:98.70ms
step:1500/1770 val_loss:3.3428 train_time:147157ms step_avg:98.76ms
step:1501/1770 train_time:147178ms step_avg:98.71ms
step:1502/1770 train_time:147274ms step_avg:98.71ms
step:1503/1770 train_time:147377ms step_avg:98.71ms
step:1504/1770 train_time:147479ms step_avg:98.71ms
step:1505/1770 train_time:147584ms step_avg:98.72ms
step:1506/1770 train_time:147687ms step_avg:98.72ms
step:1507/1770 train_time:147791ms step_avg:98.72ms
step:1508/1770 train_time:147896ms step_avg:98.73ms
step:1509/1770 train_time:147998ms step_avg:98.73ms
step:1510/1770 train_time:148101ms step_avg:98.73ms
step:1511/1770 train_time:148205ms step_avg:98.74ms
step:1512/1770 train_time:148310ms step_avg:98.74ms
step:1513/1770 train_time:148415ms step_avg:98.75ms
step:1514/1770 train_time:148519ms step_avg:98.75ms
step:1515/1770 train_time:148622ms step_avg:98.75ms
step:1516/1770 train_time:148725ms step_avg:98.75ms
step:1517/1770 train_time:148828ms step_avg:98.76ms
step:1518/1770 train_time:148933ms step_avg:98.76ms
step:1519/1770 train_time:149035ms step_avg:98.76ms
step:1520/1770 train_time:149140ms step_avg:98.77ms
step:1521/1770 train_time:149242ms step_avg:98.77ms
step:1522/1770 train_time:149346ms step_avg:98.77ms
step:1523/1770 train_time:149452ms step_avg:98.78ms
step:1524/1770 train_time:149554ms step_avg:98.78ms
step:1525/1770 train_time:149657ms step_avg:98.78ms
step:1526/1770 train_time:149760ms step_avg:98.79ms
step:1527/1770 train_time:149863ms step_avg:98.79ms
step:1528/1770 train_time:149968ms step_avg:98.79ms
step:1529/1770 train_time:150071ms step_avg:98.80ms
step:1530/1770 train_time:150174ms step_avg:98.80ms
step:1531/1770 train_time:150277ms step_avg:98.80ms
step:1532/1770 train_time:150380ms step_avg:98.80ms
step:1533/1770 train_time:150485ms step_avg:98.81ms
step:1534/1770 train_time:150589ms step_avg:98.81ms
step:1535/1770 train_time:150691ms step_avg:98.81ms
step:1536/1770 train_time:150794ms step_avg:98.82ms
step:1537/1770 train_time:150897ms step_avg:98.82ms
step:1538/1770 train_time:151002ms step_avg:98.82ms
step:1539/1770 train_time:151104ms step_avg:98.83ms
step:1540/1770 train_time:151214ms step_avg:98.83ms
step:1541/1770 train_time:151315ms step_avg:98.83ms
step:1542/1770 train_time:151418ms step_avg:98.84ms
step:1543/1770 train_time:151520ms step_avg:98.84ms
step:1544/1770 train_time:151627ms step_avg:98.84ms
step:1545/1770 train_time:151729ms step_avg:98.85ms
step:1546/1770 train_time:151833ms step_avg:98.85ms
step:1547/1770 train_time:151936ms step_avg:98.85ms
step:1548/1770 train_time:152039ms step_avg:98.86ms
step:1549/1770 train_time:152143ms step_avg:98.86ms
step:1550/1770 train_time:152247ms step_avg:98.86ms
step:1551/1770 train_time:152350ms step_avg:98.86ms
step:1552/1770 train_time:152455ms step_avg:98.87ms
step:1553/1770 train_time:152558ms step_avg:98.87ms
step:1554/1770 train_time:152661ms step_avg:98.87ms
step:1555/1770 train_time:152765ms step_avg:98.88ms
step:1556/1770 train_time:152868ms step_avg:98.88ms
step:1557/1770 train_time:152970ms step_avg:98.88ms
step:1558/1770 train_time:153074ms step_avg:98.89ms
step:1559/1770 train_time:153178ms step_avg:98.89ms
step:1560/1770 train_time:153280ms step_avg:98.89ms
step:1561/1770 train_time:153386ms step_avg:98.89ms
step:1562/1770 train_time:153489ms step_avg:98.90ms
step:1563/1770 train_time:153592ms step_avg:98.90ms
step:1564/1770 train_time:153695ms step_avg:98.90ms
step:1565/1770 train_time:153799ms step_avg:98.91ms
step:1566/1770 train_time:153901ms step_avg:98.91ms
step:1567/1770 train_time:154005ms step_avg:98.91ms
step:1568/1770 train_time:154109ms step_avg:98.91ms
step:1569/1770 train_time:154217ms step_avg:98.92ms
step:1570/1770 train_time:154318ms step_avg:98.92ms
step:1571/1770 train_time:154421ms step_avg:98.92ms
step:1572/1770 train_time:154525ms step_avg:98.93ms
step:1573/1770 train_time:154630ms step_avg:98.93ms
step:1574/1770 train_time:154733ms step_avg:98.93ms
step:1575/1770 train_time:154835ms step_avg:98.94ms
step:1576/1770 train_time:154938ms step_avg:98.94ms
step:1577/1770 train_time:155044ms step_avg:98.94ms
step:1578/1770 train_time:155148ms step_avg:98.95ms
step:1579/1770 train_time:155252ms step_avg:98.95ms
step:1580/1770 train_time:155356ms step_avg:98.95ms
step:1581/1770 train_time:155461ms step_avg:98.96ms
step:1582/1770 train_time:155566ms step_avg:98.96ms
step:1583/1770 train_time:155670ms step_avg:98.96ms
step:1584/1770 train_time:155774ms step_avg:98.97ms
step:1585/1770 train_time:155878ms step_avg:98.97ms
step:1586/1770 train_time:155985ms step_avg:98.98ms
step:1587/1770 train_time:156089ms step_avg:98.98ms
step:1588/1770 train_time:156192ms step_avg:98.98ms
step:1589/1770 train_time:156298ms step_avg:98.99ms
step:1590/1770 train_time:156401ms step_avg:98.99ms
step:1591/1770 train_time:156503ms step_avg:98.99ms
step:1592/1770 train_time:156607ms step_avg:98.99ms
step:1593/1770 train_time:156710ms step_avg:99.00ms
step:1594/1770 train_time:156812ms step_avg:99.00ms
step:1595/1770 train_time:156916ms step_avg:99.00ms
step:1596/1770 train_time:157021ms step_avg:99.00ms
step:1597/1770 train_time:157123ms step_avg:99.01ms
step:1598/1770 train_time:157228ms step_avg:99.01ms
step:1599/1770 train_time:157333ms step_avg:99.01ms
step:1600/1770 train_time:157439ms step_avg:99.02ms
step:1601/1770 train_time:157542ms step_avg:99.02ms
step:1602/1770 train_time:157646ms step_avg:99.02ms
step:1603/1770 train_time:157749ms step_avg:99.03ms
step:1604/1770 train_time:157851ms step_avg:99.03ms
step:1605/1770 train_time:157954ms step_avg:99.03ms
step:1606/1770 train_time:158057ms step_avg:99.03ms
step:1607/1770 train_time:158164ms step_avg:99.04ms
step:1608/1770 train_time:158268ms step_avg:99.04ms
step:1609/1770 train_time:158372ms step_avg:99.04ms
step:1610/1770 train_time:158478ms step_avg:99.05ms
step:1611/1770 train_time:158583ms step_avg:99.05ms
step:1612/1770 train_time:158687ms step_avg:99.06ms
step:1613/1770 train_time:158790ms step_avg:99.06ms
step:1614/1770 train_time:158893ms step_avg:99.06ms
step:1615/1770 train_time:158996ms step_avg:99.06ms
step:1616/1770 train_time:159099ms step_avg:99.07ms
step:1617/1770 train_time:159205ms step_avg:99.07ms
step:1618/1770 train_time:159309ms step_avg:99.07ms
step:1619/1770 train_time:159413ms step_avg:99.08ms
step:1620/1770 train_time:159516ms step_avg:99.08ms
step:1621/1770 train_time:159621ms step_avg:99.08ms
step:1622/1770 train_time:159724ms step_avg:99.08ms
step:1623/1770 train_time:159830ms step_avg:99.09ms
step:1624/1770 train_time:159933ms step_avg:99.09ms
step:1625/1770 train_time:160035ms step_avg:99.09ms
step:1625/1770 val_loss:3.3079 train_time:160137ms step_avg:99.16ms
step:1626/1770 train_time:160158ms step_avg:99.11ms
step:1627/1770 train_time:160251ms step_avg:99.10ms
step:1628/1770 train_time:160353ms step_avg:99.11ms
step:1629/1770 train_time:160456ms step_avg:99.11ms
step:1630/1770 train_time:160559ms step_avg:99.11ms
step:1631/1770 train_time:160661ms step_avg:99.11ms
step:1632/1770 train_time:160765ms step_avg:99.12ms
step:1633/1770 train_time:160868ms step_avg:99.12ms
step:1634/1770 train_time:160971ms step_avg:99.12ms
step:1635/1770 train_time:161074ms step_avg:99.12ms
step:1636/1770 train_time:161178ms step_avg:99.13ms
step:1637/1770 train_time:161283ms step_avg:99.13ms
step:1638/1770 train_time:161387ms step_avg:99.13ms
step:1639/1770 train_time:161490ms step_avg:99.13ms
step:1640/1770 train_time:161595ms step_avg:99.14ms
step:1641/1770 train_time:161698ms step_avg:99.14ms
step:1642/1770 train_time:161801ms step_avg:99.14ms
step:1643/1770 train_time:161903ms step_avg:99.14ms
step:1644/1770 train_time:162008ms step_avg:99.15ms
step:1645/1770 train_time:162112ms step_avg:99.15ms
step:1646/1770 train_time:162217ms step_avg:99.15ms
step:1647/1770 train_time:162321ms step_avg:99.16ms
step:1648/1770 train_time:162424ms step_avg:99.16ms
step:1649/1770 train_time:162528ms step_avg:99.16ms
step:1650/1770 train_time:162631ms step_avg:99.17ms
step:1651/1770 train_time:162734ms step_avg:99.17ms
step:1652/1770 train_time:162838ms step_avg:99.17ms
step:1653/1770 train_time:162942ms step_avg:99.17ms
step:1654/1770 train_time:163048ms step_avg:99.18ms
step:1655/1770 train_time:163154ms step_avg:99.18ms
step:1656/1770 train_time:163258ms step_avg:99.18ms
step:1657/1770 train_time:163362ms step_avg:99.19ms
step:1658/1770 train_time:163466ms step_avg:99.19ms
step:1659/1770 train_time:163570ms step_avg:99.19ms
step:1660/1770 train_time:163673ms step_avg:99.20ms
step:1661/1770 train_time:163778ms step_avg:99.20ms
step:1662/1770 train_time:163882ms step_avg:99.20ms
step:1663/1770 train_time:163985ms step_avg:99.20ms
step:1664/1770 train_time:164088ms step_avg:99.21ms
step:1665/1770 train_time:164191ms step_avg:99.21ms
step:1666/1770 train_time:164296ms step_avg:99.21ms
step:1667/1770 train_time:164399ms step_avg:99.21ms
step:1668/1770 train_time:164501ms step_avg:99.22ms
step:1669/1770 train_time:164603ms step_avg:99.22ms
step:1670/1770 train_time:164707ms step_avg:99.22ms
step:1671/1770 train_time:164811ms step_avg:99.22ms
step:1672/1770 train_time:164916ms step_avg:99.23ms
step:1673/1770 train_time:165022ms step_avg:99.23ms
step:1674/1770 train_time:165125ms step_avg:99.23ms
step:1675/1770 train_time:165228ms step_avg:99.24ms
step:1676/1770 train_time:165333ms step_avg:99.24ms
step:1677/1770 train_time:165441ms step_avg:99.24ms
step:1678/1770 train_time:165543ms step_avg:99.25ms
step:1679/1770 train_time:165646ms step_avg:99.25ms
step:1680/1770 train_time:165750ms step_avg:99.25ms
step:1681/1770 train_time:165854ms step_avg:99.25ms
step:1682/1770 train_time:165960ms step_avg:99.26ms
step:1683/1770 train_time:166062ms step_avg:99.26ms
step:1684/1770 train_time:166165ms step_avg:99.26ms
step:1685/1770 train_time:166268ms step_avg:99.26ms
step:1686/1770 train_time:166373ms step_avg:99.27ms
step:1687/1770 train_time:166478ms step_avg:99.27ms
step:1688/1770 train_time:166581ms step_avg:99.27ms
step:1689/1770 train_time:166685ms step_avg:99.28ms
step:1690/1770 train_time:166789ms step_avg:99.28ms
step:1691/1770 train_time:166892ms step_avg:99.28ms
step:1692/1770 train_time:166996ms step_avg:99.28ms
step:1693/1770 train_time:167101ms step_avg:99.29ms
step:1694/1770 train_time:167204ms step_avg:99.29ms
step:1695/1770 train_time:167308ms step_avg:99.29ms
step:1696/1770 train_time:167413ms step_avg:99.30ms
step:1697/1770 train_time:167518ms step_avg:99.30ms
step:1698/1770 train_time:167622ms step_avg:99.30ms
step:1699/1770 train_time:167725ms step_avg:99.30ms
step:1700/1770 train_time:167827ms step_avg:99.31ms
step:1701/1770 train_time:167931ms step_avg:99.31ms
step:1702/1770 train_time:168035ms step_avg:99.31ms
step:1703/1770 train_time:168139ms step_avg:99.31ms
step:1704/1770 train_time:168242ms step_avg:99.32ms
step:1705/1770 train_time:168344ms step_avg:99.32ms
step:1706/1770 train_time:168447ms step_avg:99.32ms
step:1707/1770 train_time:168552ms step_avg:99.32ms
step:1708/1770 train_time:168656ms step_avg:99.33ms
step:1709/1770 train_time:168761ms step_avg:99.33ms
step:1710/1770 train_time:168868ms step_avg:99.33ms
step:1711/1770 train_time:168974ms step_avg:99.34ms
step:1712/1770 train_time:169079ms step_avg:99.34ms
step:1713/1770 train_time:169183ms step_avg:99.34ms
step:1714/1770 train_time:169286ms step_avg:99.35ms
step:1715/1770 train_time:169390ms step_avg:99.35ms
step:1716/1770 train_time:169493ms step_avg:99.35ms
step:1717/1770 train_time:169597ms step_avg:99.35ms
step:1718/1770 train_time:169702ms step_avg:99.36ms
step:1719/1770 train_time:169807ms step_avg:99.36ms
step:1720/1770 train_time:169912ms step_avg:99.36ms
step:1721/1770 train_time:170015ms step_avg:99.37ms
step:1722/1770 train_time:170122ms step_avg:99.37ms
step:1723/1770 train_time:170227ms step_avg:99.37ms
step:1724/1770 train_time:170334ms step_avg:99.38ms
step:1725/1770 train_time:170440ms step_avg:99.38ms
step:1726/1770 train_time:170545ms step_avg:99.39ms
step:1727/1770 train_time:170649ms step_avg:99.39ms
step:1728/1770 train_time:170755ms step_avg:99.39ms
step:1729/1770 train_time:170858ms step_avg:99.39ms
step:1730/1770 train_time:170963ms step_avg:99.40ms
step:1731/1770 train_time:171069ms step_avg:99.40ms
step:1732/1770 train_time:171173ms step_avg:99.40ms
step:1733/1770 train_time:171279ms step_avg:99.41ms
step:1734/1770 train_time:171384ms step_avg:99.41ms
step:1735/1770 train_time:171487ms step_avg:99.41ms
step:1736/1770 train_time:171590ms step_avg:99.42ms
step:1737/1770 train_time:171695ms step_avg:99.42ms
step:1738/1770 train_time:171799ms step_avg:99.42ms
step:1739/1770 train_time:171904ms step_avg:99.42ms
step:1740/1770 train_time:172007ms step_avg:99.43ms
step:1741/1770 train_time:172113ms step_avg:99.43ms
step:1742/1770 train_time:172221ms step_avg:99.43ms
step:1743/1770 train_time:172326ms step_avg:99.44ms
step:1744/1770 train_time:172430ms step_avg:99.44ms
step:1745/1770 train_time:172534ms step_avg:99.44ms
step:1746/1770 train_time:172640ms step_avg:99.45ms
step:1747/1770 train_time:172743ms step_avg:99.45ms
step:1748/1770 train_time:172849ms step_avg:99.45ms
step:1749/1770 train_time:172954ms step_avg:99.46ms
step:1750/1770 train_time:173057ms step_avg:99.46ms
step:1750/1770 val_loss:3.2810 train_time:173160ms step_avg:99.52ms
step:1751/1770 train_time:173181ms step_avg:99.47ms
step:1752/1770 train_time:173277ms step_avg:99.47ms
step:1753/1770 train_time:173382ms step_avg:99.47ms
step:1754/1770 train_time:173486ms step_avg:99.48ms
step:1755/1770 train_time:173590ms step_avg:99.48ms
step:1756/1770 train_time:173694ms step_avg:99.48ms
step:1757/1770 train_time:173798ms step_avg:99.48ms
step:1758/1770 train_time:173902ms step_avg:99.49ms
step:1759/1770 train_time:174007ms step_avg:99.49ms
step:1760/1770 train_time:174111ms step_avg:99.49ms
step:1761/1770 train_time:174218ms step_avg:99.50ms
step:1762/1770 train_time:174326ms step_avg:99.50ms
step:1763/1770 train_time:174429ms step_avg:99.50ms
step:1764/1770 train_time:174533ms step_avg:99.51ms
step:1765/1770 train_time:174638ms step_avg:99.51ms
step:1766/1770 train_time:174746ms step_avg:99.51ms
step:1767/1770 train_time:174849ms step_avg:99.52ms
step:1768/1770 train_time:174953ms step_avg:99.52ms
step:1769/1770 train_time:175056ms step_avg:99.52ms
step:1770/1770 train_time:175160ms step_avg:99.52ms
step:1770/1770 val_loss:3.2780 train_time:175264ms step_avg:99.58ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
