import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:36:02 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23185ms step_avg:nanms
step:2/1770 train_time:23734ms step_avg:nanms
step:3/1770 train_time:23831ms step_avg:nanms
step:4/1770 train_time:23924ms step_avg:nanms
step:5/1770 train_time:24017ms step_avg:nanms
step:6/1770 train_time:24111ms step_avg:nanms
step:7/1770 train_time:24204ms step_avg:nanms
step:8/1770 train_time:24298ms step_avg:nanms
step:9/1770 train_time:24391ms step_avg:nanms
step:10/1770 train_time:24485ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.93ms
step:14/1770 train_time:376ms step_avg:93.99ms
step:15/1770 train_time:470ms step_avg:93.98ms
step:16/1770 train_time:564ms step_avg:93.95ms
step:17/1770 train_time:658ms step_avg:93.98ms
step:18/1770 train_time:752ms step_avg:93.96ms
step:19/1770 train_time:846ms step_avg:93.99ms
step:20/1770 train_time:939ms step_avg:93.92ms
step:21/1770 train_time:1033ms step_avg:93.89ms
step:22/1770 train_time:1126ms step_avg:93.87ms
step:23/1770 train_time:1221ms step_avg:93.90ms
step:24/1770 train_time:1314ms step_avg:93.83ms
step:25/1770 train_time:1408ms step_avg:93.86ms
step:26/1770 train_time:1502ms step_avg:93.88ms
step:27/1770 train_time:1596ms step_avg:93.90ms
step:28/1770 train_time:1691ms step_avg:93.93ms
step:29/1770 train_time:1785ms step_avg:93.93ms
step:30/1770 train_time:1878ms step_avg:93.91ms
step:31/1770 train_time:1972ms step_avg:93.92ms
step:32/1770 train_time:2066ms step_avg:93.91ms
step:33/1770 train_time:2160ms step_avg:93.90ms
step:34/1770 train_time:2253ms step_avg:93.89ms
step:35/1770 train_time:2347ms step_avg:93.87ms
step:36/1770 train_time:2441ms step_avg:93.87ms
step:37/1770 train_time:2534ms step_avg:93.85ms
step:38/1770 train_time:2628ms step_avg:93.86ms
step:39/1770 train_time:2722ms step_avg:93.85ms
step:40/1770 train_time:2816ms step_avg:93.88ms
step:41/1770 train_time:2910ms step_avg:93.88ms
step:42/1770 train_time:3004ms step_avg:93.88ms
step:43/1770 train_time:3098ms step_avg:93.89ms
step:44/1770 train_time:3192ms step_avg:93.89ms
step:45/1770 train_time:3286ms step_avg:93.89ms
step:46/1770 train_time:3380ms step_avg:93.88ms
step:47/1770 train_time:3474ms step_avg:93.88ms
step:48/1770 train_time:3568ms step_avg:93.88ms
step:49/1770 train_time:3662ms step_avg:93.89ms
step:50/1770 train_time:3756ms step_avg:93.89ms
step:51/1770 train_time:3850ms step_avg:93.89ms
step:52/1770 train_time:3944ms step_avg:93.90ms
step:53/1770 train_time:4038ms step_avg:93.90ms
step:54/1770 train_time:4131ms step_avg:93.89ms
step:55/1770 train_time:4225ms step_avg:93.89ms
step:56/1770 train_time:4319ms step_avg:93.89ms
step:57/1770 train_time:4413ms step_avg:93.89ms
step:58/1770 train_time:4507ms step_avg:93.89ms
step:59/1770 train_time:4601ms step_avg:93.89ms
step:60/1770 train_time:4695ms step_avg:93.89ms
step:61/1770 train_time:4788ms step_avg:93.89ms
step:62/1770 train_time:4882ms step_avg:93.89ms
step:63/1770 train_time:4976ms step_avg:93.90ms
step:64/1770 train_time:5070ms step_avg:93.89ms
step:65/1770 train_time:5164ms step_avg:93.89ms
step:66/1770 train_time:5258ms step_avg:93.89ms
step:67/1770 train_time:5352ms step_avg:93.89ms
step:68/1770 train_time:5446ms step_avg:93.89ms
step:69/1770 train_time:5540ms step_avg:93.89ms
step:70/1770 train_time:5633ms step_avg:93.88ms
step:71/1770 train_time:5727ms step_avg:93.89ms
step:72/1770 train_time:5822ms step_avg:93.90ms
step:73/1770 train_time:5916ms step_avg:93.90ms
step:74/1770 train_time:6010ms step_avg:93.90ms
step:75/1770 train_time:6103ms step_avg:93.89ms
step:76/1770 train_time:6197ms step_avg:93.89ms
step:77/1770 train_time:6290ms step_avg:93.89ms
step:78/1770 train_time:6384ms step_avg:93.88ms
step:79/1770 train_time:6478ms step_avg:93.89ms
step:80/1770 train_time:6572ms step_avg:93.88ms
step:81/1770 train_time:6666ms step_avg:93.88ms
step:82/1770 train_time:6762ms step_avg:93.91ms
step:83/1770 train_time:6854ms step_avg:93.89ms
step:84/1770 train_time:6948ms step_avg:93.90ms
step:85/1770 train_time:7042ms step_avg:93.90ms
step:86/1770 train_time:7136ms step_avg:93.90ms
step:87/1770 train_time:7230ms step_avg:93.90ms
step:88/1770 train_time:7324ms step_avg:93.90ms
step:89/1770 train_time:7418ms step_avg:93.90ms
step:90/1770 train_time:7512ms step_avg:93.90ms
step:91/1770 train_time:7605ms step_avg:93.89ms
step:92/1770 train_time:7700ms step_avg:93.90ms
step:93/1770 train_time:7794ms step_avg:93.90ms
step:94/1770 train_time:7888ms step_avg:93.90ms
step:95/1770 train_time:7982ms step_avg:93.90ms
step:96/1770 train_time:8076ms step_avg:93.90ms
step:97/1770 train_time:8170ms step_avg:93.91ms
step:98/1770 train_time:8264ms step_avg:93.91ms
step:99/1770 train_time:8358ms step_avg:93.91ms
step:100/1770 train_time:8452ms step_avg:93.91ms
step:101/1770 train_time:8545ms step_avg:93.90ms
step:102/1770 train_time:8639ms step_avg:93.90ms
step:103/1770 train_time:8733ms step_avg:93.90ms
step:104/1770 train_time:8827ms step_avg:93.90ms
step:105/1770 train_time:8920ms step_avg:93.90ms
step:106/1770 train_time:9014ms step_avg:93.90ms
step:107/1770 train_time:9108ms step_avg:93.89ms
step:108/1770 train_time:9202ms step_avg:93.90ms
step:109/1770 train_time:9296ms step_avg:93.90ms
step:110/1770 train_time:9390ms step_avg:93.90ms
step:111/1770 train_time:9484ms step_avg:93.90ms
step:112/1770 train_time:9578ms step_avg:93.90ms
step:113/1770 train_time:9672ms step_avg:93.90ms
step:114/1770 train_time:9766ms step_avg:93.90ms
step:115/1770 train_time:9860ms step_avg:93.90ms
step:116/1770 train_time:9953ms step_avg:93.90ms
step:117/1770 train_time:10048ms step_avg:93.90ms
step:118/1770 train_time:10141ms step_avg:93.90ms
step:119/1770 train_time:10235ms step_avg:93.90ms
step:120/1770 train_time:10329ms step_avg:93.90ms
step:121/1770 train_time:10423ms step_avg:93.90ms
step:122/1770 train_time:10517ms step_avg:93.90ms
step:123/1770 train_time:10611ms step_avg:93.90ms
step:124/1770 train_time:10705ms step_avg:93.90ms
step:125/1770 train_time:10798ms step_avg:93.90ms
step:125/1770 val_loss:4.6526 train_time:10890ms step_avg:94.70ms
step:126/1770 train_time:10912ms step_avg:94.07ms
step:127/1770 train_time:10990ms step_avg:93.93ms
step:128/1770 train_time:11088ms step_avg:93.96ms
step:129/1770 train_time:11187ms step_avg:94.01ms
step:130/1770 train_time:11284ms step_avg:94.03ms
step:131/1770 train_time:11378ms step_avg:94.03ms
step:132/1770 train_time:11471ms step_avg:94.03ms
step:133/1770 train_time:11565ms step_avg:94.02ms
step:134/1770 train_time:11659ms step_avg:94.02ms
step:135/1770 train_time:11753ms step_avg:94.02ms
step:136/1770 train_time:11847ms step_avg:94.02ms
step:137/1770 train_time:11941ms step_avg:94.02ms
step:138/1770 train_time:12035ms step_avg:94.03ms
step:139/1770 train_time:12130ms step_avg:94.03ms
step:140/1770 train_time:12226ms step_avg:94.04ms
step:141/1770 train_time:12321ms step_avg:94.05ms
step:142/1770 train_time:12415ms step_avg:94.05ms
step:143/1770 train_time:12509ms step_avg:94.05ms
step:144/1770 train_time:12603ms step_avg:94.06ms
step:145/1770 train_time:12697ms step_avg:94.05ms
step:146/1770 train_time:12792ms step_avg:94.06ms
step:147/1770 train_time:12886ms step_avg:94.06ms
step:148/1770 train_time:12980ms step_avg:94.06ms
step:149/1770 train_time:13075ms step_avg:94.06ms
step:150/1770 train_time:13170ms step_avg:94.07ms
step:151/1770 train_time:13265ms step_avg:94.08ms
step:152/1770 train_time:13359ms step_avg:94.08ms
step:153/1770 train_time:13453ms step_avg:94.08ms
step:154/1770 train_time:13548ms step_avg:94.08ms
step:155/1770 train_time:13643ms step_avg:94.09ms
step:156/1770 train_time:13737ms step_avg:94.09ms
step:157/1770 train_time:13831ms step_avg:94.09ms
step:158/1770 train_time:13925ms step_avg:94.09ms
step:159/1770 train_time:14020ms step_avg:94.09ms
step:160/1770 train_time:14114ms step_avg:94.09ms
step:161/1770 train_time:14209ms step_avg:94.10ms
step:162/1770 train_time:14303ms step_avg:94.10ms
step:163/1770 train_time:14398ms step_avg:94.10ms
step:164/1770 train_time:14492ms step_avg:94.11ms
step:165/1770 train_time:14586ms step_avg:94.10ms
step:166/1770 train_time:14681ms step_avg:94.11ms
step:167/1770 train_time:14776ms step_avg:94.12ms
step:168/1770 train_time:14871ms step_avg:94.12ms
step:169/1770 train_time:14965ms step_avg:94.12ms
step:170/1770 train_time:15060ms step_avg:94.12ms
step:171/1770 train_time:15154ms step_avg:94.12ms
step:172/1770 train_time:15248ms step_avg:94.12ms
step:173/1770 train_time:15342ms step_avg:94.12ms
step:174/1770 train_time:15436ms step_avg:94.12ms
step:175/1770 train_time:15532ms step_avg:94.13ms
step:176/1770 train_time:15627ms step_avg:94.14ms
step:177/1770 train_time:15721ms step_avg:94.14ms
step:178/1770 train_time:15816ms step_avg:94.14ms
step:179/1770 train_time:15910ms step_avg:94.14ms
step:180/1770 train_time:16005ms step_avg:94.15ms
step:181/1770 train_time:16100ms step_avg:94.15ms
step:182/1770 train_time:16196ms step_avg:94.16ms
step:183/1770 train_time:16290ms step_avg:94.16ms
step:184/1770 train_time:16385ms step_avg:94.16ms
step:185/1770 train_time:16480ms step_avg:94.17ms
step:186/1770 train_time:16574ms step_avg:94.17ms
step:187/1770 train_time:16668ms step_avg:94.17ms
step:188/1770 train_time:16763ms step_avg:94.17ms
step:189/1770 train_time:16858ms step_avg:94.18ms
step:190/1770 train_time:16952ms step_avg:94.18ms
step:191/1770 train_time:17047ms step_avg:94.18ms
step:192/1770 train_time:17142ms step_avg:94.19ms
step:193/1770 train_time:17236ms step_avg:94.19ms
step:194/1770 train_time:17330ms step_avg:94.19ms
step:195/1770 train_time:17425ms step_avg:94.19ms
step:196/1770 train_time:17520ms step_avg:94.19ms
step:197/1770 train_time:17614ms step_avg:94.19ms
step:198/1770 train_time:17709ms step_avg:94.19ms
step:199/1770 train_time:17803ms step_avg:94.19ms
step:200/1770 train_time:17898ms step_avg:94.20ms
step:201/1770 train_time:17992ms step_avg:94.20ms
step:202/1770 train_time:18086ms step_avg:94.20ms
step:203/1770 train_time:18180ms step_avg:94.20ms
step:204/1770 train_time:18275ms step_avg:94.20ms
step:205/1770 train_time:18369ms step_avg:94.20ms
step:206/1770 train_time:18463ms step_avg:94.20ms
step:207/1770 train_time:18558ms step_avg:94.20ms
step:208/1770 train_time:18653ms step_avg:94.21ms
step:209/1770 train_time:18748ms step_avg:94.21ms
step:210/1770 train_time:18842ms step_avg:94.21ms
step:211/1770 train_time:18936ms step_avg:94.21ms
step:212/1770 train_time:19032ms step_avg:94.22ms
step:213/1770 train_time:19128ms step_avg:94.23ms
step:214/1770 train_time:19222ms step_avg:94.23ms
step:215/1770 train_time:19316ms step_avg:94.22ms
step:216/1770 train_time:19411ms step_avg:94.23ms
step:217/1770 train_time:19506ms step_avg:94.23ms
step:218/1770 train_time:19600ms step_avg:94.23ms
step:219/1770 train_time:19695ms step_avg:94.23ms
step:220/1770 train_time:19789ms step_avg:94.23ms
step:221/1770 train_time:19883ms step_avg:94.23ms
step:222/1770 train_time:19977ms step_avg:94.23ms
step:223/1770 train_time:20072ms step_avg:94.23ms
step:224/1770 train_time:20166ms step_avg:94.23ms
step:225/1770 train_time:20260ms step_avg:94.23ms
step:226/1770 train_time:20355ms step_avg:94.24ms
step:227/1770 train_time:20449ms step_avg:94.24ms
step:228/1770 train_time:20544ms step_avg:94.24ms
step:229/1770 train_time:20639ms step_avg:94.24ms
step:230/1770 train_time:20733ms step_avg:94.24ms
step:231/1770 train_time:20828ms step_avg:94.24ms
step:232/1770 train_time:20923ms step_avg:94.25ms
step:233/1770 train_time:21017ms step_avg:94.25ms
step:234/1770 train_time:21111ms step_avg:94.25ms
step:235/1770 train_time:21206ms step_avg:94.25ms
step:236/1770 train_time:21301ms step_avg:94.25ms
step:237/1770 train_time:21395ms step_avg:94.25ms
step:238/1770 train_time:21490ms step_avg:94.25ms
step:239/1770 train_time:21584ms step_avg:94.25ms
step:240/1770 train_time:21678ms step_avg:94.25ms
step:241/1770 train_time:21773ms step_avg:94.26ms
step:242/1770 train_time:21868ms step_avg:94.26ms
step:243/1770 train_time:21962ms step_avg:94.26ms
step:244/1770 train_time:22057ms step_avg:94.26ms
step:245/1770 train_time:22151ms step_avg:94.26ms
step:246/1770 train_time:22245ms step_avg:94.26ms
step:247/1770 train_time:22340ms step_avg:94.26ms
step:248/1770 train_time:22434ms step_avg:94.26ms
step:249/1770 train_time:22530ms step_avg:94.27ms
step:250/1770 train_time:22624ms step_avg:94.27ms
step:250/1770 val_loss:4.1053 train_time:22717ms step_avg:94.65ms
step:251/1770 train_time:22739ms step_avg:94.35ms
step:252/1770 train_time:22822ms step_avg:94.31ms
step:253/1770 train_time:22917ms step_avg:94.31ms
step:254/1770 train_time:23012ms step_avg:94.31ms
step:255/1770 train_time:23106ms step_avg:94.31ms
step:256/1770 train_time:23200ms step_avg:94.31ms
step:257/1770 train_time:23295ms step_avg:94.31ms
step:258/1770 train_time:23388ms step_avg:94.31ms
step:259/1770 train_time:23482ms step_avg:94.30ms
step:260/1770 train_time:23576ms step_avg:94.30ms
step:261/1770 train_time:23670ms step_avg:94.30ms
step:262/1770 train_time:23766ms step_avg:94.31ms
step:263/1770 train_time:23861ms step_avg:94.31ms
step:264/1770 train_time:23956ms step_avg:94.32ms
step:265/1770 train_time:24052ms step_avg:94.32ms
step:266/1770 train_time:24150ms step_avg:94.34ms
step:267/1770 train_time:24242ms step_avg:94.33ms
step:268/1770 train_time:24336ms step_avg:94.33ms
step:269/1770 train_time:24431ms step_avg:94.33ms
step:270/1770 train_time:24525ms step_avg:94.33ms
step:271/1770 train_time:24620ms step_avg:94.33ms
step:272/1770 train_time:24715ms step_avg:94.33ms
step:273/1770 train_time:24810ms step_avg:94.34ms
step:274/1770 train_time:24906ms step_avg:94.34ms
step:275/1770 train_time:25001ms step_avg:94.34ms
step:276/1770 train_time:25097ms step_avg:94.35ms
step:277/1770 train_time:25192ms step_avg:94.35ms
step:278/1770 train_time:25287ms step_avg:94.35ms
step:279/1770 train_time:25381ms step_avg:94.35ms
step:280/1770 train_time:25476ms step_avg:94.36ms
step:281/1770 train_time:25571ms step_avg:94.36ms
step:282/1770 train_time:25666ms step_avg:94.36ms
step:283/1770 train_time:25761ms step_avg:94.36ms
step:284/1770 train_time:25856ms step_avg:94.37ms
step:285/1770 train_time:25955ms step_avg:94.38ms
step:286/1770 train_time:26047ms step_avg:94.37ms
step:287/1770 train_time:26142ms step_avg:94.38ms
step:288/1770 train_time:26237ms step_avg:94.38ms
step:289/1770 train_time:26332ms step_avg:94.38ms
step:290/1770 train_time:26427ms step_avg:94.38ms
step:291/1770 train_time:26523ms step_avg:94.39ms
step:292/1770 train_time:26617ms step_avg:94.39ms
step:293/1770 train_time:26712ms step_avg:94.39ms
step:294/1770 train_time:26808ms step_avg:94.39ms
step:295/1770 train_time:26903ms step_avg:94.40ms
step:296/1770 train_time:26999ms step_avg:94.40ms
step:297/1770 train_time:27094ms step_avg:94.40ms
step:298/1770 train_time:27188ms step_avg:94.40ms
step:299/1770 train_time:27283ms step_avg:94.41ms
step:300/1770 train_time:27379ms step_avg:94.41ms
step:301/1770 train_time:27474ms step_avg:94.41ms
step:302/1770 train_time:27568ms step_avg:94.41ms
step:303/1770 train_time:27665ms step_avg:94.42ms
step:304/1770 train_time:27758ms step_avg:94.42ms
step:305/1770 train_time:27854ms step_avg:94.42ms
step:306/1770 train_time:27948ms step_avg:94.42ms
step:307/1770 train_time:28043ms step_avg:94.42ms
step:308/1770 train_time:28139ms step_avg:94.43ms
step:309/1770 train_time:28234ms step_avg:94.43ms
step:310/1770 train_time:28329ms step_avg:94.43ms
step:311/1770 train_time:28424ms step_avg:94.43ms
step:312/1770 train_time:28519ms step_avg:94.43ms
step:313/1770 train_time:28613ms step_avg:94.43ms
step:314/1770 train_time:28708ms step_avg:94.43ms
step:315/1770 train_time:28803ms step_avg:94.44ms
step:316/1770 train_time:28898ms step_avg:94.44ms
step:317/1770 train_time:28993ms step_avg:94.44ms
step:318/1770 train_time:29088ms step_avg:94.44ms
step:319/1770 train_time:29183ms step_avg:94.44ms
step:320/1770 train_time:29279ms step_avg:94.45ms
step:321/1770 train_time:29374ms step_avg:94.45ms
step:322/1770 train_time:29469ms step_avg:94.45ms
step:323/1770 train_time:29563ms step_avg:94.45ms
step:324/1770 train_time:29658ms step_avg:94.45ms
step:325/1770 train_time:29754ms step_avg:94.46ms
step:326/1770 train_time:29849ms step_avg:94.46ms
step:327/1770 train_time:29944ms step_avg:94.46ms
step:328/1770 train_time:30038ms step_avg:94.46ms
step:329/1770 train_time:30133ms step_avg:94.46ms
step:330/1770 train_time:30230ms step_avg:94.47ms
step:331/1770 train_time:30325ms step_avg:94.47ms
step:332/1770 train_time:30420ms step_avg:94.47ms
step:333/1770 train_time:30516ms step_avg:94.48ms
step:334/1770 train_time:30611ms step_avg:94.48ms
step:335/1770 train_time:30706ms step_avg:94.48ms
step:336/1770 train_time:30801ms step_avg:94.48ms
step:337/1770 train_time:30896ms step_avg:94.48ms
step:338/1770 train_time:30991ms step_avg:94.48ms
step:339/1770 train_time:31086ms step_avg:94.49ms
step:340/1770 train_time:31180ms step_avg:94.49ms
step:341/1770 train_time:31276ms step_avg:94.49ms
step:342/1770 train_time:31370ms step_avg:94.49ms
step:343/1770 train_time:31466ms step_avg:94.49ms
step:344/1770 train_time:31560ms step_avg:94.49ms
step:345/1770 train_time:31655ms step_avg:94.49ms
step:346/1770 train_time:31750ms step_avg:94.49ms
step:347/1770 train_time:31845ms step_avg:94.50ms
step:348/1770 train_time:31940ms step_avg:94.50ms
step:349/1770 train_time:32035ms step_avg:94.50ms
step:350/1770 train_time:32130ms step_avg:94.50ms
step:351/1770 train_time:32224ms step_avg:94.50ms
step:352/1770 train_time:32319ms step_avg:94.50ms
step:353/1770 train_time:32415ms step_avg:94.50ms
step:354/1770 train_time:32509ms step_avg:94.50ms
step:355/1770 train_time:32604ms step_avg:94.51ms
step:356/1770 train_time:32699ms step_avg:94.51ms
step:357/1770 train_time:32795ms step_avg:94.51ms
step:358/1770 train_time:32890ms step_avg:94.51ms
step:359/1770 train_time:32984ms step_avg:94.51ms
step:360/1770 train_time:33079ms step_avg:94.51ms
step:361/1770 train_time:33175ms step_avg:94.52ms
step:362/1770 train_time:33270ms step_avg:94.52ms
step:363/1770 train_time:33364ms step_avg:94.52ms
step:364/1770 train_time:33459ms step_avg:94.52ms
step:365/1770 train_time:33555ms step_avg:94.52ms
step:366/1770 train_time:33649ms step_avg:94.52ms
step:367/1770 train_time:33744ms step_avg:94.52ms
step:368/1770 train_time:33839ms step_avg:94.52ms
step:369/1770 train_time:33935ms step_avg:94.53ms
step:370/1770 train_time:34030ms step_avg:94.53ms
step:371/1770 train_time:34126ms step_avg:94.53ms
step:372/1770 train_time:34220ms step_avg:94.53ms
step:373/1770 train_time:34315ms step_avg:94.53ms
step:374/1770 train_time:34410ms step_avg:94.53ms
step:375/1770 train_time:34506ms step_avg:94.54ms
step:375/1770 val_loss:3.8999 train_time:34598ms step_avg:94.79ms
step:376/1770 train_time:34621ms step_avg:94.59ms
step:377/1770 train_time:34703ms step_avg:94.56ms
step:378/1770 train_time:34801ms step_avg:94.57ms
step:379/1770 train_time:34896ms step_avg:94.57ms
step:380/1770 train_time:34991ms step_avg:94.57ms
step:381/1770 train_time:35090ms step_avg:94.58ms
step:382/1770 train_time:35180ms step_avg:94.57ms
step:383/1770 train_time:35275ms step_avg:94.57ms
step:384/1770 train_time:35369ms step_avg:94.57ms
step:385/1770 train_time:35463ms step_avg:94.57ms
step:386/1770 train_time:35558ms step_avg:94.57ms
step:387/1770 train_time:35653ms step_avg:94.57ms
step:388/1770 train_time:35750ms step_avg:94.58ms
step:389/1770 train_time:35845ms step_avg:94.58ms
step:390/1770 train_time:35940ms step_avg:94.58ms
step:391/1770 train_time:36035ms step_avg:94.58ms
step:392/1770 train_time:36130ms step_avg:94.58ms
step:393/1770 train_time:36224ms step_avg:94.58ms
step:394/1770 train_time:36319ms step_avg:94.58ms
step:395/1770 train_time:36413ms step_avg:94.58ms
step:396/1770 train_time:36510ms step_avg:94.59ms
step:397/1770 train_time:36607ms step_avg:94.59ms
step:398/1770 train_time:36704ms step_avg:94.60ms
step:399/1770 train_time:36801ms step_avg:94.60ms
step:400/1770 train_time:36898ms step_avg:94.61ms
step:401/1770 train_time:36995ms step_avg:94.62ms
step:402/1770 train_time:37091ms step_avg:94.62ms
step:403/1770 train_time:37188ms step_avg:94.63ms
step:404/1770 train_time:37285ms step_avg:94.63ms
step:405/1770 train_time:37384ms step_avg:94.64ms
step:406/1770 train_time:37479ms step_avg:94.64ms
step:407/1770 train_time:37575ms step_avg:94.65ms
step:408/1770 train_time:37672ms step_avg:94.65ms
step:409/1770 train_time:37769ms step_avg:94.66ms
step:410/1770 train_time:37865ms step_avg:94.66ms
step:411/1770 train_time:37963ms step_avg:94.67ms
step:412/1770 train_time:38060ms step_avg:94.68ms
step:413/1770 train_time:38156ms step_avg:94.68ms
step:414/1770 train_time:38253ms step_avg:94.69ms
step:415/1770 train_time:38350ms step_avg:94.69ms
step:416/1770 train_time:38447ms step_avg:94.70ms
step:417/1770 train_time:38544ms step_avg:94.70ms
step:418/1770 train_time:38640ms step_avg:94.71ms
step:419/1770 train_time:38737ms step_avg:94.71ms
step:420/1770 train_time:38834ms step_avg:94.72ms
step:421/1770 train_time:38931ms step_avg:94.72ms
step:422/1770 train_time:39028ms step_avg:94.73ms
step:423/1770 train_time:39125ms step_avg:94.73ms
step:424/1770 train_time:39222ms step_avg:94.74ms
step:425/1770 train_time:39319ms step_avg:94.74ms
step:426/1770 train_time:39415ms step_avg:94.75ms
step:427/1770 train_time:39513ms step_avg:94.75ms
step:428/1770 train_time:39610ms step_avg:94.76ms
step:429/1770 train_time:39706ms step_avg:94.76ms
step:430/1770 train_time:39803ms step_avg:94.77ms
step:431/1770 train_time:39900ms step_avg:94.77ms
step:432/1770 train_time:39997ms step_avg:94.78ms
step:433/1770 train_time:40094ms step_avg:94.78ms
step:434/1770 train_time:40191ms step_avg:94.79ms
step:435/1770 train_time:40288ms step_avg:94.80ms
step:436/1770 train_time:40385ms step_avg:94.80ms
step:437/1770 train_time:40481ms step_avg:94.80ms
step:438/1770 train_time:40578ms step_avg:94.81ms
step:439/1770 train_time:40675ms step_avg:94.81ms
step:440/1770 train_time:40771ms step_avg:94.82ms
step:441/1770 train_time:40869ms step_avg:94.82ms
step:442/1770 train_time:40965ms step_avg:94.83ms
step:443/1770 train_time:41062ms step_avg:94.83ms
step:444/1770 train_time:41158ms step_avg:94.83ms
step:445/1770 train_time:41255ms step_avg:94.84ms
step:446/1770 train_time:41352ms step_avg:94.84ms
step:447/1770 train_time:41449ms step_avg:94.85ms
step:448/1770 train_time:41546ms step_avg:94.85ms
step:449/1770 train_time:41642ms step_avg:94.86ms
step:450/1770 train_time:41739ms step_avg:94.86ms
step:451/1770 train_time:41836ms step_avg:94.87ms
step:452/1770 train_time:41933ms step_avg:94.87ms
step:453/1770 train_time:42029ms step_avg:94.87ms
step:454/1770 train_time:42126ms step_avg:94.88ms
step:455/1770 train_time:42223ms step_avg:94.88ms
step:456/1770 train_time:42320ms step_avg:94.89ms
step:457/1770 train_time:42417ms step_avg:94.89ms
step:458/1770 train_time:42514ms step_avg:94.90ms
step:459/1770 train_time:42610ms step_avg:94.90ms
step:460/1770 train_time:42707ms step_avg:94.91ms
step:461/1770 train_time:42804ms step_avg:94.91ms
step:462/1770 train_time:42900ms step_avg:94.91ms
step:463/1770 train_time:42997ms step_avg:94.92ms
step:464/1770 train_time:43094ms step_avg:94.92ms
step:465/1770 train_time:43191ms step_avg:94.93ms
step:466/1770 train_time:43289ms step_avg:94.93ms
step:467/1770 train_time:43387ms step_avg:94.94ms
step:468/1770 train_time:43483ms step_avg:94.94ms
step:469/1770 train_time:43579ms step_avg:94.94ms
step:470/1770 train_time:43676ms step_avg:94.95ms
step:471/1770 train_time:43773ms step_avg:94.95ms
step:472/1770 train_time:43870ms step_avg:94.96ms
step:473/1770 train_time:43966ms step_avg:94.96ms
step:474/1770 train_time:44063ms step_avg:94.96ms
step:475/1770 train_time:44160ms step_avg:94.97ms
step:476/1770 train_time:44257ms step_avg:94.97ms
step:477/1770 train_time:44353ms step_avg:94.98ms
step:478/1770 train_time:44451ms step_avg:94.98ms
step:479/1770 train_time:44548ms step_avg:94.98ms
step:480/1770 train_time:44644ms step_avg:94.99ms
step:481/1770 train_time:44741ms step_avg:94.99ms
step:482/1770 train_time:44838ms step_avg:95.00ms
step:483/1770 train_time:44935ms step_avg:95.00ms
step:484/1770 train_time:45032ms step_avg:95.00ms
step:485/1770 train_time:45129ms step_avg:95.01ms
step:486/1770 train_time:45226ms step_avg:95.01ms
step:487/1770 train_time:45322ms step_avg:95.01ms
step:488/1770 train_time:45419ms step_avg:95.02ms
step:489/1770 train_time:45516ms step_avg:95.02ms
step:490/1770 train_time:45612ms step_avg:95.03ms
step:491/1770 train_time:45710ms step_avg:95.03ms
step:492/1770 train_time:45808ms step_avg:95.04ms
step:493/1770 train_time:45903ms step_avg:95.04ms
step:494/1770 train_time:46000ms step_avg:95.04ms
step:495/1770 train_time:46097ms step_avg:95.05ms
step:496/1770 train_time:46194ms step_avg:95.05ms
step:497/1770 train_time:46291ms step_avg:95.05ms
step:498/1770 train_time:46387ms step_avg:95.06ms
step:499/1770 train_time:46484ms step_avg:95.06ms
step:500/1770 train_time:46581ms step_avg:95.06ms
step:500/1770 val_loss:3.7521 train_time:46676ms step_avg:95.26ms
step:501/1770 train_time:46698ms step_avg:95.11ms
step:502/1770 train_time:46786ms step_avg:95.09ms
step:503/1770 train_time:46884ms step_avg:95.10ms
step:504/1770 train_time:46982ms step_avg:95.10ms
step:505/1770 train_time:47078ms step_avg:95.11ms
step:506/1770 train_time:47174ms step_avg:95.11ms
step:507/1770 train_time:47271ms step_avg:95.11ms
step:508/1770 train_time:47368ms step_avg:95.12ms
step:509/1770 train_time:47464ms step_avg:95.12ms
step:510/1770 train_time:47560ms step_avg:95.12ms
step:511/1770 train_time:47657ms step_avg:95.12ms
step:512/1770 train_time:47754ms step_avg:95.13ms
step:513/1770 train_time:47852ms step_avg:95.13ms
step:514/1770 train_time:47950ms step_avg:95.14ms
step:515/1770 train_time:48048ms step_avg:95.14ms
step:516/1770 train_time:48144ms step_avg:95.15ms
step:517/1770 train_time:48242ms step_avg:95.15ms
step:518/1770 train_time:48339ms step_avg:95.16ms
step:519/1770 train_time:48435ms step_avg:95.16ms
step:520/1770 train_time:48532ms step_avg:95.16ms
step:521/1770 train_time:48629ms step_avg:95.16ms
step:522/1770 train_time:48726ms step_avg:95.17ms
step:523/1770 train_time:48824ms step_avg:95.17ms
step:524/1770 train_time:48921ms step_avg:95.18ms
step:525/1770 train_time:49018ms step_avg:95.18ms
step:526/1770 train_time:49115ms step_avg:95.18ms
step:527/1770 train_time:49212ms step_avg:95.19ms
step:528/1770 train_time:49309ms step_avg:95.19ms
step:529/1770 train_time:49406ms step_avg:95.20ms
step:530/1770 train_time:49503ms step_avg:95.20ms
step:531/1770 train_time:49601ms step_avg:95.20ms
step:532/1770 train_time:49698ms step_avg:95.21ms
step:533/1770 train_time:49795ms step_avg:95.21ms
step:534/1770 train_time:49892ms step_avg:95.21ms
step:535/1770 train_time:49990ms step_avg:95.22ms
step:536/1770 train_time:50087ms step_avg:95.22ms
step:537/1770 train_time:50184ms step_avg:95.23ms
step:538/1770 train_time:50281ms step_avg:95.23ms
step:539/1770 train_time:50379ms step_avg:95.23ms
step:540/1770 train_time:50476ms step_avg:95.24ms
step:541/1770 train_time:50573ms step_avg:95.24ms
step:542/1770 train_time:50670ms step_avg:95.24ms
step:543/1770 train_time:50768ms step_avg:95.25ms
step:544/1770 train_time:50865ms step_avg:95.25ms
step:545/1770 train_time:50962ms step_avg:95.26ms
step:546/1770 train_time:51060ms step_avg:95.26ms
step:547/1770 train_time:51157ms step_avg:95.26ms
step:548/1770 train_time:51253ms step_avg:95.27ms
step:549/1770 train_time:51351ms step_avg:95.27ms
step:550/1770 train_time:51448ms step_avg:95.27ms
step:551/1770 train_time:51546ms step_avg:95.28ms
step:552/1770 train_time:51643ms step_avg:95.28ms
step:553/1770 train_time:51740ms step_avg:95.29ms
step:554/1770 train_time:51837ms step_avg:95.29ms
step:555/1770 train_time:51934ms step_avg:95.29ms
step:556/1770 train_time:52031ms step_avg:95.30ms
step:557/1770 train_time:52129ms step_avg:95.30ms
step:558/1770 train_time:52227ms step_avg:95.30ms
step:559/1770 train_time:52324ms step_avg:95.31ms
step:560/1770 train_time:52421ms step_avg:95.31ms
step:561/1770 train_time:52519ms step_avg:95.32ms
step:562/1770 train_time:52615ms step_avg:95.32ms
step:563/1770 train_time:52712ms step_avg:95.32ms
step:564/1770 train_time:52810ms step_avg:95.32ms
step:565/1770 train_time:52908ms step_avg:95.33ms
step:566/1770 train_time:53005ms step_avg:95.33ms
step:567/1770 train_time:53102ms step_avg:95.34ms
step:568/1770 train_time:53199ms step_avg:95.34ms
step:569/1770 train_time:53297ms step_avg:95.34ms
step:570/1770 train_time:53394ms step_avg:95.35ms
step:571/1770 train_time:53491ms step_avg:95.35ms
step:572/1770 train_time:53588ms step_avg:95.35ms
step:573/1770 train_time:53686ms step_avg:95.36ms
step:574/1770 train_time:53783ms step_avg:95.36ms
step:575/1770 train_time:53880ms step_avg:95.36ms
step:576/1770 train_time:53977ms step_avg:95.37ms
step:577/1770 train_time:54074ms step_avg:95.37ms
step:578/1770 train_time:54171ms step_avg:95.37ms
step:579/1770 train_time:54269ms step_avg:95.38ms
step:580/1770 train_time:54366ms step_avg:95.38ms
step:581/1770 train_time:54463ms step_avg:95.38ms
step:582/1770 train_time:54560ms step_avg:95.39ms
step:583/1770 train_time:54658ms step_avg:95.39ms
step:584/1770 train_time:54755ms step_avg:95.39ms
step:585/1770 train_time:54852ms step_avg:95.39ms
step:586/1770 train_time:54950ms step_avg:95.40ms
step:587/1770 train_time:55047ms step_avg:95.40ms
step:588/1770 train_time:55144ms step_avg:95.41ms
step:589/1770 train_time:55241ms step_avg:95.41ms
step:590/1770 train_time:55339ms step_avg:95.41ms
step:591/1770 train_time:55436ms step_avg:95.41ms
step:592/1770 train_time:55534ms step_avg:95.42ms
step:593/1770 train_time:55629ms step_avg:95.42ms
step:594/1770 train_time:55727ms step_avg:95.42ms
step:595/1770 train_time:55824ms step_avg:95.43ms
step:596/1770 train_time:55921ms step_avg:95.43ms
step:597/1770 train_time:56018ms step_avg:95.43ms
step:598/1770 train_time:56115ms step_avg:95.43ms
step:599/1770 train_time:56212ms step_avg:95.44ms
step:600/1770 train_time:56310ms step_avg:95.44ms
step:601/1770 train_time:56408ms step_avg:95.44ms
step:602/1770 train_time:56505ms step_avg:95.45ms
step:603/1770 train_time:56602ms step_avg:95.45ms
step:604/1770 train_time:56698ms step_avg:95.45ms
step:605/1770 train_time:56796ms step_avg:95.45ms
step:606/1770 train_time:56892ms step_avg:95.46ms
step:607/1770 train_time:56990ms step_avg:95.46ms
step:608/1770 train_time:57087ms step_avg:95.46ms
step:609/1770 train_time:57184ms step_avg:95.47ms
step:610/1770 train_time:57281ms step_avg:95.47ms
step:611/1770 train_time:57379ms step_avg:95.47ms
step:612/1770 train_time:57476ms step_avg:95.48ms
step:613/1770 train_time:57573ms step_avg:95.48ms
step:614/1770 train_time:57671ms step_avg:95.48ms
step:615/1770 train_time:57769ms step_avg:95.49ms
step:616/1770 train_time:57866ms step_avg:95.49ms
step:617/1770 train_time:57963ms step_avg:95.49ms
step:618/1770 train_time:58059ms step_avg:95.49ms
step:619/1770 train_time:58157ms step_avg:95.50ms
step:620/1770 train_time:58254ms step_avg:95.50ms
step:621/1770 train_time:58351ms step_avg:95.50ms
step:622/1770 train_time:58449ms step_avg:95.50ms
step:623/1770 train_time:58546ms step_avg:95.51ms
step:624/1770 train_time:58644ms step_avg:95.51ms
step:625/1770 train_time:58741ms step_avg:95.51ms
step:625/1770 val_loss:3.6663 train_time:58837ms step_avg:95.67ms
step:626/1770 train_time:58859ms step_avg:95.55ms
step:627/1770 train_time:58944ms step_avg:95.53ms
step:628/1770 train_time:59044ms step_avg:95.54ms
step:629/1770 train_time:59142ms step_avg:95.54ms
step:630/1770 train_time:59239ms step_avg:95.55ms
step:631/1770 train_time:59336ms step_avg:95.55ms
step:632/1770 train_time:59433ms step_avg:95.55ms
step:633/1770 train_time:59529ms step_avg:95.55ms
step:634/1770 train_time:59626ms step_avg:95.55ms
step:635/1770 train_time:59723ms step_avg:95.56ms
step:636/1770 train_time:59821ms step_avg:95.56ms
step:637/1770 train_time:59919ms step_avg:95.56ms
step:638/1770 train_time:60018ms step_avg:95.57ms
step:639/1770 train_time:60115ms step_avg:95.57ms
step:640/1770 train_time:60212ms step_avg:95.58ms
step:641/1770 train_time:60310ms step_avg:95.58ms
step:642/1770 train_time:60407ms step_avg:95.58ms
step:643/1770 train_time:60504ms step_avg:95.58ms
step:644/1770 train_time:60601ms step_avg:95.58ms
step:645/1770 train_time:60698ms step_avg:95.59ms
step:646/1770 train_time:60795ms step_avg:95.59ms
step:647/1770 train_time:60893ms step_avg:95.59ms
step:648/1770 train_time:60991ms step_avg:95.60ms
step:649/1770 train_time:61089ms step_avg:95.60ms
step:650/1770 train_time:61186ms step_avg:95.60ms
step:651/1770 train_time:61283ms step_avg:95.61ms
step:652/1770 train_time:61380ms step_avg:95.61ms
step:653/1770 train_time:61478ms step_avg:95.61ms
step:654/1770 train_time:61575ms step_avg:95.61ms
step:655/1770 train_time:61672ms step_avg:95.62ms
step:656/1770 train_time:61770ms step_avg:95.62ms
step:657/1770 train_time:61867ms step_avg:95.62ms
step:658/1770 train_time:61967ms step_avg:95.63ms
step:659/1770 train_time:62066ms step_avg:95.63ms
step:660/1770 train_time:62165ms step_avg:95.64ms
step:661/1770 train_time:62264ms step_avg:95.64ms
step:662/1770 train_time:62363ms step_avg:95.65ms
step:663/1770 train_time:62463ms step_avg:95.66ms
step:664/1770 train_time:62562ms step_avg:95.66ms
step:665/1770 train_time:62661ms step_avg:95.67ms
step:666/1770 train_time:62760ms step_avg:95.67ms
step:667/1770 train_time:62860ms step_avg:95.68ms
step:668/1770 train_time:62961ms step_avg:95.68ms
step:669/1770 train_time:63060ms step_avg:95.69ms
step:670/1770 train_time:63160ms step_avg:95.70ms
step:671/1770 train_time:63260ms step_avg:95.70ms
step:672/1770 train_time:63359ms step_avg:95.71ms
step:673/1770 train_time:63458ms step_avg:95.71ms
step:674/1770 train_time:63557ms step_avg:95.72ms
step:675/1770 train_time:63655ms step_avg:95.72ms
step:676/1770 train_time:63754ms step_avg:95.73ms
step:677/1770 train_time:63852ms step_avg:95.73ms
step:678/1770 train_time:63952ms step_avg:95.74ms
step:679/1770 train_time:64051ms step_avg:95.74ms
step:680/1770 train_time:64151ms step_avg:95.75ms
step:681/1770 train_time:64250ms step_avg:95.75ms
step:682/1770 train_time:64349ms step_avg:95.76ms
step:683/1770 train_time:64450ms step_avg:95.76ms
step:684/1770 train_time:64549ms step_avg:95.77ms
step:685/1770 train_time:64648ms step_avg:95.77ms
step:686/1770 train_time:64746ms step_avg:95.78ms
step:687/1770 train_time:64845ms step_avg:95.78ms
step:688/1770 train_time:64944ms step_avg:95.79ms
step:689/1770 train_time:65043ms step_avg:95.79ms
step:690/1770 train_time:65142ms step_avg:95.80ms
step:691/1770 train_time:65241ms step_avg:95.80ms
step:692/1770 train_time:65341ms step_avg:95.81ms
step:693/1770 train_time:65441ms step_avg:95.81ms
step:694/1770 train_time:65541ms step_avg:95.82ms
step:695/1770 train_time:65641ms step_avg:95.83ms
step:696/1770 train_time:65739ms step_avg:95.83ms
step:697/1770 train_time:65839ms step_avg:95.84ms
step:698/1770 train_time:65938ms step_avg:95.84ms
step:699/1770 train_time:66037ms step_avg:95.84ms
step:700/1770 train_time:66138ms step_avg:95.85ms
step:701/1770 train_time:66234ms step_avg:95.85ms
step:702/1770 train_time:66334ms step_avg:95.86ms
step:703/1770 train_time:66432ms step_avg:95.86ms
step:704/1770 train_time:66532ms step_avg:95.87ms
step:705/1770 train_time:66630ms step_avg:95.87ms
step:706/1770 train_time:66729ms step_avg:95.88ms
step:707/1770 train_time:66829ms step_avg:95.88ms
step:708/1770 train_time:66928ms step_avg:95.89ms
step:709/1770 train_time:67027ms step_avg:95.89ms
step:710/1770 train_time:67126ms step_avg:95.89ms
step:711/1770 train_time:67225ms step_avg:95.90ms
step:712/1770 train_time:67325ms step_avg:95.90ms
step:713/1770 train_time:67424ms step_avg:95.91ms
step:714/1770 train_time:67523ms step_avg:95.91ms
step:715/1770 train_time:67622ms step_avg:95.92ms
step:716/1770 train_time:67721ms step_avg:95.92ms
step:717/1770 train_time:67821ms step_avg:95.93ms
step:718/1770 train_time:67920ms step_avg:95.93ms
step:719/1770 train_time:68020ms step_avg:95.94ms
step:720/1770 train_time:68119ms step_avg:95.94ms
step:721/1770 train_time:68218ms step_avg:95.95ms
step:722/1770 train_time:68317ms step_avg:95.95ms
step:723/1770 train_time:68416ms step_avg:95.95ms
step:724/1770 train_time:68515ms step_avg:95.96ms
step:725/1770 train_time:68613ms step_avg:95.96ms
step:726/1770 train_time:68712ms step_avg:95.97ms
step:727/1770 train_time:68812ms step_avg:95.97ms
step:728/1770 train_time:68911ms step_avg:95.98ms
step:729/1770 train_time:69011ms step_avg:95.98ms
step:730/1770 train_time:69111ms step_avg:95.99ms
step:731/1770 train_time:69210ms step_avg:95.99ms
step:732/1770 train_time:69309ms step_avg:96.00ms
step:733/1770 train_time:69408ms step_avg:96.00ms
step:734/1770 train_time:69507ms step_avg:96.00ms
step:735/1770 train_time:69605ms step_avg:96.01ms
step:736/1770 train_time:69704ms step_avg:96.01ms
step:737/1770 train_time:69803ms step_avg:96.02ms
step:738/1770 train_time:69902ms step_avg:96.02ms
step:739/1770 train_time:70001ms step_avg:96.02ms
step:740/1770 train_time:70101ms step_avg:96.03ms
step:741/1770 train_time:70200ms step_avg:96.03ms
step:742/1770 train_time:70300ms step_avg:96.04ms
step:743/1770 train_time:70400ms step_avg:96.04ms
step:744/1770 train_time:70500ms step_avg:96.05ms
step:745/1770 train_time:70599ms step_avg:96.05ms
step:746/1770 train_time:70698ms step_avg:96.06ms
step:747/1770 train_time:70797ms step_avg:96.06ms
step:748/1770 train_time:70895ms step_avg:96.06ms
step:749/1770 train_time:70994ms step_avg:96.07ms
step:750/1770 train_time:71093ms step_avg:96.07ms
step:750/1770 val_loss:3.6028 train_time:71190ms step_avg:96.20ms
step:751/1770 train_time:71212ms step_avg:96.10ms
step:752/1770 train_time:71300ms step_avg:96.09ms
step:753/1770 train_time:71400ms step_avg:96.10ms
step:754/1770 train_time:71500ms step_avg:96.10ms
step:755/1770 train_time:71598ms step_avg:96.11ms
step:756/1770 train_time:71697ms step_avg:96.11ms
step:757/1770 train_time:71795ms step_avg:96.11ms
step:758/1770 train_time:71894ms step_avg:96.11ms
step:759/1770 train_time:71992ms step_avg:96.12ms
step:760/1770 train_time:72090ms step_avg:96.12ms
step:761/1770 train_time:72189ms step_avg:96.12ms
step:762/1770 train_time:72289ms step_avg:96.13ms
step:763/1770 train_time:72389ms step_avg:96.13ms
step:764/1770 train_time:72488ms step_avg:96.14ms
step:765/1770 train_time:72588ms step_avg:96.14ms
step:766/1770 train_time:72687ms step_avg:96.15ms
step:767/1770 train_time:72786ms step_avg:96.15ms
step:768/1770 train_time:72884ms step_avg:96.15ms
step:769/1770 train_time:72983ms step_avg:96.16ms
step:770/1770 train_time:73082ms step_avg:96.16ms
step:771/1770 train_time:73182ms step_avg:96.17ms
step:772/1770 train_time:73282ms step_avg:96.17ms
step:773/1770 train_time:73382ms step_avg:96.18ms
step:774/1770 train_time:73482ms step_avg:96.18ms
step:775/1770 train_time:73582ms step_avg:96.19ms
step:776/1770 train_time:73681ms step_avg:96.19ms
step:777/1770 train_time:73781ms step_avg:96.19ms
step:778/1770 train_time:73880ms step_avg:96.20ms
step:779/1770 train_time:73979ms step_avg:96.20ms
step:780/1770 train_time:74078ms step_avg:96.21ms
step:781/1770 train_time:74176ms step_avg:96.21ms
step:782/1770 train_time:74275ms step_avg:96.21ms
step:783/1770 train_time:74374ms step_avg:96.22ms
step:784/1770 train_time:74474ms step_avg:96.22ms
step:785/1770 train_time:74573ms step_avg:96.22ms
step:786/1770 train_time:74672ms step_avg:96.23ms
step:787/1770 train_time:74773ms step_avg:96.23ms
step:788/1770 train_time:74872ms step_avg:96.24ms
step:789/1770 train_time:74972ms step_avg:96.24ms
step:790/1770 train_time:75071ms step_avg:96.24ms
step:791/1770 train_time:75170ms step_avg:96.25ms
step:792/1770 train_time:75269ms step_avg:96.25ms
step:793/1770 train_time:75368ms step_avg:96.26ms
step:794/1770 train_time:75467ms step_avg:96.26ms
step:795/1770 train_time:75567ms step_avg:96.26ms
step:796/1770 train_time:75666ms step_avg:96.27ms
step:797/1770 train_time:75765ms step_avg:96.27ms
step:798/1770 train_time:75864ms step_avg:96.27ms
step:799/1770 train_time:75964ms step_avg:96.28ms
step:800/1770 train_time:76063ms step_avg:96.28ms
step:801/1770 train_time:76162ms step_avg:96.29ms
step:802/1770 train_time:76261ms step_avg:96.29ms
step:803/1770 train_time:76361ms step_avg:96.29ms
step:804/1770 train_time:76461ms step_avg:96.30ms
step:805/1770 train_time:76560ms step_avg:96.30ms
step:806/1770 train_time:76661ms step_avg:96.31ms
step:807/1770 train_time:76761ms step_avg:96.31ms
step:808/1770 train_time:76859ms step_avg:96.32ms
step:809/1770 train_time:76959ms step_avg:96.32ms
step:810/1770 train_time:77059ms step_avg:96.32ms
step:811/1770 train_time:77159ms step_avg:96.33ms
step:812/1770 train_time:77258ms step_avg:96.33ms
step:813/1770 train_time:77357ms step_avg:96.33ms
step:814/1770 train_time:77455ms step_avg:96.34ms
step:815/1770 train_time:77555ms step_avg:96.34ms
step:816/1770 train_time:77654ms step_avg:96.34ms
step:817/1770 train_time:77753ms step_avg:96.35ms
step:818/1770 train_time:77852ms step_avg:96.35ms
step:819/1770 train_time:77952ms step_avg:96.36ms
step:820/1770 train_time:78052ms step_avg:96.36ms
step:821/1770 train_time:78153ms step_avg:96.37ms
step:822/1770 train_time:78252ms step_avg:96.37ms
step:823/1770 train_time:78352ms step_avg:96.37ms
step:824/1770 train_time:78451ms step_avg:96.38ms
step:825/1770 train_time:78551ms step_avg:96.38ms
step:826/1770 train_time:78650ms step_avg:96.39ms
step:827/1770 train_time:78750ms step_avg:96.39ms
step:828/1770 train_time:78849ms step_avg:96.39ms
step:829/1770 train_time:78949ms step_avg:96.40ms
step:830/1770 train_time:79048ms step_avg:96.40ms
step:831/1770 train_time:79147ms step_avg:96.40ms
step:832/1770 train_time:79247ms step_avg:96.41ms
step:833/1770 train_time:79346ms step_avg:96.41ms
step:834/1770 train_time:79446ms step_avg:96.41ms
step:835/1770 train_time:79545ms step_avg:96.42ms
step:836/1770 train_time:79644ms step_avg:96.42ms
step:837/1770 train_time:79743ms step_avg:96.42ms
step:838/1770 train_time:79843ms step_avg:96.43ms
step:839/1770 train_time:79943ms step_avg:96.43ms
step:840/1770 train_time:80044ms step_avg:96.44ms
step:841/1770 train_time:80147ms step_avg:96.45ms
step:842/1770 train_time:80243ms step_avg:96.45ms
step:843/1770 train_time:80342ms step_avg:96.45ms
step:844/1770 train_time:80441ms step_avg:96.45ms
step:845/1770 train_time:80541ms step_avg:96.46ms
step:846/1770 train_time:80640ms step_avg:96.46ms
step:847/1770 train_time:80739ms step_avg:96.46ms
step:848/1770 train_time:80838ms step_avg:96.47ms
step:849/1770 train_time:80938ms step_avg:96.47ms
step:850/1770 train_time:81037ms step_avg:96.47ms
step:851/1770 train_time:81136ms step_avg:96.48ms
step:852/1770 train_time:81236ms step_avg:96.48ms
step:853/1770 train_time:81335ms step_avg:96.48ms
step:854/1770 train_time:81435ms step_avg:96.49ms
step:855/1770 train_time:81534ms step_avg:96.49ms
step:856/1770 train_time:81633ms step_avg:96.49ms
step:857/1770 train_time:81732ms step_avg:96.50ms
step:858/1770 train_time:81832ms step_avg:96.50ms
step:859/1770 train_time:81931ms step_avg:96.50ms
step:860/1770 train_time:82031ms step_avg:96.51ms
step:861/1770 train_time:82130ms step_avg:96.51ms
step:862/1770 train_time:82230ms step_avg:96.51ms
step:863/1770 train_time:82330ms step_avg:96.52ms
step:864/1770 train_time:82430ms step_avg:96.52ms
step:865/1770 train_time:82529ms step_avg:96.53ms
step:866/1770 train_time:82628ms step_avg:96.53ms
step:867/1770 train_time:82727ms step_avg:96.53ms
step:868/1770 train_time:82826ms step_avg:96.53ms
step:869/1770 train_time:82925ms step_avg:96.54ms
step:870/1770 train_time:83024ms step_avg:96.54ms
step:871/1770 train_time:83124ms step_avg:96.54ms
step:872/1770 train_time:83223ms step_avg:96.55ms
step:873/1770 train_time:83322ms step_avg:96.55ms
step:874/1770 train_time:83422ms step_avg:96.55ms
step:875/1770 train_time:83523ms step_avg:96.56ms
step:875/1770 val_loss:3.5543 train_time:83620ms step_avg:96.67ms
step:876/1770 train_time:83641ms step_avg:96.58ms
step:877/1770 train_time:83730ms step_avg:96.57ms
step:878/1770 train_time:83831ms step_avg:96.58ms
step:879/1770 train_time:83931ms step_avg:96.58ms
step:880/1770 train_time:84030ms step_avg:96.59ms
step:881/1770 train_time:84128ms step_avg:96.59ms
step:882/1770 train_time:84227ms step_avg:96.59ms
step:883/1770 train_time:84325ms step_avg:96.59ms
step:884/1770 train_time:84423ms step_avg:96.59ms
step:885/1770 train_time:84522ms step_avg:96.60ms
step:886/1770 train_time:84621ms step_avg:96.60ms
step:887/1770 train_time:84720ms step_avg:96.60ms
step:888/1770 train_time:84821ms step_avg:96.61ms
step:889/1770 train_time:84921ms step_avg:96.61ms
step:890/1770 train_time:85021ms step_avg:96.62ms
step:891/1770 train_time:85121ms step_avg:96.62ms
step:892/1770 train_time:85220ms step_avg:96.62ms
step:893/1770 train_time:85320ms step_avg:96.63ms
step:894/1770 train_time:85419ms step_avg:96.63ms
step:895/1770 train_time:85519ms step_avg:96.63ms
step:896/1770 train_time:85619ms step_avg:96.64ms
step:897/1770 train_time:85719ms step_avg:96.64ms
step:898/1770 train_time:85818ms step_avg:96.64ms
step:899/1770 train_time:85917ms step_avg:96.64ms
step:900/1770 train_time:86017ms step_avg:96.65ms
step:901/1770 train_time:86117ms step_avg:96.65ms
step:902/1770 train_time:86218ms step_avg:96.66ms
step:903/1770 train_time:86317ms step_avg:96.66ms
step:904/1770 train_time:86417ms step_avg:96.66ms
step:905/1770 train_time:86516ms step_avg:96.67ms
step:906/1770 train_time:86615ms step_avg:96.67ms
step:907/1770 train_time:86714ms step_avg:96.67ms
step:908/1770 train_time:86814ms step_avg:96.68ms
step:909/1770 train_time:86914ms step_avg:96.68ms
step:910/1770 train_time:87013ms step_avg:96.68ms
step:911/1770 train_time:87112ms step_avg:96.68ms
step:912/1770 train_time:87212ms step_avg:96.69ms
step:913/1770 train_time:87311ms step_avg:96.69ms
step:914/1770 train_time:87411ms step_avg:96.69ms
step:915/1770 train_time:87510ms step_avg:96.70ms
step:916/1770 train_time:87610ms step_avg:96.70ms
step:917/1770 train_time:87710ms step_avg:96.70ms
step:918/1770 train_time:87809ms step_avg:96.71ms
step:919/1770 train_time:87908ms step_avg:96.71ms
step:920/1770 train_time:88009ms step_avg:96.71ms
step:921/1770 train_time:88110ms step_avg:96.72ms
step:922/1770 train_time:88211ms step_avg:96.72ms
step:923/1770 train_time:88312ms step_avg:96.73ms
step:924/1770 train_time:88413ms step_avg:96.73ms
step:925/1770 train_time:88513ms step_avg:96.74ms
step:926/1770 train_time:88615ms step_avg:96.74ms
step:927/1770 train_time:88715ms step_avg:96.74ms
step:928/1770 train_time:88816ms step_avg:96.75ms
step:929/1770 train_time:88918ms step_avg:96.75ms
step:930/1770 train_time:89019ms step_avg:96.76ms
step:931/1770 train_time:89120ms step_avg:96.76ms
step:932/1770 train_time:89221ms step_avg:96.77ms
step:933/1770 train_time:89321ms step_avg:96.77ms
step:934/1770 train_time:89422ms step_avg:96.78ms
step:935/1770 train_time:89522ms step_avg:96.78ms
step:936/1770 train_time:89623ms step_avg:96.79ms
step:937/1770 train_time:89723ms step_avg:96.79ms
step:938/1770 train_time:89824ms step_avg:96.79ms
step:939/1770 train_time:89925ms step_avg:96.80ms
step:940/1770 train_time:90025ms step_avg:96.80ms
step:941/1770 train_time:90125ms step_avg:96.80ms
step:942/1770 train_time:90226ms step_avg:96.81ms
step:943/1770 train_time:90327ms step_avg:96.81ms
step:944/1770 train_time:90427ms step_avg:96.82ms
step:945/1770 train_time:90528ms step_avg:96.82ms
step:946/1770 train_time:90630ms step_avg:96.83ms
step:947/1770 train_time:90731ms step_avg:96.83ms
step:948/1770 train_time:90832ms step_avg:96.84ms
step:949/1770 train_time:90932ms step_avg:96.84ms
step:950/1770 train_time:91033ms step_avg:96.84ms
step:951/1770 train_time:91134ms step_avg:96.85ms
step:952/1770 train_time:91235ms step_avg:96.85ms
step:953/1770 train_time:91336ms step_avg:96.86ms
step:954/1770 train_time:91436ms step_avg:96.86ms
step:955/1770 train_time:91538ms step_avg:96.87ms
step:956/1770 train_time:91640ms step_avg:96.87ms
step:957/1770 train_time:91741ms step_avg:96.88ms
step:958/1770 train_time:91846ms step_avg:96.88ms
step:959/1770 train_time:91942ms step_avg:96.88ms
step:960/1770 train_time:92042ms step_avg:96.89ms
step:961/1770 train_time:92143ms step_avg:96.89ms
step:962/1770 train_time:92244ms step_avg:96.89ms
step:963/1770 train_time:92345ms step_avg:96.90ms
step:964/1770 train_time:92445ms step_avg:96.90ms
step:965/1770 train_time:92546ms step_avg:96.91ms
step:966/1770 train_time:92647ms step_avg:96.91ms
step:967/1770 train_time:92748ms step_avg:96.92ms
step:968/1770 train_time:92849ms step_avg:96.92ms
step:969/1770 train_time:92950ms step_avg:96.92ms
step:970/1770 train_time:93052ms step_avg:96.93ms
step:971/1770 train_time:93152ms step_avg:96.93ms
step:972/1770 train_time:93253ms step_avg:96.94ms
step:973/1770 train_time:93355ms step_avg:96.94ms
step:974/1770 train_time:93456ms step_avg:96.95ms
step:975/1770 train_time:93557ms step_avg:96.95ms
step:976/1770 train_time:93659ms step_avg:96.96ms
step:977/1770 train_time:93760ms step_avg:96.96ms
step:978/1770 train_time:93861ms step_avg:96.96ms
step:979/1770 train_time:93962ms step_avg:96.97ms
step:980/1770 train_time:94063ms step_avg:96.97ms
step:981/1770 train_time:94164ms step_avg:96.98ms
step:982/1770 train_time:94265ms step_avg:96.98ms
step:983/1770 train_time:94367ms step_avg:96.99ms
step:984/1770 train_time:94467ms step_avg:96.99ms
step:985/1770 train_time:94568ms step_avg:96.99ms
step:986/1770 train_time:94667ms step_avg:97.00ms
step:987/1770 train_time:94768ms step_avg:97.00ms
step:988/1770 train_time:94869ms step_avg:97.00ms
step:989/1770 train_time:94972ms step_avg:97.01ms
step:990/1770 train_time:95073ms step_avg:97.01ms
step:991/1770 train_time:95174ms step_avg:97.02ms
step:992/1770 train_time:95274ms step_avg:97.02ms
step:993/1770 train_time:95375ms step_avg:97.02ms
step:994/1770 train_time:95476ms step_avg:97.03ms
step:995/1770 train_time:95577ms step_avg:97.03ms
step:996/1770 train_time:95677ms step_avg:97.04ms
step:997/1770 train_time:95778ms step_avg:97.04ms
step:998/1770 train_time:95880ms step_avg:97.04ms
step:999/1770 train_time:95982ms step_avg:97.05ms
step:1000/1770 train_time:96083ms step_avg:97.05ms
step:1000/1770 val_loss:3.5151 train_time:96182ms step_avg:97.15ms
step:1001/1770 train_time:96204ms step_avg:97.08ms
step:1002/1770 train_time:96295ms step_avg:97.07ms
step:1003/1770 train_time:96398ms step_avg:97.08ms
step:1004/1770 train_time:96499ms step_avg:97.08ms
step:1005/1770 train_time:96598ms step_avg:97.08ms
step:1006/1770 train_time:96699ms step_avg:97.09ms
step:1007/1770 train_time:96799ms step_avg:97.09ms
step:1008/1770 train_time:96898ms step_avg:97.09ms
step:1009/1770 train_time:96998ms step_avg:97.09ms
step:1010/1770 train_time:97097ms step_avg:97.10ms
step:1011/1770 train_time:97201ms step_avg:97.10ms
step:1012/1770 train_time:97302ms step_avg:97.11ms
step:1013/1770 train_time:97405ms step_avg:97.11ms
step:1014/1770 train_time:97505ms step_avg:97.12ms
step:1015/1770 train_time:97606ms step_avg:97.12ms
step:1016/1770 train_time:97707ms step_avg:97.12ms
step:1017/1770 train_time:97808ms step_avg:97.13ms
step:1018/1770 train_time:97908ms step_avg:97.13ms
step:1019/1770 train_time:98009ms step_avg:97.13ms
step:1020/1770 train_time:98110ms step_avg:97.14ms
step:1021/1770 train_time:98211ms step_avg:97.14ms
step:1022/1770 train_time:98312ms step_avg:97.15ms
step:1023/1770 train_time:98413ms step_avg:97.15ms
step:1024/1770 train_time:98514ms step_avg:97.15ms
step:1025/1770 train_time:98614ms step_avg:97.16ms
step:1026/1770 train_time:98716ms step_avg:97.16ms
step:1027/1770 train_time:98817ms step_avg:97.17ms
step:1028/1770 train_time:98919ms step_avg:97.17ms
step:1029/1770 train_time:99019ms step_avg:97.17ms
step:1030/1770 train_time:99120ms step_avg:97.18ms
step:1031/1770 train_time:99221ms step_avg:97.18ms
step:1032/1770 train_time:99321ms step_avg:97.18ms
step:1033/1770 train_time:99422ms step_avg:97.19ms
step:1034/1770 train_time:99523ms step_avg:97.19ms
step:1035/1770 train_time:99624ms step_avg:97.19ms
step:1036/1770 train_time:99725ms step_avg:97.20ms
step:1037/1770 train_time:99831ms step_avg:97.21ms
step:1038/1770 train_time:99928ms step_avg:97.21ms
step:1039/1770 train_time:100029ms step_avg:97.21ms
step:1040/1770 train_time:100129ms step_avg:97.21ms
step:1041/1770 train_time:100230ms step_avg:97.22ms
step:1042/1770 train_time:100330ms step_avg:97.22ms
step:1043/1770 train_time:100431ms step_avg:97.22ms
step:1044/1770 train_time:100531ms step_avg:97.23ms
step:1045/1770 train_time:100632ms step_avg:97.23ms
step:1046/1770 train_time:100733ms step_avg:97.23ms
step:1047/1770 train_time:100833ms step_avg:97.24ms
step:1048/1770 train_time:100934ms step_avg:97.24ms
step:1049/1770 train_time:101035ms step_avg:97.24ms
step:1050/1770 train_time:101135ms step_avg:97.25ms
step:1051/1770 train_time:101236ms step_avg:97.25ms
step:1052/1770 train_time:101337ms step_avg:97.25ms
step:1053/1770 train_time:101438ms step_avg:97.26ms
step:1054/1770 train_time:101538ms step_avg:97.26ms
step:1055/1770 train_time:101638ms step_avg:97.26ms
step:1056/1770 train_time:101739ms step_avg:97.26ms
step:1057/1770 train_time:101841ms step_avg:97.27ms
step:1058/1770 train_time:101941ms step_avg:97.27ms
step:1059/1770 train_time:102042ms step_avg:97.28ms
step:1060/1770 train_time:102143ms step_avg:97.28ms
step:1061/1770 train_time:102244ms step_avg:97.28ms
step:1062/1770 train_time:102346ms step_avg:97.29ms
step:1063/1770 train_time:102448ms step_avg:97.29ms
step:1064/1770 train_time:102550ms step_avg:97.30ms
step:1065/1770 train_time:102650ms step_avg:97.30ms
step:1066/1770 train_time:102751ms step_avg:97.30ms
step:1067/1770 train_time:102851ms step_avg:97.30ms
step:1068/1770 train_time:102952ms step_avg:97.31ms
step:1069/1770 train_time:103053ms step_avg:97.31ms
step:1070/1770 train_time:103154ms step_avg:97.31ms
step:1071/1770 train_time:103255ms step_avg:97.32ms
step:1072/1770 train_time:103356ms step_avg:97.32ms
step:1073/1770 train_time:103458ms step_avg:97.33ms
step:1074/1770 train_time:103559ms step_avg:97.33ms
step:1075/1770 train_time:103660ms step_avg:97.33ms
step:1076/1770 train_time:103761ms step_avg:97.34ms
step:1077/1770 train_time:103862ms step_avg:97.34ms
step:1078/1770 train_time:103964ms step_avg:97.34ms
step:1079/1770 train_time:104065ms step_avg:97.35ms
step:1080/1770 train_time:104167ms step_avg:97.35ms
step:1081/1770 train_time:104269ms step_avg:97.36ms
step:1082/1770 train_time:104370ms step_avg:97.36ms
step:1083/1770 train_time:104471ms step_avg:97.36ms
step:1084/1770 train_time:104572ms step_avg:97.37ms
step:1085/1770 train_time:104674ms step_avg:97.37ms
step:1086/1770 train_time:104775ms step_avg:97.37ms
step:1087/1770 train_time:104875ms step_avg:97.38ms
step:1088/1770 train_time:104977ms step_avg:97.38ms
step:1089/1770 train_time:105078ms step_avg:97.38ms
step:1090/1770 train_time:105180ms step_avg:97.39ms
step:1091/1770 train_time:105280ms step_avg:97.39ms
step:1092/1770 train_time:105381ms step_avg:97.39ms
step:1093/1770 train_time:105482ms step_avg:97.40ms
step:1094/1770 train_time:105584ms step_avg:97.40ms
step:1095/1770 train_time:105684ms step_avg:97.40ms
step:1096/1770 train_time:105786ms step_avg:97.41ms
step:1097/1770 train_time:105887ms step_avg:97.41ms
step:1098/1770 train_time:105988ms step_avg:97.42ms
step:1099/1770 train_time:106090ms step_avg:97.42ms
step:1100/1770 train_time:106190ms step_avg:97.42ms
step:1101/1770 train_time:106291ms step_avg:97.43ms
step:1102/1770 train_time:106391ms step_avg:97.43ms
step:1103/1770 train_time:106492ms step_avg:97.43ms
step:1104/1770 train_time:106594ms step_avg:97.44ms
step:1105/1770 train_time:106696ms step_avg:97.44ms
step:1106/1770 train_time:106798ms step_avg:97.44ms
step:1107/1770 train_time:106899ms step_avg:97.45ms
step:1108/1770 train_time:107001ms step_avg:97.45ms
step:1109/1770 train_time:107102ms step_avg:97.45ms
step:1110/1770 train_time:107203ms step_avg:97.46ms
step:1111/1770 train_time:107305ms step_avg:97.46ms
step:1112/1770 train_time:107406ms step_avg:97.46ms
step:1113/1770 train_time:107507ms step_avg:97.47ms
step:1114/1770 train_time:107609ms step_avg:97.47ms
step:1115/1770 train_time:107710ms step_avg:97.48ms
step:1116/1770 train_time:107811ms step_avg:97.48ms
step:1117/1770 train_time:107912ms step_avg:97.48ms
step:1118/1770 train_time:108012ms step_avg:97.48ms
step:1119/1770 train_time:108112ms step_avg:97.49ms
step:1120/1770 train_time:108213ms step_avg:97.49ms
step:1121/1770 train_time:108313ms step_avg:97.49ms
step:1122/1770 train_time:108415ms step_avg:97.50ms
step:1123/1770 train_time:108516ms step_avg:97.50ms
step:1124/1770 train_time:108619ms step_avg:97.50ms
step:1125/1770 train_time:108720ms step_avg:97.51ms
step:1125/1770 val_loss:3.4759 train_time:108819ms step_avg:97.60ms
step:1126/1770 train_time:108840ms step_avg:97.53ms
step:1127/1770 train_time:108932ms step_avg:97.52ms
step:1128/1770 train_time:109033ms step_avg:97.53ms
step:1129/1770 train_time:109133ms step_avg:97.53ms
step:1130/1770 train_time:109234ms step_avg:97.53ms
step:1131/1770 train_time:109335ms step_avg:97.53ms
step:1132/1770 train_time:109435ms step_avg:97.54ms
step:1133/1770 train_time:109536ms step_avg:97.54ms
step:1134/1770 train_time:109636ms step_avg:97.54ms
step:1135/1770 train_time:109736ms step_avg:97.54ms
step:1136/1770 train_time:109839ms step_avg:97.55ms
step:1137/1770 train_time:109942ms step_avg:97.55ms
step:1138/1770 train_time:110043ms step_avg:97.56ms
step:1139/1770 train_time:110145ms step_avg:97.56ms
step:1140/1770 train_time:110245ms step_avg:97.56ms
step:1141/1770 train_time:110346ms step_avg:97.56ms
step:1142/1770 train_time:110446ms step_avg:97.57ms
step:1143/1770 train_time:110548ms step_avg:97.57ms
step:1144/1770 train_time:110650ms step_avg:97.57ms
step:1145/1770 train_time:110751ms step_avg:97.58ms
step:1146/1770 train_time:110852ms step_avg:97.58ms
step:1147/1770 train_time:110955ms step_avg:97.59ms
step:1148/1770 train_time:111056ms step_avg:97.59ms
step:1149/1770 train_time:111157ms step_avg:97.59ms
step:1150/1770 train_time:111257ms step_avg:97.59ms
step:1151/1770 train_time:111358ms step_avg:97.60ms
step:1152/1770 train_time:111460ms step_avg:97.60ms
step:1153/1770 train_time:111560ms step_avg:97.60ms
step:1154/1770 train_time:111662ms step_avg:97.61ms
step:1155/1770 train_time:111764ms step_avg:97.61ms
step:1156/1770 train_time:111864ms step_avg:97.61ms
step:1157/1770 train_time:111966ms step_avg:97.62ms
step:1158/1770 train_time:112067ms step_avg:97.62ms
step:1159/1770 train_time:112169ms step_avg:97.62ms
step:1160/1770 train_time:112270ms step_avg:97.63ms
step:1161/1770 train_time:112371ms step_avg:97.63ms
step:1162/1770 train_time:112472ms step_avg:97.63ms
step:1163/1770 train_time:112573ms step_avg:97.64ms
step:1164/1770 train_time:112675ms step_avg:97.64ms
step:1165/1770 train_time:112776ms step_avg:97.64ms
step:1166/1770 train_time:112877ms step_avg:97.64ms
step:1167/1770 train_time:112978ms step_avg:97.65ms
step:1168/1770 train_time:113080ms step_avg:97.65ms
step:1169/1770 train_time:113181ms step_avg:97.65ms
step:1170/1770 train_time:113282ms step_avg:97.66ms
step:1171/1770 train_time:113384ms step_avg:97.66ms
step:1172/1770 train_time:113486ms step_avg:97.66ms
step:1173/1770 train_time:113587ms step_avg:97.67ms
step:1174/1770 train_time:113688ms step_avg:97.67ms
step:1175/1770 train_time:113790ms step_avg:97.67ms
step:1176/1770 train_time:113892ms step_avg:97.68ms
step:1177/1770 train_time:113994ms step_avg:97.68ms
step:1178/1770 train_time:114095ms step_avg:97.68ms
step:1179/1770 train_time:114195ms step_avg:97.69ms
step:1180/1770 train_time:114296ms step_avg:97.69ms
step:1181/1770 train_time:114396ms step_avg:97.69ms
step:1182/1770 train_time:114497ms step_avg:97.69ms
step:1183/1770 train_time:114600ms step_avg:97.70ms
step:1184/1770 train_time:114702ms step_avg:97.70ms
step:1185/1770 train_time:114804ms step_avg:97.71ms
step:1186/1770 train_time:114906ms step_avg:97.71ms
step:1187/1770 train_time:115010ms step_avg:97.71ms
step:1188/1770 train_time:115113ms step_avg:97.72ms
step:1189/1770 train_time:115215ms step_avg:97.72ms
step:1190/1770 train_time:115316ms step_avg:97.73ms
step:1191/1770 train_time:115418ms step_avg:97.73ms
step:1192/1770 train_time:115520ms step_avg:97.73ms
step:1193/1770 train_time:115622ms step_avg:97.74ms
step:1194/1770 train_time:115723ms step_avg:97.74ms
step:1195/1770 train_time:115825ms step_avg:97.74ms
step:1196/1770 train_time:115930ms step_avg:97.75ms
step:1197/1770 train_time:116030ms step_avg:97.75ms
step:1198/1770 train_time:116132ms step_avg:97.75ms
step:1199/1770 train_time:116235ms step_avg:97.76ms
step:1200/1770 train_time:116337ms step_avg:97.76ms
step:1201/1770 train_time:116440ms step_avg:97.77ms
step:1202/1770 train_time:116541ms step_avg:97.77ms
step:1203/1770 train_time:116643ms step_avg:97.77ms
step:1204/1770 train_time:116746ms step_avg:97.78ms
step:1205/1770 train_time:116848ms step_avg:97.78ms
step:1206/1770 train_time:116950ms step_avg:97.78ms
step:1207/1770 train_time:117052ms step_avg:97.79ms
step:1208/1770 train_time:117155ms step_avg:97.79ms
step:1209/1770 train_time:117256ms step_avg:97.79ms
step:1210/1770 train_time:117359ms step_avg:97.80ms
step:1211/1770 train_time:117460ms step_avg:97.80ms
step:1212/1770 train_time:117563ms step_avg:97.81ms
step:1213/1770 train_time:117665ms step_avg:97.81ms
step:1214/1770 train_time:117767ms step_avg:97.81ms
step:1215/1770 train_time:117869ms step_avg:97.82ms
step:1216/1770 train_time:117974ms step_avg:97.82ms
step:1217/1770 train_time:118076ms step_avg:97.83ms
step:1218/1770 train_time:118178ms step_avg:97.83ms
step:1219/1770 train_time:118280ms step_avg:97.83ms
step:1220/1770 train_time:118382ms step_avg:97.84ms
step:1221/1770 train_time:118485ms step_avg:97.84ms
step:1222/1770 train_time:118589ms step_avg:97.85ms
step:1223/1770 train_time:118690ms step_avg:97.85ms
step:1224/1770 train_time:118793ms step_avg:97.85ms
step:1225/1770 train_time:118895ms step_avg:97.86ms
step:1226/1770 train_time:118998ms step_avg:97.86ms
step:1227/1770 train_time:119101ms step_avg:97.86ms
step:1228/1770 train_time:119205ms step_avg:97.87ms
step:1229/1770 train_time:119306ms step_avg:97.87ms
step:1230/1770 train_time:119408ms step_avg:97.88ms
step:1231/1770 train_time:119510ms step_avg:97.88ms
step:1232/1770 train_time:119612ms step_avg:97.88ms
step:1233/1770 train_time:119714ms step_avg:97.89ms
step:1234/1770 train_time:119816ms step_avg:97.89ms
step:1235/1770 train_time:119918ms step_avg:97.89ms
step:1236/1770 train_time:120020ms step_avg:97.90ms
step:1237/1770 train_time:120123ms step_avg:97.90ms
step:1238/1770 train_time:120225ms step_avg:97.90ms
step:1239/1770 train_time:120327ms step_avg:97.91ms
step:1240/1770 train_time:120429ms step_avg:97.91ms
step:1241/1770 train_time:120531ms step_avg:97.91ms
step:1242/1770 train_time:120633ms step_avg:97.92ms
step:1243/1770 train_time:120735ms step_avg:97.92ms
step:1244/1770 train_time:120837ms step_avg:97.92ms
step:1245/1770 train_time:120938ms step_avg:97.93ms
step:1246/1770 train_time:121040ms step_avg:97.93ms
step:1247/1770 train_time:121142ms step_avg:97.93ms
step:1248/1770 train_time:121245ms step_avg:97.94ms
step:1249/1770 train_time:121347ms step_avg:97.94ms
step:1250/1770 train_time:121449ms step_avg:97.94ms
step:1250/1770 val_loss:3.4271 train_time:121550ms step_avg:98.02ms
step:1251/1770 train_time:121572ms step_avg:97.96ms
step:1252/1770 train_time:121662ms step_avg:97.96ms
step:1253/1770 train_time:121766ms step_avg:97.96ms
step:1254/1770 train_time:121868ms step_avg:97.97ms
step:1255/1770 train_time:121972ms step_avg:97.97ms
step:1256/1770 train_time:122073ms step_avg:97.97ms
step:1257/1770 train_time:122174ms step_avg:97.97ms
step:1258/1770 train_time:122276ms step_avg:97.98ms
step:1259/1770 train_time:122378ms step_avg:97.98ms
step:1260/1770 train_time:122479ms step_avg:97.98ms
step:1261/1770 train_time:122583ms step_avg:97.99ms
step:1262/1770 train_time:122688ms step_avg:97.99ms
step:1263/1770 train_time:122790ms step_avg:98.00ms
step:1264/1770 train_time:122895ms step_avg:98.00ms
step:1265/1770 train_time:122997ms step_avg:98.01ms
step:1266/1770 train_time:123099ms step_avg:98.01ms
step:1267/1770 train_time:123202ms step_avg:98.01ms
step:1268/1770 train_time:123304ms step_avg:98.02ms
step:1269/1770 train_time:123406ms step_avg:98.02ms
step:1270/1770 train_time:123508ms step_avg:98.02ms
step:1271/1770 train_time:123611ms step_avg:98.03ms
step:1272/1770 train_time:123713ms step_avg:98.03ms
step:1273/1770 train_time:123815ms step_avg:98.03ms
step:1274/1770 train_time:123918ms step_avg:98.04ms
step:1275/1770 train_time:124019ms step_avg:98.04ms
step:1276/1770 train_time:124121ms step_avg:98.04ms
step:1277/1770 train_time:124223ms step_avg:98.05ms
step:1278/1770 train_time:124326ms step_avg:98.05ms
step:1279/1770 train_time:124429ms step_avg:98.05ms
step:1280/1770 train_time:124532ms step_avg:98.06ms
step:1281/1770 train_time:124634ms step_avg:98.06ms
step:1282/1770 train_time:124737ms step_avg:98.06ms
step:1283/1770 train_time:124839ms step_avg:98.07ms
step:1284/1770 train_time:124942ms step_avg:98.07ms
step:1285/1770 train_time:125043ms step_avg:98.07ms
step:1286/1770 train_time:125146ms step_avg:98.08ms
step:1287/1770 train_time:125250ms step_avg:98.08ms
step:1288/1770 train_time:125352ms step_avg:98.08ms
step:1289/1770 train_time:125454ms step_avg:98.09ms
step:1290/1770 train_time:125556ms step_avg:98.09ms
step:1291/1770 train_time:125658ms step_avg:98.09ms
step:1292/1770 train_time:125761ms step_avg:98.10ms
step:1293/1770 train_time:125864ms step_avg:98.10ms
step:1294/1770 train_time:125965ms step_avg:98.10ms
step:1295/1770 train_time:126067ms step_avg:98.11ms
step:1296/1770 train_time:126169ms step_avg:98.11ms
step:1297/1770 train_time:126269ms step_avg:98.11ms
step:1298/1770 train_time:126371ms step_avg:98.11ms
step:1299/1770 train_time:126473ms step_avg:98.12ms
step:1300/1770 train_time:126575ms step_avg:98.12ms
step:1301/1770 train_time:126679ms step_avg:98.12ms
step:1302/1770 train_time:126783ms step_avg:98.13ms
step:1303/1770 train_time:126883ms step_avg:98.13ms
step:1304/1770 train_time:126986ms step_avg:98.13ms
step:1305/1770 train_time:127088ms step_avg:98.14ms
step:1306/1770 train_time:127189ms step_avg:98.14ms
step:1307/1770 train_time:127290ms step_avg:98.14ms
step:1308/1770 train_time:127392ms step_avg:98.14ms
step:1309/1770 train_time:127493ms step_avg:98.15ms
step:1310/1770 train_time:127595ms step_avg:98.15ms
step:1311/1770 train_time:127697ms step_avg:98.15ms
step:1312/1770 train_time:127799ms step_avg:98.16ms
step:1313/1770 train_time:127900ms step_avg:98.16ms
step:1314/1770 train_time:128003ms step_avg:98.16ms
step:1315/1770 train_time:128105ms step_avg:98.16ms
step:1316/1770 train_time:128208ms step_avg:98.17ms
step:1317/1770 train_time:128310ms step_avg:98.17ms
step:1318/1770 train_time:128415ms step_avg:98.18ms
step:1319/1770 train_time:128518ms step_avg:98.18ms
step:1320/1770 train_time:128619ms step_avg:98.18ms
step:1321/1770 train_time:128721ms step_avg:98.19ms
step:1322/1770 train_time:128823ms step_avg:98.19ms
step:1323/1770 train_time:128926ms step_avg:98.19ms
step:1324/1770 train_time:129029ms step_avg:98.20ms
step:1325/1770 train_time:129134ms step_avg:98.20ms
step:1326/1770 train_time:129236ms step_avg:98.20ms
step:1327/1770 train_time:129340ms step_avg:98.21ms
step:1328/1770 train_time:129443ms step_avg:98.21ms
step:1329/1770 train_time:129546ms step_avg:98.21ms
step:1330/1770 train_time:129647ms step_avg:98.22ms
step:1331/1770 train_time:129749ms step_avg:98.22ms
step:1332/1770 train_time:129850ms step_avg:98.22ms
step:1333/1770 train_time:129953ms step_avg:98.23ms
step:1334/1770 train_time:130055ms step_avg:98.23ms
step:1335/1770 train_time:130157ms step_avg:98.23ms
step:1336/1770 train_time:130258ms step_avg:98.23ms
step:1337/1770 train_time:130360ms step_avg:98.24ms
step:1338/1770 train_time:130462ms step_avg:98.24ms
step:1339/1770 train_time:130566ms step_avg:98.24ms
step:1340/1770 train_time:130669ms step_avg:98.25ms
step:1341/1770 train_time:130770ms step_avg:98.25ms
step:1342/1770 train_time:130873ms step_avg:98.25ms
step:1343/1770 train_time:130976ms step_avg:98.26ms
step:1344/1770 train_time:131078ms step_avg:98.26ms
step:1345/1770 train_time:131179ms step_avg:98.26ms
step:1346/1770 train_time:131281ms step_avg:98.26ms
step:1347/1770 train_time:131384ms step_avg:98.27ms
step:1348/1770 train_time:131487ms step_avg:98.27ms
step:1349/1770 train_time:131590ms step_avg:98.27ms
step:1350/1770 train_time:131692ms step_avg:98.28ms
step:1351/1770 train_time:131794ms step_avg:98.28ms
step:1352/1770 train_time:131896ms step_avg:98.28ms
step:1353/1770 train_time:132001ms step_avg:98.29ms
step:1354/1770 train_time:132101ms step_avg:98.29ms
step:1355/1770 train_time:132203ms step_avg:98.29ms
step:1356/1770 train_time:132304ms step_avg:98.29ms
step:1357/1770 train_time:132407ms step_avg:98.30ms
step:1358/1770 train_time:132509ms step_avg:98.30ms
step:1359/1770 train_time:132611ms step_avg:98.30ms
step:1360/1770 train_time:132714ms step_avg:98.31ms
step:1361/1770 train_time:132817ms step_avg:98.31ms
step:1362/1770 train_time:132919ms step_avg:98.31ms
step:1363/1770 train_time:133022ms step_avg:98.32ms
step:1364/1770 train_time:133124ms step_avg:98.32ms
step:1365/1770 train_time:133226ms step_avg:98.32ms
step:1366/1770 train_time:133328ms step_avg:98.32ms
step:1367/1770 train_time:133430ms step_avg:98.33ms
step:1368/1770 train_time:133532ms step_avg:98.33ms
step:1369/1770 train_time:133635ms step_avg:98.33ms
step:1370/1770 train_time:133737ms step_avg:98.34ms
step:1371/1770 train_time:133840ms step_avg:98.34ms
step:1372/1770 train_time:133941ms step_avg:98.34ms
step:1373/1770 train_time:134044ms step_avg:98.34ms
step:1374/1770 train_time:134147ms step_avg:98.35ms
step:1375/1770 train_time:134249ms step_avg:98.35ms
step:1375/1770 val_loss:3.3835 train_time:134350ms step_avg:98.43ms
step:1376/1770 train_time:134372ms step_avg:98.37ms
step:1377/1770 train_time:134464ms step_avg:98.36ms
step:1378/1770 train_time:134566ms step_avg:98.37ms
step:1379/1770 train_time:134668ms step_avg:98.37ms
step:1380/1770 train_time:134769ms step_avg:98.37ms
step:1381/1770 train_time:134871ms step_avg:98.37ms
step:1382/1770 train_time:134972ms step_avg:98.38ms
step:1383/1770 train_time:135075ms step_avg:98.38ms
step:1384/1770 train_time:135177ms step_avg:98.38ms
step:1385/1770 train_time:135279ms step_avg:98.38ms
step:1386/1770 train_time:135381ms step_avg:98.39ms
step:1387/1770 train_time:135485ms step_avg:98.39ms
step:1388/1770 train_time:135587ms step_avg:98.39ms
step:1389/1770 train_time:135689ms step_avg:98.40ms
step:1390/1770 train_time:135791ms step_avg:98.40ms
step:1391/1770 train_time:135892ms step_avg:98.40ms
step:1392/1770 train_time:135995ms step_avg:98.40ms
step:1393/1770 train_time:136097ms step_avg:98.41ms
step:1394/1770 train_time:136199ms step_avg:98.41ms
step:1395/1770 train_time:136301ms step_avg:98.41ms
step:1396/1770 train_time:136404ms step_avg:98.42ms
step:1397/1770 train_time:136507ms step_avg:98.42ms
step:1398/1770 train_time:136609ms step_avg:98.42ms
step:1399/1770 train_time:136711ms step_avg:98.42ms
step:1400/1770 train_time:136814ms step_avg:98.43ms
step:1401/1770 train_time:136916ms step_avg:98.43ms
step:1402/1770 train_time:137018ms step_avg:98.43ms
step:1403/1770 train_time:137120ms step_avg:98.44ms
step:1404/1770 train_time:137223ms step_avg:98.44ms
step:1405/1770 train_time:137325ms step_avg:98.44ms
step:1406/1770 train_time:137428ms step_avg:98.44ms
step:1407/1770 train_time:137529ms step_avg:98.45ms
step:1408/1770 train_time:137632ms step_avg:98.45ms
step:1409/1770 train_time:137734ms step_avg:98.45ms
step:1410/1770 train_time:137837ms step_avg:98.45ms
step:1411/1770 train_time:137938ms step_avg:98.46ms
step:1412/1770 train_time:138040ms step_avg:98.46ms
step:1413/1770 train_time:138141ms step_avg:98.46ms
step:1414/1770 train_time:138244ms step_avg:98.46ms
step:1415/1770 train_time:138347ms step_avg:98.47ms
step:1416/1770 train_time:138450ms step_avg:98.47ms
step:1417/1770 train_time:138552ms step_avg:98.47ms
step:1418/1770 train_time:138654ms step_avg:98.48ms
step:1419/1770 train_time:138758ms step_avg:98.48ms
step:1420/1770 train_time:138863ms step_avg:98.48ms
step:1421/1770 train_time:138963ms step_avg:98.49ms
step:1422/1770 train_time:139065ms step_avg:98.49ms
step:1423/1770 train_time:139166ms step_avg:98.49ms
step:1424/1770 train_time:139269ms step_avg:98.49ms
step:1425/1770 train_time:139370ms step_avg:98.49ms
step:1426/1770 train_time:139473ms step_avg:98.50ms
step:1427/1770 train_time:139574ms step_avg:98.50ms
step:1428/1770 train_time:139678ms step_avg:98.50ms
step:1429/1770 train_time:139781ms step_avg:98.51ms
step:1430/1770 train_time:139883ms step_avg:98.51ms
step:1431/1770 train_time:139987ms step_avg:98.51ms
step:1432/1770 train_time:140089ms step_avg:98.52ms
step:1433/1770 train_time:140190ms step_avg:98.52ms
step:1434/1770 train_time:140292ms step_avg:98.52ms
step:1435/1770 train_time:140393ms step_avg:98.52ms
step:1436/1770 train_time:140498ms step_avg:98.53ms
step:1437/1770 train_time:140600ms step_avg:98.53ms
step:1438/1770 train_time:140701ms step_avg:98.53ms
step:1439/1770 train_time:140803ms step_avg:98.53ms
step:1440/1770 train_time:140906ms step_avg:98.54ms
step:1441/1770 train_time:141011ms step_avg:98.54ms
step:1442/1770 train_time:141113ms step_avg:98.54ms
step:1443/1770 train_time:141215ms step_avg:98.54ms
step:1444/1770 train_time:141317ms step_avg:98.55ms
step:1445/1770 train_time:141420ms step_avg:98.55ms
step:1446/1770 train_time:141523ms step_avg:98.55ms
step:1447/1770 train_time:141626ms step_avg:98.56ms
step:1448/1770 train_time:141730ms step_avg:98.56ms
step:1449/1770 train_time:141834ms step_avg:98.56ms
step:1450/1770 train_time:141936ms step_avg:98.57ms
step:1451/1770 train_time:142040ms step_avg:98.57ms
step:1452/1770 train_time:142143ms step_avg:98.57ms
step:1453/1770 train_time:142246ms step_avg:98.58ms
step:1454/1770 train_time:142349ms step_avg:98.58ms
step:1455/1770 train_time:142453ms step_avg:98.58ms
step:1456/1770 train_time:142557ms step_avg:98.59ms
step:1457/1770 train_time:142660ms step_avg:98.59ms
step:1458/1770 train_time:142764ms step_avg:98.59ms
step:1459/1770 train_time:142868ms step_avg:98.60ms
step:1460/1770 train_time:142971ms step_avg:98.60ms
step:1461/1770 train_time:143074ms step_avg:98.60ms
step:1462/1770 train_time:143177ms step_avg:98.61ms
step:1463/1770 train_time:143281ms step_avg:98.61ms
step:1464/1770 train_time:143387ms step_avg:98.62ms
step:1465/1770 train_time:143491ms step_avg:98.62ms
step:1466/1770 train_time:143594ms step_avg:98.62ms
step:1467/1770 train_time:143698ms step_avg:98.63ms
step:1468/1770 train_time:143802ms step_avg:98.63ms
step:1469/1770 train_time:143904ms step_avg:98.63ms
step:1470/1770 train_time:144007ms step_avg:98.64ms
step:1471/1770 train_time:144111ms step_avg:98.64ms
step:1472/1770 train_time:144213ms step_avg:98.64ms
step:1473/1770 train_time:144317ms step_avg:98.64ms
step:1474/1770 train_time:144422ms step_avg:98.65ms
step:1475/1770 train_time:144526ms step_avg:98.65ms
step:1476/1770 train_time:144628ms step_avg:98.65ms
step:1477/1770 train_time:144735ms step_avg:98.66ms
step:1478/1770 train_time:144839ms step_avg:98.66ms
step:1479/1770 train_time:144942ms step_avg:98.67ms
step:1480/1770 train_time:145045ms step_avg:98.67ms
step:1481/1770 train_time:145152ms step_avg:98.68ms
step:1482/1770 train_time:145255ms step_avg:98.68ms
step:1483/1770 train_time:145359ms step_avg:98.68ms
step:1484/1770 train_time:145462ms step_avg:98.69ms
step:1485/1770 train_time:145564ms step_avg:98.69ms
step:1486/1770 train_time:145667ms step_avg:98.69ms
step:1487/1770 train_time:145770ms step_avg:98.69ms
step:1488/1770 train_time:145874ms step_avg:98.70ms
step:1489/1770 train_time:145979ms step_avg:98.70ms
step:1490/1770 train_time:146084ms step_avg:98.71ms
step:1491/1770 train_time:146187ms step_avg:98.71ms
step:1492/1770 train_time:146291ms step_avg:98.71ms
step:1493/1770 train_time:146396ms step_avg:98.72ms
step:1494/1770 train_time:146503ms step_avg:98.72ms
step:1495/1770 train_time:146605ms step_avg:98.72ms
step:1496/1770 train_time:146708ms step_avg:98.73ms
step:1497/1770 train_time:146812ms step_avg:98.73ms
step:1498/1770 train_time:146915ms step_avg:98.73ms
step:1499/1770 train_time:147018ms step_avg:98.74ms
step:1500/1770 train_time:147120ms step_avg:98.74ms
step:1500/1770 val_loss:3.3451 train_time:147221ms step_avg:98.81ms
step:1501/1770 train_time:147242ms step_avg:98.75ms
step:1502/1770 train_time:147333ms step_avg:98.75ms
step:1503/1770 train_time:147436ms step_avg:98.75ms
step:1504/1770 train_time:147539ms step_avg:98.75ms
step:1505/1770 train_time:147644ms step_avg:98.76ms
step:1506/1770 train_time:147748ms step_avg:98.76ms
step:1507/1770 train_time:147851ms step_avg:98.77ms
step:1508/1770 train_time:147956ms step_avg:98.77ms
step:1509/1770 train_time:148058ms step_avg:98.77ms
step:1510/1770 train_time:148161ms step_avg:98.77ms
step:1511/1770 train_time:148266ms step_avg:98.78ms
step:1512/1770 train_time:148371ms step_avg:98.78ms
step:1513/1770 train_time:148476ms step_avg:98.79ms
step:1514/1770 train_time:148579ms step_avg:98.79ms
step:1515/1770 train_time:148682ms step_avg:98.79ms
step:1516/1770 train_time:148785ms step_avg:98.79ms
step:1517/1770 train_time:148888ms step_avg:98.80ms
step:1518/1770 train_time:148993ms step_avg:98.80ms
step:1519/1770 train_time:149095ms step_avg:98.80ms
step:1520/1770 train_time:149200ms step_avg:98.81ms
step:1521/1770 train_time:149302ms step_avg:98.81ms
step:1522/1770 train_time:149406ms step_avg:98.81ms
step:1523/1770 train_time:149510ms step_avg:98.82ms
step:1524/1770 train_time:149613ms step_avg:98.82ms
step:1525/1770 train_time:149717ms step_avg:98.82ms
step:1526/1770 train_time:149820ms step_avg:98.83ms
step:1527/1770 train_time:149923ms step_avg:98.83ms
step:1528/1770 train_time:150029ms step_avg:98.83ms
step:1529/1770 train_time:150132ms step_avg:98.84ms
step:1530/1770 train_time:150234ms step_avg:98.84ms
step:1531/1770 train_time:150339ms step_avg:98.84ms
step:1532/1770 train_time:150441ms step_avg:98.84ms
step:1533/1770 train_time:150545ms step_avg:98.85ms
step:1534/1770 train_time:150648ms step_avg:98.85ms
step:1535/1770 train_time:150752ms step_avg:98.85ms
step:1536/1770 train_time:150855ms step_avg:98.86ms
step:1537/1770 train_time:150959ms step_avg:98.86ms
step:1538/1770 train_time:151065ms step_avg:98.86ms
step:1539/1770 train_time:151168ms step_avg:98.87ms
step:1540/1770 train_time:151273ms step_avg:98.87ms
step:1541/1770 train_time:151378ms step_avg:98.88ms
step:1542/1770 train_time:151482ms step_avg:98.88ms
step:1543/1770 train_time:151584ms step_avg:98.88ms
step:1544/1770 train_time:151689ms step_avg:98.88ms
step:1545/1770 train_time:151793ms step_avg:98.89ms
step:1546/1770 train_time:151897ms step_avg:98.89ms
step:1547/1770 train_time:152001ms step_avg:98.89ms
step:1548/1770 train_time:152103ms step_avg:98.90ms
step:1549/1770 train_time:152206ms step_avg:98.90ms
step:1550/1770 train_time:152309ms step_avg:98.90ms
step:1551/1770 train_time:152413ms step_avg:98.91ms
step:1552/1770 train_time:152517ms step_avg:98.91ms
step:1553/1770 train_time:152621ms step_avg:98.91ms
step:1554/1770 train_time:152723ms step_avg:98.91ms
step:1555/1770 train_time:152827ms step_avg:98.92ms
step:1556/1770 train_time:152930ms step_avg:98.92ms
step:1557/1770 train_time:153033ms step_avg:98.92ms
step:1558/1770 train_time:153137ms step_avg:98.93ms
step:1559/1770 train_time:153240ms step_avg:98.93ms
step:1560/1770 train_time:153343ms step_avg:98.93ms
step:1561/1770 train_time:153449ms step_avg:98.94ms
step:1562/1770 train_time:153552ms step_avg:98.94ms
step:1563/1770 train_time:153655ms step_avg:98.94ms
step:1564/1770 train_time:153757ms step_avg:98.94ms
step:1565/1770 train_time:153860ms step_avg:98.95ms
step:1566/1770 train_time:153963ms step_avg:98.95ms
step:1567/1770 train_time:154067ms step_avg:98.95ms
step:1568/1770 train_time:154169ms step_avg:98.95ms
step:1569/1770 train_time:154276ms step_avg:98.96ms
step:1570/1770 train_time:154379ms step_avg:98.96ms
step:1571/1770 train_time:154482ms step_avg:98.96ms
step:1572/1770 train_time:154586ms step_avg:98.97ms
step:1573/1770 train_time:154691ms step_avg:98.97ms
step:1574/1770 train_time:154795ms step_avg:98.97ms
step:1575/1770 train_time:154897ms step_avg:98.98ms
step:1576/1770 train_time:155000ms step_avg:98.98ms
step:1577/1770 train_time:155104ms step_avg:98.98ms
step:1578/1770 train_time:155209ms step_avg:98.99ms
step:1579/1770 train_time:155312ms step_avg:98.99ms
step:1580/1770 train_time:155415ms step_avg:98.99ms
step:1581/1770 train_time:155521ms step_avg:98.99ms
step:1582/1770 train_time:155626ms step_avg:99.00ms
step:1583/1770 train_time:155729ms step_avg:99.00ms
step:1584/1770 train_time:155834ms step_avg:99.01ms
step:1585/1770 train_time:155938ms step_avg:99.01ms
step:1586/1770 train_time:156045ms step_avg:99.01ms
step:1587/1770 train_time:156149ms step_avg:99.02ms
step:1588/1770 train_time:156252ms step_avg:99.02ms
step:1589/1770 train_time:156358ms step_avg:99.02ms
step:1590/1770 train_time:156461ms step_avg:99.03ms
step:1591/1770 train_time:156563ms step_avg:99.03ms
step:1592/1770 train_time:156668ms step_avg:99.03ms
step:1593/1770 train_time:156771ms step_avg:99.03ms
step:1594/1770 train_time:156873ms step_avg:99.04ms
step:1595/1770 train_time:156977ms step_avg:99.04ms
step:1596/1770 train_time:157082ms step_avg:99.04ms
step:1597/1770 train_time:157185ms step_avg:99.05ms
step:1598/1770 train_time:157290ms step_avg:99.05ms
step:1599/1770 train_time:157394ms step_avg:99.05ms
step:1600/1770 train_time:157500ms step_avg:99.06ms
step:1601/1770 train_time:157604ms step_avg:99.06ms
step:1602/1770 train_time:157708ms step_avg:99.06ms
step:1603/1770 train_time:157812ms step_avg:99.07ms
step:1604/1770 train_time:157915ms step_avg:99.07ms
step:1605/1770 train_time:158017ms step_avg:99.07ms
step:1606/1770 train_time:158121ms step_avg:99.07ms
step:1607/1770 train_time:158227ms step_avg:99.08ms
step:1608/1770 train_time:158330ms step_avg:99.08ms
step:1609/1770 train_time:158433ms step_avg:99.08ms
step:1610/1770 train_time:158539ms step_avg:99.09ms
step:1611/1770 train_time:158643ms step_avg:99.09ms
step:1612/1770 train_time:158748ms step_avg:99.09ms
step:1613/1770 train_time:158851ms step_avg:99.10ms
step:1614/1770 train_time:158954ms step_avg:99.10ms
step:1615/1770 train_time:159058ms step_avg:99.10ms
step:1616/1770 train_time:159161ms step_avg:99.10ms
step:1617/1770 train_time:159266ms step_avg:99.11ms
step:1618/1770 train_time:159370ms step_avg:99.11ms
step:1619/1770 train_time:159474ms step_avg:99.11ms
step:1620/1770 train_time:159578ms step_avg:99.12ms
step:1621/1770 train_time:159681ms step_avg:99.12ms
step:1622/1770 train_time:159785ms step_avg:99.12ms
step:1623/1770 train_time:159891ms step_avg:99.13ms
step:1624/1770 train_time:159993ms step_avg:99.13ms
step:1625/1770 train_time:160096ms step_avg:99.13ms
step:1625/1770 val_loss:3.3108 train_time:160198ms step_avg:99.19ms
step:1626/1770 train_time:160220ms step_avg:99.15ms
step:1627/1770 train_time:160311ms step_avg:99.14ms
step:1628/1770 train_time:160413ms step_avg:99.14ms
step:1629/1770 train_time:160515ms step_avg:99.14ms
step:1630/1770 train_time:160618ms step_avg:99.15ms
step:1631/1770 train_time:160721ms step_avg:99.15ms
step:1632/1770 train_time:160824ms step_avg:99.15ms
step:1633/1770 train_time:160927ms step_avg:99.15ms
step:1634/1770 train_time:161030ms step_avg:99.16ms
step:1635/1770 train_time:161133ms step_avg:99.16ms
step:1636/1770 train_time:161237ms step_avg:99.16ms
step:1637/1770 train_time:161342ms step_avg:99.17ms
step:1638/1770 train_time:161445ms step_avg:99.17ms
step:1639/1770 train_time:161549ms step_avg:99.17ms
step:1640/1770 train_time:161654ms step_avg:99.17ms
step:1641/1770 train_time:161757ms step_avg:99.18ms
step:1642/1770 train_time:161860ms step_avg:99.18ms
step:1643/1770 train_time:161963ms step_avg:99.18ms
step:1644/1770 train_time:162068ms step_avg:99.18ms
step:1645/1770 train_time:162170ms step_avg:99.19ms
step:1646/1770 train_time:162275ms step_avg:99.19ms
step:1647/1770 train_time:162379ms step_avg:99.19ms
step:1648/1770 train_time:162482ms step_avg:99.20ms
step:1649/1770 train_time:162585ms step_avg:99.20ms
step:1650/1770 train_time:162689ms step_avg:99.20ms
step:1651/1770 train_time:162793ms step_avg:99.20ms
step:1652/1770 train_time:162896ms step_avg:99.21ms
step:1653/1770 train_time:163000ms step_avg:99.21ms
step:1654/1770 train_time:163106ms step_avg:99.21ms
step:1655/1770 train_time:163213ms step_avg:99.22ms
step:1656/1770 train_time:163316ms step_avg:99.22ms
step:1657/1770 train_time:163421ms step_avg:99.22ms
step:1658/1770 train_time:163524ms step_avg:99.23ms
step:1659/1770 train_time:163629ms step_avg:99.23ms
step:1660/1770 train_time:163733ms step_avg:99.23ms
step:1661/1770 train_time:163838ms step_avg:99.24ms
step:1662/1770 train_time:163942ms step_avg:99.24ms
step:1663/1770 train_time:164045ms step_avg:99.24ms
step:1664/1770 train_time:164148ms step_avg:99.24ms
step:1665/1770 train_time:164251ms step_avg:99.25ms
step:1666/1770 train_time:164355ms step_avg:99.25ms
step:1667/1770 train_time:164458ms step_avg:99.25ms
step:1668/1770 train_time:164561ms step_avg:99.25ms
step:1669/1770 train_time:164664ms step_avg:99.25ms
step:1670/1770 train_time:164767ms step_avg:99.26ms
step:1671/1770 train_time:164870ms step_avg:99.26ms
step:1672/1770 train_time:164975ms step_avg:99.26ms
step:1673/1770 train_time:165079ms step_avg:99.27ms
step:1674/1770 train_time:165183ms step_avg:99.27ms
step:1675/1770 train_time:165286ms step_avg:99.27ms
step:1676/1770 train_time:165391ms step_avg:99.27ms
step:1677/1770 train_time:165499ms step_avg:99.28ms
step:1678/1770 train_time:165601ms step_avg:99.28ms
step:1679/1770 train_time:165706ms step_avg:99.28ms
step:1680/1770 train_time:165809ms step_avg:99.29ms
step:1681/1770 train_time:165913ms step_avg:99.29ms
step:1682/1770 train_time:166018ms step_avg:99.29ms
step:1683/1770 train_time:166120ms step_avg:99.29ms
step:1684/1770 train_time:166223ms step_avg:99.30ms
step:1685/1770 train_time:166326ms step_avg:99.30ms
step:1686/1770 train_time:166431ms step_avg:99.30ms
step:1687/1770 train_time:166536ms step_avg:99.31ms
step:1688/1770 train_time:166639ms step_avg:99.31ms
step:1689/1770 train_time:166743ms step_avg:99.31ms
step:1690/1770 train_time:166847ms step_avg:99.31ms
step:1691/1770 train_time:166951ms step_avg:99.32ms
step:1692/1770 train_time:167055ms step_avg:99.32ms
step:1693/1770 train_time:167160ms step_avg:99.32ms
step:1694/1770 train_time:167262ms step_avg:99.32ms
step:1695/1770 train_time:167366ms step_avg:99.33ms
step:1696/1770 train_time:167471ms step_avg:99.33ms
step:1697/1770 train_time:167577ms step_avg:99.33ms
step:1698/1770 train_time:167681ms step_avg:99.34ms
step:1699/1770 train_time:167784ms step_avg:99.34ms
step:1700/1770 train_time:167886ms step_avg:99.34ms
step:1701/1770 train_time:167990ms step_avg:99.34ms
step:1702/1770 train_time:168095ms step_avg:99.35ms
step:1703/1770 train_time:168198ms step_avg:99.35ms
step:1704/1770 train_time:168302ms step_avg:99.35ms
step:1705/1770 train_time:168405ms step_avg:99.35ms
step:1706/1770 train_time:168508ms step_avg:99.36ms
step:1707/1770 train_time:168612ms step_avg:99.36ms
step:1708/1770 train_time:168716ms step_avg:99.36ms
step:1709/1770 train_time:168821ms step_avg:99.37ms
step:1710/1770 train_time:168928ms step_avg:99.37ms
step:1711/1770 train_time:169034ms step_avg:99.37ms
step:1712/1770 train_time:169138ms step_avg:99.38ms
step:1713/1770 train_time:169241ms step_avg:99.38ms
step:1714/1770 train_time:169346ms step_avg:99.38ms
step:1715/1770 train_time:169449ms step_avg:99.38ms
step:1716/1770 train_time:169553ms step_avg:99.39ms
step:1717/1770 train_time:169658ms step_avg:99.39ms
step:1718/1770 train_time:169763ms step_avg:99.39ms
step:1719/1770 train_time:169868ms step_avg:99.40ms
step:1720/1770 train_time:169973ms step_avg:99.40ms
step:1721/1770 train_time:170078ms step_avg:99.40ms
step:1722/1770 train_time:170185ms step_avg:99.41ms
step:1723/1770 train_time:170290ms step_avg:99.41ms
step:1724/1770 train_time:170396ms step_avg:99.41ms
step:1725/1770 train_time:170503ms step_avg:99.42ms
step:1726/1770 train_time:170609ms step_avg:99.42ms
step:1727/1770 train_time:170713ms step_avg:99.42ms
step:1728/1770 train_time:170818ms step_avg:99.43ms
step:1729/1770 train_time:170922ms step_avg:99.43ms
step:1730/1770 train_time:171027ms step_avg:99.43ms
step:1731/1770 train_time:171133ms step_avg:99.44ms
step:1732/1770 train_time:171237ms step_avg:99.44ms
step:1733/1770 train_time:171342ms step_avg:99.44ms
step:1734/1770 train_time:171445ms step_avg:99.45ms
step:1735/1770 train_time:171550ms step_avg:99.45ms
step:1736/1770 train_time:171654ms step_avg:99.45ms
step:1737/1770 train_time:171759ms step_avg:99.46ms
step:1738/1770 train_time:171863ms step_avg:99.46ms
step:1739/1770 train_time:171967ms step_avg:99.46ms
step:1740/1770 train_time:172071ms step_avg:99.46ms
step:1741/1770 train_time:172177ms step_avg:99.47ms
step:1742/1770 train_time:172285ms step_avg:99.47ms
step:1743/1770 train_time:172390ms step_avg:99.48ms
step:1744/1770 train_time:172495ms step_avg:99.48ms
step:1745/1770 train_time:172599ms step_avg:99.48ms
step:1746/1770 train_time:172707ms step_avg:99.49ms
step:1747/1770 train_time:172810ms step_avg:99.49ms
step:1748/1770 train_time:172916ms step_avg:99.49ms
step:1749/1770 train_time:173020ms step_avg:99.49ms
step:1750/1770 train_time:173124ms step_avg:99.50ms
step:1750/1770 val_loss:3.2834 train_time:173226ms step_avg:99.56ms
step:1751/1770 train_time:173247ms step_avg:99.51ms
step:1752/1770 train_time:173336ms step_avg:99.50ms
step:1753/1770 train_time:173440ms step_avg:99.51ms
step:1754/1770 train_time:173545ms step_avg:99.51ms
step:1755/1770 train_time:173648ms step_avg:99.51ms
step:1756/1770 train_time:173753ms step_avg:99.51ms
step:1757/1770 train_time:173857ms step_avg:99.52ms
step:1758/1770 train_time:173961ms step_avg:99.52ms
step:1759/1770 train_time:174065ms step_avg:99.52ms
step:1760/1770 train_time:174169ms step_avg:99.52ms
step:1761/1770 train_time:174276ms step_avg:99.53ms
step:1762/1770 train_time:174384ms step_avg:99.53ms
step:1763/1770 train_time:174487ms step_avg:99.54ms
step:1764/1770 train_time:174591ms step_avg:99.54ms
step:1765/1770 train_time:174696ms step_avg:99.54ms
step:1766/1770 train_time:174805ms step_avg:99.55ms
step:1767/1770 train_time:174908ms step_avg:99.55ms
step:1768/1770 train_time:175012ms step_avg:99.55ms
step:1769/1770 train_time:175115ms step_avg:99.55ms
step:1770/1770 train_time:175219ms step_avg:99.56ms
step:1770/1770 val_loss:3.2803 train_time:175324ms step_avg:99.62ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
