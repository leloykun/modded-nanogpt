import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:04:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24479ms step_avg:nanms
step:2/1770 train_time:24935ms step_avg:nanms
step:3/1770 train_time:25030ms step_avg:nanms
step:4/1770 train_time:25122ms step_avg:nanms
step:5/1770 train_time:25216ms step_avg:nanms
step:6/1770 train_time:25309ms step_avg:nanms
step:7/1770 train_time:25403ms step_avg:nanms
step:8/1770 train_time:25497ms step_avg:nanms
step:9/1770 train_time:25590ms step_avg:nanms
step:10/1770 train_time:25684ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.30ms
step:14/1770 train_time:378ms step_avg:94.50ms
step:15/1770 train_time:472ms step_avg:94.39ms
step:16/1770 train_time:567ms step_avg:94.44ms
step:17/1770 train_time:660ms step_avg:94.27ms
step:18/1770 train_time:754ms step_avg:94.19ms
step:19/1770 train_time:847ms step_avg:94.15ms
step:20/1770 train_time:941ms step_avg:94.11ms
step:21/1770 train_time:1035ms step_avg:94.07ms
step:22/1770 train_time:1128ms step_avg:94.01ms
step:23/1770 train_time:1222ms step_avg:94.03ms
step:24/1770 train_time:1317ms step_avg:94.05ms
step:25/1770 train_time:1410ms step_avg:94.03ms
step:26/1770 train_time:1505ms step_avg:94.04ms
step:27/1770 train_time:1599ms step_avg:94.04ms
step:28/1770 train_time:1693ms step_avg:94.03ms
step:29/1770 train_time:1786ms step_avg:94.02ms
step:30/1770 train_time:1880ms step_avg:93.98ms
step:31/1770 train_time:1974ms step_avg:93.98ms
step:32/1770 train_time:2068ms step_avg:93.98ms
step:33/1770 train_time:2161ms step_avg:93.98ms
step:34/1770 train_time:2255ms step_avg:93.96ms
step:35/1770 train_time:2349ms step_avg:93.96ms
step:36/1770 train_time:2444ms step_avg:93.98ms
step:37/1770 train_time:2538ms step_avg:93.99ms
step:38/1770 train_time:2632ms step_avg:93.99ms
step:39/1770 train_time:2726ms step_avg:93.98ms
step:40/1770 train_time:2820ms step_avg:94.00ms
step:41/1770 train_time:2914ms step_avg:94.00ms
step:42/1770 train_time:3008ms step_avg:93.99ms
step:43/1770 train_time:3101ms step_avg:93.97ms
step:44/1770 train_time:3195ms step_avg:93.98ms
step:45/1770 train_time:3289ms step_avg:93.98ms
step:46/1770 train_time:3384ms step_avg:94.00ms
step:47/1770 train_time:3478ms step_avg:93.99ms
step:48/1770 train_time:3571ms step_avg:93.98ms
step:49/1770 train_time:3665ms step_avg:93.98ms
step:50/1770 train_time:3759ms step_avg:93.98ms
step:51/1770 train_time:3853ms step_avg:93.98ms
step:52/1770 train_time:3947ms step_avg:93.97ms
step:53/1770 train_time:4041ms step_avg:93.97ms
step:54/1770 train_time:4135ms step_avg:93.97ms
step:55/1770 train_time:4228ms step_avg:93.97ms
step:56/1770 train_time:4322ms step_avg:93.96ms
step:57/1770 train_time:4416ms step_avg:93.96ms
step:58/1770 train_time:4510ms step_avg:93.96ms
step:59/1770 train_time:4604ms step_avg:93.95ms
step:60/1770 train_time:4698ms step_avg:93.97ms
step:61/1770 train_time:4792ms step_avg:93.95ms
step:62/1770 train_time:4885ms step_avg:93.95ms
step:63/1770 train_time:4979ms step_avg:93.94ms
step:64/1770 train_time:5072ms step_avg:93.93ms
step:65/1770 train_time:5166ms step_avg:93.93ms
step:66/1770 train_time:5260ms step_avg:93.93ms
step:67/1770 train_time:5354ms step_avg:93.93ms
step:68/1770 train_time:5448ms step_avg:93.93ms
step:69/1770 train_time:5543ms step_avg:93.95ms
step:70/1770 train_time:5638ms step_avg:93.96ms
step:71/1770 train_time:5732ms step_avg:93.96ms
step:72/1770 train_time:5825ms step_avg:93.95ms
step:73/1770 train_time:5919ms step_avg:93.95ms
step:74/1770 train_time:6012ms step_avg:93.94ms
step:75/1770 train_time:6106ms step_avg:93.94ms
step:76/1770 train_time:6199ms step_avg:93.93ms
step:77/1770 train_time:6293ms step_avg:93.93ms
step:78/1770 train_time:6387ms step_avg:93.93ms
step:79/1770 train_time:6481ms step_avg:93.93ms
step:80/1770 train_time:6575ms step_avg:93.93ms
step:81/1770 train_time:6671ms step_avg:93.95ms
step:82/1770 train_time:6763ms step_avg:93.93ms
step:83/1770 train_time:6858ms step_avg:93.94ms
step:84/1770 train_time:6951ms step_avg:93.94ms
step:85/1770 train_time:7045ms step_avg:93.94ms
step:86/1770 train_time:7139ms step_avg:93.94ms
step:87/1770 train_time:7233ms step_avg:93.93ms
step:88/1770 train_time:7326ms step_avg:93.93ms
step:89/1770 train_time:7420ms step_avg:93.93ms
step:90/1770 train_time:7514ms step_avg:93.93ms
step:91/1770 train_time:7609ms step_avg:93.94ms
step:92/1770 train_time:7703ms step_avg:93.94ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7984ms step_avg:93.93ms
step:96/1770 train_time:8078ms step_avg:93.93ms
step:97/1770 train_time:8171ms step_avg:93.92ms
step:98/1770 train_time:8265ms step_avg:93.92ms
step:99/1770 train_time:8359ms step_avg:93.92ms
step:100/1770 train_time:8455ms step_avg:93.95ms
step:101/1770 train_time:8547ms step_avg:93.92ms
step:102/1770 train_time:8641ms step_avg:93.93ms
step:103/1770 train_time:8736ms step_avg:93.94ms
step:104/1770 train_time:8830ms step_avg:93.93ms
step:105/1770 train_time:8924ms step_avg:93.93ms
step:106/1770 train_time:9018ms step_avg:93.93ms
step:107/1770 train_time:9111ms step_avg:93.93ms
step:108/1770 train_time:9205ms step_avg:93.93ms
step:109/1770 train_time:9299ms step_avg:93.93ms
step:110/1770 train_time:9392ms step_avg:93.92ms
step:111/1770 train_time:9486ms step_avg:93.92ms
step:112/1770 train_time:9580ms step_avg:93.92ms
step:113/1770 train_time:9674ms step_avg:93.92ms
step:114/1770 train_time:9768ms step_avg:93.92ms
step:115/1770 train_time:9862ms step_avg:93.93ms
step:116/1770 train_time:9960ms step_avg:93.97ms
step:117/1770 train_time:10050ms step_avg:93.92ms
step:118/1770 train_time:10143ms step_avg:93.92ms
step:119/1770 train_time:10237ms step_avg:93.92ms
step:120/1770 train_time:10331ms step_avg:93.92ms
step:121/1770 train_time:10426ms step_avg:93.93ms
step:122/1770 train_time:10519ms step_avg:93.92ms
step:123/1770 train_time:10612ms step_avg:93.92ms
step:124/1770 train_time:10706ms step_avg:93.92ms
step:125/1770 train_time:10800ms step_avg:93.92ms
step:125/1770 val_loss:4.6510 train_time:10893ms step_avg:94.72ms
step:126/1770 train_time:10916ms step_avg:94.10ms
step:127/1770 train_time:10993ms step_avg:93.95ms
step:128/1770 train_time:11091ms step_avg:93.99ms
step:129/1770 train_time:11193ms step_avg:94.06ms
step:130/1770 train_time:11288ms step_avg:94.06ms
step:131/1770 train_time:11381ms step_avg:94.06ms
step:132/1770 train_time:11474ms step_avg:94.05ms
step:133/1770 train_time:11568ms step_avg:94.05ms
step:134/1770 train_time:11662ms step_avg:94.05ms
step:135/1770 train_time:11756ms step_avg:94.05ms
step:136/1770 train_time:11850ms step_avg:94.05ms
step:137/1770 train_time:11947ms step_avg:94.07ms
step:138/1770 train_time:12038ms step_avg:94.05ms
step:139/1770 train_time:12134ms step_avg:94.06ms
step:140/1770 train_time:12229ms step_avg:94.07ms
step:141/1770 train_time:12323ms step_avg:94.07ms
step:142/1770 train_time:12418ms step_avg:94.07ms
step:143/1770 train_time:12512ms step_avg:94.08ms
step:144/1770 train_time:12607ms step_avg:94.08ms
step:145/1770 train_time:12701ms step_avg:94.08ms
step:146/1770 train_time:12795ms step_avg:94.08ms
step:147/1770 train_time:12890ms step_avg:94.09ms
step:148/1770 train_time:12984ms step_avg:94.09ms
step:149/1770 train_time:13079ms step_avg:94.09ms
step:150/1770 train_time:13175ms step_avg:94.10ms
step:151/1770 train_time:13269ms step_avg:94.11ms
step:152/1770 train_time:13364ms step_avg:94.11ms
step:153/1770 train_time:13459ms step_avg:94.12ms
step:154/1770 train_time:13554ms step_avg:94.12ms
step:155/1770 train_time:13648ms step_avg:94.12ms
step:156/1770 train_time:13742ms step_avg:94.12ms
step:157/1770 train_time:13837ms step_avg:94.13ms
step:158/1770 train_time:13932ms step_avg:94.14ms
step:159/1770 train_time:14025ms step_avg:94.13ms
step:160/1770 train_time:14120ms step_avg:94.13ms
step:161/1770 train_time:14214ms step_avg:94.13ms
step:162/1770 train_time:14308ms step_avg:94.13ms
step:163/1770 train_time:14403ms step_avg:94.13ms
step:164/1770 train_time:14497ms step_avg:94.14ms
step:165/1770 train_time:14591ms step_avg:94.14ms
step:166/1770 train_time:14685ms step_avg:94.14ms
step:167/1770 train_time:14780ms step_avg:94.14ms
step:168/1770 train_time:14875ms step_avg:94.14ms
step:169/1770 train_time:14969ms step_avg:94.14ms
step:170/1770 train_time:15063ms step_avg:94.14ms
step:171/1770 train_time:15158ms step_avg:94.15ms
step:172/1770 train_time:15252ms step_avg:94.15ms
step:173/1770 train_time:15347ms step_avg:94.15ms
step:174/1770 train_time:15441ms step_avg:94.15ms
step:175/1770 train_time:15536ms step_avg:94.16ms
step:176/1770 train_time:15631ms step_avg:94.16ms
step:177/1770 train_time:15725ms step_avg:94.16ms
step:178/1770 train_time:15819ms step_avg:94.16ms
step:179/1770 train_time:15914ms step_avg:94.16ms
step:180/1770 train_time:16008ms step_avg:94.16ms
step:181/1770 train_time:16103ms step_avg:94.17ms
step:182/1770 train_time:16197ms step_avg:94.17ms
step:183/1770 train_time:16292ms step_avg:94.17ms
step:184/1770 train_time:16386ms step_avg:94.17ms
step:185/1770 train_time:16481ms step_avg:94.18ms
step:186/1770 train_time:16576ms step_avg:94.18ms
step:187/1770 train_time:16670ms step_avg:94.18ms
step:188/1770 train_time:16765ms step_avg:94.18ms
step:189/1770 train_time:16860ms step_avg:94.19ms
step:190/1770 train_time:16954ms step_avg:94.19ms
step:191/1770 train_time:17049ms step_avg:94.19ms
step:192/1770 train_time:17143ms step_avg:94.19ms
step:193/1770 train_time:17238ms step_avg:94.20ms
step:194/1770 train_time:17333ms step_avg:94.20ms
step:195/1770 train_time:17427ms step_avg:94.20ms
step:196/1770 train_time:17521ms step_avg:94.20ms
step:197/1770 train_time:17616ms step_avg:94.20ms
step:198/1770 train_time:17710ms step_avg:94.20ms
step:199/1770 train_time:17805ms step_avg:94.21ms
step:200/1770 train_time:17899ms step_avg:94.21ms
step:201/1770 train_time:17993ms step_avg:94.20ms
step:202/1770 train_time:18087ms step_avg:94.20ms
step:203/1770 train_time:18182ms step_avg:94.21ms
step:204/1770 train_time:18277ms step_avg:94.21ms
step:205/1770 train_time:18371ms step_avg:94.21ms
step:206/1770 train_time:18465ms step_avg:94.21ms
step:207/1770 train_time:18560ms step_avg:94.21ms
step:208/1770 train_time:18655ms step_avg:94.22ms
step:209/1770 train_time:18749ms step_avg:94.22ms
step:210/1770 train_time:18844ms step_avg:94.22ms
step:211/1770 train_time:18939ms step_avg:94.23ms
step:212/1770 train_time:19033ms step_avg:94.22ms
step:213/1770 train_time:19127ms step_avg:94.22ms
step:214/1770 train_time:19222ms step_avg:94.22ms
step:215/1770 train_time:19316ms step_avg:94.23ms
step:216/1770 train_time:19410ms step_avg:94.23ms
step:217/1770 train_time:19505ms step_avg:94.23ms
step:218/1770 train_time:19600ms step_avg:94.23ms
step:219/1770 train_time:19694ms step_avg:94.23ms
step:220/1770 train_time:19788ms step_avg:94.23ms
step:221/1770 train_time:19883ms step_avg:94.23ms
step:222/1770 train_time:19978ms step_avg:94.23ms
step:223/1770 train_time:20073ms step_avg:94.24ms
step:224/1770 train_time:20168ms step_avg:94.24ms
step:225/1770 train_time:20262ms step_avg:94.24ms
step:226/1770 train_time:20357ms step_avg:94.25ms
step:227/1770 train_time:20451ms step_avg:94.25ms
step:228/1770 train_time:20546ms step_avg:94.25ms
step:229/1770 train_time:20641ms step_avg:94.25ms
step:230/1770 train_time:20735ms step_avg:94.25ms
step:231/1770 train_time:20830ms step_avg:94.25ms
step:232/1770 train_time:20924ms step_avg:94.25ms
step:233/1770 train_time:21018ms step_avg:94.25ms
step:234/1770 train_time:21114ms step_avg:94.26ms
step:235/1770 train_time:21208ms step_avg:94.26ms
step:236/1770 train_time:21302ms step_avg:94.26ms
step:237/1770 train_time:21396ms step_avg:94.26ms
step:238/1770 train_time:21492ms step_avg:94.26ms
step:239/1770 train_time:21586ms step_avg:94.26ms
step:240/1770 train_time:21681ms step_avg:94.27ms
step:241/1770 train_time:21776ms step_avg:94.27ms
step:242/1770 train_time:21871ms step_avg:94.27ms
step:243/1770 train_time:21965ms step_avg:94.27ms
step:244/1770 train_time:22060ms step_avg:94.27ms
step:245/1770 train_time:22154ms step_avg:94.27ms
step:246/1770 train_time:22249ms step_avg:94.27ms
step:247/1770 train_time:22343ms step_avg:94.27ms
step:248/1770 train_time:22438ms step_avg:94.28ms
step:249/1770 train_time:22533ms step_avg:94.28ms
step:250/1770 train_time:22628ms step_avg:94.28ms
step:250/1770 val_loss:4.1205 train_time:22721ms step_avg:94.67ms
step:251/1770 train_time:22743ms step_avg:94.37ms
step:252/1770 train_time:22825ms step_avg:94.32ms
step:253/1770 train_time:22924ms step_avg:94.34ms
step:254/1770 train_time:23019ms step_avg:94.34ms
step:255/1770 train_time:23114ms step_avg:94.34ms
step:256/1770 train_time:23208ms step_avg:94.34ms
step:257/1770 train_time:23302ms step_avg:94.34ms
step:258/1770 train_time:23396ms step_avg:94.34ms
step:259/1770 train_time:23490ms step_avg:94.34ms
step:260/1770 train_time:23584ms step_avg:94.34ms
step:261/1770 train_time:23678ms step_avg:94.34ms
step:262/1770 train_time:23773ms step_avg:94.34ms
step:263/1770 train_time:23868ms step_avg:94.34ms
step:264/1770 train_time:23964ms step_avg:94.35ms
step:265/1770 train_time:24059ms step_avg:94.35ms
step:266/1770 train_time:24154ms step_avg:94.35ms
step:267/1770 train_time:24250ms step_avg:94.36ms
step:268/1770 train_time:24344ms step_avg:94.36ms
step:269/1770 train_time:24439ms step_avg:94.36ms
step:270/1770 train_time:24534ms step_avg:94.36ms
step:271/1770 train_time:24628ms step_avg:94.36ms
step:272/1770 train_time:24723ms step_avg:94.36ms
step:273/1770 train_time:24819ms step_avg:94.37ms
step:274/1770 train_time:24915ms step_avg:94.37ms
step:275/1770 train_time:25010ms step_avg:94.38ms
step:276/1770 train_time:25106ms step_avg:94.38ms
step:277/1770 train_time:25201ms step_avg:94.39ms
step:278/1770 train_time:25296ms step_avg:94.39ms
step:279/1770 train_time:25391ms step_avg:94.39ms
step:280/1770 train_time:25486ms step_avg:94.39ms
step:281/1770 train_time:25581ms step_avg:94.39ms
step:282/1770 train_time:25676ms step_avg:94.40ms
step:283/1770 train_time:25771ms step_avg:94.40ms
step:284/1770 train_time:25865ms step_avg:94.40ms
step:285/1770 train_time:25961ms step_avg:94.40ms
step:286/1770 train_time:26056ms step_avg:94.41ms
step:287/1770 train_time:26152ms step_avg:94.41ms
step:288/1770 train_time:26247ms step_avg:94.41ms
step:289/1770 train_time:26342ms step_avg:94.42ms
step:290/1770 train_time:26437ms step_avg:94.42ms
step:291/1770 train_time:26532ms step_avg:94.42ms
step:292/1770 train_time:26627ms step_avg:94.42ms
step:293/1770 train_time:26722ms step_avg:94.42ms
step:294/1770 train_time:26816ms step_avg:94.42ms
step:295/1770 train_time:26912ms step_avg:94.43ms
step:296/1770 train_time:27006ms step_avg:94.43ms
step:297/1770 train_time:27102ms step_avg:94.43ms
step:298/1770 train_time:27197ms step_avg:94.44ms
step:299/1770 train_time:27293ms step_avg:94.44ms
step:300/1770 train_time:27388ms step_avg:94.44ms
step:301/1770 train_time:27482ms step_avg:94.44ms
step:302/1770 train_time:27577ms step_avg:94.44ms
step:303/1770 train_time:27673ms step_avg:94.45ms
step:304/1770 train_time:27768ms step_avg:94.45ms
step:305/1770 train_time:27863ms step_avg:94.45ms
step:306/1770 train_time:27958ms step_avg:94.45ms
step:307/1770 train_time:28053ms step_avg:94.46ms
step:308/1770 train_time:28148ms step_avg:94.46ms
step:309/1770 train_time:28243ms step_avg:94.46ms
step:310/1770 train_time:28338ms step_avg:94.46ms
step:311/1770 train_time:28433ms step_avg:94.46ms
step:312/1770 train_time:28528ms step_avg:94.46ms
step:313/1770 train_time:28622ms step_avg:94.46ms
step:314/1770 train_time:28718ms step_avg:94.47ms
step:315/1770 train_time:28814ms step_avg:94.47ms
step:316/1770 train_time:28909ms step_avg:94.47ms
step:317/1770 train_time:29004ms step_avg:94.48ms
step:318/1770 train_time:29099ms step_avg:94.48ms
step:319/1770 train_time:29194ms step_avg:94.48ms
step:320/1770 train_time:29289ms step_avg:94.48ms
step:321/1770 train_time:29385ms step_avg:94.48ms
step:322/1770 train_time:29480ms step_avg:94.49ms
step:323/1770 train_time:29574ms step_avg:94.49ms
step:324/1770 train_time:29669ms step_avg:94.49ms
step:325/1770 train_time:29764ms step_avg:94.49ms
step:326/1770 train_time:29859ms step_avg:94.49ms
step:327/1770 train_time:29955ms step_avg:94.49ms
step:328/1770 train_time:30049ms step_avg:94.49ms
step:329/1770 train_time:30144ms step_avg:94.50ms
step:330/1770 train_time:30239ms step_avg:94.50ms
step:331/1770 train_time:30334ms step_avg:94.50ms
step:332/1770 train_time:30430ms step_avg:94.50ms
step:333/1770 train_time:30524ms step_avg:94.50ms
step:334/1770 train_time:30619ms step_avg:94.50ms
step:335/1770 train_time:30713ms step_avg:94.50ms
step:336/1770 train_time:30808ms step_avg:94.50ms
step:337/1770 train_time:30903ms step_avg:94.51ms
step:338/1770 train_time:30998ms step_avg:94.51ms
step:339/1770 train_time:31093ms step_avg:94.51ms
step:340/1770 train_time:31188ms step_avg:94.51ms
step:341/1770 train_time:31283ms step_avg:94.51ms
step:342/1770 train_time:31379ms step_avg:94.51ms
step:343/1770 train_time:31474ms step_avg:94.52ms
step:344/1770 train_time:31569ms step_avg:94.52ms
step:345/1770 train_time:31663ms step_avg:94.52ms
step:346/1770 train_time:31758ms step_avg:94.52ms
step:347/1770 train_time:31853ms step_avg:94.52ms
step:348/1770 train_time:31948ms step_avg:94.52ms
step:349/1770 train_time:32043ms step_avg:94.52ms
step:350/1770 train_time:32138ms step_avg:94.52ms
step:351/1770 train_time:32233ms step_avg:94.53ms
step:352/1770 train_time:32328ms step_avg:94.53ms
step:353/1770 train_time:32424ms step_avg:94.53ms
step:354/1770 train_time:32519ms step_avg:94.53ms
step:355/1770 train_time:32614ms step_avg:94.53ms
step:356/1770 train_time:32709ms step_avg:94.54ms
step:357/1770 train_time:32805ms step_avg:94.54ms
step:358/1770 train_time:32900ms step_avg:94.54ms
step:359/1770 train_time:32995ms step_avg:94.54ms
step:360/1770 train_time:33090ms step_avg:94.54ms
step:361/1770 train_time:33185ms step_avg:94.54ms
step:362/1770 train_time:33281ms step_avg:94.55ms
step:363/1770 train_time:33376ms step_avg:94.55ms
step:364/1770 train_time:33471ms step_avg:94.55ms
step:365/1770 train_time:33566ms step_avg:94.55ms
step:366/1770 train_time:33661ms step_avg:94.55ms
step:367/1770 train_time:33756ms step_avg:94.55ms
step:368/1770 train_time:33851ms step_avg:94.55ms
step:369/1770 train_time:33946ms step_avg:94.56ms
step:370/1770 train_time:34041ms step_avg:94.56ms
step:371/1770 train_time:34136ms step_avg:94.56ms
step:372/1770 train_time:34231ms step_avg:94.56ms
step:373/1770 train_time:34325ms step_avg:94.56ms
step:374/1770 train_time:34420ms step_avg:94.56ms
step:375/1770 train_time:34516ms step_avg:94.56ms
step:375/1770 val_loss:3.9094 train_time:34609ms step_avg:94.82ms
step:376/1770 train_time:34632ms step_avg:94.62ms
step:377/1770 train_time:34713ms step_avg:94.59ms
step:378/1770 train_time:34811ms step_avg:94.59ms
step:379/1770 train_time:34906ms step_avg:94.60ms
step:380/1770 train_time:35000ms step_avg:94.59ms
step:381/1770 train_time:35095ms step_avg:94.59ms
step:382/1770 train_time:35190ms step_avg:94.60ms
step:383/1770 train_time:35284ms step_avg:94.60ms
step:384/1770 train_time:35378ms step_avg:94.59ms
step:385/1770 train_time:35473ms step_avg:94.59ms
step:386/1770 train_time:35568ms step_avg:94.60ms
step:387/1770 train_time:35663ms step_avg:94.60ms
step:388/1770 train_time:35758ms step_avg:94.60ms
step:389/1770 train_time:35854ms step_avg:94.60ms
step:390/1770 train_time:35950ms step_avg:94.61ms
step:391/1770 train_time:36045ms step_avg:94.61ms
step:392/1770 train_time:36140ms step_avg:94.61ms
step:393/1770 train_time:36235ms step_avg:94.61ms
step:394/1770 train_time:36330ms step_avg:94.61ms
step:395/1770 train_time:36425ms step_avg:94.61ms
step:396/1770 train_time:36522ms step_avg:94.62ms
step:397/1770 train_time:36618ms step_avg:94.62ms
step:398/1770 train_time:36715ms step_avg:94.63ms
step:399/1770 train_time:36818ms step_avg:94.65ms
step:400/1770 train_time:36910ms step_avg:94.64ms
step:401/1770 train_time:37008ms step_avg:94.65ms
step:402/1770 train_time:37104ms step_avg:94.65ms
step:403/1770 train_time:37201ms step_avg:94.66ms
step:404/1770 train_time:37298ms step_avg:94.67ms
step:405/1770 train_time:37395ms step_avg:94.67ms
step:406/1770 train_time:37492ms step_avg:94.68ms
step:407/1770 train_time:37588ms step_avg:94.68ms
step:408/1770 train_time:37685ms step_avg:94.69ms
step:409/1770 train_time:37782ms step_avg:94.69ms
step:410/1770 train_time:37879ms step_avg:94.70ms
step:411/1770 train_time:37976ms step_avg:94.70ms
step:412/1770 train_time:38073ms step_avg:94.71ms
step:413/1770 train_time:38170ms step_avg:94.72ms
step:414/1770 train_time:38268ms step_avg:94.72ms
step:415/1770 train_time:38365ms step_avg:94.73ms
step:416/1770 train_time:38463ms step_avg:94.74ms
step:417/1770 train_time:38559ms step_avg:94.74ms
step:418/1770 train_time:38655ms step_avg:94.74ms
step:419/1770 train_time:38752ms step_avg:94.75ms
step:420/1770 train_time:38850ms step_avg:94.76ms
step:421/1770 train_time:38947ms step_avg:94.76ms
step:422/1770 train_time:39043ms step_avg:94.77ms
step:423/1770 train_time:39140ms step_avg:94.77ms
step:424/1770 train_time:39237ms step_avg:94.77ms
step:425/1770 train_time:39334ms step_avg:94.78ms
step:426/1770 train_time:39432ms step_avg:94.79ms
step:427/1770 train_time:39529ms step_avg:94.79ms
step:428/1770 train_time:39626ms step_avg:94.80ms
step:429/1770 train_time:39723ms step_avg:94.80ms
step:430/1770 train_time:39820ms step_avg:94.81ms
step:431/1770 train_time:39916ms step_avg:94.81ms
step:432/1770 train_time:40013ms step_avg:94.82ms
step:433/1770 train_time:40109ms step_avg:94.82ms
step:434/1770 train_time:40207ms step_avg:94.83ms
step:435/1770 train_time:40304ms step_avg:94.83ms
step:436/1770 train_time:40401ms step_avg:94.84ms
step:437/1770 train_time:40499ms step_avg:94.84ms
step:438/1770 train_time:40595ms step_avg:94.85ms
step:439/1770 train_time:40693ms step_avg:94.85ms
step:440/1770 train_time:40790ms step_avg:94.86ms
step:441/1770 train_time:40887ms step_avg:94.86ms
step:442/1770 train_time:40983ms step_avg:94.87ms
step:443/1770 train_time:41080ms step_avg:94.87ms
step:444/1770 train_time:41177ms step_avg:94.88ms
step:445/1770 train_time:41274ms step_avg:94.88ms
step:446/1770 train_time:41371ms step_avg:94.89ms
step:447/1770 train_time:41468ms step_avg:94.89ms
step:448/1770 train_time:41565ms step_avg:94.90ms
step:449/1770 train_time:41662ms step_avg:94.90ms
step:450/1770 train_time:41759ms step_avg:94.91ms
step:451/1770 train_time:41856ms step_avg:94.91ms
step:452/1770 train_time:41952ms step_avg:94.91ms
step:453/1770 train_time:42050ms step_avg:94.92ms
step:454/1770 train_time:42146ms step_avg:94.92ms
step:455/1770 train_time:42244ms step_avg:94.93ms
step:456/1770 train_time:42340ms step_avg:94.93ms
step:457/1770 train_time:42437ms step_avg:94.94ms
step:458/1770 train_time:42534ms step_avg:94.94ms
step:459/1770 train_time:42632ms step_avg:94.95ms
step:460/1770 train_time:42729ms step_avg:94.95ms
step:461/1770 train_time:42826ms step_avg:94.96ms
step:462/1770 train_time:42922ms step_avg:94.96ms
step:463/1770 train_time:43021ms step_avg:94.97ms
step:464/1770 train_time:43117ms step_avg:94.97ms
step:465/1770 train_time:43214ms step_avg:94.98ms
step:466/1770 train_time:43311ms step_avg:94.98ms
step:467/1770 train_time:43408ms step_avg:94.99ms
step:468/1770 train_time:43505ms step_avg:94.99ms
step:469/1770 train_time:43602ms step_avg:94.99ms
step:470/1770 train_time:43699ms step_avg:95.00ms
step:471/1770 train_time:43795ms step_avg:95.00ms
step:472/1770 train_time:43893ms step_avg:95.01ms
step:473/1770 train_time:43990ms step_avg:95.01ms
step:474/1770 train_time:44087ms step_avg:95.01ms
step:475/1770 train_time:44184ms step_avg:95.02ms
step:476/1770 train_time:44281ms step_avg:95.02ms
step:477/1770 train_time:44378ms step_avg:95.03ms
step:478/1770 train_time:44475ms step_avg:95.03ms
step:479/1770 train_time:44573ms step_avg:95.04ms
step:480/1770 train_time:44670ms step_avg:95.04ms
step:481/1770 train_time:44768ms step_avg:95.05ms
step:482/1770 train_time:44865ms step_avg:95.05ms
step:483/1770 train_time:44961ms step_avg:95.06ms
step:484/1770 train_time:45058ms step_avg:95.06ms
step:485/1770 train_time:45154ms step_avg:95.06ms
step:486/1770 train_time:45251ms step_avg:95.07ms
step:487/1770 train_time:45349ms step_avg:95.07ms
step:488/1770 train_time:45446ms step_avg:95.07ms
step:489/1770 train_time:45543ms step_avg:95.08ms
step:490/1770 train_time:45640ms step_avg:95.08ms
step:491/1770 train_time:45737ms step_avg:95.09ms
step:492/1770 train_time:45835ms step_avg:95.09ms
step:493/1770 train_time:45932ms step_avg:95.10ms
step:494/1770 train_time:46029ms step_avg:95.10ms
step:495/1770 train_time:46126ms step_avg:95.11ms
step:496/1770 train_time:46223ms step_avg:95.11ms
step:497/1770 train_time:46320ms step_avg:95.11ms
step:498/1770 train_time:46417ms step_avg:95.12ms
step:499/1770 train_time:46514ms step_avg:95.12ms
step:500/1770 train_time:46611ms step_avg:95.12ms
step:500/1770 val_loss:3.7562 train_time:46707ms step_avg:95.32ms
step:501/1770 train_time:46730ms step_avg:95.17ms
step:502/1770 train_time:46817ms step_avg:95.16ms
step:503/1770 train_time:46917ms step_avg:95.17ms
step:504/1770 train_time:47014ms step_avg:95.17ms
step:505/1770 train_time:47111ms step_avg:95.17ms
step:506/1770 train_time:47208ms step_avg:95.18ms
step:507/1770 train_time:47304ms step_avg:95.18ms
step:508/1770 train_time:47401ms step_avg:95.18ms
step:509/1770 train_time:47498ms step_avg:95.19ms
step:510/1770 train_time:47594ms step_avg:95.19ms
step:511/1770 train_time:47691ms step_avg:95.19ms
step:512/1770 train_time:47790ms step_avg:95.20ms
step:513/1770 train_time:47888ms step_avg:95.21ms
step:514/1770 train_time:47986ms step_avg:95.21ms
step:515/1770 train_time:48084ms step_avg:95.22ms
step:516/1770 train_time:48181ms step_avg:95.22ms
step:517/1770 train_time:48278ms step_avg:95.22ms
step:518/1770 train_time:48374ms step_avg:95.22ms
step:519/1770 train_time:48471ms step_avg:95.23ms
step:520/1770 train_time:48568ms step_avg:95.23ms
step:521/1770 train_time:48665ms step_avg:95.23ms
step:522/1770 train_time:48761ms step_avg:95.24ms
step:523/1770 train_time:48859ms step_avg:95.24ms
step:524/1770 train_time:48956ms step_avg:95.25ms
step:525/1770 train_time:49053ms step_avg:95.25ms
step:526/1770 train_time:49151ms step_avg:95.25ms
step:527/1770 train_time:49249ms step_avg:95.26ms
step:528/1770 train_time:49347ms step_avg:95.26ms
step:529/1770 train_time:49444ms step_avg:95.27ms
step:530/1770 train_time:49542ms step_avg:95.27ms
step:531/1770 train_time:49639ms step_avg:95.28ms
step:532/1770 train_time:49736ms step_avg:95.28ms
step:533/1770 train_time:49834ms step_avg:95.28ms
step:534/1770 train_time:49931ms step_avg:95.29ms
step:535/1770 train_time:50030ms step_avg:95.29ms
step:536/1770 train_time:50128ms step_avg:95.30ms
step:537/1770 train_time:50225ms step_avg:95.30ms
step:538/1770 train_time:50323ms step_avg:95.31ms
step:539/1770 train_time:50420ms step_avg:95.31ms
step:540/1770 train_time:50517ms step_avg:95.32ms
step:541/1770 train_time:50614ms step_avg:95.32ms
step:542/1770 train_time:50711ms step_avg:95.32ms
step:543/1770 train_time:50809ms step_avg:95.33ms
step:544/1770 train_time:50906ms step_avg:95.33ms
step:545/1770 train_time:51004ms step_avg:95.33ms
step:546/1770 train_time:51101ms step_avg:95.34ms
step:547/1770 train_time:51198ms step_avg:95.34ms
step:548/1770 train_time:51296ms step_avg:95.35ms
step:549/1770 train_time:51393ms step_avg:95.35ms
step:550/1770 train_time:51491ms step_avg:95.35ms
step:551/1770 train_time:51588ms step_avg:95.36ms
step:552/1770 train_time:51686ms step_avg:95.36ms
step:553/1770 train_time:51784ms step_avg:95.37ms
step:554/1770 train_time:51881ms step_avg:95.37ms
step:555/1770 train_time:51978ms step_avg:95.37ms
step:556/1770 train_time:52076ms step_avg:95.38ms
step:557/1770 train_time:52174ms step_avg:95.38ms
step:558/1770 train_time:52272ms step_avg:95.39ms
step:559/1770 train_time:52369ms step_avg:95.39ms
step:560/1770 train_time:52466ms step_avg:95.39ms
step:561/1770 train_time:52564ms step_avg:95.40ms
step:562/1770 train_time:52662ms step_avg:95.40ms
step:563/1770 train_time:52759ms step_avg:95.41ms
step:564/1770 train_time:52856ms step_avg:95.41ms
step:565/1770 train_time:52953ms step_avg:95.41ms
step:566/1770 train_time:53051ms step_avg:95.42ms
step:567/1770 train_time:53149ms step_avg:95.42ms
step:568/1770 train_time:53247ms step_avg:95.42ms
step:569/1770 train_time:53344ms step_avg:95.43ms
step:570/1770 train_time:53441ms step_avg:95.43ms
step:571/1770 train_time:53539ms step_avg:95.43ms
step:572/1770 train_time:53636ms step_avg:95.44ms
step:573/1770 train_time:53734ms step_avg:95.44ms
step:574/1770 train_time:53831ms step_avg:95.45ms
step:575/1770 train_time:53928ms step_avg:95.45ms
step:576/1770 train_time:54026ms step_avg:95.45ms
step:577/1770 train_time:54123ms step_avg:95.46ms
step:578/1770 train_time:54221ms step_avg:95.46ms
step:579/1770 train_time:54318ms step_avg:95.46ms
step:580/1770 train_time:54415ms step_avg:95.46ms
step:581/1770 train_time:54512ms step_avg:95.47ms
step:582/1770 train_time:54610ms step_avg:95.47ms
step:583/1770 train_time:54708ms step_avg:95.48ms
step:584/1770 train_time:54805ms step_avg:95.48ms
step:585/1770 train_time:54902ms step_avg:95.48ms
step:586/1770 train_time:54999ms step_avg:95.48ms
step:587/1770 train_time:55096ms step_avg:95.49ms
step:588/1770 train_time:55193ms step_avg:95.49ms
step:589/1770 train_time:55291ms step_avg:95.49ms
step:590/1770 train_time:55389ms step_avg:95.50ms
step:591/1770 train_time:55487ms step_avg:95.50ms
step:592/1770 train_time:55585ms step_avg:95.51ms
step:593/1770 train_time:55682ms step_avg:95.51ms
step:594/1770 train_time:55778ms step_avg:95.51ms
step:595/1770 train_time:55876ms step_avg:95.51ms
step:596/1770 train_time:55973ms step_avg:95.52ms
step:597/1770 train_time:56071ms step_avg:95.52ms
step:598/1770 train_time:56168ms step_avg:95.52ms
step:599/1770 train_time:56266ms step_avg:95.53ms
step:600/1770 train_time:56364ms step_avg:95.53ms
step:601/1770 train_time:56461ms step_avg:95.53ms
step:602/1770 train_time:56558ms step_avg:95.54ms
step:603/1770 train_time:56655ms step_avg:95.54ms
step:604/1770 train_time:56753ms step_avg:95.54ms
step:605/1770 train_time:56851ms step_avg:95.55ms
step:606/1770 train_time:56948ms step_avg:95.55ms
step:607/1770 train_time:57046ms step_avg:95.55ms
step:608/1770 train_time:57143ms step_avg:95.56ms
step:609/1770 train_time:57241ms step_avg:95.56ms
step:610/1770 train_time:57338ms step_avg:95.56ms
step:611/1770 train_time:57435ms step_avg:95.57ms
step:612/1770 train_time:57532ms step_avg:95.57ms
step:613/1770 train_time:57629ms step_avg:95.57ms
step:614/1770 train_time:57731ms step_avg:95.58ms
step:615/1770 train_time:57825ms step_avg:95.58ms
step:616/1770 train_time:57922ms step_avg:95.58ms
step:617/1770 train_time:58019ms step_avg:95.58ms
step:618/1770 train_time:58116ms step_avg:95.59ms
step:619/1770 train_time:58213ms step_avg:95.59ms
step:620/1770 train_time:58311ms step_avg:95.59ms
step:621/1770 train_time:58409ms step_avg:95.60ms
step:622/1770 train_time:58507ms step_avg:95.60ms
step:623/1770 train_time:58605ms step_avg:95.60ms
step:624/1770 train_time:58702ms step_avg:95.61ms
step:625/1770 train_time:58799ms step_avg:95.61ms
step:625/1770 val_loss:3.6676 train_time:58895ms step_avg:95.76ms
step:626/1770 train_time:58916ms step_avg:95.64ms
step:627/1770 train_time:59003ms step_avg:95.63ms
step:628/1770 train_time:59103ms step_avg:95.64ms
step:629/1770 train_time:59202ms step_avg:95.64ms
step:630/1770 train_time:59299ms step_avg:95.64ms
step:631/1770 train_time:59396ms step_avg:95.65ms
step:632/1770 train_time:59493ms step_avg:95.65ms
step:633/1770 train_time:59590ms step_avg:95.65ms
step:634/1770 train_time:59687ms step_avg:95.65ms
step:635/1770 train_time:59784ms step_avg:95.65ms
step:636/1770 train_time:59881ms step_avg:95.66ms
step:637/1770 train_time:59978ms step_avg:95.66ms
step:638/1770 train_time:60076ms step_avg:95.66ms
step:639/1770 train_time:60173ms step_avg:95.66ms
step:640/1770 train_time:60270ms step_avg:95.67ms
step:641/1770 train_time:60368ms step_avg:95.67ms
step:642/1770 train_time:60465ms step_avg:95.67ms
step:643/1770 train_time:60563ms step_avg:95.68ms
step:644/1770 train_time:60660ms step_avg:95.68ms
step:645/1770 train_time:60757ms step_avg:95.68ms
step:646/1770 train_time:60853ms step_avg:95.68ms
step:647/1770 train_time:60951ms step_avg:95.68ms
step:648/1770 train_time:61047ms step_avg:95.69ms
step:649/1770 train_time:61145ms step_avg:95.69ms
step:650/1770 train_time:61243ms step_avg:95.69ms
step:651/1770 train_time:61340ms step_avg:95.69ms
step:652/1770 train_time:61438ms step_avg:95.70ms
step:653/1770 train_time:61535ms step_avg:95.70ms
step:654/1770 train_time:61632ms step_avg:95.70ms
step:655/1770 train_time:61729ms step_avg:95.70ms
step:656/1770 train_time:61826ms step_avg:95.71ms
step:657/1770 train_time:61923ms step_avg:95.71ms
step:658/1770 train_time:62022ms step_avg:95.71ms
step:659/1770 train_time:62122ms step_avg:95.72ms
step:660/1770 train_time:62221ms step_avg:95.72ms
step:661/1770 train_time:62320ms step_avg:95.73ms
step:662/1770 train_time:62419ms step_avg:95.73ms
step:663/1770 train_time:62518ms step_avg:95.74ms
step:664/1770 train_time:62617ms step_avg:95.74ms
step:665/1770 train_time:62715ms step_avg:95.75ms
step:666/1770 train_time:62815ms step_avg:95.75ms
step:667/1770 train_time:62913ms step_avg:95.76ms
step:668/1770 train_time:63012ms step_avg:95.76ms
step:669/1770 train_time:63112ms step_avg:95.77ms
step:670/1770 train_time:63211ms step_avg:95.77ms
step:671/1770 train_time:63311ms step_avg:95.78ms
step:672/1770 train_time:63410ms step_avg:95.79ms
step:673/1770 train_time:63508ms step_avg:95.79ms
step:674/1770 train_time:63607ms step_avg:95.79ms
step:675/1770 train_time:63705ms step_avg:95.80ms
step:676/1770 train_time:63805ms step_avg:95.80ms
step:677/1770 train_time:63904ms step_avg:95.81ms
step:678/1770 train_time:64004ms step_avg:95.81ms
step:679/1770 train_time:64103ms step_avg:95.82ms
step:680/1770 train_time:64203ms step_avg:95.83ms
step:681/1770 train_time:64302ms step_avg:95.83ms
step:682/1770 train_time:64402ms step_avg:95.84ms
step:683/1770 train_time:64501ms step_avg:95.84ms
step:684/1770 train_time:64600ms step_avg:95.85ms
step:685/1770 train_time:64699ms step_avg:95.85ms
step:686/1770 train_time:64798ms step_avg:95.85ms
step:687/1770 train_time:64897ms step_avg:95.86ms
step:688/1770 train_time:64997ms step_avg:95.87ms
step:689/1770 train_time:65096ms step_avg:95.87ms
step:690/1770 train_time:65196ms step_avg:95.88ms
step:691/1770 train_time:65294ms step_avg:95.88ms
step:692/1770 train_time:65393ms step_avg:95.88ms
step:693/1770 train_time:65493ms step_avg:95.89ms
step:694/1770 train_time:65592ms step_avg:95.89ms
step:695/1770 train_time:65690ms step_avg:95.90ms
step:696/1770 train_time:65789ms step_avg:95.90ms
step:697/1770 train_time:65887ms step_avg:95.91ms
step:698/1770 train_time:65986ms step_avg:95.91ms
step:699/1770 train_time:66085ms step_avg:95.91ms
step:700/1770 train_time:66184ms step_avg:95.92ms
step:701/1770 train_time:66284ms step_avg:95.93ms
step:702/1770 train_time:66384ms step_avg:95.93ms
step:703/1770 train_time:66483ms step_avg:95.93ms
step:704/1770 train_time:66582ms step_avg:95.94ms
step:705/1770 train_time:66681ms step_avg:95.94ms
step:706/1770 train_time:66780ms step_avg:95.95ms
step:707/1770 train_time:66879ms step_avg:95.95ms
step:708/1770 train_time:66978ms step_avg:95.96ms
step:709/1770 train_time:67077ms step_avg:95.96ms
step:710/1770 train_time:67176ms step_avg:95.97ms
step:711/1770 train_time:67276ms step_avg:95.97ms
step:712/1770 train_time:67375ms step_avg:95.98ms
step:713/1770 train_time:67474ms step_avg:95.98ms
step:714/1770 train_time:67574ms step_avg:95.99ms
step:715/1770 train_time:67673ms step_avg:95.99ms
step:716/1770 train_time:67772ms step_avg:95.99ms
step:717/1770 train_time:67871ms step_avg:96.00ms
step:718/1770 train_time:67970ms step_avg:96.00ms
step:719/1770 train_time:68069ms step_avg:96.01ms
step:720/1770 train_time:68168ms step_avg:96.01ms
step:721/1770 train_time:68266ms step_avg:96.01ms
step:722/1770 train_time:68365ms step_avg:96.02ms
step:723/1770 train_time:68465ms step_avg:96.02ms
step:724/1770 train_time:68566ms step_avg:96.03ms
step:725/1770 train_time:68664ms step_avg:96.03ms
step:726/1770 train_time:68764ms step_avg:96.04ms
step:727/1770 train_time:68863ms step_avg:96.04ms
step:728/1770 train_time:68962ms step_avg:96.05ms
step:729/1770 train_time:69062ms step_avg:96.05ms
step:730/1770 train_time:69161ms step_avg:96.06ms
step:731/1770 train_time:69261ms step_avg:96.06ms
step:732/1770 train_time:69360ms step_avg:96.07ms
step:733/1770 train_time:69458ms step_avg:96.07ms
step:734/1770 train_time:69558ms step_avg:96.07ms
step:735/1770 train_time:69656ms step_avg:96.08ms
step:736/1770 train_time:69755ms step_avg:96.08ms
step:737/1770 train_time:69854ms step_avg:96.09ms
step:738/1770 train_time:69954ms step_avg:96.09ms
step:739/1770 train_time:70053ms step_avg:96.09ms
step:740/1770 train_time:70152ms step_avg:96.10ms
step:741/1770 train_time:70250ms step_avg:96.10ms
step:742/1770 train_time:70352ms step_avg:96.11ms
step:743/1770 train_time:70448ms step_avg:96.11ms
step:744/1770 train_time:70547ms step_avg:96.11ms
step:745/1770 train_time:70646ms step_avg:96.12ms
step:746/1770 train_time:70746ms step_avg:96.12ms
step:747/1770 train_time:70845ms step_avg:96.13ms
step:748/1770 train_time:70945ms step_avg:96.13ms
step:749/1770 train_time:71045ms step_avg:96.14ms
step:750/1770 train_time:71144ms step_avg:96.14ms
step:750/1770 val_loss:3.6026 train_time:71242ms step_avg:96.27ms
step:751/1770 train_time:71263ms step_avg:96.17ms
step:752/1770 train_time:71352ms step_avg:96.16ms
step:753/1770 train_time:71453ms step_avg:96.17ms
step:754/1770 train_time:71552ms step_avg:96.17ms
step:755/1770 train_time:71651ms step_avg:96.18ms
step:756/1770 train_time:71750ms step_avg:96.18ms
step:757/1770 train_time:71848ms step_avg:96.18ms
step:758/1770 train_time:71947ms step_avg:96.19ms
step:759/1770 train_time:72046ms step_avg:96.19ms
step:760/1770 train_time:72144ms step_avg:96.19ms
step:761/1770 train_time:72244ms step_avg:96.20ms
step:762/1770 train_time:72346ms step_avg:96.21ms
step:763/1770 train_time:72447ms step_avg:96.21ms
step:764/1770 train_time:72547ms step_avg:96.22ms
step:765/1770 train_time:72647ms step_avg:96.22ms
step:766/1770 train_time:72746ms step_avg:96.23ms
step:767/1770 train_time:72846ms step_avg:96.23ms
step:768/1770 train_time:72945ms step_avg:96.23ms
step:769/1770 train_time:73044ms step_avg:96.24ms
step:770/1770 train_time:73142ms step_avg:96.24ms
step:771/1770 train_time:73242ms step_avg:96.24ms
step:772/1770 train_time:73342ms step_avg:96.25ms
step:773/1770 train_time:73442ms step_avg:96.25ms
step:774/1770 train_time:73541ms step_avg:96.26ms
step:775/1770 train_time:73641ms step_avg:96.26ms
step:776/1770 train_time:73740ms step_avg:96.27ms
step:777/1770 train_time:73840ms step_avg:96.27ms
step:778/1770 train_time:73939ms step_avg:96.27ms
step:779/1770 train_time:74038ms step_avg:96.28ms
step:780/1770 train_time:74136ms step_avg:96.28ms
step:781/1770 train_time:74235ms step_avg:96.28ms
step:782/1770 train_time:74333ms step_avg:96.29ms
step:783/1770 train_time:74432ms step_avg:96.29ms
step:784/1770 train_time:74531ms step_avg:96.29ms
step:785/1770 train_time:74631ms step_avg:96.30ms
step:786/1770 train_time:74731ms step_avg:96.30ms
step:787/1770 train_time:74830ms step_avg:96.31ms
step:788/1770 train_time:74929ms step_avg:96.31ms
step:789/1770 train_time:75028ms step_avg:96.31ms
step:790/1770 train_time:75127ms step_avg:96.32ms
step:791/1770 train_time:75226ms step_avg:96.32ms
step:792/1770 train_time:75326ms step_avg:96.32ms
step:793/1770 train_time:75426ms step_avg:96.33ms
step:794/1770 train_time:75526ms step_avg:96.33ms
step:795/1770 train_time:75625ms step_avg:96.34ms
step:796/1770 train_time:75725ms step_avg:96.34ms
step:797/1770 train_time:75825ms step_avg:96.35ms
step:798/1770 train_time:75924ms step_avg:96.35ms
step:799/1770 train_time:76023ms step_avg:96.35ms
step:800/1770 train_time:76123ms step_avg:96.36ms
step:801/1770 train_time:76222ms step_avg:96.36ms
step:802/1770 train_time:76322ms step_avg:96.37ms
step:803/1770 train_time:76421ms step_avg:96.37ms
step:804/1770 train_time:76521ms step_avg:96.37ms
step:805/1770 train_time:76620ms step_avg:96.38ms
step:806/1770 train_time:76719ms step_avg:96.38ms
step:807/1770 train_time:76818ms step_avg:96.38ms
step:808/1770 train_time:76917ms step_avg:96.39ms
step:809/1770 train_time:77016ms step_avg:96.39ms
step:810/1770 train_time:77117ms step_avg:96.40ms
step:811/1770 train_time:77217ms step_avg:96.40ms
step:812/1770 train_time:77316ms step_avg:96.40ms
step:813/1770 train_time:77415ms step_avg:96.41ms
step:814/1770 train_time:77514ms step_avg:96.41ms
step:815/1770 train_time:77613ms step_avg:96.41ms
step:816/1770 train_time:77713ms step_avg:96.42ms
step:817/1770 train_time:77813ms step_avg:96.42ms
step:818/1770 train_time:77913ms step_avg:96.43ms
step:819/1770 train_time:78012ms step_avg:96.43ms
step:820/1770 train_time:78111ms step_avg:96.43ms
step:821/1770 train_time:78210ms step_avg:96.44ms
step:822/1770 train_time:78309ms step_avg:96.44ms
step:823/1770 train_time:78409ms step_avg:96.44ms
step:824/1770 train_time:78508ms step_avg:96.45ms
step:825/1770 train_time:78607ms step_avg:96.45ms
step:826/1770 train_time:78707ms step_avg:96.45ms
step:827/1770 train_time:78807ms step_avg:96.46ms
step:828/1770 train_time:78907ms step_avg:96.46ms
step:829/1770 train_time:79007ms step_avg:96.47ms
step:830/1770 train_time:79106ms step_avg:96.47ms
step:831/1770 train_time:79206ms step_avg:96.48ms
step:832/1770 train_time:79306ms step_avg:96.48ms
step:833/1770 train_time:79405ms step_avg:96.48ms
step:834/1770 train_time:79505ms step_avg:96.49ms
step:835/1770 train_time:79605ms step_avg:96.49ms
step:836/1770 train_time:79704ms step_avg:96.49ms
step:837/1770 train_time:79804ms step_avg:96.50ms
step:838/1770 train_time:79904ms step_avg:96.50ms
step:839/1770 train_time:80003ms step_avg:96.51ms
step:840/1770 train_time:80103ms step_avg:96.51ms
step:841/1770 train_time:80203ms step_avg:96.51ms
step:842/1770 train_time:80303ms step_avg:96.52ms
step:843/1770 train_time:80402ms step_avg:96.52ms
step:844/1770 train_time:80501ms step_avg:96.52ms
step:845/1770 train_time:80601ms step_avg:96.53ms
step:846/1770 train_time:80701ms step_avg:96.53ms
step:847/1770 train_time:80800ms step_avg:96.54ms
step:848/1770 train_time:80899ms step_avg:96.54ms
step:849/1770 train_time:80999ms step_avg:96.54ms
step:850/1770 train_time:81098ms step_avg:96.55ms
step:851/1770 train_time:81198ms step_avg:96.55ms
step:852/1770 train_time:81298ms step_avg:96.55ms
step:853/1770 train_time:81398ms step_avg:96.56ms
step:854/1770 train_time:81497ms step_avg:96.56ms
step:855/1770 train_time:81596ms step_avg:96.56ms
step:856/1770 train_time:81695ms step_avg:96.57ms
step:857/1770 train_time:81795ms step_avg:96.57ms
step:858/1770 train_time:81894ms step_avg:96.57ms
step:859/1770 train_time:81994ms step_avg:96.58ms
step:860/1770 train_time:82093ms step_avg:96.58ms
step:861/1770 train_time:82193ms step_avg:96.58ms
step:862/1770 train_time:82292ms step_avg:96.59ms
step:863/1770 train_time:82392ms step_avg:96.59ms
step:864/1770 train_time:82491ms step_avg:96.59ms
step:865/1770 train_time:82590ms step_avg:96.60ms
step:866/1770 train_time:82690ms step_avg:96.60ms
step:867/1770 train_time:82789ms step_avg:96.60ms
step:868/1770 train_time:82888ms step_avg:96.61ms
step:869/1770 train_time:82987ms step_avg:96.61ms
step:870/1770 train_time:83086ms step_avg:96.61ms
step:871/1770 train_time:83187ms step_avg:96.62ms
step:872/1770 train_time:83286ms step_avg:96.62ms
step:873/1770 train_time:83386ms step_avg:96.62ms
step:874/1770 train_time:83486ms step_avg:96.63ms
step:875/1770 train_time:83586ms step_avg:96.63ms
step:875/1770 val_loss:3.5537 train_time:83684ms step_avg:96.74ms
step:876/1770 train_time:83707ms step_avg:96.66ms
step:877/1770 train_time:83794ms step_avg:96.65ms
step:878/1770 train_time:83894ms step_avg:96.65ms
step:879/1770 train_time:83993ms step_avg:96.65ms
step:880/1770 train_time:84092ms step_avg:96.66ms
step:881/1770 train_time:84191ms step_avg:96.66ms
step:882/1770 train_time:84289ms step_avg:96.66ms
step:883/1770 train_time:84388ms step_avg:96.66ms
step:884/1770 train_time:84487ms step_avg:96.67ms
step:885/1770 train_time:84585ms step_avg:96.67ms
step:886/1770 train_time:84684ms step_avg:96.67ms
step:887/1770 train_time:84785ms step_avg:96.68ms
step:888/1770 train_time:84886ms step_avg:96.68ms
step:889/1770 train_time:84986ms step_avg:96.68ms
step:890/1770 train_time:85085ms step_avg:96.69ms
step:891/1770 train_time:85185ms step_avg:96.69ms
step:892/1770 train_time:85285ms step_avg:96.70ms
step:893/1770 train_time:85385ms step_avg:96.70ms
step:894/1770 train_time:85484ms step_avg:96.70ms
step:895/1770 train_time:85583ms step_avg:96.70ms
step:896/1770 train_time:85682ms step_avg:96.71ms
step:897/1770 train_time:85782ms step_avg:96.71ms
step:898/1770 train_time:85882ms step_avg:96.71ms
step:899/1770 train_time:85982ms step_avg:96.72ms
step:900/1770 train_time:86082ms step_avg:96.72ms
step:901/1770 train_time:86182ms step_avg:96.72ms
step:902/1770 train_time:86282ms step_avg:96.73ms
step:903/1770 train_time:86382ms step_avg:96.73ms
step:904/1770 train_time:86481ms step_avg:96.74ms
step:905/1770 train_time:86580ms step_avg:96.74ms
step:906/1770 train_time:86679ms step_avg:96.74ms
step:907/1770 train_time:86778ms step_avg:96.74ms
step:908/1770 train_time:86877ms step_avg:96.74ms
step:909/1770 train_time:86976ms step_avg:96.75ms
step:910/1770 train_time:87075ms step_avg:96.75ms
step:911/1770 train_time:87174ms step_avg:96.75ms
step:912/1770 train_time:87273ms step_avg:96.76ms
step:913/1770 train_time:87372ms step_avg:96.76ms
step:914/1770 train_time:87472ms step_avg:96.76ms
step:915/1770 train_time:87572ms step_avg:96.77ms
step:916/1770 train_time:87672ms step_avg:96.77ms
step:917/1770 train_time:87772ms step_avg:96.77ms
step:918/1770 train_time:87872ms step_avg:96.78ms
step:919/1770 train_time:87972ms step_avg:96.78ms
step:920/1770 train_time:88073ms step_avg:96.78ms
step:921/1770 train_time:88174ms step_avg:96.79ms
step:922/1770 train_time:88275ms step_avg:96.79ms
step:923/1770 train_time:88375ms step_avg:96.80ms
step:924/1770 train_time:88476ms step_avg:96.80ms
step:925/1770 train_time:88577ms step_avg:96.81ms
step:926/1770 train_time:88678ms step_avg:96.81ms
step:927/1770 train_time:88779ms step_avg:96.81ms
step:928/1770 train_time:88879ms step_avg:96.82ms
step:929/1770 train_time:88980ms step_avg:96.82ms
step:930/1770 train_time:89081ms step_avg:96.83ms
step:931/1770 train_time:89182ms step_avg:96.83ms
step:932/1770 train_time:89283ms step_avg:96.84ms
step:933/1770 train_time:89384ms step_avg:96.84ms
step:934/1770 train_time:89485ms step_avg:96.85ms
step:935/1770 train_time:89586ms step_avg:96.85ms
step:936/1770 train_time:89687ms step_avg:96.85ms
step:937/1770 train_time:89788ms step_avg:96.86ms
step:938/1770 train_time:89889ms step_avg:96.86ms
step:939/1770 train_time:89990ms step_avg:96.87ms
step:940/1770 train_time:90090ms step_avg:96.87ms
step:941/1770 train_time:90191ms step_avg:96.88ms
step:942/1770 train_time:90291ms step_avg:96.88ms
step:943/1770 train_time:90392ms step_avg:96.88ms
step:944/1770 train_time:90493ms step_avg:96.89ms
step:945/1770 train_time:90593ms step_avg:96.89ms
step:946/1770 train_time:90694ms step_avg:96.90ms
step:947/1770 train_time:90795ms step_avg:96.90ms
step:948/1770 train_time:90895ms step_avg:96.90ms
step:949/1770 train_time:90996ms step_avg:96.91ms
step:950/1770 train_time:91099ms step_avg:96.91ms
step:951/1770 train_time:91199ms step_avg:96.92ms
step:952/1770 train_time:91300ms step_avg:96.92ms
step:953/1770 train_time:91400ms step_avg:96.93ms
step:954/1770 train_time:91501ms step_avg:96.93ms
step:955/1770 train_time:91602ms step_avg:96.93ms
step:956/1770 train_time:91703ms step_avg:96.94ms
step:957/1770 train_time:91804ms step_avg:96.94ms
step:958/1770 train_time:91905ms step_avg:96.95ms
step:959/1770 train_time:92007ms step_avg:96.95ms
step:960/1770 train_time:92107ms step_avg:96.96ms
step:961/1770 train_time:92208ms step_avg:96.96ms
step:962/1770 train_time:92309ms step_avg:96.96ms
step:963/1770 train_time:92409ms step_avg:96.97ms
step:964/1770 train_time:92510ms step_avg:96.97ms
step:965/1770 train_time:92611ms step_avg:96.97ms
step:966/1770 train_time:92711ms step_avg:96.98ms
step:967/1770 train_time:92812ms step_avg:96.98ms
step:968/1770 train_time:92913ms step_avg:96.99ms
step:969/1770 train_time:93014ms step_avg:96.99ms
step:970/1770 train_time:93115ms step_avg:96.99ms
step:971/1770 train_time:93216ms step_avg:97.00ms
step:972/1770 train_time:93317ms step_avg:97.00ms
step:973/1770 train_time:93418ms step_avg:97.01ms
step:974/1770 train_time:93519ms step_avg:97.01ms
step:975/1770 train_time:93620ms step_avg:97.02ms
step:976/1770 train_time:93721ms step_avg:97.02ms
step:977/1770 train_time:93821ms step_avg:97.02ms
step:978/1770 train_time:93921ms step_avg:97.03ms
step:979/1770 train_time:94023ms step_avg:97.03ms
step:980/1770 train_time:94123ms step_avg:97.03ms
step:981/1770 train_time:94224ms step_avg:97.04ms
step:982/1770 train_time:94326ms step_avg:97.04ms
step:983/1770 train_time:94426ms step_avg:97.05ms
step:984/1770 train_time:94528ms step_avg:97.05ms
step:985/1770 train_time:94630ms step_avg:97.06ms
step:986/1770 train_time:94730ms step_avg:97.06ms
step:987/1770 train_time:94831ms step_avg:97.06ms
step:988/1770 train_time:94931ms step_avg:97.07ms
step:989/1770 train_time:95033ms step_avg:97.07ms
step:990/1770 train_time:95133ms step_avg:97.07ms
step:991/1770 train_time:95234ms step_avg:97.08ms
step:992/1770 train_time:95334ms step_avg:97.08ms
step:993/1770 train_time:95436ms step_avg:97.09ms
step:994/1770 train_time:95538ms step_avg:97.09ms
step:995/1770 train_time:95638ms step_avg:97.09ms
step:996/1770 train_time:95739ms step_avg:97.10ms
step:997/1770 train_time:95840ms step_avg:97.10ms
step:998/1770 train_time:95940ms step_avg:97.11ms
step:999/1770 train_time:96041ms step_avg:97.11ms
step:1000/1770 train_time:96142ms step_avg:97.11ms
step:1000/1770 val_loss:3.5147 train_time:96241ms step_avg:97.21ms
step:1001/1770 train_time:96262ms step_avg:97.14ms
step:1002/1770 train_time:96356ms step_avg:97.13ms
step:1003/1770 train_time:96457ms step_avg:97.14ms
step:1004/1770 train_time:96558ms step_avg:97.14ms
step:1005/1770 train_time:96657ms step_avg:97.14ms
step:1006/1770 train_time:96757ms step_avg:97.15ms
step:1007/1770 train_time:96857ms step_avg:97.15ms
step:1008/1770 train_time:96957ms step_avg:97.15ms
step:1009/1770 train_time:97057ms step_avg:97.15ms
step:1010/1770 train_time:97157ms step_avg:97.16ms
step:1011/1770 train_time:97259ms step_avg:97.16ms
step:1012/1770 train_time:97362ms step_avg:97.17ms
step:1013/1770 train_time:97463ms step_avg:97.17ms
step:1014/1770 train_time:97566ms step_avg:97.18ms
step:1015/1770 train_time:97666ms step_avg:97.18ms
step:1016/1770 train_time:97766ms step_avg:97.18ms
step:1017/1770 train_time:97867ms step_avg:97.19ms
step:1018/1770 train_time:97968ms step_avg:97.19ms
step:1019/1770 train_time:98068ms step_avg:97.19ms
step:1020/1770 train_time:98169ms step_avg:97.20ms
step:1021/1770 train_time:98270ms step_avg:97.20ms
step:1022/1770 train_time:98371ms step_avg:97.20ms
step:1023/1770 train_time:98472ms step_avg:97.21ms
step:1024/1770 train_time:98573ms step_avg:97.21ms
step:1025/1770 train_time:98675ms step_avg:97.22ms
step:1026/1770 train_time:98777ms step_avg:97.22ms
step:1027/1770 train_time:98879ms step_avg:97.23ms
step:1028/1770 train_time:98979ms step_avg:97.23ms
step:1029/1770 train_time:99079ms step_avg:97.23ms
step:1030/1770 train_time:99180ms step_avg:97.23ms
step:1031/1770 train_time:99281ms step_avg:97.24ms
step:1032/1770 train_time:99382ms step_avg:97.24ms
step:1033/1770 train_time:99483ms step_avg:97.25ms
step:1034/1770 train_time:99583ms step_avg:97.25ms
step:1035/1770 train_time:99684ms step_avg:97.25ms
step:1036/1770 train_time:99784ms step_avg:97.26ms
step:1037/1770 train_time:99885ms step_avg:97.26ms
step:1038/1770 train_time:99985ms step_avg:97.26ms
step:1039/1770 train_time:100086ms step_avg:97.27ms
step:1040/1770 train_time:100187ms step_avg:97.27ms
step:1041/1770 train_time:100287ms step_avg:97.27ms
step:1042/1770 train_time:100389ms step_avg:97.28ms
step:1043/1770 train_time:100490ms step_avg:97.28ms
step:1044/1770 train_time:100590ms step_avg:97.28ms
step:1045/1770 train_time:100691ms step_avg:97.29ms
step:1046/1770 train_time:100791ms step_avg:97.29ms
step:1047/1770 train_time:100892ms step_avg:97.29ms
step:1048/1770 train_time:100993ms step_avg:97.30ms
step:1049/1770 train_time:101094ms step_avg:97.30ms
step:1050/1770 train_time:101195ms step_avg:97.30ms
step:1051/1770 train_time:101296ms step_avg:97.31ms
step:1052/1770 train_time:101397ms step_avg:97.31ms
step:1053/1770 train_time:101498ms step_avg:97.31ms
step:1054/1770 train_time:101598ms step_avg:97.32ms
step:1055/1770 train_time:101699ms step_avg:97.32ms
step:1056/1770 train_time:101800ms step_avg:97.32ms
step:1057/1770 train_time:101900ms step_avg:97.33ms
step:1058/1770 train_time:102001ms step_avg:97.33ms
step:1059/1770 train_time:102101ms step_avg:97.33ms
step:1060/1770 train_time:102202ms step_avg:97.34ms
step:1061/1770 train_time:102303ms step_avg:97.34ms
step:1062/1770 train_time:102405ms step_avg:97.34ms
step:1063/1770 train_time:102507ms step_avg:97.35ms
step:1064/1770 train_time:102609ms step_avg:97.35ms
step:1065/1770 train_time:102710ms step_avg:97.36ms
step:1066/1770 train_time:102811ms step_avg:97.36ms
step:1067/1770 train_time:102912ms step_avg:97.36ms
step:1068/1770 train_time:103013ms step_avg:97.37ms
step:1069/1770 train_time:103114ms step_avg:97.37ms
step:1070/1770 train_time:103216ms step_avg:97.37ms
step:1071/1770 train_time:103317ms step_avg:97.38ms
step:1072/1770 train_time:103419ms step_avg:97.38ms
step:1073/1770 train_time:103520ms step_avg:97.38ms
step:1074/1770 train_time:103620ms step_avg:97.39ms
step:1075/1770 train_time:103721ms step_avg:97.39ms
step:1076/1770 train_time:103822ms step_avg:97.39ms
step:1077/1770 train_time:103924ms step_avg:97.40ms
step:1078/1770 train_time:104026ms step_avg:97.40ms
step:1079/1770 train_time:104126ms step_avg:97.41ms
step:1080/1770 train_time:104227ms step_avg:97.41ms
step:1081/1770 train_time:104328ms step_avg:97.41ms
step:1082/1770 train_time:104429ms step_avg:97.42ms
step:1083/1770 train_time:104530ms step_avg:97.42ms
step:1084/1770 train_time:104631ms step_avg:97.42ms
step:1085/1770 train_time:104733ms step_avg:97.43ms
step:1086/1770 train_time:104835ms step_avg:97.43ms
step:1087/1770 train_time:104936ms step_avg:97.43ms
step:1088/1770 train_time:105038ms step_avg:97.44ms
step:1089/1770 train_time:105139ms step_avg:97.44ms
step:1090/1770 train_time:105240ms step_avg:97.44ms
step:1091/1770 train_time:105340ms step_avg:97.45ms
step:1092/1770 train_time:105441ms step_avg:97.45ms
step:1093/1770 train_time:105542ms step_avg:97.45ms
step:1094/1770 train_time:105644ms step_avg:97.46ms
step:1095/1770 train_time:105749ms step_avg:97.46ms
step:1096/1770 train_time:105846ms step_avg:97.46ms
step:1097/1770 train_time:105947ms step_avg:97.47ms
step:1098/1770 train_time:106049ms step_avg:97.47ms
step:1099/1770 train_time:106150ms step_avg:97.47ms
step:1100/1770 train_time:106252ms step_avg:97.48ms
step:1101/1770 train_time:106353ms step_avg:97.48ms
step:1102/1770 train_time:106455ms step_avg:97.49ms
step:1103/1770 train_time:106556ms step_avg:97.49ms
step:1104/1770 train_time:106657ms step_avg:97.49ms
step:1105/1770 train_time:106758ms step_avg:97.50ms
step:1106/1770 train_time:106860ms step_avg:97.50ms
step:1107/1770 train_time:106961ms step_avg:97.50ms
step:1108/1770 train_time:107063ms step_avg:97.51ms
step:1109/1770 train_time:107164ms step_avg:97.51ms
step:1110/1770 train_time:107265ms step_avg:97.51ms
step:1111/1770 train_time:107366ms step_avg:97.52ms
step:1112/1770 train_time:107467ms step_avg:97.52ms
step:1113/1770 train_time:107568ms step_avg:97.52ms
step:1114/1770 train_time:107670ms step_avg:97.53ms
step:1115/1770 train_time:107770ms step_avg:97.53ms
step:1116/1770 train_time:107871ms step_avg:97.53ms
step:1117/1770 train_time:107973ms step_avg:97.54ms
step:1118/1770 train_time:108075ms step_avg:97.54ms
step:1119/1770 train_time:108177ms step_avg:97.54ms
step:1120/1770 train_time:108279ms step_avg:97.55ms
step:1121/1770 train_time:108379ms step_avg:97.55ms
step:1122/1770 train_time:108480ms step_avg:97.55ms
step:1123/1770 train_time:108580ms step_avg:97.56ms
step:1124/1770 train_time:108681ms step_avg:97.56ms
step:1125/1770 train_time:108783ms step_avg:97.56ms
step:1125/1770 val_loss:3.4743 train_time:108882ms step_avg:97.65ms
step:1126/1770 train_time:108906ms step_avg:97.59ms
step:1127/1770 train_time:108997ms step_avg:97.58ms
step:1128/1770 train_time:109100ms step_avg:97.58ms
step:1129/1770 train_time:109200ms step_avg:97.59ms
step:1130/1770 train_time:109300ms step_avg:97.59ms
step:1131/1770 train_time:109401ms step_avg:97.59ms
step:1132/1770 train_time:109501ms step_avg:97.59ms
step:1133/1770 train_time:109601ms step_avg:97.60ms
step:1134/1770 train_time:109702ms step_avg:97.60ms
step:1135/1770 train_time:109802ms step_avg:97.60ms
step:1136/1770 train_time:109903ms step_avg:97.60ms
step:1137/1770 train_time:110007ms step_avg:97.61ms
step:1138/1770 train_time:110107ms step_avg:97.61ms
step:1139/1770 train_time:110208ms step_avg:97.62ms
step:1140/1770 train_time:110309ms step_avg:97.62ms
step:1141/1770 train_time:110409ms step_avg:97.62ms
step:1142/1770 train_time:110510ms step_avg:97.62ms
step:1143/1770 train_time:110610ms step_avg:97.63ms
step:1144/1770 train_time:110712ms step_avg:97.63ms
step:1145/1770 train_time:110812ms step_avg:97.63ms
step:1146/1770 train_time:110914ms step_avg:97.64ms
step:1147/1770 train_time:111015ms step_avg:97.64ms
step:1148/1770 train_time:111117ms step_avg:97.64ms
step:1149/1770 train_time:111218ms step_avg:97.65ms
step:1150/1770 train_time:111318ms step_avg:97.65ms
step:1151/1770 train_time:111420ms step_avg:97.65ms
step:1152/1770 train_time:111521ms step_avg:97.65ms
step:1153/1770 train_time:111622ms step_avg:97.66ms
step:1154/1770 train_time:111723ms step_avg:97.66ms
step:1155/1770 train_time:111825ms step_avg:97.66ms
step:1156/1770 train_time:111925ms step_avg:97.67ms
step:1157/1770 train_time:112027ms step_avg:97.67ms
step:1158/1770 train_time:112128ms step_avg:97.67ms
step:1159/1770 train_time:112229ms step_avg:97.68ms
step:1160/1770 train_time:112330ms step_avg:97.68ms
step:1161/1770 train_time:112431ms step_avg:97.68ms
step:1162/1770 train_time:112533ms step_avg:97.68ms
step:1163/1770 train_time:112638ms step_avg:97.69ms
step:1164/1770 train_time:112735ms step_avg:97.69ms
step:1165/1770 train_time:112836ms step_avg:97.69ms
step:1166/1770 train_time:112938ms step_avg:97.70ms
step:1167/1770 train_time:113040ms step_avg:97.70ms
step:1168/1770 train_time:113141ms step_avg:97.70ms
step:1169/1770 train_time:113243ms step_avg:97.71ms
step:1170/1770 train_time:113343ms step_avg:97.71ms
step:1171/1770 train_time:113444ms step_avg:97.71ms
step:1172/1770 train_time:113545ms step_avg:97.71ms
step:1173/1770 train_time:113645ms step_avg:97.72ms
step:1174/1770 train_time:113747ms step_avg:97.72ms
step:1175/1770 train_time:113848ms step_avg:97.72ms
step:1176/1770 train_time:113949ms step_avg:97.73ms
step:1177/1770 train_time:114051ms step_avg:97.73ms
step:1178/1770 train_time:114152ms step_avg:97.73ms
step:1179/1770 train_time:114253ms step_avg:97.74ms
step:1180/1770 train_time:114353ms step_avg:97.74ms
step:1181/1770 train_time:114455ms step_avg:97.74ms
step:1182/1770 train_time:114557ms step_avg:97.74ms
step:1183/1770 train_time:114659ms step_avg:97.75ms
step:1184/1770 train_time:114763ms step_avg:97.75ms
step:1185/1770 train_time:114864ms step_avg:97.76ms
step:1186/1770 train_time:114966ms step_avg:97.76ms
step:1187/1770 train_time:115070ms step_avg:97.77ms
step:1188/1770 train_time:115171ms step_avg:97.77ms
step:1189/1770 train_time:115274ms step_avg:97.77ms
step:1190/1770 train_time:115375ms step_avg:97.78ms
step:1191/1770 train_time:115477ms step_avg:97.78ms
step:1192/1770 train_time:115579ms step_avg:97.78ms
step:1193/1770 train_time:115682ms step_avg:97.79ms
step:1194/1770 train_time:115784ms step_avg:97.79ms
step:1195/1770 train_time:115886ms step_avg:97.79ms
step:1196/1770 train_time:115988ms step_avg:97.80ms
step:1197/1770 train_time:116090ms step_avg:97.80ms
step:1198/1770 train_time:116193ms step_avg:97.81ms
step:1199/1770 train_time:116295ms step_avg:97.81ms
step:1200/1770 train_time:116398ms step_avg:97.81ms
step:1201/1770 train_time:116501ms step_avg:97.82ms
step:1202/1770 train_time:116603ms step_avg:97.82ms
step:1203/1770 train_time:116705ms step_avg:97.82ms
step:1204/1770 train_time:116807ms step_avg:97.83ms
step:1205/1770 train_time:116908ms step_avg:97.83ms
step:1206/1770 train_time:117010ms step_avg:97.83ms
step:1207/1770 train_time:117113ms step_avg:97.84ms
step:1208/1770 train_time:117216ms step_avg:97.84ms
step:1209/1770 train_time:117318ms step_avg:97.85ms
step:1210/1770 train_time:117419ms step_avg:97.85ms
step:1211/1770 train_time:117521ms step_avg:97.85ms
step:1212/1770 train_time:117625ms step_avg:97.86ms
step:1213/1770 train_time:117726ms step_avg:97.86ms
step:1214/1770 train_time:117827ms step_avg:97.86ms
step:1215/1770 train_time:117930ms step_avg:97.87ms
step:1216/1770 train_time:118034ms step_avg:97.87ms
step:1217/1770 train_time:118137ms step_avg:97.88ms
step:1218/1770 train_time:118238ms step_avg:97.88ms
step:1219/1770 train_time:118339ms step_avg:97.88ms
step:1220/1770 train_time:118442ms step_avg:97.89ms
step:1221/1770 train_time:118544ms step_avg:97.89ms
step:1222/1770 train_time:118647ms step_avg:97.89ms
step:1223/1770 train_time:118749ms step_avg:97.90ms
step:1224/1770 train_time:118852ms step_avg:97.90ms
step:1225/1770 train_time:118954ms step_avg:97.90ms
step:1226/1770 train_time:119056ms step_avg:97.91ms
step:1227/1770 train_time:119160ms step_avg:97.91ms
step:1228/1770 train_time:119263ms step_avg:97.92ms
step:1229/1770 train_time:119364ms step_avg:97.92ms
step:1230/1770 train_time:119466ms step_avg:97.92ms
step:1231/1770 train_time:119569ms step_avg:97.93ms
step:1232/1770 train_time:119671ms step_avg:97.93ms
step:1233/1770 train_time:119773ms step_avg:97.93ms
step:1234/1770 train_time:119875ms step_avg:97.94ms
step:1235/1770 train_time:119977ms step_avg:97.94ms
step:1236/1770 train_time:120080ms step_avg:97.94ms
step:1237/1770 train_time:120181ms step_avg:97.95ms
step:1238/1770 train_time:120283ms step_avg:97.95ms
step:1239/1770 train_time:120385ms step_avg:97.95ms
step:1240/1770 train_time:120487ms step_avg:97.96ms
step:1241/1770 train_time:120589ms step_avg:97.96ms
step:1242/1770 train_time:120692ms step_avg:97.96ms
step:1243/1770 train_time:120794ms step_avg:97.97ms
step:1244/1770 train_time:120895ms step_avg:97.97ms
step:1245/1770 train_time:120997ms step_avg:97.97ms
step:1246/1770 train_time:121100ms step_avg:97.98ms
step:1247/1770 train_time:121202ms step_avg:97.98ms
step:1248/1770 train_time:121304ms step_avg:97.98ms
step:1249/1770 train_time:121406ms step_avg:97.99ms
step:1250/1770 train_time:121508ms step_avg:97.99ms
step:1250/1770 val_loss:3.4269 train_time:121610ms step_avg:98.07ms
step:1251/1770 train_time:121630ms step_avg:98.01ms
step:1252/1770 train_time:121723ms step_avg:98.01ms
step:1253/1770 train_time:121827ms step_avg:98.01ms
step:1254/1770 train_time:121929ms step_avg:98.01ms
step:1255/1770 train_time:122034ms step_avg:98.02ms
step:1256/1770 train_time:122135ms step_avg:98.02ms
step:1257/1770 train_time:122236ms step_avg:98.02ms
step:1258/1770 train_time:122338ms step_avg:98.03ms
step:1259/1770 train_time:122440ms step_avg:98.03ms
step:1260/1770 train_time:122541ms step_avg:98.03ms
step:1261/1770 train_time:122644ms step_avg:98.04ms
step:1262/1770 train_time:122747ms step_avg:98.04ms
step:1263/1770 train_time:122849ms step_avg:98.04ms
step:1264/1770 train_time:122952ms step_avg:98.05ms
step:1265/1770 train_time:123054ms step_avg:98.05ms
step:1266/1770 train_time:123156ms step_avg:98.05ms
step:1267/1770 train_time:123258ms step_avg:98.06ms
step:1268/1770 train_time:123361ms step_avg:98.06ms
step:1269/1770 train_time:123462ms step_avg:98.06ms
step:1270/1770 train_time:123564ms step_avg:98.07ms
step:1271/1770 train_time:123667ms step_avg:98.07ms
step:1272/1770 train_time:123768ms step_avg:98.07ms
step:1273/1770 train_time:123872ms step_avg:98.08ms
step:1274/1770 train_time:123975ms step_avg:98.08ms
step:1275/1770 train_time:124076ms step_avg:98.08ms
step:1276/1770 train_time:124178ms step_avg:98.09ms
step:1277/1770 train_time:124280ms step_avg:98.09ms
step:1278/1770 train_time:124382ms step_avg:98.09ms
step:1279/1770 train_time:124484ms step_avg:98.10ms
step:1280/1770 train_time:124588ms step_avg:98.10ms
step:1281/1770 train_time:124689ms step_avg:98.10ms
step:1282/1770 train_time:124792ms step_avg:98.11ms
step:1283/1770 train_time:124895ms step_avg:98.11ms
step:1284/1770 train_time:124997ms step_avg:98.11ms
step:1285/1770 train_time:125099ms step_avg:98.12ms
step:1286/1770 train_time:125202ms step_avg:98.12ms
step:1287/1770 train_time:125306ms step_avg:98.13ms
step:1288/1770 train_time:125409ms step_avg:98.13ms
step:1289/1770 train_time:125511ms step_avg:98.13ms
step:1290/1770 train_time:125613ms step_avg:98.13ms
step:1291/1770 train_time:125717ms step_avg:98.14ms
step:1292/1770 train_time:125817ms step_avg:98.14ms
step:1293/1770 train_time:125920ms step_avg:98.15ms
step:1294/1770 train_time:126022ms step_avg:98.15ms
step:1295/1770 train_time:126125ms step_avg:98.15ms
step:1296/1770 train_time:126227ms step_avg:98.15ms
step:1297/1770 train_time:126328ms step_avg:98.16ms
step:1298/1770 train_time:126430ms step_avg:98.16ms
step:1299/1770 train_time:126532ms step_avg:98.16ms
step:1300/1770 train_time:126634ms step_avg:98.17ms
step:1301/1770 train_time:126736ms step_avg:98.17ms
step:1302/1770 train_time:126838ms step_avg:98.17ms
step:1303/1770 train_time:126940ms step_avg:98.17ms
step:1304/1770 train_time:127042ms step_avg:98.18ms
step:1305/1770 train_time:127144ms step_avg:98.18ms
step:1306/1770 train_time:127246ms step_avg:98.18ms
step:1307/1770 train_time:127350ms step_avg:98.19ms
step:1308/1770 train_time:127451ms step_avg:98.19ms
step:1309/1770 train_time:127553ms step_avg:98.19ms
step:1310/1770 train_time:127654ms step_avg:98.20ms
step:1311/1770 train_time:127756ms step_avg:98.20ms
step:1312/1770 train_time:127858ms step_avg:98.20ms
step:1313/1770 train_time:127959ms step_avg:98.20ms
step:1314/1770 train_time:128061ms step_avg:98.21ms
step:1315/1770 train_time:128163ms step_avg:98.21ms
step:1316/1770 train_time:128266ms step_avg:98.21ms
step:1317/1770 train_time:128368ms step_avg:98.22ms
step:1318/1770 train_time:128473ms step_avg:98.22ms
step:1319/1770 train_time:128576ms step_avg:98.22ms
step:1320/1770 train_time:128678ms step_avg:98.23ms
step:1321/1770 train_time:128779ms step_avg:98.23ms
step:1322/1770 train_time:128881ms step_avg:98.23ms
step:1323/1770 train_time:128984ms step_avg:98.24ms
step:1324/1770 train_time:129087ms step_avg:98.24ms
step:1325/1770 train_time:129191ms step_avg:98.24ms
step:1326/1770 train_time:129292ms step_avg:98.25ms
step:1327/1770 train_time:129397ms step_avg:98.25ms
step:1328/1770 train_time:129498ms step_avg:98.25ms
step:1329/1770 train_time:129600ms step_avg:98.26ms
step:1330/1770 train_time:129701ms step_avg:98.26ms
step:1331/1770 train_time:129803ms step_avg:98.26ms
step:1332/1770 train_time:129905ms step_avg:98.26ms
step:1333/1770 train_time:130006ms step_avg:98.27ms
step:1334/1770 train_time:130108ms step_avg:98.27ms
step:1335/1770 train_time:130211ms step_avg:98.27ms
step:1336/1770 train_time:130313ms step_avg:98.28ms
step:1337/1770 train_time:130416ms step_avg:98.28ms
step:1338/1770 train_time:130518ms step_avg:98.28ms
step:1339/1770 train_time:130621ms step_avg:98.29ms
step:1340/1770 train_time:130725ms step_avg:98.29ms
step:1341/1770 train_time:130826ms step_avg:98.29ms
step:1342/1770 train_time:130929ms step_avg:98.29ms
step:1343/1770 train_time:131031ms step_avg:98.30ms
step:1344/1770 train_time:131134ms step_avg:98.30ms
step:1345/1770 train_time:131236ms step_avg:98.30ms
step:1346/1770 train_time:131338ms step_avg:98.31ms
step:1347/1770 train_time:131441ms step_avg:98.31ms
step:1348/1770 train_time:131545ms step_avg:98.31ms
step:1349/1770 train_time:131648ms step_avg:98.32ms
step:1350/1770 train_time:131749ms step_avg:98.32ms
step:1351/1770 train_time:131851ms step_avg:98.32ms
step:1352/1770 train_time:131953ms step_avg:98.33ms
step:1353/1770 train_time:132056ms step_avg:98.33ms
step:1354/1770 train_time:132158ms step_avg:98.33ms
step:1355/1770 train_time:132260ms step_avg:98.33ms
step:1356/1770 train_time:132363ms step_avg:98.34ms
step:1357/1770 train_time:132466ms step_avg:98.34ms
step:1358/1770 train_time:132568ms step_avg:98.34ms
step:1359/1770 train_time:132671ms step_avg:98.35ms
step:1360/1770 train_time:132774ms step_avg:98.35ms
step:1361/1770 train_time:132876ms step_avg:98.35ms
step:1362/1770 train_time:132978ms step_avg:98.36ms
step:1363/1770 train_time:133081ms step_avg:98.36ms
step:1364/1770 train_time:133183ms step_avg:98.36ms
step:1365/1770 train_time:133285ms step_avg:98.37ms
step:1366/1770 train_time:133387ms step_avg:98.37ms
step:1367/1770 train_time:133489ms step_avg:98.37ms
step:1368/1770 train_time:133591ms step_avg:98.37ms
step:1369/1770 train_time:133693ms step_avg:98.38ms
step:1370/1770 train_time:133796ms step_avg:98.38ms
step:1371/1770 train_time:133899ms step_avg:98.38ms
step:1372/1770 train_time:134001ms step_avg:98.39ms
step:1373/1770 train_time:134103ms step_avg:98.39ms
step:1374/1770 train_time:134206ms step_avg:98.39ms
step:1375/1770 train_time:134308ms step_avg:98.39ms
step:1375/1770 val_loss:3.3823 train_time:134409ms step_avg:98.47ms
step:1376/1770 train_time:134429ms step_avg:98.41ms
step:1377/1770 train_time:134521ms step_avg:98.41ms
step:1378/1770 train_time:134623ms step_avg:98.41ms
step:1379/1770 train_time:134725ms step_avg:98.41ms
step:1380/1770 train_time:134827ms step_avg:98.41ms
step:1381/1770 train_time:134929ms step_avg:98.42ms
step:1382/1770 train_time:135030ms step_avg:98.42ms
step:1383/1770 train_time:135132ms step_avg:98.42ms
step:1384/1770 train_time:135235ms step_avg:98.42ms
step:1385/1770 train_time:135337ms step_avg:98.43ms
step:1386/1770 train_time:135441ms step_avg:98.43ms
step:1387/1770 train_time:135544ms step_avg:98.43ms
step:1388/1770 train_time:135646ms step_avg:98.44ms
step:1389/1770 train_time:135749ms step_avg:98.44ms
step:1390/1770 train_time:135850ms step_avg:98.44ms
step:1391/1770 train_time:135952ms step_avg:98.44ms
step:1392/1770 train_time:136054ms step_avg:98.45ms
step:1393/1770 train_time:136155ms step_avg:98.45ms
step:1394/1770 train_time:136257ms step_avg:98.45ms
step:1395/1770 train_time:136360ms step_avg:98.45ms
step:1396/1770 train_time:136464ms step_avg:98.46ms
step:1397/1770 train_time:136566ms step_avg:98.46ms
step:1398/1770 train_time:136668ms step_avg:98.46ms
step:1399/1770 train_time:136770ms step_avg:98.47ms
step:1400/1770 train_time:136873ms step_avg:98.47ms
step:1401/1770 train_time:136975ms step_avg:98.47ms
step:1402/1770 train_time:137077ms step_avg:98.47ms
step:1403/1770 train_time:137179ms step_avg:98.48ms
step:1404/1770 train_time:137282ms step_avg:98.48ms
step:1405/1770 train_time:137383ms step_avg:98.48ms
step:1406/1770 train_time:137486ms step_avg:98.49ms
step:1407/1770 train_time:137588ms step_avg:98.49ms
step:1408/1770 train_time:137690ms step_avg:98.49ms
step:1409/1770 train_time:137793ms step_avg:98.49ms
step:1410/1770 train_time:137896ms step_avg:98.50ms
step:1411/1770 train_time:137997ms step_avg:98.50ms
step:1412/1770 train_time:138099ms step_avg:98.50ms
step:1413/1770 train_time:138201ms step_avg:98.50ms
step:1414/1770 train_time:138303ms step_avg:98.51ms
step:1415/1770 train_time:138406ms step_avg:98.51ms
step:1416/1770 train_time:138509ms step_avg:98.51ms
step:1417/1770 train_time:138611ms step_avg:98.52ms
step:1418/1770 train_time:138714ms step_avg:98.52ms
step:1419/1770 train_time:138816ms step_avg:98.52ms
step:1420/1770 train_time:138919ms step_avg:98.52ms
step:1421/1770 train_time:139021ms step_avg:98.53ms
step:1422/1770 train_time:139122ms step_avg:98.53ms
step:1423/1770 train_time:139224ms step_avg:98.53ms
step:1424/1770 train_time:139327ms step_avg:98.53ms
step:1425/1770 train_time:139428ms step_avg:98.54ms
step:1426/1770 train_time:139531ms step_avg:98.54ms
step:1427/1770 train_time:139632ms step_avg:98.54ms
step:1428/1770 train_time:139736ms step_avg:98.54ms
step:1429/1770 train_time:139837ms step_avg:98.55ms
step:1430/1770 train_time:139940ms step_avg:98.55ms
step:1431/1770 train_time:140043ms step_avg:98.55ms
step:1432/1770 train_time:140144ms step_avg:98.55ms
step:1433/1770 train_time:140246ms step_avg:98.56ms
step:1434/1770 train_time:140347ms step_avg:98.56ms
step:1435/1770 train_time:140449ms step_avg:98.56ms
step:1436/1770 train_time:140552ms step_avg:98.56ms
step:1437/1770 train_time:140655ms step_avg:98.57ms
step:1438/1770 train_time:140757ms step_avg:98.57ms
step:1439/1770 train_time:140860ms step_avg:98.57ms
step:1440/1770 train_time:140961ms step_avg:98.57ms
step:1441/1770 train_time:141066ms step_avg:98.58ms
step:1442/1770 train_time:141168ms step_avg:98.58ms
step:1443/1770 train_time:141270ms step_avg:98.58ms
step:1444/1770 train_time:141372ms step_avg:98.59ms
step:1445/1770 train_time:141475ms step_avg:98.59ms
step:1446/1770 train_time:141578ms step_avg:98.59ms
step:1447/1770 train_time:141682ms step_avg:98.60ms
step:1448/1770 train_time:141785ms step_avg:98.60ms
step:1449/1770 train_time:141889ms step_avg:98.60ms
step:1450/1770 train_time:141992ms step_avg:98.61ms
step:1451/1770 train_time:142096ms step_avg:98.61ms
step:1452/1770 train_time:142200ms step_avg:98.61ms
step:1453/1770 train_time:142302ms step_avg:98.62ms
step:1454/1770 train_time:142405ms step_avg:98.62ms
step:1455/1770 train_time:142510ms step_avg:98.62ms
step:1456/1770 train_time:142614ms step_avg:98.63ms
step:1457/1770 train_time:142719ms step_avg:98.63ms
step:1458/1770 train_time:142822ms step_avg:98.63ms
step:1459/1770 train_time:142926ms step_avg:98.64ms
step:1460/1770 train_time:143030ms step_avg:98.64ms
step:1461/1770 train_time:143133ms step_avg:98.64ms
step:1462/1770 train_time:143236ms step_avg:98.65ms
step:1463/1770 train_time:143340ms step_avg:98.65ms
step:1464/1770 train_time:143446ms step_avg:98.66ms
step:1465/1770 train_time:143549ms step_avg:98.66ms
step:1466/1770 train_time:143653ms step_avg:98.66ms
step:1467/1770 train_time:143759ms step_avg:98.67ms
step:1468/1770 train_time:143862ms step_avg:98.67ms
step:1469/1770 train_time:143964ms step_avg:98.67ms
step:1470/1770 train_time:144067ms step_avg:98.68ms
step:1471/1770 train_time:144170ms step_avg:98.68ms
step:1472/1770 train_time:144273ms step_avg:98.68ms
step:1473/1770 train_time:144378ms step_avg:98.69ms
step:1474/1770 train_time:144483ms step_avg:98.69ms
step:1475/1770 train_time:144585ms step_avg:98.69ms
step:1476/1770 train_time:144689ms step_avg:98.70ms
step:1477/1770 train_time:144795ms step_avg:98.70ms
step:1478/1770 train_time:144899ms step_avg:98.70ms
step:1479/1770 train_time:145002ms step_avg:98.71ms
step:1480/1770 train_time:145105ms step_avg:98.71ms
step:1481/1770 train_time:145213ms step_avg:98.72ms
step:1482/1770 train_time:145315ms step_avg:98.72ms
step:1483/1770 train_time:145419ms step_avg:98.72ms
step:1484/1770 train_time:145522ms step_avg:98.73ms
step:1485/1770 train_time:145625ms step_avg:98.73ms
step:1486/1770 train_time:145728ms step_avg:98.73ms
step:1487/1770 train_time:145831ms step_avg:98.73ms
step:1488/1770 train_time:145935ms step_avg:98.74ms
step:1489/1770 train_time:146040ms step_avg:98.74ms
step:1490/1770 train_time:146144ms step_avg:98.75ms
step:1491/1770 train_time:146247ms step_avg:98.75ms
step:1492/1770 train_time:146350ms step_avg:98.75ms
step:1493/1770 train_time:146456ms step_avg:98.76ms
step:1494/1770 train_time:146562ms step_avg:98.76ms
step:1495/1770 train_time:146665ms step_avg:98.76ms
step:1496/1770 train_time:146767ms step_avg:98.77ms
step:1497/1770 train_time:146871ms step_avg:98.77ms
step:1498/1770 train_time:146973ms step_avg:98.77ms
step:1499/1770 train_time:147076ms step_avg:98.78ms
step:1500/1770 train_time:147179ms step_avg:98.78ms
step:1500/1770 val_loss:3.3450 train_time:147280ms step_avg:98.85ms
step:1501/1770 train_time:147302ms step_avg:98.79ms
step:1502/1770 train_time:147395ms step_avg:98.79ms
step:1503/1770 train_time:147498ms step_avg:98.79ms
step:1504/1770 train_time:147601ms step_avg:98.80ms
step:1505/1770 train_time:147706ms step_avg:98.80ms
step:1506/1770 train_time:147809ms step_avg:98.80ms
step:1507/1770 train_time:147913ms step_avg:98.81ms
step:1508/1770 train_time:148018ms step_avg:98.81ms
step:1509/1770 train_time:148120ms step_avg:98.81ms
step:1510/1770 train_time:148223ms step_avg:98.82ms
step:1511/1770 train_time:148328ms step_avg:98.82ms
step:1512/1770 train_time:148432ms step_avg:98.82ms
step:1513/1770 train_time:148537ms step_avg:98.83ms
step:1514/1770 train_time:148640ms step_avg:98.83ms
step:1515/1770 train_time:148743ms step_avg:98.83ms
step:1516/1770 train_time:148846ms step_avg:98.84ms
step:1517/1770 train_time:148950ms step_avg:98.84ms
step:1518/1770 train_time:149054ms step_avg:98.84ms
step:1519/1770 train_time:149156ms step_avg:98.84ms
step:1520/1770 train_time:149260ms step_avg:98.85ms
step:1521/1770 train_time:149362ms step_avg:98.85ms
step:1522/1770 train_time:149467ms step_avg:98.85ms
step:1523/1770 train_time:149572ms step_avg:98.86ms
step:1524/1770 train_time:149675ms step_avg:98.86ms
step:1525/1770 train_time:149778ms step_avg:98.86ms
step:1526/1770 train_time:149881ms step_avg:98.87ms
step:1527/1770 train_time:149984ms step_avg:98.87ms
step:1528/1770 train_time:150090ms step_avg:98.87ms
step:1529/1770 train_time:150193ms step_avg:98.88ms
step:1530/1770 train_time:150296ms step_avg:98.88ms
step:1531/1770 train_time:150399ms step_avg:98.88ms
step:1532/1770 train_time:150503ms step_avg:98.88ms
step:1533/1770 train_time:150606ms step_avg:98.89ms
step:1534/1770 train_time:150711ms step_avg:98.89ms
step:1535/1770 train_time:150814ms step_avg:98.89ms
step:1536/1770 train_time:150918ms step_avg:98.90ms
step:1537/1770 train_time:151021ms step_avg:98.90ms
step:1538/1770 train_time:151126ms step_avg:98.90ms
step:1539/1770 train_time:151230ms step_avg:98.91ms
step:1540/1770 train_time:151336ms step_avg:98.91ms
step:1541/1770 train_time:151440ms step_avg:98.92ms
step:1542/1770 train_time:151543ms step_avg:98.92ms
step:1543/1770 train_time:151645ms step_avg:98.92ms
step:1544/1770 train_time:151750ms step_avg:98.92ms
step:1545/1770 train_time:151854ms step_avg:98.93ms
step:1546/1770 train_time:151958ms step_avg:98.93ms
step:1547/1770 train_time:152060ms step_avg:98.93ms
step:1548/1770 train_time:152163ms step_avg:98.94ms
step:1549/1770 train_time:152267ms step_avg:98.94ms
step:1550/1770 train_time:152372ms step_avg:98.94ms
step:1551/1770 train_time:152476ms step_avg:98.95ms
step:1552/1770 train_time:152581ms step_avg:98.95ms
step:1553/1770 train_time:152684ms step_avg:98.95ms
step:1554/1770 train_time:152788ms step_avg:98.96ms
step:1555/1770 train_time:152892ms step_avg:98.96ms
step:1556/1770 train_time:152994ms step_avg:98.96ms
step:1557/1770 train_time:153097ms step_avg:98.96ms
step:1558/1770 train_time:153202ms step_avg:98.97ms
step:1559/1770 train_time:153305ms step_avg:98.97ms
step:1560/1770 train_time:153408ms step_avg:98.97ms
step:1561/1770 train_time:153513ms step_avg:98.98ms
step:1562/1770 train_time:153616ms step_avg:98.98ms
step:1563/1770 train_time:153720ms step_avg:98.98ms
step:1564/1770 train_time:153822ms step_avg:98.98ms
step:1565/1770 train_time:153925ms step_avg:98.99ms
step:1566/1770 train_time:154028ms step_avg:98.99ms
step:1567/1770 train_time:154133ms step_avg:98.99ms
step:1568/1770 train_time:154236ms step_avg:99.00ms
step:1569/1770 train_time:154344ms step_avg:99.00ms
step:1570/1770 train_time:154446ms step_avg:99.00ms
step:1571/1770 train_time:154549ms step_avg:99.01ms
step:1572/1770 train_time:154653ms step_avg:99.01ms
step:1573/1770 train_time:154759ms step_avg:99.01ms
step:1574/1770 train_time:154862ms step_avg:99.02ms
step:1575/1770 train_time:154964ms step_avg:99.02ms
step:1576/1770 train_time:155066ms step_avg:99.02ms
step:1577/1770 train_time:155170ms step_avg:99.02ms
step:1578/1770 train_time:155277ms step_avg:99.03ms
step:1579/1770 train_time:155380ms step_avg:99.03ms
step:1580/1770 train_time:155483ms step_avg:99.03ms
step:1581/1770 train_time:155590ms step_avg:99.04ms
step:1582/1770 train_time:155694ms step_avg:99.04ms
step:1583/1770 train_time:155798ms step_avg:99.05ms
step:1584/1770 train_time:155902ms step_avg:99.05ms
step:1585/1770 train_time:156006ms step_avg:99.05ms
step:1586/1770 train_time:156113ms step_avg:99.06ms
step:1587/1770 train_time:156216ms step_avg:99.06ms
step:1588/1770 train_time:156320ms step_avg:99.06ms
step:1589/1770 train_time:156424ms step_avg:99.07ms
step:1590/1770 train_time:156527ms step_avg:99.07ms
step:1591/1770 train_time:156630ms step_avg:99.07ms
step:1592/1770 train_time:156734ms step_avg:99.07ms
step:1593/1770 train_time:156837ms step_avg:99.08ms
step:1594/1770 train_time:156941ms step_avg:99.08ms
step:1595/1770 train_time:157044ms step_avg:99.08ms
step:1596/1770 train_time:157148ms step_avg:99.08ms
step:1597/1770 train_time:157251ms step_avg:99.09ms
step:1598/1770 train_time:157354ms step_avg:99.09ms
step:1599/1770 train_time:157458ms step_avg:99.09ms
step:1600/1770 train_time:157564ms step_avg:99.10ms
step:1601/1770 train_time:157668ms step_avg:99.10ms
step:1602/1770 train_time:157772ms step_avg:99.10ms
step:1603/1770 train_time:157875ms step_avg:99.11ms
step:1604/1770 train_time:157978ms step_avg:99.11ms
step:1605/1770 train_time:158080ms step_avg:99.11ms
step:1606/1770 train_time:158184ms step_avg:99.11ms
step:1607/1770 train_time:158291ms step_avg:99.12ms
step:1608/1770 train_time:158394ms step_avg:99.12ms
step:1609/1770 train_time:158497ms step_avg:99.12ms
step:1610/1770 train_time:158602ms step_avg:99.13ms
step:1611/1770 train_time:158707ms step_avg:99.13ms
step:1612/1770 train_time:158811ms step_avg:99.13ms
step:1613/1770 train_time:158914ms step_avg:99.14ms
step:1614/1770 train_time:159018ms step_avg:99.14ms
step:1615/1770 train_time:159121ms step_avg:99.14ms
step:1616/1770 train_time:159225ms step_avg:99.14ms
step:1617/1770 train_time:159330ms step_avg:99.15ms
step:1618/1770 train_time:159434ms step_avg:99.15ms
step:1619/1770 train_time:159538ms step_avg:99.15ms
step:1620/1770 train_time:159642ms step_avg:99.16ms
step:1621/1770 train_time:159746ms step_avg:99.16ms
step:1622/1770 train_time:159850ms step_avg:99.16ms
step:1623/1770 train_time:159956ms step_avg:99.17ms
step:1624/1770 train_time:160059ms step_avg:99.17ms
step:1625/1770 train_time:160162ms step_avg:99.17ms
step:1625/1770 val_loss:3.3105 train_time:160263ms step_avg:99.23ms
step:1626/1770 train_time:160284ms step_avg:99.19ms
step:1627/1770 train_time:160378ms step_avg:99.18ms
step:1628/1770 train_time:160480ms step_avg:99.18ms
step:1629/1770 train_time:160582ms step_avg:99.19ms
step:1630/1770 train_time:160685ms step_avg:99.19ms
step:1631/1770 train_time:160788ms step_avg:99.19ms
step:1632/1770 train_time:160890ms step_avg:99.19ms
step:1633/1770 train_time:160994ms step_avg:99.20ms
step:1634/1770 train_time:161097ms step_avg:99.20ms
step:1635/1770 train_time:161200ms step_avg:99.20ms
step:1636/1770 train_time:161305ms step_avg:99.20ms
step:1637/1770 train_time:161410ms step_avg:99.21ms
step:1638/1770 train_time:161513ms step_avg:99.21ms
step:1639/1770 train_time:161616ms step_avg:99.21ms
step:1640/1770 train_time:161721ms step_avg:99.22ms
step:1641/1770 train_time:161824ms step_avg:99.22ms
step:1642/1770 train_time:161926ms step_avg:99.22ms
step:1643/1770 train_time:162029ms step_avg:99.22ms
step:1644/1770 train_time:162135ms step_avg:99.23ms
step:1645/1770 train_time:162237ms step_avg:99.23ms
step:1646/1770 train_time:162343ms step_avg:99.23ms
step:1647/1770 train_time:162447ms step_avg:99.23ms
step:1648/1770 train_time:162550ms step_avg:99.24ms
step:1649/1770 train_time:162653ms step_avg:99.24ms
step:1650/1770 train_time:162757ms step_avg:99.24ms
step:1651/1770 train_time:162860ms step_avg:99.24ms
step:1652/1770 train_time:162963ms step_avg:99.25ms
step:1653/1770 train_time:163066ms step_avg:99.25ms
step:1654/1770 train_time:163173ms step_avg:99.25ms
step:1655/1770 train_time:163279ms step_avg:99.26ms
step:1656/1770 train_time:163382ms step_avg:99.26ms
step:1657/1770 train_time:163486ms step_avg:99.26ms
step:1658/1770 train_time:163590ms step_avg:99.27ms
step:1659/1770 train_time:163694ms step_avg:99.27ms
step:1660/1770 train_time:163798ms step_avg:99.27ms
step:1661/1770 train_time:163903ms step_avg:99.28ms
step:1662/1770 train_time:164007ms step_avg:99.28ms
step:1663/1770 train_time:164109ms step_avg:99.28ms
step:1664/1770 train_time:164212ms step_avg:99.28ms
step:1665/1770 train_time:164316ms step_avg:99.28ms
step:1666/1770 train_time:164419ms step_avg:99.29ms
step:1667/1770 train_time:164522ms step_avg:99.29ms
step:1668/1770 train_time:164625ms step_avg:99.29ms
step:1669/1770 train_time:164728ms step_avg:99.29ms
step:1670/1770 train_time:164832ms step_avg:99.30ms
step:1671/1770 train_time:164936ms step_avg:99.30ms
step:1672/1770 train_time:165041ms step_avg:99.30ms
step:1673/1770 train_time:165145ms step_avg:99.31ms
step:1674/1770 train_time:165249ms step_avg:99.31ms
step:1675/1770 train_time:165353ms step_avg:99.31ms
step:1676/1770 train_time:165457ms step_avg:99.31ms
step:1677/1770 train_time:165564ms step_avg:99.32ms
step:1678/1770 train_time:165666ms step_avg:99.32ms
step:1679/1770 train_time:165770ms step_avg:99.32ms
step:1680/1770 train_time:165873ms step_avg:99.33ms
step:1681/1770 train_time:165978ms step_avg:99.33ms
step:1682/1770 train_time:166084ms step_avg:99.33ms
step:1683/1770 train_time:166186ms step_avg:99.33ms
step:1684/1770 train_time:166289ms step_avg:99.34ms
step:1685/1770 train_time:166391ms step_avg:99.34ms
step:1686/1770 train_time:166497ms step_avg:99.34ms
step:1687/1770 train_time:166602ms step_avg:99.35ms
step:1688/1770 train_time:166706ms step_avg:99.35ms
step:1689/1770 train_time:166809ms step_avg:99.35ms
step:1690/1770 train_time:166913ms step_avg:99.35ms
step:1691/1770 train_time:167016ms step_avg:99.36ms
step:1692/1770 train_time:167121ms step_avg:99.36ms
step:1693/1770 train_time:167226ms step_avg:99.36ms
step:1694/1770 train_time:167329ms step_avg:99.36ms
step:1695/1770 train_time:167432ms step_avg:99.37ms
step:1696/1770 train_time:167537ms step_avg:99.37ms
step:1697/1770 train_time:167642ms step_avg:99.37ms
step:1698/1770 train_time:167747ms step_avg:99.38ms
step:1699/1770 train_time:167850ms step_avg:99.38ms
step:1700/1770 train_time:167953ms step_avg:99.38ms
step:1701/1770 train_time:168057ms step_avg:99.38ms
step:1702/1770 train_time:168161ms step_avg:99.39ms
step:1703/1770 train_time:168264ms step_avg:99.39ms
step:1704/1770 train_time:168367ms step_avg:99.39ms
step:1705/1770 train_time:168471ms step_avg:99.39ms
step:1706/1770 train_time:168574ms step_avg:99.39ms
step:1707/1770 train_time:168678ms step_avg:99.40ms
step:1708/1770 train_time:168783ms step_avg:99.40ms
step:1709/1770 train_time:168888ms step_avg:99.40ms
step:1710/1770 train_time:168995ms step_avg:99.41ms
step:1711/1770 train_time:169102ms step_avg:99.41ms
step:1712/1770 train_time:169205ms step_avg:99.42ms
step:1713/1770 train_time:169308ms step_avg:99.42ms
step:1714/1770 train_time:169412ms step_avg:99.42ms
step:1715/1770 train_time:169516ms step_avg:99.42ms
step:1716/1770 train_time:169621ms step_avg:99.43ms
step:1717/1770 train_time:169725ms step_avg:99.43ms
step:1718/1770 train_time:169830ms step_avg:99.43ms
step:1719/1770 train_time:169935ms step_avg:99.44ms
step:1720/1770 train_time:170040ms step_avg:99.44ms
step:1721/1770 train_time:170143ms step_avg:99.44ms
step:1722/1770 train_time:170250ms step_avg:99.44ms
step:1723/1770 train_time:170355ms step_avg:99.45ms
step:1724/1770 train_time:170461ms step_avg:99.45ms
step:1725/1770 train_time:170567ms step_avg:99.46ms
step:1726/1770 train_time:170673ms step_avg:99.46ms
step:1727/1770 train_time:170776ms step_avg:99.46ms
step:1728/1770 train_time:170882ms step_avg:99.47ms
step:1729/1770 train_time:170985ms step_avg:99.47ms
step:1730/1770 train_time:171091ms step_avg:99.47ms
step:1731/1770 train_time:171196ms step_avg:99.48ms
step:1732/1770 train_time:171300ms step_avg:99.48ms
step:1733/1770 train_time:171405ms step_avg:99.48ms
step:1734/1770 train_time:171508ms step_avg:99.48ms
step:1735/1770 train_time:171613ms step_avg:99.49ms
step:1736/1770 train_time:171716ms step_avg:99.49ms
step:1737/1770 train_time:171821ms step_avg:99.49ms
step:1738/1770 train_time:171925ms step_avg:99.49ms
step:1739/1770 train_time:172029ms step_avg:99.50ms
step:1740/1770 train_time:172133ms step_avg:99.50ms
step:1741/1770 train_time:172240ms step_avg:99.50ms
step:1742/1770 train_time:172347ms step_avg:99.51ms
step:1743/1770 train_time:172452ms step_avg:99.51ms
step:1744/1770 train_time:172557ms step_avg:99.51ms
step:1745/1770 train_time:172660ms step_avg:99.52ms
step:1746/1770 train_time:172767ms step_avg:99.52ms
step:1747/1770 train_time:172870ms step_avg:99.52ms
step:1748/1770 train_time:172976ms step_avg:99.53ms
step:1749/1770 train_time:173081ms step_avg:99.53ms
step:1750/1770 train_time:173185ms step_avg:99.53ms
step:1750/1770 val_loss:3.2836 train_time:173289ms step_avg:99.59ms
step:1751/1770 train_time:173309ms step_avg:99.55ms
step:1752/1770 train_time:173401ms step_avg:99.54ms
step:1753/1770 train_time:173505ms step_avg:99.54ms
step:1754/1770 train_time:173610ms step_avg:99.55ms
step:1755/1770 train_time:173714ms step_avg:99.55ms
step:1756/1770 train_time:173819ms step_avg:99.55ms
step:1757/1770 train_time:173923ms step_avg:99.56ms
step:1758/1770 train_time:174027ms step_avg:99.56ms
step:1759/1770 train_time:174131ms step_avg:99.56ms
step:1760/1770 train_time:174235ms step_avg:99.56ms
step:1761/1770 train_time:174343ms step_avg:99.57ms
step:1762/1770 train_time:174451ms step_avg:99.57ms
step:1763/1770 train_time:174554ms step_avg:99.57ms
step:1764/1770 train_time:174658ms step_avg:99.58ms
step:1765/1770 train_time:174762ms step_avg:99.58ms
step:1766/1770 train_time:174870ms step_avg:99.58ms
step:1767/1770 train_time:174972ms step_avg:99.59ms
step:1768/1770 train_time:175077ms step_avg:99.59ms
step:1769/1770 train_time:175180ms step_avg:99.59ms
step:1770/1770 train_time:175283ms step_avg:99.59ms
step:1770/1770 val_loss:3.2806 train_time:175388ms step_avg:99.65ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
