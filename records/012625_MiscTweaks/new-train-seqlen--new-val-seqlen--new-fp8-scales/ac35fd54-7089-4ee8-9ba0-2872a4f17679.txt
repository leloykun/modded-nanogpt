import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 17:03:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24602ms step_avg:nanms
step:2/1770 train_time:25204ms step_avg:nanms
step:3/1770 train_time:25298ms step_avg:nanms
step:4/1770 train_time:25391ms step_avg:nanms
step:5/1770 train_time:25485ms step_avg:nanms
step:6/1770 train_time:25578ms step_avg:nanms
step:7/1770 train_time:25672ms step_avg:nanms
step:8/1770 train_time:25766ms step_avg:nanms
step:9/1770 train_time:25860ms step_avg:nanms
step:10/1770 train_time:25953ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.15ms
step:14/1770 train_time:377ms step_avg:94.23ms
step:15/1770 train_time:472ms step_avg:94.31ms
step:16/1770 train_time:566ms step_avg:94.27ms
step:17/1770 train_time:659ms step_avg:94.20ms
step:18/1770 train_time:754ms step_avg:94.21ms
step:19/1770 train_time:847ms step_avg:94.16ms
step:20/1770 train_time:941ms step_avg:94.11ms
step:21/1770 train_time:1035ms step_avg:94.07ms
step:22/1770 train_time:1129ms step_avg:94.08ms
step:23/1770 train_time:1223ms step_avg:94.09ms
step:24/1770 train_time:1317ms step_avg:94.08ms
step:25/1770 train_time:1412ms step_avg:94.10ms
step:26/1770 train_time:1506ms step_avg:94.11ms
step:27/1770 train_time:1600ms step_avg:94.12ms
step:28/1770 train_time:1697ms step_avg:94.26ms
step:29/1770 train_time:1789ms step_avg:94.17ms
step:30/1770 train_time:1883ms step_avg:94.15ms
step:31/1770 train_time:1977ms step_avg:94.13ms
step:32/1770 train_time:2071ms step_avg:94.12ms
step:33/1770 train_time:2164ms step_avg:94.10ms
step:34/1770 train_time:2258ms step_avg:94.08ms
step:35/1770 train_time:2352ms step_avg:94.08ms
step:36/1770 train_time:2446ms step_avg:94.06ms
step:37/1770 train_time:2540ms step_avg:94.07ms
step:38/1770 train_time:2634ms step_avg:94.07ms
step:39/1770 train_time:2728ms step_avg:94.06ms
step:40/1770 train_time:2821ms step_avg:94.04ms
step:41/1770 train_time:2915ms step_avg:94.03ms
step:42/1770 train_time:3008ms step_avg:94.01ms
step:43/1770 train_time:3103ms step_avg:94.02ms
step:44/1770 train_time:3197ms step_avg:94.02ms
step:45/1770 train_time:3291ms step_avg:94.03ms
step:46/1770 train_time:3385ms step_avg:94.02ms
step:47/1770 train_time:3479ms step_avg:94.02ms
step:48/1770 train_time:3573ms step_avg:94.02ms
step:49/1770 train_time:3667ms step_avg:94.02ms
step:50/1770 train_time:3761ms step_avg:94.02ms
step:51/1770 train_time:3856ms step_avg:94.04ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4043ms step_avg:94.02ms
step:54/1770 train_time:4136ms step_avg:94.01ms
step:55/1770 train_time:4230ms step_avg:94.00ms
step:56/1770 train_time:4324ms step_avg:94.00ms
step:57/1770 train_time:4418ms step_avg:93.99ms
step:58/1770 train_time:4512ms step_avg:94.00ms
step:59/1770 train_time:4606ms step_avg:93.99ms
step:60/1770 train_time:4699ms step_avg:93.98ms
step:61/1770 train_time:4793ms step_avg:93.97ms
step:62/1770 train_time:4886ms step_avg:93.97ms
step:63/1770 train_time:4980ms step_avg:93.97ms
step:64/1770 train_time:5076ms step_avg:94.00ms
step:65/1770 train_time:5169ms step_avg:93.99ms
step:66/1770 train_time:5263ms step_avg:93.99ms
step:67/1770 train_time:5358ms step_avg:94.00ms
step:68/1770 train_time:5454ms step_avg:94.03ms
step:69/1770 train_time:5547ms step_avg:94.02ms
step:70/1770 train_time:5641ms step_avg:94.02ms
step:71/1770 train_time:5735ms step_avg:94.02ms
step:72/1770 train_time:5829ms step_avg:94.02ms
step:73/1770 train_time:5922ms step_avg:94.01ms
step:74/1770 train_time:6016ms step_avg:94.00ms
step:75/1770 train_time:6110ms step_avg:93.99ms
step:76/1770 train_time:6203ms step_avg:93.99ms
step:77/1770 train_time:6297ms step_avg:93.99ms
step:78/1770 train_time:6392ms step_avg:94.00ms
step:79/1770 train_time:6486ms step_avg:94.00ms
step:80/1770 train_time:6580ms step_avg:94.00ms
step:81/1770 train_time:6675ms step_avg:94.01ms
step:82/1770 train_time:6768ms step_avg:94.01ms
step:83/1770 train_time:6862ms step_avg:94.00ms
step:84/1770 train_time:6956ms step_avg:94.00ms
step:85/1770 train_time:7050ms step_avg:94.00ms
step:86/1770 train_time:7145ms step_avg:94.01ms
step:87/1770 train_time:7238ms step_avg:94.00ms
step:88/1770 train_time:7331ms step_avg:93.99ms
step:89/1770 train_time:7425ms step_avg:93.99ms
step:90/1770 train_time:7519ms step_avg:93.99ms
step:91/1770 train_time:7614ms step_avg:94.00ms
step:92/1770 train_time:7708ms step_avg:94.00ms
step:93/1770 train_time:7802ms step_avg:94.00ms
step:94/1770 train_time:7896ms step_avg:94.00ms
step:95/1770 train_time:7990ms step_avg:94.00ms
step:96/1770 train_time:8083ms step_avg:93.99ms
step:97/1770 train_time:8177ms step_avg:93.99ms
step:98/1770 train_time:8271ms step_avg:93.99ms
step:99/1770 train_time:8365ms step_avg:93.99ms
step:100/1770 train_time:8459ms step_avg:93.98ms
step:101/1770 train_time:8553ms step_avg:93.99ms
step:102/1770 train_time:8646ms step_avg:93.98ms
step:103/1770 train_time:8743ms step_avg:94.01ms
step:104/1770 train_time:8835ms step_avg:93.99ms
step:105/1770 train_time:8929ms step_avg:93.99ms
step:106/1770 train_time:9023ms step_avg:93.99ms
step:107/1770 train_time:9116ms step_avg:93.98ms
step:108/1770 train_time:9210ms step_avg:93.98ms
step:109/1770 train_time:9303ms step_avg:93.97ms
step:110/1770 train_time:9397ms step_avg:93.97ms
step:111/1770 train_time:9491ms step_avg:93.97ms
step:112/1770 train_time:9584ms step_avg:93.97ms
step:113/1770 train_time:9678ms step_avg:93.96ms
step:114/1770 train_time:9772ms step_avg:93.96ms
step:115/1770 train_time:9866ms step_avg:93.96ms
step:116/1770 train_time:9960ms step_avg:93.96ms
step:117/1770 train_time:10054ms step_avg:93.96ms
step:118/1770 train_time:10147ms step_avg:93.96ms
step:119/1770 train_time:10241ms step_avg:93.95ms
step:120/1770 train_time:10335ms step_avg:93.95ms
step:121/1770 train_time:10429ms step_avg:93.95ms
step:122/1770 train_time:10523ms step_avg:93.95ms
step:123/1770 train_time:10617ms step_avg:93.95ms
step:124/1770 train_time:10711ms step_avg:93.95ms
step:125/1770 train_time:10804ms step_avg:93.95ms
step:125/1770 val_loss:4.6516 train_time:10896ms step_avg:94.75ms
step:126/1770 train_time:10920ms step_avg:94.14ms
step:127/1770 train_time:10994ms step_avg:93.97ms
step:128/1770 train_time:11089ms step_avg:93.98ms
step:129/1770 train_time:11192ms step_avg:94.05ms
step:130/1770 train_time:11289ms step_avg:94.07ms
step:131/1770 train_time:11383ms step_avg:94.07ms
step:132/1770 train_time:11477ms step_avg:94.07ms
step:133/1770 train_time:11571ms step_avg:94.07ms
step:134/1770 train_time:11665ms step_avg:94.07ms
step:135/1770 train_time:11759ms step_avg:94.07ms
step:136/1770 train_time:11853ms step_avg:94.07ms
step:137/1770 train_time:11947ms step_avg:94.07ms
step:138/1770 train_time:12041ms step_avg:94.07ms
step:139/1770 train_time:12136ms step_avg:94.08ms
step:140/1770 train_time:12231ms step_avg:94.09ms
step:141/1770 train_time:12326ms step_avg:94.09ms
step:142/1770 train_time:12421ms step_avg:94.10ms
step:143/1770 train_time:12516ms step_avg:94.10ms
step:144/1770 train_time:12609ms step_avg:94.10ms
step:145/1770 train_time:12704ms step_avg:94.10ms
step:146/1770 train_time:12798ms step_avg:94.10ms
step:147/1770 train_time:12892ms step_avg:94.10ms
step:148/1770 train_time:12987ms step_avg:94.11ms
step:149/1770 train_time:13081ms step_avg:94.11ms
step:150/1770 train_time:13176ms step_avg:94.12ms
step:151/1770 train_time:13272ms step_avg:94.12ms
step:152/1770 train_time:13368ms step_avg:94.14ms
step:153/1770 train_time:13461ms step_avg:94.14ms
step:154/1770 train_time:13556ms step_avg:94.14ms
step:155/1770 train_time:13651ms step_avg:94.15ms
step:156/1770 train_time:13746ms step_avg:94.15ms
step:157/1770 train_time:13840ms step_avg:94.15ms
step:158/1770 train_time:13934ms step_avg:94.15ms
step:159/1770 train_time:14029ms step_avg:94.15ms
step:160/1770 train_time:14123ms step_avg:94.16ms
step:161/1770 train_time:14218ms step_avg:94.16ms
step:162/1770 train_time:14312ms step_avg:94.16ms
step:163/1770 train_time:14407ms step_avg:94.16ms
step:164/1770 train_time:14501ms step_avg:94.16ms
step:165/1770 train_time:14596ms step_avg:94.17ms
step:166/1770 train_time:14691ms step_avg:94.17ms
step:167/1770 train_time:14785ms step_avg:94.18ms
step:168/1770 train_time:14880ms step_avg:94.18ms
step:169/1770 train_time:14975ms step_avg:94.18ms
step:170/1770 train_time:15070ms step_avg:94.19ms
step:171/1770 train_time:15166ms step_avg:94.20ms
step:172/1770 train_time:15258ms step_avg:94.19ms
step:173/1770 train_time:15353ms step_avg:94.19ms
step:174/1770 train_time:15448ms step_avg:94.19ms
step:175/1770 train_time:15542ms step_avg:94.20ms
step:176/1770 train_time:15637ms step_avg:94.20ms
step:177/1770 train_time:15732ms step_avg:94.21ms
step:178/1770 train_time:15826ms step_avg:94.20ms
step:179/1770 train_time:15920ms step_avg:94.20ms
step:180/1770 train_time:16015ms step_avg:94.21ms
step:181/1770 train_time:16110ms step_avg:94.21ms
step:182/1770 train_time:16204ms step_avg:94.21ms
step:183/1770 train_time:16298ms step_avg:94.21ms
step:184/1770 train_time:16393ms step_avg:94.21ms
step:185/1770 train_time:16487ms step_avg:94.21ms
step:186/1770 train_time:16582ms step_avg:94.21ms
step:187/1770 train_time:16677ms step_avg:94.22ms
step:188/1770 train_time:16772ms step_avg:94.23ms
step:189/1770 train_time:16867ms step_avg:94.23ms
step:190/1770 train_time:16961ms step_avg:94.23ms
step:191/1770 train_time:17056ms step_avg:94.23ms
step:192/1770 train_time:17151ms step_avg:94.24ms
step:193/1770 train_time:17245ms step_avg:94.24ms
step:194/1770 train_time:17340ms step_avg:94.24ms
step:195/1770 train_time:17435ms step_avg:94.24ms
step:196/1770 train_time:17530ms step_avg:94.24ms
step:197/1770 train_time:17624ms step_avg:94.24ms
step:198/1770 train_time:17719ms step_avg:94.25ms
step:199/1770 train_time:17814ms step_avg:94.25ms
step:200/1770 train_time:17908ms step_avg:94.25ms
step:201/1770 train_time:18003ms step_avg:94.26ms
step:202/1770 train_time:18099ms step_avg:94.26ms
step:203/1770 train_time:18193ms step_avg:94.27ms
step:204/1770 train_time:18288ms step_avg:94.27ms
step:205/1770 train_time:18382ms step_avg:94.27ms
step:206/1770 train_time:18476ms step_avg:94.27ms
step:207/1770 train_time:18572ms step_avg:94.27ms
step:208/1770 train_time:18666ms step_avg:94.27ms
step:209/1770 train_time:18760ms step_avg:94.27ms
step:210/1770 train_time:18856ms step_avg:94.28ms
step:211/1770 train_time:18950ms step_avg:94.28ms
step:212/1770 train_time:19045ms step_avg:94.28ms
step:213/1770 train_time:19139ms step_avg:94.28ms
step:214/1770 train_time:19234ms step_avg:94.29ms
step:215/1770 train_time:19329ms step_avg:94.29ms
step:216/1770 train_time:19423ms step_avg:94.29ms
step:217/1770 train_time:19517ms step_avg:94.29ms
step:218/1770 train_time:19612ms step_avg:94.29ms
step:219/1770 train_time:19706ms step_avg:94.29ms
step:220/1770 train_time:19801ms step_avg:94.29ms
step:221/1770 train_time:19896ms step_avg:94.30ms
step:222/1770 train_time:19991ms step_avg:94.30ms
step:223/1770 train_time:20086ms step_avg:94.30ms
step:224/1770 train_time:20181ms step_avg:94.30ms
step:225/1770 train_time:20276ms step_avg:94.30ms
step:226/1770 train_time:20370ms step_avg:94.31ms
step:227/1770 train_time:20464ms step_avg:94.31ms
step:228/1770 train_time:20559ms step_avg:94.31ms
step:229/1770 train_time:20655ms step_avg:94.31ms
step:230/1770 train_time:20749ms step_avg:94.31ms
step:231/1770 train_time:20842ms step_avg:94.31ms
step:232/1770 train_time:20938ms step_avg:94.31ms
step:233/1770 train_time:21032ms step_avg:94.32ms
step:234/1770 train_time:21127ms step_avg:94.32ms
step:235/1770 train_time:21221ms step_avg:94.32ms
step:236/1770 train_time:21316ms step_avg:94.32ms
step:237/1770 train_time:21411ms step_avg:94.32ms
step:238/1770 train_time:21505ms step_avg:94.32ms
step:239/1770 train_time:21600ms step_avg:94.32ms
step:240/1770 train_time:21695ms step_avg:94.32ms
step:241/1770 train_time:21789ms step_avg:94.32ms
step:242/1770 train_time:21884ms step_avg:94.33ms
step:243/1770 train_time:21978ms step_avg:94.33ms
step:244/1770 train_time:22073ms step_avg:94.33ms
step:245/1770 train_time:22168ms step_avg:94.33ms
step:246/1770 train_time:22262ms step_avg:94.33ms
step:247/1770 train_time:22357ms step_avg:94.33ms
step:248/1770 train_time:22452ms step_avg:94.34ms
step:249/1770 train_time:22546ms step_avg:94.34ms
step:250/1770 train_time:22640ms step_avg:94.34ms
step:250/1770 val_loss:4.1196 train_time:22734ms step_avg:94.72ms
step:251/1770 train_time:22756ms step_avg:94.42ms
step:252/1770 train_time:22837ms step_avg:94.37ms
step:253/1770 train_time:22935ms step_avg:94.38ms
step:254/1770 train_time:23030ms step_avg:94.39ms
step:255/1770 train_time:23124ms step_avg:94.39ms
step:256/1770 train_time:23219ms step_avg:94.38ms
step:257/1770 train_time:23313ms step_avg:94.39ms
step:258/1770 train_time:23407ms step_avg:94.38ms
step:259/1770 train_time:23501ms step_avg:94.38ms
step:260/1770 train_time:23596ms step_avg:94.38ms
step:261/1770 train_time:23691ms step_avg:94.38ms
step:262/1770 train_time:23786ms step_avg:94.39ms
step:263/1770 train_time:23881ms step_avg:94.39ms
step:264/1770 train_time:23977ms step_avg:94.40ms
step:265/1770 train_time:24073ms step_avg:94.40ms
step:266/1770 train_time:24168ms step_avg:94.41ms
step:267/1770 train_time:24263ms step_avg:94.41ms
step:268/1770 train_time:24358ms step_avg:94.41ms
step:269/1770 train_time:24453ms step_avg:94.41ms
step:270/1770 train_time:24548ms step_avg:94.42ms
step:271/1770 train_time:24643ms step_avg:94.42ms
step:272/1770 train_time:24738ms step_avg:94.42ms
step:273/1770 train_time:24834ms step_avg:94.42ms
step:274/1770 train_time:24929ms step_avg:94.43ms
step:275/1770 train_time:25025ms step_avg:94.43ms
step:276/1770 train_time:25120ms step_avg:94.44ms
step:277/1770 train_time:25215ms step_avg:94.44ms
step:278/1770 train_time:25311ms step_avg:94.44ms
step:279/1770 train_time:25405ms step_avg:94.44ms
step:280/1770 train_time:25500ms step_avg:94.45ms
step:281/1770 train_time:25596ms step_avg:94.45ms
step:282/1770 train_time:25691ms step_avg:94.45ms
step:283/1770 train_time:25786ms step_avg:94.45ms
step:284/1770 train_time:25881ms step_avg:94.46ms
step:285/1770 train_time:25977ms step_avg:94.46ms
step:286/1770 train_time:26073ms step_avg:94.47ms
step:287/1770 train_time:26169ms step_avg:94.47ms
step:288/1770 train_time:26264ms step_avg:94.48ms
step:289/1770 train_time:26359ms step_avg:94.48ms
step:290/1770 train_time:26455ms step_avg:94.48ms
step:291/1770 train_time:26550ms step_avg:94.48ms
step:292/1770 train_time:26644ms step_avg:94.48ms
step:293/1770 train_time:26739ms step_avg:94.48ms
step:294/1770 train_time:26835ms step_avg:94.49ms
step:295/1770 train_time:26930ms step_avg:94.49ms
step:296/1770 train_time:27025ms step_avg:94.49ms
step:297/1770 train_time:27121ms step_avg:94.50ms
step:298/1770 train_time:27216ms step_avg:94.50ms
step:299/1770 train_time:27312ms step_avg:94.50ms
step:300/1770 train_time:27407ms step_avg:94.51ms
step:301/1770 train_time:27503ms step_avg:94.51ms
step:302/1770 train_time:27598ms step_avg:94.51ms
step:303/1770 train_time:27693ms step_avg:94.51ms
step:304/1770 train_time:27787ms step_avg:94.52ms
step:305/1770 train_time:27886ms step_avg:94.53ms
step:306/1770 train_time:27977ms step_avg:94.52ms
step:307/1770 train_time:28073ms step_avg:94.52ms
step:308/1770 train_time:28168ms step_avg:94.52ms
step:309/1770 train_time:28263ms step_avg:94.53ms
step:310/1770 train_time:28358ms step_avg:94.53ms
step:311/1770 train_time:28454ms step_avg:94.53ms
step:312/1770 train_time:28549ms step_avg:94.53ms
step:313/1770 train_time:28645ms step_avg:94.54ms
step:314/1770 train_time:28740ms step_avg:94.54ms
step:315/1770 train_time:28835ms step_avg:94.54ms
step:316/1770 train_time:28930ms step_avg:94.54ms
step:317/1770 train_time:29025ms step_avg:94.55ms
step:318/1770 train_time:29121ms step_avg:94.55ms
step:319/1770 train_time:29216ms step_avg:94.55ms
step:320/1770 train_time:29312ms step_avg:94.55ms
step:321/1770 train_time:29407ms step_avg:94.56ms
step:322/1770 train_time:29501ms step_avg:94.56ms
step:323/1770 train_time:29599ms step_avg:94.57ms
step:324/1770 train_time:29692ms step_avg:94.56ms
step:325/1770 train_time:29787ms step_avg:94.56ms
step:326/1770 train_time:29881ms step_avg:94.56ms
step:327/1770 train_time:29977ms step_avg:94.56ms
step:328/1770 train_time:30072ms step_avg:94.57ms
step:329/1770 train_time:30167ms step_avg:94.57ms
step:330/1770 train_time:30262ms step_avg:94.57ms
step:331/1770 train_time:30358ms step_avg:94.57ms
step:332/1770 train_time:30454ms step_avg:94.58ms
step:333/1770 train_time:30549ms step_avg:94.58ms
step:334/1770 train_time:30644ms step_avg:94.58ms
step:335/1770 train_time:30739ms step_avg:94.58ms
step:336/1770 train_time:30834ms step_avg:94.58ms
step:337/1770 train_time:30930ms step_avg:94.59ms
step:338/1770 train_time:31026ms step_avg:94.59ms
step:339/1770 train_time:31120ms step_avg:94.59ms
step:340/1770 train_time:31216ms step_avg:94.59ms
step:341/1770 train_time:31312ms step_avg:94.60ms
step:342/1770 train_time:31406ms step_avg:94.60ms
step:343/1770 train_time:31501ms step_avg:94.60ms
step:344/1770 train_time:31596ms step_avg:94.60ms
step:345/1770 train_time:31691ms step_avg:94.60ms
step:346/1770 train_time:31786ms step_avg:94.60ms
step:347/1770 train_time:31880ms step_avg:94.60ms
step:348/1770 train_time:31976ms step_avg:94.60ms
step:349/1770 train_time:32071ms step_avg:94.61ms
step:350/1770 train_time:32166ms step_avg:94.61ms
step:351/1770 train_time:32261ms step_avg:94.61ms
step:352/1770 train_time:32357ms step_avg:94.61ms
step:353/1770 train_time:32453ms step_avg:94.62ms
step:354/1770 train_time:32549ms step_avg:94.62ms
step:355/1770 train_time:32643ms step_avg:94.62ms
step:356/1770 train_time:32738ms step_avg:94.62ms
step:357/1770 train_time:32833ms step_avg:94.62ms
step:358/1770 train_time:32928ms step_avg:94.62ms
step:359/1770 train_time:33023ms step_avg:94.62ms
step:360/1770 train_time:33118ms step_avg:94.62ms
step:361/1770 train_time:33214ms step_avg:94.63ms
step:362/1770 train_time:33309ms step_avg:94.63ms
step:363/1770 train_time:33404ms step_avg:94.63ms
step:364/1770 train_time:33499ms step_avg:94.63ms
step:365/1770 train_time:33595ms step_avg:94.63ms
step:366/1770 train_time:33690ms step_avg:94.64ms
step:367/1770 train_time:33785ms step_avg:94.64ms
step:368/1770 train_time:33880ms step_avg:94.64ms
step:369/1770 train_time:33975ms step_avg:94.64ms
step:370/1770 train_time:34071ms step_avg:94.64ms
step:371/1770 train_time:34165ms step_avg:94.64ms
step:372/1770 train_time:34260ms step_avg:94.64ms
step:373/1770 train_time:34355ms step_avg:94.64ms
step:374/1770 train_time:34451ms step_avg:94.64ms
step:375/1770 train_time:34546ms step_avg:94.65ms
step:375/1770 val_loss:3.9019 train_time:34639ms step_avg:94.90ms
step:376/1770 train_time:34661ms step_avg:94.70ms
step:377/1770 train_time:34746ms step_avg:94.67ms
step:378/1770 train_time:34842ms step_avg:94.68ms
step:379/1770 train_time:34938ms step_avg:94.68ms
step:380/1770 train_time:35033ms step_avg:94.68ms
step:381/1770 train_time:35127ms step_avg:94.68ms
step:382/1770 train_time:35222ms step_avg:94.68ms
step:383/1770 train_time:35317ms step_avg:94.68ms
step:384/1770 train_time:35411ms step_avg:94.68ms
step:385/1770 train_time:35506ms step_avg:94.68ms
step:386/1770 train_time:35601ms step_avg:94.68ms
step:387/1770 train_time:35696ms step_avg:94.68ms
step:388/1770 train_time:35791ms step_avg:94.69ms
step:389/1770 train_time:35888ms step_avg:94.69ms
step:390/1770 train_time:35983ms step_avg:94.69ms
step:391/1770 train_time:36078ms step_avg:94.69ms
step:392/1770 train_time:36172ms step_avg:94.69ms
step:393/1770 train_time:36267ms step_avg:94.69ms
step:394/1770 train_time:36362ms step_avg:94.69ms
step:395/1770 train_time:36457ms step_avg:94.69ms
step:396/1770 train_time:36554ms step_avg:94.70ms
step:397/1770 train_time:36650ms step_avg:94.70ms
step:398/1770 train_time:36748ms step_avg:94.71ms
step:399/1770 train_time:36846ms step_avg:94.72ms
step:400/1770 train_time:36945ms step_avg:94.73ms
step:401/1770 train_time:37042ms step_avg:94.74ms
step:402/1770 train_time:37140ms step_avg:94.74ms
step:403/1770 train_time:37237ms step_avg:94.75ms
step:404/1770 train_time:37334ms step_avg:94.76ms
step:405/1770 train_time:37430ms step_avg:94.76ms
step:406/1770 train_time:37527ms step_avg:94.77ms
step:407/1770 train_time:37624ms step_avg:94.77ms
step:408/1770 train_time:37722ms step_avg:94.78ms
step:409/1770 train_time:37817ms step_avg:94.78ms
step:410/1770 train_time:37914ms step_avg:94.78ms
step:411/1770 train_time:38011ms step_avg:94.79ms
step:412/1770 train_time:38108ms step_avg:94.80ms
step:413/1770 train_time:38206ms step_avg:94.80ms
step:414/1770 train_time:38303ms step_avg:94.81ms
step:415/1770 train_time:38399ms step_avg:94.81ms
step:416/1770 train_time:38495ms step_avg:94.82ms
step:417/1770 train_time:38592ms step_avg:94.82ms
step:418/1770 train_time:38688ms step_avg:94.82ms
step:419/1770 train_time:38786ms step_avg:94.83ms
step:420/1770 train_time:38883ms step_avg:94.84ms
step:421/1770 train_time:38979ms step_avg:94.84ms
step:422/1770 train_time:39076ms step_avg:94.84ms
step:423/1770 train_time:39172ms step_avg:94.85ms
step:424/1770 train_time:39270ms step_avg:94.85ms
step:425/1770 train_time:39368ms step_avg:94.86ms
step:426/1770 train_time:39464ms step_avg:94.87ms
step:427/1770 train_time:39561ms step_avg:94.87ms
step:428/1770 train_time:39657ms step_avg:94.87ms
step:429/1770 train_time:39753ms step_avg:94.88ms
step:430/1770 train_time:39850ms step_avg:94.88ms
step:431/1770 train_time:39948ms step_avg:94.89ms
step:432/1770 train_time:40045ms step_avg:94.89ms
step:433/1770 train_time:40142ms step_avg:94.90ms
step:434/1770 train_time:40239ms step_avg:94.90ms
step:435/1770 train_time:40336ms step_avg:94.91ms
step:436/1770 train_time:40433ms step_avg:94.91ms
step:437/1770 train_time:40531ms step_avg:94.92ms
step:438/1770 train_time:40628ms step_avg:94.93ms
step:439/1770 train_time:40726ms step_avg:94.93ms
step:440/1770 train_time:40823ms step_avg:94.94ms
step:441/1770 train_time:40919ms step_avg:94.94ms
step:442/1770 train_time:41016ms step_avg:94.94ms
step:443/1770 train_time:41112ms step_avg:94.95ms
step:444/1770 train_time:41210ms step_avg:94.95ms
step:445/1770 train_time:41307ms step_avg:94.96ms
step:446/1770 train_time:41405ms step_avg:94.96ms
step:447/1770 train_time:41501ms step_avg:94.97ms
step:448/1770 train_time:41598ms step_avg:94.97ms
step:449/1770 train_time:41694ms step_avg:94.98ms
step:450/1770 train_time:41792ms step_avg:94.98ms
step:451/1770 train_time:41889ms step_avg:94.99ms
step:452/1770 train_time:41986ms step_avg:94.99ms
step:453/1770 train_time:42083ms step_avg:95.00ms
step:454/1770 train_time:42181ms step_avg:95.00ms
step:455/1770 train_time:42278ms step_avg:95.01ms
step:456/1770 train_time:42374ms step_avg:95.01ms
step:457/1770 train_time:42471ms step_avg:95.01ms
step:458/1770 train_time:42569ms step_avg:95.02ms
step:459/1770 train_time:42666ms step_avg:95.02ms
step:460/1770 train_time:42763ms step_avg:95.03ms
step:461/1770 train_time:42859ms step_avg:95.03ms
step:462/1770 train_time:42956ms step_avg:95.04ms
step:463/1770 train_time:43053ms step_avg:95.04ms
step:464/1770 train_time:43150ms step_avg:95.04ms
step:465/1770 train_time:43247ms step_avg:95.05ms
step:466/1770 train_time:43345ms step_avg:95.05ms
step:467/1770 train_time:43441ms step_avg:95.06ms
step:468/1770 train_time:43538ms step_avg:95.06ms
step:469/1770 train_time:43635ms step_avg:95.07ms
step:470/1770 train_time:43732ms step_avg:95.07ms
step:471/1770 train_time:43829ms step_avg:95.07ms
step:472/1770 train_time:43927ms step_avg:95.08ms
step:473/1770 train_time:44023ms step_avg:95.08ms
step:474/1770 train_time:44120ms step_avg:95.09ms
step:475/1770 train_time:44217ms step_avg:95.09ms
step:476/1770 train_time:44313ms step_avg:95.09ms
step:477/1770 train_time:44411ms step_avg:95.10ms
step:478/1770 train_time:44508ms step_avg:95.10ms
step:479/1770 train_time:44606ms step_avg:95.11ms
step:480/1770 train_time:44703ms step_avg:95.11ms
step:481/1770 train_time:44800ms step_avg:95.12ms
step:482/1770 train_time:44897ms step_avg:95.12ms
step:483/1770 train_time:44993ms step_avg:95.12ms
step:484/1770 train_time:45090ms step_avg:95.13ms
step:485/1770 train_time:45187ms step_avg:95.13ms
step:486/1770 train_time:45285ms step_avg:95.14ms
step:487/1770 train_time:45381ms step_avg:95.14ms
step:488/1770 train_time:45478ms step_avg:95.14ms
step:489/1770 train_time:45574ms step_avg:95.14ms
step:490/1770 train_time:45671ms step_avg:95.15ms
step:491/1770 train_time:45769ms step_avg:95.15ms
step:492/1770 train_time:45866ms step_avg:95.16ms
step:493/1770 train_time:45963ms step_avg:95.16ms
step:494/1770 train_time:46060ms step_avg:95.17ms
step:495/1770 train_time:46157ms step_avg:95.17ms
step:496/1770 train_time:46253ms step_avg:95.17ms
step:497/1770 train_time:46350ms step_avg:95.17ms
step:498/1770 train_time:46447ms step_avg:95.18ms
step:499/1770 train_time:46545ms step_avg:95.18ms
step:500/1770 train_time:46642ms step_avg:95.19ms
step:500/1770 val_loss:3.7513 train_time:46737ms step_avg:95.38ms
step:501/1770 train_time:46758ms step_avg:95.23ms
step:502/1770 train_time:46842ms step_avg:95.21ms
step:503/1770 train_time:46941ms step_avg:95.21ms
step:504/1770 train_time:47038ms step_avg:95.22ms
step:505/1770 train_time:47135ms step_avg:95.22ms
step:506/1770 train_time:47231ms step_avg:95.22ms
step:507/1770 train_time:47327ms step_avg:95.23ms
step:508/1770 train_time:47424ms step_avg:95.23ms
step:509/1770 train_time:47520ms step_avg:95.23ms
step:510/1770 train_time:47616ms step_avg:95.23ms
step:511/1770 train_time:47715ms step_avg:95.24ms
step:512/1770 train_time:47812ms step_avg:95.24ms
step:513/1770 train_time:47909ms step_avg:95.25ms
step:514/1770 train_time:48006ms step_avg:95.25ms
step:515/1770 train_time:48103ms step_avg:95.25ms
step:516/1770 train_time:48200ms step_avg:95.26ms
step:517/1770 train_time:48302ms step_avg:95.27ms
step:518/1770 train_time:48394ms step_avg:95.26ms
step:519/1770 train_time:48490ms step_avg:95.26ms
step:520/1770 train_time:48586ms step_avg:95.27ms
step:521/1770 train_time:48683ms step_avg:95.27ms
step:522/1770 train_time:48780ms step_avg:95.27ms
step:523/1770 train_time:48878ms step_avg:95.28ms
step:524/1770 train_time:48974ms step_avg:95.28ms
step:525/1770 train_time:49071ms step_avg:95.28ms
step:526/1770 train_time:49168ms step_avg:95.29ms
step:527/1770 train_time:49265ms step_avg:95.29ms
step:528/1770 train_time:49362ms step_avg:95.29ms
step:529/1770 train_time:49460ms step_avg:95.30ms
step:530/1770 train_time:49558ms step_avg:95.30ms
step:531/1770 train_time:49655ms step_avg:95.31ms
step:532/1770 train_time:49752ms step_avg:95.31ms
step:533/1770 train_time:49850ms step_avg:95.31ms
step:534/1770 train_time:49946ms step_avg:95.32ms
step:535/1770 train_time:50044ms step_avg:95.32ms
step:536/1770 train_time:50141ms step_avg:95.33ms
step:537/1770 train_time:50238ms step_avg:95.33ms
step:538/1770 train_time:50336ms step_avg:95.33ms
step:539/1770 train_time:50434ms step_avg:95.34ms
step:540/1770 train_time:50531ms step_avg:95.34ms
step:541/1770 train_time:50628ms step_avg:95.34ms
step:542/1770 train_time:50725ms step_avg:95.35ms
step:543/1770 train_time:50823ms step_avg:95.35ms
step:544/1770 train_time:50920ms step_avg:95.36ms
step:545/1770 train_time:51018ms step_avg:95.36ms
step:546/1770 train_time:51116ms step_avg:95.37ms
step:547/1770 train_time:51213ms step_avg:95.37ms
step:548/1770 train_time:51310ms step_avg:95.37ms
step:549/1770 train_time:51406ms step_avg:95.37ms
step:550/1770 train_time:51504ms step_avg:95.38ms
step:551/1770 train_time:51601ms step_avg:95.38ms
step:552/1770 train_time:51699ms step_avg:95.39ms
step:553/1770 train_time:51797ms step_avg:95.39ms
step:554/1770 train_time:51894ms step_avg:95.39ms
step:555/1770 train_time:51991ms step_avg:95.40ms
step:556/1770 train_time:52088ms step_avg:95.40ms
step:557/1770 train_time:52186ms step_avg:95.40ms
step:558/1770 train_time:52283ms step_avg:95.41ms
step:559/1770 train_time:52381ms step_avg:95.41ms
step:560/1770 train_time:52479ms step_avg:95.42ms
step:561/1770 train_time:52576ms step_avg:95.42ms
step:562/1770 train_time:52674ms step_avg:95.42ms
step:563/1770 train_time:52771ms step_avg:95.43ms
step:564/1770 train_time:52868ms step_avg:95.43ms
step:565/1770 train_time:52965ms step_avg:95.43ms
step:566/1770 train_time:53062ms step_avg:95.44ms
step:567/1770 train_time:53160ms step_avg:95.44ms
step:568/1770 train_time:53258ms step_avg:95.44ms
step:569/1770 train_time:53356ms step_avg:95.45ms
step:570/1770 train_time:53453ms step_avg:95.45ms
step:571/1770 train_time:53550ms step_avg:95.45ms
step:572/1770 train_time:53647ms step_avg:95.46ms
step:573/1770 train_time:53744ms step_avg:95.46ms
step:574/1770 train_time:53842ms step_avg:95.46ms
step:575/1770 train_time:53939ms step_avg:95.47ms
step:576/1770 train_time:54037ms step_avg:95.47ms
step:577/1770 train_time:54134ms step_avg:95.48ms
step:578/1770 train_time:54232ms step_avg:95.48ms
step:579/1770 train_time:54329ms step_avg:95.48ms
step:580/1770 train_time:54426ms step_avg:95.48ms
step:581/1770 train_time:54523ms step_avg:95.49ms
step:582/1770 train_time:54621ms step_avg:95.49ms
step:583/1770 train_time:54718ms step_avg:95.49ms
step:584/1770 train_time:54816ms step_avg:95.50ms
step:585/1770 train_time:54913ms step_avg:95.50ms
step:586/1770 train_time:55010ms step_avg:95.50ms
step:587/1770 train_time:55111ms step_avg:95.51ms
step:588/1770 train_time:55204ms step_avg:95.51ms
step:589/1770 train_time:55302ms step_avg:95.51ms
step:590/1770 train_time:55400ms step_avg:95.52ms
step:591/1770 train_time:55498ms step_avg:95.52ms
step:592/1770 train_time:55595ms step_avg:95.52ms
step:593/1770 train_time:55692ms step_avg:95.53ms
step:594/1770 train_time:55789ms step_avg:95.53ms
step:595/1770 train_time:55886ms step_avg:95.53ms
step:596/1770 train_time:55983ms step_avg:95.53ms
step:597/1770 train_time:56081ms step_avg:95.54ms
step:598/1770 train_time:56179ms step_avg:95.54ms
step:599/1770 train_time:56276ms step_avg:95.55ms
step:600/1770 train_time:56374ms step_avg:95.55ms
step:601/1770 train_time:56471ms step_avg:95.55ms
step:602/1770 train_time:56567ms step_avg:95.55ms
step:603/1770 train_time:56664ms step_avg:95.56ms
step:604/1770 train_time:56762ms step_avg:95.56ms
step:605/1770 train_time:56860ms step_avg:95.56ms
step:606/1770 train_time:56957ms step_avg:95.57ms
step:607/1770 train_time:57055ms step_avg:95.57ms
step:608/1770 train_time:57152ms step_avg:95.57ms
step:609/1770 train_time:57249ms step_avg:95.57ms
step:610/1770 train_time:57346ms step_avg:95.58ms
step:611/1770 train_time:57443ms step_avg:95.58ms
step:612/1770 train_time:57541ms step_avg:95.58ms
step:613/1770 train_time:57638ms step_avg:95.59ms
step:614/1770 train_time:57736ms step_avg:95.59ms
step:615/1770 train_time:57833ms step_avg:95.59ms
step:616/1770 train_time:57931ms step_avg:95.60ms
step:617/1770 train_time:58028ms step_avg:95.60ms
step:618/1770 train_time:58125ms step_avg:95.60ms
step:619/1770 train_time:58222ms step_avg:95.60ms
step:620/1770 train_time:58321ms step_avg:95.61ms
step:621/1770 train_time:58418ms step_avg:95.61ms
step:622/1770 train_time:58516ms step_avg:95.61ms
step:623/1770 train_time:58613ms step_avg:95.62ms
step:624/1770 train_time:58710ms step_avg:95.62ms
step:625/1770 train_time:58807ms step_avg:95.62ms
step:625/1770 val_loss:3.6654 train_time:58902ms step_avg:95.78ms
step:626/1770 train_time:58925ms step_avg:95.66ms
step:627/1770 train_time:59007ms step_avg:95.64ms
step:628/1770 train_time:59107ms step_avg:95.64ms
step:629/1770 train_time:59202ms step_avg:95.64ms
step:630/1770 train_time:59299ms step_avg:95.64ms
step:631/1770 train_time:59396ms step_avg:95.65ms
step:632/1770 train_time:59493ms step_avg:95.65ms
step:633/1770 train_time:59591ms step_avg:95.65ms
step:634/1770 train_time:59688ms step_avg:95.65ms
step:635/1770 train_time:59784ms step_avg:95.66ms
step:636/1770 train_time:59882ms step_avg:95.66ms
step:637/1770 train_time:59979ms step_avg:95.66ms
step:638/1770 train_time:60077ms step_avg:95.66ms
step:639/1770 train_time:60174ms step_avg:95.67ms
step:640/1770 train_time:60272ms step_avg:95.67ms
step:641/1770 train_time:60370ms step_avg:95.67ms
step:642/1770 train_time:60467ms step_avg:95.68ms
step:643/1770 train_time:60564ms step_avg:95.68ms
step:644/1770 train_time:60661ms step_avg:95.68ms
step:645/1770 train_time:60758ms step_avg:95.68ms
step:646/1770 train_time:60855ms step_avg:95.68ms
step:647/1770 train_time:60953ms step_avg:95.69ms
step:648/1770 train_time:61051ms step_avg:95.69ms
step:649/1770 train_time:61149ms step_avg:95.69ms
step:650/1770 train_time:61246ms step_avg:95.70ms
step:651/1770 train_time:61343ms step_avg:95.70ms
step:652/1770 train_time:61440ms step_avg:95.70ms
step:653/1770 train_time:61537ms step_avg:95.70ms
step:654/1770 train_time:61634ms step_avg:95.71ms
step:655/1770 train_time:61732ms step_avg:95.71ms
step:656/1770 train_time:61829ms step_avg:95.71ms
step:657/1770 train_time:61928ms step_avg:95.72ms
step:658/1770 train_time:62027ms step_avg:95.72ms
step:659/1770 train_time:62127ms step_avg:95.73ms
step:660/1770 train_time:62224ms step_avg:95.73ms
step:661/1770 train_time:62323ms step_avg:95.73ms
step:662/1770 train_time:62422ms step_avg:95.74ms
step:663/1770 train_time:62521ms step_avg:95.74ms
step:664/1770 train_time:62619ms step_avg:95.75ms
step:665/1770 train_time:62719ms step_avg:95.75ms
step:666/1770 train_time:62817ms step_avg:95.76ms
step:667/1770 train_time:62917ms step_avg:95.76ms
step:668/1770 train_time:63016ms step_avg:95.77ms
step:669/1770 train_time:63114ms step_avg:95.77ms
step:670/1770 train_time:63213ms step_avg:95.78ms
step:671/1770 train_time:63313ms step_avg:95.78ms
step:672/1770 train_time:63413ms step_avg:95.79ms
step:673/1770 train_time:63513ms step_avg:95.80ms
step:674/1770 train_time:63612ms step_avg:95.80ms
step:675/1770 train_time:63712ms step_avg:95.81ms
step:676/1770 train_time:63811ms step_avg:95.81ms
step:677/1770 train_time:63909ms step_avg:95.82ms
step:678/1770 train_time:64009ms step_avg:95.82ms
step:679/1770 train_time:64108ms step_avg:95.83ms
step:680/1770 train_time:64207ms step_avg:95.83ms
step:681/1770 train_time:64305ms step_avg:95.84ms
step:682/1770 train_time:64404ms step_avg:95.84ms
step:683/1770 train_time:64502ms step_avg:95.84ms
step:684/1770 train_time:64601ms step_avg:95.85ms
step:685/1770 train_time:64699ms step_avg:95.85ms
step:686/1770 train_time:64798ms step_avg:95.86ms
step:687/1770 train_time:64903ms step_avg:95.87ms
step:688/1770 train_time:64997ms step_avg:95.87ms
step:689/1770 train_time:65096ms step_avg:95.87ms
step:690/1770 train_time:65195ms step_avg:95.87ms
step:691/1770 train_time:65294ms step_avg:95.88ms
step:692/1770 train_time:65393ms step_avg:95.88ms
step:693/1770 train_time:65493ms step_avg:95.89ms
step:694/1770 train_time:65593ms step_avg:95.90ms
step:695/1770 train_time:65692ms step_avg:95.90ms
step:696/1770 train_time:65791ms step_avg:95.91ms
step:697/1770 train_time:65891ms step_avg:95.91ms
step:698/1770 train_time:65992ms step_avg:95.92ms
step:699/1770 train_time:66092ms step_avg:95.92ms
step:700/1770 train_time:66192ms step_avg:95.93ms
step:701/1770 train_time:66292ms step_avg:95.94ms
step:702/1770 train_time:66392ms step_avg:95.94ms
step:703/1770 train_time:66491ms step_avg:95.95ms
step:704/1770 train_time:66590ms step_avg:95.95ms
step:705/1770 train_time:66689ms step_avg:95.96ms
step:706/1770 train_time:66788ms step_avg:95.96ms
step:707/1770 train_time:66886ms step_avg:95.96ms
step:708/1770 train_time:66985ms step_avg:95.97ms
step:709/1770 train_time:67084ms step_avg:95.97ms
step:710/1770 train_time:67183ms step_avg:95.98ms
step:711/1770 train_time:67281ms step_avg:95.98ms
step:712/1770 train_time:67380ms step_avg:95.98ms
step:713/1770 train_time:67479ms step_avg:95.99ms
step:714/1770 train_time:67577ms step_avg:95.99ms
step:715/1770 train_time:67677ms step_avg:96.00ms
step:716/1770 train_time:67775ms step_avg:96.00ms
step:717/1770 train_time:67874ms step_avg:96.00ms
step:718/1770 train_time:67973ms step_avg:96.01ms
step:719/1770 train_time:68072ms step_avg:96.01ms
step:720/1770 train_time:68172ms step_avg:96.02ms
step:721/1770 train_time:68271ms step_avg:96.02ms
step:722/1770 train_time:68371ms step_avg:96.03ms
step:723/1770 train_time:68471ms step_avg:96.03ms
step:724/1770 train_time:68571ms step_avg:96.04ms
step:725/1770 train_time:68670ms step_avg:96.04ms
step:726/1770 train_time:68770ms step_avg:96.05ms
step:727/1770 train_time:68869ms step_avg:96.05ms
step:728/1770 train_time:68968ms step_avg:96.06ms
step:729/1770 train_time:69068ms step_avg:96.06ms
step:730/1770 train_time:69167ms step_avg:96.06ms
step:731/1770 train_time:69265ms step_avg:96.07ms
step:732/1770 train_time:69364ms step_avg:96.07ms
step:733/1770 train_time:69463ms step_avg:96.08ms
step:734/1770 train_time:69562ms step_avg:96.08ms
step:735/1770 train_time:69661ms step_avg:96.08ms
step:736/1770 train_time:69759ms step_avg:96.09ms
step:737/1770 train_time:69858ms step_avg:96.09ms
step:738/1770 train_time:69956ms step_avg:96.09ms
step:739/1770 train_time:70056ms step_avg:96.10ms
step:740/1770 train_time:70155ms step_avg:96.10ms
step:741/1770 train_time:70254ms step_avg:96.11ms
step:742/1770 train_time:70354ms step_avg:96.11ms
step:743/1770 train_time:70454ms step_avg:96.12ms
step:744/1770 train_time:70553ms step_avg:96.12ms
step:745/1770 train_time:70653ms step_avg:96.13ms
step:746/1770 train_time:70753ms step_avg:96.13ms
step:747/1770 train_time:70853ms step_avg:96.14ms
step:748/1770 train_time:70953ms step_avg:96.14ms
step:749/1770 train_time:71052ms step_avg:96.15ms
step:750/1770 train_time:71152ms step_avg:96.15ms
step:750/1770 val_loss:3.6018 train_time:71249ms step_avg:96.28ms
step:751/1770 train_time:71271ms step_avg:96.18ms
step:752/1770 train_time:71358ms step_avg:96.17ms
step:753/1770 train_time:71457ms step_avg:96.17ms
step:754/1770 train_time:71556ms step_avg:96.18ms
step:755/1770 train_time:71655ms step_avg:96.18ms
step:756/1770 train_time:71753ms step_avg:96.18ms
step:757/1770 train_time:71852ms step_avg:96.19ms
step:758/1770 train_time:71952ms step_avg:96.19ms
step:759/1770 train_time:72051ms step_avg:96.20ms
step:760/1770 train_time:72150ms step_avg:96.20ms
step:761/1770 train_time:72250ms step_avg:96.20ms
step:762/1770 train_time:72350ms step_avg:96.21ms
step:763/1770 train_time:72451ms step_avg:96.22ms
step:764/1770 train_time:72550ms step_avg:96.22ms
step:765/1770 train_time:72649ms step_avg:96.22ms
step:766/1770 train_time:72748ms step_avg:96.23ms
step:767/1770 train_time:72847ms step_avg:96.23ms
step:768/1770 train_time:72945ms step_avg:96.23ms
step:769/1770 train_time:73044ms step_avg:96.24ms
step:770/1770 train_time:73143ms step_avg:96.24ms
step:771/1770 train_time:73241ms step_avg:96.24ms
step:772/1770 train_time:73339ms step_avg:96.25ms
step:773/1770 train_time:73438ms step_avg:96.25ms
step:774/1770 train_time:73538ms step_avg:96.25ms
step:775/1770 train_time:73637ms step_avg:96.26ms
step:776/1770 train_time:73736ms step_avg:96.26ms
step:777/1770 train_time:73835ms step_avg:96.26ms
step:778/1770 train_time:73935ms step_avg:96.27ms
step:779/1770 train_time:74035ms step_avg:96.27ms
step:780/1770 train_time:74135ms step_avg:96.28ms
step:781/1770 train_time:74234ms step_avg:96.28ms
step:782/1770 train_time:74335ms step_avg:96.29ms
step:783/1770 train_time:74434ms step_avg:96.29ms
step:784/1770 train_time:74535ms step_avg:96.30ms
step:785/1770 train_time:74634ms step_avg:96.30ms
step:786/1770 train_time:74734ms step_avg:96.31ms
step:787/1770 train_time:74833ms step_avg:96.31ms
step:788/1770 train_time:74934ms step_avg:96.32ms
step:789/1770 train_time:75034ms step_avg:96.32ms
step:790/1770 train_time:75134ms step_avg:96.33ms
step:791/1770 train_time:75234ms step_avg:96.33ms
step:792/1770 train_time:75334ms step_avg:96.33ms
step:793/1770 train_time:75434ms step_avg:96.34ms
step:794/1770 train_time:75534ms step_avg:96.34ms
step:795/1770 train_time:75633ms step_avg:96.35ms
step:796/1770 train_time:75733ms step_avg:96.35ms
step:797/1770 train_time:75832ms step_avg:96.36ms
step:798/1770 train_time:75933ms step_avg:96.36ms
step:799/1770 train_time:76032ms step_avg:96.37ms
step:800/1770 train_time:76132ms step_avg:96.37ms
step:801/1770 train_time:76231ms step_avg:96.37ms
step:802/1770 train_time:76331ms step_avg:96.38ms
step:803/1770 train_time:76431ms step_avg:96.38ms
step:804/1770 train_time:76530ms step_avg:96.39ms
step:805/1770 train_time:76630ms step_avg:96.39ms
step:806/1770 train_time:76730ms step_avg:96.39ms
step:807/1770 train_time:76829ms step_avg:96.40ms
step:808/1770 train_time:76928ms step_avg:96.40ms
step:809/1770 train_time:77028ms step_avg:96.41ms
step:810/1770 train_time:77128ms step_avg:96.41ms
step:811/1770 train_time:77227ms step_avg:96.41ms
step:812/1770 train_time:77326ms step_avg:96.42ms
step:813/1770 train_time:77425ms step_avg:96.42ms
step:814/1770 train_time:77524ms step_avg:96.42ms
step:815/1770 train_time:77623ms step_avg:96.43ms
step:816/1770 train_time:77722ms step_avg:96.43ms
step:817/1770 train_time:77822ms step_avg:96.43ms
step:818/1770 train_time:77922ms step_avg:96.44ms
step:819/1770 train_time:78021ms step_avg:96.44ms
step:820/1770 train_time:78120ms step_avg:96.45ms
step:821/1770 train_time:78220ms step_avg:96.45ms
step:822/1770 train_time:78319ms step_avg:96.45ms
step:823/1770 train_time:78418ms step_avg:96.46ms
step:824/1770 train_time:78517ms step_avg:96.46ms
step:825/1770 train_time:78616ms step_avg:96.46ms
step:826/1770 train_time:78715ms step_avg:96.46ms
step:827/1770 train_time:78815ms step_avg:96.47ms
step:828/1770 train_time:78915ms step_avg:96.47ms
step:829/1770 train_time:79015ms step_avg:96.48ms
step:830/1770 train_time:79116ms step_avg:96.48ms
step:831/1770 train_time:79216ms step_avg:96.49ms
step:832/1770 train_time:79315ms step_avg:96.49ms
step:833/1770 train_time:79415ms step_avg:96.49ms
step:834/1770 train_time:79514ms step_avg:96.50ms
step:835/1770 train_time:79614ms step_avg:96.50ms
step:836/1770 train_time:79714ms step_avg:96.51ms
step:837/1770 train_time:79814ms step_avg:96.51ms
step:838/1770 train_time:79914ms step_avg:96.51ms
step:839/1770 train_time:80014ms step_avg:96.52ms
step:840/1770 train_time:80115ms step_avg:96.52ms
step:841/1770 train_time:80214ms step_avg:96.53ms
step:842/1770 train_time:80314ms step_avg:96.53ms
step:843/1770 train_time:80414ms step_avg:96.54ms
step:844/1770 train_time:80514ms step_avg:96.54ms
step:845/1770 train_time:80613ms step_avg:96.54ms
step:846/1770 train_time:80713ms step_avg:96.55ms
step:847/1770 train_time:80813ms step_avg:96.55ms
step:848/1770 train_time:80913ms step_avg:96.55ms
step:849/1770 train_time:81013ms step_avg:96.56ms
step:850/1770 train_time:81113ms step_avg:96.56ms
step:851/1770 train_time:81214ms step_avg:96.57ms
step:852/1770 train_time:81313ms step_avg:96.57ms
step:853/1770 train_time:81413ms step_avg:96.58ms
step:854/1770 train_time:81513ms step_avg:96.58ms
step:855/1770 train_time:81612ms step_avg:96.58ms
step:856/1770 train_time:81712ms step_avg:96.59ms
step:857/1770 train_time:81812ms step_avg:96.59ms
step:858/1770 train_time:81912ms step_avg:96.59ms
step:859/1770 train_time:82012ms step_avg:96.60ms
step:860/1770 train_time:82112ms step_avg:96.60ms
step:861/1770 train_time:82212ms step_avg:96.61ms
step:862/1770 train_time:82312ms step_avg:96.61ms
step:863/1770 train_time:82413ms step_avg:96.61ms
step:864/1770 train_time:82513ms step_avg:96.62ms
step:865/1770 train_time:82612ms step_avg:96.62ms
step:866/1770 train_time:82712ms step_avg:96.63ms
step:867/1770 train_time:82812ms step_avg:96.63ms
step:868/1770 train_time:82912ms step_avg:96.63ms
step:869/1770 train_time:83012ms step_avg:96.64ms
step:870/1770 train_time:83112ms step_avg:96.64ms
step:871/1770 train_time:83212ms step_avg:96.65ms
step:872/1770 train_time:83313ms step_avg:96.65ms
step:873/1770 train_time:83412ms step_avg:96.65ms
step:874/1770 train_time:83512ms step_avg:96.66ms
step:875/1770 train_time:83611ms step_avg:96.66ms
step:875/1770 val_loss:3.5520 train_time:83709ms step_avg:96.77ms
step:876/1770 train_time:83731ms step_avg:96.69ms
step:877/1770 train_time:83817ms step_avg:96.68ms
step:878/1770 train_time:83917ms step_avg:96.68ms
step:879/1770 train_time:84017ms step_avg:96.68ms
step:880/1770 train_time:84115ms step_avg:96.68ms
step:881/1770 train_time:84214ms step_avg:96.69ms
step:882/1770 train_time:84314ms step_avg:96.69ms
step:883/1770 train_time:84413ms step_avg:96.69ms
step:884/1770 train_time:84513ms step_avg:96.70ms
step:885/1770 train_time:84613ms step_avg:96.70ms
step:886/1770 train_time:84714ms step_avg:96.71ms
step:887/1770 train_time:84816ms step_avg:96.71ms
step:888/1770 train_time:84917ms step_avg:96.72ms
step:889/1770 train_time:85018ms step_avg:96.72ms
step:890/1770 train_time:85117ms step_avg:96.72ms
step:891/1770 train_time:85216ms step_avg:96.73ms
step:892/1770 train_time:85315ms step_avg:96.73ms
step:893/1770 train_time:85414ms step_avg:96.73ms
step:894/1770 train_time:85513ms step_avg:96.73ms
step:895/1770 train_time:85611ms step_avg:96.74ms
step:896/1770 train_time:85710ms step_avg:96.74ms
step:897/1770 train_time:85811ms step_avg:96.74ms
step:898/1770 train_time:85912ms step_avg:96.75ms
step:899/1770 train_time:86012ms step_avg:96.75ms
step:900/1770 train_time:86112ms step_avg:96.76ms
step:901/1770 train_time:86212ms step_avg:96.76ms
step:902/1770 train_time:86313ms step_avg:96.76ms
step:903/1770 train_time:86412ms step_avg:96.77ms
step:904/1770 train_time:86512ms step_avg:96.77ms
step:905/1770 train_time:86611ms step_avg:96.77ms
step:906/1770 train_time:86711ms step_avg:96.78ms
step:907/1770 train_time:86811ms step_avg:96.78ms
step:908/1770 train_time:86912ms step_avg:96.78ms
step:909/1770 train_time:87012ms step_avg:96.79ms
step:910/1770 train_time:87113ms step_avg:96.79ms
step:911/1770 train_time:87213ms step_avg:96.80ms
step:912/1770 train_time:87313ms step_avg:96.80ms
step:913/1770 train_time:87413ms step_avg:96.80ms
step:914/1770 train_time:87513ms step_avg:96.81ms
step:915/1770 train_time:87613ms step_avg:96.81ms
step:916/1770 train_time:87712ms step_avg:96.81ms
step:917/1770 train_time:87812ms step_avg:96.82ms
step:918/1770 train_time:87911ms step_avg:96.82ms
step:919/1770 train_time:88011ms step_avg:96.82ms
step:920/1770 train_time:88113ms step_avg:96.83ms
step:921/1770 train_time:88215ms step_avg:96.83ms
step:922/1770 train_time:88317ms step_avg:96.84ms
step:923/1770 train_time:88417ms step_avg:96.84ms
step:924/1770 train_time:88518ms step_avg:96.85ms
step:925/1770 train_time:88618ms step_avg:96.85ms
step:926/1770 train_time:88718ms step_avg:96.85ms
step:927/1770 train_time:88819ms step_avg:96.86ms
step:928/1770 train_time:88920ms step_avg:96.86ms
step:929/1770 train_time:89021ms step_avg:96.87ms
step:930/1770 train_time:89122ms step_avg:96.87ms
step:931/1770 train_time:89222ms step_avg:96.87ms
step:932/1770 train_time:89322ms step_avg:96.88ms
step:933/1770 train_time:89423ms step_avg:96.88ms
step:934/1770 train_time:89524ms step_avg:96.89ms
step:935/1770 train_time:89625ms step_avg:96.89ms
step:936/1770 train_time:89725ms step_avg:96.90ms
step:937/1770 train_time:89826ms step_avg:96.90ms
step:938/1770 train_time:89926ms step_avg:96.90ms
step:939/1770 train_time:90026ms step_avg:96.91ms
step:940/1770 train_time:90127ms step_avg:96.91ms
step:941/1770 train_time:90226ms step_avg:96.91ms
step:942/1770 train_time:90327ms step_avg:96.92ms
step:943/1770 train_time:90429ms step_avg:96.92ms
step:944/1770 train_time:90530ms step_avg:96.93ms
step:945/1770 train_time:90630ms step_avg:96.93ms
step:946/1770 train_time:90732ms step_avg:96.94ms
step:947/1770 train_time:90835ms step_avg:96.94ms
step:948/1770 train_time:90936ms step_avg:96.95ms
step:949/1770 train_time:91037ms step_avg:96.95ms
step:950/1770 train_time:91139ms step_avg:96.96ms
step:951/1770 train_time:91240ms step_avg:96.96ms
step:952/1770 train_time:91340ms step_avg:96.96ms
step:953/1770 train_time:91441ms step_avg:96.97ms
step:954/1770 train_time:91541ms step_avg:96.97ms
step:955/1770 train_time:91642ms step_avg:96.98ms
step:956/1770 train_time:91743ms step_avg:96.98ms
step:957/1770 train_time:91845ms step_avg:96.99ms
step:958/1770 train_time:91946ms step_avg:96.99ms
step:959/1770 train_time:92047ms step_avg:96.99ms
step:960/1770 train_time:92147ms step_avg:97.00ms
step:961/1770 train_time:92247ms step_avg:97.00ms
step:962/1770 train_time:92348ms step_avg:97.00ms
step:963/1770 train_time:92448ms step_avg:97.01ms
step:964/1770 train_time:92549ms step_avg:97.01ms
step:965/1770 train_time:92650ms step_avg:97.02ms
step:966/1770 train_time:92750ms step_avg:97.02ms
step:967/1770 train_time:92852ms step_avg:97.02ms
step:968/1770 train_time:92953ms step_avg:97.03ms
step:969/1770 train_time:93054ms step_avg:97.03ms
step:970/1770 train_time:93155ms step_avg:97.04ms
step:971/1770 train_time:93257ms step_avg:97.04ms
step:972/1770 train_time:93358ms step_avg:97.05ms
step:973/1770 train_time:93458ms step_avg:97.05ms
step:974/1770 train_time:93559ms step_avg:97.05ms
step:975/1770 train_time:93663ms step_avg:97.06ms
step:976/1770 train_time:93760ms step_avg:97.06ms
step:977/1770 train_time:93861ms step_avg:97.06ms
step:978/1770 train_time:93962ms step_avg:97.07ms
step:979/1770 train_time:94062ms step_avg:97.07ms
step:980/1770 train_time:94162ms step_avg:97.07ms
step:981/1770 train_time:94263ms step_avg:97.08ms
step:982/1770 train_time:94364ms step_avg:97.08ms
step:983/1770 train_time:94464ms step_avg:97.09ms
step:984/1770 train_time:94565ms step_avg:97.09ms
step:985/1770 train_time:94665ms step_avg:97.09ms
step:986/1770 train_time:94764ms step_avg:97.09ms
step:987/1770 train_time:94865ms step_avg:97.10ms
step:988/1770 train_time:94965ms step_avg:97.10ms
step:989/1770 train_time:95066ms step_avg:97.10ms
step:990/1770 train_time:95166ms step_avg:97.11ms
step:991/1770 train_time:95270ms step_avg:97.12ms
step:992/1770 train_time:95367ms step_avg:97.12ms
step:993/1770 train_time:95467ms step_avg:97.12ms
step:994/1770 train_time:95568ms step_avg:97.12ms
step:995/1770 train_time:95669ms step_avg:97.13ms
step:996/1770 train_time:95769ms step_avg:97.13ms
step:997/1770 train_time:95870ms step_avg:97.13ms
step:998/1770 train_time:95971ms step_avg:97.14ms
step:999/1770 train_time:96072ms step_avg:97.14ms
step:1000/1770 train_time:96174ms step_avg:97.15ms
step:1000/1770 val_loss:3.5126 train_time:96274ms step_avg:97.25ms
step:1001/1770 train_time:96295ms step_avg:97.17ms
step:1002/1770 train_time:96384ms step_avg:97.16ms
step:1003/1770 train_time:96487ms step_avg:97.17ms
step:1004/1770 train_time:96588ms step_avg:97.17ms
step:1005/1770 train_time:96687ms step_avg:97.17ms
step:1006/1770 train_time:96787ms step_avg:97.18ms
step:1007/1770 train_time:96887ms step_avg:97.18ms
step:1008/1770 train_time:96987ms step_avg:97.18ms
step:1009/1770 train_time:97087ms step_avg:97.18ms
step:1010/1770 train_time:97187ms step_avg:97.19ms
step:1011/1770 train_time:97289ms step_avg:97.19ms
step:1012/1770 train_time:97391ms step_avg:97.20ms
step:1013/1770 train_time:97492ms step_avg:97.20ms
step:1014/1770 train_time:97594ms step_avg:97.21ms
step:1015/1770 train_time:97696ms step_avg:97.21ms
step:1016/1770 train_time:97796ms step_avg:97.21ms
step:1017/1770 train_time:97896ms step_avg:97.22ms
step:1018/1770 train_time:97997ms step_avg:97.22ms
step:1019/1770 train_time:98099ms step_avg:97.22ms
step:1020/1770 train_time:98200ms step_avg:97.23ms
step:1021/1770 train_time:98302ms step_avg:97.23ms
step:1022/1770 train_time:98402ms step_avg:97.24ms
step:1023/1770 train_time:98503ms step_avg:97.24ms
step:1024/1770 train_time:98604ms step_avg:97.24ms
step:1025/1770 train_time:98704ms step_avg:97.25ms
step:1026/1770 train_time:98805ms step_avg:97.25ms
step:1027/1770 train_time:98907ms step_avg:97.25ms
step:1028/1770 train_time:99008ms step_avg:97.26ms
step:1029/1770 train_time:99108ms step_avg:97.26ms
step:1030/1770 train_time:99208ms step_avg:97.26ms
step:1031/1770 train_time:99309ms step_avg:97.27ms
step:1032/1770 train_time:99410ms step_avg:97.27ms
step:1033/1770 train_time:99511ms step_avg:97.27ms
step:1034/1770 train_time:99612ms step_avg:97.28ms
step:1035/1770 train_time:99714ms step_avg:97.28ms
step:1036/1770 train_time:99815ms step_avg:97.29ms
step:1037/1770 train_time:99916ms step_avg:97.29ms
step:1038/1770 train_time:100018ms step_avg:97.29ms
step:1039/1770 train_time:100119ms step_avg:97.30ms
step:1040/1770 train_time:100220ms step_avg:97.30ms
step:1041/1770 train_time:100321ms step_avg:97.30ms
step:1042/1770 train_time:100421ms step_avg:97.31ms
step:1043/1770 train_time:100522ms step_avg:97.31ms
step:1044/1770 train_time:100623ms step_avg:97.31ms
step:1045/1770 train_time:100724ms step_avg:97.32ms
step:1046/1770 train_time:100824ms step_avg:97.32ms
step:1047/1770 train_time:100924ms step_avg:97.32ms
step:1048/1770 train_time:101025ms step_avg:97.33ms
step:1049/1770 train_time:101125ms step_avg:97.33ms
step:1050/1770 train_time:101226ms step_avg:97.33ms
step:1051/1770 train_time:101330ms step_avg:97.34ms
step:1052/1770 train_time:101427ms step_avg:97.34ms
step:1053/1770 train_time:101527ms step_avg:97.34ms
step:1054/1770 train_time:101628ms step_avg:97.34ms
step:1055/1770 train_time:101728ms step_avg:97.35ms
step:1056/1770 train_time:101829ms step_avg:97.35ms
step:1057/1770 train_time:101930ms step_avg:97.35ms
step:1058/1770 train_time:102031ms step_avg:97.36ms
step:1059/1770 train_time:102132ms step_avg:97.36ms
step:1060/1770 train_time:102235ms step_avg:97.37ms
step:1061/1770 train_time:102336ms step_avg:97.37ms
step:1062/1770 train_time:102439ms step_avg:97.38ms
step:1063/1770 train_time:102541ms step_avg:97.38ms
step:1064/1770 train_time:102643ms step_avg:97.38ms
step:1065/1770 train_time:102743ms step_avg:97.39ms
step:1066/1770 train_time:102844ms step_avg:97.39ms
step:1067/1770 train_time:102945ms step_avg:97.39ms
step:1068/1770 train_time:103046ms step_avg:97.40ms
step:1069/1770 train_time:103148ms step_avg:97.40ms
step:1070/1770 train_time:103249ms step_avg:97.40ms
step:1071/1770 train_time:103350ms step_avg:97.41ms
step:1072/1770 train_time:103450ms step_avg:97.41ms
step:1073/1770 train_time:103550ms step_avg:97.41ms
step:1074/1770 train_time:103651ms step_avg:97.42ms
step:1075/1770 train_time:103753ms step_avg:97.42ms
step:1076/1770 train_time:103854ms step_avg:97.42ms
step:1077/1770 train_time:103956ms step_avg:97.43ms
step:1078/1770 train_time:104057ms step_avg:97.43ms
step:1079/1770 train_time:104158ms step_avg:97.44ms
step:1080/1770 train_time:104260ms step_avg:97.44ms
step:1081/1770 train_time:104361ms step_avg:97.44ms
step:1082/1770 train_time:104462ms step_avg:97.45ms
step:1083/1770 train_time:104564ms step_avg:97.45ms
step:1084/1770 train_time:104665ms step_avg:97.45ms
step:1085/1770 train_time:104767ms step_avg:97.46ms
step:1086/1770 train_time:104868ms step_avg:97.46ms
step:1087/1770 train_time:104968ms step_avg:97.46ms
step:1088/1770 train_time:105068ms step_avg:97.47ms
step:1089/1770 train_time:105168ms step_avg:97.47ms
step:1090/1770 train_time:105269ms step_avg:97.47ms
step:1091/1770 train_time:105370ms step_avg:97.47ms
step:1092/1770 train_time:105470ms step_avg:97.48ms
step:1093/1770 train_time:105572ms step_avg:97.48ms
step:1094/1770 train_time:105675ms step_avg:97.49ms
step:1095/1770 train_time:105776ms step_avg:97.49ms
step:1096/1770 train_time:105878ms step_avg:97.49ms
step:1097/1770 train_time:105978ms step_avg:97.50ms
step:1098/1770 train_time:106079ms step_avg:97.50ms
step:1099/1770 train_time:106179ms step_avg:97.50ms
step:1100/1770 train_time:106280ms step_avg:97.50ms
step:1101/1770 train_time:106381ms step_avg:97.51ms
step:1102/1770 train_time:106481ms step_avg:97.51ms
step:1103/1770 train_time:106582ms step_avg:97.51ms
step:1104/1770 train_time:106683ms step_avg:97.52ms
step:1105/1770 train_time:106784ms step_avg:97.52ms
step:1106/1770 train_time:106885ms step_avg:97.52ms
step:1107/1770 train_time:106986ms step_avg:97.53ms
step:1108/1770 train_time:107087ms step_avg:97.53ms
step:1109/1770 train_time:107188ms step_avg:97.53ms
step:1110/1770 train_time:107288ms step_avg:97.53ms
step:1111/1770 train_time:107389ms step_avg:97.54ms
step:1112/1770 train_time:107490ms step_avg:97.54ms
step:1113/1770 train_time:107590ms step_avg:97.54ms
step:1114/1770 train_time:107692ms step_avg:97.55ms
step:1115/1770 train_time:107793ms step_avg:97.55ms
step:1116/1770 train_time:107895ms step_avg:97.55ms
step:1117/1770 train_time:107997ms step_avg:97.56ms
step:1118/1770 train_time:108099ms step_avg:97.56ms
step:1119/1770 train_time:108200ms step_avg:97.57ms
step:1120/1770 train_time:108301ms step_avg:97.57ms
step:1121/1770 train_time:108402ms step_avg:97.57ms
step:1122/1770 train_time:108502ms step_avg:97.57ms
step:1123/1770 train_time:108602ms step_avg:97.58ms
step:1124/1770 train_time:108702ms step_avg:97.58ms
step:1125/1770 train_time:108804ms step_avg:97.58ms
step:1125/1770 val_loss:3.4731 train_time:108902ms step_avg:97.67ms
step:1126/1770 train_time:108924ms step_avg:97.60ms
step:1127/1770 train_time:109013ms step_avg:97.59ms
step:1128/1770 train_time:109113ms step_avg:97.60ms
step:1129/1770 train_time:109214ms step_avg:97.60ms
step:1130/1770 train_time:109315ms step_avg:97.60ms
step:1131/1770 train_time:109415ms step_avg:97.60ms
step:1132/1770 train_time:109515ms step_avg:97.61ms
step:1133/1770 train_time:109616ms step_avg:97.61ms
step:1134/1770 train_time:109716ms step_avg:97.61ms
step:1135/1770 train_time:109819ms step_avg:97.62ms
step:1136/1770 train_time:109923ms step_avg:97.62ms
step:1137/1770 train_time:110026ms step_avg:97.63ms
step:1138/1770 train_time:110127ms step_avg:97.63ms
step:1139/1770 train_time:110227ms step_avg:97.63ms
step:1140/1770 train_time:110329ms step_avg:97.64ms
step:1141/1770 train_time:110429ms step_avg:97.64ms
step:1142/1770 train_time:110530ms step_avg:97.64ms
step:1143/1770 train_time:110630ms step_avg:97.64ms
step:1144/1770 train_time:110731ms step_avg:97.65ms
step:1145/1770 train_time:110832ms step_avg:97.65ms
step:1146/1770 train_time:110934ms step_avg:97.65ms
step:1147/1770 train_time:111035ms step_avg:97.66ms
step:1148/1770 train_time:111135ms step_avg:97.66ms
step:1149/1770 train_time:111236ms step_avg:97.66ms
step:1150/1770 train_time:111338ms step_avg:97.66ms
step:1151/1770 train_time:111440ms step_avg:97.67ms
step:1152/1770 train_time:111542ms step_avg:97.67ms
step:1153/1770 train_time:111643ms step_avg:97.68ms
step:1154/1770 train_time:111745ms step_avg:97.68ms
step:1155/1770 train_time:111845ms step_avg:97.68ms
step:1156/1770 train_time:111946ms step_avg:97.68ms
step:1157/1770 train_time:112049ms step_avg:97.69ms
step:1158/1770 train_time:112150ms step_avg:97.69ms
step:1159/1770 train_time:112250ms step_avg:97.69ms
step:1160/1770 train_time:112350ms step_avg:97.70ms
step:1161/1770 train_time:112451ms step_avg:97.70ms
step:1162/1770 train_time:112552ms step_avg:97.70ms
step:1163/1770 train_time:112652ms step_avg:97.70ms
step:1164/1770 train_time:112754ms step_avg:97.71ms
step:1165/1770 train_time:112854ms step_avg:97.71ms
step:1166/1770 train_time:112956ms step_avg:97.71ms
step:1167/1770 train_time:113057ms step_avg:97.72ms
step:1168/1770 train_time:113159ms step_avg:97.72ms
step:1169/1770 train_time:113260ms step_avg:97.72ms
step:1170/1770 train_time:113361ms step_avg:97.72ms
step:1171/1770 train_time:113462ms step_avg:97.73ms
step:1172/1770 train_time:113564ms step_avg:97.73ms
step:1173/1770 train_time:113666ms step_avg:97.73ms
step:1174/1770 train_time:113767ms step_avg:97.74ms
step:1175/1770 train_time:113868ms step_avg:97.74ms
step:1176/1770 train_time:113968ms step_avg:97.74ms
step:1177/1770 train_time:114069ms step_avg:97.75ms
step:1178/1770 train_time:114172ms step_avg:97.75ms
step:1179/1770 train_time:114272ms step_avg:97.75ms
step:1180/1770 train_time:114372ms step_avg:97.75ms
step:1181/1770 train_time:114473ms step_avg:97.76ms
step:1182/1770 train_time:114573ms step_avg:97.76ms
step:1183/1770 train_time:114675ms step_avg:97.76ms
step:1184/1770 train_time:114778ms step_avg:97.77ms
step:1185/1770 train_time:114880ms step_avg:97.77ms
step:1186/1770 train_time:114983ms step_avg:97.77ms
step:1187/1770 train_time:115088ms step_avg:97.78ms
step:1188/1770 train_time:115190ms step_avg:97.78ms
step:1189/1770 train_time:115291ms step_avg:97.79ms
step:1190/1770 train_time:115393ms step_avg:97.79ms
step:1191/1770 train_time:115495ms step_avg:97.79ms
step:1192/1770 train_time:115597ms step_avg:97.80ms
step:1193/1770 train_time:115699ms step_avg:97.80ms
step:1194/1770 train_time:115801ms step_avg:97.80ms
step:1195/1770 train_time:115903ms step_avg:97.81ms
step:1196/1770 train_time:116009ms step_avg:97.81ms
step:1197/1770 train_time:116108ms step_avg:97.82ms
step:1198/1770 train_time:116210ms step_avg:97.82ms
step:1199/1770 train_time:116312ms step_avg:97.82ms
step:1200/1770 train_time:116414ms step_avg:97.83ms
step:1201/1770 train_time:116516ms step_avg:97.83ms
step:1202/1770 train_time:116617ms step_avg:97.83ms
step:1203/1770 train_time:116719ms step_avg:97.84ms
step:1204/1770 train_time:116822ms step_avg:97.84ms
step:1205/1770 train_time:116923ms step_avg:97.84ms
step:1206/1770 train_time:117026ms step_avg:97.85ms
step:1207/1770 train_time:117127ms step_avg:97.85ms
step:1208/1770 train_time:117230ms step_avg:97.85ms
step:1209/1770 train_time:117331ms step_avg:97.86ms
step:1210/1770 train_time:117433ms step_avg:97.86ms
step:1211/1770 train_time:117535ms step_avg:97.86ms
step:1212/1770 train_time:117639ms step_avg:97.87ms
step:1213/1770 train_time:117741ms step_avg:97.87ms
step:1214/1770 train_time:117842ms step_avg:97.88ms
step:1215/1770 train_time:117945ms step_avg:97.88ms
step:1216/1770 train_time:118049ms step_avg:97.89ms
step:1217/1770 train_time:118151ms step_avg:97.89ms
step:1218/1770 train_time:118253ms step_avg:97.89ms
step:1219/1770 train_time:118354ms step_avg:97.89ms
step:1220/1770 train_time:118457ms step_avg:97.90ms
step:1221/1770 train_time:118559ms step_avg:97.90ms
step:1222/1770 train_time:118663ms step_avg:97.91ms
step:1223/1770 train_time:118764ms step_avg:97.91ms
step:1224/1770 train_time:118867ms step_avg:97.91ms
step:1225/1770 train_time:118969ms step_avg:97.92ms
step:1226/1770 train_time:119071ms step_avg:97.92ms
step:1227/1770 train_time:119175ms step_avg:97.93ms
step:1228/1770 train_time:119279ms step_avg:97.93ms
step:1229/1770 train_time:119380ms step_avg:97.93ms
step:1230/1770 train_time:119484ms step_avg:97.94ms
step:1231/1770 train_time:119586ms step_avg:97.94ms
step:1232/1770 train_time:119687ms step_avg:97.94ms
step:1233/1770 train_time:119788ms step_avg:97.95ms
step:1234/1770 train_time:119894ms step_avg:97.95ms
step:1235/1770 train_time:119993ms step_avg:97.95ms
step:1236/1770 train_time:120095ms step_avg:97.96ms
step:1237/1770 train_time:120198ms step_avg:97.96ms
step:1238/1770 train_time:120301ms step_avg:97.96ms
step:1239/1770 train_time:120403ms step_avg:97.97ms
step:1240/1770 train_time:120505ms step_avg:97.97ms
step:1241/1770 train_time:120608ms step_avg:97.98ms
step:1242/1770 train_time:120709ms step_avg:97.98ms
step:1243/1770 train_time:120811ms step_avg:97.98ms
step:1244/1770 train_time:120912ms step_avg:97.98ms
step:1245/1770 train_time:121014ms step_avg:97.99ms
step:1246/1770 train_time:121117ms step_avg:97.99ms
step:1247/1770 train_time:121218ms step_avg:97.99ms
step:1248/1770 train_time:121321ms step_avg:98.00ms
step:1249/1770 train_time:121423ms step_avg:98.00ms
step:1250/1770 train_time:121525ms step_avg:98.00ms
step:1250/1770 val_loss:3.4246 train_time:121627ms step_avg:98.09ms
step:1251/1770 train_time:121649ms step_avg:98.02ms
step:1252/1770 train_time:121737ms step_avg:98.02ms
step:1253/1770 train_time:121839ms step_avg:98.02ms
step:1254/1770 train_time:121940ms step_avg:98.02ms
step:1255/1770 train_time:122044ms step_avg:98.03ms
step:1256/1770 train_time:122145ms step_avg:98.03ms
step:1257/1770 train_time:122248ms step_avg:98.03ms
step:1258/1770 train_time:122350ms step_avg:98.04ms
step:1259/1770 train_time:122452ms step_avg:98.04ms
step:1260/1770 train_time:122554ms step_avg:98.04ms
step:1261/1770 train_time:122658ms step_avg:98.05ms
step:1262/1770 train_time:122761ms step_avg:98.05ms
step:1263/1770 train_time:122863ms step_avg:98.05ms
step:1264/1770 train_time:122966ms step_avg:98.06ms
step:1265/1770 train_time:123068ms step_avg:98.06ms
step:1266/1770 train_time:123170ms step_avg:98.07ms
step:1267/1770 train_time:123273ms step_avg:98.07ms
step:1268/1770 train_time:123375ms step_avg:98.07ms
step:1269/1770 train_time:123477ms step_avg:98.08ms
step:1270/1770 train_time:123580ms step_avg:98.08ms
step:1271/1770 train_time:123681ms step_avg:98.08ms
step:1272/1770 train_time:123783ms step_avg:98.08ms
step:1273/1770 train_time:123885ms step_avg:98.09ms
step:1274/1770 train_time:123987ms step_avg:98.09ms
step:1275/1770 train_time:124090ms step_avg:98.09ms
step:1276/1770 train_time:124192ms step_avg:98.10ms
step:1277/1770 train_time:124294ms step_avg:98.10ms
step:1278/1770 train_time:124397ms step_avg:98.11ms
step:1279/1770 train_time:124500ms step_avg:98.11ms
step:1280/1770 train_time:124603ms step_avg:98.11ms
step:1281/1770 train_time:124704ms step_avg:98.12ms
step:1282/1770 train_time:124807ms step_avg:98.12ms
step:1283/1770 train_time:124910ms step_avg:98.12ms
step:1284/1770 train_time:125012ms step_avg:98.13ms
step:1285/1770 train_time:125114ms step_avg:98.13ms
step:1286/1770 train_time:125216ms step_avg:98.13ms
step:1287/1770 train_time:125320ms step_avg:98.14ms
step:1288/1770 train_time:125422ms step_avg:98.14ms
step:1289/1770 train_time:125525ms step_avg:98.14ms
step:1290/1770 train_time:125626ms step_avg:98.15ms
step:1291/1770 train_time:125729ms step_avg:98.15ms
step:1292/1770 train_time:125831ms step_avg:98.15ms
step:1293/1770 train_time:125934ms step_avg:98.16ms
step:1294/1770 train_time:126035ms step_avg:98.16ms
step:1295/1770 train_time:126138ms step_avg:98.16ms
step:1296/1770 train_time:126240ms step_avg:98.16ms
step:1297/1770 train_time:126341ms step_avg:98.17ms
step:1298/1770 train_time:126444ms step_avg:98.17ms
step:1299/1770 train_time:126545ms step_avg:98.17ms
step:1300/1770 train_time:126647ms step_avg:98.18ms
step:1301/1770 train_time:126750ms step_avg:98.18ms
step:1302/1770 train_time:126852ms step_avg:98.18ms
step:1303/1770 train_time:126954ms step_avg:98.19ms
step:1304/1770 train_time:127055ms step_avg:98.19ms
step:1305/1770 train_time:127158ms step_avg:98.19ms
step:1306/1770 train_time:127259ms step_avg:98.19ms
step:1307/1770 train_time:127360ms step_avg:98.20ms
step:1308/1770 train_time:127463ms step_avg:98.20ms
step:1309/1770 train_time:127564ms step_avg:98.20ms
step:1310/1770 train_time:127666ms step_avg:98.20ms
step:1311/1770 train_time:127768ms step_avg:98.21ms
step:1312/1770 train_time:127870ms step_avg:98.21ms
step:1313/1770 train_time:127973ms step_avg:98.21ms
step:1314/1770 train_time:128075ms step_avg:98.22ms
step:1315/1770 train_time:128177ms step_avg:98.22ms
step:1316/1770 train_time:128279ms step_avg:98.22ms
step:1317/1770 train_time:128381ms step_avg:98.23ms
step:1318/1770 train_time:128485ms step_avg:98.23ms
step:1319/1770 train_time:128587ms step_avg:98.23ms
step:1320/1770 train_time:128689ms step_avg:98.24ms
step:1321/1770 train_time:128791ms step_avg:98.24ms
step:1322/1770 train_time:128894ms step_avg:98.24ms
step:1323/1770 train_time:128996ms step_avg:98.25ms
step:1324/1770 train_time:129100ms step_avg:98.25ms
step:1325/1770 train_time:129204ms step_avg:98.25ms
step:1326/1770 train_time:129305ms step_avg:98.26ms
step:1327/1770 train_time:129411ms step_avg:98.26ms
step:1328/1770 train_time:129513ms step_avg:98.26ms
step:1329/1770 train_time:129615ms step_avg:98.27ms
step:1330/1770 train_time:129716ms step_avg:98.27ms
step:1331/1770 train_time:129818ms step_avg:98.27ms
step:1332/1770 train_time:129920ms step_avg:98.28ms
step:1333/1770 train_time:130022ms step_avg:98.28ms
step:1334/1770 train_time:130124ms step_avg:98.28ms
step:1335/1770 train_time:130226ms step_avg:98.28ms
step:1336/1770 train_time:130328ms step_avg:98.29ms
step:1337/1770 train_time:130430ms step_avg:98.29ms
step:1338/1770 train_time:130533ms step_avg:98.29ms
step:1339/1770 train_time:130636ms step_avg:98.30ms
step:1340/1770 train_time:130740ms step_avg:98.30ms
step:1341/1770 train_time:130841ms step_avg:98.30ms
step:1342/1770 train_time:130944ms step_avg:98.31ms
step:1343/1770 train_time:131047ms step_avg:98.31ms
step:1344/1770 train_time:131151ms step_avg:98.31ms
step:1345/1770 train_time:131253ms step_avg:98.32ms
step:1346/1770 train_time:131355ms step_avg:98.32ms
step:1347/1770 train_time:131457ms step_avg:98.32ms
step:1348/1770 train_time:131562ms step_avg:98.33ms
step:1349/1770 train_time:131664ms step_avg:98.33ms
step:1350/1770 train_time:131767ms step_avg:98.33ms
step:1351/1770 train_time:131869ms step_avg:98.34ms
step:1352/1770 train_time:131971ms step_avg:98.34ms
step:1353/1770 train_time:132075ms step_avg:98.34ms
step:1354/1770 train_time:132177ms step_avg:98.35ms
step:1355/1770 train_time:132280ms step_avg:98.35ms
step:1356/1770 train_time:132382ms step_avg:98.35ms
step:1357/1770 train_time:132484ms step_avg:98.35ms
step:1358/1770 train_time:132586ms step_avg:98.36ms
step:1359/1770 train_time:132688ms step_avg:98.36ms
step:1360/1770 train_time:132791ms step_avg:98.36ms
step:1361/1770 train_time:132894ms step_avg:98.37ms
step:1362/1770 train_time:132995ms step_avg:98.37ms
step:1363/1770 train_time:133098ms step_avg:98.37ms
step:1364/1770 train_time:133200ms step_avg:98.38ms
step:1365/1770 train_time:133301ms step_avg:98.38ms
step:1366/1770 train_time:133403ms step_avg:98.38ms
step:1367/1770 train_time:133506ms step_avg:98.38ms
step:1368/1770 train_time:133608ms step_avg:98.39ms
step:1369/1770 train_time:133710ms step_avg:98.39ms
step:1370/1770 train_time:133814ms step_avg:98.39ms
step:1371/1770 train_time:133916ms step_avg:98.40ms
step:1372/1770 train_time:134018ms step_avg:98.40ms
step:1373/1770 train_time:134119ms step_avg:98.40ms
step:1374/1770 train_time:134222ms step_avg:98.40ms
step:1375/1770 train_time:134324ms step_avg:98.41ms
step:1375/1770 val_loss:3.3815 train_time:134425ms step_avg:98.48ms
step:1376/1770 train_time:134446ms step_avg:98.42ms
step:1377/1770 train_time:134532ms step_avg:98.41ms
step:1378/1770 train_time:134634ms step_avg:98.42ms
step:1379/1770 train_time:134736ms step_avg:98.42ms
step:1380/1770 train_time:134838ms step_avg:98.42ms
step:1381/1770 train_time:134941ms step_avg:98.42ms
step:1382/1770 train_time:135042ms step_avg:98.43ms
step:1383/1770 train_time:135145ms step_avg:98.43ms
step:1384/1770 train_time:135247ms step_avg:98.43ms
step:1385/1770 train_time:135349ms step_avg:98.44ms
step:1386/1770 train_time:135452ms step_avg:98.44ms
step:1387/1770 train_time:135555ms step_avg:98.44ms
step:1388/1770 train_time:135656ms step_avg:98.44ms
step:1389/1770 train_time:135758ms step_avg:98.45ms
step:1390/1770 train_time:135861ms step_avg:98.45ms
step:1391/1770 train_time:135963ms step_avg:98.45ms
step:1392/1770 train_time:136065ms step_avg:98.46ms
step:1393/1770 train_time:136166ms step_avg:98.46ms
step:1394/1770 train_time:136268ms step_avg:98.46ms
step:1395/1770 train_time:136372ms step_avg:98.46ms
step:1396/1770 train_time:136476ms step_avg:98.47ms
step:1397/1770 train_time:136577ms step_avg:98.47ms
step:1398/1770 train_time:136680ms step_avg:98.47ms
step:1399/1770 train_time:136782ms step_avg:98.48ms
step:1400/1770 train_time:136885ms step_avg:98.48ms
step:1401/1770 train_time:136987ms step_avg:98.48ms
step:1402/1770 train_time:137089ms step_avg:98.48ms
step:1403/1770 train_time:137191ms step_avg:98.49ms
step:1404/1770 train_time:137294ms step_avg:98.49ms
step:1405/1770 train_time:137395ms step_avg:98.49ms
step:1406/1770 train_time:137497ms step_avg:98.49ms
step:1407/1770 train_time:137599ms step_avg:98.50ms
step:1408/1770 train_time:137702ms step_avg:98.50ms
step:1409/1770 train_time:137806ms step_avg:98.50ms
step:1410/1770 train_time:137906ms step_avg:98.50ms
step:1411/1770 train_time:138007ms step_avg:98.51ms
step:1412/1770 train_time:138109ms step_avg:98.51ms
step:1413/1770 train_time:138210ms step_avg:98.51ms
step:1414/1770 train_time:138314ms step_avg:98.51ms
step:1415/1770 train_time:138416ms step_avg:98.52ms
step:1416/1770 train_time:138519ms step_avg:98.52ms
step:1417/1770 train_time:138621ms step_avg:98.52ms
step:1418/1770 train_time:138724ms step_avg:98.53ms
step:1419/1770 train_time:138827ms step_avg:98.53ms
step:1420/1770 train_time:138929ms step_avg:98.53ms
step:1421/1770 train_time:139031ms step_avg:98.53ms
step:1422/1770 train_time:139133ms step_avg:98.54ms
step:1423/1770 train_time:139236ms step_avg:98.54ms
step:1424/1770 train_time:139338ms step_avg:98.54ms
step:1425/1770 train_time:139441ms step_avg:98.54ms
step:1426/1770 train_time:139544ms step_avg:98.55ms
step:1427/1770 train_time:139645ms step_avg:98.55ms
step:1428/1770 train_time:139749ms step_avg:98.55ms
step:1429/1770 train_time:139851ms step_avg:98.56ms
step:1430/1770 train_time:139952ms step_avg:98.56ms
step:1431/1770 train_time:140055ms step_avg:98.56ms
step:1432/1770 train_time:140157ms step_avg:98.56ms
step:1433/1770 train_time:140259ms step_avg:98.57ms
step:1434/1770 train_time:140360ms step_avg:98.57ms
step:1435/1770 train_time:140463ms step_avg:98.57ms
step:1436/1770 train_time:140567ms step_avg:98.57ms
step:1437/1770 train_time:140669ms step_avg:98.58ms
step:1438/1770 train_time:140772ms step_avg:98.58ms
step:1439/1770 train_time:140873ms step_avg:98.58ms
step:1440/1770 train_time:140975ms step_avg:98.58ms
step:1441/1770 train_time:141079ms step_avg:98.59ms
step:1442/1770 train_time:141182ms step_avg:98.59ms
step:1443/1770 train_time:141284ms step_avg:98.59ms
step:1444/1770 train_time:141386ms step_avg:98.60ms
step:1445/1770 train_time:141489ms step_avg:98.60ms
step:1446/1770 train_time:141591ms step_avg:98.60ms
step:1447/1770 train_time:141694ms step_avg:98.60ms
step:1448/1770 train_time:141802ms step_avg:98.61ms
step:1449/1770 train_time:141901ms step_avg:98.61ms
step:1450/1770 train_time:142004ms step_avg:98.61ms
step:1451/1770 train_time:142107ms step_avg:98.62ms
step:1452/1770 train_time:142211ms step_avg:98.62ms
step:1453/1770 train_time:142314ms step_avg:98.62ms
step:1454/1770 train_time:142417ms step_avg:98.63ms
step:1455/1770 train_time:142523ms step_avg:98.63ms
step:1456/1770 train_time:142627ms step_avg:98.64ms
step:1457/1770 train_time:142730ms step_avg:98.64ms
step:1458/1770 train_time:142833ms step_avg:98.64ms
step:1459/1770 train_time:142937ms step_avg:98.64ms
step:1460/1770 train_time:143041ms step_avg:98.65ms
step:1461/1770 train_time:143144ms step_avg:98.65ms
step:1462/1770 train_time:143247ms step_avg:98.65ms
step:1463/1770 train_time:143350ms step_avg:98.66ms
step:1464/1770 train_time:143454ms step_avg:98.66ms
step:1465/1770 train_time:143557ms step_avg:98.66ms
step:1466/1770 train_time:143662ms step_avg:98.67ms
step:1467/1770 train_time:143766ms step_avg:98.67ms
step:1468/1770 train_time:143869ms step_avg:98.68ms
step:1469/1770 train_time:143971ms step_avg:98.68ms
step:1470/1770 train_time:144074ms step_avg:98.68ms
step:1471/1770 train_time:144179ms step_avg:98.69ms
step:1472/1770 train_time:144283ms step_avg:98.69ms
step:1473/1770 train_time:144387ms step_avg:98.69ms
step:1474/1770 train_time:144491ms step_avg:98.70ms
step:1475/1770 train_time:144594ms step_avg:98.70ms
step:1476/1770 train_time:144696ms step_avg:98.70ms
step:1477/1770 train_time:144803ms step_avg:98.71ms
step:1478/1770 train_time:144906ms step_avg:98.71ms
step:1479/1770 train_time:145009ms step_avg:98.71ms
step:1480/1770 train_time:145111ms step_avg:98.72ms
step:1481/1770 train_time:145218ms step_avg:98.72ms
step:1482/1770 train_time:145321ms step_avg:98.72ms
step:1483/1770 train_time:145425ms step_avg:98.73ms
step:1484/1770 train_time:145529ms step_avg:98.73ms
step:1485/1770 train_time:145632ms step_avg:98.73ms
step:1486/1770 train_time:145735ms step_avg:98.74ms
step:1487/1770 train_time:145838ms step_avg:98.74ms
step:1488/1770 train_time:145942ms step_avg:98.74ms
step:1489/1770 train_time:146046ms step_avg:98.75ms
step:1490/1770 train_time:146151ms step_avg:98.75ms
step:1491/1770 train_time:146255ms step_avg:98.75ms
step:1492/1770 train_time:146359ms step_avg:98.76ms
step:1493/1770 train_time:146465ms step_avg:98.76ms
step:1494/1770 train_time:146571ms step_avg:98.77ms
step:1495/1770 train_time:146674ms step_avg:98.77ms
step:1496/1770 train_time:146776ms step_avg:98.77ms
step:1497/1770 train_time:146880ms step_avg:98.78ms
step:1498/1770 train_time:146983ms step_avg:98.78ms
step:1499/1770 train_time:147085ms step_avg:98.78ms
step:1500/1770 train_time:147188ms step_avg:98.78ms
step:1500/1770 val_loss:3.3438 train_time:147289ms step_avg:98.85ms
step:1501/1770 train_time:147310ms step_avg:98.80ms
step:1502/1770 train_time:147398ms step_avg:98.79ms
step:1503/1770 train_time:147502ms step_avg:98.80ms
step:1504/1770 train_time:147605ms step_avg:98.80ms
step:1505/1770 train_time:147710ms step_avg:98.80ms
step:1506/1770 train_time:147813ms step_avg:98.81ms
step:1507/1770 train_time:147916ms step_avg:98.81ms
step:1508/1770 train_time:148021ms step_avg:98.81ms
step:1509/1770 train_time:148125ms step_avg:98.82ms
step:1510/1770 train_time:148227ms step_avg:98.82ms
step:1511/1770 train_time:148332ms step_avg:98.82ms
step:1512/1770 train_time:148435ms step_avg:98.83ms
step:1513/1770 train_time:148539ms step_avg:98.83ms
step:1514/1770 train_time:148643ms step_avg:98.83ms
step:1515/1770 train_time:148746ms step_avg:98.83ms
step:1516/1770 train_time:148849ms step_avg:98.84ms
step:1517/1770 train_time:148952ms step_avg:98.84ms
step:1518/1770 train_time:149058ms step_avg:98.84ms
step:1519/1770 train_time:149160ms step_avg:98.85ms
step:1520/1770 train_time:149264ms step_avg:98.85ms
step:1521/1770 train_time:149367ms step_avg:98.85ms
step:1522/1770 train_time:149471ms step_avg:98.86ms
step:1523/1770 train_time:149576ms step_avg:98.86ms
step:1524/1770 train_time:149678ms step_avg:98.86ms
step:1525/1770 train_time:149782ms step_avg:98.87ms
step:1526/1770 train_time:149885ms step_avg:98.87ms
step:1527/1770 train_time:149989ms step_avg:98.87ms
step:1528/1770 train_time:150093ms step_avg:98.88ms
step:1529/1770 train_time:150196ms step_avg:98.88ms
step:1530/1770 train_time:150299ms step_avg:98.88ms
step:1531/1770 train_time:150403ms step_avg:98.88ms
step:1532/1770 train_time:150508ms step_avg:98.89ms
step:1533/1770 train_time:150611ms step_avg:98.89ms
step:1534/1770 train_time:150715ms step_avg:98.89ms
step:1535/1770 train_time:150817ms step_avg:98.90ms
step:1536/1770 train_time:150919ms step_avg:98.90ms
step:1537/1770 train_time:151023ms step_avg:98.90ms
step:1538/1770 train_time:151128ms step_avg:98.91ms
step:1539/1770 train_time:151231ms step_avg:98.91ms
step:1540/1770 train_time:151337ms step_avg:98.91ms
step:1541/1770 train_time:151441ms step_avg:98.92ms
step:1542/1770 train_time:151545ms step_avg:98.92ms
step:1543/1770 train_time:151650ms step_avg:98.92ms
step:1544/1770 train_time:151754ms step_avg:98.93ms
step:1545/1770 train_time:151857ms step_avg:98.93ms
step:1546/1770 train_time:151959ms step_avg:98.93ms
step:1547/1770 train_time:152063ms step_avg:98.93ms
step:1548/1770 train_time:152167ms step_avg:98.94ms
step:1549/1770 train_time:152270ms step_avg:98.94ms
step:1550/1770 train_time:152374ms step_avg:98.94ms
step:1551/1770 train_time:152477ms step_avg:98.95ms
step:1552/1770 train_time:152581ms step_avg:98.95ms
step:1553/1770 train_time:152685ms step_avg:98.95ms
step:1554/1770 train_time:152787ms step_avg:98.96ms
step:1555/1770 train_time:152891ms step_avg:98.96ms
step:1556/1770 train_time:152994ms step_avg:98.96ms
step:1557/1770 train_time:153097ms step_avg:98.96ms
step:1558/1770 train_time:153201ms step_avg:98.97ms
step:1559/1770 train_time:153305ms step_avg:98.97ms
step:1560/1770 train_time:153408ms step_avg:98.97ms
step:1561/1770 train_time:153514ms step_avg:98.98ms
step:1562/1770 train_time:153617ms step_avg:98.98ms
step:1563/1770 train_time:153720ms step_avg:98.98ms
step:1564/1770 train_time:153823ms step_avg:98.98ms
step:1565/1770 train_time:153926ms step_avg:98.99ms
step:1566/1770 train_time:154028ms step_avg:98.99ms
step:1567/1770 train_time:154131ms step_avg:98.99ms
step:1568/1770 train_time:154235ms step_avg:99.00ms
step:1569/1770 train_time:154342ms step_avg:99.00ms
step:1570/1770 train_time:154445ms step_avg:99.00ms
step:1571/1770 train_time:154548ms step_avg:99.01ms
step:1572/1770 train_time:154652ms step_avg:99.01ms
step:1573/1770 train_time:154758ms step_avg:99.01ms
step:1574/1770 train_time:154861ms step_avg:99.02ms
step:1575/1770 train_time:154963ms step_avg:99.02ms
step:1576/1770 train_time:155066ms step_avg:99.02ms
step:1577/1770 train_time:155171ms step_avg:99.02ms
step:1578/1770 train_time:155276ms step_avg:99.03ms
step:1579/1770 train_time:155378ms step_avg:99.03ms
step:1580/1770 train_time:155481ms step_avg:99.03ms
step:1581/1770 train_time:155587ms step_avg:99.04ms
step:1582/1770 train_time:155692ms step_avg:99.04ms
step:1583/1770 train_time:155796ms step_avg:99.04ms
step:1584/1770 train_time:155900ms step_avg:99.05ms
step:1585/1770 train_time:156004ms step_avg:99.05ms
step:1586/1770 train_time:156111ms step_avg:99.06ms
step:1587/1770 train_time:156215ms step_avg:99.06ms
step:1588/1770 train_time:156318ms step_avg:99.06ms
step:1589/1770 train_time:156423ms step_avg:99.06ms
step:1590/1770 train_time:156526ms step_avg:99.07ms
step:1591/1770 train_time:156629ms step_avg:99.07ms
step:1592/1770 train_time:156733ms step_avg:99.07ms
step:1593/1770 train_time:156836ms step_avg:99.08ms
step:1594/1770 train_time:156939ms step_avg:99.08ms
step:1595/1770 train_time:157044ms step_avg:99.08ms
step:1596/1770 train_time:157148ms step_avg:99.08ms
step:1597/1770 train_time:157251ms step_avg:99.09ms
step:1598/1770 train_time:157355ms step_avg:99.09ms
step:1599/1770 train_time:157461ms step_avg:99.09ms
step:1600/1770 train_time:157565ms step_avg:99.10ms
step:1601/1770 train_time:157669ms step_avg:99.10ms
step:1602/1770 train_time:157773ms step_avg:99.10ms
step:1603/1770 train_time:157876ms step_avg:99.11ms
step:1604/1770 train_time:157979ms step_avg:99.11ms
step:1605/1770 train_time:158082ms step_avg:99.11ms
step:1606/1770 train_time:158186ms step_avg:99.11ms
step:1607/1770 train_time:158293ms step_avg:99.12ms
step:1608/1770 train_time:158396ms step_avg:99.12ms
step:1609/1770 train_time:158500ms step_avg:99.12ms
step:1610/1770 train_time:158604ms step_avg:99.13ms
step:1611/1770 train_time:158709ms step_avg:99.13ms
step:1612/1770 train_time:158813ms step_avg:99.13ms
step:1613/1770 train_time:158916ms step_avg:99.14ms
step:1614/1770 train_time:159019ms step_avg:99.14ms
step:1615/1770 train_time:159122ms step_avg:99.14ms
step:1616/1770 train_time:159226ms step_avg:99.14ms
step:1617/1770 train_time:159331ms step_avg:99.15ms
step:1618/1770 train_time:159435ms step_avg:99.15ms
step:1619/1770 train_time:159539ms step_avg:99.15ms
step:1620/1770 train_time:159643ms step_avg:99.16ms
step:1621/1770 train_time:159747ms step_avg:99.16ms
step:1622/1770 train_time:159851ms step_avg:99.16ms
step:1623/1770 train_time:159956ms step_avg:99.17ms
step:1624/1770 train_time:160059ms step_avg:99.17ms
step:1625/1770 train_time:160162ms step_avg:99.17ms
step:1625/1770 val_loss:3.3089 train_time:160265ms step_avg:99.24ms
step:1626/1770 train_time:160287ms step_avg:99.19ms
step:1627/1770 train_time:160376ms step_avg:99.18ms
step:1628/1770 train_time:160480ms step_avg:99.18ms
step:1629/1770 train_time:160582ms step_avg:99.19ms
step:1630/1770 train_time:160685ms step_avg:99.19ms
step:1631/1770 train_time:160787ms step_avg:99.19ms
step:1632/1770 train_time:160890ms step_avg:99.19ms
step:1633/1770 train_time:160994ms step_avg:99.20ms
step:1634/1770 train_time:161096ms step_avg:99.20ms
step:1635/1770 train_time:161200ms step_avg:99.20ms
step:1636/1770 train_time:161304ms step_avg:99.20ms
step:1637/1770 train_time:161408ms step_avg:99.21ms
step:1638/1770 train_time:161512ms step_avg:99.21ms
step:1639/1770 train_time:161615ms step_avg:99.21ms
step:1640/1770 train_time:161720ms step_avg:99.21ms
step:1641/1770 train_time:161823ms step_avg:99.22ms
step:1642/1770 train_time:161925ms step_avg:99.22ms
step:1643/1770 train_time:162029ms step_avg:99.22ms
step:1644/1770 train_time:162135ms step_avg:99.23ms
step:1645/1770 train_time:162238ms step_avg:99.23ms
step:1646/1770 train_time:162343ms step_avg:99.23ms
step:1647/1770 train_time:162448ms step_avg:99.24ms
step:1648/1770 train_time:162551ms step_avg:99.24ms
step:1649/1770 train_time:162655ms step_avg:99.24ms
step:1650/1770 train_time:162758ms step_avg:99.24ms
step:1651/1770 train_time:162860ms step_avg:99.24ms
step:1652/1770 train_time:162964ms step_avg:99.25ms
step:1653/1770 train_time:163067ms step_avg:99.25ms
step:1654/1770 train_time:163174ms step_avg:99.25ms
step:1655/1770 train_time:163280ms step_avg:99.26ms
step:1656/1770 train_time:163383ms step_avg:99.26ms
step:1657/1770 train_time:163488ms step_avg:99.26ms
step:1658/1770 train_time:163597ms step_avg:99.27ms
step:1659/1770 train_time:163698ms step_avg:99.27ms
step:1660/1770 train_time:163801ms step_avg:99.27ms
step:1661/1770 train_time:163904ms step_avg:99.28ms
step:1662/1770 train_time:164008ms step_avg:99.28ms
step:1663/1770 train_time:164111ms step_avg:99.28ms
step:1664/1770 train_time:164215ms step_avg:99.28ms
step:1665/1770 train_time:164317ms step_avg:99.29ms
step:1666/1770 train_time:164422ms step_avg:99.29ms
step:1667/1770 train_time:164524ms step_avg:99.29ms
step:1668/1770 train_time:164628ms step_avg:99.29ms
step:1669/1770 train_time:164731ms step_avg:99.30ms
step:1670/1770 train_time:164834ms step_avg:99.30ms
step:1671/1770 train_time:164938ms step_avg:99.30ms
step:1672/1770 train_time:165042ms step_avg:99.30ms
step:1673/1770 train_time:165146ms step_avg:99.31ms
step:1674/1770 train_time:165249ms step_avg:99.31ms
step:1675/1770 train_time:165352ms step_avg:99.31ms
step:1676/1770 train_time:165457ms step_avg:99.31ms
step:1677/1770 train_time:165564ms step_avg:99.32ms
step:1678/1770 train_time:165667ms step_avg:99.32ms
step:1679/1770 train_time:165771ms step_avg:99.32ms
step:1680/1770 train_time:165874ms step_avg:99.33ms
step:1681/1770 train_time:165979ms step_avg:99.33ms
step:1682/1770 train_time:166083ms step_avg:99.33ms
step:1683/1770 train_time:166187ms step_avg:99.33ms
step:1684/1770 train_time:166290ms step_avg:99.34ms
step:1685/1770 train_time:166394ms step_avg:99.34ms
step:1686/1770 train_time:166498ms step_avg:99.34ms
step:1687/1770 train_time:166603ms step_avg:99.35ms
step:1688/1770 train_time:166706ms step_avg:99.35ms
step:1689/1770 train_time:166810ms step_avg:99.35ms
step:1690/1770 train_time:166913ms step_avg:99.35ms
step:1691/1770 train_time:167017ms step_avg:99.36ms
step:1692/1770 train_time:167120ms step_avg:99.36ms
step:1693/1770 train_time:167225ms step_avg:99.36ms
step:1694/1770 train_time:167328ms step_avg:99.36ms
step:1695/1770 train_time:167431ms step_avg:99.37ms
step:1696/1770 train_time:167536ms step_avg:99.37ms
step:1697/1770 train_time:167641ms step_avg:99.37ms
step:1698/1770 train_time:167745ms step_avg:99.38ms
step:1699/1770 train_time:167849ms step_avg:99.38ms
step:1700/1770 train_time:167952ms step_avg:99.38ms
step:1701/1770 train_time:168056ms step_avg:99.38ms
step:1702/1770 train_time:168160ms step_avg:99.39ms
step:1703/1770 train_time:168262ms step_avg:99.39ms
step:1704/1770 train_time:168366ms step_avg:99.39ms
step:1705/1770 train_time:168469ms step_avg:99.39ms
step:1706/1770 train_time:168572ms step_avg:99.39ms
step:1707/1770 train_time:168676ms step_avg:99.40ms
step:1708/1770 train_time:168780ms step_avg:99.40ms
step:1709/1770 train_time:168886ms step_avg:99.40ms
step:1710/1770 train_time:168992ms step_avg:99.41ms
step:1711/1770 train_time:169098ms step_avg:99.41ms
step:1712/1770 train_time:169202ms step_avg:99.41ms
step:1713/1770 train_time:169305ms step_avg:99.42ms
step:1714/1770 train_time:169410ms step_avg:99.42ms
step:1715/1770 train_time:169513ms step_avg:99.42ms
step:1716/1770 train_time:169617ms step_avg:99.42ms
step:1717/1770 train_time:169721ms step_avg:99.43ms
step:1718/1770 train_time:169826ms step_avg:99.43ms
step:1719/1770 train_time:169932ms step_avg:99.43ms
step:1720/1770 train_time:170037ms step_avg:99.44ms
step:1721/1770 train_time:170140ms step_avg:99.44ms
step:1722/1770 train_time:170247ms step_avg:99.44ms
step:1723/1770 train_time:170352ms step_avg:99.45ms
step:1724/1770 train_time:170458ms step_avg:99.45ms
step:1725/1770 train_time:170564ms step_avg:99.45ms
step:1726/1770 train_time:170670ms step_avg:99.46ms
step:1727/1770 train_time:170774ms step_avg:99.46ms
step:1728/1770 train_time:170880ms step_avg:99.46ms
step:1729/1770 train_time:170983ms step_avg:99.47ms
step:1730/1770 train_time:171088ms step_avg:99.47ms
step:1731/1770 train_time:171194ms step_avg:99.47ms
step:1732/1770 train_time:171298ms step_avg:99.48ms
step:1733/1770 train_time:171404ms step_avg:99.48ms
step:1734/1770 train_time:171508ms step_avg:99.48ms
step:1735/1770 train_time:171614ms step_avg:99.49ms
step:1736/1770 train_time:171717ms step_avg:99.49ms
step:1737/1770 train_time:171821ms step_avg:99.49ms
step:1738/1770 train_time:171925ms step_avg:99.49ms
step:1739/1770 train_time:172029ms step_avg:99.50ms
step:1740/1770 train_time:172133ms step_avg:99.50ms
step:1741/1770 train_time:172240ms step_avg:99.50ms
step:1742/1770 train_time:172347ms step_avg:99.51ms
step:1743/1770 train_time:172453ms step_avg:99.51ms
step:1744/1770 train_time:172557ms step_avg:99.51ms
step:1745/1770 train_time:172661ms step_avg:99.52ms
step:1746/1770 train_time:172768ms step_avg:99.52ms
step:1747/1770 train_time:172871ms step_avg:99.52ms
step:1748/1770 train_time:172976ms step_avg:99.53ms
step:1749/1770 train_time:173081ms step_avg:99.53ms
step:1750/1770 train_time:173185ms step_avg:99.53ms
step:1750/1770 val_loss:3.2821 train_time:173287ms step_avg:99.59ms
step:1751/1770 train_time:173309ms step_avg:99.55ms
step:1752/1770 train_time:173397ms step_avg:99.54ms
step:1753/1770 train_time:173500ms step_avg:99.54ms
step:1754/1770 train_time:173605ms step_avg:99.54ms
step:1755/1770 train_time:173709ms step_avg:99.55ms
step:1756/1770 train_time:173813ms step_avg:99.55ms
step:1757/1770 train_time:173917ms step_avg:99.55ms
step:1758/1770 train_time:174021ms step_avg:99.55ms
step:1759/1770 train_time:174125ms step_avg:99.56ms
step:1760/1770 train_time:174230ms step_avg:99.56ms
step:1761/1770 train_time:174337ms step_avg:99.56ms
step:1762/1770 train_time:174445ms step_avg:99.57ms
step:1763/1770 train_time:174548ms step_avg:99.57ms
step:1764/1770 train_time:174653ms step_avg:99.57ms
step:1765/1770 train_time:174758ms step_avg:99.58ms
step:1766/1770 train_time:174865ms step_avg:99.58ms
step:1767/1770 train_time:174968ms step_avg:99.58ms
step:1768/1770 train_time:175073ms step_avg:99.59ms
step:1769/1770 train_time:175176ms step_avg:99.59ms
step:1770/1770 train_time:175279ms step_avg:99.59ms
step:1770/1770 val_loss:3.2789 train_time:175383ms step_avg:99.65ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
