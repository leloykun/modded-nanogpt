import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:58:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24982ms step_avg:nanms
step:2/1770 train_time:25385ms step_avg:nanms
step:3/1770 train_time:25481ms step_avg:nanms
step:4/1770 train_time:25575ms step_avg:nanms
step:5/1770 train_time:25668ms step_avg:nanms
step:6/1770 train_time:25761ms step_avg:nanms
step:7/1770 train_time:25855ms step_avg:nanms
step:8/1770 train_time:25948ms step_avg:nanms
step:9/1770 train_time:26042ms step_avg:nanms
step:10/1770 train_time:26136ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.02ms
step:14/1770 train_time:376ms step_avg:94.05ms
step:15/1770 train_time:471ms step_avg:94.16ms
step:16/1770 train_time:564ms step_avg:94.05ms
step:17/1770 train_time:658ms step_avg:94.06ms
step:18/1770 train_time:752ms step_avg:94.03ms
step:19/1770 train_time:846ms step_avg:94.06ms
step:20/1770 train_time:941ms step_avg:94.06ms
step:21/1770 train_time:1034ms step_avg:93.99ms
step:22/1770 train_time:1127ms step_avg:93.95ms
step:23/1770 train_time:1221ms step_avg:93.94ms
step:24/1770 train_time:1315ms step_avg:93.96ms
step:25/1770 train_time:1409ms step_avg:93.95ms
step:26/1770 train_time:1503ms step_avg:93.96ms
step:27/1770 train_time:1598ms step_avg:93.97ms
step:28/1770 train_time:1692ms step_avg:93.99ms
step:29/1770 train_time:1786ms step_avg:94.02ms
step:30/1770 train_time:1881ms step_avg:94.04ms
step:31/1770 train_time:1975ms step_avg:94.06ms
step:32/1770 train_time:2069ms step_avg:94.06ms
step:33/1770 train_time:2164ms step_avg:94.11ms
step:34/1770 train_time:2258ms step_avg:94.10ms
step:35/1770 train_time:2352ms step_avg:94.07ms
step:36/1770 train_time:2446ms step_avg:94.08ms
step:37/1770 train_time:2540ms step_avg:94.07ms
step:38/1770 train_time:2634ms step_avg:94.07ms
step:39/1770 train_time:2728ms step_avg:94.06ms
step:40/1770 train_time:2822ms step_avg:94.07ms
step:41/1770 train_time:2916ms step_avg:94.07ms
step:42/1770 train_time:3010ms step_avg:94.06ms
step:43/1770 train_time:3104ms step_avg:94.05ms
step:44/1770 train_time:3198ms step_avg:94.04ms
step:45/1770 train_time:3292ms step_avg:94.05ms
step:46/1770 train_time:3385ms step_avg:94.04ms
step:47/1770 train_time:3480ms step_avg:94.04ms
step:48/1770 train_time:3574ms step_avg:94.05ms
step:49/1770 train_time:3668ms step_avg:94.04ms
step:50/1770 train_time:3761ms step_avg:94.03ms
step:51/1770 train_time:3855ms step_avg:94.03ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4044ms step_avg:94.04ms
step:54/1770 train_time:4138ms step_avg:94.04ms
step:55/1770 train_time:4232ms step_avg:94.05ms
step:56/1770 train_time:4326ms step_avg:94.04ms
step:57/1770 train_time:4420ms step_avg:94.04ms
step:58/1770 train_time:4514ms step_avg:94.05ms
step:59/1770 train_time:4608ms step_avg:94.04ms
step:60/1770 train_time:4702ms step_avg:94.04ms
step:61/1770 train_time:4796ms step_avg:94.04ms
step:62/1770 train_time:4890ms step_avg:94.04ms
step:63/1770 train_time:4984ms step_avg:94.04ms
step:64/1770 train_time:5078ms step_avg:94.03ms
step:65/1770 train_time:5171ms step_avg:94.02ms
step:66/1770 train_time:5265ms step_avg:94.01ms
step:67/1770 train_time:5360ms step_avg:94.03ms
step:68/1770 train_time:5453ms step_avg:94.02ms
step:69/1770 train_time:5547ms step_avg:94.01ms
step:70/1770 train_time:5640ms step_avg:94.00ms
step:71/1770 train_time:5734ms step_avg:94.00ms
step:72/1770 train_time:5828ms step_avg:94.00ms
step:73/1770 train_time:5922ms step_avg:94.01ms
step:74/1770 train_time:6016ms step_avg:94.01ms
step:75/1770 train_time:6110ms step_avg:94.00ms
step:76/1770 train_time:6204ms step_avg:94.00ms
step:77/1770 train_time:6298ms step_avg:94.00ms
step:78/1770 train_time:6392ms step_avg:94.01ms
step:79/1770 train_time:6486ms step_avg:94.00ms
step:80/1770 train_time:6580ms step_avg:94.00ms
step:81/1770 train_time:6674ms step_avg:94.00ms
step:82/1770 train_time:6768ms step_avg:94.00ms
step:83/1770 train_time:6862ms step_avg:94.00ms
step:84/1770 train_time:6956ms step_avg:94.00ms
step:85/1770 train_time:7050ms step_avg:94.00ms
step:86/1770 train_time:7144ms step_avg:94.00ms
step:87/1770 train_time:7238ms step_avg:93.99ms
step:88/1770 train_time:7332ms step_avg:94.00ms
step:89/1770 train_time:7426ms step_avg:93.99ms
step:90/1770 train_time:7519ms step_avg:93.99ms
step:91/1770 train_time:7613ms step_avg:93.99ms
step:92/1770 train_time:7706ms step_avg:93.98ms
step:93/1770 train_time:7800ms step_avg:93.98ms
step:94/1770 train_time:7894ms step_avg:93.98ms
step:95/1770 train_time:7988ms step_avg:93.98ms
step:96/1770 train_time:8082ms step_avg:93.98ms
step:97/1770 train_time:8176ms step_avg:93.98ms
step:98/1770 train_time:8271ms step_avg:93.98ms
step:99/1770 train_time:8364ms step_avg:93.98ms
step:100/1770 train_time:8458ms step_avg:93.98ms
step:101/1770 train_time:8552ms step_avg:93.98ms
step:102/1770 train_time:8646ms step_avg:93.98ms
step:103/1770 train_time:8741ms step_avg:93.99ms
step:104/1770 train_time:8835ms step_avg:93.99ms
step:105/1770 train_time:8928ms step_avg:93.98ms
step:106/1770 train_time:9022ms step_avg:93.98ms
step:107/1770 train_time:9116ms step_avg:93.98ms
step:108/1770 train_time:9210ms step_avg:93.98ms
step:109/1770 train_time:9304ms step_avg:93.98ms
step:110/1770 train_time:9397ms step_avg:93.97ms
step:111/1770 train_time:9493ms step_avg:93.99ms
step:112/1770 train_time:9585ms step_avg:93.97ms
step:113/1770 train_time:9680ms step_avg:93.98ms
step:114/1770 train_time:9773ms step_avg:93.97ms
step:115/1770 train_time:9867ms step_avg:93.98ms
step:116/1770 train_time:9961ms step_avg:93.97ms
step:117/1770 train_time:10055ms step_avg:93.97ms
step:118/1770 train_time:10149ms step_avg:93.97ms
step:119/1770 train_time:10243ms step_avg:93.97ms
step:120/1770 train_time:10337ms step_avg:93.97ms
step:121/1770 train_time:10430ms step_avg:93.97ms
step:122/1770 train_time:10524ms step_avg:93.97ms
step:123/1770 train_time:10619ms step_avg:93.97ms
step:124/1770 train_time:10713ms step_avg:93.98ms
step:125/1770 train_time:10807ms step_avg:93.97ms
step:125/1770 val_loss:4.6538 train_time:10899ms step_avg:94.78ms
step:126/1770 train_time:10925ms step_avg:94.18ms
step:127/1770 train_time:10998ms step_avg:94.00ms
step:128/1770 train_time:11098ms step_avg:94.05ms
step:129/1770 train_time:11203ms step_avg:94.15ms
step:130/1770 train_time:11298ms step_avg:94.15ms
step:131/1770 train_time:11392ms step_avg:94.15ms
step:132/1770 train_time:11486ms step_avg:94.15ms
step:133/1770 train_time:11579ms step_avg:94.14ms
step:134/1770 train_time:11674ms step_avg:94.14ms
step:135/1770 train_time:11768ms step_avg:94.15ms
step:136/1770 train_time:11862ms step_avg:94.14ms
step:137/1770 train_time:11957ms step_avg:94.15ms
step:138/1770 train_time:12051ms step_avg:94.15ms
step:139/1770 train_time:12147ms step_avg:94.16ms
step:140/1770 train_time:12242ms step_avg:94.17ms
step:141/1770 train_time:12337ms step_avg:94.17ms
step:142/1770 train_time:12432ms step_avg:94.18ms
step:143/1770 train_time:12526ms step_avg:94.18ms
step:144/1770 train_time:12620ms step_avg:94.18ms
step:145/1770 train_time:12714ms step_avg:94.18ms
step:146/1770 train_time:12808ms step_avg:94.18ms
step:147/1770 train_time:12903ms step_avg:94.18ms
step:148/1770 train_time:12997ms step_avg:94.18ms
step:149/1770 train_time:13092ms step_avg:94.19ms
step:150/1770 train_time:13187ms step_avg:94.19ms
step:151/1770 train_time:13282ms step_avg:94.20ms
step:152/1770 train_time:13377ms step_avg:94.20ms
step:153/1770 train_time:13471ms step_avg:94.20ms
step:154/1770 train_time:13566ms step_avg:94.21ms
step:155/1770 train_time:13660ms step_avg:94.21ms
step:156/1770 train_time:13754ms step_avg:94.21ms
step:157/1770 train_time:13849ms step_avg:94.21ms
step:158/1770 train_time:13943ms step_avg:94.21ms
step:159/1770 train_time:14037ms step_avg:94.21ms
step:160/1770 train_time:14133ms step_avg:94.22ms
step:161/1770 train_time:14227ms step_avg:94.22ms
step:162/1770 train_time:14322ms step_avg:94.23ms
step:163/1770 train_time:14417ms step_avg:94.23ms
step:164/1770 train_time:14512ms step_avg:94.23ms
step:165/1770 train_time:14606ms step_avg:94.23ms
step:166/1770 train_time:14701ms step_avg:94.24ms
step:167/1770 train_time:14795ms step_avg:94.24ms
step:168/1770 train_time:14890ms step_avg:94.24ms
step:169/1770 train_time:14984ms step_avg:94.24ms
step:170/1770 train_time:15079ms step_avg:94.25ms
step:171/1770 train_time:15174ms step_avg:94.25ms
step:172/1770 train_time:15269ms step_avg:94.25ms
step:173/1770 train_time:15363ms step_avg:94.25ms
step:174/1770 train_time:15458ms step_avg:94.25ms
step:175/1770 train_time:15553ms step_avg:94.26ms
step:176/1770 train_time:15648ms step_avg:94.26ms
step:177/1770 train_time:15743ms step_avg:94.27ms
step:178/1770 train_time:15837ms step_avg:94.27ms
step:179/1770 train_time:15932ms step_avg:94.27ms
step:180/1770 train_time:16026ms step_avg:94.27ms
step:181/1770 train_time:16120ms step_avg:94.27ms
step:182/1770 train_time:16215ms step_avg:94.27ms
step:183/1770 train_time:16309ms step_avg:94.27ms
step:184/1770 train_time:16404ms step_avg:94.28ms
step:185/1770 train_time:16498ms step_avg:94.27ms
step:186/1770 train_time:16593ms step_avg:94.28ms
step:187/1770 train_time:16687ms step_avg:94.28ms
step:188/1770 train_time:16782ms step_avg:94.28ms
step:189/1770 train_time:16876ms step_avg:94.28ms
step:190/1770 train_time:16970ms step_avg:94.28ms
step:191/1770 train_time:17065ms step_avg:94.28ms
step:192/1770 train_time:17160ms step_avg:94.28ms
step:193/1770 train_time:17254ms step_avg:94.29ms
step:194/1770 train_time:17349ms step_avg:94.29ms
step:195/1770 train_time:17444ms step_avg:94.29ms
step:196/1770 train_time:17538ms step_avg:94.29ms
step:197/1770 train_time:17632ms step_avg:94.29ms
step:198/1770 train_time:17727ms step_avg:94.29ms
step:199/1770 train_time:17821ms step_avg:94.29ms
step:200/1770 train_time:17916ms step_avg:94.29ms
step:201/1770 train_time:18010ms step_avg:94.29ms
step:202/1770 train_time:18105ms step_avg:94.30ms
step:203/1770 train_time:18199ms step_avg:94.30ms
step:204/1770 train_time:18294ms step_avg:94.30ms
step:205/1770 train_time:18388ms step_avg:94.30ms
step:206/1770 train_time:18482ms step_avg:94.30ms
step:207/1770 train_time:18577ms step_avg:94.30ms
step:208/1770 train_time:18672ms step_avg:94.30ms
step:209/1770 train_time:18766ms step_avg:94.30ms
step:210/1770 train_time:18860ms step_avg:94.30ms
step:211/1770 train_time:18954ms step_avg:94.30ms
step:212/1770 train_time:19049ms step_avg:94.30ms
step:213/1770 train_time:19144ms step_avg:94.31ms
step:214/1770 train_time:19239ms step_avg:94.31ms
step:215/1770 train_time:19333ms step_avg:94.31ms
step:216/1770 train_time:19428ms step_avg:94.31ms
step:217/1770 train_time:19522ms step_avg:94.31ms
step:218/1770 train_time:19618ms step_avg:94.32ms
step:219/1770 train_time:19713ms step_avg:94.32ms
step:220/1770 train_time:19808ms step_avg:94.32ms
step:221/1770 train_time:19903ms step_avg:94.33ms
step:222/1770 train_time:19998ms step_avg:94.33ms
step:223/1770 train_time:20093ms step_avg:94.33ms
step:224/1770 train_time:20188ms step_avg:94.33ms
step:225/1770 train_time:20282ms step_avg:94.33ms
step:226/1770 train_time:20376ms step_avg:94.33ms
step:227/1770 train_time:20470ms step_avg:94.33ms
step:228/1770 train_time:20565ms step_avg:94.33ms
step:229/1770 train_time:20660ms step_avg:94.34ms
step:230/1770 train_time:20754ms step_avg:94.34ms
step:231/1770 train_time:20849ms step_avg:94.34ms
step:232/1770 train_time:20944ms step_avg:94.34ms
step:233/1770 train_time:21039ms step_avg:94.34ms
step:234/1770 train_time:21134ms step_avg:94.35ms
step:235/1770 train_time:21228ms step_avg:94.35ms
step:236/1770 train_time:21322ms step_avg:94.35ms
step:237/1770 train_time:21417ms step_avg:94.35ms
step:238/1770 train_time:21512ms step_avg:94.35ms
step:239/1770 train_time:21606ms step_avg:94.35ms
step:240/1770 train_time:21700ms step_avg:94.35ms
step:241/1770 train_time:21794ms step_avg:94.35ms
step:242/1770 train_time:21889ms step_avg:94.35ms
step:243/1770 train_time:21984ms step_avg:94.35ms
step:244/1770 train_time:22079ms step_avg:94.35ms
step:245/1770 train_time:22174ms step_avg:94.36ms
step:246/1770 train_time:22269ms step_avg:94.36ms
step:247/1770 train_time:22364ms step_avg:94.36ms
step:248/1770 train_time:22458ms step_avg:94.36ms
step:249/1770 train_time:22553ms step_avg:94.36ms
step:250/1770 train_time:22647ms step_avg:94.36ms
step:250/1770 val_loss:4.1111 train_time:22740ms step_avg:94.75ms
step:251/1770 train_time:22761ms step_avg:94.44ms
step:252/1770 train_time:22843ms step_avg:94.39ms
step:253/1770 train_time:22946ms step_avg:94.43ms
step:254/1770 train_time:23041ms step_avg:94.43ms
step:255/1770 train_time:23137ms step_avg:94.44ms
step:256/1770 train_time:23230ms step_avg:94.43ms
step:257/1770 train_time:23324ms step_avg:94.43ms
step:258/1770 train_time:23418ms step_avg:94.43ms
step:259/1770 train_time:23512ms step_avg:94.43ms
step:260/1770 train_time:23606ms step_avg:94.43ms
step:261/1770 train_time:23700ms step_avg:94.42ms
step:262/1770 train_time:23795ms step_avg:94.42ms
step:263/1770 train_time:23890ms step_avg:94.43ms
step:264/1770 train_time:23985ms step_avg:94.43ms
step:265/1770 train_time:24081ms step_avg:94.43ms
step:266/1770 train_time:24176ms step_avg:94.44ms
step:267/1770 train_time:24272ms step_avg:94.44ms
step:268/1770 train_time:24366ms step_avg:94.44ms
step:269/1770 train_time:24461ms step_avg:94.44ms
step:270/1770 train_time:24556ms step_avg:94.45ms
step:271/1770 train_time:24651ms step_avg:94.45ms
step:272/1770 train_time:24746ms step_avg:94.45ms
step:273/1770 train_time:24841ms step_avg:94.45ms
step:274/1770 train_time:24936ms step_avg:94.45ms
step:275/1770 train_time:25031ms step_avg:94.46ms
step:276/1770 train_time:25126ms step_avg:94.46ms
step:277/1770 train_time:25221ms step_avg:94.46ms
step:278/1770 train_time:25317ms step_avg:94.47ms
step:279/1770 train_time:25412ms step_avg:94.47ms
step:280/1770 train_time:25506ms step_avg:94.47ms
step:281/1770 train_time:25602ms step_avg:94.47ms
step:282/1770 train_time:25697ms step_avg:94.48ms
step:283/1770 train_time:25793ms step_avg:94.48ms
step:284/1770 train_time:25888ms step_avg:94.48ms
step:285/1770 train_time:25983ms step_avg:94.48ms
step:286/1770 train_time:26078ms step_avg:94.49ms
step:287/1770 train_time:26173ms step_avg:94.49ms
step:288/1770 train_time:26268ms step_avg:94.49ms
step:289/1770 train_time:26363ms step_avg:94.49ms
step:290/1770 train_time:26458ms step_avg:94.49ms
step:291/1770 train_time:26553ms step_avg:94.50ms
step:292/1770 train_time:26649ms step_avg:94.50ms
step:293/1770 train_time:26744ms step_avg:94.50ms
step:294/1770 train_time:26839ms step_avg:94.50ms
step:295/1770 train_time:26934ms step_avg:94.51ms
step:296/1770 train_time:27030ms step_avg:94.51ms
step:297/1770 train_time:27125ms step_avg:94.51ms
step:298/1770 train_time:27220ms step_avg:94.51ms
step:299/1770 train_time:27315ms step_avg:94.52ms
step:300/1770 train_time:27410ms step_avg:94.52ms
step:301/1770 train_time:27506ms step_avg:94.52ms
step:302/1770 train_time:27601ms step_avg:94.52ms
step:303/1770 train_time:27696ms step_avg:94.52ms
step:304/1770 train_time:27791ms step_avg:94.53ms
step:305/1770 train_time:27885ms step_avg:94.53ms
step:306/1770 train_time:27981ms step_avg:94.53ms
step:307/1770 train_time:28076ms step_avg:94.53ms
step:308/1770 train_time:28172ms step_avg:94.54ms
step:309/1770 train_time:28267ms step_avg:94.54ms
step:310/1770 train_time:28362ms step_avg:94.54ms
step:311/1770 train_time:28458ms step_avg:94.54ms
step:312/1770 train_time:28553ms step_avg:94.55ms
step:313/1770 train_time:28648ms step_avg:94.55ms
step:314/1770 train_time:28743ms step_avg:94.55ms
step:315/1770 train_time:28838ms step_avg:94.55ms
step:316/1770 train_time:28933ms step_avg:94.55ms
step:317/1770 train_time:29028ms step_avg:94.55ms
step:318/1770 train_time:29123ms step_avg:94.55ms
step:319/1770 train_time:29218ms step_avg:94.56ms
step:320/1770 train_time:29314ms step_avg:94.56ms
step:321/1770 train_time:29409ms step_avg:94.56ms
step:322/1770 train_time:29504ms step_avg:94.56ms
step:323/1770 train_time:29600ms step_avg:94.57ms
step:324/1770 train_time:29695ms step_avg:94.57ms
step:325/1770 train_time:29790ms step_avg:94.57ms
step:326/1770 train_time:29885ms step_avg:94.57ms
step:327/1770 train_time:29980ms step_avg:94.57ms
step:328/1770 train_time:30074ms step_avg:94.57ms
step:329/1770 train_time:30170ms step_avg:94.58ms
step:330/1770 train_time:30266ms step_avg:94.58ms
step:331/1770 train_time:30361ms step_avg:94.58ms
step:332/1770 train_time:30456ms step_avg:94.58ms
step:333/1770 train_time:30551ms step_avg:94.58ms
step:334/1770 train_time:30646ms step_avg:94.59ms
step:335/1770 train_time:30742ms step_avg:94.59ms
step:336/1770 train_time:30837ms step_avg:94.59ms
step:337/1770 train_time:30932ms step_avg:94.59ms
step:338/1770 train_time:31027ms step_avg:94.59ms
step:339/1770 train_time:31121ms step_avg:94.59ms
step:340/1770 train_time:31217ms step_avg:94.60ms
step:341/1770 train_time:31312ms step_avg:94.60ms
step:342/1770 train_time:31408ms step_avg:94.60ms
step:343/1770 train_time:31502ms step_avg:94.60ms
step:344/1770 train_time:31597ms step_avg:94.60ms
step:345/1770 train_time:31692ms step_avg:94.60ms
step:346/1770 train_time:31787ms step_avg:94.60ms
step:347/1770 train_time:31883ms step_avg:94.61ms
step:348/1770 train_time:31978ms step_avg:94.61ms
step:349/1770 train_time:32073ms step_avg:94.61ms
step:350/1770 train_time:32167ms step_avg:94.61ms
step:351/1770 train_time:32263ms step_avg:94.61ms
step:352/1770 train_time:32358ms step_avg:94.61ms
step:353/1770 train_time:32453ms step_avg:94.61ms
step:354/1770 train_time:32548ms step_avg:94.62ms
step:355/1770 train_time:32643ms step_avg:94.62ms
step:356/1770 train_time:32739ms step_avg:94.62ms
step:357/1770 train_time:32834ms step_avg:94.62ms
step:358/1770 train_time:32929ms step_avg:94.62ms
step:359/1770 train_time:33024ms step_avg:94.62ms
step:360/1770 train_time:33119ms step_avg:94.62ms
step:361/1770 train_time:33214ms step_avg:94.63ms
step:362/1770 train_time:33309ms step_avg:94.63ms
step:363/1770 train_time:33404ms step_avg:94.63ms
step:364/1770 train_time:33499ms step_avg:94.63ms
step:365/1770 train_time:33594ms step_avg:94.63ms
step:366/1770 train_time:33689ms step_avg:94.63ms
step:367/1770 train_time:33785ms step_avg:94.64ms
step:368/1770 train_time:33880ms step_avg:94.64ms
step:369/1770 train_time:33975ms step_avg:94.64ms
step:370/1770 train_time:34070ms step_avg:94.64ms
step:371/1770 train_time:34166ms step_avg:94.64ms
step:372/1770 train_time:34261ms step_avg:94.64ms
step:373/1770 train_time:34356ms step_avg:94.64ms
step:374/1770 train_time:34450ms step_avg:94.64ms
step:375/1770 train_time:34545ms step_avg:94.64ms
step:375/1770 val_loss:3.9036 train_time:34639ms step_avg:94.90ms
step:376/1770 train_time:34660ms step_avg:94.70ms
step:377/1770 train_time:34745ms step_avg:94.67ms
step:378/1770 train_time:34843ms step_avg:94.68ms
step:379/1770 train_time:34939ms step_avg:94.68ms
step:380/1770 train_time:35034ms step_avg:94.69ms
step:381/1770 train_time:35128ms step_avg:94.68ms
step:382/1770 train_time:35223ms step_avg:94.69ms
step:383/1770 train_time:35318ms step_avg:94.69ms
step:384/1770 train_time:35413ms step_avg:94.69ms
step:385/1770 train_time:35508ms step_avg:94.69ms
step:386/1770 train_time:35603ms step_avg:94.69ms
step:387/1770 train_time:35698ms step_avg:94.69ms
step:388/1770 train_time:35794ms step_avg:94.69ms
step:389/1770 train_time:35890ms step_avg:94.70ms
step:390/1770 train_time:35985ms step_avg:94.70ms
step:391/1770 train_time:36080ms step_avg:94.70ms
step:392/1770 train_time:36175ms step_avg:94.70ms
step:393/1770 train_time:36270ms step_avg:94.70ms
step:394/1770 train_time:36365ms step_avg:94.70ms
step:395/1770 train_time:36460ms step_avg:94.70ms
step:396/1770 train_time:36557ms step_avg:94.71ms
step:397/1770 train_time:36654ms step_avg:94.71ms
step:398/1770 train_time:36750ms step_avg:94.72ms
step:399/1770 train_time:36847ms step_avg:94.72ms
step:400/1770 train_time:36944ms step_avg:94.73ms
step:401/1770 train_time:37042ms step_avg:94.74ms
step:402/1770 train_time:37139ms step_avg:94.74ms
step:403/1770 train_time:37237ms step_avg:94.75ms
step:404/1770 train_time:37334ms step_avg:94.76ms
step:405/1770 train_time:37431ms step_avg:94.76ms
step:406/1770 train_time:37528ms step_avg:94.77ms
step:407/1770 train_time:37625ms step_avg:94.77ms
step:408/1770 train_time:37722ms step_avg:94.78ms
step:409/1770 train_time:37819ms step_avg:94.78ms
step:410/1770 train_time:37917ms step_avg:94.79ms
step:411/1770 train_time:38015ms step_avg:94.80ms
step:412/1770 train_time:38112ms step_avg:94.81ms
step:413/1770 train_time:38208ms step_avg:94.81ms
step:414/1770 train_time:38305ms step_avg:94.81ms
step:415/1770 train_time:38402ms step_avg:94.82ms
step:416/1770 train_time:38500ms step_avg:94.83ms
step:417/1770 train_time:38597ms step_avg:94.83ms
step:418/1770 train_time:38694ms step_avg:94.84ms
step:419/1770 train_time:38790ms step_avg:94.84ms
step:420/1770 train_time:38887ms step_avg:94.85ms
step:421/1770 train_time:38984ms step_avg:94.85ms
step:422/1770 train_time:39081ms step_avg:94.86ms
step:423/1770 train_time:39178ms step_avg:94.86ms
step:424/1770 train_time:39275ms step_avg:94.87ms
step:425/1770 train_time:39372ms step_avg:94.87ms
step:426/1770 train_time:39469ms step_avg:94.88ms
step:427/1770 train_time:39566ms step_avg:94.88ms
step:428/1770 train_time:39663ms step_avg:94.89ms
step:429/1770 train_time:39761ms step_avg:94.89ms
step:430/1770 train_time:39858ms step_avg:94.90ms
step:431/1770 train_time:39955ms step_avg:94.91ms
step:432/1770 train_time:40052ms step_avg:94.91ms
step:433/1770 train_time:40149ms step_avg:94.92ms
step:434/1770 train_time:40246ms step_avg:94.92ms
step:435/1770 train_time:40343ms step_avg:94.93ms
step:436/1770 train_time:40440ms step_avg:94.93ms
step:437/1770 train_time:40538ms step_avg:94.94ms
step:438/1770 train_time:40635ms step_avg:94.94ms
step:439/1770 train_time:40732ms step_avg:94.95ms
step:440/1770 train_time:40830ms step_avg:94.95ms
step:441/1770 train_time:40927ms step_avg:94.96ms
step:442/1770 train_time:41024ms step_avg:94.96ms
step:443/1770 train_time:41122ms step_avg:94.97ms
step:444/1770 train_time:41219ms step_avg:94.97ms
step:445/1770 train_time:41316ms step_avg:94.98ms
step:446/1770 train_time:41414ms step_avg:94.99ms
step:447/1770 train_time:41511ms step_avg:94.99ms
step:448/1770 train_time:41607ms step_avg:94.99ms
step:449/1770 train_time:41704ms step_avg:95.00ms
step:450/1770 train_time:41802ms step_avg:95.00ms
step:451/1770 train_time:41900ms step_avg:95.01ms
step:452/1770 train_time:41997ms step_avg:95.02ms
step:453/1770 train_time:42094ms step_avg:95.02ms
step:454/1770 train_time:42190ms step_avg:95.02ms
step:455/1770 train_time:42288ms step_avg:95.03ms
step:456/1770 train_time:42385ms step_avg:95.03ms
step:457/1770 train_time:42482ms step_avg:95.04ms
step:458/1770 train_time:42579ms step_avg:95.04ms
step:459/1770 train_time:42676ms step_avg:95.05ms
step:460/1770 train_time:42773ms step_avg:95.05ms
step:461/1770 train_time:42869ms step_avg:95.05ms
step:462/1770 train_time:42966ms step_avg:95.06ms
step:463/1770 train_time:43064ms step_avg:95.06ms
step:464/1770 train_time:43162ms step_avg:95.07ms
step:465/1770 train_time:43259ms step_avg:95.07ms
step:466/1770 train_time:43356ms step_avg:95.08ms
step:467/1770 train_time:43453ms step_avg:95.08ms
step:468/1770 train_time:43550ms step_avg:95.09ms
step:469/1770 train_time:43647ms step_avg:95.09ms
step:470/1770 train_time:43744ms step_avg:95.10ms
step:471/1770 train_time:43841ms step_avg:95.10ms
step:472/1770 train_time:43938ms step_avg:95.10ms
step:473/1770 train_time:44035ms step_avg:95.11ms
step:474/1770 train_time:44132ms step_avg:95.11ms
step:475/1770 train_time:44229ms step_avg:95.12ms
step:476/1770 train_time:44326ms step_avg:95.12ms
step:477/1770 train_time:44422ms step_avg:95.12ms
step:478/1770 train_time:44519ms step_avg:95.13ms
step:479/1770 train_time:44616ms step_avg:95.13ms
step:480/1770 train_time:44713ms step_avg:95.13ms
step:481/1770 train_time:44810ms step_avg:95.14ms
step:482/1770 train_time:44907ms step_avg:95.14ms
step:483/1770 train_time:45004ms step_avg:95.15ms
step:484/1770 train_time:45101ms step_avg:95.15ms
step:485/1770 train_time:45198ms step_avg:95.15ms
step:486/1770 train_time:45297ms step_avg:95.16ms
step:487/1770 train_time:45394ms step_avg:95.16ms
step:488/1770 train_time:45491ms step_avg:95.17ms
step:489/1770 train_time:45588ms step_avg:95.17ms
step:490/1770 train_time:45685ms step_avg:95.18ms
step:491/1770 train_time:45782ms step_avg:95.18ms
step:492/1770 train_time:45880ms step_avg:95.19ms
step:493/1770 train_time:45977ms step_avg:95.19ms
step:494/1770 train_time:46073ms step_avg:95.19ms
step:495/1770 train_time:46170ms step_avg:95.20ms
step:496/1770 train_time:46267ms step_avg:95.20ms
step:497/1770 train_time:46364ms step_avg:95.20ms
step:498/1770 train_time:46461ms step_avg:95.21ms
step:499/1770 train_time:46558ms step_avg:95.21ms
step:500/1770 train_time:46655ms step_avg:95.21ms
step:500/1770 val_loss:3.7519 train_time:46751ms step_avg:95.41ms
step:501/1770 train_time:46772ms step_avg:95.26ms
step:502/1770 train_time:46859ms step_avg:95.24ms
step:503/1770 train_time:46959ms step_avg:95.25ms
step:504/1770 train_time:47057ms step_avg:95.26ms
step:505/1770 train_time:47154ms step_avg:95.26ms
step:506/1770 train_time:47251ms step_avg:95.26ms
step:507/1770 train_time:47348ms step_avg:95.27ms
step:508/1770 train_time:47444ms step_avg:95.27ms
step:509/1770 train_time:47541ms step_avg:95.27ms
step:510/1770 train_time:47637ms step_avg:95.27ms
step:511/1770 train_time:47733ms step_avg:95.28ms
step:512/1770 train_time:47831ms step_avg:95.28ms
step:513/1770 train_time:47929ms step_avg:95.29ms
step:514/1770 train_time:48027ms step_avg:95.29ms
step:515/1770 train_time:48124ms step_avg:95.30ms
step:516/1770 train_time:48221ms step_avg:95.30ms
step:517/1770 train_time:48318ms step_avg:95.30ms
step:518/1770 train_time:48415ms step_avg:95.30ms
step:519/1770 train_time:48511ms step_avg:95.31ms
step:520/1770 train_time:48608ms step_avg:95.31ms
step:521/1770 train_time:48705ms step_avg:95.31ms
step:522/1770 train_time:48802ms step_avg:95.32ms
step:523/1770 train_time:48899ms step_avg:95.32ms
step:524/1770 train_time:48996ms step_avg:95.32ms
step:525/1770 train_time:49093ms step_avg:95.33ms
step:526/1770 train_time:49190ms step_avg:95.33ms
step:527/1770 train_time:49288ms step_avg:95.33ms
step:528/1770 train_time:49385ms step_avg:95.34ms
step:529/1770 train_time:49481ms step_avg:95.34ms
step:530/1770 train_time:49578ms step_avg:95.34ms
step:531/1770 train_time:49676ms step_avg:95.35ms
step:532/1770 train_time:49773ms step_avg:95.35ms
step:533/1770 train_time:49870ms step_avg:95.35ms
step:534/1770 train_time:49968ms step_avg:95.36ms
step:535/1770 train_time:50065ms step_avg:95.36ms
step:536/1770 train_time:50162ms step_avg:95.36ms
step:537/1770 train_time:50259ms step_avg:95.37ms
step:538/1770 train_time:50356ms step_avg:95.37ms
step:539/1770 train_time:50455ms step_avg:95.38ms
step:540/1770 train_time:50552ms step_avg:95.38ms
step:541/1770 train_time:50649ms step_avg:95.38ms
step:542/1770 train_time:50746ms step_avg:95.39ms
step:543/1770 train_time:50843ms step_avg:95.39ms
step:544/1770 train_time:50941ms step_avg:95.40ms
step:545/1770 train_time:51038ms step_avg:95.40ms
step:546/1770 train_time:51135ms step_avg:95.40ms
step:547/1770 train_time:51233ms step_avg:95.41ms
step:548/1770 train_time:51331ms step_avg:95.41ms
step:549/1770 train_time:51428ms step_avg:95.41ms
step:550/1770 train_time:51525ms step_avg:95.42ms
step:551/1770 train_time:51622ms step_avg:95.42ms
step:552/1770 train_time:51719ms step_avg:95.42ms
step:553/1770 train_time:51817ms step_avg:95.43ms
step:554/1770 train_time:51914ms step_avg:95.43ms
step:555/1770 train_time:52011ms step_avg:95.43ms
step:556/1770 train_time:52109ms step_avg:95.44ms
step:557/1770 train_time:52206ms step_avg:95.44ms
step:558/1770 train_time:52304ms step_avg:95.44ms
step:559/1770 train_time:52401ms step_avg:95.45ms
step:560/1770 train_time:52499ms step_avg:95.45ms
step:561/1770 train_time:52596ms step_avg:95.46ms
step:562/1770 train_time:52694ms step_avg:95.46ms
step:563/1770 train_time:52792ms step_avg:95.46ms
step:564/1770 train_time:52889ms step_avg:95.47ms
step:565/1770 train_time:52986ms step_avg:95.47ms
step:566/1770 train_time:53083ms step_avg:95.47ms
step:567/1770 train_time:53180ms step_avg:95.48ms
step:568/1770 train_time:53277ms step_avg:95.48ms
step:569/1770 train_time:53374ms step_avg:95.48ms
step:570/1770 train_time:53472ms step_avg:95.48ms
step:571/1770 train_time:53569ms step_avg:95.49ms
step:572/1770 train_time:53667ms step_avg:95.49ms
step:573/1770 train_time:53764ms step_avg:95.50ms
step:574/1770 train_time:53862ms step_avg:95.50ms
step:575/1770 train_time:53959ms step_avg:95.50ms
step:576/1770 train_time:54056ms step_avg:95.50ms
step:577/1770 train_time:54153ms step_avg:95.51ms
step:578/1770 train_time:54250ms step_avg:95.51ms
step:579/1770 train_time:54347ms step_avg:95.51ms
step:580/1770 train_time:54445ms step_avg:95.52ms
step:581/1770 train_time:54542ms step_avg:95.52ms
step:582/1770 train_time:54640ms step_avg:95.52ms
step:583/1770 train_time:54737ms step_avg:95.53ms
step:584/1770 train_time:54835ms step_avg:95.53ms
step:585/1770 train_time:54932ms step_avg:95.53ms
step:586/1770 train_time:55029ms step_avg:95.54ms
step:587/1770 train_time:55127ms step_avg:95.54ms
step:588/1770 train_time:55224ms step_avg:95.54ms
step:589/1770 train_time:55321ms step_avg:95.55ms
step:590/1770 train_time:55419ms step_avg:95.55ms
step:591/1770 train_time:55516ms step_avg:95.55ms
step:592/1770 train_time:55613ms step_avg:95.56ms
step:593/1770 train_time:55711ms step_avg:95.56ms
step:594/1770 train_time:55808ms step_avg:95.56ms
step:595/1770 train_time:55906ms step_avg:95.57ms
step:596/1770 train_time:56003ms step_avg:95.57ms
step:597/1770 train_time:56101ms step_avg:95.57ms
step:598/1770 train_time:56198ms step_avg:95.58ms
step:599/1770 train_time:56296ms step_avg:95.58ms
step:600/1770 train_time:56394ms step_avg:95.58ms
step:601/1770 train_time:56492ms step_avg:95.59ms
step:602/1770 train_time:56589ms step_avg:95.59ms
step:603/1770 train_time:56685ms step_avg:95.59ms
step:604/1770 train_time:56783ms step_avg:95.59ms
step:605/1770 train_time:56880ms step_avg:95.60ms
step:606/1770 train_time:56978ms step_avg:95.60ms
step:607/1770 train_time:57076ms step_avg:95.60ms
step:608/1770 train_time:57173ms step_avg:95.61ms
step:609/1770 train_time:57271ms step_avg:95.61ms
step:610/1770 train_time:57368ms step_avg:95.61ms
step:611/1770 train_time:57465ms step_avg:95.62ms
step:612/1770 train_time:57562ms step_avg:95.62ms
step:613/1770 train_time:57659ms step_avg:95.62ms
step:614/1770 train_time:57757ms step_avg:95.62ms
step:615/1770 train_time:57855ms step_avg:95.63ms
step:616/1770 train_time:57952ms step_avg:95.63ms
step:617/1770 train_time:58050ms step_avg:95.63ms
step:618/1770 train_time:58147ms step_avg:95.64ms
step:619/1770 train_time:58244ms step_avg:95.64ms
step:620/1770 train_time:58341ms step_avg:95.64ms
step:621/1770 train_time:58439ms step_avg:95.64ms
step:622/1770 train_time:58536ms step_avg:95.65ms
step:623/1770 train_time:58633ms step_avg:95.65ms
step:624/1770 train_time:58731ms step_avg:95.65ms
step:625/1770 train_time:58828ms step_avg:95.66ms
step:625/1770 val_loss:3.6639 train_time:58924ms step_avg:95.81ms
step:626/1770 train_time:58945ms step_avg:95.69ms
step:627/1770 train_time:59033ms step_avg:95.68ms
step:628/1770 train_time:59135ms step_avg:95.69ms
step:629/1770 train_time:59233ms step_avg:95.69ms
step:630/1770 train_time:59330ms step_avg:95.69ms
step:631/1770 train_time:59428ms step_avg:95.70ms
step:632/1770 train_time:59525ms step_avg:95.70ms
step:633/1770 train_time:59622ms step_avg:95.70ms
step:634/1770 train_time:59719ms step_avg:95.70ms
step:635/1770 train_time:59816ms step_avg:95.71ms
step:636/1770 train_time:59912ms step_avg:95.71ms
step:637/1770 train_time:60011ms step_avg:95.71ms
step:638/1770 train_time:60109ms step_avg:95.71ms
step:639/1770 train_time:60207ms step_avg:95.72ms
step:640/1770 train_time:60304ms step_avg:95.72ms
step:641/1770 train_time:60402ms step_avg:95.72ms
step:642/1770 train_time:60498ms step_avg:95.72ms
step:643/1770 train_time:60595ms step_avg:95.73ms
step:644/1770 train_time:60692ms step_avg:95.73ms
step:645/1770 train_time:60789ms step_avg:95.73ms
step:646/1770 train_time:60886ms step_avg:95.73ms
step:647/1770 train_time:60984ms step_avg:95.74ms
step:648/1770 train_time:61081ms step_avg:95.74ms
step:649/1770 train_time:61179ms step_avg:95.74ms
step:650/1770 train_time:61276ms step_avg:95.74ms
step:651/1770 train_time:61374ms step_avg:95.75ms
step:652/1770 train_time:61471ms step_avg:95.75ms
step:653/1770 train_time:61569ms step_avg:95.75ms
step:654/1770 train_time:61665ms step_avg:95.75ms
step:655/1770 train_time:61763ms step_avg:95.76ms
step:656/1770 train_time:61860ms step_avg:95.76ms
step:657/1770 train_time:61957ms step_avg:95.76ms
step:658/1770 train_time:62056ms step_avg:95.77ms
step:659/1770 train_time:62155ms step_avg:95.77ms
step:660/1770 train_time:62255ms step_avg:95.78ms
step:661/1770 train_time:62355ms step_avg:95.78ms
step:662/1770 train_time:62455ms step_avg:95.79ms
step:663/1770 train_time:62554ms step_avg:95.79ms
step:664/1770 train_time:62654ms step_avg:95.80ms
step:665/1770 train_time:62753ms step_avg:95.81ms
step:666/1770 train_time:62852ms step_avg:95.81ms
step:667/1770 train_time:62951ms step_avg:95.82ms
step:668/1770 train_time:63050ms step_avg:95.82ms
step:669/1770 train_time:63150ms step_avg:95.83ms
step:670/1770 train_time:63249ms step_avg:95.83ms
step:671/1770 train_time:63348ms step_avg:95.84ms
step:672/1770 train_time:63448ms step_avg:95.84ms
step:673/1770 train_time:63546ms step_avg:95.85ms
step:674/1770 train_time:63645ms step_avg:95.85ms
step:675/1770 train_time:63744ms step_avg:95.86ms
step:676/1770 train_time:63843ms step_avg:95.86ms
step:677/1770 train_time:63943ms step_avg:95.87ms
step:678/1770 train_time:64042ms step_avg:95.87ms
step:679/1770 train_time:64141ms step_avg:95.88ms
step:680/1770 train_time:64241ms step_avg:95.88ms
step:681/1770 train_time:64340ms step_avg:95.89ms
step:682/1770 train_time:64440ms step_avg:95.89ms
step:683/1770 train_time:64539ms step_avg:95.90ms
step:684/1770 train_time:64638ms step_avg:95.90ms
step:685/1770 train_time:64736ms step_avg:95.91ms
step:686/1770 train_time:64835ms step_avg:95.91ms
step:687/1770 train_time:64935ms step_avg:95.92ms
step:688/1770 train_time:65035ms step_avg:95.92ms
step:689/1770 train_time:65134ms step_avg:95.93ms
step:690/1770 train_time:65234ms step_avg:95.93ms
step:691/1770 train_time:65333ms step_avg:95.94ms
step:692/1770 train_time:65432ms step_avg:95.94ms
step:693/1770 train_time:65531ms step_avg:95.95ms
step:694/1770 train_time:65631ms step_avg:95.95ms
step:695/1770 train_time:65730ms step_avg:95.96ms
step:696/1770 train_time:65829ms step_avg:95.96ms
step:697/1770 train_time:65928ms step_avg:95.97ms
step:698/1770 train_time:66028ms step_avg:95.97ms
step:699/1770 train_time:66127ms step_avg:95.98ms
step:700/1770 train_time:66226ms step_avg:95.98ms
step:701/1770 train_time:66326ms step_avg:95.98ms
step:702/1770 train_time:66425ms step_avg:95.99ms
step:703/1770 train_time:66524ms step_avg:95.99ms
step:704/1770 train_time:66623ms step_avg:96.00ms
step:705/1770 train_time:66723ms step_avg:96.00ms
step:706/1770 train_time:66822ms step_avg:96.01ms
step:707/1770 train_time:66922ms step_avg:96.01ms
step:708/1770 train_time:67021ms step_avg:96.02ms
step:709/1770 train_time:67120ms step_avg:96.02ms
step:710/1770 train_time:67219ms step_avg:96.03ms
step:711/1770 train_time:67318ms step_avg:96.03ms
step:712/1770 train_time:67418ms step_avg:96.04ms
step:713/1770 train_time:67517ms step_avg:96.04ms
step:714/1770 train_time:67617ms step_avg:96.05ms
step:715/1770 train_time:67716ms step_avg:96.05ms
step:716/1770 train_time:67815ms step_avg:96.06ms
step:717/1770 train_time:67914ms step_avg:96.06ms
step:718/1770 train_time:68015ms step_avg:96.07ms
step:719/1770 train_time:68115ms step_avg:96.07ms
step:720/1770 train_time:68215ms step_avg:96.08ms
step:721/1770 train_time:68314ms step_avg:96.08ms
step:722/1770 train_time:68413ms step_avg:96.09ms
step:723/1770 train_time:68512ms step_avg:96.09ms
step:724/1770 train_time:68611ms step_avg:96.09ms
step:725/1770 train_time:68710ms step_avg:96.10ms
step:726/1770 train_time:68810ms step_avg:96.10ms
step:727/1770 train_time:68909ms step_avg:96.11ms
step:728/1770 train_time:69007ms step_avg:96.11ms
step:729/1770 train_time:69106ms step_avg:96.11ms
step:730/1770 train_time:69205ms step_avg:96.12ms
step:731/1770 train_time:69304ms step_avg:96.12ms
step:732/1770 train_time:69403ms step_avg:96.13ms
step:733/1770 train_time:69502ms step_avg:96.13ms
step:734/1770 train_time:69601ms step_avg:96.13ms
step:735/1770 train_time:69701ms step_avg:96.14ms
step:736/1770 train_time:69801ms step_avg:96.14ms
step:737/1770 train_time:69901ms step_avg:96.15ms
step:738/1770 train_time:69999ms step_avg:96.15ms
step:739/1770 train_time:70099ms step_avg:96.16ms
step:740/1770 train_time:70198ms step_avg:96.16ms
step:741/1770 train_time:70297ms step_avg:96.17ms
step:742/1770 train_time:70395ms step_avg:96.17ms
step:743/1770 train_time:70494ms step_avg:96.17ms
step:744/1770 train_time:70594ms step_avg:96.18ms
step:745/1770 train_time:70693ms step_avg:96.18ms
step:746/1770 train_time:70792ms step_avg:96.18ms
step:747/1770 train_time:70890ms step_avg:96.19ms
step:748/1770 train_time:70990ms step_avg:96.19ms
step:749/1770 train_time:71089ms step_avg:96.20ms
step:750/1770 train_time:71188ms step_avg:96.20ms
step:750/1770 val_loss:3.5994 train_time:71285ms step_avg:96.33ms
step:751/1770 train_time:71307ms step_avg:96.23ms
step:752/1770 train_time:71400ms step_avg:96.23ms
step:753/1770 train_time:71501ms step_avg:96.23ms
step:754/1770 train_time:71600ms step_avg:96.24ms
step:755/1770 train_time:71699ms step_avg:96.24ms
step:756/1770 train_time:71798ms step_avg:96.24ms
step:757/1770 train_time:71897ms step_avg:96.25ms
step:758/1770 train_time:71996ms step_avg:96.25ms
step:759/1770 train_time:72095ms step_avg:96.25ms
step:760/1770 train_time:72193ms step_avg:96.26ms
step:761/1770 train_time:72293ms step_avg:96.26ms
step:762/1770 train_time:72393ms step_avg:96.27ms
step:763/1770 train_time:72493ms step_avg:96.27ms
step:764/1770 train_time:72592ms step_avg:96.28ms
step:765/1770 train_time:72692ms step_avg:96.28ms
step:766/1770 train_time:72791ms step_avg:96.28ms
step:767/1770 train_time:72891ms step_avg:96.29ms
step:768/1770 train_time:72990ms step_avg:96.29ms
step:769/1770 train_time:73089ms step_avg:96.30ms
step:770/1770 train_time:73188ms step_avg:96.30ms
step:771/1770 train_time:73287ms step_avg:96.30ms
step:772/1770 train_time:73386ms step_avg:96.31ms
step:773/1770 train_time:73485ms step_avg:96.31ms
step:774/1770 train_time:73584ms step_avg:96.31ms
step:775/1770 train_time:73683ms step_avg:96.32ms
step:776/1770 train_time:73782ms step_avg:96.32ms
step:777/1770 train_time:73881ms step_avg:96.32ms
step:778/1770 train_time:73981ms step_avg:96.33ms
step:779/1770 train_time:74080ms step_avg:96.33ms
step:780/1770 train_time:74179ms step_avg:96.34ms
step:781/1770 train_time:74278ms step_avg:96.34ms
step:782/1770 train_time:74377ms step_avg:96.34ms
step:783/1770 train_time:74476ms step_avg:96.35ms
step:784/1770 train_time:74576ms step_avg:96.35ms
step:785/1770 train_time:74676ms step_avg:96.36ms
step:786/1770 train_time:74776ms step_avg:96.36ms
step:787/1770 train_time:74876ms step_avg:96.36ms
step:788/1770 train_time:74975ms step_avg:96.37ms
step:789/1770 train_time:75075ms step_avg:96.37ms
step:790/1770 train_time:75175ms step_avg:96.38ms
step:791/1770 train_time:75274ms step_avg:96.38ms
step:792/1770 train_time:75373ms step_avg:96.38ms
step:793/1770 train_time:75473ms step_avg:96.39ms
step:794/1770 train_time:75572ms step_avg:96.39ms
step:795/1770 train_time:75672ms step_avg:96.40ms
step:796/1770 train_time:75772ms step_avg:96.40ms
step:797/1770 train_time:75872ms step_avg:96.41ms
step:798/1770 train_time:75972ms step_avg:96.41ms
step:799/1770 train_time:76072ms step_avg:96.42ms
step:800/1770 train_time:76172ms step_avg:96.42ms
step:801/1770 train_time:76271ms step_avg:96.42ms
step:802/1770 train_time:76370ms step_avg:96.43ms
step:803/1770 train_time:76469ms step_avg:96.43ms
step:804/1770 train_time:76569ms step_avg:96.43ms
step:805/1770 train_time:76668ms step_avg:96.44ms
step:806/1770 train_time:76767ms step_avg:96.44ms
step:807/1770 train_time:76867ms step_avg:96.45ms
step:808/1770 train_time:76967ms step_avg:96.45ms
step:809/1770 train_time:77067ms step_avg:96.45ms
step:810/1770 train_time:77167ms step_avg:96.46ms
step:811/1770 train_time:77267ms step_avg:96.46ms
step:812/1770 train_time:77366ms step_avg:96.47ms
step:813/1770 train_time:77466ms step_avg:96.47ms
step:814/1770 train_time:77565ms step_avg:96.47ms
step:815/1770 train_time:77665ms step_avg:96.48ms
step:816/1770 train_time:77765ms step_avg:96.48ms
step:817/1770 train_time:77864ms step_avg:96.49ms
step:818/1770 train_time:77964ms step_avg:96.49ms
step:819/1770 train_time:78064ms step_avg:96.49ms
step:820/1770 train_time:78163ms step_avg:96.50ms
step:821/1770 train_time:78263ms step_avg:96.50ms
step:822/1770 train_time:78362ms step_avg:96.50ms
step:823/1770 train_time:78461ms step_avg:96.51ms
step:824/1770 train_time:78560ms step_avg:96.51ms
step:825/1770 train_time:78659ms step_avg:96.51ms
step:826/1770 train_time:78758ms step_avg:96.52ms
step:827/1770 train_time:78857ms step_avg:96.52ms
step:828/1770 train_time:78956ms step_avg:96.52ms
step:829/1770 train_time:79056ms step_avg:96.53ms
step:830/1770 train_time:79156ms step_avg:96.53ms
step:831/1770 train_time:79256ms step_avg:96.54ms
step:832/1770 train_time:79356ms step_avg:96.54ms
step:833/1770 train_time:79457ms step_avg:96.55ms
step:834/1770 train_time:79557ms step_avg:96.55ms
step:835/1770 train_time:79655ms step_avg:96.55ms
step:836/1770 train_time:79755ms step_avg:96.56ms
step:837/1770 train_time:79855ms step_avg:96.56ms
step:838/1770 train_time:79955ms step_avg:96.56ms
step:839/1770 train_time:80055ms step_avg:96.57ms
step:840/1770 train_time:80155ms step_avg:96.57ms
step:841/1770 train_time:80254ms step_avg:96.58ms
step:842/1770 train_time:80354ms step_avg:96.58ms
step:843/1770 train_time:80454ms step_avg:96.58ms
step:844/1770 train_time:80553ms step_avg:96.59ms
step:845/1770 train_time:80653ms step_avg:96.59ms
step:846/1770 train_time:80752ms step_avg:96.59ms
step:847/1770 train_time:80852ms step_avg:96.60ms
step:848/1770 train_time:80951ms step_avg:96.60ms
step:849/1770 train_time:81051ms step_avg:96.60ms
step:850/1770 train_time:81150ms step_avg:96.61ms
step:851/1770 train_time:81249ms step_avg:96.61ms
step:852/1770 train_time:81349ms step_avg:96.61ms
step:853/1770 train_time:81449ms step_avg:96.62ms
step:854/1770 train_time:81549ms step_avg:96.62ms
step:855/1770 train_time:81648ms step_avg:96.62ms
step:856/1770 train_time:81747ms step_avg:96.63ms
step:857/1770 train_time:81847ms step_avg:96.63ms
step:858/1770 train_time:81947ms step_avg:96.64ms
step:859/1770 train_time:82047ms step_avg:96.64ms
step:860/1770 train_time:82147ms step_avg:96.64ms
step:861/1770 train_time:82246ms step_avg:96.65ms
step:862/1770 train_time:82346ms step_avg:96.65ms
step:863/1770 train_time:82445ms step_avg:96.65ms
step:864/1770 train_time:82544ms step_avg:96.66ms
step:865/1770 train_time:82644ms step_avg:96.66ms
step:866/1770 train_time:82743ms step_avg:96.66ms
step:867/1770 train_time:82842ms step_avg:96.67ms
step:868/1770 train_time:82941ms step_avg:96.67ms
step:869/1770 train_time:83040ms step_avg:96.67ms
step:870/1770 train_time:83139ms step_avg:96.67ms
step:871/1770 train_time:83239ms step_avg:96.68ms
step:872/1770 train_time:83338ms step_avg:96.68ms
step:873/1770 train_time:83438ms step_avg:96.68ms
step:874/1770 train_time:83538ms step_avg:96.69ms
step:875/1770 train_time:83638ms step_avg:96.69ms
step:875/1770 val_loss:3.5505 train_time:83736ms step_avg:96.80ms
step:876/1770 train_time:83757ms step_avg:96.72ms
step:877/1770 train_time:83849ms step_avg:96.71ms
step:878/1770 train_time:83950ms step_avg:96.72ms
step:879/1770 train_time:84050ms step_avg:96.72ms
step:880/1770 train_time:84149ms step_avg:96.72ms
step:881/1770 train_time:84248ms step_avg:96.73ms
step:882/1770 train_time:84348ms step_avg:96.73ms
step:883/1770 train_time:84447ms step_avg:96.73ms
step:884/1770 train_time:84546ms step_avg:96.73ms
step:885/1770 train_time:84645ms step_avg:96.74ms
step:886/1770 train_time:84744ms step_avg:96.74ms
step:887/1770 train_time:84845ms step_avg:96.74ms
step:888/1770 train_time:84946ms step_avg:96.75ms
step:889/1770 train_time:85046ms step_avg:96.75ms
step:890/1770 train_time:85146ms step_avg:96.76ms
step:891/1770 train_time:85246ms step_avg:96.76ms
step:892/1770 train_time:85346ms step_avg:96.76ms
step:893/1770 train_time:85445ms step_avg:96.77ms
step:894/1770 train_time:85545ms step_avg:96.77ms
step:895/1770 train_time:85644ms step_avg:96.77ms
step:896/1770 train_time:85743ms step_avg:96.78ms
step:897/1770 train_time:85843ms step_avg:96.78ms
step:898/1770 train_time:85943ms step_avg:96.78ms
step:899/1770 train_time:86043ms step_avg:96.79ms
step:900/1770 train_time:86143ms step_avg:96.79ms
step:901/1770 train_time:86242ms step_avg:96.79ms
step:902/1770 train_time:86342ms step_avg:96.80ms
step:903/1770 train_time:86441ms step_avg:96.80ms
step:904/1770 train_time:86541ms step_avg:96.80ms
step:905/1770 train_time:86640ms step_avg:96.80ms
step:906/1770 train_time:86739ms step_avg:96.81ms
step:907/1770 train_time:86838ms step_avg:96.81ms
step:908/1770 train_time:86938ms step_avg:96.81ms
step:909/1770 train_time:87038ms step_avg:96.82ms
step:910/1770 train_time:87137ms step_avg:96.82ms
step:911/1770 train_time:87237ms step_avg:96.82ms
step:912/1770 train_time:87336ms step_avg:96.82ms
step:913/1770 train_time:87434ms step_avg:96.83ms
step:914/1770 train_time:87534ms step_avg:96.83ms
step:915/1770 train_time:87633ms step_avg:96.83ms
step:916/1770 train_time:87732ms step_avg:96.83ms
step:917/1770 train_time:87831ms step_avg:96.84ms
step:918/1770 train_time:87930ms step_avg:96.84ms
step:919/1770 train_time:88030ms step_avg:96.84ms
step:920/1770 train_time:88132ms step_avg:96.85ms
step:921/1770 train_time:88233ms step_avg:96.85ms
step:922/1770 train_time:88333ms step_avg:96.86ms
step:923/1770 train_time:88434ms step_avg:96.86ms
step:924/1770 train_time:88534ms step_avg:96.86ms
step:925/1770 train_time:88635ms step_avg:96.87ms
step:926/1770 train_time:88735ms step_avg:96.87ms
step:927/1770 train_time:88835ms step_avg:96.88ms
step:928/1770 train_time:88935ms step_avg:96.88ms
step:929/1770 train_time:89035ms step_avg:96.88ms
step:930/1770 train_time:89136ms step_avg:96.89ms
step:931/1770 train_time:89236ms step_avg:96.89ms
step:932/1770 train_time:89336ms step_avg:96.89ms
step:933/1770 train_time:89437ms step_avg:96.90ms
step:934/1770 train_time:89538ms step_avg:96.90ms
step:935/1770 train_time:89639ms step_avg:96.91ms
step:936/1770 train_time:89739ms step_avg:96.91ms
step:937/1770 train_time:89840ms step_avg:96.91ms
step:938/1770 train_time:89941ms step_avg:96.92ms
step:939/1770 train_time:90042ms step_avg:96.92ms
step:940/1770 train_time:90142ms step_avg:96.93ms
step:941/1770 train_time:90243ms step_avg:96.93ms
step:942/1770 train_time:90344ms step_avg:96.94ms
step:943/1770 train_time:90446ms step_avg:96.94ms
step:944/1770 train_time:90546ms step_avg:96.94ms
step:945/1770 train_time:90647ms step_avg:96.95ms
step:946/1770 train_time:90750ms step_avg:96.95ms
step:947/1770 train_time:90852ms step_avg:96.96ms
step:948/1770 train_time:90953ms step_avg:96.96ms
step:949/1770 train_time:91054ms step_avg:96.97ms
step:950/1770 train_time:91154ms step_avg:96.97ms
step:951/1770 train_time:91255ms step_avg:96.98ms
step:952/1770 train_time:91355ms step_avg:96.98ms
step:953/1770 train_time:91456ms step_avg:96.98ms
step:954/1770 train_time:91556ms step_avg:96.99ms
step:955/1770 train_time:91656ms step_avg:96.99ms
step:956/1770 train_time:91756ms step_avg:96.99ms
step:957/1770 train_time:91857ms step_avg:97.00ms
step:958/1770 train_time:91957ms step_avg:97.00ms
step:959/1770 train_time:92059ms step_avg:97.01ms
step:960/1770 train_time:92160ms step_avg:97.01ms
step:961/1770 train_time:92260ms step_avg:97.01ms
step:962/1770 train_time:92362ms step_avg:97.02ms
step:963/1770 train_time:92462ms step_avg:97.02ms
step:964/1770 train_time:92563ms step_avg:97.03ms
step:965/1770 train_time:92665ms step_avg:97.03ms
step:966/1770 train_time:92766ms step_avg:97.04ms
step:967/1770 train_time:92867ms step_avg:97.04ms
step:968/1770 train_time:92968ms step_avg:97.04ms
step:969/1770 train_time:93069ms step_avg:97.05ms
step:970/1770 train_time:93170ms step_avg:97.05ms
step:971/1770 train_time:93270ms step_avg:97.06ms
step:972/1770 train_time:93372ms step_avg:97.06ms
step:973/1770 train_time:93472ms step_avg:97.06ms
step:974/1770 train_time:93574ms step_avg:97.07ms
step:975/1770 train_time:93674ms step_avg:97.07ms
step:976/1770 train_time:93775ms step_avg:97.08ms
step:977/1770 train_time:93875ms step_avg:97.08ms
step:978/1770 train_time:93975ms step_avg:97.08ms
step:979/1770 train_time:94075ms step_avg:97.08ms
step:980/1770 train_time:94176ms step_avg:97.09ms
step:981/1770 train_time:94276ms step_avg:97.09ms
step:982/1770 train_time:94376ms step_avg:97.09ms
step:983/1770 train_time:94477ms step_avg:97.10ms
step:984/1770 train_time:94577ms step_avg:97.10ms
step:985/1770 train_time:94678ms step_avg:97.11ms
step:986/1770 train_time:94779ms step_avg:97.11ms
step:987/1770 train_time:94880ms step_avg:97.11ms
step:988/1770 train_time:94980ms step_avg:97.12ms
step:989/1770 train_time:95083ms step_avg:97.12ms
step:990/1770 train_time:95183ms step_avg:97.13ms
step:991/1770 train_time:95285ms step_avg:97.13ms
step:992/1770 train_time:95387ms step_avg:97.14ms
step:993/1770 train_time:95488ms step_avg:97.14ms
step:994/1770 train_time:95590ms step_avg:97.14ms
step:995/1770 train_time:95690ms step_avg:97.15ms
step:996/1770 train_time:95792ms step_avg:97.15ms
step:997/1770 train_time:95892ms step_avg:97.16ms
step:998/1770 train_time:95993ms step_avg:97.16ms
step:999/1770 train_time:96094ms step_avg:97.16ms
step:1000/1770 train_time:96196ms step_avg:97.17ms
step:1000/1770 val_loss:3.5108 train_time:96295ms step_avg:97.27ms
step:1001/1770 train_time:96316ms step_avg:97.19ms
step:1002/1770 train_time:96407ms step_avg:97.18ms
step:1003/1770 train_time:96509ms step_avg:97.19ms
step:1004/1770 train_time:96610ms step_avg:97.19ms
step:1005/1770 train_time:96710ms step_avg:97.20ms
step:1006/1770 train_time:96810ms step_avg:97.20ms
step:1007/1770 train_time:96910ms step_avg:97.20ms
step:1008/1770 train_time:97010ms step_avg:97.20ms
step:1009/1770 train_time:97110ms step_avg:97.21ms
step:1010/1770 train_time:97210ms step_avg:97.21ms
step:1011/1770 train_time:97312ms step_avg:97.21ms
step:1012/1770 train_time:97413ms step_avg:97.22ms
step:1013/1770 train_time:97513ms step_avg:97.22ms
step:1014/1770 train_time:97614ms step_avg:97.23ms
step:1015/1770 train_time:97714ms step_avg:97.23ms
step:1016/1770 train_time:97815ms step_avg:97.23ms
step:1017/1770 train_time:97916ms step_avg:97.24ms
step:1018/1770 train_time:98017ms step_avg:97.24ms
step:1019/1770 train_time:98118ms step_avg:97.24ms
step:1020/1770 train_time:98219ms step_avg:97.25ms
step:1021/1770 train_time:98321ms step_avg:97.25ms
step:1022/1770 train_time:98422ms step_avg:97.26ms
step:1023/1770 train_time:98523ms step_avg:97.26ms
step:1024/1770 train_time:98625ms step_avg:97.26ms
step:1025/1770 train_time:98726ms step_avg:97.27ms
step:1026/1770 train_time:98827ms step_avg:97.27ms
step:1027/1770 train_time:98929ms step_avg:97.28ms
step:1028/1770 train_time:99030ms step_avg:97.28ms
step:1029/1770 train_time:99130ms step_avg:97.28ms
step:1030/1770 train_time:99231ms step_avg:97.29ms
step:1031/1770 train_time:99332ms step_avg:97.29ms
step:1032/1770 train_time:99432ms step_avg:97.29ms
step:1033/1770 train_time:99533ms step_avg:97.29ms
step:1034/1770 train_time:99633ms step_avg:97.30ms
step:1035/1770 train_time:99733ms step_avg:97.30ms
step:1036/1770 train_time:99833ms step_avg:97.30ms
step:1037/1770 train_time:99934ms step_avg:97.31ms
step:1038/1770 train_time:100035ms step_avg:97.31ms
step:1039/1770 train_time:100136ms step_avg:97.31ms
step:1040/1770 train_time:100237ms step_avg:97.32ms
step:1041/1770 train_time:100338ms step_avg:97.32ms
step:1042/1770 train_time:100439ms step_avg:97.32ms
step:1043/1770 train_time:100540ms step_avg:97.33ms
step:1044/1770 train_time:100640ms step_avg:97.33ms
step:1045/1770 train_time:100741ms step_avg:97.33ms
step:1046/1770 train_time:100842ms step_avg:97.34ms
step:1047/1770 train_time:100942ms step_avg:97.34ms
step:1048/1770 train_time:101043ms step_avg:97.34ms
step:1049/1770 train_time:101145ms step_avg:97.35ms
step:1050/1770 train_time:101246ms step_avg:97.35ms
step:1051/1770 train_time:101348ms step_avg:97.36ms
step:1052/1770 train_time:101449ms step_avg:97.36ms
step:1053/1770 train_time:101551ms step_avg:97.36ms
step:1054/1770 train_time:101652ms step_avg:97.37ms
step:1055/1770 train_time:101752ms step_avg:97.37ms
step:1056/1770 train_time:101853ms step_avg:97.37ms
step:1057/1770 train_time:101953ms step_avg:97.38ms
step:1058/1770 train_time:102054ms step_avg:97.38ms
step:1059/1770 train_time:102154ms step_avg:97.38ms
step:1060/1770 train_time:102257ms step_avg:97.39ms
step:1061/1770 train_time:102358ms step_avg:97.39ms
step:1062/1770 train_time:102459ms step_avg:97.39ms
step:1063/1770 train_time:102562ms step_avg:97.40ms
step:1064/1770 train_time:102663ms step_avg:97.40ms
step:1065/1770 train_time:102765ms step_avg:97.41ms
step:1066/1770 train_time:102866ms step_avg:97.41ms
step:1067/1770 train_time:102967ms step_avg:97.41ms
step:1068/1770 train_time:103068ms step_avg:97.42ms
step:1069/1770 train_time:103170ms step_avg:97.42ms
step:1070/1770 train_time:103271ms step_avg:97.43ms
step:1071/1770 train_time:103371ms step_avg:97.43ms
step:1072/1770 train_time:103472ms step_avg:97.43ms
step:1073/1770 train_time:103572ms step_avg:97.43ms
step:1074/1770 train_time:103673ms step_avg:97.44ms
step:1075/1770 train_time:103774ms step_avg:97.44ms
step:1076/1770 train_time:103875ms step_avg:97.44ms
step:1077/1770 train_time:103975ms step_avg:97.45ms
step:1078/1770 train_time:104077ms step_avg:97.45ms
step:1079/1770 train_time:104178ms step_avg:97.45ms
step:1080/1770 train_time:104280ms step_avg:97.46ms
step:1081/1770 train_time:104381ms step_avg:97.46ms
step:1082/1770 train_time:104482ms step_avg:97.46ms
step:1083/1770 train_time:104584ms step_avg:97.47ms
step:1084/1770 train_time:104685ms step_avg:97.47ms
step:1085/1770 train_time:104786ms step_avg:97.48ms
step:1086/1770 train_time:104888ms step_avg:97.48ms
step:1087/1770 train_time:104989ms step_avg:97.48ms
step:1088/1770 train_time:105090ms step_avg:97.49ms
step:1089/1770 train_time:105191ms step_avg:97.49ms
step:1090/1770 train_time:105293ms step_avg:97.49ms
step:1091/1770 train_time:105394ms step_avg:97.50ms
step:1092/1770 train_time:105495ms step_avg:97.50ms
step:1093/1770 train_time:105595ms step_avg:97.50ms
step:1094/1770 train_time:105697ms step_avg:97.51ms
step:1095/1770 train_time:105797ms step_avg:97.51ms
step:1096/1770 train_time:105899ms step_avg:97.51ms
step:1097/1770 train_time:106001ms step_avg:97.52ms
step:1098/1770 train_time:106102ms step_avg:97.52ms
step:1099/1770 train_time:106203ms step_avg:97.52ms
step:1100/1770 train_time:106306ms step_avg:97.53ms
step:1101/1770 train_time:106407ms step_avg:97.53ms
step:1102/1770 train_time:106507ms step_avg:97.53ms
step:1103/1770 train_time:106608ms step_avg:97.54ms
step:1104/1770 train_time:106710ms step_avg:97.54ms
step:1105/1770 train_time:106810ms step_avg:97.54ms
step:1106/1770 train_time:106912ms step_avg:97.55ms
step:1107/1770 train_time:107013ms step_avg:97.55ms
step:1108/1770 train_time:107114ms step_avg:97.55ms
step:1109/1770 train_time:107214ms step_avg:97.56ms
step:1110/1770 train_time:107316ms step_avg:97.56ms
step:1111/1770 train_time:107418ms step_avg:97.56ms
step:1112/1770 train_time:107519ms step_avg:97.57ms
step:1113/1770 train_time:107621ms step_avg:97.57ms
step:1114/1770 train_time:107722ms step_avg:97.57ms
step:1115/1770 train_time:107824ms step_avg:97.58ms
step:1116/1770 train_time:107926ms step_avg:97.58ms
step:1117/1770 train_time:108028ms step_avg:97.59ms
step:1118/1770 train_time:108129ms step_avg:97.59ms
step:1119/1770 train_time:108230ms step_avg:97.59ms
step:1120/1770 train_time:108330ms step_avg:97.60ms
step:1121/1770 train_time:108432ms step_avg:97.60ms
step:1122/1770 train_time:108533ms step_avg:97.60ms
step:1123/1770 train_time:108633ms step_avg:97.60ms
step:1124/1770 train_time:108734ms step_avg:97.61ms
step:1125/1770 train_time:108834ms step_avg:97.61ms
step:1125/1770 val_loss:3.4711 train_time:108934ms step_avg:97.70ms
step:1126/1770 train_time:108955ms step_avg:97.63ms
step:1127/1770 train_time:109049ms step_avg:97.63ms
step:1128/1770 train_time:109150ms step_avg:97.63ms
step:1129/1770 train_time:109251ms step_avg:97.63ms
step:1130/1770 train_time:109352ms step_avg:97.64ms
step:1131/1770 train_time:109452ms step_avg:97.64ms
step:1132/1770 train_time:109552ms step_avg:97.64ms
step:1133/1770 train_time:109652ms step_avg:97.64ms
step:1134/1770 train_time:109753ms step_avg:97.64ms
step:1135/1770 train_time:109852ms step_avg:97.65ms
step:1136/1770 train_time:109955ms step_avg:97.65ms
step:1137/1770 train_time:110060ms step_avg:97.66ms
step:1138/1770 train_time:110161ms step_avg:97.66ms
step:1139/1770 train_time:110261ms step_avg:97.66ms
step:1140/1770 train_time:110362ms step_avg:97.67ms
step:1141/1770 train_time:110463ms step_avg:97.67ms
step:1142/1770 train_time:110565ms step_avg:97.67ms
step:1143/1770 train_time:110666ms step_avg:97.68ms
step:1144/1770 train_time:110766ms step_avg:97.68ms
step:1145/1770 train_time:110867ms step_avg:97.68ms
step:1146/1770 train_time:110970ms step_avg:97.68ms
step:1147/1770 train_time:111071ms step_avg:97.69ms
step:1148/1770 train_time:111172ms step_avg:97.69ms
step:1149/1770 train_time:111273ms step_avg:97.69ms
step:1150/1770 train_time:111373ms step_avg:97.70ms
step:1151/1770 train_time:111474ms step_avg:97.70ms
step:1152/1770 train_time:111575ms step_avg:97.70ms
step:1153/1770 train_time:111677ms step_avg:97.71ms
step:1154/1770 train_time:111779ms step_avg:97.71ms
step:1155/1770 train_time:111880ms step_avg:97.71ms
step:1156/1770 train_time:111982ms step_avg:97.72ms
step:1157/1770 train_time:112085ms step_avg:97.72ms
step:1158/1770 train_time:112186ms step_avg:97.72ms
step:1159/1770 train_time:112286ms step_avg:97.73ms
step:1160/1770 train_time:112388ms step_avg:97.73ms
step:1161/1770 train_time:112488ms step_avg:97.73ms
step:1162/1770 train_time:112589ms step_avg:97.73ms
step:1163/1770 train_time:112691ms step_avg:97.74ms
step:1164/1770 train_time:112792ms step_avg:97.74ms
step:1165/1770 train_time:112893ms step_avg:97.74ms
step:1166/1770 train_time:112994ms step_avg:97.75ms
step:1167/1770 train_time:113096ms step_avg:97.75ms
step:1168/1770 train_time:113197ms step_avg:97.75ms
step:1169/1770 train_time:113299ms step_avg:97.76ms
step:1170/1770 train_time:113400ms step_avg:97.76ms
step:1171/1770 train_time:113502ms step_avg:97.76ms
step:1172/1770 train_time:113604ms step_avg:97.77ms
step:1173/1770 train_time:113705ms step_avg:97.77ms
step:1174/1770 train_time:113806ms step_avg:97.77ms
step:1175/1770 train_time:113907ms step_avg:97.77ms
step:1176/1770 train_time:114008ms step_avg:97.78ms
step:1177/1770 train_time:114109ms step_avg:97.78ms
step:1178/1770 train_time:114211ms step_avg:97.78ms
step:1179/1770 train_time:114312ms step_avg:97.79ms
step:1180/1770 train_time:114412ms step_avg:97.79ms
step:1181/1770 train_time:114513ms step_avg:97.79ms
step:1182/1770 train_time:114615ms step_avg:97.79ms
step:1183/1770 train_time:114718ms step_avg:97.80ms
step:1184/1770 train_time:114823ms step_avg:97.80ms
step:1185/1770 train_time:114925ms step_avg:97.81ms
step:1186/1770 train_time:115027ms step_avg:97.81ms
step:1187/1770 train_time:115132ms step_avg:97.82ms
step:1188/1770 train_time:115233ms step_avg:97.82ms
step:1189/1770 train_time:115335ms step_avg:97.82ms
step:1190/1770 train_time:115436ms step_avg:97.83ms
step:1191/1770 train_time:115539ms step_avg:97.83ms
step:1192/1770 train_time:115641ms step_avg:97.84ms
step:1193/1770 train_time:115744ms step_avg:97.84ms
step:1194/1770 train_time:115846ms step_avg:97.84ms
step:1195/1770 train_time:115948ms step_avg:97.85ms
step:1196/1770 train_time:116052ms step_avg:97.85ms
step:1197/1770 train_time:116153ms step_avg:97.85ms
step:1198/1770 train_time:116255ms step_avg:97.86ms
step:1199/1770 train_time:116356ms step_avg:97.86ms
step:1200/1770 train_time:116458ms step_avg:97.86ms
step:1201/1770 train_time:116561ms step_avg:97.87ms
step:1202/1770 train_time:116663ms step_avg:97.87ms
step:1203/1770 train_time:116765ms step_avg:97.88ms
step:1204/1770 train_time:116868ms step_avg:97.88ms
step:1205/1770 train_time:116970ms step_avg:97.88ms
step:1206/1770 train_time:117072ms step_avg:97.89ms
step:1207/1770 train_time:117175ms step_avg:97.89ms
step:1208/1770 train_time:117277ms step_avg:97.89ms
step:1209/1770 train_time:117379ms step_avg:97.90ms
step:1210/1770 train_time:117480ms step_avg:97.90ms
step:1211/1770 train_time:117582ms step_avg:97.90ms
step:1212/1770 train_time:117686ms step_avg:97.91ms
step:1213/1770 train_time:117787ms step_avg:97.91ms
step:1214/1770 train_time:117889ms step_avg:97.91ms
step:1215/1770 train_time:117991ms step_avg:97.92ms
step:1216/1770 train_time:118095ms step_avg:97.92ms
step:1217/1770 train_time:118196ms step_avg:97.93ms
step:1218/1770 train_time:118298ms step_avg:97.93ms
step:1219/1770 train_time:118400ms step_avg:97.93ms
step:1220/1770 train_time:118503ms step_avg:97.94ms
step:1221/1770 train_time:118605ms step_avg:97.94ms
step:1222/1770 train_time:118709ms step_avg:97.94ms
step:1223/1770 train_time:118810ms step_avg:97.95ms
step:1224/1770 train_time:118913ms step_avg:97.95ms
step:1225/1770 train_time:119016ms step_avg:97.96ms
step:1226/1770 train_time:119118ms step_avg:97.96ms
step:1227/1770 train_time:119222ms step_avg:97.96ms
step:1228/1770 train_time:119327ms step_avg:97.97ms
step:1229/1770 train_time:119428ms step_avg:97.97ms
step:1230/1770 train_time:119530ms step_avg:97.98ms
step:1231/1770 train_time:119633ms step_avg:97.98ms
step:1232/1770 train_time:119734ms step_avg:97.98ms
step:1233/1770 train_time:119836ms step_avg:97.99ms
step:1234/1770 train_time:119939ms step_avg:97.99ms
step:1235/1770 train_time:120040ms step_avg:97.99ms
step:1236/1770 train_time:120143ms step_avg:98.00ms
step:1237/1770 train_time:120245ms step_avg:98.00ms
step:1238/1770 train_time:120348ms step_avg:98.00ms
step:1239/1770 train_time:120450ms step_avg:98.01ms
step:1240/1770 train_time:120552ms step_avg:98.01ms
step:1241/1770 train_time:120655ms step_avg:98.01ms
step:1242/1770 train_time:120757ms step_avg:98.02ms
step:1243/1770 train_time:120859ms step_avg:98.02ms
step:1244/1770 train_time:120960ms step_avg:98.02ms
step:1245/1770 train_time:121062ms step_avg:98.03ms
step:1246/1770 train_time:121165ms step_avg:98.03ms
step:1247/1770 train_time:121267ms step_avg:98.03ms
step:1248/1770 train_time:121369ms step_avg:98.04ms
step:1249/1770 train_time:121470ms step_avg:98.04ms
step:1250/1770 train_time:121572ms step_avg:98.04ms
step:1250/1770 val_loss:3.4233 train_time:121674ms step_avg:98.12ms
step:1251/1770 train_time:121696ms step_avg:98.06ms
step:1252/1770 train_time:121788ms step_avg:98.06ms
step:1253/1770 train_time:121891ms step_avg:98.06ms
step:1254/1770 train_time:121993ms step_avg:98.07ms
step:1255/1770 train_time:122097ms step_avg:98.07ms
step:1256/1770 train_time:122198ms step_avg:98.07ms
step:1257/1770 train_time:122299ms step_avg:98.07ms
step:1258/1770 train_time:122401ms step_avg:98.08ms
step:1259/1770 train_time:122503ms step_avg:98.08ms
step:1260/1770 train_time:122605ms step_avg:98.08ms
step:1261/1770 train_time:122709ms step_avg:98.09ms
step:1262/1770 train_time:122812ms step_avg:98.09ms
step:1263/1770 train_time:122914ms step_avg:98.10ms
step:1264/1770 train_time:123018ms step_avg:98.10ms
step:1265/1770 train_time:123120ms step_avg:98.10ms
step:1266/1770 train_time:123222ms step_avg:98.11ms
step:1267/1770 train_time:123324ms step_avg:98.11ms
step:1268/1770 train_time:123427ms step_avg:98.11ms
step:1269/1770 train_time:123529ms step_avg:98.12ms
step:1270/1770 train_time:123631ms step_avg:98.12ms
step:1271/1770 train_time:123733ms step_avg:98.12ms
step:1272/1770 train_time:123835ms step_avg:98.13ms
step:1273/1770 train_time:123938ms step_avg:98.13ms
step:1274/1770 train_time:124040ms step_avg:98.13ms
step:1275/1770 train_time:124141ms step_avg:98.14ms
step:1276/1770 train_time:124243ms step_avg:98.14ms
step:1277/1770 train_time:124345ms step_avg:98.14ms
step:1278/1770 train_time:124448ms step_avg:98.15ms
step:1279/1770 train_time:124551ms step_avg:98.15ms
step:1280/1770 train_time:124654ms step_avg:98.15ms
step:1281/1770 train_time:124755ms step_avg:98.15ms
step:1282/1770 train_time:124858ms step_avg:98.16ms
step:1283/1770 train_time:124961ms step_avg:98.16ms
step:1284/1770 train_time:125062ms step_avg:98.17ms
step:1285/1770 train_time:125166ms step_avg:98.17ms
step:1286/1770 train_time:125269ms step_avg:98.17ms
step:1287/1770 train_time:125373ms step_avg:98.18ms
step:1288/1770 train_time:125475ms step_avg:98.18ms
step:1289/1770 train_time:125578ms step_avg:98.18ms
step:1290/1770 train_time:125680ms step_avg:98.19ms
step:1291/1770 train_time:125783ms step_avg:98.19ms
step:1292/1770 train_time:125885ms step_avg:98.19ms
step:1293/1770 train_time:125988ms step_avg:98.20ms
step:1294/1770 train_time:126089ms step_avg:98.20ms
step:1295/1770 train_time:126191ms step_avg:98.20ms
step:1296/1770 train_time:126292ms step_avg:98.21ms
step:1297/1770 train_time:126394ms step_avg:98.21ms
step:1298/1770 train_time:126496ms step_avg:98.21ms
step:1299/1770 train_time:126598ms step_avg:98.21ms
step:1300/1770 train_time:126699ms step_avg:98.22ms
step:1301/1770 train_time:126802ms step_avg:98.22ms
step:1302/1770 train_time:126904ms step_avg:98.22ms
step:1303/1770 train_time:127006ms step_avg:98.23ms
step:1304/1770 train_time:127108ms step_avg:98.23ms
step:1305/1770 train_time:127211ms step_avg:98.23ms
step:1306/1770 train_time:127313ms step_avg:98.24ms
step:1307/1770 train_time:127416ms step_avg:98.24ms
step:1308/1770 train_time:127518ms step_avg:98.24ms
step:1309/1770 train_time:127620ms step_avg:98.24ms
step:1310/1770 train_time:127722ms step_avg:98.25ms
step:1311/1770 train_time:127824ms step_avg:98.25ms
step:1312/1770 train_time:127925ms step_avg:98.25ms
step:1313/1770 train_time:128027ms step_avg:98.26ms
step:1314/1770 train_time:128129ms step_avg:98.26ms
step:1315/1770 train_time:128231ms step_avg:98.26ms
step:1316/1770 train_time:128333ms step_avg:98.26ms
step:1317/1770 train_time:128436ms step_avg:98.27ms
step:1318/1770 train_time:128541ms step_avg:98.27ms
step:1319/1770 train_time:128644ms step_avg:98.28ms
step:1320/1770 train_time:128746ms step_avg:98.28ms
step:1321/1770 train_time:128848ms step_avg:98.28ms
step:1322/1770 train_time:128951ms step_avg:98.29ms
step:1323/1770 train_time:129054ms step_avg:98.29ms
step:1324/1770 train_time:129156ms step_avg:98.29ms
step:1325/1770 train_time:129259ms step_avg:98.30ms
step:1326/1770 train_time:129362ms step_avg:98.30ms
step:1327/1770 train_time:129467ms step_avg:98.30ms
step:1328/1770 train_time:129569ms step_avg:98.31ms
step:1329/1770 train_time:129672ms step_avg:98.31ms
step:1330/1770 train_time:129774ms step_avg:98.31ms
step:1331/1770 train_time:129875ms step_avg:98.32ms
step:1332/1770 train_time:129977ms step_avg:98.32ms
step:1333/1770 train_time:130078ms step_avg:98.32ms
step:1334/1770 train_time:130180ms step_avg:98.32ms
step:1335/1770 train_time:130282ms step_avg:98.33ms
step:1336/1770 train_time:130384ms step_avg:98.33ms
step:1337/1770 train_time:130487ms step_avg:98.33ms
step:1338/1770 train_time:130589ms step_avg:98.34ms
step:1339/1770 train_time:130692ms step_avg:98.34ms
step:1340/1770 train_time:130796ms step_avg:98.34ms
step:1341/1770 train_time:130897ms step_avg:98.35ms
step:1342/1770 train_time:131000ms step_avg:98.35ms
step:1343/1770 train_time:131104ms step_avg:98.35ms
step:1344/1770 train_time:131206ms step_avg:98.36ms
step:1345/1770 train_time:131308ms step_avg:98.36ms
step:1346/1770 train_time:131409ms step_avg:98.36ms
step:1347/1770 train_time:131511ms step_avg:98.36ms
step:1348/1770 train_time:131615ms step_avg:98.37ms
step:1349/1770 train_time:131718ms step_avg:98.37ms
step:1350/1770 train_time:131819ms step_avg:98.37ms
step:1351/1770 train_time:131921ms step_avg:98.37ms
step:1352/1770 train_time:132023ms step_avg:98.38ms
step:1353/1770 train_time:132126ms step_avg:98.38ms
step:1354/1770 train_time:132228ms step_avg:98.38ms
step:1355/1770 train_time:132331ms step_avg:98.39ms
step:1356/1770 train_time:132432ms step_avg:98.39ms
step:1357/1770 train_time:132535ms step_avg:98.39ms
step:1358/1770 train_time:132638ms step_avg:98.40ms
step:1359/1770 train_time:132740ms step_avg:98.40ms
step:1360/1770 train_time:132843ms step_avg:98.40ms
step:1361/1770 train_time:132945ms step_avg:98.40ms
step:1362/1770 train_time:133047ms step_avg:98.41ms
step:1363/1770 train_time:133150ms step_avg:98.41ms
step:1364/1770 train_time:133253ms step_avg:98.41ms
step:1365/1770 train_time:133355ms step_avg:98.42ms
step:1366/1770 train_time:133456ms step_avg:98.42ms
step:1367/1770 train_time:133559ms step_avg:98.42ms
step:1368/1770 train_time:133660ms step_avg:98.42ms
step:1369/1770 train_time:133763ms step_avg:98.43ms
step:1370/1770 train_time:133865ms step_avg:98.43ms
step:1371/1770 train_time:133966ms step_avg:98.43ms
step:1372/1770 train_time:134068ms step_avg:98.43ms
step:1373/1770 train_time:134171ms step_avg:98.44ms
step:1374/1770 train_time:134274ms step_avg:98.44ms
step:1375/1770 train_time:134377ms step_avg:98.44ms
step:1375/1770 val_loss:3.3799 train_time:134477ms step_avg:98.52ms
step:1376/1770 train_time:134499ms step_avg:98.46ms
step:1377/1770 train_time:134591ms step_avg:98.46ms
step:1378/1770 train_time:134694ms step_avg:98.46ms
step:1379/1770 train_time:134796ms step_avg:98.46ms
step:1380/1770 train_time:134898ms step_avg:98.47ms
step:1381/1770 train_time:135000ms step_avg:98.47ms
step:1382/1770 train_time:135101ms step_avg:98.47ms
step:1383/1770 train_time:135204ms step_avg:98.47ms
step:1384/1770 train_time:135306ms step_avg:98.48ms
step:1385/1770 train_time:135407ms step_avg:98.48ms
step:1386/1770 train_time:135510ms step_avg:98.48ms
step:1387/1770 train_time:135614ms step_avg:98.49ms
step:1388/1770 train_time:135716ms step_avg:98.49ms
step:1389/1770 train_time:135819ms step_avg:98.49ms
step:1390/1770 train_time:135920ms step_avg:98.49ms
step:1391/1770 train_time:136022ms step_avg:98.50ms
step:1392/1770 train_time:136124ms step_avg:98.50ms
step:1393/1770 train_time:136225ms step_avg:98.50ms
step:1394/1770 train_time:136326ms step_avg:98.50ms
step:1395/1770 train_time:136429ms step_avg:98.50ms
step:1396/1770 train_time:136532ms step_avg:98.51ms
step:1397/1770 train_time:136636ms step_avg:98.51ms
step:1398/1770 train_time:136738ms step_avg:98.51ms
step:1399/1770 train_time:136841ms step_avg:98.52ms
step:1400/1770 train_time:136943ms step_avg:98.52ms
step:1401/1770 train_time:137045ms step_avg:98.52ms
step:1402/1770 train_time:137147ms step_avg:98.53ms
step:1403/1770 train_time:137249ms step_avg:98.53ms
step:1404/1770 train_time:137352ms step_avg:98.53ms
step:1405/1770 train_time:137453ms step_avg:98.53ms
step:1406/1770 train_time:137555ms step_avg:98.54ms
step:1407/1770 train_time:137658ms step_avg:98.54ms
step:1408/1770 train_time:137760ms step_avg:98.54ms
step:1409/1770 train_time:137862ms step_avg:98.54ms
step:1410/1770 train_time:137965ms step_avg:98.55ms
step:1411/1770 train_time:138067ms step_avg:98.55ms
step:1412/1770 train_time:138169ms step_avg:98.55ms
step:1413/1770 train_time:138271ms step_avg:98.55ms
step:1414/1770 train_time:138374ms step_avg:98.56ms
step:1415/1770 train_time:138476ms step_avg:98.56ms
step:1416/1770 train_time:138580ms step_avg:98.56ms
step:1417/1770 train_time:138682ms step_avg:98.57ms
step:1418/1770 train_time:138783ms step_avg:98.57ms
step:1419/1770 train_time:138886ms step_avg:98.57ms
step:1420/1770 train_time:138988ms step_avg:98.57ms
step:1421/1770 train_time:139089ms step_avg:98.58ms
step:1422/1770 train_time:139192ms step_avg:98.58ms
step:1423/1770 train_time:139294ms step_avg:98.58ms
step:1424/1770 train_time:139397ms step_avg:98.58ms
step:1425/1770 train_time:139499ms step_avg:98.59ms
step:1426/1770 train_time:139602ms step_avg:98.59ms
step:1427/1770 train_time:139703ms step_avg:98.59ms
step:1428/1770 train_time:139807ms step_avg:98.59ms
step:1429/1770 train_time:139909ms step_avg:98.60ms
step:1430/1770 train_time:140011ms step_avg:98.60ms
step:1431/1770 train_time:140114ms step_avg:98.60ms
step:1432/1770 train_time:140216ms step_avg:98.60ms
step:1433/1770 train_time:140317ms step_avg:98.61ms
step:1434/1770 train_time:140418ms step_avg:98.61ms
step:1435/1770 train_time:140520ms step_avg:98.61ms
step:1436/1770 train_time:140624ms step_avg:98.61ms
step:1437/1770 train_time:140726ms step_avg:98.62ms
step:1438/1770 train_time:140828ms step_avg:98.62ms
step:1439/1770 train_time:140930ms step_avg:98.62ms
step:1440/1770 train_time:141032ms step_avg:98.62ms
step:1441/1770 train_time:141137ms step_avg:98.63ms
step:1442/1770 train_time:141238ms step_avg:98.63ms
step:1443/1770 train_time:141341ms step_avg:98.63ms
step:1444/1770 train_time:141443ms step_avg:98.64ms
step:1445/1770 train_time:141545ms step_avg:98.64ms
step:1446/1770 train_time:141648ms step_avg:98.64ms
step:1447/1770 train_time:141752ms step_avg:98.64ms
step:1448/1770 train_time:141856ms step_avg:98.65ms
step:1449/1770 train_time:141960ms step_avg:98.65ms
step:1450/1770 train_time:142063ms step_avg:98.65ms
step:1451/1770 train_time:142167ms step_avg:98.66ms
step:1452/1770 train_time:142271ms step_avg:98.66ms
step:1453/1770 train_time:142374ms step_avg:98.67ms
step:1454/1770 train_time:142478ms step_avg:98.67ms
step:1455/1770 train_time:142582ms step_avg:98.67ms
step:1456/1770 train_time:142685ms step_avg:98.68ms
step:1457/1770 train_time:142789ms step_avg:98.68ms
step:1458/1770 train_time:142892ms step_avg:98.68ms
step:1459/1770 train_time:142996ms step_avg:98.69ms
step:1460/1770 train_time:143099ms step_avg:98.69ms
step:1461/1770 train_time:143202ms step_avg:98.69ms
step:1462/1770 train_time:143306ms step_avg:98.70ms
step:1463/1770 train_time:143409ms step_avg:98.70ms
step:1464/1770 train_time:143513ms step_avg:98.70ms
step:1465/1770 train_time:143618ms step_avg:98.71ms
step:1466/1770 train_time:143722ms step_avg:98.71ms
step:1467/1770 train_time:143826ms step_avg:98.71ms
step:1468/1770 train_time:143929ms step_avg:98.72ms
step:1469/1770 train_time:144032ms step_avg:98.72ms
step:1470/1770 train_time:144135ms step_avg:98.72ms
step:1471/1770 train_time:144238ms step_avg:98.73ms
step:1472/1770 train_time:144342ms step_avg:98.73ms
step:1473/1770 train_time:144446ms step_avg:98.73ms
step:1474/1770 train_time:144550ms step_avg:98.74ms
step:1475/1770 train_time:144653ms step_avg:98.74ms
step:1476/1770 train_time:144757ms step_avg:98.74ms
step:1477/1770 train_time:144863ms step_avg:98.75ms
step:1478/1770 train_time:144966ms step_avg:98.75ms
step:1479/1770 train_time:145069ms step_avg:98.75ms
step:1480/1770 train_time:145172ms step_avg:98.76ms
step:1481/1770 train_time:145280ms step_avg:98.76ms
step:1482/1770 train_time:145383ms step_avg:98.77ms
step:1483/1770 train_time:145486ms step_avg:98.77ms
step:1484/1770 train_time:145589ms step_avg:98.77ms
step:1485/1770 train_time:145692ms step_avg:98.77ms
step:1486/1770 train_time:145796ms step_avg:98.78ms
step:1487/1770 train_time:145899ms step_avg:98.78ms
step:1488/1770 train_time:146004ms step_avg:98.78ms
step:1489/1770 train_time:146109ms step_avg:98.79ms
step:1490/1770 train_time:146213ms step_avg:98.79ms
step:1491/1770 train_time:146316ms step_avg:98.80ms
step:1492/1770 train_time:146419ms step_avg:98.80ms
step:1493/1770 train_time:146526ms step_avg:98.80ms
step:1494/1770 train_time:146633ms step_avg:98.81ms
step:1495/1770 train_time:146735ms step_avg:98.81ms
step:1496/1770 train_time:146838ms step_avg:98.81ms
step:1497/1770 train_time:146942ms step_avg:98.82ms
step:1498/1770 train_time:147045ms step_avg:98.82ms
step:1499/1770 train_time:147147ms step_avg:98.82ms
step:1500/1770 train_time:147249ms step_avg:98.83ms
step:1500/1770 val_loss:3.3418 train_time:147351ms step_avg:98.89ms
step:1501/1770 train_time:147372ms step_avg:98.84ms
step:1502/1770 train_time:147463ms step_avg:98.84ms
step:1503/1770 train_time:147566ms step_avg:98.84ms
step:1504/1770 train_time:147670ms step_avg:98.84ms
step:1505/1770 train_time:147775ms step_avg:98.85ms
step:1506/1770 train_time:147878ms step_avg:98.85ms
step:1507/1770 train_time:147981ms step_avg:98.85ms
step:1508/1770 train_time:148086ms step_avg:98.86ms
step:1509/1770 train_time:148188ms step_avg:98.86ms
step:1510/1770 train_time:148290ms step_avg:98.86ms
step:1511/1770 train_time:148394ms step_avg:98.86ms
step:1512/1770 train_time:148498ms step_avg:98.87ms
step:1513/1770 train_time:148602ms step_avg:98.87ms
step:1514/1770 train_time:148705ms step_avg:98.87ms
step:1515/1770 train_time:148808ms step_avg:98.88ms
step:1516/1770 train_time:148913ms step_avg:98.88ms
step:1517/1770 train_time:149016ms step_avg:98.88ms
step:1518/1770 train_time:149121ms step_avg:98.89ms
step:1519/1770 train_time:149224ms step_avg:98.89ms
step:1520/1770 train_time:149329ms step_avg:98.89ms
step:1521/1770 train_time:149431ms step_avg:98.90ms
step:1522/1770 train_time:149535ms step_avg:98.90ms
step:1523/1770 train_time:149640ms step_avg:98.90ms
step:1524/1770 train_time:149743ms step_avg:98.91ms
step:1525/1770 train_time:149846ms step_avg:98.91ms
step:1526/1770 train_time:149950ms step_avg:98.91ms
step:1527/1770 train_time:150053ms step_avg:98.91ms
step:1528/1770 train_time:150158ms step_avg:98.92ms
step:1529/1770 train_time:150261ms step_avg:98.92ms
step:1530/1770 train_time:150365ms step_avg:98.92ms
step:1531/1770 train_time:150468ms step_avg:98.93ms
step:1532/1770 train_time:150571ms step_avg:98.93ms
step:1533/1770 train_time:150675ms step_avg:98.93ms
step:1534/1770 train_time:150778ms step_avg:98.94ms
step:1535/1770 train_time:150882ms step_avg:98.94ms
step:1536/1770 train_time:150984ms step_avg:98.94ms
step:1537/1770 train_time:151088ms step_avg:98.94ms
step:1538/1770 train_time:151193ms step_avg:98.95ms
step:1539/1770 train_time:151297ms step_avg:98.95ms
step:1540/1770 train_time:151402ms step_avg:98.96ms
step:1541/1770 train_time:151506ms step_avg:98.96ms
step:1542/1770 train_time:151610ms step_avg:98.96ms
step:1543/1770 train_time:151714ms step_avg:98.97ms
step:1544/1770 train_time:151819ms step_avg:98.97ms
step:1545/1770 train_time:151922ms step_avg:98.97ms
step:1546/1770 train_time:152026ms step_avg:98.97ms
step:1547/1770 train_time:152129ms step_avg:98.98ms
step:1548/1770 train_time:152232ms step_avg:98.98ms
step:1549/1770 train_time:152336ms step_avg:98.98ms
step:1550/1770 train_time:152440ms step_avg:98.99ms
step:1551/1770 train_time:152543ms step_avg:98.99ms
step:1552/1770 train_time:152648ms step_avg:98.99ms
step:1553/1770 train_time:152752ms step_avg:99.00ms
step:1554/1770 train_time:152855ms step_avg:99.00ms
step:1555/1770 train_time:152959ms step_avg:99.00ms
step:1556/1770 train_time:153061ms step_avg:99.00ms
step:1557/1770 train_time:153165ms step_avg:99.01ms
step:1558/1770 train_time:153268ms step_avg:99.01ms
step:1559/1770 train_time:153372ms step_avg:99.01ms
step:1560/1770 train_time:153474ms step_avg:99.02ms
step:1561/1770 train_time:153580ms step_avg:99.02ms
step:1562/1770 train_time:153683ms step_avg:99.02ms
step:1563/1770 train_time:153787ms step_avg:99.03ms
step:1564/1770 train_time:153889ms step_avg:99.03ms
step:1565/1770 train_time:153993ms step_avg:99.03ms
step:1566/1770 train_time:154097ms step_avg:99.03ms
step:1567/1770 train_time:154200ms step_avg:99.04ms
step:1568/1770 train_time:154303ms step_avg:99.04ms
step:1569/1770 train_time:154409ms step_avg:99.04ms
step:1570/1770 train_time:154512ms step_avg:99.05ms
step:1571/1770 train_time:154616ms step_avg:99.05ms
step:1572/1770 train_time:154720ms step_avg:99.05ms
step:1573/1770 train_time:154825ms step_avg:99.06ms
step:1574/1770 train_time:154929ms step_avg:99.06ms
step:1575/1770 train_time:155031ms step_avg:99.06ms
step:1576/1770 train_time:155133ms step_avg:99.06ms
step:1577/1770 train_time:155238ms step_avg:99.07ms
step:1578/1770 train_time:155342ms step_avg:99.07ms
step:1579/1770 train_time:155446ms step_avg:99.07ms
step:1580/1770 train_time:155549ms step_avg:99.08ms
step:1581/1770 train_time:155655ms step_avg:99.08ms
step:1582/1770 train_time:155760ms step_avg:99.08ms
step:1583/1770 train_time:155863ms step_avg:99.09ms
step:1584/1770 train_time:155968ms step_avg:99.09ms
step:1585/1770 train_time:156072ms step_avg:99.09ms
step:1586/1770 train_time:156179ms step_avg:99.10ms
step:1587/1770 train_time:156283ms step_avg:99.10ms
step:1588/1770 train_time:156386ms step_avg:99.10ms
step:1589/1770 train_time:156492ms step_avg:99.11ms
step:1590/1770 train_time:156594ms step_avg:99.11ms
step:1591/1770 train_time:156697ms step_avg:99.11ms
step:1592/1770 train_time:156801ms step_avg:99.12ms
step:1593/1770 train_time:156905ms step_avg:99.12ms
step:1594/1770 train_time:157009ms step_avg:99.12ms
step:1595/1770 train_time:157112ms step_avg:99.12ms
step:1596/1770 train_time:157217ms step_avg:99.13ms
step:1597/1770 train_time:157320ms step_avg:99.13ms
step:1598/1770 train_time:157422ms step_avg:99.13ms
step:1599/1770 train_time:157527ms step_avg:99.14ms
step:1600/1770 train_time:157633ms step_avg:99.14ms
step:1601/1770 train_time:157736ms step_avg:99.14ms
step:1602/1770 train_time:157841ms step_avg:99.15ms
step:1603/1770 train_time:157944ms step_avg:99.15ms
step:1604/1770 train_time:158048ms step_avg:99.15ms
step:1605/1770 train_time:158150ms step_avg:99.15ms
step:1606/1770 train_time:158255ms step_avg:99.16ms
step:1607/1770 train_time:158362ms step_avg:99.16ms
step:1608/1770 train_time:158465ms step_avg:99.16ms
step:1609/1770 train_time:158569ms step_avg:99.17ms
step:1610/1770 train_time:158673ms step_avg:99.17ms
step:1611/1770 train_time:158779ms step_avg:99.17ms
step:1612/1770 train_time:158883ms step_avg:99.18ms
step:1613/1770 train_time:158987ms step_avg:99.18ms
step:1614/1770 train_time:159090ms step_avg:99.18ms
step:1615/1770 train_time:159194ms step_avg:99.19ms
step:1616/1770 train_time:159297ms step_avg:99.19ms
step:1617/1770 train_time:159403ms step_avg:99.19ms
step:1618/1770 train_time:159507ms step_avg:99.20ms
step:1619/1770 train_time:159611ms step_avg:99.20ms
step:1620/1770 train_time:159715ms step_avg:99.20ms
step:1621/1770 train_time:159818ms step_avg:99.20ms
step:1622/1770 train_time:159922ms step_avg:99.21ms
step:1623/1770 train_time:160029ms step_avg:99.21ms
step:1624/1770 train_time:160132ms step_avg:99.21ms
step:1625/1770 train_time:160235ms step_avg:99.22ms
step:1625/1770 val_loss:3.3079 train_time:160337ms step_avg:99.28ms
step:1626/1770 train_time:160358ms step_avg:99.23ms
step:1627/1770 train_time:160450ms step_avg:99.23ms
step:1628/1770 train_time:160553ms step_avg:99.23ms
step:1629/1770 train_time:160656ms step_avg:99.23ms
step:1630/1770 train_time:160758ms step_avg:99.23ms
step:1631/1770 train_time:160861ms step_avg:99.24ms
step:1632/1770 train_time:160965ms step_avg:99.24ms
step:1633/1770 train_time:161068ms step_avg:99.24ms
step:1634/1770 train_time:161171ms step_avg:99.24ms
step:1635/1770 train_time:161274ms step_avg:99.25ms
step:1636/1770 train_time:161380ms step_avg:99.25ms
step:1637/1770 train_time:161485ms step_avg:99.25ms
step:1638/1770 train_time:161588ms step_avg:99.26ms
step:1639/1770 train_time:161691ms step_avg:99.26ms
step:1640/1770 train_time:161795ms step_avg:99.26ms
step:1641/1770 train_time:161898ms step_avg:99.26ms
step:1642/1770 train_time:162000ms step_avg:99.26ms
step:1643/1770 train_time:162103ms step_avg:99.27ms
step:1644/1770 train_time:162209ms step_avg:99.27ms
step:1645/1770 train_time:162311ms step_avg:99.27ms
step:1646/1770 train_time:162417ms step_avg:99.28ms
step:1647/1770 train_time:162522ms step_avg:99.28ms
step:1648/1770 train_time:162625ms step_avg:99.28ms
step:1649/1770 train_time:162728ms step_avg:99.29ms
step:1650/1770 train_time:162832ms step_avg:99.29ms
step:1651/1770 train_time:162934ms step_avg:99.29ms
step:1652/1770 train_time:163037ms step_avg:99.29ms
step:1653/1770 train_time:163141ms step_avg:99.29ms
step:1654/1770 train_time:163247ms step_avg:99.30ms
step:1655/1770 train_time:163353ms step_avg:99.30ms
step:1656/1770 train_time:163457ms step_avg:99.31ms
step:1657/1770 train_time:163562ms step_avg:99.31ms
step:1658/1770 train_time:163665ms step_avg:99.31ms
step:1659/1770 train_time:163770ms step_avg:99.31ms
step:1660/1770 train_time:163873ms step_avg:99.32ms
step:1661/1770 train_time:163977ms step_avg:99.32ms
step:1662/1770 train_time:164081ms step_avg:99.32ms
step:1663/1770 train_time:164184ms step_avg:99.32ms
step:1664/1770 train_time:164287ms step_avg:99.33ms
step:1665/1770 train_time:164390ms step_avg:99.33ms
step:1666/1770 train_time:164494ms step_avg:99.33ms
step:1667/1770 train_time:164597ms step_avg:99.33ms
step:1668/1770 train_time:164700ms step_avg:99.34ms
step:1669/1770 train_time:164802ms step_avg:99.34ms
step:1670/1770 train_time:164906ms step_avg:99.34ms
step:1671/1770 train_time:165011ms step_avg:99.34ms
step:1672/1770 train_time:165115ms step_avg:99.35ms
step:1673/1770 train_time:165219ms step_avg:99.35ms
step:1674/1770 train_time:165322ms step_avg:99.35ms
step:1675/1770 train_time:165425ms step_avg:99.35ms
step:1676/1770 train_time:165529ms step_avg:99.36ms
step:1677/1770 train_time:165637ms step_avg:99.36ms
step:1678/1770 train_time:165739ms step_avg:99.36ms
step:1679/1770 train_time:165843ms step_avg:99.37ms
step:1680/1770 train_time:165947ms step_avg:99.37ms
step:1681/1770 train_time:166051ms step_avg:99.37ms
step:1682/1770 train_time:166155ms step_avg:99.38ms
step:1683/1770 train_time:166258ms step_avg:99.38ms
step:1684/1770 train_time:166361ms step_avg:99.38ms
step:1685/1770 train_time:166466ms step_avg:99.38ms
step:1686/1770 train_time:166570ms step_avg:99.39ms
step:1687/1770 train_time:166675ms step_avg:99.39ms
step:1688/1770 train_time:166779ms step_avg:99.39ms
step:1689/1770 train_time:166883ms step_avg:99.39ms
step:1690/1770 train_time:166986ms step_avg:99.40ms
step:1691/1770 train_time:167090ms step_avg:99.40ms
step:1692/1770 train_time:167193ms step_avg:99.40ms
step:1693/1770 train_time:167297ms step_avg:99.40ms
step:1694/1770 train_time:167401ms step_avg:99.41ms
step:1695/1770 train_time:167504ms step_avg:99.41ms
step:1696/1770 train_time:167610ms step_avg:99.41ms
step:1697/1770 train_time:167714ms step_avg:99.42ms
step:1698/1770 train_time:167819ms step_avg:99.42ms
step:1699/1770 train_time:167922ms step_avg:99.42ms
step:1700/1770 train_time:168026ms step_avg:99.42ms
step:1701/1770 train_time:168129ms step_avg:99.43ms
step:1702/1770 train_time:168233ms step_avg:99.43ms
step:1703/1770 train_time:168336ms step_avg:99.43ms
step:1704/1770 train_time:168439ms step_avg:99.43ms
step:1705/1770 train_time:168544ms step_avg:99.44ms
step:1706/1770 train_time:168647ms step_avg:99.44ms
step:1707/1770 train_time:168751ms step_avg:99.44ms
step:1708/1770 train_time:168855ms step_avg:99.44ms
step:1709/1770 train_time:168959ms step_avg:99.45ms
step:1710/1770 train_time:169066ms step_avg:99.45ms
step:1711/1770 train_time:169173ms step_avg:99.45ms
step:1712/1770 train_time:169277ms step_avg:99.46ms
step:1713/1770 train_time:169380ms step_avg:99.46ms
step:1714/1770 train_time:169484ms step_avg:99.46ms
step:1715/1770 train_time:169587ms step_avg:99.46ms
step:1716/1770 train_time:169692ms step_avg:99.47ms
step:1717/1770 train_time:169796ms step_avg:99.47ms
step:1718/1770 train_time:169901ms step_avg:99.47ms
step:1719/1770 train_time:170006ms step_avg:99.48ms
step:1720/1770 train_time:170111ms step_avg:99.48ms
step:1721/1770 train_time:170214ms step_avg:99.48ms
step:1722/1770 train_time:170321ms step_avg:99.49ms
step:1723/1770 train_time:170426ms step_avg:99.49ms
step:1724/1770 train_time:170533ms step_avg:99.49ms
step:1725/1770 train_time:170639ms step_avg:99.50ms
step:1726/1770 train_time:170744ms step_avg:99.50ms
step:1727/1770 train_time:170848ms step_avg:99.50ms
step:1728/1770 train_time:170954ms step_avg:99.51ms
step:1729/1770 train_time:171057ms step_avg:99.51ms
step:1730/1770 train_time:171163ms step_avg:99.51ms
step:1731/1770 train_time:171269ms step_avg:99.52ms
step:1732/1770 train_time:171373ms step_avg:99.52ms
step:1733/1770 train_time:171478ms step_avg:99.52ms
step:1734/1770 train_time:171582ms step_avg:99.53ms
step:1735/1770 train_time:171687ms step_avg:99.53ms
step:1736/1770 train_time:171792ms step_avg:99.53ms
step:1737/1770 train_time:171895ms step_avg:99.53ms
step:1738/1770 train_time:172000ms step_avg:99.54ms
step:1739/1770 train_time:172104ms step_avg:99.54ms
step:1740/1770 train_time:172207ms step_avg:99.54ms
step:1741/1770 train_time:172315ms step_avg:99.55ms
step:1742/1770 train_time:172421ms step_avg:99.55ms
step:1743/1770 train_time:172527ms step_avg:99.55ms
step:1744/1770 train_time:172631ms step_avg:99.56ms
step:1745/1770 train_time:172735ms step_avg:99.56ms
step:1746/1770 train_time:172842ms step_avg:99.56ms
step:1747/1770 train_time:172945ms step_avg:99.57ms
step:1748/1770 train_time:173051ms step_avg:99.57ms
step:1749/1770 train_time:173156ms step_avg:99.57ms
step:1750/1770 train_time:173260ms step_avg:99.57ms
step:1750/1770 val_loss:3.2812 train_time:173362ms step_avg:99.63ms
step:1751/1770 train_time:173384ms step_avg:99.59ms
step:1752/1770 train_time:173479ms step_avg:99.59ms
step:1753/1770 train_time:173583ms step_avg:99.59ms
step:1754/1770 train_time:173688ms step_avg:99.59ms
step:1755/1770 train_time:173791ms step_avg:99.59ms
step:1756/1770 train_time:173896ms step_avg:99.60ms
step:1757/1770 train_time:174000ms step_avg:99.60ms
step:1758/1770 train_time:174104ms step_avg:99.60ms
step:1759/1770 train_time:174209ms step_avg:99.61ms
step:1760/1770 train_time:174314ms step_avg:99.61ms
step:1761/1770 train_time:174420ms step_avg:99.61ms
step:1762/1770 train_time:174528ms step_avg:99.62ms
step:1763/1770 train_time:174631ms step_avg:99.62ms
step:1764/1770 train_time:174736ms step_avg:99.62ms
step:1765/1770 train_time:174841ms step_avg:99.62ms
step:1766/1770 train_time:174949ms step_avg:99.63ms
step:1767/1770 train_time:175052ms step_avg:99.63ms
step:1768/1770 train_time:175156ms step_avg:99.63ms
step:1769/1770 train_time:175259ms step_avg:99.64ms
step:1770/1770 train_time:175363ms step_avg:99.64ms
step:1770/1770 val_loss:3.2779 train_time:175467ms step_avg:99.70ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
