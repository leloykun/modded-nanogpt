import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Jan 26 00:20:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23199ms step_avg:nanms
step:2/1770 train_time:23767ms step_avg:nanms
step:3/1770 train_time:23861ms step_avg:nanms
step:4/1770 train_time:23954ms step_avg:nanms
step:5/1770 train_time:24048ms step_avg:nanms
step:6/1770 train_time:24142ms step_avg:nanms
step:7/1770 train_time:24235ms step_avg:nanms
step:8/1770 train_time:24329ms step_avg:nanms
step:9/1770 train_time:24422ms step_avg:nanms
step:10/1770 train_time:24516ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.10ms
step:14/1770 train_time:377ms step_avg:94.15ms
step:15/1770 train_time:471ms step_avg:94.18ms
step:16/1770 train_time:565ms step_avg:94.17ms
step:17/1770 train_time:660ms step_avg:94.32ms
step:18/1770 train_time:753ms step_avg:94.11ms
step:19/1770 train_time:847ms step_avg:94.06ms
step:20/1770 train_time:940ms step_avg:94.03ms
step:21/1770 train_time:1034ms step_avg:94.02ms
step:22/1770 train_time:1128ms step_avg:94.01ms
step:23/1770 train_time:1222ms step_avg:94.00ms
step:24/1770 train_time:1316ms step_avg:94.02ms
step:25/1770 train_time:1410ms step_avg:94.01ms
step:26/1770 train_time:1504ms step_avg:94.00ms
step:27/1770 train_time:1598ms step_avg:94.00ms
step:28/1770 train_time:1692ms step_avg:93.99ms
step:29/1770 train_time:1786ms step_avg:94.00ms
step:30/1770 train_time:1880ms step_avg:93.99ms
step:31/1770 train_time:1973ms step_avg:93.97ms
step:32/1770 train_time:2067ms step_avg:93.96ms
step:33/1770 train_time:2162ms step_avg:94.00ms
step:34/1770 train_time:2256ms step_avg:94.00ms
step:35/1770 train_time:2350ms step_avg:94.00ms
step:36/1770 train_time:2444ms step_avg:94.00ms
step:37/1770 train_time:2538ms step_avg:94.00ms
step:38/1770 train_time:2632ms step_avg:93.99ms
step:39/1770 train_time:2726ms step_avg:93.99ms
step:40/1770 train_time:2819ms step_avg:93.97ms
step:41/1770 train_time:2913ms step_avg:93.96ms
step:42/1770 train_time:3007ms step_avg:93.98ms
step:43/1770 train_time:3102ms step_avg:93.99ms
step:44/1770 train_time:3196ms step_avg:93.99ms
step:45/1770 train_time:3289ms step_avg:93.98ms
step:46/1770 train_time:3383ms step_avg:93.98ms
step:47/1770 train_time:3477ms step_avg:93.97ms
step:48/1770 train_time:3571ms step_avg:93.97ms
step:49/1770 train_time:3665ms step_avg:93.98ms
step:50/1770 train_time:3759ms step_avg:93.96ms
step:51/1770 train_time:3852ms step_avg:93.95ms
step:52/1770 train_time:3946ms step_avg:93.94ms
step:53/1770 train_time:4039ms step_avg:93.94ms
step:54/1770 train_time:4133ms step_avg:93.93ms
step:55/1770 train_time:4227ms step_avg:93.93ms
step:56/1770 train_time:4321ms step_avg:93.93ms
step:57/1770 train_time:4415ms step_avg:93.94ms
step:58/1770 train_time:4510ms step_avg:93.95ms
step:59/1770 train_time:4604ms step_avg:93.95ms
step:60/1770 train_time:4698ms step_avg:93.96ms
step:61/1770 train_time:4791ms step_avg:93.95ms
step:62/1770 train_time:4885ms step_avg:93.94ms
step:63/1770 train_time:4979ms step_avg:93.94ms
step:64/1770 train_time:5072ms step_avg:93.93ms
step:65/1770 train_time:5166ms step_avg:93.93ms
step:66/1770 train_time:5261ms step_avg:93.94ms
step:67/1770 train_time:5355ms step_avg:93.94ms
step:68/1770 train_time:5448ms step_avg:93.94ms
step:69/1770 train_time:5542ms step_avg:93.94ms
step:70/1770 train_time:5636ms step_avg:93.94ms
step:71/1770 train_time:5730ms step_avg:93.94ms
step:72/1770 train_time:5825ms step_avg:93.94ms
step:73/1770 train_time:5919ms step_avg:93.95ms
step:74/1770 train_time:6012ms step_avg:93.94ms
step:75/1770 train_time:6106ms step_avg:93.94ms
step:76/1770 train_time:6200ms step_avg:93.94ms
step:77/1770 train_time:6294ms step_avg:93.95ms
step:78/1770 train_time:6389ms step_avg:93.96ms
step:79/1770 train_time:6483ms step_avg:93.96ms
step:80/1770 train_time:6577ms step_avg:93.96ms
step:81/1770 train_time:6671ms step_avg:93.95ms
step:82/1770 train_time:6764ms step_avg:93.94ms
step:83/1770 train_time:6858ms step_avg:93.94ms
step:84/1770 train_time:6951ms step_avg:93.94ms
step:85/1770 train_time:7045ms step_avg:93.93ms
step:86/1770 train_time:7139ms step_avg:93.93ms
step:87/1770 train_time:7233ms step_avg:93.93ms
step:88/1770 train_time:7327ms step_avg:93.93ms
step:89/1770 train_time:7421ms step_avg:93.94ms
step:90/1770 train_time:7515ms step_avg:93.93ms
step:91/1770 train_time:7609ms step_avg:93.94ms
step:92/1770 train_time:7703ms step_avg:93.94ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8079ms step_avg:93.94ms
step:97/1770 train_time:8173ms step_avg:93.94ms
step:98/1770 train_time:8266ms step_avg:93.94ms
step:99/1770 train_time:8360ms step_avg:93.94ms
step:100/1770 train_time:8454ms step_avg:93.94ms
step:101/1770 train_time:8548ms step_avg:93.94ms
step:102/1770 train_time:8642ms step_avg:93.94ms
step:103/1770 train_time:8736ms step_avg:93.94ms
step:104/1770 train_time:8831ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.94ms
step:107/1770 train_time:9112ms step_avg:93.94ms
step:108/1770 train_time:9206ms step_avg:93.94ms
step:109/1770 train_time:9300ms step_avg:93.94ms
step:110/1770 train_time:9393ms step_avg:93.93ms
step:111/1770 train_time:9488ms step_avg:93.94ms
step:112/1770 train_time:9581ms step_avg:93.94ms
step:113/1770 train_time:9675ms step_avg:93.93ms
step:114/1770 train_time:9770ms step_avg:93.94ms
step:115/1770 train_time:9864ms step_avg:93.94ms
step:116/1770 train_time:9958ms step_avg:93.94ms
step:117/1770 train_time:10052ms step_avg:93.94ms
step:118/1770 train_time:10146ms step_avg:93.94ms
step:119/1770 train_time:10240ms step_avg:93.94ms
step:120/1770 train_time:10333ms step_avg:93.94ms
step:121/1770 train_time:10427ms step_avg:93.94ms
step:122/1770 train_time:10521ms step_avg:93.94ms
step:123/1770 train_time:10615ms step_avg:93.94ms
step:124/1770 train_time:10709ms step_avg:93.94ms
step:125/1770 train_time:10803ms step_avg:93.94ms
step:125/1770 val_loss:4.6432 train_time:10895ms step_avg:94.74ms
step:126/1770 train_time:10921ms step_avg:94.14ms
step:127/1770 train_time:10994ms step_avg:93.96ms
step:128/1770 train_time:11090ms step_avg:93.98ms
step:129/1770 train_time:11186ms step_avg:94.00ms
step:130/1770 train_time:11281ms step_avg:94.01ms
step:131/1770 train_time:11375ms step_avg:94.01ms
step:132/1770 train_time:11468ms step_avg:94.00ms
step:133/1770 train_time:11562ms step_avg:94.00ms
step:134/1770 train_time:11656ms step_avg:94.00ms
step:135/1770 train_time:11750ms step_avg:94.00ms
step:136/1770 train_time:11844ms step_avg:94.00ms
step:137/1770 train_time:11939ms step_avg:94.01ms
step:138/1770 train_time:12035ms step_avg:94.02ms
step:139/1770 train_time:12129ms step_avg:94.02ms
step:140/1770 train_time:12224ms step_avg:94.03ms
step:141/1770 train_time:12318ms step_avg:94.03ms
step:142/1770 train_time:12413ms step_avg:94.04ms
step:143/1770 train_time:12507ms step_avg:94.04ms
step:144/1770 train_time:12602ms step_avg:94.05ms
step:145/1770 train_time:12696ms step_avg:94.05ms
step:146/1770 train_time:12790ms step_avg:94.05ms
step:147/1770 train_time:12885ms step_avg:94.05ms
step:148/1770 train_time:12979ms step_avg:94.05ms
step:149/1770 train_time:13074ms step_avg:94.06ms
step:150/1770 train_time:13169ms step_avg:94.06ms
step:151/1770 train_time:13263ms step_avg:94.06ms
step:152/1770 train_time:13357ms step_avg:94.07ms
step:153/1770 train_time:13452ms step_avg:94.07ms
step:154/1770 train_time:13547ms step_avg:94.07ms
step:155/1770 train_time:13641ms step_avg:94.08ms
step:156/1770 train_time:13736ms step_avg:94.09ms
step:157/1770 train_time:13830ms step_avg:94.08ms
step:158/1770 train_time:13925ms step_avg:94.09ms
step:159/1770 train_time:14020ms step_avg:94.09ms
step:160/1770 train_time:14114ms step_avg:94.09ms
step:161/1770 train_time:14209ms step_avg:94.10ms
step:162/1770 train_time:14304ms step_avg:94.11ms
step:163/1770 train_time:14398ms step_avg:94.10ms
step:164/1770 train_time:14492ms step_avg:94.11ms
step:165/1770 train_time:14587ms step_avg:94.11ms
step:166/1770 train_time:14682ms step_avg:94.11ms
step:167/1770 train_time:14777ms step_avg:94.12ms
step:168/1770 train_time:14870ms step_avg:94.12ms
step:169/1770 train_time:14966ms step_avg:94.12ms
step:170/1770 train_time:15061ms step_avg:94.13ms
step:171/1770 train_time:15155ms step_avg:94.13ms
step:172/1770 train_time:15249ms step_avg:94.13ms
step:173/1770 train_time:15343ms step_avg:94.13ms
step:174/1770 train_time:15438ms step_avg:94.13ms
step:175/1770 train_time:15532ms step_avg:94.13ms
step:176/1770 train_time:15626ms step_avg:94.13ms
step:177/1770 train_time:15721ms step_avg:94.14ms
step:178/1770 train_time:15816ms step_avg:94.14ms
step:179/1770 train_time:15910ms step_avg:94.14ms
step:180/1770 train_time:16005ms step_avg:94.15ms
step:181/1770 train_time:16100ms step_avg:94.15ms
step:182/1770 train_time:16195ms step_avg:94.15ms
step:183/1770 train_time:16289ms step_avg:94.16ms
step:184/1770 train_time:16383ms step_avg:94.16ms
step:185/1770 train_time:16478ms step_avg:94.16ms
step:186/1770 train_time:16572ms step_avg:94.16ms
step:187/1770 train_time:16667ms step_avg:94.16ms
step:188/1770 train_time:16761ms step_avg:94.16ms
step:189/1770 train_time:16856ms step_avg:94.17ms
step:190/1770 train_time:16950ms step_avg:94.17ms
step:191/1770 train_time:17045ms step_avg:94.17ms
step:192/1770 train_time:17140ms step_avg:94.17ms
step:193/1770 train_time:17234ms step_avg:94.17ms
step:194/1770 train_time:17328ms step_avg:94.17ms
step:195/1770 train_time:17423ms step_avg:94.18ms
step:196/1770 train_time:17517ms step_avg:94.18ms
step:197/1770 train_time:17612ms step_avg:94.18ms
step:198/1770 train_time:17706ms step_avg:94.18ms
step:199/1770 train_time:17801ms step_avg:94.18ms
step:200/1770 train_time:17896ms step_avg:94.19ms
step:201/1770 train_time:17990ms step_avg:94.19ms
step:202/1770 train_time:18086ms step_avg:94.20ms
step:203/1770 train_time:18179ms step_avg:94.19ms
step:204/1770 train_time:18273ms step_avg:94.19ms
step:205/1770 train_time:18368ms step_avg:94.20ms
step:206/1770 train_time:18463ms step_avg:94.20ms
step:207/1770 train_time:18557ms step_avg:94.20ms
step:208/1770 train_time:18652ms step_avg:94.20ms
step:209/1770 train_time:18746ms step_avg:94.20ms
step:210/1770 train_time:18841ms step_avg:94.21ms
step:211/1770 train_time:18936ms step_avg:94.21ms
step:212/1770 train_time:19031ms step_avg:94.21ms
step:213/1770 train_time:19125ms step_avg:94.21ms
step:214/1770 train_time:19219ms step_avg:94.21ms
step:215/1770 train_time:19314ms step_avg:94.21ms
step:216/1770 train_time:19408ms step_avg:94.21ms
step:217/1770 train_time:19503ms step_avg:94.22ms
step:218/1770 train_time:19598ms step_avg:94.22ms
step:219/1770 train_time:19692ms step_avg:94.22ms
step:220/1770 train_time:19786ms step_avg:94.22ms
step:221/1770 train_time:19880ms step_avg:94.22ms
step:222/1770 train_time:19975ms step_avg:94.22ms
step:223/1770 train_time:20069ms step_avg:94.22ms
step:224/1770 train_time:20164ms step_avg:94.22ms
step:225/1770 train_time:20258ms step_avg:94.22ms
step:226/1770 train_time:20353ms step_avg:94.23ms
step:227/1770 train_time:20447ms step_avg:94.23ms
step:228/1770 train_time:20542ms step_avg:94.23ms
step:229/1770 train_time:20636ms step_avg:94.23ms
step:230/1770 train_time:20731ms step_avg:94.23ms
step:231/1770 train_time:20826ms step_avg:94.24ms
step:232/1770 train_time:20921ms step_avg:94.24ms
step:233/1770 train_time:21016ms step_avg:94.24ms
step:234/1770 train_time:21110ms step_avg:94.24ms
step:235/1770 train_time:21205ms step_avg:94.24ms
step:236/1770 train_time:21301ms step_avg:94.25ms
step:237/1770 train_time:21394ms step_avg:94.25ms
step:238/1770 train_time:21488ms step_avg:94.25ms
step:239/1770 train_time:21583ms step_avg:94.25ms
step:240/1770 train_time:21677ms step_avg:94.25ms
step:241/1770 train_time:21775ms step_avg:94.26ms
step:242/1770 train_time:21866ms step_avg:94.25ms
step:243/1770 train_time:21960ms step_avg:94.25ms
step:244/1770 train_time:22055ms step_avg:94.25ms
step:245/1770 train_time:22149ms step_avg:94.25ms
step:246/1770 train_time:22244ms step_avg:94.25ms
step:247/1770 train_time:22338ms step_avg:94.25ms
step:248/1770 train_time:22432ms step_avg:94.25ms
step:249/1770 train_time:22526ms step_avg:94.25ms
step:250/1770 train_time:22620ms step_avg:94.25ms
step:250/1770 val_loss:4.1143 train_time:22714ms step_avg:94.64ms
step:251/1770 train_time:22738ms step_avg:94.35ms
step:252/1770 train_time:22817ms step_avg:94.29ms
step:253/1770 train_time:22919ms step_avg:94.32ms
step:254/1770 train_time:23017ms step_avg:94.33ms
step:255/1770 train_time:23110ms step_avg:94.33ms
step:256/1770 train_time:23204ms step_avg:94.32ms
step:257/1770 train_time:23298ms step_avg:94.32ms
step:258/1770 train_time:23392ms step_avg:94.32ms
step:259/1770 train_time:23485ms step_avg:94.32ms
step:260/1770 train_time:23579ms step_avg:94.32ms
step:261/1770 train_time:23673ms step_avg:94.32ms
step:262/1770 train_time:23767ms step_avg:94.31ms
step:263/1770 train_time:23862ms step_avg:94.31ms
step:264/1770 train_time:23957ms step_avg:94.32ms
step:265/1770 train_time:24053ms step_avg:94.33ms
step:266/1770 train_time:24149ms step_avg:94.33ms
step:267/1770 train_time:24243ms step_avg:94.33ms
step:268/1770 train_time:24338ms step_avg:94.33ms
step:269/1770 train_time:24434ms step_avg:94.34ms
step:270/1770 train_time:24528ms step_avg:94.34ms
step:271/1770 train_time:24623ms step_avg:94.34ms
step:272/1770 train_time:24718ms step_avg:94.34ms
step:273/1770 train_time:24813ms step_avg:94.34ms
step:274/1770 train_time:24908ms step_avg:94.35ms
step:275/1770 train_time:25004ms step_avg:94.35ms
step:276/1770 train_time:25102ms step_avg:94.37ms
step:277/1770 train_time:25194ms step_avg:94.36ms
step:278/1770 train_time:25289ms step_avg:94.36ms
step:279/1770 train_time:25384ms step_avg:94.36ms
step:280/1770 train_time:25479ms step_avg:94.37ms
step:281/1770 train_time:25573ms step_avg:94.37ms
step:282/1770 train_time:25669ms step_avg:94.37ms
step:283/1770 train_time:25763ms step_avg:94.37ms
step:284/1770 train_time:25858ms step_avg:94.37ms
step:285/1770 train_time:25954ms step_avg:94.38ms
step:286/1770 train_time:26050ms step_avg:94.38ms
step:287/1770 train_time:26145ms step_avg:94.38ms
step:288/1770 train_time:26240ms step_avg:94.39ms
step:289/1770 train_time:26335ms step_avg:94.39ms
step:290/1770 train_time:26430ms step_avg:94.39ms
step:291/1770 train_time:26525ms step_avg:94.39ms
step:292/1770 train_time:26620ms step_avg:94.40ms
step:293/1770 train_time:26715ms step_avg:94.40ms
step:294/1770 train_time:26810ms step_avg:94.40ms
step:295/1770 train_time:26905ms step_avg:94.40ms
step:296/1770 train_time:27003ms step_avg:94.42ms
step:297/1770 train_time:27096ms step_avg:94.41ms
step:298/1770 train_time:27191ms step_avg:94.41ms
step:299/1770 train_time:27286ms step_avg:94.42ms
step:300/1770 train_time:27382ms step_avg:94.42ms
step:301/1770 train_time:27478ms step_avg:94.43ms
step:302/1770 train_time:27573ms step_avg:94.43ms
step:303/1770 train_time:27668ms step_avg:94.43ms
step:304/1770 train_time:27762ms step_avg:94.43ms
step:305/1770 train_time:27858ms step_avg:94.43ms
step:306/1770 train_time:27953ms step_avg:94.44ms
step:307/1770 train_time:28048ms step_avg:94.44ms
step:308/1770 train_time:28142ms step_avg:94.44ms
step:309/1770 train_time:28238ms step_avg:94.44ms
step:310/1770 train_time:28333ms step_avg:94.44ms
step:311/1770 train_time:28428ms step_avg:94.45ms
step:312/1770 train_time:28523ms step_avg:94.45ms
step:313/1770 train_time:28619ms step_avg:94.45ms
step:314/1770 train_time:28714ms step_avg:94.45ms
step:315/1770 train_time:28809ms step_avg:94.46ms
step:316/1770 train_time:28909ms step_avg:94.47ms
step:317/1770 train_time:28999ms step_avg:94.46ms
step:318/1770 train_time:29094ms step_avg:94.46ms
step:319/1770 train_time:29189ms step_avg:94.46ms
step:320/1770 train_time:29284ms step_avg:94.47ms
step:321/1770 train_time:29379ms step_avg:94.47ms
step:322/1770 train_time:29474ms step_avg:94.47ms
step:323/1770 train_time:29570ms step_avg:94.47ms
step:324/1770 train_time:29665ms step_avg:94.47ms
step:325/1770 train_time:29760ms step_avg:94.48ms
step:326/1770 train_time:29855ms step_avg:94.48ms
step:327/1770 train_time:29951ms step_avg:94.48ms
step:328/1770 train_time:30046ms step_avg:94.48ms
step:329/1770 train_time:30141ms step_avg:94.49ms
step:330/1770 train_time:30236ms step_avg:94.49ms
step:331/1770 train_time:30331ms step_avg:94.49ms
step:332/1770 train_time:30426ms step_avg:94.49ms
step:333/1770 train_time:30521ms step_avg:94.49ms
step:334/1770 train_time:30616ms step_avg:94.49ms
step:335/1770 train_time:30713ms step_avg:94.50ms
step:336/1770 train_time:30806ms step_avg:94.50ms
step:337/1770 train_time:30901ms step_avg:94.50ms
step:338/1770 train_time:30997ms step_avg:94.50ms
step:339/1770 train_time:31092ms step_avg:94.51ms
step:340/1770 train_time:31187ms step_avg:94.51ms
step:341/1770 train_time:31282ms step_avg:94.51ms
step:342/1770 train_time:31377ms step_avg:94.51ms
step:343/1770 train_time:31472ms step_avg:94.51ms
step:344/1770 train_time:31567ms step_avg:94.51ms
step:345/1770 train_time:31662ms step_avg:94.51ms
step:346/1770 train_time:31757ms step_avg:94.52ms
step:347/1770 train_time:31853ms step_avg:94.52ms
step:348/1770 train_time:31948ms step_avg:94.52ms
step:349/1770 train_time:32043ms step_avg:94.52ms
step:350/1770 train_time:32138ms step_avg:94.52ms
step:351/1770 train_time:32233ms step_avg:94.53ms
step:352/1770 train_time:32329ms step_avg:94.53ms
step:353/1770 train_time:32423ms step_avg:94.53ms
step:354/1770 train_time:32518ms step_avg:94.53ms
step:355/1770 train_time:32613ms step_avg:94.53ms
step:356/1770 train_time:32708ms step_avg:94.53ms
step:357/1770 train_time:32803ms step_avg:94.53ms
step:358/1770 train_time:32898ms step_avg:94.54ms
step:359/1770 train_time:32994ms step_avg:94.54ms
step:360/1770 train_time:33089ms step_avg:94.54ms
step:361/1770 train_time:33183ms step_avg:94.54ms
step:362/1770 train_time:33278ms step_avg:94.54ms
step:363/1770 train_time:33373ms step_avg:94.54ms
step:364/1770 train_time:33468ms step_avg:94.54ms
step:365/1770 train_time:33563ms step_avg:94.54ms
step:366/1770 train_time:33659ms step_avg:94.55ms
step:367/1770 train_time:33754ms step_avg:94.55ms
step:368/1770 train_time:33850ms step_avg:94.55ms
step:369/1770 train_time:33944ms step_avg:94.55ms
step:370/1770 train_time:34039ms step_avg:94.55ms
step:371/1770 train_time:34135ms step_avg:94.56ms
step:372/1770 train_time:34229ms step_avg:94.56ms
step:373/1770 train_time:34323ms step_avg:94.55ms
step:374/1770 train_time:34419ms step_avg:94.56ms
step:375/1770 train_time:34514ms step_avg:94.56ms
step:375/1770 val_loss:3.9032 train_time:34608ms step_avg:94.82ms
step:376/1770 train_time:34631ms step_avg:94.62ms
step:377/1770 train_time:34716ms step_avg:94.59ms
step:378/1770 train_time:34816ms step_avg:94.61ms
step:379/1770 train_time:34912ms step_avg:94.61ms
step:380/1770 train_time:35006ms step_avg:94.61ms
step:381/1770 train_time:35101ms step_avg:94.61ms
step:382/1770 train_time:35195ms step_avg:94.61ms
step:383/1770 train_time:35290ms step_avg:94.61ms
step:384/1770 train_time:35384ms step_avg:94.61ms
step:385/1770 train_time:35478ms step_avg:94.61ms
step:386/1770 train_time:35573ms step_avg:94.61ms
step:387/1770 train_time:35668ms step_avg:94.61ms
step:388/1770 train_time:35764ms step_avg:94.61ms
step:389/1770 train_time:35861ms step_avg:94.62ms
step:390/1770 train_time:35956ms step_avg:94.62ms
step:391/1770 train_time:36051ms step_avg:94.62ms
step:392/1770 train_time:36146ms step_avg:94.62ms
step:393/1770 train_time:36241ms step_avg:94.62ms
step:394/1770 train_time:36335ms step_avg:94.62ms
step:395/1770 train_time:36429ms step_avg:94.62ms
step:396/1770 train_time:36525ms step_avg:94.62ms
step:397/1770 train_time:36621ms step_avg:94.63ms
step:398/1770 train_time:36718ms step_avg:94.63ms
step:399/1770 train_time:36815ms step_avg:94.64ms
step:400/1770 train_time:36913ms step_avg:94.65ms
step:401/1770 train_time:37009ms step_avg:94.65ms
step:402/1770 train_time:37106ms step_avg:94.66ms
step:403/1770 train_time:37203ms step_avg:94.66ms
step:404/1770 train_time:37299ms step_avg:94.67ms
step:405/1770 train_time:37396ms step_avg:94.67ms
step:406/1770 train_time:37492ms step_avg:94.68ms
step:407/1770 train_time:37589ms step_avg:94.68ms
step:408/1770 train_time:37686ms step_avg:94.69ms
step:409/1770 train_time:37783ms step_avg:94.69ms
step:410/1770 train_time:37881ms step_avg:94.70ms
step:411/1770 train_time:37979ms step_avg:94.71ms
step:412/1770 train_time:38076ms step_avg:94.72ms
step:413/1770 train_time:38173ms step_avg:94.72ms
step:414/1770 train_time:38271ms step_avg:94.73ms
step:415/1770 train_time:38368ms step_avg:94.74ms
step:416/1770 train_time:38464ms step_avg:94.74ms
step:417/1770 train_time:38562ms step_avg:94.75ms
step:418/1770 train_time:38659ms step_avg:94.75ms
step:419/1770 train_time:38756ms step_avg:94.76ms
step:420/1770 train_time:38853ms step_avg:94.76ms
step:421/1770 train_time:38951ms step_avg:94.77ms
step:422/1770 train_time:39047ms step_avg:94.77ms
step:423/1770 train_time:39144ms step_avg:94.78ms
step:424/1770 train_time:39241ms step_avg:94.79ms
step:425/1770 train_time:39338ms step_avg:94.79ms
step:426/1770 train_time:39438ms step_avg:94.80ms
step:427/1770 train_time:39532ms step_avg:94.80ms
step:428/1770 train_time:39629ms step_avg:94.81ms
step:429/1770 train_time:39725ms step_avg:94.81ms
step:430/1770 train_time:39822ms step_avg:94.81ms
step:431/1770 train_time:39919ms step_avg:94.82ms
step:432/1770 train_time:40017ms step_avg:94.83ms
step:433/1770 train_time:40114ms step_avg:94.83ms
step:434/1770 train_time:40211ms step_avg:94.84ms
step:435/1770 train_time:40307ms step_avg:94.84ms
step:436/1770 train_time:40404ms step_avg:94.85ms
step:437/1770 train_time:40501ms step_avg:94.85ms
step:438/1770 train_time:40599ms step_avg:94.86ms
step:439/1770 train_time:40696ms step_avg:94.86ms
step:440/1770 train_time:40793ms step_avg:94.87ms
step:441/1770 train_time:40890ms step_avg:94.87ms
step:442/1770 train_time:40987ms step_avg:94.88ms
step:443/1770 train_time:41083ms step_avg:94.88ms
step:444/1770 train_time:41181ms step_avg:94.89ms
step:445/1770 train_time:41278ms step_avg:94.89ms
step:446/1770 train_time:41375ms step_avg:94.90ms
step:447/1770 train_time:41472ms step_avg:94.90ms
step:448/1770 train_time:41569ms step_avg:94.91ms
step:449/1770 train_time:41666ms step_avg:94.91ms
step:450/1770 train_time:41762ms step_avg:94.91ms
step:451/1770 train_time:41859ms step_avg:94.92ms
step:452/1770 train_time:41956ms step_avg:94.92ms
step:453/1770 train_time:42053ms step_avg:94.93ms
step:454/1770 train_time:42150ms step_avg:94.93ms
step:455/1770 train_time:42247ms step_avg:94.94ms
step:456/1770 train_time:42343ms step_avg:94.94ms
step:457/1770 train_time:42440ms step_avg:94.94ms
step:458/1770 train_time:42538ms step_avg:94.95ms
step:459/1770 train_time:42638ms step_avg:94.96ms
step:460/1770 train_time:42732ms step_avg:94.96ms
step:461/1770 train_time:42829ms step_avg:94.96ms
step:462/1770 train_time:42926ms step_avg:94.97ms
step:463/1770 train_time:43022ms step_avg:94.97ms
step:464/1770 train_time:43119ms step_avg:94.98ms
step:465/1770 train_time:43216ms step_avg:94.98ms
step:466/1770 train_time:43313ms step_avg:94.98ms
step:467/1770 train_time:43410ms step_avg:94.99ms
step:468/1770 train_time:43507ms step_avg:94.99ms
step:469/1770 train_time:43603ms step_avg:95.00ms
step:470/1770 train_time:43700ms step_avg:95.00ms
step:471/1770 train_time:43797ms step_avg:95.00ms
step:472/1770 train_time:43894ms step_avg:95.01ms
step:473/1770 train_time:43991ms step_avg:95.01ms
step:474/1770 train_time:44088ms step_avg:95.02ms
step:475/1770 train_time:44184ms step_avg:95.02ms
step:476/1770 train_time:44282ms step_avg:95.03ms
step:477/1770 train_time:44379ms step_avg:95.03ms
step:478/1770 train_time:44476ms step_avg:95.03ms
step:479/1770 train_time:44572ms step_avg:95.04ms
step:480/1770 train_time:44669ms step_avg:95.04ms
step:481/1770 train_time:44765ms step_avg:95.04ms
step:482/1770 train_time:44862ms step_avg:95.05ms
step:483/1770 train_time:44960ms step_avg:95.05ms
step:484/1770 train_time:45057ms step_avg:95.06ms
step:485/1770 train_time:45155ms step_avg:95.06ms
step:486/1770 train_time:45252ms step_avg:95.07ms
step:487/1770 train_time:45350ms step_avg:95.07ms
step:488/1770 train_time:45447ms step_avg:95.08ms
step:489/1770 train_time:45544ms step_avg:95.08ms
step:490/1770 train_time:45641ms step_avg:95.09ms
step:491/1770 train_time:45738ms step_avg:95.09ms
step:492/1770 train_time:45835ms step_avg:95.09ms
step:493/1770 train_time:45933ms step_avg:95.10ms
step:494/1770 train_time:46030ms step_avg:95.10ms
step:495/1770 train_time:46127ms step_avg:95.11ms
step:496/1770 train_time:46223ms step_avg:95.11ms
step:497/1770 train_time:46320ms step_avg:95.11ms
step:498/1770 train_time:46417ms step_avg:95.12ms
step:499/1770 train_time:46515ms step_avg:95.12ms
step:500/1770 train_time:46613ms step_avg:95.13ms
step:500/1770 val_loss:3.7537 train_time:46708ms step_avg:95.32ms
step:501/1770 train_time:46731ms step_avg:95.17ms
step:502/1770 train_time:46818ms step_avg:95.16ms
step:503/1770 train_time:46919ms step_avg:95.17ms
step:504/1770 train_time:47015ms step_avg:95.17ms
step:505/1770 train_time:47112ms step_avg:95.18ms
step:506/1770 train_time:47208ms step_avg:95.18ms
step:507/1770 train_time:47305ms step_avg:95.18ms
step:508/1770 train_time:47401ms step_avg:95.18ms
step:509/1770 train_time:47497ms step_avg:95.19ms
step:510/1770 train_time:47594ms step_avg:95.19ms
step:511/1770 train_time:47690ms step_avg:95.19ms
step:512/1770 train_time:47788ms step_avg:95.20ms
step:513/1770 train_time:47887ms step_avg:95.20ms
step:514/1770 train_time:47985ms step_avg:95.21ms
step:515/1770 train_time:48083ms step_avg:95.21ms
step:516/1770 train_time:48179ms step_avg:95.22ms
step:517/1770 train_time:48275ms step_avg:95.22ms
step:518/1770 train_time:48372ms step_avg:95.22ms
step:519/1770 train_time:48469ms step_avg:95.22ms
step:520/1770 train_time:48565ms step_avg:95.23ms
step:521/1770 train_time:48662ms step_avg:95.23ms
step:522/1770 train_time:48759ms step_avg:95.23ms
step:523/1770 train_time:48856ms step_avg:95.24ms
step:524/1770 train_time:48953ms step_avg:95.24ms
step:525/1770 train_time:49051ms step_avg:95.24ms
step:526/1770 train_time:49149ms step_avg:95.25ms
step:527/1770 train_time:49249ms step_avg:95.26ms
step:528/1770 train_time:49344ms step_avg:95.26ms
step:529/1770 train_time:49441ms step_avg:95.26ms
step:530/1770 train_time:49537ms step_avg:95.26ms
step:531/1770 train_time:49634ms step_avg:95.27ms
step:532/1770 train_time:49731ms step_avg:95.27ms
step:533/1770 train_time:49829ms step_avg:95.28ms
step:534/1770 train_time:49926ms step_avg:95.28ms
step:535/1770 train_time:50024ms step_avg:95.28ms
step:536/1770 train_time:50122ms step_avg:95.29ms
step:537/1770 train_time:50219ms step_avg:95.29ms
step:538/1770 train_time:50316ms step_avg:95.30ms
step:539/1770 train_time:50414ms step_avg:95.30ms
step:540/1770 train_time:50511ms step_avg:95.30ms
step:541/1770 train_time:50609ms step_avg:95.31ms
step:542/1770 train_time:50706ms step_avg:95.31ms
step:543/1770 train_time:50804ms step_avg:95.32ms
step:544/1770 train_time:50901ms step_avg:95.32ms
step:545/1770 train_time:50999ms step_avg:95.32ms
step:546/1770 train_time:51096ms step_avg:95.33ms
step:547/1770 train_time:51193ms step_avg:95.33ms
step:548/1770 train_time:51291ms step_avg:95.34ms
step:549/1770 train_time:51388ms step_avg:95.34ms
step:550/1770 train_time:51486ms step_avg:95.34ms
step:551/1770 train_time:51583ms step_avg:95.35ms
step:552/1770 train_time:51680ms step_avg:95.35ms
step:553/1770 train_time:51777ms step_avg:95.35ms
step:554/1770 train_time:51874ms step_avg:95.36ms
step:555/1770 train_time:51971ms step_avg:95.36ms
step:556/1770 train_time:52069ms step_avg:95.36ms
step:557/1770 train_time:52166ms step_avg:95.37ms
step:558/1770 train_time:52264ms step_avg:95.37ms
step:559/1770 train_time:52362ms step_avg:95.38ms
step:560/1770 train_time:52458ms step_avg:95.38ms
step:561/1770 train_time:52555ms step_avg:95.38ms
step:562/1770 train_time:52653ms step_avg:95.39ms
step:563/1770 train_time:52752ms step_avg:95.39ms
step:564/1770 train_time:52847ms step_avg:95.39ms
step:565/1770 train_time:52945ms step_avg:95.40ms
step:566/1770 train_time:53042ms step_avg:95.40ms
step:567/1770 train_time:53140ms step_avg:95.40ms
step:568/1770 train_time:53237ms step_avg:95.41ms
step:569/1770 train_time:53334ms step_avg:95.41ms
step:570/1770 train_time:53432ms step_avg:95.41ms
step:571/1770 train_time:53530ms step_avg:95.42ms
step:572/1770 train_time:53627ms step_avg:95.42ms
step:573/1770 train_time:53724ms step_avg:95.42ms
step:574/1770 train_time:53822ms step_avg:95.43ms
step:575/1770 train_time:53919ms step_avg:95.43ms
step:576/1770 train_time:54016ms step_avg:95.43ms
step:577/1770 train_time:54113ms step_avg:95.44ms
step:578/1770 train_time:54210ms step_avg:95.44ms
step:579/1770 train_time:54308ms step_avg:95.44ms
step:580/1770 train_time:54405ms step_avg:95.45ms
step:581/1770 train_time:54503ms step_avg:95.45ms
step:582/1770 train_time:54600ms step_avg:95.46ms
step:583/1770 train_time:54698ms step_avg:95.46ms
step:584/1770 train_time:54795ms step_avg:95.46ms
step:585/1770 train_time:54893ms step_avg:95.47ms
step:586/1770 train_time:54990ms step_avg:95.47ms
step:587/1770 train_time:55087ms step_avg:95.47ms
step:588/1770 train_time:55185ms step_avg:95.48ms
step:589/1770 train_time:55282ms step_avg:95.48ms
step:590/1770 train_time:55379ms step_avg:95.48ms
step:591/1770 train_time:55476ms step_avg:95.48ms
step:592/1770 train_time:55574ms step_avg:95.49ms
step:593/1770 train_time:55672ms step_avg:95.49ms
step:594/1770 train_time:55769ms step_avg:95.50ms
step:595/1770 train_time:55867ms step_avg:95.50ms
step:596/1770 train_time:55964ms step_avg:95.50ms
step:597/1770 train_time:56062ms step_avg:95.51ms
step:598/1770 train_time:56159ms step_avg:95.51ms
step:599/1770 train_time:56257ms step_avg:95.51ms
step:600/1770 train_time:56353ms step_avg:95.51ms
step:601/1770 train_time:56451ms step_avg:95.52ms
step:602/1770 train_time:56548ms step_avg:95.52ms
step:603/1770 train_time:56646ms step_avg:95.52ms
step:604/1770 train_time:56743ms step_avg:95.53ms
step:605/1770 train_time:56841ms step_avg:95.53ms
step:606/1770 train_time:56938ms step_avg:95.53ms
step:607/1770 train_time:57036ms step_avg:95.54ms
step:608/1770 train_time:57133ms step_avg:95.54ms
step:609/1770 train_time:57230ms step_avg:95.54ms
step:610/1770 train_time:57328ms step_avg:95.55ms
step:611/1770 train_time:57425ms step_avg:95.55ms
step:612/1770 train_time:57522ms step_avg:95.55ms
step:613/1770 train_time:57619ms step_avg:95.55ms
step:614/1770 train_time:57716ms step_avg:95.56ms
step:615/1770 train_time:57814ms step_avg:95.56ms
step:616/1770 train_time:57911ms step_avg:95.56ms
step:617/1770 train_time:58009ms step_avg:95.57ms
step:618/1770 train_time:58106ms step_avg:95.57ms
step:619/1770 train_time:58203ms step_avg:95.57ms
step:620/1770 train_time:58301ms step_avg:95.58ms
step:621/1770 train_time:58398ms step_avg:95.58ms
step:622/1770 train_time:58495ms step_avg:95.58ms
step:623/1770 train_time:58593ms step_avg:95.58ms
step:624/1770 train_time:58690ms step_avg:95.59ms
step:625/1770 train_time:58788ms step_avg:95.59ms
step:625/1770 val_loss:3.6657 train_time:58884ms step_avg:95.75ms
step:626/1770 train_time:58906ms step_avg:95.63ms
step:627/1770 train_time:58994ms step_avg:95.61ms
step:628/1770 train_time:59096ms step_avg:95.62ms
step:629/1770 train_time:59194ms step_avg:95.63ms
step:630/1770 train_time:59291ms step_avg:95.63ms
step:631/1770 train_time:59388ms step_avg:95.63ms
step:632/1770 train_time:59485ms step_avg:95.64ms
step:633/1770 train_time:59582ms step_avg:95.64ms
step:634/1770 train_time:59679ms step_avg:95.64ms
step:635/1770 train_time:59776ms step_avg:95.64ms
step:636/1770 train_time:59873ms step_avg:95.64ms
step:637/1770 train_time:59970ms step_avg:95.65ms
step:638/1770 train_time:60069ms step_avg:95.65ms
step:639/1770 train_time:60167ms step_avg:95.66ms
step:640/1770 train_time:60265ms step_avg:95.66ms
step:641/1770 train_time:60363ms step_avg:95.66ms
step:642/1770 train_time:60459ms step_avg:95.66ms
step:643/1770 train_time:60557ms step_avg:95.67ms
step:644/1770 train_time:60654ms step_avg:95.67ms
step:645/1770 train_time:60750ms step_avg:95.67ms
step:646/1770 train_time:60848ms step_avg:95.67ms
step:647/1770 train_time:60945ms step_avg:95.68ms
step:648/1770 train_time:61043ms step_avg:95.68ms
step:649/1770 train_time:61140ms step_avg:95.68ms
step:650/1770 train_time:61238ms step_avg:95.68ms
step:651/1770 train_time:61335ms step_avg:95.69ms
step:652/1770 train_time:61433ms step_avg:95.69ms
step:653/1770 train_time:61530ms step_avg:95.69ms
step:654/1770 train_time:61627ms step_avg:95.69ms
step:655/1770 train_time:61724ms step_avg:95.70ms
step:656/1770 train_time:61821ms step_avg:95.70ms
step:657/1770 train_time:61918ms step_avg:95.70ms
step:658/1770 train_time:62017ms step_avg:95.71ms
step:659/1770 train_time:62117ms step_avg:95.71ms
step:660/1770 train_time:62216ms step_avg:95.72ms
step:661/1770 train_time:62315ms step_avg:95.72ms
step:662/1770 train_time:62414ms step_avg:95.73ms
step:663/1770 train_time:62513ms step_avg:95.73ms
step:664/1770 train_time:62612ms step_avg:95.74ms
step:665/1770 train_time:62711ms step_avg:95.74ms
step:666/1770 train_time:62810ms step_avg:95.75ms
step:667/1770 train_time:62909ms step_avg:95.75ms
step:668/1770 train_time:63008ms step_avg:95.76ms
step:669/1770 train_time:63108ms step_avg:95.76ms
step:670/1770 train_time:63207ms step_avg:95.77ms
step:671/1770 train_time:63308ms step_avg:95.78ms
step:672/1770 train_time:63408ms step_avg:95.78ms
step:673/1770 train_time:63507ms step_avg:95.79ms
step:674/1770 train_time:63607ms step_avg:95.79ms
step:675/1770 train_time:63707ms step_avg:95.80ms
step:676/1770 train_time:63805ms step_avg:95.80ms
step:677/1770 train_time:63904ms step_avg:95.81ms
step:678/1770 train_time:64003ms step_avg:95.81ms
step:679/1770 train_time:64101ms step_avg:95.82ms
step:680/1770 train_time:64200ms step_avg:95.82ms
step:681/1770 train_time:64298ms step_avg:95.82ms
step:682/1770 train_time:64397ms step_avg:95.83ms
step:683/1770 train_time:64497ms step_avg:95.83ms
step:684/1770 train_time:64596ms step_avg:95.84ms
step:685/1770 train_time:64695ms step_avg:95.84ms
step:686/1770 train_time:64794ms step_avg:95.85ms
step:687/1770 train_time:64894ms step_avg:95.86ms
step:688/1770 train_time:64995ms step_avg:95.86ms
step:689/1770 train_time:65095ms step_avg:95.87ms
step:690/1770 train_time:65194ms step_avg:95.87ms
step:691/1770 train_time:65293ms step_avg:95.88ms
step:692/1770 train_time:65393ms step_avg:95.88ms
step:693/1770 train_time:65492ms step_avg:95.89ms
step:694/1770 train_time:65591ms step_avg:95.89ms
step:695/1770 train_time:65690ms step_avg:95.90ms
step:696/1770 train_time:65789ms step_avg:95.90ms
step:697/1770 train_time:65889ms step_avg:95.91ms
step:698/1770 train_time:65988ms step_avg:95.91ms
step:699/1770 train_time:66087ms step_avg:95.92ms
step:700/1770 train_time:66187ms step_avg:95.92ms
step:701/1770 train_time:66286ms step_avg:95.93ms
step:702/1770 train_time:66386ms step_avg:95.93ms
step:703/1770 train_time:66486ms step_avg:95.94ms
step:704/1770 train_time:66586ms step_avg:95.94ms
step:705/1770 train_time:66686ms step_avg:95.95ms
step:706/1770 train_time:66786ms step_avg:95.96ms
step:707/1770 train_time:66885ms step_avg:95.96ms
step:708/1770 train_time:66984ms step_avg:95.97ms
step:709/1770 train_time:67083ms step_avg:95.97ms
step:710/1770 train_time:67181ms step_avg:95.97ms
step:711/1770 train_time:67280ms step_avg:95.98ms
step:712/1770 train_time:67379ms step_avg:95.98ms
step:713/1770 train_time:67478ms step_avg:95.99ms
step:714/1770 train_time:67576ms step_avg:95.99ms
step:715/1770 train_time:67676ms step_avg:95.99ms
step:716/1770 train_time:67775ms step_avg:96.00ms
step:717/1770 train_time:67874ms step_avg:96.00ms
step:718/1770 train_time:67975ms step_avg:96.01ms
step:719/1770 train_time:68074ms step_avg:96.01ms
step:720/1770 train_time:68173ms step_avg:96.02ms
step:721/1770 train_time:68273ms step_avg:96.02ms
step:722/1770 train_time:68372ms step_avg:96.03ms
step:723/1770 train_time:68472ms step_avg:96.03ms
step:724/1770 train_time:68570ms step_avg:96.04ms
step:725/1770 train_time:68669ms step_avg:96.04ms
step:726/1770 train_time:68768ms step_avg:96.05ms
step:727/1770 train_time:68867ms step_avg:96.05ms
step:728/1770 train_time:68967ms step_avg:96.05ms
step:729/1770 train_time:69066ms step_avg:96.06ms
step:730/1770 train_time:69166ms step_avg:96.06ms
step:731/1770 train_time:69266ms step_avg:96.07ms
step:732/1770 train_time:69365ms step_avg:96.07ms
step:733/1770 train_time:69465ms step_avg:96.08ms
step:734/1770 train_time:69564ms step_avg:96.08ms
step:735/1770 train_time:69663ms step_avg:96.09ms
step:736/1770 train_time:69763ms step_avg:96.09ms
step:737/1770 train_time:69861ms step_avg:96.09ms
step:738/1770 train_time:69960ms step_avg:96.10ms
step:739/1770 train_time:70058ms step_avg:96.10ms
step:740/1770 train_time:70157ms step_avg:96.11ms
step:741/1770 train_time:70256ms step_avg:96.11ms
step:742/1770 train_time:70355ms step_avg:96.11ms
step:743/1770 train_time:70455ms step_avg:96.12ms
step:744/1770 train_time:70554ms step_avg:96.12ms
step:745/1770 train_time:70654ms step_avg:96.13ms
step:746/1770 train_time:70753ms step_avg:96.13ms
step:747/1770 train_time:70852ms step_avg:96.14ms
step:748/1770 train_time:70951ms step_avg:96.14ms
step:749/1770 train_time:71050ms step_avg:96.14ms
step:750/1770 train_time:71149ms step_avg:96.15ms
step:750/1770 val_loss:3.6007 train_time:71247ms step_avg:96.28ms
step:751/1770 train_time:71267ms step_avg:96.18ms
step:752/1770 train_time:71358ms step_avg:96.17ms
step:753/1770 train_time:71460ms step_avg:96.18ms
step:754/1770 train_time:71560ms step_avg:96.18ms
step:755/1770 train_time:71658ms step_avg:96.18ms
step:756/1770 train_time:71756ms step_avg:96.19ms
step:757/1770 train_time:71855ms step_avg:96.19ms
step:758/1770 train_time:71953ms step_avg:96.19ms
step:759/1770 train_time:72051ms step_avg:96.20ms
step:760/1770 train_time:72149ms step_avg:96.20ms
step:761/1770 train_time:72250ms step_avg:96.20ms
step:762/1770 train_time:72348ms step_avg:96.21ms
step:763/1770 train_time:72448ms step_avg:96.21ms
step:764/1770 train_time:72547ms step_avg:96.22ms
step:765/1770 train_time:72646ms step_avg:96.22ms
step:766/1770 train_time:72745ms step_avg:96.22ms
step:767/1770 train_time:72845ms step_avg:96.23ms
step:768/1770 train_time:72945ms step_avg:96.23ms
step:769/1770 train_time:73044ms step_avg:96.24ms
step:770/1770 train_time:73142ms step_avg:96.24ms
step:771/1770 train_time:73241ms step_avg:96.24ms
step:772/1770 train_time:73341ms step_avg:96.25ms
step:773/1770 train_time:73441ms step_avg:96.25ms
step:774/1770 train_time:73541ms step_avg:96.26ms
step:775/1770 train_time:73640ms step_avg:96.26ms
step:776/1770 train_time:73743ms step_avg:96.27ms
step:777/1770 train_time:73839ms step_avg:96.27ms
step:778/1770 train_time:73937ms step_avg:96.27ms
step:779/1770 train_time:74037ms step_avg:96.28ms
step:780/1770 train_time:74135ms step_avg:96.28ms
step:781/1770 train_time:74234ms step_avg:96.28ms
step:782/1770 train_time:74333ms step_avg:96.29ms
step:783/1770 train_time:74432ms step_avg:96.29ms
step:784/1770 train_time:74531ms step_avg:96.29ms
step:785/1770 train_time:74630ms step_avg:96.30ms
step:786/1770 train_time:74728ms step_avg:96.30ms
step:787/1770 train_time:74827ms step_avg:96.30ms
step:788/1770 train_time:74927ms step_avg:96.31ms
step:789/1770 train_time:75027ms step_avg:96.31ms
step:790/1770 train_time:75127ms step_avg:96.32ms
step:791/1770 train_time:75227ms step_avg:96.32ms
step:792/1770 train_time:75326ms step_avg:96.33ms
step:793/1770 train_time:75426ms step_avg:96.33ms
step:794/1770 train_time:75526ms step_avg:96.33ms
step:795/1770 train_time:75626ms step_avg:96.34ms
step:796/1770 train_time:75725ms step_avg:96.34ms
step:797/1770 train_time:75823ms step_avg:96.34ms
step:798/1770 train_time:75922ms step_avg:96.35ms
step:799/1770 train_time:76022ms step_avg:96.35ms
step:800/1770 train_time:76121ms step_avg:96.36ms
step:801/1770 train_time:76222ms step_avg:96.36ms
step:802/1770 train_time:76322ms step_avg:96.37ms
step:803/1770 train_time:76421ms step_avg:96.37ms
step:804/1770 train_time:76521ms step_avg:96.37ms
step:805/1770 train_time:76622ms step_avg:96.38ms
step:806/1770 train_time:76721ms step_avg:96.38ms
step:807/1770 train_time:76821ms step_avg:96.39ms
step:808/1770 train_time:76921ms step_avg:96.39ms
step:809/1770 train_time:77020ms step_avg:96.40ms
step:810/1770 train_time:77120ms step_avg:96.40ms
step:811/1770 train_time:77219ms step_avg:96.40ms
step:812/1770 train_time:77319ms step_avg:96.41ms
step:813/1770 train_time:77420ms step_avg:96.41ms
step:814/1770 train_time:77520ms step_avg:96.42ms
step:815/1770 train_time:77620ms step_avg:96.42ms
step:816/1770 train_time:77719ms step_avg:96.43ms
step:817/1770 train_time:77819ms step_avg:96.43ms
step:818/1770 train_time:77919ms step_avg:96.43ms
step:819/1770 train_time:78018ms step_avg:96.44ms
step:820/1770 train_time:78117ms step_avg:96.44ms
step:821/1770 train_time:78217ms step_avg:96.44ms
step:822/1770 train_time:78316ms step_avg:96.45ms
step:823/1770 train_time:78416ms step_avg:96.45ms
step:824/1770 train_time:78515ms step_avg:96.46ms
step:825/1770 train_time:78614ms step_avg:96.46ms
step:826/1770 train_time:78713ms step_avg:96.46ms
step:827/1770 train_time:78812ms step_avg:96.47ms
step:828/1770 train_time:78911ms step_avg:96.47ms
step:829/1770 train_time:79010ms step_avg:96.47ms
step:830/1770 train_time:79109ms step_avg:96.47ms
step:831/1770 train_time:79209ms step_avg:96.48ms
step:832/1770 train_time:79309ms step_avg:96.48ms
step:833/1770 train_time:79409ms step_avg:96.49ms
step:834/1770 train_time:79509ms step_avg:96.49ms
step:835/1770 train_time:79609ms step_avg:96.50ms
step:836/1770 train_time:79709ms step_avg:96.50ms
step:837/1770 train_time:79809ms step_avg:96.50ms
step:838/1770 train_time:79908ms step_avg:96.51ms
step:839/1770 train_time:80006ms step_avg:96.51ms
step:840/1770 train_time:80106ms step_avg:96.51ms
step:841/1770 train_time:80205ms step_avg:96.52ms
step:842/1770 train_time:80304ms step_avg:96.52ms
step:843/1770 train_time:80403ms step_avg:96.52ms
step:844/1770 train_time:80503ms step_avg:96.53ms
step:845/1770 train_time:80603ms step_avg:96.53ms
step:846/1770 train_time:80702ms step_avg:96.53ms
step:847/1770 train_time:80801ms step_avg:96.54ms
step:848/1770 train_time:80901ms step_avg:96.54ms
step:849/1770 train_time:81001ms step_avg:96.54ms
step:850/1770 train_time:81101ms step_avg:96.55ms
step:851/1770 train_time:81201ms step_avg:96.55ms
step:852/1770 train_time:81301ms step_avg:96.56ms
step:853/1770 train_time:81401ms step_avg:96.56ms
step:854/1770 train_time:81500ms step_avg:96.56ms
step:855/1770 train_time:81599ms step_avg:96.57ms
step:856/1770 train_time:81699ms step_avg:96.57ms
step:857/1770 train_time:81799ms step_avg:96.57ms
step:858/1770 train_time:81899ms step_avg:96.58ms
step:859/1770 train_time:82000ms step_avg:96.58ms
step:860/1770 train_time:82099ms step_avg:96.59ms
step:861/1770 train_time:82199ms step_avg:96.59ms
step:862/1770 train_time:82299ms step_avg:96.60ms
step:863/1770 train_time:82398ms step_avg:96.60ms
step:864/1770 train_time:82498ms step_avg:96.60ms
step:865/1770 train_time:82597ms step_avg:96.61ms
step:866/1770 train_time:82698ms step_avg:96.61ms
step:867/1770 train_time:82797ms step_avg:96.61ms
step:868/1770 train_time:82896ms step_avg:96.62ms
step:869/1770 train_time:82995ms step_avg:96.62ms
step:870/1770 train_time:83094ms step_avg:96.62ms
step:871/1770 train_time:83194ms step_avg:96.62ms
step:872/1770 train_time:83294ms step_avg:96.63ms
step:873/1770 train_time:83393ms step_avg:96.63ms
step:874/1770 train_time:83492ms step_avg:96.63ms
step:875/1770 train_time:83591ms step_avg:96.64ms
step:875/1770 val_loss:3.5518 train_time:83688ms step_avg:96.75ms
step:876/1770 train_time:83708ms step_avg:96.66ms
step:877/1770 train_time:83802ms step_avg:96.66ms
step:878/1770 train_time:83903ms step_avg:96.66ms
step:879/1770 train_time:84003ms step_avg:96.67ms
step:880/1770 train_time:84101ms step_avg:96.67ms
step:881/1770 train_time:84200ms step_avg:96.67ms
step:882/1770 train_time:84299ms step_avg:96.67ms
step:883/1770 train_time:84398ms step_avg:96.68ms
step:884/1770 train_time:84497ms step_avg:96.68ms
step:885/1770 train_time:84595ms step_avg:96.68ms
step:886/1770 train_time:84694ms step_avg:96.68ms
step:887/1770 train_time:84794ms step_avg:96.69ms
step:888/1770 train_time:84894ms step_avg:96.69ms
step:889/1770 train_time:84995ms step_avg:96.70ms
step:890/1770 train_time:85094ms step_avg:96.70ms
step:891/1770 train_time:85194ms step_avg:96.70ms
step:892/1770 train_time:85293ms step_avg:96.70ms
step:893/1770 train_time:85392ms step_avg:96.71ms
step:894/1770 train_time:85491ms step_avg:96.71ms
step:895/1770 train_time:85590ms step_avg:96.71ms
step:896/1770 train_time:85689ms step_avg:96.71ms
step:897/1770 train_time:85789ms step_avg:96.72ms
step:898/1770 train_time:85890ms step_avg:96.72ms
step:899/1770 train_time:85989ms step_avg:96.73ms
step:900/1770 train_time:86089ms step_avg:96.73ms
step:901/1770 train_time:86189ms step_avg:96.73ms
step:902/1770 train_time:86289ms step_avg:96.74ms
step:903/1770 train_time:86389ms step_avg:96.74ms
step:904/1770 train_time:86489ms step_avg:96.74ms
step:905/1770 train_time:86588ms step_avg:96.75ms
step:906/1770 train_time:86688ms step_avg:96.75ms
step:907/1770 train_time:86787ms step_avg:96.75ms
step:908/1770 train_time:86887ms step_avg:96.76ms
step:909/1770 train_time:86987ms step_avg:96.76ms
step:910/1770 train_time:87087ms step_avg:96.76ms
step:911/1770 train_time:87187ms step_avg:96.77ms
step:912/1770 train_time:87287ms step_avg:96.77ms
step:913/1770 train_time:87387ms step_avg:96.77ms
step:914/1770 train_time:87486ms step_avg:96.78ms
step:915/1770 train_time:87586ms step_avg:96.78ms
step:916/1770 train_time:87685ms step_avg:96.78ms
step:917/1770 train_time:87785ms step_avg:96.79ms
step:918/1770 train_time:87884ms step_avg:96.79ms
step:919/1770 train_time:87984ms step_avg:96.79ms
step:920/1770 train_time:88086ms step_avg:96.80ms
step:921/1770 train_time:88188ms step_avg:96.80ms
step:922/1770 train_time:88288ms step_avg:96.81ms
step:923/1770 train_time:88389ms step_avg:96.81ms
step:924/1770 train_time:88491ms step_avg:96.82ms
step:925/1770 train_time:88592ms step_avg:96.82ms
step:926/1770 train_time:88692ms step_avg:96.83ms
step:927/1770 train_time:88793ms step_avg:96.83ms
step:928/1770 train_time:88894ms step_avg:96.83ms
step:929/1770 train_time:88994ms step_avg:96.84ms
step:930/1770 train_time:89095ms step_avg:96.84ms
step:931/1770 train_time:89197ms step_avg:96.85ms
step:932/1770 train_time:89298ms step_avg:96.85ms
step:933/1770 train_time:89400ms step_avg:96.86ms
step:934/1770 train_time:89501ms step_avg:96.86ms
step:935/1770 train_time:89601ms step_avg:96.87ms
step:936/1770 train_time:89702ms step_avg:96.87ms
step:937/1770 train_time:89801ms step_avg:96.87ms
step:938/1770 train_time:89902ms step_avg:96.88ms
step:939/1770 train_time:90002ms step_avg:96.88ms
step:940/1770 train_time:90102ms step_avg:96.88ms
step:941/1770 train_time:90202ms step_avg:96.89ms
step:942/1770 train_time:90303ms step_avg:96.89ms
step:943/1770 train_time:90405ms step_avg:96.90ms
step:944/1770 train_time:90506ms step_avg:96.90ms
step:945/1770 train_time:90606ms step_avg:96.91ms
step:946/1770 train_time:90707ms step_avg:96.91ms
step:947/1770 train_time:90807ms step_avg:96.91ms
step:948/1770 train_time:90909ms step_avg:96.92ms
step:949/1770 train_time:91010ms step_avg:96.92ms
step:950/1770 train_time:91111ms step_avg:96.93ms
step:951/1770 train_time:91214ms step_avg:96.93ms
step:952/1770 train_time:91315ms step_avg:96.94ms
step:953/1770 train_time:91415ms step_avg:96.94ms
step:954/1770 train_time:91515ms step_avg:96.94ms
step:955/1770 train_time:91616ms step_avg:96.95ms
step:956/1770 train_time:91717ms step_avg:96.95ms
step:957/1770 train_time:91818ms step_avg:96.96ms
step:958/1770 train_time:91920ms step_avg:96.96ms
step:959/1770 train_time:92022ms step_avg:96.97ms
step:960/1770 train_time:92122ms step_avg:96.97ms
step:961/1770 train_time:92222ms step_avg:96.97ms
step:962/1770 train_time:92323ms step_avg:96.98ms
step:963/1770 train_time:92422ms step_avg:96.98ms
step:964/1770 train_time:92523ms step_avg:96.98ms
step:965/1770 train_time:92624ms step_avg:96.99ms
step:966/1770 train_time:92724ms step_avg:96.99ms
step:967/1770 train_time:92826ms step_avg:97.00ms
step:968/1770 train_time:92928ms step_avg:97.00ms
step:969/1770 train_time:93028ms step_avg:97.01ms
step:970/1770 train_time:93129ms step_avg:97.01ms
step:971/1770 train_time:93231ms step_avg:97.01ms
step:972/1770 train_time:93332ms step_avg:97.02ms
step:973/1770 train_time:93433ms step_avg:97.02ms
step:974/1770 train_time:93533ms step_avg:97.03ms
step:975/1770 train_time:93634ms step_avg:97.03ms
step:976/1770 train_time:93734ms step_avg:97.03ms
step:977/1770 train_time:93836ms step_avg:97.04ms
step:978/1770 train_time:93937ms step_avg:97.04ms
step:979/1770 train_time:94038ms step_avg:97.05ms
step:980/1770 train_time:94139ms step_avg:97.05ms
step:981/1770 train_time:94240ms step_avg:97.06ms
step:982/1770 train_time:94341ms step_avg:97.06ms
step:983/1770 train_time:94442ms step_avg:97.06ms
step:984/1770 train_time:94543ms step_avg:97.07ms
step:985/1770 train_time:94645ms step_avg:97.07ms
step:986/1770 train_time:94743ms step_avg:97.07ms
step:987/1770 train_time:94843ms step_avg:97.08ms
step:988/1770 train_time:94943ms step_avg:97.08ms
step:989/1770 train_time:95045ms step_avg:97.08ms
step:990/1770 train_time:95145ms step_avg:97.09ms
step:991/1770 train_time:95246ms step_avg:97.09ms
step:992/1770 train_time:95347ms step_avg:97.09ms
step:993/1770 train_time:95448ms step_avg:97.10ms
step:994/1770 train_time:95550ms step_avg:97.10ms
step:995/1770 train_time:95651ms step_avg:97.11ms
step:996/1770 train_time:95753ms step_avg:97.11ms
step:997/1770 train_time:95854ms step_avg:97.12ms
step:998/1770 train_time:95954ms step_avg:97.12ms
step:999/1770 train_time:96055ms step_avg:97.12ms
step:1000/1770 train_time:96156ms step_avg:97.13ms
step:1000/1770 val_loss:3.5136 train_time:96255ms step_avg:97.23ms
step:1001/1770 train_time:96276ms step_avg:97.15ms
step:1002/1770 train_time:96371ms step_avg:97.15ms
step:1003/1770 train_time:96475ms step_avg:97.16ms
step:1004/1770 train_time:96575ms step_avg:97.16ms
step:1005/1770 train_time:96675ms step_avg:97.16ms
step:1006/1770 train_time:96776ms step_avg:97.16ms
step:1007/1770 train_time:96875ms step_avg:97.17ms
step:1008/1770 train_time:96975ms step_avg:97.17ms
step:1009/1770 train_time:97074ms step_avg:97.17ms
step:1010/1770 train_time:97174ms step_avg:97.17ms
step:1011/1770 train_time:97276ms step_avg:97.18ms
step:1012/1770 train_time:97377ms step_avg:97.18ms
step:1013/1770 train_time:97478ms step_avg:97.19ms
step:1014/1770 train_time:97578ms step_avg:97.19ms
step:1015/1770 train_time:97680ms step_avg:97.19ms
step:1016/1770 train_time:97779ms step_avg:97.20ms
step:1017/1770 train_time:97880ms step_avg:97.20ms
step:1018/1770 train_time:97980ms step_avg:97.20ms
step:1019/1770 train_time:98081ms step_avg:97.21ms
step:1020/1770 train_time:98181ms step_avg:97.21ms
step:1021/1770 train_time:98283ms step_avg:97.21ms
step:1022/1770 train_time:98384ms step_avg:97.22ms
step:1023/1770 train_time:98485ms step_avg:97.22ms
step:1024/1770 train_time:98587ms step_avg:97.23ms
step:1025/1770 train_time:98688ms step_avg:97.23ms
step:1026/1770 train_time:98789ms step_avg:97.23ms
step:1027/1770 train_time:98889ms step_avg:97.24ms
step:1028/1770 train_time:98990ms step_avg:97.24ms
step:1029/1770 train_time:99090ms step_avg:97.24ms
step:1030/1770 train_time:99192ms step_avg:97.25ms
step:1031/1770 train_time:99293ms step_avg:97.25ms
step:1032/1770 train_time:99394ms step_avg:97.25ms
step:1033/1770 train_time:99496ms step_avg:97.26ms
step:1034/1770 train_time:99596ms step_avg:97.26ms
step:1035/1770 train_time:99697ms step_avg:97.27ms
step:1036/1770 train_time:99797ms step_avg:97.27ms
step:1037/1770 train_time:99897ms step_avg:97.27ms
step:1038/1770 train_time:99997ms step_avg:97.27ms
step:1039/1770 train_time:100097ms step_avg:97.28ms
step:1040/1770 train_time:100198ms step_avg:97.28ms
step:1041/1770 train_time:100298ms step_avg:97.28ms
step:1042/1770 train_time:100400ms step_avg:97.29ms
step:1043/1770 train_time:100501ms step_avg:97.29ms
step:1044/1770 train_time:100601ms step_avg:97.29ms
step:1045/1770 train_time:100702ms step_avg:97.30ms
step:1046/1770 train_time:100803ms step_avg:97.30ms
step:1047/1770 train_time:100904ms step_avg:97.30ms
step:1048/1770 train_time:101006ms step_avg:97.31ms
step:1049/1770 train_time:101107ms step_avg:97.31ms
step:1050/1770 train_time:101208ms step_avg:97.32ms
step:1051/1770 train_time:101309ms step_avg:97.32ms
step:1052/1770 train_time:101410ms step_avg:97.32ms
step:1053/1770 train_time:101511ms step_avg:97.33ms
step:1054/1770 train_time:101612ms step_avg:97.33ms
step:1055/1770 train_time:101713ms step_avg:97.33ms
step:1056/1770 train_time:101814ms step_avg:97.34ms
step:1057/1770 train_time:101915ms step_avg:97.34ms
step:1058/1770 train_time:102017ms step_avg:97.34ms
step:1059/1770 train_time:102117ms step_avg:97.35ms
step:1060/1770 train_time:102221ms step_avg:97.35ms
step:1061/1770 train_time:102318ms step_avg:97.35ms
step:1062/1770 train_time:102420ms step_avg:97.36ms
step:1063/1770 train_time:102522ms step_avg:97.36ms
step:1064/1770 train_time:102624ms step_avg:97.37ms
step:1065/1770 train_time:102726ms step_avg:97.37ms
step:1066/1770 train_time:102827ms step_avg:97.37ms
step:1067/1770 train_time:102928ms step_avg:97.38ms
step:1068/1770 train_time:103030ms step_avg:97.38ms
step:1069/1770 train_time:103131ms step_avg:97.39ms
step:1070/1770 train_time:103233ms step_avg:97.39ms
step:1071/1770 train_time:103334ms step_avg:97.39ms
step:1072/1770 train_time:103435ms step_avg:97.40ms
step:1073/1770 train_time:103535ms step_avg:97.40ms
step:1074/1770 train_time:103635ms step_avg:97.40ms
step:1075/1770 train_time:103736ms step_avg:97.40ms
step:1076/1770 train_time:103837ms step_avg:97.41ms
step:1077/1770 train_time:103938ms step_avg:97.41ms
step:1078/1770 train_time:104038ms step_avg:97.41ms
step:1079/1770 train_time:104138ms step_avg:97.42ms
step:1080/1770 train_time:104240ms step_avg:97.42ms
step:1081/1770 train_time:104340ms step_avg:97.42ms
step:1082/1770 train_time:104441ms step_avg:97.43ms
step:1083/1770 train_time:104543ms step_avg:97.43ms
step:1084/1770 train_time:104644ms step_avg:97.43ms
step:1085/1770 train_time:104746ms step_avg:97.44ms
step:1086/1770 train_time:104848ms step_avg:97.44ms
step:1087/1770 train_time:104949ms step_avg:97.45ms
step:1088/1770 train_time:105050ms step_avg:97.45ms
step:1089/1770 train_time:105150ms step_avg:97.45ms
step:1090/1770 train_time:105251ms step_avg:97.45ms
step:1091/1770 train_time:105352ms step_avg:97.46ms
step:1092/1770 train_time:105454ms step_avg:97.46ms
step:1093/1770 train_time:105557ms step_avg:97.47ms
step:1094/1770 train_time:105658ms step_avg:97.47ms
step:1095/1770 train_time:105759ms step_avg:97.47ms
step:1096/1770 train_time:105859ms step_avg:97.48ms
step:1097/1770 train_time:105960ms step_avg:97.48ms
step:1098/1770 train_time:106060ms step_avg:97.48ms
step:1099/1770 train_time:106162ms step_avg:97.49ms
step:1100/1770 train_time:106262ms step_avg:97.49ms
step:1101/1770 train_time:106364ms step_avg:97.49ms
step:1102/1770 train_time:106467ms step_avg:97.50ms
step:1103/1770 train_time:106567ms step_avg:97.50ms
step:1104/1770 train_time:106668ms step_avg:97.50ms
step:1105/1770 train_time:106770ms step_avg:97.51ms
step:1106/1770 train_time:106871ms step_avg:97.51ms
step:1107/1770 train_time:106972ms step_avg:97.51ms
step:1108/1770 train_time:107073ms step_avg:97.52ms
step:1109/1770 train_time:107173ms step_avg:97.52ms
step:1110/1770 train_time:107274ms step_avg:97.52ms
step:1111/1770 train_time:107376ms step_avg:97.53ms
step:1112/1770 train_time:107477ms step_avg:97.53ms
step:1113/1770 train_time:107577ms step_avg:97.53ms
step:1114/1770 train_time:107678ms step_avg:97.53ms
step:1115/1770 train_time:107779ms step_avg:97.54ms
step:1116/1770 train_time:107880ms step_avg:97.54ms
step:1117/1770 train_time:107981ms step_avg:97.54ms
step:1118/1770 train_time:108083ms step_avg:97.55ms
step:1119/1770 train_time:108185ms step_avg:97.55ms
step:1120/1770 train_time:108286ms step_avg:97.56ms
step:1121/1770 train_time:108388ms step_avg:97.56ms
step:1122/1770 train_time:108490ms step_avg:97.56ms
step:1123/1770 train_time:108590ms step_avg:97.56ms
step:1124/1770 train_time:108690ms step_avg:97.57ms
step:1125/1770 train_time:108791ms step_avg:97.57ms
step:1125/1770 val_loss:3.4734 train_time:108891ms step_avg:97.66ms
step:1126/1770 train_time:108912ms step_avg:97.59ms
step:1127/1770 train_time:109005ms step_avg:97.59ms
step:1128/1770 train_time:109108ms step_avg:97.59ms
step:1129/1770 train_time:109209ms step_avg:97.60ms
step:1130/1770 train_time:109310ms step_avg:97.60ms
step:1131/1770 train_time:109411ms step_avg:97.60ms
step:1132/1770 train_time:109512ms step_avg:97.60ms
step:1133/1770 train_time:109612ms step_avg:97.61ms
step:1134/1770 train_time:109713ms step_avg:97.61ms
step:1135/1770 train_time:109814ms step_avg:97.61ms
step:1136/1770 train_time:109915ms step_avg:97.62ms
step:1137/1770 train_time:110016ms step_avg:97.62ms
step:1138/1770 train_time:110117ms step_avg:97.62ms
step:1139/1770 train_time:110218ms step_avg:97.62ms
step:1140/1770 train_time:110319ms step_avg:97.63ms
step:1141/1770 train_time:110420ms step_avg:97.63ms
step:1142/1770 train_time:110522ms step_avg:97.63ms
step:1143/1770 train_time:110623ms step_avg:97.64ms
step:1144/1770 train_time:110723ms step_avg:97.64ms
step:1145/1770 train_time:110824ms step_avg:97.64ms
step:1146/1770 train_time:110925ms step_avg:97.64ms
step:1147/1770 train_time:111026ms step_avg:97.65ms
step:1148/1770 train_time:111126ms step_avg:97.65ms
step:1149/1770 train_time:111228ms step_avg:97.65ms
step:1150/1770 train_time:111329ms step_avg:97.66ms
step:1151/1770 train_time:111431ms step_avg:97.66ms
step:1152/1770 train_time:111534ms step_avg:97.67ms
step:1153/1770 train_time:111635ms step_avg:97.67ms
step:1154/1770 train_time:111736ms step_avg:97.67ms
step:1155/1770 train_time:111836ms step_avg:97.67ms
step:1156/1770 train_time:111937ms step_avg:97.68ms
step:1157/1770 train_time:112039ms step_avg:97.68ms
step:1158/1770 train_time:112141ms step_avg:97.68ms
step:1159/1770 train_time:112242ms step_avg:97.69ms
step:1160/1770 train_time:112344ms step_avg:97.69ms
step:1161/1770 train_time:112446ms step_avg:97.69ms
step:1162/1770 train_time:112548ms step_avg:97.70ms
step:1163/1770 train_time:112649ms step_avg:97.70ms
step:1164/1770 train_time:112750ms step_avg:97.70ms
step:1165/1770 train_time:112851ms step_avg:97.71ms
step:1166/1770 train_time:112953ms step_avg:97.71ms
step:1167/1770 train_time:113053ms step_avg:97.71ms
step:1168/1770 train_time:113154ms step_avg:97.72ms
step:1169/1770 train_time:113254ms step_avg:97.72ms
step:1170/1770 train_time:113355ms step_avg:97.72ms
step:1171/1770 train_time:113456ms step_avg:97.72ms
step:1172/1770 train_time:113557ms step_avg:97.73ms
step:1173/1770 train_time:113658ms step_avg:97.73ms
step:1174/1770 train_time:113760ms step_avg:97.73ms
step:1175/1770 train_time:113861ms step_avg:97.73ms
step:1176/1770 train_time:113963ms step_avg:97.74ms
step:1177/1770 train_time:114064ms step_avg:97.74ms
step:1178/1770 train_time:114165ms step_avg:97.74ms
step:1179/1770 train_time:114266ms step_avg:97.75ms
step:1180/1770 train_time:114367ms step_avg:97.75ms
step:1181/1770 train_time:114468ms step_avg:97.75ms
step:1182/1770 train_time:114570ms step_avg:97.76ms
step:1183/1770 train_time:114672ms step_avg:97.76ms
step:1184/1770 train_time:114775ms step_avg:97.76ms
step:1185/1770 train_time:114876ms step_avg:97.77ms
step:1186/1770 train_time:114979ms step_avg:97.77ms
step:1187/1770 train_time:115082ms step_avg:97.78ms
step:1188/1770 train_time:115184ms step_avg:97.78ms
step:1189/1770 train_time:115286ms step_avg:97.78ms
step:1190/1770 train_time:115388ms step_avg:97.79ms
step:1191/1770 train_time:115491ms step_avg:97.79ms
step:1192/1770 train_time:115593ms step_avg:97.79ms
step:1193/1770 train_time:115696ms step_avg:97.80ms
step:1194/1770 train_time:115797ms step_avg:97.80ms
step:1195/1770 train_time:115901ms step_avg:97.81ms
step:1196/1770 train_time:116004ms step_avg:97.81ms
step:1197/1770 train_time:116106ms step_avg:97.81ms
step:1198/1770 train_time:116208ms step_avg:97.82ms
step:1199/1770 train_time:116311ms step_avg:97.82ms
step:1200/1770 train_time:116413ms step_avg:97.83ms
step:1201/1770 train_time:116515ms step_avg:97.83ms
step:1202/1770 train_time:116616ms step_avg:97.83ms
step:1203/1770 train_time:116718ms step_avg:97.84ms
step:1204/1770 train_time:116820ms step_avg:97.84ms
step:1205/1770 train_time:116922ms step_avg:97.84ms
step:1206/1770 train_time:117024ms step_avg:97.85ms
step:1207/1770 train_time:117126ms step_avg:97.85ms
step:1208/1770 train_time:117229ms step_avg:97.85ms
step:1209/1770 train_time:117332ms step_avg:97.86ms
step:1210/1770 train_time:117433ms step_avg:97.86ms
step:1211/1770 train_time:117535ms step_avg:97.86ms
step:1212/1770 train_time:117639ms step_avg:97.87ms
step:1213/1770 train_time:117741ms step_avg:97.87ms
step:1214/1770 train_time:117842ms step_avg:97.88ms
step:1215/1770 train_time:117944ms step_avg:97.88ms
step:1216/1770 train_time:118050ms step_avg:97.89ms
step:1217/1770 train_time:118152ms step_avg:97.89ms
step:1218/1770 train_time:118254ms step_avg:97.89ms
step:1219/1770 train_time:118356ms step_avg:97.90ms
step:1220/1770 train_time:118458ms step_avg:97.90ms
step:1221/1770 train_time:118561ms step_avg:97.90ms
step:1222/1770 train_time:118665ms step_avg:97.91ms
step:1223/1770 train_time:118766ms step_avg:97.91ms
step:1224/1770 train_time:118869ms step_avg:97.92ms
step:1225/1770 train_time:118971ms step_avg:97.92ms
step:1226/1770 train_time:119073ms step_avg:97.92ms
step:1227/1770 train_time:119177ms step_avg:97.93ms
step:1228/1770 train_time:119280ms step_avg:97.93ms
step:1229/1770 train_time:119382ms step_avg:97.93ms
step:1230/1770 train_time:119485ms step_avg:97.94ms
step:1231/1770 train_time:119587ms step_avg:97.94ms
step:1232/1770 train_time:119694ms step_avg:97.95ms
step:1233/1770 train_time:119791ms step_avg:97.95ms
step:1234/1770 train_time:119894ms step_avg:97.95ms
step:1235/1770 train_time:119996ms step_avg:97.96ms
step:1236/1770 train_time:120098ms step_avg:97.96ms
step:1237/1770 train_time:120200ms step_avg:97.96ms
step:1238/1770 train_time:120303ms step_avg:97.97ms
step:1239/1770 train_time:120405ms step_avg:97.97ms
step:1240/1770 train_time:120507ms step_avg:97.97ms
step:1241/1770 train_time:120610ms step_avg:97.98ms
step:1242/1770 train_time:120712ms step_avg:97.98ms
step:1243/1770 train_time:120815ms step_avg:97.98ms
step:1244/1770 train_time:120916ms step_avg:97.99ms
step:1245/1770 train_time:121018ms step_avg:97.99ms
step:1246/1770 train_time:121120ms step_avg:97.99ms
step:1247/1770 train_time:121222ms step_avg:98.00ms
step:1248/1770 train_time:121325ms step_avg:98.00ms
step:1249/1770 train_time:121427ms step_avg:98.00ms
step:1250/1770 train_time:121528ms step_avg:98.01ms
step:1250/1770 val_loss:3.4251 train_time:121631ms step_avg:98.09ms
step:1251/1770 train_time:121651ms step_avg:98.03ms
step:1252/1770 train_time:121745ms step_avg:98.02ms
step:1253/1770 train_time:121848ms step_avg:98.03ms
step:1254/1770 train_time:121952ms step_avg:98.03ms
step:1255/1770 train_time:122055ms step_avg:98.04ms
step:1256/1770 train_time:122156ms step_avg:98.04ms
step:1257/1770 train_time:122258ms step_avg:98.04ms
step:1258/1770 train_time:122360ms step_avg:98.05ms
step:1259/1770 train_time:122462ms step_avg:98.05ms
step:1260/1770 train_time:122563ms step_avg:98.05ms
step:1261/1770 train_time:122666ms step_avg:98.05ms
step:1262/1770 train_time:122768ms step_avg:98.06ms
step:1263/1770 train_time:122870ms step_avg:98.06ms
step:1264/1770 train_time:122974ms step_avg:98.07ms
step:1265/1770 train_time:123076ms step_avg:98.07ms
step:1266/1770 train_time:123178ms step_avg:98.07ms
step:1267/1770 train_time:123281ms step_avg:98.08ms
step:1268/1770 train_time:123383ms step_avg:98.08ms
step:1269/1770 train_time:123485ms step_avg:98.08ms
step:1270/1770 train_time:123588ms step_avg:98.09ms
step:1271/1770 train_time:123690ms step_avg:98.09ms
step:1272/1770 train_time:123791ms step_avg:98.09ms
step:1273/1770 train_time:123894ms step_avg:98.10ms
step:1274/1770 train_time:123996ms step_avg:98.10ms
step:1275/1770 train_time:124098ms step_avg:98.10ms
step:1276/1770 train_time:124200ms step_avg:98.10ms
step:1277/1770 train_time:124301ms step_avg:98.11ms
step:1278/1770 train_time:124405ms step_avg:98.11ms
step:1279/1770 train_time:124507ms step_avg:98.11ms
step:1280/1770 train_time:124610ms step_avg:98.12ms
step:1281/1770 train_time:124712ms step_avg:98.12ms
step:1282/1770 train_time:124815ms step_avg:98.13ms
step:1283/1770 train_time:124917ms step_avg:98.13ms
step:1284/1770 train_time:125019ms step_avg:98.13ms
step:1285/1770 train_time:125122ms step_avg:98.13ms
step:1286/1770 train_time:125225ms step_avg:98.14ms
step:1287/1770 train_time:125329ms step_avg:98.14ms
step:1288/1770 train_time:125432ms step_avg:98.15ms
step:1289/1770 train_time:125534ms step_avg:98.15ms
step:1290/1770 train_time:125636ms step_avg:98.15ms
step:1291/1770 train_time:125738ms step_avg:98.16ms
step:1292/1770 train_time:125839ms step_avg:98.16ms
step:1293/1770 train_time:125942ms step_avg:98.16ms
step:1294/1770 train_time:126043ms step_avg:98.16ms
step:1295/1770 train_time:126145ms step_avg:98.17ms
step:1296/1770 train_time:126246ms step_avg:98.17ms
step:1297/1770 train_time:126349ms step_avg:98.17ms
step:1298/1770 train_time:126451ms step_avg:98.18ms
step:1299/1770 train_time:126553ms step_avg:98.18ms
step:1300/1770 train_time:126656ms step_avg:98.18ms
step:1301/1770 train_time:126758ms step_avg:98.19ms
step:1302/1770 train_time:126861ms step_avg:98.19ms
step:1303/1770 train_time:126963ms step_avg:98.19ms
step:1304/1770 train_time:127064ms step_avg:98.20ms
step:1305/1770 train_time:127166ms step_avg:98.20ms
step:1306/1770 train_time:127267ms step_avg:98.20ms
step:1307/1770 train_time:127369ms step_avg:98.20ms
step:1308/1770 train_time:127472ms step_avg:98.21ms
step:1309/1770 train_time:127575ms step_avg:98.21ms
step:1310/1770 train_time:127676ms step_avg:98.21ms
step:1311/1770 train_time:127778ms step_avg:98.22ms
step:1312/1770 train_time:127880ms step_avg:98.22ms
step:1313/1770 train_time:127982ms step_avg:98.22ms
step:1314/1770 train_time:128084ms step_avg:98.22ms
step:1315/1770 train_time:128185ms step_avg:98.23ms
step:1316/1770 train_time:128287ms step_avg:98.23ms
step:1317/1770 train_time:128390ms step_avg:98.23ms
step:1318/1770 train_time:128495ms step_avg:98.24ms
step:1319/1770 train_time:128598ms step_avg:98.24ms
step:1320/1770 train_time:128700ms step_avg:98.24ms
step:1321/1770 train_time:128802ms step_avg:98.25ms
step:1322/1770 train_time:128904ms step_avg:98.25ms
step:1323/1770 train_time:129007ms step_avg:98.25ms
step:1324/1770 train_time:129111ms step_avg:98.26ms
step:1325/1770 train_time:129215ms step_avg:98.26ms
step:1326/1770 train_time:129317ms step_avg:98.26ms
step:1327/1770 train_time:129422ms step_avg:98.27ms
step:1328/1770 train_time:129523ms step_avg:98.27ms
step:1329/1770 train_time:129625ms step_avg:98.28ms
step:1330/1770 train_time:129727ms step_avg:98.28ms
step:1331/1770 train_time:129829ms step_avg:98.28ms
step:1332/1770 train_time:129931ms step_avg:98.28ms
step:1333/1770 train_time:130032ms step_avg:98.29ms
step:1334/1770 train_time:130134ms step_avg:98.29ms
step:1335/1770 train_time:130236ms step_avg:98.29ms
step:1336/1770 train_time:130338ms step_avg:98.29ms
step:1337/1770 train_time:130441ms step_avg:98.30ms
step:1338/1770 train_time:130543ms step_avg:98.30ms
step:1339/1770 train_time:130645ms step_avg:98.30ms
step:1340/1770 train_time:130749ms step_avg:98.31ms
step:1341/1770 train_time:130850ms step_avg:98.31ms
step:1342/1770 train_time:130953ms step_avg:98.31ms
step:1343/1770 train_time:131055ms step_avg:98.32ms
step:1344/1770 train_time:131158ms step_avg:98.32ms
step:1345/1770 train_time:131260ms step_avg:98.32ms
step:1346/1770 train_time:131362ms step_avg:98.32ms
step:1347/1770 train_time:131464ms step_avg:98.33ms
step:1348/1770 train_time:131569ms step_avg:98.33ms
step:1349/1770 train_time:131671ms step_avg:98.34ms
step:1350/1770 train_time:131773ms step_avg:98.34ms
step:1351/1770 train_time:131875ms step_avg:98.34ms
step:1352/1770 train_time:131977ms step_avg:98.34ms
step:1353/1770 train_time:132080ms step_avg:98.35ms
step:1354/1770 train_time:132182ms step_avg:98.35ms
step:1355/1770 train_time:132284ms step_avg:98.35ms
step:1356/1770 train_time:132386ms step_avg:98.35ms
step:1357/1770 train_time:132488ms step_avg:98.36ms
step:1358/1770 train_time:132590ms step_avg:98.36ms
step:1359/1770 train_time:132693ms step_avg:98.36ms
step:1360/1770 train_time:132795ms step_avg:98.37ms
step:1361/1770 train_time:132898ms step_avg:98.37ms
step:1362/1770 train_time:133000ms step_avg:98.37ms
step:1363/1770 train_time:133103ms step_avg:98.38ms
step:1364/1770 train_time:133206ms step_avg:98.38ms
step:1365/1770 train_time:133308ms step_avg:98.38ms
step:1366/1770 train_time:133411ms step_avg:98.39ms
step:1367/1770 train_time:133513ms step_avg:98.39ms
step:1368/1770 train_time:133614ms step_avg:98.39ms
step:1369/1770 train_time:133717ms step_avg:98.39ms
step:1370/1770 train_time:133820ms step_avg:98.40ms
step:1371/1770 train_time:133922ms step_avg:98.40ms
step:1372/1770 train_time:134024ms step_avg:98.40ms
step:1373/1770 train_time:134127ms step_avg:98.41ms
step:1374/1770 train_time:134230ms step_avg:98.41ms
step:1375/1770 train_time:134332ms step_avg:98.41ms
step:1375/1770 val_loss:3.3815 train_time:134434ms step_avg:98.49ms
step:1376/1770 train_time:134455ms step_avg:98.43ms
step:1377/1770 train_time:134551ms step_avg:98.43ms
step:1378/1770 train_time:134653ms step_avg:98.43ms
step:1379/1770 train_time:134755ms step_avg:98.43ms
step:1380/1770 train_time:134857ms step_avg:98.44ms
step:1381/1770 train_time:134959ms step_avg:98.44ms
step:1382/1770 train_time:135061ms step_avg:98.44ms
step:1383/1770 train_time:135164ms step_avg:98.44ms
step:1384/1770 train_time:135266ms step_avg:98.45ms
step:1385/1770 train_time:135368ms step_avg:98.45ms
step:1386/1770 train_time:135471ms step_avg:98.45ms
step:1387/1770 train_time:135574ms step_avg:98.46ms
step:1388/1770 train_time:135676ms step_avg:98.46ms
step:1389/1770 train_time:135779ms step_avg:98.46ms
step:1390/1770 train_time:135881ms step_avg:98.46ms
step:1391/1770 train_time:135982ms step_avg:98.47ms
step:1392/1770 train_time:136084ms step_avg:98.47ms
step:1393/1770 train_time:136186ms step_avg:98.47ms
step:1394/1770 train_time:136287ms step_avg:98.47ms
step:1395/1770 train_time:136390ms step_avg:98.48ms
step:1396/1770 train_time:136493ms step_avg:98.48ms
step:1397/1770 train_time:136596ms step_avg:98.48ms
step:1398/1770 train_time:136699ms step_avg:98.49ms
step:1399/1770 train_time:136801ms step_avg:98.49ms
step:1400/1770 train_time:136905ms step_avg:98.49ms
step:1401/1770 train_time:137006ms step_avg:98.49ms
step:1402/1770 train_time:137109ms step_avg:98.50ms
step:1403/1770 train_time:137211ms step_avg:98.50ms
step:1404/1770 train_time:137313ms step_avg:98.50ms
step:1405/1770 train_time:137415ms step_avg:98.51ms
step:1406/1770 train_time:137517ms step_avg:98.51ms
step:1407/1770 train_time:137622ms step_avg:98.51ms
step:1408/1770 train_time:137722ms step_avg:98.51ms
step:1409/1770 train_time:137825ms step_avg:98.52ms
step:1410/1770 train_time:137927ms step_avg:98.52ms
step:1411/1770 train_time:138029ms step_avg:98.52ms
step:1412/1770 train_time:138131ms step_avg:98.52ms
step:1413/1770 train_time:138232ms step_avg:98.53ms
step:1414/1770 train_time:138335ms step_avg:98.53ms
step:1415/1770 train_time:138438ms step_avg:98.53ms
step:1416/1770 train_time:138541ms step_avg:98.54ms
step:1417/1770 train_time:138643ms step_avg:98.54ms
step:1418/1770 train_time:138745ms step_avg:98.54ms
step:1419/1770 train_time:138848ms step_avg:98.54ms
step:1420/1770 train_time:138949ms step_avg:98.55ms
step:1421/1770 train_time:139051ms step_avg:98.55ms
step:1422/1770 train_time:139153ms step_avg:98.55ms
step:1423/1770 train_time:139256ms step_avg:98.55ms
step:1424/1770 train_time:139359ms step_avg:98.56ms
step:1425/1770 train_time:139461ms step_avg:98.56ms
step:1426/1770 train_time:139564ms step_avg:98.56ms
step:1427/1770 train_time:139665ms step_avg:98.56ms
step:1428/1770 train_time:139770ms step_avg:98.57ms
step:1429/1770 train_time:139872ms step_avg:98.57ms
step:1430/1770 train_time:139973ms step_avg:98.57ms
step:1431/1770 train_time:140076ms step_avg:98.58ms
step:1432/1770 train_time:140178ms step_avg:98.58ms
step:1433/1770 train_time:140280ms step_avg:98.58ms
step:1434/1770 train_time:140381ms step_avg:98.58ms
step:1435/1770 train_time:140483ms step_avg:98.58ms
step:1436/1770 train_time:140587ms step_avg:98.59ms
step:1437/1770 train_time:140690ms step_avg:98.59ms
step:1438/1770 train_time:140791ms step_avg:98.59ms
step:1439/1770 train_time:140893ms step_avg:98.60ms
step:1440/1770 train_time:140995ms step_avg:98.60ms
step:1441/1770 train_time:141101ms step_avg:98.60ms
step:1442/1770 train_time:141202ms step_avg:98.60ms
step:1443/1770 train_time:141304ms step_avg:98.61ms
step:1444/1770 train_time:141407ms step_avg:98.61ms
step:1445/1770 train_time:141510ms step_avg:98.61ms
step:1446/1770 train_time:141613ms step_avg:98.62ms
step:1447/1770 train_time:141717ms step_avg:98.62ms
step:1448/1770 train_time:141820ms step_avg:98.62ms
step:1449/1770 train_time:141924ms step_avg:98.63ms
step:1450/1770 train_time:142028ms step_avg:98.63ms
step:1451/1770 train_time:142130ms step_avg:98.63ms
step:1452/1770 train_time:142235ms step_avg:98.64ms
step:1453/1770 train_time:142339ms step_avg:98.64ms
step:1454/1770 train_time:142442ms step_avg:98.64ms
step:1455/1770 train_time:142546ms step_avg:98.65ms
step:1456/1770 train_time:142649ms step_avg:98.65ms
step:1457/1770 train_time:142753ms step_avg:98.65ms
step:1458/1770 train_time:142858ms step_avg:98.66ms
step:1459/1770 train_time:142963ms step_avg:98.66ms
step:1460/1770 train_time:143066ms step_avg:98.67ms
step:1461/1770 train_time:143169ms step_avg:98.67ms
step:1462/1770 train_time:143272ms step_avg:98.67ms
step:1463/1770 train_time:143376ms step_avg:98.68ms
step:1464/1770 train_time:143481ms step_avg:98.68ms
step:1465/1770 train_time:143584ms step_avg:98.68ms
step:1466/1770 train_time:143688ms step_avg:98.69ms
step:1467/1770 train_time:143793ms step_avg:98.69ms
step:1468/1770 train_time:143896ms step_avg:98.69ms
step:1469/1770 train_time:143998ms step_avg:98.70ms
step:1470/1770 train_time:144101ms step_avg:98.70ms
step:1471/1770 train_time:144205ms step_avg:98.70ms
step:1472/1770 train_time:144307ms step_avg:98.71ms
step:1473/1770 train_time:144411ms step_avg:98.71ms
step:1474/1770 train_time:144515ms step_avg:98.71ms
step:1475/1770 train_time:144617ms step_avg:98.71ms
step:1476/1770 train_time:144723ms step_avg:98.72ms
step:1477/1770 train_time:144825ms step_avg:98.72ms
step:1478/1770 train_time:144929ms step_avg:98.73ms
step:1479/1770 train_time:145032ms step_avg:98.73ms
step:1480/1770 train_time:145136ms step_avg:98.73ms
step:1481/1770 train_time:145243ms step_avg:98.74ms
step:1482/1770 train_time:145345ms step_avg:98.74ms
step:1483/1770 train_time:145449ms step_avg:98.74ms
step:1484/1770 train_time:145552ms step_avg:98.75ms
step:1485/1770 train_time:145656ms step_avg:98.75ms
step:1486/1770 train_time:145760ms step_avg:98.75ms
step:1487/1770 train_time:145863ms step_avg:98.76ms
step:1488/1770 train_time:145966ms step_avg:98.76ms
step:1489/1770 train_time:146070ms step_avg:98.76ms
step:1490/1770 train_time:146173ms step_avg:98.77ms
step:1491/1770 train_time:146276ms step_avg:98.77ms
step:1492/1770 train_time:146380ms step_avg:98.77ms
step:1493/1770 train_time:146485ms step_avg:98.78ms
step:1494/1770 train_time:146592ms step_avg:98.78ms
step:1495/1770 train_time:146694ms step_avg:98.78ms
step:1496/1770 train_time:146797ms step_avg:98.79ms
step:1497/1770 train_time:146901ms step_avg:98.79ms
step:1498/1770 train_time:147004ms step_avg:98.79ms
step:1499/1770 train_time:147107ms step_avg:98.80ms
step:1500/1770 train_time:147209ms step_avg:98.80ms
step:1500/1770 val_loss:3.3429 train_time:147310ms step_avg:98.87ms
step:1501/1770 train_time:147331ms step_avg:98.81ms
step:1502/1770 train_time:147422ms step_avg:98.81ms
step:1503/1770 train_time:147525ms step_avg:98.81ms
step:1504/1770 train_time:147629ms step_avg:98.81ms
step:1505/1770 train_time:147735ms step_avg:98.82ms
step:1506/1770 train_time:147838ms step_avg:98.82ms
step:1507/1770 train_time:147941ms step_avg:98.82ms
step:1508/1770 train_time:148045ms step_avg:98.83ms
step:1509/1770 train_time:148147ms step_avg:98.83ms
step:1510/1770 train_time:148250ms step_avg:98.83ms
step:1511/1770 train_time:148355ms step_avg:98.84ms
step:1512/1770 train_time:148459ms step_avg:98.84ms
step:1513/1770 train_time:148564ms step_avg:98.84ms
step:1514/1770 train_time:148667ms step_avg:98.85ms
step:1515/1770 train_time:148772ms step_avg:98.85ms
step:1516/1770 train_time:148875ms step_avg:98.85ms
step:1517/1770 train_time:148978ms step_avg:98.86ms
step:1518/1770 train_time:149083ms step_avg:98.86ms
step:1519/1770 train_time:149184ms step_avg:98.86ms
step:1520/1770 train_time:149289ms step_avg:98.87ms
step:1521/1770 train_time:149393ms step_avg:98.87ms
step:1522/1770 train_time:149496ms step_avg:98.87ms
step:1523/1770 train_time:149601ms step_avg:98.88ms
step:1524/1770 train_time:149703ms step_avg:98.88ms
step:1525/1770 train_time:149807ms step_avg:98.88ms
step:1526/1770 train_time:149910ms step_avg:98.89ms
step:1527/1770 train_time:150013ms step_avg:98.89ms
step:1528/1770 train_time:150118ms step_avg:98.89ms
step:1529/1770 train_time:150221ms step_avg:98.89ms
step:1530/1770 train_time:150325ms step_avg:98.90ms
step:1531/1770 train_time:150428ms step_avg:98.90ms
step:1532/1770 train_time:150533ms step_avg:98.90ms
step:1533/1770 train_time:150637ms step_avg:98.91ms
step:1534/1770 train_time:150741ms step_avg:98.91ms
step:1535/1770 train_time:150844ms step_avg:98.91ms
step:1536/1770 train_time:150947ms step_avg:98.92ms
step:1537/1770 train_time:151050ms step_avg:98.92ms
step:1538/1770 train_time:151155ms step_avg:98.92ms
step:1539/1770 train_time:151258ms step_avg:98.93ms
step:1540/1770 train_time:151364ms step_avg:98.93ms
step:1541/1770 train_time:151468ms step_avg:98.93ms
step:1542/1770 train_time:151572ms step_avg:98.94ms
step:1543/1770 train_time:151675ms step_avg:98.94ms
step:1544/1770 train_time:151781ms step_avg:98.94ms
step:1545/1770 train_time:151884ms step_avg:98.95ms
step:1546/1770 train_time:151987ms step_avg:98.95ms
step:1547/1770 train_time:152090ms step_avg:98.95ms
step:1548/1770 train_time:152193ms step_avg:98.96ms
step:1549/1770 train_time:152297ms step_avg:98.96ms
step:1550/1770 train_time:152400ms step_avg:98.96ms
step:1551/1770 train_time:152503ms step_avg:98.96ms
step:1552/1770 train_time:152608ms step_avg:98.97ms
step:1553/1770 train_time:152711ms step_avg:98.97ms
step:1554/1770 train_time:152815ms step_avg:98.97ms
step:1555/1770 train_time:152920ms step_avg:98.98ms
step:1556/1770 train_time:153022ms step_avg:98.98ms
step:1557/1770 train_time:153125ms step_avg:98.98ms
step:1558/1770 train_time:153229ms step_avg:98.99ms
step:1559/1770 train_time:153333ms step_avg:98.99ms
step:1560/1770 train_time:153435ms step_avg:98.99ms
step:1561/1770 train_time:153541ms step_avg:98.99ms
step:1562/1770 train_time:153644ms step_avg:99.00ms
step:1563/1770 train_time:153748ms step_avg:99.00ms
step:1564/1770 train_time:153851ms step_avg:99.00ms
step:1565/1770 train_time:153955ms step_avg:99.01ms
step:1566/1770 train_time:154058ms step_avg:99.01ms
step:1567/1770 train_time:154161ms step_avg:99.01ms
step:1568/1770 train_time:154264ms step_avg:99.01ms
step:1569/1770 train_time:154372ms step_avg:99.02ms
step:1570/1770 train_time:154474ms step_avg:99.02ms
step:1571/1770 train_time:154577ms step_avg:99.02ms
step:1572/1770 train_time:154681ms step_avg:99.03ms
step:1573/1770 train_time:154786ms step_avg:99.03ms
step:1574/1770 train_time:154890ms step_avg:99.03ms
step:1575/1770 train_time:154992ms step_avg:99.04ms
step:1576/1770 train_time:155095ms step_avg:99.04ms
step:1577/1770 train_time:155199ms step_avg:99.04ms
step:1578/1770 train_time:155304ms step_avg:99.05ms
step:1579/1770 train_time:155409ms step_avg:99.05ms
step:1580/1770 train_time:155512ms step_avg:99.05ms
step:1581/1770 train_time:155619ms step_avg:99.06ms
step:1582/1770 train_time:155724ms step_avg:99.06ms
step:1583/1770 train_time:155827ms step_avg:99.06ms
step:1584/1770 train_time:155931ms step_avg:99.07ms
step:1585/1770 train_time:156034ms step_avg:99.07ms
step:1586/1770 train_time:156141ms step_avg:99.07ms
step:1587/1770 train_time:156244ms step_avg:99.08ms
step:1588/1770 train_time:156348ms step_avg:99.08ms
step:1589/1770 train_time:156453ms step_avg:99.08ms
step:1590/1770 train_time:156556ms step_avg:99.09ms
step:1591/1770 train_time:156659ms step_avg:99.09ms
step:1592/1770 train_time:156764ms step_avg:99.09ms
step:1593/1770 train_time:156866ms step_avg:99.09ms
step:1594/1770 train_time:156971ms step_avg:99.10ms
step:1595/1770 train_time:157074ms step_avg:99.10ms
step:1596/1770 train_time:157178ms step_avg:99.10ms
step:1597/1770 train_time:157280ms step_avg:99.11ms
step:1598/1770 train_time:157383ms step_avg:99.11ms
step:1599/1770 train_time:157488ms step_avg:99.11ms
step:1600/1770 train_time:157594ms step_avg:99.12ms
step:1601/1770 train_time:157698ms step_avg:99.12ms
step:1602/1770 train_time:157802ms step_avg:99.12ms
step:1603/1770 train_time:157907ms step_avg:99.13ms
step:1604/1770 train_time:158010ms step_avg:99.13ms
step:1605/1770 train_time:158113ms step_avg:99.13ms
step:1606/1770 train_time:158217ms step_avg:99.13ms
step:1607/1770 train_time:158324ms step_avg:99.14ms
step:1608/1770 train_time:158427ms step_avg:99.14ms
step:1609/1770 train_time:158531ms step_avg:99.14ms
step:1610/1770 train_time:158635ms step_avg:99.15ms
step:1611/1770 train_time:158741ms step_avg:99.15ms
step:1612/1770 train_time:158845ms step_avg:99.15ms
step:1613/1770 train_time:158948ms step_avg:99.16ms
step:1614/1770 train_time:159052ms step_avg:99.16ms
step:1615/1770 train_time:159155ms step_avg:99.16ms
step:1616/1770 train_time:159259ms step_avg:99.17ms
step:1617/1770 train_time:159365ms step_avg:99.17ms
step:1618/1770 train_time:159469ms step_avg:99.17ms
step:1619/1770 train_time:159573ms step_avg:99.18ms
step:1620/1770 train_time:159677ms step_avg:99.18ms
step:1621/1770 train_time:159780ms step_avg:99.18ms
step:1622/1770 train_time:159884ms step_avg:99.18ms
step:1623/1770 train_time:159991ms step_avg:99.19ms
step:1624/1770 train_time:160094ms step_avg:99.19ms
step:1625/1770 train_time:160197ms step_avg:99.19ms
step:1625/1770 val_loss:3.3085 train_time:160299ms step_avg:99.26ms
step:1626/1770 train_time:160320ms step_avg:99.21ms
step:1627/1770 train_time:160412ms step_avg:99.20ms
step:1628/1770 train_time:160515ms step_avg:99.21ms
step:1629/1770 train_time:160618ms step_avg:99.21ms
step:1630/1770 train_time:160721ms step_avg:99.21ms
step:1631/1770 train_time:160823ms step_avg:99.21ms
step:1632/1770 train_time:160926ms step_avg:99.21ms
step:1633/1770 train_time:161029ms step_avg:99.22ms
step:1634/1770 train_time:161132ms step_avg:99.22ms
step:1635/1770 train_time:161235ms step_avg:99.22ms
step:1636/1770 train_time:161339ms step_avg:99.22ms
step:1637/1770 train_time:161444ms step_avg:99.23ms
step:1638/1770 train_time:161547ms step_avg:99.23ms
step:1639/1770 train_time:161652ms step_avg:99.23ms
step:1640/1770 train_time:161755ms step_avg:99.24ms
step:1641/1770 train_time:161858ms step_avg:99.24ms
step:1642/1770 train_time:161960ms step_avg:99.24ms
step:1643/1770 train_time:162063ms step_avg:99.24ms
step:1644/1770 train_time:162169ms step_avg:99.25ms
step:1645/1770 train_time:162271ms step_avg:99.25ms
step:1646/1770 train_time:162377ms step_avg:99.25ms
step:1647/1770 train_time:162483ms step_avg:99.26ms
step:1648/1770 train_time:162586ms step_avg:99.26ms
step:1649/1770 train_time:162690ms step_avg:99.26ms
step:1650/1770 train_time:162793ms step_avg:99.26ms
step:1651/1770 train_time:162896ms step_avg:99.27ms
step:1652/1770 train_time:162999ms step_avg:99.27ms
step:1653/1770 train_time:163103ms step_avg:99.27ms
step:1654/1770 train_time:163210ms step_avg:99.28ms
step:1655/1770 train_time:163316ms step_avg:99.28ms
step:1656/1770 train_time:163419ms step_avg:99.28ms
step:1657/1770 train_time:163524ms step_avg:99.29ms
step:1658/1770 train_time:163627ms step_avg:99.29ms
step:1659/1770 train_time:163732ms step_avg:99.29ms
step:1660/1770 train_time:163838ms step_avg:99.30ms
step:1661/1770 train_time:163939ms step_avg:99.30ms
step:1662/1770 train_time:164043ms step_avg:99.30ms
step:1663/1770 train_time:164145ms step_avg:99.30ms
step:1664/1770 train_time:164248ms step_avg:99.30ms
step:1665/1770 train_time:164351ms step_avg:99.31ms
step:1666/1770 train_time:164456ms step_avg:99.31ms
step:1667/1770 train_time:164560ms step_avg:99.31ms
step:1668/1770 train_time:164663ms step_avg:99.31ms
step:1669/1770 train_time:164765ms step_avg:99.32ms
step:1670/1770 train_time:164868ms step_avg:99.32ms
step:1671/1770 train_time:164972ms step_avg:99.32ms
step:1672/1770 train_time:165076ms step_avg:99.32ms
step:1673/1770 train_time:165182ms step_avg:99.33ms
step:1674/1770 train_time:165285ms step_avg:99.33ms
step:1675/1770 train_time:165388ms step_avg:99.33ms
step:1676/1770 train_time:165493ms step_avg:99.34ms
step:1677/1770 train_time:165600ms step_avg:99.34ms
step:1678/1770 train_time:165702ms step_avg:99.34ms
step:1679/1770 train_time:165805ms step_avg:99.34ms
step:1680/1770 train_time:165908ms step_avg:99.35ms
step:1681/1770 train_time:166012ms step_avg:99.35ms
step:1682/1770 train_time:166118ms step_avg:99.35ms
step:1683/1770 train_time:166221ms step_avg:99.35ms
step:1684/1770 train_time:166324ms step_avg:99.36ms
step:1685/1770 train_time:166429ms step_avg:99.36ms
step:1686/1770 train_time:166533ms step_avg:99.36ms
step:1687/1770 train_time:166638ms step_avg:99.37ms
step:1688/1770 train_time:166741ms step_avg:99.37ms
step:1689/1770 train_time:166844ms step_avg:99.37ms
step:1690/1770 train_time:166947ms step_avg:99.37ms
step:1691/1770 train_time:167049ms step_avg:99.37ms
step:1692/1770 train_time:167153ms step_avg:99.38ms
step:1693/1770 train_time:167258ms step_avg:99.38ms
step:1694/1770 train_time:167361ms step_avg:99.38ms
step:1695/1770 train_time:167465ms step_avg:99.39ms
step:1696/1770 train_time:167570ms step_avg:99.39ms
step:1697/1770 train_time:167676ms step_avg:99.39ms
step:1698/1770 train_time:167781ms step_avg:99.40ms
step:1699/1770 train_time:167884ms step_avg:99.40ms
step:1700/1770 train_time:167988ms step_avg:99.40ms
step:1701/1770 train_time:168091ms step_avg:99.40ms
step:1702/1770 train_time:168194ms step_avg:99.41ms
step:1703/1770 train_time:168297ms step_avg:99.41ms
step:1704/1770 train_time:168400ms step_avg:99.41ms
step:1705/1770 train_time:168504ms step_avg:99.41ms
step:1706/1770 train_time:168606ms step_avg:99.41ms
step:1707/1770 train_time:168711ms step_avg:99.42ms
step:1708/1770 train_time:168815ms step_avg:99.42ms
step:1709/1770 train_time:168920ms step_avg:99.42ms
step:1710/1770 train_time:169027ms step_avg:99.43ms
step:1711/1770 train_time:169133ms step_avg:99.43ms
step:1712/1770 train_time:169237ms step_avg:99.43ms
step:1713/1770 train_time:169341ms step_avg:99.44ms
step:1714/1770 train_time:169445ms step_avg:99.44ms
step:1715/1770 train_time:169548ms step_avg:99.44ms
step:1716/1770 train_time:169652ms step_avg:99.44ms
step:1717/1770 train_time:169755ms step_avg:99.45ms
step:1718/1770 train_time:169861ms step_avg:99.45ms
step:1719/1770 train_time:169966ms step_avg:99.45ms
step:1720/1770 train_time:170071ms step_avg:99.46ms
step:1721/1770 train_time:170175ms step_avg:99.46ms
step:1722/1770 train_time:170282ms step_avg:99.46ms
step:1723/1770 train_time:170387ms step_avg:99.47ms
step:1724/1770 train_time:170493ms step_avg:99.47ms
step:1725/1770 train_time:170599ms step_avg:99.47ms
step:1726/1770 train_time:170705ms step_avg:99.48ms
step:1727/1770 train_time:170809ms step_avg:99.48ms
step:1728/1770 train_time:170915ms step_avg:99.48ms
step:1729/1770 train_time:171018ms step_avg:99.49ms
step:1730/1770 train_time:171124ms step_avg:99.49ms
step:1731/1770 train_time:171230ms step_avg:99.49ms
step:1732/1770 train_time:171333ms step_avg:99.50ms
step:1733/1770 train_time:171439ms step_avg:99.50ms
step:1734/1770 train_time:171543ms step_avg:99.50ms
step:1735/1770 train_time:171648ms step_avg:99.51ms
step:1736/1770 train_time:171752ms step_avg:99.51ms
step:1737/1770 train_time:171856ms step_avg:99.51ms
step:1738/1770 train_time:171960ms step_avg:99.51ms
step:1739/1770 train_time:172064ms step_avg:99.52ms
step:1740/1770 train_time:172168ms step_avg:99.52ms
step:1741/1770 train_time:172274ms step_avg:99.52ms
step:1742/1770 train_time:172381ms step_avg:99.53ms
step:1743/1770 train_time:172486ms step_avg:99.53ms
step:1744/1770 train_time:172591ms step_avg:99.53ms
step:1745/1770 train_time:172695ms step_avg:99.54ms
step:1746/1770 train_time:172802ms step_avg:99.54ms
step:1747/1770 train_time:172905ms step_avg:99.54ms
step:1748/1770 train_time:173011ms step_avg:99.55ms
step:1749/1770 train_time:173116ms step_avg:99.55ms
step:1750/1770 train_time:173221ms step_avg:99.55ms
step:1750/1770 val_loss:3.2820 train_time:173323ms step_avg:99.61ms
step:1751/1770 train_time:173344ms step_avg:99.57ms
step:1752/1770 train_time:173438ms step_avg:99.56ms
step:1753/1770 train_time:173542ms step_avg:99.57ms
step:1754/1770 train_time:173647ms step_avg:99.57ms
step:1755/1770 train_time:173751ms step_avg:99.57ms
step:1756/1770 train_time:173856ms step_avg:99.57ms
step:1757/1770 train_time:173960ms step_avg:99.58ms
step:1758/1770 train_time:174063ms step_avg:99.58ms
step:1759/1770 train_time:174168ms step_avg:99.58ms
step:1760/1770 train_time:174273ms step_avg:99.58ms
step:1761/1770 train_time:174380ms step_avg:99.59ms
step:1762/1770 train_time:174489ms step_avg:99.59ms
step:1763/1770 train_time:174591ms step_avg:99.60ms
step:1764/1770 train_time:174696ms step_avg:99.60ms
step:1765/1770 train_time:174800ms step_avg:99.60ms
step:1766/1770 train_time:174908ms step_avg:99.61ms
step:1767/1770 train_time:175011ms step_avg:99.61ms
step:1768/1770 train_time:175115ms step_avg:99.61ms
step:1769/1770 train_time:175218ms step_avg:99.61ms
step:1770/1770 train_time:175323ms step_avg:99.62ms
step:1770/1770 val_loss:3.2789 train_time:175428ms step_avg:99.67ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
