import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:44:50 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23208ms step_avg:nanms
step:2/1770 train_time:23731ms step_avg:nanms
step:3/1770 train_time:23827ms step_avg:nanms
step:4/1770 train_time:23919ms step_avg:nanms
step:5/1770 train_time:24012ms step_avg:nanms
step:6/1770 train_time:24106ms step_avg:nanms
step:7/1770 train_time:24199ms step_avg:nanms
step:8/1770 train_time:24293ms step_avg:nanms
step:9/1770 train_time:24387ms step_avg:nanms
step:10/1770 train_time:24480ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.72ms
step:14/1770 train_time:379ms step_avg:94.65ms
step:15/1770 train_time:473ms step_avg:94.54ms
step:16/1770 train_time:567ms step_avg:94.43ms
step:17/1770 train_time:660ms step_avg:94.35ms
step:18/1770 train_time:754ms step_avg:94.24ms
step:19/1770 train_time:847ms step_avg:94.13ms
step:20/1770 train_time:941ms step_avg:94.14ms
step:21/1770 train_time:1035ms step_avg:94.12ms
step:22/1770 train_time:1129ms step_avg:94.09ms
step:23/1770 train_time:1223ms step_avg:94.10ms
step:24/1770 train_time:1318ms step_avg:94.12ms
step:25/1770 train_time:1412ms step_avg:94.12ms
step:26/1770 train_time:1506ms step_avg:94.14ms
step:27/1770 train_time:1600ms step_avg:94.11ms
step:28/1770 train_time:1694ms step_avg:94.11ms
step:29/1770 train_time:1788ms step_avg:94.12ms
step:30/1770 train_time:1882ms step_avg:94.11ms
step:31/1770 train_time:1976ms step_avg:94.08ms
step:32/1770 train_time:2069ms step_avg:94.06ms
step:33/1770 train_time:2163ms step_avg:94.04ms
step:34/1770 train_time:2257ms step_avg:94.06ms
step:35/1770 train_time:2352ms step_avg:94.07ms
step:36/1770 train_time:2446ms step_avg:94.07ms
step:37/1770 train_time:2540ms step_avg:94.06ms
step:38/1770 train_time:2634ms step_avg:94.06ms
step:39/1770 train_time:2727ms step_avg:94.05ms
step:40/1770 train_time:2821ms step_avg:94.04ms
step:41/1770 train_time:2915ms step_avg:94.04ms
step:42/1770 train_time:3009ms step_avg:94.03ms
step:43/1770 train_time:3102ms step_avg:94.01ms
step:44/1770 train_time:3196ms step_avg:93.99ms
step:45/1770 train_time:3290ms step_avg:93.99ms
step:46/1770 train_time:3383ms step_avg:93.98ms
step:47/1770 train_time:3477ms step_avg:93.98ms
step:48/1770 train_time:3571ms step_avg:93.98ms
step:49/1770 train_time:3666ms step_avg:94.01ms
step:50/1770 train_time:3760ms step_avg:94.00ms
step:51/1770 train_time:3854ms step_avg:93.99ms
step:52/1770 train_time:3948ms step_avg:93.99ms
step:53/1770 train_time:4041ms step_avg:93.98ms
step:54/1770 train_time:4135ms step_avg:93.97ms
step:55/1770 train_time:4228ms step_avg:93.96ms
step:56/1770 train_time:4322ms step_avg:93.95ms
step:57/1770 train_time:4416ms step_avg:93.95ms
step:58/1770 train_time:4510ms step_avg:93.96ms
step:59/1770 train_time:4604ms step_avg:93.96ms
step:60/1770 train_time:4698ms step_avg:93.96ms
step:61/1770 train_time:4792ms step_avg:93.97ms
step:62/1770 train_time:4886ms step_avg:93.96ms
step:63/1770 train_time:4979ms step_avg:93.95ms
step:64/1770 train_time:5073ms step_avg:93.94ms
step:65/1770 train_time:5167ms step_avg:93.95ms
step:66/1770 train_time:5261ms step_avg:93.95ms
step:67/1770 train_time:5355ms step_avg:93.95ms
step:68/1770 train_time:5449ms step_avg:93.95ms
step:69/1770 train_time:5543ms step_avg:93.96ms
step:70/1770 train_time:5637ms step_avg:93.95ms
step:71/1770 train_time:5731ms step_avg:93.95ms
step:72/1770 train_time:5825ms step_avg:93.96ms
step:73/1770 train_time:5919ms step_avg:93.94ms
step:74/1770 train_time:6013ms step_avg:93.95ms
step:75/1770 train_time:6107ms step_avg:93.95ms
step:76/1770 train_time:6200ms step_avg:93.95ms
step:77/1770 train_time:6294ms step_avg:93.94ms
step:78/1770 train_time:6388ms step_avg:93.94ms
step:79/1770 train_time:6481ms step_avg:93.93ms
step:80/1770 train_time:6575ms step_avg:93.93ms
step:81/1770 train_time:6669ms step_avg:93.93ms
step:82/1770 train_time:6763ms step_avg:93.93ms
step:83/1770 train_time:6857ms step_avg:93.93ms
step:84/1770 train_time:6950ms step_avg:93.92ms
step:85/1770 train_time:7044ms step_avg:93.92ms
step:86/1770 train_time:7138ms step_avg:93.92ms
step:87/1770 train_time:7232ms step_avg:93.92ms
step:88/1770 train_time:7326ms step_avg:93.93ms
step:89/1770 train_time:7420ms step_avg:93.92ms
step:90/1770 train_time:7514ms step_avg:93.93ms
step:91/1770 train_time:7608ms step_avg:93.92ms
step:92/1770 train_time:7702ms step_avg:93.92ms
step:93/1770 train_time:7796ms step_avg:93.92ms
step:94/1770 train_time:7890ms step_avg:93.92ms
step:95/1770 train_time:7983ms step_avg:93.92ms
step:96/1770 train_time:8077ms step_avg:93.91ms
step:97/1770 train_time:8170ms step_avg:93.91ms
step:98/1770 train_time:8264ms step_avg:93.91ms
step:99/1770 train_time:8358ms step_avg:93.91ms
step:100/1770 train_time:8452ms step_avg:93.91ms
step:101/1770 train_time:8546ms step_avg:93.92ms
step:102/1770 train_time:8640ms step_avg:93.91ms
step:103/1770 train_time:8734ms step_avg:93.91ms
step:104/1770 train_time:8827ms step_avg:93.91ms
step:105/1770 train_time:8921ms step_avg:93.90ms
step:106/1770 train_time:9014ms step_avg:93.90ms
step:107/1770 train_time:9108ms step_avg:93.90ms
step:108/1770 train_time:9202ms step_avg:93.89ms
step:109/1770 train_time:9295ms step_avg:93.89ms
step:110/1770 train_time:9389ms step_avg:93.89ms
step:111/1770 train_time:9483ms step_avg:93.89ms
step:112/1770 train_time:9577ms step_avg:93.89ms
step:113/1770 train_time:9670ms step_avg:93.89ms
step:114/1770 train_time:9764ms step_avg:93.89ms
step:115/1770 train_time:9858ms step_avg:93.89ms
step:116/1770 train_time:9952ms step_avg:93.88ms
step:117/1770 train_time:10046ms step_avg:93.89ms
step:118/1770 train_time:10140ms step_avg:93.89ms
step:119/1770 train_time:10233ms step_avg:93.88ms
step:120/1770 train_time:10327ms step_avg:93.88ms
step:121/1770 train_time:10420ms step_avg:93.88ms
step:122/1770 train_time:10514ms step_avg:93.87ms
step:123/1770 train_time:10607ms step_avg:93.87ms
step:124/1770 train_time:10701ms step_avg:93.87ms
step:125/1770 train_time:10795ms step_avg:93.87ms
step:125/1770 val_loss:4.6461 train_time:10887ms step_avg:94.67ms
step:126/1770 train_time:10909ms step_avg:94.04ms
step:127/1770 train_time:10987ms step_avg:93.91ms
step:128/1770 train_time:11084ms step_avg:93.93ms
step:129/1770 train_time:11189ms step_avg:94.02ms
step:130/1770 train_time:11285ms step_avg:94.04ms
step:131/1770 train_time:11379ms step_avg:94.04ms
step:132/1770 train_time:11472ms step_avg:94.03ms
step:133/1770 train_time:11566ms step_avg:94.03ms
step:134/1770 train_time:11660ms step_avg:94.03ms
step:135/1770 train_time:11754ms step_avg:94.03ms
step:136/1770 train_time:11848ms step_avg:94.03ms
step:137/1770 train_time:11942ms step_avg:94.03ms
step:138/1770 train_time:12036ms step_avg:94.03ms
step:139/1770 train_time:12131ms step_avg:94.04ms
step:140/1770 train_time:12226ms step_avg:94.04ms
step:141/1770 train_time:12321ms step_avg:94.05ms
step:142/1770 train_time:12415ms step_avg:94.06ms
step:143/1770 train_time:12510ms step_avg:94.06ms
step:144/1770 train_time:12604ms step_avg:94.06ms
step:145/1770 train_time:12698ms step_avg:94.06ms
step:146/1770 train_time:12793ms step_avg:94.06ms
step:147/1770 train_time:12887ms step_avg:94.06ms
step:148/1770 train_time:12981ms step_avg:94.06ms
step:149/1770 train_time:13075ms step_avg:94.07ms
step:150/1770 train_time:13170ms step_avg:94.07ms
step:151/1770 train_time:13264ms step_avg:94.07ms
step:152/1770 train_time:13358ms step_avg:94.07ms
step:153/1770 train_time:13452ms step_avg:94.07ms
step:154/1770 train_time:13546ms step_avg:94.07ms
step:155/1770 train_time:13642ms step_avg:94.08ms
step:156/1770 train_time:13735ms step_avg:94.08ms
step:157/1770 train_time:13830ms step_avg:94.08ms
step:158/1770 train_time:13925ms step_avg:94.09ms
step:159/1770 train_time:14019ms step_avg:94.09ms
step:160/1770 train_time:14113ms step_avg:94.09ms
step:161/1770 train_time:14208ms step_avg:94.09ms
step:162/1770 train_time:14303ms step_avg:94.10ms
step:163/1770 train_time:14397ms step_avg:94.10ms
step:164/1770 train_time:14492ms step_avg:94.10ms
step:165/1770 train_time:14587ms step_avg:94.11ms
step:166/1770 train_time:14681ms step_avg:94.11ms
step:167/1770 train_time:14775ms step_avg:94.11ms
step:168/1770 train_time:14869ms step_avg:94.11ms
step:169/1770 train_time:14964ms step_avg:94.11ms
step:170/1770 train_time:15058ms step_avg:94.11ms
step:171/1770 train_time:15153ms step_avg:94.12ms
step:172/1770 train_time:15248ms step_avg:94.12ms
step:173/1770 train_time:15342ms step_avg:94.12ms
step:174/1770 train_time:15436ms step_avg:94.12ms
step:175/1770 train_time:15531ms step_avg:94.12ms
step:176/1770 train_time:15625ms step_avg:94.13ms
step:177/1770 train_time:15719ms step_avg:94.13ms
step:178/1770 train_time:15814ms step_avg:94.13ms
step:179/1770 train_time:15908ms step_avg:94.13ms
step:180/1770 train_time:16004ms step_avg:94.14ms
step:181/1770 train_time:16098ms step_avg:94.14ms
step:182/1770 train_time:16192ms step_avg:94.14ms
step:183/1770 train_time:16287ms step_avg:94.14ms
step:184/1770 train_time:16381ms step_avg:94.15ms
step:185/1770 train_time:16476ms step_avg:94.15ms
step:186/1770 train_time:16570ms step_avg:94.15ms
step:187/1770 train_time:16664ms step_avg:94.15ms
step:188/1770 train_time:16759ms step_avg:94.15ms
step:189/1770 train_time:16854ms step_avg:94.16ms
step:190/1770 train_time:16948ms step_avg:94.16ms
step:191/1770 train_time:17043ms step_avg:94.16ms
step:192/1770 train_time:17137ms step_avg:94.16ms
step:193/1770 train_time:17231ms step_avg:94.16ms
step:194/1770 train_time:17326ms step_avg:94.16ms
step:195/1770 train_time:17421ms step_avg:94.17ms
step:196/1770 train_time:17515ms step_avg:94.17ms
step:197/1770 train_time:17609ms step_avg:94.17ms
step:198/1770 train_time:17704ms step_avg:94.17ms
step:199/1770 train_time:17799ms step_avg:94.18ms
step:200/1770 train_time:17894ms step_avg:94.18ms
step:201/1770 train_time:17988ms step_avg:94.18ms
step:202/1770 train_time:18083ms step_avg:94.18ms
step:203/1770 train_time:18177ms step_avg:94.18ms
step:204/1770 train_time:18272ms step_avg:94.19ms
step:205/1770 train_time:18366ms step_avg:94.19ms
step:206/1770 train_time:18460ms step_avg:94.18ms
step:207/1770 train_time:18554ms step_avg:94.18ms
step:208/1770 train_time:18648ms step_avg:94.18ms
step:209/1770 train_time:18743ms step_avg:94.19ms
step:210/1770 train_time:18838ms step_avg:94.19ms
step:211/1770 train_time:18932ms step_avg:94.19ms
step:212/1770 train_time:19026ms step_avg:94.19ms
step:213/1770 train_time:19121ms step_avg:94.19ms
step:214/1770 train_time:19216ms step_avg:94.19ms
step:215/1770 train_time:19310ms step_avg:94.20ms
step:216/1770 train_time:19405ms step_avg:94.20ms
step:217/1770 train_time:19500ms step_avg:94.20ms
step:218/1770 train_time:19594ms step_avg:94.20ms
step:219/1770 train_time:19688ms step_avg:94.20ms
step:220/1770 train_time:19783ms step_avg:94.20ms
step:221/1770 train_time:19877ms step_avg:94.20ms
step:222/1770 train_time:19971ms step_avg:94.20ms
step:223/1770 train_time:20066ms step_avg:94.21ms
step:224/1770 train_time:20160ms step_avg:94.20ms
step:225/1770 train_time:20255ms step_avg:94.21ms
step:226/1770 train_time:20349ms step_avg:94.21ms
step:227/1770 train_time:20444ms step_avg:94.21ms
step:228/1770 train_time:20539ms step_avg:94.22ms
step:229/1770 train_time:20633ms step_avg:94.21ms
step:230/1770 train_time:20727ms step_avg:94.21ms
step:231/1770 train_time:20822ms step_avg:94.22ms
step:232/1770 train_time:20916ms step_avg:94.22ms
step:233/1770 train_time:21011ms step_avg:94.22ms
step:234/1770 train_time:21105ms step_avg:94.22ms
step:235/1770 train_time:21200ms step_avg:94.22ms
step:236/1770 train_time:21294ms step_avg:94.22ms
step:237/1770 train_time:21389ms step_avg:94.22ms
step:238/1770 train_time:21483ms step_avg:94.23ms
step:239/1770 train_time:21578ms step_avg:94.23ms
step:240/1770 train_time:21673ms step_avg:94.23ms
step:241/1770 train_time:21768ms step_avg:94.23ms
step:242/1770 train_time:21862ms step_avg:94.23ms
step:243/1770 train_time:21956ms step_avg:94.23ms
step:244/1770 train_time:22051ms step_avg:94.23ms
step:245/1770 train_time:22145ms step_avg:94.23ms
step:246/1770 train_time:22240ms step_avg:94.24ms
step:247/1770 train_time:22334ms step_avg:94.24ms
step:248/1770 train_time:22428ms step_avg:94.23ms
step:249/1770 train_time:22523ms step_avg:94.24ms
step:250/1770 train_time:22617ms step_avg:94.24ms
step:250/1770 val_loss:4.1071 train_time:22710ms step_avg:94.62ms
step:251/1770 train_time:22732ms step_avg:94.32ms
step:252/1770 train_time:22815ms step_avg:94.28ms
step:253/1770 train_time:22912ms step_avg:94.29ms
step:254/1770 train_time:23007ms step_avg:94.29ms
step:255/1770 train_time:23101ms step_avg:94.29ms
step:256/1770 train_time:23195ms step_avg:94.29ms
step:257/1770 train_time:23288ms step_avg:94.29ms
step:258/1770 train_time:23382ms step_avg:94.28ms
step:259/1770 train_time:23476ms step_avg:94.28ms
step:260/1770 train_time:23570ms step_avg:94.28ms
step:261/1770 train_time:23664ms step_avg:94.28ms
step:262/1770 train_time:23759ms step_avg:94.28ms
step:263/1770 train_time:23854ms step_avg:94.29ms
step:264/1770 train_time:23949ms step_avg:94.29ms
step:265/1770 train_time:24044ms step_avg:94.29ms
step:266/1770 train_time:24139ms step_avg:94.29ms
step:267/1770 train_time:24233ms step_avg:94.29ms
step:268/1770 train_time:24329ms step_avg:94.30ms
step:269/1770 train_time:24424ms step_avg:94.30ms
step:270/1770 train_time:24519ms step_avg:94.30ms
step:271/1770 train_time:24613ms step_avg:94.30ms
step:272/1770 train_time:24708ms step_avg:94.30ms
step:273/1770 train_time:24803ms step_avg:94.31ms
step:274/1770 train_time:24898ms step_avg:94.31ms
step:275/1770 train_time:24993ms step_avg:94.31ms
step:276/1770 train_time:25088ms step_avg:94.32ms
step:277/1770 train_time:25184ms step_avg:94.32ms
step:278/1770 train_time:25279ms step_avg:94.33ms
step:279/1770 train_time:25374ms step_avg:94.33ms
step:280/1770 train_time:25469ms step_avg:94.33ms
step:281/1770 train_time:25564ms step_avg:94.33ms
step:282/1770 train_time:25658ms step_avg:94.33ms
step:283/1770 train_time:25753ms step_avg:94.33ms
step:284/1770 train_time:25849ms step_avg:94.34ms
step:285/1770 train_time:25944ms step_avg:94.34ms
step:286/1770 train_time:26039ms step_avg:94.34ms
step:287/1770 train_time:26134ms step_avg:94.35ms
step:288/1770 train_time:26229ms step_avg:94.35ms
step:289/1770 train_time:26323ms step_avg:94.35ms
step:290/1770 train_time:26418ms step_avg:94.35ms
step:291/1770 train_time:26513ms step_avg:94.35ms
step:292/1770 train_time:26608ms step_avg:94.36ms
step:293/1770 train_time:26703ms step_avg:94.36ms
step:294/1770 train_time:26798ms step_avg:94.36ms
step:295/1770 train_time:26893ms step_avg:94.36ms
step:296/1770 train_time:26988ms step_avg:94.36ms
step:297/1770 train_time:27083ms step_avg:94.36ms
step:298/1770 train_time:27177ms step_avg:94.37ms
step:299/1770 train_time:27272ms step_avg:94.37ms
step:300/1770 train_time:27367ms step_avg:94.37ms
step:301/1770 train_time:27462ms step_avg:94.37ms
step:302/1770 train_time:27557ms step_avg:94.37ms
step:303/1770 train_time:27652ms step_avg:94.37ms
step:304/1770 train_time:27747ms step_avg:94.38ms
step:305/1770 train_time:27842ms step_avg:94.38ms
step:306/1770 train_time:27937ms step_avg:94.38ms
step:307/1770 train_time:28031ms step_avg:94.38ms
step:308/1770 train_time:28127ms step_avg:94.39ms
step:309/1770 train_time:28222ms step_avg:94.39ms
step:310/1770 train_time:28317ms step_avg:94.39ms
step:311/1770 train_time:28412ms step_avg:94.39ms
step:312/1770 train_time:28507ms step_avg:94.39ms
step:313/1770 train_time:28602ms step_avg:94.40ms
step:314/1770 train_time:28698ms step_avg:94.40ms
step:315/1770 train_time:28792ms step_avg:94.40ms
step:316/1770 train_time:28886ms step_avg:94.40ms
step:317/1770 train_time:28981ms step_avg:94.40ms
step:318/1770 train_time:29076ms step_avg:94.40ms
step:319/1770 train_time:29171ms step_avg:94.40ms
step:320/1770 train_time:29266ms step_avg:94.41ms
step:321/1770 train_time:29361ms step_avg:94.41ms
step:322/1770 train_time:29455ms step_avg:94.41ms
step:323/1770 train_time:29550ms step_avg:94.41ms
step:324/1770 train_time:29645ms step_avg:94.41ms
step:325/1770 train_time:29741ms step_avg:94.42ms
step:326/1770 train_time:29837ms step_avg:94.42ms
step:327/1770 train_time:29931ms step_avg:94.42ms
step:328/1770 train_time:30027ms step_avg:94.42ms
step:329/1770 train_time:30122ms step_avg:94.43ms
step:330/1770 train_time:30217ms step_avg:94.43ms
step:331/1770 train_time:30312ms step_avg:94.43ms
step:332/1770 train_time:30408ms step_avg:94.43ms
step:333/1770 train_time:30503ms step_avg:94.44ms
step:334/1770 train_time:30598ms step_avg:94.44ms
step:335/1770 train_time:30693ms step_avg:94.44ms
step:336/1770 train_time:30788ms step_avg:94.44ms
step:337/1770 train_time:30882ms step_avg:94.44ms
step:338/1770 train_time:30977ms step_avg:94.44ms
step:339/1770 train_time:31072ms step_avg:94.44ms
step:340/1770 train_time:31167ms step_avg:94.44ms
step:341/1770 train_time:31261ms step_avg:94.45ms
step:342/1770 train_time:31356ms step_avg:94.45ms
step:343/1770 train_time:31451ms step_avg:94.45ms
step:344/1770 train_time:31546ms step_avg:94.45ms
step:345/1770 train_time:31641ms step_avg:94.45ms
step:346/1770 train_time:31736ms step_avg:94.45ms
step:347/1770 train_time:31831ms step_avg:94.45ms
step:348/1770 train_time:31926ms step_avg:94.46ms
step:349/1770 train_time:32021ms step_avg:94.46ms
step:350/1770 train_time:32116ms step_avg:94.46ms
step:351/1770 train_time:32211ms step_avg:94.46ms
step:352/1770 train_time:32306ms step_avg:94.46ms
step:353/1770 train_time:32401ms step_avg:94.46ms
step:354/1770 train_time:32496ms step_avg:94.47ms
step:355/1770 train_time:32591ms step_avg:94.47ms
step:356/1770 train_time:32687ms step_avg:94.47ms
step:357/1770 train_time:32782ms step_avg:94.47ms
step:358/1770 train_time:32876ms step_avg:94.47ms
step:359/1770 train_time:32971ms step_avg:94.47ms
step:360/1770 train_time:33066ms step_avg:94.47ms
step:361/1770 train_time:33161ms step_avg:94.48ms
step:362/1770 train_time:33256ms step_avg:94.48ms
step:363/1770 train_time:33351ms step_avg:94.48ms
step:364/1770 train_time:33446ms step_avg:94.48ms
step:365/1770 train_time:33541ms step_avg:94.48ms
step:366/1770 train_time:33636ms step_avg:94.48ms
step:367/1770 train_time:33731ms step_avg:94.48ms
step:368/1770 train_time:33826ms step_avg:94.49ms
step:369/1770 train_time:33922ms step_avg:94.49ms
step:370/1770 train_time:34017ms step_avg:94.49ms
step:371/1770 train_time:34111ms step_avg:94.49ms
step:372/1770 train_time:34207ms step_avg:94.49ms
step:373/1770 train_time:34301ms step_avg:94.49ms
step:374/1770 train_time:34396ms step_avg:94.50ms
step:375/1770 train_time:34491ms step_avg:94.50ms
step:375/1770 val_loss:3.8969 train_time:34584ms step_avg:94.75ms
step:376/1770 train_time:34607ms step_avg:94.55ms
step:377/1770 train_time:34692ms step_avg:94.53ms
step:378/1770 train_time:34791ms step_avg:94.54ms
step:379/1770 train_time:34887ms step_avg:94.55ms
step:380/1770 train_time:34982ms step_avg:94.55ms
step:381/1770 train_time:35077ms step_avg:94.55ms
step:382/1770 train_time:35171ms step_avg:94.55ms
step:383/1770 train_time:35266ms step_avg:94.55ms
step:384/1770 train_time:35361ms step_avg:94.55ms
step:385/1770 train_time:35455ms step_avg:94.55ms
step:386/1770 train_time:35549ms step_avg:94.55ms
step:387/1770 train_time:35645ms step_avg:94.55ms
step:388/1770 train_time:35741ms step_avg:94.55ms
step:389/1770 train_time:35837ms step_avg:94.56ms
step:390/1770 train_time:35932ms step_avg:94.56ms
step:391/1770 train_time:36028ms step_avg:94.56ms
step:392/1770 train_time:36123ms step_avg:94.56ms
step:393/1770 train_time:36217ms step_avg:94.56ms
step:394/1770 train_time:36312ms step_avg:94.56ms
step:395/1770 train_time:36407ms step_avg:94.56ms
step:396/1770 train_time:36503ms step_avg:94.57ms
step:397/1770 train_time:36600ms step_avg:94.57ms
step:398/1770 train_time:36697ms step_avg:94.58ms
step:399/1770 train_time:36794ms step_avg:94.59ms
step:400/1770 train_time:36891ms step_avg:94.59ms
step:401/1770 train_time:36987ms step_avg:94.60ms
step:402/1770 train_time:37084ms step_avg:94.60ms
step:403/1770 train_time:37180ms step_avg:94.61ms
step:404/1770 train_time:37277ms step_avg:94.61ms
step:405/1770 train_time:37374ms step_avg:94.62ms
step:406/1770 train_time:37471ms step_avg:94.62ms
step:407/1770 train_time:37569ms step_avg:94.63ms
step:408/1770 train_time:37666ms step_avg:94.64ms
step:409/1770 train_time:37764ms step_avg:94.65ms
step:410/1770 train_time:37861ms step_avg:94.65ms
step:411/1770 train_time:37957ms step_avg:94.66ms
step:412/1770 train_time:38054ms step_avg:94.66ms
step:413/1770 train_time:38150ms step_avg:94.66ms
step:414/1770 train_time:38247ms step_avg:94.67ms
step:415/1770 train_time:38344ms step_avg:94.68ms
step:416/1770 train_time:38441ms step_avg:94.68ms
step:417/1770 train_time:38538ms step_avg:94.69ms
step:418/1770 train_time:38635ms step_avg:94.69ms
step:419/1770 train_time:38732ms step_avg:94.70ms
step:420/1770 train_time:38829ms step_avg:94.71ms
step:421/1770 train_time:38926ms step_avg:94.71ms
step:422/1770 train_time:39023ms step_avg:94.72ms
step:423/1770 train_time:39120ms step_avg:94.72ms
step:424/1770 train_time:39217ms step_avg:94.73ms
step:425/1770 train_time:39313ms step_avg:94.73ms
step:426/1770 train_time:39410ms step_avg:94.74ms
step:427/1770 train_time:39508ms step_avg:94.74ms
step:428/1770 train_time:39604ms step_avg:94.75ms
step:429/1770 train_time:39701ms step_avg:94.75ms
step:430/1770 train_time:39798ms step_avg:94.76ms
step:431/1770 train_time:39895ms step_avg:94.76ms
step:432/1770 train_time:39992ms step_avg:94.77ms
step:433/1770 train_time:40089ms step_avg:94.77ms
step:434/1770 train_time:40187ms step_avg:94.78ms
step:435/1770 train_time:40284ms step_avg:94.79ms
step:436/1770 train_time:40381ms step_avg:94.79ms
step:437/1770 train_time:40477ms step_avg:94.79ms
step:438/1770 train_time:40574ms step_avg:94.80ms
step:439/1770 train_time:40671ms step_avg:94.80ms
step:440/1770 train_time:40769ms step_avg:94.81ms
step:441/1770 train_time:40865ms step_avg:94.82ms
step:442/1770 train_time:40962ms step_avg:94.82ms
step:443/1770 train_time:41060ms step_avg:94.83ms
step:444/1770 train_time:41157ms step_avg:94.83ms
step:445/1770 train_time:41253ms step_avg:94.83ms
step:446/1770 train_time:41350ms step_avg:94.84ms
step:447/1770 train_time:41447ms step_avg:94.85ms
step:448/1770 train_time:41544ms step_avg:94.85ms
step:449/1770 train_time:41641ms step_avg:94.85ms
step:450/1770 train_time:41738ms step_avg:94.86ms
step:451/1770 train_time:41836ms step_avg:94.87ms
step:452/1770 train_time:41933ms step_avg:94.87ms
step:453/1770 train_time:42030ms step_avg:94.88ms
step:454/1770 train_time:42127ms step_avg:94.88ms
step:455/1770 train_time:42224ms step_avg:94.89ms
step:456/1770 train_time:42321ms step_avg:94.89ms
step:457/1770 train_time:42418ms step_avg:94.89ms
step:458/1770 train_time:42515ms step_avg:94.90ms
step:459/1770 train_time:42612ms step_avg:94.90ms
step:460/1770 train_time:42709ms step_avg:94.91ms
step:461/1770 train_time:42807ms step_avg:94.91ms
step:462/1770 train_time:42903ms step_avg:94.92ms
step:463/1770 train_time:43001ms step_avg:94.92ms
step:464/1770 train_time:43098ms step_avg:94.93ms
step:465/1770 train_time:43196ms step_avg:94.94ms
step:466/1770 train_time:43292ms step_avg:94.94ms
step:467/1770 train_time:43389ms step_avg:94.94ms
step:468/1770 train_time:43487ms step_avg:94.95ms
step:469/1770 train_time:43584ms step_avg:94.95ms
step:470/1770 train_time:43681ms step_avg:94.96ms
step:471/1770 train_time:43778ms step_avg:94.96ms
step:472/1770 train_time:43875ms step_avg:94.97ms
step:473/1770 train_time:43972ms step_avg:94.97ms
step:474/1770 train_time:44069ms step_avg:94.98ms
step:475/1770 train_time:44166ms step_avg:94.98ms
step:476/1770 train_time:44264ms step_avg:94.99ms
step:477/1770 train_time:44361ms step_avg:94.99ms
step:478/1770 train_time:44458ms step_avg:95.00ms
step:479/1770 train_time:44555ms step_avg:95.00ms
step:480/1770 train_time:44651ms step_avg:95.00ms
step:481/1770 train_time:44749ms step_avg:95.01ms
step:482/1770 train_time:44845ms step_avg:95.01ms
step:483/1770 train_time:44942ms step_avg:95.01ms
step:484/1770 train_time:45039ms step_avg:95.02ms
step:485/1770 train_time:45135ms step_avg:95.02ms
step:486/1770 train_time:45232ms step_avg:95.03ms
step:487/1770 train_time:45329ms step_avg:95.03ms
step:488/1770 train_time:45426ms step_avg:95.03ms
step:489/1770 train_time:45523ms step_avg:95.04ms
step:490/1770 train_time:45620ms step_avg:95.04ms
step:491/1770 train_time:45717ms step_avg:95.05ms
step:492/1770 train_time:45814ms step_avg:95.05ms
step:493/1770 train_time:45912ms step_avg:95.05ms
step:494/1770 train_time:46008ms step_avg:95.06ms
step:495/1770 train_time:46105ms step_avg:95.06ms
step:496/1770 train_time:46202ms step_avg:95.07ms
step:497/1770 train_time:46299ms step_avg:95.07ms
step:498/1770 train_time:46395ms step_avg:95.07ms
step:499/1770 train_time:46492ms step_avg:95.08ms
step:500/1770 train_time:46590ms step_avg:95.08ms
step:500/1770 val_loss:3.7464 train_time:46686ms step_avg:95.28ms
step:501/1770 train_time:46707ms step_avg:95.13ms
step:502/1770 train_time:46795ms step_avg:95.11ms
step:503/1770 train_time:46894ms step_avg:95.12ms
step:504/1770 train_time:46991ms step_avg:95.12ms
step:505/1770 train_time:47087ms step_avg:95.13ms
step:506/1770 train_time:47184ms step_avg:95.13ms
step:507/1770 train_time:47280ms step_avg:95.13ms
step:508/1770 train_time:47376ms step_avg:95.13ms
step:509/1770 train_time:47473ms step_avg:95.14ms
step:510/1770 train_time:47568ms step_avg:95.14ms
step:511/1770 train_time:47665ms step_avg:95.14ms
step:512/1770 train_time:47762ms step_avg:95.14ms
step:513/1770 train_time:47860ms step_avg:95.15ms
step:514/1770 train_time:47957ms step_avg:95.15ms
step:515/1770 train_time:48055ms step_avg:95.16ms
step:516/1770 train_time:48151ms step_avg:95.16ms
step:517/1770 train_time:48249ms step_avg:95.16ms
step:518/1770 train_time:48345ms step_avg:95.17ms
step:519/1770 train_time:48441ms step_avg:95.17ms
step:520/1770 train_time:48537ms step_avg:95.17ms
step:521/1770 train_time:48634ms step_avg:95.17ms
step:522/1770 train_time:48730ms step_avg:95.18ms
step:523/1770 train_time:48827ms step_avg:95.18ms
step:524/1770 train_time:48925ms step_avg:95.18ms
step:525/1770 train_time:49022ms step_avg:95.19ms
step:526/1770 train_time:49119ms step_avg:95.19ms
step:527/1770 train_time:49217ms step_avg:95.20ms
step:528/1770 train_time:49314ms step_avg:95.20ms
step:529/1770 train_time:49411ms step_avg:95.20ms
step:530/1770 train_time:49508ms step_avg:95.21ms
step:531/1770 train_time:49605ms step_avg:95.21ms
step:532/1770 train_time:49702ms step_avg:95.21ms
step:533/1770 train_time:49799ms step_avg:95.22ms
step:534/1770 train_time:49896ms step_avg:95.22ms
step:535/1770 train_time:49994ms step_avg:95.23ms
step:536/1770 train_time:50092ms step_avg:95.23ms
step:537/1770 train_time:50190ms step_avg:95.24ms
step:538/1770 train_time:50287ms step_avg:95.24ms
step:539/1770 train_time:50384ms step_avg:95.24ms
step:540/1770 train_time:50481ms step_avg:95.25ms
step:541/1770 train_time:50578ms step_avg:95.25ms
step:542/1770 train_time:50675ms step_avg:95.25ms
step:543/1770 train_time:50772ms step_avg:95.26ms
step:544/1770 train_time:50869ms step_avg:95.26ms
step:545/1770 train_time:50967ms step_avg:95.26ms
step:546/1770 train_time:51064ms step_avg:95.27ms
step:547/1770 train_time:51161ms step_avg:95.27ms
step:548/1770 train_time:51258ms step_avg:95.27ms
step:549/1770 train_time:51355ms step_avg:95.28ms
step:550/1770 train_time:51452ms step_avg:95.28ms
step:551/1770 train_time:51549ms step_avg:95.29ms
step:552/1770 train_time:51647ms step_avg:95.29ms
step:553/1770 train_time:51744ms step_avg:95.29ms
step:554/1770 train_time:51841ms step_avg:95.30ms
step:555/1770 train_time:51938ms step_avg:95.30ms
step:556/1770 train_time:52035ms step_avg:95.30ms
step:557/1770 train_time:52132ms step_avg:95.31ms
step:558/1770 train_time:52230ms step_avg:95.31ms
step:559/1770 train_time:52327ms step_avg:95.31ms
step:560/1770 train_time:52424ms step_avg:95.32ms
step:561/1770 train_time:52521ms step_avg:95.32ms
step:562/1770 train_time:52619ms step_avg:95.32ms
step:563/1770 train_time:52717ms step_avg:95.33ms
step:564/1770 train_time:52814ms step_avg:95.33ms
step:565/1770 train_time:52910ms step_avg:95.33ms
step:566/1770 train_time:53007ms step_avg:95.34ms
step:567/1770 train_time:53104ms step_avg:95.34ms
step:568/1770 train_time:53201ms step_avg:95.34ms
step:569/1770 train_time:53299ms step_avg:95.35ms
step:570/1770 train_time:53396ms step_avg:95.35ms
step:571/1770 train_time:53493ms step_avg:95.35ms
step:572/1770 train_time:53590ms step_avg:95.36ms
step:573/1770 train_time:53687ms step_avg:95.36ms
step:574/1770 train_time:53785ms step_avg:95.36ms
step:575/1770 train_time:53882ms step_avg:95.37ms
step:576/1770 train_time:53979ms step_avg:95.37ms
step:577/1770 train_time:54077ms step_avg:95.37ms
step:578/1770 train_time:54174ms step_avg:95.38ms
step:579/1770 train_time:54272ms step_avg:95.38ms
step:580/1770 train_time:54369ms step_avg:95.38ms
step:581/1770 train_time:54466ms step_avg:95.39ms
step:582/1770 train_time:54563ms step_avg:95.39ms
step:583/1770 train_time:54661ms step_avg:95.39ms
step:584/1770 train_time:54757ms step_avg:95.40ms
step:585/1770 train_time:54854ms step_avg:95.40ms
step:586/1770 train_time:54952ms step_avg:95.40ms
step:587/1770 train_time:55049ms step_avg:95.41ms
step:588/1770 train_time:55147ms step_avg:95.41ms
step:589/1770 train_time:55244ms step_avg:95.41ms
step:590/1770 train_time:55342ms step_avg:95.42ms
step:591/1770 train_time:55439ms step_avg:95.42ms
step:592/1770 train_time:55536ms step_avg:95.42ms
step:593/1770 train_time:55633ms step_avg:95.43ms
step:594/1770 train_time:55730ms step_avg:95.43ms
step:595/1770 train_time:55827ms step_avg:95.43ms
step:596/1770 train_time:55925ms step_avg:95.43ms
step:597/1770 train_time:56021ms step_avg:95.44ms
step:598/1770 train_time:56119ms step_avg:95.44ms
step:599/1770 train_time:56217ms step_avg:95.44ms
step:600/1770 train_time:56314ms step_avg:95.45ms
step:601/1770 train_time:56411ms step_avg:95.45ms
step:602/1770 train_time:56508ms step_avg:95.45ms
step:603/1770 train_time:56605ms step_avg:95.46ms
step:604/1770 train_time:56703ms step_avg:95.46ms
step:605/1770 train_time:56799ms step_avg:95.46ms
step:606/1770 train_time:56897ms step_avg:95.46ms
step:607/1770 train_time:56995ms step_avg:95.47ms
step:608/1770 train_time:57092ms step_avg:95.47ms
step:609/1770 train_time:57189ms step_avg:95.47ms
step:610/1770 train_time:57287ms step_avg:95.48ms
step:611/1770 train_time:57384ms step_avg:95.48ms
step:612/1770 train_time:57482ms step_avg:95.48ms
step:613/1770 train_time:57578ms step_avg:95.49ms
step:614/1770 train_time:57676ms step_avg:95.49ms
step:615/1770 train_time:57773ms step_avg:95.49ms
step:616/1770 train_time:57870ms step_avg:95.50ms
step:617/1770 train_time:57968ms step_avg:95.50ms
step:618/1770 train_time:58065ms step_avg:95.50ms
step:619/1770 train_time:58163ms step_avg:95.51ms
step:620/1770 train_time:58260ms step_avg:95.51ms
step:621/1770 train_time:58357ms step_avg:95.51ms
step:622/1770 train_time:58454ms step_avg:95.51ms
step:623/1770 train_time:58551ms step_avg:95.52ms
step:624/1770 train_time:58649ms step_avg:95.52ms
step:625/1770 train_time:58746ms step_avg:95.52ms
step:625/1770 val_loss:3.6630 train_time:58842ms step_avg:95.68ms
step:626/1770 train_time:58864ms step_avg:95.56ms
step:627/1770 train_time:58951ms step_avg:95.54ms
step:628/1770 train_time:59052ms step_avg:95.55ms
step:629/1770 train_time:59150ms step_avg:95.56ms
step:630/1770 train_time:59248ms step_avg:95.56ms
step:631/1770 train_time:59344ms step_avg:95.56ms
step:632/1770 train_time:59441ms step_avg:95.56ms
step:633/1770 train_time:59537ms step_avg:95.57ms
step:634/1770 train_time:59634ms step_avg:95.57ms
step:635/1770 train_time:59731ms step_avg:95.57ms
step:636/1770 train_time:59828ms step_avg:95.57ms
step:637/1770 train_time:59925ms step_avg:95.57ms
step:638/1770 train_time:60024ms step_avg:95.58ms
step:639/1770 train_time:60122ms step_avg:95.58ms
step:640/1770 train_time:60220ms step_avg:95.59ms
step:641/1770 train_time:60317ms step_avg:95.59ms
step:642/1770 train_time:60414ms step_avg:95.59ms
step:643/1770 train_time:60511ms step_avg:95.59ms
step:644/1770 train_time:60608ms step_avg:95.60ms
step:645/1770 train_time:60705ms step_avg:95.60ms
step:646/1770 train_time:60802ms step_avg:95.60ms
step:647/1770 train_time:60898ms step_avg:95.60ms
step:648/1770 train_time:60995ms step_avg:95.60ms
step:649/1770 train_time:61093ms step_avg:95.61ms
step:650/1770 train_time:61191ms step_avg:95.61ms
step:651/1770 train_time:61290ms step_avg:95.62ms
step:652/1770 train_time:61387ms step_avg:95.62ms
step:653/1770 train_time:61484ms step_avg:95.62ms
step:654/1770 train_time:61582ms step_avg:95.62ms
step:655/1770 train_time:61679ms step_avg:95.63ms
step:656/1770 train_time:61776ms step_avg:95.63ms
step:657/1770 train_time:61873ms step_avg:95.63ms
step:658/1770 train_time:61971ms step_avg:95.63ms
step:659/1770 train_time:62070ms step_avg:95.64ms
step:660/1770 train_time:62169ms step_avg:95.64ms
step:661/1770 train_time:62268ms step_avg:95.65ms
step:662/1770 train_time:62368ms step_avg:95.66ms
step:663/1770 train_time:62467ms step_avg:95.66ms
step:664/1770 train_time:62567ms step_avg:95.67ms
step:665/1770 train_time:62666ms step_avg:95.67ms
step:666/1770 train_time:62765ms step_avg:95.68ms
step:667/1770 train_time:62864ms step_avg:95.68ms
step:668/1770 train_time:62964ms step_avg:95.69ms
step:669/1770 train_time:63063ms step_avg:95.70ms
step:670/1770 train_time:63162ms step_avg:95.70ms
step:671/1770 train_time:63261ms step_avg:95.71ms
step:672/1770 train_time:63360ms step_avg:95.71ms
step:673/1770 train_time:63459ms step_avg:95.72ms
step:674/1770 train_time:63558ms step_avg:95.72ms
step:675/1770 train_time:63657ms step_avg:95.72ms
step:676/1770 train_time:63756ms step_avg:95.73ms
step:677/1770 train_time:63855ms step_avg:95.73ms
step:678/1770 train_time:63955ms step_avg:95.74ms
step:679/1770 train_time:64054ms step_avg:95.75ms
step:680/1770 train_time:64153ms step_avg:95.75ms
step:681/1770 train_time:64252ms step_avg:95.76ms
step:682/1770 train_time:64351ms step_avg:95.76ms
step:683/1770 train_time:64450ms step_avg:95.77ms
step:684/1770 train_time:64549ms step_avg:95.77ms
step:685/1770 train_time:64649ms step_avg:95.78ms
step:686/1770 train_time:64748ms step_avg:95.78ms
step:687/1770 train_time:64847ms step_avg:95.79ms
step:688/1770 train_time:64947ms step_avg:95.79ms
step:689/1770 train_time:65046ms step_avg:95.80ms
step:690/1770 train_time:65146ms step_avg:95.80ms
step:691/1770 train_time:65244ms step_avg:95.81ms
step:692/1770 train_time:65344ms step_avg:95.81ms
step:693/1770 train_time:65443ms step_avg:95.82ms
step:694/1770 train_time:65541ms step_avg:95.82ms
step:695/1770 train_time:65640ms step_avg:95.82ms
step:696/1770 train_time:65739ms step_avg:95.83ms
step:697/1770 train_time:65838ms step_avg:95.83ms
step:698/1770 train_time:65937ms step_avg:95.84ms
step:699/1770 train_time:66037ms step_avg:95.84ms
step:700/1770 train_time:66136ms step_avg:95.85ms
step:701/1770 train_time:66235ms step_avg:95.85ms
step:702/1770 train_time:66334ms step_avg:95.86ms
step:703/1770 train_time:66434ms step_avg:95.86ms
step:704/1770 train_time:66533ms step_avg:95.87ms
step:705/1770 train_time:66632ms step_avg:95.87ms
step:706/1770 train_time:66732ms step_avg:95.88ms
step:707/1770 train_time:66831ms step_avg:95.88ms
step:708/1770 train_time:66930ms step_avg:95.89ms
step:709/1770 train_time:67029ms step_avg:95.89ms
step:710/1770 train_time:67128ms step_avg:95.90ms
step:711/1770 train_time:67227ms step_avg:95.90ms
step:712/1770 train_time:67327ms step_avg:95.91ms
step:713/1770 train_time:67427ms step_avg:95.91ms
step:714/1770 train_time:67527ms step_avg:95.92ms
step:715/1770 train_time:67626ms step_avg:95.92ms
step:716/1770 train_time:67724ms step_avg:95.93ms
step:717/1770 train_time:67823ms step_avg:95.93ms
step:718/1770 train_time:67922ms step_avg:95.94ms
step:719/1770 train_time:68020ms step_avg:95.94ms
step:720/1770 train_time:68119ms step_avg:95.94ms
step:721/1770 train_time:68218ms step_avg:95.95ms
step:722/1770 train_time:68317ms step_avg:95.95ms
step:723/1770 train_time:68416ms step_avg:95.96ms
step:724/1770 train_time:68515ms step_avg:95.96ms
step:725/1770 train_time:68614ms step_avg:95.96ms
step:726/1770 train_time:68713ms step_avg:95.97ms
step:727/1770 train_time:68813ms step_avg:95.97ms
step:728/1770 train_time:68911ms step_avg:95.98ms
step:729/1770 train_time:69010ms step_avg:95.98ms
step:730/1770 train_time:69109ms step_avg:95.99ms
step:731/1770 train_time:69209ms step_avg:95.99ms
step:732/1770 train_time:69309ms step_avg:96.00ms
step:733/1770 train_time:69408ms step_avg:96.00ms
step:734/1770 train_time:69507ms step_avg:96.00ms
step:735/1770 train_time:69607ms step_avg:96.01ms
step:736/1770 train_time:69706ms step_avg:96.01ms
step:737/1770 train_time:69805ms step_avg:96.02ms
step:738/1770 train_time:69904ms step_avg:96.02ms
step:739/1770 train_time:70003ms step_avg:96.03ms
step:740/1770 train_time:70101ms step_avg:96.03ms
step:741/1770 train_time:70200ms step_avg:96.03ms
step:742/1770 train_time:70299ms step_avg:96.04ms
step:743/1770 train_time:70398ms step_avg:96.04ms
step:744/1770 train_time:70497ms step_avg:96.04ms
step:745/1770 train_time:70596ms step_avg:96.05ms
step:746/1770 train_time:70696ms step_avg:96.05ms
step:747/1770 train_time:70796ms step_avg:96.06ms
step:748/1770 train_time:70895ms step_avg:96.06ms
step:749/1770 train_time:70995ms step_avg:96.07ms
step:750/1770 train_time:71094ms step_avg:96.07ms
step:750/1770 val_loss:3.5958 train_time:71191ms step_avg:96.20ms
step:751/1770 train_time:71212ms step_avg:96.10ms
step:752/1770 train_time:71306ms step_avg:96.10ms
step:753/1770 train_time:71406ms step_avg:96.10ms
step:754/1770 train_time:71505ms step_avg:96.11ms
step:755/1770 train_time:71604ms step_avg:96.11ms
step:756/1770 train_time:71702ms step_avg:96.12ms
step:757/1770 train_time:71801ms step_avg:96.12ms
step:758/1770 train_time:71899ms step_avg:96.12ms
step:759/1770 train_time:71998ms step_avg:96.13ms
step:760/1770 train_time:72096ms step_avg:96.13ms
step:761/1770 train_time:72195ms step_avg:96.13ms
step:762/1770 train_time:72296ms step_avg:96.14ms
step:763/1770 train_time:72396ms step_avg:96.14ms
step:764/1770 train_time:72496ms step_avg:96.15ms
step:765/1770 train_time:72595ms step_avg:96.15ms
step:766/1770 train_time:72693ms step_avg:96.16ms
step:767/1770 train_time:72792ms step_avg:96.16ms
step:768/1770 train_time:72891ms step_avg:96.16ms
step:769/1770 train_time:72990ms step_avg:96.17ms
step:770/1770 train_time:73089ms step_avg:96.17ms
step:771/1770 train_time:73187ms step_avg:96.17ms
step:772/1770 train_time:73287ms step_avg:96.18ms
step:773/1770 train_time:73387ms step_avg:96.18ms
step:774/1770 train_time:73487ms step_avg:96.19ms
step:775/1770 train_time:73586ms step_avg:96.19ms
step:776/1770 train_time:73685ms step_avg:96.19ms
step:777/1770 train_time:73785ms step_avg:96.20ms
step:778/1770 train_time:73884ms step_avg:96.20ms
step:779/1770 train_time:73983ms step_avg:96.21ms
step:780/1770 train_time:74082ms step_avg:96.21ms
step:781/1770 train_time:74181ms step_avg:96.21ms
step:782/1770 train_time:74280ms step_avg:96.22ms
step:783/1770 train_time:74379ms step_avg:96.22ms
step:784/1770 train_time:74479ms step_avg:96.23ms
step:785/1770 train_time:74578ms step_avg:96.23ms
step:786/1770 train_time:74678ms step_avg:96.24ms
step:787/1770 train_time:74778ms step_avg:96.24ms
step:788/1770 train_time:74878ms step_avg:96.24ms
step:789/1770 train_time:74978ms step_avg:96.25ms
step:790/1770 train_time:75077ms step_avg:96.25ms
step:791/1770 train_time:75177ms step_avg:96.26ms
step:792/1770 train_time:75277ms step_avg:96.26ms
step:793/1770 train_time:75376ms step_avg:96.27ms
step:794/1770 train_time:75475ms step_avg:96.27ms
step:795/1770 train_time:75574ms step_avg:96.27ms
step:796/1770 train_time:75674ms step_avg:96.28ms
step:797/1770 train_time:75774ms step_avg:96.28ms
step:798/1770 train_time:75872ms step_avg:96.28ms
step:799/1770 train_time:75972ms step_avg:96.29ms
step:800/1770 train_time:76071ms step_avg:96.29ms
step:801/1770 train_time:76170ms step_avg:96.30ms
step:802/1770 train_time:76270ms step_avg:96.30ms
step:803/1770 train_time:76369ms step_avg:96.30ms
step:804/1770 train_time:76468ms step_avg:96.31ms
step:805/1770 train_time:76567ms step_avg:96.31ms
step:806/1770 train_time:76666ms step_avg:96.31ms
step:807/1770 train_time:76765ms step_avg:96.32ms
step:808/1770 train_time:76864ms step_avg:96.32ms
step:809/1770 train_time:76963ms step_avg:96.32ms
step:810/1770 train_time:77063ms step_avg:96.33ms
step:811/1770 train_time:77162ms step_avg:96.33ms
step:812/1770 train_time:77265ms step_avg:96.34ms
step:813/1770 train_time:77362ms step_avg:96.34ms
step:814/1770 train_time:77462ms step_avg:96.35ms
step:815/1770 train_time:77561ms step_avg:96.35ms
step:816/1770 train_time:77660ms step_avg:96.35ms
step:817/1770 train_time:77760ms step_avg:96.36ms
step:818/1770 train_time:77859ms step_avg:96.36ms
step:819/1770 train_time:77958ms step_avg:96.36ms
step:820/1770 train_time:78057ms step_avg:96.37ms
step:821/1770 train_time:78156ms step_avg:96.37ms
step:822/1770 train_time:78256ms step_avg:96.37ms
step:823/1770 train_time:78355ms step_avg:96.38ms
step:824/1770 train_time:78454ms step_avg:96.38ms
step:825/1770 train_time:78553ms step_avg:96.38ms
step:826/1770 train_time:78653ms step_avg:96.39ms
step:827/1770 train_time:78753ms step_avg:96.39ms
step:828/1770 train_time:78853ms step_avg:96.40ms
step:829/1770 train_time:78953ms step_avg:96.40ms
step:830/1770 train_time:79052ms step_avg:96.40ms
step:831/1770 train_time:79150ms step_avg:96.41ms
step:832/1770 train_time:79250ms step_avg:96.41ms
step:833/1770 train_time:79349ms step_avg:96.41ms
step:834/1770 train_time:79448ms step_avg:96.42ms
step:835/1770 train_time:79547ms step_avg:96.42ms
step:836/1770 train_time:79647ms step_avg:96.43ms
step:837/1770 train_time:79747ms step_avg:96.43ms
step:838/1770 train_time:79847ms step_avg:96.43ms
step:839/1770 train_time:79946ms step_avg:96.44ms
step:840/1770 train_time:80046ms step_avg:96.44ms
step:841/1770 train_time:80146ms step_avg:96.44ms
step:842/1770 train_time:80244ms step_avg:96.45ms
step:843/1770 train_time:80343ms step_avg:96.45ms
step:844/1770 train_time:80442ms step_avg:96.45ms
step:845/1770 train_time:80541ms step_avg:96.46ms
step:846/1770 train_time:80641ms step_avg:96.46ms
step:847/1770 train_time:80741ms step_avg:96.46ms
step:848/1770 train_time:80840ms step_avg:96.47ms
step:849/1770 train_time:80940ms step_avg:96.47ms
step:850/1770 train_time:81040ms step_avg:96.48ms
step:851/1770 train_time:81140ms step_avg:96.48ms
step:852/1770 train_time:81241ms step_avg:96.49ms
step:853/1770 train_time:81340ms step_avg:96.49ms
step:854/1770 train_time:81439ms step_avg:96.49ms
step:855/1770 train_time:81539ms step_avg:96.50ms
step:856/1770 train_time:81639ms step_avg:96.50ms
step:857/1770 train_time:81738ms step_avg:96.50ms
step:858/1770 train_time:81838ms step_avg:96.51ms
step:859/1770 train_time:81938ms step_avg:96.51ms
step:860/1770 train_time:82037ms step_avg:96.51ms
step:861/1770 train_time:82137ms step_avg:96.52ms
step:862/1770 train_time:82237ms step_avg:96.52ms
step:863/1770 train_time:82336ms step_avg:96.53ms
step:864/1770 train_time:82436ms step_avg:96.53ms
step:865/1770 train_time:82535ms step_avg:96.53ms
step:866/1770 train_time:82635ms step_avg:96.54ms
step:867/1770 train_time:82735ms step_avg:96.54ms
step:868/1770 train_time:82834ms step_avg:96.54ms
step:869/1770 train_time:82933ms step_avg:96.55ms
step:870/1770 train_time:83032ms step_avg:96.55ms
step:871/1770 train_time:83131ms step_avg:96.55ms
step:872/1770 train_time:83230ms step_avg:96.55ms
step:873/1770 train_time:83329ms step_avg:96.56ms
step:874/1770 train_time:83428ms step_avg:96.56ms
step:875/1770 train_time:83528ms step_avg:96.56ms
step:875/1770 val_loss:3.5476 train_time:83626ms step_avg:96.68ms
step:876/1770 train_time:83648ms step_avg:96.59ms
step:877/1770 train_time:83738ms step_avg:96.58ms
step:878/1770 train_time:83841ms step_avg:96.59ms
step:879/1770 train_time:83941ms step_avg:96.59ms
step:880/1770 train_time:84039ms step_avg:96.60ms
step:881/1770 train_time:84138ms step_avg:96.60ms
step:882/1770 train_time:84237ms step_avg:96.60ms
step:883/1770 train_time:84336ms step_avg:96.60ms
step:884/1770 train_time:84434ms step_avg:96.61ms
step:885/1770 train_time:84532ms step_avg:96.61ms
step:886/1770 train_time:84631ms step_avg:96.61ms
step:887/1770 train_time:84732ms step_avg:96.62ms
step:888/1770 train_time:84832ms step_avg:96.62ms
step:889/1770 train_time:84933ms step_avg:96.62ms
step:890/1770 train_time:85033ms step_avg:96.63ms
step:891/1770 train_time:85132ms step_avg:96.63ms
step:892/1770 train_time:85232ms step_avg:96.63ms
step:893/1770 train_time:85331ms step_avg:96.64ms
step:894/1770 train_time:85431ms step_avg:96.64ms
step:895/1770 train_time:85530ms step_avg:96.64ms
step:896/1770 train_time:85630ms step_avg:96.65ms
step:897/1770 train_time:85729ms step_avg:96.65ms
step:898/1770 train_time:85829ms step_avg:96.65ms
step:899/1770 train_time:85928ms step_avg:96.66ms
step:900/1770 train_time:86028ms step_avg:96.66ms
step:901/1770 train_time:86127ms step_avg:96.66ms
step:902/1770 train_time:86228ms step_avg:96.67ms
step:903/1770 train_time:86327ms step_avg:96.67ms
step:904/1770 train_time:86427ms step_avg:96.67ms
step:905/1770 train_time:86527ms step_avg:96.68ms
step:906/1770 train_time:86627ms step_avg:96.68ms
step:907/1770 train_time:86727ms step_avg:96.69ms
step:908/1770 train_time:86827ms step_avg:96.69ms
step:909/1770 train_time:86927ms step_avg:96.69ms
step:910/1770 train_time:87027ms step_avg:96.70ms
step:911/1770 train_time:87127ms step_avg:96.70ms
step:912/1770 train_time:87227ms step_avg:96.70ms
step:913/1770 train_time:87326ms step_avg:96.71ms
step:914/1770 train_time:87426ms step_avg:96.71ms
step:915/1770 train_time:87525ms step_avg:96.71ms
step:916/1770 train_time:87625ms step_avg:96.72ms
step:917/1770 train_time:87725ms step_avg:96.72ms
step:918/1770 train_time:87825ms step_avg:96.72ms
step:919/1770 train_time:87924ms step_avg:96.73ms
step:920/1770 train_time:88027ms step_avg:96.73ms
step:921/1770 train_time:88128ms step_avg:96.74ms
step:922/1770 train_time:88229ms step_avg:96.74ms
step:923/1770 train_time:88330ms step_avg:96.75ms
step:924/1770 train_time:88430ms step_avg:96.75ms
step:925/1770 train_time:88530ms step_avg:96.75ms
step:926/1770 train_time:88630ms step_avg:96.76ms
step:927/1770 train_time:88731ms step_avg:96.76ms
step:928/1770 train_time:88832ms step_avg:96.77ms
step:929/1770 train_time:88933ms step_avg:96.77ms
step:930/1770 train_time:89034ms step_avg:96.78ms
step:931/1770 train_time:89135ms step_avg:96.78ms
step:932/1770 train_time:89236ms step_avg:96.79ms
step:933/1770 train_time:89336ms step_avg:96.79ms
step:934/1770 train_time:89437ms step_avg:96.79ms
step:935/1770 train_time:89537ms step_avg:96.80ms
step:936/1770 train_time:89637ms step_avg:96.80ms
step:937/1770 train_time:89738ms step_avg:96.80ms
step:938/1770 train_time:89839ms step_avg:96.81ms
step:939/1770 train_time:89941ms step_avg:96.81ms
step:940/1770 train_time:90041ms step_avg:96.82ms
step:941/1770 train_time:90142ms step_avg:96.82ms
step:942/1770 train_time:90244ms step_avg:96.83ms
step:943/1770 train_time:90344ms step_avg:96.83ms
step:944/1770 train_time:90445ms step_avg:96.84ms
step:945/1770 train_time:90547ms step_avg:96.84ms
step:946/1770 train_time:90648ms step_avg:96.85ms
step:947/1770 train_time:90749ms step_avg:96.85ms
step:948/1770 train_time:90850ms step_avg:96.86ms
step:949/1770 train_time:90951ms step_avg:96.86ms
step:950/1770 train_time:91053ms step_avg:96.86ms
step:951/1770 train_time:91153ms step_avg:96.87ms
step:952/1770 train_time:91254ms step_avg:96.87ms
step:953/1770 train_time:91354ms step_avg:96.88ms
step:954/1770 train_time:91455ms step_avg:96.88ms
step:955/1770 train_time:91556ms step_avg:96.88ms
step:956/1770 train_time:91657ms step_avg:96.89ms
step:957/1770 train_time:91758ms step_avg:96.89ms
step:958/1770 train_time:91859ms step_avg:96.90ms
step:959/1770 train_time:91959ms step_avg:96.90ms
step:960/1770 train_time:92060ms step_avg:96.90ms
step:961/1770 train_time:92161ms step_avg:96.91ms
step:962/1770 train_time:92262ms step_avg:96.91ms
step:963/1770 train_time:92363ms step_avg:96.92ms
step:964/1770 train_time:92463ms step_avg:96.92ms
step:965/1770 train_time:92564ms step_avg:96.93ms
step:966/1770 train_time:92665ms step_avg:96.93ms
step:967/1770 train_time:92765ms step_avg:96.93ms
step:968/1770 train_time:92866ms step_avg:96.94ms
step:969/1770 train_time:92968ms step_avg:96.94ms
step:970/1770 train_time:93069ms step_avg:96.95ms
step:971/1770 train_time:93170ms step_avg:96.95ms
step:972/1770 train_time:93270ms step_avg:96.95ms
step:973/1770 train_time:93371ms step_avg:96.96ms
step:974/1770 train_time:93471ms step_avg:96.96ms
step:975/1770 train_time:93572ms step_avg:96.97ms
step:976/1770 train_time:93674ms step_avg:96.97ms
step:977/1770 train_time:93775ms step_avg:96.98ms
step:978/1770 train_time:93876ms step_avg:96.98ms
step:979/1770 train_time:93977ms step_avg:96.98ms
step:980/1770 train_time:94078ms step_avg:96.99ms
step:981/1770 train_time:94179ms step_avg:96.99ms
step:982/1770 train_time:94280ms step_avg:97.00ms
step:983/1770 train_time:94381ms step_avg:97.00ms
step:984/1770 train_time:94481ms step_avg:97.00ms
step:985/1770 train_time:94583ms step_avg:97.01ms
step:986/1770 train_time:94684ms step_avg:97.01ms
step:987/1770 train_time:94785ms step_avg:97.02ms
step:988/1770 train_time:94887ms step_avg:97.02ms
step:989/1770 train_time:94989ms step_avg:97.03ms
step:990/1770 train_time:95089ms step_avg:97.03ms
step:991/1770 train_time:95190ms step_avg:97.03ms
step:992/1770 train_time:95291ms step_avg:97.04ms
step:993/1770 train_time:95392ms step_avg:97.04ms
step:994/1770 train_time:95493ms step_avg:97.05ms
step:995/1770 train_time:95594ms step_avg:97.05ms
step:996/1770 train_time:95694ms step_avg:97.05ms
step:997/1770 train_time:95794ms step_avg:97.06ms
step:998/1770 train_time:95895ms step_avg:97.06ms
step:999/1770 train_time:95996ms step_avg:97.06ms
step:1000/1770 train_time:96097ms step_avg:97.07ms
step:1000/1770 val_loss:3.5099 train_time:96195ms step_avg:97.17ms
step:1001/1770 train_time:96217ms step_avg:97.09ms
step:1002/1770 train_time:96309ms step_avg:97.09ms
step:1003/1770 train_time:96412ms step_avg:97.09ms
step:1004/1770 train_time:96514ms step_avg:97.10ms
step:1005/1770 train_time:96614ms step_avg:97.10ms
step:1006/1770 train_time:96714ms step_avg:97.10ms
step:1007/1770 train_time:96813ms step_avg:97.10ms
step:1008/1770 train_time:96913ms step_avg:97.11ms
step:1009/1770 train_time:97013ms step_avg:97.11ms
step:1010/1770 train_time:97113ms step_avg:97.11ms
step:1011/1770 train_time:97216ms step_avg:97.12ms
step:1012/1770 train_time:97319ms step_avg:97.12ms
step:1013/1770 train_time:97421ms step_avg:97.13ms
step:1014/1770 train_time:97522ms step_avg:97.13ms
step:1015/1770 train_time:97622ms step_avg:97.14ms
step:1016/1770 train_time:97722ms step_avg:97.14ms
step:1017/1770 train_time:97823ms step_avg:97.14ms
step:1018/1770 train_time:97923ms step_avg:97.15ms
step:1019/1770 train_time:98024ms step_avg:97.15ms
step:1020/1770 train_time:98125ms step_avg:97.15ms
step:1021/1770 train_time:98227ms step_avg:97.16ms
step:1022/1770 train_time:98329ms step_avg:97.16ms
step:1023/1770 train_time:98429ms step_avg:97.17ms
step:1024/1770 train_time:98530ms step_avg:97.17ms
step:1025/1770 train_time:98631ms step_avg:97.17ms
step:1026/1770 train_time:98732ms step_avg:97.18ms
step:1027/1770 train_time:98834ms step_avg:97.18ms
step:1028/1770 train_time:98935ms step_avg:97.19ms
step:1029/1770 train_time:99036ms step_avg:97.19ms
step:1030/1770 train_time:99137ms step_avg:97.19ms
step:1031/1770 train_time:99239ms step_avg:97.20ms
step:1032/1770 train_time:99339ms step_avg:97.20ms
step:1033/1770 train_time:99440ms step_avg:97.20ms
step:1034/1770 train_time:99541ms step_avg:97.21ms
step:1035/1770 train_time:99642ms step_avg:97.21ms
step:1036/1770 train_time:99744ms step_avg:97.22ms
step:1037/1770 train_time:99845ms step_avg:97.22ms
step:1038/1770 train_time:99946ms step_avg:97.22ms
step:1039/1770 train_time:100048ms step_avg:97.23ms
step:1040/1770 train_time:100149ms step_avg:97.23ms
step:1041/1770 train_time:100249ms step_avg:97.23ms
step:1042/1770 train_time:100350ms step_avg:97.24ms
step:1043/1770 train_time:100451ms step_avg:97.24ms
step:1044/1770 train_time:100553ms step_avg:97.25ms
step:1045/1770 train_time:100654ms step_avg:97.25ms
step:1046/1770 train_time:100755ms step_avg:97.25ms
step:1047/1770 train_time:100856ms step_avg:97.26ms
step:1048/1770 train_time:100957ms step_avg:97.26ms
step:1049/1770 train_time:101058ms step_avg:97.26ms
step:1050/1770 train_time:101159ms step_avg:97.27ms
step:1051/1770 train_time:101260ms step_avg:97.27ms
step:1052/1770 train_time:101360ms step_avg:97.27ms
step:1053/1770 train_time:101461ms step_avg:97.28ms
step:1054/1770 train_time:101563ms step_avg:97.28ms
step:1055/1770 train_time:101665ms step_avg:97.29ms
step:1056/1770 train_time:101766ms step_avg:97.29ms
step:1057/1770 train_time:101867ms step_avg:97.29ms
step:1058/1770 train_time:101969ms step_avg:97.30ms
step:1059/1770 train_time:102070ms step_avg:97.30ms
step:1060/1770 train_time:102171ms step_avg:97.31ms
step:1061/1770 train_time:102272ms step_avg:97.31ms
step:1062/1770 train_time:102373ms step_avg:97.31ms
step:1063/1770 train_time:102476ms step_avg:97.32ms
step:1064/1770 train_time:102577ms step_avg:97.32ms
step:1065/1770 train_time:102678ms step_avg:97.32ms
step:1066/1770 train_time:102778ms step_avg:97.33ms
step:1067/1770 train_time:102879ms step_avg:97.33ms
step:1068/1770 train_time:102982ms step_avg:97.34ms
step:1069/1770 train_time:103083ms step_avg:97.34ms
step:1070/1770 train_time:103184ms step_avg:97.34ms
step:1071/1770 train_time:103285ms step_avg:97.35ms
step:1072/1770 train_time:103387ms step_avg:97.35ms
step:1073/1770 train_time:103487ms step_avg:97.35ms
step:1074/1770 train_time:103588ms step_avg:97.36ms
step:1075/1770 train_time:103690ms step_avg:97.36ms
step:1076/1770 train_time:103791ms step_avg:97.37ms
step:1077/1770 train_time:103892ms step_avg:97.37ms
step:1078/1770 train_time:103993ms step_avg:97.37ms
step:1079/1770 train_time:104094ms step_avg:97.38ms
step:1080/1770 train_time:104194ms step_avg:97.38ms
step:1081/1770 train_time:104295ms step_avg:97.38ms
step:1082/1770 train_time:104396ms step_avg:97.38ms
step:1083/1770 train_time:104496ms step_avg:97.39ms
step:1084/1770 train_time:104598ms step_avg:97.39ms
step:1085/1770 train_time:104699ms step_avg:97.39ms
step:1086/1770 train_time:104800ms step_avg:97.40ms
step:1087/1770 train_time:104900ms step_avg:97.40ms
step:1088/1770 train_time:105002ms step_avg:97.40ms
step:1089/1770 train_time:105103ms step_avg:97.41ms
step:1090/1770 train_time:105204ms step_avg:97.41ms
step:1091/1770 train_time:105306ms step_avg:97.42ms
step:1092/1770 train_time:105407ms step_avg:97.42ms
step:1093/1770 train_time:105508ms step_avg:97.42ms
step:1094/1770 train_time:105609ms step_avg:97.43ms
step:1095/1770 train_time:105710ms step_avg:97.43ms
step:1096/1770 train_time:105811ms step_avg:97.43ms
step:1097/1770 train_time:105911ms step_avg:97.43ms
step:1098/1770 train_time:106013ms step_avg:97.44ms
step:1099/1770 train_time:106114ms step_avg:97.44ms
step:1100/1770 train_time:106214ms step_avg:97.44ms
step:1101/1770 train_time:106314ms step_avg:97.45ms
step:1102/1770 train_time:106416ms step_avg:97.45ms
step:1103/1770 train_time:106517ms step_avg:97.45ms
step:1104/1770 train_time:106620ms step_avg:97.46ms
step:1105/1770 train_time:106720ms step_avg:97.46ms
step:1106/1770 train_time:106821ms step_avg:97.46ms
step:1107/1770 train_time:106922ms step_avg:97.47ms
step:1108/1770 train_time:107023ms step_avg:97.47ms
step:1109/1770 train_time:107125ms step_avg:97.48ms
step:1110/1770 train_time:107227ms step_avg:97.48ms
step:1111/1770 train_time:107328ms step_avg:97.48ms
step:1112/1770 train_time:107429ms step_avg:97.49ms
step:1113/1770 train_time:107529ms step_avg:97.49ms
step:1114/1770 train_time:107631ms step_avg:97.49ms
step:1115/1770 train_time:107732ms step_avg:97.49ms
step:1116/1770 train_time:107833ms step_avg:97.50ms
step:1117/1770 train_time:107935ms step_avg:97.50ms
step:1118/1770 train_time:108036ms step_avg:97.51ms
step:1119/1770 train_time:108138ms step_avg:97.51ms
step:1120/1770 train_time:108239ms step_avg:97.51ms
step:1121/1770 train_time:108340ms step_avg:97.52ms
step:1122/1770 train_time:108442ms step_avg:97.52ms
step:1123/1770 train_time:108543ms step_avg:97.52ms
step:1124/1770 train_time:108644ms step_avg:97.53ms
step:1125/1770 train_time:108745ms step_avg:97.53ms
step:1125/1770 val_loss:3.4708 train_time:108845ms step_avg:97.62ms
step:1126/1770 train_time:108870ms step_avg:97.55ms
step:1127/1770 train_time:108962ms step_avg:97.55ms
step:1128/1770 train_time:109063ms step_avg:97.55ms
step:1129/1770 train_time:109164ms step_avg:97.55ms
step:1130/1770 train_time:109265ms step_avg:97.56ms
step:1131/1770 train_time:109365ms step_avg:97.56ms
step:1132/1770 train_time:109466ms step_avg:97.56ms
step:1133/1770 train_time:109566ms step_avg:97.57ms
step:1134/1770 train_time:109666ms step_avg:97.57ms
step:1135/1770 train_time:109767ms step_avg:97.57ms
step:1136/1770 train_time:109870ms step_avg:97.58ms
step:1137/1770 train_time:109973ms step_avg:97.58ms
step:1138/1770 train_time:110075ms step_avg:97.58ms
step:1139/1770 train_time:110175ms step_avg:97.59ms
step:1140/1770 train_time:110276ms step_avg:97.59ms
step:1141/1770 train_time:110376ms step_avg:97.59ms
step:1142/1770 train_time:110477ms step_avg:97.59ms
step:1143/1770 train_time:110579ms step_avg:97.60ms
step:1144/1770 train_time:110680ms step_avg:97.60ms
step:1145/1770 train_time:110782ms step_avg:97.61ms
step:1146/1770 train_time:110883ms step_avg:97.61ms
step:1147/1770 train_time:110986ms step_avg:97.61ms
step:1148/1770 train_time:111086ms step_avg:97.61ms
step:1149/1770 train_time:111186ms step_avg:97.62ms
step:1150/1770 train_time:111287ms step_avg:97.62ms
step:1151/1770 train_time:111388ms step_avg:97.62ms
step:1152/1770 train_time:111490ms step_avg:97.63ms
step:1153/1770 train_time:111590ms step_avg:97.63ms
step:1154/1770 train_time:111693ms step_avg:97.63ms
step:1155/1770 train_time:111794ms step_avg:97.64ms
step:1156/1770 train_time:111896ms step_avg:97.64ms
step:1157/1770 train_time:111998ms step_avg:97.64ms
step:1158/1770 train_time:112100ms step_avg:97.65ms
step:1159/1770 train_time:112201ms step_avg:97.65ms
step:1160/1770 train_time:112302ms step_avg:97.65ms
step:1161/1770 train_time:112402ms step_avg:97.66ms
step:1162/1770 train_time:112504ms step_avg:97.66ms
step:1163/1770 train_time:112605ms step_avg:97.66ms
step:1164/1770 train_time:112707ms step_avg:97.67ms
step:1165/1770 train_time:112808ms step_avg:97.67ms
step:1166/1770 train_time:112909ms step_avg:97.67ms
step:1167/1770 train_time:113010ms step_avg:97.68ms
step:1168/1770 train_time:113112ms step_avg:97.68ms
step:1169/1770 train_time:113212ms step_avg:97.68ms
step:1170/1770 train_time:113313ms step_avg:97.68ms
step:1171/1770 train_time:113415ms step_avg:97.69ms
step:1172/1770 train_time:113515ms step_avg:97.69ms
step:1173/1770 train_time:113617ms step_avg:97.69ms
step:1174/1770 train_time:113718ms step_avg:97.70ms
step:1175/1770 train_time:113820ms step_avg:97.70ms
step:1176/1770 train_time:113921ms step_avg:97.70ms
step:1177/1770 train_time:114023ms step_avg:97.71ms
step:1178/1770 train_time:114124ms step_avg:97.71ms
step:1179/1770 train_time:114225ms step_avg:97.71ms
step:1180/1770 train_time:114327ms step_avg:97.72ms
step:1181/1770 train_time:114428ms step_avg:97.72ms
step:1182/1770 train_time:114530ms step_avg:97.72ms
step:1183/1770 train_time:114632ms step_avg:97.73ms
step:1184/1770 train_time:114735ms step_avg:97.73ms
step:1185/1770 train_time:114836ms step_avg:97.73ms
step:1186/1770 train_time:114939ms step_avg:97.74ms
step:1187/1770 train_time:115043ms step_avg:97.74ms
step:1188/1770 train_time:115145ms step_avg:97.75ms
step:1189/1770 train_time:115246ms step_avg:97.75ms
step:1190/1770 train_time:115347ms step_avg:97.75ms
step:1191/1770 train_time:115449ms step_avg:97.76ms
step:1192/1770 train_time:115551ms step_avg:97.76ms
step:1193/1770 train_time:115653ms step_avg:97.76ms
step:1194/1770 train_time:115755ms step_avg:97.77ms
step:1195/1770 train_time:115857ms step_avg:97.77ms
step:1196/1770 train_time:115961ms step_avg:97.77ms
step:1197/1770 train_time:116062ms step_avg:97.78ms
step:1198/1770 train_time:116166ms step_avg:97.78ms
step:1199/1770 train_time:116267ms step_avg:97.79ms
step:1200/1770 train_time:116370ms step_avg:97.79ms
step:1201/1770 train_time:116473ms step_avg:97.79ms
step:1202/1770 train_time:116574ms step_avg:97.80ms
step:1203/1770 train_time:116676ms step_avg:97.80ms
step:1204/1770 train_time:116778ms step_avg:97.80ms
step:1205/1770 train_time:116880ms step_avg:97.81ms
step:1206/1770 train_time:116982ms step_avg:97.81ms
step:1207/1770 train_time:117084ms step_avg:97.81ms
step:1208/1770 train_time:117185ms step_avg:97.82ms
step:1209/1770 train_time:117287ms step_avg:97.82ms
step:1210/1770 train_time:117390ms step_avg:97.83ms
step:1211/1770 train_time:117493ms step_avg:97.83ms
step:1212/1770 train_time:117596ms step_avg:97.83ms
step:1213/1770 train_time:117697ms step_avg:97.84ms
step:1214/1770 train_time:117798ms step_avg:97.84ms
step:1215/1770 train_time:117901ms step_avg:97.84ms
step:1216/1770 train_time:118005ms step_avg:97.85ms
step:1217/1770 train_time:118107ms step_avg:97.85ms
step:1218/1770 train_time:118209ms step_avg:97.85ms
step:1219/1770 train_time:118310ms step_avg:97.86ms
step:1220/1770 train_time:118413ms step_avg:97.86ms
step:1221/1770 train_time:118514ms step_avg:97.86ms
step:1222/1770 train_time:118618ms step_avg:97.87ms
step:1223/1770 train_time:118720ms step_avg:97.87ms
step:1224/1770 train_time:118823ms step_avg:97.88ms
step:1225/1770 train_time:118926ms step_avg:97.88ms
step:1226/1770 train_time:119028ms step_avg:97.89ms
step:1227/1770 train_time:119132ms step_avg:97.89ms
step:1228/1770 train_time:119236ms step_avg:97.89ms
step:1229/1770 train_time:119337ms step_avg:97.90ms
step:1230/1770 train_time:119439ms step_avg:97.90ms
step:1231/1770 train_time:119542ms step_avg:97.91ms
step:1232/1770 train_time:119645ms step_avg:97.91ms
step:1233/1770 train_time:119747ms step_avg:97.91ms
step:1234/1770 train_time:119849ms step_avg:97.92ms
step:1235/1770 train_time:119950ms step_avg:97.92ms
step:1236/1770 train_time:120053ms step_avg:97.92ms
step:1237/1770 train_time:120154ms step_avg:97.93ms
step:1238/1770 train_time:120257ms step_avg:97.93ms
step:1239/1770 train_time:120359ms step_avg:97.93ms
step:1240/1770 train_time:120461ms step_avg:97.94ms
step:1241/1770 train_time:120564ms step_avg:97.94ms
step:1242/1770 train_time:120667ms step_avg:97.94ms
step:1243/1770 train_time:120769ms step_avg:97.95ms
step:1244/1770 train_time:120871ms step_avg:97.95ms
step:1245/1770 train_time:120973ms step_avg:97.95ms
step:1246/1770 train_time:121075ms step_avg:97.96ms
step:1247/1770 train_time:121176ms step_avg:97.96ms
step:1248/1770 train_time:121279ms step_avg:97.96ms
step:1249/1770 train_time:121381ms step_avg:97.97ms
step:1250/1770 train_time:121483ms step_avg:97.97ms
step:1250/1770 val_loss:3.4213 train_time:121585ms step_avg:98.05ms
step:1251/1770 train_time:121606ms step_avg:97.99ms
step:1252/1770 train_time:121698ms step_avg:97.99ms
step:1253/1770 train_time:121801ms step_avg:97.99ms
step:1254/1770 train_time:121903ms step_avg:97.99ms
step:1255/1770 train_time:122007ms step_avg:98.00ms
step:1256/1770 train_time:122108ms step_avg:98.00ms
step:1257/1770 train_time:122210ms step_avg:98.00ms
step:1258/1770 train_time:122312ms step_avg:98.01ms
step:1259/1770 train_time:122414ms step_avg:98.01ms
step:1260/1770 train_time:122516ms step_avg:98.01ms
step:1261/1770 train_time:122620ms step_avg:98.02ms
step:1262/1770 train_time:122723ms step_avg:98.02ms
step:1263/1770 train_time:122825ms step_avg:98.02ms
step:1264/1770 train_time:122928ms step_avg:98.03ms
step:1265/1770 train_time:123030ms step_avg:98.03ms
step:1266/1770 train_time:123132ms step_avg:98.03ms
step:1267/1770 train_time:123234ms step_avg:98.04ms
step:1268/1770 train_time:123336ms step_avg:98.04ms
step:1269/1770 train_time:123437ms step_avg:98.04ms
step:1270/1770 train_time:123540ms step_avg:98.05ms
step:1271/1770 train_time:123642ms step_avg:98.05ms
step:1272/1770 train_time:123744ms step_avg:98.05ms
step:1273/1770 train_time:123847ms step_avg:98.06ms
step:1274/1770 train_time:123950ms step_avg:98.06ms
step:1275/1770 train_time:124051ms step_avg:98.06ms
step:1276/1770 train_time:124153ms step_avg:98.07ms
step:1277/1770 train_time:124256ms step_avg:98.07ms
step:1278/1770 train_time:124359ms step_avg:98.07ms
step:1279/1770 train_time:124461ms step_avg:98.08ms
step:1280/1770 train_time:124564ms step_avg:98.08ms
step:1281/1770 train_time:124665ms step_avg:98.08ms
step:1282/1770 train_time:124768ms step_avg:98.09ms
step:1283/1770 train_time:124870ms step_avg:98.09ms
step:1284/1770 train_time:124973ms step_avg:98.09ms
step:1285/1770 train_time:125075ms step_avg:98.10ms
step:1286/1770 train_time:125178ms step_avg:98.10ms
step:1287/1770 train_time:125282ms step_avg:98.11ms
step:1288/1770 train_time:125385ms step_avg:98.11ms
step:1289/1770 train_time:125488ms step_avg:98.11ms
step:1290/1770 train_time:125590ms step_avg:98.12ms
step:1291/1770 train_time:125691ms step_avg:98.12ms
step:1292/1770 train_time:125794ms step_avg:98.12ms
step:1293/1770 train_time:125896ms step_avg:98.13ms
step:1294/1770 train_time:125998ms step_avg:98.13ms
step:1295/1770 train_time:126100ms step_avg:98.13ms
step:1296/1770 train_time:126203ms step_avg:98.14ms
step:1297/1770 train_time:126306ms step_avg:98.14ms
step:1298/1770 train_time:126410ms step_avg:98.14ms
step:1299/1770 train_time:126511ms step_avg:98.15ms
step:1300/1770 train_time:126613ms step_avg:98.15ms
step:1301/1770 train_time:126715ms step_avg:98.15ms
step:1302/1770 train_time:126817ms step_avg:98.16ms
step:1303/1770 train_time:126919ms step_avg:98.16ms
step:1304/1770 train_time:127020ms step_avg:98.16ms
step:1305/1770 train_time:127123ms step_avg:98.16ms
step:1306/1770 train_time:127225ms step_avg:98.17ms
step:1307/1770 train_time:127328ms step_avg:98.17ms
step:1308/1770 train_time:127430ms step_avg:98.17ms
step:1309/1770 train_time:127533ms step_avg:98.18ms
step:1310/1770 train_time:127634ms step_avg:98.18ms
step:1311/1770 train_time:127736ms step_avg:98.18ms
step:1312/1770 train_time:127837ms step_avg:98.18ms
step:1313/1770 train_time:127938ms step_avg:98.19ms
step:1314/1770 train_time:128039ms step_avg:98.19ms
step:1315/1770 train_time:128141ms step_avg:98.19ms
step:1316/1770 train_time:128243ms step_avg:98.20ms
step:1317/1770 train_time:128347ms step_avg:98.20ms
step:1318/1770 train_time:128453ms step_avg:98.21ms
step:1319/1770 train_time:128556ms step_avg:98.21ms
step:1320/1770 train_time:128658ms step_avg:98.21ms
step:1321/1770 train_time:128760ms step_avg:98.21ms
step:1322/1770 train_time:128861ms step_avg:98.22ms
step:1323/1770 train_time:128964ms step_avg:98.22ms
step:1324/1770 train_time:129068ms step_avg:98.23ms
step:1325/1770 train_time:129172ms step_avg:98.23ms
step:1326/1770 train_time:129274ms step_avg:98.23ms
step:1327/1770 train_time:129378ms step_avg:98.24ms
step:1328/1770 train_time:129481ms step_avg:98.24ms
step:1329/1770 train_time:129583ms step_avg:98.24ms
step:1330/1770 train_time:129685ms step_avg:98.25ms
step:1331/1770 train_time:129787ms step_avg:98.25ms
step:1332/1770 train_time:129889ms step_avg:98.25ms
step:1333/1770 train_time:129991ms step_avg:98.25ms
step:1334/1770 train_time:130092ms step_avg:98.26ms
step:1335/1770 train_time:130194ms step_avg:98.26ms
step:1336/1770 train_time:130297ms step_avg:98.26ms
step:1337/1770 train_time:130399ms step_avg:98.27ms
step:1338/1770 train_time:130501ms step_avg:98.27ms
step:1339/1770 train_time:130604ms step_avg:98.27ms
step:1340/1770 train_time:130707ms step_avg:98.28ms
step:1341/1770 train_time:130808ms step_avg:98.28ms
step:1342/1770 train_time:130911ms step_avg:98.28ms
step:1343/1770 train_time:131013ms step_avg:98.28ms
step:1344/1770 train_time:131115ms step_avg:98.29ms
step:1345/1770 train_time:131217ms step_avg:98.29ms
step:1346/1770 train_time:131319ms step_avg:98.29ms
step:1347/1770 train_time:131421ms step_avg:98.30ms
step:1348/1770 train_time:131526ms step_avg:98.30ms
step:1349/1770 train_time:131628ms step_avg:98.30ms
step:1350/1770 train_time:131731ms step_avg:98.31ms
step:1351/1770 train_time:131833ms step_avg:98.31ms
step:1352/1770 train_time:131934ms step_avg:98.31ms
step:1353/1770 train_time:132038ms step_avg:98.32ms
step:1354/1770 train_time:132139ms step_avg:98.32ms
step:1355/1770 train_time:132241ms step_avg:98.32ms
step:1356/1770 train_time:132343ms step_avg:98.32ms
step:1357/1770 train_time:132445ms step_avg:98.33ms
step:1358/1770 train_time:132548ms step_avg:98.33ms
step:1359/1770 train_time:132650ms step_avg:98.33ms
step:1360/1770 train_time:132753ms step_avg:98.34ms
step:1361/1770 train_time:132857ms step_avg:98.34ms
step:1362/1770 train_time:132959ms step_avg:98.34ms
step:1363/1770 train_time:133062ms step_avg:98.35ms
step:1364/1770 train_time:133165ms step_avg:98.35ms
step:1365/1770 train_time:133266ms step_avg:98.35ms
step:1366/1770 train_time:133368ms step_avg:98.35ms
step:1367/1770 train_time:133471ms step_avg:98.36ms
step:1368/1770 train_time:133572ms step_avg:98.36ms
step:1369/1770 train_time:133674ms step_avg:98.36ms
step:1370/1770 train_time:133777ms step_avg:98.37ms
step:1371/1770 train_time:133879ms step_avg:98.37ms
step:1372/1770 train_time:133981ms step_avg:98.37ms
step:1373/1770 train_time:134083ms step_avg:98.37ms
step:1374/1770 train_time:134186ms step_avg:98.38ms
step:1375/1770 train_time:134289ms step_avg:98.38ms
step:1375/1770 val_loss:3.3786 train_time:134390ms step_avg:98.45ms
step:1376/1770 train_time:134412ms step_avg:98.40ms
step:1377/1770 train_time:134504ms step_avg:98.39ms
step:1378/1770 train_time:134606ms step_avg:98.40ms
step:1379/1770 train_time:134710ms step_avg:98.40ms
step:1380/1770 train_time:134811ms step_avg:98.40ms
step:1381/1770 train_time:134913ms step_avg:98.40ms
step:1382/1770 train_time:135014ms step_avg:98.41ms
step:1383/1770 train_time:135117ms step_avg:98.41ms
step:1384/1770 train_time:135219ms step_avg:98.41ms
step:1385/1770 train_time:135321ms step_avg:98.42ms
step:1386/1770 train_time:135424ms step_avg:98.42ms
step:1387/1770 train_time:135528ms step_avg:98.42ms
step:1388/1770 train_time:135631ms step_avg:98.43ms
step:1389/1770 train_time:135734ms step_avg:98.43ms
step:1390/1770 train_time:135836ms step_avg:98.43ms
step:1391/1770 train_time:135938ms step_avg:98.43ms
step:1392/1770 train_time:136040ms step_avg:98.44ms
step:1393/1770 train_time:136141ms step_avg:98.44ms
step:1394/1770 train_time:136242ms step_avg:98.44ms
step:1395/1770 train_time:136346ms step_avg:98.44ms
step:1396/1770 train_time:136449ms step_avg:98.45ms
step:1397/1770 train_time:136551ms step_avg:98.45ms
step:1398/1770 train_time:136653ms step_avg:98.45ms
step:1399/1770 train_time:136756ms step_avg:98.46ms
step:1400/1770 train_time:136859ms step_avg:98.46ms
step:1401/1770 train_time:136961ms step_avg:98.46ms
step:1402/1770 train_time:137064ms step_avg:98.47ms
step:1403/1770 train_time:137165ms step_avg:98.47ms
step:1404/1770 train_time:137268ms step_avg:98.47ms
step:1405/1770 train_time:137370ms step_avg:98.47ms
step:1406/1770 train_time:137472ms step_avg:98.48ms
step:1407/1770 train_time:137574ms step_avg:98.48ms
step:1408/1770 train_time:137676ms step_avg:98.48ms
step:1409/1770 train_time:137779ms step_avg:98.48ms
step:1410/1770 train_time:137881ms step_avg:98.49ms
step:1411/1770 train_time:137983ms step_avg:98.49ms
step:1412/1770 train_time:138085ms step_avg:98.49ms
step:1413/1770 train_time:138187ms step_avg:98.49ms
step:1414/1770 train_time:138290ms step_avg:98.50ms
step:1415/1770 train_time:138393ms step_avg:98.50ms
step:1416/1770 train_time:138497ms step_avg:98.50ms
step:1417/1770 train_time:138599ms step_avg:98.51ms
step:1418/1770 train_time:138700ms step_avg:98.51ms
step:1419/1770 train_time:138803ms step_avg:98.51ms
step:1420/1770 train_time:138904ms step_avg:98.51ms
step:1421/1770 train_time:139006ms step_avg:98.52ms
step:1422/1770 train_time:139109ms step_avg:98.52ms
step:1423/1770 train_time:139211ms step_avg:98.52ms
step:1424/1770 train_time:139314ms step_avg:98.52ms
step:1425/1770 train_time:139416ms step_avg:98.53ms
step:1426/1770 train_time:139518ms step_avg:98.53ms
step:1427/1770 train_time:139620ms step_avg:98.53ms
step:1428/1770 train_time:139724ms step_avg:98.54ms
step:1429/1770 train_time:139827ms step_avg:98.54ms
step:1430/1770 train_time:139928ms step_avg:98.54ms
step:1431/1770 train_time:140031ms step_avg:98.54ms
step:1432/1770 train_time:140132ms step_avg:98.55ms
step:1433/1770 train_time:140234ms step_avg:98.55ms
step:1434/1770 train_time:140335ms step_avg:98.55ms
step:1435/1770 train_time:140436ms step_avg:98.55ms
step:1436/1770 train_time:140540ms step_avg:98.56ms
step:1437/1770 train_time:140643ms step_avg:98.56ms
step:1438/1770 train_time:140745ms step_avg:98.56ms
step:1439/1770 train_time:140847ms step_avg:98.56ms
step:1440/1770 train_time:140949ms step_avg:98.57ms
step:1441/1770 train_time:141054ms step_avg:98.57ms
step:1442/1770 train_time:141156ms step_avg:98.57ms
step:1443/1770 train_time:141258ms step_avg:98.57ms
step:1444/1770 train_time:141360ms step_avg:98.58ms
step:1445/1770 train_time:141463ms step_avg:98.58ms
step:1446/1770 train_time:141565ms step_avg:98.58ms
step:1447/1770 train_time:141669ms step_avg:98.59ms
step:1448/1770 train_time:141773ms step_avg:98.59ms
step:1449/1770 train_time:141877ms step_avg:98.59ms
step:1450/1770 train_time:141979ms step_avg:98.60ms
step:1451/1770 train_time:142082ms step_avg:98.60ms
step:1452/1770 train_time:142186ms step_avg:98.60ms
step:1453/1770 train_time:142289ms step_avg:98.61ms
step:1454/1770 train_time:142393ms step_avg:98.61ms
step:1455/1770 train_time:142498ms step_avg:98.61ms
step:1456/1770 train_time:142602ms step_avg:98.62ms
step:1457/1770 train_time:142706ms step_avg:98.62ms
step:1458/1770 train_time:142811ms step_avg:98.63ms
step:1459/1770 train_time:142916ms step_avg:98.63ms
step:1460/1770 train_time:143018ms step_avg:98.63ms
step:1461/1770 train_time:143122ms step_avg:98.64ms
step:1462/1770 train_time:143225ms step_avg:98.64ms
step:1463/1770 train_time:143329ms step_avg:98.64ms
step:1464/1770 train_time:143434ms step_avg:98.65ms
step:1465/1770 train_time:143537ms step_avg:98.65ms
step:1466/1770 train_time:143641ms step_avg:98.65ms
step:1467/1770 train_time:143746ms step_avg:98.66ms
step:1468/1770 train_time:143849ms step_avg:98.66ms
step:1469/1770 train_time:143952ms step_avg:98.67ms
step:1470/1770 train_time:144055ms step_avg:98.67ms
step:1471/1770 train_time:144159ms step_avg:98.67ms
step:1472/1770 train_time:144263ms step_avg:98.68ms
step:1473/1770 train_time:144367ms step_avg:98.68ms
step:1474/1770 train_time:144471ms step_avg:98.68ms
step:1475/1770 train_time:144575ms step_avg:98.69ms
step:1476/1770 train_time:144678ms step_avg:98.69ms
step:1477/1770 train_time:144784ms step_avg:98.69ms
step:1478/1770 train_time:144887ms step_avg:98.70ms
step:1479/1770 train_time:144991ms step_avg:98.70ms
step:1480/1770 train_time:145094ms step_avg:98.70ms
step:1481/1770 train_time:145201ms step_avg:98.71ms
step:1482/1770 train_time:145303ms step_avg:98.71ms
step:1483/1770 train_time:145407ms step_avg:98.72ms
step:1484/1770 train_time:145511ms step_avg:98.72ms
step:1485/1770 train_time:145614ms step_avg:98.72ms
step:1486/1770 train_time:145717ms step_avg:98.72ms
step:1487/1770 train_time:145820ms step_avg:98.73ms
step:1488/1770 train_time:145923ms step_avg:98.73ms
step:1489/1770 train_time:146027ms step_avg:98.73ms
step:1490/1770 train_time:146131ms step_avg:98.74ms
step:1491/1770 train_time:146235ms step_avg:98.74ms
step:1492/1770 train_time:146339ms step_avg:98.74ms
step:1493/1770 train_time:146445ms step_avg:98.75ms
step:1494/1770 train_time:146552ms step_avg:98.75ms
step:1495/1770 train_time:146654ms step_avg:98.76ms
step:1496/1770 train_time:146756ms step_avg:98.76ms
step:1497/1770 train_time:146859ms step_avg:98.76ms
step:1498/1770 train_time:146962ms step_avg:98.76ms
step:1499/1770 train_time:147065ms step_avg:98.77ms
step:1500/1770 train_time:147167ms step_avg:98.77ms
step:1500/1770 val_loss:3.3397 train_time:147269ms step_avg:98.84ms
step:1501/1770 train_time:147291ms step_avg:98.79ms
step:1502/1770 train_time:147388ms step_avg:98.79ms
step:1503/1770 train_time:147489ms step_avg:98.79ms
step:1504/1770 train_time:147592ms step_avg:98.79ms
step:1505/1770 train_time:147698ms step_avg:98.79ms
step:1506/1770 train_time:147801ms step_avg:98.80ms
step:1507/1770 train_time:147904ms step_avg:98.80ms
step:1508/1770 train_time:148009ms step_avg:98.80ms
step:1509/1770 train_time:148112ms step_avg:98.81ms
step:1510/1770 train_time:148213ms step_avg:98.81ms
step:1511/1770 train_time:148320ms step_avg:98.81ms
step:1512/1770 train_time:148425ms step_avg:98.82ms
step:1513/1770 train_time:148529ms step_avg:98.82ms
step:1514/1770 train_time:148632ms step_avg:98.82ms
step:1515/1770 train_time:148736ms step_avg:98.83ms
step:1516/1770 train_time:148840ms step_avg:98.83ms
step:1517/1770 train_time:148942ms step_avg:98.83ms
step:1518/1770 train_time:149047ms step_avg:98.84ms
step:1519/1770 train_time:149149ms step_avg:98.84ms
step:1520/1770 train_time:149253ms step_avg:98.84ms
step:1521/1770 train_time:149356ms step_avg:98.85ms
step:1522/1770 train_time:149460ms step_avg:98.85ms
step:1523/1770 train_time:149563ms step_avg:98.85ms
step:1524/1770 train_time:149667ms step_avg:98.86ms
step:1525/1770 train_time:149770ms step_avg:98.86ms
step:1526/1770 train_time:149873ms step_avg:98.86ms
step:1527/1770 train_time:149976ms step_avg:98.86ms
step:1528/1770 train_time:150083ms step_avg:98.87ms
step:1529/1770 train_time:150186ms step_avg:98.87ms
step:1530/1770 train_time:150288ms step_avg:98.87ms
step:1531/1770 train_time:150391ms step_avg:98.88ms
step:1532/1770 train_time:150495ms step_avg:98.88ms
step:1533/1770 train_time:150600ms step_avg:98.88ms
step:1534/1770 train_time:150703ms step_avg:98.89ms
step:1535/1770 train_time:150805ms step_avg:98.89ms
step:1536/1770 train_time:150910ms step_avg:98.89ms
step:1537/1770 train_time:151013ms step_avg:98.90ms
step:1538/1770 train_time:151117ms step_avg:98.90ms
step:1539/1770 train_time:151220ms step_avg:98.90ms
step:1540/1770 train_time:151326ms step_avg:98.91ms
step:1541/1770 train_time:151432ms step_avg:98.91ms
step:1542/1770 train_time:151535ms step_avg:98.91ms
step:1543/1770 train_time:151637ms step_avg:98.92ms
step:1544/1770 train_time:151743ms step_avg:98.92ms
step:1545/1770 train_time:151846ms step_avg:98.92ms
step:1546/1770 train_time:151950ms step_avg:98.93ms
step:1547/1770 train_time:152053ms step_avg:98.93ms
step:1548/1770 train_time:152156ms step_avg:98.93ms
step:1549/1770 train_time:152260ms step_avg:98.93ms
step:1550/1770 train_time:152364ms step_avg:98.94ms
step:1551/1770 train_time:152467ms step_avg:98.94ms
step:1552/1770 train_time:152572ms step_avg:98.94ms
step:1553/1770 train_time:152676ms step_avg:98.95ms
step:1554/1770 train_time:152778ms step_avg:98.95ms
step:1555/1770 train_time:152882ms step_avg:98.95ms
step:1556/1770 train_time:152985ms step_avg:98.96ms
step:1557/1770 train_time:153087ms step_avg:98.96ms
step:1558/1770 train_time:153191ms step_avg:98.96ms
step:1559/1770 train_time:153294ms step_avg:98.96ms
step:1560/1770 train_time:153397ms step_avg:98.97ms
step:1561/1770 train_time:153501ms step_avg:98.97ms
step:1562/1770 train_time:153604ms step_avg:98.97ms
step:1563/1770 train_time:153707ms step_avg:98.97ms
step:1564/1770 train_time:153810ms step_avg:98.98ms
step:1565/1770 train_time:153914ms step_avg:98.98ms
step:1566/1770 train_time:154018ms step_avg:98.98ms
step:1567/1770 train_time:154123ms step_avg:98.99ms
step:1568/1770 train_time:154225ms step_avg:98.99ms
step:1569/1770 train_time:154332ms step_avg:98.99ms
step:1570/1770 train_time:154435ms step_avg:99.00ms
step:1571/1770 train_time:154538ms step_avg:99.00ms
step:1572/1770 train_time:154643ms step_avg:99.00ms
step:1573/1770 train_time:154748ms step_avg:99.01ms
step:1574/1770 train_time:154851ms step_avg:99.01ms
step:1575/1770 train_time:154953ms step_avg:99.01ms
step:1576/1770 train_time:155056ms step_avg:99.01ms
step:1577/1770 train_time:155161ms step_avg:99.02ms
step:1578/1770 train_time:155267ms step_avg:99.02ms
step:1579/1770 train_time:155371ms step_avg:99.03ms
step:1580/1770 train_time:155474ms step_avg:99.03ms
step:1581/1770 train_time:155579ms step_avg:99.03ms
step:1582/1770 train_time:155684ms step_avg:99.04ms
step:1583/1770 train_time:155787ms step_avg:99.04ms
step:1584/1770 train_time:155892ms step_avg:99.04ms
step:1585/1770 train_time:155996ms step_avg:99.04ms
step:1586/1770 train_time:156103ms step_avg:99.05ms
step:1587/1770 train_time:156207ms step_avg:99.05ms
step:1588/1770 train_time:156310ms step_avg:99.06ms
step:1589/1770 train_time:156415ms step_avg:99.06ms
step:1590/1770 train_time:156517ms step_avg:99.06ms
step:1591/1770 train_time:156622ms step_avg:99.06ms
step:1592/1770 train_time:156726ms step_avg:99.07ms
step:1593/1770 train_time:156829ms step_avg:99.07ms
step:1594/1770 train_time:156932ms step_avg:99.07ms
step:1595/1770 train_time:157035ms step_avg:99.08ms
step:1596/1770 train_time:157140ms step_avg:99.08ms
step:1597/1770 train_time:157243ms step_avg:99.08ms
step:1598/1770 train_time:157346ms step_avg:99.08ms
step:1599/1770 train_time:157450ms step_avg:99.09ms
step:1600/1770 train_time:157556ms step_avg:99.09ms
step:1601/1770 train_time:157660ms step_avg:99.09ms
step:1602/1770 train_time:157764ms step_avg:99.10ms
step:1603/1770 train_time:157868ms step_avg:99.10ms
step:1604/1770 train_time:157971ms step_avg:99.10ms
step:1605/1770 train_time:158073ms step_avg:99.11ms
step:1606/1770 train_time:158177ms step_avg:99.11ms
step:1607/1770 train_time:158284ms step_avg:99.11ms
step:1608/1770 train_time:158387ms step_avg:99.12ms
step:1609/1770 train_time:158490ms step_avg:99.12ms
step:1610/1770 train_time:158595ms step_avg:99.12ms
step:1611/1770 train_time:158701ms step_avg:99.13ms
step:1612/1770 train_time:158805ms step_avg:99.13ms
step:1613/1770 train_time:158908ms step_avg:99.13ms
step:1614/1770 train_time:159012ms step_avg:99.13ms
step:1615/1770 train_time:159115ms step_avg:99.14ms
step:1616/1770 train_time:159218ms step_avg:99.14ms
step:1617/1770 train_time:159324ms step_avg:99.14ms
step:1618/1770 train_time:159428ms step_avg:99.15ms
step:1619/1770 train_time:159531ms step_avg:99.15ms
step:1620/1770 train_time:159637ms step_avg:99.15ms
step:1621/1770 train_time:159740ms step_avg:99.16ms
step:1622/1770 train_time:159845ms step_avg:99.16ms
step:1623/1770 train_time:159951ms step_avg:99.16ms
step:1624/1770 train_time:160054ms step_avg:99.17ms
step:1625/1770 train_time:160156ms step_avg:99.17ms
step:1625/1770 val_loss:3.3057 train_time:160258ms step_avg:99.23ms
step:1626/1770 train_time:160280ms step_avg:99.18ms
step:1627/1770 train_time:160372ms step_avg:99.18ms
step:1628/1770 train_time:160476ms step_avg:99.18ms
step:1629/1770 train_time:160578ms step_avg:99.18ms
step:1630/1770 train_time:160681ms step_avg:99.19ms
step:1631/1770 train_time:160784ms step_avg:99.19ms
step:1632/1770 train_time:160887ms step_avg:99.19ms
step:1633/1770 train_time:160990ms step_avg:99.19ms
step:1634/1770 train_time:161092ms step_avg:99.19ms
step:1635/1770 train_time:161196ms step_avg:99.20ms
step:1636/1770 train_time:161300ms step_avg:99.20ms
step:1637/1770 train_time:161405ms step_avg:99.20ms
step:1638/1770 train_time:161508ms step_avg:99.21ms
step:1639/1770 train_time:161612ms step_avg:99.21ms
step:1640/1770 train_time:161716ms step_avg:99.21ms
step:1641/1770 train_time:161818ms step_avg:99.21ms
step:1642/1770 train_time:161921ms step_avg:99.22ms
step:1643/1770 train_time:162024ms step_avg:99.22ms
step:1644/1770 train_time:162129ms step_avg:99.22ms
step:1645/1770 train_time:162231ms step_avg:99.22ms
step:1646/1770 train_time:162337ms step_avg:99.23ms
step:1647/1770 train_time:162440ms step_avg:99.23ms
step:1648/1770 train_time:162544ms step_avg:99.23ms
step:1649/1770 train_time:162647ms step_avg:99.24ms
step:1650/1770 train_time:162750ms step_avg:99.24ms
step:1651/1770 train_time:162854ms step_avg:99.24ms
step:1652/1770 train_time:162958ms step_avg:99.24ms
step:1653/1770 train_time:163060ms step_avg:99.25ms
step:1654/1770 train_time:163167ms step_avg:99.25ms
step:1655/1770 train_time:163273ms step_avg:99.25ms
step:1656/1770 train_time:163376ms step_avg:99.26ms
step:1657/1770 train_time:163481ms step_avg:99.26ms
step:1658/1770 train_time:163584ms step_avg:99.26ms
step:1659/1770 train_time:163688ms step_avg:99.27ms
step:1660/1770 train_time:163792ms step_avg:99.27ms
step:1661/1770 train_time:163896ms step_avg:99.27ms
step:1662/1770 train_time:163999ms step_avg:99.27ms
step:1663/1770 train_time:164102ms step_avg:99.28ms
step:1664/1770 train_time:164206ms step_avg:99.28ms
step:1665/1770 train_time:164309ms step_avg:99.28ms
step:1666/1770 train_time:164413ms step_avg:99.28ms
step:1667/1770 train_time:164517ms step_avg:99.29ms
step:1668/1770 train_time:164620ms step_avg:99.29ms
step:1669/1770 train_time:164722ms step_avg:99.29ms
step:1670/1770 train_time:164825ms step_avg:99.29ms
step:1671/1770 train_time:164930ms step_avg:99.30ms
step:1672/1770 train_time:165034ms step_avg:99.30ms
step:1673/1770 train_time:165139ms step_avg:99.30ms
step:1674/1770 train_time:165243ms step_avg:99.30ms
step:1675/1770 train_time:165346ms step_avg:99.31ms
step:1676/1770 train_time:165450ms step_avg:99.31ms
step:1677/1770 train_time:165558ms step_avg:99.32ms
step:1678/1770 train_time:165661ms step_avg:99.32ms
step:1679/1770 train_time:165764ms step_avg:99.32ms
step:1680/1770 train_time:165867ms step_avg:99.32ms
step:1681/1770 train_time:165971ms step_avg:99.32ms
step:1682/1770 train_time:166077ms step_avg:99.33ms
step:1683/1770 train_time:166179ms step_avg:99.33ms
step:1684/1770 train_time:166282ms step_avg:99.33ms
step:1685/1770 train_time:166384ms step_avg:99.33ms
step:1686/1770 train_time:166490ms step_avg:99.34ms
step:1687/1770 train_time:166595ms step_avg:99.34ms
step:1688/1770 train_time:166698ms step_avg:99.34ms
step:1689/1770 train_time:166801ms step_avg:99.35ms
step:1690/1770 train_time:166905ms step_avg:99.35ms
step:1691/1770 train_time:167009ms step_avg:99.35ms
step:1692/1770 train_time:167112ms step_avg:99.35ms
step:1693/1770 train_time:167217ms step_avg:99.36ms
step:1694/1770 train_time:167320ms step_avg:99.36ms
step:1695/1770 train_time:167424ms step_avg:99.36ms
step:1696/1770 train_time:167530ms step_avg:99.37ms
step:1697/1770 train_time:167635ms step_avg:99.37ms
step:1698/1770 train_time:167739ms step_avg:99.37ms
step:1699/1770 train_time:167841ms step_avg:99.37ms
step:1700/1770 train_time:167944ms step_avg:99.38ms
step:1701/1770 train_time:168049ms step_avg:99.38ms
step:1702/1770 train_time:168152ms step_avg:99.38ms
step:1703/1770 train_time:168255ms step_avg:99.38ms
step:1704/1770 train_time:168358ms step_avg:99.39ms
step:1705/1770 train_time:168462ms step_avg:99.39ms
step:1706/1770 train_time:168565ms step_avg:99.39ms
step:1707/1770 train_time:168669ms step_avg:99.39ms
step:1708/1770 train_time:168773ms step_avg:99.40ms
step:1709/1770 train_time:168878ms step_avg:99.40ms
step:1710/1770 train_time:168985ms step_avg:99.40ms
step:1711/1770 train_time:169091ms step_avg:99.41ms
step:1712/1770 train_time:169195ms step_avg:99.41ms
step:1713/1770 train_time:169299ms step_avg:99.41ms
step:1714/1770 train_time:169403ms step_avg:99.41ms
step:1715/1770 train_time:169505ms step_avg:99.42ms
step:1716/1770 train_time:169610ms step_avg:99.42ms
step:1717/1770 train_time:169715ms step_avg:99.42ms
step:1718/1770 train_time:169820ms step_avg:99.43ms
step:1719/1770 train_time:169925ms step_avg:99.43ms
step:1720/1770 train_time:170030ms step_avg:99.43ms
step:1721/1770 train_time:170133ms step_avg:99.43ms
step:1722/1770 train_time:170240ms step_avg:99.44ms
step:1723/1770 train_time:170346ms step_avg:99.44ms
step:1724/1770 train_time:170452ms step_avg:99.45ms
step:1725/1770 train_time:170558ms step_avg:99.45ms
step:1726/1770 train_time:170663ms step_avg:99.45ms
step:1727/1770 train_time:170767ms step_avg:99.46ms
step:1728/1770 train_time:170872ms step_avg:99.46ms
step:1729/1770 train_time:170976ms step_avg:99.46ms
step:1730/1770 train_time:171081ms step_avg:99.47ms
step:1731/1770 train_time:171187ms step_avg:99.47ms
step:1732/1770 train_time:171290ms step_avg:99.47ms
step:1733/1770 train_time:171396ms step_avg:99.48ms
step:1734/1770 train_time:171499ms step_avg:99.48ms
step:1735/1770 train_time:171603ms step_avg:99.48ms
step:1736/1770 train_time:171707ms step_avg:99.48ms
step:1737/1770 train_time:171812ms step_avg:99.49ms
step:1738/1770 train_time:171916ms step_avg:99.49ms
step:1739/1770 train_time:172020ms step_avg:99.49ms
step:1740/1770 train_time:172124ms step_avg:99.49ms
step:1741/1770 train_time:172231ms step_avg:99.50ms
step:1742/1770 train_time:172338ms step_avg:99.50ms
step:1743/1770 train_time:172443ms step_avg:99.51ms
step:1744/1770 train_time:172547ms step_avg:99.51ms
step:1745/1770 train_time:172651ms step_avg:99.51ms
step:1746/1770 train_time:172758ms step_avg:99.51ms
step:1747/1770 train_time:172861ms step_avg:99.52ms
step:1748/1770 train_time:172967ms step_avg:99.52ms
step:1749/1770 train_time:173072ms step_avg:99.52ms
step:1750/1770 train_time:173176ms step_avg:99.53ms
step:1750/1770 val_loss:3.2791 train_time:173279ms step_avg:99.59ms
step:1751/1770 train_time:173301ms step_avg:99.54ms
step:1752/1770 train_time:173393ms step_avg:99.54ms
step:1753/1770 train_time:173497ms step_avg:99.54ms
step:1754/1770 train_time:173602ms step_avg:99.54ms
step:1755/1770 train_time:173705ms step_avg:99.54ms
step:1756/1770 train_time:173811ms step_avg:99.55ms
step:1757/1770 train_time:173916ms step_avg:99.55ms
step:1758/1770 train_time:174020ms step_avg:99.55ms
step:1759/1770 train_time:174124ms step_avg:99.56ms
step:1760/1770 train_time:174228ms step_avg:99.56ms
step:1761/1770 train_time:174334ms step_avg:99.56ms
step:1762/1770 train_time:174442ms step_avg:99.57ms
step:1763/1770 train_time:174545ms step_avg:99.57ms
step:1764/1770 train_time:174650ms step_avg:99.57ms
step:1765/1770 train_time:174754ms step_avg:99.57ms
step:1766/1770 train_time:174862ms step_avg:99.58ms
step:1767/1770 train_time:174965ms step_avg:99.58ms
step:1768/1770 train_time:175069ms step_avg:99.58ms
step:1769/1770 train_time:175172ms step_avg:99.59ms
step:1770/1770 train_time:175275ms step_avg:99.59ms
step:1770/1770 val_loss:3.2759 train_time:175380ms step_avg:99.65ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
