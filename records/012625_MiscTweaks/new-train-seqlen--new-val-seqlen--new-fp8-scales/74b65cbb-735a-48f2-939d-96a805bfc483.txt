import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Jan 26 00:02:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    197282      C   /usr/bin/python                            3394MiB |
|    0   N/A  N/A    197283      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197284      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197285      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197286      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197287      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197288      C   /usr/bin/python                             610MiB |
|    0   N/A  N/A    197289      C   /usr/bin/python                             610MiB |
|    1   N/A  N/A    197283      C   /usr/bin/python                            3442MiB |
|    2   N/A  N/A    197284      C   /usr/bin/python                            3442MiB |
|    3   N/A  N/A    197285      C   /usr/bin/python                            3442MiB |
|    4   N/A  N/A    197286      C   /usr/bin/python                            3442MiB |
|    5   N/A  N/A    197287      C   /usr/bin/python                            3442MiB |
|    6   N/A  N/A    197288      C   /usr/bin/python                            3442MiB |
|    7   N/A  N/A    197289      C   /usr/bin/python                            3202MiB |
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23106ms step_avg:nanms
step:2/1770 train_time:23609ms step_avg:nanms
step:3/1770 train_time:23704ms step_avg:nanms
step:4/1770 train_time:23797ms step_avg:nanms
step:5/1770 train_time:23890ms step_avg:nanms
step:6/1770 train_time:23984ms step_avg:nanms
step:7/1770 train_time:24077ms step_avg:nanms
step:8/1770 train_time:24171ms step_avg:nanms
step:9/1770 train_time:24265ms step_avg:nanms
step:10/1770 train_time:24360ms step_avg:nanms
step:11/1770 train_time:92ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.88ms
step:14/1770 train_time:376ms step_avg:93.94ms
step:15/1770 train_time:470ms step_avg:93.97ms
step:16/1770 train_time:564ms step_avg:93.93ms
step:17/1770 train_time:657ms step_avg:93.85ms
step:18/1770 train_time:750ms step_avg:93.78ms
step:19/1770 train_time:844ms step_avg:93.80ms
step:20/1770 train_time:938ms step_avg:93.79ms
step:21/1770 train_time:1031ms step_avg:93.75ms
step:22/1770 train_time:1126ms step_avg:93.80ms
step:23/1770 train_time:1220ms step_avg:93.81ms
step:24/1770 train_time:1314ms step_avg:93.85ms
step:25/1770 train_time:1408ms step_avg:93.85ms
step:26/1770 train_time:1502ms step_avg:93.88ms
step:27/1770 train_time:1596ms step_avg:93.89ms
step:28/1770 train_time:1691ms step_avg:93.94ms
step:29/1770 train_time:1784ms step_avg:93.92ms
step:30/1770 train_time:1879ms step_avg:93.93ms
step:31/1770 train_time:1972ms step_avg:93.91ms
step:32/1770 train_time:2066ms step_avg:93.89ms
step:33/1770 train_time:2159ms step_avg:93.89ms
step:34/1770 train_time:2253ms step_avg:93.89ms
step:35/1770 train_time:2347ms step_avg:93.89ms
step:36/1770 train_time:2441ms step_avg:93.89ms
step:37/1770 train_time:2535ms step_avg:93.88ms
step:38/1770 train_time:2629ms step_avg:93.88ms
step:39/1770 train_time:2723ms step_avg:93.88ms
step:40/1770 train_time:2817ms step_avg:93.90ms
step:41/1770 train_time:2911ms step_avg:93.89ms
step:42/1770 train_time:3005ms step_avg:93.90ms
step:43/1770 train_time:3099ms step_avg:93.90ms
step:44/1770 train_time:3193ms step_avg:93.90ms
step:45/1770 train_time:3287ms step_avg:93.91ms
step:46/1770 train_time:3380ms step_avg:93.90ms
step:47/1770 train_time:3474ms step_avg:93.90ms
step:48/1770 train_time:3569ms step_avg:93.92ms
step:49/1770 train_time:3663ms step_avg:93.92ms
step:50/1770 train_time:3757ms step_avg:93.91ms
step:51/1770 train_time:3850ms step_avg:93.91ms
step:52/1770 train_time:3944ms step_avg:93.91ms
step:53/1770 train_time:4038ms step_avg:93.91ms
step:54/1770 train_time:4132ms step_avg:93.92ms
step:55/1770 train_time:4226ms step_avg:93.92ms
step:56/1770 train_time:4321ms step_avg:93.92ms
step:57/1770 train_time:4414ms step_avg:93.92ms
step:58/1770 train_time:4508ms step_avg:93.91ms
step:59/1770 train_time:4602ms step_avg:93.91ms
step:60/1770 train_time:4695ms step_avg:93.91ms
step:61/1770 train_time:4790ms step_avg:93.93ms
step:62/1770 train_time:4884ms step_avg:93.92ms
step:63/1770 train_time:4978ms step_avg:93.92ms
step:64/1770 train_time:5071ms step_avg:93.91ms
step:65/1770 train_time:5165ms step_avg:93.91ms
step:66/1770 train_time:5259ms step_avg:93.92ms
step:67/1770 train_time:5353ms step_avg:93.91ms
step:68/1770 train_time:5446ms step_avg:93.90ms
step:69/1770 train_time:5540ms step_avg:93.90ms
step:70/1770 train_time:5634ms step_avg:93.90ms
step:71/1770 train_time:5728ms step_avg:93.90ms
step:72/1770 train_time:5822ms step_avg:93.90ms
step:73/1770 train_time:5916ms step_avg:93.91ms
step:74/1770 train_time:6010ms step_avg:93.90ms
step:75/1770 train_time:6104ms step_avg:93.90ms
step:76/1770 train_time:6197ms step_avg:93.90ms
step:77/1770 train_time:6291ms step_avg:93.89ms
step:78/1770 train_time:6385ms step_avg:93.90ms
step:79/1770 train_time:6479ms step_avg:93.89ms
step:80/1770 train_time:6572ms step_avg:93.89ms
step:81/1770 train_time:6666ms step_avg:93.89ms
step:82/1770 train_time:6760ms step_avg:93.88ms
step:83/1770 train_time:6853ms step_avg:93.88ms
step:84/1770 train_time:6947ms step_avg:93.88ms
step:85/1770 train_time:7041ms step_avg:93.88ms
step:86/1770 train_time:7135ms step_avg:93.88ms
step:87/1770 train_time:7229ms step_avg:93.88ms
step:88/1770 train_time:7322ms step_avg:93.88ms
step:89/1770 train_time:7416ms step_avg:93.88ms
step:90/1770 train_time:7510ms step_avg:93.87ms
step:91/1770 train_time:7604ms step_avg:93.87ms
step:92/1770 train_time:7698ms step_avg:93.87ms
step:93/1770 train_time:7791ms step_avg:93.87ms
step:94/1770 train_time:7885ms step_avg:93.87ms
step:95/1770 train_time:7979ms step_avg:93.87ms
step:96/1770 train_time:8073ms step_avg:93.87ms
step:97/1770 train_time:8167ms step_avg:93.87ms
step:98/1770 train_time:8260ms step_avg:93.87ms
step:99/1770 train_time:8354ms step_avg:93.87ms
step:100/1770 train_time:8448ms step_avg:93.87ms
step:101/1770 train_time:8542ms step_avg:93.87ms
step:102/1770 train_time:8635ms step_avg:93.86ms
step:103/1770 train_time:8729ms step_avg:93.86ms
step:104/1770 train_time:8823ms step_avg:93.86ms
step:105/1770 train_time:8917ms step_avg:93.86ms
step:106/1770 train_time:9011ms step_avg:93.86ms
step:107/1770 train_time:9104ms step_avg:93.86ms
step:108/1770 train_time:9199ms step_avg:93.86ms
step:109/1770 train_time:9292ms step_avg:93.86ms
step:110/1770 train_time:9387ms step_avg:93.87ms
step:111/1770 train_time:9481ms step_avg:93.87ms
step:112/1770 train_time:9574ms step_avg:93.87ms
step:113/1770 train_time:9668ms step_avg:93.87ms
step:114/1770 train_time:9762ms step_avg:93.87ms
step:115/1770 train_time:9856ms step_avg:93.86ms
step:116/1770 train_time:9951ms step_avg:93.88ms
step:117/1770 train_time:10046ms step_avg:93.89ms
step:118/1770 train_time:10140ms step_avg:93.88ms
step:119/1770 train_time:10234ms step_avg:93.89ms
step:120/1770 train_time:10327ms step_avg:93.88ms
step:121/1770 train_time:10421ms step_avg:93.88ms
step:122/1770 train_time:10515ms step_avg:93.88ms
step:123/1770 train_time:10609ms step_avg:93.89ms
step:124/1770 train_time:10703ms step_avg:93.89ms
step:125/1770 train_time:10797ms step_avg:93.89ms
step:125/1770 val_loss:4.6508 train_time:10890ms step_avg:94.69ms
step:126/1770 train_time:10912ms step_avg:94.07ms
step:127/1770 train_time:10990ms step_avg:93.93ms
step:128/1770 train_time:11090ms step_avg:93.98ms
step:129/1770 train_time:11189ms step_avg:94.02ms
step:130/1770 train_time:11283ms step_avg:94.02ms
step:131/1770 train_time:11377ms step_avg:94.02ms
step:132/1770 train_time:11470ms step_avg:94.02ms
step:133/1770 train_time:11564ms step_avg:94.02ms
step:134/1770 train_time:11658ms step_avg:94.02ms
step:135/1770 train_time:11752ms step_avg:94.02ms
step:136/1770 train_time:11846ms step_avg:94.01ms
step:137/1770 train_time:11940ms step_avg:94.02ms
step:138/1770 train_time:12035ms step_avg:94.02ms
step:139/1770 train_time:12130ms step_avg:94.03ms
step:140/1770 train_time:12225ms step_avg:94.04ms
step:141/1770 train_time:12320ms step_avg:94.04ms
step:142/1770 train_time:12414ms step_avg:94.05ms
step:143/1770 train_time:12509ms step_avg:94.05ms
step:144/1770 train_time:12603ms step_avg:94.05ms
step:145/1770 train_time:12697ms step_avg:94.05ms
step:146/1770 train_time:12792ms step_avg:94.06ms
step:147/1770 train_time:12886ms step_avg:94.06ms
step:148/1770 train_time:12980ms step_avg:94.06ms
step:149/1770 train_time:13075ms step_avg:94.06ms
step:150/1770 train_time:13169ms step_avg:94.07ms
step:151/1770 train_time:13264ms step_avg:94.07ms
step:152/1770 train_time:13359ms step_avg:94.08ms
step:153/1770 train_time:13454ms step_avg:94.08ms
step:154/1770 train_time:13549ms step_avg:94.09ms
step:155/1770 train_time:13643ms step_avg:94.09ms
step:156/1770 train_time:13737ms step_avg:94.09ms
step:157/1770 train_time:13831ms step_avg:94.09ms
step:158/1770 train_time:13925ms step_avg:94.09ms
step:159/1770 train_time:14020ms step_avg:94.09ms
step:160/1770 train_time:14115ms step_avg:94.10ms
step:161/1770 train_time:14209ms step_avg:94.10ms
step:162/1770 train_time:14304ms step_avg:94.11ms
step:163/1770 train_time:14400ms step_avg:94.12ms
step:164/1770 train_time:14495ms step_avg:94.12ms
step:165/1770 train_time:14589ms step_avg:94.12ms
step:166/1770 train_time:14683ms step_avg:94.12ms
step:167/1770 train_time:14777ms step_avg:94.12ms
step:168/1770 train_time:14872ms step_avg:94.13ms
step:169/1770 train_time:14966ms step_avg:94.13ms
step:170/1770 train_time:15061ms step_avg:94.13ms
step:171/1770 train_time:15155ms step_avg:94.13ms
step:172/1770 train_time:15249ms step_avg:94.13ms
step:173/1770 train_time:15343ms step_avg:94.13ms
step:174/1770 train_time:15438ms step_avg:94.14ms
step:175/1770 train_time:15533ms step_avg:94.14ms
step:176/1770 train_time:15627ms step_avg:94.14ms
step:177/1770 train_time:15722ms step_avg:94.14ms
step:178/1770 train_time:15816ms step_avg:94.14ms
step:179/1770 train_time:15910ms step_avg:94.14ms
step:180/1770 train_time:16004ms step_avg:94.14ms
step:181/1770 train_time:16098ms step_avg:94.14ms
step:182/1770 train_time:16193ms step_avg:94.14ms
step:183/1770 train_time:16287ms step_avg:94.14ms
step:184/1770 train_time:16381ms step_avg:94.14ms
step:185/1770 train_time:16476ms step_avg:94.15ms
step:186/1770 train_time:16571ms step_avg:94.15ms
step:187/1770 train_time:16665ms step_avg:94.15ms
step:188/1770 train_time:16760ms step_avg:94.16ms
step:189/1770 train_time:16855ms step_avg:94.16ms
step:190/1770 train_time:16949ms step_avg:94.16ms
step:191/1770 train_time:17043ms step_avg:94.16ms
step:192/1770 train_time:17138ms step_avg:94.16ms
step:193/1770 train_time:17233ms step_avg:94.17ms
step:194/1770 train_time:17328ms step_avg:94.17ms
step:195/1770 train_time:17422ms step_avg:94.17ms
step:196/1770 train_time:17517ms step_avg:94.18ms
step:197/1770 train_time:17612ms step_avg:94.18ms
step:198/1770 train_time:17706ms step_avg:94.18ms
step:199/1770 train_time:17801ms step_avg:94.19ms
step:200/1770 train_time:17896ms step_avg:94.19ms
step:201/1770 train_time:17990ms step_avg:94.19ms
step:202/1770 train_time:18084ms step_avg:94.19ms
step:203/1770 train_time:18179ms step_avg:94.19ms
step:204/1770 train_time:18273ms step_avg:94.19ms
step:205/1770 train_time:18368ms step_avg:94.19ms
step:206/1770 train_time:18462ms step_avg:94.19ms
step:207/1770 train_time:18556ms step_avg:94.20ms
step:208/1770 train_time:18651ms step_avg:94.20ms
step:209/1770 train_time:18745ms step_avg:94.19ms
step:210/1770 train_time:18840ms step_avg:94.20ms
step:211/1770 train_time:18934ms step_avg:94.20ms
step:212/1770 train_time:19028ms step_avg:94.20ms
step:213/1770 train_time:19123ms step_avg:94.20ms
step:214/1770 train_time:19218ms step_avg:94.21ms
step:215/1770 train_time:19313ms step_avg:94.21ms
step:216/1770 train_time:19407ms step_avg:94.21ms
step:217/1770 train_time:19502ms step_avg:94.21ms
step:218/1770 train_time:19596ms step_avg:94.21ms
step:219/1770 train_time:19690ms step_avg:94.21ms
step:220/1770 train_time:19784ms step_avg:94.21ms
step:221/1770 train_time:19879ms step_avg:94.21ms
step:222/1770 train_time:19973ms step_avg:94.21ms
step:223/1770 train_time:20068ms step_avg:94.22ms
step:224/1770 train_time:20162ms step_avg:94.22ms
step:225/1770 train_time:20257ms step_avg:94.22ms
step:226/1770 train_time:20352ms step_avg:94.22ms
step:227/1770 train_time:20446ms step_avg:94.22ms
step:228/1770 train_time:20541ms step_avg:94.22ms
step:229/1770 train_time:20635ms step_avg:94.22ms
step:230/1770 train_time:20729ms step_avg:94.22ms
step:231/1770 train_time:20824ms step_avg:94.23ms
step:232/1770 train_time:20918ms step_avg:94.23ms
step:233/1770 train_time:21013ms step_avg:94.23ms
step:234/1770 train_time:21107ms step_avg:94.23ms
step:235/1770 train_time:21202ms step_avg:94.23ms
step:236/1770 train_time:21296ms step_avg:94.23ms
step:237/1770 train_time:21391ms step_avg:94.23ms
step:238/1770 train_time:21485ms step_avg:94.23ms
step:239/1770 train_time:21581ms step_avg:94.24ms
step:240/1770 train_time:21675ms step_avg:94.24ms
step:241/1770 train_time:21770ms step_avg:94.24ms
step:242/1770 train_time:21865ms step_avg:94.24ms
step:243/1770 train_time:21960ms step_avg:94.25ms
step:244/1770 train_time:22054ms step_avg:94.25ms
step:245/1770 train_time:22148ms step_avg:94.25ms
step:246/1770 train_time:22243ms step_avg:94.25ms
step:247/1770 train_time:22338ms step_avg:94.25ms
step:248/1770 train_time:22432ms step_avg:94.25ms
step:249/1770 train_time:22527ms step_avg:94.25ms
step:250/1770 train_time:22621ms step_avg:94.25ms
step:250/1770 val_loss:4.1176 train_time:22714ms step_avg:94.64ms
step:251/1770 train_time:22736ms step_avg:94.34ms
step:252/1770 train_time:22818ms step_avg:94.29ms
step:253/1770 train_time:22916ms step_avg:94.31ms
step:254/1770 train_time:23011ms step_avg:94.31ms
step:255/1770 train_time:23105ms step_avg:94.31ms
step:256/1770 train_time:23199ms step_avg:94.31ms
step:257/1770 train_time:23294ms step_avg:94.31ms
step:258/1770 train_time:23388ms step_avg:94.31ms
step:259/1770 train_time:23482ms step_avg:94.31ms
step:260/1770 train_time:23576ms step_avg:94.31ms
step:261/1770 train_time:23671ms step_avg:94.31ms
step:262/1770 train_time:23766ms step_avg:94.31ms
step:263/1770 train_time:23861ms step_avg:94.31ms
step:264/1770 train_time:23956ms step_avg:94.32ms
step:265/1770 train_time:24052ms step_avg:94.32ms
step:266/1770 train_time:24148ms step_avg:94.33ms
step:267/1770 train_time:24242ms step_avg:94.33ms
step:268/1770 train_time:24336ms step_avg:94.33ms
step:269/1770 train_time:24431ms step_avg:94.33ms
step:270/1770 train_time:24526ms step_avg:94.33ms
step:271/1770 train_time:24622ms step_avg:94.34ms
step:272/1770 train_time:24717ms step_avg:94.34ms
step:273/1770 train_time:24813ms step_avg:94.35ms
step:274/1770 train_time:24908ms step_avg:94.35ms
step:275/1770 train_time:25003ms step_avg:94.35ms
step:276/1770 train_time:25099ms step_avg:94.36ms
step:277/1770 train_time:25194ms step_avg:94.36ms
step:278/1770 train_time:25290ms step_avg:94.36ms
step:279/1770 train_time:25384ms step_avg:94.36ms
step:280/1770 train_time:25478ms step_avg:94.36ms
step:281/1770 train_time:25574ms step_avg:94.37ms
step:282/1770 train_time:25669ms step_avg:94.37ms
step:283/1770 train_time:25764ms step_avg:94.37ms
step:284/1770 train_time:25860ms step_avg:94.38ms
step:285/1770 train_time:25956ms step_avg:94.38ms
step:286/1770 train_time:26050ms step_avg:94.39ms
step:287/1770 train_time:26145ms step_avg:94.39ms
step:288/1770 train_time:26240ms step_avg:94.39ms
step:289/1770 train_time:26336ms step_avg:94.39ms
step:290/1770 train_time:26431ms step_avg:94.39ms
step:291/1770 train_time:26525ms step_avg:94.40ms
step:292/1770 train_time:26620ms step_avg:94.40ms
step:293/1770 train_time:26715ms step_avg:94.40ms
step:294/1770 train_time:26811ms step_avg:94.41ms
step:295/1770 train_time:26907ms step_avg:94.41ms
step:296/1770 train_time:27001ms step_avg:94.41ms
step:297/1770 train_time:27097ms step_avg:94.41ms
step:298/1770 train_time:27192ms step_avg:94.42ms
step:299/1770 train_time:27286ms step_avg:94.42ms
step:300/1770 train_time:27381ms step_avg:94.42ms
step:301/1770 train_time:27476ms step_avg:94.42ms
step:302/1770 train_time:27571ms step_avg:94.42ms
step:303/1770 train_time:27666ms step_avg:94.42ms
step:304/1770 train_time:27761ms step_avg:94.43ms
step:305/1770 train_time:27856ms step_avg:94.43ms
step:306/1770 train_time:27952ms step_avg:94.43ms
step:307/1770 train_time:28048ms step_avg:94.44ms
step:308/1770 train_time:28142ms step_avg:94.44ms
step:309/1770 train_time:28237ms step_avg:94.44ms
step:310/1770 train_time:28332ms step_avg:94.44ms
step:311/1770 train_time:28427ms step_avg:94.44ms
step:312/1770 train_time:28522ms step_avg:94.44ms
step:313/1770 train_time:28617ms step_avg:94.45ms
step:314/1770 train_time:28712ms step_avg:94.45ms
step:315/1770 train_time:28807ms step_avg:94.45ms
step:316/1770 train_time:28902ms step_avg:94.45ms
step:317/1770 train_time:28997ms step_avg:94.45ms
step:318/1770 train_time:29092ms step_avg:94.45ms
step:319/1770 train_time:29187ms step_avg:94.46ms
step:320/1770 train_time:29283ms step_avg:94.46ms
step:321/1770 train_time:29378ms step_avg:94.46ms
step:322/1770 train_time:29473ms step_avg:94.46ms
step:323/1770 train_time:29568ms step_avg:94.47ms
step:324/1770 train_time:29662ms step_avg:94.47ms
step:325/1770 train_time:29758ms step_avg:94.47ms
step:326/1770 train_time:29853ms step_avg:94.47ms
step:327/1770 train_time:29948ms step_avg:94.47ms
step:328/1770 train_time:30042ms step_avg:94.47ms
step:329/1770 train_time:30137ms step_avg:94.47ms
step:330/1770 train_time:30232ms step_avg:94.47ms
step:331/1770 train_time:30327ms step_avg:94.48ms
step:332/1770 train_time:30422ms step_avg:94.48ms
step:333/1770 train_time:30517ms step_avg:94.48ms
step:334/1770 train_time:30613ms step_avg:94.48ms
step:335/1770 train_time:30707ms step_avg:94.48ms
step:336/1770 train_time:30802ms step_avg:94.49ms
step:337/1770 train_time:30897ms step_avg:94.49ms
step:338/1770 train_time:30992ms step_avg:94.49ms
step:339/1770 train_time:31088ms step_avg:94.49ms
step:340/1770 train_time:31182ms step_avg:94.49ms
step:341/1770 train_time:31277ms step_avg:94.49ms
step:342/1770 train_time:31373ms step_avg:94.50ms
step:343/1770 train_time:31468ms step_avg:94.50ms
step:344/1770 train_time:31562ms step_avg:94.50ms
step:345/1770 train_time:31658ms step_avg:94.50ms
step:346/1770 train_time:31753ms step_avg:94.50ms
step:347/1770 train_time:31849ms step_avg:94.51ms
step:348/1770 train_time:31943ms step_avg:94.51ms
step:349/1770 train_time:32038ms step_avg:94.51ms
step:350/1770 train_time:32133ms step_avg:94.51ms
step:351/1770 train_time:32228ms step_avg:94.51ms
step:352/1770 train_time:32323ms step_avg:94.51ms
step:353/1770 train_time:32418ms step_avg:94.51ms
step:354/1770 train_time:32513ms step_avg:94.52ms
step:355/1770 train_time:32608ms step_avg:94.52ms
step:356/1770 train_time:32703ms step_avg:94.52ms
step:357/1770 train_time:32798ms step_avg:94.52ms
step:358/1770 train_time:32893ms step_avg:94.52ms
step:359/1770 train_time:32989ms step_avg:94.52ms
step:360/1770 train_time:33084ms step_avg:94.53ms
step:361/1770 train_time:33178ms step_avg:94.53ms
step:362/1770 train_time:33273ms step_avg:94.53ms
step:363/1770 train_time:33368ms step_avg:94.53ms
step:364/1770 train_time:33463ms step_avg:94.53ms
step:365/1770 train_time:33558ms step_avg:94.53ms
step:366/1770 train_time:33653ms step_avg:94.53ms
step:367/1770 train_time:33748ms step_avg:94.53ms
step:368/1770 train_time:33843ms step_avg:94.53ms
step:369/1770 train_time:33938ms step_avg:94.53ms
step:370/1770 train_time:34034ms step_avg:94.54ms
step:371/1770 train_time:34129ms step_avg:94.54ms
step:372/1770 train_time:34223ms step_avg:94.54ms
step:373/1770 train_time:34318ms step_avg:94.54ms
step:374/1770 train_time:34414ms step_avg:94.54ms
step:375/1770 train_time:34509ms step_avg:94.55ms
step:375/1770 val_loss:3.9137 train_time:34602ms step_avg:94.80ms
step:376/1770 train_time:34623ms step_avg:94.60ms
step:377/1770 train_time:34704ms step_avg:94.56ms
step:378/1770 train_time:34803ms step_avg:94.57ms
step:379/1770 train_time:34898ms step_avg:94.58ms
step:380/1770 train_time:34993ms step_avg:94.57ms
step:381/1770 train_time:35087ms step_avg:94.57ms
step:382/1770 train_time:35182ms step_avg:94.57ms
step:383/1770 train_time:35276ms step_avg:94.57ms
step:384/1770 train_time:35371ms step_avg:94.57ms
step:385/1770 train_time:35465ms step_avg:94.57ms
step:386/1770 train_time:35559ms step_avg:94.57ms
step:387/1770 train_time:35654ms step_avg:94.57ms
step:388/1770 train_time:35750ms step_avg:94.58ms
step:389/1770 train_time:35846ms step_avg:94.58ms
step:390/1770 train_time:35941ms step_avg:94.58ms
step:391/1770 train_time:36036ms step_avg:94.58ms
step:392/1770 train_time:36131ms step_avg:94.58ms
step:393/1770 train_time:36226ms step_avg:94.58ms
step:394/1770 train_time:36320ms step_avg:94.58ms
step:395/1770 train_time:36414ms step_avg:94.58ms
step:396/1770 train_time:36511ms step_avg:94.59ms
step:397/1770 train_time:36608ms step_avg:94.60ms
step:398/1770 train_time:36706ms step_avg:94.60ms
step:399/1770 train_time:36803ms step_avg:94.61ms
step:400/1770 train_time:36900ms step_avg:94.62ms
step:401/1770 train_time:36997ms step_avg:94.62ms
step:402/1770 train_time:37094ms step_avg:94.63ms
step:403/1770 train_time:37190ms step_avg:94.63ms
step:404/1770 train_time:37287ms step_avg:94.64ms
step:405/1770 train_time:37384ms step_avg:94.64ms
step:406/1770 train_time:37481ms step_avg:94.65ms
step:407/1770 train_time:37576ms step_avg:94.65ms
step:408/1770 train_time:37673ms step_avg:94.66ms
step:409/1770 train_time:37771ms step_avg:94.66ms
step:410/1770 train_time:37868ms step_avg:94.67ms
step:411/1770 train_time:37966ms step_avg:94.68ms
step:412/1770 train_time:38063ms step_avg:94.68ms
step:413/1770 train_time:38160ms step_avg:94.69ms
step:414/1770 train_time:38257ms step_avg:94.69ms
step:415/1770 train_time:38353ms step_avg:94.70ms
step:416/1770 train_time:38450ms step_avg:94.71ms
step:417/1770 train_time:38547ms step_avg:94.71ms
step:418/1770 train_time:38643ms step_avg:94.71ms
step:419/1770 train_time:38740ms step_avg:94.72ms
step:420/1770 train_time:38837ms step_avg:94.72ms
step:421/1770 train_time:38934ms step_avg:94.73ms
step:422/1770 train_time:39031ms step_avg:94.74ms
step:423/1770 train_time:39128ms step_avg:94.74ms
step:424/1770 train_time:39225ms step_avg:94.75ms
step:425/1770 train_time:39322ms step_avg:94.75ms
step:426/1770 train_time:39419ms step_avg:94.76ms
step:427/1770 train_time:39516ms step_avg:94.76ms
step:428/1770 train_time:39612ms step_avg:94.77ms
step:429/1770 train_time:39709ms step_avg:94.77ms
step:430/1770 train_time:39806ms step_avg:94.78ms
step:431/1770 train_time:39903ms step_avg:94.78ms
step:432/1770 train_time:40000ms step_avg:94.79ms
step:433/1770 train_time:40097ms step_avg:94.79ms
step:434/1770 train_time:40193ms step_avg:94.80ms
step:435/1770 train_time:40291ms step_avg:94.80ms
step:436/1770 train_time:40388ms step_avg:94.81ms
step:437/1770 train_time:40486ms step_avg:94.81ms
step:438/1770 train_time:40582ms step_avg:94.82ms
step:439/1770 train_time:40679ms step_avg:94.82ms
step:440/1770 train_time:40776ms step_avg:94.83ms
step:441/1770 train_time:40873ms step_avg:94.83ms
step:442/1770 train_time:40971ms step_avg:94.84ms
step:443/1770 train_time:41068ms step_avg:94.84ms
step:444/1770 train_time:41165ms step_avg:94.85ms
step:445/1770 train_time:41262ms step_avg:94.86ms
step:446/1770 train_time:41359ms step_avg:94.86ms
step:447/1770 train_time:41455ms step_avg:94.86ms
step:448/1770 train_time:41552ms step_avg:94.87ms
step:449/1770 train_time:41649ms step_avg:94.87ms
step:450/1770 train_time:41746ms step_avg:94.88ms
step:451/1770 train_time:41844ms step_avg:94.88ms
step:452/1770 train_time:41941ms step_avg:94.89ms
step:453/1770 train_time:42038ms step_avg:94.89ms
step:454/1770 train_time:42135ms step_avg:94.90ms
step:455/1770 train_time:42232ms step_avg:94.90ms
step:456/1770 train_time:42329ms step_avg:94.91ms
step:457/1770 train_time:42426ms step_avg:94.91ms
step:458/1770 train_time:42524ms step_avg:94.92ms
step:459/1770 train_time:42620ms step_avg:94.92ms
step:460/1770 train_time:42717ms step_avg:94.93ms
step:461/1770 train_time:42814ms step_avg:94.93ms
step:462/1770 train_time:42911ms step_avg:94.94ms
step:463/1770 train_time:43008ms step_avg:94.94ms
step:464/1770 train_time:43105ms step_avg:94.95ms
step:465/1770 train_time:43202ms step_avg:94.95ms
step:466/1770 train_time:43298ms step_avg:94.95ms
step:467/1770 train_time:43395ms step_avg:94.96ms
step:468/1770 train_time:43492ms step_avg:94.96ms
step:469/1770 train_time:43588ms step_avg:94.96ms
step:470/1770 train_time:43686ms step_avg:94.97ms
step:471/1770 train_time:43782ms step_avg:94.97ms
step:472/1770 train_time:43880ms step_avg:94.98ms
step:473/1770 train_time:43977ms step_avg:94.98ms
step:474/1770 train_time:44074ms step_avg:94.99ms
step:475/1770 train_time:44172ms step_avg:94.99ms
step:476/1770 train_time:44269ms step_avg:95.00ms
step:477/1770 train_time:44366ms step_avg:95.00ms
step:478/1770 train_time:44463ms step_avg:95.01ms
step:479/1770 train_time:44560ms step_avg:95.01ms
step:480/1770 train_time:44656ms step_avg:95.01ms
step:481/1770 train_time:44753ms step_avg:95.02ms
step:482/1770 train_time:44851ms step_avg:95.02ms
step:483/1770 train_time:44948ms step_avg:95.03ms
step:484/1770 train_time:45045ms step_avg:95.03ms
step:485/1770 train_time:45143ms step_avg:95.04ms
step:486/1770 train_time:45240ms step_avg:95.04ms
step:487/1770 train_time:45336ms step_avg:95.04ms
step:488/1770 train_time:45433ms step_avg:95.05ms
step:489/1770 train_time:45530ms step_avg:95.05ms
step:490/1770 train_time:45627ms step_avg:95.06ms
step:491/1770 train_time:45724ms step_avg:95.06ms
step:492/1770 train_time:45822ms step_avg:95.07ms
step:493/1770 train_time:45918ms step_avg:95.07ms
step:494/1770 train_time:46015ms step_avg:95.07ms
step:495/1770 train_time:46112ms step_avg:95.08ms
step:496/1770 train_time:46209ms step_avg:95.08ms
step:497/1770 train_time:46306ms step_avg:95.08ms
step:498/1770 train_time:46403ms step_avg:95.09ms
step:499/1770 train_time:46500ms step_avg:95.09ms
step:500/1770 train_time:46596ms step_avg:95.09ms
step:500/1770 val_loss:3.7561 train_time:46691ms step_avg:95.29ms
step:501/1770 train_time:46713ms step_avg:95.14ms
step:502/1770 train_time:46796ms step_avg:95.11ms
step:503/1770 train_time:46896ms step_avg:95.12ms
step:504/1770 train_time:46993ms step_avg:95.13ms
step:505/1770 train_time:47089ms step_avg:95.13ms
step:506/1770 train_time:47186ms step_avg:95.13ms
step:507/1770 train_time:47282ms step_avg:95.14ms
step:508/1770 train_time:47379ms step_avg:95.14ms
step:509/1770 train_time:47475ms step_avg:95.14ms
step:510/1770 train_time:47571ms step_avg:95.14ms
step:511/1770 train_time:47667ms step_avg:95.14ms
step:512/1770 train_time:47765ms step_avg:95.15ms
step:513/1770 train_time:47863ms step_avg:95.15ms
step:514/1770 train_time:47961ms step_avg:95.16ms
step:515/1770 train_time:48059ms step_avg:95.17ms
step:516/1770 train_time:48155ms step_avg:95.17ms
step:517/1770 train_time:48251ms step_avg:95.17ms
step:518/1770 train_time:48348ms step_avg:95.17ms
step:519/1770 train_time:48444ms step_avg:95.18ms
step:520/1770 train_time:48542ms step_avg:95.18ms
step:521/1770 train_time:48639ms step_avg:95.18ms
step:522/1770 train_time:48736ms step_avg:95.19ms
step:523/1770 train_time:48834ms step_avg:95.19ms
step:524/1770 train_time:48932ms step_avg:95.20ms
step:525/1770 train_time:49029ms step_avg:95.20ms
step:526/1770 train_time:49126ms step_avg:95.21ms
step:527/1770 train_time:49223ms step_avg:95.21ms
step:528/1770 train_time:49320ms step_avg:95.21ms
step:529/1770 train_time:49417ms step_avg:95.22ms
step:530/1770 train_time:49514ms step_avg:95.22ms
step:531/1770 train_time:49611ms step_avg:95.22ms
step:532/1770 train_time:49709ms step_avg:95.23ms
step:533/1770 train_time:49806ms step_avg:95.23ms
step:534/1770 train_time:49904ms step_avg:95.24ms
step:535/1770 train_time:50002ms step_avg:95.24ms
step:536/1770 train_time:50099ms step_avg:95.25ms
step:537/1770 train_time:50196ms step_avg:95.25ms
step:538/1770 train_time:50293ms step_avg:95.25ms
step:539/1770 train_time:50390ms step_avg:95.26ms
step:540/1770 train_time:50487ms step_avg:95.26ms
step:541/1770 train_time:50584ms step_avg:95.26ms
step:542/1770 train_time:50682ms step_avg:95.27ms
step:543/1770 train_time:50779ms step_avg:95.27ms
step:544/1770 train_time:50876ms step_avg:95.27ms
step:545/1770 train_time:50974ms step_avg:95.28ms
step:546/1770 train_time:51071ms step_avg:95.28ms
step:547/1770 train_time:51168ms step_avg:95.29ms
step:548/1770 train_time:51265ms step_avg:95.29ms
step:549/1770 train_time:51363ms step_avg:95.29ms
step:550/1770 train_time:51460ms step_avg:95.30ms
step:551/1770 train_time:51557ms step_avg:95.30ms
step:552/1770 train_time:51654ms step_avg:95.30ms
step:553/1770 train_time:51751ms step_avg:95.31ms
step:554/1770 train_time:51849ms step_avg:95.31ms
step:555/1770 train_time:51947ms step_avg:95.31ms
step:556/1770 train_time:52044ms step_avg:95.32ms
step:557/1770 train_time:52142ms step_avg:95.32ms
step:558/1770 train_time:52239ms step_avg:95.33ms
step:559/1770 train_time:52336ms step_avg:95.33ms
step:560/1770 train_time:52434ms step_avg:95.33ms
step:561/1770 train_time:52531ms step_avg:95.34ms
step:562/1770 train_time:52628ms step_avg:95.34ms
step:563/1770 train_time:52725ms step_avg:95.34ms
step:564/1770 train_time:52823ms step_avg:95.35ms
step:565/1770 train_time:52921ms step_avg:95.35ms
step:566/1770 train_time:53017ms step_avg:95.36ms
step:567/1770 train_time:53115ms step_avg:95.36ms
step:568/1770 train_time:53212ms step_avg:95.36ms
step:569/1770 train_time:53309ms step_avg:95.37ms
step:570/1770 train_time:53407ms step_avg:95.37ms
step:571/1770 train_time:53505ms step_avg:95.37ms
step:572/1770 train_time:53602ms step_avg:95.38ms
step:573/1770 train_time:53699ms step_avg:95.38ms
step:574/1770 train_time:53796ms step_avg:95.38ms
step:575/1770 train_time:53893ms step_avg:95.39ms
step:576/1770 train_time:53990ms step_avg:95.39ms
step:577/1770 train_time:54087ms step_avg:95.39ms
step:578/1770 train_time:54185ms step_avg:95.40ms
step:579/1770 train_time:54282ms step_avg:95.40ms
step:580/1770 train_time:54379ms step_avg:95.40ms
step:581/1770 train_time:54477ms step_avg:95.41ms
step:582/1770 train_time:54573ms step_avg:95.41ms
step:583/1770 train_time:54671ms step_avg:95.41ms
step:584/1770 train_time:54769ms step_avg:95.42ms
step:585/1770 train_time:54866ms step_avg:95.42ms
step:586/1770 train_time:54963ms step_avg:95.42ms
step:587/1770 train_time:55060ms step_avg:95.43ms
step:588/1770 train_time:55157ms step_avg:95.43ms
step:589/1770 train_time:55254ms step_avg:95.43ms
step:590/1770 train_time:55351ms step_avg:95.43ms
step:591/1770 train_time:55449ms step_avg:95.44ms
step:592/1770 train_time:55546ms step_avg:95.44ms
step:593/1770 train_time:55644ms step_avg:95.44ms
step:594/1770 train_time:55741ms step_avg:95.45ms
step:595/1770 train_time:55838ms step_avg:95.45ms
step:596/1770 train_time:55935ms step_avg:95.45ms
step:597/1770 train_time:56032ms step_avg:95.45ms
step:598/1770 train_time:56130ms step_avg:95.46ms
step:599/1770 train_time:56227ms step_avg:95.46ms
step:600/1770 train_time:56325ms step_avg:95.47ms
step:601/1770 train_time:56423ms step_avg:95.47ms
step:602/1770 train_time:56520ms step_avg:95.47ms
step:603/1770 train_time:56617ms step_avg:95.48ms
step:604/1770 train_time:56714ms step_avg:95.48ms
step:605/1770 train_time:56813ms step_avg:95.48ms
step:606/1770 train_time:56910ms step_avg:95.49ms
step:607/1770 train_time:57007ms step_avg:95.49ms
step:608/1770 train_time:57105ms step_avg:95.49ms
step:609/1770 train_time:57202ms step_avg:95.50ms
step:610/1770 train_time:57299ms step_avg:95.50ms
step:611/1770 train_time:57396ms step_avg:95.50ms
step:612/1770 train_time:57493ms step_avg:95.50ms
step:613/1770 train_time:57590ms step_avg:95.51ms
step:614/1770 train_time:57688ms step_avg:95.51ms
step:615/1770 train_time:57785ms step_avg:95.51ms
step:616/1770 train_time:57883ms step_avg:95.52ms
step:617/1770 train_time:57980ms step_avg:95.52ms
step:618/1770 train_time:58077ms step_avg:95.52ms
step:619/1770 train_time:58174ms step_avg:95.52ms
step:620/1770 train_time:58272ms step_avg:95.53ms
step:621/1770 train_time:58369ms step_avg:95.53ms
step:622/1770 train_time:58466ms step_avg:95.53ms
step:623/1770 train_time:58563ms step_avg:95.53ms
step:624/1770 train_time:58660ms step_avg:95.54ms
step:625/1770 train_time:58757ms step_avg:95.54ms
step:625/1770 val_loss:3.6675 train_time:58853ms step_avg:95.70ms
step:626/1770 train_time:58874ms step_avg:95.57ms
step:627/1770 train_time:58962ms step_avg:95.56ms
step:628/1770 train_time:59061ms step_avg:95.57ms
step:629/1770 train_time:59159ms step_avg:95.57ms
step:630/1770 train_time:59256ms step_avg:95.57ms
step:631/1770 train_time:59353ms step_avg:95.58ms
step:632/1770 train_time:59450ms step_avg:95.58ms
step:633/1770 train_time:59547ms step_avg:95.58ms
step:634/1770 train_time:59644ms step_avg:95.58ms
step:635/1770 train_time:59741ms step_avg:95.59ms
step:636/1770 train_time:59839ms step_avg:95.59ms
step:637/1770 train_time:59937ms step_avg:95.59ms
step:638/1770 train_time:60034ms step_avg:95.60ms
step:639/1770 train_time:60132ms step_avg:95.60ms
step:640/1770 train_time:60230ms step_avg:95.60ms
step:641/1770 train_time:60327ms step_avg:95.61ms
step:642/1770 train_time:60424ms step_avg:95.61ms
step:643/1770 train_time:60521ms step_avg:95.61ms
step:644/1770 train_time:60618ms step_avg:95.61ms
step:645/1770 train_time:60715ms step_avg:95.61ms
step:646/1770 train_time:60812ms step_avg:95.62ms
step:647/1770 train_time:60909ms step_avg:95.62ms
step:648/1770 train_time:61006ms step_avg:95.62ms
step:649/1770 train_time:61104ms step_avg:95.62ms
step:650/1770 train_time:61201ms step_avg:95.63ms
step:651/1770 train_time:61299ms step_avg:95.63ms
step:652/1770 train_time:61397ms step_avg:95.63ms
step:653/1770 train_time:61495ms step_avg:95.64ms
step:654/1770 train_time:61592ms step_avg:95.64ms
step:655/1770 train_time:61690ms step_avg:95.64ms
step:656/1770 train_time:61787ms step_avg:95.64ms
step:657/1770 train_time:61884ms step_avg:95.65ms
step:658/1770 train_time:61982ms step_avg:95.65ms
step:659/1770 train_time:62082ms step_avg:95.66ms
step:660/1770 train_time:62182ms step_avg:95.66ms
step:661/1770 train_time:62281ms step_avg:95.67ms
step:662/1770 train_time:62381ms step_avg:95.68ms
step:663/1770 train_time:62480ms step_avg:95.68ms
step:664/1770 train_time:62579ms step_avg:95.69ms
step:665/1770 train_time:62679ms step_avg:95.69ms
step:666/1770 train_time:62778ms step_avg:95.70ms
step:667/1770 train_time:62878ms step_avg:95.70ms
step:668/1770 train_time:62977ms step_avg:95.71ms
step:669/1770 train_time:63077ms step_avg:95.72ms
step:670/1770 train_time:63177ms step_avg:95.72ms
step:671/1770 train_time:63276ms step_avg:95.73ms
step:672/1770 train_time:63375ms step_avg:95.73ms
step:673/1770 train_time:63475ms step_avg:95.74ms
step:674/1770 train_time:63574ms step_avg:95.74ms
step:675/1770 train_time:63674ms step_avg:95.75ms
step:676/1770 train_time:63773ms step_avg:95.75ms
step:677/1770 train_time:63872ms step_avg:95.76ms
step:678/1770 train_time:63971ms step_avg:95.77ms
step:679/1770 train_time:64070ms step_avg:95.77ms
step:680/1770 train_time:64168ms step_avg:95.77ms
step:681/1770 train_time:64267ms step_avg:95.78ms
step:682/1770 train_time:64366ms step_avg:95.78ms
step:683/1770 train_time:64465ms step_avg:95.79ms
step:684/1770 train_time:64564ms step_avg:95.79ms
step:685/1770 train_time:64663ms step_avg:95.80ms
step:686/1770 train_time:64763ms step_avg:95.80ms
step:687/1770 train_time:64862ms step_avg:95.81ms
step:688/1770 train_time:64961ms step_avg:95.81ms
step:689/1770 train_time:65060ms step_avg:95.82ms
step:690/1770 train_time:65160ms step_avg:95.82ms
step:691/1770 train_time:65259ms step_avg:95.83ms
step:692/1770 train_time:65359ms step_avg:95.83ms
step:693/1770 train_time:65458ms step_avg:95.84ms
step:694/1770 train_time:65557ms step_avg:95.84ms
step:695/1770 train_time:65656ms step_avg:95.85ms
step:696/1770 train_time:65755ms step_avg:95.85ms
step:697/1770 train_time:65855ms step_avg:95.86ms
step:698/1770 train_time:65954ms step_avg:95.86ms
step:699/1770 train_time:66053ms step_avg:95.87ms
step:700/1770 train_time:66152ms step_avg:95.87ms
step:701/1770 train_time:66251ms step_avg:95.88ms
step:702/1770 train_time:66350ms step_avg:95.88ms
step:703/1770 train_time:66449ms step_avg:95.89ms
step:704/1770 train_time:66548ms step_avg:95.89ms
step:705/1770 train_time:66647ms step_avg:95.90ms
step:706/1770 train_time:66746ms step_avg:95.90ms
step:707/1770 train_time:66845ms step_avg:95.90ms
step:708/1770 train_time:66944ms step_avg:95.91ms
step:709/1770 train_time:67043ms step_avg:95.91ms
step:710/1770 train_time:67143ms step_avg:95.92ms
step:711/1770 train_time:67243ms step_avg:95.92ms
step:712/1770 train_time:67342ms step_avg:95.93ms
step:713/1770 train_time:67442ms step_avg:95.93ms
step:714/1770 train_time:67541ms step_avg:95.94ms
step:715/1770 train_time:67640ms step_avg:95.94ms
step:716/1770 train_time:67739ms step_avg:95.95ms
step:717/1770 train_time:67838ms step_avg:95.95ms
step:718/1770 train_time:67936ms step_avg:95.96ms
step:719/1770 train_time:68036ms step_avg:95.96ms
step:720/1770 train_time:68136ms step_avg:95.97ms
step:721/1770 train_time:68235ms step_avg:95.97ms
step:722/1770 train_time:68334ms step_avg:95.98ms
step:723/1770 train_time:68434ms step_avg:95.98ms
step:724/1770 train_time:68533ms step_avg:95.98ms
step:725/1770 train_time:68633ms step_avg:95.99ms
step:726/1770 train_time:68732ms step_avg:95.99ms
step:727/1770 train_time:68831ms step_avg:96.00ms
step:728/1770 train_time:68929ms step_avg:96.00ms
step:729/1770 train_time:69029ms step_avg:96.01ms
step:730/1770 train_time:69127ms step_avg:96.01ms
step:731/1770 train_time:69227ms step_avg:96.02ms
step:732/1770 train_time:69326ms step_avg:96.02ms
step:733/1770 train_time:69424ms step_avg:96.02ms
step:734/1770 train_time:69523ms step_avg:96.03ms
step:735/1770 train_time:69623ms step_avg:96.03ms
step:736/1770 train_time:69722ms step_avg:96.04ms
step:737/1770 train_time:69821ms step_avg:96.04ms
step:738/1770 train_time:69922ms step_avg:96.05ms
step:739/1770 train_time:70021ms step_avg:96.05ms
step:740/1770 train_time:70121ms step_avg:96.06ms
step:741/1770 train_time:70221ms step_avg:96.06ms
step:742/1770 train_time:70320ms step_avg:96.07ms
step:743/1770 train_time:70419ms step_avg:96.07ms
step:744/1770 train_time:70518ms step_avg:96.07ms
step:745/1770 train_time:70617ms step_avg:96.08ms
step:746/1770 train_time:70716ms step_avg:96.08ms
step:747/1770 train_time:70816ms step_avg:96.09ms
step:748/1770 train_time:70915ms step_avg:96.09ms
step:749/1770 train_time:71014ms step_avg:96.09ms
step:750/1770 train_time:71113ms step_avg:96.10ms
step:750/1770 val_loss:3.6025 train_time:71210ms step_avg:96.23ms
step:751/1770 train_time:71231ms step_avg:96.13ms
step:752/1770 train_time:71321ms step_avg:96.12ms
step:753/1770 train_time:71422ms step_avg:96.13ms
step:754/1770 train_time:71521ms step_avg:96.13ms
step:755/1770 train_time:71620ms step_avg:96.13ms
step:756/1770 train_time:71718ms step_avg:96.14ms
step:757/1770 train_time:71817ms step_avg:96.14ms
step:758/1770 train_time:71915ms step_avg:96.14ms
step:759/1770 train_time:72014ms step_avg:96.15ms
step:760/1770 train_time:72112ms step_avg:96.15ms
step:761/1770 train_time:72212ms step_avg:96.15ms
step:762/1770 train_time:72312ms step_avg:96.16ms
step:763/1770 train_time:72411ms step_avg:96.16ms
step:764/1770 train_time:72511ms step_avg:96.17ms
step:765/1770 train_time:72610ms step_avg:96.17ms
step:766/1770 train_time:72709ms step_avg:96.18ms
step:767/1770 train_time:72808ms step_avg:96.18ms
step:768/1770 train_time:72906ms step_avg:96.18ms
step:769/1770 train_time:73005ms step_avg:96.19ms
step:770/1770 train_time:73104ms step_avg:96.19ms
step:771/1770 train_time:73202ms step_avg:96.19ms
step:772/1770 train_time:73302ms step_avg:96.20ms
step:773/1770 train_time:73402ms step_avg:96.20ms
step:774/1770 train_time:73502ms step_avg:96.21ms
step:775/1770 train_time:73602ms step_avg:96.21ms
step:776/1770 train_time:73701ms step_avg:96.22ms
step:777/1770 train_time:73801ms step_avg:96.22ms
step:778/1770 train_time:73899ms step_avg:96.22ms
step:779/1770 train_time:73998ms step_avg:96.23ms
step:780/1770 train_time:74097ms step_avg:96.23ms
step:781/1770 train_time:74196ms step_avg:96.23ms
step:782/1770 train_time:74295ms step_avg:96.24ms
step:783/1770 train_time:74395ms step_avg:96.24ms
step:784/1770 train_time:74495ms step_avg:96.25ms
step:785/1770 train_time:74594ms step_avg:96.25ms
step:786/1770 train_time:74693ms step_avg:96.25ms
step:787/1770 train_time:74792ms step_avg:96.26ms
step:788/1770 train_time:74891ms step_avg:96.26ms
step:789/1770 train_time:74989ms step_avg:96.26ms
step:790/1770 train_time:75088ms step_avg:96.27ms
step:791/1770 train_time:75187ms step_avg:96.27ms
step:792/1770 train_time:75286ms step_avg:96.27ms
step:793/1770 train_time:75387ms step_avg:96.28ms
step:794/1770 train_time:75487ms step_avg:96.28ms
step:795/1770 train_time:75587ms step_avg:96.29ms
step:796/1770 train_time:75687ms step_avg:96.29ms
step:797/1770 train_time:75787ms step_avg:96.30ms
step:798/1770 train_time:75887ms step_avg:96.30ms
step:799/1770 train_time:75987ms step_avg:96.31ms
step:800/1770 train_time:76086ms step_avg:96.31ms
step:801/1770 train_time:76185ms step_avg:96.31ms
step:802/1770 train_time:76284ms step_avg:96.32ms
step:803/1770 train_time:76383ms step_avg:96.32ms
step:804/1770 train_time:76482ms step_avg:96.33ms
step:805/1770 train_time:76581ms step_avg:96.33ms
step:806/1770 train_time:76681ms step_avg:96.33ms
step:807/1770 train_time:76780ms step_avg:96.34ms
step:808/1770 train_time:76880ms step_avg:96.34ms
step:809/1770 train_time:76980ms step_avg:96.35ms
step:810/1770 train_time:77080ms step_avg:96.35ms
step:811/1770 train_time:77181ms step_avg:96.36ms
step:812/1770 train_time:77281ms step_avg:96.36ms
step:813/1770 train_time:77381ms step_avg:96.37ms
step:814/1770 train_time:77481ms step_avg:96.37ms
step:815/1770 train_time:77580ms step_avg:96.37ms
step:816/1770 train_time:77679ms step_avg:96.38ms
step:817/1770 train_time:77778ms step_avg:96.38ms
step:818/1770 train_time:77878ms step_avg:96.38ms
step:819/1770 train_time:77977ms step_avg:96.39ms
step:820/1770 train_time:78076ms step_avg:96.39ms
step:821/1770 train_time:78175ms step_avg:96.39ms
step:822/1770 train_time:78275ms step_avg:96.40ms
step:823/1770 train_time:78375ms step_avg:96.40ms
step:824/1770 train_time:78475ms step_avg:96.41ms
step:825/1770 train_time:78574ms step_avg:96.41ms
step:826/1770 train_time:78673ms step_avg:96.41ms
step:827/1770 train_time:78772ms step_avg:96.42ms
step:828/1770 train_time:78870ms step_avg:96.42ms
step:829/1770 train_time:78969ms step_avg:96.42ms
step:830/1770 train_time:79068ms step_avg:96.42ms
step:831/1770 train_time:79167ms step_avg:96.43ms
step:832/1770 train_time:79268ms step_avg:96.43ms
step:833/1770 train_time:79367ms step_avg:96.44ms
step:834/1770 train_time:79467ms step_avg:96.44ms
step:835/1770 train_time:79566ms step_avg:96.44ms
step:836/1770 train_time:79667ms step_avg:96.45ms
step:837/1770 train_time:79768ms step_avg:96.45ms
step:838/1770 train_time:79867ms step_avg:96.46ms
step:839/1770 train_time:79966ms step_avg:96.46ms
step:840/1770 train_time:80066ms step_avg:96.46ms
step:841/1770 train_time:80165ms step_avg:96.47ms
step:842/1770 train_time:80265ms step_avg:96.47ms
step:843/1770 train_time:80364ms step_avg:96.47ms
step:844/1770 train_time:80463ms step_avg:96.48ms
step:845/1770 train_time:80562ms step_avg:96.48ms
step:846/1770 train_time:80661ms step_avg:96.48ms
step:847/1770 train_time:80760ms step_avg:96.49ms
step:848/1770 train_time:80860ms step_avg:96.49ms
step:849/1770 train_time:80959ms step_avg:96.50ms
step:850/1770 train_time:81059ms step_avg:96.50ms
step:851/1770 train_time:81159ms step_avg:96.50ms
step:852/1770 train_time:81259ms step_avg:96.51ms
step:853/1770 train_time:81358ms step_avg:96.51ms
step:854/1770 train_time:81458ms step_avg:96.51ms
step:855/1770 train_time:81558ms step_avg:96.52ms
step:856/1770 train_time:81657ms step_avg:96.52ms
step:857/1770 train_time:81756ms step_avg:96.52ms
step:858/1770 train_time:81855ms step_avg:96.53ms
step:859/1770 train_time:81955ms step_avg:96.53ms
step:860/1770 train_time:82054ms step_avg:96.53ms
step:861/1770 train_time:82154ms step_avg:96.54ms
step:862/1770 train_time:82253ms step_avg:96.54ms
step:863/1770 train_time:82352ms step_avg:96.54ms
step:864/1770 train_time:82451ms step_avg:96.55ms
step:865/1770 train_time:82550ms step_avg:96.55ms
step:866/1770 train_time:82649ms step_avg:96.55ms
step:867/1770 train_time:82749ms step_avg:96.56ms
step:868/1770 train_time:82848ms step_avg:96.56ms
step:869/1770 train_time:82947ms step_avg:96.56ms
step:870/1770 train_time:83046ms step_avg:96.57ms
step:871/1770 train_time:83146ms step_avg:96.57ms
step:872/1770 train_time:83246ms step_avg:96.57ms
step:873/1770 train_time:83346ms step_avg:96.58ms
step:874/1770 train_time:83447ms step_avg:96.58ms
step:875/1770 train_time:83546ms step_avg:96.58ms
step:875/1770 val_loss:3.5538 train_time:83644ms step_avg:96.70ms
step:876/1770 train_time:83665ms step_avg:96.61ms
step:877/1770 train_time:83754ms step_avg:96.60ms
step:878/1770 train_time:83853ms step_avg:96.61ms
step:879/1770 train_time:83953ms step_avg:96.61ms
step:880/1770 train_time:84051ms step_avg:96.61ms
step:881/1770 train_time:84150ms step_avg:96.61ms
step:882/1770 train_time:84248ms step_avg:96.62ms
step:883/1770 train_time:84347ms step_avg:96.62ms
step:884/1770 train_time:84446ms step_avg:96.62ms
step:885/1770 train_time:84544ms step_avg:96.62ms
step:886/1770 train_time:84643ms step_avg:96.62ms
step:887/1770 train_time:84744ms step_avg:96.63ms
step:888/1770 train_time:84844ms step_avg:96.63ms
step:889/1770 train_time:84944ms step_avg:96.64ms
step:890/1770 train_time:85044ms step_avg:96.64ms
step:891/1770 train_time:85143ms step_avg:96.64ms
step:892/1770 train_time:85242ms step_avg:96.65ms
step:893/1770 train_time:85342ms step_avg:96.65ms
step:894/1770 train_time:85442ms step_avg:96.65ms
step:895/1770 train_time:85541ms step_avg:96.66ms
step:896/1770 train_time:85640ms step_avg:96.66ms
step:897/1770 train_time:85739ms step_avg:96.66ms
step:898/1770 train_time:85839ms step_avg:96.67ms
step:899/1770 train_time:85939ms step_avg:96.67ms
step:900/1770 train_time:86038ms step_avg:96.67ms
step:901/1770 train_time:86138ms step_avg:96.68ms
step:902/1770 train_time:86239ms step_avg:96.68ms
step:903/1770 train_time:86338ms step_avg:96.68ms
step:904/1770 train_time:86438ms step_avg:96.69ms
step:905/1770 train_time:86537ms step_avg:96.69ms
step:906/1770 train_time:86636ms step_avg:96.69ms
step:907/1770 train_time:86735ms step_avg:96.69ms
step:908/1770 train_time:86835ms step_avg:96.70ms
step:909/1770 train_time:86935ms step_avg:96.70ms
step:910/1770 train_time:87035ms step_avg:96.71ms
step:911/1770 train_time:87134ms step_avg:96.71ms
step:912/1770 train_time:87234ms step_avg:96.71ms
step:913/1770 train_time:87334ms step_avg:96.71ms
step:914/1770 train_time:87433ms step_avg:96.72ms
step:915/1770 train_time:87533ms step_avg:96.72ms
step:916/1770 train_time:87633ms step_avg:96.72ms
step:917/1770 train_time:87732ms step_avg:96.73ms
step:918/1770 train_time:87832ms step_avg:96.73ms
step:919/1770 train_time:87932ms step_avg:96.73ms
step:920/1770 train_time:88034ms step_avg:96.74ms
step:921/1770 train_time:88135ms step_avg:96.75ms
step:922/1770 train_time:88237ms step_avg:96.75ms
step:923/1770 train_time:88337ms step_avg:96.76ms
step:924/1770 train_time:88438ms step_avg:96.76ms
step:925/1770 train_time:88538ms step_avg:96.76ms
step:926/1770 train_time:88638ms step_avg:96.77ms
step:927/1770 train_time:88739ms step_avg:96.77ms
step:928/1770 train_time:88840ms step_avg:96.78ms
step:929/1770 train_time:88941ms step_avg:96.78ms
step:930/1770 train_time:89042ms step_avg:96.78ms
step:931/1770 train_time:89142ms step_avg:96.79ms
step:932/1770 train_time:89244ms step_avg:96.79ms
step:933/1770 train_time:89345ms step_avg:96.80ms
step:934/1770 train_time:89447ms step_avg:96.80ms
step:935/1770 train_time:89547ms step_avg:96.81ms
step:936/1770 train_time:89649ms step_avg:96.81ms
step:937/1770 train_time:89748ms step_avg:96.82ms
step:938/1770 train_time:89849ms step_avg:96.82ms
step:939/1770 train_time:89949ms step_avg:96.82ms
step:940/1770 train_time:90049ms step_avg:96.83ms
step:941/1770 train_time:90150ms step_avg:96.83ms
step:942/1770 train_time:90251ms step_avg:96.84ms
step:943/1770 train_time:90352ms step_avg:96.84ms
step:944/1770 train_time:90453ms step_avg:96.85ms
step:945/1770 train_time:90554ms step_avg:96.85ms
step:946/1770 train_time:90655ms step_avg:96.85ms
step:947/1770 train_time:90755ms step_avg:96.86ms
step:948/1770 train_time:90856ms step_avg:96.86ms
step:949/1770 train_time:90959ms step_avg:96.87ms
step:950/1770 train_time:91061ms step_avg:96.87ms
step:951/1770 train_time:91162ms step_avg:96.88ms
step:952/1770 train_time:91262ms step_avg:96.88ms
step:953/1770 train_time:91365ms step_avg:96.89ms
step:954/1770 train_time:91465ms step_avg:96.89ms
step:955/1770 train_time:91568ms step_avg:96.90ms
step:956/1770 train_time:91669ms step_avg:96.90ms
step:957/1770 train_time:91769ms step_avg:96.91ms
step:958/1770 train_time:91869ms step_avg:96.91ms
step:959/1770 train_time:91970ms step_avg:96.91ms
step:960/1770 train_time:92070ms step_avg:96.92ms
step:961/1770 train_time:92170ms step_avg:96.92ms
step:962/1770 train_time:92273ms step_avg:96.93ms
step:963/1770 train_time:92374ms step_avg:96.93ms
step:964/1770 train_time:92475ms step_avg:96.93ms
step:965/1770 train_time:92577ms step_avg:96.94ms
step:966/1770 train_time:92678ms step_avg:96.94ms
step:967/1770 train_time:92779ms step_avg:96.95ms
step:968/1770 train_time:92880ms step_avg:96.95ms
step:969/1770 train_time:92981ms step_avg:96.96ms
step:970/1770 train_time:93081ms step_avg:96.96ms
step:971/1770 train_time:93182ms step_avg:96.96ms
step:972/1770 train_time:93283ms step_avg:96.97ms
step:973/1770 train_time:93384ms step_avg:96.97ms
step:974/1770 train_time:93485ms step_avg:96.98ms
step:975/1770 train_time:93586ms step_avg:96.98ms
step:976/1770 train_time:93687ms step_avg:96.98ms
step:977/1770 train_time:93787ms step_avg:96.99ms
step:978/1770 train_time:93888ms step_avg:96.99ms
step:979/1770 train_time:93988ms step_avg:96.99ms
step:980/1770 train_time:94088ms step_avg:97.00ms
step:981/1770 train_time:94188ms step_avg:97.00ms
step:982/1770 train_time:94288ms step_avg:97.00ms
step:983/1770 train_time:94389ms step_avg:97.01ms
step:984/1770 train_time:94490ms step_avg:97.01ms
step:985/1770 train_time:94590ms step_avg:97.02ms
step:986/1770 train_time:94690ms step_avg:97.02ms
step:987/1770 train_time:94790ms step_avg:97.02ms
step:988/1770 train_time:94891ms step_avg:97.03ms
step:989/1770 train_time:94993ms step_avg:97.03ms
step:990/1770 train_time:95093ms step_avg:97.03ms
step:991/1770 train_time:95195ms step_avg:97.04ms
step:992/1770 train_time:95296ms step_avg:97.04ms
step:993/1770 train_time:95398ms step_avg:97.05ms
step:994/1770 train_time:95499ms step_avg:97.05ms
step:995/1770 train_time:95600ms step_avg:97.06ms
step:996/1770 train_time:95701ms step_avg:97.06ms
step:997/1770 train_time:95802ms step_avg:97.06ms
step:998/1770 train_time:95902ms step_avg:97.07ms
step:999/1770 train_time:96003ms step_avg:97.07ms
step:1000/1770 train_time:96104ms step_avg:97.07ms
step:1000/1770 val_loss:3.5159 train_time:96203ms step_avg:97.17ms
step:1001/1770 train_time:96224ms step_avg:97.10ms
step:1002/1770 train_time:96316ms step_avg:97.09ms
step:1003/1770 train_time:96418ms step_avg:97.10ms
step:1004/1770 train_time:96519ms step_avg:97.10ms
step:1005/1770 train_time:96619ms step_avg:97.10ms
step:1006/1770 train_time:96718ms step_avg:97.11ms
step:1007/1770 train_time:96818ms step_avg:97.11ms
step:1008/1770 train_time:96918ms step_avg:97.11ms
step:1009/1770 train_time:97018ms step_avg:97.12ms
step:1010/1770 train_time:97117ms step_avg:97.12ms
step:1011/1770 train_time:97219ms step_avg:97.12ms
step:1012/1770 train_time:97321ms step_avg:97.13ms
step:1013/1770 train_time:97422ms step_avg:97.13ms
step:1014/1770 train_time:97523ms step_avg:97.13ms
step:1015/1770 train_time:97623ms step_avg:97.14ms
step:1016/1770 train_time:97724ms step_avg:97.14ms
step:1017/1770 train_time:97824ms step_avg:97.14ms
step:1018/1770 train_time:97925ms step_avg:97.15ms
step:1019/1770 train_time:98025ms step_avg:97.15ms
step:1020/1770 train_time:98126ms step_avg:97.15ms
step:1021/1770 train_time:98228ms step_avg:97.16ms
step:1022/1770 train_time:98330ms step_avg:97.16ms
step:1023/1770 train_time:98431ms step_avg:97.17ms
step:1024/1770 train_time:98532ms step_avg:97.17ms
step:1025/1770 train_time:98632ms step_avg:97.17ms
step:1026/1770 train_time:98734ms step_avg:97.18ms
step:1027/1770 train_time:98835ms step_avg:97.18ms
step:1028/1770 train_time:98936ms step_avg:97.19ms
step:1029/1770 train_time:99037ms step_avg:97.19ms
step:1030/1770 train_time:99139ms step_avg:97.20ms
step:1031/1770 train_time:99240ms step_avg:97.20ms
step:1032/1770 train_time:99341ms step_avg:97.20ms
step:1033/1770 train_time:99441ms step_avg:97.21ms
step:1034/1770 train_time:99541ms step_avg:97.21ms
step:1035/1770 train_time:99641ms step_avg:97.21ms
step:1036/1770 train_time:99741ms step_avg:97.21ms
step:1037/1770 train_time:99841ms step_avg:97.22ms
step:1038/1770 train_time:99942ms step_avg:97.22ms
step:1039/1770 train_time:100043ms step_avg:97.22ms
step:1040/1770 train_time:100144ms step_avg:97.23ms
step:1041/1770 train_time:100244ms step_avg:97.23ms
step:1042/1770 train_time:100345ms step_avg:97.23ms
step:1043/1770 train_time:100446ms step_avg:97.24ms
step:1044/1770 train_time:100547ms step_avg:97.24ms
step:1045/1770 train_time:100648ms step_avg:97.24ms
step:1046/1770 train_time:100749ms step_avg:97.25ms
step:1047/1770 train_time:100850ms step_avg:97.25ms
step:1048/1770 train_time:100951ms step_avg:97.26ms
step:1049/1770 train_time:101051ms step_avg:97.26ms
step:1050/1770 train_time:101153ms step_avg:97.26ms
step:1051/1770 train_time:101255ms step_avg:97.27ms
step:1052/1770 train_time:101356ms step_avg:97.27ms
step:1053/1770 train_time:101457ms step_avg:97.27ms
step:1054/1770 train_time:101559ms step_avg:97.28ms
step:1055/1770 train_time:101660ms step_avg:97.28ms
step:1056/1770 train_time:101760ms step_avg:97.28ms
step:1057/1770 train_time:101861ms step_avg:97.29ms
step:1058/1770 train_time:101961ms step_avg:97.29ms
step:1059/1770 train_time:102061ms step_avg:97.29ms
step:1060/1770 train_time:102162ms step_avg:97.30ms
step:1061/1770 train_time:102263ms step_avg:97.30ms
step:1062/1770 train_time:102365ms step_avg:97.31ms
step:1063/1770 train_time:102467ms step_avg:97.31ms
step:1064/1770 train_time:102568ms step_avg:97.31ms
step:1065/1770 train_time:102670ms step_avg:97.32ms
step:1066/1770 train_time:102771ms step_avg:97.32ms
step:1067/1770 train_time:102873ms step_avg:97.33ms
step:1068/1770 train_time:102974ms step_avg:97.33ms
step:1069/1770 train_time:103075ms step_avg:97.33ms
step:1070/1770 train_time:103176ms step_avg:97.34ms
step:1071/1770 train_time:103277ms step_avg:97.34ms
step:1072/1770 train_time:103378ms step_avg:97.34ms
step:1073/1770 train_time:103478ms step_avg:97.35ms
step:1074/1770 train_time:103579ms step_avg:97.35ms
step:1075/1770 train_time:103680ms step_avg:97.35ms
step:1076/1770 train_time:103781ms step_avg:97.36ms
step:1077/1770 train_time:103882ms step_avg:97.36ms
step:1078/1770 train_time:103983ms step_avg:97.36ms
step:1079/1770 train_time:104084ms step_avg:97.37ms
step:1080/1770 train_time:104185ms step_avg:97.37ms
step:1081/1770 train_time:104286ms step_avg:97.37ms
step:1082/1770 train_time:104388ms step_avg:97.38ms
step:1083/1770 train_time:104489ms step_avg:97.38ms
step:1084/1770 train_time:104591ms step_avg:97.38ms
step:1085/1770 train_time:104692ms step_avg:97.39ms
step:1086/1770 train_time:104793ms step_avg:97.39ms
step:1087/1770 train_time:104895ms step_avg:97.40ms
step:1088/1770 train_time:104996ms step_avg:97.40ms
step:1089/1770 train_time:105097ms step_avg:97.40ms
step:1090/1770 train_time:105199ms step_avg:97.41ms
step:1091/1770 train_time:105300ms step_avg:97.41ms
step:1092/1770 train_time:105400ms step_avg:97.41ms
step:1093/1770 train_time:105500ms step_avg:97.41ms
step:1094/1770 train_time:105601ms step_avg:97.42ms
step:1095/1770 train_time:105701ms step_avg:97.42ms
step:1096/1770 train_time:105801ms step_avg:97.42ms
step:1097/1770 train_time:105902ms step_avg:97.43ms
step:1098/1770 train_time:106003ms step_avg:97.43ms
step:1099/1770 train_time:106104ms step_avg:97.43ms
step:1100/1770 train_time:106205ms step_avg:97.44ms
step:1101/1770 train_time:106306ms step_avg:97.44ms
step:1102/1770 train_time:106408ms step_avg:97.44ms
step:1103/1770 train_time:106509ms step_avg:97.45ms
step:1104/1770 train_time:106610ms step_avg:97.45ms
step:1105/1770 train_time:106711ms step_avg:97.45ms
step:1106/1770 train_time:106812ms step_avg:97.46ms
step:1107/1770 train_time:106913ms step_avg:97.46ms
step:1108/1770 train_time:107014ms step_avg:97.46ms
step:1109/1770 train_time:107116ms step_avg:97.47ms
step:1110/1770 train_time:107218ms step_avg:97.47ms
step:1111/1770 train_time:107319ms step_avg:97.47ms
step:1112/1770 train_time:107420ms step_avg:97.48ms
step:1113/1770 train_time:107520ms step_avg:97.48ms
step:1114/1770 train_time:107621ms step_avg:97.48ms
step:1115/1770 train_time:107722ms step_avg:97.49ms
step:1116/1770 train_time:107823ms step_avg:97.49ms
step:1117/1770 train_time:107924ms step_avg:97.49ms
step:1118/1770 train_time:108026ms step_avg:97.50ms
step:1119/1770 train_time:108127ms step_avg:97.50ms
step:1120/1770 train_time:108229ms step_avg:97.50ms
step:1121/1770 train_time:108331ms step_avg:97.51ms
step:1122/1770 train_time:108432ms step_avg:97.51ms
step:1123/1770 train_time:108532ms step_avg:97.51ms
step:1124/1770 train_time:108633ms step_avg:97.52ms
step:1125/1770 train_time:108735ms step_avg:97.52ms
step:1125/1770 val_loss:3.4734 train_time:108835ms step_avg:97.61ms
step:1126/1770 train_time:108858ms step_avg:97.54ms
step:1127/1770 train_time:108946ms step_avg:97.53ms
step:1128/1770 train_time:109047ms step_avg:97.54ms
step:1129/1770 train_time:109148ms step_avg:97.54ms
step:1130/1770 train_time:109248ms step_avg:97.54ms
step:1131/1770 train_time:109349ms step_avg:97.55ms
step:1132/1770 train_time:109449ms step_avg:97.55ms
step:1133/1770 train_time:109549ms step_avg:97.55ms
step:1134/1770 train_time:109649ms step_avg:97.55ms
step:1135/1770 train_time:109749ms step_avg:97.55ms
step:1136/1770 train_time:109850ms step_avg:97.56ms
step:1137/1770 train_time:109954ms step_avg:97.56ms
step:1138/1770 train_time:110055ms step_avg:97.57ms
step:1139/1770 train_time:110156ms step_avg:97.57ms
step:1140/1770 train_time:110256ms step_avg:97.57ms
step:1141/1770 train_time:110357ms step_avg:97.57ms
step:1142/1770 train_time:110458ms step_avg:97.58ms
step:1143/1770 train_time:110559ms step_avg:97.58ms
step:1144/1770 train_time:110660ms step_avg:97.58ms
step:1145/1770 train_time:110761ms step_avg:97.59ms
step:1146/1770 train_time:110863ms step_avg:97.59ms
step:1147/1770 train_time:110967ms step_avg:97.60ms
step:1148/1770 train_time:111068ms step_avg:97.60ms
step:1149/1770 train_time:111168ms step_avg:97.60ms
step:1150/1770 train_time:111268ms step_avg:97.60ms
step:1151/1770 train_time:111368ms step_avg:97.61ms
step:1152/1770 train_time:111469ms step_avg:97.61ms
step:1153/1770 train_time:111570ms step_avg:97.61ms
step:1154/1770 train_time:111671ms step_avg:97.61ms
step:1155/1770 train_time:111772ms step_avg:97.62ms
step:1156/1770 train_time:111873ms step_avg:97.62ms
step:1157/1770 train_time:111976ms step_avg:97.62ms
step:1158/1770 train_time:112078ms step_avg:97.63ms
step:1159/1770 train_time:112179ms step_avg:97.63ms
step:1160/1770 train_time:112279ms step_avg:97.63ms
step:1161/1770 train_time:112380ms step_avg:97.64ms
step:1162/1770 train_time:112481ms step_avg:97.64ms
step:1163/1770 train_time:112582ms step_avg:97.64ms
step:1164/1770 train_time:112683ms step_avg:97.65ms
step:1165/1770 train_time:112785ms step_avg:97.65ms
step:1166/1770 train_time:112887ms step_avg:97.65ms
step:1167/1770 train_time:112988ms step_avg:97.66ms
step:1168/1770 train_time:113089ms step_avg:97.66ms
step:1169/1770 train_time:113189ms step_avg:97.66ms
step:1170/1770 train_time:113290ms step_avg:97.66ms
step:1171/1770 train_time:113391ms step_avg:97.67ms
step:1172/1770 train_time:113492ms step_avg:97.67ms
step:1173/1770 train_time:113593ms step_avg:97.67ms
step:1174/1770 train_time:113695ms step_avg:97.68ms
step:1175/1770 train_time:113797ms step_avg:97.68ms
step:1176/1770 train_time:113900ms step_avg:97.68ms
step:1177/1770 train_time:114001ms step_avg:97.69ms
step:1178/1770 train_time:114102ms step_avg:97.69ms
step:1179/1770 train_time:114203ms step_avg:97.69ms
step:1180/1770 train_time:114304ms step_avg:97.70ms
step:1181/1770 train_time:114406ms step_avg:97.70ms
step:1182/1770 train_time:114507ms step_avg:97.70ms
step:1183/1770 train_time:114610ms step_avg:97.71ms
step:1184/1770 train_time:114712ms step_avg:97.71ms
step:1185/1770 train_time:114813ms step_avg:97.71ms
step:1186/1770 train_time:114916ms step_avg:97.72ms
step:1187/1770 train_time:115020ms step_avg:97.72ms
step:1188/1770 train_time:115121ms step_avg:97.73ms
step:1189/1770 train_time:115223ms step_avg:97.73ms
step:1190/1770 train_time:115324ms step_avg:97.73ms
step:1191/1770 train_time:115426ms step_avg:97.74ms
step:1192/1770 train_time:115528ms step_avg:97.74ms
step:1193/1770 train_time:115630ms step_avg:97.74ms
step:1194/1770 train_time:115732ms step_avg:97.75ms
step:1195/1770 train_time:115834ms step_avg:97.75ms
step:1196/1770 train_time:115937ms step_avg:97.75ms
step:1197/1770 train_time:116040ms step_avg:97.76ms
step:1198/1770 train_time:116142ms step_avg:97.76ms
step:1199/1770 train_time:116243ms step_avg:97.77ms
step:1200/1770 train_time:116345ms step_avg:97.77ms
step:1201/1770 train_time:116449ms step_avg:97.77ms
step:1202/1770 train_time:116551ms step_avg:97.78ms
step:1203/1770 train_time:116652ms step_avg:97.78ms
step:1204/1770 train_time:116755ms step_avg:97.78ms
step:1205/1770 train_time:116856ms step_avg:97.79ms
step:1206/1770 train_time:116960ms step_avg:97.79ms
step:1207/1770 train_time:117062ms step_avg:97.80ms
step:1208/1770 train_time:117163ms step_avg:97.80ms
step:1209/1770 train_time:117266ms step_avg:97.80ms
step:1210/1770 train_time:117368ms step_avg:97.81ms
step:1211/1770 train_time:117470ms step_avg:97.81ms
step:1212/1770 train_time:117574ms step_avg:97.82ms
step:1213/1770 train_time:117675ms step_avg:97.82ms
step:1214/1770 train_time:117777ms step_avg:97.82ms
step:1215/1770 train_time:117879ms step_avg:97.83ms
step:1216/1770 train_time:117984ms step_avg:97.83ms
step:1217/1770 train_time:118086ms step_avg:97.83ms
step:1218/1770 train_time:118187ms step_avg:97.84ms
step:1219/1770 train_time:118289ms step_avg:97.84ms
step:1220/1770 train_time:118392ms step_avg:97.84ms
step:1221/1770 train_time:118493ms step_avg:97.85ms
step:1222/1770 train_time:118596ms step_avg:97.85ms
step:1223/1770 train_time:118697ms step_avg:97.85ms
step:1224/1770 train_time:118800ms step_avg:97.86ms
step:1225/1770 train_time:118902ms step_avg:97.86ms
step:1226/1770 train_time:119004ms step_avg:97.87ms
step:1227/1770 train_time:119108ms step_avg:97.87ms
step:1228/1770 train_time:119211ms step_avg:97.87ms
step:1229/1770 train_time:119313ms step_avg:97.88ms
step:1230/1770 train_time:119415ms step_avg:97.88ms
step:1231/1770 train_time:119518ms step_avg:97.89ms
step:1232/1770 train_time:119620ms step_avg:97.89ms
step:1233/1770 train_time:119722ms step_avg:97.89ms
step:1234/1770 train_time:119824ms step_avg:97.90ms
step:1235/1770 train_time:119926ms step_avg:97.90ms
step:1236/1770 train_time:120028ms step_avg:97.90ms
step:1237/1770 train_time:120130ms step_avg:97.91ms
step:1238/1770 train_time:120233ms step_avg:97.91ms
step:1239/1770 train_time:120334ms step_avg:97.91ms
step:1240/1770 train_time:120436ms step_avg:97.92ms
step:1241/1770 train_time:120539ms step_avg:97.92ms
step:1242/1770 train_time:120641ms step_avg:97.92ms
step:1243/1770 train_time:120744ms step_avg:97.93ms
step:1244/1770 train_time:120845ms step_avg:97.93ms
step:1245/1770 train_time:120948ms step_avg:97.93ms
step:1246/1770 train_time:121050ms step_avg:97.94ms
step:1247/1770 train_time:121153ms step_avg:97.94ms
step:1248/1770 train_time:121256ms step_avg:97.94ms
step:1249/1770 train_time:121358ms step_avg:97.95ms
step:1250/1770 train_time:121459ms step_avg:97.95ms
step:1250/1770 val_loss:3.4249 train_time:121561ms step_avg:98.03ms
step:1251/1770 train_time:121582ms step_avg:97.97ms
step:1252/1770 train_time:121674ms step_avg:97.97ms
step:1253/1770 train_time:121776ms step_avg:97.97ms
step:1254/1770 train_time:121878ms step_avg:97.97ms
step:1255/1770 train_time:121982ms step_avg:97.98ms
step:1256/1770 train_time:122083ms step_avg:97.98ms
step:1257/1770 train_time:122185ms step_avg:97.98ms
step:1258/1770 train_time:122287ms step_avg:97.99ms
step:1259/1770 train_time:122389ms step_avg:97.99ms
step:1260/1770 train_time:122490ms step_avg:97.99ms
step:1261/1770 train_time:122593ms step_avg:98.00ms
step:1262/1770 train_time:122697ms step_avg:98.00ms
step:1263/1770 train_time:122799ms step_avg:98.00ms
step:1264/1770 train_time:122903ms step_avg:98.01ms
step:1265/1770 train_time:123005ms step_avg:98.01ms
step:1266/1770 train_time:123107ms step_avg:98.02ms
step:1267/1770 train_time:123209ms step_avg:98.02ms
step:1268/1770 train_time:123312ms step_avg:98.02ms
step:1269/1770 train_time:123412ms step_avg:98.02ms
step:1270/1770 train_time:123515ms step_avg:98.03ms
step:1271/1770 train_time:123618ms step_avg:98.03ms
step:1272/1770 train_time:123720ms step_avg:98.04ms
step:1273/1770 train_time:123823ms step_avg:98.04ms
step:1274/1770 train_time:123926ms step_avg:98.04ms
step:1275/1770 train_time:124028ms step_avg:98.05ms
step:1276/1770 train_time:124129ms step_avg:98.05ms
step:1277/1770 train_time:124231ms step_avg:98.05ms
step:1278/1770 train_time:124334ms step_avg:98.06ms
step:1279/1770 train_time:124436ms step_avg:98.06ms
step:1280/1770 train_time:124539ms step_avg:98.06ms
step:1281/1770 train_time:124641ms step_avg:98.07ms
step:1282/1770 train_time:124744ms step_avg:98.07ms
step:1283/1770 train_time:124848ms step_avg:98.07ms
step:1284/1770 train_time:124950ms step_avg:98.08ms
step:1285/1770 train_time:125052ms step_avg:98.08ms
step:1286/1770 train_time:125155ms step_avg:98.08ms
step:1287/1770 train_time:125259ms step_avg:98.09ms
step:1288/1770 train_time:125362ms step_avg:98.09ms
step:1289/1770 train_time:125464ms step_avg:98.10ms
step:1290/1770 train_time:125566ms step_avg:98.10ms
step:1291/1770 train_time:125668ms step_avg:98.10ms
step:1292/1770 train_time:125769ms step_avg:98.10ms
step:1293/1770 train_time:125871ms step_avg:98.11ms
step:1294/1770 train_time:125974ms step_avg:98.11ms
step:1295/1770 train_time:126076ms step_avg:98.11ms
step:1296/1770 train_time:126179ms step_avg:98.12ms
step:1297/1770 train_time:126281ms step_avg:98.12ms
step:1298/1770 train_time:126384ms step_avg:98.12ms
step:1299/1770 train_time:126486ms step_avg:98.13ms
step:1300/1770 train_time:126587ms step_avg:98.13ms
step:1301/1770 train_time:126689ms step_avg:98.13ms
step:1302/1770 train_time:126792ms step_avg:98.14ms
step:1303/1770 train_time:126893ms step_avg:98.14ms
step:1304/1770 train_time:126995ms step_avg:98.14ms
step:1305/1770 train_time:127098ms step_avg:98.14ms
step:1306/1770 train_time:127200ms step_avg:98.15ms
step:1307/1770 train_time:127302ms step_avg:98.15ms
step:1308/1770 train_time:127404ms step_avg:98.15ms
step:1309/1770 train_time:127507ms step_avg:98.16ms
step:1310/1770 train_time:127609ms step_avg:98.16ms
step:1311/1770 train_time:127710ms step_avg:98.16ms
step:1312/1770 train_time:127812ms step_avg:98.17ms
step:1313/1770 train_time:127913ms step_avg:98.17ms
step:1314/1770 train_time:128015ms step_avg:98.17ms
step:1315/1770 train_time:128118ms step_avg:98.17ms
step:1316/1770 train_time:128220ms step_avg:98.18ms
step:1317/1770 train_time:128323ms step_avg:98.18ms
step:1318/1770 train_time:128428ms step_avg:98.19ms
step:1319/1770 train_time:128530ms step_avg:98.19ms
step:1320/1770 train_time:128633ms step_avg:98.19ms
step:1321/1770 train_time:128735ms step_avg:98.20ms
step:1322/1770 train_time:128837ms step_avg:98.20ms
step:1323/1770 train_time:128941ms step_avg:98.20ms
step:1324/1770 train_time:129044ms step_avg:98.21ms
step:1325/1770 train_time:129146ms step_avg:98.21ms
step:1326/1770 train_time:129248ms step_avg:98.21ms
step:1327/1770 train_time:129352ms step_avg:98.22ms
step:1328/1770 train_time:129455ms step_avg:98.22ms
step:1329/1770 train_time:129557ms step_avg:98.22ms
step:1330/1770 train_time:129659ms step_avg:98.23ms
step:1331/1770 train_time:129760ms step_avg:98.23ms
step:1332/1770 train_time:129862ms step_avg:98.23ms
step:1333/1770 train_time:129964ms step_avg:98.23ms
step:1334/1770 train_time:130067ms step_avg:98.24ms
step:1335/1770 train_time:130169ms step_avg:98.24ms
step:1336/1770 train_time:130270ms step_avg:98.24ms
step:1337/1770 train_time:130372ms step_avg:98.25ms
step:1338/1770 train_time:130474ms step_avg:98.25ms
step:1339/1770 train_time:130577ms step_avg:98.25ms
step:1340/1770 train_time:130680ms step_avg:98.26ms
step:1341/1770 train_time:130782ms step_avg:98.26ms
step:1342/1770 train_time:130885ms step_avg:98.26ms
step:1343/1770 train_time:130988ms step_avg:98.27ms
step:1344/1770 train_time:131090ms step_avg:98.27ms
step:1345/1770 train_time:131192ms step_avg:98.27ms
step:1346/1770 train_time:131294ms step_avg:98.27ms
step:1347/1770 train_time:131397ms step_avg:98.28ms
step:1348/1770 train_time:131501ms step_avg:98.28ms
step:1349/1770 train_time:131604ms step_avg:98.29ms
step:1350/1770 train_time:131706ms step_avg:98.29ms
step:1351/1770 train_time:131808ms step_avg:98.29ms
step:1352/1770 train_time:131910ms step_avg:98.29ms
step:1353/1770 train_time:132013ms step_avg:98.30ms
step:1354/1770 train_time:132115ms step_avg:98.30ms
step:1355/1770 train_time:132217ms step_avg:98.30ms
step:1356/1770 train_time:132319ms step_avg:98.31ms
step:1357/1770 train_time:132421ms step_avg:98.31ms
step:1358/1770 train_time:132524ms step_avg:98.31ms
step:1359/1770 train_time:132626ms step_avg:98.31ms
step:1360/1770 train_time:132729ms step_avg:98.32ms
step:1361/1770 train_time:132831ms step_avg:98.32ms
step:1362/1770 train_time:132933ms step_avg:98.32ms
step:1363/1770 train_time:133036ms step_avg:98.33ms
step:1364/1770 train_time:133140ms step_avg:98.33ms
step:1365/1770 train_time:133242ms step_avg:98.33ms
step:1366/1770 train_time:133344ms step_avg:98.34ms
step:1367/1770 train_time:133447ms step_avg:98.34ms
step:1368/1770 train_time:133547ms step_avg:98.34ms
step:1369/1770 train_time:133650ms step_avg:98.34ms
step:1370/1770 train_time:133753ms step_avg:98.35ms
step:1371/1770 train_time:133855ms step_avg:98.35ms
step:1372/1770 train_time:133957ms step_avg:98.35ms
step:1373/1770 train_time:134059ms step_avg:98.36ms
step:1374/1770 train_time:134162ms step_avg:98.36ms
step:1375/1770 train_time:134266ms step_avg:98.36ms
step:1375/1770 val_loss:3.3812 train_time:134367ms step_avg:98.44ms
step:1376/1770 train_time:134388ms step_avg:98.38ms
step:1377/1770 train_time:134480ms step_avg:98.38ms
step:1378/1770 train_time:134581ms step_avg:98.38ms
step:1379/1770 train_time:134682ms step_avg:98.38ms
step:1380/1770 train_time:134784ms step_avg:98.38ms
step:1381/1770 train_time:134886ms step_avg:98.39ms
step:1382/1770 train_time:134988ms step_avg:98.39ms
step:1383/1770 train_time:135091ms step_avg:98.39ms
step:1384/1770 train_time:135193ms step_avg:98.39ms
step:1385/1770 train_time:135295ms step_avg:98.40ms
step:1386/1770 train_time:135398ms step_avg:98.40ms
step:1387/1770 train_time:135501ms step_avg:98.40ms
step:1388/1770 train_time:135603ms step_avg:98.41ms
step:1389/1770 train_time:135705ms step_avg:98.41ms
step:1390/1770 train_time:135807ms step_avg:98.41ms
step:1391/1770 train_time:135908ms step_avg:98.41ms
step:1392/1770 train_time:136011ms step_avg:98.42ms
step:1393/1770 train_time:136112ms step_avg:98.42ms
step:1394/1770 train_time:136214ms step_avg:98.42ms
step:1395/1770 train_time:136317ms step_avg:98.42ms
step:1396/1770 train_time:136420ms step_avg:98.43ms
step:1397/1770 train_time:136523ms step_avg:98.43ms
step:1398/1770 train_time:136624ms step_avg:98.43ms
step:1399/1770 train_time:136727ms step_avg:98.44ms
step:1400/1770 train_time:136830ms step_avg:98.44ms
step:1401/1770 train_time:136932ms step_avg:98.44ms
step:1402/1770 train_time:137035ms step_avg:98.44ms
step:1403/1770 train_time:137136ms step_avg:98.45ms
step:1404/1770 train_time:137239ms step_avg:98.45ms
step:1405/1770 train_time:137341ms step_avg:98.45ms
step:1406/1770 train_time:137443ms step_avg:98.45ms
step:1407/1770 train_time:137545ms step_avg:98.46ms
step:1408/1770 train_time:137647ms step_avg:98.46ms
step:1409/1770 train_time:137750ms step_avg:98.46ms
step:1410/1770 train_time:137852ms step_avg:98.47ms
step:1411/1770 train_time:137955ms step_avg:98.47ms
step:1412/1770 train_time:138058ms step_avg:98.47ms
step:1413/1770 train_time:138159ms step_avg:98.47ms
step:1414/1770 train_time:138261ms step_avg:98.48ms
step:1415/1770 train_time:138363ms step_avg:98.48ms
step:1416/1770 train_time:138468ms step_avg:98.48ms
step:1417/1770 train_time:138570ms step_avg:98.49ms
step:1418/1770 train_time:138672ms step_avg:98.49ms
step:1419/1770 train_time:138774ms step_avg:98.49ms
step:1420/1770 train_time:138877ms step_avg:98.49ms
step:1421/1770 train_time:138979ms step_avg:98.50ms
step:1422/1770 train_time:139082ms step_avg:98.50ms
step:1423/1770 train_time:139184ms step_avg:98.50ms
step:1424/1770 train_time:139286ms step_avg:98.51ms
step:1425/1770 train_time:139389ms step_avg:98.51ms
step:1426/1770 train_time:139492ms step_avg:98.51ms
step:1427/1770 train_time:139594ms step_avg:98.51ms
step:1428/1770 train_time:139697ms step_avg:98.52ms
step:1429/1770 train_time:139799ms step_avg:98.52ms
step:1430/1770 train_time:139900ms step_avg:98.52ms
step:1431/1770 train_time:140004ms step_avg:98.53ms
step:1432/1770 train_time:140107ms step_avg:98.53ms
step:1433/1770 train_time:140209ms step_avg:98.53ms
step:1434/1770 train_time:140310ms step_avg:98.53ms
step:1435/1770 train_time:140412ms step_avg:98.53ms
step:1436/1770 train_time:140515ms step_avg:98.54ms
step:1437/1770 train_time:140618ms step_avg:98.54ms
step:1438/1770 train_time:140719ms step_avg:98.54ms
step:1439/1770 train_time:140821ms step_avg:98.55ms
step:1440/1770 train_time:140923ms step_avg:98.55ms
step:1441/1770 train_time:141028ms step_avg:98.55ms
step:1442/1770 train_time:141129ms step_avg:98.55ms
step:1443/1770 train_time:141232ms step_avg:98.56ms
step:1444/1770 train_time:141334ms step_avg:98.56ms
step:1445/1770 train_time:141437ms step_avg:98.56ms
step:1446/1770 train_time:141540ms step_avg:98.57ms
step:1447/1770 train_time:141644ms step_avg:98.57ms
step:1448/1770 train_time:141747ms step_avg:98.57ms
step:1449/1770 train_time:141851ms step_avg:98.58ms
step:1450/1770 train_time:141954ms step_avg:98.58ms
step:1451/1770 train_time:142058ms step_avg:98.58ms
step:1452/1770 train_time:142161ms step_avg:98.59ms
step:1453/1770 train_time:142265ms step_avg:98.59ms
step:1454/1770 train_time:142368ms step_avg:98.59ms
step:1455/1770 train_time:142472ms step_avg:98.60ms
step:1456/1770 train_time:142576ms step_avg:98.60ms
step:1457/1770 train_time:142680ms step_avg:98.60ms
step:1458/1770 train_time:142784ms step_avg:98.61ms
step:1459/1770 train_time:142889ms step_avg:98.61ms
step:1460/1770 train_time:142992ms step_avg:98.61ms
step:1461/1770 train_time:143095ms step_avg:98.62ms
step:1462/1770 train_time:143197ms step_avg:98.62ms
step:1463/1770 train_time:143300ms step_avg:98.62ms
step:1464/1770 train_time:143404ms step_avg:98.63ms
step:1465/1770 train_time:143508ms step_avg:98.63ms
step:1466/1770 train_time:143612ms step_avg:98.63ms
step:1467/1770 train_time:143717ms step_avg:98.64ms
step:1468/1770 train_time:143820ms step_avg:98.64ms
step:1469/1770 train_time:143924ms step_avg:98.65ms
step:1470/1770 train_time:144027ms step_avg:98.65ms
step:1471/1770 train_time:144130ms step_avg:98.65ms
step:1472/1770 train_time:144233ms step_avg:98.65ms
step:1473/1770 train_time:144338ms step_avg:98.66ms
step:1474/1770 train_time:144441ms step_avg:98.66ms
step:1475/1770 train_time:144544ms step_avg:98.67ms
step:1476/1770 train_time:144647ms step_avg:98.67ms
step:1477/1770 train_time:144753ms step_avg:98.67ms
step:1478/1770 train_time:144857ms step_avg:98.68ms
step:1479/1770 train_time:144960ms step_avg:98.68ms
step:1480/1770 train_time:145063ms step_avg:98.68ms
step:1481/1770 train_time:145172ms step_avg:98.69ms
step:1482/1770 train_time:145274ms step_avg:98.69ms
step:1483/1770 train_time:145377ms step_avg:98.69ms
step:1484/1770 train_time:145480ms step_avg:98.70ms
step:1485/1770 train_time:145582ms step_avg:98.70ms
step:1486/1770 train_time:145686ms step_avg:98.70ms
step:1487/1770 train_time:145789ms step_avg:98.71ms
step:1488/1770 train_time:145893ms step_avg:98.71ms
step:1489/1770 train_time:145997ms step_avg:98.71ms
step:1490/1770 train_time:146101ms step_avg:98.72ms
step:1491/1770 train_time:146203ms step_avg:98.72ms
step:1492/1770 train_time:146307ms step_avg:98.72ms
step:1493/1770 train_time:146413ms step_avg:98.73ms
step:1494/1770 train_time:146519ms step_avg:98.73ms
step:1495/1770 train_time:146622ms step_avg:98.74ms
step:1496/1770 train_time:146724ms step_avg:98.74ms
step:1497/1770 train_time:146828ms step_avg:98.74ms
step:1498/1770 train_time:146930ms step_avg:98.74ms
step:1499/1770 train_time:147033ms step_avg:98.75ms
step:1500/1770 train_time:147136ms step_avg:98.75ms
step:1500/1770 val_loss:3.3436 train_time:147237ms step_avg:98.82ms
step:1501/1770 train_time:147259ms step_avg:98.77ms
step:1502/1770 train_time:147351ms step_avg:98.76ms
step:1503/1770 train_time:147454ms step_avg:98.76ms
step:1504/1770 train_time:147557ms step_avg:98.77ms
step:1505/1770 train_time:147662ms step_avg:98.77ms
step:1506/1770 train_time:147765ms step_avg:98.77ms
step:1507/1770 train_time:147868ms step_avg:98.78ms
step:1508/1770 train_time:147973ms step_avg:98.78ms
step:1509/1770 train_time:148076ms step_avg:98.78ms
step:1510/1770 train_time:148178ms step_avg:98.79ms
step:1511/1770 train_time:148282ms step_avg:98.79ms
step:1512/1770 train_time:148386ms step_avg:98.79ms
step:1513/1770 train_time:148490ms step_avg:98.80ms
step:1514/1770 train_time:148593ms step_avg:98.80ms
step:1515/1770 train_time:148696ms step_avg:98.80ms
step:1516/1770 train_time:148800ms step_avg:98.80ms
step:1517/1770 train_time:148903ms step_avg:98.81ms
step:1518/1770 train_time:149008ms step_avg:98.81ms
step:1519/1770 train_time:149111ms step_avg:98.81ms
step:1520/1770 train_time:149215ms step_avg:98.82ms
step:1521/1770 train_time:149318ms step_avg:98.82ms
step:1522/1770 train_time:149422ms step_avg:98.82ms
step:1523/1770 train_time:149526ms step_avg:98.83ms
step:1524/1770 train_time:149629ms step_avg:98.83ms
step:1525/1770 train_time:149732ms step_avg:98.83ms
step:1526/1770 train_time:149836ms step_avg:98.84ms
step:1527/1770 train_time:149939ms step_avg:98.84ms
step:1528/1770 train_time:150044ms step_avg:98.84ms
step:1529/1770 train_time:150147ms step_avg:98.85ms
step:1530/1770 train_time:150251ms step_avg:98.85ms
step:1531/1770 train_time:150354ms step_avg:98.85ms
step:1532/1770 train_time:150458ms step_avg:98.86ms
step:1533/1770 train_time:150562ms step_avg:98.86ms
step:1534/1770 train_time:150666ms step_avg:98.86ms
step:1535/1770 train_time:150768ms step_avg:98.86ms
step:1536/1770 train_time:150871ms step_avg:98.87ms
step:1537/1770 train_time:150975ms step_avg:98.87ms
step:1538/1770 train_time:151081ms step_avg:98.87ms
step:1539/1770 train_time:151183ms step_avg:98.88ms
step:1540/1770 train_time:151288ms step_avg:98.88ms
step:1541/1770 train_time:151393ms step_avg:98.88ms
step:1542/1770 train_time:151496ms step_avg:98.89ms
step:1543/1770 train_time:151599ms step_avg:98.89ms
step:1544/1770 train_time:151705ms step_avg:98.89ms
step:1545/1770 train_time:151808ms step_avg:98.90ms
step:1546/1770 train_time:151911ms step_avg:98.90ms
step:1547/1770 train_time:152015ms step_avg:98.90ms
step:1548/1770 train_time:152118ms step_avg:98.91ms
step:1549/1770 train_time:152221ms step_avg:98.91ms
step:1550/1770 train_time:152325ms step_avg:98.91ms
step:1551/1770 train_time:152428ms step_avg:98.92ms
step:1552/1770 train_time:152533ms step_avg:98.92ms
step:1553/1770 train_time:152636ms step_avg:98.92ms
step:1554/1770 train_time:152739ms step_avg:98.92ms
step:1555/1770 train_time:152843ms step_avg:98.93ms
step:1556/1770 train_time:152946ms step_avg:98.93ms
step:1557/1770 train_time:153049ms step_avg:98.93ms
step:1558/1770 train_time:153154ms step_avg:98.94ms
step:1559/1770 train_time:153257ms step_avg:98.94ms
step:1560/1770 train_time:153360ms step_avg:98.94ms
step:1561/1770 train_time:153464ms step_avg:98.95ms
step:1562/1770 train_time:153568ms step_avg:98.95ms
step:1563/1770 train_time:153672ms step_avg:98.95ms
step:1564/1770 train_time:153776ms step_avg:98.95ms
step:1565/1770 train_time:153879ms step_avg:98.96ms
step:1566/1770 train_time:153983ms step_avg:98.96ms
step:1567/1770 train_time:154086ms step_avg:98.96ms
step:1568/1770 train_time:154189ms step_avg:98.97ms
step:1569/1770 train_time:154297ms step_avg:98.97ms
step:1570/1770 train_time:154399ms step_avg:98.97ms
step:1571/1770 train_time:154502ms step_avg:98.98ms
step:1572/1770 train_time:154606ms step_avg:98.98ms
step:1573/1770 train_time:154711ms step_avg:98.98ms
step:1574/1770 train_time:154816ms step_avg:98.99ms
step:1575/1770 train_time:154918ms step_avg:98.99ms
step:1576/1770 train_time:155020ms step_avg:98.99ms
step:1577/1770 train_time:155125ms step_avg:98.99ms
step:1578/1770 train_time:155230ms step_avg:99.00ms
step:1579/1770 train_time:155334ms step_avg:99.00ms
step:1580/1770 train_time:155437ms step_avg:99.00ms
step:1581/1770 train_time:155544ms step_avg:99.01ms
step:1582/1770 train_time:155648ms step_avg:99.01ms
step:1583/1770 train_time:155751ms step_avg:99.02ms
step:1584/1770 train_time:155856ms step_avg:99.02ms
step:1585/1770 train_time:155960ms step_avg:99.02ms
step:1586/1770 train_time:156067ms step_avg:99.03ms
step:1587/1770 train_time:156171ms step_avg:99.03ms
step:1588/1770 train_time:156274ms step_avg:99.03ms
step:1589/1770 train_time:156379ms step_avg:99.04ms
step:1590/1770 train_time:156482ms step_avg:99.04ms
step:1591/1770 train_time:156585ms step_avg:99.04ms
step:1592/1770 train_time:156689ms step_avg:99.05ms
step:1593/1770 train_time:156793ms step_avg:99.05ms
step:1594/1770 train_time:156897ms step_avg:99.05ms
step:1595/1770 train_time:157000ms step_avg:99.05ms
step:1596/1770 train_time:157104ms step_avg:99.06ms
step:1597/1770 train_time:157207ms step_avg:99.06ms
step:1598/1770 train_time:157310ms step_avg:99.06ms
step:1599/1770 train_time:157414ms step_avg:99.06ms
step:1600/1770 train_time:157520ms step_avg:99.07ms
step:1601/1770 train_time:157624ms step_avg:99.07ms
step:1602/1770 train_time:157727ms step_avg:99.08ms
step:1603/1770 train_time:157832ms step_avg:99.08ms
step:1604/1770 train_time:157935ms step_avg:99.08ms
step:1605/1770 train_time:158037ms step_avg:99.08ms
step:1606/1770 train_time:158142ms step_avg:99.09ms
step:1607/1770 train_time:158248ms step_avg:99.09ms
step:1608/1770 train_time:158351ms step_avg:99.09ms
step:1609/1770 train_time:158455ms step_avg:99.10ms
step:1610/1770 train_time:158560ms step_avg:99.10ms
step:1611/1770 train_time:158665ms step_avg:99.10ms
step:1612/1770 train_time:158770ms step_avg:99.11ms
step:1613/1770 train_time:158873ms step_avg:99.11ms
step:1614/1770 train_time:158976ms step_avg:99.11ms
step:1615/1770 train_time:159080ms step_avg:99.12ms
step:1616/1770 train_time:159183ms step_avg:99.12ms
step:1617/1770 train_time:159288ms step_avg:99.12ms
step:1618/1770 train_time:159393ms step_avg:99.12ms
step:1619/1770 train_time:159497ms step_avg:99.13ms
step:1620/1770 train_time:159601ms step_avg:99.13ms
step:1621/1770 train_time:159705ms step_avg:99.13ms
step:1622/1770 train_time:159808ms step_avg:99.14ms
step:1623/1770 train_time:159915ms step_avg:99.14ms
step:1624/1770 train_time:160018ms step_avg:99.14ms
step:1625/1770 train_time:160121ms step_avg:99.15ms
step:1625/1770 val_loss:3.3097 train_time:160222ms step_avg:99.21ms
step:1626/1770 train_time:160243ms step_avg:99.16ms
step:1627/1770 train_time:160337ms step_avg:99.16ms
step:1628/1770 train_time:160439ms step_avg:99.16ms
step:1629/1770 train_time:160541ms step_avg:99.16ms
step:1630/1770 train_time:160644ms step_avg:99.16ms
step:1631/1770 train_time:160746ms step_avg:99.16ms
step:1632/1770 train_time:160850ms step_avg:99.17ms
step:1633/1770 train_time:160953ms step_avg:99.17ms
step:1634/1770 train_time:161056ms step_avg:99.17ms
step:1635/1770 train_time:161159ms step_avg:99.17ms
step:1636/1770 train_time:161264ms step_avg:99.18ms
step:1637/1770 train_time:161369ms step_avg:99.18ms
step:1638/1770 train_time:161472ms step_avg:99.18ms
step:1639/1770 train_time:161576ms step_avg:99.19ms
step:1640/1770 train_time:161680ms step_avg:99.19ms
step:1641/1770 train_time:161783ms step_avg:99.19ms
step:1642/1770 train_time:161885ms step_avg:99.19ms
step:1643/1770 train_time:161989ms step_avg:99.20ms
step:1644/1770 train_time:162093ms step_avg:99.20ms
step:1645/1770 train_time:162197ms step_avg:99.20ms
step:1646/1770 train_time:162302ms step_avg:99.21ms
step:1647/1770 train_time:162408ms step_avg:99.21ms
step:1648/1770 train_time:162511ms step_avg:99.21ms
step:1649/1770 train_time:162614ms step_avg:99.22ms
step:1650/1770 train_time:162718ms step_avg:99.22ms
step:1651/1770 train_time:162821ms step_avg:99.22ms
step:1652/1770 train_time:162926ms step_avg:99.22ms
step:1653/1770 train_time:163028ms step_avg:99.23ms
step:1654/1770 train_time:163135ms step_avg:99.23ms
step:1655/1770 train_time:163242ms step_avg:99.24ms
step:1656/1770 train_time:163345ms step_avg:99.24ms
step:1657/1770 train_time:163450ms step_avg:99.24ms
step:1658/1770 train_time:163554ms step_avg:99.24ms
step:1659/1770 train_time:163659ms step_avg:99.25ms
step:1660/1770 train_time:163763ms step_avg:99.25ms
step:1661/1770 train_time:163868ms step_avg:99.25ms
step:1662/1770 train_time:163972ms step_avg:99.26ms
step:1663/1770 train_time:164075ms step_avg:99.26ms
step:1664/1770 train_time:164179ms step_avg:99.26ms
step:1665/1770 train_time:164282ms step_avg:99.26ms
step:1666/1770 train_time:164386ms step_avg:99.27ms
step:1667/1770 train_time:164489ms step_avg:99.27ms
step:1668/1770 train_time:164591ms step_avg:99.27ms
step:1669/1770 train_time:164694ms step_avg:99.27ms
step:1670/1770 train_time:164797ms step_avg:99.28ms
step:1671/1770 train_time:164901ms step_avg:99.28ms
step:1672/1770 train_time:165005ms step_avg:99.28ms
step:1673/1770 train_time:165110ms step_avg:99.28ms
step:1674/1770 train_time:165214ms step_avg:99.29ms
step:1675/1770 train_time:165318ms step_avg:99.29ms
step:1676/1770 train_time:165423ms step_avg:99.29ms
step:1677/1770 train_time:165529ms step_avg:99.30ms
step:1678/1770 train_time:165632ms step_avg:99.30ms
step:1679/1770 train_time:165735ms step_avg:99.30ms
step:1680/1770 train_time:165839ms step_avg:99.30ms
step:1681/1770 train_time:165943ms step_avg:99.31ms
step:1682/1770 train_time:166048ms step_avg:99.31ms
step:1683/1770 train_time:166151ms step_avg:99.31ms
step:1684/1770 train_time:166253ms step_avg:99.32ms
step:1685/1770 train_time:166358ms step_avg:99.32ms
step:1686/1770 train_time:166462ms step_avg:99.32ms
step:1687/1770 train_time:166567ms step_avg:99.32ms
step:1688/1770 train_time:166670ms step_avg:99.33ms
step:1689/1770 train_time:166773ms step_avg:99.33ms
step:1690/1770 train_time:166878ms step_avg:99.33ms
step:1691/1770 train_time:166982ms step_avg:99.33ms
step:1692/1770 train_time:167085ms step_avg:99.34ms
step:1693/1770 train_time:167190ms step_avg:99.34ms
step:1694/1770 train_time:167293ms step_avg:99.34ms
step:1695/1770 train_time:167398ms step_avg:99.35ms
step:1696/1770 train_time:167503ms step_avg:99.35ms
step:1697/1770 train_time:167607ms step_avg:99.35ms
step:1698/1770 train_time:167712ms step_avg:99.36ms
step:1699/1770 train_time:167814ms step_avg:99.36ms
step:1700/1770 train_time:167917ms step_avg:99.36ms
step:1701/1770 train_time:168020ms step_avg:99.36ms
step:1702/1770 train_time:168125ms step_avg:99.36ms
step:1703/1770 train_time:168228ms step_avg:99.37ms
step:1704/1770 train_time:168332ms step_avg:99.37ms
step:1705/1770 train_time:168435ms step_avg:99.37ms
step:1706/1770 train_time:168538ms step_avg:99.37ms
step:1707/1770 train_time:168643ms step_avg:99.38ms
step:1708/1770 train_time:168747ms step_avg:99.38ms
step:1709/1770 train_time:168852ms step_avg:99.38ms
step:1710/1770 train_time:168959ms step_avg:99.39ms
step:1711/1770 train_time:169065ms step_avg:99.39ms
step:1712/1770 train_time:169169ms step_avg:99.39ms
step:1713/1770 train_time:169272ms step_avg:99.40ms
step:1714/1770 train_time:169376ms step_avg:99.40ms
step:1715/1770 train_time:169479ms step_avg:99.40ms
step:1716/1770 train_time:169584ms step_avg:99.40ms
step:1717/1770 train_time:169688ms step_avg:99.41ms
step:1718/1770 train_time:169792ms step_avg:99.41ms
step:1719/1770 train_time:169899ms step_avg:99.41ms
step:1720/1770 train_time:170004ms step_avg:99.42ms
step:1721/1770 train_time:170108ms step_avg:99.42ms
step:1722/1770 train_time:170214ms step_avg:99.42ms
step:1723/1770 train_time:170320ms step_avg:99.43ms
step:1724/1770 train_time:170427ms step_avg:99.43ms
step:1725/1770 train_time:170533ms step_avg:99.44ms
step:1726/1770 train_time:170639ms step_avg:99.44ms
step:1727/1770 train_time:170743ms step_avg:99.44ms
step:1728/1770 train_time:170848ms step_avg:99.45ms
step:1729/1770 train_time:170952ms step_avg:99.45ms
step:1730/1770 train_time:171057ms step_avg:99.45ms
step:1731/1770 train_time:171163ms step_avg:99.46ms
step:1732/1770 train_time:171267ms step_avg:99.46ms
step:1733/1770 train_time:171373ms step_avg:99.46ms
step:1734/1770 train_time:171476ms step_avg:99.46ms
step:1735/1770 train_time:171582ms step_avg:99.47ms
step:1736/1770 train_time:171684ms step_avg:99.47ms
step:1737/1770 train_time:171788ms step_avg:99.47ms
step:1738/1770 train_time:171892ms step_avg:99.47ms
step:1739/1770 train_time:171996ms step_avg:99.48ms
step:1740/1770 train_time:172101ms step_avg:99.48ms
step:1741/1770 train_time:172207ms step_avg:99.48ms
step:1742/1770 train_time:172314ms step_avg:99.49ms
step:1743/1770 train_time:172419ms step_avg:99.49ms
step:1744/1770 train_time:172524ms step_avg:99.49ms
step:1745/1770 train_time:172627ms step_avg:99.50ms
step:1746/1770 train_time:172734ms step_avg:99.50ms
step:1747/1770 train_time:172837ms step_avg:99.50ms
step:1748/1770 train_time:172943ms step_avg:99.51ms
step:1749/1770 train_time:173049ms step_avg:99.51ms
step:1750/1770 train_time:173153ms step_avg:99.51ms
step:1750/1770 val_loss:3.2827 train_time:173257ms step_avg:99.57ms
step:1751/1770 train_time:173278ms step_avg:99.53ms
step:1752/1770 train_time:173370ms step_avg:99.52ms
step:1753/1770 train_time:173474ms step_avg:99.53ms
step:1754/1770 train_time:173578ms step_avg:99.53ms
step:1755/1770 train_time:173681ms step_avg:99.53ms
step:1756/1770 train_time:173786ms step_avg:99.53ms
step:1757/1770 train_time:173891ms step_avg:99.54ms
step:1758/1770 train_time:173995ms step_avg:99.54ms
step:1759/1770 train_time:174099ms step_avg:99.54ms
step:1760/1770 train_time:174203ms step_avg:99.54ms
step:1761/1770 train_time:174311ms step_avg:99.55ms
step:1762/1770 train_time:174419ms step_avg:99.55ms
step:1763/1770 train_time:174521ms step_avg:99.56ms
step:1764/1770 train_time:174626ms step_avg:99.56ms
step:1765/1770 train_time:174730ms step_avg:99.56ms
step:1766/1770 train_time:174838ms step_avg:99.57ms
step:1767/1770 train_time:174940ms step_avg:99.57ms
step:1768/1770 train_time:175046ms step_avg:99.57ms
step:1769/1770 train_time:175149ms step_avg:99.57ms
step:1770/1770 train_time:175252ms step_avg:99.58ms
step:1770/1770 val_loss:3.2798 train_time:175357ms step_avg:99.63ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
