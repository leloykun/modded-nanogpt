import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Jan 26 00:11:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             120W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23163ms step_avg:nanms
step:2/1770 train_time:23663ms step_avg:nanms
step:3/1770 train_time:23758ms step_avg:nanms
step:4/1770 train_time:23852ms step_avg:nanms
step:5/1770 train_time:23946ms step_avg:nanms
step:6/1770 train_time:24039ms step_avg:nanms
step:7/1770 train_time:24133ms step_avg:nanms
step:8/1770 train_time:24227ms step_avg:nanms
step:9/1770 train_time:24320ms step_avg:nanms
step:10/1770 train_time:24413ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.63ms
step:14/1770 train_time:375ms step_avg:93.84ms
step:15/1770 train_time:469ms step_avg:93.84ms
step:16/1770 train_time:563ms step_avg:93.82ms
step:17/1770 train_time:657ms step_avg:93.80ms
step:18/1770 train_time:751ms step_avg:93.88ms
step:19/1770 train_time:845ms step_avg:93.87ms
step:20/1770 train_time:939ms step_avg:93.90ms
step:21/1770 train_time:1033ms step_avg:93.88ms
step:22/1770 train_time:1126ms step_avg:93.86ms
step:23/1770 train_time:1220ms step_avg:93.85ms
step:24/1770 train_time:1314ms step_avg:93.86ms
step:25/1770 train_time:1408ms step_avg:93.89ms
step:26/1770 train_time:1503ms step_avg:93.92ms
step:27/1770 train_time:1597ms step_avg:93.91ms
step:28/1770 train_time:1691ms step_avg:93.95ms
step:29/1770 train_time:1785ms step_avg:93.95ms
step:30/1770 train_time:1879ms step_avg:93.96ms
step:31/1770 train_time:1974ms step_avg:93.99ms
step:32/1770 train_time:2068ms step_avg:93.98ms
step:33/1770 train_time:2161ms step_avg:93.98ms
step:34/1770 train_time:2255ms step_avg:93.97ms
step:35/1770 train_time:2349ms step_avg:93.97ms
step:36/1770 train_time:2443ms step_avg:93.97ms
step:37/1770 train_time:2538ms step_avg:93.98ms
step:38/1770 train_time:2632ms step_avg:94.01ms
step:39/1770 train_time:2726ms step_avg:94.01ms
step:40/1770 train_time:2820ms step_avg:94.00ms
step:41/1770 train_time:2913ms step_avg:93.98ms
step:42/1770 train_time:3008ms step_avg:94.00ms
step:43/1770 train_time:3102ms step_avg:94.00ms
step:44/1770 train_time:3196ms step_avg:94.00ms
step:45/1770 train_time:3290ms step_avg:94.01ms
step:46/1770 train_time:3384ms step_avg:93.99ms
step:47/1770 train_time:3478ms step_avg:94.00ms
step:48/1770 train_time:3571ms step_avg:93.97ms
step:49/1770 train_time:3665ms step_avg:93.97ms
step:50/1770 train_time:3758ms step_avg:93.95ms
step:51/1770 train_time:3852ms step_avg:93.96ms
step:52/1770 train_time:3946ms step_avg:93.96ms
step:53/1770 train_time:4040ms step_avg:93.96ms
step:54/1770 train_time:4134ms step_avg:93.96ms
step:55/1770 train_time:4228ms step_avg:93.96ms
step:56/1770 train_time:4323ms step_avg:93.97ms
step:57/1770 train_time:4416ms step_avg:93.97ms
step:58/1770 train_time:4510ms step_avg:93.97ms
step:59/1770 train_time:4604ms step_avg:93.96ms
step:60/1770 train_time:4698ms step_avg:93.95ms
step:61/1770 train_time:4792ms step_avg:93.95ms
step:62/1770 train_time:4886ms step_avg:93.95ms
step:63/1770 train_time:4979ms step_avg:93.95ms
step:64/1770 train_time:5073ms step_avg:93.94ms
step:65/1770 train_time:5168ms step_avg:93.96ms
step:66/1770 train_time:5262ms step_avg:93.97ms
step:67/1770 train_time:5356ms step_avg:93.96ms
step:68/1770 train_time:5450ms step_avg:93.96ms
step:69/1770 train_time:5544ms step_avg:93.97ms
step:70/1770 train_time:5638ms step_avg:93.97ms
step:71/1770 train_time:5732ms step_avg:93.96ms
step:72/1770 train_time:5825ms step_avg:93.95ms
step:73/1770 train_time:5919ms step_avg:93.96ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.97ms
step:76/1770 train_time:6202ms step_avg:93.97ms
step:77/1770 train_time:6296ms step_avg:93.97ms
step:78/1770 train_time:6390ms step_avg:93.97ms
step:79/1770 train_time:6484ms step_avg:93.97ms
step:80/1770 train_time:6578ms step_avg:93.96ms
step:81/1770 train_time:6672ms step_avg:93.97ms
step:82/1770 train_time:6766ms step_avg:93.97ms
step:83/1770 train_time:6860ms step_avg:93.97ms
step:84/1770 train_time:6954ms step_avg:93.97ms
step:85/1770 train_time:7048ms step_avg:93.97ms
step:86/1770 train_time:7141ms step_avg:93.97ms
step:87/1770 train_time:7236ms step_avg:93.97ms
step:88/1770 train_time:7329ms step_avg:93.97ms
step:89/1770 train_time:7424ms step_avg:93.97ms
step:90/1770 train_time:7517ms step_avg:93.96ms
step:91/1770 train_time:7611ms step_avg:93.96ms
step:92/1770 train_time:7705ms step_avg:93.96ms
step:93/1770 train_time:7799ms step_avg:93.96ms
step:94/1770 train_time:7892ms step_avg:93.96ms
step:95/1770 train_time:7987ms step_avg:93.96ms
step:96/1770 train_time:8081ms step_avg:93.96ms
step:97/1770 train_time:8175ms step_avg:93.96ms
step:98/1770 train_time:8269ms step_avg:93.97ms
step:99/1770 train_time:8364ms step_avg:93.97ms
step:100/1770 train_time:8457ms step_avg:93.97ms
step:101/1770 train_time:8551ms step_avg:93.97ms
step:102/1770 train_time:8646ms step_avg:93.97ms
step:103/1770 train_time:8739ms step_avg:93.97ms
step:104/1770 train_time:8833ms step_avg:93.97ms
step:105/1770 train_time:8927ms step_avg:93.97ms
step:106/1770 train_time:9021ms step_avg:93.97ms
step:107/1770 train_time:9115ms step_avg:93.97ms
step:108/1770 train_time:9209ms step_avg:93.97ms
step:109/1770 train_time:9302ms step_avg:93.96ms
step:110/1770 train_time:9396ms step_avg:93.96ms
step:111/1770 train_time:9490ms step_avg:93.96ms
step:112/1770 train_time:9584ms step_avg:93.96ms
step:113/1770 train_time:9678ms step_avg:93.96ms
step:114/1770 train_time:9771ms step_avg:93.96ms
step:115/1770 train_time:9866ms step_avg:93.96ms
step:116/1770 train_time:9960ms step_avg:93.96ms
step:117/1770 train_time:10054ms step_avg:93.96ms
step:118/1770 train_time:10148ms step_avg:93.96ms
step:119/1770 train_time:10241ms step_avg:93.96ms
step:120/1770 train_time:10335ms step_avg:93.96ms
step:121/1770 train_time:10429ms step_avg:93.96ms
step:122/1770 train_time:10524ms step_avg:93.96ms
step:123/1770 train_time:10617ms step_avg:93.96ms
step:124/1770 train_time:10711ms step_avg:93.96ms
step:125/1770 train_time:10805ms step_avg:93.96ms
step:125/1770 val_loss:4.6532 train_time:10897ms step_avg:94.76ms
step:126/1770 train_time:10920ms step_avg:94.14ms
step:127/1770 train_time:10994ms step_avg:93.96ms
step:128/1770 train_time:11090ms step_avg:93.98ms
step:129/1770 train_time:11188ms step_avg:94.01ms
step:130/1770 train_time:11282ms step_avg:94.02ms
step:131/1770 train_time:11376ms step_avg:94.01ms
step:132/1770 train_time:11470ms step_avg:94.01ms
step:133/1770 train_time:11563ms step_avg:94.01ms
step:134/1770 train_time:11657ms step_avg:94.01ms
step:135/1770 train_time:11752ms step_avg:94.01ms
step:136/1770 train_time:11846ms step_avg:94.02ms
step:137/1770 train_time:11941ms step_avg:94.03ms
step:138/1770 train_time:12036ms step_avg:94.03ms
step:139/1770 train_time:12131ms step_avg:94.04ms
step:140/1770 train_time:12226ms step_avg:94.04ms
step:141/1770 train_time:12320ms step_avg:94.05ms
step:142/1770 train_time:12414ms step_avg:94.05ms
step:143/1770 train_time:12509ms step_avg:94.05ms
step:144/1770 train_time:12603ms step_avg:94.05ms
step:145/1770 train_time:12697ms step_avg:94.06ms
step:146/1770 train_time:12792ms step_avg:94.06ms
step:147/1770 train_time:12887ms step_avg:94.06ms
step:148/1770 train_time:12981ms step_avg:94.07ms
step:149/1770 train_time:13076ms step_avg:94.07ms
step:150/1770 train_time:13170ms step_avg:94.07ms
step:151/1770 train_time:13265ms step_avg:94.07ms
step:152/1770 train_time:13360ms step_avg:94.08ms
step:153/1770 train_time:13454ms step_avg:94.09ms
step:154/1770 train_time:13549ms step_avg:94.09ms
step:155/1770 train_time:13643ms step_avg:94.09ms
step:156/1770 train_time:13738ms step_avg:94.09ms
step:157/1770 train_time:13832ms step_avg:94.10ms
step:158/1770 train_time:13927ms step_avg:94.10ms
step:159/1770 train_time:14022ms step_avg:94.11ms
step:160/1770 train_time:14116ms step_avg:94.11ms
step:161/1770 train_time:14210ms step_avg:94.11ms
step:162/1770 train_time:14305ms step_avg:94.11ms
step:163/1770 train_time:14400ms step_avg:94.11ms
step:164/1770 train_time:14494ms step_avg:94.12ms
step:165/1770 train_time:14588ms step_avg:94.11ms
step:166/1770 train_time:14682ms step_avg:94.12ms
step:167/1770 train_time:14777ms step_avg:94.12ms
step:168/1770 train_time:14871ms step_avg:94.12ms
step:169/1770 train_time:14967ms step_avg:94.13ms
step:170/1770 train_time:15061ms step_avg:94.13ms
step:171/1770 train_time:15156ms step_avg:94.14ms
step:172/1770 train_time:15250ms step_avg:94.14ms
step:173/1770 train_time:15345ms step_avg:94.14ms
step:174/1770 train_time:15440ms step_avg:94.15ms
step:175/1770 train_time:15535ms step_avg:94.15ms
step:176/1770 train_time:15629ms step_avg:94.15ms
step:177/1770 train_time:15724ms step_avg:94.15ms
step:178/1770 train_time:15818ms step_avg:94.16ms
step:179/1770 train_time:15913ms step_avg:94.16ms
step:180/1770 train_time:16007ms step_avg:94.16ms
step:181/1770 train_time:16102ms step_avg:94.16ms
step:182/1770 train_time:16196ms step_avg:94.17ms
step:183/1770 train_time:16291ms step_avg:94.17ms
step:184/1770 train_time:16386ms step_avg:94.17ms
step:185/1770 train_time:16481ms step_avg:94.18ms
step:186/1770 train_time:16575ms step_avg:94.18ms
step:187/1770 train_time:16670ms step_avg:94.18ms
step:188/1770 train_time:16765ms step_avg:94.18ms
step:189/1770 train_time:16860ms step_avg:94.19ms
step:190/1770 train_time:16955ms step_avg:94.19ms
step:191/1770 train_time:17049ms step_avg:94.19ms
step:192/1770 train_time:17144ms step_avg:94.20ms
step:193/1770 train_time:17239ms step_avg:94.20ms
step:194/1770 train_time:17334ms step_avg:94.20ms
step:195/1770 train_time:17428ms step_avg:94.21ms
step:196/1770 train_time:17523ms step_avg:94.21ms
step:197/1770 train_time:17617ms step_avg:94.21ms
step:198/1770 train_time:17712ms step_avg:94.21ms
step:199/1770 train_time:17806ms step_avg:94.21ms
step:200/1770 train_time:17901ms step_avg:94.21ms
step:201/1770 train_time:17995ms step_avg:94.21ms
step:202/1770 train_time:18089ms step_avg:94.21ms
step:203/1770 train_time:18184ms step_avg:94.22ms
step:204/1770 train_time:18279ms step_avg:94.22ms
step:205/1770 train_time:18373ms step_avg:94.22ms
step:206/1770 train_time:18468ms step_avg:94.22ms
step:207/1770 train_time:18562ms step_avg:94.23ms
step:208/1770 train_time:18657ms step_avg:94.23ms
step:209/1770 train_time:18752ms step_avg:94.23ms
step:210/1770 train_time:18847ms step_avg:94.23ms
step:211/1770 train_time:18942ms step_avg:94.24ms
step:212/1770 train_time:19036ms step_avg:94.24ms
step:213/1770 train_time:19131ms step_avg:94.24ms
step:214/1770 train_time:19226ms step_avg:94.25ms
step:215/1770 train_time:19322ms step_avg:94.25ms
step:216/1770 train_time:19416ms step_avg:94.25ms
step:217/1770 train_time:19510ms step_avg:94.25ms
step:218/1770 train_time:19604ms step_avg:94.25ms
step:219/1770 train_time:19699ms step_avg:94.25ms
step:220/1770 train_time:19794ms step_avg:94.26ms
step:221/1770 train_time:19889ms step_avg:94.26ms
step:222/1770 train_time:19984ms step_avg:94.26ms
step:223/1770 train_time:20079ms step_avg:94.27ms
step:224/1770 train_time:20173ms step_avg:94.27ms
step:225/1770 train_time:20268ms step_avg:94.27ms
step:226/1770 train_time:20363ms step_avg:94.27ms
step:227/1770 train_time:20457ms step_avg:94.27ms
step:228/1770 train_time:20552ms step_avg:94.27ms
step:229/1770 train_time:20647ms step_avg:94.28ms
step:230/1770 train_time:20742ms step_avg:94.28ms
step:231/1770 train_time:20837ms step_avg:94.28ms
step:232/1770 train_time:20931ms step_avg:94.28ms
step:233/1770 train_time:21026ms step_avg:94.29ms
step:234/1770 train_time:21121ms step_avg:94.29ms
step:235/1770 train_time:21216ms step_avg:94.29ms
step:236/1770 train_time:21310ms step_avg:94.29ms
step:237/1770 train_time:21405ms step_avg:94.29ms
step:238/1770 train_time:21500ms step_avg:94.30ms
step:239/1770 train_time:21596ms step_avg:94.30ms
step:240/1770 train_time:21690ms step_avg:94.30ms
step:241/1770 train_time:21784ms step_avg:94.30ms
step:242/1770 train_time:21879ms step_avg:94.31ms
step:243/1770 train_time:21973ms step_avg:94.31ms
step:244/1770 train_time:22068ms step_avg:94.31ms
step:245/1770 train_time:22164ms step_avg:94.31ms
step:246/1770 train_time:22258ms step_avg:94.32ms
step:247/1770 train_time:22353ms step_avg:94.31ms
step:248/1770 train_time:22448ms step_avg:94.32ms
step:249/1770 train_time:22543ms step_avg:94.32ms
step:250/1770 train_time:22637ms step_avg:94.32ms
step:250/1770 val_loss:4.0971 train_time:22731ms step_avg:94.71ms
step:251/1770 train_time:22752ms step_avg:94.41ms
step:252/1770 train_time:22834ms step_avg:94.36ms
step:253/1770 train_time:22932ms step_avg:94.37ms
step:254/1770 train_time:23028ms step_avg:94.38ms
step:255/1770 train_time:23123ms step_avg:94.38ms
step:256/1770 train_time:23216ms step_avg:94.38ms
step:257/1770 train_time:23310ms step_avg:94.37ms
step:258/1770 train_time:23405ms step_avg:94.37ms
step:259/1770 train_time:23499ms step_avg:94.37ms
step:260/1770 train_time:23593ms step_avg:94.37ms
step:261/1770 train_time:23687ms step_avg:94.37ms
step:262/1770 train_time:23782ms step_avg:94.37ms
step:263/1770 train_time:23877ms step_avg:94.38ms
step:264/1770 train_time:23971ms step_avg:94.38ms
step:265/1770 train_time:24067ms step_avg:94.38ms
step:266/1770 train_time:24162ms step_avg:94.38ms
step:267/1770 train_time:24257ms step_avg:94.38ms
step:268/1770 train_time:24351ms step_avg:94.38ms
step:269/1770 train_time:24446ms step_avg:94.39ms
step:270/1770 train_time:24541ms step_avg:94.39ms
step:271/1770 train_time:24636ms step_avg:94.39ms
step:272/1770 train_time:24731ms step_avg:94.39ms
step:273/1770 train_time:24827ms step_avg:94.40ms
step:274/1770 train_time:24922ms step_avg:94.40ms
step:275/1770 train_time:25018ms step_avg:94.41ms
step:276/1770 train_time:25113ms step_avg:94.41ms
step:277/1770 train_time:25208ms step_avg:94.41ms
step:278/1770 train_time:25303ms step_avg:94.41ms
step:279/1770 train_time:25398ms step_avg:94.42ms
step:280/1770 train_time:25493ms step_avg:94.42ms
step:281/1770 train_time:25588ms step_avg:94.42ms
step:282/1770 train_time:25682ms step_avg:94.42ms
step:283/1770 train_time:25778ms step_avg:94.42ms
step:284/1770 train_time:25873ms step_avg:94.43ms
step:285/1770 train_time:25968ms step_avg:94.43ms
step:286/1770 train_time:26063ms step_avg:94.43ms
step:287/1770 train_time:26159ms step_avg:94.44ms
step:288/1770 train_time:26254ms step_avg:94.44ms
step:289/1770 train_time:26349ms step_avg:94.44ms
step:290/1770 train_time:26444ms step_avg:94.44ms
step:291/1770 train_time:26539ms step_avg:94.45ms
step:292/1770 train_time:26634ms step_avg:94.45ms
step:293/1770 train_time:26730ms step_avg:94.45ms
step:294/1770 train_time:26825ms step_avg:94.45ms
step:295/1770 train_time:26920ms step_avg:94.46ms
step:296/1770 train_time:27015ms step_avg:94.46ms
step:297/1770 train_time:27109ms step_avg:94.46ms
step:298/1770 train_time:27205ms step_avg:94.46ms
step:299/1770 train_time:27300ms step_avg:94.46ms
step:300/1770 train_time:27395ms step_avg:94.47ms
step:301/1770 train_time:27490ms step_avg:94.47ms
step:302/1770 train_time:27584ms step_avg:94.47ms
step:303/1770 train_time:27680ms step_avg:94.47ms
step:304/1770 train_time:27775ms step_avg:94.47ms
step:305/1770 train_time:27870ms step_avg:94.47ms
step:306/1770 train_time:27965ms step_avg:94.48ms
step:307/1770 train_time:28060ms step_avg:94.48ms
step:308/1770 train_time:28155ms step_avg:94.48ms
step:309/1770 train_time:28250ms step_avg:94.48ms
step:310/1770 train_time:28346ms step_avg:94.49ms
step:311/1770 train_time:28441ms step_avg:94.49ms
step:312/1770 train_time:28536ms step_avg:94.49ms
step:313/1770 train_time:28631ms step_avg:94.49ms
step:314/1770 train_time:28726ms step_avg:94.49ms
step:315/1770 train_time:28821ms step_avg:94.50ms
step:316/1770 train_time:28917ms step_avg:94.50ms
step:317/1770 train_time:29012ms step_avg:94.50ms
step:318/1770 train_time:29107ms step_avg:94.50ms
step:319/1770 train_time:29202ms step_avg:94.51ms
step:320/1770 train_time:29298ms step_avg:94.51ms
step:321/1770 train_time:29393ms step_avg:94.51ms
step:322/1770 train_time:29488ms step_avg:94.51ms
step:323/1770 train_time:29583ms step_avg:94.51ms
step:324/1770 train_time:29678ms step_avg:94.52ms
step:325/1770 train_time:29774ms step_avg:94.52ms
step:326/1770 train_time:29869ms step_avg:94.52ms
step:327/1770 train_time:29964ms step_avg:94.52ms
step:328/1770 train_time:30060ms step_avg:94.53ms
step:329/1770 train_time:30155ms step_avg:94.53ms
step:330/1770 train_time:30250ms step_avg:94.53ms
step:331/1770 train_time:30346ms step_avg:94.53ms
step:332/1770 train_time:30441ms step_avg:94.54ms
step:333/1770 train_time:30537ms step_avg:94.54ms
step:334/1770 train_time:30633ms step_avg:94.55ms
step:335/1770 train_time:30728ms step_avg:94.55ms
step:336/1770 train_time:30823ms step_avg:94.55ms
step:337/1770 train_time:30918ms step_avg:94.55ms
step:338/1770 train_time:31013ms step_avg:94.55ms
step:339/1770 train_time:31107ms step_avg:94.55ms
step:340/1770 train_time:31202ms step_avg:94.55ms
step:341/1770 train_time:31297ms step_avg:94.55ms
step:342/1770 train_time:31392ms step_avg:94.56ms
step:343/1770 train_time:31488ms step_avg:94.56ms
step:344/1770 train_time:31583ms step_avg:94.56ms
step:345/1770 train_time:31678ms step_avg:94.56ms
step:346/1770 train_time:31773ms step_avg:94.56ms
step:347/1770 train_time:31868ms step_avg:94.56ms
step:348/1770 train_time:31963ms step_avg:94.56ms
step:349/1770 train_time:32059ms step_avg:94.57ms
step:350/1770 train_time:32154ms step_avg:94.57ms
step:351/1770 train_time:32249ms step_avg:94.57ms
step:352/1770 train_time:32344ms step_avg:94.57ms
step:353/1770 train_time:32440ms step_avg:94.58ms
step:354/1770 train_time:32536ms step_avg:94.58ms
step:355/1770 train_time:32630ms step_avg:94.58ms
step:356/1770 train_time:32726ms step_avg:94.58ms
step:357/1770 train_time:32821ms step_avg:94.59ms
step:358/1770 train_time:32916ms step_avg:94.59ms
step:359/1770 train_time:33011ms step_avg:94.59ms
step:360/1770 train_time:33105ms step_avg:94.59ms
step:361/1770 train_time:33200ms step_avg:94.59ms
step:362/1770 train_time:33295ms step_avg:94.59ms
step:363/1770 train_time:33390ms step_avg:94.59ms
step:364/1770 train_time:33485ms step_avg:94.59ms
step:365/1770 train_time:33580ms step_avg:94.59ms
step:366/1770 train_time:33676ms step_avg:94.59ms
step:367/1770 train_time:33770ms step_avg:94.59ms
step:368/1770 train_time:33865ms step_avg:94.60ms
step:369/1770 train_time:33960ms step_avg:94.60ms
step:370/1770 train_time:34055ms step_avg:94.60ms
step:371/1770 train_time:34149ms step_avg:94.60ms
step:372/1770 train_time:34244ms step_avg:94.60ms
step:373/1770 train_time:34339ms step_avg:94.60ms
step:374/1770 train_time:34434ms step_avg:94.60ms
step:375/1770 train_time:34529ms step_avg:94.60ms
step:375/1770 val_loss:3.8974 train_time:34623ms step_avg:94.86ms
step:376/1770 train_time:34644ms step_avg:94.66ms
step:377/1770 train_time:34730ms step_avg:94.63ms
step:378/1770 train_time:34830ms step_avg:94.65ms
step:379/1770 train_time:34925ms step_avg:94.65ms
step:380/1770 train_time:35020ms step_avg:94.65ms
step:381/1770 train_time:35115ms step_avg:94.65ms
step:382/1770 train_time:35210ms step_avg:94.65ms
step:383/1770 train_time:35304ms step_avg:94.65ms
step:384/1770 train_time:35398ms step_avg:94.65ms
step:385/1770 train_time:35493ms step_avg:94.65ms
step:386/1770 train_time:35587ms step_avg:94.65ms
step:387/1770 train_time:35683ms step_avg:94.65ms
step:388/1770 train_time:35779ms step_avg:94.65ms
step:389/1770 train_time:35875ms step_avg:94.66ms
step:390/1770 train_time:35970ms step_avg:94.66ms
step:391/1770 train_time:36065ms step_avg:94.66ms
step:392/1770 train_time:36160ms step_avg:94.66ms
step:393/1770 train_time:36255ms step_avg:94.66ms
step:394/1770 train_time:36350ms step_avg:94.66ms
step:395/1770 train_time:36445ms step_avg:94.66ms
step:396/1770 train_time:36541ms step_avg:94.67ms
step:397/1770 train_time:36637ms step_avg:94.67ms
step:398/1770 train_time:36735ms step_avg:94.68ms
step:399/1770 train_time:36832ms step_avg:94.68ms
step:400/1770 train_time:36929ms step_avg:94.69ms
step:401/1770 train_time:37026ms step_avg:94.70ms
step:402/1770 train_time:37123ms step_avg:94.70ms
step:403/1770 train_time:37220ms step_avg:94.71ms
step:404/1770 train_time:37316ms step_avg:94.71ms
step:405/1770 train_time:37413ms step_avg:94.72ms
step:406/1770 train_time:37510ms step_avg:94.72ms
step:407/1770 train_time:37607ms step_avg:94.73ms
step:408/1770 train_time:37705ms step_avg:94.74ms
step:409/1770 train_time:37802ms step_avg:94.74ms
step:410/1770 train_time:37899ms step_avg:94.75ms
step:411/1770 train_time:37996ms step_avg:94.75ms
step:412/1770 train_time:38093ms step_avg:94.76ms
step:413/1770 train_time:38190ms step_avg:94.77ms
step:414/1770 train_time:38287ms step_avg:94.77ms
step:415/1770 train_time:38384ms step_avg:94.78ms
step:416/1770 train_time:38481ms step_avg:94.78ms
step:417/1770 train_time:38577ms step_avg:94.78ms
step:418/1770 train_time:38675ms step_avg:94.79ms
step:419/1770 train_time:38771ms step_avg:94.80ms
step:420/1770 train_time:38869ms step_avg:94.80ms
step:421/1770 train_time:38966ms step_avg:94.81ms
step:422/1770 train_time:39063ms step_avg:94.81ms
step:423/1770 train_time:39160ms step_avg:94.82ms
step:424/1770 train_time:39256ms step_avg:94.82ms
step:425/1770 train_time:39354ms step_avg:94.83ms
step:426/1770 train_time:39451ms step_avg:94.83ms
step:427/1770 train_time:39548ms step_avg:94.84ms
step:428/1770 train_time:39644ms step_avg:94.84ms
step:429/1770 train_time:39741ms step_avg:94.85ms
step:430/1770 train_time:39837ms step_avg:94.85ms
step:431/1770 train_time:39934ms step_avg:94.86ms
step:432/1770 train_time:40032ms step_avg:94.86ms
step:433/1770 train_time:40128ms step_avg:94.87ms
step:434/1770 train_time:40226ms step_avg:94.87ms
step:435/1770 train_time:40322ms step_avg:94.88ms
step:436/1770 train_time:40419ms step_avg:94.88ms
step:437/1770 train_time:40516ms step_avg:94.89ms
step:438/1770 train_time:40614ms step_avg:94.89ms
step:439/1770 train_time:40711ms step_avg:94.90ms
step:440/1770 train_time:40809ms step_avg:94.90ms
step:441/1770 train_time:40907ms step_avg:94.91ms
step:442/1770 train_time:41004ms step_avg:94.92ms
step:443/1770 train_time:41101ms step_avg:94.92ms
step:444/1770 train_time:41197ms step_avg:94.92ms
step:445/1770 train_time:41294ms step_avg:94.93ms
step:446/1770 train_time:41391ms step_avg:94.93ms
step:447/1770 train_time:41488ms step_avg:94.94ms
step:448/1770 train_time:41586ms step_avg:94.94ms
step:449/1770 train_time:41682ms step_avg:94.95ms
step:450/1770 train_time:41780ms step_avg:94.95ms
step:451/1770 train_time:41877ms step_avg:94.96ms
step:452/1770 train_time:41974ms step_avg:94.96ms
step:453/1770 train_time:42071ms step_avg:94.97ms
step:454/1770 train_time:42168ms step_avg:94.97ms
step:455/1770 train_time:42266ms step_avg:94.98ms
step:456/1770 train_time:42363ms step_avg:94.98ms
step:457/1770 train_time:42460ms step_avg:94.99ms
step:458/1770 train_time:42557ms step_avg:94.99ms
step:459/1770 train_time:42653ms step_avg:95.00ms
step:460/1770 train_time:42750ms step_avg:95.00ms
step:461/1770 train_time:42847ms step_avg:95.00ms
step:462/1770 train_time:42944ms step_avg:95.01ms
step:463/1770 train_time:43040ms step_avg:95.01ms
step:464/1770 train_time:43137ms step_avg:95.02ms
step:465/1770 train_time:43234ms step_avg:95.02ms
step:466/1770 train_time:43331ms step_avg:95.02ms
step:467/1770 train_time:43428ms step_avg:95.03ms
step:468/1770 train_time:43525ms step_avg:95.03ms
step:469/1770 train_time:43622ms step_avg:95.04ms
step:470/1770 train_time:43718ms step_avg:95.04ms
step:471/1770 train_time:43815ms step_avg:95.04ms
step:472/1770 train_time:43912ms step_avg:95.05ms
step:473/1770 train_time:44008ms step_avg:95.05ms
step:474/1770 train_time:44105ms step_avg:95.05ms
step:475/1770 train_time:44202ms step_avg:95.06ms
step:476/1770 train_time:44299ms step_avg:95.06ms
step:477/1770 train_time:44396ms step_avg:95.07ms
step:478/1770 train_time:44493ms step_avg:95.07ms
step:479/1770 train_time:44590ms step_avg:95.07ms
step:480/1770 train_time:44687ms step_avg:95.08ms
step:481/1770 train_time:44785ms step_avg:95.08ms
step:482/1770 train_time:44881ms step_avg:95.09ms
step:483/1770 train_time:44979ms step_avg:95.09ms
step:484/1770 train_time:45077ms step_avg:95.10ms
step:485/1770 train_time:45174ms step_avg:95.10ms
step:486/1770 train_time:45271ms step_avg:95.11ms
step:487/1770 train_time:45367ms step_avg:95.11ms
step:488/1770 train_time:45464ms step_avg:95.11ms
step:489/1770 train_time:45561ms step_avg:95.12ms
step:490/1770 train_time:45657ms step_avg:95.12ms
step:491/1770 train_time:45754ms step_avg:95.12ms
step:492/1770 train_time:45852ms step_avg:95.13ms
step:493/1770 train_time:45949ms step_avg:95.13ms
step:494/1770 train_time:46047ms step_avg:95.14ms
step:495/1770 train_time:46144ms step_avg:95.14ms
step:496/1770 train_time:46240ms step_avg:95.14ms
step:497/1770 train_time:46337ms step_avg:95.15ms
step:498/1770 train_time:46434ms step_avg:95.15ms
step:499/1770 train_time:46531ms step_avg:95.16ms
step:500/1770 train_time:46628ms step_avg:95.16ms
step:500/1770 val_loss:3.7472 train_time:46723ms step_avg:95.35ms
step:501/1770 train_time:46745ms step_avg:95.20ms
step:502/1770 train_time:46836ms step_avg:95.19ms
step:503/1770 train_time:46935ms step_avg:95.20ms
step:504/1770 train_time:47034ms step_avg:95.21ms
step:505/1770 train_time:47130ms step_avg:95.21ms
step:506/1770 train_time:47227ms step_avg:95.21ms
step:507/1770 train_time:47323ms step_avg:95.22ms
step:508/1770 train_time:47420ms step_avg:95.22ms
step:509/1770 train_time:47517ms step_avg:95.22ms
step:510/1770 train_time:47613ms step_avg:95.23ms
step:511/1770 train_time:47710ms step_avg:95.23ms
step:512/1770 train_time:47807ms step_avg:95.23ms
step:513/1770 train_time:47905ms step_avg:95.24ms
step:514/1770 train_time:48004ms step_avg:95.25ms
step:515/1770 train_time:48102ms step_avg:95.25ms
step:516/1770 train_time:48199ms step_avg:95.26ms
step:517/1770 train_time:48295ms step_avg:95.26ms
step:518/1770 train_time:48392ms step_avg:95.26ms
step:519/1770 train_time:48490ms step_avg:95.26ms
step:520/1770 train_time:48586ms step_avg:95.27ms
step:521/1770 train_time:48683ms step_avg:95.27ms
step:522/1770 train_time:48779ms step_avg:95.27ms
step:523/1770 train_time:48876ms step_avg:95.28ms
step:524/1770 train_time:48973ms step_avg:95.28ms
step:525/1770 train_time:49071ms step_avg:95.28ms
step:526/1770 train_time:49168ms step_avg:95.29ms
step:527/1770 train_time:49265ms step_avg:95.29ms
step:528/1770 train_time:49363ms step_avg:95.29ms
step:529/1770 train_time:49461ms step_avg:95.30ms
step:530/1770 train_time:49559ms step_avg:95.30ms
step:531/1770 train_time:49655ms step_avg:95.31ms
step:532/1770 train_time:49752ms step_avg:95.31ms
step:533/1770 train_time:49849ms step_avg:95.31ms
step:534/1770 train_time:49947ms step_avg:95.32ms
step:535/1770 train_time:50044ms step_avg:95.32ms
step:536/1770 train_time:50141ms step_avg:95.33ms
step:537/1770 train_time:50239ms step_avg:95.33ms
step:538/1770 train_time:50336ms step_avg:95.33ms
step:539/1770 train_time:50434ms step_avg:95.34ms
step:540/1770 train_time:50532ms step_avg:95.34ms
step:541/1770 train_time:50629ms step_avg:95.35ms
step:542/1770 train_time:50727ms step_avg:95.35ms
step:543/1770 train_time:50824ms step_avg:95.35ms
step:544/1770 train_time:50921ms step_avg:95.36ms
step:545/1770 train_time:51018ms step_avg:95.36ms
step:546/1770 train_time:51115ms step_avg:95.36ms
step:547/1770 train_time:51213ms step_avg:95.37ms
step:548/1770 train_time:51310ms step_avg:95.37ms
step:549/1770 train_time:51408ms step_avg:95.38ms
step:550/1770 train_time:51505ms step_avg:95.38ms
step:551/1770 train_time:51603ms step_avg:95.38ms
step:552/1770 train_time:51700ms step_avg:95.39ms
step:553/1770 train_time:51797ms step_avg:95.39ms
step:554/1770 train_time:51894ms step_avg:95.39ms
step:555/1770 train_time:51991ms step_avg:95.40ms
step:556/1770 train_time:52089ms step_avg:95.40ms
step:557/1770 train_time:52187ms step_avg:95.41ms
step:558/1770 train_time:52284ms step_avg:95.41ms
step:559/1770 train_time:52382ms step_avg:95.41ms
step:560/1770 train_time:52479ms step_avg:95.42ms
step:561/1770 train_time:52577ms step_avg:95.42ms
step:562/1770 train_time:52675ms step_avg:95.43ms
step:563/1770 train_time:52772ms step_avg:95.43ms
step:564/1770 train_time:52869ms step_avg:95.43ms
step:565/1770 train_time:52966ms step_avg:95.43ms
step:566/1770 train_time:53063ms step_avg:95.44ms
step:567/1770 train_time:53160ms step_avg:95.44ms
step:568/1770 train_time:53257ms step_avg:95.44ms
step:569/1770 train_time:53354ms step_avg:95.45ms
step:570/1770 train_time:53452ms step_avg:95.45ms
step:571/1770 train_time:53551ms step_avg:95.46ms
step:572/1770 train_time:53648ms step_avg:95.46ms
step:573/1770 train_time:53746ms step_avg:95.46ms
step:574/1770 train_time:53843ms step_avg:95.47ms
step:575/1770 train_time:53940ms step_avg:95.47ms
step:576/1770 train_time:54037ms step_avg:95.47ms
step:577/1770 train_time:54134ms step_avg:95.47ms
step:578/1770 train_time:54232ms step_avg:95.48ms
step:579/1770 train_time:54330ms step_avg:95.48ms
step:580/1770 train_time:54427ms step_avg:95.49ms
step:581/1770 train_time:54524ms step_avg:95.49ms
step:582/1770 train_time:54622ms step_avg:95.49ms
step:583/1770 train_time:54720ms step_avg:95.50ms
step:584/1770 train_time:54817ms step_avg:95.50ms
step:585/1770 train_time:54914ms step_avg:95.50ms
step:586/1770 train_time:55012ms step_avg:95.51ms
step:587/1770 train_time:55109ms step_avg:95.51ms
step:588/1770 train_time:55207ms step_avg:95.51ms
step:589/1770 train_time:55304ms step_avg:95.52ms
step:590/1770 train_time:55401ms step_avg:95.52ms
step:591/1770 train_time:55499ms step_avg:95.52ms
step:592/1770 train_time:55596ms step_avg:95.53ms
step:593/1770 train_time:55694ms step_avg:95.53ms
step:594/1770 train_time:55792ms step_avg:95.53ms
step:595/1770 train_time:55890ms step_avg:95.54ms
step:596/1770 train_time:55987ms step_avg:95.54ms
step:597/1770 train_time:56084ms step_avg:95.54ms
step:598/1770 train_time:56181ms step_avg:95.55ms
step:599/1770 train_time:56279ms step_avg:95.55ms
step:600/1770 train_time:56376ms step_avg:95.55ms
step:601/1770 train_time:56473ms step_avg:95.55ms
step:602/1770 train_time:56570ms step_avg:95.56ms
step:603/1770 train_time:56668ms step_avg:95.56ms
step:604/1770 train_time:56765ms step_avg:95.56ms
step:605/1770 train_time:56863ms step_avg:95.57ms
step:606/1770 train_time:56960ms step_avg:95.57ms
step:607/1770 train_time:57058ms step_avg:95.57ms
step:608/1770 train_time:57154ms step_avg:95.58ms
step:609/1770 train_time:57252ms step_avg:95.58ms
step:610/1770 train_time:57350ms step_avg:95.58ms
step:611/1770 train_time:57447ms step_avg:95.59ms
step:612/1770 train_time:57545ms step_avg:95.59ms
step:613/1770 train_time:57642ms step_avg:95.59ms
step:614/1770 train_time:57739ms step_avg:95.59ms
step:615/1770 train_time:57837ms step_avg:95.60ms
step:616/1770 train_time:57933ms step_avg:95.60ms
step:617/1770 train_time:58031ms step_avg:95.60ms
step:618/1770 train_time:58128ms step_avg:95.61ms
step:619/1770 train_time:58226ms step_avg:95.61ms
step:620/1770 train_time:58324ms step_avg:95.61ms
step:621/1770 train_time:58421ms step_avg:95.62ms
step:622/1770 train_time:58519ms step_avg:95.62ms
step:623/1770 train_time:58616ms step_avg:95.62ms
step:624/1770 train_time:58713ms step_avg:95.62ms
step:625/1770 train_time:58810ms step_avg:95.63ms
step:625/1770 val_loss:3.6616 train_time:58906ms step_avg:95.78ms
step:626/1770 train_time:58932ms step_avg:95.67ms
step:627/1770 train_time:59016ms step_avg:95.65ms
step:628/1770 train_time:59116ms step_avg:95.66ms
step:629/1770 train_time:59213ms step_avg:95.66ms
step:630/1770 train_time:59310ms step_avg:95.66ms
step:631/1770 train_time:59407ms step_avg:95.66ms
step:632/1770 train_time:59504ms step_avg:95.67ms
step:633/1770 train_time:59601ms step_avg:95.67ms
step:634/1770 train_time:59697ms step_avg:95.67ms
step:635/1770 train_time:59794ms step_avg:95.67ms
step:636/1770 train_time:59891ms step_avg:95.67ms
step:637/1770 train_time:59989ms step_avg:95.68ms
step:638/1770 train_time:60087ms step_avg:95.68ms
step:639/1770 train_time:60185ms step_avg:95.68ms
step:640/1770 train_time:60283ms step_avg:95.69ms
step:641/1770 train_time:60380ms step_avg:95.69ms
step:642/1770 train_time:60478ms step_avg:95.69ms
step:643/1770 train_time:60575ms step_avg:95.70ms
step:644/1770 train_time:60672ms step_avg:95.70ms
step:645/1770 train_time:60769ms step_avg:95.70ms
step:646/1770 train_time:60866ms step_avg:95.70ms
step:647/1770 train_time:60964ms step_avg:95.70ms
step:648/1770 train_time:61061ms step_avg:95.71ms
step:649/1770 train_time:61159ms step_avg:95.71ms
step:650/1770 train_time:61257ms step_avg:95.71ms
step:651/1770 train_time:61355ms step_avg:95.72ms
step:652/1770 train_time:61452ms step_avg:95.72ms
step:653/1770 train_time:61549ms step_avg:95.72ms
step:654/1770 train_time:61647ms step_avg:95.72ms
step:655/1770 train_time:61744ms step_avg:95.73ms
step:656/1770 train_time:61841ms step_avg:95.73ms
step:657/1770 train_time:61938ms step_avg:95.73ms
step:658/1770 train_time:62037ms step_avg:95.74ms
step:659/1770 train_time:62137ms step_avg:95.74ms
step:660/1770 train_time:62236ms step_avg:95.75ms
step:661/1770 train_time:62335ms step_avg:95.75ms
step:662/1770 train_time:62434ms step_avg:95.76ms
step:663/1770 train_time:62533ms step_avg:95.76ms
step:664/1770 train_time:62633ms step_avg:95.77ms
step:665/1770 train_time:62732ms step_avg:95.77ms
step:666/1770 train_time:62832ms step_avg:95.78ms
step:667/1770 train_time:62930ms step_avg:95.78ms
step:668/1770 train_time:63028ms step_avg:95.79ms
step:669/1770 train_time:63127ms step_avg:95.79ms
step:670/1770 train_time:63226ms step_avg:95.80ms
step:671/1770 train_time:63325ms step_avg:95.80ms
step:672/1770 train_time:63424ms step_avg:95.81ms
step:673/1770 train_time:63524ms step_avg:95.81ms
step:674/1770 train_time:63623ms step_avg:95.82ms
step:675/1770 train_time:63722ms step_avg:95.82ms
step:676/1770 train_time:63822ms step_avg:95.83ms
step:677/1770 train_time:63922ms step_avg:95.83ms
step:678/1770 train_time:64021ms step_avg:95.84ms
step:679/1770 train_time:64120ms step_avg:95.84ms
step:680/1770 train_time:64219ms step_avg:95.85ms
step:681/1770 train_time:64319ms step_avg:95.86ms
step:682/1770 train_time:64419ms step_avg:95.86ms
step:683/1770 train_time:64519ms step_avg:95.87ms
step:684/1770 train_time:64618ms step_avg:95.87ms
step:685/1770 train_time:64718ms step_avg:95.88ms
step:686/1770 train_time:64817ms step_avg:95.88ms
step:687/1770 train_time:64917ms step_avg:95.89ms
step:688/1770 train_time:65016ms step_avg:95.89ms
step:689/1770 train_time:65116ms step_avg:95.90ms
step:690/1770 train_time:65215ms step_avg:95.90ms
step:691/1770 train_time:65315ms step_avg:95.91ms
step:692/1770 train_time:65414ms step_avg:95.91ms
step:693/1770 train_time:65513ms step_avg:95.92ms
step:694/1770 train_time:65612ms step_avg:95.92ms
step:695/1770 train_time:65710ms step_avg:95.93ms
step:696/1770 train_time:65809ms step_avg:95.93ms
step:697/1770 train_time:65908ms step_avg:95.94ms
step:698/1770 train_time:66007ms step_avg:95.94ms
step:699/1770 train_time:66105ms step_avg:95.94ms
step:700/1770 train_time:66205ms step_avg:95.95ms
step:701/1770 train_time:66304ms step_avg:95.95ms
step:702/1770 train_time:66404ms step_avg:95.96ms
step:703/1770 train_time:66504ms step_avg:95.96ms
step:704/1770 train_time:66603ms step_avg:95.97ms
step:705/1770 train_time:66702ms step_avg:95.97ms
step:706/1770 train_time:66802ms step_avg:95.98ms
step:707/1770 train_time:66902ms step_avg:95.98ms
step:708/1770 train_time:67001ms step_avg:95.99ms
step:709/1770 train_time:67101ms step_avg:96.00ms
step:710/1770 train_time:67200ms step_avg:96.00ms
step:711/1770 train_time:67300ms step_avg:96.01ms
step:712/1770 train_time:67399ms step_avg:96.01ms
step:713/1770 train_time:67498ms step_avg:96.01ms
step:714/1770 train_time:67598ms step_avg:96.02ms
step:715/1770 train_time:67698ms step_avg:96.02ms
step:716/1770 train_time:67797ms step_avg:96.03ms
step:717/1770 train_time:67897ms step_avg:96.04ms
step:718/1770 train_time:67997ms step_avg:96.04ms
step:719/1770 train_time:68096ms step_avg:96.05ms
step:720/1770 train_time:68195ms step_avg:96.05ms
step:721/1770 train_time:68294ms step_avg:96.05ms
step:722/1770 train_time:68393ms step_avg:96.06ms
step:723/1770 train_time:68491ms step_avg:96.06ms
step:724/1770 train_time:68590ms step_avg:96.06ms
step:725/1770 train_time:68689ms step_avg:96.07ms
step:726/1770 train_time:68788ms step_avg:96.07ms
step:727/1770 train_time:68887ms step_avg:96.08ms
step:728/1770 train_time:68986ms step_avg:96.08ms
step:729/1770 train_time:69086ms step_avg:96.09ms
step:730/1770 train_time:69185ms step_avg:96.09ms
step:731/1770 train_time:69284ms step_avg:96.09ms
step:732/1770 train_time:69384ms step_avg:96.10ms
step:733/1770 train_time:69483ms step_avg:96.10ms
step:734/1770 train_time:69582ms step_avg:96.11ms
step:735/1770 train_time:69681ms step_avg:96.11ms
step:736/1770 train_time:69780ms step_avg:96.12ms
step:737/1770 train_time:69879ms step_avg:96.12ms
step:738/1770 train_time:69979ms step_avg:96.13ms
step:739/1770 train_time:70078ms step_avg:96.13ms
step:740/1770 train_time:70178ms step_avg:96.13ms
step:741/1770 train_time:70278ms step_avg:96.14ms
step:742/1770 train_time:70378ms step_avg:96.14ms
step:743/1770 train_time:70478ms step_avg:96.15ms
step:744/1770 train_time:70577ms step_avg:96.15ms
step:745/1770 train_time:70676ms step_avg:96.16ms
step:746/1770 train_time:70776ms step_avg:96.16ms
step:747/1770 train_time:70875ms step_avg:96.17ms
step:748/1770 train_time:70975ms step_avg:96.17ms
step:749/1770 train_time:71074ms step_avg:96.18ms
step:750/1770 train_time:71173ms step_avg:96.18ms
step:750/1770 val_loss:3.6005 train_time:71271ms step_avg:96.31ms
step:751/1770 train_time:71293ms step_avg:96.21ms
step:752/1770 train_time:71382ms step_avg:96.20ms
step:753/1770 train_time:71483ms step_avg:96.21ms
step:754/1770 train_time:71583ms step_avg:96.21ms
step:755/1770 train_time:71682ms step_avg:96.22ms
step:756/1770 train_time:71781ms step_avg:96.22ms
step:757/1770 train_time:71880ms step_avg:96.22ms
step:758/1770 train_time:71978ms step_avg:96.23ms
step:759/1770 train_time:72077ms step_avg:96.23ms
step:760/1770 train_time:72175ms step_avg:96.23ms
step:761/1770 train_time:72274ms step_avg:96.24ms
step:762/1770 train_time:72374ms step_avg:96.24ms
step:763/1770 train_time:72474ms step_avg:96.25ms
step:764/1770 train_time:72573ms step_avg:96.25ms
step:765/1770 train_time:72672ms step_avg:96.25ms
step:766/1770 train_time:72771ms step_avg:96.26ms
step:767/1770 train_time:72870ms step_avg:96.26ms
step:768/1770 train_time:72970ms step_avg:96.27ms
step:769/1770 train_time:73069ms step_avg:96.27ms
step:770/1770 train_time:73168ms step_avg:96.27ms
step:771/1770 train_time:73268ms step_avg:96.28ms
step:772/1770 train_time:73368ms step_avg:96.28ms
step:773/1770 train_time:73467ms step_avg:96.29ms
step:774/1770 train_time:73568ms step_avg:96.29ms
step:775/1770 train_time:73667ms step_avg:96.30ms
step:776/1770 train_time:73766ms step_avg:96.30ms
step:777/1770 train_time:73865ms step_avg:96.30ms
step:778/1770 train_time:73964ms step_avg:96.31ms
step:779/1770 train_time:74063ms step_avg:96.31ms
step:780/1770 train_time:74162ms step_avg:96.31ms
step:781/1770 train_time:74261ms step_avg:96.32ms
step:782/1770 train_time:74360ms step_avg:96.32ms
step:783/1770 train_time:74460ms step_avg:96.33ms
step:784/1770 train_time:74561ms step_avg:96.33ms
step:785/1770 train_time:74661ms step_avg:96.34ms
step:786/1770 train_time:74761ms step_avg:96.34ms
step:787/1770 train_time:74860ms step_avg:96.34ms
step:788/1770 train_time:74959ms step_avg:96.35ms
step:789/1770 train_time:75059ms step_avg:96.35ms
step:790/1770 train_time:75159ms step_avg:96.36ms
step:791/1770 train_time:75258ms step_avg:96.36ms
step:792/1770 train_time:75357ms step_avg:96.36ms
step:793/1770 train_time:75457ms step_avg:96.37ms
step:794/1770 train_time:75558ms step_avg:96.37ms
step:795/1770 train_time:75658ms step_avg:96.38ms
step:796/1770 train_time:75758ms step_avg:96.38ms
step:797/1770 train_time:75858ms step_avg:96.39ms
step:798/1770 train_time:75958ms step_avg:96.39ms
step:799/1770 train_time:76058ms step_avg:96.40ms
step:800/1770 train_time:76158ms step_avg:96.40ms
step:801/1770 train_time:76257ms step_avg:96.41ms
step:802/1770 train_time:76356ms step_avg:96.41ms
step:803/1770 train_time:76455ms step_avg:96.41ms
step:804/1770 train_time:76554ms step_avg:96.42ms
step:805/1770 train_time:76654ms step_avg:96.42ms
step:806/1770 train_time:76752ms step_avg:96.42ms
step:807/1770 train_time:76852ms step_avg:96.43ms
step:808/1770 train_time:76950ms step_avg:96.43ms
step:809/1770 train_time:77050ms step_avg:96.43ms
step:810/1770 train_time:77151ms step_avg:96.44ms
step:811/1770 train_time:77250ms step_avg:96.44ms
step:812/1770 train_time:77350ms step_avg:96.45ms
step:813/1770 train_time:77450ms step_avg:96.45ms
step:814/1770 train_time:77550ms step_avg:96.46ms
step:815/1770 train_time:77649ms step_avg:96.46ms
step:816/1770 train_time:77748ms step_avg:96.46ms
step:817/1770 train_time:77848ms step_avg:96.47ms
step:818/1770 train_time:77948ms step_avg:96.47ms
step:819/1770 train_time:78047ms step_avg:96.47ms
step:820/1770 train_time:78147ms step_avg:96.48ms
step:821/1770 train_time:78247ms step_avg:96.48ms
step:822/1770 train_time:78347ms step_avg:96.49ms
step:823/1770 train_time:78447ms step_avg:96.49ms
step:824/1770 train_time:78547ms step_avg:96.50ms
step:825/1770 train_time:78647ms step_avg:96.50ms
step:826/1770 train_time:78747ms step_avg:96.50ms
step:827/1770 train_time:78847ms step_avg:96.51ms
step:828/1770 train_time:78946ms step_avg:96.51ms
step:829/1770 train_time:79046ms step_avg:96.52ms
step:830/1770 train_time:79146ms step_avg:96.52ms
step:831/1770 train_time:79246ms step_avg:96.52ms
step:832/1770 train_time:79346ms step_avg:96.53ms
step:833/1770 train_time:79446ms step_avg:96.53ms
step:834/1770 train_time:79546ms step_avg:96.54ms
step:835/1770 train_time:79645ms step_avg:96.54ms
step:836/1770 train_time:79744ms step_avg:96.54ms
step:837/1770 train_time:79844ms step_avg:96.55ms
step:838/1770 train_time:79943ms step_avg:96.55ms
step:839/1770 train_time:80043ms step_avg:96.55ms
step:840/1770 train_time:80142ms step_avg:96.56ms
step:841/1770 train_time:80242ms step_avg:96.56ms
step:842/1770 train_time:80342ms step_avg:96.56ms
step:843/1770 train_time:80442ms step_avg:96.57ms
step:844/1770 train_time:80542ms step_avg:96.57ms
step:845/1770 train_time:80642ms step_avg:96.58ms
step:846/1770 train_time:80741ms step_avg:96.58ms
step:847/1770 train_time:80842ms step_avg:96.58ms
step:848/1770 train_time:80941ms step_avg:96.59ms
step:849/1770 train_time:81041ms step_avg:96.59ms
step:850/1770 train_time:81141ms step_avg:96.60ms
step:851/1770 train_time:81240ms step_avg:96.60ms
step:852/1770 train_time:81339ms step_avg:96.60ms
step:853/1770 train_time:81438ms step_avg:96.61ms
step:854/1770 train_time:81537ms step_avg:96.61ms
step:855/1770 train_time:81637ms step_avg:96.61ms
step:856/1770 train_time:81736ms step_avg:96.61ms
step:857/1770 train_time:81835ms step_avg:96.62ms
step:858/1770 train_time:81934ms step_avg:96.62ms
step:859/1770 train_time:82033ms step_avg:96.62ms
step:860/1770 train_time:82132ms step_avg:96.63ms
step:861/1770 train_time:82231ms step_avg:96.63ms
step:862/1770 train_time:82330ms step_avg:96.63ms
step:863/1770 train_time:82429ms step_avg:96.63ms
step:864/1770 train_time:82528ms step_avg:96.64ms
step:865/1770 train_time:82627ms step_avg:96.64ms
step:866/1770 train_time:82727ms step_avg:96.64ms
step:867/1770 train_time:82828ms step_avg:96.65ms
step:868/1770 train_time:82928ms step_avg:96.65ms
step:869/1770 train_time:83028ms step_avg:96.66ms
step:870/1770 train_time:83128ms step_avg:96.66ms
step:871/1770 train_time:83228ms step_avg:96.66ms
step:872/1770 train_time:83328ms step_avg:96.67ms
step:873/1770 train_time:83427ms step_avg:96.67ms
step:874/1770 train_time:83527ms step_avg:96.67ms
step:875/1770 train_time:83626ms step_avg:96.68ms
step:875/1770 val_loss:3.5501 train_time:83724ms step_avg:96.79ms
step:876/1770 train_time:83746ms step_avg:96.70ms
step:877/1770 train_time:83834ms step_avg:96.69ms
step:878/1770 train_time:83935ms step_avg:96.70ms
step:879/1770 train_time:84035ms step_avg:96.70ms
step:880/1770 train_time:84134ms step_avg:96.71ms
step:881/1770 train_time:84233ms step_avg:96.71ms
step:882/1770 train_time:84332ms step_avg:96.71ms
step:883/1770 train_time:84430ms step_avg:96.71ms
step:884/1770 train_time:84529ms step_avg:96.72ms
step:885/1770 train_time:84628ms step_avg:96.72ms
step:886/1770 train_time:84727ms step_avg:96.72ms
step:887/1770 train_time:84826ms step_avg:96.72ms
step:888/1770 train_time:84925ms step_avg:96.73ms
step:889/1770 train_time:85025ms step_avg:96.73ms
step:890/1770 train_time:85124ms step_avg:96.73ms
step:891/1770 train_time:85224ms step_avg:96.74ms
step:892/1770 train_time:85325ms step_avg:96.74ms
step:893/1770 train_time:85424ms step_avg:96.74ms
step:894/1770 train_time:85523ms step_avg:96.75ms
step:895/1770 train_time:85623ms step_avg:96.75ms
step:896/1770 train_time:85722ms step_avg:96.75ms
step:897/1770 train_time:85820ms step_avg:96.75ms
step:898/1770 train_time:85920ms step_avg:96.76ms
step:899/1770 train_time:86019ms step_avg:96.76ms
step:900/1770 train_time:86119ms step_avg:96.76ms
step:901/1770 train_time:86219ms step_avg:96.77ms
step:902/1770 train_time:86319ms step_avg:96.77ms
step:903/1770 train_time:86419ms step_avg:96.77ms
step:904/1770 train_time:86519ms step_avg:96.78ms
step:905/1770 train_time:86619ms step_avg:96.78ms
step:906/1770 train_time:86719ms step_avg:96.78ms
step:907/1770 train_time:86818ms step_avg:96.79ms
step:908/1770 train_time:86917ms step_avg:96.79ms
step:909/1770 train_time:87018ms step_avg:96.79ms
step:910/1770 train_time:87117ms step_avg:96.80ms
step:911/1770 train_time:87217ms step_avg:96.80ms
step:912/1770 train_time:87318ms step_avg:96.80ms
step:913/1770 train_time:87418ms step_avg:96.81ms
step:914/1770 train_time:87517ms step_avg:96.81ms
step:915/1770 train_time:87618ms step_avg:96.81ms
step:916/1770 train_time:87718ms step_avg:96.82ms
step:917/1770 train_time:87817ms step_avg:96.82ms
step:918/1770 train_time:87916ms step_avg:96.82ms
step:919/1770 train_time:88015ms step_avg:96.83ms
step:920/1770 train_time:88117ms step_avg:96.83ms
step:921/1770 train_time:88218ms step_avg:96.84ms
step:922/1770 train_time:88319ms step_avg:96.84ms
step:923/1770 train_time:88420ms step_avg:96.85ms
step:924/1770 train_time:88521ms step_avg:96.85ms
step:925/1770 train_time:88621ms step_avg:96.85ms
step:926/1770 train_time:88723ms step_avg:96.86ms
step:927/1770 train_time:88823ms step_avg:96.86ms
step:928/1770 train_time:88925ms step_avg:96.87ms
step:929/1770 train_time:89025ms step_avg:96.87ms
step:930/1770 train_time:89126ms step_avg:96.88ms
step:931/1770 train_time:89227ms step_avg:96.88ms
step:932/1770 train_time:89327ms step_avg:96.88ms
step:933/1770 train_time:89428ms step_avg:96.89ms
step:934/1770 train_time:89529ms step_avg:96.89ms
step:935/1770 train_time:89629ms step_avg:96.90ms
step:936/1770 train_time:89730ms step_avg:96.90ms
step:937/1770 train_time:89830ms step_avg:96.90ms
step:938/1770 train_time:89931ms step_avg:96.91ms
step:939/1770 train_time:90032ms step_avg:96.91ms
step:940/1770 train_time:90133ms step_avg:96.92ms
step:941/1770 train_time:90233ms step_avg:96.92ms
step:942/1770 train_time:90334ms step_avg:96.92ms
step:943/1770 train_time:90435ms step_avg:96.93ms
step:944/1770 train_time:90536ms step_avg:96.93ms
step:945/1770 train_time:90637ms step_avg:96.94ms
step:946/1770 train_time:90738ms step_avg:96.94ms
step:947/1770 train_time:90839ms step_avg:96.95ms
step:948/1770 train_time:90939ms step_avg:96.95ms
step:949/1770 train_time:91041ms step_avg:96.96ms
step:950/1770 train_time:91142ms step_avg:96.96ms
step:951/1770 train_time:91244ms step_avg:96.96ms
step:952/1770 train_time:91344ms step_avg:96.97ms
step:953/1770 train_time:91445ms step_avg:96.97ms
step:954/1770 train_time:91546ms step_avg:96.98ms
step:955/1770 train_time:91648ms step_avg:96.98ms
step:956/1770 train_time:91749ms step_avg:96.99ms
step:957/1770 train_time:91849ms step_avg:96.99ms
step:958/1770 train_time:91950ms step_avg:96.99ms
step:959/1770 train_time:92050ms step_avg:97.00ms
step:960/1770 train_time:92150ms step_avg:97.00ms
step:961/1770 train_time:92250ms step_avg:97.00ms
step:962/1770 train_time:92351ms step_avg:97.01ms
step:963/1770 train_time:92451ms step_avg:97.01ms
step:964/1770 train_time:92552ms step_avg:97.01ms
step:965/1770 train_time:92653ms step_avg:97.02ms
step:966/1770 train_time:92754ms step_avg:97.02ms
step:967/1770 train_time:92855ms step_avg:97.03ms
step:968/1770 train_time:92956ms step_avg:97.03ms
step:969/1770 train_time:93057ms step_avg:97.04ms
step:970/1770 train_time:93158ms step_avg:97.04ms
step:971/1770 train_time:93260ms step_avg:97.05ms
step:972/1770 train_time:93361ms step_avg:97.05ms
step:973/1770 train_time:93462ms step_avg:97.05ms
step:974/1770 train_time:93562ms step_avg:97.06ms
step:975/1770 train_time:93664ms step_avg:97.06ms
step:976/1770 train_time:93765ms step_avg:97.07ms
step:977/1770 train_time:93866ms step_avg:97.07ms
step:978/1770 train_time:93967ms step_avg:97.07ms
step:979/1770 train_time:94068ms step_avg:97.08ms
step:980/1770 train_time:94168ms step_avg:97.08ms
step:981/1770 train_time:94269ms step_avg:97.08ms
step:982/1770 train_time:94369ms step_avg:97.09ms
step:983/1770 train_time:94469ms step_avg:97.09ms
step:984/1770 train_time:94570ms step_avg:97.09ms
step:985/1770 train_time:94670ms step_avg:97.10ms
step:986/1770 train_time:94770ms step_avg:97.10ms
step:987/1770 train_time:94871ms step_avg:97.10ms
step:988/1770 train_time:94971ms step_avg:97.11ms
step:989/1770 train_time:95072ms step_avg:97.11ms
step:990/1770 train_time:95173ms step_avg:97.11ms
step:991/1770 train_time:95274ms step_avg:97.12ms
step:992/1770 train_time:95374ms step_avg:97.12ms
step:993/1770 train_time:95476ms step_avg:97.13ms
step:994/1770 train_time:95578ms step_avg:97.13ms
step:995/1770 train_time:95679ms step_avg:97.14ms
step:996/1770 train_time:95781ms step_avg:97.14ms
step:997/1770 train_time:95882ms step_avg:97.14ms
step:998/1770 train_time:95983ms step_avg:97.15ms
step:999/1770 train_time:96083ms step_avg:97.15ms
step:1000/1770 train_time:96184ms step_avg:97.16ms
step:1000/1770 val_loss:3.5121 train_time:96283ms step_avg:97.26ms
step:1001/1770 train_time:96305ms step_avg:97.18ms
step:1002/1770 train_time:96393ms step_avg:97.17ms
step:1003/1770 train_time:96497ms step_avg:97.18ms
step:1004/1770 train_time:96598ms step_avg:97.18ms
step:1005/1770 train_time:96698ms step_avg:97.18ms
step:1006/1770 train_time:96798ms step_avg:97.19ms
step:1007/1770 train_time:96898ms step_avg:97.19ms
step:1008/1770 train_time:96999ms step_avg:97.19ms
step:1009/1770 train_time:97099ms step_avg:97.20ms
step:1010/1770 train_time:97199ms step_avg:97.20ms
step:1011/1770 train_time:97303ms step_avg:97.21ms
step:1012/1770 train_time:97406ms step_avg:97.21ms
step:1013/1770 train_time:97508ms step_avg:97.22ms
step:1014/1770 train_time:97609ms step_avg:97.22ms
step:1015/1770 train_time:97709ms step_avg:97.22ms
step:1016/1770 train_time:97810ms step_avg:97.23ms
step:1017/1770 train_time:97910ms step_avg:97.23ms
step:1018/1770 train_time:98011ms step_avg:97.23ms
step:1019/1770 train_time:98110ms step_avg:97.24ms
step:1020/1770 train_time:98211ms step_avg:97.24ms
step:1021/1770 train_time:98312ms step_avg:97.24ms
step:1022/1770 train_time:98413ms step_avg:97.25ms
step:1023/1770 train_time:98513ms step_avg:97.25ms
step:1024/1770 train_time:98613ms step_avg:97.25ms
step:1025/1770 train_time:98714ms step_avg:97.25ms
step:1026/1770 train_time:98814ms step_avg:97.26ms
step:1027/1770 train_time:98915ms step_avg:97.26ms
step:1028/1770 train_time:99017ms step_avg:97.27ms
step:1029/1770 train_time:99118ms step_avg:97.27ms
step:1030/1770 train_time:99219ms step_avg:97.27ms
step:1031/1770 train_time:99320ms step_avg:97.28ms
step:1032/1770 train_time:99422ms step_avg:97.28ms
step:1033/1770 train_time:99523ms step_avg:97.29ms
step:1034/1770 train_time:99624ms step_avg:97.29ms
step:1035/1770 train_time:99725ms step_avg:97.29ms
step:1036/1770 train_time:99825ms step_avg:97.30ms
step:1037/1770 train_time:99926ms step_avg:97.30ms
step:1038/1770 train_time:100026ms step_avg:97.30ms
step:1039/1770 train_time:100128ms step_avg:97.31ms
step:1040/1770 train_time:100229ms step_avg:97.31ms
step:1041/1770 train_time:100330ms step_avg:97.31ms
step:1042/1770 train_time:100431ms step_avg:97.32ms
step:1043/1770 train_time:100531ms step_avg:97.32ms
step:1044/1770 train_time:100631ms step_avg:97.32ms
step:1045/1770 train_time:100731ms step_avg:97.32ms
step:1046/1770 train_time:100831ms step_avg:97.33ms
step:1047/1770 train_time:100932ms step_avg:97.33ms
step:1048/1770 train_time:101032ms step_avg:97.33ms
step:1049/1770 train_time:101132ms step_avg:97.34ms
step:1050/1770 train_time:101233ms step_avg:97.34ms
step:1051/1770 train_time:101334ms step_avg:97.34ms
step:1052/1770 train_time:101434ms step_avg:97.35ms
step:1053/1770 train_time:101535ms step_avg:97.35ms
step:1054/1770 train_time:101635ms step_avg:97.35ms
step:1055/1770 train_time:101736ms step_avg:97.35ms
step:1056/1770 train_time:101836ms step_avg:97.36ms
step:1057/1770 train_time:101938ms step_avg:97.36ms
step:1058/1770 train_time:102039ms step_avg:97.37ms
step:1059/1770 train_time:102140ms step_avg:97.37ms
step:1060/1770 train_time:102242ms step_avg:97.37ms
step:1061/1770 train_time:102343ms step_avg:97.38ms
step:1062/1770 train_time:102445ms step_avg:97.38ms
step:1063/1770 train_time:102547ms step_avg:97.39ms
step:1064/1770 train_time:102649ms step_avg:97.39ms
step:1065/1770 train_time:102750ms step_avg:97.39ms
step:1066/1770 train_time:102852ms step_avg:97.40ms
step:1067/1770 train_time:102953ms step_avg:97.40ms
step:1068/1770 train_time:103054ms step_avg:97.40ms
step:1069/1770 train_time:103154ms step_avg:97.41ms
step:1070/1770 train_time:103254ms step_avg:97.41ms
step:1071/1770 train_time:103356ms step_avg:97.41ms
step:1072/1770 train_time:103457ms step_avg:97.42ms
step:1073/1770 train_time:103558ms step_avg:97.42ms
step:1074/1770 train_time:103660ms step_avg:97.42ms
step:1075/1770 train_time:103762ms step_avg:97.43ms
step:1076/1770 train_time:103865ms step_avg:97.43ms
step:1077/1770 train_time:103966ms step_avg:97.44ms
step:1078/1770 train_time:104066ms step_avg:97.44ms
step:1079/1770 train_time:104167ms step_avg:97.44ms
step:1080/1770 train_time:104268ms step_avg:97.45ms
step:1081/1770 train_time:104368ms step_avg:97.45ms
step:1082/1770 train_time:104470ms step_avg:97.45ms
step:1083/1770 train_time:104571ms step_avg:97.46ms
step:1084/1770 train_time:104672ms step_avg:97.46ms
step:1085/1770 train_time:104773ms step_avg:97.46ms
step:1086/1770 train_time:104873ms step_avg:97.47ms
step:1087/1770 train_time:104973ms step_avg:97.47ms
step:1088/1770 train_time:105074ms step_avg:97.47ms
step:1089/1770 train_time:105174ms step_avg:97.47ms
step:1090/1770 train_time:105275ms step_avg:97.48ms
step:1091/1770 train_time:105376ms step_avg:97.48ms
step:1092/1770 train_time:105477ms step_avg:97.48ms
step:1093/1770 train_time:105579ms step_avg:97.49ms
step:1094/1770 train_time:105681ms step_avg:97.49ms
step:1095/1770 train_time:105782ms step_avg:97.50ms
step:1096/1770 train_time:105884ms step_avg:97.50ms
step:1097/1770 train_time:105985ms step_avg:97.50ms
step:1098/1770 train_time:106085ms step_avg:97.50ms
step:1099/1770 train_time:106186ms step_avg:97.51ms
step:1100/1770 train_time:106287ms step_avg:97.51ms
step:1101/1770 train_time:106389ms step_avg:97.51ms
step:1102/1770 train_time:106490ms step_avg:97.52ms
step:1103/1770 train_time:106591ms step_avg:97.52ms
step:1104/1770 train_time:106692ms step_avg:97.53ms
step:1105/1770 train_time:106793ms step_avg:97.53ms
step:1106/1770 train_time:106893ms step_avg:97.53ms
step:1107/1770 train_time:106993ms step_avg:97.53ms
step:1108/1770 train_time:107094ms step_avg:97.54ms
step:1109/1770 train_time:107195ms step_avg:97.54ms
step:1110/1770 train_time:107296ms step_avg:97.54ms
step:1111/1770 train_time:107399ms step_avg:97.55ms
step:1112/1770 train_time:107502ms step_avg:97.55ms
step:1113/1770 train_time:107603ms step_avg:97.55ms
step:1114/1770 train_time:107703ms step_avg:97.56ms
step:1115/1770 train_time:107804ms step_avg:97.56ms
step:1116/1770 train_time:107905ms step_avg:97.56ms
step:1117/1770 train_time:108007ms step_avg:97.57ms
step:1118/1770 train_time:108108ms step_avg:97.57ms
step:1119/1770 train_time:108210ms step_avg:97.57ms
step:1120/1770 train_time:108311ms step_avg:97.58ms
step:1121/1770 train_time:108412ms step_avg:97.58ms
step:1122/1770 train_time:108512ms step_avg:97.58ms
step:1123/1770 train_time:108612ms step_avg:97.59ms
step:1124/1770 train_time:108714ms step_avg:97.59ms
step:1125/1770 train_time:108814ms step_avg:97.59ms
step:1125/1770 val_loss:3.4718 train_time:108913ms step_avg:97.68ms
step:1126/1770 train_time:108935ms step_avg:97.61ms
step:1127/1770 train_time:109028ms step_avg:97.61ms
step:1128/1770 train_time:109130ms step_avg:97.61ms
step:1129/1770 train_time:109231ms step_avg:97.61ms
step:1130/1770 train_time:109331ms step_avg:97.62ms
step:1131/1770 train_time:109432ms step_avg:97.62ms
step:1132/1770 train_time:109533ms step_avg:97.62ms
step:1133/1770 train_time:109633ms step_avg:97.63ms
step:1134/1770 train_time:109734ms step_avg:97.63ms
step:1135/1770 train_time:109835ms step_avg:97.63ms
step:1136/1770 train_time:109938ms step_avg:97.64ms
step:1137/1770 train_time:110041ms step_avg:97.64ms
step:1138/1770 train_time:110142ms step_avg:97.64ms
step:1139/1770 train_time:110243ms step_avg:97.65ms
step:1140/1770 train_time:110344ms step_avg:97.65ms
step:1141/1770 train_time:110443ms step_avg:97.65ms
step:1142/1770 train_time:110544ms step_avg:97.65ms
step:1143/1770 train_time:110646ms step_avg:97.66ms
step:1144/1770 train_time:110748ms step_avg:97.66ms
step:1145/1770 train_time:110849ms step_avg:97.66ms
step:1146/1770 train_time:110951ms step_avg:97.67ms
step:1147/1770 train_time:111053ms step_avg:97.67ms
step:1148/1770 train_time:111153ms step_avg:97.67ms
step:1149/1770 train_time:111254ms step_avg:97.68ms
step:1150/1770 train_time:111355ms step_avg:97.68ms
step:1151/1770 train_time:111457ms step_avg:97.68ms
step:1152/1770 train_time:111558ms step_avg:97.69ms
step:1153/1770 train_time:111659ms step_avg:97.69ms
step:1154/1770 train_time:111760ms step_avg:97.69ms
step:1155/1770 train_time:111860ms step_avg:97.69ms
step:1156/1770 train_time:111961ms step_avg:97.70ms
step:1157/1770 train_time:112063ms step_avg:97.70ms
step:1158/1770 train_time:112164ms step_avg:97.70ms
step:1159/1770 train_time:112264ms step_avg:97.71ms
step:1160/1770 train_time:112366ms step_avg:97.71ms
step:1161/1770 train_time:112469ms step_avg:97.71ms
step:1162/1770 train_time:112571ms step_avg:97.72ms
step:1163/1770 train_time:112672ms step_avg:97.72ms
step:1164/1770 train_time:112772ms step_avg:97.72ms
step:1165/1770 train_time:112874ms step_avg:97.73ms
step:1166/1770 train_time:112976ms step_avg:97.73ms
step:1167/1770 train_time:113078ms step_avg:97.73ms
step:1168/1770 train_time:113180ms step_avg:97.74ms
step:1169/1770 train_time:113281ms step_avg:97.74ms
step:1170/1770 train_time:113381ms step_avg:97.74ms
step:1171/1770 train_time:113482ms step_avg:97.74ms
step:1172/1770 train_time:113582ms step_avg:97.75ms
step:1173/1770 train_time:113683ms step_avg:97.75ms
step:1174/1770 train_time:113785ms step_avg:97.75ms
step:1175/1770 train_time:113887ms step_avg:97.76ms
step:1176/1770 train_time:113989ms step_avg:97.76ms
step:1177/1770 train_time:114090ms step_avg:97.76ms
step:1178/1770 train_time:114191ms step_avg:97.77ms
step:1179/1770 train_time:114291ms step_avg:97.77ms
step:1180/1770 train_time:114392ms step_avg:97.77ms
step:1181/1770 train_time:114494ms step_avg:97.77ms
step:1182/1770 train_time:114594ms step_avg:97.78ms
step:1183/1770 train_time:114696ms step_avg:97.78ms
step:1184/1770 train_time:114799ms step_avg:97.78ms
step:1185/1770 train_time:114903ms step_avg:97.79ms
step:1186/1770 train_time:115005ms step_avg:97.79ms
step:1187/1770 train_time:115109ms step_avg:97.80ms
step:1188/1770 train_time:115211ms step_avg:97.80ms
step:1189/1770 train_time:115312ms step_avg:97.81ms
step:1190/1770 train_time:115414ms step_avg:97.81ms
step:1191/1770 train_time:115516ms step_avg:97.81ms
step:1192/1770 train_time:115619ms step_avg:97.82ms
step:1193/1770 train_time:115721ms step_avg:97.82ms
step:1194/1770 train_time:115822ms step_avg:97.82ms
step:1195/1770 train_time:115925ms step_avg:97.83ms
step:1196/1770 train_time:116028ms step_avg:97.83ms
step:1197/1770 train_time:116129ms step_avg:97.83ms
step:1198/1770 train_time:116231ms step_avg:97.84ms
step:1199/1770 train_time:116333ms step_avg:97.84ms
step:1200/1770 train_time:116435ms step_avg:97.84ms
step:1201/1770 train_time:116539ms step_avg:97.85ms
step:1202/1770 train_time:116641ms step_avg:97.85ms
step:1203/1770 train_time:116743ms step_avg:97.86ms
step:1204/1770 train_time:116845ms step_avg:97.86ms
step:1205/1770 train_time:116947ms step_avg:97.86ms
step:1206/1770 train_time:117051ms step_avg:97.87ms
step:1207/1770 train_time:117152ms step_avg:97.87ms
step:1208/1770 train_time:117254ms step_avg:97.87ms
step:1209/1770 train_time:117356ms step_avg:97.88ms
step:1210/1770 train_time:117457ms step_avg:97.88ms
step:1211/1770 train_time:117560ms step_avg:97.89ms
step:1212/1770 train_time:117664ms step_avg:97.89ms
step:1213/1770 train_time:117766ms step_avg:97.89ms
step:1214/1770 train_time:117867ms step_avg:97.90ms
step:1215/1770 train_time:117969ms step_avg:97.90ms
step:1216/1770 train_time:118074ms step_avg:97.91ms
step:1217/1770 train_time:118176ms step_avg:97.91ms
step:1218/1770 train_time:118277ms step_avg:97.91ms
step:1219/1770 train_time:118379ms step_avg:97.92ms
step:1220/1770 train_time:118482ms step_avg:97.92ms
step:1221/1770 train_time:118583ms step_avg:97.92ms
step:1222/1770 train_time:118687ms step_avg:97.93ms
step:1223/1770 train_time:118788ms step_avg:97.93ms
step:1224/1770 train_time:118891ms step_avg:97.93ms
step:1225/1770 train_time:118994ms step_avg:97.94ms
step:1226/1770 train_time:119096ms step_avg:97.94ms
step:1227/1770 train_time:119200ms step_avg:97.95ms
step:1228/1770 train_time:119304ms step_avg:97.95ms
step:1229/1770 train_time:119406ms step_avg:97.95ms
step:1230/1770 train_time:119508ms step_avg:97.96ms
step:1231/1770 train_time:119610ms step_avg:97.96ms
step:1232/1770 train_time:119712ms step_avg:97.96ms
step:1233/1770 train_time:119814ms step_avg:97.97ms
step:1234/1770 train_time:119916ms step_avg:97.97ms
step:1235/1770 train_time:120018ms step_avg:97.97ms
step:1236/1770 train_time:120121ms step_avg:97.98ms
step:1237/1770 train_time:120223ms step_avg:97.98ms
step:1238/1770 train_time:120325ms step_avg:97.98ms
step:1239/1770 train_time:120427ms step_avg:97.99ms
step:1240/1770 train_time:120529ms step_avg:97.99ms
step:1241/1770 train_time:120632ms step_avg:97.99ms
step:1242/1770 train_time:120734ms step_avg:98.00ms
step:1243/1770 train_time:120837ms step_avg:98.00ms
step:1244/1770 train_time:120938ms step_avg:98.01ms
step:1245/1770 train_time:121040ms step_avg:98.01ms
step:1246/1770 train_time:121143ms step_avg:98.01ms
step:1247/1770 train_time:121244ms step_avg:98.01ms
step:1248/1770 train_time:121347ms step_avg:98.02ms
step:1249/1770 train_time:121449ms step_avg:98.02ms
step:1250/1770 train_time:121551ms step_avg:98.03ms
step:1250/1770 val_loss:3.4240 train_time:121653ms step_avg:98.11ms
step:1251/1770 train_time:121675ms step_avg:98.05ms
step:1252/1770 train_time:121766ms step_avg:98.04ms
step:1253/1770 train_time:121869ms step_avg:98.04ms
step:1254/1770 train_time:121971ms step_avg:98.05ms
step:1255/1770 train_time:122076ms step_avg:98.05ms
step:1256/1770 train_time:122177ms step_avg:98.06ms
step:1257/1770 train_time:122278ms step_avg:98.06ms
step:1258/1770 train_time:122380ms step_avg:98.06ms
step:1259/1770 train_time:122482ms step_avg:98.06ms
step:1260/1770 train_time:122583ms step_avg:98.07ms
step:1261/1770 train_time:122688ms step_avg:98.07ms
step:1262/1770 train_time:122791ms step_avg:98.08ms
step:1263/1770 train_time:122893ms step_avg:98.08ms
step:1264/1770 train_time:122997ms step_avg:98.08ms
step:1265/1770 train_time:123099ms step_avg:98.09ms
step:1266/1770 train_time:123201ms step_avg:98.09ms
step:1267/1770 train_time:123304ms step_avg:98.09ms
step:1268/1770 train_time:123406ms step_avg:98.10ms
step:1269/1770 train_time:123508ms step_avg:98.10ms
step:1270/1770 train_time:123610ms step_avg:98.10ms
step:1271/1770 train_time:123712ms step_avg:98.11ms
step:1272/1770 train_time:123814ms step_avg:98.11ms
step:1273/1770 train_time:123917ms step_avg:98.11ms
step:1274/1770 train_time:124019ms step_avg:98.12ms
step:1275/1770 train_time:124121ms step_avg:98.12ms
step:1276/1770 train_time:124222ms step_avg:98.12ms
step:1277/1770 train_time:124324ms step_avg:98.12ms
step:1278/1770 train_time:124427ms step_avg:98.13ms
step:1279/1770 train_time:124529ms step_avg:98.13ms
step:1280/1770 train_time:124632ms step_avg:98.14ms
step:1281/1770 train_time:124735ms step_avg:98.14ms
step:1282/1770 train_time:124838ms step_avg:98.14ms
step:1283/1770 train_time:124940ms step_avg:98.15ms
step:1284/1770 train_time:125042ms step_avg:98.15ms
step:1285/1770 train_time:125144ms step_avg:98.15ms
step:1286/1770 train_time:125248ms step_avg:98.16ms
step:1287/1770 train_time:125351ms step_avg:98.16ms
step:1288/1770 train_time:125453ms step_avg:98.16ms
step:1289/1770 train_time:125556ms step_avg:98.17ms
step:1290/1770 train_time:125658ms step_avg:98.17ms
step:1291/1770 train_time:125760ms step_avg:98.17ms
step:1292/1770 train_time:125862ms step_avg:98.18ms
step:1293/1770 train_time:125965ms step_avg:98.18ms
step:1294/1770 train_time:126067ms step_avg:98.18ms
step:1295/1770 train_time:126170ms step_avg:98.19ms
step:1296/1770 train_time:126271ms step_avg:98.19ms
step:1297/1770 train_time:126373ms step_avg:98.19ms
step:1298/1770 train_time:126476ms step_avg:98.20ms
step:1299/1770 train_time:126578ms step_avg:98.20ms
step:1300/1770 train_time:126680ms step_avg:98.20ms
step:1301/1770 train_time:126782ms step_avg:98.20ms
step:1302/1770 train_time:126884ms step_avg:98.21ms
step:1303/1770 train_time:126986ms step_avg:98.21ms
step:1304/1770 train_time:127089ms step_avg:98.21ms
step:1305/1770 train_time:127191ms step_avg:98.22ms
step:1306/1770 train_time:127292ms step_avg:98.22ms
step:1307/1770 train_time:127395ms step_avg:98.22ms
step:1308/1770 train_time:127497ms step_avg:98.23ms
step:1309/1770 train_time:127599ms step_avg:98.23ms
step:1310/1770 train_time:127700ms step_avg:98.23ms
step:1311/1770 train_time:127802ms step_avg:98.23ms
step:1312/1770 train_time:127904ms step_avg:98.24ms
step:1313/1770 train_time:128006ms step_avg:98.24ms
step:1314/1770 train_time:128109ms step_avg:98.24ms
step:1315/1770 train_time:128211ms step_avg:98.25ms
step:1316/1770 train_time:128313ms step_avg:98.25ms
step:1317/1770 train_time:128415ms step_avg:98.25ms
step:1318/1770 train_time:128520ms step_avg:98.26ms
step:1319/1770 train_time:128622ms step_avg:98.26ms
step:1320/1770 train_time:128724ms step_avg:98.26ms
step:1321/1770 train_time:128827ms step_avg:98.27ms
step:1322/1770 train_time:128929ms step_avg:98.27ms
step:1323/1770 train_time:129031ms step_avg:98.27ms
step:1324/1770 train_time:129134ms step_avg:98.28ms
step:1325/1770 train_time:129238ms step_avg:98.28ms
step:1326/1770 train_time:129340ms step_avg:98.28ms
step:1327/1770 train_time:129445ms step_avg:98.29ms
step:1328/1770 train_time:129547ms step_avg:98.29ms
step:1329/1770 train_time:129649ms step_avg:98.29ms
step:1330/1770 train_time:129751ms step_avg:98.30ms
step:1331/1770 train_time:129854ms step_avg:98.30ms
step:1332/1770 train_time:129956ms step_avg:98.30ms
step:1333/1770 train_time:130057ms step_avg:98.30ms
step:1334/1770 train_time:130159ms step_avg:98.31ms
step:1335/1770 train_time:130261ms step_avg:98.31ms
step:1336/1770 train_time:130363ms step_avg:98.31ms
step:1337/1770 train_time:130465ms step_avg:98.32ms
step:1338/1770 train_time:130567ms step_avg:98.32ms
step:1339/1770 train_time:130670ms step_avg:98.32ms
step:1340/1770 train_time:130773ms step_avg:98.33ms
step:1341/1770 train_time:130875ms step_avg:98.33ms
step:1342/1770 train_time:130978ms step_avg:98.33ms
step:1343/1770 train_time:131081ms step_avg:98.34ms
step:1344/1770 train_time:131183ms step_avg:98.34ms
step:1345/1770 train_time:131285ms step_avg:98.34ms
step:1346/1770 train_time:131387ms step_avg:98.34ms
step:1347/1770 train_time:131490ms step_avg:98.35ms
step:1348/1770 train_time:131594ms step_avg:98.35ms
step:1349/1770 train_time:131697ms step_avg:98.35ms
step:1350/1770 train_time:131798ms step_avg:98.36ms
step:1351/1770 train_time:131900ms step_avg:98.36ms
step:1352/1770 train_time:132002ms step_avg:98.36ms
step:1353/1770 train_time:132106ms step_avg:98.37ms
step:1354/1770 train_time:132208ms step_avg:98.37ms
step:1355/1770 train_time:132311ms step_avg:98.37ms
step:1356/1770 train_time:132413ms step_avg:98.37ms
step:1357/1770 train_time:132515ms step_avg:98.38ms
step:1358/1770 train_time:132617ms step_avg:98.38ms
step:1359/1770 train_time:132719ms step_avg:98.38ms
step:1360/1770 train_time:132822ms step_avg:98.39ms
step:1361/1770 train_time:132925ms step_avg:98.39ms
step:1362/1770 train_time:133028ms step_avg:98.39ms
step:1363/1770 train_time:133130ms step_avg:98.40ms
step:1364/1770 train_time:133233ms step_avg:98.40ms
step:1365/1770 train_time:133335ms step_avg:98.40ms
step:1366/1770 train_time:133436ms step_avg:98.40ms
step:1367/1770 train_time:133539ms step_avg:98.41ms
step:1368/1770 train_time:133640ms step_avg:98.41ms
step:1369/1770 train_time:133743ms step_avg:98.41ms
step:1370/1770 train_time:133846ms step_avg:98.42ms
step:1371/1770 train_time:133948ms step_avg:98.42ms
step:1372/1770 train_time:134050ms step_avg:98.42ms
step:1373/1770 train_time:134153ms step_avg:98.42ms
step:1374/1770 train_time:134256ms step_avg:98.43ms
step:1375/1770 train_time:134358ms step_avg:98.43ms
step:1375/1770 val_loss:3.3815 train_time:134460ms step_avg:98.51ms
step:1376/1770 train_time:134482ms step_avg:98.45ms
step:1377/1770 train_time:134574ms step_avg:98.44ms
step:1378/1770 train_time:134676ms step_avg:98.45ms
step:1379/1770 train_time:134777ms step_avg:98.45ms
step:1380/1770 train_time:134879ms step_avg:98.45ms
step:1381/1770 train_time:134981ms step_avg:98.45ms
step:1382/1770 train_time:135083ms step_avg:98.46ms
step:1383/1770 train_time:135186ms step_avg:98.46ms
step:1384/1770 train_time:135288ms step_avg:98.46ms
step:1385/1770 train_time:135390ms step_avg:98.47ms
step:1386/1770 train_time:135494ms step_avg:98.47ms
step:1387/1770 train_time:135597ms step_avg:98.47ms
step:1388/1770 train_time:135699ms step_avg:98.48ms
step:1389/1770 train_time:135802ms step_avg:98.48ms
step:1390/1770 train_time:135903ms step_avg:98.48ms
step:1391/1770 train_time:136005ms step_avg:98.48ms
step:1392/1770 train_time:136107ms step_avg:98.49ms
step:1393/1770 train_time:136209ms step_avg:98.49ms
step:1394/1770 train_time:136310ms step_avg:98.49ms
step:1395/1770 train_time:136414ms step_avg:98.49ms
step:1396/1770 train_time:136518ms step_avg:98.50ms
step:1397/1770 train_time:136620ms step_avg:98.50ms
step:1398/1770 train_time:136723ms step_avg:98.50ms
step:1399/1770 train_time:136825ms step_avg:98.51ms
step:1400/1770 train_time:136928ms step_avg:98.51ms
step:1401/1770 train_time:137029ms step_avg:98.51ms
step:1402/1770 train_time:137131ms step_avg:98.51ms
step:1403/1770 train_time:137233ms step_avg:98.52ms
step:1404/1770 train_time:137337ms step_avg:98.52ms
step:1405/1770 train_time:137438ms step_avg:98.52ms
step:1406/1770 train_time:137541ms step_avg:98.53ms
step:1407/1770 train_time:137643ms step_avg:98.53ms
step:1408/1770 train_time:137746ms step_avg:98.53ms
step:1409/1770 train_time:137848ms step_avg:98.53ms
step:1410/1770 train_time:137951ms step_avg:98.54ms
step:1411/1770 train_time:138052ms step_avg:98.54ms
step:1412/1770 train_time:138154ms step_avg:98.54ms
step:1413/1770 train_time:138256ms step_avg:98.54ms
step:1414/1770 train_time:138359ms step_avg:98.55ms
step:1415/1770 train_time:138462ms step_avg:98.55ms
step:1416/1770 train_time:138566ms step_avg:98.55ms
step:1417/1770 train_time:138668ms step_avg:98.56ms
step:1418/1770 train_time:138770ms step_avg:98.56ms
step:1419/1770 train_time:138873ms step_avg:98.56ms
step:1420/1770 train_time:138976ms step_avg:98.56ms
step:1421/1770 train_time:139078ms step_avg:98.57ms
step:1422/1770 train_time:139180ms step_avg:98.57ms
step:1423/1770 train_time:139282ms step_avg:98.57ms
step:1424/1770 train_time:139385ms step_avg:98.57ms
step:1425/1770 train_time:139487ms step_avg:98.58ms
step:1426/1770 train_time:139590ms step_avg:98.58ms
step:1427/1770 train_time:139691ms step_avg:98.58ms
step:1428/1770 train_time:139796ms step_avg:98.59ms
step:1429/1770 train_time:139898ms step_avg:98.59ms
step:1430/1770 train_time:140000ms step_avg:98.59ms
step:1431/1770 train_time:140103ms step_avg:98.59ms
step:1432/1770 train_time:140204ms step_avg:98.60ms
step:1433/1770 train_time:140306ms step_avg:98.60ms
step:1434/1770 train_time:140408ms step_avg:98.60ms
step:1435/1770 train_time:140509ms step_avg:98.60ms
step:1436/1770 train_time:140615ms step_avg:98.61ms
step:1437/1770 train_time:140717ms step_avg:98.61ms
step:1438/1770 train_time:140819ms step_avg:98.61ms
step:1439/1770 train_time:140921ms step_avg:98.61ms
step:1440/1770 train_time:141022ms step_avg:98.62ms
step:1441/1770 train_time:141127ms step_avg:98.62ms
step:1442/1770 train_time:141229ms step_avg:98.62ms
step:1443/1770 train_time:141330ms step_avg:98.63ms
step:1444/1770 train_time:141433ms step_avg:98.63ms
step:1445/1770 train_time:141536ms step_avg:98.63ms
step:1446/1770 train_time:141639ms step_avg:98.63ms
step:1447/1770 train_time:141745ms step_avg:98.64ms
step:1448/1770 train_time:141848ms step_avg:98.64ms
step:1449/1770 train_time:141953ms step_avg:98.65ms
step:1450/1770 train_time:142056ms step_avg:98.65ms
step:1451/1770 train_time:142160ms step_avg:98.65ms
step:1452/1770 train_time:142264ms step_avg:98.66ms
step:1453/1770 train_time:142367ms step_avg:98.66ms
step:1454/1770 train_time:142470ms step_avg:98.66ms
step:1455/1770 train_time:142574ms step_avg:98.67ms
step:1456/1770 train_time:142680ms step_avg:98.67ms
step:1457/1770 train_time:142783ms step_avg:98.68ms
step:1458/1770 train_time:142887ms step_avg:98.68ms
step:1459/1770 train_time:142991ms step_avg:98.68ms
step:1460/1770 train_time:143095ms step_avg:98.69ms
step:1461/1770 train_time:143199ms step_avg:98.69ms
step:1462/1770 train_time:143302ms step_avg:98.69ms
step:1463/1770 train_time:143405ms step_avg:98.70ms
step:1464/1770 train_time:143509ms step_avg:98.70ms
step:1465/1770 train_time:143614ms step_avg:98.70ms
step:1466/1770 train_time:143719ms step_avg:98.71ms
step:1467/1770 train_time:143823ms step_avg:98.71ms
step:1468/1770 train_time:143926ms step_avg:98.71ms
step:1469/1770 train_time:144029ms step_avg:98.72ms
step:1470/1770 train_time:144132ms step_avg:98.72ms
step:1471/1770 train_time:144236ms step_avg:98.72ms
step:1472/1770 train_time:144339ms step_avg:98.73ms
step:1473/1770 train_time:144444ms step_avg:98.73ms
step:1474/1770 train_time:144549ms step_avg:98.74ms
step:1475/1770 train_time:144652ms step_avg:98.74ms
step:1476/1770 train_time:144755ms step_avg:98.74ms
step:1477/1770 train_time:144861ms step_avg:98.75ms
step:1478/1770 train_time:144964ms step_avg:98.75ms
step:1479/1770 train_time:145067ms step_avg:98.75ms
step:1480/1770 train_time:145170ms step_avg:98.76ms
step:1481/1770 train_time:145277ms step_avg:98.76ms
step:1482/1770 train_time:145380ms step_avg:98.76ms
step:1483/1770 train_time:145484ms step_avg:98.77ms
step:1484/1770 train_time:145587ms step_avg:98.77ms
step:1485/1770 train_time:145689ms step_avg:98.77ms
step:1486/1770 train_time:145793ms step_avg:98.78ms
step:1487/1770 train_time:145896ms step_avg:98.78ms
step:1488/1770 train_time:146000ms step_avg:98.78ms
step:1489/1770 train_time:146104ms step_avg:98.79ms
step:1490/1770 train_time:146208ms step_avg:98.79ms
step:1491/1770 train_time:146311ms step_avg:98.79ms
step:1492/1770 train_time:146415ms step_avg:98.80ms
step:1493/1770 train_time:146521ms step_avg:98.80ms
step:1494/1770 train_time:146628ms step_avg:98.81ms
step:1495/1770 train_time:146730ms step_avg:98.81ms
step:1496/1770 train_time:146833ms step_avg:98.81ms
step:1497/1770 train_time:146937ms step_avg:98.81ms
step:1498/1770 train_time:147040ms step_avg:98.82ms
step:1499/1770 train_time:147143ms step_avg:98.82ms
step:1500/1770 train_time:147246ms step_avg:98.82ms
step:1500/1770 val_loss:3.3428 train_time:147348ms step_avg:98.89ms
step:1501/1770 train_time:147369ms step_avg:98.84ms
step:1502/1770 train_time:147462ms step_avg:98.83ms
step:1503/1770 train_time:147564ms step_avg:98.84ms
step:1504/1770 train_time:147667ms step_avg:98.84ms
step:1505/1770 train_time:147772ms step_avg:98.84ms
step:1506/1770 train_time:147876ms step_avg:98.85ms
step:1507/1770 train_time:147980ms step_avg:98.85ms
step:1508/1770 train_time:148085ms step_avg:98.85ms
step:1509/1770 train_time:148187ms step_avg:98.86ms
step:1510/1770 train_time:148289ms step_avg:98.86ms
step:1511/1770 train_time:148396ms step_avg:98.86ms
step:1512/1770 train_time:148500ms step_avg:98.87ms
step:1513/1770 train_time:148604ms step_avg:98.87ms
step:1514/1770 train_time:148707ms step_avg:98.87ms
step:1515/1770 train_time:148810ms step_avg:98.88ms
step:1516/1770 train_time:148914ms step_avg:98.88ms
step:1517/1770 train_time:149017ms step_avg:98.88ms
step:1518/1770 train_time:149123ms step_avg:98.89ms
step:1519/1770 train_time:149225ms step_avg:98.89ms
step:1520/1770 train_time:149330ms step_avg:98.89ms
step:1521/1770 train_time:149433ms step_avg:98.90ms
step:1522/1770 train_time:149537ms step_avg:98.90ms
step:1523/1770 train_time:149643ms step_avg:98.90ms
step:1524/1770 train_time:149746ms step_avg:98.91ms
step:1525/1770 train_time:149848ms step_avg:98.91ms
step:1526/1770 train_time:149952ms step_avg:98.91ms
step:1527/1770 train_time:150057ms step_avg:98.92ms
step:1528/1770 train_time:150163ms step_avg:98.92ms
step:1529/1770 train_time:150265ms step_avg:98.92ms
step:1530/1770 train_time:150368ms step_avg:98.93ms
step:1531/1770 train_time:150472ms step_avg:98.93ms
step:1532/1770 train_time:150577ms step_avg:98.93ms
step:1533/1770 train_time:150681ms step_avg:98.94ms
step:1534/1770 train_time:150785ms step_avg:98.94ms
step:1535/1770 train_time:150887ms step_avg:98.94ms
step:1536/1770 train_time:150990ms step_avg:98.94ms
step:1537/1770 train_time:151094ms step_avg:98.95ms
step:1538/1770 train_time:151198ms step_avg:98.95ms
step:1539/1770 train_time:151301ms step_avg:98.95ms
step:1540/1770 train_time:151408ms step_avg:98.96ms
step:1541/1770 train_time:151511ms step_avg:98.96ms
step:1542/1770 train_time:151615ms step_avg:98.97ms
step:1543/1770 train_time:151718ms step_avg:98.97ms
step:1544/1770 train_time:151824ms step_avg:98.97ms
step:1545/1770 train_time:151927ms step_avg:98.98ms
step:1546/1770 train_time:152030ms step_avg:98.98ms
step:1547/1770 train_time:152132ms step_avg:98.98ms
step:1548/1770 train_time:152237ms step_avg:98.98ms
step:1549/1770 train_time:152340ms step_avg:98.99ms
step:1550/1770 train_time:152444ms step_avg:98.99ms
step:1551/1770 train_time:152547ms step_avg:98.99ms
step:1552/1770 train_time:152652ms step_avg:99.00ms
step:1553/1770 train_time:152755ms step_avg:99.00ms
step:1554/1770 train_time:152857ms step_avg:99.00ms
step:1555/1770 train_time:152961ms step_avg:99.00ms
step:1556/1770 train_time:153064ms step_avg:99.01ms
step:1557/1770 train_time:153167ms step_avg:99.01ms
step:1558/1770 train_time:153271ms step_avg:99.01ms
step:1559/1770 train_time:153375ms step_avg:99.02ms
step:1560/1770 train_time:153477ms step_avg:99.02ms
step:1561/1770 train_time:153582ms step_avg:99.02ms
step:1562/1770 train_time:153685ms step_avg:99.02ms
step:1563/1770 train_time:153789ms step_avg:99.03ms
step:1564/1770 train_time:153892ms step_avg:99.03ms
step:1565/1770 train_time:153996ms step_avg:99.03ms
step:1566/1770 train_time:154098ms step_avg:99.03ms
step:1567/1770 train_time:154202ms step_avg:99.04ms
step:1568/1770 train_time:154305ms step_avg:99.04ms
step:1569/1770 train_time:154412ms step_avg:99.05ms
step:1570/1770 train_time:154515ms step_avg:99.05ms
step:1571/1770 train_time:154617ms step_avg:99.05ms
step:1572/1770 train_time:154722ms step_avg:99.05ms
step:1573/1770 train_time:154827ms step_avg:99.06ms
step:1574/1770 train_time:154930ms step_avg:99.06ms
step:1575/1770 train_time:155033ms step_avg:99.06ms
step:1576/1770 train_time:155137ms step_avg:99.07ms
step:1577/1770 train_time:155241ms step_avg:99.07ms
step:1578/1770 train_time:155345ms step_avg:99.07ms
step:1579/1770 train_time:155448ms step_avg:99.07ms
step:1580/1770 train_time:155551ms step_avg:99.08ms
step:1581/1770 train_time:155658ms step_avg:99.08ms
step:1582/1770 train_time:155763ms step_avg:99.09ms
step:1583/1770 train_time:155866ms step_avg:99.09ms
step:1584/1770 train_time:155969ms step_avg:99.09ms
step:1585/1770 train_time:156074ms step_avg:99.09ms
step:1586/1770 train_time:156181ms step_avg:99.10ms
step:1587/1770 train_time:156284ms step_avg:99.10ms
step:1588/1770 train_time:156387ms step_avg:99.10ms
step:1589/1770 train_time:156492ms step_avg:99.11ms
step:1590/1770 train_time:156595ms step_avg:99.11ms
step:1591/1770 train_time:156699ms step_avg:99.11ms
step:1592/1770 train_time:156804ms step_avg:99.12ms
step:1593/1770 train_time:156907ms step_avg:99.12ms
step:1594/1770 train_time:157010ms step_avg:99.12ms
step:1595/1770 train_time:157113ms step_avg:99.12ms
step:1596/1770 train_time:157217ms step_avg:99.13ms
step:1597/1770 train_time:157320ms step_avg:99.13ms
step:1598/1770 train_time:157425ms step_avg:99.13ms
step:1599/1770 train_time:157530ms step_avg:99.14ms
step:1600/1770 train_time:157636ms step_avg:99.14ms
step:1601/1770 train_time:157739ms step_avg:99.14ms
step:1602/1770 train_time:157843ms step_avg:99.15ms
step:1603/1770 train_time:157946ms step_avg:99.15ms
step:1604/1770 train_time:158047ms step_avg:99.15ms
step:1605/1770 train_time:158151ms step_avg:99.15ms
step:1606/1770 train_time:158255ms step_avg:99.16ms
step:1607/1770 train_time:158361ms step_avg:99.16ms
step:1608/1770 train_time:158465ms step_avg:99.16ms
step:1609/1770 train_time:158569ms step_avg:99.17ms
step:1610/1770 train_time:158674ms step_avg:99.17ms
step:1611/1770 train_time:158780ms step_avg:99.18ms
step:1612/1770 train_time:158885ms step_avg:99.18ms
step:1613/1770 train_time:158987ms step_avg:99.18ms
step:1614/1770 train_time:159090ms step_avg:99.18ms
step:1615/1770 train_time:159194ms step_avg:99.19ms
step:1616/1770 train_time:159297ms step_avg:99.19ms
step:1617/1770 train_time:159403ms step_avg:99.19ms
step:1618/1770 train_time:159507ms step_avg:99.20ms
step:1619/1770 train_time:159611ms step_avg:99.20ms
step:1620/1770 train_time:159715ms step_avg:99.20ms
step:1621/1770 train_time:159818ms step_avg:99.20ms
step:1622/1770 train_time:159922ms step_avg:99.21ms
step:1623/1770 train_time:160028ms step_avg:99.21ms
step:1624/1770 train_time:160131ms step_avg:99.21ms
step:1625/1770 train_time:160234ms step_avg:99.22ms
step:1625/1770 val_loss:3.3083 train_time:160337ms step_avg:99.28ms
step:1626/1770 train_time:160358ms step_avg:99.23ms
step:1627/1770 train_time:160451ms step_avg:99.23ms
step:1628/1770 train_time:160553ms step_avg:99.23ms
step:1629/1770 train_time:160656ms step_avg:99.23ms
step:1630/1770 train_time:160759ms step_avg:99.23ms
step:1631/1770 train_time:160862ms step_avg:99.24ms
step:1632/1770 train_time:160965ms step_avg:99.24ms
step:1633/1770 train_time:161068ms step_avg:99.24ms
step:1634/1770 train_time:161170ms step_avg:99.24ms
step:1635/1770 train_time:161273ms step_avg:99.25ms
step:1636/1770 train_time:161380ms step_avg:99.25ms
step:1637/1770 train_time:161486ms step_avg:99.25ms
step:1638/1770 train_time:161589ms step_avg:99.26ms
step:1639/1770 train_time:161692ms step_avg:99.26ms
step:1640/1770 train_time:161795ms step_avg:99.26ms
step:1641/1770 train_time:161899ms step_avg:99.26ms
step:1642/1770 train_time:162001ms step_avg:99.27ms
step:1643/1770 train_time:162104ms step_avg:99.27ms
step:1644/1770 train_time:162209ms step_avg:99.27ms
step:1645/1770 train_time:162313ms step_avg:99.27ms
step:1646/1770 train_time:162419ms step_avg:99.28ms
step:1647/1770 train_time:162525ms step_avg:99.28ms
step:1648/1770 train_time:162627ms step_avg:99.28ms
step:1649/1770 train_time:162730ms step_avg:99.29ms
step:1650/1770 train_time:162833ms step_avg:99.29ms
step:1651/1770 train_time:162936ms step_avg:99.29ms
step:1652/1770 train_time:163039ms step_avg:99.29ms
step:1653/1770 train_time:163144ms step_avg:99.30ms
step:1654/1770 train_time:163251ms step_avg:99.30ms
step:1655/1770 train_time:163357ms step_avg:99.31ms
step:1656/1770 train_time:163461ms step_avg:99.31ms
step:1657/1770 train_time:163566ms step_avg:99.31ms
step:1658/1770 train_time:163670ms step_avg:99.31ms
step:1659/1770 train_time:163775ms step_avg:99.32ms
step:1660/1770 train_time:163878ms step_avg:99.32ms
step:1661/1770 train_time:163982ms step_avg:99.32ms
step:1662/1770 train_time:164086ms step_avg:99.33ms
step:1663/1770 train_time:164188ms step_avg:99.33ms
step:1664/1770 train_time:164292ms step_avg:99.33ms
step:1665/1770 train_time:164394ms step_avg:99.33ms
step:1666/1770 train_time:164499ms step_avg:99.34ms
step:1667/1770 train_time:164603ms step_avg:99.34ms
step:1668/1770 train_time:164706ms step_avg:99.34ms
step:1669/1770 train_time:164809ms step_avg:99.34ms
step:1670/1770 train_time:164911ms step_avg:99.34ms
step:1671/1770 train_time:165015ms step_avg:99.35ms
step:1672/1770 train_time:165118ms step_avg:99.35ms
step:1673/1770 train_time:165224ms step_avg:99.35ms
step:1674/1770 train_time:165327ms step_avg:99.36ms
step:1675/1770 train_time:165430ms step_avg:99.36ms
step:1676/1770 train_time:165534ms step_avg:99.36ms
step:1677/1770 train_time:165643ms step_avg:99.37ms
step:1678/1770 train_time:165745ms step_avg:99.37ms
step:1679/1770 train_time:165848ms step_avg:99.37ms
step:1680/1770 train_time:165951ms step_avg:99.37ms
step:1681/1770 train_time:166055ms step_avg:99.37ms
step:1682/1770 train_time:166161ms step_avg:99.38ms
step:1683/1770 train_time:166264ms step_avg:99.38ms
step:1684/1770 train_time:166368ms step_avg:99.38ms
step:1685/1770 train_time:166471ms step_avg:99.39ms
step:1686/1770 train_time:166576ms step_avg:99.39ms
step:1687/1770 train_time:166682ms step_avg:99.39ms
step:1688/1770 train_time:166786ms step_avg:99.40ms
step:1689/1770 train_time:166889ms step_avg:99.40ms
step:1690/1770 train_time:166992ms step_avg:99.40ms
step:1691/1770 train_time:167095ms step_avg:99.40ms
step:1692/1770 train_time:167199ms step_avg:99.40ms
step:1693/1770 train_time:167305ms step_avg:99.41ms
step:1694/1770 train_time:167408ms step_avg:99.41ms
step:1695/1770 train_time:167512ms step_avg:99.41ms
step:1696/1770 train_time:167618ms step_avg:99.42ms
step:1697/1770 train_time:167723ms step_avg:99.42ms
step:1698/1770 train_time:167828ms step_avg:99.42ms
step:1699/1770 train_time:167932ms step_avg:99.43ms
step:1700/1770 train_time:168034ms step_avg:99.43ms
step:1701/1770 train_time:168138ms step_avg:99.43ms
step:1702/1770 train_time:168242ms step_avg:99.43ms
step:1703/1770 train_time:168345ms step_avg:99.44ms
step:1704/1770 train_time:168449ms step_avg:99.44ms
step:1705/1770 train_time:168552ms step_avg:99.44ms
step:1706/1770 train_time:168654ms step_avg:99.44ms
step:1707/1770 train_time:168760ms step_avg:99.45ms
step:1708/1770 train_time:168864ms step_avg:99.45ms
step:1709/1770 train_time:168969ms step_avg:99.45ms
step:1710/1770 train_time:169076ms step_avg:99.46ms
step:1711/1770 train_time:169182ms step_avg:99.46ms
step:1712/1770 train_time:169286ms step_avg:99.46ms
step:1713/1770 train_time:169389ms step_avg:99.47ms
step:1714/1770 train_time:169493ms step_avg:99.47ms
step:1715/1770 train_time:169596ms step_avg:99.47ms
step:1716/1770 train_time:169701ms step_avg:99.47ms
step:1717/1770 train_time:169805ms step_avg:99.48ms
step:1718/1770 train_time:169910ms step_avg:99.48ms
step:1719/1770 train_time:170015ms step_avg:99.48ms
step:1720/1770 train_time:170121ms step_avg:99.49ms
step:1721/1770 train_time:170224ms step_avg:99.49ms
step:1722/1770 train_time:170331ms step_avg:99.49ms
step:1723/1770 train_time:170436ms step_avg:99.50ms
step:1724/1770 train_time:170542ms step_avg:99.50ms
step:1725/1770 train_time:170648ms step_avg:99.50ms
step:1726/1770 train_time:170754ms step_avg:99.51ms
step:1727/1770 train_time:170858ms step_avg:99.51ms
step:1728/1770 train_time:170964ms step_avg:99.51ms
step:1729/1770 train_time:171068ms step_avg:99.52ms
step:1730/1770 train_time:171173ms step_avg:99.52ms
step:1731/1770 train_time:171279ms step_avg:99.52ms
step:1732/1770 train_time:171383ms step_avg:99.53ms
step:1733/1770 train_time:171489ms step_avg:99.53ms
step:1734/1770 train_time:171592ms step_avg:99.53ms
step:1735/1770 train_time:171697ms step_avg:99.53ms
step:1736/1770 train_time:171800ms step_avg:99.54ms
step:1737/1770 train_time:171905ms step_avg:99.54ms
step:1738/1770 train_time:172009ms step_avg:99.54ms
step:1739/1770 train_time:172112ms step_avg:99.54ms
step:1740/1770 train_time:172216ms step_avg:99.55ms
step:1741/1770 train_time:172324ms step_avg:99.55ms
step:1742/1770 train_time:172431ms step_avg:99.56ms
step:1743/1770 train_time:172535ms step_avg:99.56ms
step:1744/1770 train_time:172640ms step_avg:99.56ms
step:1745/1770 train_time:172743ms step_avg:99.56ms
step:1746/1770 train_time:172850ms step_avg:99.57ms
step:1747/1770 train_time:172953ms step_avg:99.57ms
step:1748/1770 train_time:173059ms step_avg:99.57ms
step:1749/1770 train_time:173165ms step_avg:99.58ms
step:1750/1770 train_time:173269ms step_avg:99.58ms
step:1750/1770 val_loss:3.2814 train_time:173371ms step_avg:99.64ms
step:1751/1770 train_time:173393ms step_avg:99.59ms
step:1752/1770 train_time:173484ms step_avg:99.59ms
step:1753/1770 train_time:173587ms step_avg:99.59ms
step:1754/1770 train_time:173692ms step_avg:99.59ms
step:1755/1770 train_time:173796ms step_avg:99.60ms
step:1756/1770 train_time:173901ms step_avg:99.60ms
step:1757/1770 train_time:174005ms step_avg:99.60ms
step:1758/1770 train_time:174109ms step_avg:99.60ms
step:1759/1770 train_time:174214ms step_avg:99.61ms
step:1760/1770 train_time:174318ms step_avg:99.61ms
step:1761/1770 train_time:174424ms step_avg:99.61ms
step:1762/1770 train_time:174532ms step_avg:99.62ms
step:1763/1770 train_time:174634ms step_avg:99.62ms
step:1764/1770 train_time:174739ms step_avg:99.62ms
step:1765/1770 train_time:174844ms step_avg:99.63ms
step:1766/1770 train_time:174952ms step_avg:99.63ms
step:1767/1770 train_time:175056ms step_avg:99.63ms
step:1768/1770 train_time:175161ms step_avg:99.64ms
step:1769/1770 train_time:175264ms step_avg:99.64ms
step:1770/1770 train_time:175367ms step_avg:99.64ms
step:1770/1770 val_loss:3.2784 train_time:175472ms step_avg:99.70ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
