import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 03:32:49 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24234ms step_avg:nanms
step:2/1770 train_time:24661ms step_avg:nanms
step:3/1770 train_time:24757ms step_avg:nanms
step:4/1770 train_time:24849ms step_avg:nanms
step:5/1770 train_time:24943ms step_avg:nanms
step:6/1770 train_time:25038ms step_avg:nanms
step:7/1770 train_time:25132ms step_avg:nanms
step:8/1770 train_time:25226ms step_avg:nanms
step:9/1770 train_time:25321ms step_avg:nanms
step:10/1770 train_time:25415ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.38ms
step:14/1770 train_time:378ms step_avg:94.56ms
step:15/1770 train_time:472ms step_avg:94.49ms
step:16/1770 train_time:567ms step_avg:94.44ms
step:17/1770 train_time:661ms step_avg:94.41ms
step:18/1770 train_time:756ms step_avg:94.56ms
step:19/1770 train_time:850ms step_avg:94.44ms
step:20/1770 train_time:944ms step_avg:94.44ms
step:21/1770 train_time:1040ms step_avg:94.51ms
step:22/1770 train_time:1135ms step_avg:94.56ms
step:23/1770 train_time:1229ms step_avg:94.55ms
step:24/1770 train_time:1324ms step_avg:94.55ms
step:25/1770 train_time:1419ms step_avg:94.57ms
step:26/1770 train_time:1513ms step_avg:94.57ms
step:27/1770 train_time:1607ms step_avg:94.53ms
step:28/1770 train_time:1702ms step_avg:94.54ms
step:29/1770 train_time:1797ms step_avg:94.57ms
step:30/1770 train_time:1891ms step_avg:94.56ms
step:31/1770 train_time:1985ms step_avg:94.54ms
step:32/1770 train_time:2080ms step_avg:94.55ms
step:33/1770 train_time:2175ms step_avg:94.58ms
step:34/1770 train_time:2269ms step_avg:94.55ms
step:35/1770 train_time:2364ms step_avg:94.55ms
step:36/1770 train_time:2458ms step_avg:94.56ms
step:37/1770 train_time:2553ms step_avg:94.57ms
step:38/1770 train_time:2648ms step_avg:94.56ms
step:39/1770 train_time:2743ms step_avg:94.58ms
step:40/1770 train_time:2838ms step_avg:94.59ms
step:41/1770 train_time:2932ms step_avg:94.59ms
step:42/1770 train_time:3027ms step_avg:94.59ms
step:43/1770 train_time:3122ms step_avg:94.59ms
step:44/1770 train_time:3218ms step_avg:94.63ms
step:45/1770 train_time:3313ms step_avg:94.66ms
step:46/1770 train_time:3407ms step_avg:94.64ms
step:47/1770 train_time:3502ms step_avg:94.64ms
step:48/1770 train_time:3597ms step_avg:94.67ms
step:49/1770 train_time:3692ms step_avg:94.66ms
step:50/1770 train_time:3786ms step_avg:94.66ms
step:51/1770 train_time:3882ms step_avg:94.69ms
step:52/1770 train_time:3977ms step_avg:94.68ms
step:53/1770 train_time:4071ms step_avg:94.68ms
step:54/1770 train_time:4167ms step_avg:94.70ms
step:55/1770 train_time:4261ms step_avg:94.68ms
step:56/1770 train_time:4356ms step_avg:94.69ms
step:57/1770 train_time:4450ms step_avg:94.67ms
step:58/1770 train_time:4544ms step_avg:94.66ms
step:59/1770 train_time:4639ms step_avg:94.66ms
step:60/1770 train_time:4733ms step_avg:94.67ms
step:61/1770 train_time:4828ms step_avg:94.66ms
step:62/1770 train_time:4922ms step_avg:94.66ms
step:63/1770 train_time:5017ms step_avg:94.66ms
step:64/1770 train_time:5111ms step_avg:94.65ms
step:65/1770 train_time:5206ms step_avg:94.65ms
step:66/1770 train_time:5300ms step_avg:94.65ms
step:67/1770 train_time:5395ms step_avg:94.65ms
step:68/1770 train_time:5490ms step_avg:94.65ms
step:69/1770 train_time:5584ms step_avg:94.64ms
step:70/1770 train_time:5678ms step_avg:94.64ms
step:71/1770 train_time:5773ms step_avg:94.64ms
step:72/1770 train_time:5867ms step_avg:94.63ms
step:73/1770 train_time:5962ms step_avg:94.63ms
step:74/1770 train_time:6056ms step_avg:94.63ms
step:75/1770 train_time:6151ms step_avg:94.63ms
step:76/1770 train_time:6245ms step_avg:94.63ms
step:77/1770 train_time:6340ms step_avg:94.63ms
step:78/1770 train_time:6436ms step_avg:94.64ms
step:79/1770 train_time:6530ms step_avg:94.64ms
step:80/1770 train_time:6624ms step_avg:94.63ms
step:81/1770 train_time:6719ms step_avg:94.63ms
step:82/1770 train_time:6813ms step_avg:94.63ms
step:83/1770 train_time:6908ms step_avg:94.63ms
step:84/1770 train_time:7003ms step_avg:94.63ms
step:85/1770 train_time:7099ms step_avg:94.65ms
step:86/1770 train_time:7193ms step_avg:94.65ms
step:87/1770 train_time:7288ms step_avg:94.64ms
step:88/1770 train_time:7383ms step_avg:94.65ms
step:89/1770 train_time:7477ms step_avg:94.65ms
step:90/1770 train_time:7573ms step_avg:94.66ms
step:91/1770 train_time:7667ms step_avg:94.65ms
step:92/1770 train_time:7762ms step_avg:94.66ms
step:93/1770 train_time:7857ms step_avg:94.66ms
step:94/1770 train_time:7951ms step_avg:94.66ms
step:95/1770 train_time:8046ms step_avg:94.66ms
step:96/1770 train_time:8141ms step_avg:94.66ms
step:97/1770 train_time:8235ms step_avg:94.66ms
step:98/1770 train_time:8329ms step_avg:94.65ms
step:99/1770 train_time:8424ms step_avg:94.65ms
step:100/1770 train_time:8519ms step_avg:94.65ms
step:101/1770 train_time:8614ms step_avg:94.66ms
step:102/1770 train_time:8709ms step_avg:94.67ms
step:103/1770 train_time:8804ms step_avg:94.66ms
step:104/1770 train_time:8899ms step_avg:94.67ms
step:105/1770 train_time:8994ms step_avg:94.67ms
step:106/1770 train_time:9088ms step_avg:94.67ms
step:107/1770 train_time:9183ms step_avg:94.67ms
step:108/1770 train_time:9277ms step_avg:94.67ms
step:109/1770 train_time:9371ms step_avg:94.66ms
step:110/1770 train_time:9466ms step_avg:94.66ms
step:111/1770 train_time:9561ms step_avg:94.66ms
step:112/1770 train_time:9656ms step_avg:94.67ms
step:113/1770 train_time:9750ms step_avg:94.66ms
step:114/1770 train_time:9844ms step_avg:94.66ms
step:115/1770 train_time:9939ms step_avg:94.66ms
step:116/1770 train_time:10034ms step_avg:94.66ms
step:117/1770 train_time:10129ms step_avg:94.66ms
step:118/1770 train_time:10223ms step_avg:94.66ms
step:119/1770 train_time:10318ms step_avg:94.66ms
step:120/1770 train_time:10414ms step_avg:94.67ms
step:121/1770 train_time:10508ms step_avg:94.67ms
step:122/1770 train_time:10603ms step_avg:94.67ms
step:123/1770 train_time:10698ms step_avg:94.67ms
step:124/1770 train_time:10792ms step_avg:94.67ms
step:125/1770 train_time:10887ms step_avg:94.67ms
step:125/1770 val_loss:4.6664 train_time:10980ms step_avg:95.48ms
step:126/1770 train_time:11001ms step_avg:94.84ms
step:127/1770 train_time:11077ms step_avg:94.68ms
step:128/1770 train_time:11176ms step_avg:94.71ms
step:129/1770 train_time:11275ms step_avg:94.75ms
step:130/1770 train_time:11370ms step_avg:94.75ms
step:131/1770 train_time:11465ms step_avg:94.75ms
step:132/1770 train_time:11560ms step_avg:94.75ms
step:133/1770 train_time:11654ms step_avg:94.75ms
step:134/1770 train_time:11749ms step_avg:94.75ms
step:135/1770 train_time:11844ms step_avg:94.75ms
step:136/1770 train_time:11939ms step_avg:94.76ms
step:137/1770 train_time:12034ms step_avg:94.76ms
step:138/1770 train_time:12129ms step_avg:94.76ms
step:139/1770 train_time:12225ms step_avg:94.76ms
step:140/1770 train_time:12320ms step_avg:94.77ms
step:141/1770 train_time:12415ms step_avg:94.77ms
step:142/1770 train_time:12510ms step_avg:94.77ms
step:143/1770 train_time:12605ms step_avg:94.78ms
step:144/1770 train_time:12700ms step_avg:94.78ms
step:145/1770 train_time:12795ms step_avg:94.78ms
step:146/1770 train_time:12890ms step_avg:94.78ms
step:147/1770 train_time:12986ms step_avg:94.79ms
step:148/1770 train_time:13081ms step_avg:94.79ms
step:149/1770 train_time:13176ms step_avg:94.79ms
step:150/1770 train_time:13271ms step_avg:94.79ms
step:151/1770 train_time:13366ms step_avg:94.80ms
step:152/1770 train_time:13462ms step_avg:94.81ms
step:153/1770 train_time:13557ms step_avg:94.81ms
step:154/1770 train_time:13652ms step_avg:94.81ms
step:155/1770 train_time:13748ms step_avg:94.81ms
step:156/1770 train_time:13843ms step_avg:94.82ms
step:157/1770 train_time:13938ms step_avg:94.82ms
step:158/1770 train_time:14033ms step_avg:94.82ms
step:159/1770 train_time:14128ms step_avg:94.82ms
step:160/1770 train_time:14224ms step_avg:94.82ms
step:161/1770 train_time:14319ms step_avg:94.83ms
step:162/1770 train_time:14414ms step_avg:94.83ms
step:163/1770 train_time:14509ms step_avg:94.83ms
step:164/1770 train_time:14604ms step_avg:94.83ms
step:165/1770 train_time:14700ms step_avg:94.84ms
step:166/1770 train_time:14795ms step_avg:94.84ms
step:167/1770 train_time:14890ms step_avg:94.84ms
step:168/1770 train_time:14986ms step_avg:94.85ms
step:169/1770 train_time:15081ms step_avg:94.85ms
step:170/1770 train_time:15176ms step_avg:94.85ms
step:171/1770 train_time:15271ms step_avg:94.85ms
step:172/1770 train_time:15367ms step_avg:94.86ms
step:173/1770 train_time:15462ms step_avg:94.86ms
step:174/1770 train_time:15557ms step_avg:94.86ms
step:175/1770 train_time:15653ms step_avg:94.86ms
step:176/1770 train_time:15747ms step_avg:94.86ms
step:177/1770 train_time:15843ms step_avg:94.87ms
step:178/1770 train_time:15938ms step_avg:94.87ms
step:179/1770 train_time:16033ms step_avg:94.87ms
step:180/1770 train_time:16129ms step_avg:94.87ms
step:181/1770 train_time:16224ms step_avg:94.88ms
step:182/1770 train_time:16319ms step_avg:94.88ms
step:183/1770 train_time:16414ms step_avg:94.88ms
step:184/1770 train_time:16509ms step_avg:94.88ms
step:185/1770 train_time:16605ms step_avg:94.88ms
step:186/1770 train_time:16700ms step_avg:94.89ms
step:187/1770 train_time:16795ms step_avg:94.89ms
step:188/1770 train_time:16890ms step_avg:94.89ms
step:189/1770 train_time:16985ms step_avg:94.89ms
step:190/1770 train_time:17081ms step_avg:94.90ms
step:191/1770 train_time:17177ms step_avg:94.90ms
step:192/1770 train_time:17271ms step_avg:94.90ms
step:193/1770 train_time:17367ms step_avg:94.90ms
step:194/1770 train_time:17463ms step_avg:94.91ms
step:195/1770 train_time:17558ms step_avg:94.91ms
step:196/1770 train_time:17652ms step_avg:94.90ms
step:197/1770 train_time:17747ms step_avg:94.90ms
step:198/1770 train_time:17842ms step_avg:94.91ms
step:199/1770 train_time:17938ms step_avg:94.91ms
step:200/1770 train_time:18032ms step_avg:94.91ms
step:201/1770 train_time:18127ms step_avg:94.91ms
step:202/1770 train_time:18222ms step_avg:94.91ms
step:203/1770 train_time:18318ms step_avg:94.91ms
step:204/1770 train_time:18413ms step_avg:94.91ms
step:205/1770 train_time:18507ms step_avg:94.91ms
step:206/1770 train_time:18603ms step_avg:94.91ms
step:207/1770 train_time:18698ms step_avg:94.91ms
step:208/1770 train_time:18793ms step_avg:94.91ms
step:209/1770 train_time:18888ms step_avg:94.91ms
step:210/1770 train_time:18983ms step_avg:94.92ms
step:211/1770 train_time:19078ms step_avg:94.92ms
step:212/1770 train_time:19173ms step_avg:94.92ms
step:213/1770 train_time:19268ms step_avg:94.92ms
step:214/1770 train_time:19364ms step_avg:94.92ms
step:215/1770 train_time:19459ms step_avg:94.92ms
step:216/1770 train_time:19553ms step_avg:94.92ms
step:217/1770 train_time:19649ms step_avg:94.92ms
step:218/1770 train_time:19744ms step_avg:94.92ms
step:219/1770 train_time:19839ms step_avg:94.92ms
step:220/1770 train_time:19933ms step_avg:94.92ms
step:221/1770 train_time:20028ms step_avg:94.92ms
step:222/1770 train_time:20124ms step_avg:94.92ms
step:223/1770 train_time:20219ms step_avg:94.93ms
step:224/1770 train_time:20314ms step_avg:94.92ms
step:225/1770 train_time:20409ms step_avg:94.93ms
step:226/1770 train_time:20504ms step_avg:94.93ms
step:227/1770 train_time:20600ms step_avg:94.93ms
step:228/1770 train_time:20694ms step_avg:94.93ms
step:229/1770 train_time:20789ms step_avg:94.93ms
step:230/1770 train_time:20885ms step_avg:94.93ms
step:231/1770 train_time:20980ms step_avg:94.93ms
step:232/1770 train_time:21074ms step_avg:94.93ms
step:233/1770 train_time:21169ms step_avg:94.93ms
step:234/1770 train_time:21265ms step_avg:94.93ms
step:235/1770 train_time:21360ms step_avg:94.93ms
step:236/1770 train_time:21455ms step_avg:94.93ms
step:237/1770 train_time:21550ms step_avg:94.93ms
step:238/1770 train_time:21646ms step_avg:94.94ms
step:239/1770 train_time:21741ms step_avg:94.94ms
step:240/1770 train_time:21836ms step_avg:94.94ms
step:241/1770 train_time:21931ms step_avg:94.94ms
step:242/1770 train_time:22027ms step_avg:94.94ms
step:243/1770 train_time:22122ms step_avg:94.94ms
step:244/1770 train_time:22217ms step_avg:94.94ms
step:245/1770 train_time:22312ms step_avg:94.94ms
step:246/1770 train_time:22407ms step_avg:94.94ms
step:247/1770 train_time:22502ms step_avg:94.95ms
step:248/1770 train_time:22597ms step_avg:94.95ms
step:249/1770 train_time:22692ms step_avg:94.94ms
step:250/1770 train_time:22787ms step_avg:94.95ms
step:250/1770 val_loss:4.1169 train_time:22881ms step_avg:95.34ms
step:251/1770 train_time:22902ms step_avg:95.03ms
step:252/1770 train_time:22987ms step_avg:94.99ms
step:253/1770 train_time:23084ms step_avg:94.99ms
step:254/1770 train_time:23179ms step_avg:95.00ms
step:255/1770 train_time:23275ms step_avg:95.00ms
step:256/1770 train_time:23370ms step_avg:95.00ms
step:257/1770 train_time:23465ms step_avg:95.00ms
step:258/1770 train_time:23560ms step_avg:95.00ms
step:259/1770 train_time:23655ms step_avg:95.00ms
step:260/1770 train_time:23751ms step_avg:95.00ms
step:261/1770 train_time:23845ms step_avg:95.00ms
step:262/1770 train_time:23940ms step_avg:95.00ms
step:263/1770 train_time:24036ms step_avg:95.00ms
step:264/1770 train_time:24132ms step_avg:95.01ms
step:265/1770 train_time:24228ms step_avg:95.01ms
step:266/1770 train_time:24323ms step_avg:95.01ms
step:267/1770 train_time:24418ms step_avg:95.01ms
step:268/1770 train_time:24514ms step_avg:95.01ms
step:269/1770 train_time:24609ms step_avg:95.01ms
step:270/1770 train_time:24704ms step_avg:95.02ms
step:271/1770 train_time:24800ms step_avg:95.02ms
step:272/1770 train_time:24896ms step_avg:95.02ms
step:273/1770 train_time:24991ms step_avg:95.02ms
step:274/1770 train_time:25088ms step_avg:95.03ms
step:275/1770 train_time:25182ms step_avg:95.03ms
step:276/1770 train_time:25278ms step_avg:95.03ms
step:277/1770 train_time:25374ms step_avg:95.03ms
step:278/1770 train_time:25470ms step_avg:95.04ms
step:279/1770 train_time:25565ms step_avg:95.04ms
step:280/1770 train_time:25661ms step_avg:95.04ms
step:281/1770 train_time:25758ms step_avg:95.05ms
step:282/1770 train_time:25854ms step_avg:95.05ms
step:283/1770 train_time:25949ms step_avg:95.05ms
step:284/1770 train_time:26045ms step_avg:95.05ms
step:285/1770 train_time:26140ms step_avg:95.06ms
step:286/1770 train_time:26236ms step_avg:95.06ms
step:287/1770 train_time:26332ms step_avg:95.06ms
step:288/1770 train_time:26427ms step_avg:95.06ms
step:289/1770 train_time:26522ms step_avg:95.06ms
step:290/1770 train_time:26618ms step_avg:95.06ms
step:291/1770 train_time:26714ms step_avg:95.07ms
step:292/1770 train_time:26809ms step_avg:95.07ms
step:293/1770 train_time:26904ms step_avg:95.07ms
step:294/1770 train_time:27000ms step_avg:95.07ms
step:295/1770 train_time:27097ms step_avg:95.08ms
step:296/1770 train_time:27192ms step_avg:95.08ms
step:297/1770 train_time:27287ms step_avg:95.08ms
step:298/1770 train_time:27383ms step_avg:95.08ms
step:299/1770 train_time:27479ms step_avg:95.08ms
step:300/1770 train_time:27575ms step_avg:95.09ms
step:301/1770 train_time:27670ms step_avg:95.09ms
step:302/1770 train_time:27766ms step_avg:95.09ms
step:303/1770 train_time:27861ms step_avg:95.09ms
step:304/1770 train_time:27957ms step_avg:95.09ms
step:305/1770 train_time:28053ms step_avg:95.09ms
step:306/1770 train_time:28149ms step_avg:95.10ms
step:307/1770 train_time:28244ms step_avg:95.10ms
step:308/1770 train_time:28339ms step_avg:95.10ms
step:309/1770 train_time:28435ms step_avg:95.10ms
step:310/1770 train_time:28531ms step_avg:95.10ms
step:311/1770 train_time:28626ms step_avg:95.10ms
step:312/1770 train_time:28722ms step_avg:95.11ms
step:313/1770 train_time:28818ms step_avg:95.11ms
step:314/1770 train_time:28914ms step_avg:95.11ms
step:315/1770 train_time:29010ms step_avg:95.11ms
step:316/1770 train_time:29105ms step_avg:95.12ms
step:317/1770 train_time:29201ms step_avg:95.12ms
step:318/1770 train_time:29297ms step_avg:95.12ms
step:319/1770 train_time:29393ms step_avg:95.12ms
step:320/1770 train_time:29489ms step_avg:95.13ms
step:321/1770 train_time:29584ms step_avg:95.13ms
step:322/1770 train_time:29680ms step_avg:95.13ms
step:323/1770 train_time:29776ms step_avg:95.13ms
step:324/1770 train_time:29872ms step_avg:95.13ms
step:325/1770 train_time:29967ms step_avg:95.13ms
step:326/1770 train_time:30063ms step_avg:95.14ms
step:327/1770 train_time:30159ms step_avg:95.14ms
step:328/1770 train_time:30255ms step_avg:95.14ms
step:329/1770 train_time:30350ms step_avg:95.14ms
step:330/1770 train_time:30446ms step_avg:95.14ms
step:331/1770 train_time:30542ms step_avg:95.15ms
step:332/1770 train_time:30638ms step_avg:95.15ms
step:333/1770 train_time:30734ms step_avg:95.15ms
step:334/1770 train_time:30830ms step_avg:95.15ms
step:335/1770 train_time:30925ms step_avg:95.15ms
step:336/1770 train_time:31021ms step_avg:95.16ms
step:337/1770 train_time:31117ms step_avg:95.16ms
step:338/1770 train_time:31212ms step_avg:95.16ms
step:339/1770 train_time:31307ms step_avg:95.16ms
step:340/1770 train_time:31402ms step_avg:95.16ms
step:341/1770 train_time:31498ms step_avg:95.16ms
step:342/1770 train_time:31594ms step_avg:95.16ms
step:343/1770 train_time:31690ms step_avg:95.16ms
step:344/1770 train_time:31785ms step_avg:95.16ms
step:345/1770 train_time:31881ms step_avg:95.17ms
step:346/1770 train_time:31976ms step_avg:95.17ms
step:347/1770 train_time:32072ms step_avg:95.17ms
step:348/1770 train_time:32167ms step_avg:95.17ms
step:349/1770 train_time:32263ms step_avg:95.17ms
step:350/1770 train_time:32359ms step_avg:95.17ms
step:351/1770 train_time:32455ms step_avg:95.18ms
step:352/1770 train_time:32550ms step_avg:95.18ms
step:353/1770 train_time:32646ms step_avg:95.18ms
step:354/1770 train_time:32741ms step_avg:95.18ms
step:355/1770 train_time:32837ms step_avg:95.18ms
step:356/1770 train_time:32936ms step_avg:95.19ms
step:357/1770 train_time:33029ms step_avg:95.18ms
step:358/1770 train_time:33125ms step_avg:95.19ms
step:359/1770 train_time:33220ms step_avg:95.19ms
step:360/1770 train_time:33315ms step_avg:95.19ms
step:361/1770 train_time:33411ms step_avg:95.19ms
step:362/1770 train_time:33506ms step_avg:95.19ms
step:363/1770 train_time:33602ms step_avg:95.19ms
step:364/1770 train_time:33698ms step_avg:95.19ms
step:365/1770 train_time:33793ms step_avg:95.19ms
step:366/1770 train_time:33890ms step_avg:95.20ms
step:367/1770 train_time:33986ms step_avg:95.20ms
step:368/1770 train_time:34082ms step_avg:95.20ms
step:369/1770 train_time:34178ms step_avg:95.20ms
step:370/1770 train_time:34274ms step_avg:95.20ms
step:371/1770 train_time:34369ms step_avg:95.21ms
step:372/1770 train_time:34465ms step_avg:95.21ms
step:373/1770 train_time:34560ms step_avg:95.21ms
step:374/1770 train_time:34657ms step_avg:95.21ms
step:375/1770 train_time:34752ms step_avg:95.21ms
step:375/1770 val_loss:3.9058 train_time:34846ms step_avg:95.47ms
step:376/1770 train_time:34871ms step_avg:95.28ms
step:377/1770 train_time:34950ms step_avg:95.23ms
step:378/1770 train_time:35048ms step_avg:95.24ms
step:379/1770 train_time:35144ms step_avg:95.24ms
step:380/1770 train_time:35244ms step_avg:95.25ms
step:381/1770 train_time:35336ms step_avg:95.24ms
step:382/1770 train_time:35431ms step_avg:95.24ms
step:383/1770 train_time:35526ms step_avg:95.25ms
step:384/1770 train_time:35622ms step_avg:95.25ms
step:385/1770 train_time:35718ms step_avg:95.25ms
step:386/1770 train_time:35813ms step_avg:95.25ms
step:387/1770 train_time:35909ms step_avg:95.25ms
step:388/1770 train_time:36005ms step_avg:95.25ms
step:389/1770 train_time:36101ms step_avg:95.25ms
step:390/1770 train_time:36196ms step_avg:95.25ms
step:391/1770 train_time:36292ms step_avg:95.25ms
step:392/1770 train_time:36387ms step_avg:95.25ms
step:393/1770 train_time:36483ms step_avg:95.26ms
step:394/1770 train_time:36578ms step_avg:95.26ms
step:395/1770 train_time:36673ms step_avg:95.26ms
step:396/1770 train_time:36771ms step_avg:95.26ms
step:397/1770 train_time:36869ms step_avg:95.27ms
step:398/1770 train_time:36967ms step_avg:95.27ms
step:399/1770 train_time:37065ms step_avg:95.28ms
step:400/1770 train_time:37162ms step_avg:95.29ms
step:401/1770 train_time:37260ms step_avg:95.29ms
step:402/1770 train_time:37358ms step_avg:95.30ms
step:403/1770 train_time:37455ms step_avg:95.30ms
step:404/1770 train_time:37552ms step_avg:95.31ms
step:405/1770 train_time:37649ms step_avg:95.31ms
step:406/1770 train_time:37747ms step_avg:95.32ms
step:407/1770 train_time:37846ms step_avg:95.33ms
step:408/1770 train_time:37943ms step_avg:95.34ms
step:409/1770 train_time:38041ms step_avg:95.34ms
step:410/1770 train_time:38138ms step_avg:95.35ms
step:411/1770 train_time:38236ms step_avg:95.35ms
step:412/1770 train_time:38333ms step_avg:95.36ms
step:413/1770 train_time:38431ms step_avg:95.36ms
step:414/1770 train_time:38529ms step_avg:95.37ms
step:415/1770 train_time:38627ms step_avg:95.38ms
step:416/1770 train_time:38725ms step_avg:95.38ms
step:417/1770 train_time:38823ms step_avg:95.39ms
step:418/1770 train_time:38920ms step_avg:95.39ms
step:419/1770 train_time:39017ms step_avg:95.40ms
step:420/1770 train_time:39114ms step_avg:95.40ms
step:421/1770 train_time:39212ms step_avg:95.41ms
step:422/1770 train_time:39309ms step_avg:95.41ms
step:423/1770 train_time:39407ms step_avg:95.42ms
step:424/1770 train_time:39505ms step_avg:95.42ms
step:425/1770 train_time:39602ms step_avg:95.43ms
step:426/1770 train_time:39700ms step_avg:95.43ms
step:427/1770 train_time:39797ms step_avg:95.44ms
step:428/1770 train_time:39894ms step_avg:95.44ms
step:429/1770 train_time:39992ms step_avg:95.45ms
step:430/1770 train_time:40091ms step_avg:95.45ms
step:431/1770 train_time:40189ms step_avg:95.46ms
step:432/1770 train_time:40288ms step_avg:95.47ms
step:433/1770 train_time:40386ms step_avg:95.47ms
step:434/1770 train_time:40483ms step_avg:95.48ms
step:435/1770 train_time:40582ms step_avg:95.49ms
step:436/1770 train_time:40678ms step_avg:95.49ms
step:437/1770 train_time:40776ms step_avg:95.49ms
step:438/1770 train_time:40873ms step_avg:95.50ms
step:439/1770 train_time:40971ms step_avg:95.50ms
step:440/1770 train_time:41068ms step_avg:95.51ms
step:441/1770 train_time:41166ms step_avg:95.51ms
step:442/1770 train_time:41264ms step_avg:95.52ms
step:443/1770 train_time:41362ms step_avg:95.52ms
step:444/1770 train_time:41459ms step_avg:95.53ms
step:445/1770 train_time:41556ms step_avg:95.53ms
step:446/1770 train_time:41653ms step_avg:95.53ms
step:447/1770 train_time:41750ms step_avg:95.54ms
step:448/1770 train_time:41848ms step_avg:95.54ms
step:449/1770 train_time:41946ms step_avg:95.55ms
step:450/1770 train_time:42044ms step_avg:95.55ms
step:451/1770 train_time:42141ms step_avg:95.56ms
step:452/1770 train_time:42239ms step_avg:95.56ms
step:453/1770 train_time:42336ms step_avg:95.57ms
step:454/1770 train_time:42433ms step_avg:95.57ms
step:455/1770 train_time:42531ms step_avg:95.57ms
step:456/1770 train_time:42629ms step_avg:95.58ms
step:457/1770 train_time:42726ms step_avg:95.58ms
step:458/1770 train_time:42824ms step_avg:95.59ms
step:459/1770 train_time:42922ms step_avg:95.59ms
step:460/1770 train_time:43019ms step_avg:95.60ms
step:461/1770 train_time:43116ms step_avg:95.60ms
step:462/1770 train_time:43213ms step_avg:95.60ms
step:463/1770 train_time:43310ms step_avg:95.61ms
step:464/1770 train_time:43408ms step_avg:95.61ms
step:465/1770 train_time:43507ms step_avg:95.62ms
step:466/1770 train_time:43605ms step_avg:95.62ms
step:467/1770 train_time:43702ms step_avg:95.63ms
step:468/1770 train_time:43800ms step_avg:95.63ms
step:469/1770 train_time:43897ms step_avg:95.64ms
step:470/1770 train_time:43994ms step_avg:95.64ms
step:471/1770 train_time:44092ms step_avg:95.64ms
step:472/1770 train_time:44190ms step_avg:95.65ms
step:473/1770 train_time:44288ms step_avg:95.65ms
step:474/1770 train_time:44386ms step_avg:95.66ms
step:475/1770 train_time:44484ms step_avg:95.66ms
step:476/1770 train_time:44581ms step_avg:95.67ms
step:477/1770 train_time:44678ms step_avg:95.67ms
step:478/1770 train_time:44775ms step_avg:95.67ms
step:479/1770 train_time:44873ms step_avg:95.68ms
step:480/1770 train_time:44970ms step_avg:95.68ms
step:481/1770 train_time:45068ms step_avg:95.69ms
step:482/1770 train_time:45165ms step_avg:95.69ms
step:483/1770 train_time:45264ms step_avg:95.70ms
step:484/1770 train_time:45361ms step_avg:95.70ms
step:485/1770 train_time:45459ms step_avg:95.70ms
step:486/1770 train_time:45556ms step_avg:95.71ms
step:487/1770 train_time:45654ms step_avg:95.71ms
step:488/1770 train_time:45751ms step_avg:95.71ms
step:489/1770 train_time:45849ms step_avg:95.72ms
step:490/1770 train_time:45946ms step_avg:95.72ms
step:491/1770 train_time:46044ms step_avg:95.73ms
step:492/1770 train_time:46141ms step_avg:95.73ms
step:493/1770 train_time:46239ms step_avg:95.73ms
step:494/1770 train_time:46336ms step_avg:95.74ms
step:495/1770 train_time:46433ms step_avg:95.74ms
step:496/1770 train_time:46530ms step_avg:95.74ms
step:497/1770 train_time:46629ms step_avg:95.75ms
step:498/1770 train_time:46726ms step_avg:95.75ms
step:499/1770 train_time:46825ms step_avg:95.76ms
step:500/1770 train_time:46922ms step_avg:95.76ms
step:500/1770 val_loss:3.7561 train_time:47018ms step_avg:95.96ms
step:501/1770 train_time:47040ms step_avg:95.80ms
step:502/1770 train_time:47125ms step_avg:95.78ms
step:503/1770 train_time:47223ms step_avg:95.79ms
step:504/1770 train_time:47320ms step_avg:95.79ms
step:505/1770 train_time:47418ms step_avg:95.79ms
step:506/1770 train_time:47516ms step_avg:95.80ms
step:507/1770 train_time:47613ms step_avg:95.80ms
step:508/1770 train_time:47710ms step_avg:95.80ms
step:509/1770 train_time:47808ms step_avg:95.81ms
step:510/1770 train_time:47905ms step_avg:95.81ms
step:511/1770 train_time:48002ms step_avg:95.81ms
step:512/1770 train_time:48100ms step_avg:95.82ms
step:513/1770 train_time:48198ms step_avg:95.82ms
step:514/1770 train_time:48296ms step_avg:95.83ms
step:515/1770 train_time:48394ms step_avg:95.83ms
step:516/1770 train_time:48491ms step_avg:95.83ms
step:517/1770 train_time:48589ms step_avg:95.84ms
step:518/1770 train_time:48686ms step_avg:95.84ms
step:519/1770 train_time:48783ms step_avg:95.84ms
step:520/1770 train_time:48881ms step_avg:95.84ms
step:521/1770 train_time:48979ms step_avg:95.85ms
step:522/1770 train_time:49077ms step_avg:95.85ms
step:523/1770 train_time:49174ms step_avg:95.86ms
step:524/1770 train_time:49272ms step_avg:95.86ms
step:525/1770 train_time:49369ms step_avg:95.86ms
step:526/1770 train_time:49467ms step_avg:95.87ms
step:527/1770 train_time:49564ms step_avg:95.87ms
step:528/1770 train_time:49662ms step_avg:95.87ms
step:529/1770 train_time:49760ms step_avg:95.88ms
step:530/1770 train_time:49858ms step_avg:95.88ms
step:531/1770 train_time:49956ms step_avg:95.89ms
step:532/1770 train_time:50055ms step_avg:95.89ms
step:533/1770 train_time:50152ms step_avg:95.89ms
step:534/1770 train_time:50250ms step_avg:95.90ms
step:535/1770 train_time:50348ms step_avg:95.90ms
step:536/1770 train_time:50445ms step_avg:95.90ms
step:537/1770 train_time:50542ms step_avg:95.91ms
step:538/1770 train_time:50641ms step_avg:95.91ms
step:539/1770 train_time:50739ms step_avg:95.91ms
step:540/1770 train_time:50837ms step_avg:95.92ms
step:541/1770 train_time:50936ms step_avg:95.92ms
step:542/1770 train_time:51034ms step_avg:95.93ms
step:543/1770 train_time:51133ms step_avg:95.93ms
step:544/1770 train_time:51231ms step_avg:95.94ms
step:545/1770 train_time:51334ms step_avg:95.95ms
step:546/1770 train_time:51428ms step_avg:95.95ms
step:547/1770 train_time:51525ms step_avg:95.95ms
step:548/1770 train_time:51623ms step_avg:95.95ms
step:549/1770 train_time:51721ms step_avg:95.96ms
step:550/1770 train_time:51819ms step_avg:95.96ms
step:551/1770 train_time:51917ms step_avg:95.96ms
step:552/1770 train_time:52015ms step_avg:95.97ms
step:553/1770 train_time:52113ms step_avg:95.97ms
step:554/1770 train_time:52211ms step_avg:95.98ms
step:555/1770 train_time:52309ms step_avg:95.98ms
step:556/1770 train_time:52407ms step_avg:95.98ms
step:557/1770 train_time:52505ms step_avg:95.99ms
step:558/1770 train_time:52602ms step_avg:95.99ms
step:559/1770 train_time:52700ms step_avg:95.99ms
step:560/1770 train_time:52799ms step_avg:96.00ms
step:561/1770 train_time:52897ms step_avg:96.00ms
step:562/1770 train_time:52994ms step_avg:96.00ms
step:563/1770 train_time:53092ms step_avg:96.01ms
step:564/1770 train_time:53190ms step_avg:96.01ms
step:565/1770 train_time:53287ms step_avg:96.01ms
step:566/1770 train_time:53385ms step_avg:96.02ms
step:567/1770 train_time:53482ms step_avg:96.02ms
step:568/1770 train_time:53581ms step_avg:96.02ms
step:569/1770 train_time:53679ms step_avg:96.03ms
step:570/1770 train_time:53777ms step_avg:96.03ms
step:571/1770 train_time:53875ms step_avg:96.03ms
step:572/1770 train_time:53973ms step_avg:96.04ms
step:573/1770 train_time:54071ms step_avg:96.04ms
step:574/1770 train_time:54169ms step_avg:96.04ms
step:575/1770 train_time:54267ms step_avg:96.05ms
step:576/1770 train_time:54364ms step_avg:96.05ms
step:577/1770 train_time:54462ms step_avg:96.05ms
step:578/1770 train_time:54560ms step_avg:96.06ms
step:579/1770 train_time:54659ms step_avg:96.06ms
step:580/1770 train_time:54757ms step_avg:96.06ms
step:581/1770 train_time:54855ms step_avg:96.07ms
step:582/1770 train_time:54954ms step_avg:96.07ms
step:583/1770 train_time:55052ms step_avg:96.08ms
step:584/1770 train_time:55150ms step_avg:96.08ms
step:585/1770 train_time:55250ms step_avg:96.09ms
step:586/1770 train_time:55346ms step_avg:96.09ms
step:587/1770 train_time:55444ms step_avg:96.09ms
step:588/1770 train_time:55541ms step_avg:96.09ms
step:589/1770 train_time:55639ms step_avg:96.10ms
step:590/1770 train_time:55737ms step_avg:96.10ms
step:591/1770 train_time:55835ms step_avg:96.10ms
step:592/1770 train_time:55933ms step_avg:96.10ms
step:593/1770 train_time:56031ms step_avg:96.11ms
step:594/1770 train_time:56128ms step_avg:96.11ms
step:595/1770 train_time:56226ms step_avg:96.11ms
step:596/1770 train_time:56323ms step_avg:96.11ms
step:597/1770 train_time:56421ms step_avg:96.12ms
step:598/1770 train_time:56520ms step_avg:96.12ms
step:599/1770 train_time:56618ms step_avg:96.13ms
step:600/1770 train_time:56716ms step_avg:96.13ms
step:601/1770 train_time:56814ms step_avg:96.13ms
step:602/1770 train_time:56912ms step_avg:96.14ms
step:603/1770 train_time:57010ms step_avg:96.14ms
step:604/1770 train_time:57107ms step_avg:96.14ms
step:605/1770 train_time:57205ms step_avg:96.14ms
step:606/1770 train_time:57303ms step_avg:96.15ms
step:607/1770 train_time:57400ms step_avg:96.15ms
step:608/1770 train_time:57499ms step_avg:96.15ms
step:609/1770 train_time:57597ms step_avg:96.16ms
step:610/1770 train_time:57695ms step_avg:96.16ms
step:611/1770 train_time:57793ms step_avg:96.16ms
step:612/1770 train_time:57890ms step_avg:96.16ms
step:613/1770 train_time:57988ms step_avg:96.17ms
step:614/1770 train_time:58086ms step_avg:96.17ms
step:615/1770 train_time:58184ms step_avg:96.17ms
step:616/1770 train_time:58282ms step_avg:96.17ms
step:617/1770 train_time:58380ms step_avg:96.18ms
step:618/1770 train_time:58478ms step_avg:96.18ms
step:619/1770 train_time:58577ms step_avg:96.18ms
step:620/1770 train_time:58675ms step_avg:96.19ms
step:621/1770 train_time:58773ms step_avg:96.19ms
step:622/1770 train_time:58871ms step_avg:96.19ms
step:623/1770 train_time:58968ms step_avg:96.20ms
step:624/1770 train_time:59067ms step_avg:96.20ms
step:625/1770 train_time:59165ms step_avg:96.20ms
step:625/1770 val_loss:3.6670 train_time:59261ms step_avg:96.36ms
step:626/1770 train_time:59282ms step_avg:96.24ms
step:627/1770 train_time:59367ms step_avg:96.22ms
step:628/1770 train_time:59466ms step_avg:96.22ms
step:629/1770 train_time:59564ms step_avg:96.23ms
step:630/1770 train_time:59662ms step_avg:96.23ms
step:631/1770 train_time:59760ms step_avg:96.23ms
step:632/1770 train_time:59858ms step_avg:96.23ms
step:633/1770 train_time:59956ms step_avg:96.24ms
step:634/1770 train_time:60054ms step_avg:96.24ms
step:635/1770 train_time:60152ms step_avg:96.24ms
step:636/1770 train_time:60249ms step_avg:96.24ms
step:637/1770 train_time:60348ms step_avg:96.25ms
step:638/1770 train_time:60445ms step_avg:96.25ms
step:639/1770 train_time:60543ms step_avg:96.25ms
step:640/1770 train_time:60641ms step_avg:96.26ms
step:641/1770 train_time:60739ms step_avg:96.26ms
step:642/1770 train_time:60837ms step_avg:96.26ms
step:643/1770 train_time:60936ms step_avg:96.26ms
step:644/1770 train_time:61034ms step_avg:96.27ms
step:645/1770 train_time:61131ms step_avg:96.27ms
step:646/1770 train_time:61229ms step_avg:96.27ms
step:647/1770 train_time:61326ms step_avg:96.27ms
step:648/1770 train_time:61424ms step_avg:96.28ms
step:649/1770 train_time:61522ms step_avg:96.28ms
step:650/1770 train_time:61620ms step_avg:96.28ms
step:651/1770 train_time:61719ms step_avg:96.28ms
step:652/1770 train_time:61817ms step_avg:96.29ms
step:653/1770 train_time:61914ms step_avg:96.29ms
step:654/1770 train_time:62012ms step_avg:96.29ms
step:655/1770 train_time:62110ms step_avg:96.30ms
step:656/1770 train_time:62208ms step_avg:96.30ms
step:657/1770 train_time:62305ms step_avg:96.30ms
step:658/1770 train_time:62404ms step_avg:96.30ms
step:659/1770 train_time:62504ms step_avg:96.31ms
step:660/1770 train_time:62603ms step_avg:96.31ms
step:661/1770 train_time:62702ms step_avg:96.32ms
step:662/1770 train_time:62802ms step_avg:96.32ms
step:663/1770 train_time:62902ms step_avg:96.33ms
step:664/1770 train_time:63001ms step_avg:96.33ms
step:665/1770 train_time:63101ms step_avg:96.34ms
step:666/1770 train_time:63201ms step_avg:96.34ms
step:667/1770 train_time:63301ms step_avg:96.35ms
step:668/1770 train_time:63401ms step_avg:96.35ms
step:669/1770 train_time:63502ms step_avg:96.36ms
step:670/1770 train_time:63602ms step_avg:96.37ms
step:671/1770 train_time:63703ms step_avg:96.37ms
step:672/1770 train_time:63803ms step_avg:96.38ms
step:673/1770 train_time:63902ms step_avg:96.38ms
step:674/1770 train_time:64002ms step_avg:96.39ms
step:675/1770 train_time:64102ms step_avg:96.39ms
step:676/1770 train_time:64202ms step_avg:96.40ms
step:677/1770 train_time:64302ms step_avg:96.40ms
step:678/1770 train_time:64401ms step_avg:96.41ms
step:679/1770 train_time:64502ms step_avg:96.41ms
step:680/1770 train_time:64601ms step_avg:96.42ms
step:681/1770 train_time:64701ms step_avg:96.43ms
step:682/1770 train_time:64802ms step_avg:96.43ms
step:683/1770 train_time:64902ms step_avg:96.44ms
step:684/1770 train_time:65002ms step_avg:96.44ms
step:685/1770 train_time:65102ms step_avg:96.45ms
step:686/1770 train_time:65202ms step_avg:96.45ms
step:687/1770 train_time:65302ms step_avg:96.46ms
step:688/1770 train_time:65401ms step_avg:96.46ms
step:689/1770 train_time:65501ms step_avg:96.47ms
step:690/1770 train_time:65601ms step_avg:96.47ms
step:691/1770 train_time:65701ms step_avg:96.48ms
step:692/1770 train_time:65801ms step_avg:96.48ms
step:693/1770 train_time:65901ms step_avg:96.49ms
step:694/1770 train_time:66001ms step_avg:96.49ms
step:695/1770 train_time:66101ms step_avg:96.50ms
step:696/1770 train_time:66201ms step_avg:96.50ms
step:697/1770 train_time:66301ms step_avg:96.51ms
step:698/1770 train_time:66401ms step_avg:96.51ms
step:699/1770 train_time:66501ms step_avg:96.52ms
step:700/1770 train_time:66601ms step_avg:96.52ms
step:701/1770 train_time:66701ms step_avg:96.53ms
step:702/1770 train_time:66801ms step_avg:96.53ms
step:703/1770 train_time:66902ms step_avg:96.54ms
step:704/1770 train_time:67002ms step_avg:96.54ms
step:705/1770 train_time:67102ms step_avg:96.55ms
step:706/1770 train_time:67202ms step_avg:96.55ms
step:707/1770 train_time:67302ms step_avg:96.56ms
step:708/1770 train_time:67402ms step_avg:96.56ms
step:709/1770 train_time:67502ms step_avg:96.57ms
step:710/1770 train_time:67601ms step_avg:96.57ms
step:711/1770 train_time:67701ms step_avg:96.58ms
step:712/1770 train_time:67802ms step_avg:96.58ms
step:713/1770 train_time:67902ms step_avg:96.59ms
step:714/1770 train_time:68001ms step_avg:96.59ms
step:715/1770 train_time:68101ms step_avg:96.60ms
step:716/1770 train_time:68202ms step_avg:96.60ms
step:717/1770 train_time:68302ms step_avg:96.61ms
step:718/1770 train_time:68402ms step_avg:96.61ms
step:719/1770 train_time:68502ms step_avg:96.62ms
step:720/1770 train_time:68602ms step_avg:96.62ms
step:721/1770 train_time:68702ms step_avg:96.63ms
step:722/1770 train_time:68802ms step_avg:96.63ms
step:723/1770 train_time:68902ms step_avg:96.64ms
step:724/1770 train_time:69002ms step_avg:96.64ms
step:725/1770 train_time:69101ms step_avg:96.65ms
step:726/1770 train_time:69201ms step_avg:96.65ms
step:727/1770 train_time:69301ms step_avg:96.65ms
step:728/1770 train_time:69401ms step_avg:96.66ms
step:729/1770 train_time:69501ms step_avg:96.66ms
step:730/1770 train_time:69601ms step_avg:96.67ms
step:731/1770 train_time:69701ms step_avg:96.67ms
step:732/1770 train_time:69801ms step_avg:96.68ms
step:733/1770 train_time:69901ms step_avg:96.68ms
step:734/1770 train_time:70002ms step_avg:96.69ms
step:735/1770 train_time:70101ms step_avg:96.69ms
step:736/1770 train_time:70202ms step_avg:96.70ms
step:737/1770 train_time:70302ms step_avg:96.70ms
step:738/1770 train_time:70401ms step_avg:96.71ms
step:739/1770 train_time:70502ms step_avg:96.71ms
step:740/1770 train_time:70602ms step_avg:96.71ms
step:741/1770 train_time:70702ms step_avg:96.72ms
step:742/1770 train_time:70802ms step_avg:96.72ms
step:743/1770 train_time:70902ms step_avg:96.73ms
step:744/1770 train_time:71002ms step_avg:96.73ms
step:745/1770 train_time:71101ms step_avg:96.74ms
step:746/1770 train_time:71201ms step_avg:96.74ms
step:747/1770 train_time:71301ms step_avg:96.74ms
step:748/1770 train_time:71401ms step_avg:96.75ms
step:749/1770 train_time:71501ms step_avg:96.75ms
step:750/1770 train_time:71600ms step_avg:96.76ms
step:750/1770 val_loss:3.6034 train_time:71699ms step_avg:96.89ms
step:751/1770 train_time:71722ms step_avg:96.79ms
step:752/1770 train_time:71806ms step_avg:96.77ms
step:753/1770 train_time:71908ms step_avg:96.78ms
step:754/1770 train_time:72009ms step_avg:96.79ms
step:755/1770 train_time:72109ms step_avg:96.79ms
step:756/1770 train_time:72208ms step_avg:96.79ms
step:757/1770 train_time:72308ms step_avg:96.80ms
step:758/1770 train_time:72408ms step_avg:96.80ms
step:759/1770 train_time:72508ms step_avg:96.81ms
step:760/1770 train_time:72608ms step_avg:96.81ms
step:761/1770 train_time:72709ms step_avg:96.82ms
step:762/1770 train_time:72811ms step_avg:96.82ms
step:763/1770 train_time:72911ms step_avg:96.83ms
step:764/1770 train_time:73011ms step_avg:96.83ms
step:765/1770 train_time:73111ms step_avg:96.84ms
step:766/1770 train_time:73210ms step_avg:96.84ms
step:767/1770 train_time:73311ms step_avg:96.84ms
step:768/1770 train_time:73411ms step_avg:96.85ms
step:769/1770 train_time:73511ms step_avg:96.85ms
step:770/1770 train_time:73610ms step_avg:96.85ms
step:771/1770 train_time:73710ms step_avg:96.86ms
step:772/1770 train_time:73810ms step_avg:96.86ms
step:773/1770 train_time:73910ms step_avg:96.87ms
step:774/1770 train_time:74010ms step_avg:96.87ms
step:775/1770 train_time:74111ms step_avg:96.88ms
step:776/1770 train_time:74211ms step_avg:96.88ms
step:777/1770 train_time:74311ms step_avg:96.89ms
step:778/1770 train_time:74411ms step_avg:96.89ms
step:779/1770 train_time:74511ms step_avg:96.89ms
step:780/1770 train_time:74610ms step_avg:96.90ms
step:781/1770 train_time:74710ms step_avg:96.90ms
step:782/1770 train_time:74810ms step_avg:96.90ms
step:783/1770 train_time:74910ms step_avg:96.91ms
step:784/1770 train_time:75010ms step_avg:96.91ms
step:785/1770 train_time:75110ms step_avg:96.92ms
step:786/1770 train_time:75210ms step_avg:96.92ms
step:787/1770 train_time:75310ms step_avg:96.92ms
step:788/1770 train_time:75411ms step_avg:96.93ms
step:789/1770 train_time:75511ms step_avg:96.93ms
step:790/1770 train_time:75611ms step_avg:96.94ms
step:791/1770 train_time:75712ms step_avg:96.94ms
step:792/1770 train_time:75812ms step_avg:96.95ms
step:793/1770 train_time:75912ms step_avg:96.95ms
step:794/1770 train_time:76012ms step_avg:96.95ms
step:795/1770 train_time:76112ms step_avg:96.96ms
step:796/1770 train_time:76212ms step_avg:96.96ms
step:797/1770 train_time:76312ms step_avg:96.97ms
step:798/1770 train_time:76412ms step_avg:96.97ms
step:799/1770 train_time:76511ms step_avg:96.97ms
step:800/1770 train_time:76611ms step_avg:96.98ms
step:801/1770 train_time:76711ms step_avg:96.98ms
step:802/1770 train_time:76811ms step_avg:96.98ms
step:803/1770 train_time:76911ms step_avg:96.99ms
step:804/1770 train_time:77012ms step_avg:96.99ms
step:805/1770 train_time:77110ms step_avg:96.99ms
step:806/1770 train_time:77210ms step_avg:97.00ms
step:807/1770 train_time:77311ms step_avg:97.00ms
step:808/1770 train_time:77411ms step_avg:97.01ms
step:809/1770 train_time:77511ms step_avg:97.01ms
step:810/1770 train_time:77611ms step_avg:97.01ms
step:811/1770 train_time:77711ms step_avg:97.02ms
step:812/1770 train_time:77811ms step_avg:97.02ms
step:813/1770 train_time:77911ms step_avg:97.03ms
step:814/1770 train_time:78012ms step_avg:97.03ms
step:815/1770 train_time:78111ms step_avg:97.03ms
step:816/1770 train_time:78211ms step_avg:97.04ms
step:817/1770 train_time:78311ms step_avg:97.04ms
step:818/1770 train_time:78411ms step_avg:97.04ms
step:819/1770 train_time:78512ms step_avg:97.05ms
step:820/1770 train_time:78611ms step_avg:97.05ms
step:821/1770 train_time:78712ms step_avg:97.05ms
step:822/1770 train_time:78812ms step_avg:97.06ms
step:823/1770 train_time:78911ms step_avg:97.06ms
step:824/1770 train_time:79011ms step_avg:97.07ms
step:825/1770 train_time:79111ms step_avg:97.07ms
step:826/1770 train_time:79211ms step_avg:97.07ms
step:827/1770 train_time:79311ms step_avg:97.08ms
step:828/1770 train_time:79411ms step_avg:97.08ms
step:829/1770 train_time:79511ms step_avg:97.08ms
step:830/1770 train_time:79611ms step_avg:97.09ms
step:831/1770 train_time:79711ms step_avg:97.09ms
step:832/1770 train_time:79811ms step_avg:97.09ms
step:833/1770 train_time:79911ms step_avg:97.10ms
step:834/1770 train_time:80012ms step_avg:97.10ms
step:835/1770 train_time:80112ms step_avg:97.10ms
step:836/1770 train_time:80211ms step_avg:97.11ms
step:837/1770 train_time:80311ms step_avg:97.11ms
step:838/1770 train_time:80411ms step_avg:97.11ms
step:839/1770 train_time:80511ms step_avg:97.12ms
step:840/1770 train_time:80612ms step_avg:97.12ms
step:841/1770 train_time:80711ms step_avg:97.13ms
step:842/1770 train_time:80811ms step_avg:97.13ms
step:843/1770 train_time:80911ms step_avg:97.13ms
step:844/1770 train_time:81011ms step_avg:97.14ms
step:845/1770 train_time:81111ms step_avg:97.14ms
step:846/1770 train_time:81210ms step_avg:97.14ms
step:847/1770 train_time:81311ms step_avg:97.15ms
step:848/1770 train_time:81410ms step_avg:97.15ms
step:849/1770 train_time:81511ms step_avg:97.15ms
step:850/1770 train_time:81611ms step_avg:97.16ms
step:851/1770 train_time:81712ms step_avg:97.16ms
step:852/1770 train_time:81813ms step_avg:97.16ms
step:853/1770 train_time:81912ms step_avg:97.17ms
step:854/1770 train_time:82011ms step_avg:97.17ms
step:855/1770 train_time:82111ms step_avg:97.17ms
step:856/1770 train_time:82211ms step_avg:97.18ms
step:857/1770 train_time:82310ms step_avg:97.18ms
step:858/1770 train_time:82411ms step_avg:97.18ms
step:859/1770 train_time:82511ms step_avg:97.19ms
step:860/1770 train_time:82611ms step_avg:97.19ms
step:861/1770 train_time:82711ms step_avg:97.19ms
step:862/1770 train_time:82810ms step_avg:97.20ms
step:863/1770 train_time:82910ms step_avg:97.20ms
step:864/1770 train_time:83010ms step_avg:97.20ms
step:865/1770 train_time:83111ms step_avg:97.21ms
step:866/1770 train_time:83212ms step_avg:97.21ms
step:867/1770 train_time:83312ms step_avg:97.21ms
step:868/1770 train_time:83412ms step_avg:97.22ms
step:869/1770 train_time:83512ms step_avg:97.22ms
step:870/1770 train_time:83612ms step_avg:97.22ms
step:871/1770 train_time:83711ms step_avg:97.23ms
step:872/1770 train_time:83812ms step_avg:97.23ms
step:873/1770 train_time:83911ms step_avg:97.23ms
step:874/1770 train_time:84011ms step_avg:97.23ms
step:875/1770 train_time:84111ms step_avg:97.24ms
step:875/1770 val_loss:3.5544 train_time:84209ms step_avg:97.35ms
step:876/1770 train_time:84231ms step_avg:97.27ms
step:877/1770 train_time:84316ms step_avg:97.25ms
step:878/1770 train_time:84418ms step_avg:97.26ms
step:879/1770 train_time:84518ms step_avg:97.26ms
step:880/1770 train_time:84618ms step_avg:97.26ms
step:881/1770 train_time:84717ms step_avg:97.26ms
step:882/1770 train_time:84816ms step_avg:97.27ms
step:883/1770 train_time:84916ms step_avg:97.27ms
step:884/1770 train_time:85016ms step_avg:97.27ms
step:885/1770 train_time:85115ms step_avg:97.27ms
step:886/1770 train_time:85216ms step_avg:97.28ms
step:887/1770 train_time:85318ms step_avg:97.28ms
step:888/1770 train_time:85419ms step_avg:97.29ms
step:889/1770 train_time:85519ms step_avg:97.29ms
step:890/1770 train_time:85619ms step_avg:97.29ms
step:891/1770 train_time:85720ms step_avg:97.30ms
step:892/1770 train_time:85820ms step_avg:97.30ms
step:893/1770 train_time:85920ms step_avg:97.31ms
step:894/1770 train_time:86020ms step_avg:97.31ms
step:895/1770 train_time:86121ms step_avg:97.31ms
step:896/1770 train_time:86222ms step_avg:97.32ms
step:897/1770 train_time:86324ms step_avg:97.32ms
step:898/1770 train_time:86424ms step_avg:97.32ms
step:899/1770 train_time:86526ms step_avg:97.33ms
step:900/1770 train_time:86626ms step_avg:97.33ms
step:901/1770 train_time:86725ms step_avg:97.33ms
step:902/1770 train_time:86826ms step_avg:97.34ms
step:903/1770 train_time:86925ms step_avg:97.34ms
step:904/1770 train_time:87025ms step_avg:97.34ms
step:905/1770 train_time:87125ms step_avg:97.35ms
step:906/1770 train_time:87224ms step_avg:97.35ms
step:907/1770 train_time:87325ms step_avg:97.35ms
step:908/1770 train_time:87425ms step_avg:97.35ms
step:909/1770 train_time:87524ms step_avg:97.36ms
step:910/1770 train_time:87626ms step_avg:97.36ms
step:911/1770 train_time:87726ms step_avg:97.37ms
step:912/1770 train_time:87826ms step_avg:97.37ms
step:913/1770 train_time:87926ms step_avg:97.37ms
step:914/1770 train_time:88025ms step_avg:97.37ms
step:915/1770 train_time:88125ms step_avg:97.38ms
step:916/1770 train_time:88225ms step_avg:97.38ms
step:917/1770 train_time:88325ms step_avg:97.38ms
step:918/1770 train_time:88425ms step_avg:97.38ms
step:919/1770 train_time:88525ms step_avg:97.39ms
step:920/1770 train_time:88627ms step_avg:97.39ms
step:921/1770 train_time:88729ms step_avg:97.40ms
step:922/1770 train_time:88830ms step_avg:97.40ms
step:923/1770 train_time:88931ms step_avg:97.40ms
step:924/1770 train_time:89032ms step_avg:97.41ms
step:925/1770 train_time:89133ms step_avg:97.41ms
step:926/1770 train_time:89234ms step_avg:97.42ms
step:927/1770 train_time:89336ms step_avg:97.42ms
step:928/1770 train_time:89438ms step_avg:97.43ms
step:929/1770 train_time:89539ms step_avg:97.43ms
step:930/1770 train_time:89640ms step_avg:97.44ms
step:931/1770 train_time:89742ms step_avg:97.44ms
step:932/1770 train_time:89844ms step_avg:97.44ms
step:933/1770 train_time:89945ms step_avg:97.45ms
step:934/1770 train_time:90046ms step_avg:97.45ms
step:935/1770 train_time:90147ms step_avg:97.46ms
step:936/1770 train_time:90249ms step_avg:97.46ms
step:937/1770 train_time:90349ms step_avg:97.46ms
step:938/1770 train_time:90450ms step_avg:97.47ms
step:939/1770 train_time:90551ms step_avg:97.47ms
step:940/1770 train_time:90652ms step_avg:97.47ms
step:941/1770 train_time:90753ms step_avg:97.48ms
step:942/1770 train_time:90855ms step_avg:97.48ms
step:943/1770 train_time:90957ms step_avg:97.49ms
step:944/1770 train_time:91057ms step_avg:97.49ms
step:945/1770 train_time:91158ms step_avg:97.50ms
step:946/1770 train_time:91260ms step_avg:97.50ms
step:947/1770 train_time:91362ms step_avg:97.50ms
step:948/1770 train_time:91468ms step_avg:97.51ms
step:949/1770 train_time:91566ms step_avg:97.51ms
step:950/1770 train_time:91668ms step_avg:97.52ms
step:951/1770 train_time:91770ms step_avg:97.52ms
step:952/1770 train_time:91870ms step_avg:97.53ms
step:953/1770 train_time:91971ms step_avg:97.53ms
step:954/1770 train_time:92072ms step_avg:97.53ms
step:955/1770 train_time:92173ms step_avg:97.54ms
step:956/1770 train_time:92274ms step_avg:97.54ms
step:957/1770 train_time:92375ms step_avg:97.55ms
step:958/1770 train_time:92476ms step_avg:97.55ms
step:959/1770 train_time:92577ms step_avg:97.55ms
step:960/1770 train_time:92679ms step_avg:97.56ms
step:961/1770 train_time:92781ms step_avg:97.56ms
step:962/1770 train_time:92884ms step_avg:97.57ms
step:963/1770 train_time:92985ms step_avg:97.57ms
step:964/1770 train_time:93087ms step_avg:97.58ms
step:965/1770 train_time:93188ms step_avg:97.58ms
step:966/1770 train_time:93288ms step_avg:97.58ms
step:967/1770 train_time:93389ms step_avg:97.59ms
step:968/1770 train_time:93490ms step_avg:97.59ms
step:969/1770 train_time:93591ms step_avg:97.59ms
step:970/1770 train_time:93692ms step_avg:97.60ms
step:971/1770 train_time:93793ms step_avg:97.60ms
step:972/1770 train_time:93895ms step_avg:97.60ms
step:973/1770 train_time:93996ms step_avg:97.61ms
step:974/1770 train_time:94097ms step_avg:97.61ms
step:975/1770 train_time:94199ms step_avg:97.62ms
step:976/1770 train_time:94300ms step_avg:97.62ms
step:977/1770 train_time:94403ms step_avg:97.62ms
step:978/1770 train_time:94505ms step_avg:97.63ms
step:979/1770 train_time:94607ms step_avg:97.63ms
step:980/1770 train_time:94708ms step_avg:97.64ms
step:981/1770 train_time:94809ms step_avg:97.64ms
step:982/1770 train_time:94909ms step_avg:97.64ms
step:983/1770 train_time:95011ms step_avg:97.65ms
step:984/1770 train_time:95112ms step_avg:97.65ms
step:985/1770 train_time:95214ms step_avg:97.66ms
step:986/1770 train_time:95315ms step_avg:97.66ms
step:987/1770 train_time:95416ms step_avg:97.66ms
step:988/1770 train_time:95516ms step_avg:97.66ms
step:989/1770 train_time:95619ms step_avg:97.67ms
step:990/1770 train_time:95720ms step_avg:97.67ms
step:991/1770 train_time:95822ms step_avg:97.68ms
step:992/1770 train_time:95925ms step_avg:97.68ms
step:993/1770 train_time:96026ms step_avg:97.69ms
step:994/1770 train_time:96128ms step_avg:97.69ms
step:995/1770 train_time:96229ms step_avg:97.69ms
step:996/1770 train_time:96329ms step_avg:97.70ms
step:997/1770 train_time:96430ms step_avg:97.70ms
step:998/1770 train_time:96530ms step_avg:97.70ms
step:999/1770 train_time:96630ms step_avg:97.70ms
step:1000/1770 train_time:96731ms step_avg:97.71ms
step:1000/1770 val_loss:3.5149 train_time:96832ms step_avg:97.81ms
step:1001/1770 train_time:96852ms step_avg:97.73ms
step:1002/1770 train_time:96939ms step_avg:97.72ms
step:1003/1770 train_time:97044ms step_avg:97.73ms
step:1004/1770 train_time:97145ms step_avg:97.73ms
step:1005/1770 train_time:97246ms step_avg:97.73ms
step:1006/1770 train_time:97347ms step_avg:97.74ms
step:1007/1770 train_time:97448ms step_avg:97.74ms
step:1008/1770 train_time:97549ms step_avg:97.74ms
step:1009/1770 train_time:97650ms step_avg:97.75ms
step:1010/1770 train_time:97750ms step_avg:97.75ms
step:1011/1770 train_time:97852ms step_avg:97.75ms
step:1012/1770 train_time:97954ms step_avg:97.76ms
step:1013/1770 train_time:98055ms step_avg:97.76ms
step:1014/1770 train_time:98157ms step_avg:97.77ms
step:1015/1770 train_time:98257ms step_avg:97.77ms
step:1016/1770 train_time:98358ms step_avg:97.77ms
step:1017/1770 train_time:98459ms step_avg:97.78ms
step:1018/1770 train_time:98560ms step_avg:97.78ms
step:1019/1770 train_time:98662ms step_avg:97.78ms
step:1020/1770 train_time:98765ms step_avg:97.79ms
step:1021/1770 train_time:98867ms step_avg:97.79ms
step:1022/1770 train_time:98969ms step_avg:97.80ms
step:1023/1770 train_time:99070ms step_avg:97.80ms
step:1024/1770 train_time:99171ms step_avg:97.80ms
step:1025/1770 train_time:99271ms step_avg:97.80ms
step:1026/1770 train_time:99372ms step_avg:97.81ms
step:1027/1770 train_time:99474ms step_avg:97.81ms
step:1028/1770 train_time:99575ms step_avg:97.81ms
step:1029/1770 train_time:99676ms step_avg:97.82ms
step:1030/1770 train_time:99778ms step_avg:97.82ms
step:1031/1770 train_time:99879ms step_avg:97.82ms
step:1032/1770 train_time:99980ms step_avg:97.83ms
step:1033/1770 train_time:100082ms step_avg:97.83ms
step:1034/1770 train_time:100183ms step_avg:97.83ms
step:1035/1770 train_time:100285ms step_avg:97.84ms
step:1036/1770 train_time:100387ms step_avg:97.84ms
step:1037/1770 train_time:100490ms step_avg:97.85ms
step:1038/1770 train_time:100590ms step_avg:97.85ms
step:1039/1770 train_time:100691ms step_avg:97.85ms
step:1040/1770 train_time:100791ms step_avg:97.86ms
step:1041/1770 train_time:100892ms step_avg:97.86ms
step:1042/1770 train_time:100993ms step_avg:97.86ms
step:1043/1770 train_time:101094ms step_avg:97.86ms
step:1044/1770 train_time:101196ms step_avg:97.87ms
step:1045/1770 train_time:101298ms step_avg:97.87ms
step:1046/1770 train_time:101399ms step_avg:97.88ms
step:1047/1770 train_time:101501ms step_avg:97.88ms
step:1048/1770 train_time:101603ms step_avg:97.88ms
step:1049/1770 train_time:101705ms step_avg:97.89ms
step:1050/1770 train_time:101807ms step_avg:97.89ms
step:1051/1770 train_time:101909ms step_avg:97.89ms
step:1052/1770 train_time:102010ms step_avg:97.90ms
step:1053/1770 train_time:102111ms step_avg:97.90ms
step:1054/1770 train_time:102211ms step_avg:97.90ms
step:1055/1770 train_time:102312ms step_avg:97.91ms
step:1056/1770 train_time:102413ms step_avg:97.91ms
step:1057/1770 train_time:102514ms step_avg:97.91ms
step:1058/1770 train_time:102616ms step_avg:97.92ms
step:1059/1770 train_time:102718ms step_avg:97.92ms
step:1060/1770 train_time:102819ms step_avg:97.92ms
step:1061/1770 train_time:102922ms step_avg:97.93ms
step:1062/1770 train_time:103024ms step_avg:97.93ms
step:1063/1770 train_time:103127ms step_avg:97.94ms
step:1064/1770 train_time:103229ms step_avg:97.94ms
step:1065/1770 train_time:103330ms step_avg:97.94ms
step:1066/1770 train_time:103435ms step_avg:97.95ms
step:1067/1770 train_time:103532ms step_avg:97.95ms
step:1068/1770 train_time:103634ms step_avg:97.95ms
step:1069/1770 train_time:103735ms step_avg:97.96ms
step:1070/1770 train_time:103836ms step_avg:97.96ms
step:1071/1770 train_time:103938ms step_avg:97.96ms
step:1072/1770 train_time:104040ms step_avg:97.97ms
step:1073/1770 train_time:104141ms step_avg:97.97ms
step:1074/1770 train_time:104243ms step_avg:97.97ms
step:1075/1770 train_time:104345ms step_avg:97.98ms
step:1076/1770 train_time:104449ms step_avg:97.98ms
step:1077/1770 train_time:104551ms step_avg:97.99ms
step:1078/1770 train_time:104652ms step_avg:97.99ms
step:1079/1770 train_time:104752ms step_avg:97.99ms
step:1080/1770 train_time:104853ms step_avg:97.99ms
step:1081/1770 train_time:104955ms step_avg:98.00ms
step:1082/1770 train_time:105057ms step_avg:98.00ms
step:1083/1770 train_time:105158ms step_avg:98.00ms
step:1084/1770 train_time:105259ms step_avg:98.01ms
step:1085/1770 train_time:105361ms step_avg:98.01ms
step:1086/1770 train_time:105464ms step_avg:98.02ms
step:1087/1770 train_time:105566ms step_avg:98.02ms
step:1088/1770 train_time:105667ms step_avg:98.02ms
step:1089/1770 train_time:105768ms step_avg:98.02ms
step:1090/1770 train_time:105870ms step_avg:98.03ms
step:1091/1770 train_time:105971ms step_avg:98.03ms
step:1092/1770 train_time:106072ms step_avg:98.03ms
step:1093/1770 train_time:106173ms step_avg:98.04ms
step:1094/1770 train_time:106275ms step_avg:98.04ms
step:1095/1770 train_time:106376ms step_avg:98.04ms
step:1096/1770 train_time:106478ms step_avg:98.05ms
step:1097/1770 train_time:106579ms step_avg:98.05ms
step:1098/1770 train_time:106680ms step_avg:98.05ms
step:1099/1770 train_time:106781ms step_avg:98.05ms
step:1100/1770 train_time:106883ms step_avg:98.06ms
step:1101/1770 train_time:106985ms step_avg:98.06ms
step:1102/1770 train_time:107087ms step_avg:98.06ms
step:1103/1770 train_time:107189ms step_avg:98.07ms
step:1104/1770 train_time:107291ms step_avg:98.07ms
step:1105/1770 train_time:107391ms step_avg:98.07ms
step:1106/1770 train_time:107493ms step_avg:98.08ms
step:1107/1770 train_time:107594ms step_avg:98.08ms
step:1108/1770 train_time:107696ms step_avg:98.08ms
step:1109/1770 train_time:107797ms step_avg:98.09ms
step:1110/1770 train_time:107899ms step_avg:98.09ms
step:1111/1770 train_time:108001ms step_avg:98.09ms
step:1112/1770 train_time:108103ms step_avg:98.10ms
step:1113/1770 train_time:108206ms step_avg:98.10ms
step:1114/1770 train_time:108308ms step_avg:98.10ms
step:1115/1770 train_time:108410ms step_avg:98.11ms
step:1116/1770 train_time:108511ms step_avg:98.11ms
step:1117/1770 train_time:108612ms step_avg:98.11ms
step:1118/1770 train_time:108713ms step_avg:98.12ms
step:1119/1770 train_time:108815ms step_avg:98.12ms
step:1120/1770 train_time:108915ms step_avg:98.12ms
step:1121/1770 train_time:109016ms step_avg:98.12ms
step:1122/1770 train_time:109118ms step_avg:98.13ms
step:1123/1770 train_time:109219ms step_avg:98.13ms
step:1124/1770 train_time:109321ms step_avg:98.13ms
step:1125/1770 train_time:109423ms step_avg:98.14ms
step:1125/1770 val_loss:3.4767 train_time:109525ms step_avg:98.23ms
step:1126/1770 train_time:109547ms step_avg:98.16ms
step:1127/1770 train_time:109633ms step_avg:98.15ms
step:1128/1770 train_time:109736ms step_avg:98.15ms
step:1129/1770 train_time:109838ms step_avg:98.16ms
step:1130/1770 train_time:109940ms step_avg:98.16ms
step:1131/1770 train_time:110042ms step_avg:98.16ms
step:1132/1770 train_time:110143ms step_avg:98.17ms
step:1133/1770 train_time:110245ms step_avg:98.17ms
step:1134/1770 train_time:110346ms step_avg:98.17ms
step:1135/1770 train_time:110447ms step_avg:98.17ms
step:1136/1770 train_time:110548ms step_avg:98.18ms
step:1137/1770 train_time:110651ms step_avg:98.18ms
step:1138/1770 train_time:110753ms step_avg:98.18ms
step:1139/1770 train_time:110854ms step_avg:98.19ms
step:1140/1770 train_time:110955ms step_avg:98.19ms
step:1141/1770 train_time:111057ms step_avg:98.19ms
step:1142/1770 train_time:111159ms step_avg:98.20ms
step:1143/1770 train_time:111261ms step_avg:98.20ms
step:1144/1770 train_time:111362ms step_avg:98.20ms
step:1145/1770 train_time:111464ms step_avg:98.21ms
step:1146/1770 train_time:111566ms step_avg:98.21ms
step:1147/1770 train_time:111667ms step_avg:98.21ms
step:1148/1770 train_time:111768ms step_avg:98.21ms
step:1149/1770 train_time:111868ms step_avg:98.22ms
step:1150/1770 train_time:111969ms step_avg:98.22ms
step:1151/1770 train_time:112071ms step_avg:98.22ms
step:1152/1770 train_time:112173ms step_avg:98.22ms
step:1153/1770 train_time:112274ms step_avg:98.23ms
step:1154/1770 train_time:112377ms step_avg:98.23ms
step:1155/1770 train_time:112479ms step_avg:98.24ms
step:1156/1770 train_time:112581ms step_avg:98.24ms
step:1157/1770 train_time:112685ms step_avg:98.24ms
step:1158/1770 train_time:112787ms step_avg:98.25ms
step:1159/1770 train_time:112887ms step_avg:98.25ms
step:1160/1770 train_time:112988ms step_avg:98.25ms
step:1161/1770 train_time:113089ms step_avg:98.25ms
step:1162/1770 train_time:113191ms step_avg:98.26ms
step:1163/1770 train_time:113292ms step_avg:98.26ms
step:1164/1770 train_time:113394ms step_avg:98.26ms
step:1165/1770 train_time:113496ms step_avg:98.26ms
step:1166/1770 train_time:113597ms step_avg:98.27ms
step:1167/1770 train_time:113699ms step_avg:98.27ms
step:1168/1770 train_time:113803ms step_avg:98.28ms
step:1169/1770 train_time:113904ms step_avg:98.28ms
step:1170/1770 train_time:114004ms step_avg:98.28ms
step:1171/1770 train_time:114105ms step_avg:98.28ms
step:1172/1770 train_time:114206ms step_avg:98.28ms
step:1173/1770 train_time:114307ms step_avg:98.29ms
step:1174/1770 train_time:114409ms step_avg:98.29ms
step:1175/1770 train_time:114510ms step_avg:98.29ms
step:1176/1770 train_time:114613ms step_avg:98.30ms
step:1177/1770 train_time:114714ms step_avg:98.30ms
step:1178/1770 train_time:114816ms step_avg:98.30ms
step:1179/1770 train_time:114917ms step_avg:98.30ms
step:1180/1770 train_time:115019ms step_avg:98.31ms
step:1181/1770 train_time:115121ms step_avg:98.31ms
step:1182/1770 train_time:115223ms step_avg:98.31ms
step:1183/1770 train_time:115325ms step_avg:98.32ms
step:1184/1770 train_time:115429ms step_avg:98.32ms
step:1185/1770 train_time:115531ms step_avg:98.32ms
step:1186/1770 train_time:115635ms step_avg:98.33ms
step:1187/1770 train_time:115740ms step_avg:98.33ms
step:1188/1770 train_time:115843ms step_avg:98.34ms
step:1189/1770 train_time:115944ms step_avg:98.34ms
step:1190/1770 train_time:116046ms step_avg:98.34ms
step:1191/1770 train_time:116149ms step_avg:98.35ms
step:1192/1770 train_time:116252ms step_avg:98.35ms
step:1193/1770 train_time:116355ms step_avg:98.36ms
step:1194/1770 train_time:116458ms step_avg:98.36ms
step:1195/1770 train_time:116561ms step_avg:98.36ms
step:1196/1770 train_time:116665ms step_avg:98.37ms
step:1197/1770 train_time:116766ms step_avg:98.37ms
step:1198/1770 train_time:116869ms step_avg:98.37ms
step:1199/1770 train_time:116972ms step_avg:98.38ms
step:1200/1770 train_time:117075ms step_avg:98.38ms
step:1201/1770 train_time:117178ms step_avg:98.39ms
step:1202/1770 train_time:117281ms step_avg:98.39ms
step:1203/1770 train_time:117383ms step_avg:98.39ms
step:1204/1770 train_time:117486ms step_avg:98.40ms
step:1205/1770 train_time:117587ms step_avg:98.40ms
step:1206/1770 train_time:117690ms step_avg:98.40ms
step:1207/1770 train_time:117792ms step_avg:98.41ms
step:1208/1770 train_time:117895ms step_avg:98.41ms
step:1209/1770 train_time:117998ms step_avg:98.41ms
step:1210/1770 train_time:118100ms step_avg:98.42ms
step:1211/1770 train_time:118204ms step_avg:98.42ms
step:1212/1770 train_time:118308ms step_avg:98.43ms
step:1213/1770 train_time:118410ms step_avg:98.43ms
step:1214/1770 train_time:118512ms step_avg:98.43ms
step:1215/1770 train_time:118615ms step_avg:98.44ms
step:1216/1770 train_time:118721ms step_avg:98.44ms
step:1217/1770 train_time:118824ms step_avg:98.45ms
step:1218/1770 train_time:118925ms step_avg:98.45ms
step:1219/1770 train_time:119028ms step_avg:98.45ms
step:1220/1770 train_time:119130ms step_avg:98.45ms
step:1221/1770 train_time:119232ms step_avg:98.46ms
step:1222/1770 train_time:119336ms step_avg:98.46ms
step:1223/1770 train_time:119439ms step_avg:98.47ms
step:1224/1770 train_time:119543ms step_avg:98.47ms
step:1225/1770 train_time:119646ms step_avg:98.47ms
step:1226/1770 train_time:119750ms step_avg:98.48ms
step:1227/1770 train_time:119854ms step_avg:98.48ms
step:1228/1770 train_time:119959ms step_avg:98.49ms
step:1229/1770 train_time:120062ms step_avg:98.49ms
step:1230/1770 train_time:120166ms step_avg:98.50ms
step:1231/1770 train_time:120269ms step_avg:98.50ms
step:1232/1770 train_time:120372ms step_avg:98.50ms
step:1233/1770 train_time:120474ms step_avg:98.51ms
step:1234/1770 train_time:120577ms step_avg:98.51ms
step:1235/1770 train_time:120683ms step_avg:98.52ms
step:1236/1770 train_time:120782ms step_avg:98.52ms
step:1237/1770 train_time:120885ms step_avg:98.52ms
step:1238/1770 train_time:120988ms step_avg:98.52ms
step:1239/1770 train_time:121090ms step_avg:98.53ms
step:1240/1770 train_time:121193ms step_avg:98.53ms
step:1241/1770 train_time:121296ms step_avg:98.53ms
step:1242/1770 train_time:121399ms step_avg:98.54ms
step:1243/1770 train_time:121503ms step_avg:98.54ms
step:1244/1770 train_time:121604ms step_avg:98.54ms
step:1245/1770 train_time:121707ms step_avg:98.55ms
step:1246/1770 train_time:121809ms step_avg:98.55ms
step:1247/1770 train_time:121912ms step_avg:98.55ms
step:1248/1770 train_time:122015ms step_avg:98.56ms
step:1249/1770 train_time:122117ms step_avg:98.56ms
step:1250/1770 train_time:122220ms step_avg:98.56ms
step:1250/1770 val_loss:3.4300 train_time:122323ms step_avg:98.65ms
step:1251/1770 train_time:122343ms step_avg:98.58ms
step:1252/1770 train_time:122436ms step_avg:98.58ms
step:1253/1770 train_time:122538ms step_avg:98.58ms
step:1254/1770 train_time:122645ms step_avg:98.59ms
step:1255/1770 train_time:122747ms step_avg:98.59ms
step:1256/1770 train_time:122849ms step_avg:98.59ms
step:1257/1770 train_time:122952ms step_avg:98.60ms
step:1258/1770 train_time:123055ms step_avg:98.60ms
step:1259/1770 train_time:123158ms step_avg:98.61ms
step:1260/1770 train_time:123259ms step_avg:98.61ms
step:1261/1770 train_time:123362ms step_avg:98.61ms
step:1262/1770 train_time:123466ms step_avg:98.61ms
step:1263/1770 train_time:123568ms step_avg:98.62ms
step:1264/1770 train_time:123672ms step_avg:98.62ms
step:1265/1770 train_time:123775ms step_avg:98.63ms
step:1266/1770 train_time:123878ms step_avg:98.63ms
step:1267/1770 train_time:123981ms step_avg:98.63ms
step:1268/1770 train_time:124085ms step_avg:98.64ms
step:1269/1770 train_time:124187ms step_avg:98.64ms
step:1270/1770 train_time:124291ms step_avg:98.64ms
step:1271/1770 train_time:124394ms step_avg:98.65ms
step:1272/1770 train_time:124496ms step_avg:98.65ms
step:1273/1770 train_time:124599ms step_avg:98.65ms
step:1274/1770 train_time:124702ms step_avg:98.66ms
step:1275/1770 train_time:124804ms step_avg:98.66ms
step:1276/1770 train_time:124907ms step_avg:98.66ms
step:1277/1770 train_time:125009ms step_avg:98.67ms
step:1278/1770 train_time:125114ms step_avg:98.67ms
step:1279/1770 train_time:125217ms step_avg:98.67ms
step:1280/1770 train_time:125320ms step_avg:98.68ms
step:1281/1770 train_time:125422ms step_avg:98.68ms
step:1282/1770 train_time:125526ms step_avg:98.68ms
step:1283/1770 train_time:125629ms step_avg:98.69ms
step:1284/1770 train_time:125734ms step_avg:98.69ms
step:1285/1770 train_time:125836ms step_avg:98.70ms
step:1286/1770 train_time:125940ms step_avg:98.70ms
step:1287/1770 train_time:126047ms step_avg:98.71ms
step:1288/1770 train_time:126148ms step_avg:98.71ms
step:1289/1770 train_time:126251ms step_avg:98.71ms
step:1290/1770 train_time:126354ms step_avg:98.71ms
step:1291/1770 train_time:126457ms step_avg:98.72ms
step:1292/1770 train_time:126560ms step_avg:98.72ms
step:1293/1770 train_time:126663ms step_avg:98.72ms
step:1294/1770 train_time:126765ms step_avg:98.73ms
step:1295/1770 train_time:126867ms step_avg:98.73ms
step:1296/1770 train_time:126971ms step_avg:98.73ms
step:1297/1770 train_time:127074ms step_avg:98.74ms
step:1298/1770 train_time:127177ms step_avg:98.74ms
step:1299/1770 train_time:127279ms step_avg:98.74ms
step:1300/1770 train_time:127382ms step_avg:98.75ms
step:1301/1770 train_time:127486ms step_avg:98.75ms
step:1302/1770 train_time:127589ms step_avg:98.75ms
step:1303/1770 train_time:127691ms step_avg:98.76ms
step:1304/1770 train_time:127795ms step_avg:98.76ms
step:1305/1770 train_time:127898ms step_avg:98.76ms
step:1306/1770 train_time:128000ms step_avg:98.77ms
step:1307/1770 train_time:128102ms step_avg:98.77ms
step:1308/1770 train_time:128205ms step_avg:98.77ms
step:1309/1770 train_time:128308ms step_avg:98.77ms
step:1310/1770 train_time:128410ms step_avg:98.78ms
step:1311/1770 train_time:128513ms step_avg:98.78ms
step:1312/1770 train_time:128615ms step_avg:98.78ms
step:1313/1770 train_time:128717ms step_avg:98.79ms
step:1314/1770 train_time:128819ms step_avg:98.79ms
step:1315/1770 train_time:128922ms step_avg:98.79ms
step:1316/1770 train_time:129025ms step_avg:98.79ms
step:1317/1770 train_time:129128ms step_avg:98.80ms
step:1318/1770 train_time:129235ms step_avg:98.80ms
step:1319/1770 train_time:129339ms step_avg:98.81ms
step:1320/1770 train_time:129440ms step_avg:98.81ms
step:1321/1770 train_time:129543ms step_avg:98.81ms
step:1322/1770 train_time:129646ms step_avg:98.82ms
step:1323/1770 train_time:129749ms step_avg:98.82ms
step:1324/1770 train_time:129854ms step_avg:98.82ms
step:1325/1770 train_time:129957ms step_avg:98.83ms
step:1326/1770 train_time:130061ms step_avg:98.83ms
step:1327/1770 train_time:130164ms step_avg:98.83ms
step:1328/1770 train_time:130266ms step_avg:98.84ms
step:1329/1770 train_time:130369ms step_avg:98.84ms
step:1330/1770 train_time:130471ms step_avg:98.84ms
step:1331/1770 train_time:130574ms step_avg:98.84ms
step:1332/1770 train_time:130676ms step_avg:98.85ms
step:1333/1770 train_time:130779ms step_avg:98.85ms
step:1334/1770 train_time:130881ms step_avg:98.85ms
step:1335/1770 train_time:130983ms step_avg:98.86ms
step:1336/1770 train_time:131086ms step_avg:98.86ms
step:1337/1770 train_time:131189ms step_avg:98.86ms
step:1338/1770 train_time:131291ms step_avg:98.86ms
step:1339/1770 train_time:131396ms step_avg:98.87ms
step:1340/1770 train_time:131499ms step_avg:98.87ms
step:1341/1770 train_time:131602ms step_avg:98.87ms
step:1342/1770 train_time:131705ms step_avg:98.88ms
step:1343/1770 train_time:131808ms step_avg:98.88ms
step:1344/1770 train_time:131912ms step_avg:98.88ms
step:1345/1770 train_time:132014ms step_avg:98.89ms
step:1346/1770 train_time:132116ms step_avg:98.89ms
step:1347/1770 train_time:132218ms step_avg:98.89ms
step:1348/1770 train_time:132324ms step_avg:98.90ms
step:1349/1770 train_time:132427ms step_avg:98.90ms
step:1350/1770 train_time:132530ms step_avg:98.90ms
step:1351/1770 train_time:132634ms step_avg:98.91ms
step:1352/1770 train_time:132737ms step_avg:98.91ms
step:1353/1770 train_time:132841ms step_avg:98.91ms
step:1354/1770 train_time:132948ms step_avg:98.92ms
step:1355/1770 train_time:133045ms step_avg:98.92ms
step:1356/1770 train_time:133148ms step_avg:98.92ms
step:1357/1770 train_time:133251ms step_avg:98.92ms
step:1358/1770 train_time:133355ms step_avg:98.93ms
step:1359/1770 train_time:133458ms step_avg:98.93ms
step:1360/1770 train_time:133562ms step_avg:98.93ms
step:1361/1770 train_time:133665ms step_avg:98.94ms
step:1362/1770 train_time:133768ms step_avg:98.94ms
step:1363/1770 train_time:133872ms step_avg:98.94ms
step:1364/1770 train_time:133976ms step_avg:98.95ms
step:1365/1770 train_time:134078ms step_avg:98.95ms
step:1366/1770 train_time:134180ms step_avg:98.95ms
step:1367/1770 train_time:134284ms step_avg:98.96ms
step:1368/1770 train_time:134386ms step_avg:98.96ms
step:1369/1770 train_time:134489ms step_avg:98.96ms
step:1370/1770 train_time:134593ms step_avg:98.97ms
step:1371/1770 train_time:134695ms step_avg:98.97ms
step:1372/1770 train_time:134798ms step_avg:98.97ms
step:1373/1770 train_time:134902ms step_avg:98.97ms
step:1374/1770 train_time:135007ms step_avg:98.98ms
step:1375/1770 train_time:135110ms step_avg:98.98ms
step:1375/1770 val_loss:3.3881 train_time:135212ms step_avg:99.06ms
step:1376/1770 train_time:135234ms step_avg:99.00ms
step:1377/1770 train_time:135323ms step_avg:98.99ms
step:1378/1770 train_time:135425ms step_avg:98.99ms
step:1379/1770 train_time:135528ms step_avg:99.00ms
step:1380/1770 train_time:135630ms step_avg:99.00ms
step:1381/1770 train_time:135734ms step_avg:99.00ms
step:1382/1770 train_time:135836ms step_avg:99.01ms
step:1383/1770 train_time:135939ms step_avg:99.01ms
step:1384/1770 train_time:136041ms step_avg:99.01ms
step:1385/1770 train_time:136144ms step_avg:99.01ms
step:1386/1770 train_time:136248ms step_avg:99.02ms
step:1387/1770 train_time:136353ms step_avg:99.02ms
step:1388/1770 train_time:136455ms step_avg:99.02ms
step:1389/1770 train_time:136558ms step_avg:99.03ms
step:1390/1770 train_time:136660ms step_avg:99.03ms
step:1391/1770 train_time:136762ms step_avg:99.03ms
step:1392/1770 train_time:136865ms step_avg:99.03ms
step:1393/1770 train_time:136968ms step_avg:99.04ms
step:1394/1770 train_time:137071ms step_avg:99.04ms
step:1395/1770 train_time:137175ms step_avg:99.04ms
step:1396/1770 train_time:137278ms step_avg:99.05ms
step:1397/1770 train_time:137381ms step_avg:99.05ms
step:1398/1770 train_time:137484ms step_avg:99.05ms
step:1399/1770 train_time:137587ms step_avg:99.05ms
step:1400/1770 train_time:137690ms step_avg:99.06ms
step:1401/1770 train_time:137794ms step_avg:99.06ms
step:1402/1770 train_time:137897ms step_avg:99.06ms
step:1403/1770 train_time:137999ms step_avg:99.07ms
step:1404/1770 train_time:138102ms step_avg:99.07ms
step:1405/1770 train_time:138205ms step_avg:99.07ms
step:1406/1770 train_time:138308ms step_avg:99.07ms
step:1407/1770 train_time:138411ms step_avg:99.08ms
step:1408/1770 train_time:138514ms step_avg:99.08ms
step:1409/1770 train_time:138618ms step_avg:99.08ms
step:1410/1770 train_time:138721ms step_avg:99.09ms
step:1411/1770 train_time:138824ms step_avg:99.09ms
step:1412/1770 train_time:138926ms step_avg:99.09ms
step:1413/1770 train_time:139029ms step_avg:99.09ms
step:1414/1770 train_time:139133ms step_avg:99.10ms
step:1415/1770 train_time:139237ms step_avg:99.10ms
step:1416/1770 train_time:139339ms step_avg:99.10ms
step:1417/1770 train_time:139442ms step_avg:99.11ms
step:1418/1770 train_time:139544ms step_avg:99.11ms
step:1419/1770 train_time:139647ms step_avg:99.11ms
step:1420/1770 train_time:139751ms step_avg:99.11ms
step:1421/1770 train_time:139854ms step_avg:99.12ms
step:1422/1770 train_time:139956ms step_avg:99.12ms
step:1423/1770 train_time:140059ms step_avg:99.12ms
step:1424/1770 train_time:140161ms step_avg:99.12ms
step:1425/1770 train_time:140264ms step_avg:99.13ms
step:1426/1770 train_time:140367ms step_avg:99.13ms
step:1427/1770 train_time:140471ms step_avg:99.13ms
step:1428/1770 train_time:140575ms step_avg:99.14ms
step:1429/1770 train_time:140678ms step_avg:99.14ms
step:1430/1770 train_time:140780ms step_avg:99.14ms
step:1431/1770 train_time:140885ms step_avg:99.14ms
step:1432/1770 train_time:140987ms step_avg:99.15ms
step:1433/1770 train_time:141089ms step_avg:99.15ms
step:1434/1770 train_time:141192ms step_avg:99.15ms
step:1435/1770 train_time:141295ms step_avg:99.15ms
step:1436/1770 train_time:141400ms step_avg:99.16ms
step:1437/1770 train_time:141503ms step_avg:99.16ms
step:1438/1770 train_time:141605ms step_avg:99.16ms
step:1439/1770 train_time:141707ms step_avg:99.17ms
step:1440/1770 train_time:141811ms step_avg:99.17ms
step:1441/1770 train_time:141918ms step_avg:99.17ms
step:1442/1770 train_time:142021ms step_avg:99.18ms
step:1443/1770 train_time:142123ms step_avg:99.18ms
step:1444/1770 train_time:142226ms step_avg:99.18ms
step:1445/1770 train_time:142330ms step_avg:99.18ms
step:1446/1770 train_time:142434ms step_avg:99.19ms
step:1447/1770 train_time:142538ms step_avg:99.19ms
step:1448/1770 train_time:142642ms step_avg:99.19ms
step:1449/1770 train_time:142747ms step_avg:99.20ms
step:1450/1770 train_time:142851ms step_avg:99.20ms
step:1451/1770 train_time:142955ms step_avg:99.21ms
step:1452/1770 train_time:143059ms step_avg:99.21ms
step:1453/1770 train_time:143163ms step_avg:99.21ms
step:1454/1770 train_time:143266ms step_avg:99.21ms
step:1455/1770 train_time:143371ms step_avg:99.22ms
step:1456/1770 train_time:143475ms step_avg:99.22ms
step:1457/1770 train_time:143580ms step_avg:99.23ms
step:1458/1770 train_time:143684ms step_avg:99.23ms
step:1459/1770 train_time:143789ms step_avg:99.23ms
step:1460/1770 train_time:143893ms step_avg:99.24ms
step:1461/1770 train_time:143997ms step_avg:99.24ms
step:1462/1770 train_time:144100ms step_avg:99.24ms
step:1463/1770 train_time:144204ms step_avg:99.25ms
step:1464/1770 train_time:144310ms step_avg:99.25ms
step:1465/1770 train_time:144416ms step_avg:99.25ms
step:1466/1770 train_time:144518ms step_avg:99.26ms
step:1467/1770 train_time:144624ms step_avg:99.26ms
step:1468/1770 train_time:144728ms step_avg:99.26ms
step:1469/1770 train_time:144831ms step_avg:99.27ms
step:1470/1770 train_time:144935ms step_avg:99.27ms
step:1471/1770 train_time:145038ms step_avg:99.27ms
step:1472/1770 train_time:145142ms step_avg:99.28ms
step:1473/1770 train_time:145246ms step_avg:99.28ms
step:1474/1770 train_time:145352ms step_avg:99.28ms
step:1475/1770 train_time:145455ms step_avg:99.29ms
step:1476/1770 train_time:145558ms step_avg:99.29ms
step:1477/1770 train_time:145665ms step_avg:99.29ms
step:1478/1770 train_time:145769ms step_avg:99.30ms
step:1479/1770 train_time:145873ms step_avg:99.30ms
step:1480/1770 train_time:145976ms step_avg:99.30ms
step:1481/1770 train_time:146084ms step_avg:99.31ms
step:1482/1770 train_time:146187ms step_avg:99.31ms
step:1483/1770 train_time:146291ms step_avg:99.31ms
step:1484/1770 train_time:146395ms step_avg:99.32ms
step:1485/1770 train_time:146498ms step_avg:99.32ms
step:1486/1770 train_time:146601ms step_avg:99.32ms
step:1487/1770 train_time:146705ms step_avg:99.33ms
step:1488/1770 train_time:146809ms step_avg:99.33ms
step:1489/1770 train_time:146915ms step_avg:99.33ms
step:1490/1770 train_time:147019ms step_avg:99.34ms
step:1491/1770 train_time:147122ms step_avg:99.34ms
step:1492/1770 train_time:147226ms step_avg:99.34ms
step:1493/1770 train_time:147334ms step_avg:99.35ms
step:1494/1770 train_time:147442ms step_avg:99.35ms
step:1495/1770 train_time:147545ms step_avg:99.36ms
step:1496/1770 train_time:147649ms step_avg:99.36ms
step:1497/1770 train_time:147754ms step_avg:99.36ms
step:1498/1770 train_time:147857ms step_avg:99.37ms
step:1499/1770 train_time:147960ms step_avg:99.37ms
step:1500/1770 train_time:148063ms step_avg:99.37ms
step:1500/1770 val_loss:3.3532 train_time:148164ms step_avg:99.44ms
step:1501/1770 train_time:148185ms step_avg:99.39ms
step:1502/1770 train_time:148275ms step_avg:99.38ms
step:1503/1770 train_time:148379ms step_avg:99.38ms
step:1504/1770 train_time:148483ms step_avg:99.39ms
step:1505/1770 train_time:148589ms step_avg:99.39ms
step:1506/1770 train_time:148693ms step_avg:99.39ms
step:1507/1770 train_time:148798ms step_avg:99.40ms
step:1508/1770 train_time:148904ms step_avg:99.40ms
step:1509/1770 train_time:149008ms step_avg:99.41ms
step:1510/1770 train_time:149111ms step_avg:99.41ms
step:1511/1770 train_time:149218ms step_avg:99.41ms
step:1512/1770 train_time:149323ms step_avg:99.42ms
step:1513/1770 train_time:149427ms step_avg:99.42ms
step:1514/1770 train_time:149530ms step_avg:99.42ms
step:1515/1770 train_time:149635ms step_avg:99.43ms
step:1516/1770 train_time:149740ms step_avg:99.43ms
step:1517/1770 train_time:149843ms step_avg:99.43ms
step:1518/1770 train_time:149950ms step_avg:99.44ms
step:1519/1770 train_time:150053ms step_avg:99.44ms
step:1520/1770 train_time:150159ms step_avg:99.44ms
step:1521/1770 train_time:150262ms step_avg:99.45ms
step:1522/1770 train_time:150367ms step_avg:99.45ms
step:1523/1770 train_time:150471ms step_avg:99.45ms
step:1524/1770 train_time:150575ms step_avg:99.46ms
step:1525/1770 train_time:150678ms step_avg:99.46ms
step:1526/1770 train_time:150783ms step_avg:99.46ms
step:1527/1770 train_time:150887ms step_avg:99.46ms
step:1528/1770 train_time:150993ms step_avg:99.47ms
step:1529/1770 train_time:151096ms step_avg:99.47ms
step:1530/1770 train_time:151200ms step_avg:99.47ms
step:1531/1770 train_time:151304ms step_avg:99.48ms
step:1532/1770 train_time:151408ms step_avg:99.48ms
step:1533/1770 train_time:151513ms step_avg:99.48ms
step:1534/1770 train_time:151617ms step_avg:99.49ms
step:1535/1770 train_time:151721ms step_avg:99.49ms
step:1536/1770 train_time:151825ms step_avg:99.49ms
step:1537/1770 train_time:151929ms step_avg:99.50ms
step:1538/1770 train_time:152034ms step_avg:99.50ms
step:1539/1770 train_time:152138ms step_avg:99.50ms
step:1540/1770 train_time:152244ms step_avg:99.51ms
step:1541/1770 train_time:152348ms step_avg:99.51ms
step:1542/1770 train_time:152452ms step_avg:99.51ms
step:1543/1770 train_time:152555ms step_avg:99.51ms
step:1544/1770 train_time:152662ms step_avg:99.52ms
step:1545/1770 train_time:152767ms step_avg:99.52ms
step:1546/1770 train_time:152871ms step_avg:99.53ms
step:1547/1770 train_time:152974ms step_avg:99.53ms
step:1548/1770 train_time:153077ms step_avg:99.53ms
step:1549/1770 train_time:153182ms step_avg:99.53ms
step:1550/1770 train_time:153286ms step_avg:99.54ms
step:1551/1770 train_time:153389ms step_avg:99.54ms
step:1552/1770 train_time:153494ms step_avg:99.54ms
step:1553/1770 train_time:153598ms step_avg:99.55ms
step:1554/1770 train_time:153702ms step_avg:99.55ms
step:1555/1770 train_time:153807ms step_avg:99.55ms
step:1556/1770 train_time:153910ms step_avg:99.55ms
step:1557/1770 train_time:154013ms step_avg:99.56ms
step:1558/1770 train_time:154118ms step_avg:99.56ms
step:1559/1770 train_time:154223ms step_avg:99.56ms
step:1560/1770 train_time:154326ms step_avg:99.57ms
step:1561/1770 train_time:154432ms step_avg:99.57ms
step:1562/1770 train_time:154536ms step_avg:99.57ms
step:1563/1770 train_time:154640ms step_avg:99.57ms
step:1564/1770 train_time:154743ms step_avg:99.58ms
step:1565/1770 train_time:154848ms step_avg:99.58ms
step:1566/1770 train_time:154952ms step_avg:99.58ms
step:1567/1770 train_time:155056ms step_avg:99.59ms
step:1568/1770 train_time:155160ms step_avg:99.59ms
step:1569/1770 train_time:155267ms step_avg:99.59ms
step:1570/1770 train_time:155371ms step_avg:99.60ms
step:1571/1770 train_time:155474ms step_avg:99.60ms
step:1572/1770 train_time:155579ms step_avg:99.60ms
step:1573/1770 train_time:155684ms step_avg:99.61ms
step:1574/1770 train_time:155788ms step_avg:99.61ms
step:1575/1770 train_time:155891ms step_avg:99.61ms
step:1576/1770 train_time:155994ms step_avg:99.61ms
step:1577/1770 train_time:156100ms step_avg:99.62ms
step:1578/1770 train_time:156207ms step_avg:99.62ms
step:1579/1770 train_time:156310ms step_avg:99.62ms
step:1580/1770 train_time:156413ms step_avg:99.63ms
step:1581/1770 train_time:156519ms step_avg:99.63ms
step:1582/1770 train_time:156625ms step_avg:99.63ms
step:1583/1770 train_time:156728ms step_avg:99.64ms
step:1584/1770 train_time:156834ms step_avg:99.64ms
step:1585/1770 train_time:156938ms step_avg:99.64ms
step:1586/1770 train_time:157046ms step_avg:99.65ms
step:1587/1770 train_time:157150ms step_avg:99.65ms
step:1588/1770 train_time:157255ms step_avg:99.65ms
step:1589/1770 train_time:157360ms step_avg:99.66ms
step:1590/1770 train_time:157464ms step_avg:99.66ms
step:1591/1770 train_time:157568ms step_avg:99.66ms
step:1592/1770 train_time:157672ms step_avg:99.67ms
step:1593/1770 train_time:157776ms step_avg:99.67ms
step:1594/1770 train_time:157880ms step_avg:99.67ms
step:1595/1770 train_time:157984ms step_avg:99.67ms
step:1596/1770 train_time:158089ms step_avg:99.68ms
step:1597/1770 train_time:158192ms step_avg:99.68ms
step:1598/1770 train_time:158296ms step_avg:99.68ms
step:1599/1770 train_time:158402ms step_avg:99.69ms
step:1600/1770 train_time:158508ms step_avg:99.69ms
step:1601/1770 train_time:158612ms step_avg:99.69ms
step:1602/1770 train_time:158717ms step_avg:99.70ms
step:1603/1770 train_time:158821ms step_avg:99.70ms
step:1604/1770 train_time:158924ms step_avg:99.70ms
step:1605/1770 train_time:159028ms step_avg:99.70ms
step:1606/1770 train_time:159131ms step_avg:99.71ms
step:1607/1770 train_time:159239ms step_avg:99.71ms
step:1608/1770 train_time:159344ms step_avg:99.71ms
step:1609/1770 train_time:159447ms step_avg:99.72ms
step:1610/1770 train_time:159552ms step_avg:99.72ms
step:1611/1770 train_time:159658ms step_avg:99.72ms
step:1612/1770 train_time:159763ms step_avg:99.73ms
step:1613/1770 train_time:159867ms step_avg:99.73ms
step:1614/1770 train_time:159970ms step_avg:99.73ms
step:1615/1770 train_time:160074ms step_avg:99.73ms
step:1616/1770 train_time:160178ms step_avg:99.74ms
step:1617/1770 train_time:160284ms step_avg:99.74ms
step:1618/1770 train_time:160390ms step_avg:99.74ms
step:1619/1770 train_time:160494ms step_avg:99.75ms
step:1620/1770 train_time:160599ms step_avg:99.75ms
step:1621/1770 train_time:160703ms step_avg:99.75ms
step:1622/1770 train_time:160808ms step_avg:99.76ms
step:1623/1770 train_time:160915ms step_avg:99.76ms
step:1624/1770 train_time:161019ms step_avg:99.76ms
step:1625/1770 train_time:161123ms step_avg:99.77ms
step:1625/1770 val_loss:3.3225 train_time:161226ms step_avg:99.83ms
step:1626/1770 train_time:161247ms step_avg:99.78ms
step:1627/1770 train_time:161338ms step_avg:99.78ms
step:1628/1770 train_time:161443ms step_avg:99.78ms
step:1629/1770 train_time:161547ms step_avg:99.78ms
step:1630/1770 train_time:161650ms step_avg:99.78ms
step:1631/1770 train_time:161755ms step_avg:99.79ms
step:1632/1770 train_time:161859ms step_avg:99.79ms
step:1633/1770 train_time:161963ms step_avg:99.79ms
step:1634/1770 train_time:162067ms step_avg:99.80ms
step:1635/1770 train_time:162171ms step_avg:99.80ms
step:1636/1770 train_time:162276ms step_avg:99.80ms
step:1637/1770 train_time:162381ms step_avg:99.80ms
step:1638/1770 train_time:162484ms step_avg:99.81ms
step:1639/1770 train_time:162589ms step_avg:99.81ms
step:1640/1770 train_time:162694ms step_avg:99.81ms
step:1641/1770 train_time:162798ms step_avg:99.81ms
step:1642/1770 train_time:162901ms step_avg:99.82ms
step:1643/1770 train_time:163006ms step_avg:99.82ms
step:1644/1770 train_time:163112ms step_avg:99.82ms
step:1645/1770 train_time:163215ms step_avg:99.83ms
step:1646/1770 train_time:163322ms step_avg:99.83ms
step:1647/1770 train_time:163427ms step_avg:99.83ms
step:1648/1770 train_time:163530ms step_avg:99.84ms
step:1649/1770 train_time:163634ms step_avg:99.84ms
step:1650/1770 train_time:163738ms step_avg:99.84ms
step:1651/1770 train_time:163842ms step_avg:99.84ms
step:1652/1770 train_time:163946ms step_avg:99.85ms
step:1653/1770 train_time:164050ms step_avg:99.85ms
step:1654/1770 train_time:164158ms step_avg:99.85ms
step:1655/1770 train_time:164265ms step_avg:99.86ms
step:1656/1770 train_time:164369ms step_avg:99.86ms
step:1657/1770 train_time:164475ms step_avg:99.86ms
step:1658/1770 train_time:164579ms step_avg:99.87ms
step:1659/1770 train_time:164685ms step_avg:99.87ms
step:1660/1770 train_time:164788ms step_avg:99.87ms
step:1661/1770 train_time:164893ms step_avg:99.87ms
step:1662/1770 train_time:164998ms step_avg:99.88ms
step:1663/1770 train_time:165101ms step_avg:99.88ms
step:1664/1770 train_time:165205ms step_avg:99.88ms
step:1665/1770 train_time:165311ms step_avg:99.89ms
step:1666/1770 train_time:165413ms step_avg:99.89ms
step:1667/1770 train_time:165516ms step_avg:99.89ms
step:1668/1770 train_time:165620ms step_avg:99.89ms
step:1669/1770 train_time:165723ms step_avg:99.89ms
step:1670/1770 train_time:165828ms step_avg:99.90ms
step:1671/1770 train_time:165932ms step_avg:99.90ms
step:1672/1770 train_time:166036ms step_avg:99.90ms
step:1673/1770 train_time:166143ms step_avg:99.91ms
step:1674/1770 train_time:166246ms step_avg:99.91ms
step:1675/1770 train_time:166349ms step_avg:99.91ms
step:1676/1770 train_time:166455ms step_avg:99.91ms
step:1677/1770 train_time:166562ms step_avg:99.92ms
step:1678/1770 train_time:166665ms step_avg:99.92ms
step:1679/1770 train_time:166769ms step_avg:99.92ms
step:1680/1770 train_time:166873ms step_avg:99.92ms
step:1681/1770 train_time:166978ms step_avg:99.93ms
step:1682/1770 train_time:167083ms step_avg:99.93ms
step:1683/1770 train_time:167188ms step_avg:99.93ms
step:1684/1770 train_time:167291ms step_avg:99.93ms
step:1685/1770 train_time:167395ms step_avg:99.94ms
step:1686/1770 train_time:167500ms step_avg:99.94ms
step:1687/1770 train_time:167606ms step_avg:99.94ms
step:1688/1770 train_time:167709ms step_avg:99.95ms
step:1689/1770 train_time:167813ms step_avg:99.95ms
step:1690/1770 train_time:167917ms step_avg:99.95ms
step:1691/1770 train_time:168022ms step_avg:99.95ms
step:1692/1770 train_time:168127ms step_avg:99.96ms
step:1693/1770 train_time:168232ms step_avg:99.96ms
step:1694/1770 train_time:168335ms step_avg:99.96ms
step:1695/1770 train_time:168440ms step_avg:99.96ms
step:1696/1770 train_time:168546ms step_avg:99.97ms
step:1697/1770 train_time:168651ms step_avg:99.97ms
step:1698/1770 train_time:168756ms step_avg:99.97ms
step:1699/1770 train_time:168860ms step_avg:99.98ms
step:1700/1770 train_time:168964ms step_avg:99.98ms
step:1701/1770 train_time:169068ms step_avg:99.98ms
step:1702/1770 train_time:169173ms step_avg:99.98ms
step:1703/1770 train_time:169276ms step_avg:99.99ms
step:1704/1770 train_time:169380ms step_avg:99.99ms
step:1705/1770 train_time:169484ms step_avg:99.99ms
step:1706/1770 train_time:169587ms step_avg:99.99ms
step:1707/1770 train_time:169692ms step_avg:100.00ms
step:1708/1770 train_time:169797ms step_avg:100.00ms
step:1709/1770 train_time:169903ms step_avg:100.00ms
step:1710/1770 train_time:170011ms step_avg:100.01ms
step:1711/1770 train_time:170118ms step_avg:100.01ms
step:1712/1770 train_time:170223ms step_avg:100.01ms
step:1713/1770 train_time:170327ms step_avg:100.02ms
step:1714/1770 train_time:170431ms step_avg:100.02ms
step:1715/1770 train_time:170535ms step_avg:100.02ms
step:1716/1770 train_time:170640ms step_avg:100.02ms
step:1717/1770 train_time:170745ms step_avg:100.03ms
step:1718/1770 train_time:170851ms step_avg:100.03ms
step:1719/1770 train_time:170957ms step_avg:100.03ms
step:1720/1770 train_time:171063ms step_avg:100.04ms
step:1721/1770 train_time:171167ms step_avg:100.04ms
step:1722/1770 train_time:171275ms step_avg:100.04ms
step:1723/1770 train_time:171382ms step_avg:100.05ms
step:1724/1770 train_time:171489ms step_avg:100.05ms
step:1725/1770 train_time:171596ms step_avg:100.06ms
step:1726/1770 train_time:171702ms step_avg:100.06ms
step:1727/1770 train_time:171807ms step_avg:100.06ms
step:1728/1770 train_time:171913ms step_avg:100.07ms
step:1729/1770 train_time:172018ms step_avg:100.07ms
step:1730/1770 train_time:172124ms step_avg:100.07ms
step:1731/1770 train_time:172231ms step_avg:100.08ms
step:1732/1770 train_time:172334ms step_avg:100.08ms
step:1733/1770 train_time:172442ms step_avg:100.08ms
step:1734/1770 train_time:172546ms step_avg:100.08ms
step:1735/1770 train_time:172652ms step_avg:100.09ms
step:1736/1770 train_time:172756ms step_avg:100.09ms
step:1737/1770 train_time:172862ms step_avg:100.09ms
step:1738/1770 train_time:172966ms step_avg:100.10ms
step:1739/1770 train_time:173071ms step_avg:100.10ms
step:1740/1770 train_time:173175ms step_avg:100.10ms
step:1741/1770 train_time:173283ms step_avg:100.11ms
step:1742/1770 train_time:173391ms step_avg:100.11ms
step:1743/1770 train_time:173497ms step_avg:100.11ms
step:1744/1770 train_time:173601ms step_avg:100.12ms
step:1745/1770 train_time:173707ms step_avg:100.12ms
step:1746/1770 train_time:173814ms step_avg:100.12ms
step:1747/1770 train_time:173918ms step_avg:100.13ms
step:1748/1770 train_time:174025ms step_avg:100.13ms
step:1749/1770 train_time:174131ms step_avg:100.13ms
step:1750/1770 train_time:174235ms step_avg:100.13ms
step:1750/1770 val_loss:3.2970 train_time:174339ms step_avg:100.19ms
step:1751/1770 train_time:174359ms step_avg:100.15ms
step:1752/1770 train_time:174449ms step_avg:100.14ms
step:1753/1770 train_time:174554ms step_avg:100.15ms
step:1754/1770 train_time:174659ms step_avg:100.15ms
step:1755/1770 train_time:174764ms step_avg:100.15ms
step:1756/1770 train_time:174870ms step_avg:100.15ms
step:1757/1770 train_time:174976ms step_avg:100.16ms
step:1758/1770 train_time:175080ms step_avg:100.16ms
step:1759/1770 train_time:175186ms step_avg:100.16ms
step:1760/1770 train_time:175292ms step_avg:100.17ms
step:1761/1770 train_time:175399ms step_avg:100.17ms
step:1762/1770 train_time:175508ms step_avg:100.18ms
step:1763/1770 train_time:175612ms step_avg:100.18ms
step:1764/1770 train_time:175718ms step_avg:100.18ms
step:1765/1770 train_time:175823ms step_avg:100.18ms
step:1766/1770 train_time:175933ms step_avg:100.19ms
step:1767/1770 train_time:176037ms step_avg:100.19ms
step:1768/1770 train_time:176142ms step_avg:100.19ms
step:1769/1770 train_time:176245ms step_avg:100.20ms
step:1770/1770 train_time:176350ms step_avg:100.20ms
step:1770/1770 val_loss:3.2934 train_time:176455ms step_avg:100.26ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
