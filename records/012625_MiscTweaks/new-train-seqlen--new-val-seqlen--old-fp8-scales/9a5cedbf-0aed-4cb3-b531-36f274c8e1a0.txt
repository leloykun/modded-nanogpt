import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 03:37:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:1ms step_avg:nanms
step:1/1770 train_time:23254ms step_avg:nanms
step:2/1770 train_time:23687ms step_avg:nanms
step:3/1770 train_time:23782ms step_avg:nanms
step:4/1770 train_time:23876ms step_avg:nanms
step:5/1770 train_time:23970ms step_avg:nanms
step:6/1770 train_time:24064ms step_avg:nanms
step:7/1770 train_time:24159ms step_avg:nanms
step:8/1770 train_time:24253ms step_avg:nanms
step:9/1770 train_time:24347ms step_avg:nanms
step:10/1770 train_time:24442ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.39ms
step:14/1770 train_time:377ms step_avg:94.33ms
step:15/1770 train_time:472ms step_avg:94.41ms
step:16/1770 train_time:567ms step_avg:94.48ms
step:17/1770 train_time:661ms step_avg:94.42ms
step:18/1770 train_time:756ms step_avg:94.50ms
step:19/1770 train_time:851ms step_avg:94.53ms
step:20/1770 train_time:945ms step_avg:94.54ms
step:21/1770 train_time:1040ms step_avg:94.59ms
step:22/1770 train_time:1135ms step_avg:94.56ms
step:23/1770 train_time:1230ms step_avg:94.59ms
step:24/1770 train_time:1324ms step_avg:94.56ms
step:25/1770 train_time:1418ms step_avg:94.55ms
step:26/1770 train_time:1513ms step_avg:94.58ms
step:27/1770 train_time:1608ms step_avg:94.60ms
step:28/1770 train_time:1703ms step_avg:94.60ms
step:29/1770 train_time:1798ms step_avg:94.61ms
step:30/1770 train_time:1892ms step_avg:94.62ms
step:31/1770 train_time:1987ms step_avg:94.62ms
step:32/1770 train_time:2082ms step_avg:94.66ms
step:33/1770 train_time:2176ms step_avg:94.61ms
step:34/1770 train_time:2271ms step_avg:94.63ms
step:35/1770 train_time:2366ms step_avg:94.64ms
step:36/1770 train_time:2461ms step_avg:94.64ms
step:37/1770 train_time:2556ms step_avg:94.66ms
step:38/1770 train_time:2651ms step_avg:94.67ms
step:39/1770 train_time:2745ms step_avg:94.67ms
step:40/1770 train_time:2840ms step_avg:94.66ms
step:41/1770 train_time:2935ms step_avg:94.67ms
step:42/1770 train_time:3030ms step_avg:94.68ms
step:43/1770 train_time:3124ms step_avg:94.68ms
step:44/1770 train_time:3219ms step_avg:94.66ms
step:45/1770 train_time:3313ms step_avg:94.66ms
step:46/1770 train_time:3408ms step_avg:94.66ms
step:47/1770 train_time:3502ms step_avg:94.66ms
step:48/1770 train_time:3597ms step_avg:94.66ms
step:49/1770 train_time:3692ms step_avg:94.66ms
step:50/1770 train_time:3786ms step_avg:94.64ms
step:51/1770 train_time:3880ms step_avg:94.63ms
step:52/1770 train_time:3975ms step_avg:94.65ms
step:53/1770 train_time:4070ms step_avg:94.66ms
step:54/1770 train_time:4165ms step_avg:94.65ms
step:55/1770 train_time:4260ms step_avg:94.66ms
step:56/1770 train_time:4354ms step_avg:94.66ms
step:57/1770 train_time:4449ms step_avg:94.66ms
step:58/1770 train_time:4544ms step_avg:94.66ms
step:59/1770 train_time:4638ms step_avg:94.65ms
step:60/1770 train_time:4733ms step_avg:94.65ms
step:61/1770 train_time:4827ms step_avg:94.65ms
step:62/1770 train_time:4921ms step_avg:94.64ms
step:63/1770 train_time:5016ms step_avg:94.65ms
step:64/1770 train_time:5111ms step_avg:94.64ms
step:65/1770 train_time:5206ms step_avg:94.65ms
step:66/1770 train_time:5300ms step_avg:94.65ms
step:67/1770 train_time:5395ms step_avg:94.65ms
step:68/1770 train_time:5490ms step_avg:94.65ms
step:69/1770 train_time:5585ms step_avg:94.65ms
step:70/1770 train_time:5679ms step_avg:94.65ms
step:71/1770 train_time:5774ms step_avg:94.65ms
step:72/1770 train_time:5868ms step_avg:94.65ms
step:73/1770 train_time:5963ms step_avg:94.65ms
step:74/1770 train_time:6057ms step_avg:94.64ms
step:75/1770 train_time:6153ms step_avg:94.67ms
step:76/1770 train_time:6247ms step_avg:94.65ms
step:77/1770 train_time:6342ms step_avg:94.65ms
step:78/1770 train_time:6436ms step_avg:94.65ms
step:79/1770 train_time:6531ms step_avg:94.66ms
step:80/1770 train_time:6627ms step_avg:94.67ms
step:81/1770 train_time:6721ms step_avg:94.66ms
step:82/1770 train_time:6816ms step_avg:94.66ms
step:83/1770 train_time:6911ms step_avg:94.67ms
step:84/1770 train_time:7005ms step_avg:94.66ms
step:85/1770 train_time:7099ms step_avg:94.66ms
step:86/1770 train_time:7195ms step_avg:94.67ms
step:87/1770 train_time:7290ms step_avg:94.68ms
step:88/1770 train_time:7384ms step_avg:94.67ms
step:89/1770 train_time:7478ms step_avg:94.66ms
step:90/1770 train_time:7573ms step_avg:94.66ms
step:91/1770 train_time:7669ms step_avg:94.68ms
step:92/1770 train_time:7763ms step_avg:94.67ms
step:93/1770 train_time:7858ms step_avg:94.67ms
step:94/1770 train_time:7952ms step_avg:94.67ms
step:95/1770 train_time:8047ms step_avg:94.66ms
step:96/1770 train_time:8141ms step_avg:94.66ms
step:97/1770 train_time:8236ms step_avg:94.66ms
step:98/1770 train_time:8330ms step_avg:94.66ms
step:99/1770 train_time:8425ms step_avg:94.66ms
step:100/1770 train_time:8519ms step_avg:94.66ms
step:101/1770 train_time:8614ms step_avg:94.66ms
step:102/1770 train_time:8710ms step_avg:94.67ms
step:103/1770 train_time:8804ms step_avg:94.67ms
step:104/1770 train_time:8899ms step_avg:94.67ms
step:105/1770 train_time:8994ms step_avg:94.67ms
step:106/1770 train_time:9088ms step_avg:94.67ms
step:107/1770 train_time:9183ms step_avg:94.67ms
step:108/1770 train_time:9278ms step_avg:94.67ms
step:109/1770 train_time:9373ms step_avg:94.67ms
step:110/1770 train_time:9468ms step_avg:94.68ms
step:111/1770 train_time:9562ms step_avg:94.68ms
step:112/1770 train_time:9657ms step_avg:94.68ms
step:113/1770 train_time:9753ms step_avg:94.69ms
step:114/1770 train_time:9847ms step_avg:94.68ms
step:115/1770 train_time:9941ms step_avg:94.68ms
step:116/1770 train_time:10036ms step_avg:94.68ms
step:117/1770 train_time:10131ms step_avg:94.68ms
step:118/1770 train_time:10226ms step_avg:94.68ms
step:119/1770 train_time:10320ms step_avg:94.68ms
step:120/1770 train_time:10415ms step_avg:94.68ms
step:121/1770 train_time:10510ms step_avg:94.68ms
step:122/1770 train_time:10604ms step_avg:94.68ms
step:123/1770 train_time:10699ms step_avg:94.68ms
step:124/1770 train_time:10795ms step_avg:94.69ms
step:125/1770 train_time:10889ms step_avg:94.69ms
step:125/1770 val_loss:4.6507 train_time:10982ms step_avg:95.49ms
step:126/1770 train_time:11006ms step_avg:94.88ms
step:127/1770 train_time:11080ms step_avg:94.70ms
step:128/1770 train_time:11175ms step_avg:94.70ms
step:129/1770 train_time:11275ms step_avg:94.74ms
step:130/1770 train_time:11371ms step_avg:94.76ms
step:131/1770 train_time:11466ms step_avg:94.76ms
step:132/1770 train_time:11561ms step_avg:94.76ms
step:133/1770 train_time:11656ms step_avg:94.76ms
step:134/1770 train_time:11751ms step_avg:94.76ms
step:135/1770 train_time:11846ms step_avg:94.77ms
step:136/1770 train_time:11941ms step_avg:94.77ms
step:137/1770 train_time:12036ms step_avg:94.77ms
step:138/1770 train_time:12131ms step_avg:94.77ms
step:139/1770 train_time:12226ms step_avg:94.78ms
step:140/1770 train_time:12322ms step_avg:94.79ms
step:141/1770 train_time:12417ms step_avg:94.79ms
step:142/1770 train_time:12512ms step_avg:94.79ms
step:143/1770 train_time:12608ms step_avg:94.80ms
step:144/1770 train_time:12703ms step_avg:94.80ms
step:145/1770 train_time:12798ms step_avg:94.80ms
step:146/1770 train_time:12893ms step_avg:94.80ms
step:147/1770 train_time:12989ms step_avg:94.81ms
step:148/1770 train_time:13084ms step_avg:94.81ms
step:149/1770 train_time:13179ms step_avg:94.81ms
step:150/1770 train_time:13274ms step_avg:94.81ms
step:151/1770 train_time:13369ms step_avg:94.81ms
step:152/1770 train_time:13464ms step_avg:94.82ms
step:153/1770 train_time:13559ms step_avg:94.82ms
step:154/1770 train_time:13654ms step_avg:94.82ms
step:155/1770 train_time:13749ms step_avg:94.82ms
step:156/1770 train_time:13844ms step_avg:94.82ms
step:157/1770 train_time:13940ms step_avg:94.83ms
step:158/1770 train_time:14035ms step_avg:94.83ms
step:159/1770 train_time:14130ms step_avg:94.83ms
step:160/1770 train_time:14227ms step_avg:94.84ms
step:161/1770 train_time:14320ms step_avg:94.84ms
step:162/1770 train_time:14416ms step_avg:94.84ms
step:163/1770 train_time:14511ms step_avg:94.84ms
step:164/1770 train_time:14606ms step_avg:94.84ms
step:165/1770 train_time:14701ms step_avg:94.85ms
step:166/1770 train_time:14796ms step_avg:94.85ms
step:167/1770 train_time:14891ms step_avg:94.85ms
step:168/1770 train_time:14987ms step_avg:94.85ms
step:169/1770 train_time:15082ms step_avg:94.86ms
step:170/1770 train_time:15177ms step_avg:94.86ms
step:171/1770 train_time:15271ms step_avg:94.85ms
step:172/1770 train_time:15367ms step_avg:94.86ms
step:173/1770 train_time:15463ms step_avg:94.86ms
step:174/1770 train_time:15557ms step_avg:94.86ms
step:175/1770 train_time:15652ms step_avg:94.86ms
step:176/1770 train_time:15747ms step_avg:94.86ms
step:177/1770 train_time:15843ms step_avg:94.87ms
step:178/1770 train_time:15938ms step_avg:94.87ms
step:179/1770 train_time:16033ms step_avg:94.87ms
step:180/1770 train_time:16128ms step_avg:94.87ms
step:181/1770 train_time:16224ms step_avg:94.88ms
step:182/1770 train_time:16318ms step_avg:94.87ms
step:183/1770 train_time:16413ms step_avg:94.87ms
step:184/1770 train_time:16508ms step_avg:94.88ms
step:185/1770 train_time:16606ms step_avg:94.89ms
step:186/1770 train_time:16702ms step_avg:94.90ms
step:187/1770 train_time:16796ms step_avg:94.89ms
step:188/1770 train_time:16891ms step_avg:94.89ms
step:189/1770 train_time:16987ms step_avg:94.90ms
step:190/1770 train_time:17082ms step_avg:94.90ms
step:191/1770 train_time:17177ms step_avg:94.90ms
step:192/1770 train_time:17272ms step_avg:94.90ms
step:193/1770 train_time:17367ms step_avg:94.90ms
step:194/1770 train_time:17462ms step_avg:94.90ms
step:195/1770 train_time:17558ms step_avg:94.91ms
step:196/1770 train_time:17653ms step_avg:94.91ms
step:197/1770 train_time:17748ms step_avg:94.91ms
step:198/1770 train_time:17843ms step_avg:94.91ms
step:199/1770 train_time:17938ms step_avg:94.91ms
step:200/1770 train_time:18033ms step_avg:94.91ms
step:201/1770 train_time:18128ms step_avg:94.91ms
step:202/1770 train_time:18223ms step_avg:94.91ms
step:203/1770 train_time:18318ms step_avg:94.91ms
step:204/1770 train_time:18413ms step_avg:94.91ms
step:205/1770 train_time:18508ms step_avg:94.91ms
step:206/1770 train_time:18604ms step_avg:94.92ms
step:207/1770 train_time:18699ms step_avg:94.92ms
step:208/1770 train_time:18794ms step_avg:94.92ms
step:209/1770 train_time:18889ms step_avg:94.92ms
step:210/1770 train_time:18985ms step_avg:94.92ms
step:211/1770 train_time:19081ms step_avg:94.93ms
step:212/1770 train_time:19176ms step_avg:94.93ms
step:213/1770 train_time:19270ms step_avg:94.93ms
step:214/1770 train_time:19366ms step_avg:94.93ms
step:215/1770 train_time:19461ms step_avg:94.93ms
step:216/1770 train_time:19556ms step_avg:94.93ms
step:217/1770 train_time:19651ms step_avg:94.93ms
step:218/1770 train_time:19746ms step_avg:94.93ms
step:219/1770 train_time:19841ms step_avg:94.93ms
step:220/1770 train_time:19936ms step_avg:94.93ms
step:221/1770 train_time:20031ms step_avg:94.93ms
step:222/1770 train_time:20127ms step_avg:94.94ms
step:223/1770 train_time:20222ms step_avg:94.94ms
step:224/1770 train_time:20317ms step_avg:94.94ms
step:225/1770 train_time:20410ms step_avg:94.93ms
step:226/1770 train_time:20506ms step_avg:94.93ms
step:227/1770 train_time:20602ms step_avg:94.94ms
step:228/1770 train_time:20697ms step_avg:94.94ms
step:229/1770 train_time:20791ms step_avg:94.94ms
step:230/1770 train_time:20887ms step_avg:94.94ms
step:231/1770 train_time:20983ms step_avg:94.95ms
step:232/1770 train_time:21078ms step_avg:94.95ms
step:233/1770 train_time:21173ms step_avg:94.95ms
step:234/1770 train_time:21269ms step_avg:94.95ms
step:235/1770 train_time:21364ms step_avg:94.95ms
step:236/1770 train_time:21459ms step_avg:94.95ms
step:237/1770 train_time:21554ms step_avg:94.95ms
step:238/1770 train_time:21649ms step_avg:94.95ms
step:239/1770 train_time:21744ms step_avg:94.95ms
step:240/1770 train_time:21839ms step_avg:94.95ms
step:241/1770 train_time:21934ms step_avg:94.95ms
step:242/1770 train_time:22029ms step_avg:94.95ms
step:243/1770 train_time:22124ms step_avg:94.95ms
step:244/1770 train_time:22219ms step_avg:94.95ms
step:245/1770 train_time:22314ms step_avg:94.95ms
step:246/1770 train_time:22409ms step_avg:94.95ms
step:247/1770 train_time:22505ms step_avg:94.96ms
step:248/1770 train_time:22601ms step_avg:94.96ms
step:249/1770 train_time:22697ms step_avg:94.97ms
step:250/1770 train_time:22791ms step_avg:94.96ms
step:250/1770 val_loss:4.1120 train_time:22885ms step_avg:95.35ms
step:251/1770 train_time:22906ms step_avg:95.05ms
step:252/1770 train_time:22996ms step_avg:95.02ms
step:253/1770 train_time:23095ms step_avg:95.04ms
step:254/1770 train_time:23190ms step_avg:95.04ms
step:255/1770 train_time:23286ms step_avg:95.04ms
step:256/1770 train_time:23381ms step_avg:95.04ms
step:257/1770 train_time:23475ms step_avg:95.04ms
step:258/1770 train_time:23571ms step_avg:95.04ms
step:259/1770 train_time:23666ms step_avg:95.04ms
step:260/1770 train_time:23761ms step_avg:95.04ms
step:261/1770 train_time:23856ms step_avg:95.04ms
step:262/1770 train_time:23951ms step_avg:95.04ms
step:263/1770 train_time:24046ms step_avg:95.05ms
step:264/1770 train_time:24142ms step_avg:95.05ms
step:265/1770 train_time:24237ms step_avg:95.05ms
step:266/1770 train_time:24333ms step_avg:95.05ms
step:267/1770 train_time:24429ms step_avg:95.05ms
step:268/1770 train_time:24525ms step_avg:95.06ms
step:269/1770 train_time:24620ms step_avg:95.06ms
step:270/1770 train_time:24715ms step_avg:95.06ms
step:271/1770 train_time:24811ms step_avg:95.06ms
step:272/1770 train_time:24906ms step_avg:95.06ms
step:273/1770 train_time:25002ms step_avg:95.06ms
step:274/1770 train_time:25097ms step_avg:95.06ms
step:275/1770 train_time:25192ms step_avg:95.06ms
step:276/1770 train_time:25288ms step_avg:95.07ms
step:277/1770 train_time:25385ms step_avg:95.07ms
step:278/1770 train_time:25480ms step_avg:95.08ms
step:279/1770 train_time:25575ms step_avg:95.08ms
step:280/1770 train_time:25671ms step_avg:95.08ms
step:281/1770 train_time:25768ms step_avg:95.08ms
step:282/1770 train_time:25863ms step_avg:95.08ms
step:283/1770 train_time:25958ms step_avg:95.09ms
step:284/1770 train_time:26054ms step_avg:95.09ms
step:285/1770 train_time:26149ms step_avg:95.09ms
step:286/1770 train_time:26245ms step_avg:95.09ms
step:287/1770 train_time:26341ms step_avg:95.09ms
step:288/1770 train_time:26436ms step_avg:95.09ms
step:289/1770 train_time:26532ms step_avg:95.10ms
step:290/1770 train_time:26628ms step_avg:95.10ms
step:291/1770 train_time:26724ms step_avg:95.10ms
step:292/1770 train_time:26819ms step_avg:95.10ms
step:293/1770 train_time:26914ms step_avg:95.10ms
step:294/1770 train_time:27010ms step_avg:95.10ms
step:295/1770 train_time:27105ms step_avg:95.11ms
step:296/1770 train_time:27201ms step_avg:95.11ms
step:297/1770 train_time:27296ms step_avg:95.11ms
step:298/1770 train_time:27392ms step_avg:95.11ms
step:299/1770 train_time:27488ms step_avg:95.11ms
step:300/1770 train_time:27583ms step_avg:95.11ms
step:301/1770 train_time:27678ms step_avg:95.11ms
step:302/1770 train_time:27773ms step_avg:95.11ms
step:303/1770 train_time:27869ms step_avg:95.12ms
step:304/1770 train_time:27965ms step_avg:95.12ms
step:305/1770 train_time:28061ms step_avg:95.12ms
step:306/1770 train_time:28156ms step_avg:95.12ms
step:307/1770 train_time:28252ms step_avg:95.12ms
step:308/1770 train_time:28348ms step_avg:95.13ms
step:309/1770 train_time:28443ms step_avg:95.13ms
step:310/1770 train_time:28539ms step_avg:95.13ms
step:311/1770 train_time:28634ms step_avg:95.13ms
step:312/1770 train_time:28730ms step_avg:95.13ms
step:313/1770 train_time:28826ms step_avg:95.13ms
step:314/1770 train_time:28921ms step_avg:95.14ms
step:315/1770 train_time:29016ms step_avg:95.14ms
step:316/1770 train_time:29112ms step_avg:95.14ms
step:317/1770 train_time:29208ms step_avg:95.14ms
step:318/1770 train_time:29304ms step_avg:95.14ms
step:319/1770 train_time:29399ms step_avg:95.14ms
step:320/1770 train_time:29494ms step_avg:95.14ms
step:321/1770 train_time:29591ms step_avg:95.15ms
step:322/1770 train_time:29687ms step_avg:95.15ms
step:323/1770 train_time:29782ms step_avg:95.15ms
step:324/1770 train_time:29878ms step_avg:95.15ms
step:325/1770 train_time:29973ms step_avg:95.15ms
step:326/1770 train_time:30069ms step_avg:95.15ms
step:327/1770 train_time:30165ms step_avg:95.16ms
step:328/1770 train_time:30261ms step_avg:95.16ms
step:329/1770 train_time:30357ms step_avg:95.16ms
step:330/1770 train_time:30453ms step_avg:95.17ms
step:331/1770 train_time:30549ms step_avg:95.17ms
step:332/1770 train_time:30646ms step_avg:95.17ms
step:333/1770 train_time:30741ms step_avg:95.17ms
step:334/1770 train_time:30837ms step_avg:95.17ms
step:335/1770 train_time:30932ms step_avg:95.18ms
step:336/1770 train_time:31028ms step_avg:95.18ms
step:337/1770 train_time:31123ms step_avg:95.18ms
step:338/1770 train_time:31219ms step_avg:95.18ms
step:339/1770 train_time:31314ms step_avg:95.18ms
step:340/1770 train_time:31410ms step_avg:95.18ms
step:341/1770 train_time:31506ms step_avg:95.19ms
step:342/1770 train_time:31603ms step_avg:95.19ms
step:343/1770 train_time:31698ms step_avg:95.19ms
step:344/1770 train_time:31794ms step_avg:95.19ms
step:345/1770 train_time:31890ms step_avg:95.19ms
step:346/1770 train_time:31986ms step_avg:95.20ms
step:347/1770 train_time:32081ms step_avg:95.20ms
step:348/1770 train_time:32176ms step_avg:95.20ms
step:349/1770 train_time:32272ms step_avg:95.20ms
step:350/1770 train_time:32368ms step_avg:95.20ms
step:351/1770 train_time:32463ms step_avg:95.20ms
step:352/1770 train_time:32559ms step_avg:95.20ms
step:353/1770 train_time:32654ms step_avg:95.20ms
step:354/1770 train_time:32750ms step_avg:95.20ms
step:355/1770 train_time:32846ms step_avg:95.21ms
step:356/1770 train_time:32942ms step_avg:95.21ms
step:357/1770 train_time:33038ms step_avg:95.21ms
step:358/1770 train_time:33133ms step_avg:95.21ms
step:359/1770 train_time:33229ms step_avg:95.21ms
step:360/1770 train_time:33326ms step_avg:95.22ms
step:361/1770 train_time:33420ms step_avg:95.21ms
step:362/1770 train_time:33515ms step_avg:95.21ms
step:363/1770 train_time:33611ms step_avg:95.22ms
step:364/1770 train_time:33707ms step_avg:95.22ms
step:365/1770 train_time:33802ms step_avg:95.22ms
step:366/1770 train_time:33898ms step_avg:95.22ms
step:367/1770 train_time:33993ms step_avg:95.22ms
step:368/1770 train_time:34090ms step_avg:95.22ms
step:369/1770 train_time:34186ms step_avg:95.22ms
step:370/1770 train_time:34281ms step_avg:95.22ms
step:371/1770 train_time:34376ms step_avg:95.22ms
step:372/1770 train_time:34472ms step_avg:95.23ms
step:373/1770 train_time:34568ms step_avg:95.23ms
step:374/1770 train_time:34665ms step_avg:95.23ms
step:375/1770 train_time:34760ms step_avg:95.23ms
step:375/1770 val_loss:3.9033 train_time:34854ms step_avg:95.49ms
step:376/1770 train_time:34875ms step_avg:95.29ms
step:377/1770 train_time:34958ms step_avg:95.25ms
step:378/1770 train_time:35056ms step_avg:95.26ms
step:379/1770 train_time:35152ms step_avg:95.26ms
step:380/1770 train_time:35247ms step_avg:95.26ms
step:381/1770 train_time:35343ms step_avg:95.26ms
step:382/1770 train_time:35438ms step_avg:95.26ms
step:383/1770 train_time:35533ms step_avg:95.26ms
step:384/1770 train_time:35629ms step_avg:95.26ms
step:385/1770 train_time:35725ms step_avg:95.27ms
step:386/1770 train_time:35820ms step_avg:95.27ms
step:387/1770 train_time:35915ms step_avg:95.27ms
step:388/1770 train_time:36011ms step_avg:95.27ms
step:389/1770 train_time:36106ms step_avg:95.27ms
step:390/1770 train_time:36202ms step_avg:95.27ms
step:391/1770 train_time:36297ms step_avg:95.27ms
step:392/1770 train_time:36392ms step_avg:95.27ms
step:393/1770 train_time:36488ms step_avg:95.27ms
step:394/1770 train_time:36584ms step_avg:95.27ms
step:395/1770 train_time:36679ms step_avg:95.27ms
step:396/1770 train_time:36777ms step_avg:95.28ms
step:397/1770 train_time:36874ms step_avg:95.28ms
step:398/1770 train_time:36971ms step_avg:95.29ms
step:399/1770 train_time:37069ms step_avg:95.29ms
step:400/1770 train_time:37167ms step_avg:95.30ms
step:401/1770 train_time:37265ms step_avg:95.31ms
step:402/1770 train_time:37363ms step_avg:95.31ms
step:403/1770 train_time:37460ms step_avg:95.32ms
step:404/1770 train_time:37558ms step_avg:95.32ms
step:405/1770 train_time:37656ms step_avg:95.33ms
step:406/1770 train_time:37752ms step_avg:95.33ms
step:407/1770 train_time:37850ms step_avg:95.34ms
step:408/1770 train_time:37948ms step_avg:95.35ms
step:409/1770 train_time:38045ms step_avg:95.35ms
step:410/1770 train_time:38143ms step_avg:95.36ms
step:411/1770 train_time:38241ms step_avg:95.36ms
step:412/1770 train_time:38338ms step_avg:95.37ms
step:413/1770 train_time:38435ms step_avg:95.37ms
step:414/1770 train_time:38532ms step_avg:95.38ms
step:415/1770 train_time:38629ms step_avg:95.38ms
step:416/1770 train_time:38727ms step_avg:95.39ms
step:417/1770 train_time:38825ms step_avg:95.39ms
step:418/1770 train_time:38923ms step_avg:95.40ms
step:419/1770 train_time:39021ms step_avg:95.40ms
step:420/1770 train_time:39118ms step_avg:95.41ms
step:421/1770 train_time:39215ms step_avg:95.41ms
step:422/1770 train_time:39312ms step_avg:95.42ms
step:423/1770 train_time:39410ms step_avg:95.42ms
step:424/1770 train_time:39507ms step_avg:95.43ms
step:425/1770 train_time:39605ms step_avg:95.43ms
step:426/1770 train_time:39702ms step_avg:95.44ms
step:427/1770 train_time:39799ms step_avg:95.44ms
step:428/1770 train_time:39897ms step_avg:95.45ms
step:429/1770 train_time:39994ms step_avg:95.45ms
step:430/1770 train_time:40091ms step_avg:95.46ms
step:431/1770 train_time:40189ms step_avg:95.46ms
step:432/1770 train_time:40287ms step_avg:95.47ms
step:433/1770 train_time:40385ms step_avg:95.47ms
step:434/1770 train_time:40483ms step_avg:95.48ms
step:435/1770 train_time:40581ms step_avg:95.48ms
step:436/1770 train_time:40678ms step_avg:95.49ms
step:437/1770 train_time:40775ms step_avg:95.49ms
step:438/1770 train_time:40872ms step_avg:95.50ms
step:439/1770 train_time:40970ms step_avg:95.50ms
step:440/1770 train_time:41067ms step_avg:95.51ms
step:441/1770 train_time:41165ms step_avg:95.51ms
step:442/1770 train_time:41263ms step_avg:95.52ms
step:443/1770 train_time:41361ms step_avg:95.52ms
step:444/1770 train_time:41459ms step_avg:95.53ms
step:445/1770 train_time:41556ms step_avg:95.53ms
step:446/1770 train_time:41653ms step_avg:95.53ms
step:447/1770 train_time:41750ms step_avg:95.54ms
step:448/1770 train_time:41848ms step_avg:95.54ms
step:449/1770 train_time:41945ms step_avg:95.55ms
step:450/1770 train_time:42043ms step_avg:95.55ms
step:451/1770 train_time:42141ms step_avg:95.56ms
step:452/1770 train_time:42238ms step_avg:95.56ms
step:453/1770 train_time:42335ms step_avg:95.56ms
step:454/1770 train_time:42432ms step_avg:95.57ms
step:455/1770 train_time:42530ms step_avg:95.57ms
step:456/1770 train_time:42628ms step_avg:95.58ms
step:457/1770 train_time:42725ms step_avg:95.58ms
step:458/1770 train_time:42823ms step_avg:95.59ms
step:459/1770 train_time:42921ms step_avg:95.59ms
step:460/1770 train_time:43018ms step_avg:95.60ms
step:461/1770 train_time:43115ms step_avg:95.60ms
step:462/1770 train_time:43212ms step_avg:95.60ms
step:463/1770 train_time:43309ms step_avg:95.61ms
step:464/1770 train_time:43407ms step_avg:95.61ms
step:465/1770 train_time:43505ms step_avg:95.62ms
step:466/1770 train_time:43604ms step_avg:95.62ms
step:467/1770 train_time:43701ms step_avg:95.62ms
step:468/1770 train_time:43798ms step_avg:95.63ms
step:469/1770 train_time:43895ms step_avg:95.63ms
step:470/1770 train_time:43992ms step_avg:95.63ms
step:471/1770 train_time:44089ms step_avg:95.64ms
step:472/1770 train_time:44187ms step_avg:95.64ms
step:473/1770 train_time:44285ms step_avg:95.65ms
step:474/1770 train_time:44382ms step_avg:95.65ms
step:475/1770 train_time:44480ms step_avg:95.65ms
step:476/1770 train_time:44577ms step_avg:95.66ms
step:477/1770 train_time:44673ms step_avg:95.66ms
step:478/1770 train_time:44771ms step_avg:95.66ms
step:479/1770 train_time:44868ms step_avg:95.67ms
step:480/1770 train_time:44966ms step_avg:95.67ms
step:481/1770 train_time:45064ms step_avg:95.68ms
step:482/1770 train_time:45161ms step_avg:95.68ms
step:483/1770 train_time:45259ms step_avg:95.68ms
step:484/1770 train_time:45356ms step_avg:95.69ms
step:485/1770 train_time:45453ms step_avg:95.69ms
step:486/1770 train_time:45550ms step_avg:95.69ms
step:487/1770 train_time:45648ms step_avg:95.70ms
step:488/1770 train_time:45746ms step_avg:95.70ms
step:489/1770 train_time:45843ms step_avg:95.71ms
step:490/1770 train_time:45941ms step_avg:95.71ms
step:491/1770 train_time:46038ms step_avg:95.71ms
step:492/1770 train_time:46135ms step_avg:95.72ms
step:493/1770 train_time:46232ms step_avg:95.72ms
step:494/1770 train_time:46330ms step_avg:95.72ms
step:495/1770 train_time:46428ms step_avg:95.73ms
step:496/1770 train_time:46525ms step_avg:95.73ms
step:497/1770 train_time:46624ms step_avg:95.74ms
step:498/1770 train_time:46722ms step_avg:95.74ms
step:499/1770 train_time:46819ms step_avg:95.74ms
step:500/1770 train_time:46916ms step_avg:95.75ms
step:500/1770 val_loss:3.7529 train_time:47011ms step_avg:95.94ms
step:501/1770 train_time:47034ms step_avg:95.79ms
step:502/1770 train_time:47120ms step_avg:95.77ms
step:503/1770 train_time:47220ms step_avg:95.78ms
step:504/1770 train_time:47318ms step_avg:95.78ms
step:505/1770 train_time:47415ms step_avg:95.79ms
step:506/1770 train_time:47512ms step_avg:95.79ms
step:507/1770 train_time:47610ms step_avg:95.80ms
step:508/1770 train_time:47708ms step_avg:95.80ms
step:509/1770 train_time:47806ms step_avg:95.80ms
step:510/1770 train_time:47903ms step_avg:95.81ms
step:511/1770 train_time:48000ms step_avg:95.81ms
step:512/1770 train_time:48098ms step_avg:95.81ms
step:513/1770 train_time:48196ms step_avg:95.82ms
step:514/1770 train_time:48293ms step_avg:95.82ms
step:515/1770 train_time:48391ms step_avg:95.82ms
step:516/1770 train_time:48489ms step_avg:95.83ms
step:517/1770 train_time:48587ms step_avg:95.83ms
step:518/1770 train_time:48685ms step_avg:95.84ms
step:519/1770 train_time:48782ms step_avg:95.84ms
step:520/1770 train_time:48880ms step_avg:95.84ms
step:521/1770 train_time:48977ms step_avg:95.84ms
step:522/1770 train_time:49074ms step_avg:95.85ms
step:523/1770 train_time:49171ms step_avg:95.85ms
step:524/1770 train_time:49269ms step_avg:95.86ms
step:525/1770 train_time:49368ms step_avg:95.86ms
step:526/1770 train_time:49466ms step_avg:95.86ms
step:527/1770 train_time:49564ms step_avg:95.87ms
step:528/1770 train_time:49661ms step_avg:95.87ms
step:529/1770 train_time:49759ms step_avg:95.87ms
step:530/1770 train_time:49856ms step_avg:95.88ms
step:531/1770 train_time:49954ms step_avg:95.88ms
step:532/1770 train_time:50051ms step_avg:95.88ms
step:533/1770 train_time:50150ms step_avg:95.89ms
step:534/1770 train_time:50248ms step_avg:95.89ms
step:535/1770 train_time:50345ms step_avg:95.90ms
step:536/1770 train_time:50443ms step_avg:95.90ms
step:537/1770 train_time:50541ms step_avg:95.90ms
step:538/1770 train_time:50638ms step_avg:95.91ms
step:539/1770 train_time:50736ms step_avg:95.91ms
step:540/1770 train_time:50834ms step_avg:95.91ms
step:541/1770 train_time:50932ms step_avg:95.92ms
step:542/1770 train_time:51030ms step_avg:95.92ms
step:543/1770 train_time:51128ms step_avg:95.93ms
step:544/1770 train_time:51226ms step_avg:95.93ms
step:545/1770 train_time:51324ms step_avg:95.93ms
step:546/1770 train_time:51421ms step_avg:95.93ms
step:547/1770 train_time:51519ms step_avg:95.94ms
step:548/1770 train_time:51617ms step_avg:95.94ms
step:549/1770 train_time:51715ms step_avg:95.95ms
step:550/1770 train_time:51813ms step_avg:95.95ms
step:551/1770 train_time:51911ms step_avg:95.95ms
step:552/1770 train_time:52009ms step_avg:95.96ms
step:553/1770 train_time:52108ms step_avg:95.96ms
step:554/1770 train_time:52206ms step_avg:95.97ms
step:555/1770 train_time:52304ms step_avg:95.97ms
step:556/1770 train_time:52401ms step_avg:95.97ms
step:557/1770 train_time:52499ms step_avg:95.98ms
step:558/1770 train_time:52597ms step_avg:95.98ms
step:559/1770 train_time:52694ms step_avg:95.98ms
step:560/1770 train_time:52792ms step_avg:95.99ms
step:561/1770 train_time:52890ms step_avg:95.99ms
step:562/1770 train_time:52988ms step_avg:95.99ms
step:563/1770 train_time:53087ms step_avg:96.00ms
step:564/1770 train_time:53185ms step_avg:96.00ms
step:565/1770 train_time:53283ms step_avg:96.01ms
step:566/1770 train_time:53381ms step_avg:96.01ms
step:567/1770 train_time:53479ms step_avg:96.01ms
step:568/1770 train_time:53577ms step_avg:96.02ms
step:569/1770 train_time:53674ms step_avg:96.02ms
step:570/1770 train_time:53772ms step_avg:96.02ms
step:571/1770 train_time:53870ms step_avg:96.03ms
step:572/1770 train_time:53969ms step_avg:96.03ms
step:573/1770 train_time:54066ms step_avg:96.03ms
step:574/1770 train_time:54164ms step_avg:96.04ms
step:575/1770 train_time:54262ms step_avg:96.04ms
step:576/1770 train_time:54359ms step_avg:96.04ms
step:577/1770 train_time:54457ms step_avg:96.04ms
step:578/1770 train_time:54555ms step_avg:96.05ms
step:579/1770 train_time:54653ms step_avg:96.05ms
step:580/1770 train_time:54751ms step_avg:96.05ms
step:581/1770 train_time:54849ms step_avg:96.06ms
step:582/1770 train_time:54947ms step_avg:96.06ms
step:583/1770 train_time:55045ms step_avg:96.07ms
step:584/1770 train_time:55143ms step_avg:96.07ms
step:585/1770 train_time:55241ms step_avg:96.07ms
step:586/1770 train_time:55339ms step_avg:96.07ms
step:587/1770 train_time:55437ms step_avg:96.08ms
step:588/1770 train_time:55534ms step_avg:96.08ms
step:589/1770 train_time:55632ms step_avg:96.08ms
step:590/1770 train_time:55730ms step_avg:96.09ms
step:591/1770 train_time:55828ms step_avg:96.09ms
step:592/1770 train_time:55926ms step_avg:96.09ms
step:593/1770 train_time:56024ms step_avg:96.10ms
step:594/1770 train_time:56121ms step_avg:96.10ms
step:595/1770 train_time:56218ms step_avg:96.10ms
step:596/1770 train_time:56316ms step_avg:96.10ms
step:597/1770 train_time:56413ms step_avg:96.10ms
step:598/1770 train_time:56511ms step_avg:96.11ms
step:599/1770 train_time:56610ms step_avg:96.11ms
step:600/1770 train_time:56708ms step_avg:96.12ms
step:601/1770 train_time:56807ms step_avg:96.12ms
step:602/1770 train_time:56904ms step_avg:96.12ms
step:603/1770 train_time:57002ms step_avg:96.13ms
step:604/1770 train_time:57100ms step_avg:96.13ms
step:605/1770 train_time:57198ms step_avg:96.13ms
step:606/1770 train_time:57295ms step_avg:96.13ms
step:607/1770 train_time:57393ms step_avg:96.14ms
step:608/1770 train_time:57491ms step_avg:96.14ms
step:609/1770 train_time:57590ms step_avg:96.14ms
step:610/1770 train_time:57687ms step_avg:96.15ms
step:611/1770 train_time:57786ms step_avg:96.15ms
step:612/1770 train_time:57883ms step_avg:96.15ms
step:613/1770 train_time:57981ms step_avg:96.15ms
step:614/1770 train_time:58078ms step_avg:96.16ms
step:615/1770 train_time:58177ms step_avg:96.16ms
step:616/1770 train_time:58276ms step_avg:96.16ms
step:617/1770 train_time:58373ms step_avg:96.17ms
step:618/1770 train_time:58471ms step_avg:96.17ms
step:619/1770 train_time:58570ms step_avg:96.17ms
step:620/1770 train_time:58668ms step_avg:96.18ms
step:621/1770 train_time:58766ms step_avg:96.18ms
step:622/1770 train_time:58864ms step_avg:96.18ms
step:623/1770 train_time:58962ms step_avg:96.19ms
step:624/1770 train_time:59059ms step_avg:96.19ms
step:625/1770 train_time:59157ms step_avg:96.19ms
step:625/1770 val_loss:3.6667 train_time:59253ms step_avg:96.35ms
step:626/1770 train_time:59275ms step_avg:96.23ms
step:627/1770 train_time:59363ms step_avg:96.21ms
step:628/1770 train_time:59463ms step_avg:96.22ms
step:629/1770 train_time:59560ms step_avg:96.22ms
step:630/1770 train_time:59658ms step_avg:96.22ms
step:631/1770 train_time:59755ms step_avg:96.22ms
step:632/1770 train_time:59853ms step_avg:96.23ms
step:633/1770 train_time:59951ms step_avg:96.23ms
step:634/1770 train_time:60049ms step_avg:96.23ms
step:635/1770 train_time:60147ms step_avg:96.24ms
step:636/1770 train_time:60245ms step_avg:96.24ms
step:637/1770 train_time:60343ms step_avg:96.24ms
step:638/1770 train_time:60441ms step_avg:96.24ms
step:639/1770 train_time:60539ms step_avg:96.25ms
step:640/1770 train_time:60637ms step_avg:96.25ms
step:641/1770 train_time:60735ms step_avg:96.25ms
step:642/1770 train_time:60832ms step_avg:96.25ms
step:643/1770 train_time:60930ms step_avg:96.26ms
step:644/1770 train_time:61029ms step_avg:96.26ms
step:645/1770 train_time:61127ms step_avg:96.26ms
step:646/1770 train_time:61225ms step_avg:96.27ms
step:647/1770 train_time:61323ms step_avg:96.27ms
step:648/1770 train_time:61420ms step_avg:96.27ms
step:649/1770 train_time:61518ms step_avg:96.27ms
step:650/1770 train_time:61616ms step_avg:96.28ms
step:651/1770 train_time:61714ms step_avg:96.28ms
step:652/1770 train_time:61811ms step_avg:96.28ms
step:653/1770 train_time:61910ms step_avg:96.28ms
step:654/1770 train_time:62008ms step_avg:96.29ms
step:655/1770 train_time:62106ms step_avg:96.29ms
step:656/1770 train_time:62204ms step_avg:96.29ms
step:657/1770 train_time:62301ms step_avg:96.29ms
step:658/1770 train_time:62400ms step_avg:96.30ms
step:659/1770 train_time:62501ms step_avg:96.30ms
step:660/1770 train_time:62601ms step_avg:96.31ms
step:661/1770 train_time:62700ms step_avg:96.31ms
step:662/1770 train_time:62800ms step_avg:96.32ms
step:663/1770 train_time:62899ms step_avg:96.32ms
step:664/1770 train_time:62999ms step_avg:96.33ms
step:665/1770 train_time:63099ms step_avg:96.33ms
step:666/1770 train_time:63199ms step_avg:96.34ms
step:667/1770 train_time:63298ms step_avg:96.34ms
step:668/1770 train_time:63398ms step_avg:96.35ms
step:669/1770 train_time:63497ms step_avg:96.35ms
step:670/1770 train_time:63597ms step_avg:96.36ms
step:671/1770 train_time:63696ms step_avg:96.36ms
step:672/1770 train_time:63794ms step_avg:96.37ms
step:673/1770 train_time:63894ms step_avg:96.37ms
step:674/1770 train_time:63993ms step_avg:96.38ms
step:675/1770 train_time:64093ms step_avg:96.38ms
step:676/1770 train_time:64192ms step_avg:96.38ms
step:677/1770 train_time:64291ms step_avg:96.39ms
step:678/1770 train_time:64391ms step_avg:96.39ms
step:679/1770 train_time:64491ms step_avg:96.40ms
step:680/1770 train_time:64591ms step_avg:96.40ms
step:681/1770 train_time:64691ms step_avg:96.41ms
step:682/1770 train_time:64791ms step_avg:96.42ms
step:683/1770 train_time:64891ms step_avg:96.42ms
step:684/1770 train_time:64991ms step_avg:96.43ms
step:685/1770 train_time:65091ms step_avg:96.43ms
step:686/1770 train_time:65191ms step_avg:96.44ms
step:687/1770 train_time:65291ms step_avg:96.44ms
step:688/1770 train_time:65391ms step_avg:96.45ms
step:689/1770 train_time:65491ms step_avg:96.45ms
step:690/1770 train_time:65591ms step_avg:96.46ms
step:691/1770 train_time:65691ms step_avg:96.46ms
step:692/1770 train_time:65791ms step_avg:96.47ms
step:693/1770 train_time:65891ms step_avg:96.47ms
step:694/1770 train_time:65992ms step_avg:96.48ms
step:695/1770 train_time:66092ms step_avg:96.48ms
step:696/1770 train_time:66191ms step_avg:96.49ms
step:697/1770 train_time:66291ms step_avg:96.49ms
step:698/1770 train_time:66391ms step_avg:96.50ms
step:699/1770 train_time:66491ms step_avg:96.50ms
step:700/1770 train_time:66591ms step_avg:96.51ms
step:701/1770 train_time:66690ms step_avg:96.51ms
step:702/1770 train_time:66790ms step_avg:96.52ms
step:703/1770 train_time:66890ms step_avg:96.52ms
step:704/1770 train_time:66990ms step_avg:96.53ms
step:705/1770 train_time:67090ms step_avg:96.53ms
step:706/1770 train_time:67191ms step_avg:96.54ms
step:707/1770 train_time:67291ms step_avg:96.54ms
step:708/1770 train_time:67391ms step_avg:96.55ms
step:709/1770 train_time:67491ms step_avg:96.55ms
step:710/1770 train_time:67591ms step_avg:96.56ms
step:711/1770 train_time:67690ms step_avg:96.56ms
step:712/1770 train_time:67791ms step_avg:96.57ms
step:713/1770 train_time:67891ms step_avg:96.57ms
step:714/1770 train_time:67991ms step_avg:96.58ms
step:715/1770 train_time:68092ms step_avg:96.58ms
step:716/1770 train_time:68191ms step_avg:96.59ms
step:717/1770 train_time:68291ms step_avg:96.59ms
step:718/1770 train_time:68391ms step_avg:96.60ms
step:719/1770 train_time:68492ms step_avg:96.60ms
step:720/1770 train_time:68591ms step_avg:96.61ms
step:721/1770 train_time:68691ms step_avg:96.61ms
step:722/1770 train_time:68791ms step_avg:96.62ms
step:723/1770 train_time:68891ms step_avg:96.62ms
step:724/1770 train_time:68991ms step_avg:96.63ms
step:725/1770 train_time:69091ms step_avg:96.63ms
step:726/1770 train_time:69191ms step_avg:96.64ms
step:727/1770 train_time:69290ms step_avg:96.64ms
step:728/1770 train_time:69391ms step_avg:96.64ms
step:729/1770 train_time:69490ms step_avg:96.65ms
step:730/1770 train_time:69590ms step_avg:96.65ms
step:731/1770 train_time:69690ms step_avg:96.66ms
step:732/1770 train_time:69791ms step_avg:96.66ms
step:733/1770 train_time:69891ms step_avg:96.67ms
step:734/1770 train_time:69990ms step_avg:96.67ms
step:735/1770 train_time:70090ms step_avg:96.68ms
step:736/1770 train_time:70190ms step_avg:96.68ms
step:737/1770 train_time:70290ms step_avg:96.68ms
step:738/1770 train_time:70389ms step_avg:96.69ms
step:739/1770 train_time:70489ms step_avg:96.69ms
step:740/1770 train_time:70588ms step_avg:96.70ms
step:741/1770 train_time:70689ms step_avg:96.70ms
step:742/1770 train_time:70789ms step_avg:96.71ms
step:743/1770 train_time:70889ms step_avg:96.71ms
step:744/1770 train_time:70989ms step_avg:96.72ms
step:745/1770 train_time:71090ms step_avg:96.72ms
step:746/1770 train_time:71191ms step_avg:96.73ms
step:747/1770 train_time:71291ms step_avg:96.73ms
step:748/1770 train_time:71391ms step_avg:96.74ms
step:749/1770 train_time:71490ms step_avg:96.74ms
step:750/1770 train_time:71590ms step_avg:96.74ms
step:750/1770 val_loss:3.6022 train_time:71688ms step_avg:96.88ms
step:751/1770 train_time:71710ms step_avg:96.77ms
step:752/1770 train_time:71799ms step_avg:96.76ms
step:753/1770 train_time:71899ms step_avg:96.77ms
step:754/1770 train_time:71998ms step_avg:96.77ms
step:755/1770 train_time:72098ms step_avg:96.78ms
step:756/1770 train_time:72197ms step_avg:96.78ms
step:757/1770 train_time:72296ms step_avg:96.78ms
step:758/1770 train_time:72395ms step_avg:96.79ms
step:759/1770 train_time:72495ms step_avg:96.79ms
step:760/1770 train_time:72594ms step_avg:96.79ms
step:761/1770 train_time:72693ms step_avg:96.80ms
step:762/1770 train_time:72793ms step_avg:96.80ms
step:763/1770 train_time:72893ms step_avg:96.80ms
step:764/1770 train_time:72993ms step_avg:96.81ms
step:765/1770 train_time:73094ms step_avg:96.81ms
step:766/1770 train_time:73195ms step_avg:96.82ms
step:767/1770 train_time:73296ms step_avg:96.82ms
step:768/1770 train_time:73396ms step_avg:96.83ms
step:769/1770 train_time:73496ms step_avg:96.83ms
step:770/1770 train_time:73596ms step_avg:96.84ms
step:771/1770 train_time:73695ms step_avg:96.84ms
step:772/1770 train_time:73794ms step_avg:96.84ms
step:773/1770 train_time:73894ms step_avg:96.85ms
step:774/1770 train_time:73994ms step_avg:96.85ms
step:775/1770 train_time:74094ms step_avg:96.86ms
step:776/1770 train_time:74195ms step_avg:96.86ms
step:777/1770 train_time:74295ms step_avg:96.86ms
step:778/1770 train_time:74395ms step_avg:96.87ms
step:779/1770 train_time:74495ms step_avg:96.87ms
step:780/1770 train_time:74595ms step_avg:96.88ms
step:781/1770 train_time:74695ms step_avg:96.88ms
step:782/1770 train_time:74795ms step_avg:96.88ms
step:783/1770 train_time:74895ms step_avg:96.89ms
step:784/1770 train_time:74994ms step_avg:96.89ms
step:785/1770 train_time:75094ms step_avg:96.90ms
step:786/1770 train_time:75194ms step_avg:96.90ms
step:787/1770 train_time:75295ms step_avg:96.90ms
step:788/1770 train_time:75395ms step_avg:96.91ms
step:789/1770 train_time:75496ms step_avg:96.91ms
step:790/1770 train_time:75596ms step_avg:96.92ms
step:791/1770 train_time:75696ms step_avg:96.92ms
step:792/1770 train_time:75796ms step_avg:96.93ms
step:793/1770 train_time:75896ms step_avg:96.93ms
step:794/1770 train_time:75996ms step_avg:96.93ms
step:795/1770 train_time:76096ms step_avg:96.94ms
step:796/1770 train_time:76196ms step_avg:96.94ms
step:797/1770 train_time:76296ms step_avg:96.95ms
step:798/1770 train_time:76396ms step_avg:96.95ms
step:799/1770 train_time:76496ms step_avg:96.95ms
step:800/1770 train_time:76596ms step_avg:96.96ms
step:801/1770 train_time:76696ms step_avg:96.96ms
step:802/1770 train_time:76795ms step_avg:96.96ms
step:803/1770 train_time:76896ms step_avg:96.97ms
step:804/1770 train_time:76997ms step_avg:96.97ms
step:805/1770 train_time:77096ms step_avg:96.98ms
step:806/1770 train_time:77196ms step_avg:96.98ms
step:807/1770 train_time:77295ms step_avg:96.98ms
step:808/1770 train_time:77395ms step_avg:96.99ms
step:809/1770 train_time:77495ms step_avg:96.99ms
step:810/1770 train_time:77595ms step_avg:96.99ms
step:811/1770 train_time:77695ms step_avg:97.00ms
step:812/1770 train_time:77795ms step_avg:97.00ms
step:813/1770 train_time:77896ms step_avg:97.01ms
step:814/1770 train_time:77996ms step_avg:97.01ms
step:815/1770 train_time:78097ms step_avg:97.01ms
step:816/1770 train_time:78197ms step_avg:97.02ms
step:817/1770 train_time:78297ms step_avg:97.02ms
step:818/1770 train_time:78397ms step_avg:97.03ms
step:819/1770 train_time:78497ms step_avg:97.03ms
step:820/1770 train_time:78596ms step_avg:97.03ms
step:821/1770 train_time:78697ms step_avg:97.04ms
step:822/1770 train_time:78796ms step_avg:97.04ms
step:823/1770 train_time:78896ms step_avg:97.04ms
step:824/1770 train_time:78996ms step_avg:97.05ms
step:825/1770 train_time:79095ms step_avg:97.05ms
step:826/1770 train_time:79196ms step_avg:97.05ms
step:827/1770 train_time:79297ms step_avg:97.06ms
step:828/1770 train_time:79396ms step_avg:97.06ms
step:829/1770 train_time:79496ms step_avg:97.06ms
step:830/1770 train_time:79596ms step_avg:97.07ms
step:831/1770 train_time:79696ms step_avg:97.07ms
step:832/1770 train_time:79796ms step_avg:97.07ms
step:833/1770 train_time:79896ms step_avg:97.08ms
step:834/1770 train_time:79996ms step_avg:97.08ms
step:835/1770 train_time:80095ms step_avg:97.09ms
step:836/1770 train_time:80196ms step_avg:97.09ms
step:837/1770 train_time:80296ms step_avg:97.09ms
step:838/1770 train_time:80396ms step_avg:97.10ms
step:839/1770 train_time:80496ms step_avg:97.10ms
step:840/1770 train_time:80597ms step_avg:97.10ms
step:841/1770 train_time:80696ms step_avg:97.11ms
step:842/1770 train_time:80796ms step_avg:97.11ms
step:843/1770 train_time:80896ms step_avg:97.11ms
step:844/1770 train_time:80995ms step_avg:97.12ms
step:845/1770 train_time:81095ms step_avg:97.12ms
step:846/1770 train_time:81195ms step_avg:97.12ms
step:847/1770 train_time:81295ms step_avg:97.13ms
step:848/1770 train_time:81395ms step_avg:97.13ms
step:849/1770 train_time:81495ms step_avg:97.13ms
step:850/1770 train_time:81596ms step_avg:97.14ms
step:851/1770 train_time:81697ms step_avg:97.14ms
step:852/1770 train_time:81797ms step_avg:97.15ms
step:853/1770 train_time:81897ms step_avg:97.15ms
step:854/1770 train_time:81996ms step_avg:97.15ms
step:855/1770 train_time:82096ms step_avg:97.15ms
step:856/1770 train_time:82195ms step_avg:97.16ms
step:857/1770 train_time:82295ms step_avg:97.16ms
step:858/1770 train_time:82395ms step_avg:97.16ms
step:859/1770 train_time:82496ms step_avg:97.17ms
step:860/1770 train_time:82596ms step_avg:97.17ms
step:861/1770 train_time:82696ms step_avg:97.18ms
step:862/1770 train_time:82796ms step_avg:97.18ms
step:863/1770 train_time:82896ms step_avg:97.18ms
step:864/1770 train_time:82996ms step_avg:97.18ms
step:865/1770 train_time:83096ms step_avg:97.19ms
step:866/1770 train_time:83197ms step_avg:97.19ms
step:867/1770 train_time:83296ms step_avg:97.19ms
step:868/1770 train_time:83396ms step_avg:97.20ms
step:869/1770 train_time:83495ms step_avg:97.20ms
step:870/1770 train_time:83595ms step_avg:97.20ms
step:871/1770 train_time:83695ms step_avg:97.21ms
step:872/1770 train_time:83795ms step_avg:97.21ms
step:873/1770 train_time:83895ms step_avg:97.21ms
step:874/1770 train_time:83995ms step_avg:97.22ms
step:875/1770 train_time:84096ms step_avg:97.22ms
step:875/1770 val_loss:3.5555 train_time:84195ms step_avg:97.33ms
step:876/1770 train_time:84216ms step_avg:97.25ms
step:877/1770 train_time:84303ms step_avg:97.24ms
step:878/1770 train_time:84404ms step_avg:97.24ms
step:879/1770 train_time:84504ms step_avg:97.24ms
step:880/1770 train_time:84603ms step_avg:97.24ms
step:881/1770 train_time:84702ms step_avg:97.25ms
step:882/1770 train_time:84802ms step_avg:97.25ms
step:883/1770 train_time:84901ms step_avg:97.25ms
step:884/1770 train_time:85001ms step_avg:97.26ms
step:885/1770 train_time:85101ms step_avg:97.26ms
step:886/1770 train_time:85200ms step_avg:97.26ms
step:887/1770 train_time:85300ms step_avg:97.26ms
step:888/1770 train_time:85400ms step_avg:97.27ms
step:889/1770 train_time:85500ms step_avg:97.27ms
step:890/1770 train_time:85600ms step_avg:97.27ms
step:891/1770 train_time:85699ms step_avg:97.28ms
step:892/1770 train_time:85800ms step_avg:97.28ms
step:893/1770 train_time:85900ms step_avg:97.28ms
step:894/1770 train_time:86001ms step_avg:97.29ms
step:895/1770 train_time:86101ms step_avg:97.29ms
step:896/1770 train_time:86201ms step_avg:97.29ms
step:897/1770 train_time:86301ms step_avg:97.30ms
step:898/1770 train_time:86401ms step_avg:97.30ms
step:899/1770 train_time:86500ms step_avg:97.30ms
step:900/1770 train_time:86599ms step_avg:97.30ms
step:901/1770 train_time:86701ms step_avg:97.31ms
step:902/1770 train_time:86803ms step_avg:97.31ms
step:903/1770 train_time:86902ms step_avg:97.31ms
step:904/1770 train_time:87002ms step_avg:97.32ms
step:905/1770 train_time:87101ms step_avg:97.32ms
step:906/1770 train_time:87201ms step_avg:97.32ms
step:907/1770 train_time:87300ms step_avg:97.32ms
step:908/1770 train_time:87400ms step_avg:97.33ms
step:909/1770 train_time:87499ms step_avg:97.33ms
step:910/1770 train_time:87600ms step_avg:97.33ms
step:911/1770 train_time:87699ms step_avg:97.34ms
step:912/1770 train_time:87799ms step_avg:97.34ms
step:913/1770 train_time:87899ms step_avg:97.34ms
step:914/1770 train_time:88000ms step_avg:97.34ms
step:915/1770 train_time:88100ms step_avg:97.35ms
step:916/1770 train_time:88201ms step_avg:97.35ms
step:917/1770 train_time:88300ms step_avg:97.35ms
step:918/1770 train_time:88400ms step_avg:97.36ms
step:919/1770 train_time:88500ms step_avg:97.36ms
step:920/1770 train_time:88602ms step_avg:97.36ms
step:921/1770 train_time:88703ms step_avg:97.37ms
step:922/1770 train_time:88804ms step_avg:97.37ms
step:923/1770 train_time:88904ms step_avg:97.38ms
step:924/1770 train_time:89005ms step_avg:97.38ms
step:925/1770 train_time:89106ms step_avg:97.38ms
step:926/1770 train_time:89206ms step_avg:97.39ms
step:927/1770 train_time:89307ms step_avg:97.39ms
step:928/1770 train_time:89408ms step_avg:97.39ms
step:929/1770 train_time:89510ms step_avg:97.40ms
step:930/1770 train_time:89612ms step_avg:97.40ms
step:931/1770 train_time:89714ms step_avg:97.41ms
step:932/1770 train_time:89816ms step_avg:97.41ms
step:933/1770 train_time:89918ms step_avg:97.42ms
step:934/1770 train_time:90019ms step_avg:97.42ms
step:935/1770 train_time:90120ms step_avg:97.43ms
step:936/1770 train_time:90221ms step_avg:97.43ms
step:937/1770 train_time:90322ms step_avg:97.43ms
step:938/1770 train_time:90423ms step_avg:97.44ms
step:939/1770 train_time:90523ms step_avg:97.44ms
step:940/1770 train_time:90624ms step_avg:97.44ms
step:941/1770 train_time:90724ms step_avg:97.45ms
step:942/1770 train_time:90825ms step_avg:97.45ms
step:943/1770 train_time:90926ms step_avg:97.46ms
step:944/1770 train_time:91027ms step_avg:97.46ms
step:945/1770 train_time:91128ms step_avg:97.46ms
step:946/1770 train_time:91230ms step_avg:97.47ms
step:947/1770 train_time:91331ms step_avg:97.47ms
step:948/1770 train_time:91432ms step_avg:97.48ms
step:949/1770 train_time:91534ms step_avg:97.48ms
step:950/1770 train_time:91636ms step_avg:97.49ms
step:951/1770 train_time:91739ms step_avg:97.49ms
step:952/1770 train_time:91840ms step_avg:97.49ms
step:953/1770 train_time:91942ms step_avg:97.50ms
step:954/1770 train_time:92043ms step_avg:97.50ms
step:955/1770 train_time:92144ms step_avg:97.51ms
step:956/1770 train_time:92244ms step_avg:97.51ms
step:957/1770 train_time:92345ms step_avg:97.51ms
step:958/1770 train_time:92445ms step_avg:97.52ms
step:959/1770 train_time:92546ms step_avg:97.52ms
step:960/1770 train_time:92647ms step_avg:97.52ms
step:961/1770 train_time:92748ms step_avg:97.53ms
step:962/1770 train_time:92850ms step_avg:97.53ms
step:963/1770 train_time:92951ms step_avg:97.54ms
step:964/1770 train_time:93052ms step_avg:97.54ms
step:965/1770 train_time:93155ms step_avg:97.54ms
step:966/1770 train_time:93257ms step_avg:97.55ms
step:967/1770 train_time:93358ms step_avg:97.55ms
step:968/1770 train_time:93460ms step_avg:97.56ms
step:969/1770 train_time:93561ms step_avg:97.56ms
step:970/1770 train_time:93663ms step_avg:97.57ms
step:971/1770 train_time:93763ms step_avg:97.57ms
step:972/1770 train_time:93864ms step_avg:97.57ms
step:973/1770 train_time:93965ms step_avg:97.57ms
step:974/1770 train_time:94065ms step_avg:97.58ms
step:975/1770 train_time:94167ms step_avg:97.58ms
step:976/1770 train_time:94268ms step_avg:97.59ms
step:977/1770 train_time:94369ms step_avg:97.59ms
step:978/1770 train_time:94471ms step_avg:97.59ms
step:979/1770 train_time:94573ms step_avg:97.60ms
step:980/1770 train_time:94675ms step_avg:97.60ms
step:981/1770 train_time:94776ms step_avg:97.61ms
step:982/1770 train_time:94879ms step_avg:97.61ms
step:983/1770 train_time:94980ms step_avg:97.62ms
step:984/1770 train_time:95082ms step_avg:97.62ms
step:985/1770 train_time:95183ms step_avg:97.62ms
step:986/1770 train_time:95284ms step_avg:97.63ms
step:987/1770 train_time:95385ms step_avg:97.63ms
step:988/1770 train_time:95486ms step_avg:97.63ms
step:989/1770 train_time:95589ms step_avg:97.64ms
step:990/1770 train_time:95690ms step_avg:97.64ms
step:991/1770 train_time:95791ms step_avg:97.65ms
step:992/1770 train_time:95893ms step_avg:97.65ms
step:993/1770 train_time:95994ms step_avg:97.65ms
step:994/1770 train_time:96096ms step_avg:97.66ms
step:995/1770 train_time:96198ms step_avg:97.66ms
step:996/1770 train_time:96299ms step_avg:97.67ms
step:997/1770 train_time:96401ms step_avg:97.67ms
step:998/1770 train_time:96504ms step_avg:97.68ms
step:999/1770 train_time:96604ms step_avg:97.68ms
step:1000/1770 train_time:96705ms step_avg:97.68ms
step:1000/1770 val_loss:3.5146 train_time:96804ms step_avg:97.78ms
step:1001/1770 train_time:96826ms step_avg:97.70ms
step:1002/1770 train_time:96919ms step_avg:97.70ms
step:1003/1770 train_time:97022ms step_avg:97.71ms
step:1004/1770 train_time:97123ms step_avg:97.71ms
step:1005/1770 train_time:97224ms step_avg:97.71ms
step:1006/1770 train_time:97324ms step_avg:97.72ms
step:1007/1770 train_time:97425ms step_avg:97.72ms
step:1008/1770 train_time:97526ms step_avg:97.72ms
step:1009/1770 train_time:97626ms step_avg:97.72ms
step:1010/1770 train_time:97727ms step_avg:97.73ms
step:1011/1770 train_time:97830ms step_avg:97.73ms
step:1012/1770 train_time:97934ms step_avg:97.74ms
step:1013/1770 train_time:98038ms step_avg:97.74ms
step:1014/1770 train_time:98138ms step_avg:97.75ms
step:1015/1770 train_time:98239ms step_avg:97.75ms
step:1016/1770 train_time:98339ms step_avg:97.75ms
step:1017/1770 train_time:98441ms step_avg:97.76ms
step:1018/1770 train_time:98542ms step_avg:97.76ms
step:1019/1770 train_time:98642ms step_avg:97.76ms
step:1020/1770 train_time:98743ms step_avg:97.77ms
step:1021/1770 train_time:98845ms step_avg:97.77ms
step:1022/1770 train_time:98946ms step_avg:97.77ms
step:1023/1770 train_time:99048ms step_avg:97.78ms
step:1024/1770 train_time:99149ms step_avg:97.78ms
step:1025/1770 train_time:99252ms step_avg:97.78ms
step:1026/1770 train_time:99354ms step_avg:97.79ms
step:1027/1770 train_time:99455ms step_avg:97.79ms
step:1028/1770 train_time:99556ms step_avg:97.80ms
step:1029/1770 train_time:99657ms step_avg:97.80ms
step:1030/1770 train_time:99760ms step_avg:97.80ms
step:1031/1770 train_time:99862ms step_avg:97.81ms
step:1032/1770 train_time:99963ms step_avg:97.81ms
step:1033/1770 train_time:100064ms step_avg:97.81ms
step:1034/1770 train_time:100165ms step_avg:97.82ms
step:1035/1770 train_time:100265ms step_avg:97.82ms
step:1036/1770 train_time:100366ms step_avg:97.82ms
step:1037/1770 train_time:100468ms step_avg:97.83ms
step:1038/1770 train_time:100571ms step_avg:97.83ms
step:1039/1770 train_time:100673ms step_avg:97.84ms
step:1040/1770 train_time:100775ms step_avg:97.84ms
step:1041/1770 train_time:100877ms step_avg:97.84ms
step:1042/1770 train_time:100977ms step_avg:97.85ms
step:1043/1770 train_time:101078ms step_avg:97.85ms
step:1044/1770 train_time:101179ms step_avg:97.85ms
step:1045/1770 train_time:101279ms step_avg:97.85ms
step:1046/1770 train_time:101380ms step_avg:97.86ms
step:1047/1770 train_time:101481ms step_avg:97.86ms
step:1048/1770 train_time:101582ms step_avg:97.86ms
step:1049/1770 train_time:101682ms step_avg:97.87ms
step:1050/1770 train_time:101784ms step_avg:97.87ms
step:1051/1770 train_time:101886ms step_avg:97.87ms
step:1052/1770 train_time:101987ms step_avg:97.88ms
step:1053/1770 train_time:102089ms step_avg:97.88ms
step:1054/1770 train_time:102190ms step_avg:97.88ms
step:1055/1770 train_time:102291ms step_avg:97.89ms
step:1056/1770 train_time:102393ms step_avg:97.89ms
step:1057/1770 train_time:102495ms step_avg:97.89ms
step:1058/1770 train_time:102597ms step_avg:97.90ms
step:1059/1770 train_time:102699ms step_avg:97.90ms
step:1060/1770 train_time:102801ms step_avg:97.91ms
step:1061/1770 train_time:102902ms step_avg:97.91ms
step:1062/1770 train_time:103004ms step_avg:97.91ms
step:1063/1770 train_time:103108ms step_avg:97.92ms
step:1064/1770 train_time:103210ms step_avg:97.92ms
step:1065/1770 train_time:103311ms step_avg:97.92ms
step:1066/1770 train_time:103412ms step_avg:97.93ms
step:1067/1770 train_time:103516ms step_avg:97.93ms
step:1068/1770 train_time:103618ms step_avg:97.94ms
step:1069/1770 train_time:103719ms step_avg:97.94ms
step:1070/1770 train_time:103820ms step_avg:97.94ms
step:1071/1770 train_time:103922ms step_avg:97.95ms
step:1072/1770 train_time:104023ms step_avg:97.95ms
step:1073/1770 train_time:104123ms step_avg:97.95ms
step:1074/1770 train_time:104224ms step_avg:97.96ms
step:1075/1770 train_time:104326ms step_avg:97.96ms
step:1076/1770 train_time:104428ms step_avg:97.96ms
step:1077/1770 train_time:104531ms step_avg:97.97ms
step:1078/1770 train_time:104632ms step_avg:97.97ms
step:1079/1770 train_time:104734ms step_avg:97.97ms
step:1080/1770 train_time:104836ms step_avg:97.98ms
step:1081/1770 train_time:104939ms step_avg:97.98ms
step:1082/1770 train_time:105039ms step_avg:97.98ms
step:1083/1770 train_time:105140ms step_avg:97.99ms
step:1084/1770 train_time:105241ms step_avg:97.99ms
step:1085/1770 train_time:105342ms step_avg:97.99ms
step:1086/1770 train_time:105443ms step_avg:98.00ms
step:1087/1770 train_time:105544ms step_avg:98.00ms
step:1088/1770 train_time:105645ms step_avg:98.00ms
step:1089/1770 train_time:105747ms step_avg:98.00ms
step:1090/1770 train_time:105849ms step_avg:98.01ms
step:1091/1770 train_time:105951ms step_avg:98.01ms
step:1092/1770 train_time:106053ms step_avg:98.02ms
step:1093/1770 train_time:106155ms step_avg:98.02ms
step:1094/1770 train_time:106256ms step_avg:98.02ms
step:1095/1770 train_time:106357ms step_avg:98.03ms
step:1096/1770 train_time:106459ms step_avg:98.03ms
step:1097/1770 train_time:106559ms step_avg:98.03ms
step:1098/1770 train_time:106660ms step_avg:98.03ms
step:1099/1770 train_time:106761ms step_avg:98.04ms
step:1100/1770 train_time:106862ms step_avg:98.04ms
step:1101/1770 train_time:106964ms step_avg:98.04ms
step:1102/1770 train_time:107067ms step_avg:98.05ms
step:1103/1770 train_time:107168ms step_avg:98.05ms
step:1104/1770 train_time:107270ms step_avg:98.05ms
step:1105/1770 train_time:107372ms step_avg:98.06ms
step:1106/1770 train_time:107474ms step_avg:98.06ms
step:1107/1770 train_time:107575ms step_avg:98.06ms
step:1108/1770 train_time:107677ms step_avg:98.07ms
step:1109/1770 train_time:107778ms step_avg:98.07ms
step:1110/1770 train_time:107879ms step_avg:98.07ms
step:1111/1770 train_time:107981ms step_avg:98.08ms
step:1112/1770 train_time:108082ms step_avg:98.08ms
step:1113/1770 train_time:108184ms step_avg:98.08ms
step:1114/1770 train_time:108286ms step_avg:98.08ms
step:1115/1770 train_time:108387ms step_avg:98.09ms
step:1116/1770 train_time:108489ms step_avg:98.09ms
step:1117/1770 train_time:108590ms step_avg:98.09ms
step:1118/1770 train_time:108691ms step_avg:98.10ms
step:1119/1770 train_time:108794ms step_avg:98.10ms
step:1120/1770 train_time:108896ms step_avg:98.10ms
step:1121/1770 train_time:108998ms step_avg:98.11ms
step:1122/1770 train_time:109099ms step_avg:98.11ms
step:1123/1770 train_time:109200ms step_avg:98.11ms
step:1124/1770 train_time:109301ms step_avg:98.12ms
step:1125/1770 train_time:109402ms step_avg:98.12ms
step:1125/1770 val_loss:3.4738 train_time:109502ms step_avg:98.21ms
step:1126/1770 train_time:109523ms step_avg:98.14ms
step:1127/1770 train_time:109616ms step_avg:98.13ms
step:1128/1770 train_time:109718ms step_avg:98.14ms
step:1129/1770 train_time:109819ms step_avg:98.14ms
step:1130/1770 train_time:109920ms step_avg:98.14ms
step:1131/1770 train_time:110021ms step_avg:98.15ms
step:1132/1770 train_time:110122ms step_avg:98.15ms
step:1133/1770 train_time:110223ms step_avg:98.15ms
step:1134/1770 train_time:110325ms step_avg:98.15ms
step:1135/1770 train_time:110426ms step_avg:98.16ms
step:1136/1770 train_time:110528ms step_avg:98.16ms
step:1137/1770 train_time:110631ms step_avg:98.16ms
step:1138/1770 train_time:110734ms step_avg:98.17ms
step:1139/1770 train_time:110835ms step_avg:98.17ms
step:1140/1770 train_time:110936ms step_avg:98.17ms
step:1141/1770 train_time:111037ms step_avg:98.18ms
step:1142/1770 train_time:111138ms step_avg:98.18ms
step:1143/1770 train_time:111238ms step_avg:98.18ms
step:1144/1770 train_time:111339ms step_avg:98.18ms
step:1145/1770 train_time:111440ms step_avg:98.19ms
step:1146/1770 train_time:111542ms step_avg:98.19ms
step:1147/1770 train_time:111644ms step_avg:98.19ms
step:1148/1770 train_time:111745ms step_avg:98.19ms
step:1149/1770 train_time:111847ms step_avg:98.20ms
step:1150/1770 train_time:111948ms step_avg:98.20ms
step:1151/1770 train_time:112050ms step_avg:98.20ms
step:1152/1770 train_time:112153ms step_avg:98.21ms
step:1153/1770 train_time:112254ms step_avg:98.21ms
step:1154/1770 train_time:112356ms step_avg:98.21ms
step:1155/1770 train_time:112457ms step_avg:98.22ms
step:1156/1770 train_time:112558ms step_avg:98.22ms
step:1157/1770 train_time:112660ms step_avg:98.22ms
step:1158/1770 train_time:112762ms step_avg:98.22ms
step:1159/1770 train_time:112863ms step_avg:98.23ms
step:1160/1770 train_time:112964ms step_avg:98.23ms
step:1161/1770 train_time:113065ms step_avg:98.23ms
step:1162/1770 train_time:113167ms step_avg:98.24ms
step:1163/1770 train_time:113270ms step_avg:98.24ms
step:1164/1770 train_time:113372ms step_avg:98.24ms
step:1165/1770 train_time:113473ms step_avg:98.25ms
step:1166/1770 train_time:113575ms step_avg:98.25ms
step:1167/1770 train_time:113677ms step_avg:98.25ms
step:1168/1770 train_time:113778ms step_avg:98.25ms
step:1169/1770 train_time:113878ms step_avg:98.26ms
step:1170/1770 train_time:113979ms step_avg:98.26ms
step:1171/1770 train_time:114080ms step_avg:98.26ms
step:1172/1770 train_time:114181ms step_avg:98.26ms
step:1173/1770 train_time:114282ms step_avg:98.26ms
step:1174/1770 train_time:114384ms step_avg:98.27ms
step:1175/1770 train_time:114485ms step_avg:98.27ms
step:1176/1770 train_time:114587ms step_avg:98.27ms
step:1177/1770 train_time:114689ms step_avg:98.28ms
step:1178/1770 train_time:114792ms step_avg:98.28ms
step:1179/1770 train_time:114893ms step_avg:98.28ms
step:1180/1770 train_time:114994ms step_avg:98.29ms
step:1181/1770 train_time:115096ms step_avg:98.29ms
step:1182/1770 train_time:115198ms step_avg:98.29ms
step:1183/1770 train_time:115300ms step_avg:98.29ms
step:1184/1770 train_time:115403ms step_avg:98.30ms
step:1185/1770 train_time:115505ms step_avg:98.30ms
step:1186/1770 train_time:115608ms step_avg:98.31ms
step:1187/1770 train_time:115713ms step_avg:98.31ms
step:1188/1770 train_time:115816ms step_avg:98.32ms
step:1189/1770 train_time:115918ms step_avg:98.32ms
step:1190/1770 train_time:116020ms step_avg:98.32ms
step:1191/1770 train_time:116122ms step_avg:98.33ms
step:1192/1770 train_time:116225ms step_avg:98.33ms
step:1193/1770 train_time:116329ms step_avg:98.33ms
step:1194/1770 train_time:116431ms step_avg:98.34ms
step:1195/1770 train_time:116535ms step_avg:98.34ms
step:1196/1770 train_time:116638ms step_avg:98.35ms
step:1197/1770 train_time:116740ms step_avg:98.35ms
step:1198/1770 train_time:116842ms step_avg:98.35ms
step:1199/1770 train_time:116944ms step_avg:98.36ms
step:1200/1770 train_time:117047ms step_avg:98.36ms
step:1201/1770 train_time:117150ms step_avg:98.36ms
step:1202/1770 train_time:117253ms step_avg:98.37ms
step:1203/1770 train_time:117355ms step_avg:98.37ms
step:1204/1770 train_time:117458ms step_avg:98.37ms
step:1205/1770 train_time:117560ms step_avg:98.38ms
step:1206/1770 train_time:117664ms step_avg:98.38ms
step:1207/1770 train_time:117766ms step_avg:98.38ms
step:1208/1770 train_time:117869ms step_avg:98.39ms
step:1209/1770 train_time:117972ms step_avg:98.39ms
step:1210/1770 train_time:118075ms step_avg:98.40ms
step:1211/1770 train_time:118178ms step_avg:98.40ms
step:1212/1770 train_time:118283ms step_avg:98.40ms
step:1213/1770 train_time:118385ms step_avg:98.41ms
step:1214/1770 train_time:118487ms step_avg:98.41ms
step:1215/1770 train_time:118591ms step_avg:98.42ms
step:1216/1770 train_time:118696ms step_avg:98.42ms
step:1217/1770 train_time:118798ms step_avg:98.42ms
step:1218/1770 train_time:118900ms step_avg:98.43ms
step:1219/1770 train_time:119003ms step_avg:98.43ms
step:1220/1770 train_time:119105ms step_avg:98.43ms
step:1221/1770 train_time:119208ms step_avg:98.44ms
step:1222/1770 train_time:119312ms step_avg:98.44ms
step:1223/1770 train_time:119415ms step_avg:98.45ms
step:1224/1770 train_time:119519ms step_avg:98.45ms
step:1225/1770 train_time:119622ms step_avg:98.45ms
step:1226/1770 train_time:119724ms step_avg:98.46ms
step:1227/1770 train_time:119829ms step_avg:98.46ms
step:1228/1770 train_time:119933ms step_avg:98.47ms
step:1229/1770 train_time:120035ms step_avg:98.47ms
step:1230/1770 train_time:120138ms step_avg:98.47ms
step:1231/1770 train_time:120241ms step_avg:98.48ms
step:1232/1770 train_time:120343ms step_avg:98.48ms
step:1233/1770 train_time:120445ms step_avg:98.48ms
step:1234/1770 train_time:120548ms step_avg:98.49ms
step:1235/1770 train_time:120650ms step_avg:98.49ms
step:1236/1770 train_time:120753ms step_avg:98.49ms
step:1237/1770 train_time:120856ms step_avg:98.50ms
step:1238/1770 train_time:120959ms step_avg:98.50ms
step:1239/1770 train_time:121061ms step_avg:98.50ms
step:1240/1770 train_time:121164ms step_avg:98.51ms
step:1241/1770 train_time:121267ms step_avg:98.51ms
step:1242/1770 train_time:121370ms step_avg:98.51ms
step:1243/1770 train_time:121473ms step_avg:98.52ms
step:1244/1770 train_time:121575ms step_avg:98.52ms
step:1245/1770 train_time:121677ms step_avg:98.52ms
step:1246/1770 train_time:121779ms step_avg:98.53ms
step:1247/1770 train_time:121882ms step_avg:98.53ms
step:1248/1770 train_time:121985ms step_avg:98.53ms
step:1249/1770 train_time:122087ms step_avg:98.54ms
step:1250/1770 train_time:122190ms step_avg:98.54ms
step:1250/1770 val_loss:3.4289 train_time:122293ms step_avg:98.62ms
step:1251/1770 train_time:122315ms step_avg:98.56ms
step:1252/1770 train_time:122404ms step_avg:98.55ms
step:1253/1770 train_time:122507ms step_avg:98.56ms
step:1254/1770 train_time:122610ms step_avg:98.56ms
step:1255/1770 train_time:122715ms step_avg:98.57ms
step:1256/1770 train_time:122818ms step_avg:98.57ms
step:1257/1770 train_time:122920ms step_avg:98.57ms
step:1258/1770 train_time:123022ms step_avg:98.58ms
step:1259/1770 train_time:123126ms step_avg:98.58ms
step:1260/1770 train_time:123228ms step_avg:98.58ms
step:1261/1770 train_time:123333ms step_avg:98.59ms
step:1262/1770 train_time:123436ms step_avg:98.59ms
step:1263/1770 train_time:123538ms step_avg:98.59ms
step:1264/1770 train_time:123641ms step_avg:98.60ms
step:1265/1770 train_time:123744ms step_avg:98.60ms
step:1266/1770 train_time:123847ms step_avg:98.60ms
step:1267/1770 train_time:123950ms step_avg:98.61ms
step:1268/1770 train_time:124053ms step_avg:98.61ms
step:1269/1770 train_time:124156ms step_avg:98.61ms
step:1270/1770 train_time:124259ms step_avg:98.62ms
step:1271/1770 train_time:124361ms step_avg:98.62ms
step:1272/1770 train_time:124462ms step_avg:98.62ms
step:1273/1770 train_time:124566ms step_avg:98.63ms
step:1274/1770 train_time:124668ms step_avg:98.63ms
step:1275/1770 train_time:124770ms step_avg:98.63ms
step:1276/1770 train_time:124874ms step_avg:98.64ms
step:1277/1770 train_time:124976ms step_avg:98.64ms
step:1278/1770 train_time:125080ms step_avg:98.64ms
step:1279/1770 train_time:125183ms step_avg:98.65ms
step:1280/1770 train_time:125286ms step_avg:98.65ms
step:1281/1770 train_time:125388ms step_avg:98.65ms
step:1282/1770 train_time:125491ms step_avg:98.66ms
step:1283/1770 train_time:125596ms step_avg:98.66ms
step:1284/1770 train_time:125699ms step_avg:98.66ms
step:1285/1770 train_time:125801ms step_avg:98.67ms
step:1286/1770 train_time:125905ms step_avg:98.67ms
step:1287/1770 train_time:126009ms step_avg:98.68ms
step:1288/1770 train_time:126113ms step_avg:98.68ms
step:1289/1770 train_time:126216ms step_avg:98.68ms
step:1290/1770 train_time:126318ms step_avg:98.69ms
step:1291/1770 train_time:126420ms step_avg:98.69ms
step:1292/1770 train_time:126522ms step_avg:98.69ms
step:1293/1770 train_time:126625ms step_avg:98.69ms
step:1294/1770 train_time:126727ms step_avg:98.70ms
step:1295/1770 train_time:126831ms step_avg:98.70ms
step:1296/1770 train_time:126933ms step_avg:98.70ms
step:1297/1770 train_time:127036ms step_avg:98.71ms
step:1298/1770 train_time:127139ms step_avg:98.71ms
step:1299/1770 train_time:127241ms step_avg:98.71ms
step:1300/1770 train_time:127343ms step_avg:98.72ms
step:1301/1770 train_time:127446ms step_avg:98.72ms
step:1302/1770 train_time:127549ms step_avg:98.72ms
step:1303/1770 train_time:127651ms step_avg:98.72ms
step:1304/1770 train_time:127754ms step_avg:98.73ms
step:1305/1770 train_time:127858ms step_avg:98.73ms
step:1306/1770 train_time:127959ms step_avg:98.73ms
step:1307/1770 train_time:128061ms step_avg:98.74ms
step:1308/1770 train_time:128164ms step_avg:98.74ms
step:1309/1770 train_time:128266ms step_avg:98.74ms
step:1310/1770 train_time:128369ms step_avg:98.75ms
step:1311/1770 train_time:128471ms step_avg:98.75ms
step:1312/1770 train_time:128573ms step_avg:98.75ms
step:1313/1770 train_time:128675ms step_avg:98.75ms
step:1314/1770 train_time:128778ms step_avg:98.76ms
step:1315/1770 train_time:128880ms step_avg:98.76ms
step:1316/1770 train_time:128983ms step_avg:98.76ms
step:1317/1770 train_time:129086ms step_avg:98.77ms
step:1318/1770 train_time:129191ms step_avg:98.77ms
step:1319/1770 train_time:129295ms step_avg:98.77ms
step:1320/1770 train_time:129397ms step_avg:98.78ms
step:1321/1770 train_time:129500ms step_avg:98.78ms
step:1322/1770 train_time:129602ms step_avg:98.78ms
step:1323/1770 train_time:129706ms step_avg:98.79ms
step:1324/1770 train_time:129809ms step_avg:98.79ms
step:1325/1770 train_time:129914ms step_avg:98.79ms
step:1326/1770 train_time:130016ms step_avg:98.80ms
step:1327/1770 train_time:130123ms step_avg:98.80ms
step:1328/1770 train_time:130224ms step_avg:98.80ms
step:1329/1770 train_time:130327ms step_avg:98.81ms
step:1330/1770 train_time:130429ms step_avg:98.81ms
step:1331/1770 train_time:130532ms step_avg:98.81ms
step:1332/1770 train_time:130635ms step_avg:98.82ms
step:1333/1770 train_time:130738ms step_avg:98.82ms
step:1334/1770 train_time:130840ms step_avg:98.82ms
step:1335/1770 train_time:130942ms step_avg:98.82ms
step:1336/1770 train_time:131044ms step_avg:98.83ms
step:1337/1770 train_time:131146ms step_avg:98.83ms
step:1338/1770 train_time:131249ms step_avg:98.83ms
step:1339/1770 train_time:131352ms step_avg:98.84ms
step:1340/1770 train_time:131457ms step_avg:98.84ms
step:1341/1770 train_time:131559ms step_avg:98.84ms
step:1342/1770 train_time:131663ms step_avg:98.85ms
step:1343/1770 train_time:131766ms step_avg:98.85ms
step:1344/1770 train_time:131869ms step_avg:98.85ms
step:1345/1770 train_time:131971ms step_avg:98.85ms
step:1346/1770 train_time:132074ms step_avg:98.86ms
step:1347/1770 train_time:132177ms step_avg:98.86ms
step:1348/1770 train_time:132282ms step_avg:98.87ms
step:1349/1770 train_time:132385ms step_avg:98.87ms
step:1350/1770 train_time:132487ms step_avg:98.87ms
step:1351/1770 train_time:132590ms step_avg:98.87ms
step:1352/1770 train_time:132692ms step_avg:98.88ms
step:1353/1770 train_time:132796ms step_avg:98.88ms
step:1354/1770 train_time:132898ms step_avg:98.88ms
step:1355/1770 train_time:133001ms step_avg:98.89ms
step:1356/1770 train_time:133103ms step_avg:98.89ms
step:1357/1770 train_time:133206ms step_avg:98.89ms
step:1358/1770 train_time:133309ms step_avg:98.89ms
step:1359/1770 train_time:133413ms step_avg:98.90ms
step:1360/1770 train_time:133516ms step_avg:98.90ms
step:1361/1770 train_time:133619ms step_avg:98.90ms
step:1362/1770 train_time:133721ms step_avg:98.91ms
step:1363/1770 train_time:133825ms step_avg:98.91ms
step:1364/1770 train_time:133928ms step_avg:98.91ms
step:1365/1770 train_time:134032ms step_avg:98.92ms
step:1366/1770 train_time:134134ms step_avg:98.92ms
step:1367/1770 train_time:134237ms step_avg:98.92ms
step:1368/1770 train_time:134340ms step_avg:98.92ms
step:1369/1770 train_time:134443ms step_avg:98.93ms
step:1370/1770 train_time:134545ms step_avg:98.93ms
step:1371/1770 train_time:134648ms step_avg:98.93ms
step:1372/1770 train_time:134750ms step_avg:98.94ms
step:1373/1770 train_time:134853ms step_avg:98.94ms
step:1374/1770 train_time:134957ms step_avg:98.94ms
step:1375/1770 train_time:135061ms step_avg:98.95ms
step:1375/1770 val_loss:3.3870 train_time:135162ms step_avg:99.02ms
step:1376/1770 train_time:135184ms step_avg:98.96ms
step:1377/1770 train_time:135273ms step_avg:98.96ms
step:1378/1770 train_time:135377ms step_avg:98.96ms
step:1379/1770 train_time:135479ms step_avg:98.96ms
step:1380/1770 train_time:135582ms step_avg:98.96ms
step:1381/1770 train_time:135685ms step_avg:98.97ms
step:1382/1770 train_time:135788ms step_avg:98.97ms
step:1383/1770 train_time:135891ms step_avg:98.97ms
step:1384/1770 train_time:135994ms step_avg:98.98ms
step:1385/1770 train_time:136097ms step_avg:98.98ms
step:1386/1770 train_time:136200ms step_avg:98.98ms
step:1387/1770 train_time:136302ms step_avg:98.99ms
step:1388/1770 train_time:136405ms step_avg:98.99ms
step:1389/1770 train_time:136508ms step_avg:98.99ms
step:1390/1770 train_time:136610ms step_avg:98.99ms
step:1391/1770 train_time:136713ms step_avg:99.00ms
step:1392/1770 train_time:136817ms step_avg:99.00ms
step:1393/1770 train_time:136919ms step_avg:99.00ms
step:1394/1770 train_time:137021ms step_avg:99.00ms
step:1395/1770 train_time:137125ms step_avg:99.01ms
step:1396/1770 train_time:137229ms step_avg:99.01ms
step:1397/1770 train_time:137332ms step_avg:99.01ms
step:1398/1770 train_time:137435ms step_avg:99.02ms
step:1399/1770 train_time:137538ms step_avg:99.02ms
step:1400/1770 train_time:137640ms step_avg:99.02ms
step:1401/1770 train_time:137742ms step_avg:99.02ms
step:1402/1770 train_time:137845ms step_avg:99.03ms
step:1403/1770 train_time:137948ms step_avg:99.03ms
step:1404/1770 train_time:138051ms step_avg:99.03ms
step:1405/1770 train_time:138154ms step_avg:99.03ms
step:1406/1770 train_time:138257ms step_avg:99.04ms
step:1407/1770 train_time:138359ms step_avg:99.04ms
step:1408/1770 train_time:138462ms step_avg:99.04ms
step:1409/1770 train_time:138564ms step_avg:99.05ms
step:1410/1770 train_time:138667ms step_avg:99.05ms
step:1411/1770 train_time:138770ms step_avg:99.05ms
step:1412/1770 train_time:138872ms step_avg:99.05ms
step:1413/1770 train_time:138975ms step_avg:99.06ms
step:1414/1770 train_time:139079ms step_avg:99.06ms
step:1415/1770 train_time:139181ms step_avg:99.06ms
step:1416/1770 train_time:139285ms step_avg:99.06ms
step:1417/1770 train_time:139387ms step_avg:99.07ms
step:1418/1770 train_time:139489ms step_avg:99.07ms
step:1419/1770 train_time:139593ms step_avg:99.07ms
step:1420/1770 train_time:139696ms step_avg:99.08ms
step:1421/1770 train_time:139799ms step_avg:99.08ms
step:1422/1770 train_time:139901ms step_avg:99.08ms
step:1423/1770 train_time:140003ms step_avg:99.08ms
step:1424/1770 train_time:140107ms step_avg:99.09ms
step:1425/1770 train_time:140209ms step_avg:99.09ms
step:1426/1770 train_time:140313ms step_avg:99.09ms
step:1427/1770 train_time:140415ms step_avg:99.09ms
step:1428/1770 train_time:140519ms step_avg:99.10ms
step:1429/1770 train_time:140621ms step_avg:99.10ms
step:1430/1770 train_time:140723ms step_avg:99.10ms
step:1431/1770 train_time:140827ms step_avg:99.10ms
step:1432/1770 train_time:140929ms step_avg:99.11ms
step:1433/1770 train_time:141031ms step_avg:99.11ms
step:1434/1770 train_time:141134ms step_avg:99.11ms
step:1435/1770 train_time:141236ms step_avg:99.11ms
step:1436/1770 train_time:141341ms step_avg:99.12ms
step:1437/1770 train_time:141444ms step_avg:99.12ms
step:1438/1770 train_time:141546ms step_avg:99.12ms
step:1439/1770 train_time:141649ms step_avg:99.12ms
step:1440/1770 train_time:141753ms step_avg:99.13ms
step:1441/1770 train_time:141859ms step_avg:99.13ms
step:1442/1770 train_time:141961ms step_avg:99.13ms
step:1443/1770 train_time:142064ms step_avg:99.14ms
step:1444/1770 train_time:142167ms step_avg:99.14ms
step:1445/1770 train_time:142272ms step_avg:99.14ms
step:1446/1770 train_time:142376ms step_avg:99.15ms
step:1447/1770 train_time:142480ms step_avg:99.15ms
step:1448/1770 train_time:142584ms step_avg:99.15ms
step:1449/1770 train_time:142688ms step_avg:99.16ms
step:1450/1770 train_time:142791ms step_avg:99.16ms
step:1451/1770 train_time:142896ms step_avg:99.16ms
step:1452/1770 train_time:142999ms step_avg:99.17ms
step:1453/1770 train_time:143102ms step_avg:99.17ms
step:1454/1770 train_time:143205ms step_avg:99.17ms
step:1455/1770 train_time:143311ms step_avg:99.18ms
step:1456/1770 train_time:143415ms step_avg:99.18ms
step:1457/1770 train_time:143519ms step_avg:99.18ms
step:1458/1770 train_time:143622ms step_avg:99.19ms
step:1459/1770 train_time:143727ms step_avg:99.19ms
step:1460/1770 train_time:143831ms step_avg:99.19ms
step:1461/1770 train_time:143935ms step_avg:99.20ms
step:1462/1770 train_time:144039ms step_avg:99.20ms
step:1463/1770 train_time:144142ms step_avg:99.20ms
step:1464/1770 train_time:144247ms step_avg:99.21ms
step:1465/1770 train_time:144351ms step_avg:99.21ms
step:1466/1770 train_time:144455ms step_avg:99.21ms
step:1467/1770 train_time:144560ms step_avg:99.22ms
step:1468/1770 train_time:144664ms step_avg:99.22ms
step:1469/1770 train_time:144768ms step_avg:99.22ms
step:1470/1770 train_time:144871ms step_avg:99.23ms
step:1471/1770 train_time:144975ms step_avg:99.23ms
step:1472/1770 train_time:145079ms step_avg:99.23ms
step:1473/1770 train_time:145182ms step_avg:99.24ms
step:1474/1770 train_time:145287ms step_avg:99.24ms
step:1475/1770 train_time:145391ms step_avg:99.24ms
step:1476/1770 train_time:145495ms step_avg:99.25ms
step:1477/1770 train_time:145600ms step_avg:99.25ms
step:1478/1770 train_time:145705ms step_avg:99.25ms
step:1479/1770 train_time:145808ms step_avg:99.26ms
step:1480/1770 train_time:145912ms step_avg:99.26ms
step:1481/1770 train_time:146020ms step_avg:99.27ms
step:1482/1770 train_time:146124ms step_avg:99.27ms
step:1483/1770 train_time:146228ms step_avg:99.27ms
step:1484/1770 train_time:146331ms step_avg:99.28ms
step:1485/1770 train_time:146436ms step_avg:99.28ms
step:1486/1770 train_time:146539ms step_avg:99.28ms
step:1487/1770 train_time:146642ms step_avg:99.28ms
step:1488/1770 train_time:146747ms step_avg:99.29ms
step:1489/1770 train_time:146852ms step_avg:99.29ms
step:1490/1770 train_time:146957ms step_avg:99.30ms
step:1491/1770 train_time:147060ms step_avg:99.30ms
step:1492/1770 train_time:147164ms step_avg:99.30ms
step:1493/1770 train_time:147271ms step_avg:99.31ms
step:1494/1770 train_time:147379ms step_avg:99.31ms
step:1495/1770 train_time:147482ms step_avg:99.31ms
step:1496/1770 train_time:147585ms step_avg:99.32ms
step:1497/1770 train_time:147689ms step_avg:99.32ms
step:1498/1770 train_time:147793ms step_avg:99.32ms
step:1499/1770 train_time:147896ms step_avg:99.33ms
step:1500/1770 train_time:147999ms step_avg:99.33ms
step:1500/1770 val_loss:3.3519 train_time:148100ms step_avg:99.40ms
step:1501/1770 train_time:148121ms step_avg:99.34ms
step:1502/1770 train_time:148215ms step_avg:99.34ms
step:1503/1770 train_time:148317ms step_avg:99.34ms
step:1504/1770 train_time:148421ms step_avg:99.34ms
step:1505/1770 train_time:148528ms step_avg:99.35ms
step:1506/1770 train_time:148632ms step_avg:99.35ms
step:1507/1770 train_time:148736ms step_avg:99.36ms
step:1508/1770 train_time:148841ms step_avg:99.36ms
step:1509/1770 train_time:148945ms step_avg:99.36ms
step:1510/1770 train_time:149049ms step_avg:99.37ms
step:1511/1770 train_time:149154ms step_avg:99.37ms
step:1512/1770 train_time:149258ms step_avg:99.37ms
step:1513/1770 train_time:149362ms step_avg:99.38ms
step:1514/1770 train_time:149466ms step_avg:99.38ms
step:1515/1770 train_time:149571ms step_avg:99.38ms
step:1516/1770 train_time:149675ms step_avg:99.39ms
step:1517/1770 train_time:149778ms step_avg:99.39ms
step:1518/1770 train_time:149884ms step_avg:99.39ms
step:1519/1770 train_time:149988ms step_avg:99.40ms
step:1520/1770 train_time:150093ms step_avg:99.40ms
step:1521/1770 train_time:150197ms step_avg:99.40ms
step:1522/1770 train_time:150301ms step_avg:99.41ms
step:1523/1770 train_time:150406ms step_avg:99.41ms
step:1524/1770 train_time:150509ms step_avg:99.41ms
step:1525/1770 train_time:150613ms step_avg:99.41ms
step:1526/1770 train_time:150717ms step_avg:99.42ms
step:1527/1770 train_time:150820ms step_avg:99.42ms
step:1528/1770 train_time:150926ms step_avg:99.42ms
step:1529/1770 train_time:151030ms step_avg:99.43ms
step:1530/1770 train_time:151134ms step_avg:99.43ms
step:1531/1770 train_time:151238ms step_avg:99.43ms
step:1532/1770 train_time:151343ms step_avg:99.44ms
step:1533/1770 train_time:151448ms step_avg:99.44ms
step:1534/1770 train_time:151553ms step_avg:99.44ms
step:1535/1770 train_time:151655ms step_avg:99.45ms
step:1536/1770 train_time:151758ms step_avg:99.45ms
step:1537/1770 train_time:151863ms step_avg:99.45ms
step:1538/1770 train_time:151968ms step_avg:99.46ms
step:1539/1770 train_time:152073ms step_avg:99.46ms
step:1540/1770 train_time:152179ms step_avg:99.46ms
step:1541/1770 train_time:152285ms step_avg:99.47ms
step:1542/1770 train_time:152389ms step_avg:99.47ms
step:1543/1770 train_time:152493ms step_avg:99.47ms
step:1544/1770 train_time:152600ms step_avg:99.48ms
step:1545/1770 train_time:152703ms step_avg:99.48ms
step:1546/1770 train_time:152808ms step_avg:99.48ms
step:1547/1770 train_time:152913ms step_avg:99.49ms
step:1548/1770 train_time:153016ms step_avg:99.49ms
step:1549/1770 train_time:153119ms step_avg:99.49ms
step:1550/1770 train_time:153224ms step_avg:99.50ms
step:1551/1770 train_time:153328ms step_avg:99.50ms
step:1552/1770 train_time:153433ms step_avg:99.50ms
step:1553/1770 train_time:153537ms step_avg:99.51ms
step:1554/1770 train_time:153640ms step_avg:99.51ms
step:1555/1770 train_time:153746ms step_avg:99.51ms
step:1556/1770 train_time:153849ms step_avg:99.51ms
step:1557/1770 train_time:153953ms step_avg:99.52ms
step:1558/1770 train_time:154057ms step_avg:99.52ms
step:1559/1770 train_time:154160ms step_avg:99.52ms
step:1560/1770 train_time:154264ms step_avg:99.52ms
step:1561/1770 train_time:154370ms step_avg:99.53ms
step:1562/1770 train_time:154474ms step_avg:99.53ms
step:1563/1770 train_time:154577ms step_avg:99.53ms
step:1564/1770 train_time:154680ms step_avg:99.54ms
step:1565/1770 train_time:154785ms step_avg:99.54ms
step:1566/1770 train_time:154888ms step_avg:99.54ms
step:1567/1770 train_time:154993ms step_avg:99.55ms
step:1568/1770 train_time:155096ms step_avg:99.55ms
step:1569/1770 train_time:155204ms step_avg:99.55ms
step:1570/1770 train_time:155308ms step_avg:99.56ms
step:1571/1770 train_time:155411ms step_avg:99.56ms
step:1572/1770 train_time:155516ms step_avg:99.56ms
step:1573/1770 train_time:155623ms step_avg:99.57ms
step:1574/1770 train_time:155726ms step_avg:99.57ms
step:1575/1770 train_time:155829ms step_avg:99.57ms
step:1576/1770 train_time:155933ms step_avg:99.57ms
step:1577/1770 train_time:156039ms step_avg:99.58ms
step:1578/1770 train_time:156145ms step_avg:99.58ms
step:1579/1770 train_time:156250ms step_avg:99.59ms
step:1580/1770 train_time:156353ms step_avg:99.59ms
step:1581/1770 train_time:156459ms step_avg:99.59ms
step:1582/1770 train_time:156564ms step_avg:99.60ms
step:1583/1770 train_time:156668ms step_avg:99.60ms
step:1584/1770 train_time:156773ms step_avg:99.60ms
step:1585/1770 train_time:156877ms step_avg:99.60ms
step:1586/1770 train_time:156984ms step_avg:99.61ms
step:1587/1770 train_time:157089ms step_avg:99.61ms
step:1588/1770 train_time:157194ms step_avg:99.62ms
step:1589/1770 train_time:157299ms step_avg:99.62ms
step:1590/1770 train_time:157403ms step_avg:99.62ms
step:1591/1770 train_time:157506ms step_avg:99.62ms
step:1592/1770 train_time:157610ms step_avg:99.63ms
step:1593/1770 train_time:157714ms step_avg:99.63ms
step:1594/1770 train_time:157818ms step_avg:99.63ms
step:1595/1770 train_time:157922ms step_avg:99.64ms
step:1596/1770 train_time:158027ms step_avg:99.64ms
step:1597/1770 train_time:158131ms step_avg:99.64ms
step:1598/1770 train_time:158235ms step_avg:99.64ms
step:1599/1770 train_time:158340ms step_avg:99.65ms
step:1600/1770 train_time:158447ms step_avg:99.65ms
step:1601/1770 train_time:158552ms step_avg:99.66ms
step:1602/1770 train_time:158656ms step_avg:99.66ms
step:1603/1770 train_time:158760ms step_avg:99.66ms
step:1604/1770 train_time:158863ms step_avg:99.66ms
step:1605/1770 train_time:158966ms step_avg:99.67ms
step:1606/1770 train_time:159071ms step_avg:99.67ms
step:1607/1770 train_time:159178ms step_avg:99.67ms
step:1608/1770 train_time:159282ms step_avg:99.68ms
step:1609/1770 train_time:159387ms step_avg:99.68ms
step:1610/1770 train_time:159493ms step_avg:99.68ms
step:1611/1770 train_time:159599ms step_avg:99.69ms
step:1612/1770 train_time:159704ms step_avg:99.69ms
step:1613/1770 train_time:159808ms step_avg:99.69ms
step:1614/1770 train_time:159912ms step_avg:99.70ms
step:1615/1770 train_time:160016ms step_avg:99.70ms
step:1616/1770 train_time:160120ms step_avg:99.70ms
step:1617/1770 train_time:160226ms step_avg:99.71ms
step:1618/1770 train_time:160331ms step_avg:99.71ms
step:1619/1770 train_time:160436ms step_avg:99.71ms
step:1620/1770 train_time:160541ms step_avg:99.71ms
step:1621/1770 train_time:160645ms step_avg:99.72ms
step:1622/1770 train_time:160751ms step_avg:99.72ms
step:1623/1770 train_time:160858ms step_avg:99.73ms
step:1624/1770 train_time:160962ms step_avg:99.73ms
step:1625/1770 train_time:161065ms step_avg:99.73ms
step:1625/1770 val_loss:3.3203 train_time:161167ms step_avg:99.79ms
step:1626/1770 train_time:161189ms step_avg:99.75ms
step:1627/1770 train_time:161278ms step_avg:99.74ms
step:1628/1770 train_time:161382ms step_avg:99.74ms
step:1629/1770 train_time:161486ms step_avg:99.74ms
step:1630/1770 train_time:161590ms step_avg:99.75ms
step:1631/1770 train_time:161694ms step_avg:99.75ms
step:1632/1770 train_time:161798ms step_avg:99.75ms
step:1633/1770 train_time:161902ms step_avg:99.75ms
step:1634/1770 train_time:162006ms step_avg:99.76ms
step:1635/1770 train_time:162110ms step_avg:99.76ms
step:1636/1770 train_time:162214ms step_avg:99.76ms
step:1637/1770 train_time:162319ms step_avg:99.77ms
step:1638/1770 train_time:162423ms step_avg:99.77ms
step:1639/1770 train_time:162527ms step_avg:99.77ms
step:1640/1770 train_time:162631ms step_avg:99.77ms
step:1641/1770 train_time:162735ms step_avg:99.78ms
step:1642/1770 train_time:162838ms step_avg:99.78ms
step:1643/1770 train_time:162942ms step_avg:99.78ms
step:1644/1770 train_time:163048ms step_avg:99.78ms
step:1645/1770 train_time:163151ms step_avg:99.79ms
step:1646/1770 train_time:163257ms step_avg:99.79ms
step:1647/1770 train_time:163362ms step_avg:99.79ms
step:1648/1770 train_time:163465ms step_avg:99.80ms
step:1649/1770 train_time:163569ms step_avg:99.80ms
step:1650/1770 train_time:163673ms step_avg:99.80ms
step:1651/1770 train_time:163776ms step_avg:99.80ms
step:1652/1770 train_time:163880ms step_avg:99.80ms
step:1653/1770 train_time:163984ms step_avg:99.81ms
step:1654/1770 train_time:164092ms step_avg:99.81ms
step:1655/1770 train_time:164199ms step_avg:99.82ms
step:1656/1770 train_time:164303ms step_avg:99.82ms
step:1657/1770 train_time:164410ms step_avg:99.82ms
step:1658/1770 train_time:164513ms step_avg:99.83ms
step:1659/1770 train_time:164619ms step_avg:99.83ms
step:1660/1770 train_time:164723ms step_avg:99.83ms
step:1661/1770 train_time:164828ms step_avg:99.84ms
step:1662/1770 train_time:164932ms step_avg:99.84ms
step:1663/1770 train_time:165035ms step_avg:99.84ms
step:1664/1770 train_time:165139ms step_avg:99.84ms
step:1665/1770 train_time:165242ms step_avg:99.84ms
step:1666/1770 train_time:165348ms step_avg:99.85ms
step:1667/1770 train_time:165451ms step_avg:99.85ms
step:1668/1770 train_time:165554ms step_avg:99.85ms
step:1669/1770 train_time:165656ms step_avg:99.85ms
step:1670/1770 train_time:165760ms step_avg:99.86ms
step:1671/1770 train_time:165865ms step_avg:99.86ms
step:1672/1770 train_time:165970ms step_avg:99.86ms
step:1673/1770 train_time:166074ms step_avg:99.86ms
step:1674/1770 train_time:166178ms step_avg:99.87ms
step:1675/1770 train_time:166282ms step_avg:99.87ms
step:1676/1770 train_time:166387ms step_avg:99.87ms
step:1677/1770 train_time:166496ms step_avg:99.88ms
step:1678/1770 train_time:166598ms step_avg:99.88ms
step:1679/1770 train_time:166703ms step_avg:99.88ms
step:1680/1770 train_time:166807ms step_avg:99.88ms
step:1681/1770 train_time:166912ms step_avg:99.89ms
step:1682/1770 train_time:167018ms step_avg:99.89ms
step:1683/1770 train_time:167121ms step_avg:99.89ms
step:1684/1770 train_time:167225ms step_avg:99.90ms
step:1685/1770 train_time:167330ms step_avg:99.90ms
step:1686/1770 train_time:167435ms step_avg:99.90ms
step:1687/1770 train_time:167540ms step_avg:99.90ms
step:1688/1770 train_time:167644ms step_avg:99.91ms
step:1689/1770 train_time:167748ms step_avg:99.91ms
step:1690/1770 train_time:167852ms step_avg:99.91ms
step:1691/1770 train_time:167955ms step_avg:99.91ms
step:1692/1770 train_time:168060ms step_avg:99.92ms
step:1693/1770 train_time:168166ms step_avg:99.92ms
step:1694/1770 train_time:168270ms step_avg:99.92ms
step:1695/1770 train_time:168374ms step_avg:99.93ms
step:1696/1770 train_time:168480ms step_avg:99.93ms
step:1697/1770 train_time:168586ms step_avg:99.93ms
step:1698/1770 train_time:168691ms step_avg:99.94ms
step:1699/1770 train_time:168794ms step_avg:99.94ms
step:1700/1770 train_time:168898ms step_avg:99.94ms
step:1701/1770 train_time:169001ms step_avg:99.94ms
step:1702/1770 train_time:169106ms step_avg:99.94ms
step:1703/1770 train_time:169210ms step_avg:99.95ms
step:1704/1770 train_time:169314ms step_avg:99.95ms
step:1705/1770 train_time:169417ms step_avg:99.95ms
step:1706/1770 train_time:169521ms step_avg:99.95ms
step:1707/1770 train_time:169626ms step_avg:99.96ms
step:1708/1770 train_time:169731ms step_avg:99.96ms
step:1709/1770 train_time:169836ms step_avg:99.96ms
step:1710/1770 train_time:169944ms step_avg:99.97ms
step:1711/1770 train_time:170050ms step_avg:99.97ms
step:1712/1770 train_time:170155ms step_avg:99.97ms
step:1713/1770 train_time:170259ms step_avg:99.98ms
step:1714/1770 train_time:170364ms step_avg:99.98ms
step:1715/1770 train_time:170468ms step_avg:99.98ms
step:1716/1770 train_time:170573ms step_avg:99.98ms
step:1717/1770 train_time:170677ms step_avg:99.99ms
step:1718/1770 train_time:170783ms step_avg:99.99ms
step:1719/1770 train_time:170889ms step_avg:99.99ms
step:1720/1770 train_time:170995ms step_avg:100.00ms
step:1721/1770 train_time:171098ms step_avg:100.00ms
step:1722/1770 train_time:171206ms step_avg:100.00ms
step:1723/1770 train_time:171312ms step_avg:100.01ms
step:1724/1770 train_time:171419ms step_avg:100.01ms
step:1725/1770 train_time:171526ms step_avg:100.02ms
step:1726/1770 train_time:171633ms step_avg:100.02ms
step:1727/1770 train_time:171737ms step_avg:100.02ms
step:1728/1770 train_time:171844ms step_avg:100.03ms
step:1729/1770 train_time:171949ms step_avg:100.03ms
step:1730/1770 train_time:172055ms step_avg:100.03ms
step:1731/1770 train_time:172161ms step_avg:100.04ms
step:1732/1770 train_time:172266ms step_avg:100.04ms
step:1733/1770 train_time:172372ms step_avg:100.04ms
step:1734/1770 train_time:172476ms step_avg:100.04ms
step:1735/1770 train_time:172582ms step_avg:100.05ms
step:1736/1770 train_time:172686ms step_avg:100.05ms
step:1737/1770 train_time:172791ms step_avg:100.05ms
step:1738/1770 train_time:172896ms step_avg:100.06ms
step:1739/1770 train_time:173001ms step_avg:100.06ms
step:1740/1770 train_time:173106ms step_avg:100.06ms
step:1741/1770 train_time:173213ms step_avg:100.07ms
step:1742/1770 train_time:173321ms step_avg:100.07ms
step:1743/1770 train_time:173427ms step_avg:100.07ms
step:1744/1770 train_time:173532ms step_avg:100.08ms
step:1745/1770 train_time:173636ms step_avg:100.08ms
step:1746/1770 train_time:173743ms step_avg:100.08ms
step:1747/1770 train_time:173847ms step_avg:100.08ms
step:1748/1770 train_time:173954ms step_avg:100.09ms
step:1749/1770 train_time:174060ms step_avg:100.09ms
step:1750/1770 train_time:174165ms step_avg:100.09ms
step:1750/1770 val_loss:3.2967 train_time:174268ms step_avg:100.15ms
step:1751/1770 train_time:174289ms step_avg:100.11ms
step:1752/1770 train_time:174380ms step_avg:100.10ms
step:1753/1770 train_time:174484ms step_avg:100.11ms
step:1754/1770 train_time:174590ms step_avg:100.11ms
step:1755/1770 train_time:174694ms step_avg:100.11ms
step:1756/1770 train_time:174799ms step_avg:100.11ms
step:1757/1770 train_time:174905ms step_avg:100.12ms
step:1758/1770 train_time:175009ms step_avg:100.12ms
step:1759/1770 train_time:175115ms step_avg:100.12ms
step:1760/1770 train_time:175220ms step_avg:100.13ms
step:1761/1770 train_time:175327ms step_avg:100.13ms
step:1762/1770 train_time:175435ms step_avg:100.13ms
step:1763/1770 train_time:175538ms step_avg:100.14ms
step:1764/1770 train_time:175643ms step_avg:100.14ms
step:1765/1770 train_time:175748ms step_avg:100.14ms
step:1766/1770 train_time:175857ms step_avg:100.15ms
step:1767/1770 train_time:175961ms step_avg:100.15ms
step:1768/1770 train_time:176066ms step_avg:100.15ms
step:1769/1770 train_time:176170ms step_avg:100.15ms
step:1770/1770 train_time:176274ms step_avg:100.16ms
step:1770/1770 val_loss:3.2937 train_time:176379ms step_avg:100.22ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
