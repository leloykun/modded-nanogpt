import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 forward & backward by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# Custom operators : FP8 forward & bfloat16 backward

@torch.library.custom_op("nanogpt::mm_mixed", mutates_args=())
def mm_mixed_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_mixed_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_mixed_backward", mutates_args=())
def mm_mixed_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        x_bfloat16 = x_f8.to(torch.bfloat16)
        w_bfloat16 = w_f8.to(torch.bfloat16)
        grad_bfloat16 = grad.mul(grad_s).to(torch.bfloat16)
        grad_x = torch._scaled_mm(
            grad_bfloat16,
            w_bfloat16.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        grad_w = torch._scaled_mm(
            x_bfloat16.t().contiguous(),
            grad_bfloat16.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_mixed_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_mixed_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_mixed_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_mixed_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_mixed_op.register_autograd(mm_mixed_backward, setup_context=mm_mixed_setup_context)

def linear_mixed(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm_mixed(_x, w, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        # x = linear_mixed(x, self.c_fc.weight)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 22:09:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:25117ms step_avg:nanms
step:2/1770 train_time:25541ms step_avg:nanms
step:3/1770 train_time:25636ms step_avg:nanms
step:4/1770 train_time:25729ms step_avg:nanms
step:5/1770 train_time:25823ms step_avg:nanms
step:6/1770 train_time:25918ms step_avg:nanms
step:7/1770 train_time:26013ms step_avg:nanms
step:8/1770 train_time:26106ms step_avg:nanms
step:9/1770 train_time:26200ms step_avg:nanms
step:10/1770 train_time:26295ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.29ms
step:14/1770 train_time:378ms step_avg:94.43ms
step:15/1770 train_time:472ms step_avg:94.33ms
step:16/1770 train_time:566ms step_avg:94.29ms
step:17/1770 train_time:660ms step_avg:94.32ms
step:18/1770 train_time:755ms step_avg:94.41ms
step:19/1770 train_time:849ms step_avg:94.38ms
step:20/1770 train_time:945ms step_avg:94.46ms
step:21/1770 train_time:1039ms step_avg:94.44ms
step:22/1770 train_time:1133ms step_avg:94.40ms
step:23/1770 train_time:1227ms step_avg:94.41ms
step:24/1770 train_time:1322ms step_avg:94.45ms
step:25/1770 train_time:1416ms step_avg:94.43ms
step:26/1770 train_time:1511ms step_avg:94.42ms
step:27/1770 train_time:1605ms step_avg:94.43ms
step:28/1770 train_time:1701ms step_avg:94.48ms
step:29/1770 train_time:1794ms step_avg:94.44ms
step:30/1770 train_time:1888ms step_avg:94.42ms
step:31/1770 train_time:1984ms step_avg:94.49ms
step:32/1770 train_time:2078ms step_avg:94.45ms
step:33/1770 train_time:2172ms step_avg:94.43ms
step:34/1770 train_time:2266ms step_avg:94.41ms
step:35/1770 train_time:2360ms step_avg:94.41ms
step:36/1770 train_time:2455ms step_avg:94.41ms
step:37/1770 train_time:2549ms step_avg:94.40ms
step:38/1770 train_time:2643ms step_avg:94.41ms
step:39/1770 train_time:2738ms step_avg:94.42ms
step:40/1770 train_time:2832ms step_avg:94.41ms
step:41/1770 train_time:2927ms step_avg:94.41ms
step:42/1770 train_time:3022ms step_avg:94.43ms
step:43/1770 train_time:3116ms step_avg:94.44ms
step:44/1770 train_time:3210ms step_avg:94.43ms
step:45/1770 train_time:3305ms step_avg:94.42ms
step:46/1770 train_time:3400ms step_avg:94.44ms
step:47/1770 train_time:3494ms step_avg:94.44ms
step:48/1770 train_time:3589ms step_avg:94.44ms
step:49/1770 train_time:3683ms step_avg:94.44ms
step:50/1770 train_time:3778ms step_avg:94.45ms
step:51/1770 train_time:3872ms step_avg:94.44ms
step:52/1770 train_time:3966ms step_avg:94.44ms
step:53/1770 train_time:4062ms step_avg:94.45ms
step:54/1770 train_time:4156ms step_avg:94.47ms
step:55/1770 train_time:4251ms step_avg:94.46ms
step:56/1770 train_time:4346ms step_avg:94.47ms
step:57/1770 train_time:4441ms step_avg:94.49ms
step:58/1770 train_time:4536ms step_avg:94.49ms
step:59/1770 train_time:4630ms step_avg:94.48ms
step:60/1770 train_time:4725ms step_avg:94.49ms
step:61/1770 train_time:4819ms step_avg:94.50ms
step:62/1770 train_time:4914ms step_avg:94.49ms
step:63/1770 train_time:5008ms step_avg:94.48ms
step:64/1770 train_time:5102ms step_avg:94.48ms
step:65/1770 train_time:5197ms step_avg:94.48ms
step:66/1770 train_time:5291ms step_avg:94.48ms
step:67/1770 train_time:5385ms step_avg:94.48ms
step:68/1770 train_time:5480ms step_avg:94.48ms
step:69/1770 train_time:5575ms step_avg:94.49ms
step:70/1770 train_time:5669ms step_avg:94.48ms
step:71/1770 train_time:5763ms step_avg:94.48ms
step:72/1770 train_time:5857ms step_avg:94.47ms
step:73/1770 train_time:5952ms step_avg:94.47ms
step:74/1770 train_time:6046ms step_avg:94.46ms
step:75/1770 train_time:6141ms step_avg:94.47ms
step:76/1770 train_time:6235ms step_avg:94.47ms
step:77/1770 train_time:6329ms step_avg:94.46ms
step:78/1770 train_time:6424ms step_avg:94.48ms
step:79/1770 train_time:6518ms step_avg:94.47ms
step:80/1770 train_time:6613ms step_avg:94.47ms
step:81/1770 train_time:6706ms step_avg:94.45ms
step:82/1770 train_time:6801ms step_avg:94.46ms
step:83/1770 train_time:6895ms step_avg:94.45ms
step:84/1770 train_time:6989ms step_avg:94.45ms
step:85/1770 train_time:7084ms step_avg:94.45ms
step:86/1770 train_time:7178ms step_avg:94.45ms
step:87/1770 train_time:7272ms step_avg:94.45ms
step:88/1770 train_time:7367ms step_avg:94.45ms
step:89/1770 train_time:7461ms step_avg:94.45ms
step:90/1770 train_time:7555ms step_avg:94.44ms
step:91/1770 train_time:7650ms step_avg:94.44ms
step:92/1770 train_time:7744ms step_avg:94.44ms
step:93/1770 train_time:7838ms step_avg:94.44ms
step:94/1770 train_time:7933ms step_avg:94.44ms
step:95/1770 train_time:8027ms step_avg:94.44ms
step:96/1770 train_time:8121ms step_avg:94.43ms
step:97/1770 train_time:8216ms step_avg:94.43ms
step:98/1770 train_time:8310ms step_avg:94.43ms
step:99/1770 train_time:8405ms step_avg:94.43ms
step:100/1770 train_time:8499ms step_avg:94.43ms
step:101/1770 train_time:8593ms step_avg:94.43ms
step:102/1770 train_time:8687ms step_avg:94.43ms
step:103/1770 train_time:8782ms step_avg:94.43ms
step:104/1770 train_time:8877ms step_avg:94.43ms
step:105/1770 train_time:8971ms step_avg:94.43ms
step:106/1770 train_time:9065ms step_avg:94.43ms
step:107/1770 train_time:9159ms step_avg:94.43ms
step:108/1770 train_time:9253ms step_avg:94.42ms
step:109/1770 train_time:9348ms step_avg:94.42ms
step:110/1770 train_time:9442ms step_avg:94.42ms
step:111/1770 train_time:9537ms step_avg:94.43ms
step:112/1770 train_time:9631ms step_avg:94.42ms
step:113/1770 train_time:9725ms step_avg:94.42ms
step:114/1770 train_time:9820ms step_avg:94.42ms
step:115/1770 train_time:9914ms step_avg:94.42ms
step:116/1770 train_time:10008ms step_avg:94.42ms
step:117/1770 train_time:10103ms step_avg:94.42ms
step:118/1770 train_time:10197ms step_avg:94.42ms
step:119/1770 train_time:10291ms step_avg:94.41ms
step:120/1770 train_time:10385ms step_avg:94.41ms
step:121/1770 train_time:10480ms step_avg:94.41ms
step:122/1770 train_time:10574ms step_avg:94.41ms
step:123/1770 train_time:10668ms step_avg:94.41ms
step:124/1770 train_time:10762ms step_avg:94.41ms
step:125/1770 train_time:10857ms step_avg:94.41ms
step:125/1770 val_loss:4.6539 train_time:10949ms step_avg:95.21ms
step:126/1770 train_time:10975ms step_avg:94.61ms
step:127/1770 train_time:11049ms step_avg:94.44ms
step:128/1770 train_time:11147ms step_avg:94.46ms
step:129/1770 train_time:11248ms step_avg:94.52ms
step:130/1770 train_time:11344ms step_avg:94.53ms
step:131/1770 train_time:11438ms step_avg:94.53ms
step:132/1770 train_time:11533ms step_avg:94.53ms
step:133/1770 train_time:11626ms step_avg:94.52ms
step:134/1770 train_time:11721ms step_avg:94.53ms
step:135/1770 train_time:11817ms step_avg:94.54ms
step:136/1770 train_time:11912ms step_avg:94.54ms
step:137/1770 train_time:12006ms step_avg:94.54ms
step:138/1770 train_time:12102ms step_avg:94.55ms
step:139/1770 train_time:12197ms step_avg:94.55ms
step:140/1770 train_time:12292ms step_avg:94.55ms
step:141/1770 train_time:12387ms step_avg:94.56ms
step:142/1770 train_time:12482ms step_avg:94.56ms
step:143/1770 train_time:12578ms step_avg:94.57ms
step:144/1770 train_time:12673ms step_avg:94.57ms
step:145/1770 train_time:12767ms step_avg:94.57ms
step:146/1770 train_time:12862ms step_avg:94.58ms
step:147/1770 train_time:12958ms step_avg:94.58ms
step:148/1770 train_time:13052ms step_avg:94.58ms
step:149/1770 train_time:13147ms step_avg:94.58ms
step:150/1770 train_time:13242ms step_avg:94.59ms
step:151/1770 train_time:13337ms step_avg:94.59ms
step:152/1770 train_time:13433ms step_avg:94.60ms
step:153/1770 train_time:13527ms step_avg:94.60ms
step:154/1770 train_time:13623ms step_avg:94.60ms
step:155/1770 train_time:13718ms step_avg:94.61ms
step:156/1770 train_time:13813ms step_avg:94.61ms
step:157/1770 train_time:13908ms step_avg:94.61ms
step:158/1770 train_time:14002ms step_avg:94.61ms
step:159/1770 train_time:14098ms step_avg:94.62ms
step:160/1770 train_time:14193ms step_avg:94.62ms
step:161/1770 train_time:14288ms step_avg:94.62ms
step:162/1770 train_time:14383ms step_avg:94.62ms
step:163/1770 train_time:14479ms step_avg:94.63ms
step:164/1770 train_time:14574ms step_avg:94.64ms
step:165/1770 train_time:14668ms step_avg:94.63ms
step:166/1770 train_time:14763ms step_avg:94.63ms
step:167/1770 train_time:14858ms step_avg:94.64ms
step:168/1770 train_time:14953ms step_avg:94.64ms
step:169/1770 train_time:15048ms step_avg:94.64ms
step:170/1770 train_time:15143ms step_avg:94.64ms
step:171/1770 train_time:15238ms step_avg:94.65ms
step:172/1770 train_time:15333ms step_avg:94.65ms
step:173/1770 train_time:15428ms step_avg:94.65ms
step:174/1770 train_time:15523ms step_avg:94.65ms
step:175/1770 train_time:15618ms step_avg:94.66ms
step:176/1770 train_time:15713ms step_avg:94.66ms
step:177/1770 train_time:15808ms step_avg:94.66ms
step:178/1770 train_time:15902ms step_avg:94.66ms
step:179/1770 train_time:15998ms step_avg:94.66ms
step:180/1770 train_time:16093ms step_avg:94.66ms
step:181/1770 train_time:16188ms step_avg:94.67ms
step:182/1770 train_time:16283ms step_avg:94.67ms
step:183/1770 train_time:16378ms step_avg:94.67ms
step:184/1770 train_time:16473ms step_avg:94.67ms
step:185/1770 train_time:16568ms step_avg:94.67ms
step:186/1770 train_time:16662ms step_avg:94.67ms
step:187/1770 train_time:16758ms step_avg:94.68ms
step:188/1770 train_time:16853ms step_avg:94.68ms
step:189/1770 train_time:16948ms step_avg:94.68ms
step:190/1770 train_time:17043ms step_avg:94.68ms
step:191/1770 train_time:17139ms step_avg:94.69ms
step:192/1770 train_time:17234ms step_avg:94.69ms
step:193/1770 train_time:17330ms step_avg:94.70ms
step:194/1770 train_time:17424ms step_avg:94.70ms
step:195/1770 train_time:17519ms step_avg:94.70ms
step:196/1770 train_time:17614ms step_avg:94.70ms
step:197/1770 train_time:17709ms step_avg:94.70ms
step:198/1770 train_time:17804ms step_avg:94.70ms
step:199/1770 train_time:17899ms step_avg:94.70ms
step:200/1770 train_time:17995ms step_avg:94.71ms
step:201/1770 train_time:18089ms step_avg:94.71ms
step:202/1770 train_time:18184ms step_avg:94.71ms
step:203/1770 train_time:18279ms step_avg:94.71ms
step:204/1770 train_time:18375ms step_avg:94.72ms
step:205/1770 train_time:18470ms step_avg:94.72ms
step:206/1770 train_time:18565ms step_avg:94.72ms
step:207/1770 train_time:18660ms step_avg:94.72ms
step:208/1770 train_time:18755ms step_avg:94.72ms
step:209/1770 train_time:18850ms step_avg:94.72ms
step:210/1770 train_time:18945ms step_avg:94.72ms
step:211/1770 train_time:19040ms step_avg:94.73ms
step:212/1770 train_time:19135ms step_avg:94.73ms
step:213/1770 train_time:19230ms step_avg:94.73ms
step:214/1770 train_time:19325ms step_avg:94.73ms
step:215/1770 train_time:19420ms step_avg:94.73ms
step:216/1770 train_time:19515ms step_avg:94.73ms
step:217/1770 train_time:19610ms step_avg:94.73ms
step:218/1770 train_time:19705ms step_avg:94.74ms
step:219/1770 train_time:19800ms step_avg:94.74ms
step:220/1770 train_time:19895ms step_avg:94.74ms
step:221/1770 train_time:19990ms step_avg:94.74ms
step:222/1770 train_time:20085ms step_avg:94.74ms
step:223/1770 train_time:20180ms step_avg:94.74ms
step:224/1770 train_time:20274ms step_avg:94.74ms
step:225/1770 train_time:20369ms step_avg:94.74ms
step:226/1770 train_time:20464ms step_avg:94.74ms
step:227/1770 train_time:20559ms step_avg:94.74ms
step:228/1770 train_time:20655ms step_avg:94.75ms
step:229/1770 train_time:20750ms step_avg:94.75ms
step:230/1770 train_time:20844ms step_avg:94.75ms
step:231/1770 train_time:20939ms step_avg:94.75ms
step:232/1770 train_time:21035ms step_avg:94.75ms
step:233/1770 train_time:21129ms step_avg:94.75ms
step:234/1770 train_time:21225ms step_avg:94.76ms
step:235/1770 train_time:21320ms step_avg:94.76ms
step:236/1770 train_time:21416ms step_avg:94.76ms
step:237/1770 train_time:21512ms step_avg:94.77ms
step:238/1770 train_time:21606ms step_avg:94.76ms
step:239/1770 train_time:21701ms step_avg:94.76ms
step:240/1770 train_time:21796ms step_avg:94.77ms
step:241/1770 train_time:21891ms step_avg:94.77ms
step:242/1770 train_time:21986ms step_avg:94.77ms
step:243/1770 train_time:22081ms step_avg:94.77ms
step:244/1770 train_time:22176ms step_avg:94.77ms
step:245/1770 train_time:22271ms step_avg:94.77ms
step:246/1770 train_time:22366ms step_avg:94.77ms
step:247/1770 train_time:22461ms step_avg:94.77ms
step:248/1770 train_time:22558ms step_avg:94.78ms
step:249/1770 train_time:22654ms step_avg:94.79ms
step:250/1770 train_time:22749ms step_avg:94.79ms
step:250/1770 val_loss:4.1081 train_time:22842ms step_avg:95.18ms
step:251/1770 train_time:22865ms step_avg:94.87ms
step:252/1770 train_time:22949ms step_avg:94.83ms
step:253/1770 train_time:23047ms step_avg:94.84ms
step:254/1770 train_time:23141ms step_avg:94.84ms
step:255/1770 train_time:23236ms step_avg:94.84ms
step:256/1770 train_time:23332ms step_avg:94.84ms
step:257/1770 train_time:23426ms step_avg:94.84ms
step:258/1770 train_time:23522ms step_avg:94.85ms
step:259/1770 train_time:23618ms step_avg:94.85ms
step:260/1770 train_time:23712ms step_avg:94.85ms
step:261/1770 train_time:23807ms step_avg:94.85ms
step:262/1770 train_time:23902ms step_avg:94.85ms
step:263/1770 train_time:23998ms step_avg:94.85ms
step:264/1770 train_time:24093ms step_avg:94.85ms
step:265/1770 train_time:24188ms step_avg:94.86ms
step:266/1770 train_time:24284ms step_avg:94.86ms
step:267/1770 train_time:24380ms step_avg:94.86ms
step:268/1770 train_time:24475ms step_avg:94.87ms
step:269/1770 train_time:24571ms step_avg:94.87ms
step:270/1770 train_time:24667ms step_avg:94.87ms
step:271/1770 train_time:24763ms step_avg:94.88ms
step:272/1770 train_time:24859ms step_avg:94.88ms
step:273/1770 train_time:24954ms step_avg:94.88ms
step:274/1770 train_time:25050ms step_avg:94.88ms
step:275/1770 train_time:25145ms step_avg:94.89ms
step:276/1770 train_time:25241ms step_avg:94.89ms
step:277/1770 train_time:25336ms step_avg:94.89ms
step:278/1770 train_time:25431ms step_avg:94.89ms
step:279/1770 train_time:25527ms step_avg:94.90ms
step:280/1770 train_time:25623ms step_avg:94.90ms
step:281/1770 train_time:25719ms step_avg:94.90ms
step:282/1770 train_time:25814ms step_avg:94.90ms
step:283/1770 train_time:25910ms step_avg:94.91ms
step:284/1770 train_time:26006ms step_avg:94.91ms
step:285/1770 train_time:26102ms step_avg:94.92ms
step:286/1770 train_time:26197ms step_avg:94.92ms
step:287/1770 train_time:26293ms step_avg:94.92ms
step:288/1770 train_time:26388ms step_avg:94.92ms
step:289/1770 train_time:26484ms step_avg:94.93ms
step:290/1770 train_time:26580ms step_avg:94.93ms
step:291/1770 train_time:26675ms step_avg:94.93ms
step:292/1770 train_time:26770ms step_avg:94.93ms
step:293/1770 train_time:26866ms step_avg:94.93ms
step:294/1770 train_time:26961ms step_avg:94.93ms
step:295/1770 train_time:27056ms step_avg:94.93ms
step:296/1770 train_time:27152ms step_avg:94.94ms
step:297/1770 train_time:27248ms step_avg:94.94ms
step:298/1770 train_time:27343ms step_avg:94.94ms
step:299/1770 train_time:27438ms step_avg:94.94ms
step:300/1770 train_time:27534ms step_avg:94.94ms
step:301/1770 train_time:27629ms step_avg:94.95ms
step:302/1770 train_time:27725ms step_avg:94.95ms
step:303/1770 train_time:27821ms step_avg:94.95ms
step:304/1770 train_time:27916ms step_avg:94.95ms
step:305/1770 train_time:28011ms step_avg:94.95ms
step:306/1770 train_time:28107ms step_avg:94.96ms
step:307/1770 train_time:28203ms step_avg:94.96ms
step:308/1770 train_time:28298ms step_avg:94.96ms
step:309/1770 train_time:28393ms step_avg:94.96ms
step:310/1770 train_time:28489ms step_avg:94.96ms
step:311/1770 train_time:28584ms step_avg:94.96ms
step:312/1770 train_time:28680ms step_avg:94.97ms
step:313/1770 train_time:28775ms step_avg:94.97ms
step:314/1770 train_time:28870ms step_avg:94.97ms
step:315/1770 train_time:28966ms step_avg:94.97ms
step:316/1770 train_time:29061ms step_avg:94.97ms
step:317/1770 train_time:29156ms step_avg:94.97ms
step:318/1770 train_time:29252ms step_avg:94.97ms
step:319/1770 train_time:29348ms step_avg:94.98ms
step:320/1770 train_time:29443ms step_avg:94.98ms
step:321/1770 train_time:29538ms step_avg:94.98ms
step:322/1770 train_time:29633ms step_avg:94.98ms
step:323/1770 train_time:29729ms step_avg:94.98ms
step:324/1770 train_time:29825ms step_avg:94.98ms
step:325/1770 train_time:29921ms step_avg:94.99ms
step:326/1770 train_time:30016ms step_avg:94.99ms
step:327/1770 train_time:30112ms step_avg:94.99ms
step:328/1770 train_time:30208ms step_avg:94.99ms
step:329/1770 train_time:30304ms step_avg:95.00ms
step:330/1770 train_time:30399ms step_avg:95.00ms
step:331/1770 train_time:30495ms step_avg:95.00ms
step:332/1770 train_time:30590ms step_avg:95.00ms
step:333/1770 train_time:30686ms step_avg:95.00ms
step:334/1770 train_time:30782ms step_avg:95.01ms
step:335/1770 train_time:30877ms step_avg:95.01ms
step:336/1770 train_time:30973ms step_avg:95.01ms
step:337/1770 train_time:31069ms step_avg:95.01ms
step:338/1770 train_time:31164ms step_avg:95.01ms
step:339/1770 train_time:31259ms step_avg:95.01ms
step:340/1770 train_time:31354ms step_avg:95.01ms
step:341/1770 train_time:31450ms step_avg:95.02ms
step:342/1770 train_time:31546ms step_avg:95.02ms
step:343/1770 train_time:31641ms step_avg:95.02ms
step:344/1770 train_time:31737ms step_avg:95.02ms
step:345/1770 train_time:31832ms step_avg:95.02ms
step:346/1770 train_time:31928ms step_avg:95.03ms
step:347/1770 train_time:32025ms step_avg:95.03ms
step:348/1770 train_time:32119ms step_avg:95.03ms
step:349/1770 train_time:32215ms step_avg:95.03ms
step:350/1770 train_time:32310ms step_avg:95.03ms
step:351/1770 train_time:32406ms step_avg:95.03ms
step:352/1770 train_time:32502ms step_avg:95.04ms
step:353/1770 train_time:32597ms step_avg:95.04ms
step:354/1770 train_time:32693ms step_avg:95.04ms
step:355/1770 train_time:32789ms step_avg:95.04ms
step:356/1770 train_time:32885ms step_avg:95.04ms
step:357/1770 train_time:32982ms step_avg:95.05ms
step:358/1770 train_time:33078ms step_avg:95.05ms
step:359/1770 train_time:33173ms step_avg:95.05ms
step:360/1770 train_time:33268ms step_avg:95.05ms
step:361/1770 train_time:33364ms step_avg:95.06ms
step:362/1770 train_time:33460ms step_avg:95.06ms
step:363/1770 train_time:33555ms step_avg:95.06ms
step:364/1770 train_time:33651ms step_avg:95.06ms
step:365/1770 train_time:33747ms step_avg:95.06ms
step:366/1770 train_time:33843ms step_avg:95.06ms
step:367/1770 train_time:33938ms step_avg:95.06ms
step:368/1770 train_time:34033ms step_avg:95.06ms
step:369/1770 train_time:34130ms step_avg:95.07ms
step:370/1770 train_time:34226ms step_avg:95.07ms
step:371/1770 train_time:34322ms step_avg:95.07ms
step:372/1770 train_time:34418ms step_avg:95.08ms
step:373/1770 train_time:34513ms step_avg:95.08ms
step:374/1770 train_time:34608ms step_avg:95.08ms
step:375/1770 train_time:34704ms step_avg:95.08ms
step:375/1770 val_loss:3.9045 train_time:34798ms step_avg:95.34ms
step:376/1770 train_time:34821ms step_avg:95.14ms
step:377/1770 train_time:34905ms step_avg:95.11ms
step:378/1770 train_time:35002ms step_avg:95.11ms
step:379/1770 train_time:35097ms step_avg:95.11ms
step:380/1770 train_time:35193ms step_avg:95.12ms
step:381/1770 train_time:35288ms step_avg:95.12ms
step:382/1770 train_time:35383ms step_avg:95.12ms
step:383/1770 train_time:35478ms step_avg:95.12ms
step:384/1770 train_time:35574ms step_avg:95.12ms
step:385/1770 train_time:35670ms step_avg:95.12ms
step:386/1770 train_time:35764ms step_avg:95.12ms
step:387/1770 train_time:35859ms step_avg:95.12ms
step:388/1770 train_time:35955ms step_avg:95.12ms
step:389/1770 train_time:36051ms step_avg:95.12ms
step:390/1770 train_time:36147ms step_avg:95.12ms
step:391/1770 train_time:36242ms step_avg:95.12ms
step:392/1770 train_time:36337ms step_avg:95.12ms
step:393/1770 train_time:36433ms step_avg:95.12ms
step:394/1770 train_time:36529ms step_avg:95.13ms
step:395/1770 train_time:36624ms step_avg:95.13ms
step:396/1770 train_time:36721ms step_avg:95.13ms
step:397/1770 train_time:36818ms step_avg:95.14ms
step:398/1770 train_time:36916ms step_avg:95.14ms
step:399/1770 train_time:37014ms step_avg:95.15ms
step:400/1770 train_time:37112ms step_avg:95.16ms
step:401/1770 train_time:37210ms step_avg:95.17ms
step:402/1770 train_time:37307ms step_avg:95.17ms
step:403/1770 train_time:37404ms step_avg:95.18ms
step:404/1770 train_time:37502ms step_avg:95.18ms
step:405/1770 train_time:37599ms step_avg:95.19ms
step:406/1770 train_time:37696ms step_avg:95.19ms
step:407/1770 train_time:37793ms step_avg:95.20ms
step:408/1770 train_time:37891ms step_avg:95.20ms
step:409/1770 train_time:37988ms step_avg:95.21ms
step:410/1770 train_time:38085ms step_avg:95.21ms
step:411/1770 train_time:38182ms step_avg:95.22ms
step:412/1770 train_time:38279ms step_avg:95.22ms
step:413/1770 train_time:38376ms step_avg:95.23ms
step:414/1770 train_time:38474ms step_avg:95.23ms
step:415/1770 train_time:38572ms step_avg:95.24ms
step:416/1770 train_time:38669ms step_avg:95.24ms
step:417/1770 train_time:38767ms step_avg:95.25ms
step:418/1770 train_time:38864ms step_avg:95.25ms
step:419/1770 train_time:38960ms step_avg:95.26ms
step:420/1770 train_time:39058ms step_avg:95.26ms
step:421/1770 train_time:39156ms step_avg:95.27ms
step:422/1770 train_time:39253ms step_avg:95.28ms
step:423/1770 train_time:39351ms step_avg:95.28ms
step:424/1770 train_time:39449ms step_avg:95.29ms
step:425/1770 train_time:39546ms step_avg:95.29ms
step:426/1770 train_time:39643ms step_avg:95.30ms
step:427/1770 train_time:39740ms step_avg:95.30ms
step:428/1770 train_time:39837ms step_avg:95.30ms
step:429/1770 train_time:39934ms step_avg:95.31ms
step:430/1770 train_time:40032ms step_avg:95.31ms
step:431/1770 train_time:40130ms step_avg:95.32ms
step:432/1770 train_time:40227ms step_avg:95.32ms
step:433/1770 train_time:40324ms step_avg:95.33ms
step:434/1770 train_time:40421ms step_avg:95.33ms
step:435/1770 train_time:40518ms step_avg:95.34ms
step:436/1770 train_time:40615ms step_avg:95.34ms
step:437/1770 train_time:40713ms step_avg:95.35ms
step:438/1770 train_time:40810ms step_avg:95.35ms
step:439/1770 train_time:40908ms step_avg:95.36ms
step:440/1770 train_time:41005ms step_avg:95.36ms
step:441/1770 train_time:41102ms step_avg:95.36ms
step:442/1770 train_time:41199ms step_avg:95.37ms
step:443/1770 train_time:41296ms step_avg:95.37ms
step:444/1770 train_time:41393ms step_avg:95.38ms
step:445/1770 train_time:41491ms step_avg:95.38ms
step:446/1770 train_time:41588ms step_avg:95.39ms
step:447/1770 train_time:41686ms step_avg:95.39ms
step:448/1770 train_time:41783ms step_avg:95.39ms
step:449/1770 train_time:41880ms step_avg:95.40ms
step:450/1770 train_time:41977ms step_avg:95.40ms
step:451/1770 train_time:42074ms step_avg:95.41ms
step:452/1770 train_time:42172ms step_avg:95.41ms
step:453/1770 train_time:42270ms step_avg:95.42ms
step:454/1770 train_time:42368ms step_avg:95.42ms
step:455/1770 train_time:42465ms step_avg:95.43ms
step:456/1770 train_time:42563ms step_avg:95.43ms
step:457/1770 train_time:42660ms step_avg:95.44ms
step:458/1770 train_time:42758ms step_avg:95.44ms
step:459/1770 train_time:42855ms step_avg:95.45ms
step:460/1770 train_time:42953ms step_avg:95.45ms
step:461/1770 train_time:43050ms step_avg:95.46ms
step:462/1770 train_time:43148ms step_avg:95.46ms
step:463/1770 train_time:43245ms step_avg:95.46ms
step:464/1770 train_time:43342ms step_avg:95.47ms
step:465/1770 train_time:43439ms step_avg:95.47ms
step:466/1770 train_time:43536ms step_avg:95.47ms
step:467/1770 train_time:43635ms step_avg:95.48ms
step:468/1770 train_time:43733ms step_avg:95.49ms
step:469/1770 train_time:43831ms step_avg:95.49ms
step:470/1770 train_time:43928ms step_avg:95.50ms
step:471/1770 train_time:44026ms step_avg:95.50ms
step:472/1770 train_time:44123ms step_avg:95.50ms
step:473/1770 train_time:44220ms step_avg:95.51ms
step:474/1770 train_time:44317ms step_avg:95.51ms
step:475/1770 train_time:44415ms step_avg:95.52ms
step:476/1770 train_time:44513ms step_avg:95.52ms
step:477/1770 train_time:44610ms step_avg:95.52ms
step:478/1770 train_time:44707ms step_avg:95.53ms
step:479/1770 train_time:44805ms step_avg:95.53ms
step:480/1770 train_time:44902ms step_avg:95.54ms
step:481/1770 train_time:44999ms step_avg:95.54ms
step:482/1770 train_time:45096ms step_avg:95.54ms
step:483/1770 train_time:45193ms step_avg:95.55ms
step:484/1770 train_time:45291ms step_avg:95.55ms
step:485/1770 train_time:45389ms step_avg:95.56ms
step:486/1770 train_time:45486ms step_avg:95.56ms
step:487/1770 train_time:45583ms step_avg:95.56ms
step:488/1770 train_time:45681ms step_avg:95.57ms
step:489/1770 train_time:45778ms step_avg:95.57ms
step:490/1770 train_time:45875ms step_avg:95.57ms
step:491/1770 train_time:45973ms step_avg:95.58ms
step:492/1770 train_time:46071ms step_avg:95.58ms
step:493/1770 train_time:46168ms step_avg:95.59ms
step:494/1770 train_time:46265ms step_avg:95.59ms
step:495/1770 train_time:46362ms step_avg:95.59ms
step:496/1770 train_time:46459ms step_avg:95.59ms
step:497/1770 train_time:46557ms step_avg:95.60ms
step:498/1770 train_time:46654ms step_avg:95.60ms
step:499/1770 train_time:46752ms step_avg:95.61ms
step:500/1770 train_time:46850ms step_avg:95.61ms
step:500/1770 val_loss:3.7533 train_time:46946ms step_avg:95.81ms
step:501/1770 train_time:46968ms step_avg:95.66ms
step:502/1770 train_time:47054ms step_avg:95.64ms
step:503/1770 train_time:47153ms step_avg:95.64ms
step:504/1770 train_time:47250ms step_avg:95.65ms
step:505/1770 train_time:47347ms step_avg:95.65ms
step:506/1770 train_time:47444ms step_avg:95.65ms
step:507/1770 train_time:47542ms step_avg:95.66ms
step:508/1770 train_time:47640ms step_avg:95.66ms
step:509/1770 train_time:47737ms step_avg:95.67ms
step:510/1770 train_time:47834ms step_avg:95.67ms
step:511/1770 train_time:47931ms step_avg:95.67ms
step:512/1770 train_time:48028ms step_avg:95.67ms
step:513/1770 train_time:48126ms step_avg:95.68ms
step:514/1770 train_time:48224ms step_avg:95.68ms
step:515/1770 train_time:48322ms step_avg:95.69ms
step:516/1770 train_time:48419ms step_avg:95.69ms
step:517/1770 train_time:48517ms step_avg:95.69ms
step:518/1770 train_time:48614ms step_avg:95.70ms
step:519/1770 train_time:48711ms step_avg:95.70ms
step:520/1770 train_time:48808ms step_avg:95.70ms
step:521/1770 train_time:48905ms step_avg:95.71ms
step:522/1770 train_time:49003ms step_avg:95.71ms
step:523/1770 train_time:49101ms step_avg:95.71ms
step:524/1770 train_time:49198ms step_avg:95.72ms
step:525/1770 train_time:49297ms step_avg:95.72ms
step:526/1770 train_time:49393ms step_avg:95.72ms
step:527/1770 train_time:49491ms step_avg:95.73ms
step:528/1770 train_time:49588ms step_avg:95.73ms
step:529/1770 train_time:49686ms step_avg:95.73ms
step:530/1770 train_time:49784ms step_avg:95.74ms
step:531/1770 train_time:49882ms step_avg:95.74ms
step:532/1770 train_time:49980ms step_avg:95.75ms
step:533/1770 train_time:50078ms step_avg:95.75ms
step:534/1770 train_time:50175ms step_avg:95.75ms
step:535/1770 train_time:50273ms step_avg:95.76ms
step:536/1770 train_time:50371ms step_avg:95.76ms
step:537/1770 train_time:50468ms step_avg:95.76ms
step:538/1770 train_time:50565ms step_avg:95.77ms
step:539/1770 train_time:50663ms step_avg:95.77ms
step:540/1770 train_time:50762ms step_avg:95.78ms
step:541/1770 train_time:50859ms step_avg:95.78ms
step:542/1770 train_time:50957ms step_avg:95.78ms
step:543/1770 train_time:51055ms step_avg:95.79ms
step:544/1770 train_time:51153ms step_avg:95.79ms
step:545/1770 train_time:51250ms step_avg:95.79ms
step:546/1770 train_time:51348ms step_avg:95.80ms
step:547/1770 train_time:51446ms step_avg:95.80ms
step:548/1770 train_time:51543ms step_avg:95.81ms
step:549/1770 train_time:51642ms step_avg:95.81ms
step:550/1770 train_time:51740ms step_avg:95.81ms
step:551/1770 train_time:51838ms step_avg:95.82ms
step:552/1770 train_time:51935ms step_avg:95.82ms
step:553/1770 train_time:52033ms step_avg:95.83ms
step:554/1770 train_time:52131ms step_avg:95.83ms
step:555/1770 train_time:52228ms step_avg:95.83ms
step:556/1770 train_time:52326ms step_avg:95.83ms
step:557/1770 train_time:52424ms step_avg:95.84ms
step:558/1770 train_time:52522ms step_avg:95.84ms
step:559/1770 train_time:52620ms step_avg:95.85ms
step:560/1770 train_time:52718ms step_avg:95.85ms
step:561/1770 train_time:52816ms step_avg:95.85ms
step:562/1770 train_time:52913ms step_avg:95.86ms
step:563/1770 train_time:53010ms step_avg:95.86ms
step:564/1770 train_time:53107ms step_avg:95.86ms
step:565/1770 train_time:53205ms step_avg:95.87ms
step:566/1770 train_time:53303ms step_avg:95.87ms
step:567/1770 train_time:53402ms step_avg:95.87ms
step:568/1770 train_time:53500ms step_avg:95.88ms
step:569/1770 train_time:53597ms step_avg:95.88ms
step:570/1770 train_time:53694ms step_avg:95.88ms
step:571/1770 train_time:53792ms step_avg:95.89ms
step:572/1770 train_time:53889ms step_avg:95.89ms
step:573/1770 train_time:53987ms step_avg:95.89ms
step:574/1770 train_time:54085ms step_avg:95.90ms
step:575/1770 train_time:54183ms step_avg:95.90ms
step:576/1770 train_time:54281ms step_avg:95.90ms
step:577/1770 train_time:54379ms step_avg:95.91ms
step:578/1770 train_time:54477ms step_avg:95.91ms
step:579/1770 train_time:54574ms step_avg:95.91ms
step:580/1770 train_time:54672ms step_avg:95.92ms
step:581/1770 train_time:54769ms step_avg:95.92ms
step:582/1770 train_time:54867ms step_avg:95.92ms
step:583/1770 train_time:54965ms step_avg:95.93ms
step:584/1770 train_time:55063ms step_avg:95.93ms
step:585/1770 train_time:55161ms step_avg:95.93ms
step:586/1770 train_time:55259ms step_avg:95.94ms
step:587/1770 train_time:55357ms step_avg:95.94ms
step:588/1770 train_time:55455ms step_avg:95.94ms
step:589/1770 train_time:55552ms step_avg:95.95ms
step:590/1770 train_time:55650ms step_avg:95.95ms
step:591/1770 train_time:55748ms step_avg:95.95ms
step:592/1770 train_time:55846ms step_avg:95.95ms
step:593/1770 train_time:55943ms step_avg:95.96ms
step:594/1770 train_time:56042ms step_avg:95.96ms
step:595/1770 train_time:56140ms step_avg:95.97ms
step:596/1770 train_time:56237ms step_avg:95.97ms
step:597/1770 train_time:56335ms step_avg:95.97ms
step:598/1770 train_time:56432ms step_avg:95.97ms
step:599/1770 train_time:56529ms step_avg:95.98ms
step:600/1770 train_time:56627ms step_avg:95.98ms
step:601/1770 train_time:56725ms step_avg:95.98ms
step:602/1770 train_time:56823ms step_avg:95.98ms
step:603/1770 train_time:56921ms step_avg:95.99ms
step:604/1770 train_time:57019ms step_avg:95.99ms
step:605/1770 train_time:57117ms step_avg:95.99ms
step:606/1770 train_time:57214ms step_avg:96.00ms
step:607/1770 train_time:57312ms step_avg:96.00ms
step:608/1770 train_time:57409ms step_avg:96.00ms
step:609/1770 train_time:57507ms step_avg:96.00ms
step:610/1770 train_time:57605ms step_avg:96.01ms
step:611/1770 train_time:57703ms step_avg:96.01ms
step:612/1770 train_time:57801ms step_avg:96.01ms
step:613/1770 train_time:57898ms step_avg:96.02ms
step:614/1770 train_time:57996ms step_avg:96.02ms
step:615/1770 train_time:58094ms step_avg:96.02ms
step:616/1770 train_time:58191ms step_avg:96.02ms
step:617/1770 train_time:58289ms step_avg:96.03ms
step:618/1770 train_time:58388ms step_avg:96.03ms
step:619/1770 train_time:58485ms step_avg:96.03ms
step:620/1770 train_time:58583ms step_avg:96.04ms
step:621/1770 train_time:58680ms step_avg:96.04ms
step:622/1770 train_time:58778ms step_avg:96.04ms
step:623/1770 train_time:58876ms step_avg:96.05ms
step:624/1770 train_time:58973ms step_avg:96.05ms
step:625/1770 train_time:59071ms step_avg:96.05ms
step:625/1770 val_loss:3.6648 train_time:59167ms step_avg:96.21ms
step:626/1770 train_time:59189ms step_avg:96.09ms
step:627/1770 train_time:59275ms step_avg:96.07ms
step:628/1770 train_time:59374ms step_avg:96.07ms
step:629/1770 train_time:59471ms step_avg:96.08ms
step:630/1770 train_time:59568ms step_avg:96.08ms
step:631/1770 train_time:59666ms step_avg:96.08ms
step:632/1770 train_time:59764ms step_avg:96.08ms
step:633/1770 train_time:59862ms step_avg:96.09ms
step:634/1770 train_time:59960ms step_avg:96.09ms
step:635/1770 train_time:60058ms step_avg:96.09ms
step:636/1770 train_time:60155ms step_avg:96.09ms
step:637/1770 train_time:60253ms step_avg:96.10ms
step:638/1770 train_time:60350ms step_avg:96.10ms
step:639/1770 train_time:60448ms step_avg:96.10ms
step:640/1770 train_time:60546ms step_avg:96.10ms
step:641/1770 train_time:60644ms step_avg:96.11ms
step:642/1770 train_time:60742ms step_avg:96.11ms
step:643/1770 train_time:60839ms step_avg:96.11ms
step:644/1770 train_time:60937ms step_avg:96.12ms
step:645/1770 train_time:61035ms step_avg:96.12ms
step:646/1770 train_time:61132ms step_avg:96.12ms
step:647/1770 train_time:61229ms step_avg:96.12ms
step:648/1770 train_time:61327ms step_avg:96.12ms
step:649/1770 train_time:61425ms step_avg:96.13ms
step:650/1770 train_time:61523ms step_avg:96.13ms
step:651/1770 train_time:61621ms step_avg:96.13ms
step:652/1770 train_time:61720ms step_avg:96.14ms
step:653/1770 train_time:61817ms step_avg:96.14ms
step:654/1770 train_time:61915ms step_avg:96.14ms
step:655/1770 train_time:62012ms step_avg:96.14ms
step:656/1770 train_time:62110ms step_avg:96.15ms
step:657/1770 train_time:62208ms step_avg:96.15ms
step:658/1770 train_time:62307ms step_avg:96.15ms
step:659/1770 train_time:62407ms step_avg:96.16ms
step:660/1770 train_time:62506ms step_avg:96.16ms
step:661/1770 train_time:62606ms step_avg:96.17ms
step:662/1770 train_time:62706ms step_avg:96.18ms
step:663/1770 train_time:62806ms step_avg:96.18ms
step:664/1770 train_time:62906ms step_avg:96.19ms
step:665/1770 train_time:63006ms step_avg:96.19ms
step:666/1770 train_time:63106ms step_avg:96.20ms
step:667/1770 train_time:63206ms step_avg:96.20ms
step:668/1770 train_time:63305ms step_avg:96.21ms
step:669/1770 train_time:63405ms step_avg:96.21ms
step:670/1770 train_time:63504ms step_avg:96.22ms
step:671/1770 train_time:63604ms step_avg:96.22ms
step:672/1770 train_time:63704ms step_avg:96.23ms
step:673/1770 train_time:63804ms step_avg:96.23ms
step:674/1770 train_time:63903ms step_avg:96.24ms
step:675/1770 train_time:64003ms step_avg:96.25ms
step:676/1770 train_time:64104ms step_avg:96.25ms
step:677/1770 train_time:64206ms step_avg:96.26ms
step:678/1770 train_time:64306ms step_avg:96.27ms
step:679/1770 train_time:64405ms step_avg:96.27ms
step:680/1770 train_time:64505ms step_avg:96.28ms
step:681/1770 train_time:64605ms step_avg:96.28ms
step:682/1770 train_time:64705ms step_avg:96.29ms
step:683/1770 train_time:64805ms step_avg:96.29ms
step:684/1770 train_time:64904ms step_avg:96.30ms
step:685/1770 train_time:65004ms step_avg:96.30ms
step:686/1770 train_time:65105ms step_avg:96.31ms
step:687/1770 train_time:65205ms step_avg:96.31ms
step:688/1770 train_time:65305ms step_avg:96.32ms
step:689/1770 train_time:65407ms step_avg:96.33ms
step:690/1770 train_time:65506ms step_avg:96.33ms
step:691/1770 train_time:65606ms step_avg:96.34ms
step:692/1770 train_time:65706ms step_avg:96.34ms
step:693/1770 train_time:65806ms step_avg:96.35ms
step:694/1770 train_time:65906ms step_avg:96.35ms
step:695/1770 train_time:66006ms step_avg:96.36ms
step:696/1770 train_time:66106ms step_avg:96.36ms
step:697/1770 train_time:66206ms step_avg:96.37ms
step:698/1770 train_time:66307ms step_avg:96.38ms
step:699/1770 train_time:66408ms step_avg:96.38ms
step:700/1770 train_time:66508ms step_avg:96.39ms
step:701/1770 train_time:66607ms step_avg:96.39ms
step:702/1770 train_time:66706ms step_avg:96.40ms
step:703/1770 train_time:66806ms step_avg:96.40ms
step:704/1770 train_time:66906ms step_avg:96.41ms
step:705/1770 train_time:67006ms step_avg:96.41ms
step:706/1770 train_time:67105ms step_avg:96.42ms
step:707/1770 train_time:67205ms step_avg:96.42ms
step:708/1770 train_time:67305ms step_avg:96.43ms
step:709/1770 train_time:67405ms step_avg:96.43ms
step:710/1770 train_time:67506ms step_avg:96.44ms
step:711/1770 train_time:67606ms step_avg:96.44ms
step:712/1770 train_time:67706ms step_avg:96.45ms
step:713/1770 train_time:67806ms step_avg:96.45ms
step:714/1770 train_time:67906ms step_avg:96.46ms
step:715/1770 train_time:68006ms step_avg:96.46ms
step:716/1770 train_time:68105ms step_avg:96.47ms
step:717/1770 train_time:68205ms step_avg:96.47ms
step:718/1770 train_time:68305ms step_avg:96.48ms
step:719/1770 train_time:68405ms step_avg:96.48ms
step:720/1770 train_time:68505ms step_avg:96.49ms
step:721/1770 train_time:68605ms step_avg:96.49ms
step:722/1770 train_time:68705ms step_avg:96.50ms
step:723/1770 train_time:68805ms step_avg:96.50ms
step:724/1770 train_time:68906ms step_avg:96.51ms
step:725/1770 train_time:69006ms step_avg:96.51ms
step:726/1770 train_time:69106ms step_avg:96.52ms
step:727/1770 train_time:69205ms step_avg:96.52ms
step:728/1770 train_time:69305ms step_avg:96.52ms
step:729/1770 train_time:69404ms step_avg:96.53ms
step:730/1770 train_time:69504ms step_avg:96.53ms
step:731/1770 train_time:69605ms step_avg:96.54ms
step:732/1770 train_time:69704ms step_avg:96.54ms
step:733/1770 train_time:69804ms step_avg:96.55ms
step:734/1770 train_time:69904ms step_avg:96.55ms
step:735/1770 train_time:70005ms step_avg:96.56ms
step:736/1770 train_time:70105ms step_avg:96.56ms
step:737/1770 train_time:70205ms step_avg:96.57ms
step:738/1770 train_time:70305ms step_avg:96.57ms
step:739/1770 train_time:70404ms step_avg:96.58ms
step:740/1770 train_time:70504ms step_avg:96.58ms
step:741/1770 train_time:70604ms step_avg:96.59ms
step:742/1770 train_time:70704ms step_avg:96.59ms
step:743/1770 train_time:70804ms step_avg:96.60ms
step:744/1770 train_time:70904ms step_avg:96.60ms
step:745/1770 train_time:71004ms step_avg:96.60ms
step:746/1770 train_time:71104ms step_avg:96.61ms
step:747/1770 train_time:71204ms step_avg:96.61ms
step:748/1770 train_time:71304ms step_avg:96.62ms
step:749/1770 train_time:71404ms step_avg:96.62ms
step:750/1770 train_time:71504ms step_avg:96.63ms
step:750/1770 val_loss:3.6010 train_time:71602ms step_avg:96.76ms
step:751/1770 train_time:71624ms step_avg:96.66ms
step:752/1770 train_time:71712ms step_avg:96.65ms
step:753/1770 train_time:71813ms step_avg:96.65ms
step:754/1770 train_time:71913ms step_avg:96.66ms
step:755/1770 train_time:72013ms step_avg:96.66ms
step:756/1770 train_time:72113ms step_avg:96.67ms
step:757/1770 train_time:72213ms step_avg:96.67ms
step:758/1770 train_time:72312ms step_avg:96.67ms
step:759/1770 train_time:72412ms step_avg:96.68ms
step:760/1770 train_time:72512ms step_avg:96.68ms
step:761/1770 train_time:72612ms step_avg:96.69ms
step:762/1770 train_time:72713ms step_avg:96.69ms
step:763/1770 train_time:72813ms step_avg:96.70ms
step:764/1770 train_time:72914ms step_avg:96.70ms
step:765/1770 train_time:73015ms step_avg:96.71ms
step:766/1770 train_time:73115ms step_avg:96.71ms
step:767/1770 train_time:73215ms step_avg:96.72ms
step:768/1770 train_time:73314ms step_avg:96.72ms
step:769/1770 train_time:73414ms step_avg:96.73ms
step:770/1770 train_time:73514ms step_avg:96.73ms
step:771/1770 train_time:73615ms step_avg:96.73ms
step:772/1770 train_time:73716ms step_avg:96.74ms
step:773/1770 train_time:73816ms step_avg:96.74ms
step:774/1770 train_time:73915ms step_avg:96.75ms
step:775/1770 train_time:74015ms step_avg:96.75ms
step:776/1770 train_time:74115ms step_avg:96.76ms
step:777/1770 train_time:74215ms step_avg:96.76ms
step:778/1770 train_time:74315ms step_avg:96.76ms
step:779/1770 train_time:74414ms step_avg:96.77ms
step:780/1770 train_time:74515ms step_avg:96.77ms
step:781/1770 train_time:74614ms step_avg:96.78ms
step:782/1770 train_time:74713ms step_avg:96.78ms
step:783/1770 train_time:74814ms step_avg:96.78ms
step:784/1770 train_time:74914ms step_avg:96.79ms
step:785/1770 train_time:75014ms step_avg:96.79ms
step:786/1770 train_time:75114ms step_avg:96.80ms
step:787/1770 train_time:75213ms step_avg:96.80ms
step:788/1770 train_time:75313ms step_avg:96.80ms
step:789/1770 train_time:75414ms step_avg:96.81ms
step:790/1770 train_time:75513ms step_avg:96.81ms
step:791/1770 train_time:75614ms step_avg:96.82ms
step:792/1770 train_time:75714ms step_avg:96.82ms
step:793/1770 train_time:75814ms step_avg:96.83ms
step:794/1770 train_time:75915ms step_avg:96.83ms
step:795/1770 train_time:76016ms step_avg:96.84ms
step:796/1770 train_time:76117ms step_avg:96.84ms
step:797/1770 train_time:76216ms step_avg:96.84ms
step:798/1770 train_time:76316ms step_avg:96.85ms
step:799/1770 train_time:76416ms step_avg:96.85ms
step:800/1770 train_time:76516ms step_avg:96.86ms
step:801/1770 train_time:76616ms step_avg:96.86ms
step:802/1770 train_time:76716ms step_avg:96.86ms
step:803/1770 train_time:76816ms step_avg:96.87ms
step:804/1770 train_time:76916ms step_avg:96.87ms
step:805/1770 train_time:77016ms step_avg:96.88ms
step:806/1770 train_time:77116ms step_avg:96.88ms
step:807/1770 train_time:77217ms step_avg:96.88ms
step:808/1770 train_time:77317ms step_avg:96.89ms
step:809/1770 train_time:77417ms step_avg:96.89ms
step:810/1770 train_time:77518ms step_avg:96.90ms
step:811/1770 train_time:77617ms step_avg:96.90ms
step:812/1770 train_time:77717ms step_avg:96.90ms
step:813/1770 train_time:77817ms step_avg:96.91ms
step:814/1770 train_time:77917ms step_avg:96.91ms
step:815/1770 train_time:78016ms step_avg:96.91ms
step:816/1770 train_time:78116ms step_avg:96.92ms
step:817/1770 train_time:78216ms step_avg:96.92ms
step:818/1770 train_time:78316ms step_avg:96.93ms
step:819/1770 train_time:78416ms step_avg:96.93ms
step:820/1770 train_time:78517ms step_avg:96.93ms
step:821/1770 train_time:78617ms step_avg:96.94ms
step:822/1770 train_time:78717ms step_avg:96.94ms
step:823/1770 train_time:78816ms step_avg:96.95ms
step:824/1770 train_time:78916ms step_avg:96.95ms
step:825/1770 train_time:79016ms step_avg:96.95ms
step:826/1770 train_time:79115ms step_avg:96.96ms
step:827/1770 train_time:79215ms step_avg:96.96ms
step:828/1770 train_time:79316ms step_avg:96.96ms
step:829/1770 train_time:79415ms step_avg:96.97ms
step:830/1770 train_time:79515ms step_avg:96.97ms
step:831/1770 train_time:79615ms step_avg:96.97ms
step:832/1770 train_time:79715ms step_avg:96.98ms
step:833/1770 train_time:79815ms step_avg:96.98ms
step:834/1770 train_time:79915ms step_avg:96.98ms
step:835/1770 train_time:80015ms step_avg:96.99ms
step:836/1770 train_time:80115ms step_avg:96.99ms
step:837/1770 train_time:80216ms step_avg:97.00ms
step:838/1770 train_time:80316ms step_avg:97.00ms
step:839/1770 train_time:80417ms step_avg:97.00ms
step:840/1770 train_time:80517ms step_avg:97.01ms
step:841/1770 train_time:80616ms step_avg:97.01ms
step:842/1770 train_time:80716ms step_avg:97.01ms
step:843/1770 train_time:80816ms step_avg:97.02ms
step:844/1770 train_time:80915ms step_avg:97.02ms
step:845/1770 train_time:81016ms step_avg:97.02ms
step:846/1770 train_time:81116ms step_avg:97.03ms
step:847/1770 train_time:81216ms step_avg:97.03ms
step:848/1770 train_time:81316ms step_avg:97.04ms
step:849/1770 train_time:81416ms step_avg:97.04ms
step:850/1770 train_time:81516ms step_avg:97.04ms
step:851/1770 train_time:81616ms step_avg:97.05ms
step:852/1770 train_time:81716ms step_avg:97.05ms
step:853/1770 train_time:81816ms step_avg:97.05ms
step:854/1770 train_time:81915ms step_avg:97.06ms
step:855/1770 train_time:82015ms step_avg:97.06ms
step:856/1770 train_time:82115ms step_avg:97.06ms
step:857/1770 train_time:82215ms step_avg:97.07ms
step:858/1770 train_time:82315ms step_avg:97.07ms
step:859/1770 train_time:82415ms step_avg:97.07ms
step:860/1770 train_time:82516ms step_avg:97.08ms
step:861/1770 train_time:82617ms step_avg:97.08ms
step:862/1770 train_time:82716ms step_avg:97.08ms
step:863/1770 train_time:82816ms step_avg:97.09ms
step:864/1770 train_time:82916ms step_avg:97.09ms
step:865/1770 train_time:83016ms step_avg:97.09ms
step:866/1770 train_time:83116ms step_avg:97.10ms
step:867/1770 train_time:83216ms step_avg:97.10ms
step:868/1770 train_time:83316ms step_avg:97.10ms
step:869/1770 train_time:83416ms step_avg:97.11ms
step:870/1770 train_time:83516ms step_avg:97.11ms
step:871/1770 train_time:83617ms step_avg:97.12ms
step:872/1770 train_time:83717ms step_avg:97.12ms
step:873/1770 train_time:83817ms step_avg:97.12ms
step:874/1770 train_time:83917ms step_avg:97.13ms
step:875/1770 train_time:84016ms step_avg:97.13ms
step:875/1770 val_loss:3.5535 train_time:84114ms step_avg:97.24ms
step:876/1770 train_time:84136ms step_avg:97.15ms
step:877/1770 train_time:84228ms step_avg:97.15ms
step:878/1770 train_time:84329ms step_avg:97.15ms
step:879/1770 train_time:84429ms step_avg:97.16ms
step:880/1770 train_time:84529ms step_avg:97.16ms
step:881/1770 train_time:84628ms step_avg:97.16ms
step:882/1770 train_time:84727ms step_avg:97.16ms
step:883/1770 train_time:84827ms step_avg:97.17ms
step:884/1770 train_time:84926ms step_avg:97.17ms
step:885/1770 train_time:85026ms step_avg:97.17ms
step:886/1770 train_time:85125ms step_avg:97.17ms
step:887/1770 train_time:85224ms step_avg:97.18ms
step:888/1770 train_time:85323ms step_avg:97.18ms
step:889/1770 train_time:85423ms step_avg:97.18ms
step:890/1770 train_time:85522ms step_avg:97.18ms
step:891/1770 train_time:85621ms step_avg:97.19ms
step:892/1770 train_time:85721ms step_avg:97.19ms
step:893/1770 train_time:85821ms step_avg:97.19ms
step:894/1770 train_time:85920ms step_avg:97.19ms
step:895/1770 train_time:86019ms step_avg:97.20ms
step:896/1770 train_time:86119ms step_avg:97.20ms
step:897/1770 train_time:86219ms step_avg:97.20ms
step:898/1770 train_time:86319ms step_avg:97.21ms
step:899/1770 train_time:86419ms step_avg:97.21ms
step:900/1770 train_time:86519ms step_avg:97.21ms
step:901/1770 train_time:86619ms step_avg:97.22ms
step:902/1770 train_time:86720ms step_avg:97.22ms
step:903/1770 train_time:86819ms step_avg:97.22ms
step:904/1770 train_time:86919ms step_avg:97.22ms
step:905/1770 train_time:87019ms step_avg:97.23ms
step:906/1770 train_time:87118ms step_avg:97.23ms
step:907/1770 train_time:87218ms step_avg:97.23ms
step:908/1770 train_time:87318ms step_avg:97.24ms
step:909/1770 train_time:87418ms step_avg:97.24ms
step:910/1770 train_time:87518ms step_avg:97.24ms
step:911/1770 train_time:87618ms step_avg:97.25ms
step:912/1770 train_time:87718ms step_avg:97.25ms
step:913/1770 train_time:87818ms step_avg:97.25ms
step:914/1770 train_time:87918ms step_avg:97.25ms
step:915/1770 train_time:88019ms step_avg:97.26ms
step:916/1770 train_time:88118ms step_avg:97.26ms
step:917/1770 train_time:88218ms step_avg:97.26ms
step:918/1770 train_time:88317ms step_avg:97.27ms
step:919/1770 train_time:88417ms step_avg:97.27ms
step:920/1770 train_time:88519ms step_avg:97.27ms
step:921/1770 train_time:88621ms step_avg:97.28ms
step:922/1770 train_time:88722ms step_avg:97.28ms
step:923/1770 train_time:88823ms step_avg:97.29ms
step:924/1770 train_time:88923ms step_avg:97.29ms
step:925/1770 train_time:89024ms step_avg:97.29ms
step:926/1770 train_time:89125ms step_avg:97.30ms
step:927/1770 train_time:89225ms step_avg:97.30ms
step:928/1770 train_time:89326ms step_avg:97.30ms
step:929/1770 train_time:89426ms step_avg:97.31ms
step:930/1770 train_time:89527ms step_avg:97.31ms
step:931/1770 train_time:89628ms step_avg:97.32ms
step:932/1770 train_time:89728ms step_avg:97.32ms
step:933/1770 train_time:89829ms step_avg:97.32ms
step:934/1770 train_time:89929ms step_avg:97.33ms
step:935/1770 train_time:90031ms step_avg:97.33ms
step:936/1770 train_time:90133ms step_avg:97.34ms
step:937/1770 train_time:90235ms step_avg:97.34ms
step:938/1770 train_time:90338ms step_avg:97.35ms
step:939/1770 train_time:90439ms step_avg:97.35ms
step:940/1770 train_time:90540ms step_avg:97.36ms
step:941/1770 train_time:90641ms step_avg:97.36ms
step:942/1770 train_time:90743ms step_avg:97.36ms
step:943/1770 train_time:90844ms step_avg:97.37ms
step:944/1770 train_time:90944ms step_avg:97.37ms
step:945/1770 train_time:91044ms step_avg:97.37ms
step:946/1770 train_time:91146ms step_avg:97.38ms
step:947/1770 train_time:91248ms step_avg:97.38ms
step:948/1770 train_time:91348ms step_avg:97.39ms
step:949/1770 train_time:91450ms step_avg:97.39ms
step:950/1770 train_time:91552ms step_avg:97.40ms
step:951/1770 train_time:91654ms step_avg:97.40ms
step:952/1770 train_time:91756ms step_avg:97.41ms
step:953/1770 train_time:91858ms step_avg:97.41ms
step:954/1770 train_time:91959ms step_avg:97.41ms
step:955/1770 train_time:92061ms step_avg:97.42ms
step:956/1770 train_time:92162ms step_avg:97.42ms
step:957/1770 train_time:92262ms step_avg:97.43ms
step:958/1770 train_time:92363ms step_avg:97.43ms
step:959/1770 train_time:92464ms step_avg:97.43ms
step:960/1770 train_time:92564ms step_avg:97.44ms
step:961/1770 train_time:92665ms step_avg:97.44ms
step:962/1770 train_time:92766ms step_avg:97.44ms
step:963/1770 train_time:92868ms step_avg:97.45ms
step:964/1770 train_time:92969ms step_avg:97.45ms
step:965/1770 train_time:93070ms step_avg:97.46ms
step:966/1770 train_time:93171ms step_avg:97.46ms
step:967/1770 train_time:93272ms step_avg:97.46ms
step:968/1770 train_time:93374ms step_avg:97.47ms
step:969/1770 train_time:93475ms step_avg:97.47ms
step:970/1770 train_time:93577ms step_avg:97.48ms
step:971/1770 train_time:93680ms step_avg:97.48ms
step:972/1770 train_time:93782ms step_avg:97.49ms
step:973/1770 train_time:93883ms step_avg:97.49ms
step:974/1770 train_time:93984ms step_avg:97.49ms
step:975/1770 train_time:94085ms step_avg:97.50ms
step:976/1770 train_time:94186ms step_avg:97.50ms
step:977/1770 train_time:94286ms step_avg:97.50ms
step:978/1770 train_time:94387ms step_avg:97.51ms
step:979/1770 train_time:94488ms step_avg:97.51ms
step:980/1770 train_time:94589ms step_avg:97.51ms
step:981/1770 train_time:94690ms step_avg:97.52ms
step:982/1770 train_time:94792ms step_avg:97.52ms
step:983/1770 train_time:94893ms step_avg:97.53ms
step:984/1770 train_time:94995ms step_avg:97.53ms
step:985/1770 train_time:95097ms step_avg:97.54ms
step:986/1770 train_time:95199ms step_avg:97.54ms
step:987/1770 train_time:95300ms step_avg:97.54ms
step:988/1770 train_time:95401ms step_avg:97.55ms
step:989/1770 train_time:95503ms step_avg:97.55ms
step:990/1770 train_time:95603ms step_avg:97.55ms
step:991/1770 train_time:95704ms step_avg:97.56ms
step:992/1770 train_time:95805ms step_avg:97.56ms
step:993/1770 train_time:95905ms step_avg:97.56ms
step:994/1770 train_time:96007ms step_avg:97.57ms
step:995/1770 train_time:96108ms step_avg:97.57ms
step:996/1770 train_time:96208ms step_avg:97.57ms
step:997/1770 train_time:96309ms step_avg:97.58ms
step:998/1770 train_time:96410ms step_avg:97.58ms
step:999/1770 train_time:96510ms step_avg:97.58ms
step:1000/1770 train_time:96611ms step_avg:97.59ms
step:1000/1770 val_loss:3.5141 train_time:96712ms step_avg:97.69ms
step:1001/1770 train_time:96734ms step_avg:97.61ms
step:1002/1770 train_time:96821ms step_avg:97.60ms
step:1003/1770 train_time:96925ms step_avg:97.61ms
step:1004/1770 train_time:97027ms step_avg:97.61ms
step:1005/1770 train_time:97128ms step_avg:97.62ms
step:1006/1770 train_time:97229ms step_avg:97.62ms
step:1007/1770 train_time:97329ms step_avg:97.62ms
step:1008/1770 train_time:97430ms step_avg:97.63ms
step:1009/1770 train_time:97531ms step_avg:97.63ms
step:1010/1770 train_time:97631ms step_avg:97.63ms
step:1011/1770 train_time:97733ms step_avg:97.64ms
step:1012/1770 train_time:97834ms step_avg:97.64ms
step:1013/1770 train_time:97935ms step_avg:97.64ms
step:1014/1770 train_time:98036ms step_avg:97.65ms
step:1015/1770 train_time:98136ms step_avg:97.65ms
step:1016/1770 train_time:98237ms step_avg:97.65ms
step:1017/1770 train_time:98338ms step_avg:97.65ms
step:1018/1770 train_time:98439ms step_avg:97.66ms
step:1019/1770 train_time:98540ms step_avg:97.66ms
step:1020/1770 train_time:98642ms step_avg:97.67ms
step:1021/1770 train_time:98744ms step_avg:97.67ms
step:1022/1770 train_time:98847ms step_avg:97.67ms
step:1023/1770 train_time:98948ms step_avg:97.68ms
step:1024/1770 train_time:99050ms step_avg:97.68ms
step:1025/1770 train_time:99151ms step_avg:97.69ms
step:1026/1770 train_time:99253ms step_avg:97.69ms
step:1027/1770 train_time:99354ms step_avg:97.69ms
step:1028/1770 train_time:99455ms step_avg:97.70ms
step:1029/1770 train_time:99555ms step_avg:97.70ms
step:1030/1770 train_time:99656ms step_avg:97.70ms
step:1031/1770 train_time:99757ms step_avg:97.71ms
step:1032/1770 train_time:99859ms step_avg:97.71ms
step:1033/1770 train_time:99960ms step_avg:97.71ms
step:1034/1770 train_time:100061ms step_avg:97.72ms
step:1035/1770 train_time:100164ms step_avg:97.72ms
step:1036/1770 train_time:100267ms step_avg:97.73ms
step:1037/1770 train_time:100368ms step_avg:97.73ms
step:1038/1770 train_time:100470ms step_avg:97.73ms
step:1039/1770 train_time:100572ms step_avg:97.74ms
step:1040/1770 train_time:100672ms step_avg:97.74ms
step:1041/1770 train_time:100773ms step_avg:97.74ms
step:1042/1770 train_time:100874ms step_avg:97.75ms
step:1043/1770 train_time:100975ms step_avg:97.75ms
step:1044/1770 train_time:101075ms step_avg:97.75ms
step:1045/1770 train_time:101176ms step_avg:97.75ms
step:1046/1770 train_time:101277ms step_avg:97.76ms
step:1047/1770 train_time:101378ms step_avg:97.76ms
step:1048/1770 train_time:101479ms step_avg:97.76ms
step:1049/1770 train_time:101581ms step_avg:97.77ms
step:1050/1770 train_time:101683ms step_avg:97.77ms
step:1051/1770 train_time:101786ms step_avg:97.78ms
step:1052/1770 train_time:101887ms step_avg:97.78ms
step:1053/1770 train_time:101989ms step_avg:97.78ms
step:1054/1770 train_time:102090ms step_avg:97.79ms
step:1055/1770 train_time:102191ms step_avg:97.79ms
step:1056/1770 train_time:102292ms step_avg:97.79ms
step:1057/1770 train_time:102393ms step_avg:97.80ms
step:1058/1770 train_time:102494ms step_avg:97.80ms
step:1059/1770 train_time:102595ms step_avg:97.80ms
step:1060/1770 train_time:102697ms step_avg:97.81ms
step:1061/1770 train_time:102798ms step_avg:97.81ms
step:1062/1770 train_time:102900ms step_avg:97.81ms
step:1063/1770 train_time:103003ms step_avg:97.82ms
step:1064/1770 train_time:103106ms step_avg:97.82ms
step:1065/1770 train_time:103208ms step_avg:97.83ms
step:1066/1770 train_time:103309ms step_avg:97.83ms
step:1067/1770 train_time:103410ms step_avg:97.83ms
step:1068/1770 train_time:103512ms step_avg:97.84ms
step:1069/1770 train_time:103613ms step_avg:97.84ms
step:1070/1770 train_time:103714ms step_avg:97.84ms
step:1071/1770 train_time:103815ms step_avg:97.85ms
step:1072/1770 train_time:103915ms step_avg:97.85ms
step:1073/1770 train_time:104015ms step_avg:97.85ms
step:1074/1770 train_time:104117ms step_avg:97.85ms
step:1075/1770 train_time:104218ms step_avg:97.86ms
step:1076/1770 train_time:104321ms step_avg:97.86ms
step:1077/1770 train_time:104424ms step_avg:97.87ms
step:1078/1770 train_time:104526ms step_avg:97.87ms
step:1079/1770 train_time:104628ms step_avg:97.87ms
step:1080/1770 train_time:104729ms step_avg:97.88ms
step:1081/1770 train_time:104829ms step_avg:97.88ms
step:1082/1770 train_time:104930ms step_avg:97.88ms
step:1083/1770 train_time:105031ms step_avg:97.89ms
step:1084/1770 train_time:105133ms step_avg:97.89ms
step:1085/1770 train_time:105234ms step_avg:97.89ms
step:1086/1770 train_time:105335ms step_avg:97.89ms
step:1087/1770 train_time:105436ms step_avg:97.90ms
step:1088/1770 train_time:105537ms step_avg:97.90ms
step:1089/1770 train_time:105637ms step_avg:97.90ms
step:1090/1770 train_time:105739ms step_avg:97.91ms
step:1091/1770 train_time:105840ms step_avg:97.91ms
step:1092/1770 train_time:105941ms step_avg:97.91ms
step:1093/1770 train_time:106043ms step_avg:97.92ms
step:1094/1770 train_time:106145ms step_avg:97.92ms
step:1095/1770 train_time:106247ms step_avg:97.92ms
step:1096/1770 train_time:106349ms step_avg:97.93ms
step:1097/1770 train_time:106450ms step_avg:97.93ms
step:1098/1770 train_time:106551ms step_avg:97.93ms
step:1099/1770 train_time:106652ms step_avg:97.94ms
step:1100/1770 train_time:106754ms step_avg:97.94ms
step:1101/1770 train_time:106855ms step_avg:97.94ms
step:1102/1770 train_time:106956ms step_avg:97.95ms
step:1103/1770 train_time:107058ms step_avg:97.95ms
step:1104/1770 train_time:107159ms step_avg:97.95ms
step:1105/1770 train_time:107261ms step_avg:97.96ms
step:1106/1770 train_time:107364ms step_avg:97.96ms
step:1107/1770 train_time:107465ms step_avg:97.96ms
step:1108/1770 train_time:107567ms step_avg:97.97ms
step:1109/1770 train_time:107669ms step_avg:97.97ms
step:1110/1770 train_time:107770ms step_avg:97.97ms
step:1111/1770 train_time:107871ms step_avg:97.98ms
step:1112/1770 train_time:107973ms step_avg:97.98ms
step:1113/1770 train_time:108074ms step_avg:97.98ms
step:1114/1770 train_time:108175ms step_avg:97.99ms
step:1115/1770 train_time:108276ms step_avg:97.99ms
step:1116/1770 train_time:108378ms step_avg:97.99ms
step:1117/1770 train_time:108479ms step_avg:97.99ms
step:1118/1770 train_time:108580ms step_avg:98.00ms
step:1119/1770 train_time:108682ms step_avg:98.00ms
step:1120/1770 train_time:108785ms step_avg:98.00ms
step:1121/1770 train_time:108886ms step_avg:98.01ms
step:1122/1770 train_time:108988ms step_avg:98.01ms
step:1123/1770 train_time:109089ms step_avg:98.01ms
step:1124/1770 train_time:109191ms step_avg:98.02ms
step:1125/1770 train_time:109292ms step_avg:98.02ms
step:1125/1770 val_loss:3.4757 train_time:109392ms step_avg:98.11ms
step:1126/1770 train_time:109413ms step_avg:98.04ms
step:1127/1770 train_time:109504ms step_avg:98.03ms
step:1128/1770 train_time:109606ms step_avg:98.04ms
step:1129/1770 train_time:109707ms step_avg:98.04ms
step:1130/1770 train_time:109810ms step_avg:98.04ms
step:1131/1770 train_time:109911ms step_avg:98.05ms
step:1132/1770 train_time:110013ms step_avg:98.05ms
step:1133/1770 train_time:110115ms step_avg:98.05ms
step:1134/1770 train_time:110216ms step_avg:98.06ms
step:1135/1770 train_time:110317ms step_avg:98.06ms
step:1136/1770 train_time:110418ms step_avg:98.06ms
step:1137/1770 train_time:110520ms step_avg:98.07ms
step:1138/1770 train_time:110620ms step_avg:98.07ms
step:1139/1770 train_time:110721ms step_avg:98.07ms
step:1140/1770 train_time:110822ms step_avg:98.07ms
step:1141/1770 train_time:110924ms step_avg:98.08ms
step:1142/1770 train_time:111025ms step_avg:98.08ms
step:1143/1770 train_time:111127ms step_avg:98.08ms
step:1144/1770 train_time:111229ms step_avg:98.09ms
step:1145/1770 train_time:111331ms step_avg:98.09ms
step:1146/1770 train_time:111433ms step_avg:98.09ms
step:1147/1770 train_time:111535ms step_avg:98.10ms
step:1148/1770 train_time:111636ms step_avg:98.10ms
step:1149/1770 train_time:111737ms step_avg:98.10ms
step:1150/1770 train_time:111838ms step_avg:98.10ms
step:1151/1770 train_time:111940ms step_avg:98.11ms
step:1152/1770 train_time:112042ms step_avg:98.11ms
step:1153/1770 train_time:112143ms step_avg:98.11ms
step:1154/1770 train_time:112245ms step_avg:98.12ms
step:1155/1770 train_time:112346ms step_avg:98.12ms
step:1156/1770 train_time:112448ms step_avg:98.12ms
step:1157/1770 train_time:112551ms step_avg:98.13ms
step:1158/1770 train_time:112654ms step_avg:98.13ms
step:1159/1770 train_time:112755ms step_avg:98.13ms
step:1160/1770 train_time:112856ms step_avg:98.14ms
step:1161/1770 train_time:112957ms step_avg:98.14ms
step:1162/1770 train_time:113059ms step_avg:98.14ms
step:1163/1770 train_time:113160ms step_avg:98.14ms
step:1164/1770 train_time:113261ms step_avg:98.15ms
step:1165/1770 train_time:113362ms step_avg:98.15ms
step:1166/1770 train_time:113464ms step_avg:98.15ms
step:1167/1770 train_time:113564ms step_avg:98.15ms
step:1168/1770 train_time:113666ms step_avg:98.16ms
step:1169/1770 train_time:113768ms step_avg:98.16ms
step:1170/1770 train_time:113870ms step_avg:98.16ms
step:1171/1770 train_time:113973ms step_avg:98.17ms
step:1172/1770 train_time:114075ms step_avg:98.17ms
step:1173/1770 train_time:114176ms step_avg:98.17ms
step:1174/1770 train_time:114277ms step_avg:98.18ms
step:1175/1770 train_time:114377ms step_avg:98.18ms
step:1176/1770 train_time:114478ms step_avg:98.18ms
step:1177/1770 train_time:114579ms step_avg:98.18ms
step:1178/1770 train_time:114680ms step_avg:98.18ms
step:1179/1770 train_time:114781ms step_avg:98.19ms
step:1180/1770 train_time:114882ms step_avg:98.19ms
step:1181/1770 train_time:114984ms step_avg:98.19ms
step:1182/1770 train_time:115086ms step_avg:98.20ms
step:1183/1770 train_time:115189ms step_avg:98.20ms
step:1184/1770 train_time:115295ms step_avg:98.21ms
step:1185/1770 train_time:115397ms step_avg:98.21ms
step:1186/1770 train_time:115500ms step_avg:98.21ms
step:1187/1770 train_time:115605ms step_avg:98.22ms
step:1188/1770 train_time:115708ms step_avg:98.22ms
step:1189/1770 train_time:115810ms step_avg:98.23ms
step:1190/1770 train_time:115913ms step_avg:98.23ms
step:1191/1770 train_time:116016ms step_avg:98.24ms
step:1192/1770 train_time:116118ms step_avg:98.24ms
step:1193/1770 train_time:116221ms step_avg:98.24ms
step:1194/1770 train_time:116323ms step_avg:98.25ms
step:1195/1770 train_time:116427ms step_avg:98.25ms
step:1196/1770 train_time:116532ms step_avg:98.26ms
step:1197/1770 train_time:116634ms step_avg:98.26ms
step:1198/1770 train_time:116737ms step_avg:98.26ms
step:1199/1770 train_time:116838ms step_avg:98.27ms
step:1200/1770 train_time:116941ms step_avg:98.27ms
step:1201/1770 train_time:117044ms step_avg:98.27ms
step:1202/1770 train_time:117145ms step_avg:98.28ms
step:1203/1770 train_time:117249ms step_avg:98.28ms
step:1204/1770 train_time:117352ms step_avg:98.28ms
step:1205/1770 train_time:117454ms step_avg:98.29ms
step:1206/1770 train_time:117557ms step_avg:98.29ms
step:1207/1770 train_time:117659ms step_avg:98.29ms
step:1208/1770 train_time:117760ms step_avg:98.30ms
step:1209/1770 train_time:117863ms step_avg:98.30ms
step:1210/1770 train_time:117965ms step_avg:98.30ms
step:1211/1770 train_time:118068ms step_avg:98.31ms
step:1212/1770 train_time:118173ms step_avg:98.31ms
step:1213/1770 train_time:118276ms step_avg:98.32ms
step:1214/1770 train_time:118377ms step_avg:98.32ms
step:1215/1770 train_time:118480ms step_avg:98.32ms
step:1216/1770 train_time:118584ms step_avg:98.33ms
step:1217/1770 train_time:118687ms step_avg:98.33ms
step:1218/1770 train_time:118791ms step_avg:98.34ms
step:1219/1770 train_time:118894ms step_avg:98.34ms
step:1220/1770 train_time:118996ms step_avg:98.34ms
step:1221/1770 train_time:119098ms step_avg:98.35ms
step:1222/1770 train_time:119201ms step_avg:98.35ms
step:1223/1770 train_time:119303ms step_avg:98.35ms
step:1224/1770 train_time:119407ms step_avg:98.36ms
step:1225/1770 train_time:119510ms step_avg:98.36ms
step:1226/1770 train_time:119613ms step_avg:98.37ms
step:1227/1770 train_time:119717ms step_avg:98.37ms
step:1228/1770 train_time:119822ms step_avg:98.38ms
step:1229/1770 train_time:119924ms step_avg:98.38ms
step:1230/1770 train_time:120028ms step_avg:98.38ms
step:1231/1770 train_time:120131ms step_avg:98.39ms
step:1232/1770 train_time:120234ms step_avg:98.39ms
step:1233/1770 train_time:120336ms step_avg:98.39ms
step:1234/1770 train_time:120438ms step_avg:98.40ms
step:1235/1770 train_time:120541ms step_avg:98.40ms
step:1236/1770 train_time:120644ms step_avg:98.40ms
step:1237/1770 train_time:120747ms step_avg:98.41ms
step:1238/1770 train_time:120851ms step_avg:98.41ms
step:1239/1770 train_time:120954ms step_avg:98.42ms
step:1240/1770 train_time:121056ms step_avg:98.42ms
step:1241/1770 train_time:121159ms step_avg:98.42ms
step:1242/1770 train_time:121261ms step_avg:98.43ms
step:1243/1770 train_time:121365ms step_avg:98.43ms
step:1244/1770 train_time:121467ms step_avg:98.43ms
step:1245/1770 train_time:121570ms step_avg:98.44ms
step:1246/1770 train_time:121674ms step_avg:98.44ms
step:1247/1770 train_time:121775ms step_avg:98.44ms
step:1248/1770 train_time:121878ms step_avg:98.45ms
step:1249/1770 train_time:121980ms step_avg:98.45ms
step:1250/1770 train_time:122082ms step_avg:98.45ms
step:1250/1770 val_loss:3.4288 train_time:122184ms step_avg:98.54ms
step:1251/1770 train_time:122205ms step_avg:98.47ms
step:1252/1770 train_time:122293ms step_avg:98.46ms
step:1253/1770 train_time:122395ms step_avg:98.47ms
step:1254/1770 train_time:122497ms step_avg:98.47ms
step:1255/1770 train_time:122603ms step_avg:98.48ms
step:1256/1770 train_time:122706ms step_avg:98.48ms
step:1257/1770 train_time:122807ms step_avg:98.48ms
step:1258/1770 train_time:122910ms step_avg:98.49ms
step:1259/1770 train_time:123013ms step_avg:98.49ms
step:1260/1770 train_time:123116ms step_avg:98.49ms
step:1261/1770 train_time:123219ms step_avg:98.50ms
step:1262/1770 train_time:123323ms step_avg:98.50ms
step:1263/1770 train_time:123425ms step_avg:98.50ms
step:1264/1770 train_time:123530ms step_avg:98.51ms
step:1265/1770 train_time:123632ms step_avg:98.51ms
step:1266/1770 train_time:123735ms step_avg:98.52ms
step:1267/1770 train_time:123838ms step_avg:98.52ms
step:1268/1770 train_time:123941ms step_avg:98.52ms
step:1269/1770 train_time:124045ms step_avg:98.53ms
step:1270/1770 train_time:124147ms step_avg:98.53ms
step:1271/1770 train_time:124250ms step_avg:98.53ms
step:1272/1770 train_time:124352ms step_avg:98.54ms
step:1273/1770 train_time:124455ms step_avg:98.54ms
step:1274/1770 train_time:124558ms step_avg:98.54ms
step:1275/1770 train_time:124661ms step_avg:98.55ms
step:1276/1770 train_time:124765ms step_avg:98.55ms
step:1277/1770 train_time:124867ms step_avg:98.55ms
step:1278/1770 train_time:124970ms step_avg:98.56ms
step:1279/1770 train_time:125073ms step_avg:98.56ms
step:1280/1770 train_time:125176ms step_avg:98.56ms
step:1281/1770 train_time:125278ms step_avg:98.57ms
step:1282/1770 train_time:125382ms step_avg:98.57ms
step:1283/1770 train_time:125486ms step_avg:98.57ms
step:1284/1770 train_time:125588ms step_avg:98.58ms
step:1285/1770 train_time:125691ms step_avg:98.58ms
step:1286/1770 train_time:125795ms step_avg:98.59ms
step:1287/1770 train_time:125899ms step_avg:98.59ms
step:1288/1770 train_time:126002ms step_avg:98.59ms
step:1289/1770 train_time:126106ms step_avg:98.60ms
step:1290/1770 train_time:126208ms step_avg:98.60ms
step:1291/1770 train_time:126311ms step_avg:98.60ms
step:1292/1770 train_time:126414ms step_avg:98.61ms
step:1293/1770 train_time:126517ms step_avg:98.61ms
step:1294/1770 train_time:126619ms step_avg:98.61ms
step:1295/1770 train_time:126723ms step_avg:98.62ms
step:1296/1770 train_time:126826ms step_avg:98.62ms
step:1297/1770 train_time:126928ms step_avg:98.62ms
step:1298/1770 train_time:127030ms step_avg:98.63ms
step:1299/1770 train_time:127132ms step_avg:98.63ms
step:1300/1770 train_time:127234ms step_avg:98.63ms
step:1301/1770 train_time:127337ms step_avg:98.63ms
step:1302/1770 train_time:127439ms step_avg:98.64ms
step:1303/1770 train_time:127542ms step_avg:98.64ms
step:1304/1770 train_time:127645ms step_avg:98.64ms
step:1305/1770 train_time:127748ms step_avg:98.65ms
step:1306/1770 train_time:127850ms step_avg:98.65ms
step:1307/1770 train_time:127952ms step_avg:98.65ms
step:1308/1770 train_time:128056ms step_avg:98.66ms
step:1309/1770 train_time:128158ms step_avg:98.66ms
step:1310/1770 train_time:128261ms step_avg:98.66ms
step:1311/1770 train_time:128363ms step_avg:98.67ms
step:1312/1770 train_time:128466ms step_avg:98.67ms
step:1313/1770 train_time:128567ms step_avg:98.67ms
step:1314/1770 train_time:128670ms step_avg:98.67ms
step:1315/1770 train_time:128772ms step_avg:98.68ms
step:1316/1770 train_time:128875ms step_avg:98.68ms
step:1317/1770 train_time:128978ms step_avg:98.68ms
step:1318/1770 train_time:129084ms step_avg:98.69ms
step:1319/1770 train_time:129188ms step_avg:98.69ms
step:1320/1770 train_time:129290ms step_avg:98.69ms
step:1321/1770 train_time:129392ms step_avg:98.70ms
step:1322/1770 train_time:129495ms step_avg:98.70ms
step:1323/1770 train_time:129599ms step_avg:98.70ms
step:1324/1770 train_time:129702ms step_avg:98.71ms
step:1325/1770 train_time:129807ms step_avg:98.71ms
step:1326/1770 train_time:129909ms step_avg:98.72ms
step:1327/1770 train_time:130015ms step_avg:98.72ms
step:1328/1770 train_time:130117ms step_avg:98.72ms
step:1329/1770 train_time:130220ms step_avg:98.73ms
step:1330/1770 train_time:130323ms step_avg:98.73ms
step:1331/1770 train_time:130426ms step_avg:98.73ms
step:1332/1770 train_time:130528ms step_avg:98.74ms
step:1333/1770 train_time:130630ms step_avg:98.74ms
step:1334/1770 train_time:130732ms step_avg:98.74ms
step:1335/1770 train_time:130834ms step_avg:98.74ms
step:1336/1770 train_time:130936ms step_avg:98.75ms
step:1337/1770 train_time:131039ms step_avg:98.75ms
step:1338/1770 train_time:131142ms step_avg:98.75ms
step:1339/1770 train_time:131246ms step_avg:98.76ms
step:1340/1770 train_time:131350ms step_avg:98.76ms
step:1341/1770 train_time:131453ms step_avg:98.76ms
step:1342/1770 train_time:131555ms step_avg:98.77ms
step:1343/1770 train_time:131659ms step_avg:98.77ms
step:1344/1770 train_time:131763ms step_avg:98.77ms
step:1345/1770 train_time:131866ms step_avg:98.78ms
step:1346/1770 train_time:131969ms step_avg:98.78ms
step:1347/1770 train_time:132071ms step_avg:98.78ms
step:1348/1770 train_time:132176ms step_avg:98.79ms
step:1349/1770 train_time:132279ms step_avg:98.79ms
step:1350/1770 train_time:132382ms step_avg:98.79ms
step:1351/1770 train_time:132485ms step_avg:98.80ms
step:1352/1770 train_time:132588ms step_avg:98.80ms
step:1353/1770 train_time:132691ms step_avg:98.80ms
step:1354/1770 train_time:132793ms step_avg:98.80ms
step:1355/1770 train_time:132895ms step_avg:98.81ms
step:1356/1770 train_time:132998ms step_avg:98.81ms
step:1357/1770 train_time:133100ms step_avg:98.81ms
step:1358/1770 train_time:133204ms step_avg:98.82ms
step:1359/1770 train_time:133307ms step_avg:98.82ms
step:1360/1770 train_time:133411ms step_avg:98.82ms
step:1361/1770 train_time:133514ms step_avg:98.83ms
step:1362/1770 train_time:133617ms step_avg:98.83ms
step:1363/1770 train_time:133721ms step_avg:98.83ms
step:1364/1770 train_time:133825ms step_avg:98.84ms
step:1365/1770 train_time:133928ms step_avg:98.84ms
step:1366/1770 train_time:134030ms step_avg:98.84ms
step:1367/1770 train_time:134134ms step_avg:98.85ms
step:1368/1770 train_time:134236ms step_avg:98.85ms
step:1369/1770 train_time:134340ms step_avg:98.85ms
step:1370/1770 train_time:134443ms step_avg:98.86ms
step:1371/1770 train_time:134546ms step_avg:98.86ms
step:1372/1770 train_time:134648ms step_avg:98.86ms
step:1373/1770 train_time:134750ms step_avg:98.86ms
step:1374/1770 train_time:134853ms step_avg:98.87ms
step:1375/1770 train_time:134956ms step_avg:98.87ms
step:1375/1770 val_loss:3.3871 train_time:135057ms step_avg:98.94ms
step:1376/1770 train_time:135081ms step_avg:98.89ms
step:1377/1770 train_time:135167ms step_avg:98.88ms
step:1378/1770 train_time:135269ms step_avg:98.88ms
step:1379/1770 train_time:135371ms step_avg:98.88ms
step:1380/1770 train_time:135474ms step_avg:98.89ms
step:1381/1770 train_time:135577ms step_avg:98.89ms
step:1382/1770 train_time:135679ms step_avg:98.89ms
step:1383/1770 train_time:135783ms step_avg:98.89ms
step:1384/1770 train_time:135885ms step_avg:98.90ms
step:1385/1770 train_time:135987ms step_avg:98.90ms
step:1386/1770 train_time:136090ms step_avg:98.90ms
step:1387/1770 train_time:136193ms step_avg:98.91ms
step:1388/1770 train_time:136296ms step_avg:98.91ms
step:1389/1770 train_time:136399ms step_avg:98.91ms
step:1390/1770 train_time:136502ms step_avg:98.91ms
step:1391/1770 train_time:136604ms step_avg:98.92ms
step:1392/1770 train_time:136707ms step_avg:98.92ms
step:1393/1770 train_time:136809ms step_avg:98.92ms
step:1394/1770 train_time:136912ms step_avg:98.92ms
step:1395/1770 train_time:137016ms step_avg:98.93ms
step:1396/1770 train_time:137120ms step_avg:98.93ms
step:1397/1770 train_time:137223ms step_avg:98.94ms
step:1398/1770 train_time:137326ms step_avg:98.94ms
step:1399/1770 train_time:137429ms step_avg:98.94ms
step:1400/1770 train_time:137533ms step_avg:98.94ms
step:1401/1770 train_time:137637ms step_avg:98.95ms
step:1402/1770 train_time:137740ms step_avg:98.95ms
step:1403/1770 train_time:137842ms step_avg:98.95ms
step:1404/1770 train_time:137945ms step_avg:98.96ms
step:1405/1770 train_time:138047ms step_avg:98.96ms
step:1406/1770 train_time:138150ms step_avg:98.96ms
step:1407/1770 train_time:138251ms step_avg:98.96ms
step:1408/1770 train_time:138354ms step_avg:98.97ms
step:1409/1770 train_time:138458ms step_avg:98.97ms
step:1410/1770 train_time:138561ms step_avg:98.97ms
step:1411/1770 train_time:138663ms step_avg:98.97ms
step:1412/1770 train_time:138766ms step_avg:98.98ms
step:1413/1770 train_time:138868ms step_avg:98.98ms
step:1414/1770 train_time:138972ms step_avg:98.98ms
step:1415/1770 train_time:139075ms step_avg:98.99ms
step:1416/1770 train_time:139180ms step_avg:98.99ms
step:1417/1770 train_time:139282ms step_avg:98.99ms
step:1418/1770 train_time:139384ms step_avg:98.99ms
step:1419/1770 train_time:139487ms step_avg:99.00ms
step:1420/1770 train_time:139589ms step_avg:99.00ms
step:1421/1770 train_time:139691ms step_avg:99.00ms
step:1422/1770 train_time:139795ms step_avg:99.00ms
step:1423/1770 train_time:139897ms step_avg:99.01ms
step:1424/1770 train_time:140001ms step_avg:99.01ms
step:1425/1770 train_time:140103ms step_avg:99.01ms
step:1426/1770 train_time:140206ms step_avg:99.02ms
step:1427/1770 train_time:140308ms step_avg:99.02ms
step:1428/1770 train_time:140412ms step_avg:99.02ms
step:1429/1770 train_time:140515ms step_avg:99.02ms
step:1430/1770 train_time:140618ms step_avg:99.03ms
step:1431/1770 train_time:140722ms step_avg:99.03ms
step:1432/1770 train_time:140824ms step_avg:99.03ms
step:1433/1770 train_time:140926ms step_avg:99.03ms
step:1434/1770 train_time:141028ms step_avg:99.04ms
step:1435/1770 train_time:141130ms step_avg:99.04ms
step:1436/1770 train_time:141234ms step_avg:99.04ms
step:1437/1770 train_time:141337ms step_avg:99.04ms
step:1438/1770 train_time:141440ms step_avg:99.05ms
step:1439/1770 train_time:141543ms step_avg:99.05ms
step:1440/1770 train_time:141645ms step_avg:99.05ms
step:1441/1770 train_time:141750ms step_avg:99.06ms
step:1442/1770 train_time:141853ms step_avg:99.06ms
step:1443/1770 train_time:141956ms step_avg:99.06ms
step:1444/1770 train_time:142060ms step_avg:99.07ms
step:1445/1770 train_time:142163ms step_avg:99.07ms
step:1446/1770 train_time:142266ms step_avg:99.07ms
step:1447/1770 train_time:142371ms step_avg:99.08ms
step:1448/1770 train_time:142475ms step_avg:99.08ms
step:1449/1770 train_time:142580ms step_avg:99.08ms
step:1450/1770 train_time:142683ms step_avg:99.09ms
step:1451/1770 train_time:142787ms step_avg:99.09ms
step:1452/1770 train_time:142891ms step_avg:99.09ms
step:1453/1770 train_time:142995ms step_avg:99.10ms
step:1454/1770 train_time:143099ms step_avg:99.10ms
step:1455/1770 train_time:143204ms step_avg:99.10ms
step:1456/1770 train_time:143308ms step_avg:99.11ms
step:1457/1770 train_time:143413ms step_avg:99.11ms
step:1458/1770 train_time:143517ms step_avg:99.11ms
step:1459/1770 train_time:143623ms step_avg:99.12ms
step:1460/1770 train_time:143726ms step_avg:99.12ms
step:1461/1770 train_time:143830ms step_avg:99.12ms
step:1462/1770 train_time:143933ms step_avg:99.13ms
step:1463/1770 train_time:144038ms step_avg:99.13ms
step:1464/1770 train_time:144145ms step_avg:99.14ms
step:1465/1770 train_time:144248ms step_avg:99.14ms
step:1466/1770 train_time:144352ms step_avg:99.14ms
step:1467/1770 train_time:144458ms step_avg:99.15ms
step:1468/1770 train_time:144561ms step_avg:99.15ms
step:1469/1770 train_time:144665ms step_avg:99.15ms
step:1470/1770 train_time:144768ms step_avg:99.16ms
step:1471/1770 train_time:144872ms step_avg:99.16ms
step:1472/1770 train_time:144976ms step_avg:99.16ms
step:1473/1770 train_time:145080ms step_avg:99.17ms
step:1474/1770 train_time:145185ms step_avg:99.17ms
step:1475/1770 train_time:145289ms step_avg:99.17ms
step:1476/1770 train_time:145392ms step_avg:99.18ms
step:1477/1770 train_time:145499ms step_avg:99.18ms
step:1478/1770 train_time:145603ms step_avg:99.18ms
step:1479/1770 train_time:145707ms step_avg:99.19ms
step:1480/1770 train_time:145810ms step_avg:99.19ms
step:1481/1770 train_time:145918ms step_avg:99.20ms
step:1482/1770 train_time:146022ms step_avg:99.20ms
step:1483/1770 train_time:146125ms step_avg:99.20ms
step:1484/1770 train_time:146228ms step_avg:99.20ms
step:1485/1770 train_time:146332ms step_avg:99.21ms
step:1486/1770 train_time:146435ms step_avg:99.21ms
step:1487/1770 train_time:146540ms step_avg:99.21ms
step:1488/1770 train_time:146644ms step_avg:99.22ms
step:1489/1770 train_time:146748ms step_avg:99.22ms
step:1490/1770 train_time:146852ms step_avg:99.22ms
step:1491/1770 train_time:146956ms step_avg:99.23ms
step:1492/1770 train_time:147061ms step_avg:99.23ms
step:1493/1770 train_time:147168ms step_avg:99.24ms
step:1494/1770 train_time:147275ms step_avg:99.24ms
step:1495/1770 train_time:147378ms step_avg:99.24ms
step:1496/1770 train_time:147482ms step_avg:99.25ms
step:1497/1770 train_time:147586ms step_avg:99.25ms
step:1498/1770 train_time:147689ms step_avg:99.25ms
step:1499/1770 train_time:147793ms step_avg:99.26ms
step:1500/1770 train_time:147897ms step_avg:99.26ms
step:1500/1770 val_loss:3.3506 train_time:147999ms step_avg:99.33ms
step:1501/1770 train_time:148021ms step_avg:99.28ms
step:1502/1770 train_time:148116ms step_avg:99.27ms
step:1503/1770 train_time:148219ms step_avg:99.28ms
step:1504/1770 train_time:148324ms step_avg:99.28ms
step:1505/1770 train_time:148430ms step_avg:99.28ms
step:1506/1770 train_time:148534ms step_avg:99.29ms
step:1507/1770 train_time:148638ms step_avg:99.29ms
step:1508/1770 train_time:148744ms step_avg:99.29ms
step:1509/1770 train_time:148848ms step_avg:99.30ms
step:1510/1770 train_time:148951ms step_avg:99.30ms
step:1511/1770 train_time:149056ms step_avg:99.30ms
step:1512/1770 train_time:149160ms step_avg:99.31ms
step:1513/1770 train_time:149264ms step_avg:99.31ms
step:1514/1770 train_time:149369ms step_avg:99.31ms
step:1515/1770 train_time:149473ms step_avg:99.32ms
step:1516/1770 train_time:149577ms step_avg:99.32ms
step:1517/1770 train_time:149680ms step_avg:99.32ms
step:1518/1770 train_time:149786ms step_avg:99.33ms
step:1519/1770 train_time:149890ms step_avg:99.33ms
step:1520/1770 train_time:149995ms step_avg:99.33ms
step:1521/1770 train_time:150098ms step_avg:99.34ms
step:1522/1770 train_time:150202ms step_avg:99.34ms
step:1523/1770 train_time:150307ms step_avg:99.34ms
step:1524/1770 train_time:150411ms step_avg:99.35ms
step:1525/1770 train_time:150515ms step_avg:99.35ms
step:1526/1770 train_time:150619ms step_avg:99.35ms
step:1527/1770 train_time:150723ms step_avg:99.36ms
step:1528/1770 train_time:150830ms step_avg:99.36ms
step:1529/1770 train_time:150934ms step_avg:99.36ms
step:1530/1770 train_time:151037ms step_avg:99.37ms
step:1531/1770 train_time:151141ms step_avg:99.37ms
step:1532/1770 train_time:151245ms step_avg:99.37ms
step:1533/1770 train_time:151350ms step_avg:99.38ms
step:1534/1770 train_time:151454ms step_avg:99.38ms
step:1535/1770 train_time:151558ms step_avg:99.38ms
step:1536/1770 train_time:151661ms step_avg:99.38ms
step:1537/1770 train_time:151765ms step_avg:99.39ms
step:1538/1770 train_time:151871ms step_avg:99.39ms
step:1539/1770 train_time:151974ms step_avg:99.39ms
step:1540/1770 train_time:152080ms step_avg:99.40ms
step:1541/1770 train_time:152185ms step_avg:99.40ms
step:1542/1770 train_time:152290ms step_avg:99.41ms
step:1543/1770 train_time:152393ms step_avg:99.41ms
step:1544/1770 train_time:152499ms step_avg:99.41ms
step:1545/1770 train_time:152603ms step_avg:99.42ms
step:1546/1770 train_time:152707ms step_avg:99.42ms
step:1547/1770 train_time:152811ms step_avg:99.42ms
step:1548/1770 train_time:152915ms step_avg:99.42ms
step:1549/1770 train_time:153019ms step_avg:99.43ms
step:1550/1770 train_time:153123ms step_avg:99.43ms
step:1551/1770 train_time:153227ms step_avg:99.43ms
step:1552/1770 train_time:153333ms step_avg:99.44ms
step:1553/1770 train_time:153437ms step_avg:99.44ms
step:1554/1770 train_time:153540ms step_avg:99.44ms
step:1555/1770 train_time:153644ms step_avg:99.45ms
step:1556/1770 train_time:153749ms step_avg:99.45ms
step:1557/1770 train_time:153853ms step_avg:99.45ms
step:1558/1770 train_time:153956ms step_avg:99.46ms
step:1559/1770 train_time:154060ms step_avg:99.46ms
step:1560/1770 train_time:154163ms step_avg:99.46ms
step:1561/1770 train_time:154270ms step_avg:99.46ms
step:1562/1770 train_time:154373ms step_avg:99.47ms
step:1563/1770 train_time:154477ms step_avg:99.47ms
step:1564/1770 train_time:154580ms step_avg:99.47ms
step:1565/1770 train_time:154683ms step_avg:99.47ms
step:1566/1770 train_time:154787ms step_avg:99.48ms
step:1567/1770 train_time:154892ms step_avg:99.48ms
step:1568/1770 train_time:154996ms step_avg:99.48ms
step:1569/1770 train_time:155103ms step_avg:99.49ms
step:1570/1770 train_time:155207ms step_avg:99.49ms
step:1571/1770 train_time:155311ms step_avg:99.49ms
step:1572/1770 train_time:155415ms step_avg:99.50ms
step:1573/1770 train_time:155521ms step_avg:99.50ms
step:1574/1770 train_time:155625ms step_avg:99.50ms
step:1575/1770 train_time:155728ms step_avg:99.51ms
step:1576/1770 train_time:155833ms step_avg:99.51ms
step:1577/1770 train_time:155938ms step_avg:99.51ms
step:1578/1770 train_time:156043ms step_avg:99.52ms
step:1579/1770 train_time:156148ms step_avg:99.52ms
step:1580/1770 train_time:156252ms step_avg:99.52ms
step:1581/1770 train_time:156359ms step_avg:99.53ms
step:1582/1770 train_time:156463ms step_avg:99.53ms
step:1583/1770 train_time:156567ms step_avg:99.53ms
step:1584/1770 train_time:156673ms step_avg:99.54ms
step:1585/1770 train_time:156776ms step_avg:99.54ms
step:1586/1770 train_time:156884ms step_avg:99.55ms
step:1587/1770 train_time:156989ms step_avg:99.55ms
step:1588/1770 train_time:157094ms step_avg:99.55ms
step:1589/1770 train_time:157200ms step_avg:99.56ms
step:1590/1770 train_time:157303ms step_avg:99.56ms
step:1591/1770 train_time:157407ms step_avg:99.56ms
step:1592/1770 train_time:157512ms step_avg:99.56ms
step:1593/1770 train_time:157615ms step_avg:99.57ms
step:1594/1770 train_time:157718ms step_avg:99.57ms
step:1595/1770 train_time:157822ms step_avg:99.57ms
step:1596/1770 train_time:157927ms step_avg:99.58ms
step:1597/1770 train_time:158031ms step_avg:99.58ms
step:1598/1770 train_time:158136ms step_avg:99.58ms
step:1599/1770 train_time:158240ms step_avg:99.58ms
step:1600/1770 train_time:158346ms step_avg:99.59ms
step:1601/1770 train_time:158451ms step_avg:99.59ms
step:1602/1770 train_time:158556ms step_avg:99.60ms
step:1603/1770 train_time:158660ms step_avg:99.60ms
step:1604/1770 train_time:158762ms step_avg:99.60ms
step:1605/1770 train_time:158866ms step_avg:99.60ms
step:1606/1770 train_time:158972ms step_avg:99.61ms
step:1607/1770 train_time:159079ms step_avg:99.61ms
step:1608/1770 train_time:159183ms step_avg:99.61ms
step:1609/1770 train_time:159287ms step_avg:99.62ms
step:1610/1770 train_time:159394ms step_avg:99.62ms
step:1611/1770 train_time:159500ms step_avg:99.63ms
step:1612/1770 train_time:159605ms step_avg:99.63ms
step:1613/1770 train_time:159710ms step_avg:99.63ms
step:1614/1770 train_time:159815ms step_avg:99.64ms
step:1615/1770 train_time:159918ms step_avg:99.64ms
step:1616/1770 train_time:160022ms step_avg:99.64ms
step:1617/1770 train_time:160129ms step_avg:99.64ms
step:1618/1770 train_time:160233ms step_avg:99.65ms
step:1619/1770 train_time:160338ms step_avg:99.65ms
step:1620/1770 train_time:160442ms step_avg:99.65ms
step:1621/1770 train_time:160546ms step_avg:99.66ms
step:1622/1770 train_time:160651ms step_avg:99.66ms
step:1623/1770 train_time:160757ms step_avg:99.66ms
step:1624/1770 train_time:160860ms step_avg:99.67ms
step:1625/1770 train_time:160964ms step_avg:99.67ms
step:1625/1770 val_loss:3.3194 train_time:161067ms step_avg:99.73ms
step:1626/1770 train_time:161088ms step_avg:99.68ms
step:1627/1770 train_time:161178ms step_avg:99.68ms
step:1628/1770 train_time:161282ms step_avg:99.68ms
step:1629/1770 train_time:161384ms step_avg:99.68ms
step:1630/1770 train_time:161489ms step_avg:99.68ms
step:1631/1770 train_time:161592ms step_avg:99.69ms
step:1632/1770 train_time:161696ms step_avg:99.69ms
step:1633/1770 train_time:161800ms step_avg:99.69ms
step:1634/1770 train_time:161903ms step_avg:99.69ms
step:1635/1770 train_time:162007ms step_avg:99.70ms
step:1636/1770 train_time:162111ms step_avg:99.70ms
step:1637/1770 train_time:162216ms step_avg:99.70ms
step:1638/1770 train_time:162320ms step_avg:99.71ms
step:1639/1770 train_time:162424ms step_avg:99.71ms
step:1640/1770 train_time:162528ms step_avg:99.71ms
step:1641/1770 train_time:162633ms step_avg:99.71ms
step:1642/1770 train_time:162737ms step_avg:99.72ms
step:1643/1770 train_time:162841ms step_avg:99.72ms
step:1644/1770 train_time:162946ms step_avg:99.72ms
step:1645/1770 train_time:163050ms step_avg:99.72ms
step:1646/1770 train_time:163156ms step_avg:99.73ms
step:1647/1770 train_time:163261ms step_avg:99.73ms
step:1648/1770 train_time:163364ms step_avg:99.73ms
step:1649/1770 train_time:163468ms step_avg:99.74ms
step:1650/1770 train_time:163572ms step_avg:99.74ms
step:1651/1770 train_time:163677ms step_avg:99.74ms
step:1652/1770 train_time:163781ms step_avg:99.74ms
step:1653/1770 train_time:163885ms step_avg:99.75ms
step:1654/1770 train_time:163993ms step_avg:99.75ms
step:1655/1770 train_time:164100ms step_avg:99.76ms
step:1656/1770 train_time:164203ms step_avg:99.76ms
step:1657/1770 train_time:164309ms step_avg:99.76ms
step:1658/1770 train_time:164413ms step_avg:99.77ms
step:1659/1770 train_time:164519ms step_avg:99.77ms
step:1660/1770 train_time:164623ms step_avg:99.77ms
step:1661/1770 train_time:164728ms step_avg:99.77ms
step:1662/1770 train_time:164834ms step_avg:99.78ms
step:1663/1770 train_time:164937ms step_avg:99.78ms
step:1664/1770 train_time:165041ms step_avg:99.78ms
step:1665/1770 train_time:165144ms step_avg:99.79ms
step:1666/1770 train_time:165249ms step_avg:99.79ms
step:1667/1770 train_time:165353ms step_avg:99.79ms
step:1668/1770 train_time:165457ms step_avg:99.79ms
step:1669/1770 train_time:165560ms step_avg:99.80ms
step:1670/1770 train_time:165663ms step_avg:99.80ms
step:1671/1770 train_time:165767ms step_avg:99.80ms
step:1672/1770 train_time:165872ms step_avg:99.80ms
step:1673/1770 train_time:165978ms step_avg:99.81ms
step:1674/1770 train_time:166082ms step_avg:99.81ms
step:1675/1770 train_time:166185ms step_avg:99.81ms
step:1676/1770 train_time:166290ms step_avg:99.81ms
step:1677/1770 train_time:166399ms step_avg:99.82ms
step:1678/1770 train_time:166502ms step_avg:99.82ms
step:1679/1770 train_time:166606ms step_avg:99.82ms
step:1680/1770 train_time:166711ms step_avg:99.83ms
step:1681/1770 train_time:166816ms step_avg:99.83ms
step:1682/1770 train_time:166922ms step_avg:99.83ms
step:1683/1770 train_time:167026ms step_avg:99.84ms
step:1684/1770 train_time:167130ms step_avg:99.84ms
step:1685/1770 train_time:167234ms step_avg:99.84ms
step:1686/1770 train_time:167339ms step_avg:99.84ms
step:1687/1770 train_time:167444ms step_avg:99.85ms
step:1688/1770 train_time:167548ms step_avg:99.85ms
step:1689/1770 train_time:167652ms step_avg:99.85ms
step:1690/1770 train_time:167757ms step_avg:99.86ms
step:1691/1770 train_time:167860ms step_avg:99.86ms
step:1692/1770 train_time:167964ms step_avg:99.86ms
step:1693/1770 train_time:168070ms step_avg:99.86ms
step:1694/1770 train_time:168174ms step_avg:99.87ms
step:1695/1770 train_time:168279ms step_avg:99.87ms
step:1696/1770 train_time:168384ms step_avg:99.87ms
step:1697/1770 train_time:168490ms step_avg:99.88ms
step:1698/1770 train_time:168595ms step_avg:99.88ms
step:1699/1770 train_time:168699ms step_avg:99.88ms
step:1700/1770 train_time:168802ms step_avg:99.88ms
step:1701/1770 train_time:168906ms step_avg:99.89ms
step:1702/1770 train_time:169011ms step_avg:99.89ms
step:1703/1770 train_time:169115ms step_avg:99.89ms
step:1704/1770 train_time:169219ms step_avg:99.89ms
step:1705/1770 train_time:169322ms step_avg:99.89ms
step:1706/1770 train_time:169425ms step_avg:99.90ms
step:1707/1770 train_time:169530ms step_avg:99.90ms
step:1708/1770 train_time:169636ms step_avg:99.90ms
step:1709/1770 train_time:169742ms step_avg:99.91ms
step:1710/1770 train_time:169849ms step_avg:99.91ms
step:1711/1770 train_time:169955ms step_avg:99.91ms
step:1712/1770 train_time:170060ms step_avg:99.92ms
step:1713/1770 train_time:170164ms step_avg:99.92ms
step:1714/1770 train_time:170269ms step_avg:99.92ms
step:1715/1770 train_time:170374ms step_avg:99.93ms
step:1716/1770 train_time:170479ms step_avg:99.93ms
step:1717/1770 train_time:170583ms step_avg:99.93ms
step:1718/1770 train_time:170689ms step_avg:99.93ms
step:1719/1770 train_time:170795ms step_avg:99.94ms
step:1720/1770 train_time:170901ms step_avg:99.94ms
step:1721/1770 train_time:171005ms step_avg:99.94ms
step:1722/1770 train_time:171112ms step_avg:99.95ms
step:1723/1770 train_time:171219ms step_avg:99.95ms
step:1724/1770 train_time:171326ms step_avg:99.96ms
step:1725/1770 train_time:171434ms step_avg:99.96ms
step:1726/1770 train_time:171540ms step_avg:99.97ms
step:1727/1770 train_time:171644ms step_avg:99.97ms
step:1728/1770 train_time:171751ms step_avg:99.97ms
step:1729/1770 train_time:171856ms step_avg:99.97ms
step:1730/1770 train_time:171961ms step_avg:99.98ms
step:1731/1770 train_time:172068ms step_avg:99.98ms
step:1732/1770 train_time:172172ms step_avg:99.98ms
step:1733/1770 train_time:172279ms step_avg:99.99ms
step:1734/1770 train_time:172382ms step_avg:99.99ms
step:1735/1770 train_time:172488ms step_avg:99.99ms
step:1736/1770 train_time:172592ms step_avg:100.00ms
step:1737/1770 train_time:172698ms step_avg:100.00ms
step:1738/1770 train_time:172802ms step_avg:100.00ms
step:1739/1770 train_time:172907ms step_avg:100.00ms
step:1740/1770 train_time:173012ms step_avg:100.01ms
step:1741/1770 train_time:173120ms step_avg:100.01ms
step:1742/1770 train_time:173227ms step_avg:100.02ms
step:1743/1770 train_time:173334ms step_avg:100.02ms
step:1744/1770 train_time:173439ms step_avg:100.02ms
step:1745/1770 train_time:173543ms step_avg:100.02ms
step:1746/1770 train_time:173651ms step_avg:100.03ms
step:1747/1770 train_time:173755ms step_avg:100.03ms
step:1748/1770 train_time:173863ms step_avg:100.04ms
step:1749/1770 train_time:173968ms step_avg:100.04ms
step:1750/1770 train_time:174073ms step_avg:100.04ms
step:1750/1770 val_loss:3.2951 train_time:174176ms step_avg:100.10ms
step:1751/1770 train_time:174198ms step_avg:100.06ms
step:1752/1770 train_time:174292ms step_avg:100.05ms
step:1753/1770 train_time:174398ms step_avg:100.06ms
step:1754/1770 train_time:174503ms step_avg:100.06ms
step:1755/1770 train_time:174608ms step_avg:100.06ms
step:1756/1770 train_time:174713ms step_avg:100.06ms
step:1757/1770 train_time:174819ms step_avg:100.07ms
step:1758/1770 train_time:174923ms step_avg:100.07ms
step:1759/1770 train_time:175029ms step_avg:100.07ms
step:1760/1770 train_time:175134ms step_avg:100.08ms
step:1761/1770 train_time:175242ms step_avg:100.08ms
step:1762/1770 train_time:175350ms step_avg:100.09ms
step:1763/1770 train_time:175454ms step_avg:100.09ms
step:1764/1770 train_time:175560ms step_avg:100.09ms
step:1765/1770 train_time:175664ms step_avg:100.09ms
step:1766/1770 train_time:175773ms step_avg:100.10ms
step:1767/1770 train_time:175877ms step_avg:100.10ms
step:1768/1770 train_time:175982ms step_avg:100.10ms
step:1769/1770 train_time:176086ms step_avg:100.11ms
step:1770/1770 train_time:176190ms step_avg:100.11ms
step:1770/1770 val_loss:3.2913 train_time:176295ms step_avg:100.17ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
