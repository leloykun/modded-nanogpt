import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 03:54:56 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23334ms step_avg:nanms
step:2/1770 train_time:23747ms step_avg:nanms
step:3/1770 train_time:23841ms step_avg:nanms
step:4/1770 train_time:23935ms step_avg:nanms
step:5/1770 train_time:24029ms step_avg:nanms
step:6/1770 train_time:24123ms step_avg:nanms
step:7/1770 train_time:24217ms step_avg:nanms
step:8/1770 train_time:24311ms step_avg:nanms
step:9/1770 train_time:24406ms step_avg:nanms
step:10/1770 train_time:24500ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.45ms
step:14/1770 train_time:378ms step_avg:94.52ms
step:15/1770 train_time:473ms step_avg:94.59ms
step:16/1770 train_time:567ms step_avg:94.57ms
step:17/1770 train_time:662ms step_avg:94.55ms
step:18/1770 train_time:757ms step_avg:94.60ms
step:19/1770 train_time:851ms step_avg:94.58ms
step:20/1770 train_time:947ms step_avg:94.66ms
step:21/1770 train_time:1041ms step_avg:94.65ms
step:22/1770 train_time:1136ms step_avg:94.66ms
step:23/1770 train_time:1230ms step_avg:94.62ms
step:24/1770 train_time:1324ms step_avg:94.61ms
step:25/1770 train_time:1419ms step_avg:94.60ms
step:26/1770 train_time:1514ms step_avg:94.60ms
step:27/1770 train_time:1609ms step_avg:94.63ms
step:28/1770 train_time:1703ms step_avg:94.61ms
step:29/1770 train_time:1798ms step_avg:94.61ms
step:30/1770 train_time:1892ms step_avg:94.60ms
step:31/1770 train_time:1986ms step_avg:94.59ms
step:32/1770 train_time:2081ms step_avg:94.61ms
step:33/1770 train_time:2176ms step_avg:94.62ms
step:34/1770 train_time:2271ms step_avg:94.62ms
step:35/1770 train_time:2365ms step_avg:94.61ms
step:36/1770 train_time:2460ms step_avg:94.61ms
step:37/1770 train_time:2555ms step_avg:94.62ms
step:38/1770 train_time:2650ms step_avg:94.64ms
step:39/1770 train_time:2744ms step_avg:94.63ms
step:40/1770 train_time:2840ms step_avg:94.66ms
step:41/1770 train_time:2935ms step_avg:94.68ms
step:42/1770 train_time:3030ms step_avg:94.69ms
step:43/1770 train_time:3125ms step_avg:94.69ms
step:44/1770 train_time:3219ms step_avg:94.68ms
step:45/1770 train_time:3314ms step_avg:94.69ms
step:46/1770 train_time:3409ms step_avg:94.69ms
step:47/1770 train_time:3503ms step_avg:94.68ms
step:48/1770 train_time:3598ms step_avg:94.69ms
step:49/1770 train_time:3693ms step_avg:94.69ms
step:50/1770 train_time:3788ms step_avg:94.70ms
step:51/1770 train_time:3882ms step_avg:94.68ms
step:52/1770 train_time:3977ms step_avg:94.69ms
step:53/1770 train_time:4071ms step_avg:94.68ms
step:54/1770 train_time:4165ms step_avg:94.67ms
step:55/1770 train_time:4260ms step_avg:94.67ms
step:56/1770 train_time:4355ms step_avg:94.68ms
step:57/1770 train_time:4450ms step_avg:94.68ms
step:58/1770 train_time:4544ms step_avg:94.67ms
step:59/1770 train_time:4640ms step_avg:94.68ms
step:60/1770 train_time:4734ms step_avg:94.69ms
step:61/1770 train_time:4829ms step_avg:94.68ms
step:62/1770 train_time:4923ms step_avg:94.68ms
step:63/1770 train_time:5018ms step_avg:94.69ms
step:64/1770 train_time:5113ms step_avg:94.69ms
step:65/1770 train_time:5208ms step_avg:94.69ms
step:66/1770 train_time:5302ms step_avg:94.68ms
step:67/1770 train_time:5397ms step_avg:94.68ms
step:68/1770 train_time:5492ms step_avg:94.69ms
step:69/1770 train_time:5586ms step_avg:94.68ms
step:70/1770 train_time:5681ms step_avg:94.68ms
step:71/1770 train_time:5776ms step_avg:94.69ms
step:72/1770 train_time:5872ms step_avg:94.71ms
step:73/1770 train_time:5966ms step_avg:94.69ms
step:74/1770 train_time:6060ms step_avg:94.69ms
step:75/1770 train_time:6155ms step_avg:94.70ms
step:76/1770 train_time:6250ms step_avg:94.69ms
step:77/1770 train_time:6344ms step_avg:94.69ms
step:78/1770 train_time:6439ms step_avg:94.70ms
step:79/1770 train_time:6534ms step_avg:94.70ms
step:80/1770 train_time:6629ms step_avg:94.70ms
step:81/1770 train_time:6724ms step_avg:94.70ms
step:82/1770 train_time:6819ms step_avg:94.70ms
step:83/1770 train_time:6913ms step_avg:94.70ms
step:84/1770 train_time:7008ms step_avg:94.70ms
step:85/1770 train_time:7103ms step_avg:94.70ms
step:86/1770 train_time:7197ms step_avg:94.70ms
step:87/1770 train_time:7292ms step_avg:94.70ms
step:88/1770 train_time:7386ms step_avg:94.69ms
step:89/1770 train_time:7481ms step_avg:94.70ms
step:90/1770 train_time:7575ms step_avg:94.69ms
step:91/1770 train_time:7669ms step_avg:94.68ms
step:92/1770 train_time:7764ms step_avg:94.68ms
step:93/1770 train_time:7859ms step_avg:94.68ms
step:94/1770 train_time:7954ms step_avg:94.70ms
step:95/1770 train_time:8050ms step_avg:94.70ms
step:96/1770 train_time:8145ms step_avg:94.70ms
step:97/1770 train_time:8239ms step_avg:94.70ms
step:98/1770 train_time:8334ms step_avg:94.70ms
step:99/1770 train_time:8428ms step_avg:94.70ms
step:100/1770 train_time:8523ms step_avg:94.70ms
step:101/1770 train_time:8617ms step_avg:94.70ms
step:102/1770 train_time:8712ms step_avg:94.70ms
step:103/1770 train_time:8806ms step_avg:94.69ms
step:104/1770 train_time:8901ms step_avg:94.69ms
step:105/1770 train_time:8996ms step_avg:94.69ms
step:106/1770 train_time:9090ms step_avg:94.69ms
step:107/1770 train_time:9184ms step_avg:94.68ms
step:108/1770 train_time:9280ms step_avg:94.69ms
step:109/1770 train_time:9375ms step_avg:94.70ms
step:110/1770 train_time:9470ms step_avg:94.70ms
step:111/1770 train_time:9564ms step_avg:94.69ms
step:112/1770 train_time:9659ms step_avg:94.70ms
step:113/1770 train_time:9754ms step_avg:94.70ms
step:114/1770 train_time:9848ms step_avg:94.69ms
step:115/1770 train_time:9942ms step_avg:94.69ms
step:116/1770 train_time:10037ms step_avg:94.69ms
step:117/1770 train_time:10132ms step_avg:94.69ms
step:118/1770 train_time:10226ms step_avg:94.68ms
step:119/1770 train_time:10320ms step_avg:94.68ms
step:120/1770 train_time:10416ms step_avg:94.69ms
step:121/1770 train_time:10510ms step_avg:94.68ms
step:122/1770 train_time:10604ms step_avg:94.68ms
step:123/1770 train_time:10699ms step_avg:94.68ms
step:124/1770 train_time:10793ms step_avg:94.68ms
step:125/1770 train_time:10888ms step_avg:94.68ms
step:125/1770 val_loss:4.6461 train_time:10980ms step_avg:95.48ms
step:126/1770 train_time:11003ms step_avg:94.85ms
step:127/1770 train_time:11081ms step_avg:94.71ms
step:128/1770 train_time:11179ms step_avg:94.74ms
step:129/1770 train_time:11279ms step_avg:94.78ms
step:130/1770 train_time:11375ms step_avg:94.79ms
step:131/1770 train_time:11470ms step_avg:94.79ms
step:132/1770 train_time:11565ms step_avg:94.79ms
step:133/1770 train_time:11659ms step_avg:94.79ms
step:134/1770 train_time:11754ms step_avg:94.79ms
step:135/1770 train_time:11849ms step_avg:94.79ms
step:136/1770 train_time:11944ms step_avg:94.79ms
step:137/1770 train_time:12039ms step_avg:94.80ms
step:138/1770 train_time:12135ms step_avg:94.80ms
step:139/1770 train_time:12230ms step_avg:94.80ms
step:140/1770 train_time:12325ms step_avg:94.81ms
step:141/1770 train_time:12420ms step_avg:94.81ms
step:142/1770 train_time:12515ms step_avg:94.81ms
step:143/1770 train_time:12610ms step_avg:94.81ms
step:144/1770 train_time:12705ms step_avg:94.82ms
step:145/1770 train_time:12801ms step_avg:94.82ms
step:146/1770 train_time:12896ms step_avg:94.82ms
step:147/1770 train_time:12991ms step_avg:94.82ms
step:148/1770 train_time:13086ms step_avg:94.82ms
step:149/1770 train_time:13181ms step_avg:94.83ms
step:150/1770 train_time:13276ms step_avg:94.83ms
step:151/1770 train_time:13371ms step_avg:94.83ms
step:152/1770 train_time:13466ms step_avg:94.83ms
step:153/1770 train_time:13562ms step_avg:94.84ms
step:154/1770 train_time:13658ms step_avg:94.85ms
step:155/1770 train_time:13753ms step_avg:94.85ms
step:156/1770 train_time:13848ms step_avg:94.85ms
step:157/1770 train_time:13943ms step_avg:94.85ms
step:158/1770 train_time:14039ms step_avg:94.86ms
step:159/1770 train_time:14134ms step_avg:94.86ms
step:160/1770 train_time:14229ms step_avg:94.86ms
step:161/1770 train_time:14324ms step_avg:94.86ms
step:162/1770 train_time:14420ms step_avg:94.87ms
step:163/1770 train_time:14516ms step_avg:94.87ms
step:164/1770 train_time:14611ms step_avg:94.88ms
step:165/1770 train_time:14706ms step_avg:94.88ms
step:166/1770 train_time:14802ms step_avg:94.88ms
step:167/1770 train_time:14897ms step_avg:94.88ms
step:168/1770 train_time:14992ms step_avg:94.88ms
step:169/1770 train_time:15087ms step_avg:94.88ms
step:170/1770 train_time:15182ms step_avg:94.89ms
step:171/1770 train_time:15277ms step_avg:94.89ms
step:172/1770 train_time:15372ms step_avg:94.89ms
step:173/1770 train_time:15467ms step_avg:94.89ms
step:174/1770 train_time:15563ms step_avg:94.89ms
step:175/1770 train_time:15659ms step_avg:94.90ms
step:176/1770 train_time:15754ms step_avg:94.90ms
step:177/1770 train_time:15850ms step_avg:94.91ms
step:178/1770 train_time:15945ms step_avg:94.91ms
step:179/1770 train_time:16040ms step_avg:94.91ms
step:180/1770 train_time:16135ms step_avg:94.91ms
step:181/1770 train_time:16231ms step_avg:94.92ms
step:182/1770 train_time:16326ms step_avg:94.92ms
step:183/1770 train_time:16421ms step_avg:94.92ms
step:184/1770 train_time:16517ms step_avg:94.92ms
step:185/1770 train_time:16612ms step_avg:94.92ms
step:186/1770 train_time:16707ms step_avg:94.92ms
step:187/1770 train_time:16802ms step_avg:94.93ms
step:188/1770 train_time:16898ms step_avg:94.93ms
step:189/1770 train_time:16993ms step_avg:94.93ms
step:190/1770 train_time:17088ms step_avg:94.93ms
step:191/1770 train_time:17183ms step_avg:94.94ms
step:192/1770 train_time:17278ms step_avg:94.94ms
step:193/1770 train_time:17373ms step_avg:94.94ms
step:194/1770 train_time:17468ms step_avg:94.93ms
step:195/1770 train_time:17564ms step_avg:94.94ms
step:196/1770 train_time:17659ms step_avg:94.94ms
step:197/1770 train_time:17755ms step_avg:94.94ms
step:198/1770 train_time:17849ms step_avg:94.94ms
step:199/1770 train_time:17945ms step_avg:94.95ms
step:200/1770 train_time:18040ms step_avg:94.95ms
step:201/1770 train_time:18136ms step_avg:94.95ms
step:202/1770 train_time:18231ms step_avg:94.95ms
step:203/1770 train_time:18326ms step_avg:94.95ms
step:204/1770 train_time:18421ms step_avg:94.95ms
step:205/1770 train_time:18517ms step_avg:94.96ms
step:206/1770 train_time:18612ms step_avg:94.96ms
step:207/1770 train_time:18706ms step_avg:94.96ms
step:208/1770 train_time:18802ms step_avg:94.96ms
step:209/1770 train_time:18898ms step_avg:94.96ms
step:210/1770 train_time:18993ms step_avg:94.96ms
step:211/1770 train_time:19088ms step_avg:94.96ms
step:212/1770 train_time:19183ms step_avg:94.97ms
step:213/1770 train_time:19278ms step_avg:94.97ms
step:214/1770 train_time:19373ms step_avg:94.97ms
step:215/1770 train_time:19468ms step_avg:94.97ms
step:216/1770 train_time:19564ms step_avg:94.97ms
step:217/1770 train_time:19659ms step_avg:94.97ms
step:218/1770 train_time:19754ms step_avg:94.97ms
step:219/1770 train_time:19849ms step_avg:94.97ms
step:220/1770 train_time:19944ms step_avg:94.97ms
step:221/1770 train_time:20040ms step_avg:94.97ms
step:222/1770 train_time:20135ms step_avg:94.98ms
step:223/1770 train_time:20230ms step_avg:94.98ms
step:224/1770 train_time:20325ms step_avg:94.98ms
step:225/1770 train_time:20421ms step_avg:94.98ms
step:226/1770 train_time:20516ms step_avg:94.98ms
step:227/1770 train_time:20612ms step_avg:94.98ms
step:228/1770 train_time:20707ms step_avg:94.98ms
step:229/1770 train_time:20802ms step_avg:94.99ms
step:230/1770 train_time:20898ms step_avg:94.99ms
step:231/1770 train_time:20993ms step_avg:94.99ms
step:232/1770 train_time:21088ms step_avg:94.99ms
step:233/1770 train_time:21184ms step_avg:94.99ms
step:234/1770 train_time:21279ms step_avg:95.00ms
step:235/1770 train_time:21374ms step_avg:95.00ms
step:236/1770 train_time:21469ms step_avg:95.00ms
step:237/1770 train_time:21564ms step_avg:95.00ms
step:238/1770 train_time:21660ms step_avg:95.00ms
step:239/1770 train_time:21755ms step_avg:95.00ms
step:240/1770 train_time:21850ms step_avg:95.00ms
step:241/1770 train_time:21945ms step_avg:95.00ms
step:242/1770 train_time:22040ms step_avg:95.00ms
step:243/1770 train_time:22135ms step_avg:95.00ms
step:244/1770 train_time:22230ms step_avg:95.00ms
step:245/1770 train_time:22325ms step_avg:95.00ms
step:246/1770 train_time:22421ms step_avg:95.00ms
step:247/1770 train_time:22516ms step_avg:95.01ms
step:248/1770 train_time:22611ms step_avg:95.01ms
step:249/1770 train_time:22706ms step_avg:95.01ms
step:250/1770 train_time:22802ms step_avg:95.01ms
step:250/1770 val_loss:4.1105 train_time:22897ms step_avg:95.40ms
step:251/1770 train_time:22918ms step_avg:95.10ms
step:252/1770 train_time:22999ms step_avg:95.04ms
step:253/1770 train_time:23097ms step_avg:95.05ms
step:254/1770 train_time:23192ms step_avg:95.05ms
step:255/1770 train_time:23287ms step_avg:95.05ms
step:256/1770 train_time:23382ms step_avg:95.05ms
step:257/1770 train_time:23477ms step_avg:95.05ms
step:258/1770 train_time:23572ms step_avg:95.05ms
step:259/1770 train_time:23667ms step_avg:95.05ms
step:260/1770 train_time:23762ms step_avg:95.05ms
step:261/1770 train_time:23857ms step_avg:95.05ms
step:262/1770 train_time:23953ms step_avg:95.05ms
step:263/1770 train_time:24048ms step_avg:95.05ms
step:264/1770 train_time:24143ms step_avg:95.05ms
step:265/1770 train_time:24239ms step_avg:95.05ms
step:266/1770 train_time:24335ms step_avg:95.06ms
step:267/1770 train_time:24431ms step_avg:95.06ms
step:268/1770 train_time:24527ms step_avg:95.07ms
step:269/1770 train_time:24622ms step_avg:95.07ms
step:270/1770 train_time:24718ms step_avg:95.07ms
step:271/1770 train_time:24814ms step_avg:95.07ms
step:272/1770 train_time:24909ms step_avg:95.07ms
step:273/1770 train_time:25005ms step_avg:95.07ms
step:274/1770 train_time:25099ms step_avg:95.07ms
step:275/1770 train_time:25195ms step_avg:95.08ms
step:276/1770 train_time:25291ms step_avg:95.08ms
step:277/1770 train_time:25386ms step_avg:95.08ms
step:278/1770 train_time:25481ms step_avg:95.08ms
step:279/1770 train_time:25577ms step_avg:95.08ms
step:280/1770 train_time:25673ms step_avg:95.08ms
step:281/1770 train_time:25769ms step_avg:95.09ms
step:282/1770 train_time:25864ms step_avg:95.09ms
step:283/1770 train_time:25960ms step_avg:95.09ms
step:284/1770 train_time:26055ms step_avg:95.09ms
step:285/1770 train_time:26151ms step_avg:95.10ms
step:286/1770 train_time:26247ms step_avg:95.10ms
step:287/1770 train_time:26342ms step_avg:95.10ms
step:288/1770 train_time:26438ms step_avg:95.10ms
step:289/1770 train_time:26533ms step_avg:95.10ms
step:290/1770 train_time:26629ms step_avg:95.10ms
step:291/1770 train_time:26724ms step_avg:95.10ms
step:292/1770 train_time:26820ms step_avg:95.11ms
step:293/1770 train_time:26916ms step_avg:95.11ms
step:294/1770 train_time:27012ms step_avg:95.11ms
step:295/1770 train_time:27107ms step_avg:95.11ms
step:296/1770 train_time:27202ms step_avg:95.11ms
step:297/1770 train_time:27298ms step_avg:95.12ms
step:298/1770 train_time:27394ms step_avg:95.12ms
step:299/1770 train_time:27489ms step_avg:95.12ms
step:300/1770 train_time:27585ms step_avg:95.12ms
step:301/1770 train_time:27681ms step_avg:95.12ms
step:302/1770 train_time:27776ms step_avg:95.12ms
step:303/1770 train_time:27872ms step_avg:95.13ms
step:304/1770 train_time:27968ms step_avg:95.13ms
step:305/1770 train_time:28063ms step_avg:95.13ms
step:306/1770 train_time:28159ms step_avg:95.13ms
step:307/1770 train_time:28255ms step_avg:95.14ms
step:308/1770 train_time:28351ms step_avg:95.14ms
step:309/1770 train_time:28447ms step_avg:95.14ms
step:310/1770 train_time:28542ms step_avg:95.14ms
step:311/1770 train_time:28638ms step_avg:95.14ms
step:312/1770 train_time:28734ms step_avg:95.14ms
step:313/1770 train_time:28829ms step_avg:95.15ms
step:314/1770 train_time:28925ms step_avg:95.15ms
step:315/1770 train_time:29020ms step_avg:95.15ms
step:316/1770 train_time:29116ms step_avg:95.15ms
step:317/1770 train_time:29211ms step_avg:95.15ms
step:318/1770 train_time:29307ms step_avg:95.15ms
step:319/1770 train_time:29402ms step_avg:95.15ms
step:320/1770 train_time:29498ms step_avg:95.15ms
step:321/1770 train_time:29594ms step_avg:95.16ms
step:322/1770 train_time:29690ms step_avg:95.16ms
step:323/1770 train_time:29785ms step_avg:95.16ms
step:324/1770 train_time:29880ms step_avg:95.16ms
step:325/1770 train_time:29976ms step_avg:95.16ms
step:326/1770 train_time:30072ms step_avg:95.16ms
step:327/1770 train_time:30167ms step_avg:95.16ms
step:328/1770 train_time:30263ms step_avg:95.17ms
step:329/1770 train_time:30359ms step_avg:95.17ms
step:330/1770 train_time:30454ms step_avg:95.17ms
step:331/1770 train_time:30550ms step_avg:95.17ms
step:332/1770 train_time:30646ms step_avg:95.17ms
step:333/1770 train_time:30741ms step_avg:95.17ms
step:334/1770 train_time:30837ms step_avg:95.17ms
step:335/1770 train_time:30932ms step_avg:95.18ms
step:336/1770 train_time:31028ms step_avg:95.18ms
step:337/1770 train_time:31124ms step_avg:95.18ms
step:338/1770 train_time:31220ms step_avg:95.18ms
step:339/1770 train_time:31315ms step_avg:95.18ms
step:340/1770 train_time:31411ms step_avg:95.18ms
step:341/1770 train_time:31506ms step_avg:95.19ms
step:342/1770 train_time:31602ms step_avg:95.19ms
step:343/1770 train_time:31698ms step_avg:95.19ms
step:344/1770 train_time:31793ms step_avg:95.19ms
step:345/1770 train_time:31888ms step_avg:95.19ms
step:346/1770 train_time:31984ms step_avg:95.19ms
step:347/1770 train_time:32079ms step_avg:95.19ms
step:348/1770 train_time:32175ms step_avg:95.19ms
step:349/1770 train_time:32271ms step_avg:95.20ms
step:350/1770 train_time:32367ms step_avg:95.20ms
step:351/1770 train_time:32463ms step_avg:95.20ms
step:352/1770 train_time:32558ms step_avg:95.20ms
step:353/1770 train_time:32654ms step_avg:95.20ms
step:354/1770 train_time:32750ms step_avg:95.20ms
step:355/1770 train_time:32846ms step_avg:95.20ms
step:356/1770 train_time:32941ms step_avg:95.21ms
step:357/1770 train_time:33037ms step_avg:95.21ms
step:358/1770 train_time:33133ms step_avg:95.21ms
step:359/1770 train_time:33229ms step_avg:95.21ms
step:360/1770 train_time:33324ms step_avg:95.21ms
step:361/1770 train_time:33420ms step_avg:95.21ms
step:362/1770 train_time:33516ms step_avg:95.21ms
step:363/1770 train_time:33611ms step_avg:95.22ms
step:364/1770 train_time:33707ms step_avg:95.22ms
step:365/1770 train_time:33802ms step_avg:95.22ms
step:366/1770 train_time:33898ms step_avg:95.22ms
step:367/1770 train_time:33993ms step_avg:95.22ms
step:368/1770 train_time:34089ms step_avg:95.22ms
step:369/1770 train_time:34184ms step_avg:95.22ms
step:370/1770 train_time:34280ms step_avg:95.22ms
step:371/1770 train_time:34376ms step_avg:95.22ms
step:372/1770 train_time:34472ms step_avg:95.23ms
step:373/1770 train_time:34568ms step_avg:95.23ms
step:374/1770 train_time:34664ms step_avg:95.23ms
step:375/1770 train_time:34759ms step_avg:95.23ms
step:375/1770 val_loss:3.9125 train_time:34853ms step_avg:95.49ms
step:376/1770 train_time:34878ms step_avg:95.29ms
step:377/1770 train_time:34960ms step_avg:95.26ms
step:378/1770 train_time:35060ms step_avg:95.27ms
step:379/1770 train_time:35156ms step_avg:95.27ms
step:380/1770 train_time:35252ms step_avg:95.27ms
step:381/1770 train_time:35348ms step_avg:95.28ms
step:382/1770 train_time:35442ms step_avg:95.28ms
step:383/1770 train_time:35538ms step_avg:95.28ms
step:384/1770 train_time:35634ms step_avg:95.28ms
step:385/1770 train_time:35729ms step_avg:95.28ms
step:386/1770 train_time:35825ms step_avg:95.28ms
step:387/1770 train_time:35920ms step_avg:95.28ms
step:388/1770 train_time:36016ms step_avg:95.28ms
step:389/1770 train_time:36111ms step_avg:95.28ms
step:390/1770 train_time:36208ms step_avg:95.28ms
step:391/1770 train_time:36303ms step_avg:95.28ms
step:392/1770 train_time:36399ms step_avg:95.29ms
step:393/1770 train_time:36495ms step_avg:95.29ms
step:394/1770 train_time:36591ms step_avg:95.29ms
step:395/1770 train_time:36688ms step_avg:95.29ms
step:396/1770 train_time:36785ms step_avg:95.30ms
step:397/1770 train_time:36882ms step_avg:95.30ms
step:398/1770 train_time:36980ms step_avg:95.31ms
step:399/1770 train_time:37078ms step_avg:95.32ms
step:400/1770 train_time:37176ms step_avg:95.32ms
step:401/1770 train_time:37274ms step_avg:95.33ms
step:402/1770 train_time:37372ms step_avg:95.34ms
step:403/1770 train_time:37469ms step_avg:95.34ms
step:404/1770 train_time:37567ms step_avg:95.35ms
step:405/1770 train_time:37664ms step_avg:95.35ms
step:406/1770 train_time:37761ms step_avg:95.36ms
step:407/1770 train_time:37859ms step_avg:95.36ms
step:408/1770 train_time:37956ms step_avg:95.37ms
step:409/1770 train_time:38054ms step_avg:95.37ms
step:410/1770 train_time:38152ms step_avg:95.38ms
step:411/1770 train_time:38249ms step_avg:95.39ms
step:412/1770 train_time:38347ms step_avg:95.39ms
step:413/1770 train_time:38444ms step_avg:95.39ms
step:414/1770 train_time:38542ms step_avg:95.40ms
step:415/1770 train_time:38639ms step_avg:95.40ms
step:416/1770 train_time:38737ms step_avg:95.41ms
step:417/1770 train_time:38835ms step_avg:95.42ms
step:418/1770 train_time:38932ms step_avg:95.42ms
step:419/1770 train_time:39030ms step_avg:95.43ms
step:420/1770 train_time:39127ms step_avg:95.43ms
step:421/1770 train_time:39224ms step_avg:95.44ms
step:422/1770 train_time:39322ms step_avg:95.44ms
step:423/1770 train_time:39419ms step_avg:95.45ms
step:424/1770 train_time:39517ms step_avg:95.45ms
step:425/1770 train_time:39615ms step_avg:95.46ms
step:426/1770 train_time:39712ms step_avg:95.46ms
step:427/1770 train_time:39810ms step_avg:95.47ms
step:428/1770 train_time:39907ms step_avg:95.47ms
step:429/1770 train_time:40005ms step_avg:95.48ms
step:430/1770 train_time:40102ms step_avg:95.48ms
step:431/1770 train_time:40199ms step_avg:95.48ms
step:432/1770 train_time:40297ms step_avg:95.49ms
step:433/1770 train_time:40395ms step_avg:95.50ms
step:434/1770 train_time:40492ms step_avg:95.50ms
step:435/1770 train_time:40590ms step_avg:95.51ms
step:436/1770 train_time:40687ms step_avg:95.51ms
step:437/1770 train_time:40784ms step_avg:95.51ms
step:438/1770 train_time:40881ms step_avg:95.52ms
step:439/1770 train_time:40979ms step_avg:95.52ms
step:440/1770 train_time:41077ms step_avg:95.53ms
step:441/1770 train_time:41175ms step_avg:95.53ms
step:442/1770 train_time:41272ms step_avg:95.54ms
step:443/1770 train_time:41370ms step_avg:95.54ms
step:444/1770 train_time:41467ms step_avg:95.55ms
step:445/1770 train_time:41565ms step_avg:95.55ms
step:446/1770 train_time:41661ms step_avg:95.55ms
step:447/1770 train_time:41759ms step_avg:95.56ms
step:448/1770 train_time:41857ms step_avg:95.56ms
step:449/1770 train_time:41955ms step_avg:95.57ms
step:450/1770 train_time:42052ms step_avg:95.57ms
step:451/1770 train_time:42150ms step_avg:95.58ms
step:452/1770 train_time:42248ms step_avg:95.58ms
step:453/1770 train_time:42345ms step_avg:95.59ms
step:454/1770 train_time:42443ms step_avg:95.59ms
step:455/1770 train_time:42540ms step_avg:95.59ms
step:456/1770 train_time:42637ms step_avg:95.60ms
step:457/1770 train_time:42735ms step_avg:95.60ms
step:458/1770 train_time:42832ms step_avg:95.61ms
step:459/1770 train_time:42930ms step_avg:95.61ms
step:460/1770 train_time:43027ms step_avg:95.62ms
step:461/1770 train_time:43124ms step_avg:95.62ms
step:462/1770 train_time:43222ms step_avg:95.62ms
step:463/1770 train_time:43319ms step_avg:95.63ms
step:464/1770 train_time:43417ms step_avg:95.63ms
step:465/1770 train_time:43515ms step_avg:95.64ms
step:466/1770 train_time:43614ms step_avg:95.64ms
step:467/1770 train_time:43711ms step_avg:95.65ms
step:468/1770 train_time:43809ms step_avg:95.65ms
step:469/1770 train_time:43907ms step_avg:95.66ms
step:470/1770 train_time:44004ms step_avg:95.66ms
step:471/1770 train_time:44101ms step_avg:95.66ms
step:472/1770 train_time:44199ms step_avg:95.67ms
step:473/1770 train_time:44297ms step_avg:95.67ms
step:474/1770 train_time:44394ms step_avg:95.68ms
step:475/1770 train_time:44492ms step_avg:95.68ms
step:476/1770 train_time:44590ms step_avg:95.69ms
step:477/1770 train_time:44687ms step_avg:95.69ms
step:478/1770 train_time:44784ms step_avg:95.69ms
step:479/1770 train_time:44882ms step_avg:95.70ms
step:480/1770 train_time:44979ms step_avg:95.70ms
step:481/1770 train_time:45077ms step_avg:95.71ms
step:482/1770 train_time:45176ms step_avg:95.71ms
step:483/1770 train_time:45273ms step_avg:95.71ms
step:484/1770 train_time:45371ms step_avg:95.72ms
step:485/1770 train_time:45468ms step_avg:95.72ms
step:486/1770 train_time:45565ms step_avg:95.72ms
step:487/1770 train_time:45662ms step_avg:95.73ms
step:488/1770 train_time:45760ms step_avg:95.73ms
step:489/1770 train_time:45857ms step_avg:95.74ms
step:490/1770 train_time:45955ms step_avg:95.74ms
step:491/1770 train_time:46052ms step_avg:95.74ms
step:492/1770 train_time:46150ms step_avg:95.75ms
step:493/1770 train_time:46247ms step_avg:95.75ms
step:494/1770 train_time:46345ms step_avg:95.75ms
step:495/1770 train_time:46442ms step_avg:95.76ms
step:496/1770 train_time:46540ms step_avg:95.76ms
step:497/1770 train_time:46638ms step_avg:95.77ms
step:498/1770 train_time:46736ms step_avg:95.77ms
step:499/1770 train_time:46833ms step_avg:95.77ms
step:500/1770 train_time:46931ms step_avg:95.78ms
step:500/1770 val_loss:3.7580 train_time:47026ms step_avg:95.97ms
step:501/1770 train_time:47048ms step_avg:95.82ms
step:502/1770 train_time:47132ms step_avg:95.80ms
step:503/1770 train_time:47230ms step_avg:95.80ms
step:504/1770 train_time:47328ms step_avg:95.81ms
step:505/1770 train_time:47425ms step_avg:95.81ms
step:506/1770 train_time:47522ms step_avg:95.81ms
step:507/1770 train_time:47620ms step_avg:95.81ms
step:508/1770 train_time:47717ms step_avg:95.82ms
step:509/1770 train_time:47815ms step_avg:95.82ms
step:510/1770 train_time:47912ms step_avg:95.82ms
step:511/1770 train_time:48010ms step_avg:95.83ms
step:512/1770 train_time:48108ms step_avg:95.83ms
step:513/1770 train_time:48205ms step_avg:95.84ms
step:514/1770 train_time:48303ms step_avg:95.84ms
step:515/1770 train_time:48400ms step_avg:95.84ms
step:516/1770 train_time:48497ms step_avg:95.84ms
step:517/1770 train_time:48595ms step_avg:95.85ms
step:518/1770 train_time:48693ms step_avg:95.85ms
step:519/1770 train_time:48791ms step_avg:95.86ms
step:520/1770 train_time:48889ms step_avg:95.86ms
step:521/1770 train_time:48986ms step_avg:95.86ms
step:522/1770 train_time:49083ms step_avg:95.87ms
step:523/1770 train_time:49181ms step_avg:95.87ms
step:524/1770 train_time:49278ms step_avg:95.87ms
step:525/1770 train_time:49376ms step_avg:95.88ms
step:526/1770 train_time:49473ms step_avg:95.88ms
step:527/1770 train_time:49572ms step_avg:95.88ms
step:528/1770 train_time:49670ms step_avg:95.89ms
step:529/1770 train_time:49768ms step_avg:95.89ms
step:530/1770 train_time:49866ms step_avg:95.90ms
step:531/1770 train_time:49964ms step_avg:95.90ms
step:532/1770 train_time:50062ms step_avg:95.90ms
step:533/1770 train_time:50159ms step_avg:95.91ms
step:534/1770 train_time:50257ms step_avg:95.91ms
step:535/1770 train_time:50355ms step_avg:95.91ms
step:536/1770 train_time:50453ms step_avg:95.92ms
step:537/1770 train_time:50551ms step_avg:95.92ms
step:538/1770 train_time:50649ms step_avg:95.93ms
step:539/1770 train_time:50748ms step_avg:95.93ms
step:540/1770 train_time:50846ms step_avg:95.94ms
step:541/1770 train_time:50944ms step_avg:95.94ms
step:542/1770 train_time:51042ms step_avg:95.94ms
step:543/1770 train_time:51139ms step_avg:95.95ms
step:544/1770 train_time:51237ms step_avg:95.95ms
step:545/1770 train_time:51335ms step_avg:95.95ms
step:546/1770 train_time:51433ms step_avg:95.96ms
step:547/1770 train_time:51530ms step_avg:95.96ms
step:548/1770 train_time:51628ms step_avg:95.96ms
step:549/1770 train_time:51726ms step_avg:95.97ms
step:550/1770 train_time:51824ms step_avg:95.97ms
step:551/1770 train_time:51922ms step_avg:95.97ms
step:552/1770 train_time:52020ms step_avg:95.98ms
step:553/1770 train_time:52118ms step_avg:95.98ms
step:554/1770 train_time:52215ms step_avg:95.98ms
step:555/1770 train_time:52313ms step_avg:95.99ms
step:556/1770 train_time:52411ms step_avg:95.99ms
step:557/1770 train_time:52510ms step_avg:96.00ms
step:558/1770 train_time:52607ms step_avg:96.00ms
step:559/1770 train_time:52705ms step_avg:96.00ms
step:560/1770 train_time:52803ms step_avg:96.01ms
step:561/1770 train_time:52901ms step_avg:96.01ms
step:562/1770 train_time:52998ms step_avg:96.01ms
step:563/1770 train_time:53096ms step_avg:96.01ms
step:564/1770 train_time:53194ms step_avg:96.02ms
step:565/1770 train_time:53292ms step_avg:96.02ms
step:566/1770 train_time:53390ms step_avg:96.03ms
step:567/1770 train_time:53488ms step_avg:96.03ms
step:568/1770 train_time:53586ms step_avg:96.03ms
step:569/1770 train_time:53684ms step_avg:96.04ms
step:570/1770 train_time:53781ms step_avg:96.04ms
step:571/1770 train_time:53879ms step_avg:96.04ms
step:572/1770 train_time:53977ms step_avg:96.04ms
step:573/1770 train_time:54075ms step_avg:96.05ms
step:574/1770 train_time:54173ms step_avg:96.05ms
step:575/1770 train_time:54271ms step_avg:96.06ms
step:576/1770 train_time:54369ms step_avg:96.06ms
step:577/1770 train_time:54467ms step_avg:96.06ms
step:578/1770 train_time:54565ms step_avg:96.06ms
step:579/1770 train_time:54663ms step_avg:96.07ms
step:580/1770 train_time:54760ms step_avg:96.07ms
step:581/1770 train_time:54858ms step_avg:96.07ms
step:582/1770 train_time:54956ms step_avg:96.08ms
step:583/1770 train_time:55054ms step_avg:96.08ms
step:584/1770 train_time:55152ms step_avg:96.08ms
step:585/1770 train_time:55250ms step_avg:96.09ms
step:586/1770 train_time:55348ms step_avg:96.09ms
step:587/1770 train_time:55446ms step_avg:96.09ms
step:588/1770 train_time:55544ms step_avg:96.10ms
step:589/1770 train_time:55642ms step_avg:96.10ms
step:590/1770 train_time:55740ms step_avg:96.10ms
step:591/1770 train_time:55838ms step_avg:96.11ms
step:592/1770 train_time:55936ms step_avg:96.11ms
step:593/1770 train_time:56034ms step_avg:96.11ms
step:594/1770 train_time:56133ms step_avg:96.12ms
step:595/1770 train_time:56231ms step_avg:96.12ms
step:596/1770 train_time:56329ms step_avg:96.12ms
step:597/1770 train_time:56427ms step_avg:96.13ms
step:598/1770 train_time:56524ms step_avg:96.13ms
step:599/1770 train_time:56622ms step_avg:96.13ms
step:600/1770 train_time:56720ms step_avg:96.14ms
step:601/1770 train_time:56817ms step_avg:96.14ms
step:602/1770 train_time:56915ms step_avg:96.14ms
step:603/1770 train_time:57013ms step_avg:96.14ms
step:604/1770 train_time:57111ms step_avg:96.15ms
step:605/1770 train_time:57209ms step_avg:96.15ms
step:606/1770 train_time:57308ms step_avg:96.15ms
step:607/1770 train_time:57406ms step_avg:96.16ms
step:608/1770 train_time:57503ms step_avg:96.16ms
step:609/1770 train_time:57601ms step_avg:96.16ms
step:610/1770 train_time:57698ms step_avg:96.16ms
step:611/1770 train_time:57796ms step_avg:96.17ms
step:612/1770 train_time:57894ms step_avg:96.17ms
step:613/1770 train_time:57992ms step_avg:96.17ms
step:614/1770 train_time:58090ms step_avg:96.18ms
step:615/1770 train_time:58188ms step_avg:96.18ms
step:616/1770 train_time:58285ms step_avg:96.18ms
step:617/1770 train_time:58383ms step_avg:96.18ms
step:618/1770 train_time:58481ms step_avg:96.19ms
step:619/1770 train_time:58578ms step_avg:96.19ms
step:620/1770 train_time:58676ms step_avg:96.19ms
step:621/1770 train_time:58774ms step_avg:96.19ms
step:622/1770 train_time:58872ms step_avg:96.20ms
step:623/1770 train_time:58970ms step_avg:96.20ms
step:624/1770 train_time:59069ms step_avg:96.20ms
step:625/1770 train_time:59167ms step_avg:96.21ms
step:625/1770 val_loss:3.6710 train_time:59262ms step_avg:96.36ms
step:626/1770 train_time:59284ms step_avg:96.24ms
step:627/1770 train_time:59371ms step_avg:96.23ms
step:628/1770 train_time:59473ms step_avg:96.23ms
step:629/1770 train_time:59571ms step_avg:96.24ms
step:630/1770 train_time:59669ms step_avg:96.24ms
step:631/1770 train_time:59767ms step_avg:96.24ms
step:632/1770 train_time:59865ms step_avg:96.25ms
step:633/1770 train_time:59963ms step_avg:96.25ms
step:634/1770 train_time:60061ms step_avg:96.25ms
step:635/1770 train_time:60159ms step_avg:96.25ms
step:636/1770 train_time:60256ms step_avg:96.26ms
step:637/1770 train_time:60354ms step_avg:96.26ms
step:638/1770 train_time:60453ms step_avg:96.26ms
step:639/1770 train_time:60551ms step_avg:96.26ms
step:640/1770 train_time:60649ms step_avg:96.27ms
step:641/1770 train_time:60747ms step_avg:96.27ms
step:642/1770 train_time:60844ms step_avg:96.27ms
step:643/1770 train_time:60942ms step_avg:96.27ms
step:644/1770 train_time:61040ms step_avg:96.28ms
step:645/1770 train_time:61137ms step_avg:96.28ms
step:646/1770 train_time:61234ms step_avg:96.28ms
step:647/1770 train_time:61333ms step_avg:96.28ms
step:648/1770 train_time:61431ms step_avg:96.29ms
step:649/1770 train_time:61529ms step_avg:96.29ms
step:650/1770 train_time:61628ms step_avg:96.29ms
step:651/1770 train_time:61726ms step_avg:96.30ms
step:652/1770 train_time:61824ms step_avg:96.30ms
step:653/1770 train_time:61922ms step_avg:96.30ms
step:654/1770 train_time:62019ms step_avg:96.30ms
step:655/1770 train_time:62117ms step_avg:96.31ms
step:656/1770 train_time:62215ms step_avg:96.31ms
step:657/1770 train_time:62313ms step_avg:96.31ms
step:658/1770 train_time:62413ms step_avg:96.32ms
step:659/1770 train_time:62512ms step_avg:96.32ms
step:660/1770 train_time:62612ms step_avg:96.33ms
step:661/1770 train_time:62712ms step_avg:96.33ms
step:662/1770 train_time:62812ms step_avg:96.34ms
step:663/1770 train_time:62912ms step_avg:96.34ms
step:664/1770 train_time:63013ms step_avg:96.35ms
step:665/1770 train_time:63113ms step_avg:96.36ms
step:666/1770 train_time:63213ms step_avg:96.36ms
step:667/1770 train_time:63313ms step_avg:96.37ms
step:668/1770 train_time:63413ms step_avg:96.37ms
step:669/1770 train_time:63512ms step_avg:96.38ms
step:670/1770 train_time:63612ms step_avg:96.38ms
step:671/1770 train_time:63711ms step_avg:96.39ms
step:672/1770 train_time:63811ms step_avg:96.39ms
step:673/1770 train_time:63911ms step_avg:96.40ms
step:674/1770 train_time:64011ms step_avg:96.40ms
step:675/1770 train_time:64112ms step_avg:96.41ms
step:676/1770 train_time:64212ms step_avg:96.41ms
step:677/1770 train_time:64312ms step_avg:96.42ms
step:678/1770 train_time:64412ms step_avg:96.43ms
step:679/1770 train_time:64512ms step_avg:96.43ms
step:680/1770 train_time:64612ms step_avg:96.44ms
step:681/1770 train_time:64712ms step_avg:96.44ms
step:682/1770 train_time:64812ms step_avg:96.45ms
step:683/1770 train_time:64912ms step_avg:96.45ms
step:684/1770 train_time:65012ms step_avg:96.46ms
step:685/1770 train_time:65112ms step_avg:96.46ms
step:686/1770 train_time:65212ms step_avg:96.47ms
step:687/1770 train_time:65312ms step_avg:96.47ms
step:688/1770 train_time:65413ms step_avg:96.48ms
step:689/1770 train_time:65512ms step_avg:96.48ms
step:690/1770 train_time:65612ms step_avg:96.49ms
step:691/1770 train_time:65712ms step_avg:96.49ms
step:692/1770 train_time:65813ms step_avg:96.50ms
step:693/1770 train_time:65912ms step_avg:96.50ms
step:694/1770 train_time:66013ms step_avg:96.51ms
step:695/1770 train_time:66112ms step_avg:96.51ms
step:696/1770 train_time:66212ms step_avg:96.52ms
step:697/1770 train_time:66312ms step_avg:96.52ms
step:698/1770 train_time:66412ms step_avg:96.53ms
step:699/1770 train_time:66513ms step_avg:96.54ms
step:700/1770 train_time:66613ms step_avg:96.54ms
step:701/1770 train_time:66713ms step_avg:96.54ms
step:702/1770 train_time:66813ms step_avg:96.55ms
step:703/1770 train_time:66912ms step_avg:96.55ms
step:704/1770 train_time:67013ms step_avg:96.56ms
step:705/1770 train_time:67112ms step_avg:96.56ms
step:706/1770 train_time:67213ms step_avg:96.57ms
step:707/1770 train_time:67313ms step_avg:96.57ms
step:708/1770 train_time:67413ms step_avg:96.58ms
step:709/1770 train_time:67512ms step_avg:96.58ms
step:710/1770 train_time:67612ms step_avg:96.59ms
step:711/1770 train_time:67712ms step_avg:96.59ms
step:712/1770 train_time:67813ms step_avg:96.60ms
step:713/1770 train_time:67913ms step_avg:96.60ms
step:714/1770 train_time:68013ms step_avg:96.61ms
step:715/1770 train_time:68113ms step_avg:96.61ms
step:716/1770 train_time:68212ms step_avg:96.62ms
step:717/1770 train_time:68312ms step_avg:96.62ms
step:718/1770 train_time:68412ms step_avg:96.63ms
step:719/1770 train_time:68512ms step_avg:96.63ms
step:720/1770 train_time:68612ms step_avg:96.64ms
step:721/1770 train_time:68712ms step_avg:96.64ms
step:722/1770 train_time:68812ms step_avg:96.65ms
step:723/1770 train_time:68912ms step_avg:96.65ms
step:724/1770 train_time:69012ms step_avg:96.66ms
step:725/1770 train_time:69112ms step_avg:96.66ms
step:726/1770 train_time:69213ms step_avg:96.67ms
step:727/1770 train_time:69313ms step_avg:96.67ms
step:728/1770 train_time:69413ms step_avg:96.68ms
step:729/1770 train_time:69513ms step_avg:96.68ms
step:730/1770 train_time:69613ms step_avg:96.68ms
step:731/1770 train_time:69712ms step_avg:96.69ms
step:732/1770 train_time:69812ms step_avg:96.69ms
step:733/1770 train_time:69912ms step_avg:96.70ms
step:734/1770 train_time:70012ms step_avg:96.70ms
step:735/1770 train_time:70112ms step_avg:96.71ms
step:736/1770 train_time:70212ms step_avg:96.71ms
step:737/1770 train_time:70312ms step_avg:96.72ms
step:738/1770 train_time:70412ms step_avg:96.72ms
step:739/1770 train_time:70512ms step_avg:96.72ms
step:740/1770 train_time:70611ms step_avg:96.73ms
step:741/1770 train_time:70712ms step_avg:96.73ms
step:742/1770 train_time:70811ms step_avg:96.74ms
step:743/1770 train_time:70911ms step_avg:96.74ms
step:744/1770 train_time:71011ms step_avg:96.75ms
step:745/1770 train_time:71111ms step_avg:96.75ms
step:746/1770 train_time:71211ms step_avg:96.75ms
step:747/1770 train_time:71311ms step_avg:96.76ms
step:748/1770 train_time:71411ms step_avg:96.76ms
step:749/1770 train_time:71511ms step_avg:96.77ms
step:750/1770 train_time:71611ms step_avg:96.77ms
step:750/1770 val_loss:3.6045 train_time:71710ms step_avg:96.91ms
step:751/1770 train_time:71732ms step_avg:96.80ms
step:752/1770 train_time:71815ms step_avg:96.79ms
step:753/1770 train_time:71916ms step_avg:96.79ms
step:754/1770 train_time:72016ms step_avg:96.80ms
step:755/1770 train_time:72115ms step_avg:96.80ms
step:756/1770 train_time:72215ms step_avg:96.80ms
step:757/1770 train_time:72314ms step_avg:96.81ms
step:758/1770 train_time:72414ms step_avg:96.81ms
step:759/1770 train_time:72513ms step_avg:96.81ms
step:760/1770 train_time:72613ms step_avg:96.82ms
step:761/1770 train_time:72712ms step_avg:96.82ms
step:762/1770 train_time:72812ms step_avg:96.82ms
step:763/1770 train_time:72912ms step_avg:96.83ms
step:764/1770 train_time:73012ms step_avg:96.83ms
step:765/1770 train_time:73113ms step_avg:96.84ms
step:766/1770 train_time:73213ms step_avg:96.84ms
step:767/1770 train_time:73314ms step_avg:96.85ms
step:768/1770 train_time:73413ms step_avg:96.85ms
step:769/1770 train_time:73514ms step_avg:96.86ms
step:770/1770 train_time:73613ms step_avg:96.86ms
step:771/1770 train_time:73713ms step_avg:96.86ms
step:772/1770 train_time:73812ms step_avg:96.87ms
step:773/1770 train_time:73912ms step_avg:96.87ms
step:774/1770 train_time:74012ms step_avg:96.87ms
step:775/1770 train_time:74112ms step_avg:96.88ms
step:776/1770 train_time:74213ms step_avg:96.88ms
step:777/1770 train_time:74313ms step_avg:96.89ms
step:778/1770 train_time:74413ms step_avg:96.89ms
step:779/1770 train_time:74513ms step_avg:96.90ms
step:780/1770 train_time:74613ms step_avg:96.90ms
step:781/1770 train_time:74713ms step_avg:96.90ms
step:782/1770 train_time:74813ms step_avg:96.91ms
step:783/1770 train_time:74912ms step_avg:96.91ms
step:784/1770 train_time:75013ms step_avg:96.92ms
step:785/1770 train_time:75112ms step_avg:96.92ms
step:786/1770 train_time:75213ms step_avg:96.92ms
step:787/1770 train_time:75313ms step_avg:96.93ms
step:788/1770 train_time:75413ms step_avg:96.93ms
step:789/1770 train_time:75514ms step_avg:96.94ms
step:790/1770 train_time:75613ms step_avg:96.94ms
step:791/1770 train_time:75714ms step_avg:96.94ms
step:792/1770 train_time:75813ms step_avg:96.95ms
step:793/1770 train_time:75914ms step_avg:96.95ms
step:794/1770 train_time:76014ms step_avg:96.96ms
step:795/1770 train_time:76113ms step_avg:96.96ms
step:796/1770 train_time:76213ms step_avg:96.96ms
step:797/1770 train_time:76313ms step_avg:96.97ms
step:798/1770 train_time:76413ms step_avg:96.97ms
step:799/1770 train_time:76513ms step_avg:96.98ms
step:800/1770 train_time:76613ms step_avg:96.98ms
step:801/1770 train_time:76713ms step_avg:96.98ms
step:802/1770 train_time:76814ms step_avg:96.99ms
step:803/1770 train_time:76914ms step_avg:96.99ms
step:804/1770 train_time:77014ms step_avg:96.99ms
step:805/1770 train_time:77113ms step_avg:97.00ms
step:806/1770 train_time:77213ms step_avg:97.00ms
step:807/1770 train_time:77313ms step_avg:97.00ms
step:808/1770 train_time:77412ms step_avg:97.01ms
step:809/1770 train_time:77513ms step_avg:97.01ms
step:810/1770 train_time:77613ms step_avg:97.02ms
step:811/1770 train_time:77713ms step_avg:97.02ms
step:812/1770 train_time:77813ms step_avg:97.02ms
step:813/1770 train_time:77914ms step_avg:97.03ms
step:814/1770 train_time:78013ms step_avg:97.03ms
step:815/1770 train_time:78113ms step_avg:97.04ms
step:816/1770 train_time:78213ms step_avg:97.04ms
step:817/1770 train_time:78313ms step_avg:97.04ms
step:818/1770 train_time:78413ms step_avg:97.05ms
step:819/1770 train_time:78513ms step_avg:97.05ms
step:820/1770 train_time:78613ms step_avg:97.05ms
step:821/1770 train_time:78714ms step_avg:97.06ms
step:822/1770 train_time:78814ms step_avg:97.06ms
step:823/1770 train_time:78914ms step_avg:97.07ms
step:824/1770 train_time:79014ms step_avg:97.07ms
step:825/1770 train_time:79114ms step_avg:97.07ms
step:826/1770 train_time:79213ms step_avg:97.07ms
step:827/1770 train_time:79313ms step_avg:97.08ms
step:828/1770 train_time:79413ms step_avg:97.08ms
step:829/1770 train_time:79513ms step_avg:97.09ms
step:830/1770 train_time:79613ms step_avg:97.09ms
step:831/1770 train_time:79713ms step_avg:97.09ms
step:832/1770 train_time:79813ms step_avg:97.10ms
step:833/1770 train_time:79913ms step_avg:97.10ms
step:834/1770 train_time:80013ms step_avg:97.10ms
step:835/1770 train_time:80113ms step_avg:97.11ms
step:836/1770 train_time:80213ms step_avg:97.11ms
step:837/1770 train_time:80314ms step_avg:97.11ms
step:838/1770 train_time:80414ms step_avg:97.12ms
step:839/1770 train_time:80513ms step_avg:97.12ms
step:840/1770 train_time:80613ms step_avg:97.12ms
step:841/1770 train_time:80713ms step_avg:97.13ms
step:842/1770 train_time:80813ms step_avg:97.13ms
step:843/1770 train_time:80913ms step_avg:97.13ms
step:844/1770 train_time:81013ms step_avg:97.14ms
step:845/1770 train_time:81113ms step_avg:97.14ms
step:846/1770 train_time:81213ms step_avg:97.14ms
step:847/1770 train_time:81313ms step_avg:97.15ms
step:848/1770 train_time:81414ms step_avg:97.15ms
step:849/1770 train_time:81514ms step_avg:97.16ms
step:850/1770 train_time:81614ms step_avg:97.16ms
step:851/1770 train_time:81714ms step_avg:97.16ms
step:852/1770 train_time:81813ms step_avg:97.17ms
step:853/1770 train_time:81913ms step_avg:97.17ms
step:854/1770 train_time:82013ms step_avg:97.17ms
step:855/1770 train_time:82113ms step_avg:97.18ms
step:856/1770 train_time:82214ms step_avg:97.18ms
step:857/1770 train_time:82314ms step_avg:97.18ms
step:858/1770 train_time:82413ms step_avg:97.19ms
step:859/1770 train_time:82513ms step_avg:97.19ms
step:860/1770 train_time:82613ms step_avg:97.19ms
step:861/1770 train_time:82713ms step_avg:97.20ms
step:862/1770 train_time:82814ms step_avg:97.20ms
step:863/1770 train_time:82914ms step_avg:97.20ms
step:864/1770 train_time:83014ms step_avg:97.21ms
step:865/1770 train_time:83113ms step_avg:97.21ms
step:866/1770 train_time:83214ms step_avg:97.21ms
step:867/1770 train_time:83314ms step_avg:97.22ms
step:868/1770 train_time:83413ms step_avg:97.22ms
step:869/1770 train_time:83513ms step_avg:97.22ms
step:870/1770 train_time:83613ms step_avg:97.22ms
step:871/1770 train_time:83713ms step_avg:97.23ms
step:872/1770 train_time:83813ms step_avg:97.23ms
step:873/1770 train_time:83913ms step_avg:97.23ms
step:874/1770 train_time:84013ms step_avg:97.24ms
step:875/1770 train_time:84113ms step_avg:97.24ms
step:875/1770 val_loss:3.5558 train_time:84212ms step_avg:97.35ms
step:876/1770 train_time:84233ms step_avg:97.27ms
step:877/1770 train_time:84323ms step_avg:97.26ms
step:878/1770 train_time:84425ms step_avg:97.26ms
step:879/1770 train_time:84525ms step_avg:97.27ms
step:880/1770 train_time:84625ms step_avg:97.27ms
step:881/1770 train_time:84724ms step_avg:97.27ms
step:882/1770 train_time:84824ms step_avg:97.28ms
step:883/1770 train_time:84924ms step_avg:97.28ms
step:884/1770 train_time:85024ms step_avg:97.28ms
step:885/1770 train_time:85123ms step_avg:97.28ms
step:886/1770 train_time:85224ms step_avg:97.29ms
step:887/1770 train_time:85326ms step_avg:97.29ms
step:888/1770 train_time:85428ms step_avg:97.30ms
step:889/1770 train_time:85527ms step_avg:97.30ms
step:890/1770 train_time:85627ms step_avg:97.30ms
step:891/1770 train_time:85727ms step_avg:97.31ms
step:892/1770 train_time:85828ms step_avg:97.31ms
step:893/1770 train_time:85929ms step_avg:97.31ms
step:894/1770 train_time:86029ms step_avg:97.32ms
step:895/1770 train_time:86129ms step_avg:97.32ms
step:896/1770 train_time:86230ms step_avg:97.32ms
step:897/1770 train_time:86331ms step_avg:97.33ms
step:898/1770 train_time:86433ms step_avg:97.33ms
step:899/1770 train_time:86532ms step_avg:97.34ms
step:900/1770 train_time:86632ms step_avg:97.34ms
step:901/1770 train_time:86732ms step_avg:97.34ms
step:902/1770 train_time:86833ms step_avg:97.35ms
step:903/1770 train_time:86933ms step_avg:97.35ms
step:904/1770 train_time:87033ms step_avg:97.35ms
step:905/1770 train_time:87133ms step_avg:97.35ms
step:906/1770 train_time:87232ms step_avg:97.36ms
step:907/1770 train_time:87333ms step_avg:97.36ms
step:908/1770 train_time:87433ms step_avg:97.36ms
step:909/1770 train_time:87533ms step_avg:97.37ms
step:910/1770 train_time:87633ms step_avg:97.37ms
step:911/1770 train_time:87733ms step_avg:97.37ms
step:912/1770 train_time:87833ms step_avg:97.38ms
step:913/1770 train_time:87933ms step_avg:97.38ms
step:914/1770 train_time:88033ms step_avg:97.38ms
step:915/1770 train_time:88132ms step_avg:97.38ms
step:916/1770 train_time:88232ms step_avg:97.39ms
step:917/1770 train_time:88332ms step_avg:97.39ms
step:918/1770 train_time:88433ms step_avg:97.39ms
step:919/1770 train_time:88533ms step_avg:97.40ms
step:920/1770 train_time:88635ms step_avg:97.40ms
step:921/1770 train_time:88736ms step_avg:97.41ms
step:922/1770 train_time:88837ms step_avg:97.41ms
step:923/1770 train_time:88937ms step_avg:97.41ms
step:924/1770 train_time:89037ms step_avg:97.41ms
step:925/1770 train_time:89138ms step_avg:97.42ms
step:926/1770 train_time:89238ms step_avg:97.42ms
step:927/1770 train_time:89339ms step_avg:97.43ms
step:928/1770 train_time:89440ms step_avg:97.43ms
step:929/1770 train_time:89542ms step_avg:97.43ms
step:930/1770 train_time:89643ms step_avg:97.44ms
step:931/1770 train_time:89745ms step_avg:97.44ms
step:932/1770 train_time:89847ms step_avg:97.45ms
step:933/1770 train_time:89948ms step_avg:97.45ms
step:934/1770 train_time:90050ms step_avg:97.46ms
step:935/1770 train_time:90151ms step_avg:97.46ms
step:936/1770 train_time:90253ms step_avg:97.47ms
step:937/1770 train_time:90354ms step_avg:97.47ms
step:938/1770 train_time:90456ms step_avg:97.47ms
step:939/1770 train_time:90557ms step_avg:97.48ms
step:940/1770 train_time:90657ms step_avg:97.48ms
step:941/1770 train_time:90759ms step_avg:97.49ms
step:942/1770 train_time:90860ms step_avg:97.49ms
step:943/1770 train_time:90961ms step_avg:97.49ms
step:944/1770 train_time:91062ms step_avg:97.50ms
step:945/1770 train_time:91163ms step_avg:97.50ms
step:946/1770 train_time:91265ms step_avg:97.51ms
step:947/1770 train_time:91368ms step_avg:97.51ms
step:948/1770 train_time:91469ms step_avg:97.52ms
step:949/1770 train_time:91571ms step_avg:97.52ms
step:950/1770 train_time:91672ms step_avg:97.52ms
step:951/1770 train_time:91774ms step_avg:97.53ms
step:952/1770 train_time:91874ms step_avg:97.53ms
step:953/1770 train_time:91976ms step_avg:97.54ms
step:954/1770 train_time:92077ms step_avg:97.54ms
step:955/1770 train_time:92177ms step_avg:97.54ms
step:956/1770 train_time:92278ms step_avg:97.55ms
step:957/1770 train_time:92379ms step_avg:97.55ms
step:958/1770 train_time:92479ms step_avg:97.55ms
step:959/1770 train_time:92581ms step_avg:97.56ms
step:960/1770 train_time:92682ms step_avg:97.56ms
step:961/1770 train_time:92783ms step_avg:97.56ms
step:962/1770 train_time:92885ms step_avg:97.57ms
step:963/1770 train_time:92986ms step_avg:97.57ms
step:964/1770 train_time:93089ms step_avg:97.58ms
step:965/1770 train_time:93191ms step_avg:97.58ms
step:966/1770 train_time:93292ms step_avg:97.59ms
step:967/1770 train_time:93394ms step_avg:97.59ms
step:968/1770 train_time:93496ms step_avg:97.59ms
step:969/1770 train_time:93597ms step_avg:97.60ms
step:970/1770 train_time:93698ms step_avg:97.60ms
step:971/1770 train_time:93799ms step_avg:97.61ms
step:972/1770 train_time:93899ms step_avg:97.61ms
step:973/1770 train_time:94000ms step_avg:97.61ms
step:974/1770 train_time:94101ms step_avg:97.62ms
step:975/1770 train_time:94204ms step_avg:97.62ms
step:976/1770 train_time:94305ms step_avg:97.62ms
step:977/1770 train_time:94408ms step_avg:97.63ms
step:978/1770 train_time:94510ms step_avg:97.63ms
step:979/1770 train_time:94611ms step_avg:97.64ms
step:980/1770 train_time:94713ms step_avg:97.64ms
step:981/1770 train_time:94814ms step_avg:97.65ms
step:982/1770 train_time:94915ms step_avg:97.65ms
step:983/1770 train_time:95017ms step_avg:97.65ms
step:984/1770 train_time:95118ms step_avg:97.66ms
step:985/1770 train_time:95219ms step_avg:97.66ms
step:986/1770 train_time:95320ms step_avg:97.66ms
step:987/1770 train_time:95421ms step_avg:97.67ms
step:988/1770 train_time:95522ms step_avg:97.67ms
step:989/1770 train_time:95626ms step_avg:97.68ms
step:990/1770 train_time:95728ms step_avg:97.68ms
step:991/1770 train_time:95830ms step_avg:97.69ms
step:992/1770 train_time:95933ms step_avg:97.69ms
step:993/1770 train_time:96034ms step_avg:97.70ms
step:994/1770 train_time:96136ms step_avg:97.70ms
step:995/1770 train_time:96237ms step_avg:97.70ms
step:996/1770 train_time:96338ms step_avg:97.71ms
step:997/1770 train_time:96438ms step_avg:97.71ms
step:998/1770 train_time:96539ms step_avg:97.71ms
step:999/1770 train_time:96639ms step_avg:97.71ms
step:1000/1770 train_time:96741ms step_avg:97.72ms
step:1000/1770 val_loss:3.5199 train_time:96841ms step_avg:97.82ms
step:1001/1770 train_time:96862ms step_avg:97.74ms
step:1002/1770 train_time:96955ms step_avg:97.74ms
step:1003/1770 train_time:97057ms step_avg:97.74ms
step:1004/1770 train_time:97159ms step_avg:97.75ms
step:1005/1770 train_time:97260ms step_avg:97.75ms
step:1006/1770 train_time:97361ms step_avg:97.75ms
step:1007/1770 train_time:97461ms step_avg:97.75ms
step:1008/1770 train_time:97563ms step_avg:97.76ms
step:1009/1770 train_time:97664ms step_avg:97.76ms
step:1010/1770 train_time:97765ms step_avg:97.76ms
step:1011/1770 train_time:97867ms step_avg:97.77ms
step:1012/1770 train_time:97968ms step_avg:97.77ms
step:1013/1770 train_time:98069ms step_avg:97.78ms
step:1014/1770 train_time:98171ms step_avg:97.78ms
step:1015/1770 train_time:98272ms step_avg:97.78ms
step:1016/1770 train_time:98373ms step_avg:97.79ms
step:1017/1770 train_time:98476ms step_avg:97.79ms
step:1018/1770 train_time:98578ms step_avg:97.80ms
step:1019/1770 train_time:98679ms step_avg:97.80ms
step:1020/1770 train_time:98780ms step_avg:97.80ms
step:1021/1770 train_time:98882ms step_avg:97.81ms
step:1022/1770 train_time:98983ms step_avg:97.81ms
step:1023/1770 train_time:99085ms step_avg:97.81ms
step:1024/1770 train_time:99186ms step_avg:97.82ms
step:1025/1770 train_time:99287ms step_avg:97.82ms
step:1026/1770 train_time:99388ms step_avg:97.82ms
step:1027/1770 train_time:99490ms step_avg:97.83ms
step:1028/1770 train_time:99592ms step_avg:97.83ms
step:1029/1770 train_time:99693ms step_avg:97.83ms
step:1030/1770 train_time:99794ms step_avg:97.84ms
step:1031/1770 train_time:99896ms step_avg:97.84ms
step:1032/1770 train_time:99997ms step_avg:97.84ms
step:1033/1770 train_time:100099ms step_avg:97.85ms
step:1034/1770 train_time:100200ms step_avg:97.85ms
step:1035/1770 train_time:100303ms step_avg:97.86ms
step:1036/1770 train_time:100404ms step_avg:97.86ms
step:1037/1770 train_time:100505ms step_avg:97.86ms
step:1038/1770 train_time:100606ms step_avg:97.87ms
step:1039/1770 train_time:100706ms step_avg:97.87ms
step:1040/1770 train_time:100807ms step_avg:97.87ms
step:1041/1770 train_time:100907ms step_avg:97.87ms
step:1042/1770 train_time:101008ms step_avg:97.88ms
step:1043/1770 train_time:101110ms step_avg:97.88ms
step:1044/1770 train_time:101212ms step_avg:97.88ms
step:1045/1770 train_time:101314ms step_avg:97.89ms
step:1046/1770 train_time:101417ms step_avg:97.89ms
step:1047/1770 train_time:101518ms step_avg:97.90ms
step:1048/1770 train_time:101619ms step_avg:97.90ms
step:1049/1770 train_time:101721ms step_avg:97.90ms
step:1050/1770 train_time:101823ms step_avg:97.91ms
step:1051/1770 train_time:101924ms step_avg:97.91ms
step:1052/1770 train_time:102025ms step_avg:97.91ms
step:1053/1770 train_time:102126ms step_avg:97.92ms
step:1054/1770 train_time:102227ms step_avg:97.92ms
step:1055/1770 train_time:102328ms step_avg:97.92ms
step:1056/1770 train_time:102429ms step_avg:97.92ms
step:1057/1770 train_time:102531ms step_avg:97.93ms
step:1058/1770 train_time:102633ms step_avg:97.93ms
step:1059/1770 train_time:102736ms step_avg:97.94ms
step:1060/1770 train_time:102837ms step_avg:97.94ms
step:1061/1770 train_time:102939ms step_avg:97.94ms
step:1062/1770 train_time:103040ms step_avg:97.95ms
step:1063/1770 train_time:103143ms step_avg:97.95ms
step:1064/1770 train_time:103245ms step_avg:97.96ms
step:1065/1770 train_time:103346ms step_avg:97.96ms
step:1066/1770 train_time:103448ms step_avg:97.96ms
step:1067/1770 train_time:103549ms step_avg:97.96ms
step:1068/1770 train_time:103651ms step_avg:97.97ms
step:1069/1770 train_time:103752ms step_avg:97.97ms
step:1070/1770 train_time:103855ms step_avg:97.98ms
step:1071/1770 train_time:103957ms step_avg:97.98ms
step:1072/1770 train_time:104059ms step_avg:97.98ms
step:1073/1770 train_time:104160ms step_avg:97.99ms
step:1074/1770 train_time:104262ms step_avg:97.99ms
step:1075/1770 train_time:104363ms step_avg:97.99ms
step:1076/1770 train_time:104465ms step_avg:98.00ms
step:1077/1770 train_time:104566ms step_avg:98.00ms
step:1078/1770 train_time:104667ms step_avg:98.00ms
step:1079/1770 train_time:104769ms step_avg:98.01ms
step:1080/1770 train_time:104869ms step_avg:98.01ms
step:1081/1770 train_time:104971ms step_avg:98.01ms
step:1082/1770 train_time:105073ms step_avg:98.02ms
step:1083/1770 train_time:105176ms step_avg:98.02ms
step:1084/1770 train_time:105278ms step_avg:98.02ms
step:1085/1770 train_time:105380ms step_avg:98.03ms
step:1086/1770 train_time:105481ms step_avg:98.03ms
step:1087/1770 train_time:105582ms step_avg:98.03ms
step:1088/1770 train_time:105683ms step_avg:98.04ms
step:1089/1770 train_time:105784ms step_avg:98.04ms
step:1090/1770 train_time:105886ms step_avg:98.04ms
step:1091/1770 train_time:105987ms step_avg:98.05ms
step:1092/1770 train_time:106088ms step_avg:98.05ms
step:1093/1770 train_time:106190ms step_avg:98.05ms
step:1094/1770 train_time:106292ms step_avg:98.06ms
step:1095/1770 train_time:106395ms step_avg:98.06ms
step:1096/1770 train_time:106497ms step_avg:98.06ms
step:1097/1770 train_time:106598ms step_avg:98.07ms
step:1098/1770 train_time:106700ms step_avg:98.07ms
step:1099/1770 train_time:106801ms step_avg:98.07ms
step:1100/1770 train_time:106904ms step_avg:98.08ms
step:1101/1770 train_time:107005ms step_avg:98.08ms
step:1102/1770 train_time:107106ms step_avg:98.08ms
step:1103/1770 train_time:107207ms step_avg:98.08ms
step:1104/1770 train_time:107308ms step_avg:98.09ms
step:1105/1770 train_time:107409ms step_avg:98.09ms
step:1106/1770 train_time:107511ms step_avg:98.09ms
step:1107/1770 train_time:107612ms step_avg:98.10ms
step:1108/1770 train_time:107714ms step_avg:98.10ms
step:1109/1770 train_time:107817ms step_avg:98.10ms
step:1110/1770 train_time:107919ms step_avg:98.11ms
step:1111/1770 train_time:108020ms step_avg:98.11ms
step:1112/1770 train_time:108123ms step_avg:98.12ms
step:1113/1770 train_time:108224ms step_avg:98.12ms
step:1114/1770 train_time:108326ms step_avg:98.12ms
step:1115/1770 train_time:108427ms step_avg:98.12ms
step:1116/1770 train_time:108529ms step_avg:98.13ms
step:1117/1770 train_time:108630ms step_avg:98.13ms
step:1118/1770 train_time:108731ms step_avg:98.13ms
step:1119/1770 train_time:108833ms step_avg:98.14ms
step:1120/1770 train_time:108936ms step_avg:98.14ms
step:1121/1770 train_time:109038ms step_avg:98.14ms
step:1122/1770 train_time:109139ms step_avg:98.15ms
step:1123/1770 train_time:109241ms step_avg:98.15ms
step:1124/1770 train_time:109342ms step_avg:98.15ms
step:1125/1770 train_time:109444ms step_avg:98.16ms
step:1125/1770 val_loss:3.4790 train_time:109543ms step_avg:98.24ms
step:1126/1770 train_time:109565ms step_avg:98.18ms
step:1127/1770 train_time:109651ms step_avg:98.17ms
step:1128/1770 train_time:109752ms step_avg:98.17ms
step:1129/1770 train_time:109853ms step_avg:98.17ms
step:1130/1770 train_time:109954ms step_avg:98.17ms
step:1131/1770 train_time:110056ms step_avg:98.18ms
step:1132/1770 train_time:110157ms step_avg:98.18ms
step:1133/1770 train_time:110259ms step_avg:98.18ms
step:1134/1770 train_time:110360ms step_avg:98.19ms
step:1135/1770 train_time:110463ms step_avg:98.19ms
step:1136/1770 train_time:110567ms step_avg:98.19ms
step:1137/1770 train_time:110669ms step_avg:98.20ms
step:1138/1770 train_time:110769ms step_avg:98.20ms
step:1139/1770 train_time:110870ms step_avg:98.20ms
step:1140/1770 train_time:110971ms step_avg:98.20ms
step:1141/1770 train_time:111072ms step_avg:98.21ms
step:1142/1770 train_time:111173ms step_avg:98.21ms
step:1143/1770 train_time:111273ms step_avg:98.21ms
step:1144/1770 train_time:111375ms step_avg:98.21ms
step:1145/1770 train_time:111476ms step_avg:98.22ms
step:1146/1770 train_time:111578ms step_avg:98.22ms
step:1147/1770 train_time:111679ms step_avg:98.22ms
step:1148/1770 train_time:111781ms step_avg:98.23ms
step:1149/1770 train_time:111883ms step_avg:98.23ms
step:1150/1770 train_time:111984ms step_avg:98.23ms
step:1151/1770 train_time:112088ms step_avg:98.24ms
step:1152/1770 train_time:112190ms step_avg:98.24ms
step:1153/1770 train_time:112292ms step_avg:98.24ms
step:1154/1770 train_time:112393ms step_avg:98.25ms
step:1155/1770 train_time:112495ms step_avg:98.25ms
step:1156/1770 train_time:112596ms step_avg:98.25ms
step:1157/1770 train_time:112699ms step_avg:98.26ms
step:1158/1770 train_time:112802ms step_avg:98.26ms
step:1159/1770 train_time:112903ms step_avg:98.26ms
step:1160/1770 train_time:113004ms step_avg:98.26ms
step:1161/1770 train_time:113106ms step_avg:98.27ms
step:1162/1770 train_time:113207ms step_avg:98.27ms
step:1163/1770 train_time:113309ms step_avg:98.27ms
step:1164/1770 train_time:113410ms step_avg:98.28ms
step:1165/1770 train_time:113511ms step_avg:98.28ms
step:1166/1770 train_time:113612ms step_avg:98.28ms
step:1167/1770 train_time:113713ms step_avg:98.28ms
step:1168/1770 train_time:113815ms step_avg:98.29ms
step:1169/1770 train_time:113915ms step_avg:98.29ms
step:1170/1770 train_time:114017ms step_avg:98.29ms
step:1171/1770 train_time:114119ms step_avg:98.29ms
step:1172/1770 train_time:114222ms step_avg:98.30ms
step:1173/1770 train_time:114324ms step_avg:98.30ms
step:1174/1770 train_time:114426ms step_avg:98.30ms
step:1175/1770 train_time:114527ms step_avg:98.31ms
step:1176/1770 train_time:114629ms step_avg:98.31ms
step:1177/1770 train_time:114730ms step_avg:98.31ms
step:1178/1770 train_time:114832ms step_avg:98.32ms
step:1179/1770 train_time:114933ms step_avg:98.32ms
step:1180/1770 train_time:115034ms step_avg:98.32ms
step:1181/1770 train_time:115135ms step_avg:98.32ms
step:1182/1770 train_time:115237ms step_avg:98.32ms
step:1183/1770 train_time:115341ms step_avg:98.33ms
step:1184/1770 train_time:115446ms step_avg:98.34ms
step:1185/1770 train_time:115548ms step_avg:98.34ms
step:1186/1770 train_time:115651ms step_avg:98.34ms
step:1187/1770 train_time:115756ms step_avg:98.35ms
step:1188/1770 train_time:115858ms step_avg:98.35ms
step:1189/1770 train_time:115959ms step_avg:98.35ms
step:1190/1770 train_time:116061ms step_avg:98.36ms
step:1191/1770 train_time:116164ms step_avg:98.36ms
step:1192/1770 train_time:116267ms step_avg:98.36ms
step:1193/1770 train_time:116370ms step_avg:98.37ms
step:1194/1770 train_time:116472ms step_avg:98.37ms
step:1195/1770 train_time:116575ms step_avg:98.38ms
step:1196/1770 train_time:116678ms step_avg:98.38ms
step:1197/1770 train_time:116780ms step_avg:98.38ms
step:1198/1770 train_time:116883ms step_avg:98.39ms
step:1199/1770 train_time:116986ms step_avg:98.39ms
step:1200/1770 train_time:117089ms step_avg:98.39ms
step:1201/1770 train_time:117192ms step_avg:98.40ms
step:1202/1770 train_time:117293ms step_avg:98.40ms
step:1203/1770 train_time:117395ms step_avg:98.40ms
step:1204/1770 train_time:117499ms step_avg:98.41ms
step:1205/1770 train_time:117601ms step_avg:98.41ms
step:1206/1770 train_time:117705ms step_avg:98.42ms
step:1207/1770 train_time:117808ms step_avg:98.42ms
step:1208/1770 train_time:117910ms step_avg:98.42ms
step:1209/1770 train_time:118012ms step_avg:98.43ms
step:1210/1770 train_time:118114ms step_avg:98.43ms
step:1211/1770 train_time:118217ms step_avg:98.43ms
step:1212/1770 train_time:118321ms step_avg:98.44ms
step:1213/1770 train_time:118424ms step_avg:98.44ms
step:1214/1770 train_time:118526ms step_avg:98.44ms
step:1215/1770 train_time:118629ms step_avg:98.45ms
step:1216/1770 train_time:118734ms step_avg:98.45ms
step:1217/1770 train_time:118835ms step_avg:98.46ms
step:1218/1770 train_time:118938ms step_avg:98.46ms
step:1219/1770 train_time:119040ms step_avg:98.46ms
step:1220/1770 train_time:119144ms step_avg:98.47ms
step:1221/1770 train_time:119246ms step_avg:98.47ms
step:1222/1770 train_time:119350ms step_avg:98.47ms
step:1223/1770 train_time:119452ms step_avg:98.48ms
step:1224/1770 train_time:119555ms step_avg:98.48ms
step:1225/1770 train_time:119658ms step_avg:98.48ms
step:1226/1770 train_time:119760ms step_avg:98.49ms
step:1227/1770 train_time:119865ms step_avg:98.49ms
step:1228/1770 train_time:119971ms step_avg:98.50ms
step:1229/1770 train_time:120073ms step_avg:98.50ms
step:1230/1770 train_time:120175ms step_avg:98.50ms
step:1231/1770 train_time:120278ms step_avg:98.51ms
step:1232/1770 train_time:120381ms step_avg:98.51ms
step:1233/1770 train_time:120483ms step_avg:98.51ms
step:1234/1770 train_time:120586ms step_avg:98.52ms
step:1235/1770 train_time:120689ms step_avg:98.52ms
step:1236/1770 train_time:120793ms step_avg:98.53ms
step:1237/1770 train_time:120895ms step_avg:98.53ms
step:1238/1770 train_time:120998ms step_avg:98.53ms
step:1239/1770 train_time:121102ms step_avg:98.54ms
step:1240/1770 train_time:121204ms step_avg:98.54ms
step:1241/1770 train_time:121308ms step_avg:98.54ms
step:1242/1770 train_time:121411ms step_avg:98.55ms
step:1243/1770 train_time:121513ms step_avg:98.55ms
step:1244/1770 train_time:121615ms step_avg:98.55ms
step:1245/1770 train_time:121718ms step_avg:98.56ms
step:1246/1770 train_time:121821ms step_avg:98.56ms
step:1247/1770 train_time:121924ms step_avg:98.56ms
step:1248/1770 train_time:122027ms step_avg:98.57ms
step:1249/1770 train_time:122129ms step_avg:98.57ms
step:1250/1770 train_time:122231ms step_avg:98.57ms
step:1250/1770 val_loss:3.4308 train_time:122334ms step_avg:98.66ms
step:1251/1770 train_time:122355ms step_avg:98.59ms
step:1252/1770 train_time:122444ms step_avg:98.59ms
step:1253/1770 train_time:122547ms step_avg:98.59ms
step:1254/1770 train_time:122649ms step_avg:98.59ms
step:1255/1770 train_time:122754ms step_avg:98.60ms
step:1256/1770 train_time:122856ms step_avg:98.60ms
step:1257/1770 train_time:122958ms step_avg:98.60ms
step:1258/1770 train_time:123062ms step_avg:98.61ms
step:1259/1770 train_time:123164ms step_avg:98.61ms
step:1260/1770 train_time:123267ms step_avg:98.61ms
step:1261/1770 train_time:123370ms step_avg:98.62ms
step:1262/1770 train_time:123474ms step_avg:98.62ms
step:1263/1770 train_time:123576ms step_avg:98.62ms
step:1264/1770 train_time:123681ms step_avg:98.63ms
step:1265/1770 train_time:123783ms step_avg:98.63ms
step:1266/1770 train_time:123887ms step_avg:98.64ms
step:1267/1770 train_time:123989ms step_avg:98.64ms
step:1268/1770 train_time:124092ms step_avg:98.64ms
step:1269/1770 train_time:124195ms step_avg:98.65ms
step:1270/1770 train_time:124298ms step_avg:98.65ms
step:1271/1770 train_time:124401ms step_avg:98.65ms
step:1272/1770 train_time:124503ms step_avg:98.66ms
step:1273/1770 train_time:124606ms step_avg:98.66ms
step:1274/1770 train_time:124709ms step_avg:98.66ms
step:1275/1770 train_time:124811ms step_avg:98.66ms
step:1276/1770 train_time:124914ms step_avg:98.67ms
step:1277/1770 train_time:125017ms step_avg:98.67ms
step:1278/1770 train_time:125121ms step_avg:98.68ms
step:1279/1770 train_time:125224ms step_avg:98.68ms
step:1280/1770 train_time:125327ms step_avg:98.68ms
step:1281/1770 train_time:125429ms step_avg:98.69ms
step:1282/1770 train_time:125533ms step_avg:98.69ms
step:1283/1770 train_time:125636ms step_avg:98.69ms
step:1284/1770 train_time:125740ms step_avg:98.70ms
step:1285/1770 train_time:125843ms step_avg:98.70ms
step:1286/1770 train_time:125946ms step_avg:98.70ms
step:1287/1770 train_time:126051ms step_avg:98.71ms
step:1288/1770 train_time:126154ms step_avg:98.71ms
step:1289/1770 train_time:126257ms step_avg:98.72ms
step:1290/1770 train_time:126359ms step_avg:98.72ms
step:1291/1770 train_time:126462ms step_avg:98.72ms
step:1292/1770 train_time:126566ms step_avg:98.73ms
step:1293/1770 train_time:126668ms step_avg:98.73ms
step:1294/1770 train_time:126770ms step_avg:98.73ms
step:1295/1770 train_time:126872ms step_avg:98.73ms
step:1296/1770 train_time:126975ms step_avg:98.74ms
step:1297/1770 train_time:127078ms step_avg:98.74ms
step:1298/1770 train_time:127181ms step_avg:98.74ms
step:1299/1770 train_time:127284ms step_avg:98.75ms
step:1300/1770 train_time:127386ms step_avg:98.75ms
step:1301/1770 train_time:127489ms step_avg:98.75ms
step:1302/1770 train_time:127592ms step_avg:98.76ms
step:1303/1770 train_time:127695ms step_avg:98.76ms
step:1304/1770 train_time:127798ms step_avg:98.76ms
step:1305/1770 train_time:127901ms step_avg:98.77ms
step:1306/1770 train_time:128003ms step_avg:98.77ms
step:1307/1770 train_time:128106ms step_avg:98.77ms
step:1308/1770 train_time:128209ms step_avg:98.77ms
step:1309/1770 train_time:128311ms step_avg:98.78ms
step:1310/1770 train_time:128414ms step_avg:98.78ms
step:1311/1770 train_time:128516ms step_avg:98.78ms
step:1312/1770 train_time:128618ms step_avg:98.79ms
step:1313/1770 train_time:128721ms step_avg:98.79ms
step:1314/1770 train_time:128824ms step_avg:98.79ms
step:1315/1770 train_time:128926ms step_avg:98.79ms
step:1316/1770 train_time:129030ms step_avg:98.80ms
step:1317/1770 train_time:129133ms step_avg:98.80ms
step:1318/1770 train_time:129239ms step_avg:98.81ms
step:1319/1770 train_time:129342ms step_avg:98.81ms
step:1320/1770 train_time:129445ms step_avg:98.81ms
step:1321/1770 train_time:129547ms step_avg:98.82ms
step:1322/1770 train_time:129650ms step_avg:98.82ms
step:1323/1770 train_time:129754ms step_avg:98.82ms
step:1324/1770 train_time:129858ms step_avg:98.83ms
step:1325/1770 train_time:129962ms step_avg:98.83ms
step:1326/1770 train_time:130064ms step_avg:98.83ms
step:1327/1770 train_time:130169ms step_avg:98.84ms
step:1328/1770 train_time:130271ms step_avg:98.84ms
step:1329/1770 train_time:130374ms step_avg:98.84ms
step:1330/1770 train_time:130476ms step_avg:98.85ms
step:1331/1770 train_time:130579ms step_avg:98.85ms
step:1332/1770 train_time:130681ms step_avg:98.85ms
step:1333/1770 train_time:130785ms step_avg:98.85ms
step:1334/1770 train_time:130887ms step_avg:98.86ms
step:1335/1770 train_time:130989ms step_avg:98.86ms
step:1336/1770 train_time:131091ms step_avg:98.86ms
step:1337/1770 train_time:131194ms step_avg:98.87ms
step:1338/1770 train_time:131296ms step_avg:98.87ms
step:1339/1770 train_time:131400ms step_avg:98.87ms
step:1340/1770 train_time:131504ms step_avg:98.88ms
step:1341/1770 train_time:131606ms step_avg:98.88ms
step:1342/1770 train_time:131710ms step_avg:98.88ms
step:1343/1770 train_time:131813ms step_avg:98.88ms
step:1344/1770 train_time:131917ms step_avg:98.89ms
step:1345/1770 train_time:132019ms step_avg:98.89ms
step:1346/1770 train_time:132122ms step_avg:98.89ms
step:1347/1770 train_time:132225ms step_avg:98.90ms
step:1348/1770 train_time:132329ms step_avg:98.90ms
step:1349/1770 train_time:132432ms step_avg:98.90ms
step:1350/1770 train_time:132534ms step_avg:98.91ms
step:1351/1770 train_time:132637ms step_avg:98.91ms
step:1352/1770 train_time:132741ms step_avg:98.91ms
step:1353/1770 train_time:132844ms step_avg:98.92ms
step:1354/1770 train_time:132946ms step_avg:98.92ms
step:1355/1770 train_time:133048ms step_avg:98.92ms
step:1356/1770 train_time:133150ms step_avg:98.92ms
step:1357/1770 train_time:133254ms step_avg:98.93ms
step:1358/1770 train_time:133357ms step_avg:98.93ms
step:1359/1770 train_time:133461ms step_avg:98.93ms
step:1360/1770 train_time:133564ms step_avg:98.94ms
step:1361/1770 train_time:133667ms step_avg:98.94ms
step:1362/1770 train_time:133770ms step_avg:98.94ms
step:1363/1770 train_time:133873ms step_avg:98.95ms
step:1364/1770 train_time:133977ms step_avg:98.95ms
step:1365/1770 train_time:134079ms step_avg:98.95ms
step:1366/1770 train_time:134182ms step_avg:98.95ms
step:1367/1770 train_time:134285ms step_avg:98.96ms
step:1368/1770 train_time:134387ms step_avg:98.96ms
step:1369/1770 train_time:134491ms step_avg:98.96ms
step:1370/1770 train_time:134593ms step_avg:98.97ms
step:1371/1770 train_time:134697ms step_avg:98.97ms
step:1372/1770 train_time:134799ms step_avg:98.97ms
step:1373/1770 train_time:134902ms step_avg:98.97ms
step:1374/1770 train_time:135005ms step_avg:98.98ms
step:1375/1770 train_time:135108ms step_avg:98.98ms
step:1375/1770 val_loss:3.3902 train_time:135209ms step_avg:99.05ms
step:1376/1770 train_time:135230ms step_avg:99.00ms
step:1377/1770 train_time:135322ms step_avg:98.99ms
step:1378/1770 train_time:135426ms step_avg:99.00ms
step:1379/1770 train_time:135528ms step_avg:99.00ms
step:1380/1770 train_time:135630ms step_avg:99.00ms
step:1381/1770 train_time:135734ms step_avg:99.00ms
step:1382/1770 train_time:135836ms step_avg:99.01ms
step:1383/1770 train_time:135940ms step_avg:99.01ms
step:1384/1770 train_time:136042ms step_avg:99.01ms
step:1385/1770 train_time:136145ms step_avg:99.01ms
step:1386/1770 train_time:136249ms step_avg:99.02ms
step:1387/1770 train_time:136352ms step_avg:99.02ms
step:1388/1770 train_time:136454ms step_avg:99.02ms
step:1389/1770 train_time:136557ms step_avg:99.03ms
step:1390/1770 train_time:136660ms step_avg:99.03ms
step:1391/1770 train_time:136762ms step_avg:99.03ms
step:1392/1770 train_time:136865ms step_avg:99.03ms
step:1393/1770 train_time:136968ms step_avg:99.04ms
step:1394/1770 train_time:137070ms step_avg:99.04ms
step:1395/1770 train_time:137174ms step_avg:99.04ms
step:1396/1770 train_time:137279ms step_avg:99.05ms
step:1397/1770 train_time:137382ms step_avg:99.05ms
step:1398/1770 train_time:137485ms step_avg:99.05ms
step:1399/1770 train_time:137587ms step_avg:99.05ms
step:1400/1770 train_time:137690ms step_avg:99.06ms
step:1401/1770 train_time:137793ms step_avg:99.06ms
step:1402/1770 train_time:137896ms step_avg:99.06ms
step:1403/1770 train_time:137999ms step_avg:99.07ms
step:1404/1770 train_time:138102ms step_avg:99.07ms
step:1405/1770 train_time:138204ms step_avg:99.07ms
step:1406/1770 train_time:138306ms step_avg:99.07ms
step:1407/1770 train_time:138409ms step_avg:99.08ms
step:1408/1770 train_time:138511ms step_avg:99.08ms
step:1409/1770 train_time:138614ms step_avg:99.08ms
step:1410/1770 train_time:138716ms step_avg:99.08ms
step:1411/1770 train_time:138820ms step_avg:99.09ms
step:1412/1770 train_time:138922ms step_avg:99.09ms
step:1413/1770 train_time:139024ms step_avg:99.09ms
step:1414/1770 train_time:139127ms step_avg:99.09ms
step:1415/1770 train_time:139231ms step_avg:99.10ms
step:1416/1770 train_time:139335ms step_avg:99.10ms
step:1417/1770 train_time:139437ms step_avg:99.10ms
step:1418/1770 train_time:139540ms step_avg:99.10ms
step:1419/1770 train_time:139642ms step_avg:99.11ms
step:1420/1770 train_time:139745ms step_avg:99.11ms
step:1421/1770 train_time:139847ms step_avg:99.11ms
step:1422/1770 train_time:139949ms step_avg:99.11ms
step:1423/1770 train_time:140053ms step_avg:99.12ms
step:1424/1770 train_time:140157ms step_avg:99.12ms
step:1425/1770 train_time:140260ms step_avg:99.12ms
step:1426/1770 train_time:140362ms step_avg:99.13ms
step:1427/1770 train_time:140465ms step_avg:99.13ms
step:1428/1770 train_time:140569ms step_avg:99.13ms
step:1429/1770 train_time:140672ms step_avg:99.13ms
step:1430/1770 train_time:140775ms step_avg:99.14ms
step:1431/1770 train_time:140879ms step_avg:99.14ms
step:1432/1770 train_time:140981ms step_avg:99.14ms
step:1433/1770 train_time:141083ms step_avg:99.14ms
step:1434/1770 train_time:141185ms step_avg:99.15ms
step:1435/1770 train_time:141288ms step_avg:99.15ms
step:1436/1770 train_time:141392ms step_avg:99.15ms
step:1437/1770 train_time:141494ms step_avg:99.16ms
step:1438/1770 train_time:141596ms step_avg:99.16ms
step:1439/1770 train_time:141699ms step_avg:99.16ms
step:1440/1770 train_time:141802ms step_avg:99.16ms
step:1441/1770 train_time:141908ms step_avg:99.17ms
step:1442/1770 train_time:142010ms step_avg:99.17ms
step:1443/1770 train_time:142113ms step_avg:99.17ms
step:1444/1770 train_time:142218ms step_avg:99.18ms
step:1445/1770 train_time:142322ms step_avg:99.18ms
step:1446/1770 train_time:142425ms step_avg:99.18ms
step:1447/1770 train_time:142529ms step_avg:99.19ms
step:1448/1770 train_time:142633ms step_avg:99.19ms
step:1449/1770 train_time:142739ms step_avg:99.19ms
step:1450/1770 train_time:142842ms step_avg:99.20ms
step:1451/1770 train_time:142946ms step_avg:99.20ms
step:1452/1770 train_time:143051ms step_avg:99.20ms
step:1453/1770 train_time:143154ms step_avg:99.21ms
step:1454/1770 train_time:143258ms step_avg:99.21ms
step:1455/1770 train_time:143362ms step_avg:99.21ms
step:1456/1770 train_time:143467ms step_avg:99.22ms
step:1457/1770 train_time:143572ms step_avg:99.22ms
step:1458/1770 train_time:143676ms step_avg:99.22ms
step:1459/1770 train_time:143780ms step_avg:99.23ms
step:1460/1770 train_time:143884ms step_avg:99.23ms
step:1461/1770 train_time:143988ms step_avg:99.23ms
step:1462/1770 train_time:144091ms step_avg:99.24ms
step:1463/1770 train_time:144196ms step_avg:99.24ms
step:1464/1770 train_time:144301ms step_avg:99.24ms
step:1465/1770 train_time:144404ms step_avg:99.25ms
step:1466/1770 train_time:144509ms step_avg:99.25ms
step:1467/1770 train_time:144615ms step_avg:99.26ms
step:1468/1770 train_time:144719ms step_avg:99.26ms
step:1469/1770 train_time:144823ms step_avg:99.26ms
step:1470/1770 train_time:144926ms step_avg:99.26ms
step:1471/1770 train_time:145030ms step_avg:99.27ms
step:1472/1770 train_time:145133ms step_avg:99.27ms
step:1473/1770 train_time:145238ms step_avg:99.27ms
step:1474/1770 train_time:145343ms step_avg:99.28ms
step:1475/1770 train_time:145447ms step_avg:99.28ms
step:1476/1770 train_time:145551ms step_avg:99.28ms
step:1477/1770 train_time:145657ms step_avg:99.29ms
step:1478/1770 train_time:145762ms step_avg:99.29ms
step:1479/1770 train_time:145865ms step_avg:99.30ms
step:1480/1770 train_time:145969ms step_avg:99.30ms
step:1481/1770 train_time:146077ms step_avg:99.30ms
step:1482/1770 train_time:146180ms step_avg:99.31ms
step:1483/1770 train_time:146284ms step_avg:99.31ms
step:1484/1770 train_time:146387ms step_avg:99.31ms
step:1485/1770 train_time:146491ms step_avg:99.32ms
step:1486/1770 train_time:146595ms step_avg:99.32ms
step:1487/1770 train_time:146699ms step_avg:99.32ms
step:1488/1770 train_time:146804ms step_avg:99.33ms
step:1489/1770 train_time:146909ms step_avg:99.33ms
step:1490/1770 train_time:147014ms step_avg:99.33ms
step:1491/1770 train_time:147117ms step_avg:99.34ms
step:1492/1770 train_time:147222ms step_avg:99.34ms
step:1493/1770 train_time:147328ms step_avg:99.34ms
step:1494/1770 train_time:147436ms step_avg:99.35ms
step:1495/1770 train_time:147540ms step_avg:99.35ms
step:1496/1770 train_time:147643ms step_avg:99.36ms
step:1497/1770 train_time:147747ms step_avg:99.36ms
step:1498/1770 train_time:147851ms step_avg:99.36ms
step:1499/1770 train_time:147954ms step_avg:99.36ms
step:1500/1770 train_time:148058ms step_avg:99.37ms
step:1500/1770 val_loss:3.3527 train_time:148159ms step_avg:99.44ms
step:1501/1770 train_time:148181ms step_avg:99.38ms
step:1502/1770 train_time:148272ms step_avg:99.38ms
step:1503/1770 train_time:148376ms step_avg:99.38ms
step:1504/1770 train_time:148480ms step_avg:99.38ms
step:1505/1770 train_time:148586ms step_avg:99.39ms
step:1506/1770 train_time:148690ms step_avg:99.39ms
step:1507/1770 train_time:148794ms step_avg:99.39ms
step:1508/1770 train_time:148899ms step_avg:99.40ms
step:1509/1770 train_time:149003ms step_avg:99.40ms
step:1510/1770 train_time:149105ms step_avg:99.40ms
step:1511/1770 train_time:149210ms step_avg:99.41ms
step:1512/1770 train_time:149315ms step_avg:99.41ms
step:1513/1770 train_time:149419ms step_avg:99.41ms
step:1514/1770 train_time:149523ms step_avg:99.42ms
step:1515/1770 train_time:149627ms step_avg:99.42ms
step:1516/1770 train_time:149731ms step_avg:99.42ms
step:1517/1770 train_time:149835ms step_avg:99.43ms
step:1518/1770 train_time:149941ms step_avg:99.43ms
step:1519/1770 train_time:150044ms step_avg:99.43ms
step:1520/1770 train_time:150148ms step_avg:99.44ms
step:1521/1770 train_time:150253ms step_avg:99.44ms
step:1522/1770 train_time:150356ms step_avg:99.44ms
step:1523/1770 train_time:150462ms step_avg:99.45ms
step:1524/1770 train_time:150565ms step_avg:99.45ms
step:1525/1770 train_time:150669ms step_avg:99.45ms
step:1526/1770 train_time:150772ms step_avg:99.45ms
step:1527/1770 train_time:150876ms step_avg:99.46ms
step:1528/1770 train_time:150981ms step_avg:99.46ms
step:1529/1770 train_time:151085ms step_avg:99.46ms
step:1530/1770 train_time:151188ms step_avg:99.47ms
step:1531/1770 train_time:151292ms step_avg:99.47ms
step:1532/1770 train_time:151395ms step_avg:99.47ms
step:1533/1770 train_time:151500ms step_avg:99.47ms
step:1534/1770 train_time:151604ms step_avg:99.48ms
step:1535/1770 train_time:151708ms step_avg:99.48ms
step:1536/1770 train_time:151811ms step_avg:99.48ms
step:1537/1770 train_time:151915ms step_avg:99.49ms
step:1538/1770 train_time:152021ms step_avg:99.49ms
step:1539/1770 train_time:152125ms step_avg:99.49ms
step:1540/1770 train_time:152231ms step_avg:99.50ms
step:1541/1770 train_time:152336ms step_avg:99.50ms
step:1542/1770 train_time:152441ms step_avg:99.50ms
step:1543/1770 train_time:152544ms step_avg:99.51ms
step:1544/1770 train_time:152650ms step_avg:99.51ms
step:1545/1770 train_time:152754ms step_avg:99.51ms
step:1546/1770 train_time:152857ms step_avg:99.52ms
step:1547/1770 train_time:152961ms step_avg:99.52ms
step:1548/1770 train_time:153065ms step_avg:99.52ms
step:1549/1770 train_time:153169ms step_avg:99.53ms
step:1550/1770 train_time:153273ms step_avg:99.53ms
step:1551/1770 train_time:153376ms step_avg:99.53ms
step:1552/1770 train_time:153482ms step_avg:99.53ms
step:1553/1770 train_time:153586ms step_avg:99.54ms
step:1554/1770 train_time:153689ms step_avg:99.54ms
step:1555/1770 train_time:153793ms step_avg:99.54ms
step:1556/1770 train_time:153897ms step_avg:99.55ms
step:1557/1770 train_time:154001ms step_avg:99.55ms
step:1558/1770 train_time:154105ms step_avg:99.55ms
step:1559/1770 train_time:154209ms step_avg:99.55ms
step:1560/1770 train_time:154312ms step_avg:99.56ms
step:1561/1770 train_time:154418ms step_avg:99.56ms
step:1562/1770 train_time:154522ms step_avg:99.56ms
step:1563/1770 train_time:154626ms step_avg:99.57ms
step:1564/1770 train_time:154729ms step_avg:99.57ms
step:1565/1770 train_time:154833ms step_avg:99.57ms
step:1566/1770 train_time:154937ms step_avg:99.57ms
step:1567/1770 train_time:155041ms step_avg:99.58ms
step:1568/1770 train_time:155145ms step_avg:99.58ms
step:1569/1770 train_time:155252ms step_avg:99.58ms
step:1570/1770 train_time:155356ms step_avg:99.59ms
step:1571/1770 train_time:155460ms step_avg:99.59ms
step:1572/1770 train_time:155565ms step_avg:99.59ms
step:1573/1770 train_time:155672ms step_avg:99.60ms
step:1574/1770 train_time:155775ms step_avg:99.60ms
step:1575/1770 train_time:155878ms step_avg:99.60ms
step:1576/1770 train_time:155982ms step_avg:99.61ms
step:1577/1770 train_time:156088ms step_avg:99.61ms
step:1578/1770 train_time:156193ms step_avg:99.61ms
step:1579/1770 train_time:156297ms step_avg:99.62ms
step:1580/1770 train_time:156401ms step_avg:99.62ms
step:1581/1770 train_time:156507ms step_avg:99.62ms
step:1582/1770 train_time:156612ms step_avg:99.63ms
step:1583/1770 train_time:156716ms step_avg:99.63ms
step:1584/1770 train_time:156821ms step_avg:99.63ms
step:1585/1770 train_time:156925ms step_avg:99.64ms
step:1586/1770 train_time:157033ms step_avg:99.64ms
step:1587/1770 train_time:157137ms step_avg:99.64ms
step:1588/1770 train_time:157242ms step_avg:99.65ms
step:1589/1770 train_time:157347ms step_avg:99.65ms
step:1590/1770 train_time:157451ms step_avg:99.65ms
step:1591/1770 train_time:157554ms step_avg:99.65ms
step:1592/1770 train_time:157659ms step_avg:99.66ms
step:1593/1770 train_time:157762ms step_avg:99.66ms
step:1594/1770 train_time:157866ms step_avg:99.66ms
step:1595/1770 train_time:157970ms step_avg:99.67ms
step:1596/1770 train_time:158075ms step_avg:99.67ms
step:1597/1770 train_time:158178ms step_avg:99.67ms
step:1598/1770 train_time:158282ms step_avg:99.67ms
step:1599/1770 train_time:158387ms step_avg:99.68ms
step:1600/1770 train_time:158493ms step_avg:99.68ms
step:1601/1770 train_time:158597ms step_avg:99.68ms
step:1602/1770 train_time:158702ms step_avg:99.69ms
step:1603/1770 train_time:158806ms step_avg:99.69ms
step:1604/1770 train_time:158908ms step_avg:99.69ms
step:1605/1770 train_time:159012ms step_avg:99.69ms
step:1606/1770 train_time:159116ms step_avg:99.70ms
step:1607/1770 train_time:159223ms step_avg:99.70ms
step:1608/1770 train_time:159328ms step_avg:99.70ms
step:1609/1770 train_time:159432ms step_avg:99.71ms
step:1610/1770 train_time:159537ms step_avg:99.71ms
step:1611/1770 train_time:159643ms step_avg:99.71ms
step:1612/1770 train_time:159748ms step_avg:99.72ms
step:1613/1770 train_time:159852ms step_avg:99.72ms
step:1614/1770 train_time:159956ms step_avg:99.72ms
step:1615/1770 train_time:160060ms step_avg:99.73ms
step:1616/1770 train_time:160164ms step_avg:99.73ms
step:1617/1770 train_time:160271ms step_avg:99.73ms
step:1618/1770 train_time:160375ms step_avg:99.74ms
step:1619/1770 train_time:160479ms step_avg:99.74ms
step:1620/1770 train_time:160585ms step_avg:99.74ms
step:1621/1770 train_time:160688ms step_avg:99.74ms
step:1622/1770 train_time:160793ms step_avg:99.75ms
step:1623/1770 train_time:160899ms step_avg:99.75ms
step:1624/1770 train_time:161003ms step_avg:99.75ms
step:1625/1770 train_time:161106ms step_avg:99.76ms
step:1625/1770 val_loss:3.3197 train_time:161208ms step_avg:99.82ms
step:1626/1770 train_time:161229ms step_avg:99.77ms
step:1627/1770 train_time:161320ms step_avg:99.77ms
step:1628/1770 train_time:161423ms step_avg:99.77ms
step:1629/1770 train_time:161526ms step_avg:99.77ms
step:1630/1770 train_time:161631ms step_avg:99.77ms
step:1631/1770 train_time:161734ms step_avg:99.77ms
step:1632/1770 train_time:161838ms step_avg:99.78ms
step:1633/1770 train_time:161943ms step_avg:99.78ms
step:1634/1770 train_time:162047ms step_avg:99.78ms
step:1635/1770 train_time:162151ms step_avg:99.79ms
step:1636/1770 train_time:162256ms step_avg:99.79ms
step:1637/1770 train_time:162360ms step_avg:99.79ms
step:1638/1770 train_time:162464ms step_avg:99.79ms
step:1639/1770 train_time:162567ms step_avg:99.80ms
step:1640/1770 train_time:162672ms step_avg:99.80ms
step:1641/1770 train_time:162777ms step_avg:99.80ms
step:1642/1770 train_time:162880ms step_avg:99.80ms
step:1643/1770 train_time:162984ms step_avg:99.81ms
step:1644/1770 train_time:163090ms step_avg:99.81ms
step:1645/1770 train_time:163194ms step_avg:99.81ms
step:1646/1770 train_time:163300ms step_avg:99.82ms
step:1647/1770 train_time:163404ms step_avg:99.82ms
step:1648/1770 train_time:163507ms step_avg:99.82ms
step:1649/1770 train_time:163610ms step_avg:99.82ms
step:1650/1770 train_time:163715ms step_avg:99.83ms
step:1651/1770 train_time:163818ms step_avg:99.83ms
step:1652/1770 train_time:163922ms step_avg:99.83ms
step:1653/1770 train_time:164027ms step_avg:99.83ms
step:1654/1770 train_time:164134ms step_avg:99.84ms
step:1655/1770 train_time:164240ms step_avg:99.84ms
step:1656/1770 train_time:164344ms step_avg:99.84ms
step:1657/1770 train_time:164450ms step_avg:99.85ms
step:1658/1770 train_time:164553ms step_avg:99.85ms
step:1659/1770 train_time:164658ms step_avg:99.85ms
step:1660/1770 train_time:164762ms step_avg:99.86ms
step:1661/1770 train_time:164867ms step_avg:99.86ms
step:1662/1770 train_time:164972ms step_avg:99.86ms
step:1663/1770 train_time:165075ms step_avg:99.86ms
step:1664/1770 train_time:165179ms step_avg:99.87ms
step:1665/1770 train_time:165282ms step_avg:99.87ms
step:1666/1770 train_time:165386ms step_avg:99.87ms
step:1667/1770 train_time:165490ms step_avg:99.87ms
step:1668/1770 train_time:165594ms step_avg:99.88ms
step:1669/1770 train_time:165697ms step_avg:99.88ms
step:1670/1770 train_time:165800ms step_avg:99.88ms
step:1671/1770 train_time:165904ms step_avg:99.88ms
step:1672/1770 train_time:166008ms step_avg:99.88ms
step:1673/1770 train_time:166114ms step_avg:99.89ms
step:1674/1770 train_time:166217ms step_avg:99.89ms
step:1675/1770 train_time:166320ms step_avg:99.89ms
step:1676/1770 train_time:166426ms step_avg:99.90ms
step:1677/1770 train_time:166534ms step_avg:99.90ms
step:1678/1770 train_time:166638ms step_avg:99.90ms
step:1679/1770 train_time:166741ms step_avg:99.90ms
step:1680/1770 train_time:166845ms step_avg:99.91ms
step:1681/1770 train_time:166950ms step_avg:99.91ms
step:1682/1770 train_time:167056ms step_avg:99.91ms
step:1683/1770 train_time:167160ms step_avg:99.92ms
step:1684/1770 train_time:167263ms step_avg:99.92ms
step:1685/1770 train_time:167367ms step_avg:99.92ms
step:1686/1770 train_time:167472ms step_avg:99.92ms
step:1687/1770 train_time:167577ms step_avg:99.93ms
step:1688/1770 train_time:167681ms step_avg:99.93ms
step:1689/1770 train_time:167785ms step_avg:99.93ms
step:1690/1770 train_time:167889ms step_avg:99.93ms
step:1691/1770 train_time:167993ms step_avg:99.94ms
step:1692/1770 train_time:168097ms step_avg:99.94ms
step:1693/1770 train_time:168203ms step_avg:99.94ms
step:1694/1770 train_time:168306ms step_avg:99.94ms
step:1695/1770 train_time:168410ms step_avg:99.95ms
step:1696/1770 train_time:168516ms step_avg:99.95ms
step:1697/1770 train_time:168623ms step_avg:99.95ms
step:1698/1770 train_time:168727ms step_avg:99.96ms
step:1699/1770 train_time:168832ms step_avg:99.96ms
step:1700/1770 train_time:168936ms step_avg:99.96ms
step:1701/1770 train_time:169040ms step_avg:99.96ms
step:1702/1770 train_time:169144ms step_avg:99.97ms
step:1703/1770 train_time:169248ms step_avg:99.97ms
step:1704/1770 train_time:169352ms step_avg:99.97ms
step:1705/1770 train_time:169456ms step_avg:99.97ms
step:1706/1770 train_time:169559ms step_avg:99.98ms
step:1707/1770 train_time:169663ms step_avg:99.98ms
step:1708/1770 train_time:169768ms step_avg:99.98ms
step:1709/1770 train_time:169873ms step_avg:99.98ms
step:1710/1770 train_time:169982ms step_avg:99.99ms
step:1711/1770 train_time:170088ms step_avg:99.99ms
step:1712/1770 train_time:170194ms step_avg:100.00ms
step:1713/1770 train_time:170298ms step_avg:100.00ms
step:1714/1770 train_time:170403ms step_avg:100.00ms
step:1715/1770 train_time:170506ms step_avg:100.00ms
step:1716/1770 train_time:170611ms step_avg:100.01ms
step:1717/1770 train_time:170716ms step_avg:100.01ms
step:1718/1770 train_time:170821ms step_avg:100.01ms
step:1719/1770 train_time:170927ms step_avg:100.02ms
step:1720/1770 train_time:171033ms step_avg:100.02ms
step:1721/1770 train_time:171137ms step_avg:100.02ms
step:1722/1770 train_time:171246ms step_avg:100.03ms
step:1723/1770 train_time:171352ms step_avg:100.03ms
step:1724/1770 train_time:171459ms step_avg:100.03ms
step:1725/1770 train_time:171565ms step_avg:100.04ms
step:1726/1770 train_time:171672ms step_avg:100.04ms
step:1727/1770 train_time:171776ms step_avg:100.04ms
step:1728/1770 train_time:171884ms step_avg:100.05ms
step:1729/1770 train_time:171988ms step_avg:100.05ms
step:1730/1770 train_time:172095ms step_avg:100.06ms
step:1731/1770 train_time:172201ms step_avg:100.06ms
step:1732/1770 train_time:172305ms step_avg:100.06ms
step:1733/1770 train_time:172411ms step_avg:100.06ms
step:1734/1770 train_time:172516ms step_avg:100.07ms
step:1735/1770 train_time:172622ms step_avg:100.07ms
step:1736/1770 train_time:172726ms step_avg:100.07ms
step:1737/1770 train_time:172831ms step_avg:100.08ms
step:1738/1770 train_time:172936ms step_avg:100.08ms
step:1739/1770 train_time:173041ms step_avg:100.08ms
step:1740/1770 train_time:173145ms step_avg:100.08ms
step:1741/1770 train_time:173253ms step_avg:100.09ms
step:1742/1770 train_time:173361ms step_avg:100.09ms
step:1743/1770 train_time:173467ms step_avg:100.10ms
step:1744/1770 train_time:173573ms step_avg:100.10ms
step:1745/1770 train_time:173677ms step_avg:100.10ms
step:1746/1770 train_time:173786ms step_avg:100.11ms
step:1747/1770 train_time:173890ms step_avg:100.11ms
step:1748/1770 train_time:173996ms step_avg:100.11ms
step:1749/1770 train_time:174102ms step_avg:100.12ms
step:1750/1770 train_time:174206ms step_avg:100.12ms
step:1750/1770 val_loss:3.2957 train_time:174309ms step_avg:100.18ms
step:1751/1770 train_time:174330ms step_avg:100.13ms
step:1752/1770 train_time:174420ms step_avg:100.13ms
step:1753/1770 train_time:174524ms step_avg:100.13ms
step:1754/1770 train_time:174630ms step_avg:100.13ms
step:1755/1770 train_time:174735ms step_avg:100.13ms
step:1756/1770 train_time:174841ms step_avg:100.14ms
step:1757/1770 train_time:174946ms step_avg:100.14ms
step:1758/1770 train_time:175051ms step_avg:100.14ms
step:1759/1770 train_time:175156ms step_avg:100.15ms
step:1760/1770 train_time:175262ms step_avg:100.15ms
step:1761/1770 train_time:175371ms step_avg:100.15ms
step:1762/1770 train_time:175480ms step_avg:100.16ms
step:1763/1770 train_time:175583ms step_avg:100.16ms
step:1764/1770 train_time:175689ms step_avg:100.16ms
step:1765/1770 train_time:175795ms step_avg:100.17ms
step:1766/1770 train_time:175904ms step_avg:100.17ms
step:1767/1770 train_time:176007ms step_avg:100.17ms
step:1768/1770 train_time:176112ms step_avg:100.18ms
step:1769/1770 train_time:176216ms step_avg:100.18ms
step:1770/1770 train_time:176320ms step_avg:100.18ms
step:1770/1770 val_loss:3.2920 train_time:176424ms step_avg:100.24ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
