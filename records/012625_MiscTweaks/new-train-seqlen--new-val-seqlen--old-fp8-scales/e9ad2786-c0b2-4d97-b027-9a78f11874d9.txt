import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 forward & backward by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# Custom operators : FP8 forward & bfloat16 backward

@torch.library.custom_op("nanogpt::mm_mixed", mutates_args=())
def mm_mixed_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_mixed_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_mixed_backward", mutates_args=())
def mm_mixed_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        x_bfloat16 = x_f8.to(torch.bfloat16)
        w_bfloat16 = w_f8.to(torch.bfloat16)
        grad_bfloat16 = grad.mul(grad_s).to(torch.bfloat16)
        grad_x = torch._scaled_mm(
            grad_bfloat16,
            w_bfloat16.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        grad_w = torch._scaled_mm(
            x_bfloat16.t().contiguous(),
            grad_bfloat16.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_mixed_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_mixed_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_mixed_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_mixed_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_mixed_op.register_autograd(mm_mixed_backward, setup_context=mm_mixed_setup_context)

def linear_mixed(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm_mixed(_x, w, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 22:30:48 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:1ms step_avg:nanms
step:1/1770 train_time:23485ms step_avg:nanms
step:2/1770 train_time:23907ms step_avg:nanms
step:3/1770 train_time:24000ms step_avg:nanms
step:4/1770 train_time:24094ms step_avg:nanms
step:5/1770 train_time:24188ms step_avg:nanms
step:6/1770 train_time:24282ms step_avg:nanms
step:7/1770 train_time:24376ms step_avg:nanms
step:8/1770 train_time:24470ms step_avg:nanms
step:9/1770 train_time:24565ms step_avg:nanms
step:10/1770 train_time:24659ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.21ms
step:14/1770 train_time:377ms step_avg:94.23ms
step:15/1770 train_time:472ms step_avg:94.32ms
step:16/1770 train_time:566ms step_avg:94.28ms
step:17/1770 train_time:660ms step_avg:94.24ms
step:18/1770 train_time:754ms step_avg:94.29ms
step:19/1770 train_time:849ms step_avg:94.32ms
step:20/1770 train_time:943ms step_avg:94.30ms
step:21/1770 train_time:1037ms step_avg:94.26ms
step:22/1770 train_time:1132ms step_avg:94.30ms
step:23/1770 train_time:1226ms step_avg:94.33ms
step:24/1770 train_time:1321ms step_avg:94.33ms
step:25/1770 train_time:1415ms step_avg:94.35ms
step:26/1770 train_time:1510ms step_avg:94.37ms
step:27/1770 train_time:1604ms step_avg:94.38ms
step:28/1770 train_time:1699ms step_avg:94.37ms
step:29/1770 train_time:1794ms step_avg:94.40ms
step:30/1770 train_time:1888ms step_avg:94.38ms
step:31/1770 train_time:1981ms step_avg:94.36ms
step:32/1770 train_time:2076ms step_avg:94.37ms
step:33/1770 train_time:2171ms step_avg:94.37ms
step:34/1770 train_time:2265ms step_avg:94.38ms
step:35/1770 train_time:2359ms step_avg:94.37ms
step:36/1770 train_time:2454ms step_avg:94.39ms
step:37/1770 train_time:2548ms step_avg:94.39ms
step:38/1770 train_time:2643ms step_avg:94.38ms
step:39/1770 train_time:2736ms step_avg:94.36ms
step:40/1770 train_time:2831ms step_avg:94.38ms
step:41/1770 train_time:2926ms step_avg:94.38ms
step:42/1770 train_time:3020ms step_avg:94.37ms
step:43/1770 train_time:3114ms step_avg:94.38ms
step:44/1770 train_time:3209ms step_avg:94.38ms
step:45/1770 train_time:3303ms step_avg:94.38ms
step:46/1770 train_time:3398ms step_avg:94.40ms
step:47/1770 train_time:3493ms step_avg:94.40ms
step:48/1770 train_time:3587ms step_avg:94.40ms
step:49/1770 train_time:3681ms step_avg:94.39ms
step:50/1770 train_time:3776ms step_avg:94.39ms
step:51/1770 train_time:3870ms step_avg:94.40ms
step:52/1770 train_time:3965ms step_avg:94.41ms
step:53/1770 train_time:4059ms step_avg:94.40ms
step:54/1770 train_time:4154ms step_avg:94.40ms
step:55/1770 train_time:4248ms step_avg:94.41ms
step:56/1770 train_time:4342ms step_avg:94.40ms
step:57/1770 train_time:4437ms step_avg:94.40ms
step:58/1770 train_time:4532ms step_avg:94.41ms
step:59/1770 train_time:4626ms step_avg:94.41ms
step:60/1770 train_time:4720ms step_avg:94.40ms
step:61/1770 train_time:4815ms step_avg:94.41ms
step:62/1770 train_time:4910ms step_avg:94.43ms
step:63/1770 train_time:5004ms step_avg:94.42ms
step:64/1770 train_time:5098ms step_avg:94.41ms
step:65/1770 train_time:5193ms step_avg:94.43ms
step:66/1770 train_time:5288ms step_avg:94.43ms
step:67/1770 train_time:5382ms step_avg:94.43ms
step:68/1770 train_time:5477ms step_avg:94.43ms
step:69/1770 train_time:5571ms step_avg:94.43ms
step:70/1770 train_time:5666ms step_avg:94.44ms
step:71/1770 train_time:5761ms step_avg:94.44ms
step:72/1770 train_time:5855ms step_avg:94.44ms
step:73/1770 train_time:5950ms step_avg:94.44ms
step:74/1770 train_time:6044ms step_avg:94.44ms
step:75/1770 train_time:6138ms step_avg:94.43ms
step:76/1770 train_time:6233ms step_avg:94.44ms
step:77/1770 train_time:6328ms step_avg:94.44ms
step:78/1770 train_time:6422ms step_avg:94.44ms
step:79/1770 train_time:6517ms step_avg:94.44ms
step:80/1770 train_time:6611ms step_avg:94.44ms
step:81/1770 train_time:6705ms step_avg:94.44ms
step:82/1770 train_time:6800ms step_avg:94.44ms
step:83/1770 train_time:6894ms step_avg:94.44ms
step:84/1770 train_time:6989ms step_avg:94.44ms
step:85/1770 train_time:7083ms step_avg:94.43ms
step:86/1770 train_time:7177ms step_avg:94.43ms
step:87/1770 train_time:7271ms step_avg:94.43ms
step:88/1770 train_time:7366ms step_avg:94.43ms
step:89/1770 train_time:7460ms step_avg:94.43ms
step:90/1770 train_time:7554ms step_avg:94.43ms
step:91/1770 train_time:7649ms step_avg:94.44ms
step:92/1770 train_time:7744ms step_avg:94.43ms
step:93/1770 train_time:7838ms step_avg:94.43ms
step:94/1770 train_time:7932ms step_avg:94.43ms
step:95/1770 train_time:8027ms step_avg:94.44ms
step:96/1770 train_time:8122ms step_avg:94.44ms
step:97/1770 train_time:8216ms step_avg:94.43ms
step:98/1770 train_time:8311ms step_avg:94.44ms
step:99/1770 train_time:8405ms step_avg:94.44ms
step:100/1770 train_time:8499ms step_avg:94.43ms
step:101/1770 train_time:8593ms step_avg:94.43ms
step:102/1770 train_time:8688ms step_avg:94.43ms
step:103/1770 train_time:8782ms step_avg:94.42ms
step:104/1770 train_time:8876ms step_avg:94.43ms
step:105/1770 train_time:8971ms step_avg:94.43ms
step:106/1770 train_time:9065ms step_avg:94.42ms
step:107/1770 train_time:9159ms step_avg:94.42ms
step:108/1770 train_time:9253ms step_avg:94.42ms
step:109/1770 train_time:9348ms step_avg:94.42ms
step:110/1770 train_time:9442ms step_avg:94.42ms
step:111/1770 train_time:9536ms step_avg:94.42ms
step:112/1770 train_time:9631ms step_avg:94.42ms
step:113/1770 train_time:9727ms step_avg:94.43ms
step:114/1770 train_time:9821ms step_avg:94.43ms
step:115/1770 train_time:9915ms step_avg:94.43ms
step:116/1770 train_time:10010ms step_avg:94.44ms
step:117/1770 train_time:10105ms step_avg:94.44ms
step:118/1770 train_time:10199ms step_avg:94.44ms
step:119/1770 train_time:10294ms step_avg:94.44ms
step:120/1770 train_time:10388ms step_avg:94.44ms
step:121/1770 train_time:10482ms step_avg:94.43ms
step:122/1770 train_time:10576ms step_avg:94.43ms
step:123/1770 train_time:10671ms step_avg:94.43ms
step:124/1770 train_time:10765ms step_avg:94.43ms
step:125/1770 train_time:10860ms step_avg:94.43ms
step:125/1770 val_loss:4.6399 train_time:10952ms step_avg:95.24ms
step:126/1770 train_time:10977ms step_avg:94.63ms
step:127/1770 train_time:11052ms step_avg:94.46ms
step:128/1770 train_time:11148ms step_avg:94.48ms
step:129/1770 train_time:11250ms step_avg:94.54ms
step:130/1770 train_time:11347ms step_avg:94.56ms
step:131/1770 train_time:11441ms step_avg:94.55ms
step:132/1770 train_time:11535ms step_avg:94.55ms
step:133/1770 train_time:11630ms step_avg:94.55ms
step:134/1770 train_time:11724ms step_avg:94.55ms
step:135/1770 train_time:11820ms step_avg:94.56ms
step:136/1770 train_time:11914ms step_avg:94.56ms
step:137/1770 train_time:12009ms step_avg:94.56ms
step:138/1770 train_time:12104ms step_avg:94.57ms
step:139/1770 train_time:12200ms step_avg:94.57ms
step:140/1770 train_time:12295ms step_avg:94.57ms
step:141/1770 train_time:12390ms step_avg:94.58ms
step:142/1770 train_time:12485ms step_avg:94.58ms
step:143/1770 train_time:12580ms step_avg:94.59ms
step:144/1770 train_time:12675ms step_avg:94.59ms
step:145/1770 train_time:12769ms step_avg:94.59ms
step:146/1770 train_time:12864ms step_avg:94.59ms
step:147/1770 train_time:12960ms step_avg:94.60ms
step:148/1770 train_time:13054ms step_avg:94.60ms
step:149/1770 train_time:13149ms step_avg:94.60ms
step:150/1770 train_time:13244ms step_avg:94.60ms
step:151/1770 train_time:13340ms step_avg:94.61ms
step:152/1770 train_time:13434ms step_avg:94.61ms
step:153/1770 train_time:13529ms step_avg:94.61ms
step:154/1770 train_time:13624ms step_avg:94.61ms
step:155/1770 train_time:13720ms step_avg:94.62ms
step:156/1770 train_time:13814ms step_avg:94.62ms
step:157/1770 train_time:13909ms step_avg:94.62ms
step:158/1770 train_time:14004ms step_avg:94.62ms
step:159/1770 train_time:14100ms step_avg:94.63ms
step:160/1770 train_time:14195ms step_avg:94.64ms
step:161/1770 train_time:14290ms step_avg:94.64ms
step:162/1770 train_time:14386ms step_avg:94.64ms
step:163/1770 train_time:14481ms step_avg:94.65ms
step:164/1770 train_time:14576ms step_avg:94.65ms
step:165/1770 train_time:14671ms step_avg:94.65ms
step:166/1770 train_time:14766ms step_avg:94.65ms
step:167/1770 train_time:14861ms step_avg:94.66ms
step:168/1770 train_time:14956ms step_avg:94.66ms
step:169/1770 train_time:15051ms step_avg:94.66ms
step:170/1770 train_time:15146ms step_avg:94.66ms
step:171/1770 train_time:15241ms step_avg:94.66ms
step:172/1770 train_time:15336ms step_avg:94.66ms
step:173/1770 train_time:15430ms step_avg:94.66ms
step:174/1770 train_time:15526ms step_avg:94.67ms
step:175/1770 train_time:15621ms step_avg:94.67ms
step:176/1770 train_time:15716ms step_avg:94.68ms
step:177/1770 train_time:15811ms step_avg:94.68ms
step:178/1770 train_time:15906ms step_avg:94.68ms
step:179/1770 train_time:16002ms step_avg:94.69ms
step:180/1770 train_time:16098ms step_avg:94.69ms
step:181/1770 train_time:16193ms step_avg:94.69ms
step:182/1770 train_time:16287ms step_avg:94.69ms
step:183/1770 train_time:16383ms step_avg:94.70ms
step:184/1770 train_time:16478ms step_avg:94.70ms
step:185/1770 train_time:16572ms step_avg:94.70ms
step:186/1770 train_time:16667ms step_avg:94.70ms
step:187/1770 train_time:16763ms step_avg:94.70ms
step:188/1770 train_time:16858ms step_avg:94.71ms
step:189/1770 train_time:16954ms step_avg:94.71ms
step:190/1770 train_time:17047ms step_avg:94.71ms
step:191/1770 train_time:17143ms step_avg:94.71ms
step:192/1770 train_time:17238ms step_avg:94.71ms
step:193/1770 train_time:17333ms step_avg:94.71ms
step:194/1770 train_time:17428ms step_avg:94.72ms
step:195/1770 train_time:17523ms step_avg:94.72ms
step:196/1770 train_time:17619ms step_avg:94.72ms
step:197/1770 train_time:17714ms step_avg:94.73ms
step:198/1770 train_time:17808ms step_avg:94.72ms
step:199/1770 train_time:17904ms step_avg:94.73ms
step:200/1770 train_time:17999ms step_avg:94.73ms
step:201/1770 train_time:18094ms step_avg:94.73ms
step:202/1770 train_time:18188ms step_avg:94.73ms
step:203/1770 train_time:18284ms step_avg:94.74ms
step:204/1770 train_time:18379ms step_avg:94.74ms
step:205/1770 train_time:18474ms step_avg:94.74ms
step:206/1770 train_time:18569ms step_avg:94.74ms
step:207/1770 train_time:18664ms step_avg:94.74ms
step:208/1770 train_time:18759ms step_avg:94.74ms
step:209/1770 train_time:18855ms step_avg:94.75ms
step:210/1770 train_time:18950ms step_avg:94.75ms
step:211/1770 train_time:19045ms step_avg:94.75ms
step:212/1770 train_time:19140ms step_avg:94.75ms
step:213/1770 train_time:19236ms step_avg:94.76ms
step:214/1770 train_time:19330ms step_avg:94.76ms
step:215/1770 train_time:19426ms step_avg:94.76ms
step:216/1770 train_time:19521ms step_avg:94.76ms
step:217/1770 train_time:19616ms step_avg:94.76ms
step:218/1770 train_time:19710ms step_avg:94.76ms
step:219/1770 train_time:19805ms step_avg:94.76ms
step:220/1770 train_time:19901ms step_avg:94.77ms
step:221/1770 train_time:19996ms step_avg:94.77ms
step:222/1770 train_time:20091ms step_avg:94.77ms
step:223/1770 train_time:20186ms step_avg:94.77ms
step:224/1770 train_time:20281ms step_avg:94.77ms
step:225/1770 train_time:20377ms step_avg:94.78ms
step:226/1770 train_time:20471ms step_avg:94.77ms
step:227/1770 train_time:20566ms step_avg:94.78ms
step:228/1770 train_time:20662ms step_avg:94.78ms
step:229/1770 train_time:20757ms step_avg:94.78ms
step:230/1770 train_time:20851ms step_avg:94.78ms
step:231/1770 train_time:20946ms step_avg:94.78ms
step:232/1770 train_time:21042ms step_avg:94.78ms
step:233/1770 train_time:21137ms step_avg:94.78ms
step:234/1770 train_time:21231ms step_avg:94.78ms
step:235/1770 train_time:21326ms step_avg:94.78ms
step:236/1770 train_time:21422ms step_avg:94.79ms
step:237/1770 train_time:21517ms step_avg:94.79ms
step:238/1770 train_time:21612ms step_avg:94.79ms
step:239/1770 train_time:21707ms step_avg:94.79ms
step:240/1770 train_time:21802ms step_avg:94.79ms
step:241/1770 train_time:21898ms step_avg:94.80ms
step:242/1770 train_time:21992ms step_avg:94.79ms
step:243/1770 train_time:22087ms step_avg:94.80ms
step:244/1770 train_time:22183ms step_avg:94.80ms
step:245/1770 train_time:22278ms step_avg:94.80ms
step:246/1770 train_time:22372ms step_avg:94.80ms
step:247/1770 train_time:22467ms step_avg:94.80ms
step:248/1770 train_time:22563ms step_avg:94.80ms
step:249/1770 train_time:22658ms step_avg:94.80ms
step:250/1770 train_time:22753ms step_avg:94.80ms
step:250/1770 val_loss:4.0987 train_time:22846ms step_avg:95.19ms
step:251/1770 train_time:22868ms step_avg:94.89ms
step:252/1770 train_time:22952ms step_avg:94.84ms
step:253/1770 train_time:23051ms step_avg:94.86ms
step:254/1770 train_time:23147ms step_avg:94.86ms
step:255/1770 train_time:23242ms step_avg:94.86ms
step:256/1770 train_time:23337ms step_avg:94.86ms
step:257/1770 train_time:23431ms step_avg:94.86ms
step:258/1770 train_time:23526ms step_avg:94.86ms
step:259/1770 train_time:23621ms step_avg:94.86ms
step:260/1770 train_time:23716ms step_avg:94.86ms
step:261/1770 train_time:23811ms step_avg:94.86ms
step:262/1770 train_time:23906ms step_avg:94.87ms
step:263/1770 train_time:24001ms step_avg:94.86ms
step:264/1770 train_time:24096ms step_avg:94.87ms
step:265/1770 train_time:24192ms step_avg:94.87ms
step:266/1770 train_time:24288ms step_avg:94.88ms
step:267/1770 train_time:24383ms step_avg:94.88ms
step:268/1770 train_time:24478ms step_avg:94.88ms
step:269/1770 train_time:24574ms step_avg:94.88ms
step:270/1770 train_time:24670ms step_avg:94.89ms
step:271/1770 train_time:24766ms step_avg:94.89ms
step:272/1770 train_time:24861ms step_avg:94.89ms
step:273/1770 train_time:24957ms step_avg:94.89ms
step:274/1770 train_time:25053ms step_avg:94.90ms
step:275/1770 train_time:25149ms step_avg:94.90ms
step:276/1770 train_time:25244ms step_avg:94.90ms
step:277/1770 train_time:25339ms step_avg:94.90ms
step:278/1770 train_time:25435ms step_avg:94.91ms
step:279/1770 train_time:25531ms step_avg:94.91ms
step:280/1770 train_time:25626ms step_avg:94.91ms
step:281/1770 train_time:25722ms step_avg:94.92ms
step:282/1770 train_time:25817ms step_avg:94.92ms
step:283/1770 train_time:25914ms step_avg:94.92ms
step:284/1770 train_time:26009ms step_avg:94.92ms
step:285/1770 train_time:26105ms step_avg:94.93ms
step:286/1770 train_time:26200ms step_avg:94.93ms
step:287/1770 train_time:26296ms step_avg:94.93ms
step:288/1770 train_time:26392ms step_avg:94.93ms
step:289/1770 train_time:26488ms step_avg:94.94ms
step:290/1770 train_time:26583ms step_avg:94.94ms
step:291/1770 train_time:26678ms step_avg:94.94ms
step:292/1770 train_time:26775ms step_avg:94.95ms
step:293/1770 train_time:26870ms step_avg:94.95ms
step:294/1770 train_time:26966ms step_avg:94.95ms
step:295/1770 train_time:27061ms step_avg:94.95ms
step:296/1770 train_time:27157ms step_avg:94.95ms
step:297/1770 train_time:27253ms step_avg:94.96ms
step:298/1770 train_time:27349ms step_avg:94.96ms
step:299/1770 train_time:27444ms step_avg:94.96ms
step:300/1770 train_time:27539ms step_avg:94.96ms
step:301/1770 train_time:27636ms step_avg:94.97ms
step:302/1770 train_time:27732ms step_avg:94.97ms
step:303/1770 train_time:27827ms step_avg:94.97ms
step:304/1770 train_time:27923ms step_avg:94.98ms
step:305/1770 train_time:28019ms step_avg:94.98ms
step:306/1770 train_time:28116ms step_avg:94.99ms
step:307/1770 train_time:28213ms step_avg:94.99ms
step:308/1770 train_time:28308ms step_avg:94.99ms
step:309/1770 train_time:28404ms step_avg:95.00ms
step:310/1770 train_time:28499ms step_avg:95.00ms
step:311/1770 train_time:28595ms step_avg:95.00ms
step:312/1770 train_time:28691ms step_avg:95.00ms
step:313/1770 train_time:28786ms step_avg:95.00ms
step:314/1770 train_time:28882ms step_avg:95.01ms
step:315/1770 train_time:28977ms step_avg:95.01ms
step:316/1770 train_time:29073ms step_avg:95.01ms
step:317/1770 train_time:29170ms step_avg:95.02ms
step:318/1770 train_time:29266ms step_avg:95.02ms
step:319/1770 train_time:29363ms step_avg:95.02ms
step:320/1770 train_time:29458ms step_avg:95.03ms
step:321/1770 train_time:29554ms step_avg:95.03ms
step:322/1770 train_time:29650ms step_avg:95.03ms
step:323/1770 train_time:29745ms step_avg:95.03ms
step:324/1770 train_time:29841ms step_avg:95.03ms
step:325/1770 train_time:29936ms step_avg:95.03ms
step:326/1770 train_time:30032ms step_avg:95.04ms
step:327/1770 train_time:30128ms step_avg:95.04ms
step:328/1770 train_time:30224ms step_avg:95.04ms
step:329/1770 train_time:30319ms step_avg:95.04ms
step:330/1770 train_time:30415ms step_avg:95.05ms
step:331/1770 train_time:30511ms step_avg:95.05ms
step:332/1770 train_time:30607ms step_avg:95.05ms
step:333/1770 train_time:30703ms step_avg:95.05ms
step:334/1770 train_time:30798ms step_avg:95.06ms
step:335/1770 train_time:30894ms step_avg:95.06ms
step:336/1770 train_time:30990ms step_avg:95.06ms
step:337/1770 train_time:31085ms step_avg:95.06ms
step:338/1770 train_time:31181ms step_avg:95.06ms
step:339/1770 train_time:31276ms step_avg:95.06ms
step:340/1770 train_time:31373ms step_avg:95.07ms
step:341/1770 train_time:31468ms step_avg:95.07ms
step:342/1770 train_time:31564ms step_avg:95.07ms
step:343/1770 train_time:31659ms step_avg:95.07ms
step:344/1770 train_time:31755ms step_avg:95.07ms
step:345/1770 train_time:31851ms step_avg:95.08ms
step:346/1770 train_time:31947ms step_avg:95.08ms
step:347/1770 train_time:32042ms step_avg:95.08ms
step:348/1770 train_time:32138ms step_avg:95.08ms
step:349/1770 train_time:32234ms step_avg:95.09ms
step:350/1770 train_time:32330ms step_avg:95.09ms
step:351/1770 train_time:32426ms step_avg:95.09ms
step:352/1770 train_time:32521ms step_avg:95.09ms
step:353/1770 train_time:32617ms step_avg:95.09ms
step:354/1770 train_time:32713ms step_avg:95.10ms
step:355/1770 train_time:32808ms step_avg:95.10ms
step:356/1770 train_time:32903ms step_avg:95.10ms
step:357/1770 train_time:32999ms step_avg:95.10ms
step:358/1770 train_time:33095ms step_avg:95.10ms
step:359/1770 train_time:33190ms step_avg:95.10ms
step:360/1770 train_time:33286ms step_avg:95.10ms
step:361/1770 train_time:33381ms step_avg:95.10ms
step:362/1770 train_time:33477ms step_avg:95.11ms
step:363/1770 train_time:33573ms step_avg:95.11ms
step:364/1770 train_time:33669ms step_avg:95.11ms
step:365/1770 train_time:33764ms step_avg:95.11ms
step:366/1770 train_time:33860ms step_avg:95.11ms
step:367/1770 train_time:33955ms step_avg:95.11ms
step:368/1770 train_time:34051ms step_avg:95.12ms
step:369/1770 train_time:34147ms step_avg:95.12ms
step:370/1770 train_time:34242ms step_avg:95.12ms
step:371/1770 train_time:34338ms step_avg:95.12ms
step:372/1770 train_time:34434ms step_avg:95.12ms
step:373/1770 train_time:34530ms step_avg:95.12ms
step:374/1770 train_time:34625ms step_avg:95.12ms
step:375/1770 train_time:34720ms step_avg:95.12ms
step:375/1770 val_loss:3.9032 train_time:34814ms step_avg:95.38ms
step:376/1770 train_time:34839ms step_avg:95.19ms
step:377/1770 train_time:34918ms step_avg:95.15ms
step:378/1770 train_time:35018ms step_avg:95.16ms
step:379/1770 train_time:35116ms step_avg:95.17ms
step:380/1770 train_time:35212ms step_avg:95.17ms
step:381/1770 train_time:35307ms step_avg:95.17ms
step:382/1770 train_time:35402ms step_avg:95.17ms
step:383/1770 train_time:35498ms step_avg:95.17ms
step:384/1770 train_time:35593ms step_avg:95.17ms
step:385/1770 train_time:35689ms step_avg:95.17ms
step:386/1770 train_time:35784ms step_avg:95.17ms
step:387/1770 train_time:35879ms step_avg:95.17ms
step:388/1770 train_time:35976ms step_avg:95.17ms
step:389/1770 train_time:36072ms step_avg:95.18ms
step:390/1770 train_time:36168ms step_avg:95.18ms
step:391/1770 train_time:36263ms step_avg:95.18ms
step:392/1770 train_time:36359ms step_avg:95.18ms
step:393/1770 train_time:36455ms step_avg:95.18ms
step:394/1770 train_time:36550ms step_avg:95.18ms
step:395/1770 train_time:36646ms step_avg:95.18ms
step:396/1770 train_time:36743ms step_avg:95.19ms
step:397/1770 train_time:36840ms step_avg:95.19ms
step:398/1770 train_time:36938ms step_avg:95.20ms
step:399/1770 train_time:37035ms step_avg:95.21ms
step:400/1770 train_time:37133ms step_avg:95.21ms
step:401/1770 train_time:37231ms step_avg:95.22ms
step:402/1770 train_time:37328ms step_avg:95.23ms
step:403/1770 train_time:37425ms step_avg:95.23ms
step:404/1770 train_time:37523ms step_avg:95.23ms
step:405/1770 train_time:37621ms step_avg:95.24ms
step:406/1770 train_time:37718ms step_avg:95.25ms
step:407/1770 train_time:37817ms step_avg:95.26ms
step:408/1770 train_time:37914ms step_avg:95.26ms
step:409/1770 train_time:38012ms step_avg:95.27ms
step:410/1770 train_time:38109ms step_avg:95.27ms
step:411/1770 train_time:38206ms step_avg:95.28ms
step:412/1770 train_time:38303ms step_avg:95.28ms
step:413/1770 train_time:38401ms step_avg:95.29ms
step:414/1770 train_time:38498ms step_avg:95.29ms
step:415/1770 train_time:38597ms step_avg:95.30ms
step:416/1770 train_time:38695ms step_avg:95.31ms
step:417/1770 train_time:38792ms step_avg:95.31ms
step:418/1770 train_time:38890ms step_avg:95.32ms
step:419/1770 train_time:38987ms step_avg:95.32ms
step:420/1770 train_time:39084ms step_avg:95.33ms
step:421/1770 train_time:39181ms step_avg:95.33ms
step:422/1770 train_time:39279ms step_avg:95.34ms
step:423/1770 train_time:39377ms step_avg:95.34ms
step:424/1770 train_time:39475ms step_avg:95.35ms
step:425/1770 train_time:39573ms step_avg:95.36ms
step:426/1770 train_time:39671ms step_avg:95.36ms
step:427/1770 train_time:39768ms step_avg:95.37ms
step:428/1770 train_time:39865ms step_avg:95.37ms
step:429/1770 train_time:39963ms step_avg:95.38ms
step:430/1770 train_time:40060ms step_avg:95.38ms
step:431/1770 train_time:40158ms step_avg:95.39ms
step:432/1770 train_time:40255ms step_avg:95.39ms
step:433/1770 train_time:40353ms step_avg:95.40ms
step:434/1770 train_time:40450ms step_avg:95.40ms
step:435/1770 train_time:40547ms step_avg:95.41ms
step:436/1770 train_time:40644ms step_avg:95.41ms
step:437/1770 train_time:40741ms step_avg:95.41ms
step:438/1770 train_time:40839ms step_avg:95.42ms
step:439/1770 train_time:40937ms step_avg:95.42ms
step:440/1770 train_time:41035ms step_avg:95.43ms
step:441/1770 train_time:41132ms step_avg:95.43ms
step:442/1770 train_time:41229ms step_avg:95.44ms
step:443/1770 train_time:41326ms step_avg:95.44ms
step:444/1770 train_time:41423ms step_avg:95.45ms
step:445/1770 train_time:41521ms step_avg:95.45ms
step:446/1770 train_time:41619ms step_avg:95.46ms
step:447/1770 train_time:41717ms step_avg:95.46ms
step:448/1770 train_time:41814ms step_avg:95.47ms
step:449/1770 train_time:41911ms step_avg:95.47ms
step:450/1770 train_time:42009ms step_avg:95.47ms
step:451/1770 train_time:42106ms step_avg:95.48ms
step:452/1770 train_time:42203ms step_avg:95.48ms
step:453/1770 train_time:42300ms step_avg:95.49ms
step:454/1770 train_time:42398ms step_avg:95.49ms
step:455/1770 train_time:42496ms step_avg:95.50ms
step:456/1770 train_time:42593ms step_avg:95.50ms
step:457/1770 train_time:42691ms step_avg:95.51ms
step:458/1770 train_time:42788ms step_avg:95.51ms
step:459/1770 train_time:42886ms step_avg:95.51ms
step:460/1770 train_time:42983ms step_avg:95.52ms
step:461/1770 train_time:43080ms step_avg:95.52ms
step:462/1770 train_time:43178ms step_avg:95.53ms
step:463/1770 train_time:43276ms step_avg:95.53ms
step:464/1770 train_time:43374ms step_avg:95.54ms
step:465/1770 train_time:43471ms step_avg:95.54ms
step:466/1770 train_time:43568ms step_avg:95.54ms
step:467/1770 train_time:43666ms step_avg:95.55ms
step:468/1770 train_time:43763ms step_avg:95.55ms
step:469/1770 train_time:43861ms step_avg:95.56ms
step:470/1770 train_time:43958ms step_avg:95.56ms
step:471/1770 train_time:44057ms step_avg:95.57ms
step:472/1770 train_time:44155ms step_avg:95.57ms
step:473/1770 train_time:44252ms step_avg:95.58ms
step:474/1770 train_time:44349ms step_avg:95.58ms
step:475/1770 train_time:44447ms step_avg:95.58ms
step:476/1770 train_time:44543ms step_avg:95.59ms
step:477/1770 train_time:44641ms step_avg:95.59ms
step:478/1770 train_time:44738ms step_avg:95.59ms
step:479/1770 train_time:44836ms step_avg:95.60ms
step:480/1770 train_time:44934ms step_avg:95.60ms
step:481/1770 train_time:45031ms step_avg:95.61ms
step:482/1770 train_time:45129ms step_avg:95.61ms
step:483/1770 train_time:45226ms step_avg:95.62ms
step:484/1770 train_time:45323ms step_avg:95.62ms
step:485/1770 train_time:45421ms step_avg:95.62ms
step:486/1770 train_time:45518ms step_avg:95.63ms
step:487/1770 train_time:45616ms step_avg:95.63ms
step:488/1770 train_time:45713ms step_avg:95.63ms
step:489/1770 train_time:45811ms step_avg:95.64ms
step:490/1770 train_time:45908ms step_avg:95.64ms
step:491/1770 train_time:46005ms step_avg:95.65ms
step:492/1770 train_time:46102ms step_avg:95.65ms
step:493/1770 train_time:46200ms step_avg:95.65ms
step:494/1770 train_time:46298ms step_avg:95.66ms
step:495/1770 train_time:46395ms step_avg:95.66ms
step:496/1770 train_time:46493ms step_avg:95.66ms
step:497/1770 train_time:46590ms step_avg:95.67ms
step:498/1770 train_time:46687ms step_avg:95.67ms
step:499/1770 train_time:46785ms step_avg:95.67ms
step:500/1770 train_time:46882ms step_avg:95.68ms
step:500/1770 val_loss:3.7521 train_time:46978ms step_avg:95.87ms
step:501/1770 train_time:47000ms step_avg:95.72ms
step:502/1770 train_time:47087ms step_avg:95.70ms
step:503/1770 train_time:47186ms step_avg:95.71ms
step:504/1770 train_time:47284ms step_avg:95.72ms
step:505/1770 train_time:47382ms step_avg:95.72ms
step:506/1770 train_time:47479ms step_avg:95.72ms
step:507/1770 train_time:47576ms step_avg:95.73ms
step:508/1770 train_time:47673ms step_avg:95.73ms
step:509/1770 train_time:47771ms step_avg:95.73ms
step:510/1770 train_time:47868ms step_avg:95.74ms
step:511/1770 train_time:47965ms step_avg:95.74ms
step:512/1770 train_time:48064ms step_avg:95.74ms
step:513/1770 train_time:48162ms step_avg:95.75ms
step:514/1770 train_time:48260ms step_avg:95.75ms
step:515/1770 train_time:48357ms step_avg:95.76ms
step:516/1770 train_time:48454ms step_avg:95.76ms
step:517/1770 train_time:48551ms step_avg:95.76ms
step:518/1770 train_time:48649ms step_avg:95.76ms
step:519/1770 train_time:48746ms step_avg:95.77ms
step:520/1770 train_time:48844ms step_avg:95.77ms
step:521/1770 train_time:48941ms step_avg:95.78ms
step:522/1770 train_time:49039ms step_avg:95.78ms
step:523/1770 train_time:49136ms step_avg:95.78ms
step:524/1770 train_time:49233ms step_avg:95.78ms
step:525/1770 train_time:49330ms step_avg:95.79ms
step:526/1770 train_time:49428ms step_avg:95.79ms
step:527/1770 train_time:49526ms step_avg:95.79ms
step:528/1770 train_time:49624ms step_avg:95.80ms
step:529/1770 train_time:49722ms step_avg:95.80ms
step:530/1770 train_time:49820ms step_avg:95.81ms
step:531/1770 train_time:49917ms step_avg:95.81ms
step:532/1770 train_time:50015ms step_avg:95.81ms
step:533/1770 train_time:50113ms step_avg:95.82ms
step:534/1770 train_time:50211ms step_avg:95.82ms
step:535/1770 train_time:50309ms step_avg:95.83ms
step:536/1770 train_time:50407ms step_avg:95.83ms
step:537/1770 train_time:50504ms step_avg:95.83ms
step:538/1770 train_time:50602ms step_avg:95.84ms
step:539/1770 train_time:50700ms step_avg:95.84ms
step:540/1770 train_time:50798ms step_avg:95.85ms
step:541/1770 train_time:50896ms step_avg:95.85ms
step:542/1770 train_time:50993ms step_avg:95.85ms
step:543/1770 train_time:51091ms step_avg:95.86ms
step:544/1770 train_time:51188ms step_avg:95.86ms
step:545/1770 train_time:51286ms step_avg:95.86ms
step:546/1770 train_time:51385ms step_avg:95.87ms
step:547/1770 train_time:51482ms step_avg:95.87ms
step:548/1770 train_time:51580ms step_avg:95.87ms
step:549/1770 train_time:51677ms step_avg:95.88ms
step:550/1770 train_time:51775ms step_avg:95.88ms
step:551/1770 train_time:51872ms step_avg:95.88ms
step:552/1770 train_time:51970ms step_avg:95.89ms
step:553/1770 train_time:52067ms step_avg:95.89ms
step:554/1770 train_time:52166ms step_avg:95.89ms
step:555/1770 train_time:52264ms step_avg:95.90ms
step:556/1770 train_time:52362ms step_avg:95.90ms
step:557/1770 train_time:52460ms step_avg:95.90ms
step:558/1770 train_time:52558ms step_avg:95.91ms
step:559/1770 train_time:52655ms step_avg:95.91ms
step:560/1770 train_time:52753ms step_avg:95.91ms
step:561/1770 train_time:52850ms step_avg:95.92ms
step:562/1770 train_time:52948ms step_avg:95.92ms
step:563/1770 train_time:53046ms step_avg:95.92ms
step:564/1770 train_time:53144ms step_avg:95.93ms
step:565/1770 train_time:53242ms step_avg:95.93ms
step:566/1770 train_time:53340ms step_avg:95.93ms
step:567/1770 train_time:53437ms step_avg:95.94ms
step:568/1770 train_time:53534ms step_avg:95.94ms
step:569/1770 train_time:53632ms step_avg:95.94ms
step:570/1770 train_time:53729ms step_avg:95.95ms
step:571/1770 train_time:53827ms step_avg:95.95ms
step:572/1770 train_time:53925ms step_avg:95.95ms
step:573/1770 train_time:54023ms step_avg:95.96ms
step:574/1770 train_time:54122ms step_avg:95.96ms
step:575/1770 train_time:54219ms step_avg:95.96ms
step:576/1770 train_time:54317ms step_avg:95.97ms
step:577/1770 train_time:54415ms step_avg:95.97ms
step:578/1770 train_time:54513ms step_avg:95.97ms
step:579/1770 train_time:54610ms step_avg:95.98ms
step:580/1770 train_time:54708ms step_avg:95.98ms
step:581/1770 train_time:54806ms step_avg:95.98ms
step:582/1770 train_time:54904ms step_avg:95.99ms
step:583/1770 train_time:55002ms step_avg:95.99ms
step:584/1770 train_time:55100ms step_avg:95.99ms
step:585/1770 train_time:55198ms step_avg:96.00ms
step:586/1770 train_time:55296ms step_avg:96.00ms
step:587/1770 train_time:55394ms step_avg:96.00ms
step:588/1770 train_time:55491ms step_avg:96.01ms
step:589/1770 train_time:55589ms step_avg:96.01ms
step:590/1770 train_time:55687ms step_avg:96.01ms
step:591/1770 train_time:55785ms step_avg:96.02ms
step:592/1770 train_time:55884ms step_avg:96.02ms
step:593/1770 train_time:55981ms step_avg:96.02ms
step:594/1770 train_time:56079ms step_avg:96.03ms
step:595/1770 train_time:56177ms step_avg:96.03ms
step:596/1770 train_time:56274ms step_avg:96.03ms
step:597/1770 train_time:56372ms step_avg:96.03ms
step:598/1770 train_time:56470ms step_avg:96.04ms
step:599/1770 train_time:56568ms step_avg:96.04ms
step:600/1770 train_time:56666ms step_avg:96.04ms
step:601/1770 train_time:56764ms step_avg:96.05ms
step:602/1770 train_time:56863ms step_avg:96.05ms
step:603/1770 train_time:56960ms step_avg:96.05ms
step:604/1770 train_time:57058ms step_avg:96.06ms
step:605/1770 train_time:57155ms step_avg:96.06ms
step:606/1770 train_time:57253ms step_avg:96.06ms
step:607/1770 train_time:57350ms step_avg:96.06ms
step:608/1770 train_time:57448ms step_avg:96.07ms
step:609/1770 train_time:57546ms step_avg:96.07ms
step:610/1770 train_time:57645ms step_avg:96.08ms
step:611/1770 train_time:57743ms step_avg:96.08ms
step:612/1770 train_time:57842ms step_avg:96.08ms
step:613/1770 train_time:57940ms step_avg:96.09ms
step:614/1770 train_time:58037ms step_avg:96.09ms
step:615/1770 train_time:58135ms step_avg:96.09ms
step:616/1770 train_time:58232ms step_avg:96.09ms
step:617/1770 train_time:58330ms step_avg:96.10ms
step:618/1770 train_time:58429ms step_avg:96.10ms
step:619/1770 train_time:58527ms step_avg:96.10ms
step:620/1770 train_time:58625ms step_avg:96.11ms
step:621/1770 train_time:58723ms step_avg:96.11ms
step:622/1770 train_time:58821ms step_avg:96.11ms
step:623/1770 train_time:58919ms step_avg:96.12ms
step:624/1770 train_time:59016ms step_avg:96.12ms
step:625/1770 train_time:59114ms step_avg:96.12ms
step:625/1770 val_loss:3.6656 train_time:59210ms step_avg:96.28ms
step:626/1770 train_time:59231ms step_avg:96.15ms
step:627/1770 train_time:59319ms step_avg:96.14ms
step:628/1770 train_time:59421ms step_avg:96.15ms
step:629/1770 train_time:59519ms step_avg:96.15ms
step:630/1770 train_time:59616ms step_avg:96.16ms
step:631/1770 train_time:59714ms step_avg:96.16ms
step:632/1770 train_time:59813ms step_avg:96.16ms
step:633/1770 train_time:59911ms step_avg:96.16ms
step:634/1770 train_time:60008ms step_avg:96.17ms
step:635/1770 train_time:60106ms step_avg:96.17ms
step:636/1770 train_time:60204ms step_avg:96.17ms
step:637/1770 train_time:60301ms step_avg:96.17ms
step:638/1770 train_time:60399ms step_avg:96.18ms
step:639/1770 train_time:60496ms step_avg:96.18ms
step:640/1770 train_time:60594ms step_avg:96.18ms
step:641/1770 train_time:60693ms step_avg:96.19ms
step:642/1770 train_time:60791ms step_avg:96.19ms
step:643/1770 train_time:60889ms step_avg:96.19ms
step:644/1770 train_time:60987ms step_avg:96.19ms
step:645/1770 train_time:61084ms step_avg:96.20ms
step:646/1770 train_time:61182ms step_avg:96.20ms
step:647/1770 train_time:61279ms step_avg:96.20ms
step:648/1770 train_time:61377ms step_avg:96.20ms
step:649/1770 train_time:61475ms step_avg:96.20ms
step:650/1770 train_time:61573ms step_avg:96.21ms
step:651/1770 train_time:61671ms step_avg:96.21ms
step:652/1770 train_time:61769ms step_avg:96.21ms
step:653/1770 train_time:61867ms step_avg:96.22ms
step:654/1770 train_time:61964ms step_avg:96.22ms
step:655/1770 train_time:62061ms step_avg:96.22ms
step:656/1770 train_time:62159ms step_avg:96.22ms
step:657/1770 train_time:62257ms step_avg:96.22ms
step:658/1770 train_time:62356ms step_avg:96.23ms
step:659/1770 train_time:62455ms step_avg:96.23ms
step:660/1770 train_time:62555ms step_avg:96.24ms
step:661/1770 train_time:62655ms step_avg:96.24ms
step:662/1770 train_time:62755ms step_avg:96.25ms
step:663/1770 train_time:62855ms step_avg:96.26ms
step:664/1770 train_time:62955ms step_avg:96.26ms
step:665/1770 train_time:63055ms step_avg:96.27ms
step:666/1770 train_time:63154ms step_avg:96.27ms
step:667/1770 train_time:63255ms step_avg:96.28ms
step:668/1770 train_time:63355ms step_avg:96.28ms
step:669/1770 train_time:63455ms step_avg:96.29ms
step:670/1770 train_time:63555ms step_avg:96.30ms
step:671/1770 train_time:63655ms step_avg:96.30ms
step:672/1770 train_time:63755ms step_avg:96.31ms
step:673/1770 train_time:63856ms step_avg:96.31ms
step:674/1770 train_time:63955ms step_avg:96.32ms
step:675/1770 train_time:64055ms step_avg:96.32ms
step:676/1770 train_time:64154ms step_avg:96.33ms
step:677/1770 train_time:64255ms step_avg:96.33ms
step:678/1770 train_time:64355ms step_avg:96.34ms
step:679/1770 train_time:64455ms step_avg:96.35ms
step:680/1770 train_time:64555ms step_avg:96.35ms
step:681/1770 train_time:64655ms step_avg:96.36ms
step:682/1770 train_time:64755ms step_avg:96.36ms
step:683/1770 train_time:64855ms step_avg:96.37ms
step:684/1770 train_time:64955ms step_avg:96.37ms
step:685/1770 train_time:65055ms step_avg:96.38ms
step:686/1770 train_time:65154ms step_avg:96.38ms
step:687/1770 train_time:65254ms step_avg:96.39ms
step:688/1770 train_time:65354ms step_avg:96.39ms
step:689/1770 train_time:65454ms step_avg:96.40ms
step:690/1770 train_time:65554ms step_avg:96.40ms
step:691/1770 train_time:65654ms step_avg:96.41ms
step:692/1770 train_time:65754ms step_avg:96.41ms
step:693/1770 train_time:65855ms step_avg:96.42ms
step:694/1770 train_time:65955ms step_avg:96.43ms
step:695/1770 train_time:66055ms step_avg:96.43ms
step:696/1770 train_time:66155ms step_avg:96.44ms
step:697/1770 train_time:66254ms step_avg:96.44ms
step:698/1770 train_time:66354ms step_avg:96.45ms
step:699/1770 train_time:66455ms step_avg:96.45ms
step:700/1770 train_time:66554ms step_avg:96.46ms
step:701/1770 train_time:66654ms step_avg:96.46ms
step:702/1770 train_time:66754ms step_avg:96.47ms
step:703/1770 train_time:66854ms step_avg:96.47ms
step:704/1770 train_time:66955ms step_avg:96.48ms
step:705/1770 train_time:67056ms step_avg:96.48ms
step:706/1770 train_time:67155ms step_avg:96.49ms
step:707/1770 train_time:67255ms step_avg:96.49ms
step:708/1770 train_time:67355ms step_avg:96.50ms
step:709/1770 train_time:67455ms step_avg:96.50ms
step:710/1770 train_time:67555ms step_avg:96.51ms
step:711/1770 train_time:67654ms step_avg:96.51ms
step:712/1770 train_time:67755ms step_avg:96.52ms
step:713/1770 train_time:67856ms step_avg:96.52ms
step:714/1770 train_time:67956ms step_avg:96.53ms
step:715/1770 train_time:68056ms step_avg:96.53ms
step:716/1770 train_time:68155ms step_avg:96.54ms
step:717/1770 train_time:68255ms step_avg:96.54ms
step:718/1770 train_time:68355ms step_avg:96.55ms
step:719/1770 train_time:68455ms step_avg:96.55ms
step:720/1770 train_time:68555ms step_avg:96.56ms
step:721/1770 train_time:68654ms step_avg:96.56ms
step:722/1770 train_time:68754ms step_avg:96.56ms
step:723/1770 train_time:68854ms step_avg:96.57ms
step:724/1770 train_time:68954ms step_avg:96.57ms
step:725/1770 train_time:69055ms step_avg:96.58ms
step:726/1770 train_time:69155ms step_avg:96.58ms
step:727/1770 train_time:69255ms step_avg:96.59ms
step:728/1770 train_time:69355ms step_avg:96.59ms
step:729/1770 train_time:69456ms step_avg:96.60ms
step:730/1770 train_time:69555ms step_avg:96.60ms
step:731/1770 train_time:69655ms step_avg:96.61ms
step:732/1770 train_time:69755ms step_avg:96.61ms
step:733/1770 train_time:69855ms step_avg:96.62ms
step:734/1770 train_time:69955ms step_avg:96.62ms
step:735/1770 train_time:70055ms step_avg:96.63ms
step:736/1770 train_time:70155ms step_avg:96.63ms
step:737/1770 train_time:70255ms step_avg:96.64ms
step:738/1770 train_time:70355ms step_avg:96.64ms
step:739/1770 train_time:70455ms step_avg:96.65ms
step:740/1770 train_time:70555ms step_avg:96.65ms
step:741/1770 train_time:70655ms step_avg:96.66ms
step:742/1770 train_time:70755ms step_avg:96.66ms
step:743/1770 train_time:70855ms step_avg:96.66ms
step:744/1770 train_time:70955ms step_avg:96.67ms
step:745/1770 train_time:71055ms step_avg:96.67ms
step:746/1770 train_time:71155ms step_avg:96.68ms
step:747/1770 train_time:71255ms step_avg:96.68ms
step:748/1770 train_time:71355ms step_avg:96.69ms
step:749/1770 train_time:71454ms step_avg:96.69ms
step:750/1770 train_time:71554ms step_avg:96.70ms
step:750/1770 val_loss:3.6011 train_time:71653ms step_avg:96.83ms
step:751/1770 train_time:71674ms step_avg:96.73ms
step:752/1770 train_time:71761ms step_avg:96.71ms
step:753/1770 train_time:71862ms step_avg:96.72ms
step:754/1770 train_time:71962ms step_avg:96.72ms
step:755/1770 train_time:72061ms step_avg:96.73ms
step:756/1770 train_time:72161ms step_avg:96.73ms
step:757/1770 train_time:72260ms step_avg:96.73ms
step:758/1770 train_time:72360ms step_avg:96.74ms
step:759/1770 train_time:72459ms step_avg:96.74ms
step:760/1770 train_time:72560ms step_avg:96.75ms
step:761/1770 train_time:72661ms step_avg:96.75ms
step:762/1770 train_time:72764ms step_avg:96.76ms
step:763/1770 train_time:72867ms step_avg:96.77ms
step:764/1770 train_time:72967ms step_avg:96.77ms
step:765/1770 train_time:73066ms step_avg:96.78ms
step:766/1770 train_time:73166ms step_avg:96.78ms
step:767/1770 train_time:73267ms step_avg:96.79ms
step:768/1770 train_time:73366ms step_avg:96.79ms
step:769/1770 train_time:73466ms step_avg:96.79ms
step:770/1770 train_time:73566ms step_avg:96.80ms
step:771/1770 train_time:73666ms step_avg:96.80ms
step:772/1770 train_time:73766ms step_avg:96.81ms
step:773/1770 train_time:73866ms step_avg:96.81ms
step:774/1770 train_time:73966ms step_avg:96.81ms
step:775/1770 train_time:74066ms step_avg:96.82ms
step:776/1770 train_time:74166ms step_avg:96.82ms
step:777/1770 train_time:74266ms step_avg:96.83ms
step:778/1770 train_time:74366ms step_avg:96.83ms
step:779/1770 train_time:74466ms step_avg:96.84ms
step:780/1770 train_time:74566ms step_avg:96.84ms
step:781/1770 train_time:74667ms step_avg:96.84ms
step:782/1770 train_time:74766ms step_avg:96.85ms
step:783/1770 train_time:74867ms step_avg:96.85ms
step:784/1770 train_time:74967ms step_avg:96.86ms
step:785/1770 train_time:75066ms step_avg:96.86ms
step:786/1770 train_time:75166ms step_avg:96.86ms
step:787/1770 train_time:75266ms step_avg:96.87ms
step:788/1770 train_time:75366ms step_avg:96.87ms
step:789/1770 train_time:75466ms step_avg:96.88ms
step:790/1770 train_time:75566ms step_avg:96.88ms
step:791/1770 train_time:75667ms step_avg:96.88ms
step:792/1770 train_time:75767ms step_avg:96.89ms
step:793/1770 train_time:75867ms step_avg:96.89ms
step:794/1770 train_time:75967ms step_avg:96.90ms
step:795/1770 train_time:76069ms step_avg:96.90ms
step:796/1770 train_time:76169ms step_avg:96.91ms
step:797/1770 train_time:76269ms step_avg:96.91ms
step:798/1770 train_time:76369ms step_avg:96.92ms
step:799/1770 train_time:76469ms step_avg:96.92ms
step:800/1770 train_time:76570ms step_avg:96.92ms
step:801/1770 train_time:76669ms step_avg:96.93ms
step:802/1770 train_time:76769ms step_avg:96.93ms
step:803/1770 train_time:76868ms step_avg:96.93ms
step:804/1770 train_time:76969ms step_avg:96.94ms
step:805/1770 train_time:77069ms step_avg:96.94ms
step:806/1770 train_time:77169ms step_avg:96.95ms
step:807/1770 train_time:77269ms step_avg:96.95ms
step:808/1770 train_time:77369ms step_avg:96.95ms
step:809/1770 train_time:77468ms step_avg:96.96ms
step:810/1770 train_time:77569ms step_avg:96.96ms
step:811/1770 train_time:77668ms step_avg:96.96ms
step:812/1770 train_time:77769ms step_avg:96.97ms
step:813/1770 train_time:77869ms step_avg:96.97ms
step:814/1770 train_time:77969ms step_avg:96.98ms
step:815/1770 train_time:78069ms step_avg:96.98ms
step:816/1770 train_time:78169ms step_avg:96.98ms
step:817/1770 train_time:78269ms step_avg:96.99ms
step:818/1770 train_time:78369ms step_avg:96.99ms
step:819/1770 train_time:78469ms step_avg:96.99ms
step:820/1770 train_time:78568ms step_avg:97.00ms
step:821/1770 train_time:78669ms step_avg:97.00ms
step:822/1770 train_time:78769ms step_avg:97.01ms
step:823/1770 train_time:78868ms step_avg:97.01ms
step:824/1770 train_time:78968ms step_avg:97.01ms
step:825/1770 train_time:79068ms step_avg:97.02ms
step:826/1770 train_time:79168ms step_avg:97.02ms
step:827/1770 train_time:79268ms step_avg:97.02ms
step:828/1770 train_time:79368ms step_avg:97.03ms
step:829/1770 train_time:79468ms step_avg:97.03ms
step:830/1770 train_time:79568ms step_avg:97.03ms
step:831/1770 train_time:79668ms step_avg:97.04ms
step:832/1770 train_time:79768ms step_avg:97.04ms
step:833/1770 train_time:79868ms step_avg:97.05ms
step:834/1770 train_time:79969ms step_avg:97.05ms
step:835/1770 train_time:80070ms step_avg:97.05ms
step:836/1770 train_time:80170ms step_avg:97.06ms
step:837/1770 train_time:80269ms step_avg:97.06ms
step:838/1770 train_time:80369ms step_avg:97.06ms
step:839/1770 train_time:80469ms step_avg:97.07ms
step:840/1770 train_time:80569ms step_avg:97.07ms
step:841/1770 train_time:80669ms step_avg:97.07ms
step:842/1770 train_time:80769ms step_avg:97.08ms
step:843/1770 train_time:80869ms step_avg:97.08ms
step:844/1770 train_time:80969ms step_avg:97.08ms
step:845/1770 train_time:81069ms step_avg:97.09ms
step:846/1770 train_time:81170ms step_avg:97.09ms
step:847/1770 train_time:81269ms step_avg:97.10ms
step:848/1770 train_time:81369ms step_avg:97.10ms
step:849/1770 train_time:81468ms step_avg:97.10ms
step:850/1770 train_time:81568ms step_avg:97.11ms
step:851/1770 train_time:81669ms step_avg:97.11ms
step:852/1770 train_time:81768ms step_avg:97.11ms
step:853/1770 train_time:81868ms step_avg:97.12ms
step:854/1770 train_time:81968ms step_avg:97.12ms
step:855/1770 train_time:82068ms step_avg:97.12ms
step:856/1770 train_time:82168ms step_avg:97.12ms
step:857/1770 train_time:82268ms step_avg:97.13ms
step:858/1770 train_time:82367ms step_avg:97.13ms
step:859/1770 train_time:82467ms step_avg:97.13ms
step:860/1770 train_time:82567ms step_avg:97.14ms
step:861/1770 train_time:82668ms step_avg:97.14ms
step:862/1770 train_time:82768ms step_avg:97.15ms
step:863/1770 train_time:82868ms step_avg:97.15ms
step:864/1770 train_time:82968ms step_avg:97.15ms
step:865/1770 train_time:83068ms step_avg:97.16ms
step:866/1770 train_time:83168ms step_avg:97.16ms
step:867/1770 train_time:83269ms step_avg:97.16ms
step:868/1770 train_time:83369ms step_avg:97.17ms
step:869/1770 train_time:83469ms step_avg:97.17ms
step:870/1770 train_time:83569ms step_avg:97.17ms
step:871/1770 train_time:83668ms step_avg:97.18ms
step:872/1770 train_time:83768ms step_avg:97.18ms
step:873/1770 train_time:83868ms step_avg:97.18ms
step:874/1770 train_time:83969ms step_avg:97.19ms
step:875/1770 train_time:84069ms step_avg:97.19ms
step:875/1770 val_loss:3.5518 train_time:84167ms step_avg:97.30ms
step:876/1770 train_time:84191ms step_avg:97.22ms
step:877/1770 train_time:84276ms step_avg:97.20ms
step:878/1770 train_time:84376ms step_avg:97.21ms
step:879/1770 train_time:84476ms step_avg:97.21ms
step:880/1770 train_time:84576ms step_avg:97.21ms
step:881/1770 train_time:84676ms step_avg:97.22ms
step:882/1770 train_time:84775ms step_avg:97.22ms
step:883/1770 train_time:84875ms step_avg:97.22ms
step:884/1770 train_time:84976ms step_avg:97.23ms
step:885/1770 train_time:85075ms step_avg:97.23ms
step:886/1770 train_time:85175ms step_avg:97.23ms
step:887/1770 train_time:85275ms step_avg:97.24ms
step:888/1770 train_time:85376ms step_avg:97.24ms
step:889/1770 train_time:85477ms step_avg:97.24ms
step:890/1770 train_time:85576ms step_avg:97.25ms
step:891/1770 train_time:85677ms step_avg:97.25ms
step:892/1770 train_time:85777ms step_avg:97.25ms
step:893/1770 train_time:85877ms step_avg:97.26ms
step:894/1770 train_time:85977ms step_avg:97.26ms
step:895/1770 train_time:86078ms step_avg:97.26ms
step:896/1770 train_time:86178ms step_avg:97.27ms
step:897/1770 train_time:86278ms step_avg:97.27ms
step:898/1770 train_time:86378ms step_avg:97.27ms
step:899/1770 train_time:86478ms step_avg:97.28ms
step:900/1770 train_time:86578ms step_avg:97.28ms
step:901/1770 train_time:86679ms step_avg:97.28ms
step:902/1770 train_time:86779ms step_avg:97.29ms
step:903/1770 train_time:86878ms step_avg:97.29ms
step:904/1770 train_time:86979ms step_avg:97.29ms
step:905/1770 train_time:87079ms step_avg:97.30ms
step:906/1770 train_time:87179ms step_avg:97.30ms
step:907/1770 train_time:87279ms step_avg:97.30ms
step:908/1770 train_time:87379ms step_avg:97.30ms
step:909/1770 train_time:87479ms step_avg:97.31ms
step:910/1770 train_time:87579ms step_avg:97.31ms
step:911/1770 train_time:87679ms step_avg:97.31ms
step:912/1770 train_time:87778ms step_avg:97.31ms
step:913/1770 train_time:87878ms step_avg:97.32ms
step:914/1770 train_time:87978ms step_avg:97.32ms
step:915/1770 train_time:88078ms step_avg:97.32ms
step:916/1770 train_time:88178ms step_avg:97.33ms
step:917/1770 train_time:88278ms step_avg:97.33ms
step:918/1770 train_time:88377ms step_avg:97.33ms
step:919/1770 train_time:88477ms step_avg:97.33ms
step:920/1770 train_time:88579ms step_avg:97.34ms
step:921/1770 train_time:88680ms step_avg:97.34ms
step:922/1770 train_time:88781ms step_avg:97.35ms
step:923/1770 train_time:88882ms step_avg:97.35ms
step:924/1770 train_time:88983ms step_avg:97.36ms
step:925/1770 train_time:89083ms step_avg:97.36ms
step:926/1770 train_time:89184ms step_avg:97.36ms
step:927/1770 train_time:89286ms step_avg:97.37ms
step:928/1770 train_time:89386ms step_avg:97.37ms
step:929/1770 train_time:89487ms step_avg:97.37ms
step:930/1770 train_time:89588ms step_avg:97.38ms
step:931/1770 train_time:89690ms step_avg:97.38ms
step:932/1770 train_time:89791ms step_avg:97.39ms
step:933/1770 train_time:89894ms step_avg:97.39ms
step:934/1770 train_time:89996ms step_avg:97.40ms
step:935/1770 train_time:90098ms step_avg:97.40ms
step:936/1770 train_time:90199ms step_avg:97.41ms
step:937/1770 train_time:90299ms step_avg:97.41ms
step:938/1770 train_time:90401ms step_avg:97.41ms
step:939/1770 train_time:90502ms step_avg:97.42ms
step:940/1770 train_time:90603ms step_avg:97.42ms
step:941/1770 train_time:90704ms step_avg:97.43ms
step:942/1770 train_time:90805ms step_avg:97.43ms
step:943/1770 train_time:90907ms step_avg:97.44ms
step:944/1770 train_time:91008ms step_avg:97.44ms
step:945/1770 train_time:91108ms step_avg:97.44ms
step:946/1770 train_time:91210ms step_avg:97.45ms
step:947/1770 train_time:91313ms step_avg:97.45ms
step:948/1770 train_time:91415ms step_avg:97.46ms
step:949/1770 train_time:91517ms step_avg:97.46ms
step:950/1770 train_time:91619ms step_avg:97.47ms
step:951/1770 train_time:91720ms step_avg:97.47ms
step:952/1770 train_time:91821ms step_avg:97.47ms
step:953/1770 train_time:91922ms step_avg:97.48ms
step:954/1770 train_time:92023ms step_avg:97.48ms
step:955/1770 train_time:92125ms step_avg:97.49ms
step:956/1770 train_time:92225ms step_avg:97.49ms
step:957/1770 train_time:92327ms step_avg:97.49ms
step:958/1770 train_time:92428ms step_avg:97.50ms
step:959/1770 train_time:92529ms step_avg:97.50ms
step:960/1770 train_time:92631ms step_avg:97.51ms
step:961/1770 train_time:92732ms step_avg:97.51ms
step:962/1770 train_time:92834ms step_avg:97.51ms
step:963/1770 train_time:92936ms step_avg:97.52ms
step:964/1770 train_time:93038ms step_avg:97.52ms
step:965/1770 train_time:93139ms step_avg:97.53ms
step:966/1770 train_time:93240ms step_avg:97.53ms
step:967/1770 train_time:93342ms step_avg:97.54ms
step:968/1770 train_time:93444ms step_avg:97.54ms
step:969/1770 train_time:93544ms step_avg:97.54ms
step:970/1770 train_time:93645ms step_avg:97.55ms
step:971/1770 train_time:93747ms step_avg:97.55ms
step:972/1770 train_time:93847ms step_avg:97.55ms
step:973/1770 train_time:93948ms step_avg:97.56ms
step:974/1770 train_time:94050ms step_avg:97.56ms
step:975/1770 train_time:94152ms step_avg:97.57ms
step:976/1770 train_time:94255ms step_avg:97.57ms
step:977/1770 train_time:94357ms step_avg:97.58ms
step:978/1770 train_time:94458ms step_avg:97.58ms
step:979/1770 train_time:94560ms step_avg:97.59ms
step:980/1770 train_time:94661ms step_avg:97.59ms
step:981/1770 train_time:94761ms step_avg:97.59ms
step:982/1770 train_time:94863ms step_avg:97.60ms
step:983/1770 train_time:94964ms step_avg:97.60ms
step:984/1770 train_time:95065ms step_avg:97.60ms
step:985/1770 train_time:95167ms step_avg:97.61ms
step:986/1770 train_time:95269ms step_avg:97.61ms
step:987/1770 train_time:95370ms step_avg:97.62ms
step:988/1770 train_time:95471ms step_avg:97.62ms
step:989/1770 train_time:95575ms step_avg:97.62ms
step:990/1770 train_time:95676ms step_avg:97.63ms
step:991/1770 train_time:95779ms step_avg:97.63ms
step:992/1770 train_time:95880ms step_avg:97.64ms
step:993/1770 train_time:95981ms step_avg:97.64ms
step:994/1770 train_time:96083ms step_avg:97.65ms
step:995/1770 train_time:96184ms step_avg:97.65ms
step:996/1770 train_time:96284ms step_avg:97.65ms
step:997/1770 train_time:96386ms step_avg:97.66ms
step:998/1770 train_time:96486ms step_avg:97.66ms
step:999/1770 train_time:96587ms step_avg:97.66ms
step:1000/1770 train_time:96690ms step_avg:97.67ms
step:1000/1770 val_loss:3.5142 train_time:96790ms step_avg:97.77ms
step:1001/1770 train_time:96812ms step_avg:97.69ms
step:1002/1770 train_time:96901ms step_avg:97.68ms
step:1003/1770 train_time:97004ms step_avg:97.69ms
step:1004/1770 train_time:97106ms step_avg:97.69ms
step:1005/1770 train_time:97206ms step_avg:97.69ms
step:1006/1770 train_time:97307ms step_avg:97.70ms
step:1007/1770 train_time:97408ms step_avg:97.70ms
step:1008/1770 train_time:97508ms step_avg:97.70ms
step:1009/1770 train_time:97609ms step_avg:97.71ms
step:1010/1770 train_time:97710ms step_avg:97.71ms
step:1011/1770 train_time:97812ms step_avg:97.71ms
step:1012/1770 train_time:97916ms step_avg:97.72ms
step:1013/1770 train_time:98018ms step_avg:97.73ms
step:1014/1770 train_time:98120ms step_avg:97.73ms
step:1015/1770 train_time:98222ms step_avg:97.73ms
step:1016/1770 train_time:98323ms step_avg:97.74ms
step:1017/1770 train_time:98424ms step_avg:97.74ms
step:1018/1770 train_time:98525ms step_avg:97.74ms
step:1019/1770 train_time:98626ms step_avg:97.75ms
step:1020/1770 train_time:98727ms step_avg:97.75ms
step:1021/1770 train_time:98828ms step_avg:97.75ms
step:1022/1770 train_time:98931ms step_avg:97.76ms
step:1023/1770 train_time:99031ms step_avg:97.76ms
step:1024/1770 train_time:99134ms step_avg:97.77ms
step:1025/1770 train_time:99236ms step_avg:97.77ms
step:1026/1770 train_time:99338ms step_avg:97.77ms
step:1027/1770 train_time:99439ms step_avg:97.78ms
step:1028/1770 train_time:99540ms step_avg:97.78ms
step:1029/1770 train_time:99641ms step_avg:97.78ms
step:1030/1770 train_time:99742ms step_avg:97.79ms
step:1031/1770 train_time:99843ms step_avg:97.79ms
step:1032/1770 train_time:99944ms step_avg:97.79ms
step:1033/1770 train_time:100045ms step_avg:97.80ms
step:1034/1770 train_time:100147ms step_avg:97.80ms
step:1035/1770 train_time:100248ms step_avg:97.80ms
step:1036/1770 train_time:100351ms step_avg:97.81ms
step:1037/1770 train_time:100452ms step_avg:97.81ms
step:1038/1770 train_time:100554ms step_avg:97.81ms
step:1039/1770 train_time:100656ms step_avg:97.82ms
step:1040/1770 train_time:100757ms step_avg:97.82ms
step:1041/1770 train_time:100858ms step_avg:97.83ms
step:1042/1770 train_time:100959ms step_avg:97.83ms
step:1043/1770 train_time:101060ms step_avg:97.83ms
step:1044/1770 train_time:101160ms step_avg:97.83ms
step:1045/1770 train_time:101261ms step_avg:97.84ms
step:1046/1770 train_time:101362ms step_avg:97.84ms
step:1047/1770 train_time:101464ms step_avg:97.84ms
step:1048/1770 train_time:101565ms step_avg:97.85ms
step:1049/1770 train_time:101666ms step_avg:97.85ms
step:1050/1770 train_time:101767ms step_avg:97.85ms
step:1051/1770 train_time:101869ms step_avg:97.86ms
step:1052/1770 train_time:101970ms step_avg:97.86ms
step:1053/1770 train_time:102072ms step_avg:97.86ms
step:1054/1770 train_time:102174ms step_avg:97.87ms
step:1055/1770 train_time:102275ms step_avg:97.87ms
step:1056/1770 train_time:102377ms step_avg:97.87ms
step:1057/1770 train_time:102479ms step_avg:97.88ms
step:1058/1770 train_time:102580ms step_avg:97.88ms
step:1059/1770 train_time:102681ms step_avg:97.88ms
step:1060/1770 train_time:102783ms step_avg:97.89ms
step:1061/1770 train_time:102884ms step_avg:97.89ms
step:1062/1770 train_time:102986ms step_avg:97.90ms
step:1063/1770 train_time:103089ms step_avg:97.90ms
step:1064/1770 train_time:103191ms step_avg:97.90ms
step:1065/1770 train_time:103293ms step_avg:97.91ms
step:1066/1770 train_time:103394ms step_avg:97.91ms
step:1067/1770 train_time:103496ms step_avg:97.92ms
step:1068/1770 train_time:103598ms step_avg:97.92ms
step:1069/1770 train_time:103700ms step_avg:97.92ms
step:1070/1770 train_time:103801ms step_avg:97.93ms
step:1071/1770 train_time:103903ms step_avg:97.93ms
step:1072/1770 train_time:104004ms step_avg:97.93ms
step:1073/1770 train_time:104104ms step_avg:97.93ms
step:1074/1770 train_time:104205ms step_avg:97.94ms
step:1075/1770 train_time:104306ms step_avg:97.94ms
step:1076/1770 train_time:104410ms step_avg:97.95ms
step:1077/1770 train_time:104512ms step_avg:97.95ms
step:1078/1770 train_time:104614ms step_avg:97.95ms
step:1079/1770 train_time:104716ms step_avg:97.96ms
step:1080/1770 train_time:104817ms step_avg:97.96ms
step:1081/1770 train_time:104918ms step_avg:97.96ms
step:1082/1770 train_time:105020ms step_avg:97.97ms
step:1083/1770 train_time:105121ms step_avg:97.97ms
step:1084/1770 train_time:105222ms step_avg:97.97ms
step:1085/1770 train_time:105323ms step_avg:97.98ms
step:1086/1770 train_time:105425ms step_avg:97.98ms
step:1087/1770 train_time:105526ms step_avg:97.98ms
step:1088/1770 train_time:105626ms step_avg:97.98ms
step:1089/1770 train_time:105728ms step_avg:97.99ms
step:1090/1770 train_time:105830ms step_avg:97.99ms
step:1091/1770 train_time:105932ms step_avg:97.99ms
step:1092/1770 train_time:106034ms step_avg:98.00ms
step:1093/1770 train_time:106137ms step_avg:98.00ms
step:1094/1770 train_time:106239ms step_avg:98.01ms
step:1095/1770 train_time:106340ms step_avg:98.01ms
step:1096/1770 train_time:106441ms step_avg:98.01ms
step:1097/1770 train_time:106542ms step_avg:98.02ms
step:1098/1770 train_time:106643ms step_avg:98.02ms
step:1099/1770 train_time:106744ms step_avg:98.02ms
step:1100/1770 train_time:106846ms step_avg:98.02ms
step:1101/1770 train_time:106947ms step_avg:98.03ms
step:1102/1770 train_time:107048ms step_avg:98.03ms
step:1103/1770 train_time:107149ms step_avg:98.03ms
step:1104/1770 train_time:107252ms step_avg:98.04ms
step:1105/1770 train_time:107354ms step_avg:98.04ms
step:1106/1770 train_time:107457ms step_avg:98.04ms
step:1107/1770 train_time:107558ms step_avg:98.05ms
step:1108/1770 train_time:107659ms step_avg:98.05ms
step:1109/1770 train_time:107761ms step_avg:98.05ms
step:1110/1770 train_time:107861ms step_avg:98.06ms
step:1111/1770 train_time:107963ms step_avg:98.06ms
step:1112/1770 train_time:108065ms step_avg:98.06ms
step:1113/1770 train_time:108166ms step_avg:98.07ms
step:1114/1770 train_time:108268ms step_avg:98.07ms
step:1115/1770 train_time:108370ms step_avg:98.07ms
step:1116/1770 train_time:108472ms step_avg:98.08ms
step:1117/1770 train_time:108574ms step_avg:98.08ms
step:1118/1770 train_time:108676ms step_avg:98.08ms
step:1119/1770 train_time:108777ms step_avg:98.09ms
step:1120/1770 train_time:108879ms step_avg:98.09ms
step:1121/1770 train_time:108979ms step_avg:98.09ms
step:1122/1770 train_time:109081ms step_avg:98.09ms
step:1123/1770 train_time:109181ms step_avg:98.10ms
step:1124/1770 train_time:109283ms step_avg:98.10ms
step:1125/1770 train_time:109384ms step_avg:98.10ms
step:1125/1770 val_loss:3.4724 train_time:109483ms step_avg:98.19ms
step:1126/1770 train_time:109506ms step_avg:98.12ms
step:1127/1770 train_time:109597ms step_avg:98.12ms
step:1128/1770 train_time:109698ms step_avg:98.12ms
step:1129/1770 train_time:109799ms step_avg:98.12ms
step:1130/1770 train_time:109901ms step_avg:98.13ms
step:1131/1770 train_time:110002ms step_avg:98.13ms
step:1132/1770 train_time:110104ms step_avg:98.13ms
step:1133/1770 train_time:110205ms step_avg:98.13ms
step:1134/1770 train_time:110306ms step_avg:98.14ms
step:1135/1770 train_time:110408ms step_avg:98.14ms
step:1136/1770 train_time:110510ms step_avg:98.14ms
step:1137/1770 train_time:110612ms step_avg:98.15ms
step:1138/1770 train_time:110714ms step_avg:98.15ms
step:1139/1770 train_time:110816ms step_avg:98.15ms
step:1140/1770 train_time:110917ms step_avg:98.16ms
step:1141/1770 train_time:111018ms step_avg:98.16ms
step:1142/1770 train_time:111119ms step_avg:98.16ms
step:1143/1770 train_time:111221ms step_avg:98.16ms
step:1144/1770 train_time:111321ms step_avg:98.17ms
step:1145/1770 train_time:111423ms step_avg:98.17ms
step:1146/1770 train_time:111524ms step_avg:98.17ms
step:1147/1770 train_time:111626ms step_avg:98.18ms
step:1148/1770 train_time:111728ms step_avg:98.18ms
step:1149/1770 train_time:111830ms step_avg:98.18ms
step:1150/1770 train_time:111932ms step_avg:98.19ms
step:1151/1770 train_time:112035ms step_avg:98.19ms
step:1152/1770 train_time:112137ms step_avg:98.19ms
step:1153/1770 train_time:112238ms step_avg:98.20ms
step:1154/1770 train_time:112340ms step_avg:98.20ms
step:1155/1770 train_time:112441ms step_avg:98.20ms
step:1156/1770 train_time:112542ms step_avg:98.20ms
step:1157/1770 train_time:112645ms step_avg:98.21ms
step:1158/1770 train_time:112747ms step_avg:98.21ms
step:1159/1770 train_time:112849ms step_avg:98.21ms
step:1160/1770 train_time:112950ms step_avg:98.22ms
step:1161/1770 train_time:113052ms step_avg:98.22ms
step:1162/1770 train_time:113154ms step_avg:98.22ms
step:1163/1770 train_time:113256ms step_avg:98.23ms
step:1164/1770 train_time:113357ms step_avg:98.23ms
step:1165/1770 train_time:113459ms step_avg:98.23ms
step:1166/1770 train_time:113560ms step_avg:98.24ms
step:1167/1770 train_time:113661ms step_avg:98.24ms
step:1168/1770 train_time:113762ms step_avg:98.24ms
step:1169/1770 train_time:113863ms step_avg:98.24ms
step:1170/1770 train_time:113964ms step_avg:98.24ms
step:1171/1770 train_time:114066ms step_avg:98.25ms
step:1172/1770 train_time:114169ms step_avg:98.25ms
step:1173/1770 train_time:114271ms step_avg:98.26ms
step:1174/1770 train_time:114373ms step_avg:98.26ms
step:1175/1770 train_time:114475ms step_avg:98.26ms
step:1176/1770 train_time:114576ms step_avg:98.26ms
step:1177/1770 train_time:114677ms step_avg:98.27ms
step:1178/1770 train_time:114779ms step_avg:98.27ms
step:1179/1770 train_time:114880ms step_avg:98.27ms
step:1180/1770 train_time:114982ms step_avg:98.28ms
step:1181/1770 train_time:115083ms step_avg:98.28ms
step:1182/1770 train_time:115185ms step_avg:98.28ms
step:1183/1770 train_time:115287ms step_avg:98.28ms
step:1184/1770 train_time:115392ms step_avg:98.29ms
step:1185/1770 train_time:115494ms step_avg:98.29ms
step:1186/1770 train_time:115597ms step_avg:98.30ms
step:1187/1770 train_time:115701ms step_avg:98.30ms
step:1188/1770 train_time:115803ms step_avg:98.31ms
step:1189/1770 train_time:115905ms step_avg:98.31ms
step:1190/1770 train_time:116008ms step_avg:98.31ms
step:1191/1770 train_time:116112ms step_avg:98.32ms
step:1192/1770 train_time:116215ms step_avg:98.32ms
step:1193/1770 train_time:116318ms step_avg:98.32ms
step:1194/1770 train_time:116420ms step_avg:98.33ms
step:1195/1770 train_time:116523ms step_avg:98.33ms
step:1196/1770 train_time:116627ms step_avg:98.34ms
step:1197/1770 train_time:116729ms step_avg:98.34ms
step:1198/1770 train_time:116832ms step_avg:98.34ms
step:1199/1770 train_time:116935ms step_avg:98.35ms
step:1200/1770 train_time:117038ms step_avg:98.35ms
step:1201/1770 train_time:117141ms step_avg:98.36ms
step:1202/1770 train_time:117244ms step_avg:98.36ms
step:1203/1770 train_time:117346ms step_avg:98.36ms
step:1204/1770 train_time:117450ms step_avg:98.37ms
step:1205/1770 train_time:117553ms step_avg:98.37ms
step:1206/1770 train_time:117656ms step_avg:98.37ms
step:1207/1770 train_time:117758ms step_avg:98.38ms
step:1208/1770 train_time:117860ms step_avg:98.38ms
step:1209/1770 train_time:117963ms step_avg:98.38ms
step:1210/1770 train_time:118065ms step_avg:98.39ms
step:1211/1770 train_time:118168ms step_avg:98.39ms
step:1212/1770 train_time:118273ms step_avg:98.40ms
step:1213/1770 train_time:118375ms step_avg:98.40ms
step:1214/1770 train_time:118476ms step_avg:98.40ms
step:1215/1770 train_time:118579ms step_avg:98.41ms
step:1216/1770 train_time:118685ms step_avg:98.41ms
step:1217/1770 train_time:118787ms step_avg:98.42ms
step:1218/1770 train_time:118889ms step_avg:98.42ms
step:1219/1770 train_time:118992ms step_avg:98.42ms
step:1220/1770 train_time:119095ms step_avg:98.43ms
step:1221/1770 train_time:119197ms step_avg:98.43ms
step:1222/1770 train_time:119301ms step_avg:98.43ms
step:1223/1770 train_time:119403ms step_avg:98.44ms
step:1224/1770 train_time:119506ms step_avg:98.44ms
step:1225/1770 train_time:119609ms step_avg:98.44ms
step:1226/1770 train_time:119712ms step_avg:98.45ms
step:1227/1770 train_time:119817ms step_avg:98.45ms
step:1228/1770 train_time:119921ms step_avg:98.46ms
step:1229/1770 train_time:120024ms step_avg:98.46ms
step:1230/1770 train_time:120127ms step_avg:98.46ms
step:1231/1770 train_time:120230ms step_avg:98.47ms
step:1232/1770 train_time:120333ms step_avg:98.47ms
step:1233/1770 train_time:120436ms step_avg:98.48ms
step:1234/1770 train_time:120538ms step_avg:98.48ms
step:1235/1770 train_time:120641ms step_avg:98.48ms
step:1236/1770 train_time:120744ms step_avg:98.49ms
step:1237/1770 train_time:120847ms step_avg:98.49ms
step:1238/1770 train_time:120951ms step_avg:98.49ms
step:1239/1770 train_time:121054ms step_avg:98.50ms
step:1240/1770 train_time:121157ms step_avg:98.50ms
step:1241/1770 train_time:121261ms step_avg:98.51ms
step:1242/1770 train_time:121362ms step_avg:98.51ms
step:1243/1770 train_time:121465ms step_avg:98.51ms
step:1244/1770 train_time:121566ms step_avg:98.51ms
step:1245/1770 train_time:121669ms step_avg:98.52ms
step:1246/1770 train_time:121772ms step_avg:98.52ms
step:1247/1770 train_time:121876ms step_avg:98.53ms
step:1248/1770 train_time:121980ms step_avg:98.53ms
step:1249/1770 train_time:122083ms step_avg:98.53ms
step:1250/1770 train_time:122185ms step_avg:98.54ms
step:1250/1770 val_loss:3.4261 train_time:122287ms step_avg:98.62ms
step:1251/1770 train_time:122308ms step_avg:98.56ms
step:1252/1770 train_time:122397ms step_avg:98.55ms
step:1253/1770 train_time:122500ms step_avg:98.55ms
step:1254/1770 train_time:122603ms step_avg:98.56ms
step:1255/1770 train_time:122708ms step_avg:98.56ms
step:1256/1770 train_time:122810ms step_avg:98.56ms
step:1257/1770 train_time:122912ms step_avg:98.57ms
step:1258/1770 train_time:123016ms step_avg:98.57ms
step:1259/1770 train_time:123119ms step_avg:98.57ms
step:1260/1770 train_time:123221ms step_avg:98.58ms
step:1261/1770 train_time:123324ms step_avg:98.58ms
step:1262/1770 train_time:123427ms step_avg:98.58ms
step:1263/1770 train_time:123530ms step_avg:98.59ms
step:1264/1770 train_time:123633ms step_avg:98.59ms
step:1265/1770 train_time:123736ms step_avg:98.59ms
step:1266/1770 train_time:123840ms step_avg:98.60ms
step:1267/1770 train_time:123942ms step_avg:98.60ms
step:1268/1770 train_time:124046ms step_avg:98.61ms
step:1269/1770 train_time:124148ms step_avg:98.61ms
step:1270/1770 train_time:124251ms step_avg:98.61ms
step:1271/1770 train_time:124355ms step_avg:98.62ms
step:1272/1770 train_time:124458ms step_avg:98.62ms
step:1273/1770 train_time:124562ms step_avg:98.62ms
step:1274/1770 train_time:124664ms step_avg:98.63ms
step:1275/1770 train_time:124766ms step_avg:98.63ms
step:1276/1770 train_time:124869ms step_avg:98.63ms
step:1277/1770 train_time:124972ms step_avg:98.64ms
step:1278/1770 train_time:125076ms step_avg:98.64ms
step:1279/1770 train_time:125179ms step_avg:98.64ms
step:1280/1770 train_time:125284ms step_avg:98.65ms
step:1281/1770 train_time:125386ms step_avg:98.65ms
step:1282/1770 train_time:125489ms step_avg:98.65ms
step:1283/1770 train_time:125593ms step_avg:98.66ms
step:1284/1770 train_time:125697ms step_avg:98.66ms
step:1285/1770 train_time:125800ms step_avg:98.67ms
step:1286/1770 train_time:125904ms step_avg:98.67ms
step:1287/1770 train_time:126009ms step_avg:98.68ms
step:1288/1770 train_time:126112ms step_avg:98.68ms
step:1289/1770 train_time:126216ms step_avg:98.68ms
step:1290/1770 train_time:126317ms step_avg:98.69ms
step:1291/1770 train_time:126420ms step_avg:98.69ms
step:1292/1770 train_time:126522ms step_avg:98.69ms
step:1293/1770 train_time:126626ms step_avg:98.69ms
step:1294/1770 train_time:126728ms step_avg:98.70ms
step:1295/1770 train_time:126830ms step_avg:98.70ms
step:1296/1770 train_time:126933ms step_avg:98.70ms
step:1297/1770 train_time:127036ms step_avg:98.71ms
step:1298/1770 train_time:127139ms step_avg:98.71ms
step:1299/1770 train_time:127242ms step_avg:98.71ms
step:1300/1770 train_time:127344ms step_avg:98.72ms
step:1301/1770 train_time:127447ms step_avg:98.72ms
step:1302/1770 train_time:127550ms step_avg:98.72ms
step:1303/1770 train_time:127652ms step_avg:98.73ms
step:1304/1770 train_time:127755ms step_avg:98.73ms
step:1305/1770 train_time:127858ms step_avg:98.73ms
step:1306/1770 train_time:127960ms step_avg:98.73ms
step:1307/1770 train_time:128061ms step_avg:98.74ms
step:1308/1770 train_time:128164ms step_avg:98.74ms
step:1309/1770 train_time:128266ms step_avg:98.74ms
step:1310/1770 train_time:128369ms step_avg:98.75ms
step:1311/1770 train_time:128471ms step_avg:98.75ms
step:1312/1770 train_time:128575ms step_avg:98.75ms
step:1313/1770 train_time:128677ms step_avg:98.75ms
step:1314/1770 train_time:128780ms step_avg:98.76ms
step:1315/1770 train_time:128882ms step_avg:98.76ms
step:1316/1770 train_time:128986ms step_avg:98.76ms
step:1317/1770 train_time:129088ms step_avg:98.77ms
step:1318/1770 train_time:129194ms step_avg:98.77ms
step:1319/1770 train_time:129298ms step_avg:98.78ms
step:1320/1770 train_time:129400ms step_avg:98.78ms
step:1321/1770 train_time:129503ms step_avg:98.78ms
step:1322/1770 train_time:129605ms step_avg:98.78ms
step:1323/1770 train_time:129709ms step_avg:98.79ms
step:1324/1770 train_time:129813ms step_avg:98.79ms
step:1325/1770 train_time:129917ms step_avg:98.80ms
step:1326/1770 train_time:130019ms step_avg:98.80ms
step:1327/1770 train_time:130125ms step_avg:98.80ms
step:1328/1770 train_time:130227ms step_avg:98.81ms
step:1329/1770 train_time:130330ms step_avg:98.81ms
step:1330/1770 train_time:130432ms step_avg:98.81ms
step:1331/1770 train_time:130535ms step_avg:98.82ms
step:1332/1770 train_time:130639ms step_avg:98.82ms
step:1333/1770 train_time:130741ms step_avg:98.82ms
step:1334/1770 train_time:130843ms step_avg:98.82ms
step:1335/1770 train_time:130945ms step_avg:98.83ms
step:1336/1770 train_time:131048ms step_avg:98.83ms
step:1337/1770 train_time:131150ms step_avg:98.83ms
step:1338/1770 train_time:131252ms step_avg:98.83ms
step:1339/1770 train_time:131356ms step_avg:98.84ms
step:1340/1770 train_time:131460ms step_avg:98.84ms
step:1341/1770 train_time:131562ms step_avg:98.84ms
step:1342/1770 train_time:131666ms step_avg:98.85ms
step:1343/1770 train_time:131768ms step_avg:98.85ms
step:1344/1770 train_time:131872ms step_avg:98.85ms
step:1345/1770 train_time:131974ms step_avg:98.86ms
step:1346/1770 train_time:132077ms step_avg:98.86ms
step:1347/1770 train_time:132180ms step_avg:98.86ms
step:1348/1770 train_time:132285ms step_avg:98.87ms
step:1349/1770 train_time:132388ms step_avg:98.87ms
step:1350/1770 train_time:132490ms step_avg:98.87ms
step:1351/1770 train_time:132594ms step_avg:98.88ms
step:1352/1770 train_time:132697ms step_avg:98.88ms
step:1353/1770 train_time:132801ms step_avg:98.88ms
step:1354/1770 train_time:132903ms step_avg:98.89ms
step:1355/1770 train_time:133006ms step_avg:98.89ms
step:1356/1770 train_time:133108ms step_avg:98.89ms
step:1357/1770 train_time:133210ms step_avg:98.89ms
step:1358/1770 train_time:133314ms step_avg:98.90ms
step:1359/1770 train_time:133417ms step_avg:98.90ms
step:1360/1770 train_time:133521ms step_avg:98.90ms
step:1361/1770 train_time:133624ms step_avg:98.91ms
step:1362/1770 train_time:133727ms step_avg:98.91ms
step:1363/1770 train_time:133830ms step_avg:98.91ms
step:1364/1770 train_time:133935ms step_avg:98.92ms
step:1365/1770 train_time:134037ms step_avg:98.92ms
step:1366/1770 train_time:134139ms step_avg:98.92ms
step:1367/1770 train_time:134243ms step_avg:98.93ms
step:1368/1770 train_time:134345ms step_avg:98.93ms
step:1369/1770 train_time:134448ms step_avg:98.93ms
step:1370/1770 train_time:134551ms step_avg:98.93ms
step:1371/1770 train_time:134654ms step_avg:98.94ms
step:1372/1770 train_time:134757ms step_avg:98.94ms
step:1373/1770 train_time:134860ms step_avg:98.94ms
step:1374/1770 train_time:134964ms step_avg:98.95ms
step:1375/1770 train_time:135067ms step_avg:98.95ms
step:1375/1770 val_loss:3.3847 train_time:135169ms step_avg:99.02ms
step:1376/1770 train_time:135190ms step_avg:98.97ms
step:1377/1770 train_time:135278ms step_avg:98.96ms
step:1378/1770 train_time:135381ms step_avg:98.96ms
step:1379/1770 train_time:135484ms step_avg:98.97ms
step:1380/1770 train_time:135587ms step_avg:98.97ms
step:1381/1770 train_time:135691ms step_avg:98.97ms
step:1382/1770 train_time:135793ms step_avg:98.97ms
step:1383/1770 train_time:135897ms step_avg:98.98ms
step:1384/1770 train_time:136000ms step_avg:98.98ms
step:1385/1770 train_time:136103ms step_avg:98.98ms
step:1386/1770 train_time:136207ms step_avg:98.99ms
step:1387/1770 train_time:136310ms step_avg:98.99ms
step:1388/1770 train_time:136412ms step_avg:98.99ms
step:1389/1770 train_time:136515ms step_avg:99.00ms
step:1390/1770 train_time:136618ms step_avg:99.00ms
step:1391/1770 train_time:136721ms step_avg:99.00ms
step:1392/1770 train_time:136824ms step_avg:99.00ms
step:1393/1770 train_time:136927ms step_avg:99.01ms
step:1394/1770 train_time:137030ms step_avg:99.01ms
step:1395/1770 train_time:137133ms step_avg:99.01ms
step:1396/1770 train_time:137238ms step_avg:99.02ms
step:1397/1770 train_time:137341ms step_avg:99.02ms
step:1398/1770 train_time:137444ms step_avg:99.02ms
step:1399/1770 train_time:137546ms step_avg:99.03ms
step:1400/1770 train_time:137649ms step_avg:99.03ms
step:1401/1770 train_time:137752ms step_avg:99.03ms
step:1402/1770 train_time:137855ms step_avg:99.03ms
step:1403/1770 train_time:137959ms step_avg:99.04ms
step:1404/1770 train_time:138062ms step_avg:99.04ms
step:1405/1770 train_time:138164ms step_avg:99.04ms
step:1406/1770 train_time:138267ms step_avg:99.05ms
step:1407/1770 train_time:138369ms step_avg:99.05ms
step:1408/1770 train_time:138472ms step_avg:99.05ms
step:1409/1770 train_time:138575ms step_avg:99.05ms
step:1410/1770 train_time:138677ms step_avg:99.06ms
step:1411/1770 train_time:138781ms step_avg:99.06ms
step:1412/1770 train_time:138883ms step_avg:99.06ms
step:1413/1770 train_time:138986ms step_avg:99.06ms
step:1414/1770 train_time:139089ms step_avg:99.07ms
step:1415/1770 train_time:139192ms step_avg:99.07ms
step:1416/1770 train_time:139297ms step_avg:99.07ms
step:1417/1770 train_time:139400ms step_avg:99.08ms
step:1418/1770 train_time:139503ms step_avg:99.08ms
step:1419/1770 train_time:139606ms step_avg:99.08ms
step:1420/1770 train_time:139708ms step_avg:99.08ms
step:1421/1770 train_time:139810ms step_avg:99.09ms
step:1422/1770 train_time:139912ms step_avg:99.09ms
step:1423/1770 train_time:140015ms step_avg:99.09ms
step:1424/1770 train_time:140119ms step_avg:99.09ms
step:1425/1770 train_time:140222ms step_avg:99.10ms
step:1426/1770 train_time:140325ms step_avg:99.10ms
step:1427/1770 train_time:140427ms step_avg:99.10ms
step:1428/1770 train_time:140531ms step_avg:99.11ms
step:1429/1770 train_time:140635ms step_avg:99.11ms
step:1430/1770 train_time:140737ms step_avg:99.11ms
step:1431/1770 train_time:140841ms step_avg:99.11ms
step:1432/1770 train_time:140943ms step_avg:99.12ms
step:1433/1770 train_time:141046ms step_avg:99.12ms
step:1434/1770 train_time:141148ms step_avg:99.12ms
step:1435/1770 train_time:141251ms step_avg:99.12ms
step:1436/1770 train_time:141355ms step_avg:99.13ms
step:1437/1770 train_time:141458ms step_avg:99.13ms
step:1438/1770 train_time:141561ms step_avg:99.13ms
step:1439/1770 train_time:141663ms step_avg:99.13ms
step:1440/1770 train_time:141766ms step_avg:99.14ms
step:1441/1770 train_time:141871ms step_avg:99.14ms
step:1442/1770 train_time:141974ms step_avg:99.14ms
step:1443/1770 train_time:142077ms step_avg:99.15ms
step:1444/1770 train_time:142180ms step_avg:99.15ms
step:1445/1770 train_time:142283ms step_avg:99.15ms
step:1446/1770 train_time:142387ms step_avg:99.16ms
step:1447/1770 train_time:142491ms step_avg:99.16ms
step:1448/1770 train_time:142595ms step_avg:99.16ms
step:1449/1770 train_time:142700ms step_avg:99.17ms
step:1450/1770 train_time:142803ms step_avg:99.17ms
step:1451/1770 train_time:142907ms step_avg:99.17ms
step:1452/1770 train_time:143011ms step_avg:99.18ms
step:1453/1770 train_time:143114ms step_avg:99.18ms
step:1454/1770 train_time:143218ms step_avg:99.18ms
step:1455/1770 train_time:143323ms step_avg:99.19ms
step:1456/1770 train_time:143429ms step_avg:99.19ms
step:1457/1770 train_time:143532ms step_avg:99.19ms
step:1458/1770 train_time:143637ms step_avg:99.20ms
step:1459/1770 train_time:143743ms step_avg:99.20ms
step:1460/1770 train_time:143847ms step_avg:99.20ms
step:1461/1770 train_time:143951ms step_avg:99.21ms
step:1462/1770 train_time:144054ms step_avg:99.21ms
step:1463/1770 train_time:144158ms step_avg:99.21ms
step:1464/1770 train_time:144264ms step_avg:99.22ms
step:1465/1770 train_time:144367ms step_avg:99.22ms
step:1466/1770 train_time:144472ms step_avg:99.23ms
step:1467/1770 train_time:144577ms step_avg:99.23ms
step:1468/1770 train_time:144681ms step_avg:99.23ms
step:1469/1770 train_time:144785ms step_avg:99.24ms
step:1470/1770 train_time:144888ms step_avg:99.24ms
step:1471/1770 train_time:144992ms step_avg:99.24ms
step:1472/1770 train_time:145095ms step_avg:99.24ms
step:1473/1770 train_time:145200ms step_avg:99.25ms
step:1474/1770 train_time:145306ms step_avg:99.25ms
step:1475/1770 train_time:145410ms step_avg:99.26ms
step:1476/1770 train_time:145513ms step_avg:99.26ms
step:1477/1770 train_time:145620ms step_avg:99.26ms
step:1478/1770 train_time:145725ms step_avg:99.27ms
step:1479/1770 train_time:145828ms step_avg:99.27ms
step:1480/1770 train_time:145931ms step_avg:99.27ms
step:1481/1770 train_time:146040ms step_avg:99.28ms
step:1482/1770 train_time:146144ms step_avg:99.28ms
step:1483/1770 train_time:146248ms step_avg:99.29ms
step:1484/1770 train_time:146350ms step_avg:99.29ms
step:1485/1770 train_time:146454ms step_avg:99.29ms
step:1486/1770 train_time:146557ms step_avg:99.29ms
step:1487/1770 train_time:146662ms step_avg:99.30ms
step:1488/1770 train_time:146767ms step_avg:99.30ms
step:1489/1770 train_time:146871ms step_avg:99.30ms
step:1490/1770 train_time:146975ms step_avg:99.31ms
step:1491/1770 train_time:147078ms step_avg:99.31ms
step:1492/1770 train_time:147183ms step_avg:99.31ms
step:1493/1770 train_time:147290ms step_avg:99.32ms
step:1494/1770 train_time:147397ms step_avg:99.32ms
step:1495/1770 train_time:147500ms step_avg:99.33ms
step:1496/1770 train_time:147604ms step_avg:99.33ms
step:1497/1770 train_time:147708ms step_avg:99.33ms
step:1498/1770 train_time:147811ms step_avg:99.34ms
step:1499/1770 train_time:147914ms step_avg:99.34ms
step:1500/1770 train_time:148018ms step_avg:99.34ms
step:1500/1770 val_loss:3.3489 train_time:148120ms step_avg:99.41ms
step:1501/1770 train_time:148142ms step_avg:99.36ms
step:1502/1770 train_time:148232ms step_avg:99.35ms
step:1503/1770 train_time:148335ms step_avg:99.35ms
step:1504/1770 train_time:148439ms step_avg:99.36ms
step:1505/1770 train_time:148545ms step_avg:99.36ms
step:1506/1770 train_time:148650ms step_avg:99.36ms
step:1507/1770 train_time:148754ms step_avg:99.37ms
step:1508/1770 train_time:148860ms step_avg:99.37ms
step:1509/1770 train_time:148964ms step_avg:99.38ms
step:1510/1770 train_time:149068ms step_avg:99.38ms
step:1511/1770 train_time:149173ms step_avg:99.38ms
step:1512/1770 train_time:149278ms step_avg:99.39ms
step:1513/1770 train_time:149383ms step_avg:99.39ms
step:1514/1770 train_time:149486ms step_avg:99.39ms
step:1515/1770 train_time:149590ms step_avg:99.40ms
step:1516/1770 train_time:149695ms step_avg:99.40ms
step:1517/1770 train_time:149799ms step_avg:99.40ms
step:1518/1770 train_time:149906ms step_avg:99.41ms
step:1519/1770 train_time:150009ms step_avg:99.41ms
step:1520/1770 train_time:150115ms step_avg:99.41ms
step:1521/1770 train_time:150218ms step_avg:99.42ms
step:1522/1770 train_time:150323ms step_avg:99.42ms
step:1523/1770 train_time:150427ms step_avg:99.42ms
step:1524/1770 train_time:150531ms step_avg:99.43ms
step:1525/1770 train_time:150634ms step_avg:99.43ms
step:1526/1770 train_time:150738ms step_avg:99.43ms
step:1527/1770 train_time:150844ms step_avg:99.44ms
step:1528/1770 train_time:150950ms step_avg:99.44ms
step:1529/1770 train_time:151053ms step_avg:99.44ms
step:1530/1770 train_time:151157ms step_avg:99.45ms
step:1531/1770 train_time:151261ms step_avg:99.45ms
step:1532/1770 train_time:151366ms step_avg:99.45ms
step:1533/1770 train_time:151470ms step_avg:99.46ms
step:1534/1770 train_time:151575ms step_avg:99.46ms
step:1535/1770 train_time:151678ms step_avg:99.46ms
step:1536/1770 train_time:151784ms step_avg:99.47ms
step:1537/1770 train_time:151887ms step_avg:99.47ms
step:1538/1770 train_time:151993ms step_avg:99.47ms
step:1539/1770 train_time:152096ms step_avg:99.47ms
step:1540/1770 train_time:152203ms step_avg:99.48ms
step:1541/1770 train_time:152308ms step_avg:99.48ms
step:1542/1770 train_time:152412ms step_avg:99.49ms
step:1543/1770 train_time:152515ms step_avg:99.49ms
step:1544/1770 train_time:152622ms step_avg:99.49ms
step:1545/1770 train_time:152726ms step_avg:99.50ms
step:1546/1770 train_time:152829ms step_avg:99.50ms
step:1547/1770 train_time:152933ms step_avg:99.50ms
step:1548/1770 train_time:153036ms step_avg:99.50ms
step:1549/1770 train_time:153141ms step_avg:99.51ms
step:1550/1770 train_time:153245ms step_avg:99.51ms
step:1551/1770 train_time:153348ms step_avg:99.51ms
step:1552/1770 train_time:153453ms step_avg:99.52ms
step:1553/1770 train_time:153557ms step_avg:99.52ms
step:1554/1770 train_time:153661ms step_avg:99.52ms
step:1555/1770 train_time:153766ms step_avg:99.52ms
step:1556/1770 train_time:153869ms step_avg:99.53ms
step:1557/1770 train_time:153972ms step_avg:99.53ms
step:1558/1770 train_time:154076ms step_avg:99.53ms
step:1559/1770 train_time:154180ms step_avg:99.54ms
step:1560/1770 train_time:154283ms step_avg:99.54ms
step:1561/1770 train_time:154389ms step_avg:99.54ms
step:1562/1770 train_time:154493ms step_avg:99.54ms
step:1563/1770 train_time:154596ms step_avg:99.55ms
step:1564/1770 train_time:154700ms step_avg:99.55ms
step:1565/1770 train_time:154804ms step_avg:99.55ms
step:1566/1770 train_time:154908ms step_avg:99.56ms
step:1567/1770 train_time:155013ms step_avg:99.56ms
step:1568/1770 train_time:155116ms step_avg:99.56ms
step:1569/1770 train_time:155223ms step_avg:99.57ms
step:1570/1770 train_time:155327ms step_avg:99.57ms
step:1571/1770 train_time:155430ms step_avg:99.57ms
step:1572/1770 train_time:155535ms step_avg:99.57ms
step:1573/1770 train_time:155641ms step_avg:99.58ms
step:1574/1770 train_time:155745ms step_avg:99.58ms
step:1575/1770 train_time:155848ms step_avg:99.58ms
step:1576/1770 train_time:155952ms step_avg:99.59ms
step:1577/1770 train_time:156059ms step_avg:99.59ms
step:1578/1770 train_time:156165ms step_avg:99.60ms
step:1579/1770 train_time:156268ms step_avg:99.60ms
step:1580/1770 train_time:156372ms step_avg:99.60ms
step:1581/1770 train_time:156479ms step_avg:99.60ms
step:1582/1770 train_time:156585ms step_avg:99.61ms
step:1583/1770 train_time:156689ms step_avg:99.61ms
step:1584/1770 train_time:156794ms step_avg:99.61ms
step:1585/1770 train_time:156898ms step_avg:99.62ms
step:1586/1770 train_time:157006ms step_avg:99.62ms
step:1587/1770 train_time:157111ms step_avg:99.63ms
step:1588/1770 train_time:157215ms step_avg:99.63ms
step:1589/1770 train_time:157322ms step_avg:99.63ms
step:1590/1770 train_time:157425ms step_avg:99.64ms
step:1591/1770 train_time:157528ms step_avg:99.64ms
step:1592/1770 train_time:157633ms step_avg:99.64ms
step:1593/1770 train_time:157737ms step_avg:99.64ms
step:1594/1770 train_time:157841ms step_avg:99.65ms
step:1595/1770 train_time:157944ms step_avg:99.65ms
step:1596/1770 train_time:158050ms step_avg:99.65ms
step:1597/1770 train_time:158153ms step_avg:99.66ms
step:1598/1770 train_time:158256ms step_avg:99.66ms
step:1599/1770 train_time:158363ms step_avg:99.66ms
step:1600/1770 train_time:158469ms step_avg:99.67ms
step:1601/1770 train_time:158573ms step_avg:99.67ms
step:1602/1770 train_time:158678ms step_avg:99.67ms
step:1603/1770 train_time:158783ms step_avg:99.68ms
step:1604/1770 train_time:158886ms step_avg:99.68ms
step:1605/1770 train_time:158989ms step_avg:99.68ms
step:1606/1770 train_time:159093ms step_avg:99.68ms
step:1607/1770 train_time:159201ms step_avg:99.69ms
step:1608/1770 train_time:159304ms step_avg:99.69ms
step:1609/1770 train_time:159408ms step_avg:99.69ms
step:1610/1770 train_time:159514ms step_avg:99.70ms
step:1611/1770 train_time:159620ms step_avg:99.70ms
step:1612/1770 train_time:159725ms step_avg:99.70ms
step:1613/1770 train_time:159829ms step_avg:99.71ms
step:1614/1770 train_time:159932ms step_avg:99.71ms
step:1615/1770 train_time:160037ms step_avg:99.71ms
step:1616/1770 train_time:160141ms step_avg:99.71ms
step:1617/1770 train_time:160247ms step_avg:99.72ms
step:1618/1770 train_time:160352ms step_avg:99.72ms
step:1619/1770 train_time:160457ms step_avg:99.72ms
step:1620/1770 train_time:160561ms step_avg:99.73ms
step:1621/1770 train_time:160665ms step_avg:99.73ms
step:1622/1770 train_time:160770ms step_avg:99.73ms
step:1623/1770 train_time:160876ms step_avg:99.74ms
step:1624/1770 train_time:160979ms step_avg:99.74ms
step:1625/1770 train_time:161083ms step_avg:99.74ms
step:1625/1770 val_loss:3.3170 train_time:161186ms step_avg:99.81ms
step:1626/1770 train_time:161209ms step_avg:99.76ms
step:1627/1770 train_time:161296ms step_avg:99.75ms
step:1628/1770 train_time:161400ms step_avg:99.75ms
step:1629/1770 train_time:161504ms step_avg:99.76ms
step:1630/1770 train_time:161608ms step_avg:99.76ms
step:1631/1770 train_time:161712ms step_avg:99.76ms
step:1632/1770 train_time:161815ms step_avg:99.76ms
step:1633/1770 train_time:161920ms step_avg:99.77ms
step:1634/1770 train_time:162024ms step_avg:99.77ms
step:1635/1770 train_time:162127ms step_avg:99.77ms
step:1636/1770 train_time:162231ms step_avg:99.77ms
step:1637/1770 train_time:162336ms step_avg:99.78ms
step:1638/1770 train_time:162440ms step_avg:99.78ms
step:1639/1770 train_time:162544ms step_avg:99.78ms
step:1640/1770 train_time:162649ms step_avg:99.78ms
step:1641/1770 train_time:162753ms step_avg:99.79ms
step:1642/1770 train_time:162856ms step_avg:99.79ms
step:1643/1770 train_time:162961ms step_avg:99.79ms
step:1644/1770 train_time:163066ms step_avg:99.80ms
step:1645/1770 train_time:163170ms step_avg:99.80ms
step:1646/1770 train_time:163275ms step_avg:99.80ms
step:1647/1770 train_time:163380ms step_avg:99.80ms
step:1648/1770 train_time:163483ms step_avg:99.81ms
step:1649/1770 train_time:163587ms step_avg:99.81ms
step:1650/1770 train_time:163691ms step_avg:99.81ms
step:1651/1770 train_time:163795ms step_avg:99.81ms
step:1652/1770 train_time:163898ms step_avg:99.82ms
step:1653/1770 train_time:164003ms step_avg:99.82ms
step:1654/1770 train_time:164110ms step_avg:99.82ms
step:1655/1770 train_time:164217ms step_avg:99.83ms
step:1656/1770 train_time:164320ms step_avg:99.83ms
step:1657/1770 train_time:164426ms step_avg:99.83ms
step:1658/1770 train_time:164530ms step_avg:99.84ms
step:1659/1770 train_time:164636ms step_avg:99.84ms
step:1660/1770 train_time:164740ms step_avg:99.84ms
step:1661/1770 train_time:164845ms step_avg:99.85ms
step:1662/1770 train_time:164949ms step_avg:99.85ms
step:1663/1770 train_time:165052ms step_avg:99.85ms
step:1664/1770 train_time:165157ms step_avg:99.85ms
step:1665/1770 train_time:165260ms step_avg:99.86ms
step:1666/1770 train_time:165364ms step_avg:99.86ms
step:1667/1770 train_time:165468ms step_avg:99.86ms
step:1668/1770 train_time:165571ms step_avg:99.86ms
step:1669/1770 train_time:165675ms step_avg:99.86ms
step:1670/1770 train_time:165779ms step_avg:99.87ms
step:1671/1770 train_time:165884ms step_avg:99.87ms
step:1672/1770 train_time:165988ms step_avg:99.87ms
step:1673/1770 train_time:166094ms step_avg:99.88ms
step:1674/1770 train_time:166197ms step_avg:99.88ms
step:1675/1770 train_time:166301ms step_avg:99.88ms
step:1676/1770 train_time:166407ms step_avg:99.88ms
step:1677/1770 train_time:166515ms step_avg:99.89ms
step:1678/1770 train_time:166619ms step_avg:99.89ms
step:1679/1770 train_time:166723ms step_avg:99.89ms
step:1680/1770 train_time:166827ms step_avg:99.90ms
step:1681/1770 train_time:166932ms step_avg:99.90ms
step:1682/1770 train_time:167039ms step_avg:99.90ms
step:1683/1770 train_time:167142ms step_avg:99.91ms
step:1684/1770 train_time:167245ms step_avg:99.91ms
step:1685/1770 train_time:167349ms step_avg:99.91ms
step:1686/1770 train_time:167454ms step_avg:99.91ms
step:1687/1770 train_time:167559ms step_avg:99.92ms
step:1688/1770 train_time:167663ms step_avg:99.92ms
step:1689/1770 train_time:167768ms step_avg:99.92ms
step:1690/1770 train_time:167871ms step_avg:99.92ms
step:1691/1770 train_time:167976ms step_avg:99.93ms
step:1692/1770 train_time:168080ms step_avg:99.93ms
step:1693/1770 train_time:168185ms step_avg:99.93ms
step:1694/1770 train_time:168289ms step_avg:99.93ms
step:1695/1770 train_time:168394ms step_avg:99.94ms
step:1696/1770 train_time:168500ms step_avg:99.94ms
step:1697/1770 train_time:168607ms step_avg:99.94ms
step:1698/1770 train_time:168711ms step_avg:99.95ms
step:1699/1770 train_time:168815ms step_avg:99.95ms
step:1700/1770 train_time:168919ms step_avg:99.95ms
step:1701/1770 train_time:169022ms step_avg:99.95ms
step:1702/1770 train_time:169127ms step_avg:99.96ms
step:1703/1770 train_time:169231ms step_avg:99.96ms
step:1704/1770 train_time:169335ms step_avg:99.96ms
step:1705/1770 train_time:169438ms step_avg:99.96ms
step:1706/1770 train_time:169541ms step_avg:99.97ms
step:1707/1770 train_time:169646ms step_avg:99.97ms
step:1708/1770 train_time:169750ms step_avg:99.97ms
step:1709/1770 train_time:169857ms step_avg:99.97ms
step:1710/1770 train_time:169964ms step_avg:99.98ms
step:1711/1770 train_time:170070ms step_avg:99.98ms
step:1712/1770 train_time:170175ms step_avg:99.99ms
step:1713/1770 train_time:170280ms step_avg:99.99ms
step:1714/1770 train_time:170385ms step_avg:99.99ms
step:1715/1770 train_time:170489ms step_avg:99.99ms
step:1716/1770 train_time:170595ms step_avg:100.00ms
step:1717/1770 train_time:170700ms step_avg:100.00ms
step:1718/1770 train_time:170806ms step_avg:100.00ms
step:1719/1770 train_time:170912ms step_avg:100.01ms
step:1720/1770 train_time:171018ms step_avg:100.01ms
step:1721/1770 train_time:171122ms step_avg:100.01ms
step:1722/1770 train_time:171230ms step_avg:100.02ms
step:1723/1770 train_time:171337ms step_avg:100.02ms
step:1724/1770 train_time:171443ms step_avg:100.03ms
step:1725/1770 train_time:171551ms step_avg:100.03ms
step:1726/1770 train_time:171658ms step_avg:100.03ms
step:1727/1770 train_time:171762ms step_avg:100.04ms
step:1728/1770 train_time:171869ms step_avg:100.04ms
step:1729/1770 train_time:171974ms step_avg:100.04ms
step:1730/1770 train_time:172080ms step_avg:100.05ms
step:1731/1770 train_time:172186ms step_avg:100.05ms
step:1732/1770 train_time:172290ms step_avg:100.05ms
step:1733/1770 train_time:172397ms step_avg:100.06ms
step:1734/1770 train_time:172501ms step_avg:100.06ms
step:1735/1770 train_time:172607ms step_avg:100.06ms
step:1736/1770 train_time:172711ms step_avg:100.06ms
step:1737/1770 train_time:172816ms step_avg:100.07ms
step:1738/1770 train_time:172921ms step_avg:100.07ms
step:1739/1770 train_time:173026ms step_avg:100.07ms
step:1740/1770 train_time:173130ms step_avg:100.08ms
step:1741/1770 train_time:173238ms step_avg:100.08ms
step:1742/1770 train_time:173346ms step_avg:100.08ms
step:1743/1770 train_time:173452ms step_avg:100.09ms
step:1744/1770 train_time:173557ms step_avg:100.09ms
step:1745/1770 train_time:173661ms step_avg:100.09ms
step:1746/1770 train_time:173769ms step_avg:100.10ms
step:1747/1770 train_time:173874ms step_avg:100.10ms
step:1748/1770 train_time:173981ms step_avg:100.10ms
step:1749/1770 train_time:174087ms step_avg:100.11ms
step:1750/1770 train_time:174192ms step_avg:100.11ms
step:1750/1770 val_loss:3.2924 train_time:174296ms step_avg:100.17ms
step:1751/1770 train_time:174317ms step_avg:100.12ms
step:1752/1770 train_time:174406ms step_avg:100.12ms
step:1753/1770 train_time:174510ms step_avg:100.12ms
step:1754/1770 train_time:174616ms step_avg:100.12ms
step:1755/1770 train_time:174721ms step_avg:100.13ms
step:1756/1770 train_time:174826ms step_avg:100.13ms
step:1757/1770 train_time:174931ms step_avg:100.13ms
step:1758/1770 train_time:175036ms step_avg:100.14ms
step:1759/1770 train_time:175141ms step_avg:100.14ms
step:1760/1770 train_time:175247ms step_avg:100.14ms
step:1761/1770 train_time:175355ms step_avg:100.15ms
step:1762/1770 train_time:175463ms step_avg:100.15ms
step:1763/1770 train_time:175567ms step_avg:100.15ms
step:1764/1770 train_time:175673ms step_avg:100.16ms
step:1765/1770 train_time:175778ms step_avg:100.16ms
step:1766/1770 train_time:175887ms step_avg:100.16ms
step:1767/1770 train_time:175991ms step_avg:100.17ms
step:1768/1770 train_time:176096ms step_avg:100.17ms
step:1769/1770 train_time:176200ms step_avg:100.17ms
step:1770/1770 train_time:176304ms step_avg:100.17ms
step:1770/1770 val_loss:3.2895 train_time:176410ms step_avg:100.23ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
