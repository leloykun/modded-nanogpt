import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 03:50:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24235ms step_avg:nanms
step:2/1770 train_time:24644ms step_avg:nanms
step:3/1770 train_time:24740ms step_avg:nanms
step:4/1770 train_time:24833ms step_avg:nanms
step:5/1770 train_time:24928ms step_avg:nanms
step:6/1770 train_time:25022ms step_avg:nanms
step:7/1770 train_time:25116ms step_avg:nanms
step:8/1770 train_time:25211ms step_avg:nanms
step:9/1770 train_time:25305ms step_avg:nanms
step:10/1770 train_time:25399ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.52ms
step:14/1770 train_time:378ms step_avg:94.52ms
step:15/1770 train_time:473ms step_avg:94.63ms
step:16/1770 train_time:568ms step_avg:94.73ms
step:17/1770 train_time:663ms step_avg:94.70ms
step:18/1770 train_time:757ms step_avg:94.64ms
step:19/1770 train_time:852ms step_avg:94.68ms
step:20/1770 train_time:947ms step_avg:94.68ms
step:21/1770 train_time:1043ms step_avg:94.79ms
step:22/1770 train_time:1136ms step_avg:94.67ms
step:23/1770 train_time:1231ms step_avg:94.66ms
step:24/1770 train_time:1325ms step_avg:94.68ms
step:25/1770 train_time:1420ms step_avg:94.66ms
step:26/1770 train_time:1515ms step_avg:94.69ms
step:27/1770 train_time:1610ms step_avg:94.68ms
step:28/1770 train_time:1704ms step_avg:94.68ms
step:29/1770 train_time:1799ms step_avg:94.67ms
step:30/1770 train_time:1894ms step_avg:94.69ms
step:31/1770 train_time:1989ms step_avg:94.70ms
step:32/1770 train_time:2084ms step_avg:94.71ms
step:33/1770 train_time:2178ms step_avg:94.70ms
step:34/1770 train_time:2273ms step_avg:94.70ms
step:35/1770 train_time:2367ms step_avg:94.69ms
step:36/1770 train_time:2461ms step_avg:94.67ms
step:37/1770 train_time:2556ms step_avg:94.68ms
step:38/1770 train_time:2651ms step_avg:94.68ms
step:39/1770 train_time:2746ms step_avg:94.70ms
step:40/1770 train_time:2841ms step_avg:94.70ms
step:41/1770 train_time:2935ms step_avg:94.69ms
step:42/1770 train_time:3030ms step_avg:94.69ms
step:43/1770 train_time:3125ms step_avg:94.70ms
step:44/1770 train_time:3220ms step_avg:94.70ms
step:45/1770 train_time:3314ms step_avg:94.69ms
step:46/1770 train_time:3409ms step_avg:94.70ms
step:47/1770 train_time:3504ms step_avg:94.72ms
step:48/1770 train_time:3599ms step_avg:94.71ms
step:49/1770 train_time:3694ms step_avg:94.72ms
step:50/1770 train_time:3788ms step_avg:94.71ms
step:51/1770 train_time:3883ms step_avg:94.70ms
step:52/1770 train_time:3978ms step_avg:94.71ms
step:53/1770 train_time:4073ms step_avg:94.72ms
step:54/1770 train_time:4168ms step_avg:94.72ms
step:55/1770 train_time:4262ms step_avg:94.71ms
step:56/1770 train_time:4357ms step_avg:94.72ms
step:57/1770 train_time:4452ms step_avg:94.73ms
step:58/1770 train_time:4547ms step_avg:94.72ms
step:59/1770 train_time:4641ms step_avg:94.72ms
step:60/1770 train_time:4736ms step_avg:94.71ms
step:61/1770 train_time:4830ms step_avg:94.71ms
step:62/1770 train_time:4925ms step_avg:94.72ms
step:63/1770 train_time:5020ms step_avg:94.72ms
step:64/1770 train_time:5115ms step_avg:94.73ms
step:65/1770 train_time:5210ms step_avg:94.73ms
step:66/1770 train_time:5305ms step_avg:94.74ms
step:67/1770 train_time:5400ms step_avg:94.73ms
step:68/1770 train_time:5495ms step_avg:94.74ms
step:69/1770 train_time:5589ms step_avg:94.74ms
step:70/1770 train_time:5684ms step_avg:94.73ms
step:71/1770 train_time:5778ms step_avg:94.72ms
step:72/1770 train_time:5873ms step_avg:94.72ms
step:73/1770 train_time:5967ms step_avg:94.72ms
step:74/1770 train_time:6062ms step_avg:94.71ms
step:75/1770 train_time:6156ms step_avg:94.71ms
step:76/1770 train_time:6251ms step_avg:94.72ms
step:77/1770 train_time:6346ms step_avg:94.72ms
step:78/1770 train_time:6441ms step_avg:94.71ms
step:79/1770 train_time:6535ms step_avg:94.71ms
step:80/1770 train_time:6630ms step_avg:94.71ms
step:81/1770 train_time:6725ms step_avg:94.72ms
step:82/1770 train_time:6819ms step_avg:94.71ms
step:83/1770 train_time:6914ms step_avg:94.71ms
step:84/1770 train_time:7009ms step_avg:94.71ms
step:85/1770 train_time:7103ms step_avg:94.71ms
step:86/1770 train_time:7200ms step_avg:94.73ms
step:87/1770 train_time:7293ms step_avg:94.71ms
step:88/1770 train_time:7387ms step_avg:94.71ms
step:89/1770 train_time:7481ms step_avg:94.70ms
step:90/1770 train_time:7576ms step_avg:94.70ms
step:91/1770 train_time:7671ms step_avg:94.70ms
step:92/1770 train_time:7766ms step_avg:94.70ms
step:93/1770 train_time:7860ms step_avg:94.70ms
step:94/1770 train_time:7955ms step_avg:94.70ms
step:95/1770 train_time:8050ms step_avg:94.71ms
step:96/1770 train_time:8144ms step_avg:94.70ms
step:97/1770 train_time:8239ms step_avg:94.70ms
step:98/1770 train_time:8333ms step_avg:94.70ms
step:99/1770 train_time:8428ms step_avg:94.70ms
step:100/1770 train_time:8523ms step_avg:94.70ms
step:101/1770 train_time:8618ms step_avg:94.70ms
step:102/1770 train_time:8713ms step_avg:94.70ms
step:103/1770 train_time:8808ms step_avg:94.71ms
step:104/1770 train_time:8907ms step_avg:94.75ms
step:105/1770 train_time:8997ms step_avg:94.70ms
step:106/1770 train_time:9091ms step_avg:94.70ms
step:107/1770 train_time:9186ms step_avg:94.70ms
step:108/1770 train_time:9280ms step_avg:94.69ms
step:109/1770 train_time:9375ms step_avg:94.70ms
step:110/1770 train_time:9470ms step_avg:94.70ms
step:111/1770 train_time:9565ms step_avg:94.70ms
step:112/1770 train_time:9659ms step_avg:94.70ms
step:113/1770 train_time:9754ms step_avg:94.70ms
step:114/1770 train_time:9849ms step_avg:94.70ms
step:115/1770 train_time:9943ms step_avg:94.69ms
step:116/1770 train_time:10038ms step_avg:94.70ms
step:117/1770 train_time:10133ms step_avg:94.70ms
step:118/1770 train_time:10227ms step_avg:94.70ms
step:119/1770 train_time:10323ms step_avg:94.70ms
step:120/1770 train_time:10417ms step_avg:94.70ms
step:121/1770 train_time:10512ms step_avg:94.70ms
step:122/1770 train_time:10607ms step_avg:94.70ms
step:123/1770 train_time:10701ms step_avg:94.70ms
step:124/1770 train_time:10796ms step_avg:94.70ms
step:125/1770 train_time:10891ms step_avg:94.70ms
step:125/1770 val_loss:4.6451 train_time:10984ms step_avg:95.51ms
step:126/1770 train_time:11021ms step_avg:95.01ms
step:127/1770 train_time:11085ms step_avg:94.75ms
step:128/1770 train_time:11184ms step_avg:94.78ms
step:129/1770 train_time:11281ms step_avg:94.80ms
step:130/1770 train_time:11376ms step_avg:94.80ms
step:131/1770 train_time:11470ms step_avg:94.79ms
step:132/1770 train_time:11565ms step_avg:94.79ms
step:133/1770 train_time:11660ms step_avg:94.80ms
step:134/1770 train_time:11755ms step_avg:94.80ms
step:135/1770 train_time:11850ms step_avg:94.80ms
step:136/1770 train_time:11945ms step_avg:94.80ms
step:137/1770 train_time:12041ms step_avg:94.81ms
step:138/1770 train_time:12136ms step_avg:94.81ms
step:139/1770 train_time:12231ms step_avg:94.81ms
step:140/1770 train_time:12327ms step_avg:94.82ms
step:141/1770 train_time:12422ms step_avg:94.82ms
step:142/1770 train_time:12517ms step_avg:94.83ms
step:143/1770 train_time:12612ms step_avg:94.83ms
step:144/1770 train_time:12709ms step_avg:94.85ms
step:145/1770 train_time:12802ms step_avg:94.83ms
step:146/1770 train_time:12897ms step_avg:94.83ms
step:147/1770 train_time:12993ms step_avg:94.84ms
step:148/1770 train_time:13088ms step_avg:94.84ms
step:149/1770 train_time:13183ms step_avg:94.84ms
step:150/1770 train_time:13278ms step_avg:94.84ms
step:151/1770 train_time:13373ms step_avg:94.85ms
step:152/1770 train_time:13469ms step_avg:94.85ms
step:153/1770 train_time:13564ms step_avg:94.86ms
step:154/1770 train_time:13660ms step_avg:94.86ms
step:155/1770 train_time:13755ms step_avg:94.86ms
step:156/1770 train_time:13850ms step_avg:94.86ms
step:157/1770 train_time:13946ms step_avg:94.87ms
step:158/1770 train_time:14041ms step_avg:94.87ms
step:159/1770 train_time:14136ms step_avg:94.87ms
step:160/1770 train_time:14231ms step_avg:94.87ms
step:161/1770 train_time:14326ms step_avg:94.88ms
step:162/1770 train_time:14422ms step_avg:94.88ms
step:163/1770 train_time:14517ms step_avg:94.88ms
step:164/1770 train_time:14611ms step_avg:94.88ms
step:165/1770 train_time:14706ms step_avg:94.88ms
step:166/1770 train_time:14801ms step_avg:94.88ms
step:167/1770 train_time:14897ms step_avg:94.89ms
step:168/1770 train_time:14992ms step_avg:94.89ms
step:169/1770 train_time:15087ms step_avg:94.89ms
step:170/1770 train_time:15183ms step_avg:94.89ms
step:171/1770 train_time:15278ms step_avg:94.89ms
step:172/1770 train_time:15374ms step_avg:94.90ms
step:173/1770 train_time:15468ms step_avg:94.90ms
step:174/1770 train_time:15564ms step_avg:94.90ms
step:175/1770 train_time:15659ms step_avg:94.90ms
step:176/1770 train_time:15754ms step_avg:94.90ms
step:177/1770 train_time:15849ms step_avg:94.91ms
step:178/1770 train_time:15944ms step_avg:94.91ms
step:179/1770 train_time:16040ms step_avg:94.91ms
step:180/1770 train_time:16135ms step_avg:94.91ms
step:181/1770 train_time:16229ms step_avg:94.91ms
step:182/1770 train_time:16326ms step_avg:94.92ms
step:183/1770 train_time:16420ms step_avg:94.91ms
step:184/1770 train_time:16515ms step_avg:94.91ms
step:185/1770 train_time:16610ms step_avg:94.91ms
step:186/1770 train_time:16705ms step_avg:94.91ms
step:187/1770 train_time:16802ms step_avg:94.92ms
step:188/1770 train_time:16898ms step_avg:94.93ms
step:189/1770 train_time:16993ms step_avg:94.93ms
step:190/1770 train_time:17088ms step_avg:94.93ms
step:191/1770 train_time:17183ms step_avg:94.93ms
step:192/1770 train_time:17279ms step_avg:94.94ms
step:193/1770 train_time:17374ms step_avg:94.94ms
step:194/1770 train_time:17469ms step_avg:94.94ms
step:195/1770 train_time:17564ms step_avg:94.94ms
step:196/1770 train_time:17659ms step_avg:94.94ms
step:197/1770 train_time:17754ms step_avg:94.94ms
step:198/1770 train_time:17848ms step_avg:94.94ms
step:199/1770 train_time:17944ms step_avg:94.94ms
step:200/1770 train_time:18041ms step_avg:94.95ms
step:201/1770 train_time:18135ms step_avg:94.95ms
step:202/1770 train_time:18229ms step_avg:94.95ms
step:203/1770 train_time:18324ms step_avg:94.95ms
step:204/1770 train_time:18421ms step_avg:94.95ms
step:205/1770 train_time:18516ms step_avg:94.95ms
step:206/1770 train_time:18612ms step_avg:94.96ms
step:207/1770 train_time:18706ms step_avg:94.95ms
step:208/1770 train_time:18801ms step_avg:94.96ms
step:209/1770 train_time:18897ms step_avg:94.96ms
step:210/1770 train_time:18992ms step_avg:94.96ms
step:211/1770 train_time:19087ms step_avg:94.96ms
step:212/1770 train_time:19184ms step_avg:94.97ms
step:213/1770 train_time:19279ms step_avg:94.97ms
step:214/1770 train_time:19375ms step_avg:94.98ms
step:215/1770 train_time:19470ms step_avg:94.97ms
step:216/1770 train_time:19565ms step_avg:94.97ms
step:217/1770 train_time:19662ms step_avg:94.98ms
step:218/1770 train_time:19757ms step_avg:94.99ms
step:219/1770 train_time:19852ms step_avg:94.98ms
step:220/1770 train_time:19947ms step_avg:94.99ms
step:221/1770 train_time:20042ms step_avg:94.99ms
step:222/1770 train_time:20138ms step_avg:94.99ms
step:223/1770 train_time:20233ms step_avg:94.99ms
step:224/1770 train_time:20328ms step_avg:94.99ms
step:225/1770 train_time:20423ms step_avg:94.99ms
step:226/1770 train_time:20523ms step_avg:95.01ms
step:227/1770 train_time:20614ms step_avg:95.00ms
step:228/1770 train_time:20709ms step_avg:94.99ms
step:229/1770 train_time:20804ms step_avg:95.00ms
step:230/1770 train_time:20899ms step_avg:95.00ms
step:231/1770 train_time:20995ms step_avg:95.00ms
step:232/1770 train_time:21090ms step_avg:95.00ms
step:233/1770 train_time:21185ms step_avg:95.00ms
step:234/1770 train_time:21280ms step_avg:95.00ms
step:235/1770 train_time:21375ms step_avg:95.00ms
step:236/1770 train_time:21470ms step_avg:95.00ms
step:237/1770 train_time:21566ms step_avg:95.00ms
step:238/1770 train_time:21662ms step_avg:95.01ms
step:239/1770 train_time:21757ms step_avg:95.01ms
step:240/1770 train_time:21852ms step_avg:95.01ms
step:241/1770 train_time:21947ms step_avg:95.01ms
step:242/1770 train_time:22043ms step_avg:95.01ms
step:243/1770 train_time:22138ms step_avg:95.01ms
step:244/1770 train_time:22233ms step_avg:95.01ms
step:245/1770 train_time:22328ms step_avg:95.01ms
step:246/1770 train_time:22423ms step_avg:95.01ms
step:247/1770 train_time:22519ms step_avg:95.02ms
step:248/1770 train_time:22614ms step_avg:95.02ms
step:249/1770 train_time:22709ms step_avg:95.02ms
step:250/1770 train_time:22805ms step_avg:95.02ms
step:250/1770 val_loss:4.1155 train_time:22898ms step_avg:95.41ms
step:251/1770 train_time:22926ms step_avg:95.13ms
step:252/1770 train_time:22999ms step_avg:95.04ms
step:253/1770 train_time:23095ms step_avg:95.04ms
step:254/1770 train_time:23191ms step_avg:95.04ms
step:255/1770 train_time:23286ms step_avg:95.04ms
step:256/1770 train_time:23381ms step_avg:95.04ms
step:257/1770 train_time:23476ms step_avg:95.04ms
step:258/1770 train_time:23572ms step_avg:95.05ms
step:259/1770 train_time:23667ms step_avg:95.05ms
step:260/1770 train_time:23761ms step_avg:95.04ms
step:261/1770 train_time:23856ms step_avg:95.04ms
step:262/1770 train_time:23952ms step_avg:95.05ms
step:263/1770 train_time:24048ms step_avg:95.05ms
step:264/1770 train_time:24143ms step_avg:95.05ms
step:265/1770 train_time:24239ms step_avg:95.05ms
step:266/1770 train_time:24335ms step_avg:95.06ms
step:267/1770 train_time:24430ms step_avg:95.06ms
step:268/1770 train_time:24526ms step_avg:95.06ms
step:269/1770 train_time:24621ms step_avg:95.06ms
step:270/1770 train_time:24717ms step_avg:95.07ms
step:271/1770 train_time:24813ms step_avg:95.07ms
step:272/1770 train_time:24909ms step_avg:95.07ms
step:273/1770 train_time:25006ms step_avg:95.08ms
step:274/1770 train_time:25101ms step_avg:95.08ms
step:275/1770 train_time:25197ms step_avg:95.08ms
step:276/1770 train_time:25293ms step_avg:95.09ms
step:277/1770 train_time:25390ms step_avg:95.09ms
step:278/1770 train_time:25486ms step_avg:95.10ms
step:279/1770 train_time:25582ms step_avg:95.10ms
step:280/1770 train_time:25678ms step_avg:95.10ms
step:281/1770 train_time:25774ms step_avg:95.11ms
step:282/1770 train_time:25870ms step_avg:95.11ms
step:283/1770 train_time:25967ms step_avg:95.12ms
step:284/1770 train_time:26063ms step_avg:95.12ms
step:285/1770 train_time:26158ms step_avg:95.12ms
step:286/1770 train_time:26254ms step_avg:95.12ms
step:287/1770 train_time:26350ms step_avg:95.13ms
step:288/1770 train_time:26446ms step_avg:95.13ms
step:289/1770 train_time:26541ms step_avg:95.13ms
step:290/1770 train_time:26636ms step_avg:95.13ms
step:291/1770 train_time:26732ms step_avg:95.13ms
step:292/1770 train_time:26828ms step_avg:95.13ms
step:293/1770 train_time:26924ms step_avg:95.14ms
step:294/1770 train_time:27019ms step_avg:95.14ms
step:295/1770 train_time:27116ms step_avg:95.14ms
step:296/1770 train_time:27213ms step_avg:95.15ms
step:297/1770 train_time:27309ms step_avg:95.15ms
step:298/1770 train_time:27406ms step_avg:95.16ms
step:299/1770 train_time:27502ms step_avg:95.16ms
step:300/1770 train_time:27598ms step_avg:95.16ms
step:301/1770 train_time:27694ms step_avg:95.17ms
step:302/1770 train_time:27789ms step_avg:95.17ms
step:303/1770 train_time:27885ms step_avg:95.17ms
step:304/1770 train_time:27980ms step_avg:95.17ms
step:305/1770 train_time:28076ms step_avg:95.17ms
step:306/1770 train_time:28171ms step_avg:95.17ms
step:307/1770 train_time:28267ms step_avg:95.18ms
step:308/1770 train_time:28363ms step_avg:95.18ms
step:309/1770 train_time:28459ms step_avg:95.18ms
step:310/1770 train_time:28555ms step_avg:95.18ms
step:311/1770 train_time:28651ms step_avg:95.18ms
step:312/1770 train_time:28746ms step_avg:95.19ms
step:313/1770 train_time:28845ms step_avg:95.20ms
step:314/1770 train_time:28938ms step_avg:95.19ms
step:315/1770 train_time:29034ms step_avg:95.19ms
step:316/1770 train_time:29130ms step_avg:95.20ms
step:317/1770 train_time:29225ms step_avg:95.20ms
step:318/1770 train_time:29321ms step_avg:95.20ms
step:319/1770 train_time:29417ms step_avg:95.20ms
step:320/1770 train_time:29513ms step_avg:95.20ms
step:321/1770 train_time:29609ms step_avg:95.21ms
step:322/1770 train_time:29705ms step_avg:95.21ms
step:323/1770 train_time:29800ms step_avg:95.21ms
step:324/1770 train_time:29897ms step_avg:95.21ms
step:325/1770 train_time:29993ms step_avg:95.21ms
step:326/1770 train_time:30088ms step_avg:95.22ms
step:327/1770 train_time:30185ms step_avg:95.22ms
step:328/1770 train_time:30279ms step_avg:95.22ms
step:329/1770 train_time:30376ms step_avg:95.22ms
step:330/1770 train_time:30472ms step_avg:95.22ms
step:331/1770 train_time:30567ms step_avg:95.22ms
step:332/1770 train_time:30662ms step_avg:95.22ms
step:333/1770 train_time:30758ms step_avg:95.23ms
step:334/1770 train_time:30854ms step_avg:95.23ms
step:335/1770 train_time:30951ms step_avg:95.23ms
step:336/1770 train_time:31046ms step_avg:95.23ms
step:337/1770 train_time:31141ms step_avg:95.23ms
step:338/1770 train_time:31237ms step_avg:95.23ms
step:339/1770 train_time:31333ms step_avg:95.24ms
step:340/1770 train_time:31428ms step_avg:95.24ms
step:341/1770 train_time:31524ms step_avg:95.24ms
step:342/1770 train_time:31619ms step_avg:95.24ms
step:343/1770 train_time:31715ms step_avg:95.24ms
step:344/1770 train_time:31810ms step_avg:95.24ms
step:345/1770 train_time:31906ms step_avg:95.24ms
step:346/1770 train_time:32001ms step_avg:95.24ms
step:347/1770 train_time:32097ms step_avg:95.24ms
step:348/1770 train_time:32193ms step_avg:95.25ms
step:349/1770 train_time:32289ms step_avg:95.25ms
step:350/1770 train_time:32385ms step_avg:95.25ms
step:351/1770 train_time:32481ms step_avg:95.25ms
step:352/1770 train_time:32576ms step_avg:95.25ms
step:353/1770 train_time:32672ms step_avg:95.25ms
step:354/1770 train_time:32768ms step_avg:95.26ms
step:355/1770 train_time:32863ms step_avg:95.26ms
step:356/1770 train_time:32958ms step_avg:95.26ms
step:357/1770 train_time:33054ms step_avg:95.26ms
step:358/1770 train_time:33150ms step_avg:95.26ms
step:359/1770 train_time:33247ms step_avg:95.26ms
step:360/1770 train_time:33342ms step_avg:95.26ms
step:361/1770 train_time:33437ms step_avg:95.26ms
step:362/1770 train_time:33533ms step_avg:95.27ms
step:363/1770 train_time:33629ms step_avg:95.27ms
step:364/1770 train_time:33725ms step_avg:95.27ms
step:365/1770 train_time:33820ms step_avg:95.27ms
step:366/1770 train_time:33916ms step_avg:95.27ms
step:367/1770 train_time:34012ms step_avg:95.27ms
step:368/1770 train_time:34108ms step_avg:95.27ms
step:369/1770 train_time:34204ms step_avg:95.27ms
step:370/1770 train_time:34299ms step_avg:95.28ms
step:371/1770 train_time:34395ms step_avg:95.28ms
step:372/1770 train_time:34491ms step_avg:95.28ms
step:373/1770 train_time:34587ms step_avg:95.28ms
step:374/1770 train_time:34682ms step_avg:95.28ms
step:375/1770 train_time:34778ms step_avg:95.28ms
step:375/1770 val_loss:3.9153 train_time:34872ms step_avg:95.54ms
step:376/1770 train_time:34892ms step_avg:95.33ms
step:377/1770 train_time:34978ms step_avg:95.31ms
step:378/1770 train_time:35076ms step_avg:95.32ms
step:379/1770 train_time:35172ms step_avg:95.32ms
step:380/1770 train_time:35268ms step_avg:95.32ms
step:381/1770 train_time:35364ms step_avg:95.32ms
step:382/1770 train_time:35459ms step_avg:95.32ms
step:383/1770 train_time:35555ms step_avg:95.32ms
step:384/1770 train_time:35650ms step_avg:95.32ms
step:385/1770 train_time:35746ms step_avg:95.32ms
step:386/1770 train_time:35842ms step_avg:95.32ms
step:387/1770 train_time:35937ms step_avg:95.32ms
step:388/1770 train_time:36032ms step_avg:95.32ms
step:389/1770 train_time:36128ms step_avg:95.32ms
step:390/1770 train_time:36224ms step_avg:95.33ms
step:391/1770 train_time:36320ms step_avg:95.33ms
step:392/1770 train_time:36415ms step_avg:95.33ms
step:393/1770 train_time:36511ms step_avg:95.33ms
step:394/1770 train_time:36608ms step_avg:95.33ms
step:395/1770 train_time:36704ms step_avg:95.33ms
step:396/1770 train_time:36801ms step_avg:95.34ms
step:397/1770 train_time:36899ms step_avg:95.35ms
step:398/1770 train_time:36996ms step_avg:95.35ms
step:399/1770 train_time:37093ms step_avg:95.36ms
step:400/1770 train_time:37191ms step_avg:95.36ms
step:401/1770 train_time:37289ms step_avg:95.37ms
step:402/1770 train_time:37387ms step_avg:95.37ms
step:403/1770 train_time:37484ms step_avg:95.38ms
step:404/1770 train_time:37582ms step_avg:95.39ms
step:405/1770 train_time:37679ms step_avg:95.39ms
step:406/1770 train_time:37776ms step_avg:95.40ms
step:407/1770 train_time:37874ms step_avg:95.40ms
step:408/1770 train_time:37972ms step_avg:95.41ms
step:409/1770 train_time:38070ms step_avg:95.41ms
step:410/1770 train_time:38167ms step_avg:95.42ms
step:411/1770 train_time:38265ms step_avg:95.42ms
step:412/1770 train_time:38363ms step_avg:95.43ms
step:413/1770 train_time:38461ms step_avg:95.44ms
step:414/1770 train_time:38559ms step_avg:95.44ms
step:415/1770 train_time:38656ms step_avg:95.45ms
step:416/1770 train_time:38753ms step_avg:95.45ms
step:417/1770 train_time:38851ms step_avg:95.46ms
step:418/1770 train_time:38949ms step_avg:95.46ms
step:419/1770 train_time:39046ms step_avg:95.47ms
step:420/1770 train_time:39144ms step_avg:95.47ms
step:421/1770 train_time:39242ms step_avg:95.48ms
step:422/1770 train_time:39339ms step_avg:95.48ms
step:423/1770 train_time:39436ms step_avg:95.49ms
step:424/1770 train_time:39534ms step_avg:95.49ms
step:425/1770 train_time:39631ms step_avg:95.50ms
step:426/1770 train_time:39729ms step_avg:95.50ms
step:427/1770 train_time:39827ms step_avg:95.51ms
step:428/1770 train_time:39925ms step_avg:95.51ms
step:429/1770 train_time:40022ms step_avg:95.52ms
step:430/1770 train_time:40119ms step_avg:95.52ms
step:431/1770 train_time:40217ms step_avg:95.53ms
step:432/1770 train_time:40314ms step_avg:95.53ms
step:433/1770 train_time:40412ms step_avg:95.54ms
step:434/1770 train_time:40509ms step_avg:95.54ms
step:435/1770 train_time:40607ms step_avg:95.55ms
step:436/1770 train_time:40705ms step_avg:95.55ms
step:437/1770 train_time:40802ms step_avg:95.56ms
step:438/1770 train_time:40900ms step_avg:95.56ms
step:439/1770 train_time:40997ms step_avg:95.56ms
step:440/1770 train_time:41095ms step_avg:95.57ms
step:441/1770 train_time:41192ms step_avg:95.57ms
step:442/1770 train_time:41290ms step_avg:95.58ms
step:443/1770 train_time:41388ms step_avg:95.58ms
step:444/1770 train_time:41485ms step_avg:95.59ms
step:445/1770 train_time:41583ms step_avg:95.59ms
step:446/1770 train_time:41681ms step_avg:95.60ms
step:447/1770 train_time:41778ms step_avg:95.60ms
step:448/1770 train_time:41875ms step_avg:95.61ms
step:449/1770 train_time:41973ms step_avg:95.61ms
step:450/1770 train_time:42071ms step_avg:95.62ms
step:451/1770 train_time:42168ms step_avg:95.62ms
step:452/1770 train_time:42266ms step_avg:95.62ms
step:453/1770 train_time:42363ms step_avg:95.63ms
step:454/1770 train_time:42461ms step_avg:95.63ms
step:455/1770 train_time:42559ms step_avg:95.64ms
step:456/1770 train_time:42656ms step_avg:95.64ms
step:457/1770 train_time:42753ms step_avg:95.64ms
step:458/1770 train_time:42851ms step_avg:95.65ms
step:459/1770 train_time:42948ms step_avg:95.65ms
step:460/1770 train_time:43046ms step_avg:95.66ms
step:461/1770 train_time:43143ms step_avg:95.66ms
step:462/1770 train_time:43241ms step_avg:95.67ms
step:463/1770 train_time:43338ms step_avg:95.67ms
step:464/1770 train_time:43437ms step_avg:95.68ms
step:465/1770 train_time:43533ms step_avg:95.68ms
step:466/1770 train_time:43630ms step_avg:95.68ms
step:467/1770 train_time:43728ms step_avg:95.69ms
step:468/1770 train_time:43827ms step_avg:95.69ms
step:469/1770 train_time:43924ms step_avg:95.70ms
step:470/1770 train_time:44022ms step_avg:95.70ms
step:471/1770 train_time:44120ms step_avg:95.70ms
step:472/1770 train_time:44217ms step_avg:95.71ms
step:473/1770 train_time:44314ms step_avg:95.71ms
step:474/1770 train_time:44412ms step_avg:95.71ms
step:475/1770 train_time:44509ms step_avg:95.72ms
step:476/1770 train_time:44607ms step_avg:95.72ms
step:477/1770 train_time:44706ms step_avg:95.73ms
step:478/1770 train_time:44804ms step_avg:95.74ms
step:479/1770 train_time:44902ms step_avg:95.74ms
step:480/1770 train_time:44999ms step_avg:95.74ms
step:481/1770 train_time:45097ms step_avg:95.75ms
step:482/1770 train_time:45194ms step_avg:95.75ms
step:483/1770 train_time:45292ms step_avg:95.75ms
step:484/1770 train_time:45389ms step_avg:95.76ms
step:485/1770 train_time:45487ms step_avg:95.76ms
step:486/1770 train_time:45585ms step_avg:95.77ms
step:487/1770 train_time:45683ms step_avg:95.77ms
step:488/1770 train_time:45780ms step_avg:95.77ms
step:489/1770 train_time:45877ms step_avg:95.78ms
step:490/1770 train_time:45975ms step_avg:95.78ms
step:491/1770 train_time:46072ms step_avg:95.78ms
step:492/1770 train_time:46170ms step_avg:95.79ms
step:493/1770 train_time:46267ms step_avg:95.79ms
step:494/1770 train_time:46365ms step_avg:95.80ms
step:495/1770 train_time:46463ms step_avg:95.80ms
step:496/1770 train_time:46561ms step_avg:95.80ms
step:497/1770 train_time:46658ms step_avg:95.81ms
step:498/1770 train_time:46755ms step_avg:95.81ms
step:499/1770 train_time:46854ms step_avg:95.81ms
step:500/1770 train_time:46955ms step_avg:95.83ms
step:500/1770 val_loss:3.7593 train_time:47047ms step_avg:96.01ms
step:501/1770 train_time:47070ms step_avg:95.87ms
step:502/1770 train_time:47154ms step_avg:95.84ms
step:503/1770 train_time:47253ms step_avg:95.85ms
step:504/1770 train_time:47351ms step_avg:95.85ms
step:505/1770 train_time:47448ms step_avg:95.86ms
step:506/1770 train_time:47546ms step_avg:95.86ms
step:507/1770 train_time:47643ms step_avg:95.86ms
step:508/1770 train_time:47741ms step_avg:95.87ms
step:509/1770 train_time:47839ms step_avg:95.87ms
step:510/1770 train_time:47937ms step_avg:95.87ms
step:511/1770 train_time:48034ms step_avg:95.88ms
step:512/1770 train_time:48132ms step_avg:95.88ms
step:513/1770 train_time:48229ms step_avg:95.88ms
step:514/1770 train_time:48326ms step_avg:95.89ms
step:515/1770 train_time:48424ms step_avg:95.89ms
step:516/1770 train_time:48522ms step_avg:95.89ms
step:517/1770 train_time:48619ms step_avg:95.90ms
step:518/1770 train_time:48717ms step_avg:95.90ms
step:519/1770 train_time:48814ms step_avg:95.90ms
step:520/1770 train_time:48912ms step_avg:95.91ms
step:521/1770 train_time:49009ms step_avg:95.91ms
step:522/1770 train_time:49106ms step_avg:95.91ms
step:523/1770 train_time:49204ms step_avg:95.91ms
step:524/1770 train_time:49302ms step_avg:95.92ms
step:525/1770 train_time:49400ms step_avg:95.92ms
step:526/1770 train_time:49497ms step_avg:95.92ms
step:527/1770 train_time:49595ms step_avg:95.93ms
step:528/1770 train_time:49692ms step_avg:95.93ms
step:529/1770 train_time:49790ms step_avg:95.93ms
step:530/1770 train_time:49887ms step_avg:95.94ms
step:531/1770 train_time:49985ms step_avg:95.94ms
step:532/1770 train_time:50083ms step_avg:95.94ms
step:533/1770 train_time:50182ms step_avg:95.95ms
step:534/1770 train_time:50280ms step_avg:95.95ms
step:535/1770 train_time:50379ms step_avg:95.96ms
step:536/1770 train_time:50477ms step_avg:95.96ms
step:537/1770 train_time:50574ms step_avg:95.97ms
step:538/1770 train_time:50673ms step_avg:95.97ms
step:539/1770 train_time:50771ms step_avg:95.98ms
step:540/1770 train_time:50869ms step_avg:95.98ms
step:541/1770 train_time:50967ms step_avg:95.98ms
step:542/1770 train_time:51064ms step_avg:95.99ms
step:543/1770 train_time:51163ms step_avg:95.99ms
step:544/1770 train_time:51261ms step_avg:95.99ms
step:545/1770 train_time:51359ms step_avg:96.00ms
step:546/1770 train_time:51457ms step_avg:96.00ms
step:547/1770 train_time:51555ms step_avg:96.01ms
step:548/1770 train_time:51653ms step_avg:96.01ms
step:549/1770 train_time:51751ms step_avg:96.01ms
step:550/1770 train_time:51848ms step_avg:96.02ms
step:551/1770 train_time:51946ms step_avg:96.02ms
step:552/1770 train_time:52044ms step_avg:96.02ms
step:553/1770 train_time:52143ms step_avg:96.03ms
step:554/1770 train_time:52241ms step_avg:96.03ms
step:555/1770 train_time:52339ms step_avg:96.04ms
step:556/1770 train_time:52437ms step_avg:96.04ms
step:557/1770 train_time:52535ms step_avg:96.04ms
step:558/1770 train_time:52633ms step_avg:96.05ms
step:559/1770 train_time:52730ms step_avg:96.05ms
step:560/1770 train_time:52828ms step_avg:96.05ms
step:561/1770 train_time:52926ms step_avg:96.05ms
step:562/1770 train_time:53024ms step_avg:96.06ms
step:563/1770 train_time:53122ms step_avg:96.06ms
step:564/1770 train_time:53220ms step_avg:96.07ms
step:565/1770 train_time:53318ms step_avg:96.07ms
step:566/1770 train_time:53416ms step_avg:96.07ms
step:567/1770 train_time:53514ms step_avg:96.08ms
step:568/1770 train_time:53612ms step_avg:96.08ms
step:569/1770 train_time:53709ms step_avg:96.08ms
step:570/1770 train_time:53807ms step_avg:96.08ms
step:571/1770 train_time:53906ms step_avg:96.09ms
step:572/1770 train_time:54003ms step_avg:96.09ms
step:573/1770 train_time:54101ms step_avg:96.09ms
step:574/1770 train_time:54200ms step_avg:96.10ms
step:575/1770 train_time:54298ms step_avg:96.10ms
step:576/1770 train_time:54396ms step_avg:96.11ms
step:577/1770 train_time:54494ms step_avg:96.11ms
step:578/1770 train_time:54592ms step_avg:96.11ms
step:579/1770 train_time:54691ms step_avg:96.12ms
step:580/1770 train_time:54789ms step_avg:96.12ms
step:581/1770 train_time:54887ms step_avg:96.12ms
step:582/1770 train_time:54984ms step_avg:96.13ms
step:583/1770 train_time:55083ms step_avg:96.13ms
step:584/1770 train_time:55180ms step_avg:96.13ms
step:585/1770 train_time:55278ms step_avg:96.14ms
step:586/1770 train_time:55376ms step_avg:96.14ms
step:587/1770 train_time:55473ms step_avg:96.14ms
step:588/1770 train_time:55571ms step_avg:96.14ms
step:589/1770 train_time:55668ms step_avg:96.15ms
step:590/1770 train_time:55766ms step_avg:96.15ms
step:591/1770 train_time:55864ms step_avg:96.15ms
step:592/1770 train_time:55963ms step_avg:96.16ms
step:593/1770 train_time:56061ms step_avg:96.16ms
step:594/1770 train_time:56158ms step_avg:96.16ms
step:595/1770 train_time:56257ms step_avg:96.17ms
step:596/1770 train_time:56355ms step_avg:96.17ms
step:597/1770 train_time:56453ms step_avg:96.17ms
step:598/1770 train_time:56551ms step_avg:96.17ms
step:599/1770 train_time:56648ms step_avg:96.18ms
step:600/1770 train_time:56745ms step_avg:96.18ms
step:601/1770 train_time:56844ms step_avg:96.18ms
step:602/1770 train_time:56942ms step_avg:96.19ms
step:603/1770 train_time:57040ms step_avg:96.19ms
step:604/1770 train_time:57138ms step_avg:96.19ms
step:605/1770 train_time:57236ms step_avg:96.20ms
step:606/1770 train_time:57334ms step_avg:96.20ms
step:607/1770 train_time:57432ms step_avg:96.20ms
step:608/1770 train_time:57530ms step_avg:96.20ms
step:609/1770 train_time:57628ms step_avg:96.21ms
step:610/1770 train_time:57725ms step_avg:96.21ms
step:611/1770 train_time:57824ms step_avg:96.21ms
step:612/1770 train_time:57922ms step_avg:96.22ms
step:613/1770 train_time:58020ms step_avg:96.22ms
step:614/1770 train_time:58118ms step_avg:96.22ms
step:615/1770 train_time:58216ms step_avg:96.22ms
step:616/1770 train_time:58313ms step_avg:96.23ms
step:617/1770 train_time:58411ms step_avg:96.23ms
step:618/1770 train_time:58509ms step_avg:96.23ms
step:619/1770 train_time:58607ms step_avg:96.24ms
step:620/1770 train_time:58705ms step_avg:96.24ms
step:621/1770 train_time:58803ms step_avg:96.24ms
step:622/1770 train_time:58901ms step_avg:96.24ms
step:623/1770 train_time:58999ms step_avg:96.25ms
step:624/1770 train_time:59097ms step_avg:96.25ms
step:625/1770 train_time:59195ms step_avg:96.25ms
step:625/1770 val_loss:3.6717 train_time:59292ms step_avg:96.41ms
step:626/1770 train_time:59312ms step_avg:96.29ms
step:627/1770 train_time:59400ms step_avg:96.27ms
step:628/1770 train_time:59499ms step_avg:96.28ms
step:629/1770 train_time:59597ms step_avg:96.28ms
step:630/1770 train_time:59696ms step_avg:96.28ms
step:631/1770 train_time:59793ms step_avg:96.29ms
step:632/1770 train_time:59891ms step_avg:96.29ms
step:633/1770 train_time:59989ms step_avg:96.29ms
step:634/1770 train_time:60088ms step_avg:96.29ms
step:635/1770 train_time:60186ms step_avg:96.30ms
step:636/1770 train_time:60284ms step_avg:96.30ms
step:637/1770 train_time:60382ms step_avg:96.30ms
step:638/1770 train_time:60480ms step_avg:96.31ms
step:639/1770 train_time:60578ms step_avg:96.31ms
step:640/1770 train_time:60675ms step_avg:96.31ms
step:641/1770 train_time:60773ms step_avg:96.31ms
step:642/1770 train_time:60871ms step_avg:96.31ms
step:643/1770 train_time:60968ms step_avg:96.32ms
step:644/1770 train_time:61067ms step_avg:96.32ms
step:645/1770 train_time:61165ms step_avg:96.32ms
step:646/1770 train_time:61263ms step_avg:96.33ms
step:647/1770 train_time:61361ms step_avg:96.33ms
step:648/1770 train_time:61459ms step_avg:96.33ms
step:649/1770 train_time:61557ms step_avg:96.33ms
step:650/1770 train_time:61655ms step_avg:96.34ms
step:651/1770 train_time:61752ms step_avg:96.34ms
step:652/1770 train_time:61850ms step_avg:96.34ms
step:653/1770 train_time:61948ms step_avg:96.34ms
step:654/1770 train_time:62046ms step_avg:96.35ms
step:655/1770 train_time:62144ms step_avg:96.35ms
step:656/1770 train_time:62243ms step_avg:96.35ms
step:657/1770 train_time:62340ms step_avg:96.35ms
step:658/1770 train_time:62440ms step_avg:96.36ms
step:659/1770 train_time:62539ms step_avg:96.36ms
step:660/1770 train_time:62639ms step_avg:96.37ms
step:661/1770 train_time:62738ms step_avg:96.37ms
step:662/1770 train_time:62838ms step_avg:96.38ms
step:663/1770 train_time:62939ms step_avg:96.38ms
step:664/1770 train_time:63039ms step_avg:96.39ms
step:665/1770 train_time:63139ms step_avg:96.40ms
step:666/1770 train_time:63240ms step_avg:96.40ms
step:667/1770 train_time:63339ms step_avg:96.41ms
step:668/1770 train_time:63438ms step_avg:96.41ms
step:669/1770 train_time:63538ms step_avg:96.42ms
step:670/1770 train_time:63642ms step_avg:96.43ms
step:671/1770 train_time:63739ms step_avg:96.43ms
step:672/1770 train_time:63838ms step_avg:96.43ms
step:673/1770 train_time:63939ms step_avg:96.44ms
step:674/1770 train_time:64039ms step_avg:96.44ms
step:675/1770 train_time:64139ms step_avg:96.45ms
step:676/1770 train_time:64239ms step_avg:96.46ms
step:677/1770 train_time:64339ms step_avg:96.46ms
step:678/1770 train_time:64439ms step_avg:96.47ms
step:679/1770 train_time:64538ms step_avg:96.47ms
step:680/1770 train_time:64638ms step_avg:96.47ms
step:681/1770 train_time:64737ms step_avg:96.48ms
step:682/1770 train_time:64836ms step_avg:96.48ms
step:683/1770 train_time:64937ms step_avg:96.49ms
step:684/1770 train_time:65037ms step_avg:96.49ms
step:685/1770 train_time:65137ms step_avg:96.50ms
step:686/1770 train_time:65237ms step_avg:96.50ms
step:687/1770 train_time:65337ms step_avg:96.51ms
step:688/1770 train_time:65437ms step_avg:96.52ms
step:689/1770 train_time:65538ms step_avg:96.52ms
step:690/1770 train_time:65638ms step_avg:96.53ms
step:691/1770 train_time:65739ms step_avg:96.53ms
step:692/1770 train_time:65839ms step_avg:96.54ms
step:693/1770 train_time:65938ms step_avg:96.54ms
step:694/1770 train_time:66038ms step_avg:96.55ms
step:695/1770 train_time:66139ms step_avg:96.55ms
step:696/1770 train_time:66238ms step_avg:96.56ms
step:697/1770 train_time:66338ms step_avg:96.56ms
step:698/1770 train_time:66438ms step_avg:96.57ms
step:699/1770 train_time:66538ms step_avg:96.57ms
step:700/1770 train_time:66638ms step_avg:96.58ms
step:701/1770 train_time:66738ms step_avg:96.58ms
step:702/1770 train_time:66838ms step_avg:96.59ms
step:703/1770 train_time:66938ms step_avg:96.59ms
step:704/1770 train_time:67043ms step_avg:96.60ms
step:705/1770 train_time:67137ms step_avg:96.60ms
step:706/1770 train_time:67237ms step_avg:96.61ms
step:707/1770 train_time:67337ms step_avg:96.61ms
step:708/1770 train_time:67437ms step_avg:96.61ms
step:709/1770 train_time:67537ms step_avg:96.62ms
step:710/1770 train_time:67637ms step_avg:96.62ms
step:711/1770 train_time:67737ms step_avg:96.63ms
step:712/1770 train_time:67838ms step_avg:96.63ms
step:713/1770 train_time:67938ms step_avg:96.64ms
step:714/1770 train_time:68038ms step_avg:96.64ms
step:715/1770 train_time:68138ms step_avg:96.65ms
step:716/1770 train_time:68239ms step_avg:96.66ms
step:717/1770 train_time:68338ms step_avg:96.66ms
step:718/1770 train_time:68438ms step_avg:96.66ms
step:719/1770 train_time:68537ms step_avg:96.67ms
step:720/1770 train_time:68637ms step_avg:96.67ms
step:721/1770 train_time:68743ms step_avg:96.68ms
step:722/1770 train_time:68838ms step_avg:96.68ms
step:723/1770 train_time:68938ms step_avg:96.69ms
step:724/1770 train_time:69038ms step_avg:96.69ms
step:725/1770 train_time:69137ms step_avg:96.70ms
step:726/1770 train_time:69237ms step_avg:96.70ms
step:727/1770 train_time:69337ms step_avg:96.70ms
step:728/1770 train_time:69436ms step_avg:96.71ms
step:729/1770 train_time:69536ms step_avg:96.71ms
step:730/1770 train_time:69636ms step_avg:96.72ms
step:731/1770 train_time:69736ms step_avg:96.72ms
step:732/1770 train_time:69836ms step_avg:96.73ms
step:733/1770 train_time:69936ms step_avg:96.73ms
step:734/1770 train_time:70036ms step_avg:96.74ms
step:735/1770 train_time:70136ms step_avg:96.74ms
step:736/1770 train_time:70235ms step_avg:96.74ms
step:737/1770 train_time:70335ms step_avg:96.75ms
step:738/1770 train_time:70435ms step_avg:96.75ms
step:739/1770 train_time:70534ms step_avg:96.75ms
step:740/1770 train_time:70638ms step_avg:96.76ms
step:741/1770 train_time:70734ms step_avg:96.76ms
step:742/1770 train_time:70834ms step_avg:96.77ms
step:743/1770 train_time:70933ms step_avg:96.77ms
step:744/1770 train_time:71033ms step_avg:96.77ms
step:745/1770 train_time:71131ms step_avg:96.78ms
step:746/1770 train_time:71231ms step_avg:96.78ms
step:747/1770 train_time:71330ms step_avg:96.78ms
step:748/1770 train_time:71430ms step_avg:96.79ms
step:749/1770 train_time:71530ms step_avg:96.79ms
step:750/1770 train_time:71629ms step_avg:96.80ms
step:750/1770 val_loss:3.6075 train_time:71727ms step_avg:96.93ms
step:751/1770 train_time:71752ms step_avg:96.83ms
step:752/1770 train_time:71840ms step_avg:96.82ms
step:753/1770 train_time:71940ms step_avg:96.82ms
step:754/1770 train_time:72040ms step_avg:96.83ms
step:755/1770 train_time:72139ms step_avg:96.83ms
step:756/1770 train_time:72238ms step_avg:96.83ms
step:757/1770 train_time:72337ms step_avg:96.84ms
step:758/1770 train_time:72436ms step_avg:96.84ms
step:759/1770 train_time:72535ms step_avg:96.84ms
step:760/1770 train_time:72635ms step_avg:96.85ms
step:761/1770 train_time:72734ms step_avg:96.85ms
step:762/1770 train_time:72834ms step_avg:96.85ms
step:763/1770 train_time:72934ms step_avg:96.86ms
step:764/1770 train_time:73033ms step_avg:96.86ms
step:765/1770 train_time:73133ms step_avg:96.87ms
step:766/1770 train_time:73232ms step_avg:96.87ms
step:767/1770 train_time:73333ms step_avg:96.87ms
step:768/1770 train_time:73433ms step_avg:96.88ms
step:769/1770 train_time:73533ms step_avg:96.88ms
step:770/1770 train_time:73632ms step_avg:96.88ms
step:771/1770 train_time:73732ms step_avg:96.89ms
step:772/1770 train_time:73834ms step_avg:96.89ms
step:773/1770 train_time:73932ms step_avg:96.90ms
step:774/1770 train_time:74032ms step_avg:96.90ms
step:775/1770 train_time:74132ms step_avg:96.90ms
step:776/1770 train_time:74232ms step_avg:96.91ms
step:777/1770 train_time:74333ms step_avg:96.91ms
step:778/1770 train_time:74433ms step_avg:96.92ms
step:779/1770 train_time:74533ms step_avg:96.92ms
step:780/1770 train_time:74632ms step_avg:96.92ms
step:781/1770 train_time:74732ms step_avg:96.93ms
step:782/1770 train_time:74831ms step_avg:96.93ms
step:783/1770 train_time:74931ms step_avg:96.93ms
step:784/1770 train_time:75031ms step_avg:96.94ms
step:785/1770 train_time:75131ms step_avg:96.94ms
step:786/1770 train_time:75231ms step_avg:96.95ms
step:787/1770 train_time:75333ms step_avg:96.95ms
step:788/1770 train_time:75433ms step_avg:96.96ms
step:789/1770 train_time:75534ms step_avg:96.96ms
step:790/1770 train_time:75633ms step_avg:96.97ms
step:791/1770 train_time:75736ms step_avg:96.97ms
step:792/1770 train_time:75833ms step_avg:96.97ms
step:793/1770 train_time:75933ms step_avg:96.98ms
step:794/1770 train_time:76033ms step_avg:96.98ms
step:795/1770 train_time:76133ms step_avg:96.98ms
step:796/1770 train_time:76233ms step_avg:96.99ms
step:797/1770 train_time:76333ms step_avg:96.99ms
step:798/1770 train_time:76433ms step_avg:97.00ms
step:799/1770 train_time:76533ms step_avg:97.00ms
step:800/1770 train_time:76633ms step_avg:97.00ms
step:801/1770 train_time:76733ms step_avg:97.01ms
step:802/1770 train_time:76833ms step_avg:97.01ms
step:803/1770 train_time:76933ms step_avg:97.01ms
step:804/1770 train_time:77033ms step_avg:97.02ms
step:805/1770 train_time:77133ms step_avg:97.02ms
step:806/1770 train_time:77233ms step_avg:97.03ms
step:807/1770 train_time:77332ms step_avg:97.03ms
step:808/1770 train_time:77434ms step_avg:97.04ms
step:809/1770 train_time:77532ms step_avg:97.04ms
step:810/1770 train_time:77633ms step_avg:97.04ms
step:811/1770 train_time:77733ms step_avg:97.04ms
step:812/1770 train_time:77833ms step_avg:97.05ms
step:813/1770 train_time:77934ms step_avg:97.05ms
step:814/1770 train_time:78033ms step_avg:97.06ms
step:815/1770 train_time:78133ms step_avg:97.06ms
step:816/1770 train_time:78233ms step_avg:97.06ms
step:817/1770 train_time:78333ms step_avg:97.07ms
step:818/1770 train_time:78432ms step_avg:97.07ms
step:819/1770 train_time:78532ms step_avg:97.07ms
step:820/1770 train_time:78632ms step_avg:97.08ms
step:821/1770 train_time:78733ms step_avg:97.08ms
step:822/1770 train_time:78833ms step_avg:97.09ms
step:823/1770 train_time:78935ms step_avg:97.09ms
step:824/1770 train_time:79035ms step_avg:97.09ms
step:825/1770 train_time:79137ms step_avg:97.10ms
step:826/1770 train_time:79235ms step_avg:97.10ms
step:827/1770 train_time:79335ms step_avg:97.11ms
step:828/1770 train_time:79434ms step_avg:97.11ms
step:829/1770 train_time:79535ms step_avg:97.11ms
step:830/1770 train_time:79635ms step_avg:97.12ms
step:831/1770 train_time:79734ms step_avg:97.12ms
step:832/1770 train_time:79834ms step_avg:97.12ms
step:833/1770 train_time:79934ms step_avg:97.13ms
step:834/1770 train_time:80034ms step_avg:97.13ms
step:835/1770 train_time:80134ms step_avg:97.13ms
step:836/1770 train_time:80233ms step_avg:97.13ms
step:837/1770 train_time:80334ms step_avg:97.14ms
step:838/1770 train_time:80433ms step_avg:97.14ms
step:839/1770 train_time:80534ms step_avg:97.15ms
step:840/1770 train_time:80634ms step_avg:97.15ms
step:841/1770 train_time:80736ms step_avg:97.15ms
step:842/1770 train_time:80833ms step_avg:97.16ms
step:843/1770 train_time:80933ms step_avg:97.16ms
step:844/1770 train_time:81033ms step_avg:97.16ms
step:845/1770 train_time:81132ms step_avg:97.16ms
step:846/1770 train_time:81234ms step_avg:97.17ms
step:847/1770 train_time:81332ms step_avg:97.17ms
step:848/1770 train_time:81432ms step_avg:97.17ms
step:849/1770 train_time:81532ms step_avg:97.18ms
step:850/1770 train_time:81632ms step_avg:97.18ms
step:851/1770 train_time:81733ms step_avg:97.19ms
step:852/1770 train_time:81833ms step_avg:97.19ms
step:853/1770 train_time:81933ms step_avg:97.19ms
step:854/1770 train_time:82033ms step_avg:97.20ms
step:855/1770 train_time:82133ms step_avg:97.20ms
step:856/1770 train_time:82233ms step_avg:97.20ms
step:857/1770 train_time:82333ms step_avg:97.21ms
step:858/1770 train_time:82433ms step_avg:97.21ms
step:859/1770 train_time:82535ms step_avg:97.21ms
step:860/1770 train_time:82633ms step_avg:97.21ms
step:861/1770 train_time:82732ms step_avg:97.22ms
step:862/1770 train_time:82832ms step_avg:97.22ms
step:863/1770 train_time:82933ms step_avg:97.22ms
step:864/1770 train_time:83033ms step_avg:97.23ms
step:865/1770 train_time:83134ms step_avg:97.23ms
step:866/1770 train_time:83234ms step_avg:97.24ms
step:867/1770 train_time:83334ms step_avg:97.24ms
step:868/1770 train_time:83434ms step_avg:97.24ms
step:869/1770 train_time:83534ms step_avg:97.25ms
step:870/1770 train_time:83634ms step_avg:97.25ms
step:871/1770 train_time:83733ms step_avg:97.25ms
step:872/1770 train_time:83833ms step_avg:97.25ms
step:873/1770 train_time:83932ms step_avg:97.26ms
step:874/1770 train_time:84033ms step_avg:97.26ms
step:875/1770 train_time:84133ms step_avg:97.26ms
step:875/1770 val_loss:3.5574 train_time:84232ms step_avg:97.38ms
step:876/1770 train_time:84255ms step_avg:97.29ms
step:877/1770 train_time:84337ms step_avg:97.27ms
step:878/1770 train_time:84439ms step_avg:97.28ms
step:879/1770 train_time:84539ms step_avg:97.28ms
step:880/1770 train_time:84638ms step_avg:97.29ms
step:881/1770 train_time:84738ms step_avg:97.29ms
step:882/1770 train_time:84838ms step_avg:97.29ms
step:883/1770 train_time:84937ms step_avg:97.29ms
step:884/1770 train_time:85037ms step_avg:97.30ms
step:885/1770 train_time:85137ms step_avg:97.30ms
step:886/1770 train_time:85239ms step_avg:97.30ms
step:887/1770 train_time:85342ms step_avg:97.31ms
step:888/1770 train_time:85443ms step_avg:97.32ms
step:889/1770 train_time:85544ms step_avg:97.32ms
step:890/1770 train_time:85644ms step_avg:97.32ms
step:891/1770 train_time:85744ms step_avg:97.33ms
step:892/1770 train_time:85845ms step_avg:97.33ms
step:893/1770 train_time:85945ms step_avg:97.33ms
step:894/1770 train_time:86046ms step_avg:97.34ms
step:895/1770 train_time:86146ms step_avg:97.34ms
step:896/1770 train_time:86247ms step_avg:97.34ms
step:897/1770 train_time:86347ms step_avg:97.35ms
step:898/1770 train_time:86449ms step_avg:97.35ms
step:899/1770 train_time:86549ms step_avg:97.36ms
step:900/1770 train_time:86648ms step_avg:97.36ms
step:901/1770 train_time:86749ms step_avg:97.36ms
step:902/1770 train_time:86849ms step_avg:97.36ms
step:903/1770 train_time:86949ms step_avg:97.37ms
step:904/1770 train_time:87049ms step_avg:97.37ms
step:905/1770 train_time:87149ms step_avg:97.37ms
step:906/1770 train_time:87248ms step_avg:97.38ms
step:907/1770 train_time:87348ms step_avg:97.38ms
step:908/1770 train_time:87448ms step_avg:97.38ms
step:909/1770 train_time:87548ms step_avg:97.38ms
step:910/1770 train_time:87649ms step_avg:97.39ms
step:911/1770 train_time:87749ms step_avg:97.39ms
step:912/1770 train_time:87849ms step_avg:97.39ms
step:913/1770 train_time:87950ms step_avg:97.40ms
step:914/1770 train_time:88050ms step_avg:97.40ms
step:915/1770 train_time:88150ms step_avg:97.40ms
step:916/1770 train_time:88250ms step_avg:97.41ms
step:917/1770 train_time:88350ms step_avg:97.41ms
step:918/1770 train_time:88450ms step_avg:97.41ms
step:919/1770 train_time:88550ms step_avg:97.41ms
step:920/1770 train_time:88651ms step_avg:97.42ms
step:921/1770 train_time:88753ms step_avg:97.42ms
step:922/1770 train_time:88854ms step_avg:97.43ms
step:923/1770 train_time:88955ms step_avg:97.43ms
step:924/1770 train_time:89056ms step_avg:97.44ms
step:925/1770 train_time:89157ms step_avg:97.44ms
step:926/1770 train_time:89258ms step_avg:97.44ms
step:927/1770 train_time:89361ms step_avg:97.45ms
step:928/1770 train_time:89462ms step_avg:97.45ms
step:929/1770 train_time:89564ms step_avg:97.46ms
step:930/1770 train_time:89666ms step_avg:97.46ms
step:931/1770 train_time:89767ms step_avg:97.47ms
step:932/1770 train_time:89869ms step_avg:97.47ms
step:933/1770 train_time:89970ms step_avg:97.48ms
step:934/1770 train_time:90071ms step_avg:97.48ms
step:935/1770 train_time:90172ms step_avg:97.48ms
step:936/1770 train_time:90272ms step_avg:97.49ms
step:937/1770 train_time:90373ms step_avg:97.49ms
step:938/1770 train_time:90475ms step_avg:97.49ms
step:939/1770 train_time:90576ms step_avg:97.50ms
step:940/1770 train_time:90678ms step_avg:97.50ms
step:941/1770 train_time:90779ms step_avg:97.51ms
step:942/1770 train_time:90881ms step_avg:97.51ms
step:943/1770 train_time:90983ms step_avg:97.52ms
step:944/1770 train_time:91084ms step_avg:97.52ms
step:945/1770 train_time:91186ms step_avg:97.53ms
step:946/1770 train_time:91290ms step_avg:97.53ms
step:947/1770 train_time:91391ms step_avg:97.54ms
step:948/1770 train_time:91492ms step_avg:97.54ms
step:949/1770 train_time:91594ms step_avg:97.54ms
step:950/1770 train_time:91696ms step_avg:97.55ms
step:951/1770 train_time:91797ms step_avg:97.55ms
step:952/1770 train_time:91898ms step_avg:97.56ms
step:953/1770 train_time:91999ms step_avg:97.56ms
step:954/1770 train_time:92101ms step_avg:97.56ms
step:955/1770 train_time:92204ms step_avg:97.57ms
step:956/1770 train_time:92307ms step_avg:97.58ms
step:957/1770 train_time:92407ms step_avg:97.58ms
step:958/1770 train_time:92508ms step_avg:97.58ms
step:959/1770 train_time:92610ms step_avg:97.59ms
step:960/1770 train_time:92712ms step_avg:97.59ms
step:961/1770 train_time:92812ms step_avg:97.59ms
step:962/1770 train_time:92914ms step_avg:97.60ms
step:963/1770 train_time:93015ms step_avg:97.60ms
step:964/1770 train_time:93116ms step_avg:97.61ms
step:965/1770 train_time:93217ms step_avg:97.61ms
step:966/1770 train_time:93318ms step_avg:97.61ms
step:967/1770 train_time:93420ms step_avg:97.62ms
step:968/1770 train_time:93523ms step_avg:97.62ms
step:969/1770 train_time:93624ms step_avg:97.63ms
step:970/1770 train_time:93726ms step_avg:97.63ms
step:971/1770 train_time:93828ms step_avg:97.64ms
step:972/1770 train_time:93929ms step_avg:97.64ms
step:973/1770 train_time:94030ms step_avg:97.64ms
step:974/1770 train_time:94132ms step_avg:97.65ms
step:975/1770 train_time:94233ms step_avg:97.65ms
step:976/1770 train_time:94335ms step_avg:97.66ms
step:977/1770 train_time:94436ms step_avg:97.66ms
step:978/1770 train_time:94536ms step_avg:97.66ms
step:979/1770 train_time:94637ms step_avg:97.67ms
step:980/1770 train_time:94739ms step_avg:97.67ms
step:981/1770 train_time:94841ms step_avg:97.67ms
step:982/1770 train_time:94944ms step_avg:97.68ms
step:983/1770 train_time:95045ms step_avg:97.68ms
step:984/1770 train_time:95148ms step_avg:97.69ms
step:985/1770 train_time:95249ms step_avg:97.69ms
step:986/1770 train_time:95351ms step_avg:97.70ms
step:987/1770 train_time:95452ms step_avg:97.70ms
step:988/1770 train_time:95552ms step_avg:97.70ms
step:989/1770 train_time:95655ms step_avg:97.71ms
step:990/1770 train_time:95755ms step_avg:97.71ms
step:991/1770 train_time:95857ms step_avg:97.71ms
step:992/1770 train_time:95958ms step_avg:97.72ms
step:993/1770 train_time:96059ms step_avg:97.72ms
step:994/1770 train_time:96162ms step_avg:97.73ms
step:995/1770 train_time:96264ms step_avg:97.73ms
step:996/1770 train_time:96366ms step_avg:97.73ms
step:997/1770 train_time:96467ms step_avg:97.74ms
step:998/1770 train_time:96567ms step_avg:97.74ms
step:999/1770 train_time:96669ms step_avg:97.74ms
step:1000/1770 train_time:96770ms step_avg:97.75ms
step:1000/1770 val_loss:3.5205 train_time:96869ms step_avg:97.85ms
step:1001/1770 train_time:96892ms step_avg:97.77ms
step:1002/1770 train_time:96977ms step_avg:97.76ms
step:1003/1770 train_time:97082ms step_avg:97.77ms
step:1004/1770 train_time:97183ms step_avg:97.77ms
step:1005/1770 train_time:97284ms step_avg:97.77ms
step:1006/1770 train_time:97385ms step_avg:97.78ms
step:1007/1770 train_time:97486ms step_avg:97.78ms
step:1008/1770 train_time:97587ms step_avg:97.78ms
step:1009/1770 train_time:97688ms step_avg:97.79ms
step:1010/1770 train_time:97789ms step_avg:97.79ms
step:1011/1770 train_time:97891ms step_avg:97.79ms
step:1012/1770 train_time:97992ms step_avg:97.80ms
step:1013/1770 train_time:98093ms step_avg:97.80ms
step:1014/1770 train_time:98196ms step_avg:97.80ms
step:1015/1770 train_time:98298ms step_avg:97.81ms
step:1016/1770 train_time:98400ms step_avg:97.81ms
step:1017/1770 train_time:98501ms step_avg:97.82ms
step:1018/1770 train_time:98602ms step_avg:97.82ms
step:1019/1770 train_time:98704ms step_avg:97.82ms
step:1020/1770 train_time:98806ms step_avg:97.83ms
step:1021/1770 train_time:98908ms step_avg:97.83ms
step:1022/1770 train_time:99009ms step_avg:97.84ms
step:1023/1770 train_time:99110ms step_avg:97.84ms
step:1024/1770 train_time:99212ms step_avg:97.84ms
step:1025/1770 train_time:99313ms step_avg:97.85ms
step:1026/1770 train_time:99415ms step_avg:97.85ms
step:1027/1770 train_time:99517ms step_avg:97.85ms
step:1028/1770 train_time:99619ms step_avg:97.86ms
step:1029/1770 train_time:99721ms step_avg:97.86ms
step:1030/1770 train_time:99823ms step_avg:97.87ms
step:1031/1770 train_time:99925ms step_avg:97.87ms
step:1032/1770 train_time:100027ms step_avg:97.87ms
step:1033/1770 train_time:100128ms step_avg:97.88ms
step:1034/1770 train_time:100229ms step_avg:97.88ms
step:1035/1770 train_time:100329ms step_avg:97.88ms
step:1036/1770 train_time:100430ms step_avg:97.88ms
step:1037/1770 train_time:100531ms step_avg:97.89ms
step:1038/1770 train_time:100632ms step_avg:97.89ms
step:1039/1770 train_time:100734ms step_avg:97.89ms
step:1040/1770 train_time:100835ms step_avg:97.90ms
step:1041/1770 train_time:100937ms step_avg:97.90ms
step:1042/1770 train_time:101039ms step_avg:97.91ms
step:1043/1770 train_time:101142ms step_avg:97.91ms
step:1044/1770 train_time:101243ms step_avg:97.91ms
step:1045/1770 train_time:101344ms step_avg:97.92ms
step:1046/1770 train_time:101445ms step_avg:97.92ms
step:1047/1770 train_time:101546ms step_avg:97.92ms
step:1048/1770 train_time:101646ms step_avg:97.93ms
step:1049/1770 train_time:101748ms step_avg:97.93ms
step:1050/1770 train_time:101849ms step_avg:97.93ms
step:1051/1770 train_time:101952ms step_avg:97.94ms
step:1052/1770 train_time:102053ms step_avg:97.94ms
step:1053/1770 train_time:102154ms step_avg:97.94ms
step:1054/1770 train_time:102255ms step_avg:97.95ms
step:1055/1770 train_time:102357ms step_avg:97.95ms
step:1056/1770 train_time:102459ms step_avg:97.95ms
step:1057/1770 train_time:102561ms step_avg:97.96ms
step:1058/1770 train_time:102663ms step_avg:97.96ms
step:1059/1770 train_time:102765ms step_avg:97.96ms
step:1060/1770 train_time:102868ms step_avg:97.97ms
step:1061/1770 train_time:102969ms step_avg:97.97ms
step:1062/1770 train_time:103071ms step_avg:97.98ms
step:1063/1770 train_time:103174ms step_avg:97.98ms
step:1064/1770 train_time:103277ms step_avg:97.99ms
step:1065/1770 train_time:103378ms step_avg:97.99ms
step:1066/1770 train_time:103480ms step_avg:97.99ms
step:1067/1770 train_time:103582ms step_avg:98.00ms
step:1068/1770 train_time:103685ms step_avg:98.00ms
step:1069/1770 train_time:103786ms step_avg:98.00ms
step:1070/1770 train_time:103887ms step_avg:98.01ms
step:1071/1770 train_time:103989ms step_avg:98.01ms
step:1072/1770 train_time:104090ms step_avg:98.01ms
step:1073/1770 train_time:104190ms step_avg:98.02ms
step:1074/1770 train_time:104291ms step_avg:98.02ms
step:1075/1770 train_time:104393ms step_avg:98.02ms
step:1076/1770 train_time:104496ms step_avg:98.03ms
step:1077/1770 train_time:104598ms step_avg:98.03ms
step:1078/1770 train_time:104700ms step_avg:98.03ms
step:1079/1770 train_time:104801ms step_avg:98.04ms
step:1080/1770 train_time:104903ms step_avg:98.04ms
step:1081/1770 train_time:105005ms step_avg:98.04ms
step:1082/1770 train_time:105107ms step_avg:98.05ms
step:1083/1770 train_time:105208ms step_avg:98.05ms
step:1084/1770 train_time:105309ms step_avg:98.05ms
step:1085/1770 train_time:105411ms step_avg:98.06ms
step:1086/1770 train_time:105512ms step_avg:98.06ms
step:1087/1770 train_time:105613ms step_avg:98.06ms
step:1088/1770 train_time:105715ms step_avg:98.07ms
step:1089/1770 train_time:105818ms step_avg:98.07ms
step:1090/1770 train_time:105922ms step_avg:98.08ms
step:1091/1770 train_time:106022ms step_avg:98.08ms
step:1092/1770 train_time:106123ms step_avg:98.08ms
step:1093/1770 train_time:106225ms step_avg:98.08ms
step:1094/1770 train_time:106326ms step_avg:98.09ms
step:1095/1770 train_time:106427ms step_avg:98.09ms
step:1096/1770 train_time:106528ms step_avg:98.09ms
step:1097/1770 train_time:106629ms step_avg:98.10ms
step:1098/1770 train_time:106730ms step_avg:98.10ms
step:1099/1770 train_time:106831ms step_avg:98.10ms
step:1100/1770 train_time:106932ms step_avg:98.10ms
step:1101/1770 train_time:107033ms step_avg:98.11ms
step:1102/1770 train_time:107136ms step_avg:98.11ms
step:1103/1770 train_time:107238ms step_avg:98.11ms
step:1104/1770 train_time:107340ms step_avg:98.12ms
step:1105/1770 train_time:107442ms step_avg:98.12ms
step:1106/1770 train_time:107544ms step_avg:98.12ms
step:1107/1770 train_time:107645ms step_avg:98.13ms
step:1108/1770 train_time:107747ms step_avg:98.13ms
step:1109/1770 train_time:107848ms step_avg:98.13ms
step:1110/1770 train_time:107949ms step_avg:98.14ms
step:1111/1770 train_time:108051ms step_avg:98.14ms
step:1112/1770 train_time:108153ms step_avg:98.14ms
step:1113/1770 train_time:108254ms step_avg:98.15ms
step:1114/1770 train_time:108356ms step_avg:98.15ms
step:1115/1770 train_time:108458ms step_avg:98.15ms
step:1116/1770 train_time:108561ms step_avg:98.16ms
step:1117/1770 train_time:108663ms step_avg:98.16ms
step:1118/1770 train_time:108764ms step_avg:98.16ms
step:1119/1770 train_time:108865ms step_avg:98.17ms
step:1120/1770 train_time:108966ms step_avg:98.17ms
step:1121/1770 train_time:109067ms step_avg:98.17ms
step:1122/1770 train_time:109168ms step_avg:98.17ms
step:1123/1770 train_time:109270ms step_avg:98.18ms
step:1124/1770 train_time:109374ms step_avg:98.18ms
step:1125/1770 train_time:109473ms step_avg:98.18ms
step:1125/1770 val_loss:3.4786 train_time:109572ms step_avg:98.27ms
step:1126/1770 train_time:109593ms step_avg:98.20ms
step:1127/1770 train_time:109681ms step_avg:98.19ms
step:1128/1770 train_time:109783ms step_avg:98.20ms
step:1129/1770 train_time:109884ms step_avg:98.20ms
step:1130/1770 train_time:109986ms step_avg:98.20ms
step:1131/1770 train_time:110088ms step_avg:98.21ms
step:1132/1770 train_time:110189ms step_avg:98.21ms
step:1133/1770 train_time:110290ms step_avg:98.21ms
step:1134/1770 train_time:110391ms step_avg:98.21ms
step:1135/1770 train_time:110492ms step_avg:98.22ms
step:1136/1770 train_time:110593ms step_avg:98.22ms
step:1137/1770 train_time:110696ms step_avg:98.22ms
step:1138/1770 train_time:110799ms step_avg:98.23ms
step:1139/1770 train_time:110901ms step_avg:98.23ms
step:1140/1770 train_time:111002ms step_avg:98.23ms
step:1141/1770 train_time:111104ms step_avg:98.23ms
step:1142/1770 train_time:111205ms step_avg:98.24ms
step:1143/1770 train_time:111306ms step_avg:98.24ms
step:1144/1770 train_time:111408ms step_avg:98.24ms
step:1145/1770 train_time:111509ms step_avg:98.25ms
step:1146/1770 train_time:111611ms step_avg:98.25ms
step:1147/1770 train_time:111712ms step_avg:98.25ms
step:1148/1770 train_time:111813ms step_avg:98.25ms
step:1149/1770 train_time:111915ms step_avg:98.26ms
step:1150/1770 train_time:112017ms step_avg:98.26ms
step:1151/1770 train_time:112119ms step_avg:98.26ms
step:1152/1770 train_time:112221ms step_avg:98.27ms
step:1153/1770 train_time:112323ms step_avg:98.27ms
step:1154/1770 train_time:112425ms step_avg:98.27ms
step:1155/1770 train_time:112527ms step_avg:98.28ms
step:1156/1770 train_time:112628ms step_avg:98.28ms
step:1157/1770 train_time:112731ms step_avg:98.28ms
step:1158/1770 train_time:112833ms step_avg:98.29ms
step:1159/1770 train_time:112933ms step_avg:98.29ms
step:1160/1770 train_time:113035ms step_avg:98.29ms
step:1161/1770 train_time:113137ms step_avg:98.29ms
step:1162/1770 train_time:113240ms step_avg:98.30ms
step:1163/1770 train_time:113341ms step_avg:98.30ms
step:1164/1770 train_time:113443ms step_avg:98.30ms
step:1165/1770 train_time:113544ms step_avg:98.31ms
step:1166/1770 train_time:113646ms step_avg:98.31ms
step:1167/1770 train_time:113747ms step_avg:98.31ms
step:1168/1770 train_time:113849ms step_avg:98.32ms
step:1169/1770 train_time:113950ms step_avg:98.32ms
step:1170/1770 train_time:114050ms step_avg:98.32ms
step:1171/1770 train_time:114151ms step_avg:98.32ms
step:1172/1770 train_time:114253ms step_avg:98.32ms
step:1173/1770 train_time:114354ms step_avg:98.33ms
step:1174/1770 train_time:114456ms step_avg:98.33ms
step:1175/1770 train_time:114558ms step_avg:98.33ms
step:1176/1770 train_time:114661ms step_avg:98.34ms
step:1177/1770 train_time:114763ms step_avg:98.34ms
step:1178/1770 train_time:114865ms step_avg:98.34ms
step:1179/1770 train_time:114966ms step_avg:98.35ms
step:1180/1770 train_time:115068ms step_avg:98.35ms
step:1181/1770 train_time:115169ms step_avg:98.35ms
step:1182/1770 train_time:115271ms step_avg:98.35ms
step:1183/1770 train_time:115373ms step_avg:98.36ms
step:1184/1770 train_time:115477ms step_avg:98.36ms
step:1185/1770 train_time:115579ms step_avg:98.37ms
step:1186/1770 train_time:115683ms step_avg:98.37ms
step:1187/1770 train_time:115788ms step_avg:98.38ms
step:1188/1770 train_time:115890ms step_avg:98.38ms
step:1189/1770 train_time:115992ms step_avg:98.38ms
step:1190/1770 train_time:116094ms step_avg:98.38ms
step:1191/1770 train_time:116198ms step_avg:98.39ms
step:1192/1770 train_time:116300ms step_avg:98.39ms
step:1193/1770 train_time:116404ms step_avg:98.40ms
step:1194/1770 train_time:116506ms step_avg:98.40ms
step:1195/1770 train_time:116609ms step_avg:98.40ms
step:1196/1770 train_time:116713ms step_avg:98.41ms
step:1197/1770 train_time:116817ms step_avg:98.41ms
step:1198/1770 train_time:116919ms step_avg:98.42ms
step:1199/1770 train_time:117023ms step_avg:98.42ms
step:1200/1770 train_time:117126ms step_avg:98.42ms
step:1201/1770 train_time:117229ms step_avg:98.43ms
step:1202/1770 train_time:117330ms step_avg:98.43ms
step:1203/1770 train_time:117433ms step_avg:98.44ms
step:1204/1770 train_time:117536ms step_avg:98.44ms
step:1205/1770 train_time:117638ms step_avg:98.44ms
step:1206/1770 train_time:117741ms step_avg:98.45ms
step:1207/1770 train_time:117844ms step_avg:98.45ms
step:1208/1770 train_time:117946ms step_avg:98.45ms
step:1209/1770 train_time:118048ms step_avg:98.46ms
step:1210/1770 train_time:118151ms step_avg:98.46ms
step:1211/1770 train_time:118253ms step_avg:98.46ms
step:1212/1770 train_time:118357ms step_avg:98.47ms
step:1213/1770 train_time:118460ms step_avg:98.47ms
step:1214/1770 train_time:118563ms step_avg:98.47ms
step:1215/1770 train_time:118665ms step_avg:98.48ms
step:1216/1770 train_time:118769ms step_avg:98.48ms
step:1217/1770 train_time:118873ms step_avg:98.49ms
step:1218/1770 train_time:118975ms step_avg:98.49ms
step:1219/1770 train_time:119078ms step_avg:98.49ms
step:1220/1770 train_time:119182ms step_avg:98.50ms
step:1221/1770 train_time:119284ms step_avg:98.50ms
step:1222/1770 train_time:119388ms step_avg:98.50ms
step:1223/1770 train_time:119489ms step_avg:98.51ms
step:1224/1770 train_time:119593ms step_avg:98.51ms
step:1225/1770 train_time:119696ms step_avg:98.52ms
step:1226/1770 train_time:119798ms step_avg:98.52ms
step:1227/1770 train_time:119903ms step_avg:98.52ms
step:1228/1770 train_time:120008ms step_avg:98.53ms
step:1229/1770 train_time:120110ms step_avg:98.53ms
step:1230/1770 train_time:120213ms step_avg:98.54ms
step:1231/1770 train_time:120317ms step_avg:98.54ms
step:1232/1770 train_time:120420ms step_avg:98.54ms
step:1233/1770 train_time:120522ms step_avg:98.55ms
step:1234/1770 train_time:120625ms step_avg:98.55ms
step:1235/1770 train_time:120728ms step_avg:98.55ms
step:1236/1770 train_time:120831ms step_avg:98.56ms
step:1237/1770 train_time:120935ms step_avg:98.56ms
step:1238/1770 train_time:121037ms step_avg:98.56ms
step:1239/1770 train_time:121140ms step_avg:98.57ms
step:1240/1770 train_time:121242ms step_avg:98.57ms
step:1241/1770 train_time:121346ms step_avg:98.57ms
step:1242/1770 train_time:121448ms step_avg:98.58ms
step:1243/1770 train_time:121551ms step_avg:98.58ms
step:1244/1770 train_time:121653ms step_avg:98.58ms
step:1245/1770 train_time:121756ms step_avg:98.59ms
step:1246/1770 train_time:121859ms step_avg:98.59ms
step:1247/1770 train_time:121962ms step_avg:98.59ms
step:1248/1770 train_time:122065ms step_avg:98.60ms
step:1249/1770 train_time:122167ms step_avg:98.60ms
step:1250/1770 train_time:122269ms step_avg:98.60ms
step:1250/1770 val_loss:3.4310 train_time:122371ms step_avg:98.69ms
step:1251/1770 train_time:122396ms step_avg:98.63ms
step:1252/1770 train_time:122479ms step_avg:98.61ms
step:1253/1770 train_time:122582ms step_avg:98.62ms
step:1254/1770 train_time:122685ms step_avg:98.62ms
step:1255/1770 train_time:122790ms step_avg:98.63ms
step:1256/1770 train_time:122892ms step_avg:98.63ms
step:1257/1770 train_time:122994ms step_avg:98.63ms
step:1258/1770 train_time:123098ms step_avg:98.64ms
step:1259/1770 train_time:123201ms step_avg:98.64ms
step:1260/1770 train_time:123303ms step_avg:98.64ms
step:1261/1770 train_time:123407ms step_avg:98.65ms
step:1262/1770 train_time:123511ms step_avg:98.65ms
step:1263/1770 train_time:123613ms step_avg:98.65ms
step:1264/1770 train_time:123717ms step_avg:98.66ms
step:1265/1770 train_time:123818ms step_avg:98.66ms
step:1266/1770 train_time:123921ms step_avg:98.66ms
step:1267/1770 train_time:124024ms step_avg:98.67ms
step:1268/1770 train_time:124127ms step_avg:98.67ms
step:1269/1770 train_time:124231ms step_avg:98.67ms
step:1270/1770 train_time:124334ms step_avg:98.68ms
step:1271/1770 train_time:124436ms step_avg:98.68ms
step:1272/1770 train_time:124539ms step_avg:98.68ms
step:1273/1770 train_time:124641ms step_avg:98.69ms
step:1274/1770 train_time:124744ms step_avg:98.69ms
step:1275/1770 train_time:124846ms step_avg:98.69ms
step:1276/1770 train_time:124949ms step_avg:98.70ms
step:1277/1770 train_time:125051ms step_avg:98.70ms
step:1278/1770 train_time:125155ms step_avg:98.70ms
step:1279/1770 train_time:125258ms step_avg:98.71ms
step:1280/1770 train_time:125362ms step_avg:98.71ms
step:1281/1770 train_time:125464ms step_avg:98.71ms
step:1282/1770 train_time:125568ms step_avg:98.72ms
step:1283/1770 train_time:125672ms step_avg:98.72ms
step:1284/1770 train_time:125775ms step_avg:98.72ms
step:1285/1770 train_time:125878ms step_avg:98.73ms
step:1286/1770 train_time:125982ms step_avg:98.73ms
step:1287/1770 train_time:126086ms step_avg:98.74ms
step:1288/1770 train_time:126189ms step_avg:98.74ms
step:1289/1770 train_time:126292ms step_avg:98.74ms
step:1290/1770 train_time:126394ms step_avg:98.75ms
step:1291/1770 train_time:126497ms step_avg:98.75ms
step:1292/1770 train_time:126600ms step_avg:98.75ms
step:1293/1770 train_time:126702ms step_avg:98.75ms
step:1294/1770 train_time:126805ms step_avg:98.76ms
step:1295/1770 train_time:126907ms step_avg:98.76ms
step:1296/1770 train_time:127010ms step_avg:98.76ms
step:1297/1770 train_time:127113ms step_avg:98.77ms
step:1298/1770 train_time:127215ms step_avg:98.77ms
step:1299/1770 train_time:127317ms step_avg:98.77ms
step:1300/1770 train_time:127420ms step_avg:98.78ms
step:1301/1770 train_time:127524ms step_avg:98.78ms
step:1302/1770 train_time:127626ms step_avg:98.78ms
step:1303/1770 train_time:127729ms step_avg:98.78ms
step:1304/1770 train_time:127832ms step_avg:98.79ms
step:1305/1770 train_time:127935ms step_avg:98.79ms
step:1306/1770 train_time:128037ms step_avg:98.79ms
step:1307/1770 train_time:128140ms step_avg:98.80ms
step:1308/1770 train_time:128242ms step_avg:98.80ms
step:1309/1770 train_time:128344ms step_avg:98.80ms
step:1310/1770 train_time:128446ms step_avg:98.80ms
step:1311/1770 train_time:128549ms step_avg:98.81ms
step:1312/1770 train_time:128652ms step_avg:98.81ms
step:1313/1770 train_time:128754ms step_avg:98.81ms
step:1314/1770 train_time:128857ms step_avg:98.82ms
step:1315/1770 train_time:128960ms step_avg:98.82ms
step:1316/1770 train_time:129062ms step_avg:98.82ms
step:1317/1770 train_time:129165ms step_avg:98.83ms
step:1318/1770 train_time:129271ms step_avg:98.83ms
step:1319/1770 train_time:129375ms step_avg:98.83ms
step:1320/1770 train_time:129477ms step_avg:98.84ms
step:1321/1770 train_time:129580ms step_avg:98.84ms
step:1322/1770 train_time:129682ms step_avg:98.84ms
step:1323/1770 train_time:129786ms step_avg:98.85ms
step:1324/1770 train_time:129890ms step_avg:98.85ms
step:1325/1770 train_time:129994ms step_avg:98.85ms
step:1326/1770 train_time:130096ms step_avg:98.86ms
step:1327/1770 train_time:130202ms step_avg:98.86ms
step:1328/1770 train_time:130305ms step_avg:98.87ms
step:1329/1770 train_time:130408ms step_avg:98.87ms
step:1330/1770 train_time:130510ms step_avg:98.87ms
step:1331/1770 train_time:130612ms step_avg:98.87ms
step:1332/1770 train_time:130716ms step_avg:98.88ms
step:1333/1770 train_time:130820ms step_avg:98.88ms
step:1334/1770 train_time:130922ms step_avg:98.88ms
step:1335/1770 train_time:131025ms step_avg:98.89ms
step:1336/1770 train_time:131127ms step_avg:98.89ms
step:1337/1770 train_time:131231ms step_avg:98.89ms
step:1338/1770 train_time:131333ms step_avg:98.90ms
step:1339/1770 train_time:131438ms step_avg:98.90ms
step:1340/1770 train_time:131542ms step_avg:98.90ms
step:1341/1770 train_time:131644ms step_avg:98.91ms
step:1342/1770 train_time:131748ms step_avg:98.91ms
step:1343/1770 train_time:131851ms step_avg:98.91ms
step:1344/1770 train_time:131955ms step_avg:98.92ms
step:1345/1770 train_time:132058ms step_avg:98.92ms
step:1346/1770 train_time:132160ms step_avg:98.92ms
step:1347/1770 train_time:132263ms step_avg:98.92ms
step:1348/1770 train_time:132368ms step_avg:98.93ms
step:1349/1770 train_time:132471ms step_avg:98.93ms
step:1350/1770 train_time:132573ms step_avg:98.94ms
step:1351/1770 train_time:132676ms step_avg:98.94ms
step:1352/1770 train_time:132779ms step_avg:98.94ms
step:1353/1770 train_time:132887ms step_avg:98.95ms
step:1354/1770 train_time:132985ms step_avg:98.95ms
step:1355/1770 train_time:133087ms step_avg:98.95ms
step:1356/1770 train_time:133191ms step_avg:98.95ms
step:1357/1770 train_time:133293ms step_avg:98.96ms
step:1358/1770 train_time:133396ms step_avg:98.96ms
step:1359/1770 train_time:133499ms step_avg:98.96ms
step:1360/1770 train_time:133602ms step_avg:98.96ms
step:1361/1770 train_time:133705ms step_avg:98.97ms
step:1362/1770 train_time:133807ms step_avg:98.97ms
step:1363/1770 train_time:133912ms step_avg:98.97ms
step:1364/1770 train_time:134015ms step_avg:98.98ms
step:1365/1770 train_time:134117ms step_avg:98.98ms
step:1366/1770 train_time:134220ms step_avg:98.98ms
step:1367/1770 train_time:134323ms step_avg:98.99ms
step:1368/1770 train_time:134425ms step_avg:98.99ms
step:1369/1770 train_time:134529ms step_avg:98.99ms
step:1370/1770 train_time:134632ms step_avg:98.99ms
step:1371/1770 train_time:134734ms step_avg:99.00ms
step:1372/1770 train_time:134837ms step_avg:99.00ms
step:1373/1770 train_time:134940ms step_avg:99.00ms
step:1374/1770 train_time:135043ms step_avg:99.01ms
step:1375/1770 train_time:135146ms step_avg:99.01ms
step:1375/1770 val_loss:3.3903 train_time:135248ms step_avg:99.08ms
step:1376/1770 train_time:135269ms step_avg:99.03ms
step:1377/1770 train_time:135358ms step_avg:99.02ms
step:1378/1770 train_time:135461ms step_avg:99.02ms
step:1379/1770 train_time:135563ms step_avg:99.02ms
step:1380/1770 train_time:135665ms step_avg:99.03ms
step:1381/1770 train_time:135768ms step_avg:99.03ms
step:1382/1770 train_time:135872ms step_avg:99.03ms
step:1383/1770 train_time:135975ms step_avg:99.04ms
step:1384/1770 train_time:136078ms step_avg:99.04ms
step:1385/1770 train_time:136181ms step_avg:99.04ms
step:1386/1770 train_time:136285ms step_avg:99.04ms
step:1387/1770 train_time:136389ms step_avg:99.05ms
step:1388/1770 train_time:136491ms step_avg:99.05ms
step:1389/1770 train_time:136593ms step_avg:99.05ms
step:1390/1770 train_time:136696ms step_avg:99.05ms
step:1391/1770 train_time:136798ms step_avg:99.06ms
step:1392/1770 train_time:136902ms step_avg:99.06ms
step:1393/1770 train_time:137004ms step_avg:99.06ms
step:1394/1770 train_time:137106ms step_avg:99.07ms
step:1395/1770 train_time:137210ms step_avg:99.07ms
step:1396/1770 train_time:137315ms step_avg:99.07ms
step:1397/1770 train_time:137418ms step_avg:99.08ms
step:1398/1770 train_time:137522ms step_avg:99.08ms
step:1399/1770 train_time:137624ms step_avg:99.08ms
step:1400/1770 train_time:137728ms step_avg:99.09ms
step:1401/1770 train_time:137832ms step_avg:99.09ms
step:1402/1770 train_time:137934ms step_avg:99.09ms
step:1403/1770 train_time:138037ms step_avg:99.09ms
step:1404/1770 train_time:138140ms step_avg:99.10ms
step:1405/1770 train_time:138243ms step_avg:99.10ms
step:1406/1770 train_time:138346ms step_avg:99.10ms
step:1407/1770 train_time:138448ms step_avg:99.10ms
step:1408/1770 train_time:138552ms step_avg:99.11ms
step:1409/1770 train_time:138654ms step_avg:99.11ms
step:1410/1770 train_time:138757ms step_avg:99.11ms
step:1411/1770 train_time:138860ms step_avg:99.11ms
step:1412/1770 train_time:138962ms step_avg:99.12ms
step:1413/1770 train_time:139065ms step_avg:99.12ms
step:1414/1770 train_time:139169ms step_avg:99.12ms
step:1415/1770 train_time:139271ms step_avg:99.13ms
step:1416/1770 train_time:139375ms step_avg:99.13ms
step:1417/1770 train_time:139477ms step_avg:99.13ms
step:1418/1770 train_time:139579ms step_avg:99.13ms
step:1419/1770 train_time:139682ms step_avg:99.14ms
step:1420/1770 train_time:139785ms step_avg:99.14ms
step:1421/1770 train_time:139888ms step_avg:99.14ms
step:1422/1770 train_time:139991ms step_avg:99.14ms
step:1423/1770 train_time:140094ms step_avg:99.15ms
step:1424/1770 train_time:140196ms step_avg:99.15ms
step:1425/1770 train_time:140299ms step_avg:99.15ms
step:1426/1770 train_time:140402ms step_avg:99.15ms
step:1427/1770 train_time:140505ms step_avg:99.16ms
step:1428/1770 train_time:140609ms step_avg:99.16ms
step:1429/1770 train_time:140713ms step_avg:99.16ms
step:1430/1770 train_time:140815ms step_avg:99.17ms
step:1431/1770 train_time:140918ms step_avg:99.17ms
step:1432/1770 train_time:141021ms step_avg:99.17ms
step:1433/1770 train_time:141125ms step_avg:99.17ms
step:1434/1770 train_time:141227ms step_avg:99.18ms
step:1435/1770 train_time:141329ms step_avg:99.18ms
step:1436/1770 train_time:141433ms step_avg:99.18ms
step:1437/1770 train_time:141535ms step_avg:99.18ms
step:1438/1770 train_time:141637ms step_avg:99.19ms
step:1439/1770 train_time:141741ms step_avg:99.19ms
step:1440/1770 train_time:141843ms step_avg:99.19ms
step:1441/1770 train_time:141950ms step_avg:99.20ms
step:1442/1770 train_time:142052ms step_avg:99.20ms
step:1443/1770 train_time:142156ms step_avg:99.20ms
step:1444/1770 train_time:142258ms step_avg:99.20ms
step:1445/1770 train_time:142362ms step_avg:99.21ms
step:1446/1770 train_time:142466ms step_avg:99.21ms
step:1447/1770 train_time:142571ms step_avg:99.21ms
step:1448/1770 train_time:142675ms step_avg:99.22ms
step:1449/1770 train_time:142779ms step_avg:99.22ms
step:1450/1770 train_time:142883ms step_avg:99.22ms
step:1451/1770 train_time:142987ms step_avg:99.23ms
step:1452/1770 train_time:143091ms step_avg:99.23ms
step:1453/1770 train_time:143195ms step_avg:99.23ms
step:1454/1770 train_time:143298ms step_avg:99.24ms
step:1455/1770 train_time:143404ms step_avg:99.24ms
step:1456/1770 train_time:143508ms step_avg:99.24ms
step:1457/1770 train_time:143613ms step_avg:99.25ms
step:1458/1770 train_time:143717ms step_avg:99.25ms
step:1459/1770 train_time:143823ms step_avg:99.26ms
step:1460/1770 train_time:143926ms step_avg:99.26ms
step:1461/1770 train_time:144030ms step_avg:99.26ms
step:1462/1770 train_time:144133ms step_avg:99.27ms
step:1463/1770 train_time:144237ms step_avg:99.27ms
step:1464/1770 train_time:144343ms step_avg:99.27ms
step:1465/1770 train_time:144447ms step_avg:99.28ms
step:1466/1770 train_time:144551ms step_avg:99.28ms
step:1467/1770 train_time:144656ms step_avg:99.28ms
step:1468/1770 train_time:144760ms step_avg:99.29ms
step:1469/1770 train_time:144864ms step_avg:99.29ms
step:1470/1770 train_time:144968ms step_avg:99.29ms
step:1471/1770 train_time:145072ms step_avg:99.30ms
step:1472/1770 train_time:145176ms step_avg:99.30ms
step:1473/1770 train_time:145280ms step_avg:99.30ms
step:1474/1770 train_time:145386ms step_avg:99.31ms
step:1475/1770 train_time:145489ms step_avg:99.31ms
step:1476/1770 train_time:145594ms step_avg:99.31ms
step:1477/1770 train_time:145700ms step_avg:99.32ms
step:1478/1770 train_time:145805ms step_avg:99.32ms
step:1479/1770 train_time:145909ms step_avg:99.33ms
step:1480/1770 train_time:146013ms step_avg:99.33ms
step:1481/1770 train_time:146120ms step_avg:99.33ms
step:1482/1770 train_time:146224ms step_avg:99.34ms
step:1483/1770 train_time:146328ms step_avg:99.34ms
step:1484/1770 train_time:146432ms step_avg:99.34ms
step:1485/1770 train_time:146535ms step_avg:99.35ms
step:1486/1770 train_time:146638ms step_avg:99.35ms
step:1487/1770 train_time:146742ms step_avg:99.35ms
step:1488/1770 train_time:146847ms step_avg:99.35ms
step:1489/1770 train_time:146954ms step_avg:99.36ms
step:1490/1770 train_time:147055ms step_avg:99.36ms
step:1491/1770 train_time:147160ms step_avg:99.36ms
step:1492/1770 train_time:147264ms step_avg:99.37ms
step:1493/1770 train_time:147371ms step_avg:99.37ms
step:1494/1770 train_time:147479ms step_avg:99.38ms
step:1495/1770 train_time:147582ms step_avg:99.38ms
step:1496/1770 train_time:147686ms step_avg:99.38ms
step:1497/1770 train_time:147790ms step_avg:99.39ms
step:1498/1770 train_time:147893ms step_avg:99.39ms
step:1499/1770 train_time:147996ms step_avg:99.39ms
step:1500/1770 train_time:148099ms step_avg:99.40ms
step:1500/1770 val_loss:3.3540 train_time:148201ms step_avg:99.46ms
step:1501/1770 train_time:148222ms step_avg:99.41ms
step:1502/1770 train_time:148313ms step_avg:99.41ms
step:1503/1770 train_time:148416ms step_avg:99.41ms
step:1504/1770 train_time:148521ms step_avg:99.41ms
step:1505/1770 train_time:148627ms step_avg:99.42ms
step:1506/1770 train_time:148731ms step_avg:99.42ms
step:1507/1770 train_time:148835ms step_avg:99.42ms
step:1508/1770 train_time:148940ms step_avg:99.43ms
step:1509/1770 train_time:149044ms step_avg:99.43ms
step:1510/1770 train_time:149147ms step_avg:99.43ms
step:1511/1770 train_time:149253ms step_avg:99.44ms
step:1512/1770 train_time:149357ms step_avg:99.44ms
step:1513/1770 train_time:149461ms step_avg:99.44ms
step:1514/1770 train_time:149566ms step_avg:99.45ms
step:1515/1770 train_time:149670ms step_avg:99.45ms
step:1516/1770 train_time:149773ms step_avg:99.45ms
step:1517/1770 train_time:149877ms step_avg:99.45ms
step:1518/1770 train_time:149983ms step_avg:99.46ms
step:1519/1770 train_time:150086ms step_avg:99.46ms
step:1520/1770 train_time:150191ms step_avg:99.46ms
step:1521/1770 train_time:150295ms step_avg:99.47ms
step:1522/1770 train_time:150399ms step_avg:99.47ms
step:1523/1770 train_time:150504ms step_avg:99.47ms
step:1524/1770 train_time:150608ms step_avg:99.48ms
step:1525/1770 train_time:150711ms step_avg:99.48ms
step:1526/1770 train_time:150815ms step_avg:99.48ms
step:1527/1770 train_time:150919ms step_avg:99.48ms
step:1528/1770 train_time:151024ms step_avg:99.49ms
step:1529/1770 train_time:151127ms step_avg:99.49ms
step:1530/1770 train_time:151231ms step_avg:99.49ms
step:1531/1770 train_time:151335ms step_avg:99.50ms
step:1532/1770 train_time:151439ms step_avg:99.50ms
step:1533/1770 train_time:151544ms step_avg:99.50ms
step:1534/1770 train_time:151648ms step_avg:99.51ms
step:1535/1770 train_time:151751ms step_avg:99.51ms
step:1536/1770 train_time:151854ms step_avg:99.51ms
step:1537/1770 train_time:151959ms step_avg:99.51ms
step:1538/1770 train_time:152065ms step_avg:99.52ms
step:1539/1770 train_time:152169ms step_avg:99.52ms
step:1540/1770 train_time:152275ms step_avg:99.53ms
step:1541/1770 train_time:152380ms step_avg:99.53ms
step:1542/1770 train_time:152484ms step_avg:99.53ms
step:1543/1770 train_time:152588ms step_avg:99.54ms
step:1544/1770 train_time:152694ms step_avg:99.54ms
step:1545/1770 train_time:152797ms step_avg:99.54ms
step:1546/1770 train_time:152901ms step_avg:99.55ms
step:1547/1770 train_time:153006ms step_avg:99.55ms
step:1548/1770 train_time:153109ms step_avg:99.55ms
step:1549/1770 train_time:153214ms step_avg:99.55ms
step:1550/1770 train_time:153318ms step_avg:99.56ms
step:1551/1770 train_time:153421ms step_avg:99.56ms
step:1552/1770 train_time:153528ms step_avg:99.56ms
step:1553/1770 train_time:153632ms step_avg:99.57ms
step:1554/1770 train_time:153736ms step_avg:99.57ms
step:1555/1770 train_time:153841ms step_avg:99.57ms
step:1556/1770 train_time:153944ms step_avg:99.58ms
step:1557/1770 train_time:154048ms step_avg:99.58ms
step:1558/1770 train_time:154152ms step_avg:99.58ms
step:1559/1770 train_time:154256ms step_avg:99.58ms
step:1560/1770 train_time:154359ms step_avg:99.59ms
step:1561/1770 train_time:154465ms step_avg:99.59ms
step:1562/1770 train_time:154568ms step_avg:99.59ms
step:1563/1770 train_time:154673ms step_avg:99.60ms
step:1564/1770 train_time:154775ms step_avg:99.60ms
step:1565/1770 train_time:154879ms step_avg:99.60ms
step:1566/1770 train_time:154983ms step_avg:99.60ms
step:1567/1770 train_time:155089ms step_avg:99.61ms
step:1568/1770 train_time:155192ms step_avg:99.61ms
step:1569/1770 train_time:155299ms step_avg:99.61ms
step:1570/1770 train_time:155403ms step_avg:99.62ms
step:1571/1770 train_time:155507ms step_avg:99.62ms
step:1572/1770 train_time:155612ms step_avg:99.62ms
step:1573/1770 train_time:155718ms step_avg:99.63ms
step:1574/1770 train_time:155823ms step_avg:99.63ms
step:1575/1770 train_time:155926ms step_avg:99.63ms
step:1576/1770 train_time:156029ms step_avg:99.64ms
step:1577/1770 train_time:156135ms step_avg:99.64ms
step:1578/1770 train_time:156241ms step_avg:99.64ms
step:1579/1770 train_time:156345ms step_avg:99.65ms
step:1580/1770 train_time:156449ms step_avg:99.65ms
step:1581/1770 train_time:156555ms step_avg:99.65ms
step:1582/1770 train_time:156661ms step_avg:99.66ms
step:1583/1770 train_time:156766ms step_avg:99.66ms
step:1584/1770 train_time:156871ms step_avg:99.66ms
step:1585/1770 train_time:156975ms step_avg:99.67ms
step:1586/1770 train_time:157083ms step_avg:99.67ms
step:1587/1770 train_time:157188ms step_avg:99.68ms
step:1588/1770 train_time:157292ms step_avg:99.68ms
step:1589/1770 train_time:157399ms step_avg:99.68ms
step:1590/1770 train_time:157502ms step_avg:99.68ms
step:1591/1770 train_time:157605ms step_avg:99.69ms
step:1592/1770 train_time:157710ms step_avg:99.69ms
step:1593/1770 train_time:157814ms step_avg:99.69ms
step:1594/1770 train_time:157918ms step_avg:99.70ms
step:1595/1770 train_time:158022ms step_avg:99.70ms
step:1596/1770 train_time:158128ms step_avg:99.70ms
step:1597/1770 train_time:158232ms step_avg:99.71ms
step:1598/1770 train_time:158336ms step_avg:99.71ms
step:1599/1770 train_time:158441ms step_avg:99.71ms
step:1600/1770 train_time:158548ms step_avg:99.72ms
step:1601/1770 train_time:158653ms step_avg:99.72ms
step:1602/1770 train_time:158758ms step_avg:99.72ms
step:1603/1770 train_time:158862ms step_avg:99.72ms
step:1604/1770 train_time:158966ms step_avg:99.73ms
step:1605/1770 train_time:159069ms step_avg:99.73ms
step:1606/1770 train_time:159173ms step_avg:99.73ms
step:1607/1770 train_time:159280ms step_avg:99.74ms
step:1608/1770 train_time:159384ms step_avg:99.74ms
step:1609/1770 train_time:159488ms step_avg:99.74ms
step:1610/1770 train_time:159594ms step_avg:99.75ms
step:1611/1770 train_time:159700ms step_avg:99.75ms
step:1612/1770 train_time:159805ms step_avg:99.75ms
step:1613/1770 train_time:159910ms step_avg:99.76ms
step:1614/1770 train_time:160013ms step_avg:99.76ms
step:1615/1770 train_time:160118ms step_avg:99.76ms
step:1616/1770 train_time:160221ms step_avg:99.76ms
step:1617/1770 train_time:160328ms step_avg:99.77ms
step:1618/1770 train_time:160433ms step_avg:99.77ms
step:1619/1770 train_time:160538ms step_avg:99.77ms
step:1620/1770 train_time:160642ms step_avg:99.78ms
step:1621/1770 train_time:160745ms step_avg:99.78ms
step:1622/1770 train_time:160851ms step_avg:99.78ms
step:1623/1770 train_time:160957ms step_avg:99.79ms
step:1624/1770 train_time:161061ms step_avg:99.79ms
step:1625/1770 train_time:161164ms step_avg:99.79ms
step:1625/1770 val_loss:3.3219 train_time:161267ms step_avg:99.86ms
step:1626/1770 train_time:161288ms step_avg:99.81ms
step:1627/1770 train_time:161376ms step_avg:99.80ms
step:1628/1770 train_time:161479ms step_avg:99.80ms
step:1629/1770 train_time:161582ms step_avg:99.80ms
step:1630/1770 train_time:161687ms step_avg:99.81ms
step:1631/1770 train_time:161791ms step_avg:99.81ms
step:1632/1770 train_time:161894ms step_avg:99.81ms
step:1633/1770 train_time:161999ms step_avg:99.81ms
step:1634/1770 train_time:162103ms step_avg:99.82ms
step:1635/1770 train_time:162207ms step_avg:99.82ms
step:1636/1770 train_time:162312ms step_avg:99.82ms
step:1637/1770 train_time:162416ms step_avg:99.83ms
step:1638/1770 train_time:162519ms step_avg:99.83ms
step:1639/1770 train_time:162623ms step_avg:99.83ms
step:1640/1770 train_time:162728ms step_avg:99.83ms
step:1641/1770 train_time:162832ms step_avg:99.84ms
step:1642/1770 train_time:162936ms step_avg:99.84ms
step:1643/1770 train_time:163040ms step_avg:99.84ms
step:1644/1770 train_time:163146ms step_avg:99.84ms
step:1645/1770 train_time:163249ms step_avg:99.85ms
step:1646/1770 train_time:163355ms step_avg:99.85ms
step:1647/1770 train_time:163460ms step_avg:99.85ms
step:1648/1770 train_time:163563ms step_avg:99.86ms
step:1649/1770 train_time:163667ms step_avg:99.86ms
step:1650/1770 train_time:163771ms step_avg:99.86ms
step:1651/1770 train_time:163874ms step_avg:99.86ms
step:1652/1770 train_time:163978ms step_avg:99.87ms
step:1653/1770 train_time:164083ms step_avg:99.87ms
step:1654/1770 train_time:164191ms step_avg:99.87ms
step:1655/1770 train_time:164297ms step_avg:99.88ms
step:1656/1770 train_time:164401ms step_avg:99.88ms
step:1657/1770 train_time:164507ms step_avg:99.88ms
step:1658/1770 train_time:164611ms step_avg:99.89ms
step:1659/1770 train_time:164716ms step_avg:99.89ms
step:1660/1770 train_time:164819ms step_avg:99.89ms
step:1661/1770 train_time:164924ms step_avg:99.89ms
step:1662/1770 train_time:165029ms step_avg:99.90ms
step:1663/1770 train_time:165132ms step_avg:99.90ms
step:1664/1770 train_time:165236ms step_avg:99.90ms
step:1665/1770 train_time:165339ms step_avg:99.90ms
step:1666/1770 train_time:165443ms step_avg:99.91ms
step:1667/1770 train_time:165547ms step_avg:99.91ms
step:1668/1770 train_time:165652ms step_avg:99.91ms
step:1669/1770 train_time:165755ms step_avg:99.91ms
step:1670/1770 train_time:165859ms step_avg:99.91ms
step:1671/1770 train_time:165963ms step_avg:99.92ms
step:1672/1770 train_time:166068ms step_avg:99.92ms
step:1673/1770 train_time:166173ms step_avg:99.92ms
step:1674/1770 train_time:166276ms step_avg:99.93ms
step:1675/1770 train_time:166380ms step_avg:99.93ms
step:1676/1770 train_time:166485ms step_avg:99.93ms
step:1677/1770 train_time:166594ms step_avg:99.94ms
step:1678/1770 train_time:166697ms step_avg:99.94ms
step:1679/1770 train_time:166801ms step_avg:99.94ms
step:1680/1770 train_time:166906ms step_avg:99.94ms
step:1681/1770 train_time:167011ms step_avg:99.95ms
step:1682/1770 train_time:167116ms step_avg:99.95ms
step:1683/1770 train_time:167219ms step_avg:99.95ms
step:1684/1770 train_time:167323ms step_avg:99.95ms
step:1685/1770 train_time:167428ms step_avg:99.96ms
step:1686/1770 train_time:167533ms step_avg:99.96ms
step:1687/1770 train_time:167638ms step_avg:99.96ms
step:1688/1770 train_time:167742ms step_avg:99.97ms
step:1689/1770 train_time:167847ms step_avg:99.97ms
step:1690/1770 train_time:167951ms step_avg:99.97ms
step:1691/1770 train_time:168055ms step_avg:99.97ms
step:1692/1770 train_time:168159ms step_avg:99.98ms
step:1693/1770 train_time:168264ms step_avg:99.98ms
step:1694/1770 train_time:168368ms step_avg:99.98ms
step:1695/1770 train_time:168473ms step_avg:99.98ms
step:1696/1770 train_time:168579ms step_avg:99.99ms
step:1697/1770 train_time:168684ms step_avg:99.99ms
step:1698/1770 train_time:168789ms step_avg:99.99ms
step:1699/1770 train_time:168893ms step_avg:100.00ms
step:1700/1770 train_time:168997ms step_avg:100.00ms
step:1701/1770 train_time:169101ms step_avg:100.00ms
step:1702/1770 train_time:169205ms step_avg:100.00ms
step:1703/1770 train_time:169309ms step_avg:100.01ms
step:1704/1770 train_time:169414ms step_avg:100.01ms
step:1705/1770 train_time:169517ms step_avg:100.01ms
step:1706/1770 train_time:169620ms step_avg:100.01ms
step:1707/1770 train_time:169726ms step_avg:100.02ms
step:1708/1770 train_time:169832ms step_avg:100.02ms
step:1709/1770 train_time:169938ms step_avg:100.02ms
step:1710/1770 train_time:170047ms step_avg:100.03ms
step:1711/1770 train_time:170152ms step_avg:100.03ms
step:1712/1770 train_time:170258ms step_avg:100.03ms
step:1713/1770 train_time:170362ms step_avg:100.04ms
step:1714/1770 train_time:170467ms step_avg:100.04ms
step:1715/1770 train_time:170571ms step_avg:100.04ms
step:1716/1770 train_time:170677ms step_avg:100.05ms
step:1717/1770 train_time:170781ms step_avg:100.05ms
step:1718/1770 train_time:170887ms step_avg:100.05ms
step:1719/1770 train_time:170993ms step_avg:100.05ms
step:1720/1770 train_time:171099ms step_avg:100.06ms
step:1721/1770 train_time:171203ms step_avg:100.06ms
step:1722/1770 train_time:171310ms step_avg:100.06ms
step:1723/1770 train_time:171416ms step_avg:100.07ms
step:1724/1770 train_time:171523ms step_avg:100.07ms
step:1725/1770 train_time:171630ms step_avg:100.08ms
step:1726/1770 train_time:171737ms step_avg:100.08ms
step:1727/1770 train_time:171841ms step_avg:100.08ms
step:1728/1770 train_time:171948ms step_avg:100.09ms
step:1729/1770 train_time:172053ms step_avg:100.09ms
step:1730/1770 train_time:172159ms step_avg:100.09ms
step:1731/1770 train_time:172266ms step_avg:100.10ms
step:1732/1770 train_time:172370ms step_avg:100.10ms
step:1733/1770 train_time:172477ms step_avg:100.10ms
step:1734/1770 train_time:172580ms step_avg:100.10ms
step:1735/1770 train_time:172687ms step_avg:100.11ms
step:1736/1770 train_time:172791ms step_avg:100.11ms
step:1737/1770 train_time:172897ms step_avg:100.11ms
step:1738/1770 train_time:173002ms step_avg:100.12ms
step:1739/1770 train_time:173107ms step_avg:100.12ms
step:1740/1770 train_time:173212ms step_avg:100.12ms
step:1741/1770 train_time:173319ms step_avg:100.13ms
step:1742/1770 train_time:173427ms step_avg:100.13ms
step:1743/1770 train_time:173534ms step_avg:100.14ms
step:1744/1770 train_time:173639ms step_avg:100.14ms
step:1745/1770 train_time:173744ms step_avg:100.14ms
step:1746/1770 train_time:173852ms step_avg:100.14ms
step:1747/1770 train_time:173956ms step_avg:100.15ms
step:1748/1770 train_time:174063ms step_avg:100.15ms
step:1749/1770 train_time:174169ms step_avg:100.15ms
step:1750/1770 train_time:174274ms step_avg:100.16ms
step:1750/1770 val_loss:3.2969 train_time:174377ms step_avg:100.22ms
step:1751/1770 train_time:174398ms step_avg:100.17ms
step:1752/1770 train_time:174488ms step_avg:100.17ms
step:1753/1770 train_time:174592ms step_avg:100.17ms
step:1754/1770 train_time:174698ms step_avg:100.17ms
step:1755/1770 train_time:174803ms step_avg:100.17ms
step:1756/1770 train_time:174909ms step_avg:100.18ms
step:1757/1770 train_time:175014ms step_avg:100.18ms
step:1758/1770 train_time:175119ms step_avg:100.18ms
step:1759/1770 train_time:175224ms step_avg:100.19ms
step:1760/1770 train_time:175330ms step_avg:100.19ms
step:1761/1770 train_time:175438ms step_avg:100.19ms
step:1762/1770 train_time:175546ms step_avg:100.20ms
step:1763/1770 train_time:175649ms step_avg:100.20ms
step:1764/1770 train_time:175755ms step_avg:100.20ms
step:1765/1770 train_time:175860ms step_avg:100.21ms
step:1766/1770 train_time:175971ms step_avg:100.21ms
step:1767/1770 train_time:176073ms step_avg:100.21ms
step:1768/1770 train_time:176179ms step_avg:100.22ms
step:1769/1770 train_time:176283ms step_avg:100.22ms
step:1770/1770 train_time:176388ms step_avg:100.22ms
step:1770/1770 val_loss:3.2942 train_time:176492ms step_avg:100.28ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
