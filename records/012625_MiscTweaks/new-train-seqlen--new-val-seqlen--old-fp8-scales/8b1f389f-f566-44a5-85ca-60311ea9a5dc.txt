import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:17:11 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24211ms step_avg:nanms
step:2/1770 train_time:24620ms step_avg:nanms
step:3/1770 train_time:24715ms step_avg:nanms
step:4/1770 train_time:24808ms step_avg:nanms
step:5/1770 train_time:24903ms step_avg:nanms
step:6/1770 train_time:24996ms step_avg:nanms
step:7/1770 train_time:25090ms step_avg:nanms
step:8/1770 train_time:25185ms step_avg:nanms
step:9/1770 train_time:25279ms step_avg:nanms
step:10/1770 train_time:25374ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:190ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.70ms
step:14/1770 train_time:379ms step_avg:94.69ms
step:15/1770 train_time:473ms step_avg:94.69ms
step:16/1770 train_time:568ms step_avg:94.71ms
step:17/1770 train_time:663ms step_avg:94.65ms
step:18/1770 train_time:757ms step_avg:94.65ms
step:19/1770 train_time:852ms step_avg:94.67ms
step:20/1770 train_time:947ms step_avg:94.68ms
step:21/1770 train_time:1041ms step_avg:94.67ms
step:22/1770 train_time:1136ms step_avg:94.67ms
step:23/1770 train_time:1231ms step_avg:94.69ms
step:24/1770 train_time:1325ms step_avg:94.65ms
step:25/1770 train_time:1420ms step_avg:94.67ms
step:26/1770 train_time:1515ms step_avg:94.68ms
step:27/1770 train_time:1609ms step_avg:94.67ms
step:28/1770 train_time:1704ms step_avg:94.68ms
step:29/1770 train_time:1799ms step_avg:94.69ms
step:30/1770 train_time:1894ms step_avg:94.71ms
step:31/1770 train_time:1989ms step_avg:94.71ms
step:32/1770 train_time:2084ms step_avg:94.72ms
step:33/1770 train_time:2178ms step_avg:94.70ms
step:34/1770 train_time:2273ms step_avg:94.72ms
step:35/1770 train_time:2368ms step_avg:94.72ms
step:36/1770 train_time:2463ms step_avg:94.72ms
step:37/1770 train_time:2557ms step_avg:94.71ms
step:38/1770 train_time:2652ms step_avg:94.73ms
step:39/1770 train_time:2747ms step_avg:94.73ms
step:40/1770 train_time:2843ms step_avg:94.76ms
step:41/1770 train_time:2937ms step_avg:94.74ms
step:42/1770 train_time:3032ms step_avg:94.75ms
step:43/1770 train_time:3127ms step_avg:94.74ms
step:44/1770 train_time:3221ms step_avg:94.75ms
step:45/1770 train_time:3317ms step_avg:94.77ms
step:46/1770 train_time:3411ms step_avg:94.75ms
step:47/1770 train_time:3505ms step_avg:94.74ms
step:48/1770 train_time:3601ms step_avg:94.75ms
step:49/1770 train_time:3695ms step_avg:94.75ms
step:50/1770 train_time:3790ms step_avg:94.76ms
step:51/1770 train_time:3885ms step_avg:94.75ms
step:52/1770 train_time:3979ms step_avg:94.74ms
step:53/1770 train_time:4074ms step_avg:94.75ms
step:54/1770 train_time:4169ms step_avg:94.75ms
step:55/1770 train_time:4264ms step_avg:94.75ms
step:56/1770 train_time:4358ms step_avg:94.75ms
step:57/1770 train_time:4453ms step_avg:94.74ms
step:58/1770 train_time:4549ms step_avg:94.76ms
step:59/1770 train_time:4643ms step_avg:94.75ms
step:60/1770 train_time:4738ms step_avg:94.75ms
step:61/1770 train_time:4833ms step_avg:94.76ms
step:62/1770 train_time:4928ms step_avg:94.77ms
step:63/1770 train_time:5023ms step_avg:94.77ms
step:64/1770 train_time:5117ms step_avg:94.76ms
step:65/1770 train_time:5212ms step_avg:94.77ms
step:66/1770 train_time:5305ms step_avg:94.74ms
step:67/1770 train_time:5400ms step_avg:94.74ms
step:68/1770 train_time:5494ms step_avg:94.73ms
step:69/1770 train_time:5589ms step_avg:94.73ms
step:70/1770 train_time:5684ms step_avg:94.73ms
step:71/1770 train_time:5779ms step_avg:94.73ms
step:72/1770 train_time:5874ms step_avg:94.74ms
step:73/1770 train_time:5969ms step_avg:94.74ms
step:74/1770 train_time:6063ms step_avg:94.74ms
step:75/1770 train_time:6158ms step_avg:94.74ms
step:76/1770 train_time:6253ms step_avg:94.74ms
step:77/1770 train_time:6348ms step_avg:94.74ms
step:78/1770 train_time:6443ms step_avg:94.75ms
step:79/1770 train_time:6537ms step_avg:94.75ms
step:80/1770 train_time:6632ms step_avg:94.75ms
step:81/1770 train_time:6726ms step_avg:94.74ms
step:82/1770 train_time:6821ms step_avg:94.74ms
step:83/1770 train_time:6916ms step_avg:94.74ms
step:84/1770 train_time:7012ms step_avg:94.75ms
step:85/1770 train_time:7106ms step_avg:94.74ms
step:86/1770 train_time:7204ms step_avg:94.79ms
step:87/1770 train_time:7295ms step_avg:94.74ms
step:88/1770 train_time:7391ms step_avg:94.75ms
step:89/1770 train_time:7485ms step_avg:94.75ms
step:90/1770 train_time:7580ms step_avg:94.75ms
step:91/1770 train_time:7674ms step_avg:94.75ms
step:92/1770 train_time:7769ms step_avg:94.75ms
step:93/1770 train_time:7865ms step_avg:94.75ms
step:94/1770 train_time:7958ms step_avg:94.74ms
step:95/1770 train_time:8053ms step_avg:94.74ms
step:96/1770 train_time:8148ms step_avg:94.75ms
step:97/1770 train_time:8242ms step_avg:94.74ms
step:98/1770 train_time:8337ms step_avg:94.74ms
step:99/1770 train_time:8431ms step_avg:94.74ms
step:100/1770 train_time:8526ms step_avg:94.73ms
step:101/1770 train_time:8621ms step_avg:94.74ms
step:102/1770 train_time:8716ms step_avg:94.74ms
step:103/1770 train_time:8811ms step_avg:94.75ms
step:104/1770 train_time:8905ms step_avg:94.74ms
step:105/1770 train_time:9000ms step_avg:94.74ms
step:106/1770 train_time:9095ms step_avg:94.74ms
step:107/1770 train_time:9190ms step_avg:94.74ms
step:108/1770 train_time:9284ms step_avg:94.74ms
step:109/1770 train_time:9379ms step_avg:94.73ms
step:110/1770 train_time:9473ms step_avg:94.73ms
step:111/1770 train_time:9568ms step_avg:94.74ms
step:112/1770 train_time:9663ms step_avg:94.73ms
step:113/1770 train_time:9757ms step_avg:94.73ms
step:114/1770 train_time:9853ms step_avg:94.74ms
step:115/1770 train_time:9948ms step_avg:94.74ms
step:116/1770 train_time:10043ms step_avg:94.74ms
step:117/1770 train_time:10138ms step_avg:94.74ms
step:118/1770 train_time:10233ms step_avg:94.75ms
step:119/1770 train_time:10327ms step_avg:94.75ms
step:120/1770 train_time:10422ms step_avg:94.74ms
step:121/1770 train_time:10516ms step_avg:94.74ms
step:122/1770 train_time:10612ms step_avg:94.75ms
step:123/1770 train_time:10707ms step_avg:94.75ms
step:124/1770 train_time:10801ms step_avg:94.75ms
step:125/1770 train_time:10896ms step_avg:94.74ms
step:125/1770 val_loss:4.6512 train_time:10989ms step_avg:95.55ms
step:126/1770 train_time:11011ms step_avg:94.92ms
step:127/1770 train_time:11087ms step_avg:94.76ms
step:128/1770 train_time:11185ms step_avg:94.79ms
step:129/1770 train_time:11283ms step_avg:94.82ms
step:130/1770 train_time:11378ms step_avg:94.82ms
step:131/1770 train_time:11473ms step_avg:94.82ms
step:132/1770 train_time:11568ms step_avg:94.82ms
step:133/1770 train_time:11663ms step_avg:94.82ms
step:134/1770 train_time:11758ms step_avg:94.82ms
step:135/1770 train_time:11853ms step_avg:94.82ms
step:136/1770 train_time:11947ms step_avg:94.82ms
step:137/1770 train_time:12043ms step_avg:94.83ms
step:138/1770 train_time:12138ms step_avg:94.83ms
step:139/1770 train_time:12233ms step_avg:94.83ms
step:140/1770 train_time:12328ms step_avg:94.83ms
step:141/1770 train_time:12424ms step_avg:94.84ms
step:142/1770 train_time:12520ms step_avg:94.85ms
step:143/1770 train_time:12615ms step_avg:94.85ms
step:144/1770 train_time:12710ms step_avg:94.85ms
step:145/1770 train_time:12805ms step_avg:94.86ms
step:146/1770 train_time:12901ms step_avg:94.86ms
step:147/1770 train_time:12996ms step_avg:94.86ms
step:148/1770 train_time:13091ms step_avg:94.86ms
step:149/1770 train_time:13186ms step_avg:94.86ms
step:150/1770 train_time:13282ms step_avg:94.87ms
step:151/1770 train_time:13378ms step_avg:94.88ms
step:152/1770 train_time:13473ms step_avg:94.88ms
step:153/1770 train_time:13568ms step_avg:94.88ms
step:154/1770 train_time:13664ms step_avg:94.89ms
step:155/1770 train_time:13759ms step_avg:94.89ms
step:156/1770 train_time:13855ms step_avg:94.90ms
step:157/1770 train_time:13950ms step_avg:94.90ms
step:158/1770 train_time:14045ms step_avg:94.90ms
step:159/1770 train_time:14141ms step_avg:94.91ms
step:160/1770 train_time:14236ms step_avg:94.90ms
step:161/1770 train_time:14331ms step_avg:94.91ms
step:162/1770 train_time:14426ms step_avg:94.91ms
step:163/1770 train_time:14522ms step_avg:94.91ms
step:164/1770 train_time:14617ms step_avg:94.91ms
step:165/1770 train_time:14712ms step_avg:94.92ms
step:166/1770 train_time:14807ms step_avg:94.92ms
step:167/1770 train_time:14903ms step_avg:94.92ms
step:168/1770 train_time:14998ms step_avg:94.93ms
step:169/1770 train_time:15094ms step_avg:94.93ms
step:170/1770 train_time:15189ms step_avg:94.93ms
step:171/1770 train_time:15285ms step_avg:94.94ms
step:172/1770 train_time:15380ms step_avg:94.94ms
step:173/1770 train_time:15475ms step_avg:94.94ms
step:174/1770 train_time:15571ms step_avg:94.94ms
step:175/1770 train_time:15666ms step_avg:94.95ms
step:176/1770 train_time:15761ms step_avg:94.95ms
step:177/1770 train_time:15857ms step_avg:94.95ms
step:178/1770 train_time:15952ms step_avg:94.95ms
step:179/1770 train_time:16047ms step_avg:94.95ms
step:180/1770 train_time:16143ms step_avg:94.96ms
step:181/1770 train_time:16239ms step_avg:94.96ms
step:182/1770 train_time:16334ms step_avg:94.96ms
step:183/1770 train_time:16429ms step_avg:94.96ms
step:184/1770 train_time:16524ms step_avg:94.96ms
step:185/1770 train_time:16619ms step_avg:94.97ms
step:186/1770 train_time:16715ms step_avg:94.97ms
step:187/1770 train_time:16810ms step_avg:94.97ms
step:188/1770 train_time:16906ms step_avg:94.97ms
step:189/1770 train_time:17001ms step_avg:94.98ms
step:190/1770 train_time:17096ms step_avg:94.98ms
step:191/1770 train_time:17192ms step_avg:94.98ms
step:192/1770 train_time:17286ms step_avg:94.98ms
step:193/1770 train_time:17382ms step_avg:94.98ms
step:194/1770 train_time:17477ms step_avg:94.98ms
step:195/1770 train_time:17572ms step_avg:94.99ms
step:196/1770 train_time:17667ms step_avg:94.99ms
step:197/1770 train_time:17763ms step_avg:94.99ms
step:198/1770 train_time:17858ms step_avg:94.99ms
step:199/1770 train_time:17954ms step_avg:94.99ms
step:200/1770 train_time:18050ms step_avg:95.00ms
step:201/1770 train_time:18145ms step_avg:95.00ms
step:202/1770 train_time:18240ms step_avg:95.00ms
step:203/1770 train_time:18336ms step_avg:95.00ms
step:204/1770 train_time:18430ms step_avg:95.00ms
step:205/1770 train_time:18525ms step_avg:95.00ms
step:206/1770 train_time:18621ms step_avg:95.01ms
step:207/1770 train_time:18716ms step_avg:95.01ms
step:208/1770 train_time:18812ms step_avg:95.01ms
step:209/1770 train_time:18907ms step_avg:95.01ms
step:210/1770 train_time:19003ms step_avg:95.01ms
step:211/1770 train_time:19098ms step_avg:95.02ms
step:212/1770 train_time:19193ms step_avg:95.02ms
step:213/1770 train_time:19289ms step_avg:95.02ms
step:214/1770 train_time:19384ms step_avg:95.02ms
step:215/1770 train_time:19480ms step_avg:95.02ms
step:216/1770 train_time:19575ms step_avg:95.02ms
step:217/1770 train_time:19670ms step_avg:95.02ms
step:218/1770 train_time:19765ms step_avg:95.02ms
step:219/1770 train_time:19861ms step_avg:95.03ms
step:220/1770 train_time:19956ms step_avg:95.03ms
step:221/1770 train_time:20051ms step_avg:95.03ms
step:222/1770 train_time:20146ms step_avg:95.03ms
step:223/1770 train_time:20242ms step_avg:95.03ms
step:224/1770 train_time:20337ms step_avg:95.03ms
step:225/1770 train_time:20433ms step_avg:95.04ms
step:226/1770 train_time:20528ms step_avg:95.04ms
step:227/1770 train_time:20624ms step_avg:95.04ms
step:228/1770 train_time:20719ms step_avg:95.04ms
step:229/1770 train_time:20814ms step_avg:95.04ms
step:230/1770 train_time:20910ms step_avg:95.05ms
step:231/1770 train_time:21005ms step_avg:95.05ms
step:232/1770 train_time:21101ms step_avg:95.05ms
step:233/1770 train_time:21197ms step_avg:95.05ms
step:234/1770 train_time:21292ms step_avg:95.05ms
step:235/1770 train_time:21387ms step_avg:95.05ms
step:236/1770 train_time:21482ms step_avg:95.05ms
step:237/1770 train_time:21578ms step_avg:95.06ms
step:238/1770 train_time:21673ms step_avg:95.06ms
step:239/1770 train_time:21768ms step_avg:95.06ms
step:240/1770 train_time:21864ms step_avg:95.06ms
step:241/1770 train_time:21960ms step_avg:95.06ms
step:242/1770 train_time:22055ms step_avg:95.07ms
step:243/1770 train_time:22151ms step_avg:95.07ms
step:244/1770 train_time:22246ms step_avg:95.07ms
step:245/1770 train_time:22342ms step_avg:95.07ms
step:246/1770 train_time:22438ms step_avg:95.08ms
step:247/1770 train_time:22533ms step_avg:95.07ms
step:248/1770 train_time:22628ms step_avg:95.07ms
step:249/1770 train_time:22723ms step_avg:95.07ms
step:250/1770 train_time:22818ms step_avg:95.08ms
step:250/1770 val_loss:4.1145 train_time:22911ms step_avg:95.46ms
step:251/1770 train_time:22933ms step_avg:95.16ms
step:252/1770 train_time:23017ms step_avg:95.11ms
step:253/1770 train_time:23115ms step_avg:95.12ms
step:254/1770 train_time:23211ms step_avg:95.13ms
step:255/1770 train_time:23307ms step_avg:95.13ms
step:256/1770 train_time:23402ms step_avg:95.13ms
step:257/1770 train_time:23497ms step_avg:95.13ms
step:258/1770 train_time:23592ms step_avg:95.13ms
step:259/1770 train_time:23687ms step_avg:95.13ms
step:260/1770 train_time:23782ms step_avg:95.13ms
step:261/1770 train_time:23877ms step_avg:95.13ms
step:262/1770 train_time:23972ms step_avg:95.13ms
step:263/1770 train_time:24068ms step_avg:95.13ms
step:264/1770 train_time:24163ms step_avg:95.13ms
step:265/1770 train_time:24259ms step_avg:95.13ms
step:266/1770 train_time:24355ms step_avg:95.14ms
step:267/1770 train_time:24451ms step_avg:95.14ms
step:268/1770 train_time:24547ms step_avg:95.14ms
step:269/1770 train_time:24642ms step_avg:95.14ms
step:270/1770 train_time:24738ms step_avg:95.15ms
step:271/1770 train_time:24834ms step_avg:95.15ms
step:272/1770 train_time:24930ms step_avg:95.15ms
step:273/1770 train_time:25025ms step_avg:95.15ms
step:274/1770 train_time:25121ms step_avg:95.16ms
step:275/1770 train_time:25217ms step_avg:95.16ms
step:276/1770 train_time:25313ms step_avg:95.16ms
step:277/1770 train_time:25409ms step_avg:95.16ms
step:278/1770 train_time:25505ms step_avg:95.17ms
step:279/1770 train_time:25600ms step_avg:95.17ms
step:280/1770 train_time:25696ms step_avg:95.17ms
step:281/1770 train_time:25792ms step_avg:95.17ms
step:282/1770 train_time:25888ms step_avg:95.18ms
step:283/1770 train_time:25984ms step_avg:95.18ms
step:284/1770 train_time:26079ms step_avg:95.18ms
step:285/1770 train_time:26175ms step_avg:95.18ms
step:286/1770 train_time:26271ms step_avg:95.19ms
step:287/1770 train_time:26367ms step_avg:95.19ms
step:288/1770 train_time:26463ms step_avg:95.19ms
step:289/1770 train_time:26559ms step_avg:95.19ms
step:290/1770 train_time:26655ms step_avg:95.20ms
step:291/1770 train_time:26751ms step_avg:95.20ms
step:292/1770 train_time:26847ms step_avg:95.20ms
step:293/1770 train_time:26943ms step_avg:95.20ms
step:294/1770 train_time:27039ms step_avg:95.21ms
step:295/1770 train_time:27135ms step_avg:95.21ms
step:296/1770 train_time:27231ms step_avg:95.21ms
step:297/1770 train_time:27328ms step_avg:95.22ms
step:298/1770 train_time:27423ms step_avg:95.22ms
step:299/1770 train_time:27518ms step_avg:95.22ms
step:300/1770 train_time:27614ms step_avg:95.22ms
step:301/1770 train_time:27710ms step_avg:95.22ms
step:302/1770 train_time:27806ms step_avg:95.23ms
step:303/1770 train_time:27902ms step_avg:95.23ms
step:304/1770 train_time:27998ms step_avg:95.23ms
step:305/1770 train_time:28096ms step_avg:95.24ms
step:306/1770 train_time:28190ms step_avg:95.24ms
step:307/1770 train_time:28286ms step_avg:95.24ms
step:308/1770 train_time:28381ms step_avg:95.24ms
step:309/1770 train_time:28477ms step_avg:95.24ms
step:310/1770 train_time:28573ms step_avg:95.24ms
step:311/1770 train_time:28669ms step_avg:95.25ms
step:312/1770 train_time:28765ms step_avg:95.25ms
step:313/1770 train_time:28861ms step_avg:95.25ms
step:314/1770 train_time:28956ms step_avg:95.25ms
step:315/1770 train_time:29052ms step_avg:95.25ms
step:316/1770 train_time:29148ms step_avg:95.25ms
step:317/1770 train_time:29243ms step_avg:95.25ms
step:318/1770 train_time:29339ms step_avg:95.26ms
step:319/1770 train_time:29435ms step_avg:95.26ms
step:320/1770 train_time:29530ms step_avg:95.26ms
step:321/1770 train_time:29626ms step_avg:95.26ms
step:322/1770 train_time:29722ms step_avg:95.26ms
step:323/1770 train_time:29818ms step_avg:95.26ms
step:324/1770 train_time:29914ms step_avg:95.27ms
step:325/1770 train_time:30009ms step_avg:95.27ms
step:326/1770 train_time:30105ms step_avg:95.27ms
step:327/1770 train_time:30200ms step_avg:95.27ms
step:328/1770 train_time:30296ms step_avg:95.27ms
step:329/1770 train_time:30392ms step_avg:95.27ms
step:330/1770 train_time:30488ms step_avg:95.27ms
step:331/1770 train_time:30583ms step_avg:95.28ms
step:332/1770 train_time:30679ms step_avg:95.28ms
step:333/1770 train_time:30774ms step_avg:95.28ms
step:334/1770 train_time:30870ms step_avg:95.28ms
step:335/1770 train_time:30966ms step_avg:95.28ms
step:336/1770 train_time:31063ms step_avg:95.28ms
step:337/1770 train_time:31159ms step_avg:95.29ms
step:338/1770 train_time:31254ms step_avg:95.29ms
step:339/1770 train_time:31350ms step_avg:95.29ms
step:340/1770 train_time:31445ms step_avg:95.29ms
step:341/1770 train_time:31540ms step_avg:95.29ms
step:342/1770 train_time:31636ms step_avg:95.29ms
step:343/1770 train_time:31733ms step_avg:95.29ms
step:344/1770 train_time:31829ms step_avg:95.29ms
step:345/1770 train_time:31924ms step_avg:95.30ms
step:346/1770 train_time:32020ms step_avg:95.30ms
step:347/1770 train_time:32115ms step_avg:95.30ms
step:348/1770 train_time:32211ms step_avg:95.30ms
step:349/1770 train_time:32307ms step_avg:95.30ms
step:350/1770 train_time:32402ms step_avg:95.30ms
step:351/1770 train_time:32498ms step_avg:95.30ms
step:352/1770 train_time:32593ms step_avg:95.30ms
step:353/1770 train_time:32689ms step_avg:95.30ms
step:354/1770 train_time:32784ms step_avg:95.30ms
step:355/1770 train_time:32880ms step_avg:95.30ms
step:356/1770 train_time:32975ms step_avg:95.30ms
step:357/1770 train_time:33071ms step_avg:95.30ms
step:358/1770 train_time:33166ms step_avg:95.31ms
step:359/1770 train_time:33263ms step_avg:95.31ms
step:360/1770 train_time:33358ms step_avg:95.31ms
step:361/1770 train_time:33454ms step_avg:95.31ms
step:362/1770 train_time:33550ms step_avg:95.31ms
step:363/1770 train_time:33646ms step_avg:95.31ms
step:364/1770 train_time:33741ms step_avg:95.31ms
step:365/1770 train_time:33837ms step_avg:95.31ms
step:366/1770 train_time:33932ms step_avg:95.32ms
step:367/1770 train_time:34028ms step_avg:95.32ms
step:368/1770 train_time:34124ms step_avg:95.32ms
step:369/1770 train_time:34219ms step_avg:95.32ms
step:370/1770 train_time:34315ms step_avg:95.32ms
step:371/1770 train_time:34411ms step_avg:95.32ms
step:372/1770 train_time:34509ms step_avg:95.33ms
step:373/1770 train_time:34602ms step_avg:95.32ms
step:374/1770 train_time:34698ms step_avg:95.32ms
step:375/1770 train_time:34793ms step_avg:95.32ms
step:375/1770 val_loss:3.9177 train_time:34888ms step_avg:95.58ms
step:376/1770 train_time:34910ms step_avg:95.38ms
step:377/1770 train_time:34991ms step_avg:95.34ms
step:378/1770 train_time:35089ms step_avg:95.35ms
step:379/1770 train_time:35185ms step_avg:95.35ms
step:380/1770 train_time:35280ms step_avg:95.35ms
step:381/1770 train_time:35376ms step_avg:95.35ms
step:382/1770 train_time:35472ms step_avg:95.35ms
step:383/1770 train_time:35567ms step_avg:95.35ms
step:384/1770 train_time:35662ms step_avg:95.35ms
step:385/1770 train_time:35758ms step_avg:95.35ms
step:386/1770 train_time:35854ms step_avg:95.36ms
step:387/1770 train_time:35949ms step_avg:95.36ms
step:388/1770 train_time:36046ms step_avg:95.36ms
step:389/1770 train_time:36142ms step_avg:95.36ms
step:390/1770 train_time:36238ms step_avg:95.36ms
step:391/1770 train_time:36334ms step_avg:95.37ms
step:392/1770 train_time:36430ms step_avg:95.37ms
step:393/1770 train_time:36525ms step_avg:95.37ms
step:394/1770 train_time:36620ms step_avg:95.37ms
step:395/1770 train_time:36717ms step_avg:95.37ms
step:396/1770 train_time:36814ms step_avg:95.37ms
step:397/1770 train_time:36912ms step_avg:95.38ms
step:398/1770 train_time:37009ms step_avg:95.38ms
step:399/1770 train_time:37106ms step_avg:95.39ms
step:400/1770 train_time:37203ms step_avg:95.39ms
step:401/1770 train_time:37301ms step_avg:95.40ms
step:402/1770 train_time:37403ms step_avg:95.41ms
step:403/1770 train_time:37496ms step_avg:95.41ms
step:404/1770 train_time:37594ms step_avg:95.42ms
step:405/1770 train_time:37691ms step_avg:95.42ms
step:406/1770 train_time:37789ms step_avg:95.43ms
step:407/1770 train_time:37886ms step_avg:95.43ms
step:408/1770 train_time:37984ms step_avg:95.44ms
step:409/1770 train_time:38082ms step_avg:95.44ms
step:410/1770 train_time:38179ms step_avg:95.45ms
step:411/1770 train_time:38277ms step_avg:95.45ms
step:412/1770 train_time:38375ms step_avg:95.46ms
step:413/1770 train_time:38473ms step_avg:95.47ms
step:414/1770 train_time:38570ms step_avg:95.47ms
step:415/1770 train_time:38667ms step_avg:95.47ms
step:416/1770 train_time:38764ms step_avg:95.48ms
step:417/1770 train_time:38862ms step_avg:95.48ms
step:418/1770 train_time:38959ms step_avg:95.49ms
step:419/1770 train_time:39057ms step_avg:95.49ms
step:420/1770 train_time:39154ms step_avg:95.50ms
step:421/1770 train_time:39252ms step_avg:95.50ms
step:422/1770 train_time:39351ms step_avg:95.51ms
step:423/1770 train_time:39449ms step_avg:95.52ms
step:424/1770 train_time:39546ms step_avg:95.52ms
step:425/1770 train_time:39643ms step_avg:95.53ms
step:426/1770 train_time:39740ms step_avg:95.53ms
step:427/1770 train_time:39838ms step_avg:95.54ms
step:428/1770 train_time:39936ms step_avg:95.54ms
step:429/1770 train_time:40034ms step_avg:95.55ms
step:430/1770 train_time:40132ms step_avg:95.55ms
step:431/1770 train_time:40229ms step_avg:95.56ms
step:432/1770 train_time:40327ms step_avg:95.56ms
step:433/1770 train_time:40424ms step_avg:95.57ms
step:434/1770 train_time:40522ms step_avg:95.57ms
step:435/1770 train_time:40619ms step_avg:95.57ms
step:436/1770 train_time:40717ms step_avg:95.58ms
step:437/1770 train_time:40815ms step_avg:95.59ms
step:438/1770 train_time:40913ms step_avg:95.59ms
step:439/1770 train_time:41011ms step_avg:95.60ms
step:440/1770 train_time:41109ms step_avg:95.60ms
step:441/1770 train_time:41206ms step_avg:95.61ms
step:442/1770 train_time:41304ms step_avg:95.61ms
step:443/1770 train_time:41401ms step_avg:95.61ms
step:444/1770 train_time:41498ms step_avg:95.62ms
step:445/1770 train_time:41596ms step_avg:95.62ms
step:446/1770 train_time:41694ms step_avg:95.63ms
step:447/1770 train_time:41791ms step_avg:95.63ms
step:448/1770 train_time:41889ms step_avg:95.64ms
step:449/1770 train_time:41986ms step_avg:95.64ms
step:450/1770 train_time:42084ms step_avg:95.65ms
step:451/1770 train_time:42182ms step_avg:95.65ms
step:452/1770 train_time:42279ms step_avg:95.65ms
step:453/1770 train_time:42377ms step_avg:95.66ms
step:454/1770 train_time:42474ms step_avg:95.66ms
step:455/1770 train_time:42572ms step_avg:95.67ms
step:456/1770 train_time:42669ms step_avg:95.67ms
step:457/1770 train_time:42767ms step_avg:95.68ms
step:458/1770 train_time:42864ms step_avg:95.68ms
step:459/1770 train_time:42962ms step_avg:95.68ms
step:460/1770 train_time:43059ms step_avg:95.69ms
step:461/1770 train_time:43158ms step_avg:95.69ms
step:462/1770 train_time:43256ms step_avg:95.70ms
step:463/1770 train_time:43354ms step_avg:95.70ms
step:464/1770 train_time:43451ms step_avg:95.71ms
step:465/1770 train_time:43549ms step_avg:95.71ms
step:466/1770 train_time:43646ms step_avg:95.71ms
step:467/1770 train_time:43743ms step_avg:95.72ms
step:468/1770 train_time:43841ms step_avg:95.72ms
step:469/1770 train_time:43938ms step_avg:95.73ms
step:470/1770 train_time:44036ms step_avg:95.73ms
step:471/1770 train_time:44134ms step_avg:95.74ms
step:472/1770 train_time:44232ms step_avg:95.74ms
step:473/1770 train_time:44330ms step_avg:95.74ms
step:474/1770 train_time:44427ms step_avg:95.75ms
step:475/1770 train_time:44525ms step_avg:95.75ms
step:476/1770 train_time:44623ms step_avg:95.76ms
step:477/1770 train_time:44720ms step_avg:95.76ms
step:478/1770 train_time:44818ms step_avg:95.77ms
step:479/1770 train_time:44916ms step_avg:95.77ms
step:480/1770 train_time:45014ms step_avg:95.77ms
step:481/1770 train_time:45111ms step_avg:95.78ms
step:482/1770 train_time:45209ms step_avg:95.78ms
step:483/1770 train_time:45306ms step_avg:95.79ms
step:484/1770 train_time:45404ms step_avg:95.79ms
step:485/1770 train_time:45501ms step_avg:95.79ms
step:486/1770 train_time:45598ms step_avg:95.79ms
step:487/1770 train_time:45696ms step_avg:95.80ms
step:488/1770 train_time:45794ms step_avg:95.80ms
step:489/1770 train_time:45892ms step_avg:95.81ms
step:490/1770 train_time:45989ms step_avg:95.81ms
step:491/1770 train_time:46087ms step_avg:95.81ms
step:492/1770 train_time:46184ms step_avg:95.82ms
step:493/1770 train_time:46282ms step_avg:95.82ms
step:494/1770 train_time:46379ms step_avg:95.82ms
step:495/1770 train_time:46477ms step_avg:95.83ms
step:496/1770 train_time:46575ms step_avg:95.83ms
step:497/1770 train_time:46673ms step_avg:95.84ms
step:498/1770 train_time:46770ms step_avg:95.84ms
step:499/1770 train_time:46868ms step_avg:95.84ms
step:500/1770 train_time:46965ms step_avg:95.85ms
step:500/1770 val_loss:3.7620 train_time:47061ms step_avg:96.04ms
step:501/1770 train_time:47082ms step_avg:95.89ms
step:502/1770 train_time:47170ms step_avg:95.87ms
step:503/1770 train_time:47270ms step_avg:95.88ms
step:504/1770 train_time:47369ms step_avg:95.89ms
step:505/1770 train_time:47467ms step_avg:95.89ms
step:506/1770 train_time:47565ms step_avg:95.90ms
step:507/1770 train_time:47663ms step_avg:95.90ms
step:508/1770 train_time:47760ms step_avg:95.90ms
step:509/1770 train_time:47858ms step_avg:95.91ms
step:510/1770 train_time:47955ms step_avg:95.91ms
step:511/1770 train_time:48053ms step_avg:95.91ms
step:512/1770 train_time:48150ms step_avg:95.92ms
step:513/1770 train_time:48248ms step_avg:95.92ms
step:514/1770 train_time:48346ms step_avg:95.92ms
step:515/1770 train_time:48444ms step_avg:95.93ms
step:516/1770 train_time:48542ms step_avg:95.93ms
step:517/1770 train_time:48639ms step_avg:95.94ms
step:518/1770 train_time:48737ms step_avg:95.94ms
step:519/1770 train_time:48835ms step_avg:95.94ms
step:520/1770 train_time:48933ms step_avg:95.95ms
step:521/1770 train_time:49030ms step_avg:95.95ms
step:522/1770 train_time:49128ms step_avg:95.95ms
step:523/1770 train_time:49226ms step_avg:95.96ms
step:524/1770 train_time:49323ms step_avg:95.96ms
step:525/1770 train_time:49421ms step_avg:95.96ms
step:526/1770 train_time:49518ms step_avg:95.97ms
step:527/1770 train_time:49617ms step_avg:95.97ms
step:528/1770 train_time:49715ms step_avg:95.97ms
step:529/1770 train_time:49813ms step_avg:95.98ms
step:530/1770 train_time:49911ms step_avg:95.98ms
step:531/1770 train_time:50010ms step_avg:95.99ms
step:532/1770 train_time:50108ms step_avg:95.99ms
step:533/1770 train_time:50206ms step_avg:96.00ms
step:534/1770 train_time:50305ms step_avg:96.00ms
step:535/1770 train_time:50403ms step_avg:96.01ms
step:536/1770 train_time:50501ms step_avg:96.01ms
step:537/1770 train_time:50599ms step_avg:96.01ms
step:538/1770 train_time:50697ms step_avg:96.02ms
step:539/1770 train_time:50800ms step_avg:96.03ms
step:540/1770 train_time:50893ms step_avg:96.02ms
step:541/1770 train_time:50991ms step_avg:96.03ms
step:542/1770 train_time:51089ms step_avg:96.03ms
step:543/1770 train_time:51187ms step_avg:96.04ms
step:544/1770 train_time:51286ms step_avg:96.04ms
step:545/1770 train_time:51383ms step_avg:96.04ms
step:546/1770 train_time:51481ms step_avg:96.05ms
step:547/1770 train_time:51579ms step_avg:96.05ms
step:548/1770 train_time:51677ms step_avg:96.05ms
step:549/1770 train_time:51774ms step_avg:96.06ms
step:550/1770 train_time:51872ms step_avg:96.06ms
step:551/1770 train_time:51970ms step_avg:96.06ms
step:552/1770 train_time:52069ms step_avg:96.07ms
step:553/1770 train_time:52167ms step_avg:96.07ms
step:554/1770 train_time:52265ms step_avg:96.08ms
step:555/1770 train_time:52363ms step_avg:96.08ms
step:556/1770 train_time:52462ms step_avg:96.08ms
step:557/1770 train_time:52560ms step_avg:96.09ms
step:558/1770 train_time:52659ms step_avg:96.09ms
step:559/1770 train_time:52756ms step_avg:96.10ms
step:560/1770 train_time:52854ms step_avg:96.10ms
step:561/1770 train_time:52953ms step_avg:96.10ms
step:562/1770 train_time:53051ms step_avg:96.11ms
step:563/1770 train_time:53149ms step_avg:96.11ms
step:564/1770 train_time:53247ms step_avg:96.11ms
step:565/1770 train_time:53345ms step_avg:96.12ms
step:566/1770 train_time:53443ms step_avg:96.12ms
step:567/1770 train_time:53541ms step_avg:96.12ms
step:568/1770 train_time:53639ms step_avg:96.13ms
step:569/1770 train_time:53737ms step_avg:96.13ms
step:570/1770 train_time:53835ms step_avg:96.13ms
step:571/1770 train_time:53934ms step_avg:96.14ms
step:572/1770 train_time:54032ms step_avg:96.14ms
step:573/1770 train_time:54130ms step_avg:96.15ms
step:574/1770 train_time:54229ms step_avg:96.15ms
step:575/1770 train_time:54327ms step_avg:96.15ms
step:576/1770 train_time:54425ms step_avg:96.16ms
step:577/1770 train_time:54523ms step_avg:96.16ms
step:578/1770 train_time:54621ms step_avg:96.16ms
step:579/1770 train_time:54719ms step_avg:96.17ms
step:580/1770 train_time:54817ms step_avg:96.17ms
step:581/1770 train_time:54915ms step_avg:96.17ms
step:582/1770 train_time:55014ms step_avg:96.18ms
step:583/1770 train_time:55112ms step_avg:96.18ms
step:584/1770 train_time:55210ms step_avg:96.18ms
step:585/1770 train_time:55308ms step_avg:96.19ms
step:586/1770 train_time:55406ms step_avg:96.19ms
step:587/1770 train_time:55504ms step_avg:96.19ms
step:588/1770 train_time:55602ms step_avg:96.20ms
step:589/1770 train_time:55699ms step_avg:96.20ms
step:590/1770 train_time:55797ms step_avg:96.20ms
step:591/1770 train_time:55895ms step_avg:96.20ms
step:592/1770 train_time:55993ms step_avg:96.21ms
step:593/1770 train_time:56091ms step_avg:96.21ms
step:594/1770 train_time:56189ms step_avg:96.21ms
step:595/1770 train_time:56287ms step_avg:96.22ms
step:596/1770 train_time:56385ms step_avg:96.22ms
step:597/1770 train_time:56483ms step_avg:96.22ms
step:598/1770 train_time:56581ms step_avg:96.23ms
step:599/1770 train_time:56679ms step_avg:96.23ms
step:600/1770 train_time:56777ms step_avg:96.23ms
step:601/1770 train_time:56874ms step_avg:96.23ms
step:602/1770 train_time:56972ms step_avg:96.24ms
step:603/1770 train_time:57069ms step_avg:96.24ms
step:604/1770 train_time:57168ms step_avg:96.24ms
step:605/1770 train_time:57266ms step_avg:96.24ms
step:606/1770 train_time:57363ms step_avg:96.25ms
step:607/1770 train_time:57461ms step_avg:96.25ms
step:608/1770 train_time:57559ms step_avg:96.25ms
step:609/1770 train_time:57658ms step_avg:96.26ms
step:610/1770 train_time:57755ms step_avg:96.26ms
step:611/1770 train_time:57853ms step_avg:96.26ms
step:612/1770 train_time:57951ms step_avg:96.26ms
step:613/1770 train_time:58050ms step_avg:96.27ms
step:614/1770 train_time:58148ms step_avg:96.27ms
step:615/1770 train_time:58246ms step_avg:96.27ms
step:616/1770 train_time:58345ms step_avg:96.28ms
step:617/1770 train_time:58443ms step_avg:96.28ms
step:618/1770 train_time:58541ms step_avg:96.29ms
step:619/1770 train_time:58639ms step_avg:96.29ms
step:620/1770 train_time:58737ms step_avg:96.29ms
step:621/1770 train_time:58835ms step_avg:96.29ms
step:622/1770 train_time:58933ms step_avg:96.30ms
step:623/1770 train_time:59031ms step_avg:96.30ms
step:624/1770 train_time:59129ms step_avg:96.30ms
step:625/1770 train_time:59228ms step_avg:96.31ms
step:625/1770 val_loss:3.6743 train_time:59324ms step_avg:96.46ms
step:626/1770 train_time:59346ms step_avg:96.34ms
step:627/1770 train_time:59432ms step_avg:96.32ms
step:628/1770 train_time:59531ms step_avg:96.33ms
step:629/1770 train_time:59629ms step_avg:96.33ms
step:630/1770 train_time:59727ms step_avg:96.33ms
step:631/1770 train_time:59825ms step_avg:96.34ms
step:632/1770 train_time:59922ms step_avg:96.34ms
step:633/1770 train_time:60020ms step_avg:96.34ms
step:634/1770 train_time:60118ms step_avg:96.34ms
step:635/1770 train_time:60217ms step_avg:96.35ms
step:636/1770 train_time:60315ms step_avg:96.35ms
step:637/1770 train_time:60413ms step_avg:96.35ms
step:638/1770 train_time:60511ms step_avg:96.35ms
step:639/1770 train_time:60609ms step_avg:96.36ms
step:640/1770 train_time:60707ms step_avg:96.36ms
step:641/1770 train_time:60805ms step_avg:96.36ms
step:642/1770 train_time:60904ms step_avg:96.37ms
step:643/1770 train_time:61000ms step_avg:96.37ms
step:644/1770 train_time:61098ms step_avg:96.37ms
step:645/1770 train_time:61197ms step_avg:96.37ms
step:646/1770 train_time:61295ms step_avg:96.38ms
step:647/1770 train_time:61393ms step_avg:96.38ms
step:648/1770 train_time:61491ms step_avg:96.38ms
step:649/1770 train_time:61589ms step_avg:96.38ms
step:650/1770 train_time:61687ms step_avg:96.39ms
step:651/1770 train_time:61785ms step_avg:96.39ms
step:652/1770 train_time:61883ms step_avg:96.39ms
step:653/1770 train_time:61981ms step_avg:96.39ms
step:654/1770 train_time:62078ms step_avg:96.39ms
step:655/1770 train_time:62177ms step_avg:96.40ms
step:656/1770 train_time:62274ms step_avg:96.40ms
step:657/1770 train_time:62372ms step_avg:96.40ms
step:658/1770 train_time:62472ms step_avg:96.41ms
step:659/1770 train_time:62571ms step_avg:96.41ms
step:660/1770 train_time:62670ms step_avg:96.42ms
step:661/1770 train_time:62770ms step_avg:96.42ms
step:662/1770 train_time:62870ms step_avg:96.43ms
step:663/1770 train_time:62969ms step_avg:96.43ms
step:664/1770 train_time:63069ms step_avg:96.44ms
step:665/1770 train_time:63168ms step_avg:96.44ms
step:666/1770 train_time:63268ms step_avg:96.44ms
step:667/1770 train_time:63368ms step_avg:96.45ms
step:668/1770 train_time:63468ms step_avg:96.46ms
step:669/1770 train_time:63568ms step_avg:96.46ms
step:670/1770 train_time:63668ms step_avg:96.47ms
step:671/1770 train_time:63769ms step_avg:96.47ms
step:672/1770 train_time:63869ms step_avg:96.48ms
step:673/1770 train_time:63968ms step_avg:96.48ms
step:674/1770 train_time:64068ms step_avg:96.49ms
step:675/1770 train_time:64168ms step_avg:96.49ms
step:676/1770 train_time:64268ms step_avg:96.50ms
step:677/1770 train_time:64367ms step_avg:96.50ms
step:678/1770 train_time:64466ms step_avg:96.51ms
step:679/1770 train_time:64566ms step_avg:96.51ms
step:680/1770 train_time:64666ms step_avg:96.52ms
step:681/1770 train_time:64766ms step_avg:96.52ms
step:682/1770 train_time:64867ms step_avg:96.53ms
step:683/1770 train_time:64967ms step_avg:96.53ms
step:684/1770 train_time:65067ms step_avg:96.54ms
step:685/1770 train_time:65167ms step_avg:96.54ms
step:686/1770 train_time:65267ms step_avg:96.55ms
step:687/1770 train_time:65367ms step_avg:96.55ms
step:688/1770 train_time:65467ms step_avg:96.56ms
step:689/1770 train_time:65567ms step_avg:96.56ms
step:690/1770 train_time:65667ms step_avg:96.57ms
step:691/1770 train_time:65766ms step_avg:96.57ms
step:692/1770 train_time:65867ms step_avg:96.58ms
step:693/1770 train_time:65966ms step_avg:96.58ms
step:694/1770 train_time:66066ms step_avg:96.59ms
step:695/1770 train_time:66167ms step_avg:96.59ms
step:696/1770 train_time:66267ms step_avg:96.60ms
step:697/1770 train_time:66368ms step_avg:96.61ms
step:698/1770 train_time:66469ms step_avg:96.61ms
step:699/1770 train_time:66569ms step_avg:96.62ms
step:700/1770 train_time:66668ms step_avg:96.62ms
step:701/1770 train_time:66768ms step_avg:96.63ms
step:702/1770 train_time:66868ms step_avg:96.63ms
step:703/1770 train_time:66967ms step_avg:96.63ms
step:704/1770 train_time:67067ms step_avg:96.64ms
step:705/1770 train_time:67166ms step_avg:96.64ms
step:706/1770 train_time:67266ms step_avg:96.65ms
step:707/1770 train_time:67365ms step_avg:96.65ms
step:708/1770 train_time:67465ms step_avg:96.66ms
step:709/1770 train_time:67566ms step_avg:96.66ms
step:710/1770 train_time:67666ms step_avg:96.67ms
step:711/1770 train_time:67766ms step_avg:96.67ms
step:712/1770 train_time:67867ms step_avg:96.68ms
step:713/1770 train_time:67968ms step_avg:96.68ms
step:714/1770 train_time:68069ms step_avg:96.69ms
step:715/1770 train_time:68169ms step_avg:96.69ms
step:716/1770 train_time:68269ms step_avg:96.70ms
step:717/1770 train_time:68369ms step_avg:96.70ms
step:718/1770 train_time:68468ms step_avg:96.71ms
step:719/1770 train_time:68568ms step_avg:96.71ms
step:720/1770 train_time:68668ms step_avg:96.72ms
step:721/1770 train_time:68768ms step_avg:96.72ms
step:722/1770 train_time:68868ms step_avg:96.72ms
step:723/1770 train_time:68968ms step_avg:96.73ms
step:724/1770 train_time:69068ms step_avg:96.73ms
step:725/1770 train_time:69168ms step_avg:96.74ms
step:726/1770 train_time:69268ms step_avg:96.74ms
step:727/1770 train_time:69368ms step_avg:96.75ms
step:728/1770 train_time:69468ms step_avg:96.75ms
step:729/1770 train_time:69568ms step_avg:96.76ms
step:730/1770 train_time:69667ms step_avg:96.76ms
step:731/1770 train_time:69766ms step_avg:96.76ms
step:732/1770 train_time:69866ms step_avg:96.77ms
step:733/1770 train_time:69967ms step_avg:96.77ms
step:734/1770 train_time:70067ms step_avg:96.78ms
step:735/1770 train_time:70167ms step_avg:96.78ms
step:736/1770 train_time:70267ms step_avg:96.79ms
step:737/1770 train_time:70367ms step_avg:96.79ms
step:738/1770 train_time:70468ms step_avg:96.80ms
step:739/1770 train_time:70570ms step_avg:96.80ms
step:740/1770 train_time:70669ms step_avg:96.81ms
step:741/1770 train_time:70769ms step_avg:96.81ms
step:742/1770 train_time:70869ms step_avg:96.82ms
step:743/1770 train_time:70969ms step_avg:96.82ms
step:744/1770 train_time:71069ms step_avg:96.82ms
step:745/1770 train_time:71169ms step_avg:96.83ms
step:746/1770 train_time:71269ms step_avg:96.83ms
step:747/1770 train_time:71368ms step_avg:96.84ms
step:748/1770 train_time:71469ms step_avg:96.84ms
step:749/1770 train_time:71569ms step_avg:96.85ms
step:750/1770 train_time:71669ms step_avg:96.85ms
step:750/1770 val_loss:3.6096 train_time:71767ms step_avg:96.98ms
step:751/1770 train_time:71789ms step_avg:96.88ms
step:752/1770 train_time:71876ms step_avg:96.87ms
step:753/1770 train_time:71976ms step_avg:96.87ms
step:754/1770 train_time:72076ms step_avg:96.88ms
step:755/1770 train_time:72176ms step_avg:96.88ms
step:756/1770 train_time:72276ms step_avg:96.88ms
step:757/1770 train_time:72375ms step_avg:96.89ms
step:758/1770 train_time:72476ms step_avg:96.89ms
step:759/1770 train_time:72574ms step_avg:96.89ms
step:760/1770 train_time:72674ms step_avg:96.90ms
step:761/1770 train_time:72774ms step_avg:96.90ms
step:762/1770 train_time:72875ms step_avg:96.91ms
step:763/1770 train_time:72977ms step_avg:96.92ms
step:764/1770 train_time:73077ms step_avg:96.92ms
step:765/1770 train_time:73177ms step_avg:96.92ms
step:766/1770 train_time:73276ms step_avg:96.93ms
step:767/1770 train_time:73376ms step_avg:96.93ms
step:768/1770 train_time:73476ms step_avg:96.93ms
step:769/1770 train_time:73575ms step_avg:96.94ms
step:770/1770 train_time:73674ms step_avg:96.94ms
step:771/1770 train_time:73774ms step_avg:96.94ms
step:772/1770 train_time:73873ms step_avg:96.95ms
step:773/1770 train_time:73975ms step_avg:96.95ms
step:774/1770 train_time:74074ms step_avg:96.95ms
step:775/1770 train_time:74173ms step_avg:96.96ms
step:776/1770 train_time:74273ms step_avg:96.96ms
step:777/1770 train_time:74373ms step_avg:96.97ms
step:778/1770 train_time:74474ms step_avg:96.97ms
step:779/1770 train_time:74573ms step_avg:96.97ms
step:780/1770 train_time:74672ms step_avg:96.98ms
step:781/1770 train_time:74772ms step_avg:96.98ms
step:782/1770 train_time:74873ms step_avg:96.99ms
step:783/1770 train_time:74973ms step_avg:96.99ms
step:784/1770 train_time:75074ms step_avg:96.99ms
step:785/1770 train_time:75175ms step_avg:97.00ms
step:786/1770 train_time:75276ms step_avg:97.00ms
step:787/1770 train_time:75376ms step_avg:97.01ms
step:788/1770 train_time:75478ms step_avg:97.02ms
step:789/1770 train_time:75576ms step_avg:97.02ms
step:790/1770 train_time:75676ms step_avg:97.02ms
step:791/1770 train_time:75776ms step_avg:97.02ms
step:792/1770 train_time:75876ms step_avg:97.03ms
step:793/1770 train_time:75976ms step_avg:97.03ms
step:794/1770 train_time:76076ms step_avg:97.04ms
step:795/1770 train_time:76176ms step_avg:97.04ms
step:796/1770 train_time:76277ms step_avg:97.04ms
step:797/1770 train_time:76378ms step_avg:97.05ms
step:798/1770 train_time:76478ms step_avg:97.05ms
step:799/1770 train_time:76577ms step_avg:97.06ms
step:800/1770 train_time:76677ms step_avg:97.06ms
step:801/1770 train_time:76777ms step_avg:97.06ms
step:802/1770 train_time:76878ms step_avg:97.07ms
step:803/1770 train_time:76977ms step_avg:97.07ms
step:804/1770 train_time:77079ms step_avg:97.08ms
step:805/1770 train_time:77177ms step_avg:97.08ms
step:806/1770 train_time:77277ms step_avg:97.08ms
step:807/1770 train_time:77377ms step_avg:97.08ms
step:808/1770 train_time:77477ms step_avg:97.09ms
step:809/1770 train_time:77577ms step_avg:97.09ms
step:810/1770 train_time:77677ms step_avg:97.10ms
step:811/1770 train_time:77776ms step_avg:97.10ms
step:812/1770 train_time:77876ms step_avg:97.10ms
step:813/1770 train_time:77976ms step_avg:97.11ms
step:814/1770 train_time:78076ms step_avg:97.11ms
step:815/1770 train_time:78176ms step_avg:97.11ms
step:816/1770 train_time:78275ms step_avg:97.12ms
step:817/1770 train_time:78375ms step_avg:97.12ms
step:818/1770 train_time:78476ms step_avg:97.12ms
step:819/1770 train_time:78576ms step_avg:97.13ms
step:820/1770 train_time:78675ms step_avg:97.13ms
step:821/1770 train_time:78776ms step_avg:97.13ms
step:822/1770 train_time:78876ms step_avg:97.14ms
step:823/1770 train_time:78978ms step_avg:97.14ms
step:824/1770 train_time:79077ms step_avg:97.15ms
step:825/1770 train_time:79177ms step_avg:97.15ms
step:826/1770 train_time:79277ms step_avg:97.15ms
step:827/1770 train_time:79376ms step_avg:97.16ms
step:828/1770 train_time:79477ms step_avg:97.16ms
step:829/1770 train_time:79577ms step_avg:97.16ms
step:830/1770 train_time:79677ms step_avg:97.17ms
step:831/1770 train_time:79777ms step_avg:97.17ms
step:832/1770 train_time:79877ms step_avg:97.17ms
step:833/1770 train_time:79977ms step_avg:97.18ms
step:834/1770 train_time:80077ms step_avg:97.18ms
step:835/1770 train_time:80177ms step_avg:97.18ms
step:836/1770 train_time:80277ms step_avg:97.19ms
step:837/1770 train_time:80377ms step_avg:97.19ms
step:838/1770 train_time:80476ms step_avg:97.19ms
step:839/1770 train_time:80576ms step_avg:97.20ms
step:840/1770 train_time:80676ms step_avg:97.20ms
step:841/1770 train_time:80776ms step_avg:97.20ms
step:842/1770 train_time:80876ms step_avg:97.21ms
step:843/1770 train_time:80977ms step_avg:97.21ms
step:844/1770 train_time:81075ms step_avg:97.21ms
step:845/1770 train_time:81176ms step_avg:97.22ms
step:846/1770 train_time:81276ms step_avg:97.22ms
step:847/1770 train_time:81377ms step_avg:97.22ms
step:848/1770 train_time:81477ms step_avg:97.23ms
step:849/1770 train_time:81578ms step_avg:97.23ms
step:850/1770 train_time:81678ms step_avg:97.24ms
step:851/1770 train_time:81778ms step_avg:97.24ms
step:852/1770 train_time:81877ms step_avg:97.24ms
step:853/1770 train_time:81977ms step_avg:97.24ms
step:854/1770 train_time:82077ms step_avg:97.25ms
step:855/1770 train_time:82177ms step_avg:97.25ms
step:856/1770 train_time:82277ms step_avg:97.25ms
step:857/1770 train_time:82378ms step_avg:97.26ms
step:858/1770 train_time:82478ms step_avg:97.26ms
step:859/1770 train_time:82578ms step_avg:97.26ms
step:860/1770 train_time:82677ms step_avg:97.27ms
step:861/1770 train_time:82777ms step_avg:97.27ms
step:862/1770 train_time:82877ms step_avg:97.27ms
step:863/1770 train_time:82978ms step_avg:97.28ms
step:864/1770 train_time:83077ms step_avg:97.28ms
step:865/1770 train_time:83176ms step_avg:97.28ms
step:866/1770 train_time:83277ms step_avg:97.29ms
step:867/1770 train_time:83378ms step_avg:97.29ms
step:868/1770 train_time:83478ms step_avg:97.29ms
step:869/1770 train_time:83577ms step_avg:97.30ms
step:870/1770 train_time:83677ms step_avg:97.30ms
step:871/1770 train_time:83776ms step_avg:97.30ms
step:872/1770 train_time:83876ms step_avg:97.30ms
step:873/1770 train_time:83976ms step_avg:97.31ms
step:874/1770 train_time:84076ms step_avg:97.31ms
step:875/1770 train_time:84176ms step_avg:97.31ms
step:875/1770 val_loss:3.5618 train_time:84274ms step_avg:97.43ms
step:876/1770 train_time:84296ms step_avg:97.34ms
step:877/1770 train_time:84381ms step_avg:97.32ms
step:878/1770 train_time:84482ms step_avg:97.33ms
step:879/1770 train_time:84582ms step_avg:97.33ms
step:880/1770 train_time:84682ms step_avg:97.34ms
step:881/1770 train_time:84782ms step_avg:97.34ms
step:882/1770 train_time:84882ms step_avg:97.34ms
step:883/1770 train_time:84982ms step_avg:97.34ms
step:884/1770 train_time:85081ms step_avg:97.35ms
step:885/1770 train_time:85181ms step_avg:97.35ms
step:886/1770 train_time:85281ms step_avg:97.35ms
step:887/1770 train_time:85383ms step_avg:97.36ms
step:888/1770 train_time:85484ms step_avg:97.36ms
step:889/1770 train_time:85585ms step_avg:97.37ms
step:890/1770 train_time:85685ms step_avg:97.37ms
step:891/1770 train_time:85785ms step_avg:97.37ms
step:892/1770 train_time:85885ms step_avg:97.38ms
step:893/1770 train_time:85985ms step_avg:97.38ms
step:894/1770 train_time:86084ms step_avg:97.38ms
step:895/1770 train_time:86184ms step_avg:97.38ms
step:896/1770 train_time:86284ms step_avg:97.39ms
step:897/1770 train_time:86384ms step_avg:97.39ms
step:898/1770 train_time:86484ms step_avg:97.39ms
step:899/1770 train_time:86584ms step_avg:97.40ms
step:900/1770 train_time:86684ms step_avg:97.40ms
step:901/1770 train_time:86784ms step_avg:97.40ms
step:902/1770 train_time:86884ms step_avg:97.40ms
step:903/1770 train_time:86985ms step_avg:97.41ms
step:904/1770 train_time:87084ms step_avg:97.41ms
step:905/1770 train_time:87184ms step_avg:97.41ms
step:906/1770 train_time:87285ms step_avg:97.42ms
step:907/1770 train_time:87385ms step_avg:97.42ms
step:908/1770 train_time:87485ms step_avg:97.42ms
step:909/1770 train_time:87586ms step_avg:97.43ms
step:910/1770 train_time:87687ms step_avg:97.43ms
step:911/1770 train_time:87788ms step_avg:97.43ms
step:912/1770 train_time:87889ms step_avg:97.44ms
step:913/1770 train_time:87990ms step_avg:97.44ms
step:914/1770 train_time:88090ms step_avg:97.44ms
step:915/1770 train_time:88191ms step_avg:97.45ms
step:916/1770 train_time:88291ms step_avg:97.45ms
step:917/1770 train_time:88391ms step_avg:97.45ms
step:918/1770 train_time:88491ms step_avg:97.46ms
step:919/1770 train_time:88592ms step_avg:97.46ms
step:920/1770 train_time:88694ms step_avg:97.47ms
step:921/1770 train_time:88796ms step_avg:97.47ms
step:922/1770 train_time:88898ms step_avg:97.48ms
step:923/1770 train_time:88998ms step_avg:97.48ms
step:924/1770 train_time:89099ms step_avg:97.48ms
step:925/1770 train_time:89200ms step_avg:97.49ms
step:926/1770 train_time:89301ms step_avg:97.49ms
step:927/1770 train_time:89402ms step_avg:97.49ms
step:928/1770 train_time:89505ms step_avg:97.50ms
step:929/1770 train_time:89607ms step_avg:97.50ms
step:930/1770 train_time:89708ms step_avg:97.51ms
step:931/1770 train_time:89809ms step_avg:97.51ms
step:932/1770 train_time:89910ms step_avg:97.52ms
step:933/1770 train_time:90011ms step_avg:97.52ms
step:934/1770 train_time:90113ms step_avg:97.52ms
step:935/1770 train_time:90213ms step_avg:97.53ms
step:936/1770 train_time:90315ms step_avg:97.53ms
step:937/1770 train_time:90416ms step_avg:97.54ms
step:938/1770 train_time:90517ms step_avg:97.54ms
step:939/1770 train_time:90619ms step_avg:97.54ms
step:940/1770 train_time:90720ms step_avg:97.55ms
step:941/1770 train_time:90821ms step_avg:97.55ms
step:942/1770 train_time:90924ms step_avg:97.56ms
step:943/1770 train_time:91026ms step_avg:97.56ms
step:944/1770 train_time:91127ms step_avg:97.57ms
step:945/1770 train_time:91227ms step_avg:97.57ms
step:946/1770 train_time:91330ms step_avg:97.57ms
step:947/1770 train_time:91432ms step_avg:97.58ms
step:948/1770 train_time:91534ms step_avg:97.58ms
step:949/1770 train_time:91635ms step_avg:97.59ms
step:950/1770 train_time:91738ms step_avg:97.59ms
step:951/1770 train_time:91838ms step_avg:97.60ms
step:952/1770 train_time:91940ms step_avg:97.60ms
step:953/1770 train_time:92041ms step_avg:97.60ms
step:954/1770 train_time:92143ms step_avg:97.61ms
step:955/1770 train_time:92245ms step_avg:97.61ms
step:956/1770 train_time:92347ms step_avg:97.62ms
step:957/1770 train_time:92449ms step_avg:97.62ms
step:958/1770 train_time:92550ms step_avg:97.63ms
step:959/1770 train_time:92651ms step_avg:97.63ms
step:960/1770 train_time:92752ms step_avg:97.63ms
step:961/1770 train_time:92854ms step_avg:97.64ms
step:962/1770 train_time:92955ms step_avg:97.64ms
step:963/1770 train_time:93056ms step_avg:97.65ms
step:964/1770 train_time:93159ms step_avg:97.65ms
step:965/1770 train_time:93260ms step_avg:97.65ms
step:966/1770 train_time:93361ms step_avg:97.66ms
step:967/1770 train_time:93463ms step_avg:97.66ms
step:968/1770 train_time:93566ms step_avg:97.67ms
step:969/1770 train_time:93668ms step_avg:97.67ms
step:970/1770 train_time:93769ms step_avg:97.68ms
step:971/1770 train_time:93870ms step_avg:97.68ms
step:972/1770 train_time:93971ms step_avg:97.68ms
step:973/1770 train_time:94073ms step_avg:97.69ms
step:974/1770 train_time:94174ms step_avg:97.69ms
step:975/1770 train_time:94276ms step_avg:97.69ms
step:976/1770 train_time:94376ms step_avg:97.70ms
step:977/1770 train_time:94477ms step_avg:97.70ms
step:978/1770 train_time:94578ms step_avg:97.70ms
step:979/1770 train_time:94679ms step_avg:97.71ms
step:980/1770 train_time:94781ms step_avg:97.71ms
step:981/1770 train_time:94883ms step_avg:97.72ms
step:982/1770 train_time:94986ms step_avg:97.72ms
step:983/1770 train_time:95087ms step_avg:97.73ms
step:984/1770 train_time:95189ms step_avg:97.73ms
step:985/1770 train_time:95290ms step_avg:97.73ms
step:986/1770 train_time:95391ms step_avg:97.74ms
step:987/1770 train_time:95493ms step_avg:97.74ms
step:988/1770 train_time:95595ms step_avg:97.75ms
step:989/1770 train_time:95698ms step_avg:97.75ms
step:990/1770 train_time:95798ms step_avg:97.75ms
step:991/1770 train_time:95899ms step_avg:97.76ms
step:992/1770 train_time:96001ms step_avg:97.76ms
step:993/1770 train_time:96103ms step_avg:97.76ms
step:994/1770 train_time:96205ms step_avg:97.77ms
step:995/1770 train_time:96307ms step_avg:97.77ms
step:996/1770 train_time:96409ms step_avg:97.78ms
step:997/1770 train_time:96510ms step_avg:97.78ms
step:998/1770 train_time:96611ms step_avg:97.78ms
step:999/1770 train_time:96712ms step_avg:97.79ms
step:1000/1770 train_time:96814ms step_avg:97.79ms
step:1000/1770 val_loss:3.5222 train_time:96913ms step_avg:97.89ms
step:1001/1770 train_time:96935ms step_avg:97.81ms
step:1002/1770 train_time:97021ms step_avg:97.80ms
step:1003/1770 train_time:97124ms step_avg:97.81ms
step:1004/1770 train_time:97225ms step_avg:97.81ms
step:1005/1770 train_time:97326ms step_avg:97.82ms
step:1006/1770 train_time:97427ms step_avg:97.82ms
step:1007/1770 train_time:97528ms step_avg:97.82ms
step:1008/1770 train_time:97630ms step_avg:97.83ms
step:1009/1770 train_time:97731ms step_avg:97.83ms
step:1010/1770 train_time:97832ms step_avg:97.83ms
step:1011/1770 train_time:97934ms step_avg:97.84ms
step:1012/1770 train_time:98035ms step_avg:97.84ms
step:1013/1770 train_time:98136ms step_avg:97.84ms
step:1014/1770 train_time:98237ms step_avg:97.85ms
step:1015/1770 train_time:98338ms step_avg:97.85ms
step:1016/1770 train_time:98440ms step_avg:97.85ms
step:1017/1770 train_time:98542ms step_avg:97.86ms
step:1018/1770 train_time:98643ms step_avg:97.86ms
step:1019/1770 train_time:98745ms step_avg:97.86ms
step:1020/1770 train_time:98847ms step_avg:97.87ms
step:1021/1770 train_time:98948ms step_avg:97.87ms
step:1022/1770 train_time:99051ms step_avg:97.88ms
step:1023/1770 train_time:99152ms step_avg:97.88ms
step:1024/1770 train_time:99253ms step_avg:97.88ms
step:1025/1770 train_time:99355ms step_avg:97.89ms
step:1026/1770 train_time:99455ms step_avg:97.89ms
step:1027/1770 train_time:99557ms step_avg:97.89ms
step:1028/1770 train_time:99658ms step_avg:97.90ms
step:1029/1770 train_time:99761ms step_avg:97.90ms
step:1030/1770 train_time:99863ms step_avg:97.90ms
step:1031/1770 train_time:99965ms step_avg:97.91ms
step:1032/1770 train_time:100067ms step_avg:97.91ms
step:1033/1770 train_time:100168ms step_avg:97.92ms
step:1034/1770 train_time:100269ms step_avg:97.92ms
step:1035/1770 train_time:100371ms step_avg:97.92ms
step:1036/1770 train_time:100471ms step_avg:97.93ms
step:1037/1770 train_time:100573ms step_avg:97.93ms
step:1038/1770 train_time:100674ms step_avg:97.93ms
step:1039/1770 train_time:100775ms step_avg:97.93ms
step:1040/1770 train_time:100876ms step_avg:97.94ms
step:1041/1770 train_time:100977ms step_avg:97.94ms
step:1042/1770 train_time:101079ms step_avg:97.95ms
step:1043/1770 train_time:101182ms step_avg:97.95ms
step:1044/1770 train_time:101284ms step_avg:97.95ms
step:1045/1770 train_time:101385ms step_avg:97.96ms
step:1046/1770 train_time:101487ms step_avg:97.96ms
step:1047/1770 train_time:101587ms step_avg:97.96ms
step:1048/1770 train_time:101688ms step_avg:97.97ms
step:1049/1770 train_time:101790ms step_avg:97.97ms
step:1050/1770 train_time:101892ms step_avg:97.97ms
step:1051/1770 train_time:101995ms step_avg:97.98ms
step:1052/1770 train_time:102101ms step_avg:97.99ms
step:1053/1770 train_time:102197ms step_avg:97.98ms
step:1054/1770 train_time:102299ms step_avg:97.99ms
step:1055/1770 train_time:102400ms step_avg:97.99ms
step:1056/1770 train_time:102502ms step_avg:97.99ms
step:1057/1770 train_time:102604ms step_avg:98.00ms
step:1058/1770 train_time:102706ms step_avg:98.00ms
step:1059/1770 train_time:102808ms step_avg:98.01ms
step:1060/1770 train_time:102911ms step_avg:98.01ms
step:1061/1770 train_time:103013ms step_avg:98.01ms
step:1062/1770 train_time:103115ms step_avg:98.02ms
step:1063/1770 train_time:103218ms step_avg:98.02ms
step:1064/1770 train_time:103320ms step_avg:98.03ms
step:1065/1770 train_time:103422ms step_avg:98.03ms
step:1066/1770 train_time:103524ms step_avg:98.03ms
step:1067/1770 train_time:103625ms step_avg:98.04ms
step:1068/1770 train_time:103727ms step_avg:98.04ms
step:1069/1770 train_time:103829ms step_avg:98.04ms
step:1070/1770 train_time:103931ms step_avg:98.05ms
step:1071/1770 train_time:104032ms step_avg:98.05ms
step:1072/1770 train_time:104134ms step_avg:98.05ms
step:1073/1770 train_time:104235ms step_avg:98.06ms
step:1074/1770 train_time:104336ms step_avg:98.06ms
step:1075/1770 train_time:104437ms step_avg:98.06ms
step:1076/1770 train_time:104540ms step_avg:98.07ms
step:1077/1770 train_time:104642ms step_avg:98.07ms
step:1078/1770 train_time:104744ms step_avg:98.08ms
step:1079/1770 train_time:104846ms step_avg:98.08ms
step:1080/1770 train_time:104947ms step_avg:98.08ms
step:1081/1770 train_time:105048ms step_avg:98.08ms
step:1082/1770 train_time:105151ms step_avg:98.09ms
step:1083/1770 train_time:105252ms step_avg:98.09ms
step:1084/1770 train_time:105353ms step_avg:98.09ms
step:1085/1770 train_time:105455ms step_avg:98.10ms
step:1086/1770 train_time:105556ms step_avg:98.10ms
step:1087/1770 train_time:105657ms step_avg:98.10ms
step:1088/1770 train_time:105759ms step_avg:98.11ms
step:1089/1770 train_time:105860ms step_avg:98.11ms
step:1090/1770 train_time:105963ms step_avg:98.11ms
step:1091/1770 train_time:106065ms step_avg:98.12ms
step:1092/1770 train_time:106166ms step_avg:98.12ms
step:1093/1770 train_time:106268ms step_avg:98.12ms
step:1094/1770 train_time:106370ms step_avg:98.13ms
step:1095/1770 train_time:106471ms step_avg:98.13ms
step:1096/1770 train_time:106572ms step_avg:98.13ms
step:1097/1770 train_time:106674ms step_avg:98.14ms
step:1098/1770 train_time:106775ms step_avg:98.14ms
step:1099/1770 train_time:106876ms step_avg:98.14ms
step:1100/1770 train_time:106978ms step_avg:98.14ms
step:1101/1770 train_time:107079ms step_avg:98.15ms
step:1102/1770 train_time:107182ms step_avg:98.15ms
step:1103/1770 train_time:107283ms step_avg:98.16ms
step:1104/1770 train_time:107385ms step_avg:98.16ms
step:1105/1770 train_time:107486ms step_avg:98.16ms
step:1106/1770 train_time:107588ms step_avg:98.16ms
step:1107/1770 train_time:107689ms step_avg:98.17ms
step:1108/1770 train_time:107791ms step_avg:98.17ms
step:1109/1770 train_time:107892ms step_avg:98.17ms
step:1110/1770 train_time:107994ms step_avg:98.18ms
step:1111/1770 train_time:108095ms step_avg:98.18ms
step:1112/1770 train_time:108201ms step_avg:98.19ms
step:1113/1770 train_time:108298ms step_avg:98.19ms
step:1114/1770 train_time:108400ms step_avg:98.19ms
step:1115/1770 train_time:108503ms step_avg:98.19ms
step:1116/1770 train_time:108606ms step_avg:98.20ms
step:1117/1770 train_time:108708ms step_avg:98.20ms
step:1118/1770 train_time:108809ms step_avg:98.20ms
step:1119/1770 train_time:108910ms step_avg:98.21ms
step:1120/1770 train_time:109012ms step_avg:98.21ms
step:1121/1770 train_time:109113ms step_avg:98.21ms
step:1122/1770 train_time:109215ms step_avg:98.21ms
step:1123/1770 train_time:109315ms step_avg:98.22ms
step:1124/1770 train_time:109417ms step_avg:98.22ms
step:1125/1770 train_time:109519ms step_avg:98.22ms
step:1125/1770 val_loss:3.4824 train_time:109621ms step_avg:98.31ms
step:1126/1770 train_time:109642ms step_avg:98.25ms
step:1127/1770 train_time:109729ms step_avg:98.24ms
step:1128/1770 train_time:109830ms step_avg:98.24ms
step:1129/1770 train_time:109932ms step_avg:98.24ms
step:1130/1770 train_time:110033ms step_avg:98.24ms
step:1131/1770 train_time:110135ms step_avg:98.25ms
step:1132/1770 train_time:110237ms step_avg:98.25ms
step:1133/1770 train_time:110338ms step_avg:98.25ms
step:1134/1770 train_time:110440ms step_avg:98.26ms
step:1135/1770 train_time:110542ms step_avg:98.26ms
step:1136/1770 train_time:110644ms step_avg:98.26ms
step:1137/1770 train_time:110746ms step_avg:98.27ms
step:1138/1770 train_time:110847ms step_avg:98.27ms
step:1139/1770 train_time:110949ms step_avg:98.27ms
step:1140/1770 train_time:111050ms step_avg:98.27ms
step:1141/1770 train_time:111151ms step_avg:98.28ms
step:1142/1770 train_time:111253ms step_avg:98.28ms
step:1143/1770 train_time:111355ms step_avg:98.28ms
step:1144/1770 train_time:111457ms step_avg:98.29ms
step:1145/1770 train_time:111558ms step_avg:98.29ms
step:1146/1770 train_time:111660ms step_avg:98.29ms
step:1147/1770 train_time:111763ms step_avg:98.30ms
step:1148/1770 train_time:111866ms step_avg:98.30ms
step:1149/1770 train_time:111966ms step_avg:98.30ms
step:1150/1770 train_time:112067ms step_avg:98.30ms
step:1151/1770 train_time:112169ms step_avg:98.31ms
step:1152/1770 train_time:112271ms step_avg:98.31ms
step:1153/1770 train_time:112373ms step_avg:98.31ms
step:1154/1770 train_time:112474ms step_avg:98.32ms
step:1155/1770 train_time:112576ms step_avg:98.32ms
step:1156/1770 train_time:112677ms step_avg:98.32ms
step:1157/1770 train_time:112780ms step_avg:98.33ms
step:1158/1770 train_time:112881ms step_avg:98.33ms
step:1159/1770 train_time:112984ms step_avg:98.33ms
step:1160/1770 train_time:113086ms step_avg:98.34ms
step:1161/1770 train_time:113187ms step_avg:98.34ms
step:1162/1770 train_time:113289ms step_avg:98.34ms
step:1163/1770 train_time:113391ms step_avg:98.34ms
step:1164/1770 train_time:113492ms step_avg:98.35ms
step:1165/1770 train_time:113594ms step_avg:98.35ms
step:1166/1770 train_time:113696ms step_avg:98.35ms
step:1167/1770 train_time:113798ms step_avg:98.36ms
step:1168/1770 train_time:113900ms step_avg:98.36ms
step:1169/1770 train_time:114002ms step_avg:98.36ms
step:1170/1770 train_time:114103ms step_avg:98.36ms
step:1171/1770 train_time:114205ms step_avg:98.37ms
step:1172/1770 train_time:114306ms step_avg:98.37ms
step:1173/1770 train_time:114408ms step_avg:98.37ms
step:1174/1770 train_time:114509ms step_avg:98.38ms
step:1175/1770 train_time:114610ms step_avg:98.38ms
step:1176/1770 train_time:114713ms step_avg:98.38ms
step:1177/1770 train_time:114817ms step_avg:98.39ms
step:1178/1770 train_time:114919ms step_avg:98.39ms
step:1179/1770 train_time:115020ms step_avg:98.39ms
step:1180/1770 train_time:115122ms step_avg:98.39ms
step:1181/1770 train_time:115223ms step_avg:98.40ms
step:1182/1770 train_time:115325ms step_avg:98.40ms
step:1183/1770 train_time:115427ms step_avg:98.40ms
step:1184/1770 train_time:115531ms step_avg:98.41ms
step:1185/1770 train_time:115633ms step_avg:98.41ms
step:1186/1770 train_time:115737ms step_avg:98.42ms
step:1187/1770 train_time:115842ms step_avg:98.42ms
step:1188/1770 train_time:115945ms step_avg:98.43ms
step:1189/1770 train_time:116048ms step_avg:98.43ms
step:1190/1770 train_time:116150ms step_avg:98.43ms
step:1191/1770 train_time:116252ms step_avg:98.44ms
step:1192/1770 train_time:116356ms step_avg:98.44ms
step:1193/1770 train_time:116458ms step_avg:98.44ms
step:1194/1770 train_time:116561ms step_avg:98.45ms
step:1195/1770 train_time:116666ms step_avg:98.45ms
step:1196/1770 train_time:116770ms step_avg:98.46ms
step:1197/1770 train_time:116872ms step_avg:98.46ms
step:1198/1770 train_time:116974ms step_avg:98.46ms
step:1199/1770 train_time:117078ms step_avg:98.47ms
step:1200/1770 train_time:117181ms step_avg:98.47ms
step:1201/1770 train_time:117284ms step_avg:98.48ms
step:1202/1770 train_time:117386ms step_avg:98.48ms
step:1203/1770 train_time:117488ms step_avg:98.48ms
step:1204/1770 train_time:117592ms step_avg:98.49ms
step:1205/1770 train_time:117693ms step_avg:98.49ms
step:1206/1770 train_time:117796ms step_avg:98.49ms
step:1207/1770 train_time:117899ms step_avg:98.50ms
step:1208/1770 train_time:118001ms step_avg:98.50ms
step:1209/1770 train_time:118104ms step_avg:98.50ms
step:1210/1770 train_time:118205ms step_avg:98.50ms
step:1211/1770 train_time:118309ms step_avg:98.51ms
step:1212/1770 train_time:118414ms step_avg:98.51ms
step:1213/1770 train_time:118517ms step_avg:98.52ms
step:1214/1770 train_time:118618ms step_avg:98.52ms
step:1215/1770 train_time:118721ms step_avg:98.52ms
step:1216/1770 train_time:118826ms step_avg:98.53ms
step:1217/1770 train_time:118928ms step_avg:98.53ms
step:1218/1770 train_time:119030ms step_avg:98.53ms
step:1219/1770 train_time:119133ms step_avg:98.54ms
step:1220/1770 train_time:119238ms step_avg:98.54ms
step:1221/1770 train_time:119340ms step_avg:98.55ms
step:1222/1770 train_time:119444ms step_avg:98.55ms
step:1223/1770 train_time:119546ms step_avg:98.55ms
step:1224/1770 train_time:119655ms step_avg:98.56ms
step:1225/1770 train_time:119753ms step_avg:98.56ms
step:1226/1770 train_time:119855ms step_avg:98.57ms
step:1227/1770 train_time:119960ms step_avg:98.57ms
step:1228/1770 train_time:120066ms step_avg:98.58ms
step:1229/1770 train_time:120168ms step_avg:98.58ms
step:1230/1770 train_time:120270ms step_avg:98.58ms
step:1231/1770 train_time:120373ms step_avg:98.59ms
step:1232/1770 train_time:120476ms step_avg:98.59ms
step:1233/1770 train_time:120578ms step_avg:98.59ms
step:1234/1770 train_time:120681ms step_avg:98.60ms
step:1235/1770 train_time:120784ms step_avg:98.60ms
step:1236/1770 train_time:120888ms step_avg:98.60ms
step:1237/1770 train_time:120990ms step_avg:98.61ms
step:1238/1770 train_time:121093ms step_avg:98.61ms
step:1239/1770 train_time:121196ms step_avg:98.61ms
step:1240/1770 train_time:121299ms step_avg:98.62ms
step:1241/1770 train_time:121404ms step_avg:98.62ms
step:1242/1770 train_time:121506ms step_avg:98.62ms
step:1243/1770 train_time:121609ms step_avg:98.63ms
step:1244/1770 train_time:121711ms step_avg:98.63ms
step:1245/1770 train_time:121815ms step_avg:98.64ms
step:1246/1770 train_time:121919ms step_avg:98.64ms
step:1247/1770 train_time:122021ms step_avg:98.64ms
step:1248/1770 train_time:122124ms step_avg:98.65ms
step:1249/1770 train_time:122227ms step_avg:98.65ms
step:1250/1770 train_time:122330ms step_avg:98.65ms
step:1250/1770 val_loss:3.4330 train_time:122432ms step_avg:98.74ms
step:1251/1770 train_time:122454ms step_avg:98.67ms
step:1252/1770 train_time:122542ms step_avg:98.66ms
step:1253/1770 train_time:122645ms step_avg:98.67ms
step:1254/1770 train_time:122749ms step_avg:98.67ms
step:1255/1770 train_time:122854ms step_avg:98.68ms
step:1256/1770 train_time:122956ms step_avg:98.68ms
step:1257/1770 train_time:123058ms step_avg:98.68ms
step:1258/1770 train_time:123161ms step_avg:98.69ms
step:1259/1770 train_time:123264ms step_avg:98.69ms
step:1260/1770 train_time:123366ms step_avg:98.69ms
step:1261/1770 train_time:123470ms step_avg:98.70ms
step:1262/1770 train_time:123573ms step_avg:98.70ms
step:1263/1770 train_time:123676ms step_avg:98.70ms
step:1264/1770 train_time:123780ms step_avg:98.71ms
step:1265/1770 train_time:123883ms step_avg:98.71ms
step:1266/1770 train_time:123986ms step_avg:98.72ms
step:1267/1770 train_time:124090ms step_avg:98.72ms
step:1268/1770 train_time:124193ms step_avg:98.72ms
step:1269/1770 train_time:124296ms step_avg:98.73ms
step:1270/1770 train_time:124399ms step_avg:98.73ms
step:1271/1770 train_time:124502ms step_avg:98.73ms
step:1272/1770 train_time:124605ms step_avg:98.74ms
step:1273/1770 train_time:124709ms step_avg:98.74ms
step:1274/1770 train_time:124812ms step_avg:98.74ms
step:1275/1770 train_time:124914ms step_avg:98.75ms
step:1276/1770 train_time:125017ms step_avg:98.75ms
step:1277/1770 train_time:125118ms step_avg:98.75ms
step:1278/1770 train_time:125222ms step_avg:98.76ms
step:1279/1770 train_time:125326ms step_avg:98.76ms
step:1280/1770 train_time:125430ms step_avg:98.76ms
step:1281/1770 train_time:125532ms step_avg:98.77ms
step:1282/1770 train_time:125636ms step_avg:98.77ms
step:1283/1770 train_time:125739ms step_avg:98.77ms
step:1284/1770 train_time:125842ms step_avg:98.78ms
step:1285/1770 train_time:125945ms step_avg:98.78ms
step:1286/1770 train_time:126049ms step_avg:98.78ms
step:1287/1770 train_time:126154ms step_avg:98.79ms
step:1288/1770 train_time:126257ms step_avg:98.79ms
step:1289/1770 train_time:126360ms step_avg:98.80ms
step:1290/1770 train_time:126462ms step_avg:98.80ms
step:1291/1770 train_time:126565ms step_avg:98.80ms
step:1292/1770 train_time:126668ms step_avg:98.80ms
step:1293/1770 train_time:126770ms step_avg:98.81ms
step:1294/1770 train_time:126873ms step_avg:98.81ms
step:1295/1770 train_time:126975ms step_avg:98.81ms
step:1296/1770 train_time:127078ms step_avg:98.82ms
step:1297/1770 train_time:127181ms step_avg:98.82ms
step:1298/1770 train_time:127285ms step_avg:98.82ms
step:1299/1770 train_time:127387ms step_avg:98.83ms
step:1300/1770 train_time:127491ms step_avg:98.83ms
step:1301/1770 train_time:127593ms step_avg:98.83ms
step:1302/1770 train_time:127695ms step_avg:98.84ms
step:1303/1770 train_time:127798ms step_avg:98.84ms
step:1304/1770 train_time:127902ms step_avg:98.84ms
step:1305/1770 train_time:128004ms step_avg:98.85ms
step:1306/1770 train_time:128107ms step_avg:98.85ms
step:1307/1770 train_time:128210ms step_avg:98.85ms
step:1308/1770 train_time:128314ms step_avg:98.85ms
step:1309/1770 train_time:128416ms step_avg:98.86ms
step:1310/1770 train_time:128521ms step_avg:98.86ms
step:1311/1770 train_time:128621ms step_avg:98.86ms
step:1312/1770 train_time:128724ms step_avg:98.87ms
step:1313/1770 train_time:128826ms step_avg:98.87ms
step:1314/1770 train_time:128929ms step_avg:98.87ms
step:1315/1770 train_time:129032ms step_avg:98.87ms
step:1316/1770 train_time:129135ms step_avg:98.88ms
step:1317/1770 train_time:129237ms step_avg:98.88ms
step:1318/1770 train_time:129343ms step_avg:98.89ms
step:1319/1770 train_time:129447ms step_avg:98.89ms
step:1320/1770 train_time:129549ms step_avg:98.89ms
step:1321/1770 train_time:129652ms step_avg:98.90ms
step:1322/1770 train_time:129755ms step_avg:98.90ms
step:1323/1770 train_time:129859ms step_avg:98.90ms
step:1324/1770 train_time:129963ms step_avg:98.91ms
step:1325/1770 train_time:130066ms step_avg:98.91ms
step:1326/1770 train_time:130168ms step_avg:98.91ms
step:1327/1770 train_time:130274ms step_avg:98.92ms
step:1328/1770 train_time:130376ms step_avg:98.92ms
step:1329/1770 train_time:130479ms step_avg:98.92ms
step:1330/1770 train_time:130581ms step_avg:98.92ms
step:1331/1770 train_time:130684ms step_avg:98.93ms
step:1332/1770 train_time:130786ms step_avg:98.93ms
step:1333/1770 train_time:130889ms step_avg:98.93ms
step:1334/1770 train_time:130992ms step_avg:98.94ms
step:1335/1770 train_time:131094ms step_avg:98.94ms
step:1336/1770 train_time:131197ms step_avg:98.94ms
step:1337/1770 train_time:131300ms step_avg:98.94ms
step:1338/1770 train_time:131402ms step_avg:98.95ms
step:1339/1770 train_time:131505ms step_avg:98.95ms
step:1340/1770 train_time:131610ms step_avg:98.95ms
step:1341/1770 train_time:131714ms step_avg:98.96ms
step:1342/1770 train_time:131816ms step_avg:98.96ms
step:1343/1770 train_time:131920ms step_avg:98.96ms
step:1344/1770 train_time:132023ms step_avg:98.97ms
step:1345/1770 train_time:132125ms step_avg:98.97ms
step:1346/1770 train_time:132228ms step_avg:98.97ms
step:1347/1770 train_time:132330ms step_avg:98.98ms
step:1348/1770 train_time:132435ms step_avg:98.98ms
step:1349/1770 train_time:132537ms step_avg:98.98ms
step:1350/1770 train_time:132640ms step_avg:98.98ms
step:1351/1770 train_time:132743ms step_avg:98.99ms
step:1352/1770 train_time:132845ms step_avg:98.99ms
step:1353/1770 train_time:132949ms step_avg:98.99ms
step:1354/1770 train_time:133052ms step_avg:99.00ms
step:1355/1770 train_time:133154ms step_avg:99.00ms
step:1356/1770 train_time:133257ms step_avg:99.00ms
step:1357/1770 train_time:133359ms step_avg:99.00ms
step:1358/1770 train_time:133462ms step_avg:99.01ms
step:1359/1770 train_time:133565ms step_avg:99.01ms
step:1360/1770 train_time:133669ms step_avg:99.01ms
step:1361/1770 train_time:133772ms step_avg:99.02ms
step:1362/1770 train_time:133874ms step_avg:99.02ms
step:1363/1770 train_time:133979ms step_avg:99.02ms
step:1364/1770 train_time:134083ms step_avg:99.03ms
step:1365/1770 train_time:134185ms step_avg:99.03ms
step:1366/1770 train_time:134287ms step_avg:99.03ms
step:1367/1770 train_time:134391ms step_avg:99.04ms
step:1368/1770 train_time:134493ms step_avg:99.04ms
step:1369/1770 train_time:134596ms step_avg:99.04ms
step:1370/1770 train_time:134699ms step_avg:99.04ms
step:1371/1770 train_time:134802ms step_avg:99.05ms
step:1372/1770 train_time:134905ms step_avg:99.05ms
step:1373/1770 train_time:135007ms step_avg:99.05ms
step:1374/1770 train_time:135111ms step_avg:99.05ms
step:1375/1770 train_time:135214ms step_avg:99.06ms
step:1375/1770 val_loss:3.3925 train_time:135315ms step_avg:99.13ms
step:1376/1770 train_time:135337ms step_avg:99.08ms
step:1377/1770 train_time:135427ms step_avg:99.07ms
step:1378/1770 train_time:135529ms step_avg:99.07ms
step:1379/1770 train_time:135632ms step_avg:99.07ms
step:1380/1770 train_time:135733ms step_avg:99.08ms
step:1381/1770 train_time:135837ms step_avg:99.08ms
step:1382/1770 train_time:135940ms step_avg:99.08ms
step:1383/1770 train_time:136044ms step_avg:99.08ms
step:1384/1770 train_time:136146ms step_avg:99.09ms
step:1385/1770 train_time:136249ms step_avg:99.09ms
step:1386/1770 train_time:136353ms step_avg:99.09ms
step:1387/1770 train_time:136456ms step_avg:99.10ms
step:1388/1770 train_time:136558ms step_avg:99.10ms
step:1389/1770 train_time:136661ms step_avg:99.10ms
step:1390/1770 train_time:136763ms step_avg:99.10ms
step:1391/1770 train_time:136866ms step_avg:99.11ms
step:1392/1770 train_time:136969ms step_avg:99.11ms
step:1393/1770 train_time:137072ms step_avg:99.11ms
step:1394/1770 train_time:137175ms step_avg:99.11ms
step:1395/1770 train_time:137278ms step_avg:99.12ms
step:1396/1770 train_time:137382ms step_avg:99.12ms
step:1397/1770 train_time:137485ms step_avg:99.12ms
step:1398/1770 train_time:137588ms step_avg:99.13ms
step:1399/1770 train_time:137691ms step_avg:99.13ms
step:1400/1770 train_time:137794ms step_avg:99.13ms
step:1401/1770 train_time:137898ms step_avg:99.14ms
step:1402/1770 train_time:138001ms step_avg:99.14ms
step:1403/1770 train_time:138103ms step_avg:99.14ms
step:1404/1770 train_time:138207ms step_avg:99.14ms
step:1405/1770 train_time:138310ms step_avg:99.15ms
step:1406/1770 train_time:138412ms step_avg:99.15ms
step:1407/1770 train_time:138515ms step_avg:99.15ms
step:1408/1770 train_time:138618ms step_avg:99.15ms
step:1409/1770 train_time:138721ms step_avg:99.16ms
step:1410/1770 train_time:138824ms step_avg:99.16ms
step:1411/1770 train_time:138926ms step_avg:99.16ms
step:1412/1770 train_time:139029ms step_avg:99.16ms
step:1413/1770 train_time:139131ms step_avg:99.17ms
step:1414/1770 train_time:139235ms step_avg:99.17ms
step:1415/1770 train_time:139339ms step_avg:99.17ms
step:1416/1770 train_time:139443ms step_avg:99.18ms
step:1417/1770 train_time:139546ms step_avg:99.18ms
step:1418/1770 train_time:139649ms step_avg:99.18ms
step:1419/1770 train_time:139753ms step_avg:99.19ms
step:1420/1770 train_time:139855ms step_avg:99.19ms
step:1421/1770 train_time:139959ms step_avg:99.19ms
step:1422/1770 train_time:140062ms step_avg:99.19ms
step:1423/1770 train_time:140165ms step_avg:99.20ms
step:1424/1770 train_time:140268ms step_avg:99.20ms
step:1425/1770 train_time:140371ms step_avg:99.20ms
step:1426/1770 train_time:140474ms step_avg:99.20ms
step:1427/1770 train_time:140577ms step_avg:99.21ms
step:1428/1770 train_time:140682ms step_avg:99.21ms
step:1429/1770 train_time:140784ms step_avg:99.21ms
step:1430/1770 train_time:140887ms step_avg:99.22ms
step:1431/1770 train_time:140991ms step_avg:99.22ms
step:1432/1770 train_time:141094ms step_avg:99.22ms
step:1433/1770 train_time:141197ms step_avg:99.22ms
step:1434/1770 train_time:141299ms step_avg:99.23ms
step:1435/1770 train_time:141402ms step_avg:99.23ms
step:1436/1770 train_time:141506ms step_avg:99.23ms
step:1437/1770 train_time:141609ms step_avg:99.24ms
step:1438/1770 train_time:141711ms step_avg:99.24ms
step:1439/1770 train_time:141814ms step_avg:99.24ms
step:1440/1770 train_time:141915ms step_avg:99.24ms
step:1441/1770 train_time:142021ms step_avg:99.25ms
step:1442/1770 train_time:142124ms step_avg:99.25ms
step:1443/1770 train_time:142227ms step_avg:99.25ms
step:1444/1770 train_time:142331ms step_avg:99.25ms
step:1445/1770 train_time:142435ms step_avg:99.26ms
step:1446/1770 train_time:142538ms step_avg:99.26ms
step:1447/1770 train_time:142642ms step_avg:99.26ms
step:1448/1770 train_time:142746ms step_avg:99.27ms
step:1449/1770 train_time:142851ms step_avg:99.27ms
step:1450/1770 train_time:142954ms step_avg:99.27ms
step:1451/1770 train_time:143058ms step_avg:99.28ms
step:1452/1770 train_time:143162ms step_avg:99.28ms
step:1453/1770 train_time:143266ms step_avg:99.28ms
step:1454/1770 train_time:143370ms step_avg:99.29ms
step:1455/1770 train_time:143475ms step_avg:99.29ms
step:1456/1770 train_time:143579ms step_avg:99.29ms
step:1457/1770 train_time:143683ms step_avg:99.30ms
step:1458/1770 train_time:143787ms step_avg:99.30ms
step:1459/1770 train_time:143892ms step_avg:99.30ms
step:1460/1770 train_time:143995ms step_avg:99.31ms
step:1461/1770 train_time:144100ms step_avg:99.31ms
step:1462/1770 train_time:144203ms step_avg:99.31ms
step:1463/1770 train_time:144307ms step_avg:99.32ms
step:1464/1770 train_time:144412ms step_avg:99.32ms
step:1465/1770 train_time:144516ms step_avg:99.32ms
step:1466/1770 train_time:144620ms step_avg:99.33ms
step:1467/1770 train_time:144725ms step_avg:99.33ms
step:1468/1770 train_time:144829ms step_avg:99.33ms
step:1469/1770 train_time:144934ms step_avg:99.34ms
step:1470/1770 train_time:145038ms step_avg:99.34ms
step:1471/1770 train_time:145141ms step_avg:99.34ms
step:1472/1770 train_time:145245ms step_avg:99.35ms
step:1473/1770 train_time:145350ms step_avg:99.35ms
step:1474/1770 train_time:145455ms step_avg:99.35ms
step:1475/1770 train_time:145559ms step_avg:99.36ms
step:1476/1770 train_time:145662ms step_avg:99.36ms
step:1477/1770 train_time:145768ms step_avg:99.37ms
step:1478/1770 train_time:145874ms step_avg:99.37ms
step:1479/1770 train_time:145977ms step_avg:99.37ms
step:1480/1770 train_time:146080ms step_avg:99.37ms
step:1481/1770 train_time:146188ms step_avg:99.38ms
step:1482/1770 train_time:146292ms step_avg:99.38ms
step:1483/1770 train_time:146396ms step_avg:99.39ms
step:1484/1770 train_time:146499ms step_avg:99.39ms
step:1485/1770 train_time:146603ms step_avg:99.39ms
step:1486/1770 train_time:146706ms step_avg:99.39ms
step:1487/1770 train_time:146810ms step_avg:99.40ms
step:1488/1770 train_time:146915ms step_avg:99.40ms
step:1489/1770 train_time:147019ms step_avg:99.40ms
step:1490/1770 train_time:147124ms step_avg:99.41ms
step:1491/1770 train_time:147228ms step_avg:99.41ms
step:1492/1770 train_time:147332ms step_avg:99.41ms
step:1493/1770 train_time:147439ms step_avg:99.42ms
step:1494/1770 train_time:147546ms step_avg:99.42ms
step:1495/1770 train_time:147649ms step_avg:99.43ms
step:1496/1770 train_time:147753ms step_avg:99.43ms
step:1497/1770 train_time:147858ms step_avg:99.43ms
step:1498/1770 train_time:147961ms step_avg:99.44ms
step:1499/1770 train_time:148064ms step_avg:99.44ms
step:1500/1770 train_time:148170ms step_avg:99.44ms
step:1500/1770 val_loss:3.3561 train_time:148270ms step_avg:99.51ms
step:1501/1770 train_time:148291ms step_avg:99.46ms
step:1502/1770 train_time:148382ms step_avg:99.45ms
step:1503/1770 train_time:148486ms step_avg:99.45ms
step:1504/1770 train_time:148590ms step_avg:99.46ms
step:1505/1770 train_time:148696ms step_avg:99.46ms
step:1506/1770 train_time:148800ms step_avg:99.47ms
step:1507/1770 train_time:148905ms step_avg:99.47ms
step:1508/1770 train_time:149010ms step_avg:99.47ms
step:1509/1770 train_time:149115ms step_avg:99.48ms
step:1510/1770 train_time:149218ms step_avg:99.48ms
step:1511/1770 train_time:149323ms step_avg:99.48ms
step:1512/1770 train_time:149428ms step_avg:99.49ms
step:1513/1770 train_time:149533ms step_avg:99.49ms
step:1514/1770 train_time:149636ms step_avg:99.49ms
step:1515/1770 train_time:149740ms step_avg:99.49ms
step:1516/1770 train_time:149844ms step_avg:99.50ms
step:1517/1770 train_time:149948ms step_avg:99.50ms
step:1518/1770 train_time:150054ms step_avg:99.51ms
step:1519/1770 train_time:150158ms step_avg:99.51ms
step:1520/1770 train_time:150264ms step_avg:99.51ms
step:1521/1770 train_time:150367ms step_avg:99.52ms
step:1522/1770 train_time:150472ms step_avg:99.52ms
step:1523/1770 train_time:150578ms step_avg:99.52ms
step:1524/1770 train_time:150682ms step_avg:99.53ms
step:1525/1770 train_time:150786ms step_avg:99.53ms
step:1526/1770 train_time:150890ms step_avg:99.53ms
step:1527/1770 train_time:150994ms step_avg:99.53ms
step:1528/1770 train_time:151100ms step_avg:99.54ms
step:1529/1770 train_time:151204ms step_avg:99.54ms
step:1530/1770 train_time:151307ms step_avg:99.54ms
step:1531/1770 train_time:151411ms step_avg:99.55ms
step:1532/1770 train_time:151516ms step_avg:99.55ms
step:1533/1770 train_time:151620ms step_avg:99.55ms
step:1534/1770 train_time:151725ms step_avg:99.56ms
step:1535/1770 train_time:151829ms step_avg:99.56ms
step:1536/1770 train_time:151932ms step_avg:99.56ms
step:1537/1770 train_time:152036ms step_avg:99.56ms
step:1538/1770 train_time:152141ms step_avg:99.57ms
step:1539/1770 train_time:152245ms step_avg:99.57ms
step:1540/1770 train_time:152351ms step_avg:99.58ms
step:1541/1770 train_time:152456ms step_avg:99.58ms
step:1542/1770 train_time:152560ms step_avg:99.58ms
step:1543/1770 train_time:152664ms step_avg:99.58ms
step:1544/1770 train_time:152770ms step_avg:99.59ms
step:1545/1770 train_time:152874ms step_avg:99.59ms
step:1546/1770 train_time:152978ms step_avg:99.59ms
step:1547/1770 train_time:153082ms step_avg:99.60ms
step:1548/1770 train_time:153186ms step_avg:99.60ms
step:1549/1770 train_time:153291ms step_avg:99.60ms
step:1550/1770 train_time:153395ms step_avg:99.61ms
step:1551/1770 train_time:153498ms step_avg:99.61ms
step:1552/1770 train_time:153604ms step_avg:99.61ms
step:1553/1770 train_time:153708ms step_avg:99.62ms
step:1554/1770 train_time:153811ms step_avg:99.62ms
step:1555/1770 train_time:153916ms step_avg:99.62ms
step:1556/1770 train_time:154019ms step_avg:99.62ms
step:1557/1770 train_time:154122ms step_avg:99.63ms
step:1558/1770 train_time:154227ms step_avg:99.63ms
step:1559/1770 train_time:154331ms step_avg:99.63ms
step:1560/1770 train_time:154435ms step_avg:99.64ms
step:1561/1770 train_time:154541ms step_avg:99.64ms
step:1562/1770 train_time:154645ms step_avg:99.64ms
step:1563/1770 train_time:154749ms step_avg:99.65ms
step:1564/1770 train_time:154852ms step_avg:99.65ms
step:1565/1770 train_time:154957ms step_avg:99.65ms
step:1566/1770 train_time:155060ms step_avg:99.65ms
step:1567/1770 train_time:155164ms step_avg:99.66ms
step:1568/1770 train_time:155268ms step_avg:99.66ms
step:1569/1770 train_time:155376ms step_avg:99.66ms
step:1570/1770 train_time:155479ms step_avg:99.67ms
step:1571/1770 train_time:155583ms step_avg:99.67ms
step:1572/1770 train_time:155688ms step_avg:99.67ms
step:1573/1770 train_time:155794ms step_avg:99.68ms
step:1574/1770 train_time:155898ms step_avg:99.68ms
step:1575/1770 train_time:156001ms step_avg:99.68ms
step:1576/1770 train_time:156105ms step_avg:99.68ms
step:1577/1770 train_time:156210ms step_avg:99.69ms
step:1578/1770 train_time:156315ms step_avg:99.69ms
step:1579/1770 train_time:156419ms step_avg:99.69ms
step:1580/1770 train_time:156523ms step_avg:99.70ms
step:1581/1770 train_time:156630ms step_avg:99.70ms
step:1582/1770 train_time:156735ms step_avg:99.70ms
step:1583/1770 train_time:156838ms step_avg:99.71ms
step:1584/1770 train_time:156943ms step_avg:99.71ms
step:1585/1770 train_time:157047ms step_avg:99.71ms
step:1586/1770 train_time:157155ms step_avg:99.72ms
step:1587/1770 train_time:157259ms step_avg:99.72ms
step:1588/1770 train_time:157363ms step_avg:99.72ms
step:1589/1770 train_time:157470ms step_avg:99.73ms
step:1590/1770 train_time:157573ms step_avg:99.73ms
step:1591/1770 train_time:157677ms step_avg:99.73ms
step:1592/1770 train_time:157782ms step_avg:99.74ms
step:1593/1770 train_time:157885ms step_avg:99.74ms
step:1594/1770 train_time:157990ms step_avg:99.74ms
step:1595/1770 train_time:158094ms step_avg:99.74ms
step:1596/1770 train_time:158199ms step_avg:99.75ms
step:1597/1770 train_time:158303ms step_avg:99.75ms
step:1598/1770 train_time:158407ms step_avg:99.75ms
step:1599/1770 train_time:158512ms step_avg:99.76ms
step:1600/1770 train_time:158619ms step_avg:99.76ms
step:1601/1770 train_time:158724ms step_avg:99.76ms
step:1602/1770 train_time:158829ms step_avg:99.77ms
step:1603/1770 train_time:158933ms step_avg:99.77ms
step:1604/1770 train_time:159036ms step_avg:99.77ms
step:1605/1770 train_time:159139ms step_avg:99.77ms
step:1606/1770 train_time:159243ms step_avg:99.78ms
step:1607/1770 train_time:159351ms step_avg:99.78ms
step:1608/1770 train_time:159455ms step_avg:99.78ms
step:1609/1770 train_time:159558ms step_avg:99.79ms
step:1610/1770 train_time:159663ms step_avg:99.79ms
step:1611/1770 train_time:159770ms step_avg:99.79ms
step:1612/1770 train_time:159876ms step_avg:99.80ms
step:1613/1770 train_time:159980ms step_avg:99.80ms
step:1614/1770 train_time:160084ms step_avg:99.80ms
step:1615/1770 train_time:160188ms step_avg:99.81ms
step:1616/1770 train_time:160292ms step_avg:99.81ms
step:1617/1770 train_time:160399ms step_avg:99.81ms
step:1618/1770 train_time:160504ms step_avg:99.82ms
step:1619/1770 train_time:160609ms step_avg:99.82ms
step:1620/1770 train_time:160713ms step_avg:99.82ms
step:1621/1770 train_time:160817ms step_avg:99.82ms
step:1622/1770 train_time:160921ms step_avg:99.83ms
step:1623/1770 train_time:161028ms step_avg:99.83ms
step:1624/1770 train_time:161132ms step_avg:99.83ms
step:1625/1770 train_time:161235ms step_avg:99.84ms
step:1625/1770 val_loss:3.3243 train_time:161338ms step_avg:99.90ms
step:1626/1770 train_time:161359ms step_avg:99.85ms
step:1627/1770 train_time:161448ms step_avg:99.84ms
step:1628/1770 train_time:161551ms step_avg:99.85ms
step:1629/1770 train_time:161654ms step_avg:99.85ms
step:1630/1770 train_time:161759ms step_avg:99.85ms
step:1631/1770 train_time:161862ms step_avg:99.85ms
step:1632/1770 train_time:161966ms step_avg:99.86ms
step:1633/1770 train_time:162069ms step_avg:99.86ms
step:1634/1770 train_time:162173ms step_avg:99.86ms
step:1635/1770 train_time:162276ms step_avg:99.86ms
step:1636/1770 train_time:162382ms step_avg:99.87ms
step:1637/1770 train_time:162487ms step_avg:99.87ms
step:1638/1770 train_time:162590ms step_avg:99.87ms
step:1639/1770 train_time:162694ms step_avg:99.87ms
step:1640/1770 train_time:162799ms step_avg:99.88ms
step:1641/1770 train_time:162903ms step_avg:99.88ms
step:1642/1770 train_time:163007ms step_avg:99.88ms
step:1643/1770 train_time:163112ms step_avg:99.88ms
step:1644/1770 train_time:163217ms step_avg:99.89ms
step:1645/1770 train_time:163320ms step_avg:99.89ms
step:1646/1770 train_time:163427ms step_avg:99.89ms
step:1647/1770 train_time:163531ms step_avg:99.90ms
step:1648/1770 train_time:163634ms step_avg:99.90ms
step:1649/1770 train_time:163738ms step_avg:99.90ms
step:1650/1770 train_time:163844ms step_avg:99.90ms
step:1651/1770 train_time:163947ms step_avg:99.91ms
step:1652/1770 train_time:164051ms step_avg:99.91ms
step:1653/1770 train_time:164155ms step_avg:99.91ms
step:1654/1770 train_time:164263ms step_avg:99.92ms
step:1655/1770 train_time:164370ms step_avg:99.92ms
step:1656/1770 train_time:164473ms step_avg:99.92ms
step:1657/1770 train_time:164580ms step_avg:99.93ms
step:1658/1770 train_time:164684ms step_avg:99.93ms
step:1659/1770 train_time:164789ms step_avg:99.93ms
step:1660/1770 train_time:164893ms step_avg:99.94ms
step:1661/1770 train_time:164998ms step_avg:99.94ms
step:1662/1770 train_time:165104ms step_avg:99.94ms
step:1663/1770 train_time:165207ms step_avg:99.94ms
step:1664/1770 train_time:165310ms step_avg:99.95ms
step:1665/1770 train_time:165414ms step_avg:99.95ms
step:1666/1770 train_time:165518ms step_avg:99.95ms
step:1667/1770 train_time:165622ms step_avg:99.95ms
step:1668/1770 train_time:165725ms step_avg:99.96ms
step:1669/1770 train_time:165829ms step_avg:99.96ms
step:1670/1770 train_time:165932ms step_avg:99.96ms
step:1671/1770 train_time:166037ms step_avg:99.96ms
step:1672/1770 train_time:166142ms step_avg:99.97ms
step:1673/1770 train_time:166248ms step_avg:99.97ms
step:1674/1770 train_time:166351ms step_avg:99.97ms
step:1675/1770 train_time:166455ms step_avg:99.97ms
step:1676/1770 train_time:166560ms step_avg:99.98ms
step:1677/1770 train_time:166669ms step_avg:99.98ms
step:1678/1770 train_time:166772ms step_avg:99.98ms
step:1679/1770 train_time:166876ms step_avg:99.99ms
step:1680/1770 train_time:166980ms step_avg:99.99ms
step:1681/1770 train_time:167085ms step_avg:99.99ms
step:1682/1770 train_time:167191ms step_avg:99.99ms
step:1683/1770 train_time:167295ms step_avg:100.00ms
step:1684/1770 train_time:167398ms step_avg:100.00ms
step:1685/1770 train_time:167502ms step_avg:100.00ms
step:1686/1770 train_time:167608ms step_avg:100.00ms
step:1687/1770 train_time:167713ms step_avg:100.01ms
step:1688/1770 train_time:167817ms step_avg:100.01ms
step:1689/1770 train_time:167921ms step_avg:100.01ms
step:1690/1770 train_time:168026ms step_avg:100.02ms
step:1691/1770 train_time:168130ms step_avg:100.02ms
step:1692/1770 train_time:168234ms step_avg:100.02ms
step:1693/1770 train_time:168339ms step_avg:100.02ms
step:1694/1770 train_time:168442ms step_avg:100.03ms
step:1695/1770 train_time:168547ms step_avg:100.03ms
step:1696/1770 train_time:168653ms step_avg:100.03ms
step:1697/1770 train_time:168758ms step_avg:100.03ms
step:1698/1770 train_time:168863ms step_avg:100.04ms
step:1699/1770 train_time:168968ms step_avg:100.04ms
step:1700/1770 train_time:169072ms step_avg:100.04ms
step:1701/1770 train_time:169176ms step_avg:100.04ms
step:1702/1770 train_time:169280ms step_avg:100.05ms
step:1703/1770 train_time:169384ms step_avg:100.05ms
step:1704/1770 train_time:169488ms step_avg:100.05ms
step:1705/1770 train_time:169592ms step_avg:100.05ms
step:1706/1770 train_time:169695ms step_avg:100.06ms
step:1707/1770 train_time:169799ms step_avg:100.06ms
step:1708/1770 train_time:169906ms step_avg:100.06ms
step:1709/1770 train_time:170012ms step_avg:100.07ms
step:1710/1770 train_time:170120ms step_avg:100.07ms
step:1711/1770 train_time:170226ms step_avg:100.07ms
step:1712/1770 train_time:170331ms step_avg:100.08ms
step:1713/1770 train_time:170435ms step_avg:100.08ms
step:1714/1770 train_time:170540ms step_avg:100.08ms
step:1715/1770 train_time:170645ms step_avg:100.08ms
step:1716/1770 train_time:170749ms step_avg:100.09ms
step:1717/1770 train_time:170853ms step_avg:100.09ms
step:1718/1770 train_time:170959ms step_avg:100.09ms
step:1719/1770 train_time:171065ms step_avg:100.10ms
step:1720/1770 train_time:171171ms step_avg:100.10ms
step:1721/1770 train_time:171275ms step_avg:100.10ms
step:1722/1770 train_time:171383ms step_avg:100.11ms
step:1723/1770 train_time:171490ms step_avg:100.11ms
step:1724/1770 train_time:171597ms step_avg:100.11ms
step:1725/1770 train_time:171704ms step_avg:100.12ms
step:1726/1770 train_time:171811ms step_avg:100.12ms
step:1727/1770 train_time:171916ms step_avg:100.13ms
step:1728/1770 train_time:172022ms step_avg:100.13ms
step:1729/1770 train_time:172127ms step_avg:100.13ms
step:1730/1770 train_time:172232ms step_avg:100.14ms
step:1731/1770 train_time:172340ms step_avg:100.14ms
step:1732/1770 train_time:172444ms step_avg:100.14ms
step:1733/1770 train_time:172550ms step_avg:100.15ms
step:1734/1770 train_time:172655ms step_avg:100.15ms
step:1735/1770 train_time:172760ms step_avg:100.15ms
step:1736/1770 train_time:172865ms step_avg:100.15ms
step:1737/1770 train_time:172970ms step_avg:100.16ms
step:1738/1770 train_time:173075ms step_avg:100.16ms
step:1739/1770 train_time:173180ms step_avg:100.16ms
step:1740/1770 train_time:173285ms step_avg:100.16ms
step:1741/1770 train_time:173392ms step_avg:100.17ms
step:1742/1770 train_time:173500ms step_avg:100.17ms
step:1743/1770 train_time:173606ms step_avg:100.18ms
step:1744/1770 train_time:173711ms step_avg:100.18ms
step:1745/1770 train_time:173815ms step_avg:100.18ms
step:1746/1770 train_time:173923ms step_avg:100.19ms
step:1747/1770 train_time:174027ms step_avg:100.19ms
step:1748/1770 train_time:174133ms step_avg:100.19ms
step:1749/1770 train_time:174239ms step_avg:100.20ms
step:1750/1770 train_time:174345ms step_avg:100.20ms
step:1750/1770 val_loss:3.2996 train_time:174448ms step_avg:100.26ms
step:1751/1770 train_time:174478ms step_avg:100.22ms
step:1752/1770 train_time:174559ms step_avg:100.21ms
step:1753/1770 train_time:174663ms step_avg:100.21ms
step:1754/1770 train_time:174770ms step_avg:100.21ms
step:1755/1770 train_time:174875ms step_avg:100.21ms
step:1756/1770 train_time:174980ms step_avg:100.22ms
step:1757/1770 train_time:175086ms step_avg:100.22ms
step:1758/1770 train_time:175190ms step_avg:100.22ms
step:1759/1770 train_time:175296ms step_avg:100.23ms
step:1760/1770 train_time:175402ms step_avg:100.23ms
step:1761/1770 train_time:175510ms step_avg:100.23ms
step:1762/1770 train_time:175619ms step_avg:100.24ms
step:1763/1770 train_time:175723ms step_avg:100.24ms
step:1764/1770 train_time:175828ms step_avg:100.24ms
step:1765/1770 train_time:175933ms step_avg:100.25ms
step:1766/1770 train_time:176043ms step_avg:100.25ms
step:1767/1770 train_time:176146ms step_avg:100.25ms
step:1768/1770 train_time:176251ms step_avg:100.26ms
step:1769/1770 train_time:176356ms step_avg:100.26ms
step:1770/1770 train_time:176460ms step_avg:100.26ms
step:1770/1770 val_loss:3.2960 train_time:176565ms step_avg:100.32ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
