import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 forward & backward by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# Custom operators : FP8 forward & bfloat16 backward

@torch.library.custom_op("nanogpt::mm_mixed", mutates_args=())
def mm_mixed_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_mixed_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_mixed_backward", mutates_args=())
def mm_mixed_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        x_bfloat16 = x_f8.to(torch.bfloat16)
        w_bfloat16 = w_f8.to(torch.bfloat16)
        grad_bfloat16 = grad.mul(grad_s).to(torch.bfloat16)
        grad_x = torch._scaled_mm(
            grad_bfloat16,
            w_bfloat16.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        grad_w = torch._scaled_mm(
            x_bfloat16.t().contiguous(),
            grad_bfloat16.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_mixed_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_mixed_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_mixed_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_mixed_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_mixed_op.register_autograd(mm_mixed_backward, setup_context=mm_mixed_setup_context)

def linear_mixed(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm_mixed(_x, w, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 22:23:29 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             116W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24345ms step_avg:nanms
step:2/1770 train_time:24754ms step_avg:nanms
step:3/1770 train_time:24848ms step_avg:nanms
step:4/1770 train_time:24941ms step_avg:nanms
step:5/1770 train_time:25035ms step_avg:nanms
step:6/1770 train_time:25129ms step_avg:nanms
step:7/1770 train_time:25224ms step_avg:nanms
step:8/1770 train_time:25318ms step_avg:nanms
step:9/1770 train_time:25412ms step_avg:nanms
step:10/1770 train_time:25507ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:190ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.75ms
step:14/1770 train_time:378ms step_avg:94.62ms
step:15/1770 train_time:473ms step_avg:94.56ms
step:16/1770 train_time:568ms step_avg:94.63ms
step:17/1770 train_time:662ms step_avg:94.57ms
step:18/1770 train_time:756ms step_avg:94.56ms
step:19/1770 train_time:851ms step_avg:94.58ms
step:20/1770 train_time:946ms step_avg:94.58ms
step:21/1770 train_time:1040ms step_avg:94.52ms
step:22/1770 train_time:1135ms step_avg:94.55ms
step:23/1770 train_time:1229ms step_avg:94.55ms
step:24/1770 train_time:1324ms step_avg:94.58ms
step:25/1770 train_time:1418ms step_avg:94.55ms
step:26/1770 train_time:1513ms step_avg:94.57ms
step:27/1770 train_time:1608ms step_avg:94.59ms
step:28/1770 train_time:1702ms step_avg:94.57ms
step:29/1770 train_time:1797ms step_avg:94.56ms
step:30/1770 train_time:1892ms step_avg:94.59ms
step:31/1770 train_time:1986ms step_avg:94.57ms
step:32/1770 train_time:2080ms step_avg:94.56ms
step:33/1770 train_time:2175ms step_avg:94.55ms
step:34/1770 train_time:2269ms step_avg:94.55ms
step:35/1770 train_time:2363ms step_avg:94.53ms
step:36/1770 train_time:2457ms step_avg:94.52ms
step:37/1770 train_time:2552ms step_avg:94.52ms
step:38/1770 train_time:2647ms step_avg:94.53ms
step:39/1770 train_time:2741ms step_avg:94.52ms
step:40/1770 train_time:2836ms step_avg:94.52ms
step:41/1770 train_time:2931ms step_avg:94.55ms
step:42/1770 train_time:3025ms step_avg:94.53ms
step:43/1770 train_time:3119ms step_avg:94.51ms
step:44/1770 train_time:3214ms step_avg:94.54ms
step:45/1770 train_time:3308ms step_avg:94.52ms
step:46/1770 train_time:3403ms step_avg:94.52ms
step:47/1770 train_time:3497ms step_avg:94.52ms
step:48/1770 train_time:3593ms step_avg:94.54ms
step:49/1770 train_time:3688ms step_avg:94.56ms
step:50/1770 train_time:3782ms step_avg:94.55ms
step:51/1770 train_time:3876ms step_avg:94.54ms
step:52/1770 train_time:3971ms step_avg:94.55ms
step:53/1770 train_time:4065ms step_avg:94.54ms
step:54/1770 train_time:4160ms step_avg:94.53ms
step:55/1770 train_time:4254ms step_avg:94.53ms
step:56/1770 train_time:4348ms step_avg:94.53ms
step:57/1770 train_time:4443ms step_avg:94.53ms
step:58/1770 train_time:4538ms step_avg:94.54ms
step:59/1770 train_time:4632ms step_avg:94.54ms
step:60/1770 train_time:4727ms step_avg:94.54ms
step:61/1770 train_time:4821ms step_avg:94.53ms
step:62/1770 train_time:4916ms step_avg:94.53ms
step:63/1770 train_time:5010ms step_avg:94.53ms
step:64/1770 train_time:5104ms step_avg:94.53ms
step:65/1770 train_time:5199ms step_avg:94.52ms
step:66/1770 train_time:5294ms step_avg:94.53ms
step:67/1770 train_time:5388ms step_avg:94.53ms
step:68/1770 train_time:5483ms step_avg:94.53ms
step:69/1770 train_time:5577ms step_avg:94.52ms
step:70/1770 train_time:5672ms step_avg:94.53ms
step:71/1770 train_time:5767ms step_avg:94.54ms
step:72/1770 train_time:5861ms step_avg:94.53ms
step:73/1770 train_time:5955ms step_avg:94.52ms
step:74/1770 train_time:6049ms step_avg:94.52ms
step:75/1770 train_time:6144ms step_avg:94.52ms
step:76/1770 train_time:6238ms step_avg:94.52ms
step:77/1770 train_time:6333ms step_avg:94.52ms
step:78/1770 train_time:6428ms step_avg:94.53ms
step:79/1770 train_time:6522ms step_avg:94.53ms
step:80/1770 train_time:6617ms step_avg:94.52ms
step:81/1770 train_time:6711ms step_avg:94.53ms
step:82/1770 train_time:6806ms step_avg:94.53ms
step:83/1770 train_time:6900ms step_avg:94.52ms
step:84/1770 train_time:6994ms step_avg:94.52ms
step:85/1770 train_time:7089ms step_avg:94.52ms
step:86/1770 train_time:7184ms step_avg:94.53ms
step:87/1770 train_time:7278ms step_avg:94.52ms
step:88/1770 train_time:7373ms step_avg:94.53ms
step:89/1770 train_time:7468ms step_avg:94.53ms
step:90/1770 train_time:7563ms step_avg:94.53ms
step:91/1770 train_time:7656ms step_avg:94.52ms
step:92/1770 train_time:7751ms step_avg:94.53ms
step:93/1770 train_time:7845ms step_avg:94.52ms
step:94/1770 train_time:7940ms step_avg:94.52ms
step:95/1770 train_time:8034ms step_avg:94.52ms
step:96/1770 train_time:8129ms step_avg:94.52ms
step:97/1770 train_time:8223ms step_avg:94.52ms
step:98/1770 train_time:8317ms step_avg:94.51ms
step:99/1770 train_time:8412ms step_avg:94.51ms
step:100/1770 train_time:8506ms step_avg:94.51ms
step:101/1770 train_time:8600ms step_avg:94.51ms
step:102/1770 train_time:8695ms step_avg:94.51ms
step:103/1770 train_time:8790ms step_avg:94.51ms
step:104/1770 train_time:8884ms step_avg:94.51ms
step:105/1770 train_time:8978ms step_avg:94.50ms
step:106/1770 train_time:9073ms step_avg:94.51ms
step:107/1770 train_time:9168ms step_avg:94.51ms
step:108/1770 train_time:9262ms step_avg:94.51ms
step:109/1770 train_time:9356ms step_avg:94.51ms
step:110/1770 train_time:9451ms step_avg:94.51ms
step:111/1770 train_time:9546ms step_avg:94.51ms
step:112/1770 train_time:9640ms step_avg:94.51ms
step:113/1770 train_time:9734ms step_avg:94.51ms
step:114/1770 train_time:9828ms step_avg:94.50ms
step:115/1770 train_time:9922ms step_avg:94.50ms
step:116/1770 train_time:10017ms step_avg:94.50ms
step:117/1770 train_time:10111ms step_avg:94.50ms
step:118/1770 train_time:10206ms step_avg:94.50ms
step:119/1770 train_time:10300ms step_avg:94.50ms
step:120/1770 train_time:10395ms step_avg:94.50ms
step:121/1770 train_time:10489ms step_avg:94.50ms
step:122/1770 train_time:10584ms step_avg:94.50ms
step:123/1770 train_time:10678ms step_avg:94.49ms
step:124/1770 train_time:10773ms step_avg:94.50ms
step:125/1770 train_time:10868ms step_avg:94.50ms
step:125/1770 val_loss:4.6485 train_time:10960ms step_avg:95.31ms
step:126/1770 train_time:10982ms step_avg:94.67ms
step:127/1770 train_time:11059ms step_avg:94.52ms
step:128/1770 train_time:11159ms step_avg:94.56ms
step:129/1770 train_time:11259ms step_avg:94.62ms
step:130/1770 train_time:11356ms step_avg:94.63ms
step:131/1770 train_time:11450ms step_avg:94.63ms
step:132/1770 train_time:11545ms step_avg:94.63ms
step:133/1770 train_time:11639ms step_avg:94.63ms
step:134/1770 train_time:11734ms step_avg:94.63ms
step:135/1770 train_time:11829ms step_avg:94.63ms
step:136/1770 train_time:11924ms step_avg:94.64ms
step:137/1770 train_time:12019ms step_avg:94.64ms
step:138/1770 train_time:12114ms step_avg:94.64ms
step:139/1770 train_time:12210ms step_avg:94.65ms
step:140/1770 train_time:12305ms step_avg:94.65ms
step:141/1770 train_time:12400ms step_avg:94.65ms
step:142/1770 train_time:12495ms step_avg:94.66ms
step:143/1770 train_time:12591ms step_avg:94.67ms
step:144/1770 train_time:12686ms step_avg:94.67ms
step:145/1770 train_time:12781ms step_avg:94.67ms
step:146/1770 train_time:12876ms step_avg:94.68ms
step:147/1770 train_time:12971ms step_avg:94.68ms
step:148/1770 train_time:13066ms step_avg:94.68ms
step:149/1770 train_time:13161ms step_avg:94.69ms
step:150/1770 train_time:13256ms step_avg:94.69ms
step:151/1770 train_time:13351ms step_avg:94.69ms
step:152/1770 train_time:13447ms step_avg:94.69ms
step:153/1770 train_time:13541ms step_avg:94.70ms
step:154/1770 train_time:13636ms step_avg:94.70ms
step:155/1770 train_time:13732ms step_avg:94.70ms
step:156/1770 train_time:13827ms step_avg:94.70ms
step:157/1770 train_time:13922ms step_avg:94.71ms
step:158/1770 train_time:14017ms step_avg:94.71ms
step:159/1770 train_time:14112ms step_avg:94.71ms
step:160/1770 train_time:14207ms step_avg:94.72ms
step:161/1770 train_time:14303ms step_avg:94.72ms
step:162/1770 train_time:14397ms step_avg:94.72ms
step:163/1770 train_time:14492ms step_avg:94.72ms
step:164/1770 train_time:14588ms step_avg:94.73ms
step:165/1770 train_time:14683ms step_avg:94.73ms
step:166/1770 train_time:14778ms step_avg:94.73ms
step:167/1770 train_time:14873ms step_avg:94.73ms
step:168/1770 train_time:14968ms step_avg:94.73ms
step:169/1770 train_time:15063ms step_avg:94.73ms
step:170/1770 train_time:15157ms step_avg:94.73ms
step:171/1770 train_time:15253ms step_avg:94.74ms
step:172/1770 train_time:15348ms step_avg:94.74ms
step:173/1770 train_time:15443ms step_avg:94.74ms
step:174/1770 train_time:15537ms step_avg:94.74ms
step:175/1770 train_time:15633ms step_avg:94.75ms
step:176/1770 train_time:15729ms step_avg:94.75ms
step:177/1770 train_time:15824ms step_avg:94.75ms
step:178/1770 train_time:15919ms step_avg:94.75ms
step:179/1770 train_time:16014ms step_avg:94.76ms
step:180/1770 train_time:16110ms step_avg:94.77ms
step:181/1770 train_time:16205ms step_avg:94.77ms
step:182/1770 train_time:16300ms step_avg:94.77ms
step:183/1770 train_time:16395ms step_avg:94.77ms
step:184/1770 train_time:16490ms step_avg:94.77ms
step:185/1770 train_time:16585ms step_avg:94.77ms
step:186/1770 train_time:16680ms step_avg:94.77ms
step:187/1770 train_time:16775ms step_avg:94.77ms
step:188/1770 train_time:16870ms step_avg:94.78ms
step:189/1770 train_time:16965ms step_avg:94.78ms
step:190/1770 train_time:17060ms step_avg:94.78ms
step:191/1770 train_time:17155ms step_avg:94.78ms
step:192/1770 train_time:17251ms step_avg:94.78ms
step:193/1770 train_time:17346ms step_avg:94.79ms
step:194/1770 train_time:17441ms step_avg:94.79ms
step:195/1770 train_time:17536ms step_avg:94.79ms
step:196/1770 train_time:17631ms step_avg:94.79ms
step:197/1770 train_time:17727ms step_avg:94.80ms
step:198/1770 train_time:17822ms step_avg:94.80ms
step:199/1770 train_time:17917ms step_avg:94.80ms
step:200/1770 train_time:18012ms step_avg:94.80ms
step:201/1770 train_time:18107ms step_avg:94.80ms
step:202/1770 train_time:18203ms step_avg:94.81ms
step:203/1770 train_time:18297ms step_avg:94.80ms
step:204/1770 train_time:18392ms step_avg:94.81ms
step:205/1770 train_time:18487ms step_avg:94.81ms
step:206/1770 train_time:18582ms step_avg:94.81ms
step:207/1770 train_time:18677ms step_avg:94.81ms
step:208/1770 train_time:18773ms step_avg:94.81ms
step:209/1770 train_time:18868ms step_avg:94.81ms
step:210/1770 train_time:18963ms step_avg:94.82ms
step:211/1770 train_time:19058ms step_avg:94.81ms
step:212/1770 train_time:19153ms step_avg:94.82ms
step:213/1770 train_time:19248ms step_avg:94.82ms
step:214/1770 train_time:19343ms step_avg:94.82ms
step:215/1770 train_time:19438ms step_avg:94.82ms
step:216/1770 train_time:19533ms step_avg:94.82ms
step:217/1770 train_time:19629ms step_avg:94.82ms
step:218/1770 train_time:19723ms step_avg:94.82ms
step:219/1770 train_time:19818ms step_avg:94.82ms
step:220/1770 train_time:19913ms step_avg:94.83ms
step:221/1770 train_time:20009ms step_avg:94.83ms
step:222/1770 train_time:20105ms step_avg:94.83ms
step:223/1770 train_time:20199ms step_avg:94.83ms
step:224/1770 train_time:20294ms step_avg:94.83ms
step:225/1770 train_time:20390ms step_avg:94.84ms
step:226/1770 train_time:20485ms step_avg:94.84ms
step:227/1770 train_time:20580ms step_avg:94.84ms
step:228/1770 train_time:20675ms step_avg:94.84ms
step:229/1770 train_time:20771ms step_avg:94.84ms
step:230/1770 train_time:20866ms step_avg:94.85ms
step:231/1770 train_time:20961ms step_avg:94.85ms
step:232/1770 train_time:21056ms step_avg:94.85ms
step:233/1770 train_time:21153ms step_avg:94.85ms
step:234/1770 train_time:21248ms step_avg:94.86ms
step:235/1770 train_time:21344ms step_avg:94.86ms
step:236/1770 train_time:21439ms step_avg:94.86ms
step:237/1770 train_time:21534ms step_avg:94.86ms
step:238/1770 train_time:21629ms step_avg:94.87ms
step:239/1770 train_time:21725ms step_avg:94.87ms
step:240/1770 train_time:21819ms step_avg:94.87ms
step:241/1770 train_time:21915ms step_avg:94.87ms
step:242/1770 train_time:22010ms step_avg:94.87ms
step:243/1770 train_time:22105ms step_avg:94.87ms
step:244/1770 train_time:22200ms step_avg:94.87ms
step:245/1770 train_time:22295ms step_avg:94.87ms
step:246/1770 train_time:22391ms step_avg:94.88ms
step:247/1770 train_time:22487ms step_avg:94.88ms
step:248/1770 train_time:22582ms step_avg:94.88ms
step:249/1770 train_time:22677ms step_avg:94.88ms
step:250/1770 train_time:22772ms step_avg:94.88ms
step:250/1770 val_loss:4.1133 train_time:22866ms step_avg:95.27ms
step:251/1770 train_time:22887ms step_avg:94.97ms
step:252/1770 train_time:22969ms step_avg:94.91ms
step:253/1770 train_time:23068ms step_avg:94.93ms
step:254/1770 train_time:23163ms step_avg:94.93ms
step:255/1770 train_time:23259ms step_avg:94.93ms
step:256/1770 train_time:23354ms step_avg:94.94ms
step:257/1770 train_time:23448ms step_avg:94.93ms
step:258/1770 train_time:23543ms step_avg:94.93ms
step:259/1770 train_time:23639ms step_avg:94.94ms
step:260/1770 train_time:23734ms step_avg:94.94ms
step:261/1770 train_time:23829ms step_avg:94.94ms
step:262/1770 train_time:23924ms step_avg:94.94ms
step:263/1770 train_time:24019ms step_avg:94.94ms
step:264/1770 train_time:24115ms step_avg:94.94ms
step:265/1770 train_time:24210ms step_avg:94.94ms
step:266/1770 train_time:24306ms step_avg:94.94ms
step:267/1770 train_time:24401ms step_avg:94.95ms
step:268/1770 train_time:24497ms step_avg:94.95ms
step:269/1770 train_time:24592ms step_avg:94.95ms
step:270/1770 train_time:24688ms step_avg:94.95ms
step:271/1770 train_time:24783ms step_avg:94.96ms
step:272/1770 train_time:24880ms step_avg:94.96ms
step:273/1770 train_time:24975ms step_avg:94.96ms
step:274/1770 train_time:25071ms step_avg:94.97ms
step:275/1770 train_time:25166ms step_avg:94.97ms
step:276/1770 train_time:25263ms step_avg:94.97ms
step:277/1770 train_time:25358ms step_avg:94.98ms
step:278/1770 train_time:25455ms step_avg:94.98ms
step:279/1770 train_time:25550ms step_avg:94.98ms
step:280/1770 train_time:25645ms step_avg:94.98ms
step:281/1770 train_time:25741ms step_avg:94.98ms
step:282/1770 train_time:25837ms step_avg:94.99ms
step:283/1770 train_time:25933ms step_avg:94.99ms
step:284/1770 train_time:26028ms step_avg:94.99ms
step:285/1770 train_time:26125ms step_avg:95.00ms
step:286/1770 train_time:26221ms step_avg:95.00ms
step:287/1770 train_time:26317ms step_avg:95.01ms
step:288/1770 train_time:26412ms step_avg:95.01ms
step:289/1770 train_time:26508ms step_avg:95.01ms
step:290/1770 train_time:26603ms step_avg:95.01ms
step:291/1770 train_time:26699ms step_avg:95.01ms
step:292/1770 train_time:26795ms step_avg:95.02ms
step:293/1770 train_time:26891ms step_avg:95.02ms
step:294/1770 train_time:26987ms step_avg:95.02ms
step:295/1770 train_time:27082ms step_avg:95.02ms
step:296/1770 train_time:27178ms step_avg:95.03ms
step:297/1770 train_time:27274ms step_avg:95.03ms
step:298/1770 train_time:27369ms step_avg:95.03ms
step:299/1770 train_time:27465ms step_avg:95.03ms
step:300/1770 train_time:27561ms step_avg:95.04ms
step:301/1770 train_time:27657ms step_avg:95.04ms
step:302/1770 train_time:27752ms step_avg:95.04ms
step:303/1770 train_time:27847ms step_avg:95.04ms
step:304/1770 train_time:27943ms step_avg:95.04ms
step:305/1770 train_time:28039ms step_avg:95.05ms
step:306/1770 train_time:28135ms step_avg:95.05ms
step:307/1770 train_time:28230ms step_avg:95.05ms
step:308/1770 train_time:28326ms step_avg:95.05ms
step:309/1770 train_time:28421ms step_avg:95.05ms
step:310/1770 train_time:28517ms step_avg:95.06ms
step:311/1770 train_time:28612ms step_avg:95.06ms
step:312/1770 train_time:28707ms step_avg:95.06ms
step:313/1770 train_time:28803ms step_avg:95.06ms
step:314/1770 train_time:28899ms step_avg:95.06ms
step:315/1770 train_time:28995ms step_avg:95.06ms
step:316/1770 train_time:29090ms step_avg:95.07ms
step:317/1770 train_time:29186ms step_avg:95.07ms
step:318/1770 train_time:29282ms step_avg:95.07ms
step:319/1770 train_time:29377ms step_avg:95.07ms
step:320/1770 train_time:29473ms step_avg:95.07ms
step:321/1770 train_time:29568ms step_avg:95.08ms
step:322/1770 train_time:29664ms step_avg:95.08ms
step:323/1770 train_time:29759ms step_avg:95.08ms
step:324/1770 train_time:29855ms step_avg:95.08ms
step:325/1770 train_time:29950ms step_avg:95.08ms
step:326/1770 train_time:30045ms step_avg:95.08ms
step:327/1770 train_time:30141ms step_avg:95.08ms
step:328/1770 train_time:30237ms step_avg:95.08ms
step:329/1770 train_time:30333ms step_avg:95.09ms
step:330/1770 train_time:30428ms step_avg:95.09ms
step:331/1770 train_time:30523ms step_avg:95.09ms
step:332/1770 train_time:30619ms step_avg:95.09ms
step:333/1770 train_time:30715ms step_avg:95.09ms
step:334/1770 train_time:30810ms step_avg:95.09ms
step:335/1770 train_time:30905ms step_avg:95.09ms
step:336/1770 train_time:31001ms step_avg:95.09ms
step:337/1770 train_time:31096ms step_avg:95.10ms
step:338/1770 train_time:31192ms step_avg:95.10ms
step:339/1770 train_time:31287ms step_avg:95.10ms
step:340/1770 train_time:31383ms step_avg:95.10ms
step:341/1770 train_time:31479ms step_avg:95.10ms
step:342/1770 train_time:31575ms step_avg:95.10ms
step:343/1770 train_time:31670ms step_avg:95.11ms
step:344/1770 train_time:31766ms step_avg:95.11ms
step:345/1770 train_time:31862ms step_avg:95.11ms
step:346/1770 train_time:31958ms step_avg:95.11ms
step:347/1770 train_time:32053ms step_avg:95.11ms
step:348/1770 train_time:32149ms step_avg:95.12ms
step:349/1770 train_time:32244ms step_avg:95.12ms
step:350/1770 train_time:32340ms step_avg:95.12ms
step:351/1770 train_time:32436ms step_avg:95.12ms
step:352/1770 train_time:32532ms step_avg:95.12ms
step:353/1770 train_time:32627ms step_avg:95.12ms
step:354/1770 train_time:32723ms step_avg:95.12ms
step:355/1770 train_time:32819ms step_avg:95.13ms
step:356/1770 train_time:32915ms step_avg:95.13ms
step:357/1770 train_time:33011ms step_avg:95.13ms
step:358/1770 train_time:33106ms step_avg:95.13ms
step:359/1770 train_time:33201ms step_avg:95.13ms
step:360/1770 train_time:33298ms step_avg:95.14ms
step:361/1770 train_time:33394ms step_avg:95.14ms
step:362/1770 train_time:33489ms step_avg:95.14ms
step:363/1770 train_time:33584ms step_avg:95.14ms
step:364/1770 train_time:33680ms step_avg:95.14ms
step:365/1770 train_time:33776ms step_avg:95.14ms
step:366/1770 train_time:33873ms step_avg:95.15ms
step:367/1770 train_time:33969ms step_avg:95.15ms
step:368/1770 train_time:34065ms step_avg:95.15ms
step:369/1770 train_time:34160ms step_avg:95.15ms
step:370/1770 train_time:34256ms step_avg:95.15ms
step:371/1770 train_time:34351ms step_avg:95.16ms
step:372/1770 train_time:34446ms step_avg:95.16ms
step:373/1770 train_time:34542ms step_avg:95.16ms
step:374/1770 train_time:34638ms step_avg:95.16ms
step:375/1770 train_time:34733ms step_avg:95.16ms
step:375/1770 val_loss:3.9154 train_time:34827ms step_avg:95.42ms
step:376/1770 train_time:34848ms step_avg:95.21ms
step:377/1770 train_time:34933ms step_avg:95.19ms
step:378/1770 train_time:35031ms step_avg:95.19ms
step:379/1770 train_time:35127ms step_avg:95.19ms
step:380/1770 train_time:35222ms step_avg:95.20ms
step:381/1770 train_time:35318ms step_avg:95.20ms
step:382/1770 train_time:35413ms step_avg:95.20ms
step:383/1770 train_time:35509ms step_avg:95.20ms
step:384/1770 train_time:35604ms step_avg:95.20ms
step:385/1770 train_time:35700ms step_avg:95.20ms
step:386/1770 train_time:35796ms step_avg:95.20ms
step:387/1770 train_time:35891ms step_avg:95.20ms
step:388/1770 train_time:35987ms step_avg:95.20ms
step:389/1770 train_time:36083ms step_avg:95.21ms
step:390/1770 train_time:36179ms step_avg:95.21ms
step:391/1770 train_time:36274ms step_avg:95.21ms
step:392/1770 train_time:36369ms step_avg:95.21ms
step:393/1770 train_time:36465ms step_avg:95.21ms
step:394/1770 train_time:36561ms step_avg:95.21ms
step:395/1770 train_time:36657ms step_avg:95.21ms
step:396/1770 train_time:36754ms step_avg:95.22ms
step:397/1770 train_time:36851ms step_avg:95.22ms
step:398/1770 train_time:36949ms step_avg:95.23ms
step:399/1770 train_time:37047ms step_avg:95.24ms
step:400/1770 train_time:37144ms step_avg:95.24ms
step:401/1770 train_time:37242ms step_avg:95.25ms
step:402/1770 train_time:37340ms step_avg:95.25ms
step:403/1770 train_time:37437ms step_avg:95.26ms
step:404/1770 train_time:37534ms step_avg:95.26ms
step:405/1770 train_time:37631ms step_avg:95.27ms
step:406/1770 train_time:37729ms step_avg:95.28ms
step:407/1770 train_time:37827ms step_avg:95.28ms
step:408/1770 train_time:37924ms step_avg:95.29ms
step:409/1770 train_time:38022ms step_avg:95.29ms
step:410/1770 train_time:38119ms step_avg:95.30ms
step:411/1770 train_time:38217ms step_avg:95.30ms
step:412/1770 train_time:38314ms step_avg:95.31ms
step:413/1770 train_time:38411ms step_avg:95.31ms
step:414/1770 train_time:38508ms step_avg:95.32ms
step:415/1770 train_time:38606ms step_avg:95.32ms
step:416/1770 train_time:38704ms step_avg:95.33ms
step:417/1770 train_time:38801ms step_avg:95.33ms
step:418/1770 train_time:38898ms step_avg:95.34ms
step:419/1770 train_time:38996ms step_avg:95.34ms
step:420/1770 train_time:39093ms step_avg:95.35ms
step:421/1770 train_time:39190ms step_avg:95.35ms
step:422/1770 train_time:39287ms step_avg:95.36ms
step:423/1770 train_time:39386ms step_avg:95.36ms
step:424/1770 train_time:39484ms step_avg:95.37ms
step:425/1770 train_time:39582ms step_avg:95.38ms
step:426/1770 train_time:39679ms step_avg:95.38ms
step:427/1770 train_time:39777ms step_avg:95.39ms
step:428/1770 train_time:39874ms step_avg:95.39ms
step:429/1770 train_time:39971ms step_avg:95.40ms
step:430/1770 train_time:40068ms step_avg:95.40ms
step:431/1770 train_time:40166ms step_avg:95.41ms
step:432/1770 train_time:40264ms step_avg:95.41ms
step:433/1770 train_time:40361ms step_avg:95.42ms
step:434/1770 train_time:40459ms step_avg:95.42ms
step:435/1770 train_time:40556ms step_avg:95.43ms
step:436/1770 train_time:40653ms step_avg:95.43ms
step:437/1770 train_time:40751ms step_avg:95.43ms
step:438/1770 train_time:40848ms step_avg:95.44ms
step:439/1770 train_time:40946ms step_avg:95.44ms
step:440/1770 train_time:41044ms step_avg:95.45ms
step:441/1770 train_time:41142ms step_avg:95.46ms
step:442/1770 train_time:41238ms step_avg:95.46ms
step:443/1770 train_time:41336ms step_avg:95.46ms
step:444/1770 train_time:41433ms step_avg:95.47ms
step:445/1770 train_time:41530ms step_avg:95.47ms
step:446/1770 train_time:41628ms step_avg:95.48ms
step:447/1770 train_time:41726ms step_avg:95.48ms
step:448/1770 train_time:41823ms step_avg:95.49ms
step:449/1770 train_time:41922ms step_avg:95.49ms
step:450/1770 train_time:42019ms step_avg:95.50ms
step:451/1770 train_time:42117ms step_avg:95.50ms
step:452/1770 train_time:42214ms step_avg:95.51ms
step:453/1770 train_time:42312ms step_avg:95.51ms
step:454/1770 train_time:42409ms step_avg:95.52ms
step:455/1770 train_time:42507ms step_avg:95.52ms
step:456/1770 train_time:42604ms step_avg:95.53ms
step:457/1770 train_time:42702ms step_avg:95.53ms
step:458/1770 train_time:42800ms step_avg:95.54ms
step:459/1770 train_time:42897ms step_avg:95.54ms
step:460/1770 train_time:42995ms step_avg:95.54ms
step:461/1770 train_time:43092ms step_avg:95.55ms
step:462/1770 train_time:43189ms step_avg:95.55ms
step:463/1770 train_time:43287ms step_avg:95.56ms
step:464/1770 train_time:43385ms step_avg:95.56ms
step:465/1770 train_time:43483ms step_avg:95.57ms
step:466/1770 train_time:43581ms step_avg:95.57ms
step:467/1770 train_time:43678ms step_avg:95.58ms
step:468/1770 train_time:43775ms step_avg:95.58ms
step:469/1770 train_time:43872ms step_avg:95.58ms
step:470/1770 train_time:43970ms step_avg:95.59ms
step:471/1770 train_time:44068ms step_avg:95.59ms
step:472/1770 train_time:44166ms step_avg:95.60ms
step:473/1770 train_time:44264ms step_avg:95.60ms
step:474/1770 train_time:44361ms step_avg:95.61ms
step:475/1770 train_time:44459ms step_avg:95.61ms
step:476/1770 train_time:44556ms step_avg:95.61ms
step:477/1770 train_time:44652ms step_avg:95.62ms
step:478/1770 train_time:44750ms step_avg:95.62ms
step:479/1770 train_time:44847ms step_avg:95.62ms
step:480/1770 train_time:44945ms step_avg:95.63ms
step:481/1770 train_time:45042ms step_avg:95.63ms
step:482/1770 train_time:45140ms step_avg:95.64ms
step:483/1770 train_time:45238ms step_avg:95.64ms
step:484/1770 train_time:45336ms step_avg:95.64ms
step:485/1770 train_time:45433ms step_avg:95.65ms
step:486/1770 train_time:45530ms step_avg:95.65ms
step:487/1770 train_time:45628ms step_avg:95.66ms
step:488/1770 train_time:45725ms step_avg:95.66ms
step:489/1770 train_time:45823ms step_avg:95.66ms
step:490/1770 train_time:45920ms step_avg:95.67ms
step:491/1770 train_time:46017ms step_avg:95.67ms
step:492/1770 train_time:46114ms step_avg:95.67ms
step:493/1770 train_time:46212ms step_avg:95.68ms
step:494/1770 train_time:46309ms step_avg:95.68ms
step:495/1770 train_time:46406ms step_avg:95.68ms
step:496/1770 train_time:46504ms step_avg:95.69ms
step:497/1770 train_time:46602ms step_avg:95.69ms
step:498/1770 train_time:46699ms step_avg:95.70ms
step:499/1770 train_time:46796ms step_avg:95.70ms
step:500/1770 train_time:46894ms step_avg:95.70ms
step:500/1770 val_loss:3.7589 train_time:46989ms step_avg:95.90ms
step:501/1770 train_time:47011ms step_avg:95.74ms
step:502/1770 train_time:47098ms step_avg:95.73ms
step:503/1770 train_time:47201ms step_avg:95.74ms
step:504/1770 train_time:47299ms step_avg:95.75ms
step:505/1770 train_time:47396ms step_avg:95.75ms
step:506/1770 train_time:47494ms step_avg:95.75ms
step:507/1770 train_time:47591ms step_avg:95.76ms
step:508/1770 train_time:47688ms step_avg:95.76ms
step:509/1770 train_time:47785ms step_avg:95.76ms
step:510/1770 train_time:47882ms step_avg:95.76ms
step:511/1770 train_time:47980ms step_avg:95.77ms
step:512/1770 train_time:48078ms step_avg:95.77ms
step:513/1770 train_time:48176ms step_avg:95.78ms
step:514/1770 train_time:48274ms step_avg:95.78ms
step:515/1770 train_time:48372ms step_avg:95.79ms
step:516/1770 train_time:48470ms step_avg:95.79ms
step:517/1770 train_time:48567ms step_avg:95.79ms
step:518/1770 train_time:48664ms step_avg:95.80ms
step:519/1770 train_time:48761ms step_avg:95.80ms
step:520/1770 train_time:48859ms step_avg:95.80ms
step:521/1770 train_time:48957ms step_avg:95.81ms
step:522/1770 train_time:49055ms step_avg:95.81ms
step:523/1770 train_time:49152ms step_avg:95.81ms
step:524/1770 train_time:49249ms step_avg:95.82ms
step:525/1770 train_time:49347ms step_avg:95.82ms
step:526/1770 train_time:49444ms step_avg:95.82ms
step:527/1770 train_time:49542ms step_avg:95.83ms
step:528/1770 train_time:49641ms step_avg:95.83ms
step:529/1770 train_time:49739ms step_avg:95.84ms
step:530/1770 train_time:49838ms step_avg:95.84ms
step:531/1770 train_time:49937ms step_avg:95.85ms
step:532/1770 train_time:50035ms step_avg:95.85ms
step:533/1770 train_time:50133ms step_avg:95.86ms
step:534/1770 train_time:50230ms step_avg:95.86ms
step:535/1770 train_time:50329ms step_avg:95.86ms
step:536/1770 train_time:50426ms step_avg:95.87ms
step:537/1770 train_time:50523ms step_avg:95.87ms
step:538/1770 train_time:50621ms step_avg:95.87ms
step:539/1770 train_time:50720ms step_avg:95.88ms
step:540/1770 train_time:50818ms step_avg:95.88ms
step:541/1770 train_time:50916ms step_avg:95.89ms
step:542/1770 train_time:51014ms step_avg:95.89ms
step:543/1770 train_time:51112ms step_avg:95.90ms
step:544/1770 train_time:51210ms step_avg:95.90ms
step:545/1770 train_time:51308ms step_avg:95.90ms
step:546/1770 train_time:51405ms step_avg:95.91ms
step:547/1770 train_time:51503ms step_avg:95.91ms
step:548/1770 train_time:51601ms step_avg:95.91ms
step:549/1770 train_time:51699ms step_avg:95.92ms
step:550/1770 train_time:51797ms step_avg:95.92ms
step:551/1770 train_time:51895ms step_avg:95.92ms
step:552/1770 train_time:51993ms step_avg:95.93ms
step:553/1770 train_time:52091ms step_avg:95.93ms
step:554/1770 train_time:52188ms step_avg:95.93ms
step:555/1770 train_time:52286ms step_avg:95.94ms
step:556/1770 train_time:52383ms step_avg:95.94ms
step:557/1770 train_time:52481ms step_avg:95.94ms
step:558/1770 train_time:52580ms step_avg:95.95ms
step:559/1770 train_time:52678ms step_avg:95.95ms
step:560/1770 train_time:52776ms step_avg:95.96ms
step:561/1770 train_time:52875ms step_avg:95.96ms
step:562/1770 train_time:52972ms step_avg:95.96ms
step:563/1770 train_time:53070ms step_avg:95.97ms
step:564/1770 train_time:53167ms step_avg:95.97ms
step:565/1770 train_time:53265ms step_avg:95.97ms
step:566/1770 train_time:53363ms step_avg:95.98ms
step:567/1770 train_time:53461ms step_avg:95.98ms
step:568/1770 train_time:53560ms step_avg:95.99ms
step:569/1770 train_time:53658ms step_avg:95.99ms
step:570/1770 train_time:53757ms step_avg:95.99ms
step:571/1770 train_time:53855ms step_avg:96.00ms
step:572/1770 train_time:53954ms step_avg:96.00ms
step:573/1770 train_time:54051ms step_avg:96.01ms
step:574/1770 train_time:54149ms step_avg:96.01ms
step:575/1770 train_time:54247ms step_avg:96.01ms
step:576/1770 train_time:54345ms step_avg:96.02ms
step:577/1770 train_time:54443ms step_avg:96.02ms
step:578/1770 train_time:54541ms step_avg:96.02ms
step:579/1770 train_time:54639ms step_avg:96.03ms
step:580/1770 train_time:54737ms step_avg:96.03ms
step:581/1770 train_time:54835ms step_avg:96.03ms
step:582/1770 train_time:54933ms step_avg:96.04ms
step:583/1770 train_time:55031ms step_avg:96.04ms
step:584/1770 train_time:55128ms step_avg:96.04ms
step:585/1770 train_time:55226ms step_avg:96.05ms
step:586/1770 train_time:55324ms step_avg:96.05ms
step:587/1770 train_time:55421ms step_avg:96.05ms
step:588/1770 train_time:55520ms step_avg:96.05ms
step:589/1770 train_time:55619ms step_avg:96.06ms
step:590/1770 train_time:55717ms step_avg:96.06ms
step:591/1770 train_time:55815ms step_avg:96.07ms
step:592/1770 train_time:55913ms step_avg:96.07ms
step:593/1770 train_time:56011ms step_avg:96.07ms
step:594/1770 train_time:56109ms step_avg:96.08ms
step:595/1770 train_time:56206ms step_avg:96.08ms
step:596/1770 train_time:56304ms step_avg:96.08ms
step:597/1770 train_time:56401ms step_avg:96.08ms
step:598/1770 train_time:56499ms step_avg:96.09ms
step:599/1770 train_time:56597ms step_avg:96.09ms
step:600/1770 train_time:56695ms step_avg:96.09ms
step:601/1770 train_time:56793ms step_avg:96.10ms
step:602/1770 train_time:56891ms step_avg:96.10ms
step:603/1770 train_time:56988ms step_avg:96.10ms
step:604/1770 train_time:57086ms step_avg:96.10ms
step:605/1770 train_time:57184ms step_avg:96.11ms
step:606/1770 train_time:57282ms step_avg:96.11ms
step:607/1770 train_time:57379ms step_avg:96.11ms
step:608/1770 train_time:57478ms step_avg:96.12ms
step:609/1770 train_time:57576ms step_avg:96.12ms
step:610/1770 train_time:57674ms step_avg:96.12ms
step:611/1770 train_time:57772ms step_avg:96.13ms
step:612/1770 train_time:57869ms step_avg:96.13ms
step:613/1770 train_time:57967ms step_avg:96.13ms
step:614/1770 train_time:58064ms step_avg:96.13ms
step:615/1770 train_time:58162ms step_avg:96.14ms
step:616/1770 train_time:58260ms step_avg:96.14ms
step:617/1770 train_time:58359ms step_avg:96.14ms
step:618/1770 train_time:58457ms step_avg:96.15ms
step:619/1770 train_time:58555ms step_avg:96.15ms
step:620/1770 train_time:58653ms step_avg:96.15ms
step:621/1770 train_time:58751ms step_avg:96.15ms
step:622/1770 train_time:58848ms step_avg:96.16ms
step:623/1770 train_time:58946ms step_avg:96.16ms
step:624/1770 train_time:59043ms step_avg:96.16ms
step:625/1770 train_time:59141ms step_avg:96.16ms
step:625/1770 val_loss:3.6706 train_time:59238ms step_avg:96.32ms
step:626/1770 train_time:59260ms step_avg:96.20ms
step:627/1770 train_time:59348ms step_avg:96.19ms
step:628/1770 train_time:59448ms step_avg:96.19ms
step:629/1770 train_time:59546ms step_avg:96.20ms
step:630/1770 train_time:59645ms step_avg:96.20ms
step:631/1770 train_time:59743ms step_avg:96.20ms
step:632/1770 train_time:59841ms step_avg:96.21ms
step:633/1770 train_time:59938ms step_avg:96.21ms
step:634/1770 train_time:60036ms step_avg:96.21ms
step:635/1770 train_time:60133ms step_avg:96.21ms
step:636/1770 train_time:60231ms step_avg:96.22ms
step:637/1770 train_time:60329ms step_avg:96.22ms
step:638/1770 train_time:60427ms step_avg:96.22ms
step:639/1770 train_time:60525ms step_avg:96.22ms
step:640/1770 train_time:60623ms step_avg:96.23ms
step:641/1770 train_time:60721ms step_avg:96.23ms
step:642/1770 train_time:60819ms step_avg:96.23ms
step:643/1770 train_time:60916ms step_avg:96.23ms
step:644/1770 train_time:61014ms step_avg:96.24ms
step:645/1770 train_time:61112ms step_avg:96.24ms
step:646/1770 train_time:61211ms step_avg:96.24ms
step:647/1770 train_time:61309ms step_avg:96.25ms
step:648/1770 train_time:61407ms step_avg:96.25ms
step:649/1770 train_time:61505ms step_avg:96.25ms
step:650/1770 train_time:61603ms step_avg:96.26ms
step:651/1770 train_time:61702ms step_avg:96.26ms
step:652/1770 train_time:61799ms step_avg:96.26ms
step:653/1770 train_time:61897ms step_avg:96.26ms
step:654/1770 train_time:61995ms step_avg:96.27ms
step:655/1770 train_time:62093ms step_avg:96.27ms
step:656/1770 train_time:62190ms step_avg:96.27ms
step:657/1770 train_time:62288ms step_avg:96.27ms
step:658/1770 train_time:62388ms step_avg:96.28ms
step:659/1770 train_time:62488ms step_avg:96.28ms
step:660/1770 train_time:62587ms step_avg:96.29ms
step:661/1770 train_time:62687ms step_avg:96.29ms
step:662/1770 train_time:62787ms step_avg:96.30ms
step:663/1770 train_time:62887ms step_avg:96.30ms
step:664/1770 train_time:62987ms step_avg:96.31ms
step:665/1770 train_time:63087ms step_avg:96.32ms
step:666/1770 train_time:63187ms step_avg:96.32ms
step:667/1770 train_time:63287ms step_avg:96.33ms
step:668/1770 train_time:63388ms step_avg:96.33ms
step:669/1770 train_time:63488ms step_avg:96.34ms
step:670/1770 train_time:63587ms step_avg:96.34ms
step:671/1770 train_time:63687ms step_avg:96.35ms
step:672/1770 train_time:63787ms step_avg:96.35ms
step:673/1770 train_time:63887ms step_avg:96.36ms
step:674/1770 train_time:63987ms step_avg:96.37ms
step:675/1770 train_time:64087ms step_avg:96.37ms
step:676/1770 train_time:64187ms step_avg:96.38ms
step:677/1770 train_time:64288ms step_avg:96.38ms
step:678/1770 train_time:64387ms step_avg:96.39ms
step:679/1770 train_time:64487ms step_avg:96.39ms
step:680/1770 train_time:64587ms step_avg:96.40ms
step:681/1770 train_time:64687ms step_avg:96.40ms
step:682/1770 train_time:64787ms step_avg:96.41ms
step:683/1770 train_time:64887ms step_avg:96.42ms
step:684/1770 train_time:64987ms step_avg:96.42ms
step:685/1770 train_time:65088ms step_avg:96.43ms
step:686/1770 train_time:65187ms step_avg:96.43ms
step:687/1770 train_time:65287ms step_avg:96.44ms
step:688/1770 train_time:65387ms step_avg:96.44ms
step:689/1770 train_time:65487ms step_avg:96.45ms
step:690/1770 train_time:65587ms step_avg:96.45ms
step:691/1770 train_time:65687ms step_avg:96.46ms
step:692/1770 train_time:65787ms step_avg:96.46ms
step:693/1770 train_time:65887ms step_avg:96.47ms
step:694/1770 train_time:65987ms step_avg:96.47ms
step:695/1770 train_time:66087ms step_avg:96.48ms
step:696/1770 train_time:66187ms step_avg:96.48ms
step:697/1770 train_time:66287ms step_avg:96.49ms
step:698/1770 train_time:66387ms step_avg:96.49ms
step:699/1770 train_time:66487ms step_avg:96.50ms
step:700/1770 train_time:66587ms step_avg:96.50ms
step:701/1770 train_time:66687ms step_avg:96.51ms
step:702/1770 train_time:66787ms step_avg:96.51ms
step:703/1770 train_time:66887ms step_avg:96.52ms
step:704/1770 train_time:66987ms step_avg:96.52ms
step:705/1770 train_time:67087ms step_avg:96.53ms
step:706/1770 train_time:67187ms step_avg:96.53ms
step:707/1770 train_time:67288ms step_avg:96.54ms
step:708/1770 train_time:67388ms step_avg:96.54ms
step:709/1770 train_time:67488ms step_avg:96.55ms
step:710/1770 train_time:67587ms step_avg:96.55ms
step:711/1770 train_time:67687ms step_avg:96.56ms
step:712/1770 train_time:67787ms step_avg:96.56ms
step:713/1770 train_time:67888ms step_avg:96.57ms
step:714/1770 train_time:67987ms step_avg:96.57ms
step:715/1770 train_time:68088ms step_avg:96.58ms
step:716/1770 train_time:68187ms step_avg:96.58ms
step:717/1770 train_time:68288ms step_avg:96.59ms
step:718/1770 train_time:68388ms step_avg:96.59ms
step:719/1770 train_time:68487ms step_avg:96.60ms
step:720/1770 train_time:68587ms step_avg:96.60ms
step:721/1770 train_time:68687ms step_avg:96.61ms
step:722/1770 train_time:68787ms step_avg:96.61ms
step:723/1770 train_time:68887ms step_avg:96.62ms
step:724/1770 train_time:68987ms step_avg:96.62ms
step:725/1770 train_time:69086ms step_avg:96.62ms
step:726/1770 train_time:69186ms step_avg:96.63ms
step:727/1770 train_time:69286ms step_avg:96.63ms
step:728/1770 train_time:69387ms step_avg:96.64ms
step:729/1770 train_time:69488ms step_avg:96.64ms
step:730/1770 train_time:69587ms step_avg:96.65ms
step:731/1770 train_time:69687ms step_avg:96.65ms
step:732/1770 train_time:69788ms step_avg:96.66ms
step:733/1770 train_time:69888ms step_avg:96.66ms
step:734/1770 train_time:69988ms step_avg:96.67ms
step:735/1770 train_time:70087ms step_avg:96.67ms
step:736/1770 train_time:70187ms step_avg:96.68ms
step:737/1770 train_time:70287ms step_avg:96.68ms
step:738/1770 train_time:70387ms step_avg:96.68ms
step:739/1770 train_time:70487ms step_avg:96.69ms
step:740/1770 train_time:70587ms step_avg:96.69ms
step:741/1770 train_time:70688ms step_avg:96.70ms
step:742/1770 train_time:70787ms step_avg:96.70ms
step:743/1770 train_time:70887ms step_avg:96.71ms
step:744/1770 train_time:70988ms step_avg:96.71ms
step:745/1770 train_time:71088ms step_avg:96.72ms
step:746/1770 train_time:71187ms step_avg:96.72ms
step:747/1770 train_time:71287ms step_avg:96.73ms
step:748/1770 train_time:71387ms step_avg:96.73ms
step:749/1770 train_time:71487ms step_avg:96.73ms
step:750/1770 train_time:71587ms step_avg:96.74ms
step:750/1770 val_loss:3.6036 train_time:71685ms step_avg:96.87ms
step:751/1770 train_time:71707ms step_avg:96.77ms
step:752/1770 train_time:71791ms step_avg:96.75ms
step:753/1770 train_time:71891ms step_avg:96.76ms
step:754/1770 train_time:71991ms step_avg:96.76ms
step:755/1770 train_time:72090ms step_avg:96.77ms
step:756/1770 train_time:72189ms step_avg:96.77ms
step:757/1770 train_time:72288ms step_avg:96.77ms
step:758/1770 train_time:72388ms step_avg:96.78ms
step:759/1770 train_time:72488ms step_avg:96.78ms
step:760/1770 train_time:72587ms step_avg:96.78ms
step:761/1770 train_time:72686ms step_avg:96.79ms
step:762/1770 train_time:72785ms step_avg:96.79ms
step:763/1770 train_time:72885ms step_avg:96.79ms
step:764/1770 train_time:72983ms step_avg:96.79ms
step:765/1770 train_time:73083ms step_avg:96.80ms
step:766/1770 train_time:73182ms step_avg:96.80ms
step:767/1770 train_time:73282ms step_avg:96.81ms
step:768/1770 train_time:73382ms step_avg:96.81ms
step:769/1770 train_time:73482ms step_avg:96.81ms
step:770/1770 train_time:73582ms step_avg:96.82ms
step:771/1770 train_time:73682ms step_avg:96.82ms
step:772/1770 train_time:73783ms step_avg:96.83ms
step:773/1770 train_time:73883ms step_avg:96.83ms
step:774/1770 train_time:73983ms step_avg:96.84ms
step:775/1770 train_time:74083ms step_avg:96.84ms
step:776/1770 train_time:74183ms step_avg:96.84ms
step:777/1770 train_time:74283ms step_avg:96.85ms
step:778/1770 train_time:74383ms step_avg:96.85ms
step:779/1770 train_time:74483ms step_avg:96.86ms
step:780/1770 train_time:74583ms step_avg:96.86ms
step:781/1770 train_time:74683ms step_avg:96.86ms
step:782/1770 train_time:74783ms step_avg:96.87ms
step:783/1770 train_time:74883ms step_avg:96.87ms
step:784/1770 train_time:74983ms step_avg:96.88ms
step:785/1770 train_time:75083ms step_avg:96.88ms
step:786/1770 train_time:75183ms step_avg:96.89ms
step:787/1770 train_time:75283ms step_avg:96.89ms
step:788/1770 train_time:75382ms step_avg:96.89ms
step:789/1770 train_time:75482ms step_avg:96.90ms
step:790/1770 train_time:75582ms step_avg:96.90ms
step:791/1770 train_time:75683ms step_avg:96.90ms
step:792/1770 train_time:75783ms step_avg:96.91ms
step:793/1770 train_time:75883ms step_avg:96.91ms
step:794/1770 train_time:75983ms step_avg:96.92ms
step:795/1770 train_time:76084ms step_avg:96.92ms
step:796/1770 train_time:76184ms step_avg:96.93ms
step:797/1770 train_time:76283ms step_avg:96.93ms
step:798/1770 train_time:76383ms step_avg:96.93ms
step:799/1770 train_time:76483ms step_avg:96.94ms
step:800/1770 train_time:76583ms step_avg:96.94ms
step:801/1770 train_time:76683ms step_avg:96.94ms
step:802/1770 train_time:76783ms step_avg:96.95ms
step:803/1770 train_time:76884ms step_avg:96.95ms
step:804/1770 train_time:76984ms step_avg:96.96ms
step:805/1770 train_time:77083ms step_avg:96.96ms
step:806/1770 train_time:77184ms step_avg:96.96ms
step:807/1770 train_time:77284ms step_avg:96.97ms
step:808/1770 train_time:77384ms step_avg:96.97ms
step:809/1770 train_time:77484ms step_avg:96.98ms
step:810/1770 train_time:77584ms step_avg:96.98ms
step:811/1770 train_time:77684ms step_avg:96.98ms
step:812/1770 train_time:77784ms step_avg:96.99ms
step:813/1770 train_time:77884ms step_avg:96.99ms
step:814/1770 train_time:77984ms step_avg:96.99ms
step:815/1770 train_time:78084ms step_avg:97.00ms
step:816/1770 train_time:78184ms step_avg:97.00ms
step:817/1770 train_time:78284ms step_avg:97.01ms
step:818/1770 train_time:78384ms step_avg:97.01ms
step:819/1770 train_time:78486ms step_avg:97.02ms
step:820/1770 train_time:78585ms step_avg:97.02ms
step:821/1770 train_time:78684ms step_avg:97.02ms
step:822/1770 train_time:78784ms step_avg:97.02ms
step:823/1770 train_time:78884ms step_avg:97.03ms
step:824/1770 train_time:78983ms step_avg:97.03ms
step:825/1770 train_time:79083ms step_avg:97.03ms
step:826/1770 train_time:79183ms step_avg:97.04ms
step:827/1770 train_time:79283ms step_avg:97.04ms
step:828/1770 train_time:79383ms step_avg:97.05ms
step:829/1770 train_time:79483ms step_avg:97.05ms
step:830/1770 train_time:79583ms step_avg:97.05ms
step:831/1770 train_time:79683ms step_avg:97.06ms
step:832/1770 train_time:79783ms step_avg:97.06ms
step:833/1770 train_time:79883ms step_avg:97.06ms
step:834/1770 train_time:79983ms step_avg:97.07ms
step:835/1770 train_time:80083ms step_avg:97.07ms
step:836/1770 train_time:80183ms step_avg:97.07ms
step:837/1770 train_time:80283ms step_avg:97.08ms
step:838/1770 train_time:80383ms step_avg:97.08ms
step:839/1770 train_time:80483ms step_avg:97.08ms
step:840/1770 train_time:80583ms step_avg:97.09ms
step:841/1770 train_time:80683ms step_avg:97.09ms
step:842/1770 train_time:80783ms step_avg:97.09ms
step:843/1770 train_time:80883ms step_avg:97.10ms
step:844/1770 train_time:80983ms step_avg:97.10ms
step:845/1770 train_time:81083ms step_avg:97.11ms
step:846/1770 train_time:81183ms step_avg:97.11ms
step:847/1770 train_time:81283ms step_avg:97.11ms
step:848/1770 train_time:81383ms step_avg:97.12ms
step:849/1770 train_time:81483ms step_avg:97.12ms
step:850/1770 train_time:81583ms step_avg:97.12ms
step:851/1770 train_time:81683ms step_avg:97.13ms
step:852/1770 train_time:81783ms step_avg:97.13ms
step:853/1770 train_time:81883ms step_avg:97.13ms
step:854/1770 train_time:81983ms step_avg:97.14ms
step:855/1770 train_time:82083ms step_avg:97.14ms
step:856/1770 train_time:82182ms step_avg:97.14ms
step:857/1770 train_time:82282ms step_avg:97.15ms
step:858/1770 train_time:82382ms step_avg:97.15ms
step:859/1770 train_time:82482ms step_avg:97.15ms
step:860/1770 train_time:82582ms step_avg:97.16ms
step:861/1770 train_time:82682ms step_avg:97.16ms
step:862/1770 train_time:82782ms step_avg:97.16ms
step:863/1770 train_time:82882ms step_avg:97.17ms
step:864/1770 train_time:82982ms step_avg:97.17ms
step:865/1770 train_time:83083ms step_avg:97.17ms
step:866/1770 train_time:83183ms step_avg:97.18ms
step:867/1770 train_time:83283ms step_avg:97.18ms
step:868/1770 train_time:83384ms step_avg:97.18ms
step:869/1770 train_time:83484ms step_avg:97.19ms
step:870/1770 train_time:83584ms step_avg:97.19ms
step:871/1770 train_time:83684ms step_avg:97.19ms
step:872/1770 train_time:83784ms step_avg:97.20ms
step:873/1770 train_time:83883ms step_avg:97.20ms
step:874/1770 train_time:83983ms step_avg:97.20ms
step:875/1770 train_time:84083ms step_avg:97.21ms
step:875/1770 val_loss:3.5546 train_time:84181ms step_avg:97.32ms
step:876/1770 train_time:84202ms step_avg:97.23ms
step:877/1770 train_time:84293ms step_avg:97.22ms
step:878/1770 train_time:84394ms step_avg:97.23ms
step:879/1770 train_time:84494ms step_avg:97.23ms
step:880/1770 train_time:84594ms step_avg:97.23ms
step:881/1770 train_time:84693ms step_avg:97.24ms
step:882/1770 train_time:84793ms step_avg:97.24ms
step:883/1770 train_time:84893ms step_avg:97.24ms
step:884/1770 train_time:84993ms step_avg:97.25ms
step:885/1770 train_time:85092ms step_avg:97.25ms
step:886/1770 train_time:85192ms step_avg:97.25ms
step:887/1770 train_time:85292ms step_avg:97.25ms
step:888/1770 train_time:85392ms step_avg:97.26ms
step:889/1770 train_time:85491ms step_avg:97.26ms
step:890/1770 train_time:85590ms step_avg:97.26ms
step:891/1770 train_time:85690ms step_avg:97.26ms
step:892/1770 train_time:85789ms step_avg:97.27ms
step:893/1770 train_time:85889ms step_avg:97.27ms
step:894/1770 train_time:85989ms step_avg:97.27ms
step:895/1770 train_time:86088ms step_avg:97.27ms
step:896/1770 train_time:86188ms step_avg:97.28ms
step:897/1770 train_time:86287ms step_avg:97.28ms
step:898/1770 train_time:86387ms step_avg:97.28ms
step:899/1770 train_time:86487ms step_avg:97.29ms
step:900/1770 train_time:86587ms step_avg:97.29ms
step:901/1770 train_time:86687ms step_avg:97.29ms
step:902/1770 train_time:86788ms step_avg:97.30ms
step:903/1770 train_time:86887ms step_avg:97.30ms
step:904/1770 train_time:86987ms step_avg:97.30ms
step:905/1770 train_time:87087ms step_avg:97.30ms
step:906/1770 train_time:87186ms step_avg:97.31ms
step:907/1770 train_time:87287ms step_avg:97.31ms
step:908/1770 train_time:87386ms step_avg:97.31ms
step:909/1770 train_time:87486ms step_avg:97.31ms
step:910/1770 train_time:87587ms step_avg:97.32ms
step:911/1770 train_time:87687ms step_avg:97.32ms
step:912/1770 train_time:87787ms step_avg:97.32ms
step:913/1770 train_time:87887ms step_avg:97.33ms
step:914/1770 train_time:87987ms step_avg:97.33ms
step:915/1770 train_time:88086ms step_avg:97.33ms
step:916/1770 train_time:88186ms step_avg:97.34ms
step:917/1770 train_time:88286ms step_avg:97.34ms
step:918/1770 train_time:88386ms step_avg:97.34ms
step:919/1770 train_time:88487ms step_avg:97.34ms
step:920/1770 train_time:88588ms step_avg:97.35ms
step:921/1770 train_time:88690ms step_avg:97.35ms
step:922/1770 train_time:88791ms step_avg:97.36ms
step:923/1770 train_time:88892ms step_avg:97.36ms
step:924/1770 train_time:88992ms step_avg:97.37ms
step:925/1770 train_time:89093ms step_avg:97.37ms
step:926/1770 train_time:89194ms step_avg:97.37ms
step:927/1770 train_time:89296ms step_avg:97.38ms
step:928/1770 train_time:89397ms step_avg:97.38ms
step:929/1770 train_time:89498ms step_avg:97.39ms
step:930/1770 train_time:89599ms step_avg:97.39ms
step:931/1770 train_time:89700ms step_avg:97.39ms
step:932/1770 train_time:89802ms step_avg:97.40ms
step:933/1770 train_time:89904ms step_avg:97.40ms
step:934/1770 train_time:90006ms step_avg:97.41ms
step:935/1770 train_time:90107ms step_avg:97.41ms
step:936/1770 train_time:90209ms step_avg:97.42ms
step:937/1770 train_time:90310ms step_avg:97.42ms
step:938/1770 train_time:90411ms step_avg:97.43ms
step:939/1770 train_time:90512ms step_avg:97.43ms
step:940/1770 train_time:90613ms step_avg:97.43ms
step:941/1770 train_time:90714ms step_avg:97.44ms
step:942/1770 train_time:90816ms step_avg:97.44ms
step:943/1770 train_time:90918ms step_avg:97.45ms
step:944/1770 train_time:91018ms step_avg:97.45ms
step:945/1770 train_time:91119ms step_avg:97.45ms
step:946/1770 train_time:91222ms step_avg:97.46ms
step:947/1770 train_time:91324ms step_avg:97.46ms
step:948/1770 train_time:91426ms step_avg:97.47ms
step:949/1770 train_time:91528ms step_avg:97.47ms
step:950/1770 train_time:91630ms step_avg:97.48ms
step:951/1770 train_time:91731ms step_avg:97.48ms
step:952/1770 train_time:91831ms step_avg:97.49ms
step:953/1770 train_time:91933ms step_avg:97.49ms
step:954/1770 train_time:92034ms step_avg:97.49ms
step:955/1770 train_time:92135ms step_avg:97.50ms
step:956/1770 train_time:92236ms step_avg:97.50ms
step:957/1770 train_time:92337ms step_avg:97.50ms
step:958/1770 train_time:92438ms step_avg:97.51ms
step:959/1770 train_time:92540ms step_avg:97.51ms
step:960/1770 train_time:92642ms step_avg:97.52ms
step:961/1770 train_time:92744ms step_avg:97.52ms
step:962/1770 train_time:92846ms step_avg:97.53ms
step:963/1770 train_time:92948ms step_avg:97.53ms
step:964/1770 train_time:93050ms step_avg:97.54ms
step:965/1770 train_time:93150ms step_avg:97.54ms
step:966/1770 train_time:93251ms step_avg:97.54ms
step:967/1770 train_time:93352ms step_avg:97.55ms
step:968/1770 train_time:93453ms step_avg:97.55ms
step:969/1770 train_time:93554ms step_avg:97.55ms
step:970/1770 train_time:93655ms step_avg:97.56ms
step:971/1770 train_time:93756ms step_avg:97.56ms
step:972/1770 train_time:93857ms step_avg:97.56ms
step:973/1770 train_time:93958ms step_avg:97.57ms
step:974/1770 train_time:94059ms step_avg:97.57ms
step:975/1770 train_time:94161ms step_avg:97.58ms
step:976/1770 train_time:94264ms step_avg:97.58ms
step:977/1770 train_time:94366ms step_avg:97.59ms
step:978/1770 train_time:94468ms step_avg:97.59ms
step:979/1770 train_time:94570ms step_avg:97.60ms
step:980/1770 train_time:94671ms step_avg:97.60ms
step:981/1770 train_time:94772ms step_avg:97.60ms
step:982/1770 train_time:94873ms step_avg:97.61ms
step:983/1770 train_time:94974ms step_avg:97.61ms
step:984/1770 train_time:95076ms step_avg:97.61ms
step:985/1770 train_time:95178ms step_avg:97.62ms
step:986/1770 train_time:95279ms step_avg:97.62ms
step:987/1770 train_time:95381ms step_avg:97.63ms
step:988/1770 train_time:95483ms step_avg:97.63ms
step:989/1770 train_time:95587ms step_avg:97.64ms
step:990/1770 train_time:95688ms step_avg:97.64ms
step:991/1770 train_time:95790ms step_avg:97.64ms
step:992/1770 train_time:95891ms step_avg:97.65ms
step:993/1770 train_time:95992ms step_avg:97.65ms
step:994/1770 train_time:96094ms step_avg:97.66ms
step:995/1770 train_time:96195ms step_avg:97.66ms
step:996/1770 train_time:96296ms step_avg:97.66ms
step:997/1770 train_time:96397ms step_avg:97.67ms
step:998/1770 train_time:96497ms step_avg:97.67ms
step:999/1770 train_time:96598ms step_avg:97.67ms
step:1000/1770 train_time:96700ms step_avg:97.68ms
step:1000/1770 val_loss:3.5162 train_time:96801ms step_avg:97.78ms
step:1001/1770 train_time:96823ms step_avg:97.70ms
step:1002/1770 train_time:96914ms step_avg:97.70ms
step:1003/1770 train_time:97018ms step_avg:97.70ms
step:1004/1770 train_time:97120ms step_avg:97.71ms
step:1005/1770 train_time:97221ms step_avg:97.71ms
step:1006/1770 train_time:97322ms step_avg:97.71ms
step:1007/1770 train_time:97423ms step_avg:97.72ms
step:1008/1770 train_time:97525ms step_avg:97.72ms
step:1009/1770 train_time:97626ms step_avg:97.72ms
step:1010/1770 train_time:97727ms step_avg:97.73ms
step:1011/1770 train_time:97830ms step_avg:97.73ms
step:1012/1770 train_time:97931ms step_avg:97.74ms
step:1013/1770 train_time:98032ms step_avg:97.74ms
step:1014/1770 train_time:98133ms step_avg:97.74ms
step:1015/1770 train_time:98234ms step_avg:97.75ms
step:1016/1770 train_time:98335ms step_avg:97.75ms
step:1017/1770 train_time:98436ms step_avg:97.75ms
step:1018/1770 train_time:98537ms step_avg:97.76ms
step:1019/1770 train_time:98639ms step_avg:97.76ms
step:1020/1770 train_time:98741ms step_avg:97.76ms
step:1021/1770 train_time:98843ms step_avg:97.77ms
step:1022/1770 train_time:98945ms step_avg:97.77ms
step:1023/1770 train_time:99047ms step_avg:97.78ms
step:1024/1770 train_time:99148ms step_avg:97.78ms
step:1025/1770 train_time:99249ms step_avg:97.78ms
step:1026/1770 train_time:99351ms step_avg:97.79ms
step:1027/1770 train_time:99452ms step_avg:97.79ms
step:1028/1770 train_time:99553ms step_avg:97.79ms
step:1029/1770 train_time:99653ms step_avg:97.80ms
step:1030/1770 train_time:99755ms step_avg:97.80ms
step:1031/1770 train_time:99856ms step_avg:97.80ms
step:1032/1770 train_time:99957ms step_avg:97.81ms
step:1033/1770 train_time:100059ms step_avg:97.81ms
step:1034/1770 train_time:100161ms step_avg:97.81ms
step:1035/1770 train_time:100263ms step_avg:97.82ms
step:1036/1770 train_time:100365ms step_avg:97.82ms
step:1037/1770 train_time:100466ms step_avg:97.83ms
step:1038/1770 train_time:100567ms step_avg:97.83ms
step:1039/1770 train_time:100668ms step_avg:97.83ms
step:1040/1770 train_time:100770ms step_avg:97.83ms
step:1041/1770 train_time:100870ms step_avg:97.84ms
step:1042/1770 train_time:100971ms step_avg:97.84ms
step:1043/1770 train_time:101072ms step_avg:97.84ms
step:1044/1770 train_time:101173ms step_avg:97.85ms
step:1045/1770 train_time:101274ms step_avg:97.85ms
step:1046/1770 train_time:101374ms step_avg:97.85ms
step:1047/1770 train_time:101477ms step_avg:97.86ms
step:1048/1770 train_time:101578ms step_avg:97.86ms
step:1049/1770 train_time:101679ms step_avg:97.86ms
step:1050/1770 train_time:101781ms step_avg:97.87ms
step:1051/1770 train_time:101884ms step_avg:97.87ms
step:1052/1770 train_time:101987ms step_avg:97.88ms
step:1053/1770 train_time:102088ms step_avg:97.88ms
step:1054/1770 train_time:102189ms step_avg:97.88ms
step:1055/1770 train_time:102290ms step_avg:97.89ms
step:1056/1770 train_time:102391ms step_avg:97.89ms
step:1057/1770 train_time:102491ms step_avg:97.89ms
step:1058/1770 train_time:102593ms step_avg:97.89ms
step:1059/1770 train_time:102695ms step_avg:97.90ms
step:1060/1770 train_time:102796ms step_avg:97.90ms
step:1061/1770 train_time:102898ms step_avg:97.90ms
step:1062/1770 train_time:103000ms step_avg:97.91ms
step:1063/1770 train_time:103104ms step_avg:97.91ms
step:1064/1770 train_time:103206ms step_avg:97.92ms
step:1065/1770 train_time:103307ms step_avg:97.92ms
step:1066/1770 train_time:103409ms step_avg:97.92ms
step:1067/1770 train_time:103510ms step_avg:97.93ms
step:1068/1770 train_time:103612ms step_avg:97.93ms
step:1069/1770 train_time:103713ms step_avg:97.93ms
step:1070/1770 train_time:103814ms step_avg:97.94ms
step:1071/1770 train_time:103915ms step_avg:97.94ms
step:1072/1770 train_time:104016ms step_avg:97.94ms
step:1073/1770 train_time:104117ms step_avg:97.95ms
step:1074/1770 train_time:104218ms step_avg:97.95ms
step:1075/1770 train_time:104320ms step_avg:97.95ms
step:1076/1770 train_time:104424ms step_avg:97.96ms
step:1077/1770 train_time:104526ms step_avg:97.96ms
step:1078/1770 train_time:104628ms step_avg:97.97ms
step:1079/1770 train_time:104729ms step_avg:97.97ms
step:1080/1770 train_time:104830ms step_avg:97.97ms
step:1081/1770 train_time:104931ms step_avg:97.98ms
step:1082/1770 train_time:105032ms step_avg:97.98ms
step:1083/1770 train_time:105134ms step_avg:97.98ms
step:1084/1770 train_time:105235ms step_avg:97.98ms
step:1085/1770 train_time:105336ms step_avg:97.99ms
step:1086/1770 train_time:105438ms step_avg:97.99ms
step:1087/1770 train_time:105539ms step_avg:97.99ms
step:1088/1770 train_time:105641ms step_avg:98.00ms
step:1089/1770 train_time:105742ms step_avg:98.00ms
step:1090/1770 train_time:105844ms step_avg:98.00ms
step:1091/1770 train_time:105946ms step_avg:98.01ms
step:1092/1770 train_time:106048ms step_avg:98.01ms
step:1093/1770 train_time:106150ms step_avg:98.01ms
step:1094/1770 train_time:106252ms step_avg:98.02ms
step:1095/1770 train_time:106353ms step_avg:98.02ms
step:1096/1770 train_time:106454ms step_avg:98.02ms
step:1097/1770 train_time:106554ms step_avg:98.03ms
step:1098/1770 train_time:106656ms step_avg:98.03ms
step:1099/1770 train_time:106757ms step_avg:98.03ms
step:1100/1770 train_time:106859ms step_avg:98.04ms
step:1101/1770 train_time:106961ms step_avg:98.04ms
step:1102/1770 train_time:107063ms step_avg:98.04ms
step:1103/1770 train_time:107166ms step_avg:98.05ms
step:1104/1770 train_time:107269ms step_avg:98.05ms
step:1105/1770 train_time:107370ms step_avg:98.05ms
step:1106/1770 train_time:107471ms step_avg:98.06ms
step:1107/1770 train_time:107573ms step_avg:98.06ms
step:1108/1770 train_time:107674ms step_avg:98.06ms
step:1109/1770 train_time:107775ms step_avg:98.07ms
step:1110/1770 train_time:107876ms step_avg:98.07ms
step:1111/1770 train_time:107978ms step_avg:98.07ms
step:1112/1770 train_time:108080ms step_avg:98.08ms
step:1113/1770 train_time:108181ms step_avg:98.08ms
step:1114/1770 train_time:108285ms step_avg:98.08ms
step:1115/1770 train_time:108387ms step_avg:98.09ms
step:1116/1770 train_time:108489ms step_avg:98.09ms
step:1117/1770 train_time:108591ms step_avg:98.09ms
step:1118/1770 train_time:108692ms step_avg:98.10ms
step:1119/1770 train_time:108793ms step_avg:98.10ms
step:1120/1770 train_time:108894ms step_avg:98.10ms
step:1121/1770 train_time:108995ms step_avg:98.11ms
step:1122/1770 train_time:109096ms step_avg:98.11ms
step:1123/1770 train_time:109196ms step_avg:98.11ms
step:1124/1770 train_time:109297ms step_avg:98.11ms
step:1125/1770 train_time:109399ms step_avg:98.12ms
step:1125/1770 val_loss:3.4767 train_time:109500ms step_avg:98.21ms
step:1126/1770 train_time:109522ms step_avg:98.14ms
step:1127/1770 train_time:109610ms step_avg:98.13ms
step:1128/1770 train_time:109712ms step_avg:98.13ms
step:1129/1770 train_time:109813ms step_avg:98.14ms
step:1130/1770 train_time:109916ms step_avg:98.14ms
step:1131/1770 train_time:110017ms step_avg:98.14ms
step:1132/1770 train_time:110119ms step_avg:98.15ms
step:1133/1770 train_time:110221ms step_avg:98.15ms
step:1134/1770 train_time:110323ms step_avg:98.15ms
step:1135/1770 train_time:110423ms step_avg:98.15ms
step:1136/1770 train_time:110525ms step_avg:98.16ms
step:1137/1770 train_time:110627ms step_avg:98.16ms
step:1138/1770 train_time:110728ms step_avg:98.16ms
step:1139/1770 train_time:110829ms step_avg:98.17ms
step:1140/1770 train_time:110930ms step_avg:98.17ms
step:1141/1770 train_time:111032ms step_avg:98.17ms
step:1142/1770 train_time:111134ms step_avg:98.18ms
step:1143/1770 train_time:111237ms step_avg:98.18ms
step:1144/1770 train_time:111338ms step_avg:98.18ms
step:1145/1770 train_time:111440ms step_avg:98.19ms
step:1146/1770 train_time:111542ms step_avg:98.19ms
step:1147/1770 train_time:111644ms step_avg:98.19ms
step:1148/1770 train_time:111745ms step_avg:98.19ms
step:1149/1770 train_time:111846ms step_avg:98.20ms
step:1150/1770 train_time:111947ms step_avg:98.20ms
step:1151/1770 train_time:112049ms step_avg:98.20ms
step:1152/1770 train_time:112151ms step_avg:98.21ms
step:1153/1770 train_time:112252ms step_avg:98.21ms
step:1154/1770 train_time:112354ms step_avg:98.21ms
step:1155/1770 train_time:112456ms step_avg:98.21ms
step:1156/1770 train_time:112558ms step_avg:98.22ms
step:1157/1770 train_time:112662ms step_avg:98.22ms
step:1158/1770 train_time:112764ms step_avg:98.23ms
step:1159/1770 train_time:112865ms step_avg:98.23ms
step:1160/1770 train_time:112966ms step_avg:98.23ms
step:1161/1770 train_time:113067ms step_avg:98.23ms
step:1162/1770 train_time:113168ms step_avg:98.24ms
step:1163/1770 train_time:113269ms step_avg:98.24ms
step:1164/1770 train_time:113370ms step_avg:98.24ms
step:1165/1770 train_time:113472ms step_avg:98.24ms
step:1166/1770 train_time:113574ms step_avg:98.25ms
step:1167/1770 train_time:113677ms step_avg:98.25ms
step:1168/1770 train_time:113779ms step_avg:98.25ms
step:1169/1770 train_time:113880ms step_avg:98.26ms
step:1170/1770 train_time:113982ms step_avg:98.26ms
step:1171/1770 train_time:114083ms step_avg:98.26ms
step:1172/1770 train_time:114184ms step_avg:98.27ms
step:1173/1770 train_time:114285ms step_avg:98.27ms
step:1174/1770 train_time:114386ms step_avg:98.27ms
step:1175/1770 train_time:114487ms step_avg:98.27ms
step:1176/1770 train_time:114588ms step_avg:98.27ms
step:1177/1770 train_time:114689ms step_avg:98.28ms
step:1178/1770 train_time:114791ms step_avg:98.28ms
step:1179/1770 train_time:114892ms step_avg:98.28ms
step:1180/1770 train_time:114994ms step_avg:98.29ms
step:1181/1770 train_time:115096ms step_avg:98.29ms
step:1182/1770 train_time:115199ms step_avg:98.29ms
step:1183/1770 train_time:115301ms step_avg:98.30ms
step:1184/1770 train_time:115404ms step_avg:98.30ms
step:1185/1770 train_time:115507ms step_avg:98.30ms
step:1186/1770 train_time:115610ms step_avg:98.31ms
step:1187/1770 train_time:115715ms step_avg:98.31ms
step:1188/1770 train_time:115819ms step_avg:98.32ms
step:1189/1770 train_time:115921ms step_avg:98.32ms
step:1190/1770 train_time:116023ms step_avg:98.32ms
step:1191/1770 train_time:116126ms step_avg:98.33ms
step:1192/1770 train_time:116229ms step_avg:98.33ms
step:1193/1770 train_time:116332ms step_avg:98.34ms
step:1194/1770 train_time:116434ms step_avg:98.34ms
step:1195/1770 train_time:116538ms step_avg:98.34ms
step:1196/1770 train_time:116641ms step_avg:98.35ms
step:1197/1770 train_time:116743ms step_avg:98.35ms
step:1198/1770 train_time:116845ms step_avg:98.35ms
step:1199/1770 train_time:116948ms step_avg:98.36ms
step:1200/1770 train_time:117051ms step_avg:98.36ms
step:1201/1770 train_time:117155ms step_avg:98.37ms
step:1202/1770 train_time:117257ms step_avg:98.37ms
step:1203/1770 train_time:117360ms step_avg:98.37ms
step:1204/1770 train_time:117463ms step_avg:98.38ms
step:1205/1770 train_time:117564ms step_avg:98.38ms
step:1206/1770 train_time:117667ms step_avg:98.38ms
step:1207/1770 train_time:117769ms step_avg:98.39ms
step:1208/1770 train_time:117871ms step_avg:98.39ms
step:1209/1770 train_time:117974ms step_avg:98.39ms
step:1210/1770 train_time:118077ms step_avg:98.40ms
step:1211/1770 train_time:118180ms step_avg:98.40ms
step:1212/1770 train_time:118285ms step_avg:98.41ms
step:1213/1770 train_time:118387ms step_avg:98.41ms
step:1214/1770 train_time:118489ms step_avg:98.41ms
step:1215/1770 train_time:118592ms step_avg:98.42ms
step:1216/1770 train_time:118698ms step_avg:98.42ms
step:1217/1770 train_time:118800ms step_avg:98.43ms
step:1218/1770 train_time:118903ms step_avg:98.43ms
step:1219/1770 train_time:119005ms step_avg:98.43ms
step:1220/1770 train_time:119108ms step_avg:98.44ms
step:1221/1770 train_time:119210ms step_avg:98.44ms
step:1222/1770 train_time:119314ms step_avg:98.44ms
step:1223/1770 train_time:119417ms step_avg:98.45ms
step:1224/1770 train_time:119521ms step_avg:98.45ms
step:1225/1770 train_time:119624ms step_avg:98.46ms
step:1226/1770 train_time:119726ms step_avg:98.46ms
step:1227/1770 train_time:119831ms step_avg:98.46ms
step:1228/1770 train_time:119935ms step_avg:98.47ms
step:1229/1770 train_time:120038ms step_avg:98.47ms
step:1230/1770 train_time:120141ms step_avg:98.48ms
step:1231/1770 train_time:120244ms step_avg:98.48ms
step:1232/1770 train_time:120346ms step_avg:98.48ms
step:1233/1770 train_time:120449ms step_avg:98.49ms
step:1234/1770 train_time:120551ms step_avg:98.49ms
step:1235/1770 train_time:120654ms step_avg:98.49ms
step:1236/1770 train_time:120757ms step_avg:98.50ms
step:1237/1770 train_time:120860ms step_avg:98.50ms
step:1238/1770 train_time:120963ms step_avg:98.50ms
step:1239/1770 train_time:121066ms step_avg:98.51ms
step:1240/1770 train_time:121169ms step_avg:98.51ms
step:1241/1770 train_time:121272ms step_avg:98.52ms
step:1242/1770 train_time:121375ms step_avg:98.52ms
step:1243/1770 train_time:121480ms step_avg:98.52ms
step:1244/1770 train_time:121582ms step_avg:98.53ms
step:1245/1770 train_time:121684ms step_avg:98.53ms
step:1246/1770 train_time:121786ms step_avg:98.53ms
step:1247/1770 train_time:121889ms step_avg:98.54ms
step:1248/1770 train_time:121992ms step_avg:98.54ms
step:1249/1770 train_time:122094ms step_avg:98.54ms
step:1250/1770 train_time:122196ms step_avg:98.55ms
step:1250/1770 val_loss:3.4280 train_time:122299ms step_avg:98.63ms
step:1251/1770 train_time:122320ms step_avg:98.57ms
step:1252/1770 train_time:122410ms step_avg:98.56ms
step:1253/1770 train_time:122513ms step_avg:98.56ms
step:1254/1770 train_time:122616ms step_avg:98.57ms
step:1255/1770 train_time:122721ms step_avg:98.57ms
step:1256/1770 train_time:122823ms step_avg:98.57ms
step:1257/1770 train_time:122925ms step_avg:98.58ms
step:1258/1770 train_time:123028ms step_avg:98.58ms
step:1259/1770 train_time:123131ms step_avg:98.58ms
step:1260/1770 train_time:123233ms step_avg:98.59ms
step:1261/1770 train_time:123337ms step_avg:98.59ms
step:1262/1770 train_time:123441ms step_avg:98.59ms
step:1263/1770 train_time:123543ms step_avg:98.60ms
step:1264/1770 train_time:123648ms step_avg:98.60ms
step:1265/1770 train_time:123750ms step_avg:98.61ms
step:1266/1770 train_time:123853ms step_avg:98.61ms
step:1267/1770 train_time:123956ms step_avg:98.61ms
step:1268/1770 train_time:124060ms step_avg:98.62ms
step:1269/1770 train_time:124163ms step_avg:98.62ms
step:1270/1770 train_time:124266ms step_avg:98.62ms
step:1271/1770 train_time:124368ms step_avg:98.63ms
step:1272/1770 train_time:124470ms step_avg:98.63ms
step:1273/1770 train_time:124573ms step_avg:98.63ms
step:1274/1770 train_time:124677ms step_avg:98.64ms
step:1275/1770 train_time:124780ms step_avg:98.64ms
step:1276/1770 train_time:124884ms step_avg:98.64ms
step:1277/1770 train_time:124986ms step_avg:98.65ms
step:1278/1770 train_time:125089ms step_avg:98.65ms
step:1279/1770 train_time:125192ms step_avg:98.65ms
step:1280/1770 train_time:125296ms step_avg:98.66ms
step:1281/1770 train_time:125399ms step_avg:98.66ms
step:1282/1770 train_time:125503ms step_avg:98.67ms
step:1283/1770 train_time:125605ms step_avg:98.67ms
step:1284/1770 train_time:125708ms step_avg:98.67ms
step:1285/1770 train_time:125810ms step_avg:98.67ms
step:1286/1770 train_time:125915ms step_avg:98.68ms
step:1287/1770 train_time:126020ms step_avg:98.68ms
step:1288/1770 train_time:126123ms step_avg:98.69ms
step:1289/1770 train_time:126226ms step_avg:98.69ms
step:1290/1770 train_time:126328ms step_avg:98.69ms
step:1291/1770 train_time:126430ms step_avg:98.70ms
step:1292/1770 train_time:126533ms step_avg:98.70ms
step:1293/1770 train_time:126636ms step_avg:98.70ms
step:1294/1770 train_time:126738ms step_avg:98.71ms
step:1295/1770 train_time:126842ms step_avg:98.71ms
step:1296/1770 train_time:126945ms step_avg:98.71ms
step:1297/1770 train_time:127046ms step_avg:98.72ms
step:1298/1770 train_time:127149ms step_avg:98.72ms
step:1299/1770 train_time:127251ms step_avg:98.72ms
step:1300/1770 train_time:127353ms step_avg:98.72ms
step:1301/1770 train_time:127456ms step_avg:98.73ms
step:1302/1770 train_time:127558ms step_avg:98.73ms
step:1303/1770 train_time:127662ms step_avg:98.73ms
step:1304/1770 train_time:127765ms step_avg:98.74ms
step:1305/1770 train_time:127867ms step_avg:98.74ms
step:1306/1770 train_time:127969ms step_avg:98.74ms
step:1307/1770 train_time:128071ms step_avg:98.74ms
step:1308/1770 train_time:128174ms step_avg:98.75ms
step:1309/1770 train_time:128276ms step_avg:98.75ms
step:1310/1770 train_time:128378ms step_avg:98.75ms
step:1311/1770 train_time:128481ms step_avg:98.76ms
step:1312/1770 train_time:128583ms step_avg:98.76ms
step:1313/1770 train_time:128685ms step_avg:98.76ms
step:1314/1770 train_time:128788ms step_avg:98.76ms
step:1315/1770 train_time:128890ms step_avg:98.77ms
step:1316/1770 train_time:128994ms step_avg:98.77ms
step:1317/1770 train_time:129096ms step_avg:98.77ms
step:1318/1770 train_time:129202ms step_avg:98.78ms
step:1319/1770 train_time:129305ms step_avg:98.78ms
step:1320/1770 train_time:129408ms step_avg:98.78ms
step:1321/1770 train_time:129510ms step_avg:98.79ms
step:1322/1770 train_time:129614ms step_avg:98.79ms
step:1323/1770 train_time:129718ms step_avg:98.79ms
step:1324/1770 train_time:129822ms step_avg:98.80ms
step:1325/1770 train_time:129927ms step_avg:98.80ms
step:1326/1770 train_time:130029ms step_avg:98.81ms
step:1327/1770 train_time:130135ms step_avg:98.81ms
step:1328/1770 train_time:130237ms step_avg:98.81ms
step:1329/1770 train_time:130340ms step_avg:98.82ms
step:1330/1770 train_time:130444ms step_avg:98.82ms
step:1331/1770 train_time:130546ms step_avg:98.82ms
step:1332/1770 train_time:130649ms step_avg:98.83ms
step:1333/1770 train_time:130751ms step_avg:98.83ms
step:1334/1770 train_time:130853ms step_avg:98.83ms
step:1335/1770 train_time:130955ms step_avg:98.83ms
step:1336/1770 train_time:131058ms step_avg:98.84ms
step:1337/1770 train_time:131162ms step_avg:98.84ms
step:1338/1770 train_time:131264ms step_avg:98.84ms
step:1339/1770 train_time:131366ms step_avg:98.85ms
step:1340/1770 train_time:131470ms step_avg:98.85ms
step:1341/1770 train_time:131572ms step_avg:98.85ms
step:1342/1770 train_time:131676ms step_avg:98.86ms
step:1343/1770 train_time:131779ms step_avg:98.86ms
step:1344/1770 train_time:131883ms step_avg:98.86ms
step:1345/1770 train_time:131985ms step_avg:98.87ms
step:1346/1770 train_time:132088ms step_avg:98.87ms
step:1347/1770 train_time:132190ms step_avg:98.87ms
step:1348/1770 train_time:132296ms step_avg:98.88ms
step:1349/1770 train_time:132399ms step_avg:98.88ms
step:1350/1770 train_time:132502ms step_avg:98.88ms
step:1351/1770 train_time:132605ms step_avg:98.89ms
step:1352/1770 train_time:132708ms step_avg:98.89ms
step:1353/1770 train_time:132811ms step_avg:98.89ms
step:1354/1770 train_time:132914ms step_avg:98.89ms
step:1355/1770 train_time:133016ms step_avg:98.90ms
step:1356/1770 train_time:133119ms step_avg:98.90ms
step:1357/1770 train_time:133223ms step_avg:98.90ms
step:1358/1770 train_time:133326ms step_avg:98.91ms
step:1359/1770 train_time:133429ms step_avg:98.91ms
step:1360/1770 train_time:133532ms step_avg:98.91ms
step:1361/1770 train_time:133635ms step_avg:98.92ms
step:1362/1770 train_time:133738ms step_avg:98.92ms
step:1363/1770 train_time:133842ms step_avg:98.92ms
step:1364/1770 train_time:133945ms step_avg:98.93ms
step:1365/1770 train_time:134047ms step_avg:98.93ms
step:1366/1770 train_time:134148ms step_avg:98.93ms
step:1367/1770 train_time:134252ms step_avg:98.93ms
step:1368/1770 train_time:134354ms step_avg:98.94ms
step:1369/1770 train_time:134458ms step_avg:98.94ms
step:1370/1770 train_time:134562ms step_avg:98.94ms
step:1371/1770 train_time:134665ms step_avg:98.95ms
step:1372/1770 train_time:134767ms step_avg:98.95ms
step:1373/1770 train_time:134869ms step_avg:98.95ms
step:1374/1770 train_time:134973ms step_avg:98.95ms
step:1375/1770 train_time:135076ms step_avg:98.96ms
step:1375/1770 val_loss:3.3878 train_time:135178ms step_avg:99.03ms
step:1376/1770 train_time:135199ms step_avg:98.97ms
step:1377/1770 train_time:135290ms step_avg:98.97ms
step:1378/1770 train_time:135393ms step_avg:98.97ms
step:1379/1770 train_time:135495ms step_avg:98.97ms
step:1380/1770 train_time:135597ms step_avg:98.98ms
step:1381/1770 train_time:135700ms step_avg:98.98ms
step:1382/1770 train_time:135803ms step_avg:98.98ms
step:1383/1770 train_time:135906ms step_avg:98.99ms
step:1384/1770 train_time:136010ms step_avg:98.99ms
step:1385/1770 train_time:136113ms step_avg:98.99ms
step:1386/1770 train_time:136217ms step_avg:98.99ms
step:1387/1770 train_time:136320ms step_avg:99.00ms
step:1388/1770 train_time:136422ms step_avg:99.00ms
step:1389/1770 train_time:136525ms step_avg:99.00ms
step:1390/1770 train_time:136628ms step_avg:99.01ms
step:1391/1770 train_time:136732ms step_avg:99.01ms
step:1392/1770 train_time:136834ms step_avg:99.01ms
step:1393/1770 train_time:136937ms step_avg:99.01ms
step:1394/1770 train_time:137039ms step_avg:99.02ms
step:1395/1770 train_time:137142ms step_avg:99.02ms
step:1396/1770 train_time:137247ms step_avg:99.02ms
step:1397/1770 train_time:137350ms step_avg:99.03ms
step:1398/1770 train_time:137453ms step_avg:99.03ms
step:1399/1770 train_time:137555ms step_avg:99.03ms
step:1400/1770 train_time:137658ms step_avg:99.03ms
step:1401/1770 train_time:137761ms step_avg:99.04ms
step:1402/1770 train_time:137864ms step_avg:99.04ms
step:1403/1770 train_time:137967ms step_avg:99.04ms
step:1404/1770 train_time:138070ms step_avg:99.05ms
step:1405/1770 train_time:138173ms step_avg:99.05ms
step:1406/1770 train_time:138276ms step_avg:99.05ms
step:1407/1770 train_time:138378ms step_avg:99.05ms
step:1408/1770 train_time:138481ms step_avg:99.06ms
step:1409/1770 train_time:138583ms step_avg:99.06ms
step:1410/1770 train_time:138686ms step_avg:99.06ms
step:1411/1770 train_time:138790ms step_avg:99.06ms
step:1412/1770 train_time:138893ms step_avg:99.07ms
step:1413/1770 train_time:138995ms step_avg:99.07ms
step:1414/1770 train_time:139098ms step_avg:99.07ms
step:1415/1770 train_time:139201ms step_avg:99.08ms
step:1416/1770 train_time:139306ms step_avg:99.08ms
step:1417/1770 train_time:139410ms step_avg:99.08ms
step:1418/1770 train_time:139512ms step_avg:99.09ms
step:1419/1770 train_time:139615ms step_avg:99.09ms
step:1420/1770 train_time:139717ms step_avg:99.09ms
step:1421/1770 train_time:139820ms step_avg:99.09ms
step:1422/1770 train_time:139922ms step_avg:99.10ms
step:1423/1770 train_time:140025ms step_avg:99.10ms
step:1424/1770 train_time:140130ms step_avg:99.10ms
step:1425/1770 train_time:140232ms step_avg:99.10ms
step:1426/1770 train_time:140335ms step_avg:99.11ms
step:1427/1770 train_time:140437ms step_avg:99.11ms
step:1428/1770 train_time:140541ms step_avg:99.11ms
step:1429/1770 train_time:140645ms step_avg:99.12ms
step:1430/1770 train_time:140747ms step_avg:99.12ms
step:1431/1770 train_time:140851ms step_avg:99.12ms
step:1432/1770 train_time:140953ms step_avg:99.12ms
step:1433/1770 train_time:141056ms step_avg:99.13ms
step:1434/1770 train_time:141158ms step_avg:99.13ms
step:1435/1770 train_time:141260ms step_avg:99.13ms
step:1436/1770 train_time:141364ms step_avg:99.13ms
step:1437/1770 train_time:141467ms step_avg:99.14ms
step:1438/1770 train_time:141569ms step_avg:99.14ms
step:1439/1770 train_time:141672ms step_avg:99.14ms
step:1440/1770 train_time:141775ms step_avg:99.14ms
step:1441/1770 train_time:141881ms step_avg:99.15ms
step:1442/1770 train_time:141983ms step_avg:99.15ms
step:1443/1770 train_time:142087ms step_avg:99.15ms
step:1444/1770 train_time:142190ms step_avg:99.16ms
step:1445/1770 train_time:142294ms step_avg:99.16ms
step:1446/1770 train_time:142397ms step_avg:99.16ms
step:1447/1770 train_time:142501ms step_avg:99.17ms
step:1448/1770 train_time:142605ms step_avg:99.17ms
step:1449/1770 train_time:142710ms step_avg:99.17ms
step:1450/1770 train_time:142814ms step_avg:99.18ms
step:1451/1770 train_time:142918ms step_avg:99.18ms
step:1452/1770 train_time:143022ms step_avg:99.18ms
step:1453/1770 train_time:143126ms step_avg:99.19ms
step:1454/1770 train_time:143231ms step_avg:99.19ms
step:1455/1770 train_time:143336ms step_avg:99.19ms
step:1456/1770 train_time:143440ms step_avg:99.20ms
step:1457/1770 train_time:143544ms step_avg:99.20ms
step:1458/1770 train_time:143649ms step_avg:99.21ms
step:1459/1770 train_time:143755ms step_avg:99.21ms
step:1460/1770 train_time:143858ms step_avg:99.21ms
step:1461/1770 train_time:143962ms step_avg:99.22ms
step:1462/1770 train_time:144066ms step_avg:99.22ms
step:1463/1770 train_time:144171ms step_avg:99.22ms
step:1464/1770 train_time:144277ms step_avg:99.23ms
step:1465/1770 train_time:144380ms step_avg:99.23ms
step:1466/1770 train_time:144484ms step_avg:99.23ms
step:1467/1770 train_time:144590ms step_avg:99.24ms
step:1468/1770 train_time:144694ms step_avg:99.24ms
step:1469/1770 train_time:144797ms step_avg:99.24ms
step:1470/1770 train_time:144901ms step_avg:99.25ms
step:1471/1770 train_time:145005ms step_avg:99.25ms
step:1472/1770 train_time:145109ms step_avg:99.25ms
step:1473/1770 train_time:145214ms step_avg:99.26ms
step:1474/1770 train_time:145319ms step_avg:99.26ms
step:1475/1770 train_time:145424ms step_avg:99.27ms
step:1476/1770 train_time:145528ms step_avg:99.27ms
step:1477/1770 train_time:145634ms step_avg:99.27ms
step:1478/1770 train_time:145739ms step_avg:99.28ms
step:1479/1770 train_time:145843ms step_avg:99.28ms
step:1480/1770 train_time:145948ms step_avg:99.28ms
step:1481/1770 train_time:146056ms step_avg:99.29ms
step:1482/1770 train_time:146159ms step_avg:99.29ms
step:1483/1770 train_time:146263ms step_avg:99.30ms
step:1484/1770 train_time:146367ms step_avg:99.30ms
step:1485/1770 train_time:146471ms step_avg:99.30ms
step:1486/1770 train_time:146574ms step_avg:99.31ms
step:1487/1770 train_time:146678ms step_avg:99.31ms
step:1488/1770 train_time:146783ms step_avg:99.31ms
step:1489/1770 train_time:146888ms step_avg:99.32ms
step:1490/1770 train_time:146993ms step_avg:99.32ms
step:1491/1770 train_time:147098ms step_avg:99.32ms
step:1492/1770 train_time:147201ms step_avg:99.33ms
step:1493/1770 train_time:147308ms step_avg:99.33ms
step:1494/1770 train_time:147416ms step_avg:99.34ms
step:1495/1770 train_time:147520ms step_avg:99.34ms
step:1496/1770 train_time:147623ms step_avg:99.34ms
step:1497/1770 train_time:147728ms step_avg:99.35ms
step:1498/1770 train_time:147831ms step_avg:99.35ms
step:1499/1770 train_time:147935ms step_avg:99.35ms
step:1500/1770 train_time:148038ms step_avg:99.35ms
step:1500/1770 val_loss:3.3509 train_time:148140ms step_avg:99.42ms
step:1501/1770 train_time:148161ms step_avg:99.37ms
step:1502/1770 train_time:148253ms step_avg:99.37ms
step:1503/1770 train_time:148356ms step_avg:99.37ms
step:1504/1770 train_time:148460ms step_avg:99.37ms
step:1505/1770 train_time:148567ms step_avg:99.38ms
step:1506/1770 train_time:148671ms step_avg:99.38ms
step:1507/1770 train_time:148776ms step_avg:99.38ms
step:1508/1770 train_time:148881ms step_avg:99.39ms
step:1509/1770 train_time:148985ms step_avg:99.39ms
step:1510/1770 train_time:149088ms step_avg:99.39ms
step:1511/1770 train_time:149193ms step_avg:99.40ms
step:1512/1770 train_time:149297ms step_avg:99.40ms
step:1513/1770 train_time:149401ms step_avg:99.40ms
step:1514/1770 train_time:149505ms step_avg:99.40ms
step:1515/1770 train_time:149609ms step_avg:99.41ms
step:1516/1770 train_time:149713ms step_avg:99.41ms
step:1517/1770 train_time:149817ms step_avg:99.41ms
step:1518/1770 train_time:149924ms step_avg:99.42ms
step:1519/1770 train_time:150028ms step_avg:99.42ms
step:1520/1770 train_time:150132ms step_avg:99.43ms
step:1521/1770 train_time:150236ms step_avg:99.43ms
step:1522/1770 train_time:150340ms step_avg:99.43ms
step:1523/1770 train_time:150446ms step_avg:99.44ms
step:1524/1770 train_time:150549ms step_avg:99.44ms
step:1525/1770 train_time:150653ms step_avg:99.44ms
step:1526/1770 train_time:150756ms step_avg:99.44ms
step:1527/1770 train_time:150860ms step_avg:99.45ms
step:1528/1770 train_time:150967ms step_avg:99.45ms
step:1529/1770 train_time:151071ms step_avg:99.45ms
step:1530/1770 train_time:151174ms step_avg:99.46ms
step:1531/1770 train_time:151277ms step_avg:99.46ms
step:1532/1770 train_time:151382ms step_avg:99.46ms
step:1533/1770 train_time:151487ms step_avg:99.47ms
step:1534/1770 train_time:151592ms step_avg:99.47ms
step:1535/1770 train_time:151695ms step_avg:99.47ms
step:1536/1770 train_time:151798ms step_avg:99.47ms
step:1537/1770 train_time:151903ms step_avg:99.48ms
step:1538/1770 train_time:152009ms step_avg:99.48ms
step:1539/1770 train_time:152112ms step_avg:99.48ms
step:1540/1770 train_time:152219ms step_avg:99.49ms
step:1541/1770 train_time:152324ms step_avg:99.49ms
step:1542/1770 train_time:152429ms step_avg:99.50ms
step:1543/1770 train_time:152532ms step_avg:99.50ms
step:1544/1770 train_time:152638ms step_avg:99.50ms
step:1545/1770 train_time:152743ms step_avg:99.51ms
step:1546/1770 train_time:152847ms step_avg:99.51ms
step:1547/1770 train_time:152951ms step_avg:99.51ms
step:1548/1770 train_time:153054ms step_avg:99.52ms
step:1549/1770 train_time:153158ms step_avg:99.52ms
step:1550/1770 train_time:153262ms step_avg:99.52ms
step:1551/1770 train_time:153366ms step_avg:99.52ms
step:1552/1770 train_time:153472ms step_avg:99.53ms
step:1553/1770 train_time:153577ms step_avg:99.53ms
step:1554/1770 train_time:153679ms step_avg:99.53ms
step:1555/1770 train_time:153784ms step_avg:99.54ms
step:1556/1770 train_time:153888ms step_avg:99.54ms
step:1557/1770 train_time:153992ms step_avg:99.54ms
step:1558/1770 train_time:154096ms step_avg:99.55ms
step:1559/1770 train_time:154200ms step_avg:99.55ms
step:1560/1770 train_time:154302ms step_avg:99.55ms
step:1561/1770 train_time:154409ms step_avg:99.55ms
step:1562/1770 train_time:154512ms step_avg:99.56ms
step:1563/1770 train_time:154616ms step_avg:99.56ms
step:1564/1770 train_time:154720ms step_avg:99.56ms
step:1565/1770 train_time:154824ms step_avg:99.57ms
step:1566/1770 train_time:154928ms step_avg:99.57ms
step:1567/1770 train_time:155031ms step_avg:99.57ms
step:1568/1770 train_time:155135ms step_avg:99.57ms
step:1569/1770 train_time:155242ms step_avg:99.58ms
step:1570/1770 train_time:155346ms step_avg:99.58ms
step:1571/1770 train_time:155449ms step_avg:99.58ms
step:1572/1770 train_time:155554ms step_avg:99.59ms
step:1573/1770 train_time:155660ms step_avg:99.59ms
step:1574/1770 train_time:155764ms step_avg:99.59ms
step:1575/1770 train_time:155867ms step_avg:99.60ms
step:1576/1770 train_time:155970ms step_avg:99.60ms
step:1577/1770 train_time:156076ms step_avg:99.60ms
step:1578/1770 train_time:156181ms step_avg:99.61ms
step:1579/1770 train_time:156286ms step_avg:99.61ms
step:1580/1770 train_time:156390ms step_avg:99.61ms
step:1581/1770 train_time:156497ms step_avg:99.62ms
step:1582/1770 train_time:156602ms step_avg:99.62ms
step:1583/1770 train_time:156706ms step_avg:99.62ms
step:1584/1770 train_time:156811ms step_avg:99.63ms
step:1585/1770 train_time:156915ms step_avg:99.63ms
step:1586/1770 train_time:157023ms step_avg:99.63ms
step:1587/1770 train_time:157129ms step_avg:99.64ms
step:1588/1770 train_time:157233ms step_avg:99.64ms
step:1589/1770 train_time:157339ms step_avg:99.64ms
step:1590/1770 train_time:157443ms step_avg:99.65ms
step:1591/1770 train_time:157546ms step_avg:99.65ms
step:1592/1770 train_time:157652ms step_avg:99.65ms
step:1593/1770 train_time:157755ms step_avg:99.66ms
step:1594/1770 train_time:157859ms step_avg:99.66ms
step:1595/1770 train_time:157962ms step_avg:99.66ms
step:1596/1770 train_time:158068ms step_avg:99.66ms
step:1597/1770 train_time:158173ms step_avg:99.67ms
step:1598/1770 train_time:158275ms step_avg:99.67ms
step:1599/1770 train_time:158380ms step_avg:99.67ms
step:1600/1770 train_time:158487ms step_avg:99.68ms
step:1601/1770 train_time:158591ms step_avg:99.68ms
step:1602/1770 train_time:158696ms step_avg:99.68ms
step:1603/1770 train_time:158800ms step_avg:99.69ms
step:1604/1770 train_time:158904ms step_avg:99.69ms
step:1605/1770 train_time:159007ms step_avg:99.69ms
step:1606/1770 train_time:159112ms step_avg:99.69ms
step:1607/1770 train_time:159219ms step_avg:99.70ms
step:1608/1770 train_time:159323ms step_avg:99.70ms
step:1609/1770 train_time:159428ms step_avg:99.70ms
step:1610/1770 train_time:159533ms step_avg:99.71ms
step:1611/1770 train_time:159640ms step_avg:99.71ms
step:1612/1770 train_time:159745ms step_avg:99.72ms
step:1613/1770 train_time:159849ms step_avg:99.72ms
step:1614/1770 train_time:159953ms step_avg:99.72ms
step:1615/1770 train_time:160058ms step_avg:99.72ms
step:1616/1770 train_time:160162ms step_avg:99.73ms
step:1617/1770 train_time:160269ms step_avg:99.73ms
step:1618/1770 train_time:160374ms step_avg:99.73ms
step:1619/1770 train_time:160478ms step_avg:99.74ms
step:1620/1770 train_time:160583ms step_avg:99.74ms
step:1621/1770 train_time:160687ms step_avg:99.74ms
step:1622/1770 train_time:160792ms step_avg:99.75ms
step:1623/1770 train_time:160898ms step_avg:99.75ms
step:1624/1770 train_time:161001ms step_avg:99.75ms
step:1625/1770 train_time:161105ms step_avg:99.76ms
step:1625/1770 val_loss:3.3204 train_time:161207ms step_avg:99.82ms
step:1626/1770 train_time:161229ms step_avg:99.77ms
step:1627/1770 train_time:161316ms step_avg:99.76ms
step:1628/1770 train_time:161421ms step_avg:99.77ms
step:1629/1770 train_time:161525ms step_avg:99.77ms
step:1630/1770 train_time:161628ms step_avg:99.77ms
step:1631/1770 train_time:161732ms step_avg:99.77ms
step:1632/1770 train_time:161835ms step_avg:99.77ms
step:1633/1770 train_time:161939ms step_avg:99.78ms
step:1634/1770 train_time:162044ms step_avg:99.78ms
step:1635/1770 train_time:162148ms step_avg:99.78ms
step:1636/1770 train_time:162252ms step_avg:99.79ms
step:1637/1770 train_time:162356ms step_avg:99.79ms
step:1638/1770 train_time:162460ms step_avg:99.79ms
step:1639/1770 train_time:162565ms step_avg:99.79ms
step:1640/1770 train_time:162669ms step_avg:99.80ms
step:1641/1770 train_time:162773ms step_avg:99.80ms
step:1642/1770 train_time:162876ms step_avg:99.80ms
step:1643/1770 train_time:162980ms step_avg:99.80ms
step:1644/1770 train_time:163087ms step_avg:99.81ms
step:1645/1770 train_time:163190ms step_avg:99.81ms
step:1646/1770 train_time:163295ms step_avg:99.81ms
step:1647/1770 train_time:163400ms step_avg:99.82ms
step:1648/1770 train_time:163504ms step_avg:99.82ms
step:1649/1770 train_time:163607ms step_avg:99.82ms
step:1650/1770 train_time:163711ms step_avg:99.82ms
step:1651/1770 train_time:163815ms step_avg:99.83ms
step:1652/1770 train_time:163919ms step_avg:99.83ms
step:1653/1770 train_time:164024ms step_avg:99.83ms
step:1654/1770 train_time:164132ms step_avg:99.84ms
step:1655/1770 train_time:164238ms step_avg:99.84ms
step:1656/1770 train_time:164342ms step_avg:99.84ms
step:1657/1770 train_time:164448ms step_avg:99.85ms
step:1658/1770 train_time:164552ms step_avg:99.85ms
step:1659/1770 train_time:164657ms step_avg:99.85ms
step:1660/1770 train_time:164761ms step_avg:99.86ms
step:1661/1770 train_time:164866ms step_avg:99.86ms
step:1662/1770 train_time:164971ms step_avg:99.86ms
step:1663/1770 train_time:165074ms step_avg:99.86ms
step:1664/1770 train_time:165178ms step_avg:99.87ms
step:1665/1770 train_time:165282ms step_avg:99.87ms
step:1666/1770 train_time:165387ms step_avg:99.87ms
step:1667/1770 train_time:165491ms step_avg:99.87ms
step:1668/1770 train_time:165594ms step_avg:99.88ms
step:1669/1770 train_time:165697ms step_avg:99.88ms
step:1670/1770 train_time:165801ms step_avg:99.88ms
step:1671/1770 train_time:165906ms step_avg:99.88ms
step:1672/1770 train_time:166010ms step_avg:99.89ms
step:1673/1770 train_time:166115ms step_avg:99.89ms
step:1674/1770 train_time:166219ms step_avg:99.89ms
step:1675/1770 train_time:166323ms step_avg:99.89ms
step:1676/1770 train_time:166428ms step_avg:99.90ms
step:1677/1770 train_time:166536ms step_avg:99.90ms
step:1678/1770 train_time:166640ms step_avg:99.90ms
step:1679/1770 train_time:166744ms step_avg:99.91ms
step:1680/1770 train_time:166848ms step_avg:99.91ms
step:1681/1770 train_time:166953ms step_avg:99.91ms
step:1682/1770 train_time:167058ms step_avg:99.92ms
step:1683/1770 train_time:167163ms step_avg:99.92ms
step:1684/1770 train_time:167266ms step_avg:99.92ms
step:1685/1770 train_time:167370ms step_avg:99.92ms
step:1686/1770 train_time:167475ms step_avg:99.93ms
step:1687/1770 train_time:167581ms step_avg:99.93ms
step:1688/1770 train_time:167685ms step_avg:99.93ms
step:1689/1770 train_time:167789ms step_avg:99.93ms
step:1690/1770 train_time:167892ms step_avg:99.94ms
step:1691/1770 train_time:167996ms step_avg:99.94ms
step:1692/1770 train_time:168101ms step_avg:99.94ms
step:1693/1770 train_time:168207ms step_avg:99.94ms
step:1694/1770 train_time:168310ms step_avg:99.95ms
step:1695/1770 train_time:168414ms step_avg:99.95ms
step:1696/1770 train_time:168519ms step_avg:99.95ms
step:1697/1770 train_time:168625ms step_avg:99.96ms
step:1698/1770 train_time:168730ms step_avg:99.96ms
step:1699/1770 train_time:168834ms step_avg:99.96ms
step:1700/1770 train_time:168938ms step_avg:99.96ms
step:1701/1770 train_time:169042ms step_avg:99.97ms
step:1702/1770 train_time:169147ms step_avg:99.97ms
step:1703/1770 train_time:169251ms step_avg:99.97ms
step:1704/1770 train_time:169355ms step_avg:99.97ms
step:1705/1770 train_time:169458ms step_avg:99.98ms
step:1706/1770 train_time:169561ms step_avg:99.98ms
step:1707/1770 train_time:169667ms step_avg:99.98ms
step:1708/1770 train_time:169771ms step_avg:99.98ms
step:1709/1770 train_time:169877ms step_avg:99.99ms
step:1710/1770 train_time:169984ms step_avg:99.99ms
step:1711/1770 train_time:170091ms step_avg:99.99ms
step:1712/1770 train_time:170196ms step_avg:100.00ms
step:1713/1770 train_time:170300ms step_avg:100.00ms
step:1714/1770 train_time:170404ms step_avg:100.00ms
step:1715/1770 train_time:170508ms step_avg:100.00ms
step:1716/1770 train_time:170613ms step_avg:100.01ms
step:1717/1770 train_time:170717ms step_avg:100.01ms
step:1718/1770 train_time:170823ms step_avg:100.01ms
step:1719/1770 train_time:170929ms step_avg:100.02ms
step:1720/1770 train_time:171034ms step_avg:100.02ms
step:1721/1770 train_time:171138ms step_avg:100.02ms
step:1722/1770 train_time:171246ms step_avg:100.03ms
step:1723/1770 train_time:171352ms step_avg:100.03ms
step:1724/1770 train_time:171459ms step_avg:100.03ms
step:1725/1770 train_time:171566ms step_avg:100.04ms
step:1726/1770 train_time:171673ms step_avg:100.04ms
step:1727/1770 train_time:171778ms step_avg:100.05ms
step:1728/1770 train_time:171884ms step_avg:100.05ms
step:1729/1770 train_time:171988ms step_avg:100.05ms
step:1730/1770 train_time:172094ms step_avg:100.05ms
step:1731/1770 train_time:172201ms step_avg:100.06ms
step:1732/1770 train_time:172305ms step_avg:100.06ms
step:1733/1770 train_time:172412ms step_avg:100.06ms
step:1734/1770 train_time:172516ms step_avg:100.07ms
step:1735/1770 train_time:172622ms step_avg:100.07ms
step:1736/1770 train_time:172727ms step_avg:100.07ms
step:1737/1770 train_time:172832ms step_avg:100.08ms
step:1738/1770 train_time:172937ms step_avg:100.08ms
step:1739/1770 train_time:173043ms step_avg:100.08ms
step:1740/1770 train_time:173147ms step_avg:100.09ms
step:1741/1770 train_time:173255ms step_avg:100.09ms
step:1742/1770 train_time:173363ms step_avg:100.09ms
step:1743/1770 train_time:173469ms step_avg:100.10ms
step:1744/1770 train_time:173574ms step_avg:100.10ms
step:1745/1770 train_time:173679ms step_avg:100.10ms
step:1746/1770 train_time:173787ms step_avg:100.11ms
step:1747/1770 train_time:173890ms step_avg:100.11ms
step:1748/1770 train_time:173997ms step_avg:100.11ms
step:1749/1770 train_time:174103ms step_avg:100.12ms
step:1750/1770 train_time:174207ms step_avg:100.12ms
step:1750/1770 val_loss:3.2960 train_time:174310ms step_avg:100.18ms
step:1751/1770 train_time:174331ms step_avg:100.13ms
step:1752/1770 train_time:174421ms step_avg:100.13ms
step:1753/1770 train_time:174525ms step_avg:100.13ms
step:1754/1770 train_time:174630ms step_avg:100.13ms
step:1755/1770 train_time:174735ms step_avg:100.13ms
step:1756/1770 train_time:174841ms step_avg:100.14ms
step:1757/1770 train_time:174946ms step_avg:100.14ms
step:1758/1770 train_time:175050ms step_avg:100.14ms
step:1759/1770 train_time:175156ms step_avg:100.15ms
step:1760/1770 train_time:175261ms step_avg:100.15ms
step:1761/1770 train_time:175369ms step_avg:100.15ms
step:1762/1770 train_time:175478ms step_avg:100.16ms
step:1763/1770 train_time:175581ms step_avg:100.16ms
step:1764/1770 train_time:175687ms step_avg:100.16ms
step:1765/1770 train_time:175792ms step_avg:100.17ms
step:1766/1770 train_time:175901ms step_avg:100.17ms
step:1767/1770 train_time:176005ms step_avg:100.17ms
step:1768/1770 train_time:176110ms step_avg:100.18ms
step:1769/1770 train_time:176215ms step_avg:100.18ms
step:1770/1770 train_time:176319ms step_avg:100.18ms
step:1770/1770 val_loss:3.2929 train_time:176424ms step_avg:100.24ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
