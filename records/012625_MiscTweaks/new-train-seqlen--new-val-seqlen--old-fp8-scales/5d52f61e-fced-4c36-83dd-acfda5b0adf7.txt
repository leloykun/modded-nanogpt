import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 forward & backward by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# Custom operators : FP8 forward & bfloat16 backward

@torch.library.custom_op("nanogpt::mm_mixed", mutates_args=())
def mm_mixed_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_mixed_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_mixed_backward", mutates_args=())
def mm_mixed_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        x_bfloat16 = x_f8.to(torch.bfloat16)
        w_bfloat16 = w_f8.to(torch.bfloat16)
        grad_bfloat16 = grad.mul(grad_s).to(torch.bfloat16)
        grad_x = torch._scaled_mm(
            grad_bfloat16,
            w_bfloat16.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        grad_w = torch._scaled_mm(
            x_bfloat16.t().contiguous(),
            grad_bfloat16.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_mixed_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_mixed_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_mixed_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_mixed_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_mixed_op.register_autograd(mm_mixed_backward, setup_context=mm_mixed_setup_context)

def linear_mixed(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm_mixed(_x, w, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 22:44:09 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24479ms step_avg:nanms
step:2/1770 train_time:24906ms step_avg:nanms
step:3/1770 train_time:25002ms step_avg:nanms
step:4/1770 train_time:25095ms step_avg:nanms
step:5/1770 train_time:25189ms step_avg:nanms
step:6/1770 train_time:25284ms step_avg:nanms
step:7/1770 train_time:25378ms step_avg:nanms
step:8/1770 train_time:25472ms step_avg:nanms
step:9/1770 train_time:25567ms step_avg:nanms
step:10/1770 train_time:25662ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.60ms
step:14/1770 train_time:379ms step_avg:94.67ms
step:15/1770 train_time:473ms step_avg:94.62ms
step:16/1770 train_time:567ms step_avg:94.54ms
step:17/1770 train_time:662ms step_avg:94.59ms
step:18/1770 train_time:757ms step_avg:94.59ms
step:19/1770 train_time:851ms step_avg:94.58ms
step:20/1770 train_time:946ms step_avg:94.59ms
step:21/1770 train_time:1041ms step_avg:94.60ms
step:22/1770 train_time:1135ms step_avg:94.58ms
step:23/1770 train_time:1230ms step_avg:94.63ms
step:24/1770 train_time:1324ms step_avg:94.60ms
step:25/1770 train_time:1420ms step_avg:94.63ms
step:26/1770 train_time:1514ms step_avg:94.66ms
step:27/1770 train_time:1609ms step_avg:94.66ms
step:28/1770 train_time:1705ms step_avg:94.72ms
step:29/1770 train_time:1800ms step_avg:94.73ms
step:30/1770 train_time:1895ms step_avg:94.74ms
step:31/1770 train_time:1989ms step_avg:94.74ms
step:32/1770 train_time:2085ms step_avg:94.76ms
step:33/1770 train_time:2180ms step_avg:94.77ms
step:34/1770 train_time:2274ms step_avg:94.75ms
step:35/1770 train_time:2368ms step_avg:94.74ms
step:36/1770 train_time:2464ms step_avg:94.76ms
step:37/1770 train_time:2559ms step_avg:94.77ms
step:38/1770 train_time:2657ms step_avg:94.88ms
step:39/1770 train_time:2748ms step_avg:94.75ms
step:40/1770 train_time:2843ms step_avg:94.75ms
step:41/1770 train_time:2937ms step_avg:94.76ms
step:42/1770 train_time:3032ms step_avg:94.75ms
step:43/1770 train_time:3127ms step_avg:94.74ms
step:44/1770 train_time:3222ms step_avg:94.76ms
step:45/1770 train_time:3317ms step_avg:94.76ms
step:46/1770 train_time:3411ms step_avg:94.75ms
step:47/1770 train_time:3506ms step_avg:94.75ms
step:48/1770 train_time:3601ms step_avg:94.76ms
step:49/1770 train_time:3695ms step_avg:94.76ms
step:50/1770 train_time:3790ms step_avg:94.75ms
step:51/1770 train_time:3885ms step_avg:94.76ms
step:52/1770 train_time:3980ms step_avg:94.76ms
step:53/1770 train_time:4074ms step_avg:94.75ms
step:54/1770 train_time:4169ms step_avg:94.76ms
step:55/1770 train_time:4265ms step_avg:94.78ms
step:56/1770 train_time:4360ms step_avg:94.79ms
step:57/1770 train_time:4458ms step_avg:94.84ms
step:58/1770 train_time:4549ms step_avg:94.78ms
step:59/1770 train_time:4645ms step_avg:94.81ms
step:60/1770 train_time:4740ms step_avg:94.79ms
step:61/1770 train_time:4834ms step_avg:94.79ms
step:62/1770 train_time:4929ms step_avg:94.78ms
step:63/1770 train_time:5024ms step_avg:94.79ms
step:64/1770 train_time:5118ms step_avg:94.78ms
step:65/1770 train_time:5213ms step_avg:94.77ms
step:66/1770 train_time:5307ms step_avg:94.77ms
step:67/1770 train_time:5402ms step_avg:94.77ms
step:68/1770 train_time:5498ms step_avg:94.79ms
step:69/1770 train_time:5592ms step_avg:94.78ms
step:70/1770 train_time:5687ms step_avg:94.79ms
step:71/1770 train_time:5782ms step_avg:94.79ms
step:72/1770 train_time:5877ms step_avg:94.79ms
step:73/1770 train_time:5971ms step_avg:94.78ms
step:74/1770 train_time:6066ms step_avg:94.78ms
step:75/1770 train_time:6161ms step_avg:94.79ms
step:76/1770 train_time:6259ms step_avg:94.83ms
step:77/1770 train_time:6350ms step_avg:94.78ms
step:78/1770 train_time:6445ms step_avg:94.78ms
step:79/1770 train_time:6540ms step_avg:94.78ms
step:80/1770 train_time:6635ms step_avg:94.78ms
step:81/1770 train_time:6729ms step_avg:94.78ms
step:82/1770 train_time:6824ms step_avg:94.78ms
step:83/1770 train_time:6920ms step_avg:94.79ms
step:84/1770 train_time:7015ms step_avg:94.79ms
step:85/1770 train_time:7109ms step_avg:94.78ms
step:86/1770 train_time:7204ms step_avg:94.79ms
step:87/1770 train_time:7298ms step_avg:94.79ms
step:88/1770 train_time:7393ms step_avg:94.78ms
step:89/1770 train_time:7487ms step_avg:94.77ms
step:90/1770 train_time:7582ms step_avg:94.77ms
step:91/1770 train_time:7676ms step_avg:94.77ms
step:92/1770 train_time:7771ms step_avg:94.77ms
step:93/1770 train_time:7866ms step_avg:94.77ms
step:94/1770 train_time:7962ms step_avg:94.78ms
step:95/1770 train_time:8057ms step_avg:94.78ms
step:96/1770 train_time:8155ms step_avg:94.83ms
step:97/1770 train_time:8246ms step_avg:94.78ms
step:98/1770 train_time:8341ms step_avg:94.78ms
step:99/1770 train_time:8435ms step_avg:94.78ms
step:100/1770 train_time:8530ms step_avg:94.77ms
step:101/1770 train_time:8625ms step_avg:94.78ms
step:102/1770 train_time:8720ms step_avg:94.78ms
step:103/1770 train_time:8814ms step_avg:94.77ms
step:104/1770 train_time:8908ms step_avg:94.77ms
step:105/1770 train_time:9004ms step_avg:94.78ms
step:106/1770 train_time:9099ms step_avg:94.78ms
step:107/1770 train_time:9193ms step_avg:94.78ms
step:108/1770 train_time:9288ms step_avg:94.77ms
step:109/1770 train_time:9383ms step_avg:94.78ms
step:110/1770 train_time:9478ms step_avg:94.78ms
step:111/1770 train_time:9572ms step_avg:94.78ms
step:112/1770 train_time:9667ms step_avg:94.77ms
step:113/1770 train_time:9762ms step_avg:94.77ms
step:114/1770 train_time:9856ms step_avg:94.77ms
step:115/1770 train_time:9951ms step_avg:94.77ms
step:116/1770 train_time:10045ms step_avg:94.77ms
step:117/1770 train_time:10140ms step_avg:94.77ms
step:118/1770 train_time:10234ms step_avg:94.76ms
step:119/1770 train_time:10329ms step_avg:94.76ms
step:120/1770 train_time:10424ms step_avg:94.76ms
step:121/1770 train_time:10519ms step_avg:94.77ms
step:122/1770 train_time:10614ms step_avg:94.77ms
step:123/1770 train_time:10709ms step_avg:94.77ms
step:124/1770 train_time:10804ms step_avg:94.77ms
step:125/1770 train_time:10899ms step_avg:94.78ms
step:125/1770 val_loss:4.6530 train_time:10992ms step_avg:95.58ms
step:126/1770 train_time:11017ms step_avg:94.97ms
step:127/1770 train_time:11091ms step_avg:94.80ms
step:128/1770 train_time:11191ms step_avg:94.84ms
step:129/1770 train_time:11290ms step_avg:94.87ms
step:130/1770 train_time:11384ms step_avg:94.87ms
step:131/1770 train_time:11479ms step_avg:94.87ms
step:132/1770 train_time:11575ms step_avg:94.87ms
step:133/1770 train_time:11668ms step_avg:94.87ms
step:134/1770 train_time:11764ms step_avg:94.87ms
step:135/1770 train_time:11859ms step_avg:94.87ms
step:136/1770 train_time:11954ms step_avg:94.87ms
step:137/1770 train_time:12050ms step_avg:94.88ms
step:138/1770 train_time:12145ms step_avg:94.88ms
step:139/1770 train_time:12241ms step_avg:94.89ms
step:140/1770 train_time:12336ms step_avg:94.90ms
step:141/1770 train_time:12432ms step_avg:94.90ms
step:142/1770 train_time:12527ms step_avg:94.90ms
step:143/1770 train_time:12622ms step_avg:94.90ms
step:144/1770 train_time:12717ms step_avg:94.91ms
step:145/1770 train_time:12813ms step_avg:94.91ms
step:146/1770 train_time:12908ms step_avg:94.91ms
step:147/1770 train_time:13003ms step_avg:94.91ms
step:148/1770 train_time:13099ms step_avg:94.92ms
step:149/1770 train_time:13194ms step_avg:94.92ms
step:150/1770 train_time:13290ms step_avg:94.93ms
step:151/1770 train_time:13385ms step_avg:94.93ms
step:152/1770 train_time:13481ms step_avg:94.94ms
step:153/1770 train_time:13576ms step_avg:94.94ms
step:154/1770 train_time:13671ms step_avg:94.94ms
step:155/1770 train_time:13767ms step_avg:94.94ms
step:156/1770 train_time:13862ms step_avg:94.95ms
step:157/1770 train_time:13958ms step_avg:94.95ms
step:158/1770 train_time:14053ms step_avg:94.95ms
step:159/1770 train_time:14148ms step_avg:94.95ms
step:160/1770 train_time:14243ms step_avg:94.95ms
step:161/1770 train_time:14339ms step_avg:94.96ms
step:162/1770 train_time:14435ms step_avg:94.97ms
step:163/1770 train_time:14530ms step_avg:94.97ms
step:164/1770 train_time:14625ms step_avg:94.97ms
step:165/1770 train_time:14721ms step_avg:94.98ms
step:166/1770 train_time:14816ms step_avg:94.98ms
step:167/1770 train_time:14911ms step_avg:94.98ms
step:168/1770 train_time:15006ms step_avg:94.98ms
step:169/1770 train_time:15102ms step_avg:94.98ms
step:170/1770 train_time:15197ms step_avg:94.98ms
step:171/1770 train_time:15293ms step_avg:94.99ms
step:172/1770 train_time:15388ms step_avg:94.99ms
step:173/1770 train_time:15483ms step_avg:94.99ms
step:174/1770 train_time:15579ms step_avg:94.99ms
step:175/1770 train_time:15674ms step_avg:94.99ms
step:176/1770 train_time:15769ms step_avg:94.99ms
step:177/1770 train_time:15864ms step_avg:94.99ms
step:178/1770 train_time:15960ms step_avg:95.00ms
step:179/1770 train_time:16055ms step_avg:95.00ms
step:180/1770 train_time:16153ms step_avg:95.02ms
step:181/1770 train_time:16246ms step_avg:95.00ms
step:182/1770 train_time:16341ms step_avg:95.01ms
step:183/1770 train_time:16436ms step_avg:95.01ms
step:184/1770 train_time:16531ms step_avg:95.01ms
step:185/1770 train_time:16626ms step_avg:95.01ms
step:186/1770 train_time:16722ms step_avg:95.01ms
step:187/1770 train_time:16817ms step_avg:95.01ms
step:188/1770 train_time:16913ms step_avg:95.02ms
step:189/1770 train_time:17008ms step_avg:95.01ms
step:190/1770 train_time:17103ms step_avg:95.02ms
step:191/1770 train_time:17199ms step_avg:95.02ms
step:192/1770 train_time:17294ms step_avg:95.02ms
step:193/1770 train_time:17390ms step_avg:95.03ms
step:194/1770 train_time:17485ms step_avg:95.03ms
step:195/1770 train_time:17581ms step_avg:95.03ms
step:196/1770 train_time:17677ms step_avg:95.04ms
step:197/1770 train_time:17773ms step_avg:95.04ms
step:198/1770 train_time:17868ms step_avg:95.04ms
step:199/1770 train_time:17963ms step_avg:95.04ms
step:200/1770 train_time:18058ms step_avg:95.04ms
step:201/1770 train_time:18158ms step_avg:95.07ms
step:202/1770 train_time:18248ms step_avg:95.04ms
step:203/1770 train_time:18344ms step_avg:95.04ms
step:204/1770 train_time:18439ms step_avg:95.05ms
step:205/1770 train_time:18535ms step_avg:95.05ms
step:206/1770 train_time:18630ms step_avg:95.05ms
step:207/1770 train_time:18725ms step_avg:95.05ms
step:208/1770 train_time:18821ms step_avg:95.05ms
step:209/1770 train_time:18916ms step_avg:95.06ms
step:210/1770 train_time:19012ms step_avg:95.06ms
step:211/1770 train_time:19107ms step_avg:95.06ms
step:212/1770 train_time:19202ms step_avg:95.06ms
step:213/1770 train_time:19298ms step_avg:95.06ms
step:214/1770 train_time:19393ms step_avg:95.06ms
step:215/1770 train_time:19488ms step_avg:95.06ms
step:216/1770 train_time:19583ms step_avg:95.07ms
step:217/1770 train_time:19679ms step_avg:95.07ms
step:218/1770 train_time:19774ms step_avg:95.07ms
step:219/1770 train_time:19869ms step_avg:95.07ms
step:220/1770 train_time:19967ms step_avg:95.08ms
step:221/1770 train_time:20060ms step_avg:95.07ms
step:222/1770 train_time:20156ms step_avg:95.07ms
step:223/1770 train_time:20250ms step_avg:95.07ms
step:224/1770 train_time:20346ms step_avg:95.07ms
step:225/1770 train_time:20441ms step_avg:95.08ms
step:226/1770 train_time:20537ms step_avg:95.08ms
step:227/1770 train_time:20632ms step_avg:95.08ms
step:228/1770 train_time:20728ms step_avg:95.08ms
step:229/1770 train_time:20823ms step_avg:95.08ms
step:230/1770 train_time:20919ms step_avg:95.09ms
step:231/1770 train_time:21015ms step_avg:95.09ms
step:232/1770 train_time:21110ms step_avg:95.09ms
step:233/1770 train_time:21205ms step_avg:95.09ms
step:234/1770 train_time:21300ms step_avg:95.09ms
step:235/1770 train_time:21396ms step_avg:95.10ms
step:236/1770 train_time:21492ms step_avg:95.10ms
step:237/1770 train_time:21586ms step_avg:95.09ms
step:238/1770 train_time:21682ms step_avg:95.09ms
step:239/1770 train_time:21777ms step_avg:95.10ms
step:240/1770 train_time:21873ms step_avg:95.10ms
step:241/1770 train_time:21970ms step_avg:95.11ms
step:242/1770 train_time:22064ms step_avg:95.10ms
step:243/1770 train_time:22159ms step_avg:95.11ms
step:244/1770 train_time:22255ms step_avg:95.11ms
step:245/1770 train_time:22351ms step_avg:95.11ms
step:246/1770 train_time:22446ms step_avg:95.11ms
step:247/1770 train_time:22541ms step_avg:95.11ms
step:248/1770 train_time:22637ms step_avg:95.11ms
step:249/1770 train_time:22732ms step_avg:95.11ms
step:250/1770 train_time:22827ms step_avg:95.11ms
step:250/1770 val_loss:4.1138 train_time:22921ms step_avg:95.51ms
step:251/1770 train_time:22946ms step_avg:95.21ms
step:252/1770 train_time:23026ms step_avg:95.15ms
step:253/1770 train_time:23123ms step_avg:95.15ms
step:254/1770 train_time:23218ms step_avg:95.16ms
step:255/1770 train_time:23314ms step_avg:95.16ms
step:256/1770 train_time:23409ms step_avg:95.16ms
step:257/1770 train_time:23504ms step_avg:95.16ms
step:258/1770 train_time:23599ms step_avg:95.16ms
step:259/1770 train_time:23694ms step_avg:95.16ms
step:260/1770 train_time:23789ms step_avg:95.16ms
step:261/1770 train_time:23883ms step_avg:95.15ms
step:262/1770 train_time:23978ms step_avg:95.15ms
step:263/1770 train_time:24074ms step_avg:95.16ms
step:264/1770 train_time:24170ms step_avg:95.16ms
step:265/1770 train_time:24265ms step_avg:95.16ms
step:266/1770 train_time:24361ms step_avg:95.16ms
step:267/1770 train_time:24457ms step_avg:95.16ms
step:268/1770 train_time:24553ms step_avg:95.16ms
step:269/1770 train_time:24649ms step_avg:95.17ms
step:270/1770 train_time:24744ms step_avg:95.17ms
step:271/1770 train_time:24840ms step_avg:95.17ms
step:272/1770 train_time:24936ms step_avg:95.17ms
step:273/1770 train_time:25031ms step_avg:95.18ms
step:274/1770 train_time:25126ms step_avg:95.18ms
step:275/1770 train_time:25222ms step_avg:95.18ms
step:276/1770 train_time:25317ms step_avg:95.18ms
step:277/1770 train_time:25413ms step_avg:95.18ms
step:278/1770 train_time:25509ms step_avg:95.18ms
step:279/1770 train_time:25605ms step_avg:95.19ms
step:280/1770 train_time:25700ms step_avg:95.19ms
step:281/1770 train_time:25796ms step_avg:95.19ms
step:282/1770 train_time:25892ms step_avg:95.19ms
step:283/1770 train_time:25989ms step_avg:95.20ms
step:284/1770 train_time:26083ms step_avg:95.19ms
step:285/1770 train_time:26178ms step_avg:95.19ms
step:286/1770 train_time:26275ms step_avg:95.20ms
step:287/1770 train_time:26370ms step_avg:95.20ms
step:288/1770 train_time:26466ms step_avg:95.20ms
step:289/1770 train_time:26562ms step_avg:95.20ms
step:290/1770 train_time:26657ms step_avg:95.20ms
step:291/1770 train_time:26754ms step_avg:95.21ms
step:292/1770 train_time:26849ms step_avg:95.21ms
step:293/1770 train_time:26945ms step_avg:95.21ms
step:294/1770 train_time:27040ms step_avg:95.21ms
step:295/1770 train_time:27136ms step_avg:95.21ms
step:296/1770 train_time:27232ms step_avg:95.22ms
step:297/1770 train_time:27327ms step_avg:95.22ms
step:298/1770 train_time:27422ms step_avg:95.22ms
step:299/1770 train_time:27518ms step_avg:95.22ms
step:300/1770 train_time:27614ms step_avg:95.22ms
step:301/1770 train_time:27710ms step_avg:95.22ms
step:302/1770 train_time:27806ms step_avg:95.22ms
step:303/1770 train_time:27901ms step_avg:95.23ms
step:304/1770 train_time:27997ms step_avg:95.23ms
step:305/1770 train_time:28093ms step_avg:95.23ms
step:306/1770 train_time:28188ms step_avg:95.23ms
step:307/1770 train_time:28283ms step_avg:95.23ms
step:308/1770 train_time:28379ms step_avg:95.23ms
step:309/1770 train_time:28475ms step_avg:95.23ms
step:310/1770 train_time:28570ms step_avg:95.23ms
step:311/1770 train_time:28666ms step_avg:95.24ms
step:312/1770 train_time:28762ms step_avg:95.24ms
step:313/1770 train_time:28857ms step_avg:95.24ms
step:314/1770 train_time:28953ms step_avg:95.24ms
step:315/1770 train_time:29049ms step_avg:95.24ms
step:316/1770 train_time:29144ms step_avg:95.24ms
step:317/1770 train_time:29240ms step_avg:95.24ms
step:318/1770 train_time:29336ms step_avg:95.25ms
step:319/1770 train_time:29432ms step_avg:95.25ms
step:320/1770 train_time:29528ms step_avg:95.25ms
step:321/1770 train_time:29623ms step_avg:95.25ms
step:322/1770 train_time:29718ms step_avg:95.25ms
step:323/1770 train_time:29815ms step_avg:95.25ms
step:324/1770 train_time:29910ms step_avg:95.25ms
step:325/1770 train_time:30005ms step_avg:95.26ms
step:326/1770 train_time:30101ms step_avg:95.26ms
step:327/1770 train_time:30197ms step_avg:95.26ms
step:328/1770 train_time:30292ms step_avg:95.26ms
step:329/1770 train_time:30388ms step_avg:95.26ms
step:330/1770 train_time:30484ms step_avg:95.26ms
step:331/1770 train_time:30579ms step_avg:95.26ms
step:332/1770 train_time:30675ms step_avg:95.27ms
step:333/1770 train_time:30777ms step_avg:95.29ms
step:334/1770 train_time:30867ms step_avg:95.27ms
step:335/1770 train_time:30963ms step_avg:95.27ms
step:336/1770 train_time:31058ms step_avg:95.27ms
step:337/1770 train_time:31154ms step_avg:95.27ms
step:338/1770 train_time:31250ms step_avg:95.27ms
step:339/1770 train_time:31345ms step_avg:95.27ms
step:340/1770 train_time:31440ms step_avg:95.27ms
step:341/1770 train_time:31536ms step_avg:95.27ms
step:342/1770 train_time:31632ms step_avg:95.28ms
step:343/1770 train_time:31728ms step_avg:95.28ms
step:344/1770 train_time:31824ms step_avg:95.28ms
step:345/1770 train_time:31920ms step_avg:95.28ms
step:346/1770 train_time:32016ms step_avg:95.28ms
step:347/1770 train_time:32112ms step_avg:95.29ms
step:348/1770 train_time:32208ms step_avg:95.29ms
step:349/1770 train_time:32304ms step_avg:95.29ms
step:350/1770 train_time:32399ms step_avg:95.29ms
step:351/1770 train_time:32495ms step_avg:95.29ms
step:352/1770 train_time:32591ms step_avg:95.29ms
step:353/1770 train_time:32688ms step_avg:95.30ms
step:354/1770 train_time:32781ms step_avg:95.29ms
step:355/1770 train_time:32877ms step_avg:95.30ms
step:356/1770 train_time:32973ms step_avg:95.30ms
step:357/1770 train_time:33069ms step_avg:95.30ms
step:358/1770 train_time:33165ms step_avg:95.30ms
step:359/1770 train_time:33260ms step_avg:95.30ms
step:360/1770 train_time:33356ms step_avg:95.30ms
step:361/1770 train_time:33452ms step_avg:95.31ms
step:362/1770 train_time:33548ms step_avg:95.31ms
step:363/1770 train_time:33643ms step_avg:95.31ms
step:364/1770 train_time:33739ms step_avg:95.31ms
step:365/1770 train_time:33835ms step_avg:95.31ms
step:366/1770 train_time:33930ms step_avg:95.31ms
step:367/1770 train_time:34026ms step_avg:95.31ms
step:368/1770 train_time:34122ms step_avg:95.31ms
step:369/1770 train_time:34218ms step_avg:95.31ms
step:370/1770 train_time:34313ms step_avg:95.31ms
step:371/1770 train_time:34409ms step_avg:95.32ms
step:372/1770 train_time:34505ms step_avg:95.32ms
step:373/1770 train_time:34600ms step_avg:95.32ms
step:374/1770 train_time:34696ms step_avg:95.32ms
step:375/1770 train_time:34792ms step_avg:95.32ms
step:375/1770 val_loss:3.9106 train_time:34886ms step_avg:95.58ms
step:376/1770 train_time:34907ms step_avg:95.37ms
step:377/1770 train_time:34991ms step_avg:95.34ms
step:378/1770 train_time:35091ms step_avg:95.35ms
step:379/1770 train_time:35186ms step_avg:95.36ms
step:380/1770 train_time:35283ms step_avg:95.36ms
step:381/1770 train_time:35378ms step_avg:95.36ms
step:382/1770 train_time:35474ms step_avg:95.36ms
step:383/1770 train_time:35569ms step_avg:95.36ms
step:384/1770 train_time:35665ms step_avg:95.36ms
step:385/1770 train_time:35761ms step_avg:95.36ms
step:386/1770 train_time:35856ms step_avg:95.36ms
step:387/1770 train_time:35952ms step_avg:95.36ms
step:388/1770 train_time:36048ms step_avg:95.36ms
step:389/1770 train_time:36143ms step_avg:95.37ms
step:390/1770 train_time:36239ms step_avg:95.37ms
step:391/1770 train_time:36335ms step_avg:95.37ms
step:392/1770 train_time:36431ms step_avg:95.37ms
step:393/1770 train_time:36526ms step_avg:95.37ms
step:394/1770 train_time:36622ms step_avg:95.37ms
step:395/1770 train_time:36718ms step_avg:95.37ms
step:396/1770 train_time:36815ms step_avg:95.38ms
step:397/1770 train_time:36913ms step_avg:95.38ms
step:398/1770 train_time:37010ms step_avg:95.39ms
step:399/1770 train_time:37108ms step_avg:95.39ms
step:400/1770 train_time:37205ms step_avg:95.40ms
step:401/1770 train_time:37303ms step_avg:95.40ms
step:402/1770 train_time:37401ms step_avg:95.41ms
step:403/1770 train_time:37499ms step_avg:95.42ms
step:404/1770 train_time:37597ms step_avg:95.42ms
step:405/1770 train_time:37694ms step_avg:95.43ms
step:406/1770 train_time:37791ms step_avg:95.43ms
step:407/1770 train_time:37888ms step_avg:95.44ms
step:408/1770 train_time:37986ms step_avg:95.44ms
step:409/1770 train_time:38084ms step_avg:95.45ms
step:410/1770 train_time:38183ms step_avg:95.46ms
step:411/1770 train_time:38280ms step_avg:95.46ms
step:412/1770 train_time:38378ms step_avg:95.47ms
step:413/1770 train_time:38476ms step_avg:95.47ms
step:414/1770 train_time:38573ms step_avg:95.48ms
step:415/1770 train_time:38670ms step_avg:95.48ms
step:416/1770 train_time:38768ms step_avg:95.49ms
step:417/1770 train_time:38865ms step_avg:95.49ms
step:418/1770 train_time:38963ms step_avg:95.50ms
step:419/1770 train_time:39061ms step_avg:95.50ms
step:420/1770 train_time:39158ms step_avg:95.51ms
step:421/1770 train_time:39256ms step_avg:95.51ms
step:422/1770 train_time:39353ms step_avg:95.52ms
step:423/1770 train_time:39451ms step_avg:95.52ms
step:424/1770 train_time:39548ms step_avg:95.53ms
step:425/1770 train_time:39646ms step_avg:95.53ms
step:426/1770 train_time:39744ms step_avg:95.54ms
step:427/1770 train_time:39842ms step_avg:95.54ms
step:428/1770 train_time:39939ms step_avg:95.55ms
step:429/1770 train_time:40037ms step_avg:95.55ms
step:430/1770 train_time:40134ms step_avg:95.56ms
step:431/1770 train_time:40232ms step_avg:95.56ms
step:432/1770 train_time:40329ms step_avg:95.57ms
step:433/1770 train_time:40426ms step_avg:95.57ms
step:434/1770 train_time:40525ms step_avg:95.58ms
step:435/1770 train_time:40623ms step_avg:95.58ms
step:436/1770 train_time:40720ms step_avg:95.59ms
step:437/1770 train_time:40818ms step_avg:95.59ms
step:438/1770 train_time:40916ms step_avg:95.60ms
step:439/1770 train_time:41013ms step_avg:95.60ms
step:440/1770 train_time:41111ms step_avg:95.61ms
step:441/1770 train_time:41208ms step_avg:95.61ms
step:442/1770 train_time:41306ms step_avg:95.62ms
step:443/1770 train_time:41404ms step_avg:95.62ms
step:444/1770 train_time:41502ms step_avg:95.63ms
step:445/1770 train_time:41599ms step_avg:95.63ms
step:446/1770 train_time:41697ms step_avg:95.64ms
step:447/1770 train_time:41795ms step_avg:95.64ms
step:448/1770 train_time:41892ms step_avg:95.64ms
step:449/1770 train_time:41989ms step_avg:95.65ms
step:450/1770 train_time:42087ms step_avg:95.65ms
step:451/1770 train_time:42185ms step_avg:95.66ms
step:452/1770 train_time:42283ms step_avg:95.66ms
step:453/1770 train_time:42381ms step_avg:95.67ms
step:454/1770 train_time:42478ms step_avg:95.67ms
step:455/1770 train_time:42576ms step_avg:95.68ms
step:456/1770 train_time:42673ms step_avg:95.68ms
step:457/1770 train_time:42771ms step_avg:95.68ms
step:458/1770 train_time:42868ms step_avg:95.69ms
step:459/1770 train_time:42966ms step_avg:95.69ms
step:460/1770 train_time:43063ms step_avg:95.70ms
step:461/1770 train_time:43161ms step_avg:95.70ms
step:462/1770 train_time:43259ms step_avg:95.71ms
step:463/1770 train_time:43356ms step_avg:95.71ms
step:464/1770 train_time:43454ms step_avg:95.71ms
step:465/1770 train_time:43551ms step_avg:95.72ms
step:466/1770 train_time:43649ms step_avg:95.72ms
step:467/1770 train_time:43747ms step_avg:95.73ms
step:468/1770 train_time:43844ms step_avg:95.73ms
step:469/1770 train_time:43942ms step_avg:95.73ms
step:470/1770 train_time:44040ms step_avg:95.74ms
step:471/1770 train_time:44138ms step_avg:95.74ms
step:472/1770 train_time:44235ms step_avg:95.75ms
step:473/1770 train_time:44332ms step_avg:95.75ms
step:474/1770 train_time:44430ms step_avg:95.75ms
step:475/1770 train_time:44527ms step_avg:95.76ms
step:476/1770 train_time:44625ms step_avg:95.76ms
step:477/1770 train_time:44723ms step_avg:95.77ms
step:478/1770 train_time:44822ms step_avg:95.77ms
step:479/1770 train_time:44920ms step_avg:95.78ms
step:480/1770 train_time:45017ms step_avg:95.78ms
step:481/1770 train_time:45115ms step_avg:95.79ms
step:482/1770 train_time:45212ms step_avg:95.79ms
step:483/1770 train_time:45310ms step_avg:95.79ms
step:484/1770 train_time:45407ms step_avg:95.80ms
step:485/1770 train_time:45505ms step_avg:95.80ms
step:486/1770 train_time:45603ms step_avg:95.80ms
step:487/1770 train_time:45700ms step_avg:95.81ms
step:488/1770 train_time:45798ms step_avg:95.81ms
step:489/1770 train_time:45896ms step_avg:95.82ms
step:490/1770 train_time:45993ms step_avg:95.82ms
step:491/1770 train_time:46091ms step_avg:95.82ms
step:492/1770 train_time:46188ms step_avg:95.83ms
step:493/1770 train_time:46286ms step_avg:95.83ms
step:494/1770 train_time:46384ms step_avg:95.83ms
step:495/1770 train_time:46482ms step_avg:95.84ms
step:496/1770 train_time:46581ms step_avg:95.85ms
step:497/1770 train_time:46679ms step_avg:95.85ms
step:498/1770 train_time:46777ms step_avg:95.85ms
step:499/1770 train_time:46874ms step_avg:95.86ms
step:500/1770 train_time:46972ms step_avg:95.86ms
step:500/1770 val_loss:3.7580 train_time:47068ms step_avg:96.06ms
step:501/1770 train_time:47089ms step_avg:95.90ms
step:502/1770 train_time:47176ms step_avg:95.89ms
step:503/1770 train_time:47275ms step_avg:95.89ms
step:504/1770 train_time:47373ms step_avg:95.90ms
step:505/1770 train_time:47471ms step_avg:95.90ms
step:506/1770 train_time:47569ms step_avg:95.91ms
step:507/1770 train_time:47667ms step_avg:95.91ms
step:508/1770 train_time:47764ms step_avg:95.91ms
step:509/1770 train_time:47861ms step_avg:95.91ms
step:510/1770 train_time:47959ms step_avg:95.92ms
step:511/1770 train_time:48057ms step_avg:95.92ms
step:512/1770 train_time:48154ms step_avg:95.92ms
step:513/1770 train_time:48252ms step_avg:95.93ms
step:514/1770 train_time:48350ms step_avg:95.93ms
step:515/1770 train_time:48448ms step_avg:95.94ms
step:516/1770 train_time:48545ms step_avg:95.94ms
step:517/1770 train_time:48642ms step_avg:95.94ms
step:518/1770 train_time:48740ms step_avg:95.94ms
step:519/1770 train_time:48837ms step_avg:95.95ms
step:520/1770 train_time:48935ms step_avg:95.95ms
step:521/1770 train_time:49033ms step_avg:95.95ms
step:522/1770 train_time:49131ms step_avg:95.96ms
step:523/1770 train_time:49229ms step_avg:95.96ms
step:524/1770 train_time:49326ms step_avg:95.97ms
step:525/1770 train_time:49424ms step_avg:95.97ms
step:526/1770 train_time:49521ms step_avg:95.97ms
step:527/1770 train_time:49618ms step_avg:95.97ms
step:528/1770 train_time:49716ms step_avg:95.98ms
step:529/1770 train_time:49815ms step_avg:95.98ms
step:530/1770 train_time:49913ms step_avg:95.99ms
step:531/1770 train_time:50012ms step_avg:95.99ms
step:532/1770 train_time:50110ms step_avg:96.00ms
step:533/1770 train_time:50208ms step_avg:96.00ms
step:534/1770 train_time:50306ms step_avg:96.00ms
step:535/1770 train_time:50403ms step_avg:96.01ms
step:536/1770 train_time:50501ms step_avg:96.01ms
step:537/1770 train_time:50599ms step_avg:96.01ms
step:538/1770 train_time:50697ms step_avg:96.02ms
step:539/1770 train_time:50795ms step_avg:96.02ms
step:540/1770 train_time:50893ms step_avg:96.03ms
step:541/1770 train_time:50992ms step_avg:96.03ms
step:542/1770 train_time:51089ms step_avg:96.03ms
step:543/1770 train_time:51187ms step_avg:96.04ms
step:544/1770 train_time:51285ms step_avg:96.04ms
step:545/1770 train_time:51383ms step_avg:96.04ms
step:546/1770 train_time:51480ms step_avg:96.05ms
step:547/1770 train_time:51578ms step_avg:96.05ms
step:548/1770 train_time:51676ms step_avg:96.05ms
step:549/1770 train_time:51775ms step_avg:96.06ms
step:550/1770 train_time:51873ms step_avg:96.06ms
step:551/1770 train_time:51971ms step_avg:96.06ms
step:552/1770 train_time:52069ms step_avg:96.07ms
step:553/1770 train_time:52167ms step_avg:96.07ms
step:554/1770 train_time:52265ms step_avg:96.08ms
step:555/1770 train_time:52363ms step_avg:96.08ms
step:556/1770 train_time:52460ms step_avg:96.08ms
step:557/1770 train_time:52558ms step_avg:96.08ms
step:558/1770 train_time:52656ms step_avg:96.09ms
step:559/1770 train_time:52755ms step_avg:96.09ms
step:560/1770 train_time:52853ms step_avg:96.10ms
step:561/1770 train_time:52952ms step_avg:96.10ms
step:562/1770 train_time:53050ms step_avg:96.10ms
step:563/1770 train_time:53148ms step_avg:96.11ms
step:564/1770 train_time:53246ms step_avg:96.11ms
step:565/1770 train_time:53344ms step_avg:96.12ms
step:566/1770 train_time:53442ms step_avg:96.12ms
step:567/1770 train_time:53540ms step_avg:96.12ms
step:568/1770 train_time:53638ms step_avg:96.13ms
step:569/1770 train_time:53736ms step_avg:96.13ms
step:570/1770 train_time:53835ms step_avg:96.13ms
step:571/1770 train_time:53933ms step_avg:96.14ms
step:572/1770 train_time:54031ms step_avg:96.14ms
step:573/1770 train_time:54129ms step_avg:96.14ms
step:574/1770 train_time:54227ms step_avg:96.15ms
step:575/1770 train_time:54325ms step_avg:96.15ms
step:576/1770 train_time:54423ms step_avg:96.15ms
step:577/1770 train_time:54521ms step_avg:96.16ms
step:578/1770 train_time:54619ms step_avg:96.16ms
step:579/1770 train_time:54717ms step_avg:96.16ms
step:580/1770 train_time:54815ms step_avg:96.17ms
step:581/1770 train_time:54914ms step_avg:96.17ms
step:582/1770 train_time:55012ms step_avg:96.17ms
step:583/1770 train_time:55110ms step_avg:96.18ms
step:584/1770 train_time:55208ms step_avg:96.18ms
step:585/1770 train_time:55306ms step_avg:96.18ms
step:586/1770 train_time:55404ms step_avg:96.19ms
step:587/1770 train_time:55502ms step_avg:96.19ms
step:588/1770 train_time:55600ms step_avg:96.19ms
step:589/1770 train_time:55698ms step_avg:96.20ms
step:590/1770 train_time:55796ms step_avg:96.20ms
step:591/1770 train_time:55895ms step_avg:96.20ms
step:592/1770 train_time:55997ms step_avg:96.21ms
step:593/1770 train_time:56091ms step_avg:96.21ms
step:594/1770 train_time:56189ms step_avg:96.21ms
step:595/1770 train_time:56287ms step_avg:96.22ms
step:596/1770 train_time:56385ms step_avg:96.22ms
step:597/1770 train_time:56483ms step_avg:96.22ms
step:598/1770 train_time:56580ms step_avg:96.23ms
step:599/1770 train_time:56678ms step_avg:96.23ms
step:600/1770 train_time:56776ms step_avg:96.23ms
step:601/1770 train_time:56875ms step_avg:96.23ms
step:602/1770 train_time:56973ms step_avg:96.24ms
step:603/1770 train_time:57071ms step_avg:96.24ms
step:604/1770 train_time:57168ms step_avg:96.24ms
step:605/1770 train_time:57266ms step_avg:96.25ms
step:606/1770 train_time:57365ms step_avg:96.25ms
step:607/1770 train_time:57462ms step_avg:96.25ms
step:608/1770 train_time:57560ms step_avg:96.25ms
step:609/1770 train_time:57658ms step_avg:96.26ms
step:610/1770 train_time:57756ms step_avg:96.26ms
step:611/1770 train_time:57855ms step_avg:96.26ms
step:612/1770 train_time:57953ms step_avg:96.27ms
step:613/1770 train_time:58051ms step_avg:96.27ms
step:614/1770 train_time:58149ms step_avg:96.27ms
step:615/1770 train_time:58247ms step_avg:96.28ms
step:616/1770 train_time:58345ms step_avg:96.28ms
step:617/1770 train_time:58443ms step_avg:96.28ms
step:618/1770 train_time:58541ms step_avg:96.28ms
step:619/1770 train_time:58639ms step_avg:96.29ms
step:620/1770 train_time:58738ms step_avg:96.29ms
step:621/1770 train_time:58835ms step_avg:96.29ms
step:622/1770 train_time:58933ms step_avg:96.30ms
step:623/1770 train_time:59032ms step_avg:96.30ms
step:624/1770 train_time:59130ms step_avg:96.30ms
step:625/1770 train_time:59228ms step_avg:96.31ms
step:625/1770 val_loss:3.6719 train_time:59324ms step_avg:96.46ms
step:626/1770 train_time:59347ms step_avg:96.34ms
step:627/1770 train_time:59432ms step_avg:96.32ms
step:628/1770 train_time:59531ms step_avg:96.33ms
step:629/1770 train_time:59629ms step_avg:96.33ms
step:630/1770 train_time:59726ms step_avg:96.33ms
step:631/1770 train_time:59824ms step_avg:96.33ms
step:632/1770 train_time:59922ms step_avg:96.34ms
step:633/1770 train_time:60020ms step_avg:96.34ms
step:634/1770 train_time:60118ms step_avg:96.34ms
step:635/1770 train_time:60216ms step_avg:96.35ms
step:636/1770 train_time:60314ms step_avg:96.35ms
step:637/1770 train_time:60412ms step_avg:96.35ms
step:638/1770 train_time:60510ms step_avg:96.35ms
step:639/1770 train_time:60608ms step_avg:96.36ms
step:640/1770 train_time:60706ms step_avg:96.36ms
step:641/1770 train_time:60804ms step_avg:96.36ms
step:642/1770 train_time:60901ms step_avg:96.36ms
step:643/1770 train_time:60999ms step_avg:96.37ms
step:644/1770 train_time:61098ms step_avg:96.37ms
step:645/1770 train_time:61196ms step_avg:96.37ms
step:646/1770 train_time:61294ms step_avg:96.37ms
step:647/1770 train_time:61393ms step_avg:96.38ms
step:648/1770 train_time:61491ms step_avg:96.38ms
step:649/1770 train_time:61589ms step_avg:96.38ms
step:650/1770 train_time:61687ms step_avg:96.39ms
step:651/1770 train_time:61785ms step_avg:96.39ms
step:652/1770 train_time:61883ms step_avg:96.39ms
step:653/1770 train_time:61981ms step_avg:96.39ms
step:654/1770 train_time:62082ms step_avg:96.40ms
step:655/1770 train_time:62177ms step_avg:96.40ms
step:656/1770 train_time:62275ms step_avg:96.40ms
step:657/1770 train_time:62373ms step_avg:96.40ms
step:658/1770 train_time:62473ms step_avg:96.41ms
step:659/1770 train_time:62572ms step_avg:96.41ms
step:660/1770 train_time:62673ms step_avg:96.42ms
step:661/1770 train_time:62773ms step_avg:96.43ms
step:662/1770 train_time:62874ms step_avg:96.43ms
step:663/1770 train_time:62975ms step_avg:96.44ms
step:664/1770 train_time:63075ms step_avg:96.44ms
step:665/1770 train_time:63174ms step_avg:96.45ms
step:666/1770 train_time:63275ms step_avg:96.46ms
step:667/1770 train_time:63374ms step_avg:96.46ms
step:668/1770 train_time:63474ms step_avg:96.46ms
step:669/1770 train_time:63574ms step_avg:96.47ms
step:670/1770 train_time:63674ms step_avg:96.48ms
step:671/1770 train_time:63775ms step_avg:96.48ms
step:672/1770 train_time:63879ms step_avg:96.49ms
step:673/1770 train_time:63975ms step_avg:96.49ms
step:674/1770 train_time:64076ms step_avg:96.50ms
step:675/1770 train_time:64175ms step_avg:96.50ms
step:676/1770 train_time:64275ms step_avg:96.51ms
step:677/1770 train_time:64375ms step_avg:96.51ms
step:678/1770 train_time:64475ms step_avg:96.52ms
step:679/1770 train_time:64576ms step_avg:96.53ms
step:680/1770 train_time:64676ms step_avg:96.53ms
step:681/1770 train_time:64777ms step_avg:96.54ms
step:682/1770 train_time:64879ms step_avg:96.55ms
step:683/1770 train_time:64979ms step_avg:96.55ms
step:684/1770 train_time:65079ms step_avg:96.56ms
step:685/1770 train_time:65179ms step_avg:96.56ms
step:686/1770 train_time:65279ms step_avg:96.57ms
step:687/1770 train_time:65379ms step_avg:96.57ms
step:688/1770 train_time:65480ms step_avg:96.58ms
step:689/1770 train_time:65580ms step_avg:96.58ms
step:690/1770 train_time:65681ms step_avg:96.59ms
step:691/1770 train_time:65781ms step_avg:96.60ms
step:692/1770 train_time:65884ms step_avg:96.60ms
step:693/1770 train_time:65981ms step_avg:96.61ms
step:694/1770 train_time:66081ms step_avg:96.61ms
step:695/1770 train_time:66181ms step_avg:96.61ms
step:696/1770 train_time:66281ms step_avg:96.62ms
step:697/1770 train_time:66381ms step_avg:96.62ms
step:698/1770 train_time:66481ms step_avg:96.63ms
step:699/1770 train_time:66581ms step_avg:96.63ms
step:700/1770 train_time:66681ms step_avg:96.64ms
step:701/1770 train_time:66781ms step_avg:96.64ms
step:702/1770 train_time:66881ms step_avg:96.65ms
step:703/1770 train_time:66981ms step_avg:96.65ms
step:704/1770 train_time:67081ms step_avg:96.66ms
step:705/1770 train_time:67181ms step_avg:96.66ms
step:706/1770 train_time:67281ms step_avg:96.67ms
step:707/1770 train_time:67381ms step_avg:96.67ms
step:708/1770 train_time:67481ms step_avg:96.68ms
step:709/1770 train_time:67581ms step_avg:96.68ms
step:710/1770 train_time:67681ms step_avg:96.69ms
step:711/1770 train_time:67784ms step_avg:96.70ms
step:712/1770 train_time:67882ms step_avg:96.70ms
step:713/1770 train_time:67982ms step_avg:96.70ms
step:714/1770 train_time:68082ms step_avg:96.71ms
step:715/1770 train_time:68182ms step_avg:96.71ms
step:716/1770 train_time:68281ms step_avg:96.72ms
step:717/1770 train_time:68381ms step_avg:96.72ms
step:718/1770 train_time:68481ms step_avg:96.72ms
step:719/1770 train_time:68581ms step_avg:96.73ms
step:720/1770 train_time:68681ms step_avg:96.73ms
step:721/1770 train_time:68781ms step_avg:96.74ms
step:722/1770 train_time:68881ms step_avg:96.74ms
step:723/1770 train_time:68981ms step_avg:96.75ms
step:724/1770 train_time:69081ms step_avg:96.75ms
step:725/1770 train_time:69180ms step_avg:96.76ms
step:726/1770 train_time:69281ms step_avg:96.76ms
step:727/1770 train_time:69380ms step_avg:96.76ms
step:728/1770 train_time:69481ms step_avg:96.77ms
step:729/1770 train_time:69584ms step_avg:96.78ms
step:730/1770 train_time:69681ms step_avg:96.78ms
step:731/1770 train_time:69780ms step_avg:96.78ms
step:732/1770 train_time:69881ms step_avg:96.79ms
step:733/1770 train_time:69981ms step_avg:96.79ms
step:734/1770 train_time:70081ms step_avg:96.80ms
step:735/1770 train_time:70181ms step_avg:96.80ms
step:736/1770 train_time:70281ms step_avg:96.81ms
step:737/1770 train_time:70381ms step_avg:96.81ms
step:738/1770 train_time:70481ms step_avg:96.81ms
step:739/1770 train_time:70581ms step_avg:96.82ms
step:740/1770 train_time:70681ms step_avg:96.82ms
step:741/1770 train_time:70781ms step_avg:96.83ms
step:742/1770 train_time:70881ms step_avg:96.83ms
step:743/1770 train_time:70981ms step_avg:96.84ms
step:744/1770 train_time:71085ms step_avg:96.85ms
step:745/1770 train_time:71181ms step_avg:96.84ms
step:746/1770 train_time:71281ms step_avg:96.85ms
step:747/1770 train_time:71380ms step_avg:96.85ms
step:748/1770 train_time:71481ms step_avg:96.86ms
step:749/1770 train_time:71580ms step_avg:96.86ms
step:750/1770 train_time:71680ms step_avg:96.87ms
step:750/1770 val_loss:3.6068 train_time:71779ms step_avg:97.00ms
step:751/1770 train_time:71800ms step_avg:96.90ms
step:752/1770 train_time:71887ms step_avg:96.88ms
step:753/1770 train_time:71989ms step_avg:96.89ms
step:754/1770 train_time:72088ms step_avg:96.89ms
step:755/1770 train_time:72187ms step_avg:96.90ms
step:756/1770 train_time:72287ms step_avg:96.90ms
step:757/1770 train_time:72387ms step_avg:96.90ms
step:758/1770 train_time:72487ms step_avg:96.91ms
step:759/1770 train_time:72587ms step_avg:96.91ms
step:760/1770 train_time:72686ms step_avg:96.91ms
step:761/1770 train_time:72785ms step_avg:96.92ms
step:762/1770 train_time:72885ms step_avg:96.92ms
step:763/1770 train_time:72986ms step_avg:96.93ms
step:764/1770 train_time:73085ms step_avg:96.93ms
step:765/1770 train_time:73186ms step_avg:96.93ms
step:766/1770 train_time:73285ms step_avg:96.94ms
step:767/1770 train_time:73386ms step_avg:96.94ms
step:768/1770 train_time:73486ms step_avg:96.95ms
step:769/1770 train_time:73586ms step_avg:96.95ms
step:770/1770 train_time:73685ms step_avg:96.95ms
step:771/1770 train_time:73786ms step_avg:96.96ms
step:772/1770 train_time:73885ms step_avg:96.96ms
step:773/1770 train_time:73985ms step_avg:96.97ms
step:774/1770 train_time:74085ms step_avg:96.97ms
step:775/1770 train_time:74185ms step_avg:96.97ms
step:776/1770 train_time:74285ms step_avg:96.98ms
step:777/1770 train_time:74385ms step_avg:96.98ms
step:778/1770 train_time:74485ms step_avg:96.99ms
step:779/1770 train_time:74585ms step_avg:96.99ms
step:780/1770 train_time:74685ms step_avg:96.99ms
step:781/1770 train_time:74785ms step_avg:97.00ms
step:782/1770 train_time:74886ms step_avg:97.00ms
step:783/1770 train_time:74986ms step_avg:97.01ms
step:784/1770 train_time:75086ms step_avg:97.01ms
step:785/1770 train_time:75186ms step_avg:97.01ms
step:786/1770 train_time:75285ms step_avg:97.02ms
step:787/1770 train_time:75385ms step_avg:97.02ms
step:788/1770 train_time:75486ms step_avg:97.03ms
step:789/1770 train_time:75586ms step_avg:97.03ms
step:790/1770 train_time:75686ms step_avg:97.03ms
step:791/1770 train_time:75787ms step_avg:97.04ms
step:792/1770 train_time:75887ms step_avg:97.04ms
step:793/1770 train_time:75987ms step_avg:97.05ms
step:794/1770 train_time:76086ms step_avg:97.05ms
step:795/1770 train_time:76187ms step_avg:97.05ms
step:796/1770 train_time:76286ms step_avg:97.06ms
step:797/1770 train_time:76386ms step_avg:97.06ms
step:798/1770 train_time:76486ms step_avg:97.06ms
step:799/1770 train_time:76586ms step_avg:97.07ms
step:800/1770 train_time:76686ms step_avg:97.07ms
step:801/1770 train_time:76786ms step_avg:97.07ms
step:802/1770 train_time:76886ms step_avg:97.08ms
step:803/1770 train_time:76986ms step_avg:97.08ms
step:804/1770 train_time:77086ms step_avg:97.09ms
step:805/1770 train_time:77186ms step_avg:97.09ms
step:806/1770 train_time:77286ms step_avg:97.09ms
step:807/1770 train_time:77386ms step_avg:97.10ms
step:808/1770 train_time:77486ms step_avg:97.10ms
step:809/1770 train_time:77586ms step_avg:97.10ms
step:810/1770 train_time:77686ms step_avg:97.11ms
step:811/1770 train_time:77786ms step_avg:97.11ms
step:812/1770 train_time:77886ms step_avg:97.11ms
step:813/1770 train_time:77986ms step_avg:97.12ms
step:814/1770 train_time:78087ms step_avg:97.12ms
step:815/1770 train_time:78187ms step_avg:97.13ms
step:816/1770 train_time:78287ms step_avg:97.13ms
step:817/1770 train_time:78387ms step_avg:97.13ms
step:818/1770 train_time:78488ms step_avg:97.14ms
step:819/1770 train_time:78588ms step_avg:97.14ms
step:820/1770 train_time:78688ms step_avg:97.15ms
step:821/1770 train_time:78789ms step_avg:97.15ms
step:822/1770 train_time:78888ms step_avg:97.15ms
step:823/1770 train_time:78989ms step_avg:97.16ms
step:824/1770 train_time:79089ms step_avg:97.16ms
step:825/1770 train_time:79189ms step_avg:97.16ms
step:826/1770 train_time:79289ms step_avg:97.17ms
step:827/1770 train_time:79389ms step_avg:97.17ms
step:828/1770 train_time:79489ms step_avg:97.17ms
step:829/1770 train_time:79589ms step_avg:97.18ms
step:830/1770 train_time:79688ms step_avg:97.18ms
step:831/1770 train_time:79788ms step_avg:97.18ms
step:832/1770 train_time:79888ms step_avg:97.19ms
step:833/1770 train_time:79988ms step_avg:97.19ms
step:834/1770 train_time:80089ms step_avg:97.20ms
step:835/1770 train_time:80188ms step_avg:97.20ms
step:836/1770 train_time:80289ms step_avg:97.20ms
step:837/1770 train_time:80389ms step_avg:97.21ms
step:838/1770 train_time:80489ms step_avg:97.21ms
step:839/1770 train_time:80588ms step_avg:97.21ms
step:840/1770 train_time:80688ms step_avg:97.21ms
step:841/1770 train_time:80788ms step_avg:97.22ms
step:842/1770 train_time:80888ms step_avg:97.22ms
step:843/1770 train_time:80988ms step_avg:97.22ms
step:844/1770 train_time:81088ms step_avg:97.23ms
step:845/1770 train_time:81187ms step_avg:97.23ms
step:846/1770 train_time:81287ms step_avg:97.23ms
step:847/1770 train_time:81388ms step_avg:97.24ms
step:848/1770 train_time:81488ms step_avg:97.24ms
step:849/1770 train_time:81588ms step_avg:97.24ms
step:850/1770 train_time:81689ms step_avg:97.25ms
step:851/1770 train_time:81789ms step_avg:97.25ms
step:852/1770 train_time:81888ms step_avg:97.25ms
step:853/1770 train_time:81988ms step_avg:97.26ms
step:854/1770 train_time:82088ms step_avg:97.26ms
step:855/1770 train_time:82188ms step_avg:97.26ms
step:856/1770 train_time:82288ms step_avg:97.27ms
step:857/1770 train_time:82388ms step_avg:97.27ms
step:858/1770 train_time:82488ms step_avg:97.27ms
step:859/1770 train_time:82588ms step_avg:97.28ms
step:860/1770 train_time:82688ms step_avg:97.28ms
step:861/1770 train_time:82788ms step_avg:97.28ms
step:862/1770 train_time:82889ms step_avg:97.29ms
step:863/1770 train_time:82988ms step_avg:97.29ms
step:864/1770 train_time:83088ms step_avg:97.29ms
step:865/1770 train_time:83188ms step_avg:97.30ms
step:866/1770 train_time:83288ms step_avg:97.30ms
step:867/1770 train_time:83387ms step_avg:97.30ms
step:868/1770 train_time:83488ms step_avg:97.30ms
step:869/1770 train_time:83588ms step_avg:97.31ms
step:870/1770 train_time:83688ms step_avg:97.31ms
step:871/1770 train_time:83789ms step_avg:97.32ms
step:872/1770 train_time:83890ms step_avg:97.32ms
step:873/1770 train_time:83989ms step_avg:97.32ms
step:874/1770 train_time:84089ms step_avg:97.32ms
step:875/1770 train_time:84188ms step_avg:97.33ms
step:875/1770 val_loss:3.5592 train_time:84287ms step_avg:97.44ms
step:876/1770 train_time:84308ms step_avg:97.35ms
step:877/1770 train_time:84392ms step_avg:97.34ms
step:878/1770 train_time:84494ms step_avg:97.34ms
step:879/1770 train_time:84595ms step_avg:97.35ms
step:880/1770 train_time:84695ms step_avg:97.35ms
step:881/1770 train_time:84795ms step_avg:97.35ms
step:882/1770 train_time:84895ms step_avg:97.36ms
step:883/1770 train_time:84995ms step_avg:97.36ms
step:884/1770 train_time:85095ms step_avg:97.36ms
step:885/1770 train_time:85195ms step_avg:97.37ms
step:886/1770 train_time:85295ms step_avg:97.37ms
step:887/1770 train_time:85396ms step_avg:97.37ms
step:888/1770 train_time:85497ms step_avg:97.38ms
step:889/1770 train_time:85596ms step_avg:97.38ms
step:890/1770 train_time:85696ms step_avg:97.38ms
step:891/1770 train_time:85795ms step_avg:97.38ms
step:892/1770 train_time:85896ms step_avg:97.39ms
step:893/1770 train_time:85995ms step_avg:97.39ms
step:894/1770 train_time:86096ms step_avg:97.39ms
step:895/1770 train_time:86196ms step_avg:97.40ms
step:896/1770 train_time:86295ms step_avg:97.40ms
step:897/1770 train_time:86395ms step_avg:97.40ms
step:898/1770 train_time:86496ms step_avg:97.41ms
step:899/1770 train_time:86596ms step_avg:97.41ms
step:900/1770 train_time:86696ms step_avg:97.41ms
step:901/1770 train_time:86796ms step_avg:97.41ms
step:902/1770 train_time:86896ms step_avg:97.42ms
step:903/1770 train_time:86996ms step_avg:97.42ms
step:904/1770 train_time:87096ms step_avg:97.42ms
step:905/1770 train_time:87195ms step_avg:97.42ms
step:906/1770 train_time:87295ms step_avg:97.43ms
step:907/1770 train_time:87395ms step_avg:97.43ms
step:908/1770 train_time:87496ms step_avg:97.43ms
step:909/1770 train_time:87596ms step_avg:97.44ms
step:910/1770 train_time:87696ms step_avg:97.44ms
step:911/1770 train_time:87796ms step_avg:97.44ms
step:912/1770 train_time:87895ms step_avg:97.45ms
step:913/1770 train_time:87996ms step_avg:97.45ms
step:914/1770 train_time:88096ms step_avg:97.45ms
step:915/1770 train_time:88196ms step_avg:97.45ms
step:916/1770 train_time:88296ms step_avg:97.46ms
step:917/1770 train_time:88396ms step_avg:97.46ms
step:918/1770 train_time:88496ms step_avg:97.46ms
step:919/1770 train_time:88596ms step_avg:97.47ms
step:920/1770 train_time:88699ms step_avg:97.47ms
step:921/1770 train_time:88800ms step_avg:97.48ms
step:922/1770 train_time:88902ms step_avg:97.48ms
step:923/1770 train_time:89003ms step_avg:97.48ms
step:924/1770 train_time:89104ms step_avg:97.49ms
step:925/1770 train_time:89205ms step_avg:97.49ms
step:926/1770 train_time:89307ms step_avg:97.50ms
step:927/1770 train_time:89406ms step_avg:97.50ms
step:928/1770 train_time:89509ms step_avg:97.50ms
step:929/1770 train_time:89610ms step_avg:97.51ms
step:930/1770 train_time:89712ms step_avg:97.51ms
step:931/1770 train_time:89815ms step_avg:97.52ms
step:932/1770 train_time:89917ms step_avg:97.52ms
step:933/1770 train_time:90018ms step_avg:97.53ms
step:934/1770 train_time:90119ms step_avg:97.53ms
step:935/1770 train_time:90219ms step_avg:97.53ms
step:936/1770 train_time:90320ms step_avg:97.54ms
step:937/1770 train_time:90421ms step_avg:97.54ms
step:938/1770 train_time:90522ms step_avg:97.55ms
step:939/1770 train_time:90623ms step_avg:97.55ms
step:940/1770 train_time:90724ms step_avg:97.55ms
step:941/1770 train_time:90825ms step_avg:97.56ms
step:942/1770 train_time:90928ms step_avg:97.56ms
step:943/1770 train_time:91028ms step_avg:97.57ms
step:944/1770 train_time:91129ms step_avg:97.57ms
step:945/1770 train_time:91231ms step_avg:97.57ms
step:946/1770 train_time:91333ms step_avg:97.58ms
step:947/1770 train_time:91436ms step_avg:97.58ms
step:948/1770 train_time:91537ms step_avg:97.59ms
step:949/1770 train_time:91638ms step_avg:97.59ms
step:950/1770 train_time:91740ms step_avg:97.60ms
step:951/1770 train_time:91841ms step_avg:97.60ms
step:952/1770 train_time:91942ms step_avg:97.60ms
step:953/1770 train_time:92044ms step_avg:97.61ms
step:954/1770 train_time:92144ms step_avg:97.61ms
step:955/1770 train_time:92246ms step_avg:97.61ms
step:956/1770 train_time:92347ms step_avg:97.62ms
step:957/1770 train_time:92448ms step_avg:97.62ms
step:958/1770 train_time:92550ms step_avg:97.63ms
step:959/1770 train_time:92651ms step_avg:97.63ms
step:960/1770 train_time:92753ms step_avg:97.64ms
step:961/1770 train_time:92855ms step_avg:97.64ms
step:962/1770 train_time:92957ms step_avg:97.64ms
step:963/1770 train_time:93058ms step_avg:97.65ms
step:964/1770 train_time:93159ms step_avg:97.65ms
step:965/1770 train_time:93260ms step_avg:97.65ms
step:966/1770 train_time:93361ms step_avg:97.66ms
step:967/1770 train_time:93462ms step_avg:97.66ms
step:968/1770 train_time:93563ms step_avg:97.66ms
step:969/1770 train_time:93664ms step_avg:97.67ms
step:970/1770 train_time:93765ms step_avg:97.67ms
step:971/1770 train_time:93866ms step_avg:97.68ms
step:972/1770 train_time:93967ms step_avg:97.68ms
step:973/1770 train_time:94068ms step_avg:97.68ms
step:974/1770 train_time:94170ms step_avg:97.69ms
step:975/1770 train_time:94271ms step_avg:97.69ms
step:976/1770 train_time:94373ms step_avg:97.69ms
step:977/1770 train_time:94476ms step_avg:97.70ms
step:978/1770 train_time:94578ms step_avg:97.70ms
step:979/1770 train_time:94680ms step_avg:97.71ms
step:980/1770 train_time:94781ms step_avg:97.71ms
step:981/1770 train_time:94882ms step_avg:97.72ms
step:982/1770 train_time:94983ms step_avg:97.72ms
step:983/1770 train_time:95085ms step_avg:97.72ms
step:984/1770 train_time:95186ms step_avg:97.73ms
step:985/1770 train_time:95287ms step_avg:97.73ms
step:986/1770 train_time:95388ms step_avg:97.73ms
step:987/1770 train_time:95491ms step_avg:97.74ms
step:988/1770 train_time:95593ms step_avg:97.74ms
step:989/1770 train_time:95698ms step_avg:97.75ms
step:990/1770 train_time:95799ms step_avg:97.75ms
step:991/1770 train_time:95900ms step_avg:97.76ms
step:992/1770 train_time:96001ms step_avg:97.76ms
step:993/1770 train_time:96102ms step_avg:97.76ms
step:994/1770 train_time:96204ms step_avg:97.77ms
step:995/1770 train_time:96304ms step_avg:97.77ms
step:996/1770 train_time:96405ms step_avg:97.77ms
step:997/1770 train_time:96506ms step_avg:97.78ms
step:998/1770 train_time:96607ms step_avg:97.78ms
step:999/1770 train_time:96709ms step_avg:97.78ms
step:1000/1770 train_time:96811ms step_avg:97.79ms
step:1000/1770 val_loss:3.5191 train_time:96912ms step_avg:97.89ms
step:1001/1770 train_time:96933ms step_avg:97.81ms
step:1002/1770 train_time:97026ms step_avg:97.81ms
step:1003/1770 train_time:97128ms step_avg:97.81ms
step:1004/1770 train_time:97230ms step_avg:97.82ms
step:1005/1770 train_time:97330ms step_avg:97.82ms
step:1006/1770 train_time:97431ms step_avg:97.82ms
step:1007/1770 train_time:97532ms step_avg:97.83ms
step:1008/1770 train_time:97632ms step_avg:97.83ms
step:1009/1770 train_time:97733ms step_avg:97.83ms
step:1010/1770 train_time:97833ms step_avg:97.83ms
step:1011/1770 train_time:97936ms step_avg:97.84ms
step:1012/1770 train_time:98040ms step_avg:97.84ms
step:1013/1770 train_time:98143ms step_avg:97.85ms
step:1014/1770 train_time:98245ms step_avg:97.85ms
step:1015/1770 train_time:98346ms step_avg:97.86ms
step:1016/1770 train_time:98447ms step_avg:97.86ms
step:1017/1770 train_time:98548ms step_avg:97.86ms
step:1018/1770 train_time:98649ms step_avg:97.87ms
step:1019/1770 train_time:98749ms step_avg:97.87ms
step:1020/1770 train_time:98850ms step_avg:97.87ms
step:1021/1770 train_time:98952ms step_avg:97.88ms
step:1022/1770 train_time:99053ms step_avg:97.88ms
step:1023/1770 train_time:99154ms step_avg:97.88ms
step:1024/1770 train_time:99257ms step_avg:97.89ms
step:1025/1770 train_time:99358ms step_avg:97.89ms
step:1026/1770 train_time:99460ms step_avg:97.89ms
step:1027/1770 train_time:99563ms step_avg:97.90ms
step:1028/1770 train_time:99664ms step_avg:97.90ms
step:1029/1770 train_time:99765ms step_avg:97.90ms
step:1030/1770 train_time:99866ms step_avg:97.91ms
step:1031/1770 train_time:99967ms step_avg:97.91ms
step:1032/1770 train_time:100068ms step_avg:97.91ms
step:1033/1770 train_time:100169ms step_avg:97.92ms
step:1034/1770 train_time:100270ms step_avg:97.92ms
step:1035/1770 train_time:100371ms step_avg:97.92ms
step:1036/1770 train_time:100472ms step_avg:97.93ms
step:1037/1770 train_time:100573ms step_avg:97.93ms
step:1038/1770 train_time:100677ms step_avg:97.94ms
step:1039/1770 train_time:100776ms step_avg:97.94ms
step:1040/1770 train_time:100878ms step_avg:97.94ms
step:1041/1770 train_time:100981ms step_avg:97.94ms
step:1042/1770 train_time:101082ms step_avg:97.95ms
step:1043/1770 train_time:101185ms step_avg:97.95ms
step:1044/1770 train_time:101286ms step_avg:97.96ms
step:1045/1770 train_time:101386ms step_avg:97.96ms
step:1046/1770 train_time:101487ms step_avg:97.96ms
step:1047/1770 train_time:101588ms step_avg:97.96ms
step:1048/1770 train_time:101689ms step_avg:97.97ms
step:1049/1770 train_time:101790ms step_avg:97.97ms
step:1050/1770 train_time:101891ms step_avg:97.97ms
step:1051/1770 train_time:101993ms step_avg:97.98ms
step:1052/1770 train_time:102094ms step_avg:97.98ms
step:1053/1770 train_time:102196ms step_avg:97.98ms
step:1054/1770 train_time:102297ms step_avg:97.99ms
step:1055/1770 train_time:102399ms step_avg:97.99ms
step:1056/1770 train_time:102501ms step_avg:97.99ms
step:1057/1770 train_time:102603ms step_avg:98.00ms
step:1058/1770 train_time:102707ms step_avg:98.00ms
step:1059/1770 train_time:102809ms step_avg:98.01ms
step:1060/1770 train_time:102910ms step_avg:98.01ms
step:1061/1770 train_time:103012ms step_avg:98.01ms
step:1062/1770 train_time:103114ms step_avg:98.02ms
step:1063/1770 train_time:103217ms step_avg:98.02ms
step:1064/1770 train_time:103319ms step_avg:98.03ms
step:1065/1770 train_time:103421ms step_avg:98.03ms
step:1066/1770 train_time:103523ms step_avg:98.03ms
step:1067/1770 train_time:103625ms step_avg:98.04ms
step:1068/1770 train_time:103727ms step_avg:98.04ms
step:1069/1770 train_time:103828ms step_avg:98.04ms
step:1070/1770 train_time:103929ms step_avg:98.05ms
step:1071/1770 train_time:104031ms step_avg:98.05ms
step:1072/1770 train_time:104132ms step_avg:98.05ms
step:1073/1770 train_time:104233ms step_avg:98.06ms
step:1074/1770 train_time:104334ms step_avg:98.06ms
step:1075/1770 train_time:104437ms step_avg:98.06ms
step:1076/1770 train_time:104542ms step_avg:98.07ms
step:1077/1770 train_time:104644ms step_avg:98.07ms
step:1078/1770 train_time:104745ms step_avg:98.08ms
step:1079/1770 train_time:104846ms step_avg:98.08ms
step:1080/1770 train_time:104947ms step_avg:98.08ms
step:1081/1770 train_time:105048ms step_avg:98.08ms
step:1082/1770 train_time:105149ms step_avg:98.09ms
step:1083/1770 train_time:105250ms step_avg:98.09ms
step:1084/1770 train_time:105351ms step_avg:98.09ms
step:1085/1770 train_time:105453ms step_avg:98.10ms
step:1086/1770 train_time:105554ms step_avg:98.10ms
step:1087/1770 train_time:105657ms step_avg:98.10ms
step:1088/1770 train_time:105759ms step_avg:98.11ms
step:1089/1770 train_time:105861ms step_avg:98.11ms
step:1090/1770 train_time:105962ms step_avg:98.11ms
step:1091/1770 train_time:106064ms step_avg:98.12ms
step:1092/1770 train_time:106165ms step_avg:98.12ms
step:1093/1770 train_time:106266ms step_avg:98.12ms
step:1094/1770 train_time:106369ms step_avg:98.13ms
step:1095/1770 train_time:106470ms step_avg:98.13ms
step:1096/1770 train_time:106572ms step_avg:98.13ms
step:1097/1770 train_time:106673ms step_avg:98.14ms
step:1098/1770 train_time:106774ms step_avg:98.14ms
step:1099/1770 train_time:106875ms step_avg:98.14ms
step:1100/1770 train_time:106978ms step_avg:98.14ms
step:1101/1770 train_time:107082ms step_avg:98.15ms
step:1102/1770 train_time:107184ms step_avg:98.15ms
step:1103/1770 train_time:107286ms step_avg:98.16ms
step:1104/1770 train_time:107389ms step_avg:98.16ms
step:1105/1770 train_time:107490ms step_avg:98.16ms
step:1106/1770 train_time:107591ms step_avg:98.17ms
step:1107/1770 train_time:107692ms step_avg:98.17ms
step:1108/1770 train_time:107793ms step_avg:98.17ms
step:1109/1770 train_time:107894ms step_avg:98.17ms
step:1110/1770 train_time:107996ms step_avg:98.18ms
step:1111/1770 train_time:108098ms step_avg:98.18ms
step:1112/1770 train_time:108202ms step_avg:98.19ms
step:1113/1770 train_time:108304ms step_avg:98.19ms
step:1114/1770 train_time:108406ms step_avg:98.19ms
step:1115/1770 train_time:108508ms step_avg:98.20ms
step:1116/1770 train_time:108609ms step_avg:98.20ms
step:1117/1770 train_time:108711ms step_avg:98.20ms
step:1118/1770 train_time:108812ms step_avg:98.21ms
step:1119/1770 train_time:108913ms step_avg:98.21ms
step:1120/1770 train_time:109015ms step_avg:98.21ms
step:1121/1770 train_time:109116ms step_avg:98.21ms
step:1122/1770 train_time:109218ms step_avg:98.22ms
step:1123/1770 train_time:109321ms step_avg:98.22ms
step:1124/1770 train_time:109423ms step_avg:98.23ms
step:1125/1770 train_time:109525ms step_avg:98.23ms
step:1125/1770 val_loss:3.4806 train_time:109625ms step_avg:98.32ms
step:1126/1770 train_time:109649ms step_avg:98.25ms
step:1127/1770 train_time:109735ms step_avg:98.24ms
step:1128/1770 train_time:109838ms step_avg:98.25ms
step:1129/1770 train_time:109940ms step_avg:98.25ms
step:1130/1770 train_time:110042ms step_avg:98.25ms
step:1131/1770 train_time:110143ms step_avg:98.25ms
step:1132/1770 train_time:110245ms step_avg:98.26ms
step:1133/1770 train_time:110346ms step_avg:98.26ms
step:1134/1770 train_time:110448ms step_avg:98.26ms
step:1135/1770 train_time:110549ms step_avg:98.27ms
step:1136/1770 train_time:110652ms step_avg:98.27ms
step:1137/1770 train_time:110755ms step_avg:98.27ms
step:1138/1770 train_time:110857ms step_avg:98.28ms
step:1139/1770 train_time:110958ms step_avg:98.28ms
step:1140/1770 train_time:111059ms step_avg:98.28ms
step:1141/1770 train_time:111161ms step_avg:98.29ms
step:1142/1770 train_time:111262ms step_avg:98.29ms
step:1143/1770 train_time:111363ms step_avg:98.29ms
step:1144/1770 train_time:111465ms step_avg:98.29ms
step:1145/1770 train_time:111566ms step_avg:98.30ms
step:1146/1770 train_time:111668ms step_avg:98.30ms
step:1147/1770 train_time:111770ms step_avg:98.30ms
step:1148/1770 train_time:111871ms step_avg:98.30ms
step:1149/1770 train_time:111972ms step_avg:98.31ms
step:1150/1770 train_time:112074ms step_avg:98.31ms
step:1151/1770 train_time:112177ms step_avg:98.31ms
step:1152/1770 train_time:112279ms step_avg:98.32ms
step:1153/1770 train_time:112381ms step_avg:98.32ms
step:1154/1770 train_time:112482ms step_avg:98.32ms
step:1155/1770 train_time:112584ms step_avg:98.33ms
step:1156/1770 train_time:112684ms step_avg:98.33ms
step:1157/1770 train_time:112787ms step_avg:98.33ms
step:1158/1770 train_time:112889ms step_avg:98.34ms
step:1159/1770 train_time:112990ms step_avg:98.34ms
step:1160/1770 train_time:113091ms step_avg:98.34ms
step:1161/1770 train_time:113193ms step_avg:98.34ms
step:1162/1770 train_time:113295ms step_avg:98.35ms
step:1163/1770 train_time:113397ms step_avg:98.35ms
step:1164/1770 train_time:113499ms step_avg:98.35ms
step:1165/1770 train_time:113601ms step_avg:98.36ms
step:1166/1770 train_time:113703ms step_avg:98.36ms
step:1167/1770 train_time:113804ms step_avg:98.36ms
step:1168/1770 train_time:113905ms step_avg:98.36ms
step:1169/1770 train_time:114006ms step_avg:98.37ms
step:1170/1770 train_time:114107ms step_avg:98.37ms
step:1171/1770 train_time:114208ms step_avg:98.37ms
step:1172/1770 train_time:114310ms step_avg:98.37ms
step:1173/1770 train_time:114411ms step_avg:98.38ms
step:1174/1770 train_time:114513ms step_avg:98.38ms
step:1175/1770 train_time:114616ms step_avg:98.38ms
step:1176/1770 train_time:114719ms step_avg:98.39ms
step:1177/1770 train_time:114821ms step_avg:98.39ms
step:1178/1770 train_time:114922ms step_avg:98.39ms
step:1179/1770 train_time:115023ms step_avg:98.39ms
step:1180/1770 train_time:115125ms step_avg:98.40ms
step:1181/1770 train_time:115226ms step_avg:98.40ms
step:1182/1770 train_time:115327ms step_avg:98.40ms
step:1183/1770 train_time:115430ms step_avg:98.41ms
step:1184/1770 train_time:115534ms step_avg:98.41ms
step:1185/1770 train_time:115636ms step_avg:98.41ms
step:1186/1770 train_time:115740ms step_avg:98.42ms
step:1187/1770 train_time:115845ms step_avg:98.42ms
step:1188/1770 train_time:115947ms step_avg:98.43ms
step:1189/1770 train_time:116050ms step_avg:98.43ms
step:1190/1770 train_time:116152ms step_avg:98.43ms
step:1191/1770 train_time:116255ms step_avg:98.44ms
step:1192/1770 train_time:116358ms step_avg:98.44ms
step:1193/1770 train_time:116461ms step_avg:98.45ms
step:1194/1770 train_time:116563ms step_avg:98.45ms
step:1195/1770 train_time:116665ms step_avg:98.45ms
step:1196/1770 train_time:116769ms step_avg:98.46ms
step:1197/1770 train_time:116872ms step_avg:98.46ms
step:1198/1770 train_time:116975ms step_avg:98.46ms
step:1199/1770 train_time:117079ms step_avg:98.47ms
step:1200/1770 train_time:117182ms step_avg:98.47ms
step:1201/1770 train_time:117284ms step_avg:98.48ms
step:1202/1770 train_time:117386ms step_avg:98.48ms
step:1203/1770 train_time:117489ms step_avg:98.48ms
step:1204/1770 train_time:117591ms step_avg:98.49ms
step:1205/1770 train_time:117694ms step_avg:98.49ms
step:1206/1770 train_time:117796ms step_avg:98.49ms
step:1207/1770 train_time:117900ms step_avg:98.50ms
step:1208/1770 train_time:118002ms step_avg:98.50ms
step:1209/1770 train_time:118105ms step_avg:98.50ms
step:1210/1770 train_time:118207ms step_avg:98.51ms
step:1211/1770 train_time:118309ms step_avg:98.51ms
step:1212/1770 train_time:118414ms step_avg:98.51ms
step:1213/1770 train_time:118517ms step_avg:98.52ms
step:1214/1770 train_time:118620ms step_avg:98.52ms
step:1215/1770 train_time:118724ms step_avg:98.53ms
step:1216/1770 train_time:118828ms step_avg:98.53ms
step:1217/1770 train_time:118932ms step_avg:98.54ms
step:1218/1770 train_time:119036ms step_avg:98.54ms
step:1219/1770 train_time:119139ms step_avg:98.54ms
step:1220/1770 train_time:119241ms step_avg:98.55ms
step:1221/1770 train_time:119343ms step_avg:98.55ms
step:1222/1770 train_time:119447ms step_avg:98.55ms
step:1223/1770 train_time:119551ms step_avg:98.56ms
step:1224/1770 train_time:119653ms step_avg:98.56ms
step:1225/1770 train_time:119757ms step_avg:98.57ms
step:1226/1770 train_time:119859ms step_avg:98.57ms
step:1227/1770 train_time:119964ms step_avg:98.57ms
step:1228/1770 train_time:120068ms step_avg:98.58ms
step:1229/1770 train_time:120170ms step_avg:98.58ms
step:1230/1770 train_time:120273ms step_avg:98.58ms
step:1231/1770 train_time:120376ms step_avg:98.59ms
step:1232/1770 train_time:120479ms step_avg:98.59ms
step:1233/1770 train_time:120581ms step_avg:98.59ms
step:1234/1770 train_time:120683ms step_avg:98.60ms
step:1235/1770 train_time:120785ms step_avg:98.60ms
step:1236/1770 train_time:120888ms step_avg:98.60ms
step:1237/1770 train_time:120991ms step_avg:98.61ms
step:1238/1770 train_time:121094ms step_avg:98.61ms
step:1239/1770 train_time:121197ms step_avg:98.61ms
step:1240/1770 train_time:121300ms step_avg:98.62ms
step:1241/1770 train_time:121403ms step_avg:98.62ms
step:1242/1770 train_time:121506ms step_avg:98.63ms
step:1243/1770 train_time:121608ms step_avg:98.63ms
step:1244/1770 train_time:121710ms step_avg:98.63ms
step:1245/1770 train_time:121813ms step_avg:98.63ms
step:1246/1770 train_time:121917ms step_avg:98.64ms
step:1247/1770 train_time:122020ms step_avg:98.64ms
step:1248/1770 train_time:122124ms step_avg:98.65ms
step:1249/1770 train_time:122226ms step_avg:98.65ms
step:1250/1770 train_time:122328ms step_avg:98.65ms
step:1250/1770 val_loss:3.4335 train_time:122431ms step_avg:98.73ms
step:1251/1770 train_time:122452ms step_avg:98.67ms
step:1252/1770 train_time:122543ms step_avg:98.67ms
step:1253/1770 train_time:122646ms step_avg:98.67ms
step:1254/1770 train_time:122750ms step_avg:98.67ms
step:1255/1770 train_time:122854ms step_avg:98.68ms
step:1256/1770 train_time:122957ms step_avg:98.68ms
step:1257/1770 train_time:123059ms step_avg:98.68ms
step:1258/1770 train_time:123163ms step_avg:98.69ms
step:1259/1770 train_time:123266ms step_avg:98.69ms
step:1260/1770 train_time:123368ms step_avg:98.69ms
step:1261/1770 train_time:123472ms step_avg:98.70ms
step:1262/1770 train_time:123575ms step_avg:98.70ms
step:1263/1770 train_time:123678ms step_avg:98.71ms
step:1264/1770 train_time:123782ms step_avg:98.71ms
step:1265/1770 train_time:123884ms step_avg:98.71ms
step:1266/1770 train_time:123988ms step_avg:98.72ms
step:1267/1770 train_time:124091ms step_avg:98.72ms
step:1268/1770 train_time:124194ms step_avg:98.72ms
step:1269/1770 train_time:124296ms step_avg:98.73ms
step:1270/1770 train_time:124399ms step_avg:98.73ms
step:1271/1770 train_time:124503ms step_avg:98.73ms
step:1272/1770 train_time:124606ms step_avg:98.74ms
step:1273/1770 train_time:124709ms step_avg:98.74ms
step:1274/1770 train_time:124811ms step_avg:98.74ms
step:1275/1770 train_time:124913ms step_avg:98.75ms
step:1276/1770 train_time:125016ms step_avg:98.75ms
step:1277/1770 train_time:125120ms step_avg:98.75ms
step:1278/1770 train_time:125224ms step_avg:98.76ms
step:1279/1770 train_time:125328ms step_avg:98.76ms
step:1280/1770 train_time:125431ms step_avg:98.76ms
step:1281/1770 train_time:125533ms step_avg:98.77ms
step:1282/1770 train_time:125636ms step_avg:98.77ms
step:1283/1770 train_time:125739ms step_avg:98.77ms
step:1284/1770 train_time:125843ms step_avg:98.78ms
step:1285/1770 train_time:125947ms step_avg:98.78ms
step:1286/1770 train_time:126051ms step_avg:98.79ms
step:1287/1770 train_time:126156ms step_avg:98.79ms
step:1288/1770 train_time:126259ms step_avg:98.79ms
step:1289/1770 train_time:126362ms step_avg:98.80ms
step:1290/1770 train_time:126465ms step_avg:98.80ms
step:1291/1770 train_time:126567ms step_avg:98.80ms
step:1292/1770 train_time:126670ms step_avg:98.81ms
step:1293/1770 train_time:126773ms step_avg:98.81ms
step:1294/1770 train_time:126875ms step_avg:98.81ms
step:1295/1770 train_time:126978ms step_avg:98.82ms
step:1296/1770 train_time:127081ms step_avg:98.82ms
step:1297/1770 train_time:127184ms step_avg:98.82ms
step:1298/1770 train_time:127287ms step_avg:98.83ms
step:1299/1770 train_time:127389ms step_avg:98.83ms
step:1300/1770 train_time:127492ms step_avg:98.83ms
step:1301/1770 train_time:127595ms step_avg:98.83ms
step:1302/1770 train_time:127698ms step_avg:98.84ms
step:1303/1770 train_time:127801ms step_avg:98.84ms
step:1304/1770 train_time:127904ms step_avg:98.84ms
step:1305/1770 train_time:128007ms step_avg:98.85ms
step:1306/1770 train_time:128109ms step_avg:98.85ms
step:1307/1770 train_time:128211ms step_avg:98.85ms
step:1308/1770 train_time:128314ms step_avg:98.85ms
step:1309/1770 train_time:128417ms step_avg:98.86ms
step:1310/1770 train_time:128519ms step_avg:98.86ms
step:1311/1770 train_time:128622ms step_avg:98.86ms
step:1312/1770 train_time:128725ms step_avg:98.87ms
step:1313/1770 train_time:128827ms step_avg:98.87ms
step:1314/1770 train_time:128929ms step_avg:98.87ms
step:1315/1770 train_time:129032ms step_avg:98.87ms
step:1316/1770 train_time:129134ms step_avg:98.88ms
step:1317/1770 train_time:129237ms step_avg:98.88ms
step:1318/1770 train_time:129343ms step_avg:98.89ms
step:1319/1770 train_time:129447ms step_avg:98.89ms
step:1320/1770 train_time:129549ms step_avg:98.89ms
step:1321/1770 train_time:129652ms step_avg:98.90ms
step:1322/1770 train_time:129755ms step_avg:98.90ms
step:1323/1770 train_time:129859ms step_avg:98.90ms
step:1324/1770 train_time:129964ms step_avg:98.91ms
step:1325/1770 train_time:130068ms step_avg:98.91ms
step:1326/1770 train_time:130171ms step_avg:98.91ms
step:1327/1770 train_time:130276ms step_avg:98.92ms
step:1328/1770 train_time:130378ms step_avg:98.92ms
step:1329/1770 train_time:130481ms step_avg:98.92ms
step:1330/1770 train_time:130584ms step_avg:98.93ms
step:1331/1770 train_time:130687ms step_avg:98.93ms
step:1332/1770 train_time:130790ms step_avg:98.93ms
step:1333/1770 train_time:130891ms step_avg:98.94ms
step:1334/1770 train_time:130994ms step_avg:98.94ms
step:1335/1770 train_time:131096ms step_avg:98.94ms
step:1336/1770 train_time:131199ms step_avg:98.94ms
step:1337/1770 train_time:131302ms step_avg:98.95ms
step:1338/1770 train_time:131405ms step_avg:98.95ms
step:1339/1770 train_time:131509ms step_avg:98.95ms
step:1340/1770 train_time:131614ms step_avg:98.96ms
step:1341/1770 train_time:131716ms step_avg:98.96ms
step:1342/1770 train_time:131820ms step_avg:98.96ms
step:1343/1770 train_time:131923ms step_avg:98.97ms
step:1344/1770 train_time:132027ms step_avg:98.97ms
step:1345/1770 train_time:132130ms step_avg:98.97ms
step:1346/1770 train_time:132232ms step_avg:98.98ms
step:1347/1770 train_time:132335ms step_avg:98.98ms
step:1348/1770 train_time:132440ms step_avg:98.98ms
step:1349/1770 train_time:132544ms step_avg:98.99ms
step:1350/1770 train_time:132647ms step_avg:98.99ms
step:1351/1770 train_time:132750ms step_avg:98.99ms
step:1352/1770 train_time:132852ms step_avg:99.00ms
step:1353/1770 train_time:132956ms step_avg:99.00ms
step:1354/1770 train_time:133059ms step_avg:99.00ms
step:1355/1770 train_time:133163ms step_avg:99.01ms
step:1356/1770 train_time:133265ms step_avg:99.01ms
step:1357/1770 train_time:133368ms step_avg:99.01ms
step:1358/1770 train_time:133471ms step_avg:99.01ms
step:1359/1770 train_time:133574ms step_avg:99.02ms
step:1360/1770 train_time:133677ms step_avg:99.02ms
step:1361/1770 train_time:133781ms step_avg:99.02ms
step:1362/1770 train_time:133889ms step_avg:99.03ms
step:1363/1770 train_time:133987ms step_avg:99.03ms
step:1364/1770 train_time:134090ms step_avg:99.03ms
step:1365/1770 train_time:134192ms step_avg:99.03ms
step:1366/1770 train_time:134295ms step_avg:99.04ms
step:1367/1770 train_time:134398ms step_avg:99.04ms
step:1368/1770 train_time:134500ms step_avg:99.04ms
step:1369/1770 train_time:134605ms step_avg:99.05ms
step:1370/1770 train_time:134708ms step_avg:99.05ms
step:1371/1770 train_time:134810ms step_avg:99.05ms
step:1372/1770 train_time:134913ms step_avg:99.06ms
step:1373/1770 train_time:135015ms step_avg:99.06ms
step:1374/1770 train_time:135119ms step_avg:99.06ms
step:1375/1770 train_time:135222ms step_avg:99.06ms
step:1375/1770 val_loss:3.3920 train_time:135325ms step_avg:99.14ms
step:1376/1770 train_time:135346ms step_avg:99.08ms
step:1377/1770 train_time:135435ms step_avg:99.07ms
step:1378/1770 train_time:135537ms step_avg:99.08ms
step:1379/1770 train_time:135639ms step_avg:99.08ms
step:1380/1770 train_time:135742ms step_avg:99.08ms
step:1381/1770 train_time:135845ms step_avg:99.08ms
step:1382/1770 train_time:135948ms step_avg:99.09ms
step:1383/1770 train_time:136052ms step_avg:99.09ms
step:1384/1770 train_time:136154ms step_avg:99.09ms
step:1385/1770 train_time:136257ms step_avg:99.10ms
step:1386/1770 train_time:136360ms step_avg:99.10ms
step:1387/1770 train_time:136464ms step_avg:99.10ms
step:1388/1770 train_time:136567ms step_avg:99.11ms
step:1389/1770 train_time:136671ms step_avg:99.11ms
step:1390/1770 train_time:136773ms step_avg:99.11ms
step:1391/1770 train_time:136876ms step_avg:99.11ms
step:1392/1770 train_time:136979ms step_avg:99.12ms
step:1393/1770 train_time:137082ms step_avg:99.12ms
step:1394/1770 train_time:137184ms step_avg:99.12ms
step:1395/1770 train_time:137288ms step_avg:99.12ms
step:1396/1770 train_time:137394ms step_avg:99.13ms
step:1397/1770 train_time:137496ms step_avg:99.13ms
step:1398/1770 train_time:137599ms step_avg:99.13ms
step:1399/1770 train_time:137701ms step_avg:99.14ms
step:1400/1770 train_time:137805ms step_avg:99.14ms
step:1401/1770 train_time:137910ms step_avg:99.14ms
step:1402/1770 train_time:138013ms step_avg:99.15ms
step:1403/1770 train_time:138115ms step_avg:99.15ms
step:1404/1770 train_time:138218ms step_avg:99.15ms
step:1405/1770 train_time:138320ms step_avg:99.15ms
step:1406/1770 train_time:138423ms step_avg:99.16ms
step:1407/1770 train_time:138527ms step_avg:99.16ms
step:1408/1770 train_time:138631ms step_avg:99.16ms
step:1409/1770 train_time:138734ms step_avg:99.17ms
step:1410/1770 train_time:138837ms step_avg:99.17ms
step:1411/1770 train_time:138941ms step_avg:99.17ms
step:1412/1770 train_time:139044ms step_avg:99.18ms
step:1413/1770 train_time:139147ms step_avg:99.18ms
step:1414/1770 train_time:139250ms step_avg:99.18ms
step:1415/1770 train_time:139353ms step_avg:99.18ms
step:1416/1770 train_time:139456ms step_avg:99.19ms
step:1417/1770 train_time:139558ms step_avg:99.19ms
step:1418/1770 train_time:139660ms step_avg:99.19ms
step:1419/1770 train_time:139764ms step_avg:99.19ms
step:1420/1770 train_time:139867ms step_avg:99.20ms
step:1421/1770 train_time:139971ms step_avg:99.20ms
step:1422/1770 train_time:140074ms step_avg:99.20ms
step:1423/1770 train_time:140176ms step_avg:99.20ms
step:1424/1770 train_time:140280ms step_avg:99.21ms
step:1425/1770 train_time:140382ms step_avg:99.21ms
step:1426/1770 train_time:140486ms step_avg:99.21ms
step:1427/1770 train_time:140589ms step_avg:99.22ms
step:1428/1770 train_time:140694ms step_avg:99.22ms
step:1429/1770 train_time:140796ms step_avg:99.22ms
step:1430/1770 train_time:140899ms step_avg:99.22ms
step:1431/1770 train_time:141003ms step_avg:99.23ms
step:1432/1770 train_time:141107ms step_avg:99.23ms
step:1433/1770 train_time:141211ms step_avg:99.23ms
step:1434/1770 train_time:141312ms step_avg:99.24ms
step:1435/1770 train_time:141415ms step_avg:99.24ms
step:1436/1770 train_time:141520ms step_avg:99.24ms
step:1437/1770 train_time:141622ms step_avg:99.24ms
step:1438/1770 train_time:141725ms step_avg:99.25ms
step:1439/1770 train_time:141828ms step_avg:99.25ms
step:1440/1770 train_time:141930ms step_avg:99.25ms
step:1441/1770 train_time:142035ms step_avg:99.26ms
step:1442/1770 train_time:142137ms step_avg:99.26ms
step:1443/1770 train_time:142240ms step_avg:99.26ms
step:1444/1770 train_time:142344ms step_avg:99.26ms
step:1445/1770 train_time:142448ms step_avg:99.27ms
step:1446/1770 train_time:142552ms step_avg:99.27ms
step:1447/1770 train_time:142655ms step_avg:99.27ms
step:1448/1770 train_time:142760ms step_avg:99.28ms
step:1449/1770 train_time:142864ms step_avg:99.28ms
step:1450/1770 train_time:142969ms step_avg:99.28ms
step:1451/1770 train_time:143073ms step_avg:99.29ms
step:1452/1770 train_time:143176ms step_avg:99.29ms
step:1453/1770 train_time:143280ms step_avg:99.29ms
step:1454/1770 train_time:143384ms step_avg:99.30ms
step:1455/1770 train_time:143488ms step_avg:99.30ms
step:1456/1770 train_time:143594ms step_avg:99.30ms
step:1457/1770 train_time:143698ms step_avg:99.31ms
step:1458/1770 train_time:143803ms step_avg:99.31ms
step:1459/1770 train_time:143908ms step_avg:99.32ms
step:1460/1770 train_time:144012ms step_avg:99.32ms
step:1461/1770 train_time:144116ms step_avg:99.32ms
step:1462/1770 train_time:144220ms step_avg:99.32ms
step:1463/1770 train_time:144324ms step_avg:99.33ms
step:1464/1770 train_time:144429ms step_avg:99.33ms
step:1465/1770 train_time:144533ms step_avg:99.34ms
step:1466/1770 train_time:144638ms step_avg:99.34ms
step:1467/1770 train_time:144743ms step_avg:99.34ms
step:1468/1770 train_time:144847ms step_avg:99.35ms
step:1469/1770 train_time:144951ms step_avg:99.35ms
step:1470/1770 train_time:145056ms step_avg:99.35ms
step:1471/1770 train_time:145159ms step_avg:99.36ms
step:1472/1770 train_time:145263ms step_avg:99.36ms
step:1473/1770 train_time:145368ms step_avg:99.36ms
step:1474/1770 train_time:145473ms step_avg:99.37ms
step:1475/1770 train_time:145577ms step_avg:99.37ms
step:1476/1770 train_time:145680ms step_avg:99.37ms
step:1477/1770 train_time:145787ms step_avg:99.38ms
step:1478/1770 train_time:145891ms step_avg:99.38ms
step:1479/1770 train_time:145995ms step_avg:99.38ms
step:1480/1770 train_time:146098ms step_avg:99.39ms
step:1481/1770 train_time:146206ms step_avg:99.39ms
step:1482/1770 train_time:146310ms step_avg:99.40ms
step:1483/1770 train_time:146414ms step_avg:99.40ms
step:1484/1770 train_time:146517ms step_avg:99.40ms
step:1485/1770 train_time:146620ms step_avg:99.40ms
step:1486/1770 train_time:146724ms step_avg:99.41ms
step:1487/1770 train_time:146828ms step_avg:99.41ms
step:1488/1770 train_time:146933ms step_avg:99.41ms
step:1489/1770 train_time:147037ms step_avg:99.42ms
step:1490/1770 train_time:147142ms step_avg:99.42ms
step:1491/1770 train_time:147246ms step_avg:99.42ms
step:1492/1770 train_time:147351ms step_avg:99.43ms
step:1493/1770 train_time:147457ms step_avg:99.43ms
step:1494/1770 train_time:147565ms step_avg:99.44ms
step:1495/1770 train_time:147669ms step_avg:99.44ms
step:1496/1770 train_time:147773ms step_avg:99.44ms
step:1497/1770 train_time:147877ms step_avg:99.45ms
step:1498/1770 train_time:147980ms step_avg:99.45ms
step:1499/1770 train_time:148084ms step_avg:99.45ms
step:1500/1770 train_time:148188ms step_avg:99.45ms
step:1500/1770 val_loss:3.3566 train_time:148290ms step_avg:99.52ms
step:1501/1770 train_time:148311ms step_avg:99.47ms
step:1502/1770 train_time:148400ms step_avg:99.46ms
step:1503/1770 train_time:148505ms step_avg:99.47ms
step:1504/1770 train_time:148609ms step_avg:99.47ms
step:1505/1770 train_time:148715ms step_avg:99.47ms
step:1506/1770 train_time:148819ms step_avg:99.48ms
step:1507/1770 train_time:148924ms step_avg:99.48ms
step:1508/1770 train_time:149031ms step_avg:99.49ms
step:1509/1770 train_time:149134ms step_avg:99.49ms
step:1510/1770 train_time:149237ms step_avg:99.49ms
step:1511/1770 train_time:149343ms step_avg:99.50ms
step:1512/1770 train_time:149447ms step_avg:99.50ms
step:1513/1770 train_time:149551ms step_avg:99.50ms
step:1514/1770 train_time:149654ms step_avg:99.50ms
step:1515/1770 train_time:149758ms step_avg:99.51ms
step:1516/1770 train_time:149862ms step_avg:99.51ms
step:1517/1770 train_time:149967ms step_avg:99.51ms
step:1518/1770 train_time:150073ms step_avg:99.52ms
step:1519/1770 train_time:150176ms step_avg:99.52ms
step:1520/1770 train_time:150282ms step_avg:99.52ms
step:1521/1770 train_time:150386ms step_avg:99.53ms
step:1522/1770 train_time:150491ms step_avg:99.53ms
step:1523/1770 train_time:150595ms step_avg:99.53ms
step:1524/1770 train_time:150699ms step_avg:99.54ms
step:1525/1770 train_time:150802ms step_avg:99.54ms
step:1526/1770 train_time:150906ms step_avg:99.54ms
step:1527/1770 train_time:151010ms step_avg:99.54ms
step:1528/1770 train_time:151115ms step_avg:99.55ms
step:1529/1770 train_time:151219ms step_avg:99.55ms
step:1530/1770 train_time:151322ms step_avg:99.55ms
step:1531/1770 train_time:151427ms step_avg:99.56ms
step:1532/1770 train_time:151531ms step_avg:99.56ms
step:1533/1770 train_time:151636ms step_avg:99.56ms
step:1534/1770 train_time:151740ms step_avg:99.57ms
step:1535/1770 train_time:151845ms step_avg:99.57ms
step:1536/1770 train_time:151949ms step_avg:99.57ms
step:1537/1770 train_time:152054ms step_avg:99.58ms
step:1538/1770 train_time:152159ms step_avg:99.58ms
step:1539/1770 train_time:152263ms step_avg:99.58ms
step:1540/1770 train_time:152370ms step_avg:99.59ms
step:1541/1770 train_time:152475ms step_avg:99.59ms
step:1542/1770 train_time:152579ms step_avg:99.59ms
step:1543/1770 train_time:152683ms step_avg:99.60ms
step:1544/1770 train_time:152789ms step_avg:99.60ms
step:1545/1770 train_time:152893ms step_avg:99.60ms
step:1546/1770 train_time:152997ms step_avg:99.61ms
step:1547/1770 train_time:153101ms step_avg:99.61ms
step:1548/1770 train_time:153206ms step_avg:99.61ms
step:1549/1770 train_time:153309ms step_avg:99.62ms
step:1550/1770 train_time:153414ms step_avg:99.62ms
step:1551/1770 train_time:153517ms step_avg:99.62ms
step:1552/1770 train_time:153623ms step_avg:99.63ms
step:1553/1770 train_time:153727ms step_avg:99.63ms
step:1554/1770 train_time:153830ms step_avg:99.63ms
step:1555/1770 train_time:153935ms step_avg:99.63ms
step:1556/1770 train_time:154038ms step_avg:99.64ms
step:1557/1770 train_time:154142ms step_avg:99.64ms
step:1558/1770 train_time:154246ms step_avg:99.64ms
step:1559/1770 train_time:154350ms step_avg:99.65ms
step:1560/1770 train_time:154453ms step_avg:99.65ms
step:1561/1770 train_time:154560ms step_avg:99.65ms
step:1562/1770 train_time:154664ms step_avg:99.65ms
step:1563/1770 train_time:154769ms step_avg:99.66ms
step:1564/1770 train_time:154871ms step_avg:99.66ms
step:1565/1770 train_time:154975ms step_avg:99.66ms
step:1566/1770 train_time:155079ms step_avg:99.66ms
step:1567/1770 train_time:155183ms step_avg:99.67ms
step:1568/1770 train_time:155288ms step_avg:99.67ms
step:1569/1770 train_time:155396ms step_avg:99.68ms
step:1570/1770 train_time:155499ms step_avg:99.68ms
step:1571/1770 train_time:155603ms step_avg:99.68ms
step:1572/1770 train_time:155708ms step_avg:99.69ms
step:1573/1770 train_time:155814ms step_avg:99.69ms
step:1574/1770 train_time:155919ms step_avg:99.69ms
step:1575/1770 train_time:156022ms step_avg:99.69ms
step:1576/1770 train_time:156125ms step_avg:99.70ms
step:1577/1770 train_time:156231ms step_avg:99.70ms
step:1578/1770 train_time:156337ms step_avg:99.70ms
step:1579/1770 train_time:156440ms step_avg:99.71ms
step:1580/1770 train_time:156545ms step_avg:99.71ms
step:1581/1770 train_time:156651ms step_avg:99.71ms
step:1582/1770 train_time:156756ms step_avg:99.72ms
step:1583/1770 train_time:156859ms step_avg:99.72ms
step:1584/1770 train_time:156966ms step_avg:99.72ms
step:1585/1770 train_time:157069ms step_avg:99.73ms
step:1586/1770 train_time:157177ms step_avg:99.73ms
step:1587/1770 train_time:157282ms step_avg:99.74ms
step:1588/1770 train_time:157387ms step_avg:99.74ms
step:1589/1770 train_time:157492ms step_avg:99.74ms
step:1590/1770 train_time:157596ms step_avg:99.74ms
step:1591/1770 train_time:157700ms step_avg:99.75ms
step:1592/1770 train_time:157804ms step_avg:99.75ms
step:1593/1770 train_time:157908ms step_avg:99.75ms
step:1594/1770 train_time:158012ms step_avg:99.76ms
step:1595/1770 train_time:158116ms step_avg:99.76ms
step:1596/1770 train_time:158222ms step_avg:99.76ms
step:1597/1770 train_time:158326ms step_avg:99.76ms
step:1598/1770 train_time:158430ms step_avg:99.77ms
step:1599/1770 train_time:158535ms step_avg:99.77ms
step:1600/1770 train_time:158642ms step_avg:99.77ms
step:1601/1770 train_time:158747ms step_avg:99.78ms
step:1602/1770 train_time:158852ms step_avg:99.78ms
step:1603/1770 train_time:158956ms step_avg:99.78ms
step:1604/1770 train_time:159059ms step_avg:99.79ms
step:1605/1770 train_time:159163ms step_avg:99.79ms
step:1606/1770 train_time:159268ms step_avg:99.79ms
step:1607/1770 train_time:159376ms step_avg:99.80ms
step:1608/1770 train_time:159480ms step_avg:99.80ms
step:1609/1770 train_time:159584ms step_avg:99.80ms
step:1610/1770 train_time:159692ms step_avg:99.81ms
step:1611/1770 train_time:159796ms step_avg:99.81ms
step:1612/1770 train_time:159902ms step_avg:99.81ms
step:1613/1770 train_time:160007ms step_avg:99.82ms
step:1614/1770 train_time:160111ms step_avg:99.82ms
step:1615/1770 train_time:160214ms step_avg:99.82ms
step:1616/1770 train_time:160318ms step_avg:99.82ms
step:1617/1770 train_time:160425ms step_avg:99.83ms
step:1618/1770 train_time:160530ms step_avg:99.83ms
step:1619/1770 train_time:160635ms step_avg:99.83ms
step:1620/1770 train_time:160739ms step_avg:99.84ms
step:1621/1770 train_time:160843ms step_avg:99.84ms
step:1622/1770 train_time:160948ms step_avg:99.84ms
step:1623/1770 train_time:161055ms step_avg:99.85ms
step:1624/1770 train_time:161158ms step_avg:99.85ms
step:1625/1770 train_time:161261ms step_avg:99.85ms
step:1625/1770 val_loss:3.3236 train_time:161364ms step_avg:99.92ms
step:1626/1770 train_time:161385ms step_avg:99.87ms
step:1627/1770 train_time:161474ms step_avg:99.86ms
step:1628/1770 train_time:161578ms step_avg:99.86ms
step:1629/1770 train_time:161681ms step_avg:99.86ms
step:1630/1770 train_time:161785ms step_avg:99.87ms
step:1631/1770 train_time:161889ms step_avg:99.87ms
step:1632/1770 train_time:161992ms step_avg:99.87ms
step:1633/1770 train_time:162096ms step_avg:99.87ms
step:1634/1770 train_time:162200ms step_avg:99.88ms
step:1635/1770 train_time:162304ms step_avg:99.88ms
step:1636/1770 train_time:162408ms step_avg:99.88ms
step:1637/1770 train_time:162515ms step_avg:99.89ms
step:1638/1770 train_time:162619ms step_avg:99.89ms
step:1639/1770 train_time:162723ms step_avg:99.89ms
step:1640/1770 train_time:162827ms step_avg:99.89ms
step:1641/1770 train_time:162931ms step_avg:99.90ms
step:1642/1770 train_time:163035ms step_avg:99.90ms
step:1643/1770 train_time:163139ms step_avg:99.90ms
step:1644/1770 train_time:163245ms step_avg:99.90ms
step:1645/1770 train_time:163348ms step_avg:99.91ms
step:1646/1770 train_time:163453ms step_avg:99.91ms
step:1647/1770 train_time:163559ms step_avg:99.91ms
step:1648/1770 train_time:163662ms step_avg:99.92ms
step:1649/1770 train_time:163766ms step_avg:99.92ms
step:1650/1770 train_time:163870ms step_avg:99.92ms
step:1651/1770 train_time:163974ms step_avg:99.92ms
step:1652/1770 train_time:164079ms step_avg:99.93ms
step:1653/1770 train_time:164183ms step_avg:99.93ms
step:1654/1770 train_time:164291ms step_avg:99.93ms
step:1655/1770 train_time:164397ms step_avg:99.94ms
step:1656/1770 train_time:164501ms step_avg:99.94ms
step:1657/1770 train_time:164607ms step_avg:99.94ms
step:1658/1770 train_time:164711ms step_avg:99.95ms
step:1659/1770 train_time:164817ms step_avg:99.95ms
step:1660/1770 train_time:164921ms step_avg:99.95ms
step:1661/1770 train_time:165026ms step_avg:99.96ms
step:1662/1770 train_time:165130ms step_avg:99.96ms
step:1663/1770 train_time:165233ms step_avg:99.96ms
step:1664/1770 train_time:165338ms step_avg:99.96ms
step:1665/1770 train_time:165442ms step_avg:99.97ms
step:1666/1770 train_time:165546ms step_avg:99.97ms
step:1667/1770 train_time:165649ms step_avg:99.97ms
step:1668/1770 train_time:165753ms step_avg:99.97ms
step:1669/1770 train_time:165857ms step_avg:99.97ms
step:1670/1770 train_time:165961ms step_avg:99.98ms
step:1671/1770 train_time:166065ms step_avg:99.98ms
step:1672/1770 train_time:166170ms step_avg:99.98ms
step:1673/1770 train_time:166276ms step_avg:99.99ms
step:1674/1770 train_time:166380ms step_avg:99.99ms
step:1675/1770 train_time:166484ms step_avg:99.99ms
step:1676/1770 train_time:166589ms step_avg:99.99ms
step:1677/1770 train_time:166697ms step_avg:100.00ms
step:1678/1770 train_time:166800ms step_avg:100.00ms
step:1679/1770 train_time:166904ms step_avg:100.00ms
step:1680/1770 train_time:167008ms step_avg:100.00ms
step:1681/1770 train_time:167113ms step_avg:100.01ms
step:1682/1770 train_time:167219ms step_avg:100.01ms
step:1683/1770 train_time:167323ms step_avg:100.01ms
step:1684/1770 train_time:167427ms step_avg:100.02ms
step:1685/1770 train_time:167530ms step_avg:100.02ms
step:1686/1770 train_time:167635ms step_avg:100.02ms
step:1687/1770 train_time:167741ms step_avg:100.02ms
step:1688/1770 train_time:167845ms step_avg:100.03ms
step:1689/1770 train_time:167950ms step_avg:100.03ms
step:1690/1770 train_time:168054ms step_avg:100.03ms
step:1691/1770 train_time:168158ms step_avg:100.03ms
step:1692/1770 train_time:168262ms step_avg:100.04ms
step:1693/1770 train_time:168368ms step_avg:100.04ms
step:1694/1770 train_time:168471ms step_avg:100.04ms
step:1695/1770 train_time:168576ms step_avg:100.04ms
step:1696/1770 train_time:168682ms step_avg:100.05ms
step:1697/1770 train_time:168787ms step_avg:100.05ms
step:1698/1770 train_time:168892ms step_avg:100.05ms
step:1699/1770 train_time:168996ms step_avg:100.06ms
step:1700/1770 train_time:169100ms step_avg:100.06ms
step:1701/1770 train_time:169204ms step_avg:100.06ms
step:1702/1770 train_time:169308ms step_avg:100.06ms
step:1703/1770 train_time:169411ms step_avg:100.07ms
step:1704/1770 train_time:169516ms step_avg:100.07ms
step:1705/1770 train_time:169620ms step_avg:100.07ms
step:1706/1770 train_time:169724ms step_avg:100.07ms
step:1707/1770 train_time:169829ms step_avg:100.08ms
step:1708/1770 train_time:169934ms step_avg:100.08ms
step:1709/1770 train_time:170040ms step_avg:100.08ms
step:1710/1770 train_time:170148ms step_avg:100.09ms
step:1711/1770 train_time:170254ms step_avg:100.09ms
step:1712/1770 train_time:170360ms step_avg:100.09ms
step:1713/1770 train_time:170464ms step_avg:100.10ms
step:1714/1770 train_time:170569ms step_avg:100.10ms
step:1715/1770 train_time:170672ms step_avg:100.10ms
step:1716/1770 train_time:170777ms step_avg:100.10ms
step:1717/1770 train_time:170882ms step_avg:100.11ms
step:1718/1770 train_time:170988ms step_avg:100.11ms
step:1719/1770 train_time:171093ms step_avg:100.11ms
step:1720/1770 train_time:171199ms step_avg:100.12ms
step:1721/1770 train_time:171303ms step_avg:100.12ms
step:1722/1770 train_time:171411ms step_avg:100.12ms
step:1723/1770 train_time:171517ms step_avg:100.13ms
step:1724/1770 train_time:171625ms step_avg:100.13ms
step:1725/1770 train_time:171732ms step_avg:100.14ms
step:1726/1770 train_time:171839ms step_avg:100.14ms
step:1727/1770 train_time:171943ms step_avg:100.14ms
step:1728/1770 train_time:172050ms step_avg:100.15ms
step:1729/1770 train_time:172154ms step_avg:100.15ms
step:1730/1770 train_time:172260ms step_avg:100.15ms
step:1731/1770 train_time:172367ms step_avg:100.15ms
step:1732/1770 train_time:172471ms step_avg:100.16ms
step:1733/1770 train_time:172577ms step_avg:100.16ms
step:1734/1770 train_time:172682ms step_avg:100.16ms
step:1735/1770 train_time:172788ms step_avg:100.17ms
step:1736/1770 train_time:172892ms step_avg:100.17ms
step:1737/1770 train_time:172998ms step_avg:100.17ms
step:1738/1770 train_time:173103ms step_avg:100.18ms
step:1739/1770 train_time:173208ms step_avg:100.18ms
step:1740/1770 train_time:173313ms step_avg:100.18ms
step:1741/1770 train_time:173420ms step_avg:100.18ms
step:1742/1770 train_time:173528ms step_avg:100.19ms
step:1743/1770 train_time:173634ms step_avg:100.19ms
step:1744/1770 train_time:173739ms step_avg:100.20ms
step:1745/1770 train_time:173844ms step_avg:100.20ms
step:1746/1770 train_time:173952ms step_avg:100.20ms
step:1747/1770 train_time:174055ms step_avg:100.20ms
step:1748/1770 train_time:174162ms step_avg:100.21ms
step:1749/1770 train_time:174268ms step_avg:100.21ms
step:1750/1770 train_time:174373ms step_avg:100.21ms
step:1750/1770 val_loss:3.2988 train_time:174476ms step_avg:100.27ms
step:1751/1770 train_time:174497ms step_avg:100.23ms
step:1752/1770 train_time:174586ms step_avg:100.22ms
step:1753/1770 train_time:174691ms step_avg:100.22ms
step:1754/1770 train_time:174796ms step_avg:100.23ms
step:1755/1770 train_time:174902ms step_avg:100.23ms
step:1756/1770 train_time:175008ms step_avg:100.23ms
step:1757/1770 train_time:175113ms step_avg:100.24ms
step:1758/1770 train_time:175218ms step_avg:100.24ms
step:1759/1770 train_time:175323ms step_avg:100.24ms
step:1760/1770 train_time:175429ms step_avg:100.25ms
step:1761/1770 train_time:175537ms step_avg:100.25ms
step:1762/1770 train_time:175646ms step_avg:100.25ms
step:1763/1770 train_time:175749ms step_avg:100.26ms
step:1764/1770 train_time:175857ms step_avg:100.26ms
step:1765/1770 train_time:175960ms step_avg:100.26ms
step:1766/1770 train_time:176070ms step_avg:100.27ms
step:1767/1770 train_time:176173ms step_avg:100.27ms
step:1768/1770 train_time:176277ms step_avg:100.27ms
step:1769/1770 train_time:176381ms step_avg:100.27ms
step:1770/1770 train_time:176485ms step_avg:100.28ms
step:1770/1770 val_loss:3.2964 train_time:176590ms step_avg:100.34ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
