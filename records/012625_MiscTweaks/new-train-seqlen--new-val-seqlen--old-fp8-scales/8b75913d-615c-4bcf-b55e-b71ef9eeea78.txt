import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:12:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23456ms step_avg:nanms
step:2/1770 train_time:23888ms step_avg:nanms
step:3/1770 train_time:23983ms step_avg:nanms
step:4/1770 train_time:24076ms step_avg:nanms
step:5/1770 train_time:24170ms step_avg:nanms
step:6/1770 train_time:24264ms step_avg:nanms
step:7/1770 train_time:24359ms step_avg:nanms
step:8/1770 train_time:24454ms step_avg:nanms
step:9/1770 train_time:24548ms step_avg:nanms
step:10/1770 train_time:24642ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:284ms step_avg:94.62ms
step:14/1770 train_time:380ms step_avg:94.93ms
step:15/1770 train_time:474ms step_avg:94.80ms
step:16/1770 train_time:568ms step_avg:94.73ms
step:17/1770 train_time:663ms step_avg:94.71ms
step:18/1770 train_time:757ms step_avg:94.67ms
step:19/1770 train_time:852ms step_avg:94.69ms
step:20/1770 train_time:947ms step_avg:94.65ms
step:21/1770 train_time:1041ms step_avg:94.63ms
step:22/1770 train_time:1136ms step_avg:94.65ms
step:23/1770 train_time:1231ms step_avg:94.66ms
step:24/1770 train_time:1325ms step_avg:94.68ms
step:25/1770 train_time:1420ms step_avg:94.69ms
step:26/1770 train_time:1515ms step_avg:94.67ms
step:27/1770 train_time:1610ms step_avg:94.68ms
step:28/1770 train_time:1704ms step_avg:94.68ms
step:29/1770 train_time:1799ms step_avg:94.68ms
step:30/1770 train_time:1894ms step_avg:94.69ms
step:31/1770 train_time:1989ms step_avg:94.69ms
step:32/1770 train_time:2083ms step_avg:94.70ms
step:33/1770 train_time:2178ms step_avg:94.71ms
step:34/1770 train_time:2273ms step_avg:94.73ms
step:35/1770 train_time:2368ms step_avg:94.73ms
step:36/1770 train_time:2464ms step_avg:94.76ms
step:37/1770 train_time:2558ms step_avg:94.75ms
step:38/1770 train_time:2653ms step_avg:94.76ms
step:39/1770 train_time:2748ms step_avg:94.77ms
step:40/1770 train_time:2843ms step_avg:94.76ms
step:41/1770 train_time:2937ms step_avg:94.75ms
step:42/1770 train_time:3033ms step_avg:94.77ms
step:43/1770 train_time:3128ms step_avg:94.79ms
step:44/1770 train_time:3223ms step_avg:94.79ms
step:45/1770 train_time:3318ms step_avg:94.79ms
step:46/1770 train_time:3413ms step_avg:94.82ms
step:47/1770 train_time:3508ms step_avg:94.82ms
step:48/1770 train_time:3603ms step_avg:94.82ms
step:49/1770 train_time:3698ms step_avg:94.82ms
step:50/1770 train_time:3793ms step_avg:94.83ms
step:51/1770 train_time:3888ms step_avg:94.83ms
step:52/1770 train_time:3983ms step_avg:94.83ms
step:53/1770 train_time:4077ms step_avg:94.81ms
step:54/1770 train_time:4172ms step_avg:94.81ms
step:55/1770 train_time:4267ms step_avg:94.82ms
step:56/1770 train_time:4361ms step_avg:94.81ms
step:57/1770 train_time:4456ms step_avg:94.81ms
step:58/1770 train_time:4551ms step_avg:94.82ms
step:59/1770 train_time:4646ms step_avg:94.81ms
step:60/1770 train_time:4741ms step_avg:94.82ms
step:61/1770 train_time:4835ms step_avg:94.81ms
step:62/1770 train_time:4931ms step_avg:94.82ms
step:63/1770 train_time:5026ms step_avg:94.82ms
step:64/1770 train_time:5121ms step_avg:94.83ms
step:65/1770 train_time:5215ms step_avg:94.82ms
step:66/1770 train_time:5310ms step_avg:94.82ms
step:67/1770 train_time:5405ms step_avg:94.82ms
step:68/1770 train_time:5500ms step_avg:94.83ms
step:69/1770 train_time:5595ms step_avg:94.82ms
step:70/1770 train_time:5689ms step_avg:94.82ms
step:71/1770 train_time:5784ms step_avg:94.82ms
step:72/1770 train_time:5879ms step_avg:94.82ms
step:73/1770 train_time:5973ms step_avg:94.81ms
step:74/1770 train_time:6069ms step_avg:94.82ms
step:75/1770 train_time:6163ms step_avg:94.82ms
step:76/1770 train_time:6258ms step_avg:94.82ms
step:77/1770 train_time:6353ms step_avg:94.82ms
step:78/1770 train_time:6448ms step_avg:94.82ms
step:79/1770 train_time:6542ms step_avg:94.81ms
step:80/1770 train_time:6637ms step_avg:94.81ms
step:81/1770 train_time:6732ms step_avg:94.82ms
step:82/1770 train_time:6827ms step_avg:94.82ms
step:83/1770 train_time:6921ms step_avg:94.81ms
step:84/1770 train_time:7015ms step_avg:94.80ms
step:85/1770 train_time:7111ms step_avg:94.81ms
step:86/1770 train_time:7205ms step_avg:94.81ms
step:87/1770 train_time:7300ms step_avg:94.81ms
step:88/1770 train_time:7395ms step_avg:94.81ms
step:89/1770 train_time:7490ms step_avg:94.81ms
step:90/1770 train_time:7585ms step_avg:94.81ms
step:91/1770 train_time:7680ms step_avg:94.81ms
step:92/1770 train_time:7775ms step_avg:94.81ms
step:93/1770 train_time:7870ms step_avg:94.81ms
step:94/1770 train_time:7964ms step_avg:94.81ms
step:95/1770 train_time:8059ms step_avg:94.81ms
step:96/1770 train_time:8154ms step_avg:94.81ms
step:97/1770 train_time:8248ms step_avg:94.81ms
step:98/1770 train_time:8344ms step_avg:94.81ms
step:99/1770 train_time:8438ms step_avg:94.81ms
step:100/1770 train_time:8533ms step_avg:94.81ms
step:101/1770 train_time:8628ms step_avg:94.82ms
step:102/1770 train_time:8724ms step_avg:94.83ms
step:103/1770 train_time:8819ms step_avg:94.82ms
step:104/1770 train_time:8913ms step_avg:94.82ms
step:105/1770 train_time:9008ms step_avg:94.82ms
step:106/1770 train_time:9103ms step_avg:94.82ms
step:107/1770 train_time:9197ms step_avg:94.81ms
step:108/1770 train_time:9292ms step_avg:94.81ms
step:109/1770 train_time:9386ms step_avg:94.81ms
step:110/1770 train_time:9481ms step_avg:94.81ms
step:111/1770 train_time:9576ms step_avg:94.81ms
step:112/1770 train_time:9671ms step_avg:94.81ms
step:113/1770 train_time:9765ms step_avg:94.81ms
step:114/1770 train_time:9860ms step_avg:94.81ms
step:115/1770 train_time:9955ms step_avg:94.81ms
step:116/1770 train_time:10050ms step_avg:94.81ms
step:117/1770 train_time:10144ms step_avg:94.81ms
step:118/1770 train_time:10239ms step_avg:94.81ms
step:119/1770 train_time:10334ms step_avg:94.81ms
step:120/1770 train_time:10429ms step_avg:94.81ms
step:121/1770 train_time:10524ms step_avg:94.81ms
step:122/1770 train_time:10619ms step_avg:94.81ms
step:123/1770 train_time:10714ms step_avg:94.81ms
step:124/1770 train_time:10808ms step_avg:94.81ms
step:125/1770 train_time:10903ms step_avg:94.81ms
step:125/1770 val_loss:4.6514 train_time:10996ms step_avg:95.61ms
step:126/1770 train_time:11017ms step_avg:94.98ms
step:127/1770 train_time:11095ms step_avg:94.83ms
step:128/1770 train_time:11193ms step_avg:94.85ms
step:129/1770 train_time:11292ms step_avg:94.89ms
step:130/1770 train_time:11388ms step_avg:94.90ms
step:131/1770 train_time:11483ms step_avg:94.90ms
step:132/1770 train_time:11579ms step_avg:94.91ms
step:133/1770 train_time:11673ms step_avg:94.90ms
step:134/1770 train_time:11769ms step_avg:94.91ms
step:135/1770 train_time:11865ms step_avg:94.92ms
step:136/1770 train_time:11959ms step_avg:94.91ms
step:137/1770 train_time:12054ms step_avg:94.91ms
step:138/1770 train_time:12149ms step_avg:94.92ms
step:139/1770 train_time:12245ms step_avg:94.92ms
step:140/1770 train_time:12340ms step_avg:94.92ms
step:141/1770 train_time:12435ms step_avg:94.92ms
step:142/1770 train_time:12530ms step_avg:94.93ms
step:143/1770 train_time:12626ms step_avg:94.93ms
step:144/1770 train_time:12721ms step_avg:94.93ms
step:145/1770 train_time:12817ms step_avg:94.94ms
step:146/1770 train_time:12912ms step_avg:94.94ms
step:147/1770 train_time:13007ms step_avg:94.94ms
step:148/1770 train_time:13102ms step_avg:94.95ms
step:149/1770 train_time:13197ms step_avg:94.95ms
step:150/1770 train_time:13293ms step_avg:94.95ms
step:151/1770 train_time:13389ms step_avg:94.96ms
step:152/1770 train_time:13485ms step_avg:94.96ms
step:153/1770 train_time:13580ms step_avg:94.96ms
step:154/1770 train_time:13675ms step_avg:94.97ms
step:155/1770 train_time:13771ms step_avg:94.97ms
step:156/1770 train_time:13867ms step_avg:94.98ms
step:157/1770 train_time:13962ms step_avg:94.98ms
step:158/1770 train_time:14057ms step_avg:94.98ms
step:159/1770 train_time:14152ms step_avg:94.98ms
step:160/1770 train_time:14248ms step_avg:94.98ms
step:161/1770 train_time:14343ms step_avg:94.99ms
step:162/1770 train_time:14438ms step_avg:94.99ms
step:163/1770 train_time:14534ms step_avg:94.99ms
step:164/1770 train_time:14629ms step_avg:94.99ms
step:165/1770 train_time:14724ms step_avg:95.00ms
step:166/1770 train_time:14820ms step_avg:95.00ms
step:167/1770 train_time:14917ms step_avg:95.01ms
step:168/1770 train_time:15012ms step_avg:95.01ms
step:169/1770 train_time:15107ms step_avg:95.02ms
step:170/1770 train_time:15202ms step_avg:95.02ms
step:171/1770 train_time:15298ms step_avg:95.02ms
step:172/1770 train_time:15393ms step_avg:95.02ms
step:173/1770 train_time:15489ms step_avg:95.02ms
step:174/1770 train_time:15584ms step_avg:95.03ms
step:175/1770 train_time:15679ms step_avg:95.02ms
step:176/1770 train_time:15774ms step_avg:95.03ms
step:177/1770 train_time:15870ms step_avg:95.03ms
step:178/1770 train_time:15965ms step_avg:95.03ms
step:179/1770 train_time:16060ms step_avg:95.03ms
step:180/1770 train_time:16156ms step_avg:95.03ms
step:181/1770 train_time:16251ms step_avg:95.03ms
step:182/1770 train_time:16347ms step_avg:95.04ms
step:183/1770 train_time:16442ms step_avg:95.04ms
step:184/1770 train_time:16537ms step_avg:95.04ms
step:185/1770 train_time:16632ms step_avg:95.04ms
step:186/1770 train_time:16728ms step_avg:95.05ms
step:187/1770 train_time:16823ms step_avg:95.05ms
step:188/1770 train_time:16918ms step_avg:95.05ms
step:189/1770 train_time:17014ms step_avg:95.05ms
step:190/1770 train_time:17109ms step_avg:95.05ms
step:191/1770 train_time:17204ms step_avg:95.05ms
step:192/1770 train_time:17300ms step_avg:95.05ms
step:193/1770 train_time:17395ms step_avg:95.06ms
step:194/1770 train_time:17490ms step_avg:95.06ms
step:195/1770 train_time:17586ms step_avg:95.06ms
step:196/1770 train_time:17681ms step_avg:95.06ms
step:197/1770 train_time:17777ms step_avg:95.06ms
step:198/1770 train_time:17872ms step_avg:95.06ms
step:199/1770 train_time:17968ms step_avg:95.07ms
step:200/1770 train_time:18063ms step_avg:95.07ms
step:201/1770 train_time:18158ms step_avg:95.07ms
step:202/1770 train_time:18253ms step_avg:95.07ms
step:203/1770 train_time:18348ms step_avg:95.07ms
step:204/1770 train_time:18443ms step_avg:95.07ms
step:205/1770 train_time:18538ms step_avg:95.07ms
step:206/1770 train_time:18634ms step_avg:95.07ms
step:207/1770 train_time:18729ms step_avg:95.07ms
step:208/1770 train_time:18825ms step_avg:95.08ms
step:209/1770 train_time:18920ms step_avg:95.08ms
step:210/1770 train_time:19015ms step_avg:95.08ms
step:211/1770 train_time:19110ms step_avg:95.08ms
step:212/1770 train_time:19206ms step_avg:95.08ms
step:213/1770 train_time:19301ms step_avg:95.08ms
step:214/1770 train_time:19396ms step_avg:95.08ms
step:215/1770 train_time:19491ms step_avg:95.08ms
step:216/1770 train_time:19587ms step_avg:95.08ms
step:217/1770 train_time:19682ms step_avg:95.08ms
step:218/1770 train_time:19778ms step_avg:95.09ms
step:219/1770 train_time:19873ms step_avg:95.09ms
step:220/1770 train_time:19969ms step_avg:95.09ms
step:221/1770 train_time:20064ms step_avg:95.09ms
step:222/1770 train_time:20160ms step_avg:95.09ms
step:223/1770 train_time:20255ms step_avg:95.09ms
step:224/1770 train_time:20350ms step_avg:95.09ms
step:225/1770 train_time:20446ms step_avg:95.10ms
step:226/1770 train_time:20541ms step_avg:95.10ms
step:227/1770 train_time:20637ms step_avg:95.10ms
step:228/1770 train_time:20732ms step_avg:95.10ms
step:229/1770 train_time:20828ms step_avg:95.10ms
step:230/1770 train_time:20924ms step_avg:95.11ms
step:231/1770 train_time:21020ms step_avg:95.11ms
step:232/1770 train_time:21115ms step_avg:95.11ms
step:233/1770 train_time:21211ms step_avg:95.12ms
step:234/1770 train_time:21306ms step_avg:95.12ms
step:235/1770 train_time:21402ms step_avg:95.12ms
step:236/1770 train_time:21497ms step_avg:95.12ms
step:237/1770 train_time:21592ms step_avg:95.12ms
step:238/1770 train_time:21687ms step_avg:95.12ms
step:239/1770 train_time:21783ms step_avg:95.12ms
step:240/1770 train_time:21877ms step_avg:95.12ms
step:241/1770 train_time:21973ms step_avg:95.12ms
step:242/1770 train_time:22068ms step_avg:95.12ms
step:243/1770 train_time:22164ms step_avg:95.12ms
step:244/1770 train_time:22259ms step_avg:95.12ms
step:245/1770 train_time:22354ms step_avg:95.12ms
step:246/1770 train_time:22449ms step_avg:95.12ms
step:247/1770 train_time:22545ms step_avg:95.13ms
step:248/1770 train_time:22640ms step_avg:95.13ms
step:249/1770 train_time:22735ms step_avg:95.13ms
step:250/1770 train_time:22830ms step_avg:95.13ms
step:250/1770 val_loss:4.1152 train_time:22925ms step_avg:95.52ms
step:251/1770 train_time:22946ms step_avg:95.21ms
step:252/1770 train_time:23030ms step_avg:95.17ms
step:253/1770 train_time:23128ms step_avg:95.18ms
step:254/1770 train_time:23223ms step_avg:95.18ms
step:255/1770 train_time:23319ms step_avg:95.18ms
step:256/1770 train_time:23414ms step_avg:95.18ms
step:257/1770 train_time:23509ms step_avg:95.18ms
step:258/1770 train_time:23604ms step_avg:95.18ms
step:259/1770 train_time:23699ms step_avg:95.18ms
step:260/1770 train_time:23795ms step_avg:95.18ms
step:261/1770 train_time:23890ms step_avg:95.18ms
step:262/1770 train_time:23985ms step_avg:95.18ms
step:263/1770 train_time:24080ms step_avg:95.18ms
step:264/1770 train_time:24175ms step_avg:95.18ms
step:265/1770 train_time:24272ms step_avg:95.18ms
step:266/1770 train_time:24368ms step_avg:95.19ms
step:267/1770 train_time:24464ms step_avg:95.19ms
step:268/1770 train_time:24559ms step_avg:95.19ms
step:269/1770 train_time:24655ms step_avg:95.19ms
step:270/1770 train_time:24751ms step_avg:95.19ms
step:271/1770 train_time:24846ms step_avg:95.19ms
step:272/1770 train_time:24942ms step_avg:95.20ms
step:273/1770 train_time:25037ms step_avg:95.20ms
step:274/1770 train_time:25134ms step_avg:95.20ms
step:275/1770 train_time:25229ms step_avg:95.20ms
step:276/1770 train_time:25325ms step_avg:95.21ms
step:277/1770 train_time:25421ms step_avg:95.21ms
step:278/1770 train_time:25517ms step_avg:95.21ms
step:279/1770 train_time:25613ms step_avg:95.22ms
step:280/1770 train_time:25709ms step_avg:95.22ms
step:281/1770 train_time:25805ms step_avg:95.22ms
step:282/1770 train_time:25901ms step_avg:95.22ms
step:283/1770 train_time:25997ms step_avg:95.23ms
step:284/1770 train_time:26092ms step_avg:95.23ms
step:285/1770 train_time:26187ms step_avg:95.23ms
step:286/1770 train_time:26283ms step_avg:95.23ms
step:287/1770 train_time:26379ms step_avg:95.23ms
step:288/1770 train_time:26475ms step_avg:95.23ms
step:289/1770 train_time:26572ms step_avg:95.24ms
step:290/1770 train_time:26667ms step_avg:95.24ms
step:291/1770 train_time:26763ms step_avg:95.24ms
step:292/1770 train_time:26859ms step_avg:95.24ms
step:293/1770 train_time:26955ms step_avg:95.25ms
step:294/1770 train_time:27050ms step_avg:95.25ms
step:295/1770 train_time:27146ms step_avg:95.25ms
step:296/1770 train_time:27242ms step_avg:95.25ms
step:297/1770 train_time:27337ms step_avg:95.25ms
step:298/1770 train_time:27433ms step_avg:95.25ms
step:299/1770 train_time:27528ms step_avg:95.25ms
step:300/1770 train_time:27624ms step_avg:95.25ms
step:301/1770 train_time:27720ms step_avg:95.26ms
step:302/1770 train_time:27815ms step_avg:95.26ms
step:303/1770 train_time:27911ms step_avg:95.26ms
step:304/1770 train_time:28007ms step_avg:95.26ms
step:305/1770 train_time:28102ms step_avg:95.26ms
step:306/1770 train_time:28198ms step_avg:95.26ms
step:307/1770 train_time:28293ms step_avg:95.26ms
step:308/1770 train_time:28390ms step_avg:95.27ms
step:309/1770 train_time:28485ms step_avg:95.27ms
step:310/1770 train_time:28581ms step_avg:95.27ms
step:311/1770 train_time:28677ms step_avg:95.27ms
step:312/1770 train_time:28772ms step_avg:95.27ms
step:313/1770 train_time:28868ms step_avg:95.27ms
step:314/1770 train_time:28964ms step_avg:95.27ms
step:315/1770 train_time:29059ms step_avg:95.28ms
step:316/1770 train_time:29155ms step_avg:95.28ms
step:317/1770 train_time:29251ms step_avg:95.28ms
step:318/1770 train_time:29346ms step_avg:95.28ms
step:319/1770 train_time:29443ms step_avg:95.28ms
step:320/1770 train_time:29539ms step_avg:95.29ms
step:321/1770 train_time:29634ms step_avg:95.29ms
step:322/1770 train_time:29730ms step_avg:95.29ms
step:323/1770 train_time:29826ms step_avg:95.29ms
step:324/1770 train_time:29921ms step_avg:95.29ms
step:325/1770 train_time:30017ms step_avg:95.29ms
step:326/1770 train_time:30112ms step_avg:95.29ms
step:327/1770 train_time:30208ms step_avg:95.29ms
step:328/1770 train_time:30303ms step_avg:95.29ms
step:329/1770 train_time:30399ms step_avg:95.30ms
step:330/1770 train_time:30495ms step_avg:95.30ms
step:331/1770 train_time:30590ms step_avg:95.30ms
step:332/1770 train_time:30686ms step_avg:95.30ms
step:333/1770 train_time:30782ms step_avg:95.30ms
step:334/1770 train_time:30877ms step_avg:95.30ms
step:335/1770 train_time:30973ms step_avg:95.30ms
step:336/1770 train_time:31069ms step_avg:95.30ms
step:337/1770 train_time:31164ms step_avg:95.30ms
step:338/1770 train_time:31260ms step_avg:95.30ms
step:339/1770 train_time:31356ms step_avg:95.31ms
step:340/1770 train_time:31452ms step_avg:95.31ms
step:341/1770 train_time:31548ms step_avg:95.31ms
step:342/1770 train_time:31644ms step_avg:95.31ms
step:343/1770 train_time:31740ms step_avg:95.32ms
step:344/1770 train_time:31836ms step_avg:95.32ms
step:345/1770 train_time:31933ms step_avg:95.32ms
step:346/1770 train_time:32028ms step_avg:95.32ms
step:347/1770 train_time:32124ms step_avg:95.32ms
step:348/1770 train_time:32220ms step_avg:95.33ms
step:349/1770 train_time:32315ms step_avg:95.33ms
step:350/1770 train_time:32411ms step_avg:95.33ms
step:351/1770 train_time:32507ms step_avg:95.33ms
step:352/1770 train_time:32603ms step_avg:95.33ms
step:353/1770 train_time:32699ms step_avg:95.33ms
step:354/1770 train_time:32796ms step_avg:95.34ms
step:355/1770 train_time:32891ms step_avg:95.34ms
step:356/1770 train_time:32987ms step_avg:95.34ms
step:357/1770 train_time:33082ms step_avg:95.34ms
step:358/1770 train_time:33178ms step_avg:95.34ms
step:359/1770 train_time:33273ms step_avg:95.34ms
step:360/1770 train_time:33369ms step_avg:95.34ms
step:361/1770 train_time:33464ms step_avg:95.34ms
step:362/1770 train_time:33560ms step_avg:95.34ms
step:363/1770 train_time:33656ms step_avg:95.34ms
step:364/1770 train_time:33752ms step_avg:95.34ms
step:365/1770 train_time:33848ms step_avg:95.35ms
step:366/1770 train_time:33943ms step_avg:95.35ms
step:367/1770 train_time:34040ms step_avg:95.35ms
step:368/1770 train_time:34135ms step_avg:95.35ms
step:369/1770 train_time:34230ms step_avg:95.35ms
step:370/1770 train_time:34326ms step_avg:95.35ms
step:371/1770 train_time:34421ms step_avg:95.35ms
step:372/1770 train_time:34517ms step_avg:95.35ms
step:373/1770 train_time:34613ms step_avg:95.35ms
step:374/1770 train_time:34708ms step_avg:95.35ms
step:375/1770 train_time:34804ms step_avg:95.35ms
step:375/1770 val_loss:3.9062 train_time:34898ms step_avg:95.61ms
step:376/1770 train_time:34919ms step_avg:95.41ms
step:377/1770 train_time:35004ms step_avg:95.38ms
step:378/1770 train_time:35104ms step_avg:95.39ms
step:379/1770 train_time:35201ms step_avg:95.40ms
step:380/1770 train_time:35297ms step_avg:95.40ms
step:381/1770 train_time:35392ms step_avg:95.40ms
step:382/1770 train_time:35488ms step_avg:95.40ms
step:383/1770 train_time:35584ms step_avg:95.40ms
step:384/1770 train_time:35678ms step_avg:95.40ms
step:385/1770 train_time:35774ms step_avg:95.40ms
step:386/1770 train_time:35870ms step_avg:95.40ms
step:387/1770 train_time:35966ms step_avg:95.40ms
step:388/1770 train_time:36063ms step_avg:95.41ms
step:389/1770 train_time:36158ms step_avg:95.40ms
step:390/1770 train_time:36254ms step_avg:95.41ms
step:391/1770 train_time:36351ms step_avg:95.41ms
step:392/1770 train_time:36447ms step_avg:95.41ms
step:393/1770 train_time:36542ms step_avg:95.41ms
step:394/1770 train_time:36638ms step_avg:95.41ms
step:395/1770 train_time:36733ms step_avg:95.41ms
step:396/1770 train_time:36831ms step_avg:95.42ms
step:397/1770 train_time:36929ms step_avg:95.42ms
step:398/1770 train_time:37027ms step_avg:95.43ms
step:399/1770 train_time:37125ms step_avg:95.44ms
step:400/1770 train_time:37223ms step_avg:95.44ms
step:401/1770 train_time:37320ms step_avg:95.45ms
step:402/1770 train_time:37418ms step_avg:95.45ms
step:403/1770 train_time:37516ms step_avg:95.46ms
step:404/1770 train_time:37613ms step_avg:95.47ms
step:405/1770 train_time:37711ms step_avg:95.47ms
step:406/1770 train_time:37808ms step_avg:95.48ms
step:407/1770 train_time:37906ms step_avg:95.48ms
step:408/1770 train_time:38003ms step_avg:95.48ms
step:409/1770 train_time:38100ms step_avg:95.49ms
step:410/1770 train_time:38198ms step_avg:95.49ms
step:411/1770 train_time:38296ms step_avg:95.50ms
step:412/1770 train_time:38394ms step_avg:95.51ms
step:413/1770 train_time:38491ms step_avg:95.51ms
step:414/1770 train_time:38588ms step_avg:95.52ms
step:415/1770 train_time:38686ms step_avg:95.52ms
step:416/1770 train_time:38783ms step_avg:95.52ms
step:417/1770 train_time:38880ms step_avg:95.53ms
step:418/1770 train_time:38978ms step_avg:95.53ms
step:419/1770 train_time:39076ms step_avg:95.54ms
step:420/1770 train_time:39174ms step_avg:95.55ms
step:421/1770 train_time:39272ms step_avg:95.55ms
step:422/1770 train_time:39369ms step_avg:95.56ms
step:423/1770 train_time:39467ms step_avg:95.56ms
step:424/1770 train_time:39564ms step_avg:95.57ms
step:425/1770 train_time:39662ms step_avg:95.57ms
step:426/1770 train_time:39759ms step_avg:95.57ms
step:427/1770 train_time:39857ms step_avg:95.58ms
step:428/1770 train_time:39955ms step_avg:95.59ms
step:429/1770 train_time:40052ms step_avg:95.59ms
step:430/1770 train_time:40150ms step_avg:95.60ms
step:431/1770 train_time:40249ms step_avg:95.60ms
step:432/1770 train_time:40348ms step_avg:95.61ms
step:433/1770 train_time:40445ms step_avg:95.61ms
step:434/1770 train_time:40542ms step_avg:95.62ms
step:435/1770 train_time:40639ms step_avg:95.62ms
step:436/1770 train_time:40737ms step_avg:95.63ms
step:437/1770 train_time:40834ms step_avg:95.63ms
step:438/1770 train_time:40932ms step_avg:95.64ms
step:439/1770 train_time:41030ms step_avg:95.64ms
step:440/1770 train_time:41128ms step_avg:95.65ms
step:441/1770 train_time:41225ms step_avg:95.65ms
step:442/1770 train_time:41322ms step_avg:95.65ms
step:443/1770 train_time:41419ms step_avg:95.66ms
step:444/1770 train_time:41517ms step_avg:95.66ms
step:445/1770 train_time:41615ms step_avg:95.67ms
step:446/1770 train_time:41712ms step_avg:95.67ms
step:447/1770 train_time:41809ms step_avg:95.67ms
step:448/1770 train_time:41907ms step_avg:95.68ms
step:449/1770 train_time:42004ms step_avg:95.68ms
step:450/1770 train_time:42101ms step_avg:95.68ms
step:451/1770 train_time:42198ms step_avg:95.69ms
step:452/1770 train_time:42296ms step_avg:95.69ms
step:453/1770 train_time:42394ms step_avg:95.70ms
step:454/1770 train_time:42492ms step_avg:95.70ms
step:455/1770 train_time:42590ms step_avg:95.71ms
step:456/1770 train_time:42688ms step_avg:95.71ms
step:457/1770 train_time:42785ms step_avg:95.72ms
step:458/1770 train_time:42883ms step_avg:95.72ms
step:459/1770 train_time:42980ms step_avg:95.72ms
step:460/1770 train_time:43078ms step_avg:95.73ms
step:461/1770 train_time:43175ms step_avg:95.73ms
step:462/1770 train_time:43273ms step_avg:95.74ms
step:463/1770 train_time:43370ms step_avg:95.74ms
step:464/1770 train_time:43468ms step_avg:95.74ms
step:465/1770 train_time:43566ms step_avg:95.75ms
step:466/1770 train_time:43663ms step_avg:95.75ms
step:467/1770 train_time:43761ms step_avg:95.76ms
step:468/1770 train_time:43858ms step_avg:95.76ms
step:469/1770 train_time:43956ms step_avg:95.76ms
step:470/1770 train_time:44054ms step_avg:95.77ms
step:471/1770 train_time:44151ms step_avg:95.77ms
step:472/1770 train_time:44249ms step_avg:95.78ms
step:473/1770 train_time:44346ms step_avg:95.78ms
step:474/1770 train_time:44444ms step_avg:95.78ms
step:475/1770 train_time:44541ms step_avg:95.79ms
step:476/1770 train_time:44638ms step_avg:95.79ms
step:477/1770 train_time:44736ms step_avg:95.80ms
step:478/1770 train_time:44834ms step_avg:95.80ms
step:479/1770 train_time:44932ms step_avg:95.80ms
step:480/1770 train_time:45030ms step_avg:95.81ms
step:481/1770 train_time:45128ms step_avg:95.81ms
step:482/1770 train_time:45225ms step_avg:95.82ms
step:483/1770 train_time:45323ms step_avg:95.82ms
step:484/1770 train_time:45420ms step_avg:95.82ms
step:485/1770 train_time:45517ms step_avg:95.83ms
step:486/1770 train_time:45615ms step_avg:95.83ms
step:487/1770 train_time:45713ms step_avg:95.83ms
step:488/1770 train_time:45811ms step_avg:95.84ms
step:489/1770 train_time:45909ms step_avg:95.84ms
step:490/1770 train_time:46006ms step_avg:95.84ms
step:491/1770 train_time:46103ms step_avg:95.85ms
step:492/1770 train_time:46201ms step_avg:95.85ms
step:493/1770 train_time:46298ms step_avg:95.85ms
step:494/1770 train_time:46396ms step_avg:95.86ms
step:495/1770 train_time:46494ms step_avg:95.86ms
step:496/1770 train_time:46592ms step_avg:95.87ms
step:497/1770 train_time:46689ms step_avg:95.87ms
step:498/1770 train_time:46787ms step_avg:95.87ms
step:499/1770 train_time:46884ms step_avg:95.88ms
step:500/1770 train_time:46982ms step_avg:95.88ms
step:500/1770 val_loss:3.7567 train_time:47077ms step_avg:96.08ms
step:501/1770 train_time:47099ms step_avg:95.92ms
step:502/1770 train_time:47183ms step_avg:95.90ms
step:503/1770 train_time:47283ms step_avg:95.91ms
step:504/1770 train_time:47380ms step_avg:95.91ms
step:505/1770 train_time:47478ms step_avg:95.91ms
step:506/1770 train_time:47575ms step_avg:95.92ms
step:507/1770 train_time:47673ms step_avg:95.92ms
step:508/1770 train_time:47770ms step_avg:95.92ms
step:509/1770 train_time:47868ms step_avg:95.93ms
step:510/1770 train_time:47966ms step_avg:95.93ms
step:511/1770 train_time:48063ms step_avg:95.93ms
step:512/1770 train_time:48160ms step_avg:95.94ms
step:513/1770 train_time:48258ms step_avg:95.94ms
step:514/1770 train_time:48355ms step_avg:95.94ms
step:515/1770 train_time:48454ms step_avg:95.95ms
step:516/1770 train_time:48552ms step_avg:95.95ms
step:517/1770 train_time:48650ms step_avg:95.96ms
step:518/1770 train_time:48748ms step_avg:95.96ms
step:519/1770 train_time:48846ms step_avg:95.96ms
step:520/1770 train_time:48943ms step_avg:95.97ms
step:521/1770 train_time:49041ms step_avg:95.97ms
step:522/1770 train_time:49138ms step_avg:95.97ms
step:523/1770 train_time:49236ms step_avg:95.98ms
step:524/1770 train_time:49333ms step_avg:95.98ms
step:525/1770 train_time:49431ms step_avg:95.98ms
step:526/1770 train_time:49529ms step_avg:95.99ms
step:527/1770 train_time:49627ms step_avg:95.99ms
step:528/1770 train_time:49726ms step_avg:96.00ms
step:529/1770 train_time:49824ms step_avg:96.00ms
step:530/1770 train_time:49922ms step_avg:96.00ms
step:531/1770 train_time:50019ms step_avg:96.01ms
step:532/1770 train_time:50117ms step_avg:96.01ms
step:533/1770 train_time:50215ms step_avg:96.01ms
step:534/1770 train_time:50313ms step_avg:96.02ms
step:535/1770 train_time:50412ms step_avg:96.02ms
step:536/1770 train_time:50510ms step_avg:96.03ms
step:537/1770 train_time:50609ms step_avg:96.03ms
step:538/1770 train_time:50706ms step_avg:96.03ms
step:539/1770 train_time:50804ms step_avg:96.04ms
step:540/1770 train_time:50902ms step_avg:96.04ms
step:541/1770 train_time:51000ms step_avg:96.05ms
step:542/1770 train_time:51098ms step_avg:96.05ms
step:543/1770 train_time:51196ms step_avg:96.05ms
step:544/1770 train_time:51294ms step_avg:96.06ms
step:545/1770 train_time:51392ms step_avg:96.06ms
step:546/1770 train_time:51490ms step_avg:96.06ms
step:547/1770 train_time:51588ms step_avg:96.07ms
step:548/1770 train_time:51686ms step_avg:96.07ms
step:549/1770 train_time:51783ms step_avg:96.07ms
step:550/1770 train_time:51881ms step_avg:96.08ms
step:551/1770 train_time:51979ms step_avg:96.08ms
step:552/1770 train_time:52077ms step_avg:96.08ms
step:553/1770 train_time:52174ms step_avg:96.09ms
step:554/1770 train_time:52272ms step_avg:96.09ms
step:555/1770 train_time:52371ms step_avg:96.09ms
step:556/1770 train_time:52469ms step_avg:96.10ms
step:557/1770 train_time:52567ms step_avg:96.10ms
step:558/1770 train_time:52665ms step_avg:96.10ms
step:559/1770 train_time:52763ms step_avg:96.11ms
step:560/1770 train_time:52861ms step_avg:96.11ms
step:561/1770 train_time:52958ms step_avg:96.11ms
step:562/1770 train_time:53056ms step_avg:96.12ms
step:563/1770 train_time:53154ms step_avg:96.12ms
step:564/1770 train_time:53252ms step_avg:96.12ms
step:565/1770 train_time:53351ms step_avg:96.13ms
step:566/1770 train_time:53449ms step_avg:96.13ms
step:567/1770 train_time:53548ms step_avg:96.14ms
step:568/1770 train_time:53646ms step_avg:96.14ms
step:569/1770 train_time:53743ms step_avg:96.14ms
step:570/1770 train_time:53842ms step_avg:96.15ms
step:571/1770 train_time:53939ms step_avg:96.15ms
step:572/1770 train_time:54037ms step_avg:96.15ms
step:573/1770 train_time:54135ms step_avg:96.15ms
step:574/1770 train_time:54233ms step_avg:96.16ms
step:575/1770 train_time:54332ms step_avg:96.16ms
step:576/1770 train_time:54430ms step_avg:96.17ms
step:577/1770 train_time:54528ms step_avg:96.17ms
step:578/1770 train_time:54626ms step_avg:96.17ms
step:579/1770 train_time:54724ms step_avg:96.18ms
step:580/1770 train_time:54822ms step_avg:96.18ms
step:581/1770 train_time:54920ms step_avg:96.18ms
step:582/1770 train_time:55018ms step_avg:96.18ms
step:583/1770 train_time:55115ms step_avg:96.19ms
step:584/1770 train_time:55213ms step_avg:96.19ms
step:585/1770 train_time:55311ms step_avg:96.19ms
step:586/1770 train_time:55409ms step_avg:96.20ms
step:587/1770 train_time:55508ms step_avg:96.20ms
step:588/1770 train_time:55605ms step_avg:96.20ms
step:589/1770 train_time:55703ms step_avg:96.21ms
step:590/1770 train_time:55801ms step_avg:96.21ms
step:591/1770 train_time:55899ms step_avg:96.21ms
step:592/1770 train_time:55997ms step_avg:96.21ms
step:593/1770 train_time:56096ms step_avg:96.22ms
step:594/1770 train_time:56194ms step_avg:96.22ms
step:595/1770 train_time:56292ms step_avg:96.23ms
step:596/1770 train_time:56390ms step_avg:96.23ms
step:597/1770 train_time:56488ms step_avg:96.23ms
step:598/1770 train_time:56586ms step_avg:96.23ms
step:599/1770 train_time:56684ms step_avg:96.24ms
step:600/1770 train_time:56782ms step_avg:96.24ms
step:601/1770 train_time:56880ms step_avg:96.24ms
step:602/1770 train_time:56977ms step_avg:96.25ms
step:603/1770 train_time:57075ms step_avg:96.25ms
step:604/1770 train_time:57173ms step_avg:96.25ms
step:605/1770 train_time:57272ms step_avg:96.26ms
step:606/1770 train_time:57370ms step_avg:96.26ms
step:607/1770 train_time:57468ms step_avg:96.26ms
step:608/1770 train_time:57566ms step_avg:96.26ms
step:609/1770 train_time:57664ms step_avg:96.27ms
step:610/1770 train_time:57762ms step_avg:96.27ms
step:611/1770 train_time:57860ms step_avg:96.27ms
step:612/1770 train_time:57957ms step_avg:96.27ms
step:613/1770 train_time:58055ms step_avg:96.28ms
step:614/1770 train_time:58153ms step_avg:96.28ms
step:615/1770 train_time:58251ms step_avg:96.28ms
step:616/1770 train_time:58350ms step_avg:96.29ms
step:617/1770 train_time:58448ms step_avg:96.29ms
step:618/1770 train_time:58545ms step_avg:96.29ms
step:619/1770 train_time:58644ms step_avg:96.30ms
step:620/1770 train_time:58742ms step_avg:96.30ms
step:621/1770 train_time:58839ms step_avg:96.30ms
step:622/1770 train_time:58937ms step_avg:96.30ms
step:623/1770 train_time:59035ms step_avg:96.31ms
step:624/1770 train_time:59133ms step_avg:96.31ms
step:625/1770 train_time:59231ms step_avg:96.31ms
step:625/1770 val_loss:3.6674 train_time:59328ms step_avg:96.47ms
step:626/1770 train_time:59349ms step_avg:96.35ms
step:627/1770 train_time:59436ms step_avg:96.33ms
step:628/1770 train_time:59536ms step_avg:96.34ms
step:629/1770 train_time:59633ms step_avg:96.34ms
step:630/1770 train_time:59731ms step_avg:96.34ms
step:631/1770 train_time:59829ms step_avg:96.34ms
step:632/1770 train_time:59926ms step_avg:96.34ms
step:633/1770 train_time:60025ms step_avg:96.35ms
step:634/1770 train_time:60123ms step_avg:96.35ms
step:635/1770 train_time:60221ms step_avg:96.35ms
step:636/1770 train_time:60319ms step_avg:96.36ms
step:637/1770 train_time:60417ms step_avg:96.36ms
step:638/1770 train_time:60515ms step_avg:96.36ms
step:639/1770 train_time:60613ms step_avg:96.36ms
step:640/1770 train_time:60711ms step_avg:96.37ms
step:641/1770 train_time:60809ms step_avg:96.37ms
step:642/1770 train_time:60907ms step_avg:96.37ms
step:643/1770 train_time:61005ms step_avg:96.37ms
step:644/1770 train_time:61103ms step_avg:96.38ms
step:645/1770 train_time:61201ms step_avg:96.38ms
step:646/1770 train_time:61299ms step_avg:96.38ms
step:647/1770 train_time:61397ms step_avg:96.38ms
step:648/1770 train_time:61495ms step_avg:96.39ms
step:649/1770 train_time:61593ms step_avg:96.39ms
step:650/1770 train_time:61691ms step_avg:96.39ms
step:651/1770 train_time:61788ms step_avg:96.39ms
step:652/1770 train_time:61886ms step_avg:96.40ms
step:653/1770 train_time:61984ms step_avg:96.40ms
step:654/1770 train_time:62083ms step_avg:96.40ms
step:655/1770 train_time:62181ms step_avg:96.40ms
step:656/1770 train_time:62280ms step_avg:96.41ms
step:657/1770 train_time:62377ms step_avg:96.41ms
step:658/1770 train_time:62477ms step_avg:96.42ms
step:659/1770 train_time:62577ms step_avg:96.42ms
step:660/1770 train_time:62677ms step_avg:96.43ms
step:661/1770 train_time:62777ms step_avg:96.43ms
step:662/1770 train_time:62877ms step_avg:96.44ms
step:663/1770 train_time:62978ms step_avg:96.44ms
step:664/1770 train_time:63077ms step_avg:96.45ms
step:665/1770 train_time:63177ms step_avg:96.45ms
step:666/1770 train_time:63277ms step_avg:96.46ms
step:667/1770 train_time:63377ms step_avg:96.46ms
step:668/1770 train_time:63476ms step_avg:96.47ms
step:669/1770 train_time:63576ms step_avg:96.47ms
step:670/1770 train_time:63676ms step_avg:96.48ms
step:671/1770 train_time:63777ms step_avg:96.49ms
step:672/1770 train_time:63876ms step_avg:96.49ms
step:673/1770 train_time:63977ms step_avg:96.50ms
step:674/1770 train_time:64077ms step_avg:96.50ms
step:675/1770 train_time:64178ms step_avg:96.51ms
step:676/1770 train_time:64278ms step_avg:96.51ms
step:677/1770 train_time:64378ms step_avg:96.52ms
step:678/1770 train_time:64477ms step_avg:96.52ms
step:679/1770 train_time:64577ms step_avg:96.53ms
step:680/1770 train_time:64677ms step_avg:96.53ms
step:681/1770 train_time:64777ms step_avg:96.54ms
step:682/1770 train_time:64877ms step_avg:96.54ms
step:683/1770 train_time:64977ms step_avg:96.55ms
step:684/1770 train_time:65078ms step_avg:96.55ms
step:685/1770 train_time:65178ms step_avg:96.56ms
step:686/1770 train_time:65278ms step_avg:96.56ms
step:687/1770 train_time:65377ms step_avg:96.57ms
step:688/1770 train_time:65477ms step_avg:96.57ms
step:689/1770 train_time:65577ms step_avg:96.58ms
step:690/1770 train_time:65677ms step_avg:96.58ms
step:691/1770 train_time:65777ms step_avg:96.59ms
step:692/1770 train_time:65877ms step_avg:96.59ms
step:693/1770 train_time:65977ms step_avg:96.60ms
step:694/1770 train_time:66077ms step_avg:96.60ms
step:695/1770 train_time:66177ms step_avg:96.61ms
step:696/1770 train_time:66277ms step_avg:96.61ms
step:697/1770 train_time:66377ms step_avg:96.62ms
step:698/1770 train_time:66477ms step_avg:96.62ms
step:699/1770 train_time:66577ms step_avg:96.63ms
step:700/1770 train_time:66677ms step_avg:96.63ms
step:701/1770 train_time:66776ms step_avg:96.64ms
step:702/1770 train_time:66876ms step_avg:96.64ms
step:703/1770 train_time:66976ms step_avg:96.65ms
step:704/1770 train_time:67076ms step_avg:96.65ms
step:705/1770 train_time:67177ms step_avg:96.66ms
step:706/1770 train_time:67278ms step_avg:96.66ms
step:707/1770 train_time:67378ms step_avg:96.67ms
step:708/1770 train_time:67477ms step_avg:96.67ms
step:709/1770 train_time:67578ms step_avg:96.68ms
step:710/1770 train_time:67677ms step_avg:96.68ms
step:711/1770 train_time:67777ms step_avg:96.69ms
step:712/1770 train_time:67877ms step_avg:96.69ms
step:713/1770 train_time:67977ms step_avg:96.70ms
step:714/1770 train_time:68077ms step_avg:96.70ms
step:715/1770 train_time:68177ms step_avg:96.71ms
step:716/1770 train_time:68277ms step_avg:96.71ms
step:717/1770 train_time:68377ms step_avg:96.71ms
step:718/1770 train_time:68477ms step_avg:96.72ms
step:719/1770 train_time:68577ms step_avg:96.72ms
step:720/1770 train_time:68678ms step_avg:96.73ms
step:721/1770 train_time:68777ms step_avg:96.73ms
step:722/1770 train_time:68877ms step_avg:96.74ms
step:723/1770 train_time:68977ms step_avg:96.74ms
step:724/1770 train_time:69077ms step_avg:96.75ms
step:725/1770 train_time:69177ms step_avg:96.75ms
step:726/1770 train_time:69277ms step_avg:96.76ms
step:727/1770 train_time:69377ms step_avg:96.76ms
step:728/1770 train_time:69477ms step_avg:96.76ms
step:729/1770 train_time:69576ms step_avg:96.77ms
step:730/1770 train_time:69677ms step_avg:96.77ms
step:731/1770 train_time:69777ms step_avg:96.78ms
step:732/1770 train_time:69878ms step_avg:96.78ms
step:733/1770 train_time:69977ms step_avg:96.79ms
step:734/1770 train_time:70077ms step_avg:96.79ms
step:735/1770 train_time:70177ms step_avg:96.80ms
step:736/1770 train_time:70277ms step_avg:96.80ms
step:737/1770 train_time:70377ms step_avg:96.80ms
step:738/1770 train_time:70477ms step_avg:96.81ms
step:739/1770 train_time:70576ms step_avg:96.81ms
step:740/1770 train_time:70676ms step_avg:96.82ms
step:741/1770 train_time:70775ms step_avg:96.82ms
step:742/1770 train_time:70875ms step_avg:96.82ms
step:743/1770 train_time:70975ms step_avg:96.83ms
step:744/1770 train_time:71075ms step_avg:96.83ms
step:745/1770 train_time:71174ms step_avg:96.84ms
step:746/1770 train_time:71275ms step_avg:96.84ms
step:747/1770 train_time:71375ms step_avg:96.85ms
step:748/1770 train_time:71475ms step_avg:96.85ms
step:749/1770 train_time:71576ms step_avg:96.86ms
step:750/1770 train_time:71676ms step_avg:96.86ms
step:750/1770 val_loss:3.6033 train_time:71774ms step_avg:96.99ms
step:751/1770 train_time:71795ms step_avg:96.89ms
step:752/1770 train_time:71883ms step_avg:96.88ms
step:753/1770 train_time:71982ms step_avg:96.88ms
step:754/1770 train_time:72082ms step_avg:96.88ms
step:755/1770 train_time:72182ms step_avg:96.89ms
step:756/1770 train_time:72282ms step_avg:96.89ms
step:757/1770 train_time:72382ms step_avg:96.90ms
step:758/1770 train_time:72482ms step_avg:96.90ms
step:759/1770 train_time:72582ms step_avg:96.91ms
step:760/1770 train_time:72682ms step_avg:96.91ms
step:761/1770 train_time:72783ms step_avg:96.91ms
step:762/1770 train_time:72883ms step_avg:96.92ms
step:763/1770 train_time:72984ms step_avg:96.92ms
step:764/1770 train_time:73084ms step_avg:96.93ms
step:765/1770 train_time:73184ms step_avg:96.93ms
step:766/1770 train_time:73283ms step_avg:96.93ms
step:767/1770 train_time:73383ms step_avg:96.94ms
step:768/1770 train_time:73483ms step_avg:96.94ms
step:769/1770 train_time:73583ms step_avg:96.95ms
step:770/1770 train_time:73683ms step_avg:96.95ms
step:771/1770 train_time:73782ms step_avg:96.95ms
step:772/1770 train_time:73882ms step_avg:96.96ms
step:773/1770 train_time:73981ms step_avg:96.96ms
step:774/1770 train_time:74081ms step_avg:96.97ms
step:775/1770 train_time:74182ms step_avg:96.97ms
step:776/1770 train_time:74283ms step_avg:96.98ms
step:777/1770 train_time:74383ms step_avg:96.98ms
step:778/1770 train_time:74483ms step_avg:96.98ms
step:779/1770 train_time:74584ms step_avg:96.99ms
step:780/1770 train_time:74683ms step_avg:96.99ms
step:781/1770 train_time:74783ms step_avg:96.99ms
step:782/1770 train_time:74882ms step_avg:97.00ms
step:783/1770 train_time:74982ms step_avg:97.00ms
step:784/1770 train_time:75081ms step_avg:97.00ms
step:785/1770 train_time:75182ms step_avg:97.01ms
step:786/1770 train_time:75281ms step_avg:97.01ms
step:787/1770 train_time:75381ms step_avg:97.02ms
step:788/1770 train_time:75482ms step_avg:97.02ms
step:789/1770 train_time:75583ms step_avg:97.03ms
step:790/1770 train_time:75683ms step_avg:97.03ms
step:791/1770 train_time:75784ms step_avg:97.03ms
step:792/1770 train_time:75884ms step_avg:97.04ms
step:793/1770 train_time:75984ms step_avg:97.04ms
step:794/1770 train_time:76084ms step_avg:97.05ms
step:795/1770 train_time:76184ms step_avg:97.05ms
step:796/1770 train_time:76284ms step_avg:97.05ms
step:797/1770 train_time:76384ms step_avg:97.06ms
step:798/1770 train_time:76483ms step_avg:97.06ms
step:799/1770 train_time:76583ms step_avg:97.06ms
step:800/1770 train_time:76683ms step_avg:97.07ms
step:801/1770 train_time:76783ms step_avg:97.07ms
step:802/1770 train_time:76883ms step_avg:97.07ms
step:803/1770 train_time:76983ms step_avg:97.08ms
step:804/1770 train_time:77084ms step_avg:97.08ms
step:805/1770 train_time:77184ms step_avg:97.09ms
step:806/1770 train_time:77283ms step_avg:97.09ms
step:807/1770 train_time:77383ms step_avg:97.09ms
step:808/1770 train_time:77483ms step_avg:97.10ms
step:809/1770 train_time:77583ms step_avg:97.10ms
step:810/1770 train_time:77683ms step_avg:97.10ms
step:811/1770 train_time:77783ms step_avg:97.11ms
step:812/1770 train_time:77883ms step_avg:97.11ms
step:813/1770 train_time:77984ms step_avg:97.12ms
step:814/1770 train_time:78084ms step_avg:97.12ms
step:815/1770 train_time:78184ms step_avg:97.12ms
step:816/1770 train_time:78283ms step_avg:97.13ms
step:817/1770 train_time:78383ms step_avg:97.13ms
step:818/1770 train_time:78483ms step_avg:97.13ms
step:819/1770 train_time:78583ms step_avg:97.14ms
step:820/1770 train_time:78683ms step_avg:97.14ms
step:821/1770 train_time:78784ms step_avg:97.14ms
step:822/1770 train_time:78883ms step_avg:97.15ms
step:823/1770 train_time:78984ms step_avg:97.15ms
step:824/1770 train_time:79083ms step_avg:97.15ms
step:825/1770 train_time:79183ms step_avg:97.16ms
step:826/1770 train_time:79282ms step_avg:97.16ms
step:827/1770 train_time:79383ms step_avg:97.16ms
step:828/1770 train_time:79482ms step_avg:97.17ms
step:829/1770 train_time:79582ms step_avg:97.17ms
step:830/1770 train_time:79683ms step_avg:97.17ms
step:831/1770 train_time:79783ms step_avg:97.18ms
step:832/1770 train_time:79882ms step_avg:97.18ms
step:833/1770 train_time:79983ms step_avg:97.18ms
step:834/1770 train_time:80083ms step_avg:97.19ms
step:835/1770 train_time:80183ms step_avg:97.19ms
step:836/1770 train_time:80283ms step_avg:97.20ms
step:837/1770 train_time:80383ms step_avg:97.20ms
step:838/1770 train_time:80483ms step_avg:97.20ms
step:839/1770 train_time:80583ms step_avg:97.21ms
step:840/1770 train_time:80684ms step_avg:97.21ms
step:841/1770 train_time:80783ms step_avg:97.21ms
step:842/1770 train_time:80883ms step_avg:97.22ms
step:843/1770 train_time:80983ms step_avg:97.22ms
step:844/1770 train_time:81083ms step_avg:97.22ms
step:845/1770 train_time:81183ms step_avg:97.23ms
step:846/1770 train_time:81283ms step_avg:97.23ms
step:847/1770 train_time:81383ms step_avg:97.23ms
step:848/1770 train_time:81483ms step_avg:97.24ms
step:849/1770 train_time:81583ms step_avg:97.24ms
step:850/1770 train_time:81683ms step_avg:97.24ms
step:851/1770 train_time:81784ms step_avg:97.25ms
step:852/1770 train_time:81884ms step_avg:97.25ms
step:853/1770 train_time:81984ms step_avg:97.25ms
step:854/1770 train_time:82083ms step_avg:97.25ms
step:855/1770 train_time:82183ms step_avg:97.26ms
step:856/1770 train_time:82283ms step_avg:97.26ms
step:857/1770 train_time:82383ms step_avg:97.26ms
step:858/1770 train_time:82483ms step_avg:97.27ms
step:859/1770 train_time:82583ms step_avg:97.27ms
step:860/1770 train_time:82683ms step_avg:97.27ms
step:861/1770 train_time:82783ms step_avg:97.28ms
step:862/1770 train_time:82882ms step_avg:97.28ms
step:863/1770 train_time:82982ms step_avg:97.28ms
step:864/1770 train_time:83082ms step_avg:97.29ms
step:865/1770 train_time:83184ms step_avg:97.29ms
step:866/1770 train_time:83284ms step_avg:97.29ms
step:867/1770 train_time:83384ms step_avg:97.30ms
step:868/1770 train_time:83484ms step_avg:97.30ms
step:869/1770 train_time:83584ms step_avg:97.30ms
step:870/1770 train_time:83685ms step_avg:97.31ms
step:871/1770 train_time:83784ms step_avg:97.31ms
step:872/1770 train_time:83884ms step_avg:97.31ms
step:873/1770 train_time:83984ms step_avg:97.32ms
step:874/1770 train_time:84084ms step_avg:97.32ms
step:875/1770 train_time:84183ms step_avg:97.32ms
step:875/1770 val_loss:3.5538 train_time:84281ms step_avg:97.43ms
step:876/1770 train_time:84302ms step_avg:97.35ms
step:877/1770 train_time:84391ms step_avg:97.34ms
step:878/1770 train_time:84491ms step_avg:97.34ms
step:879/1770 train_time:84592ms step_avg:97.34ms
step:880/1770 train_time:84691ms step_avg:97.35ms
step:881/1770 train_time:84791ms step_avg:97.35ms
step:882/1770 train_time:84891ms step_avg:97.35ms
step:883/1770 train_time:84991ms step_avg:97.35ms
step:884/1770 train_time:85091ms step_avg:97.36ms
step:885/1770 train_time:85192ms step_avg:97.36ms
step:886/1770 train_time:85295ms step_avg:97.37ms
step:887/1770 train_time:85396ms step_avg:97.37ms
step:888/1770 train_time:85496ms step_avg:97.38ms
step:889/1770 train_time:85596ms step_avg:97.38ms
step:890/1770 train_time:85696ms step_avg:97.38ms
step:891/1770 train_time:85795ms step_avg:97.38ms
step:892/1770 train_time:85896ms step_avg:97.39ms
step:893/1770 train_time:85995ms step_avg:97.39ms
step:894/1770 train_time:86096ms step_avg:97.39ms
step:895/1770 train_time:86196ms step_avg:97.40ms
step:896/1770 train_time:86297ms step_avg:97.40ms
step:897/1770 train_time:86398ms step_avg:97.40ms
step:898/1770 train_time:86499ms step_avg:97.41ms
step:899/1770 train_time:86599ms step_avg:97.41ms
step:900/1770 train_time:86699ms step_avg:97.41ms
step:901/1770 train_time:86799ms step_avg:97.42ms
step:902/1770 train_time:86900ms step_avg:97.42ms
step:903/1770 train_time:87000ms step_avg:97.42ms
step:904/1770 train_time:87101ms step_avg:97.43ms
step:905/1770 train_time:87201ms step_avg:97.43ms
step:906/1770 train_time:87301ms step_avg:97.43ms
step:907/1770 train_time:87401ms step_avg:97.44ms
step:908/1770 train_time:87501ms step_avg:97.44ms
step:909/1770 train_time:87601ms step_avg:97.44ms
step:910/1770 train_time:87702ms step_avg:97.45ms
step:911/1770 train_time:87802ms step_avg:97.45ms
step:912/1770 train_time:87901ms step_avg:97.45ms
step:913/1770 train_time:88001ms step_avg:97.45ms
step:914/1770 train_time:88101ms step_avg:97.46ms
step:915/1770 train_time:88201ms step_avg:97.46ms
step:916/1770 train_time:88301ms step_avg:97.46ms
step:917/1770 train_time:88401ms step_avg:97.47ms
step:918/1770 train_time:88501ms step_avg:97.47ms
step:919/1770 train_time:88601ms step_avg:97.47ms
step:920/1770 train_time:88704ms step_avg:97.48ms
step:921/1770 train_time:88805ms step_avg:97.48ms
step:922/1770 train_time:88907ms step_avg:97.49ms
step:923/1770 train_time:89008ms step_avg:97.49ms
step:924/1770 train_time:89110ms step_avg:97.49ms
step:925/1770 train_time:89210ms step_avg:97.50ms
step:926/1770 train_time:89312ms step_avg:97.50ms
step:927/1770 train_time:89413ms step_avg:97.51ms
step:928/1770 train_time:89515ms step_avg:97.51ms
step:929/1770 train_time:89616ms step_avg:97.51ms
step:930/1770 train_time:89717ms step_avg:97.52ms
step:931/1770 train_time:89819ms step_avg:97.52ms
step:932/1770 train_time:89921ms step_avg:97.53ms
step:933/1770 train_time:90023ms step_avg:97.53ms
step:934/1770 train_time:90124ms step_avg:97.54ms
step:935/1770 train_time:90225ms step_avg:97.54ms
step:936/1770 train_time:90326ms step_avg:97.54ms
step:937/1770 train_time:90427ms step_avg:97.55ms
step:938/1770 train_time:90528ms step_avg:97.55ms
step:939/1770 train_time:90629ms step_avg:97.55ms
step:940/1770 train_time:90730ms step_avg:97.56ms
step:941/1770 train_time:90831ms step_avg:97.56ms
step:942/1770 train_time:90935ms step_avg:97.57ms
step:943/1770 train_time:91037ms step_avg:97.57ms
step:944/1770 train_time:91138ms step_avg:97.58ms
step:945/1770 train_time:91239ms step_avg:97.58ms
step:946/1770 train_time:91342ms step_avg:97.59ms
step:947/1770 train_time:91444ms step_avg:97.59ms
step:948/1770 train_time:91545ms step_avg:97.60ms
step:949/1770 train_time:91646ms step_avg:97.60ms
step:950/1770 train_time:91748ms step_avg:97.60ms
step:951/1770 train_time:91849ms step_avg:97.61ms
step:952/1770 train_time:91950ms step_avg:97.61ms
step:953/1770 train_time:92052ms step_avg:97.62ms
step:954/1770 train_time:92154ms step_avg:97.62ms
step:955/1770 train_time:92256ms step_avg:97.63ms
step:956/1770 train_time:92358ms step_avg:97.63ms
step:957/1770 train_time:92460ms step_avg:97.63ms
step:958/1770 train_time:92561ms step_avg:97.64ms
step:959/1770 train_time:92662ms step_avg:97.64ms
step:960/1770 train_time:92763ms step_avg:97.65ms
step:961/1770 train_time:92865ms step_avg:97.65ms
step:962/1770 train_time:92967ms step_avg:97.65ms
step:963/1770 train_time:93068ms step_avg:97.66ms
step:964/1770 train_time:93169ms step_avg:97.66ms
step:965/1770 train_time:93271ms step_avg:97.67ms
step:966/1770 train_time:93372ms step_avg:97.67ms
step:967/1770 train_time:93474ms step_avg:97.67ms
step:968/1770 train_time:93575ms step_avg:97.68ms
step:969/1770 train_time:93676ms step_avg:97.68ms
step:970/1770 train_time:93778ms step_avg:97.68ms
step:971/1770 train_time:93879ms step_avg:97.69ms
step:972/1770 train_time:93980ms step_avg:97.69ms
step:973/1770 train_time:94082ms step_avg:97.70ms
step:974/1770 train_time:94183ms step_avg:97.70ms
step:975/1770 train_time:94285ms step_avg:97.71ms
step:976/1770 train_time:94386ms step_avg:97.71ms
step:977/1770 train_time:94487ms step_avg:97.71ms
step:978/1770 train_time:94589ms step_avg:97.72ms
step:979/1770 train_time:94690ms step_avg:97.72ms
step:980/1770 train_time:94792ms step_avg:97.72ms
step:981/1770 train_time:94893ms step_avg:97.73ms
step:982/1770 train_time:94996ms step_avg:97.73ms
step:983/1770 train_time:95097ms step_avg:97.74ms
step:984/1770 train_time:95200ms step_avg:97.74ms
step:985/1770 train_time:95301ms step_avg:97.74ms
step:986/1770 train_time:95403ms step_avg:97.75ms
step:987/1770 train_time:95504ms step_avg:97.75ms
step:988/1770 train_time:95604ms step_avg:97.75ms
step:989/1770 train_time:95707ms step_avg:97.76ms
step:990/1770 train_time:95808ms step_avg:97.76ms
step:991/1770 train_time:95909ms step_avg:97.77ms
step:992/1770 train_time:96011ms step_avg:97.77ms
step:993/1770 train_time:96113ms step_avg:97.78ms
step:994/1770 train_time:96215ms step_avg:97.78ms
step:995/1770 train_time:96316ms step_avg:97.78ms
step:996/1770 train_time:96417ms step_avg:97.79ms
step:997/1770 train_time:96519ms step_avg:97.79ms
step:998/1770 train_time:96620ms step_avg:97.79ms
step:999/1770 train_time:96721ms step_avg:97.80ms
step:1000/1770 train_time:96823ms step_avg:97.80ms
step:1000/1770 val_loss:3.5168 train_time:96922ms step_avg:97.90ms
step:1001/1770 train_time:96944ms step_avg:97.82ms
step:1002/1770 train_time:97034ms step_avg:97.82ms
step:1003/1770 train_time:97137ms step_avg:97.82ms
step:1004/1770 train_time:97238ms step_avg:97.83ms
step:1005/1770 train_time:97339ms step_avg:97.83ms
step:1006/1770 train_time:97440ms step_avg:97.83ms
step:1007/1770 train_time:97541ms step_avg:97.83ms
step:1008/1770 train_time:97642ms step_avg:97.84ms
step:1009/1770 train_time:97744ms step_avg:97.84ms
step:1010/1770 train_time:97844ms step_avg:97.84ms
step:1011/1770 train_time:97947ms step_avg:97.85ms
step:1012/1770 train_time:98049ms step_avg:97.85ms
step:1013/1770 train_time:98150ms step_avg:97.86ms
step:1014/1770 train_time:98251ms step_avg:97.86ms
step:1015/1770 train_time:98351ms step_avg:97.86ms
step:1016/1770 train_time:98452ms step_avg:97.86ms
step:1017/1770 train_time:98554ms step_avg:97.87ms
step:1018/1770 train_time:98656ms step_avg:97.87ms
step:1019/1770 train_time:98758ms step_avg:97.88ms
step:1020/1770 train_time:98861ms step_avg:97.88ms
step:1021/1770 train_time:98962ms step_avg:97.89ms
step:1022/1770 train_time:99064ms step_avg:97.89ms
step:1023/1770 train_time:99165ms step_avg:97.89ms
step:1024/1770 train_time:99266ms step_avg:97.90ms
step:1025/1770 train_time:99368ms step_avg:97.90ms
step:1026/1770 train_time:99470ms step_avg:97.90ms
step:1027/1770 train_time:99570ms step_avg:97.91ms
step:1028/1770 train_time:99671ms step_avg:97.91ms
step:1029/1770 train_time:99773ms step_avg:97.91ms
step:1030/1770 train_time:99873ms step_avg:97.92ms
step:1031/1770 train_time:99974ms step_avg:97.92ms
step:1032/1770 train_time:100076ms step_avg:97.92ms
step:1033/1770 train_time:100178ms step_avg:97.93ms
step:1034/1770 train_time:100279ms step_avg:97.93ms
step:1035/1770 train_time:100381ms step_avg:97.93ms
step:1036/1770 train_time:100482ms step_avg:97.94ms
step:1037/1770 train_time:100585ms step_avg:97.94ms
step:1038/1770 train_time:100687ms step_avg:97.94ms
step:1039/1770 train_time:100789ms step_avg:97.95ms
step:1040/1770 train_time:100890ms step_avg:97.95ms
step:1041/1770 train_time:100991ms step_avg:97.95ms
step:1042/1770 train_time:101092ms step_avg:97.96ms
step:1043/1770 train_time:101193ms step_avg:97.96ms
step:1044/1770 train_time:101294ms step_avg:97.96ms
step:1045/1770 train_time:101396ms step_avg:97.97ms
step:1046/1770 train_time:101498ms step_avg:97.97ms
step:1047/1770 train_time:101601ms step_avg:97.98ms
step:1048/1770 train_time:101702ms step_avg:97.98ms
step:1049/1770 train_time:101803ms step_avg:97.98ms
step:1050/1770 train_time:101905ms step_avg:97.99ms
step:1051/1770 train_time:102008ms step_avg:97.99ms
step:1052/1770 train_time:102110ms step_avg:97.99ms
step:1053/1770 train_time:102211ms step_avg:98.00ms
step:1054/1770 train_time:102312ms step_avg:98.00ms
step:1055/1770 train_time:102413ms step_avg:98.00ms
step:1056/1770 train_time:102513ms step_avg:98.01ms
step:1057/1770 train_time:102614ms step_avg:98.01ms
step:1058/1770 train_time:102716ms step_avg:98.01ms
step:1059/1770 train_time:102819ms step_avg:98.02ms
step:1060/1770 train_time:102921ms step_avg:98.02ms
step:1061/1770 train_time:103022ms step_avg:98.02ms
step:1062/1770 train_time:103123ms step_avg:98.03ms
step:1063/1770 train_time:103226ms step_avg:98.03ms
step:1064/1770 train_time:103328ms step_avg:98.03ms
step:1065/1770 train_time:103429ms step_avg:98.04ms
step:1066/1770 train_time:103530ms step_avg:98.04ms
step:1067/1770 train_time:103631ms step_avg:98.04ms
step:1068/1770 train_time:103733ms step_avg:98.05ms
step:1069/1770 train_time:103834ms step_avg:98.05ms
step:1070/1770 train_time:103935ms step_avg:98.05ms
step:1071/1770 train_time:104037ms step_avg:98.06ms
step:1072/1770 train_time:104139ms step_avg:98.06ms
step:1073/1770 train_time:104241ms step_avg:98.06ms
step:1074/1770 train_time:104343ms step_avg:98.07ms
step:1075/1770 train_time:104445ms step_avg:98.07ms
step:1076/1770 train_time:104546ms step_avg:98.07ms
step:1077/1770 train_time:104647ms step_avg:98.08ms
step:1078/1770 train_time:104749ms step_avg:98.08ms
step:1079/1770 train_time:104850ms step_avg:98.08ms
step:1080/1770 train_time:104951ms step_avg:98.09ms
step:1081/1770 train_time:105053ms step_avg:98.09ms
step:1082/1770 train_time:105154ms step_avg:98.09ms
step:1083/1770 train_time:105257ms step_avg:98.10ms
step:1084/1770 train_time:105359ms step_avg:98.10ms
step:1085/1770 train_time:105461ms step_avg:98.10ms
step:1086/1770 train_time:105563ms step_avg:98.11ms
step:1087/1770 train_time:105664ms step_avg:98.11ms
step:1088/1770 train_time:105765ms step_avg:98.11ms
step:1089/1770 train_time:105867ms step_avg:98.12ms
step:1090/1770 train_time:105969ms step_avg:98.12ms
step:1091/1770 train_time:106070ms step_avg:98.12ms
step:1092/1770 train_time:106171ms step_avg:98.12ms
step:1093/1770 train_time:106273ms step_avg:98.13ms
step:1094/1770 train_time:106374ms step_avg:98.13ms
step:1095/1770 train_time:106476ms step_avg:98.13ms
step:1096/1770 train_time:106578ms step_avg:98.14ms
step:1097/1770 train_time:106681ms step_avg:98.14ms
step:1098/1770 train_time:106782ms step_avg:98.15ms
step:1099/1770 train_time:106885ms step_avg:98.15ms
step:1100/1770 train_time:106987ms step_avg:98.15ms
step:1101/1770 train_time:107089ms step_avg:98.16ms
step:1102/1770 train_time:107190ms step_avg:98.16ms
step:1103/1770 train_time:107291ms step_avg:98.16ms
step:1104/1770 train_time:107392ms step_avg:98.16ms
step:1105/1770 train_time:107493ms step_avg:98.17ms
step:1106/1770 train_time:107595ms step_avg:98.17ms
step:1107/1770 train_time:107697ms step_avg:98.17ms
step:1108/1770 train_time:107799ms step_avg:98.18ms
step:1109/1770 train_time:107902ms step_avg:98.18ms
step:1110/1770 train_time:108004ms step_avg:98.19ms
step:1111/1770 train_time:108105ms step_avg:98.19ms
step:1112/1770 train_time:108207ms step_avg:98.19ms
step:1113/1770 train_time:108309ms step_avg:98.19ms
step:1114/1770 train_time:108410ms step_avg:98.20ms
step:1115/1770 train_time:108511ms step_avg:98.20ms
step:1116/1770 train_time:108613ms step_avg:98.20ms
step:1117/1770 train_time:108714ms step_avg:98.21ms
step:1118/1770 train_time:108815ms step_avg:98.21ms
step:1119/1770 train_time:108916ms step_avg:98.21ms
step:1120/1770 train_time:109019ms step_avg:98.22ms
step:1121/1770 train_time:109121ms step_avg:98.22ms
step:1122/1770 train_time:109223ms step_avg:98.22ms
step:1123/1770 train_time:109324ms step_avg:98.22ms
step:1124/1770 train_time:109425ms step_avg:98.23ms
step:1125/1770 train_time:109527ms step_avg:98.23ms
step:1125/1770 val_loss:3.4763 train_time:109627ms step_avg:98.32ms
step:1126/1770 train_time:109648ms step_avg:98.25ms
step:1127/1770 train_time:109739ms step_avg:98.24ms
step:1128/1770 train_time:109840ms step_avg:98.25ms
step:1129/1770 train_time:109941ms step_avg:98.25ms
step:1130/1770 train_time:110043ms step_avg:98.25ms
step:1131/1770 train_time:110145ms step_avg:98.26ms
step:1132/1770 train_time:110246ms step_avg:98.26ms
step:1133/1770 train_time:110348ms step_avg:98.26ms
step:1134/1770 train_time:110451ms step_avg:98.27ms
step:1135/1770 train_time:110552ms step_avg:98.27ms
step:1136/1770 train_time:110654ms step_avg:98.27ms
step:1137/1770 train_time:110756ms step_avg:98.27ms
step:1138/1770 train_time:110857ms step_avg:98.28ms
step:1139/1770 train_time:110958ms step_avg:98.28ms
step:1140/1770 train_time:111060ms step_avg:98.28ms
step:1141/1770 train_time:111161ms step_avg:98.29ms
step:1142/1770 train_time:111263ms step_avg:98.29ms
step:1143/1770 train_time:111365ms step_avg:98.29ms
step:1144/1770 train_time:111466ms step_avg:98.29ms
step:1145/1770 train_time:111568ms step_avg:98.30ms
step:1146/1770 train_time:111670ms step_avg:98.30ms
step:1147/1770 train_time:111772ms step_avg:98.30ms
step:1148/1770 train_time:111874ms step_avg:98.31ms
step:1149/1770 train_time:111975ms step_avg:98.31ms
step:1150/1770 train_time:112076ms step_avg:98.31ms
step:1151/1770 train_time:112178ms step_avg:98.32ms
step:1152/1770 train_time:112281ms step_avg:98.32ms
step:1153/1770 train_time:112381ms step_avg:98.32ms
step:1154/1770 train_time:112483ms step_avg:98.32ms
step:1155/1770 train_time:112585ms step_avg:98.33ms
step:1156/1770 train_time:112686ms step_avg:98.33ms
step:1157/1770 train_time:112789ms step_avg:98.33ms
step:1158/1770 train_time:112891ms step_avg:98.34ms
step:1159/1770 train_time:112993ms step_avg:98.34ms
step:1160/1770 train_time:113094ms step_avg:98.34ms
step:1161/1770 train_time:113196ms step_avg:98.35ms
step:1162/1770 train_time:113297ms step_avg:98.35ms
step:1163/1770 train_time:113398ms step_avg:98.35ms
step:1164/1770 train_time:113500ms step_avg:98.35ms
step:1165/1770 train_time:113602ms step_avg:98.36ms
step:1166/1770 train_time:113704ms step_avg:98.36ms
step:1167/1770 train_time:113806ms step_avg:98.36ms
step:1168/1770 train_time:113908ms step_avg:98.37ms
step:1169/1770 train_time:114009ms step_avg:98.37ms
step:1170/1770 train_time:114110ms step_avg:98.37ms
step:1171/1770 train_time:114212ms step_avg:98.37ms
step:1172/1770 train_time:114314ms step_avg:98.38ms
step:1173/1770 train_time:114415ms step_avg:98.38ms
step:1174/1770 train_time:114516ms step_avg:98.38ms
step:1175/1770 train_time:114617ms step_avg:98.38ms
step:1176/1770 train_time:114719ms step_avg:98.39ms
step:1177/1770 train_time:114820ms step_avg:98.39ms
step:1178/1770 train_time:114923ms step_avg:98.39ms
step:1179/1770 train_time:115025ms step_avg:98.40ms
step:1180/1770 train_time:115127ms step_avg:98.40ms
step:1181/1770 train_time:115228ms step_avg:98.40ms
step:1182/1770 train_time:115329ms step_avg:98.40ms
step:1183/1770 train_time:115432ms step_avg:98.41ms
step:1184/1770 train_time:115536ms step_avg:98.41ms
step:1185/1770 train_time:115638ms step_avg:98.42ms
step:1186/1770 train_time:115740ms step_avg:98.42ms
step:1187/1770 train_time:115846ms step_avg:98.42ms
step:1188/1770 train_time:115948ms step_avg:98.43ms
step:1189/1770 train_time:116051ms step_avg:98.43ms
step:1190/1770 train_time:116153ms step_avg:98.43ms
step:1191/1770 train_time:116256ms step_avg:98.44ms
step:1192/1770 train_time:116359ms step_avg:98.44ms
step:1193/1770 train_time:116462ms step_avg:98.45ms
step:1194/1770 train_time:116565ms step_avg:98.45ms
step:1195/1770 train_time:116668ms step_avg:98.45ms
step:1196/1770 train_time:116771ms step_avg:98.46ms
step:1197/1770 train_time:116873ms step_avg:98.46ms
step:1198/1770 train_time:116975ms step_avg:98.46ms
step:1199/1770 train_time:117078ms step_avg:98.47ms
step:1200/1770 train_time:117181ms step_avg:98.47ms
step:1201/1770 train_time:117284ms step_avg:98.48ms
step:1202/1770 train_time:117386ms step_avg:98.48ms
step:1203/1770 train_time:117488ms step_avg:98.48ms
step:1204/1770 train_time:117590ms step_avg:98.48ms
step:1205/1770 train_time:117693ms step_avg:98.49ms
step:1206/1770 train_time:117796ms step_avg:98.49ms
step:1207/1770 train_time:117899ms step_avg:98.50ms
step:1208/1770 train_time:118001ms step_avg:98.50ms
step:1209/1770 train_time:118104ms step_avg:98.50ms
step:1210/1770 train_time:118206ms step_avg:98.51ms
step:1211/1770 train_time:118310ms step_avg:98.51ms
step:1212/1770 train_time:118413ms step_avg:98.51ms
step:1213/1770 train_time:118516ms step_avg:98.52ms
step:1214/1770 train_time:118618ms step_avg:98.52ms
step:1215/1770 train_time:118721ms step_avg:98.52ms
step:1216/1770 train_time:118827ms step_avg:98.53ms
step:1217/1770 train_time:118929ms step_avg:98.53ms
step:1218/1770 train_time:119032ms step_avg:98.54ms
step:1219/1770 train_time:119135ms step_avg:98.54ms
step:1220/1770 train_time:119237ms step_avg:98.54ms
step:1221/1770 train_time:119340ms step_avg:98.55ms
step:1222/1770 train_time:119444ms step_avg:98.55ms
step:1223/1770 train_time:119546ms step_avg:98.55ms
step:1224/1770 train_time:119650ms step_avg:98.56ms
step:1225/1770 train_time:119753ms step_avg:98.56ms
step:1226/1770 train_time:119855ms step_avg:98.57ms
step:1227/1770 train_time:119959ms step_avg:98.57ms
step:1228/1770 train_time:120063ms step_avg:98.57ms
step:1229/1770 train_time:120166ms step_avg:98.58ms
step:1230/1770 train_time:120270ms step_avg:98.58ms
step:1231/1770 train_time:120373ms step_avg:98.59ms
step:1232/1770 train_time:120475ms step_avg:98.59ms
step:1233/1770 train_time:120577ms step_avg:98.59ms
step:1234/1770 train_time:120679ms step_avg:98.59ms
step:1235/1770 train_time:120783ms step_avg:98.60ms
step:1236/1770 train_time:120887ms step_avg:98.60ms
step:1237/1770 train_time:120989ms step_avg:98.61ms
step:1238/1770 train_time:121093ms step_avg:98.61ms
step:1239/1770 train_time:121196ms step_avg:98.61ms
step:1240/1770 train_time:121298ms step_avg:98.62ms
step:1241/1770 train_time:121401ms step_avg:98.62ms
step:1242/1770 train_time:121504ms step_avg:98.62ms
step:1243/1770 train_time:121606ms step_avg:98.63ms
step:1244/1770 train_time:121709ms step_avg:98.63ms
step:1245/1770 train_time:121812ms step_avg:98.63ms
step:1246/1770 train_time:121915ms step_avg:98.64ms
step:1247/1770 train_time:122018ms step_avg:98.64ms
step:1248/1770 train_time:122121ms step_avg:98.64ms
step:1249/1770 train_time:122223ms step_avg:98.65ms
step:1250/1770 train_time:122326ms step_avg:98.65ms
step:1250/1770 val_loss:3.4308 train_time:122427ms step_avg:98.73ms
step:1251/1770 train_time:122449ms step_avg:98.67ms
step:1252/1770 train_time:122538ms step_avg:98.66ms
step:1253/1770 train_time:122641ms step_avg:98.66ms
step:1254/1770 train_time:122743ms step_avg:98.67ms
step:1255/1770 train_time:122849ms step_avg:98.67ms
step:1256/1770 train_time:122951ms step_avg:98.68ms
step:1257/1770 train_time:123052ms step_avg:98.68ms
step:1258/1770 train_time:123155ms step_avg:98.68ms
step:1259/1770 train_time:123258ms step_avg:98.69ms
step:1260/1770 train_time:123360ms step_avg:98.69ms
step:1261/1770 train_time:123463ms step_avg:98.69ms
step:1262/1770 train_time:123568ms step_avg:98.70ms
step:1263/1770 train_time:123669ms step_avg:98.70ms
step:1264/1770 train_time:123774ms step_avg:98.70ms
step:1265/1770 train_time:123876ms step_avg:98.71ms
step:1266/1770 train_time:123978ms step_avg:98.71ms
step:1267/1770 train_time:124082ms step_avg:98.71ms
step:1268/1770 train_time:124184ms step_avg:98.72ms
step:1269/1770 train_time:124287ms step_avg:98.72ms
step:1270/1770 train_time:124390ms step_avg:98.72ms
step:1271/1770 train_time:124493ms step_avg:98.73ms
step:1272/1770 train_time:124595ms step_avg:98.73ms
step:1273/1770 train_time:124699ms step_avg:98.73ms
step:1274/1770 train_time:124802ms step_avg:98.74ms
step:1275/1770 train_time:124904ms step_avg:98.74ms
step:1276/1770 train_time:125008ms step_avg:98.74ms
step:1277/1770 train_time:125110ms step_avg:98.74ms
step:1278/1770 train_time:125213ms step_avg:98.75ms
step:1279/1770 train_time:125317ms step_avg:98.75ms
step:1280/1770 train_time:125421ms step_avg:98.76ms
step:1281/1770 train_time:125523ms step_avg:98.76ms
step:1282/1770 train_time:125626ms step_avg:98.76ms
step:1283/1770 train_time:125729ms step_avg:98.77ms
step:1284/1770 train_time:125831ms step_avg:98.77ms
step:1285/1770 train_time:125934ms step_avg:98.77ms
step:1286/1770 train_time:126038ms step_avg:98.78ms
step:1287/1770 train_time:126142ms step_avg:98.78ms
step:1288/1770 train_time:126245ms step_avg:98.78ms
step:1289/1770 train_time:126348ms step_avg:98.79ms
step:1290/1770 train_time:126450ms step_avg:98.79ms
step:1291/1770 train_time:126552ms step_avg:98.79ms
step:1292/1770 train_time:126655ms step_avg:98.79ms
step:1293/1770 train_time:126759ms step_avg:98.80ms
step:1294/1770 train_time:126861ms step_avg:98.80ms
step:1295/1770 train_time:126964ms step_avg:98.80ms
step:1296/1770 train_time:127067ms step_avg:98.81ms
step:1297/1770 train_time:127169ms step_avg:98.81ms
step:1298/1770 train_time:127272ms step_avg:98.81ms
step:1299/1770 train_time:127374ms step_avg:98.82ms
step:1300/1770 train_time:127477ms step_avg:98.82ms
step:1301/1770 train_time:127580ms step_avg:98.82ms
step:1302/1770 train_time:127682ms step_avg:98.83ms
step:1303/1770 train_time:127786ms step_avg:98.83ms
step:1304/1770 train_time:127888ms step_avg:98.83ms
step:1305/1770 train_time:127991ms step_avg:98.83ms
step:1306/1770 train_time:128093ms step_avg:98.84ms
step:1307/1770 train_time:128196ms step_avg:98.84ms
step:1308/1770 train_time:128299ms step_avg:98.84ms
step:1309/1770 train_time:128402ms step_avg:98.85ms
step:1310/1770 train_time:128504ms step_avg:98.85ms
step:1311/1770 train_time:128606ms step_avg:98.85ms
step:1312/1770 train_time:128708ms step_avg:98.85ms
step:1313/1770 train_time:128810ms step_avg:98.86ms
step:1314/1770 train_time:128912ms step_avg:98.86ms
step:1315/1770 train_time:129015ms step_avg:98.86ms
step:1316/1770 train_time:129117ms step_avg:98.86ms
step:1317/1770 train_time:129221ms step_avg:98.87ms
step:1318/1770 train_time:129327ms step_avg:98.87ms
step:1319/1770 train_time:129430ms step_avg:98.88ms
step:1320/1770 train_time:129533ms step_avg:98.88ms
step:1321/1770 train_time:129636ms step_avg:98.88ms
step:1322/1770 train_time:129739ms step_avg:98.89ms
step:1323/1770 train_time:129843ms step_avg:98.89ms
step:1324/1770 train_time:129946ms step_avg:98.89ms
step:1325/1770 train_time:130050ms step_avg:98.90ms
step:1326/1770 train_time:130152ms step_avg:98.90ms
step:1327/1770 train_time:130258ms step_avg:98.90ms
step:1328/1770 train_time:130361ms step_avg:98.91ms
step:1329/1770 train_time:130464ms step_avg:98.91ms
step:1330/1770 train_time:130567ms step_avg:98.91ms
step:1331/1770 train_time:130669ms step_avg:98.92ms
step:1332/1770 train_time:130772ms step_avg:98.92ms
step:1333/1770 train_time:130874ms step_avg:98.92ms
step:1334/1770 train_time:130977ms step_avg:98.93ms
step:1335/1770 train_time:131080ms step_avg:98.93ms
step:1336/1770 train_time:131182ms step_avg:98.93ms
step:1337/1770 train_time:131285ms step_avg:98.93ms
step:1338/1770 train_time:131387ms step_avg:98.94ms
step:1339/1770 train_time:131490ms step_avg:98.94ms
step:1340/1770 train_time:131594ms step_avg:98.94ms
step:1341/1770 train_time:131697ms step_avg:98.95ms
step:1342/1770 train_time:131800ms step_avg:98.95ms
step:1343/1770 train_time:131904ms step_avg:98.95ms
step:1344/1770 train_time:132008ms step_avg:98.96ms
step:1345/1770 train_time:132110ms step_avg:98.96ms
step:1346/1770 train_time:132212ms step_avg:98.96ms
step:1347/1770 train_time:132316ms step_avg:98.97ms
step:1348/1770 train_time:132422ms step_avg:98.97ms
step:1349/1770 train_time:132526ms step_avg:98.97ms
step:1350/1770 train_time:132630ms step_avg:98.98ms
step:1351/1770 train_time:132733ms step_avg:98.98ms
step:1352/1770 train_time:132836ms step_avg:98.98ms
step:1353/1770 train_time:132939ms step_avg:98.99ms
step:1354/1770 train_time:133042ms step_avg:98.99ms
step:1355/1770 train_time:133145ms step_avg:98.99ms
step:1356/1770 train_time:133247ms step_avg:98.99ms
step:1357/1770 train_time:133349ms step_avg:99.00ms
step:1358/1770 train_time:133453ms step_avg:99.00ms
step:1359/1770 train_time:133557ms step_avg:99.00ms
step:1360/1770 train_time:133660ms step_avg:99.01ms
step:1361/1770 train_time:133763ms step_avg:99.01ms
step:1362/1770 train_time:133867ms step_avg:99.01ms
step:1363/1770 train_time:133970ms step_avg:99.02ms
step:1364/1770 train_time:134073ms step_avg:99.02ms
step:1365/1770 train_time:134175ms step_avg:99.02ms
step:1366/1770 train_time:134278ms step_avg:99.03ms
step:1367/1770 train_time:134382ms step_avg:99.03ms
step:1368/1770 train_time:134485ms step_avg:99.03ms
step:1369/1770 train_time:134588ms step_avg:99.03ms
step:1370/1770 train_time:134691ms step_avg:99.04ms
step:1371/1770 train_time:134795ms step_avg:99.04ms
step:1372/1770 train_time:134897ms step_avg:99.04ms
step:1373/1770 train_time:135001ms step_avg:99.05ms
step:1374/1770 train_time:135104ms step_avg:99.05ms
step:1375/1770 train_time:135207ms step_avg:99.05ms
step:1375/1770 val_loss:3.3899 train_time:135309ms step_avg:99.13ms
step:1376/1770 train_time:135330ms step_avg:99.07ms
step:1377/1770 train_time:135416ms step_avg:99.06ms
step:1378/1770 train_time:135518ms step_avg:99.06ms
step:1379/1770 train_time:135621ms step_avg:99.07ms
step:1380/1770 train_time:135723ms step_avg:99.07ms
step:1381/1770 train_time:135826ms step_avg:99.07ms
step:1382/1770 train_time:135928ms step_avg:99.07ms
step:1383/1770 train_time:136032ms step_avg:99.08ms
step:1384/1770 train_time:136135ms step_avg:99.08ms
step:1385/1770 train_time:136238ms step_avg:99.08ms
step:1386/1770 train_time:136342ms step_avg:99.09ms
step:1387/1770 train_time:136446ms step_avg:99.09ms
step:1388/1770 train_time:136548ms step_avg:99.09ms
step:1389/1770 train_time:136651ms step_avg:99.09ms
step:1390/1770 train_time:136754ms step_avg:99.10ms
step:1391/1770 train_time:136857ms step_avg:99.10ms
step:1392/1770 train_time:136961ms step_avg:99.10ms
step:1393/1770 train_time:137063ms step_avg:99.11ms
step:1394/1770 train_time:137165ms step_avg:99.11ms
step:1395/1770 train_time:137268ms step_avg:99.11ms
step:1396/1770 train_time:137373ms step_avg:99.11ms
step:1397/1770 train_time:137476ms step_avg:99.12ms
step:1398/1770 train_time:137579ms step_avg:99.12ms
step:1399/1770 train_time:137682ms step_avg:99.12ms
step:1400/1770 train_time:137785ms step_avg:99.13ms
step:1401/1770 train_time:137888ms step_avg:99.13ms
step:1402/1770 train_time:137991ms step_avg:99.13ms
step:1403/1770 train_time:138093ms step_avg:99.13ms
step:1404/1770 train_time:138197ms step_avg:99.14ms
step:1405/1770 train_time:138299ms step_avg:99.14ms
step:1406/1770 train_time:138403ms step_avg:99.14ms
step:1407/1770 train_time:138505ms step_avg:99.14ms
step:1408/1770 train_time:138608ms step_avg:99.15ms
step:1409/1770 train_time:138711ms step_avg:99.15ms
step:1410/1770 train_time:138814ms step_avg:99.15ms
step:1411/1770 train_time:138919ms step_avg:99.16ms
step:1412/1770 train_time:139020ms step_avg:99.16ms
step:1413/1770 train_time:139122ms step_avg:99.16ms
step:1414/1770 train_time:139226ms step_avg:99.16ms
step:1415/1770 train_time:139330ms step_avg:99.17ms
step:1416/1770 train_time:139434ms step_avg:99.17ms
step:1417/1770 train_time:139536ms step_avg:99.17ms
step:1418/1770 train_time:139639ms step_avg:99.18ms
step:1419/1770 train_time:139742ms step_avg:99.18ms
step:1420/1770 train_time:139845ms step_avg:99.18ms
step:1421/1770 train_time:139947ms step_avg:99.18ms
step:1422/1770 train_time:140050ms step_avg:99.19ms
step:1423/1770 train_time:140152ms step_avg:99.19ms
step:1424/1770 train_time:140256ms step_avg:99.19ms
step:1425/1770 train_time:140359ms step_avg:99.19ms
step:1426/1770 train_time:140462ms step_avg:99.20ms
step:1427/1770 train_time:140564ms step_avg:99.20ms
step:1428/1770 train_time:140669ms step_avg:99.20ms
step:1429/1770 train_time:140771ms step_avg:99.20ms
step:1430/1770 train_time:140874ms step_avg:99.21ms
step:1431/1770 train_time:140978ms step_avg:99.21ms
step:1432/1770 train_time:141080ms step_avg:99.21ms
step:1433/1770 train_time:141182ms step_avg:99.21ms
step:1434/1770 train_time:141284ms step_avg:99.22ms
step:1435/1770 train_time:141386ms step_avg:99.22ms
step:1436/1770 train_time:141491ms step_avg:99.22ms
step:1437/1770 train_time:141593ms step_avg:99.22ms
step:1438/1770 train_time:141695ms step_avg:99.23ms
step:1439/1770 train_time:141798ms step_avg:99.23ms
step:1440/1770 train_time:141900ms step_avg:99.23ms
step:1441/1770 train_time:142005ms step_avg:99.24ms
step:1442/1770 train_time:142108ms step_avg:99.24ms
step:1443/1770 train_time:142211ms step_avg:99.24ms
step:1444/1770 train_time:142315ms step_avg:99.24ms
step:1445/1770 train_time:142418ms step_avg:99.25ms
step:1446/1770 train_time:142521ms step_avg:99.25ms
step:1447/1770 train_time:142626ms step_avg:99.25ms
step:1448/1770 train_time:142730ms step_avg:99.26ms
step:1449/1770 train_time:142835ms step_avg:99.26ms
step:1450/1770 train_time:142938ms step_avg:99.26ms
step:1451/1770 train_time:143043ms step_avg:99.27ms
step:1452/1770 train_time:143147ms step_avg:99.27ms
step:1453/1770 train_time:143251ms step_avg:99.27ms
step:1454/1770 train_time:143354ms step_avg:99.28ms
step:1455/1770 train_time:143459ms step_avg:99.28ms
step:1456/1770 train_time:143563ms step_avg:99.28ms
step:1457/1770 train_time:143667ms step_avg:99.29ms
step:1458/1770 train_time:143771ms step_avg:99.29ms
step:1459/1770 train_time:143875ms step_avg:99.29ms
step:1460/1770 train_time:143979ms step_avg:99.30ms
step:1461/1770 train_time:144083ms step_avg:99.30ms
step:1462/1770 train_time:144187ms step_avg:99.30ms
step:1463/1770 train_time:144291ms step_avg:99.31ms
step:1464/1770 train_time:144396ms step_avg:99.31ms
step:1465/1770 train_time:144499ms step_avg:99.31ms
step:1466/1770 train_time:144604ms step_avg:99.32ms
step:1467/1770 train_time:144709ms step_avg:99.32ms
step:1468/1770 train_time:144814ms step_avg:99.32ms
step:1469/1770 train_time:144917ms step_avg:99.33ms
step:1470/1770 train_time:145021ms step_avg:99.33ms
step:1471/1770 train_time:145125ms step_avg:99.33ms
step:1472/1770 train_time:145229ms step_avg:99.34ms
step:1473/1770 train_time:145333ms step_avg:99.34ms
step:1474/1770 train_time:145437ms step_avg:99.34ms
step:1475/1770 train_time:145540ms step_avg:99.34ms
step:1476/1770 train_time:145644ms step_avg:99.35ms
step:1477/1770 train_time:145750ms step_avg:99.35ms
step:1478/1770 train_time:145856ms step_avg:99.36ms
step:1479/1770 train_time:145959ms step_avg:99.36ms
step:1480/1770 train_time:146063ms step_avg:99.36ms
step:1481/1770 train_time:146170ms step_avg:99.37ms
step:1482/1770 train_time:146273ms step_avg:99.37ms
step:1483/1770 train_time:146378ms step_avg:99.37ms
step:1484/1770 train_time:146481ms step_avg:99.38ms
step:1485/1770 train_time:146585ms step_avg:99.38ms
step:1486/1770 train_time:146688ms step_avg:99.38ms
step:1487/1770 train_time:146792ms step_avg:99.39ms
step:1488/1770 train_time:146896ms step_avg:99.39ms
step:1489/1770 train_time:147000ms step_avg:99.39ms
step:1490/1770 train_time:147105ms step_avg:99.40ms
step:1491/1770 train_time:147209ms step_avg:99.40ms
step:1492/1770 train_time:147313ms step_avg:99.40ms
step:1493/1770 train_time:147420ms step_avg:99.41ms
step:1494/1770 train_time:147527ms step_avg:99.41ms
step:1495/1770 train_time:147630ms step_avg:99.41ms
step:1496/1770 train_time:147734ms step_avg:99.42ms
step:1497/1770 train_time:147838ms step_avg:99.42ms
step:1498/1770 train_time:147941ms step_avg:99.42ms
step:1499/1770 train_time:148044ms step_avg:99.43ms
step:1500/1770 train_time:148147ms step_avg:99.43ms
step:1500/1770 val_loss:3.3552 train_time:148249ms step_avg:99.50ms
step:1501/1770 train_time:148270ms step_avg:99.44ms
step:1502/1770 train_time:148359ms step_avg:99.44ms
step:1503/1770 train_time:148462ms step_avg:99.44ms
step:1504/1770 train_time:148567ms step_avg:99.44ms
step:1505/1770 train_time:148673ms step_avg:99.45ms
step:1506/1770 train_time:148777ms step_avg:99.45ms
step:1507/1770 train_time:148883ms step_avg:99.45ms
step:1508/1770 train_time:148989ms step_avg:99.46ms
step:1509/1770 train_time:149093ms step_avg:99.46ms
step:1510/1770 train_time:149196ms step_avg:99.46ms
step:1511/1770 train_time:149303ms step_avg:99.47ms
step:1512/1770 train_time:149407ms step_avg:99.47ms
step:1513/1770 train_time:149511ms step_avg:99.48ms
step:1514/1770 train_time:149615ms step_avg:99.48ms
step:1515/1770 train_time:149720ms step_avg:99.48ms
step:1516/1770 train_time:149824ms step_avg:99.48ms
step:1517/1770 train_time:149927ms step_avg:99.49ms
step:1518/1770 train_time:150034ms step_avg:99.49ms
step:1519/1770 train_time:150137ms step_avg:99.49ms
step:1520/1770 train_time:150241ms step_avg:99.50ms
step:1521/1770 train_time:150345ms step_avg:99.50ms
step:1522/1770 train_time:150449ms step_avg:99.50ms
step:1523/1770 train_time:150554ms step_avg:99.51ms
step:1524/1770 train_time:150657ms step_avg:99.51ms
step:1525/1770 train_time:150761ms step_avg:99.51ms
step:1526/1770 train_time:150865ms step_avg:99.52ms
step:1527/1770 train_time:150968ms step_avg:99.52ms
step:1528/1770 train_time:151075ms step_avg:99.52ms
step:1529/1770 train_time:151178ms step_avg:99.52ms
step:1530/1770 train_time:151282ms step_avg:99.53ms
step:1531/1770 train_time:151385ms step_avg:99.53ms
step:1532/1770 train_time:151489ms step_avg:99.53ms
step:1533/1770 train_time:151594ms step_avg:99.54ms
step:1534/1770 train_time:151698ms step_avg:99.54ms
step:1535/1770 train_time:151802ms step_avg:99.54ms
step:1536/1770 train_time:151905ms step_avg:99.54ms
step:1537/1770 train_time:152009ms step_avg:99.55ms
step:1538/1770 train_time:152115ms step_avg:99.55ms
step:1539/1770 train_time:152219ms step_avg:99.55ms
step:1540/1770 train_time:152325ms step_avg:99.56ms
step:1541/1770 train_time:152430ms step_avg:99.56ms
step:1542/1770 train_time:152534ms step_avg:99.57ms
step:1543/1770 train_time:152637ms step_avg:99.57ms
step:1544/1770 train_time:152743ms step_avg:99.57ms
step:1545/1770 train_time:152847ms step_avg:99.57ms
step:1546/1770 train_time:152951ms step_avg:99.58ms
step:1547/1770 train_time:153054ms step_avg:99.58ms
step:1548/1770 train_time:153158ms step_avg:99.58ms
step:1549/1770 train_time:153263ms step_avg:99.59ms
step:1550/1770 train_time:153367ms step_avg:99.59ms
step:1551/1770 train_time:153470ms step_avg:99.59ms
step:1552/1770 train_time:153576ms step_avg:99.60ms
step:1553/1770 train_time:153680ms step_avg:99.60ms
step:1554/1770 train_time:153783ms step_avg:99.60ms
step:1555/1770 train_time:153888ms step_avg:99.60ms
step:1556/1770 train_time:153991ms step_avg:99.61ms
step:1557/1770 train_time:154095ms step_avg:99.61ms
step:1558/1770 train_time:154200ms step_avg:99.61ms
step:1559/1770 train_time:154304ms step_avg:99.62ms
step:1560/1770 train_time:154408ms step_avg:99.62ms
step:1561/1770 train_time:154514ms step_avg:99.62ms
step:1562/1770 train_time:154617ms step_avg:99.62ms
step:1563/1770 train_time:154722ms step_avg:99.63ms
step:1564/1770 train_time:154825ms step_avg:99.63ms
step:1565/1770 train_time:154930ms step_avg:99.63ms
step:1566/1770 train_time:155033ms step_avg:99.64ms
step:1567/1770 train_time:155138ms step_avg:99.64ms
step:1568/1770 train_time:155242ms step_avg:99.64ms
step:1569/1770 train_time:155349ms step_avg:99.65ms
step:1570/1770 train_time:155452ms step_avg:99.65ms
step:1571/1770 train_time:155556ms step_avg:99.65ms
step:1572/1770 train_time:155662ms step_avg:99.66ms
step:1573/1770 train_time:155768ms step_avg:99.66ms
step:1574/1770 train_time:155872ms step_avg:99.66ms
step:1575/1770 train_time:155976ms step_avg:99.66ms
step:1576/1770 train_time:156079ms step_avg:99.67ms
step:1577/1770 train_time:156185ms step_avg:99.67ms
step:1578/1770 train_time:156290ms step_avg:99.67ms
step:1579/1770 train_time:156394ms step_avg:99.68ms
step:1580/1770 train_time:156498ms step_avg:99.68ms
step:1581/1770 train_time:156604ms step_avg:99.68ms
step:1582/1770 train_time:156709ms step_avg:99.69ms
step:1583/1770 train_time:156813ms step_avg:99.69ms
step:1584/1770 train_time:156918ms step_avg:99.69ms
step:1585/1770 train_time:157023ms step_avg:99.70ms
step:1586/1770 train_time:157130ms step_avg:99.70ms
step:1587/1770 train_time:157235ms step_avg:99.70ms
step:1588/1770 train_time:157338ms step_avg:99.71ms
step:1589/1770 train_time:157445ms step_avg:99.71ms
step:1590/1770 train_time:157549ms step_avg:99.71ms
step:1591/1770 train_time:157653ms step_avg:99.72ms
step:1592/1770 train_time:157758ms step_avg:99.72ms
step:1593/1770 train_time:157862ms step_avg:99.72ms
step:1594/1770 train_time:157965ms step_avg:99.73ms
step:1595/1770 train_time:158069ms step_avg:99.73ms
step:1596/1770 train_time:158175ms step_avg:99.73ms
step:1597/1770 train_time:158278ms step_avg:99.73ms
step:1598/1770 train_time:158382ms step_avg:99.74ms
step:1599/1770 train_time:158487ms step_avg:99.74ms
step:1600/1770 train_time:158593ms step_avg:99.74ms
step:1601/1770 train_time:158698ms step_avg:99.75ms
step:1602/1770 train_time:158803ms step_avg:99.75ms
step:1603/1770 train_time:158907ms step_avg:99.75ms
step:1604/1770 train_time:159010ms step_avg:99.76ms
step:1605/1770 train_time:159114ms step_avg:99.76ms
step:1606/1770 train_time:159218ms step_avg:99.76ms
step:1607/1770 train_time:159325ms step_avg:99.77ms
step:1608/1770 train_time:159429ms step_avg:99.77ms
step:1609/1770 train_time:159533ms step_avg:99.77ms
step:1610/1770 train_time:159639ms step_avg:99.77ms
step:1611/1770 train_time:159745ms step_avg:99.78ms
step:1612/1770 train_time:159851ms step_avg:99.78ms
step:1613/1770 train_time:159955ms step_avg:99.78ms
step:1614/1770 train_time:160059ms step_avg:99.79ms
step:1615/1770 train_time:160164ms step_avg:99.79ms
step:1616/1770 train_time:160268ms step_avg:99.79ms
step:1617/1770 train_time:160374ms step_avg:99.80ms
step:1618/1770 train_time:160479ms step_avg:99.80ms
step:1619/1770 train_time:160584ms step_avg:99.80ms
step:1620/1770 train_time:160688ms step_avg:99.81ms
step:1621/1770 train_time:160792ms step_avg:99.81ms
step:1622/1770 train_time:160897ms step_avg:99.81ms
step:1623/1770 train_time:161004ms step_avg:99.82ms
step:1624/1770 train_time:161107ms step_avg:99.82ms
step:1625/1770 train_time:161211ms step_avg:99.82ms
step:1625/1770 val_loss:3.3213 train_time:161313ms step_avg:99.88ms
step:1626/1770 train_time:161334ms step_avg:99.84ms
step:1627/1770 train_time:161424ms step_avg:99.83ms
step:1628/1770 train_time:161527ms step_avg:99.83ms
step:1629/1770 train_time:161631ms step_avg:99.83ms
step:1630/1770 train_time:161735ms step_avg:99.84ms
step:1631/1770 train_time:161838ms step_avg:99.84ms
step:1632/1770 train_time:161943ms step_avg:99.84ms
step:1633/1770 train_time:162047ms step_avg:99.84ms
step:1634/1770 train_time:162151ms step_avg:99.85ms
step:1635/1770 train_time:162255ms step_avg:99.85ms
step:1636/1770 train_time:162360ms step_avg:99.85ms
step:1637/1770 train_time:162466ms step_avg:99.86ms
step:1638/1770 train_time:162569ms step_avg:99.86ms
step:1639/1770 train_time:162674ms step_avg:99.86ms
step:1640/1770 train_time:162778ms step_avg:99.86ms
step:1641/1770 train_time:162883ms step_avg:99.87ms
step:1642/1770 train_time:162986ms step_avg:99.87ms
step:1643/1770 train_time:163091ms step_avg:99.87ms
step:1644/1770 train_time:163197ms step_avg:99.88ms
step:1645/1770 train_time:163300ms step_avg:99.88ms
step:1646/1770 train_time:163407ms step_avg:99.88ms
step:1647/1770 train_time:163511ms step_avg:99.88ms
step:1648/1770 train_time:163615ms step_avg:99.89ms
step:1649/1770 train_time:163719ms step_avg:99.89ms
step:1650/1770 train_time:163822ms step_avg:99.89ms
step:1651/1770 train_time:163926ms step_avg:99.89ms
step:1652/1770 train_time:164030ms step_avg:99.90ms
step:1653/1770 train_time:164134ms step_avg:99.90ms
step:1654/1770 train_time:164242ms step_avg:99.90ms
step:1655/1770 train_time:164349ms step_avg:99.91ms
step:1656/1770 train_time:164453ms step_avg:99.91ms
step:1657/1770 train_time:164558ms step_avg:99.91ms
step:1658/1770 train_time:164662ms step_avg:99.92ms
step:1659/1770 train_time:164768ms step_avg:99.92ms
step:1660/1770 train_time:164873ms step_avg:99.92ms
step:1661/1770 train_time:164978ms step_avg:99.93ms
step:1662/1770 train_time:165083ms step_avg:99.93ms
step:1663/1770 train_time:165186ms step_avg:99.93ms
step:1664/1770 train_time:165290ms step_avg:99.93ms
step:1665/1770 train_time:165394ms step_avg:99.94ms
step:1666/1770 train_time:165499ms step_avg:99.94ms
step:1667/1770 train_time:165603ms step_avg:99.94ms
step:1668/1770 train_time:165707ms step_avg:99.94ms
step:1669/1770 train_time:165810ms step_avg:99.95ms
step:1670/1770 train_time:165914ms step_avg:99.95ms
step:1671/1770 train_time:166019ms step_avg:99.95ms
step:1672/1770 train_time:166123ms step_avg:99.95ms
step:1673/1770 train_time:166229ms step_avg:99.96ms
step:1674/1770 train_time:166333ms step_avg:99.96ms
step:1675/1770 train_time:166435ms step_avg:99.96ms
step:1676/1770 train_time:166541ms step_avg:99.96ms
step:1677/1770 train_time:166650ms step_avg:99.97ms
step:1678/1770 train_time:166753ms step_avg:99.97ms
step:1679/1770 train_time:166857ms step_avg:99.97ms
step:1680/1770 train_time:166961ms step_avg:99.98ms
step:1681/1770 train_time:167067ms step_avg:99.98ms
step:1682/1770 train_time:167173ms step_avg:99.98ms
step:1683/1770 train_time:167276ms step_avg:99.99ms
step:1684/1770 train_time:167379ms step_avg:99.99ms
step:1685/1770 train_time:167484ms step_avg:99.99ms
step:1686/1770 train_time:167589ms step_avg:99.99ms
step:1687/1770 train_time:167694ms step_avg:100.00ms
step:1688/1770 train_time:167798ms step_avg:100.00ms
step:1689/1770 train_time:167902ms step_avg:100.00ms
step:1690/1770 train_time:168007ms step_avg:100.00ms
step:1691/1770 train_time:168111ms step_avg:100.01ms
step:1692/1770 train_time:168215ms step_avg:100.01ms
step:1693/1770 train_time:168320ms step_avg:100.01ms
step:1694/1770 train_time:168424ms step_avg:100.01ms
step:1695/1770 train_time:168528ms step_avg:100.02ms
step:1696/1770 train_time:168634ms step_avg:100.02ms
step:1697/1770 train_time:168740ms step_avg:100.02ms
step:1698/1770 train_time:168845ms step_avg:100.03ms
step:1699/1770 train_time:168949ms step_avg:100.03ms
step:1700/1770 train_time:169054ms step_avg:100.03ms
step:1701/1770 train_time:169157ms step_avg:100.03ms
step:1702/1770 train_time:169261ms step_avg:100.04ms
step:1703/1770 train_time:169365ms step_avg:100.04ms
step:1704/1770 train_time:169469ms step_avg:100.04ms
step:1705/1770 train_time:169573ms step_avg:100.04ms
step:1706/1770 train_time:169676ms step_avg:100.04ms
step:1707/1770 train_time:169781ms step_avg:100.05ms
step:1708/1770 train_time:169886ms step_avg:100.05ms
step:1709/1770 train_time:169991ms step_avg:100.05ms
step:1710/1770 train_time:170099ms step_avg:100.06ms
step:1711/1770 train_time:170205ms step_avg:100.06ms
step:1712/1770 train_time:170311ms step_avg:100.07ms
step:1713/1770 train_time:170415ms step_avg:100.07ms
step:1714/1770 train_time:170519ms step_avg:100.07ms
step:1715/1770 train_time:170623ms step_avg:100.07ms
step:1716/1770 train_time:170728ms step_avg:100.08ms
step:1717/1770 train_time:170832ms step_avg:100.08ms
step:1718/1770 train_time:170938ms step_avg:100.08ms
step:1719/1770 train_time:171044ms step_avg:100.08ms
step:1720/1770 train_time:171149ms step_avg:100.09ms
step:1721/1770 train_time:171254ms step_avg:100.09ms
step:1722/1770 train_time:171361ms step_avg:100.09ms
step:1723/1770 train_time:171467ms step_avg:100.10ms
step:1724/1770 train_time:171573ms step_avg:100.10ms
step:1725/1770 train_time:171680ms step_avg:100.11ms
step:1726/1770 train_time:171787ms step_avg:100.11ms
step:1727/1770 train_time:171891ms step_avg:100.11ms
step:1728/1770 train_time:171998ms step_avg:100.12ms
step:1729/1770 train_time:172102ms step_avg:100.12ms
step:1730/1770 train_time:172209ms step_avg:100.12ms
step:1731/1770 train_time:172316ms step_avg:100.13ms
step:1732/1770 train_time:172421ms step_avg:100.13ms
step:1733/1770 train_time:172528ms step_avg:100.13ms
step:1734/1770 train_time:172633ms step_avg:100.14ms
step:1735/1770 train_time:172739ms step_avg:100.14ms
step:1736/1770 train_time:172843ms step_avg:100.14ms
step:1737/1770 train_time:172949ms step_avg:100.14ms
step:1738/1770 train_time:173054ms step_avg:100.15ms
step:1739/1770 train_time:173160ms step_avg:100.15ms
step:1740/1770 train_time:173265ms step_avg:100.15ms
step:1741/1770 train_time:173371ms step_avg:100.16ms
step:1742/1770 train_time:173479ms step_avg:100.16ms
step:1743/1770 train_time:173585ms step_avg:100.16ms
step:1744/1770 train_time:173690ms step_avg:100.17ms
step:1745/1770 train_time:173794ms step_avg:100.17ms
step:1746/1770 train_time:173902ms step_avg:100.17ms
step:1747/1770 train_time:174006ms step_avg:100.18ms
step:1748/1770 train_time:174112ms step_avg:100.18ms
step:1749/1770 train_time:174218ms step_avg:100.18ms
step:1750/1770 train_time:174322ms step_avg:100.19ms
step:1750/1770 val_loss:3.2971 train_time:174426ms step_avg:100.24ms
step:1751/1770 train_time:174447ms step_avg:100.20ms
step:1752/1770 train_time:174538ms step_avg:100.19ms
step:1753/1770 train_time:174643ms step_avg:100.20ms
step:1754/1770 train_time:174748ms step_avg:100.20ms
step:1755/1770 train_time:174853ms step_avg:100.20ms
step:1756/1770 train_time:174959ms step_avg:100.21ms
step:1757/1770 train_time:175064ms step_avg:100.21ms
step:1758/1770 train_time:175169ms step_avg:100.21ms
step:1759/1770 train_time:175274ms step_avg:100.21ms
step:1760/1770 train_time:175379ms step_avg:100.22ms
step:1761/1770 train_time:175487ms step_avg:100.22ms
step:1762/1770 train_time:175595ms step_avg:100.23ms
step:1763/1770 train_time:175698ms step_avg:100.23ms
step:1764/1770 train_time:175803ms step_avg:100.23ms
step:1765/1770 train_time:175908ms step_avg:100.23ms
step:1766/1770 train_time:176017ms step_avg:100.24ms
step:1767/1770 train_time:176121ms step_avg:100.24ms
step:1768/1770 train_time:176226ms step_avg:100.24ms
step:1769/1770 train_time:176331ms step_avg:100.24ms
step:1770/1770 train_time:176434ms step_avg:100.25ms
step:1770/1770 val_loss:3.2933 train_time:176539ms step_avg:100.31ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
