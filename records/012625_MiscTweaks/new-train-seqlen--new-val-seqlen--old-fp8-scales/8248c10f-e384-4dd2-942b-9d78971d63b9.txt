import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 03:41:39 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24195ms step_avg:nanms
step:2/1770 train_time:24656ms step_avg:nanms
step:3/1770 train_time:24751ms step_avg:nanms
step:4/1770 train_time:24844ms step_avg:nanms
step:5/1770 train_time:24939ms step_avg:nanms
step:6/1770 train_time:25033ms step_avg:nanms
step:7/1770 train_time:25127ms step_avg:nanms
step:8/1770 train_time:25222ms step_avg:nanms
step:9/1770 train_time:25316ms step_avg:nanms
step:10/1770 train_time:25411ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:190ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.48ms
step:14/1770 train_time:378ms step_avg:94.53ms
step:15/1770 train_time:472ms step_avg:94.45ms
step:16/1770 train_time:567ms step_avg:94.56ms
step:17/1770 train_time:662ms step_avg:94.61ms
step:18/1770 train_time:757ms step_avg:94.60ms
step:19/1770 train_time:851ms step_avg:94.56ms
step:20/1770 train_time:946ms step_avg:94.58ms
step:21/1770 train_time:1040ms step_avg:94.57ms
step:22/1770 train_time:1135ms step_avg:94.58ms
step:23/1770 train_time:1229ms step_avg:94.52ms
step:24/1770 train_time:1324ms step_avg:94.54ms
step:25/1770 train_time:1418ms step_avg:94.52ms
step:26/1770 train_time:1512ms step_avg:94.48ms
step:27/1770 train_time:1606ms step_avg:94.48ms
step:28/1770 train_time:1702ms step_avg:94.56ms
step:29/1770 train_time:1797ms step_avg:94.58ms
step:30/1770 train_time:1891ms step_avg:94.57ms
step:31/1770 train_time:1986ms step_avg:94.57ms
step:32/1770 train_time:2081ms step_avg:94.60ms
step:33/1770 train_time:2178ms step_avg:94.71ms
step:34/1770 train_time:2270ms step_avg:94.57ms
step:35/1770 train_time:2365ms step_avg:94.58ms
step:36/1770 train_time:2459ms step_avg:94.58ms
step:37/1770 train_time:2554ms step_avg:94.58ms
step:38/1770 train_time:2648ms step_avg:94.58ms
step:39/1770 train_time:2744ms step_avg:94.62ms
step:40/1770 train_time:2838ms step_avg:94.61ms
step:41/1770 train_time:2932ms step_avg:94.60ms
step:42/1770 train_time:3027ms step_avg:94.61ms
step:43/1770 train_time:3123ms step_avg:94.65ms
step:44/1770 train_time:3218ms step_avg:94.65ms
step:45/1770 train_time:3312ms step_avg:94.63ms
step:46/1770 train_time:3407ms step_avg:94.64ms
step:47/1770 train_time:3502ms step_avg:94.64ms
step:48/1770 train_time:3597ms step_avg:94.65ms
step:49/1770 train_time:3690ms step_avg:94.63ms
step:50/1770 train_time:3785ms step_avg:94.63ms
step:51/1770 train_time:3880ms step_avg:94.64ms
step:52/1770 train_time:3975ms step_avg:94.64ms
step:53/1770 train_time:4074ms step_avg:94.75ms
step:54/1770 train_time:4164ms step_avg:94.63ms
step:55/1770 train_time:4258ms step_avg:94.62ms
step:56/1770 train_time:4352ms step_avg:94.61ms
step:57/1770 train_time:4447ms step_avg:94.61ms
step:58/1770 train_time:4542ms step_avg:94.63ms
step:59/1770 train_time:4636ms step_avg:94.62ms
step:60/1770 train_time:4730ms step_avg:94.61ms
step:61/1770 train_time:4825ms step_avg:94.62ms
step:62/1770 train_time:4920ms step_avg:94.62ms
step:63/1770 train_time:5016ms step_avg:94.64ms
step:64/1770 train_time:5111ms step_avg:94.64ms
step:65/1770 train_time:5205ms step_avg:94.64ms
step:66/1770 train_time:5300ms step_avg:94.64ms
step:67/1770 train_time:5394ms step_avg:94.64ms
step:68/1770 train_time:5489ms step_avg:94.64ms
step:69/1770 train_time:5583ms step_avg:94.63ms
step:70/1770 train_time:5678ms step_avg:94.64ms
step:71/1770 train_time:5773ms step_avg:94.64ms
step:72/1770 train_time:5868ms step_avg:94.64ms
step:73/1770 train_time:5962ms step_avg:94.64ms
step:74/1770 train_time:6057ms step_avg:94.65ms
step:75/1770 train_time:6151ms step_avg:94.64ms
step:76/1770 train_time:6246ms step_avg:94.64ms
step:77/1770 train_time:6341ms step_avg:94.65ms
step:78/1770 train_time:6436ms step_avg:94.64ms
step:79/1770 train_time:6530ms step_avg:94.64ms
step:80/1770 train_time:6626ms step_avg:94.65ms
step:81/1770 train_time:6721ms step_avg:94.66ms
step:82/1770 train_time:6816ms step_avg:94.66ms
step:83/1770 train_time:6910ms step_avg:94.66ms
step:84/1770 train_time:7006ms step_avg:94.67ms
step:85/1770 train_time:7101ms step_avg:94.68ms
step:86/1770 train_time:7195ms step_avg:94.68ms
step:87/1770 train_time:7289ms step_avg:94.67ms
step:88/1770 train_time:7384ms step_avg:94.67ms
step:89/1770 train_time:7478ms step_avg:94.66ms
step:90/1770 train_time:7573ms step_avg:94.66ms
step:91/1770 train_time:7668ms step_avg:94.66ms
step:92/1770 train_time:7763ms step_avg:94.67ms
step:93/1770 train_time:7857ms step_avg:94.67ms
step:94/1770 train_time:7952ms step_avg:94.66ms
step:95/1770 train_time:8046ms step_avg:94.66ms
step:96/1770 train_time:8141ms step_avg:94.67ms
step:97/1770 train_time:8236ms step_avg:94.66ms
step:98/1770 train_time:8330ms step_avg:94.66ms
step:99/1770 train_time:8425ms step_avg:94.66ms
step:100/1770 train_time:8520ms step_avg:94.67ms
step:101/1770 train_time:8615ms step_avg:94.67ms
step:102/1770 train_time:8709ms step_avg:94.67ms
step:103/1770 train_time:8804ms step_avg:94.67ms
step:104/1770 train_time:8899ms step_avg:94.67ms
step:105/1770 train_time:8994ms step_avg:94.67ms
step:106/1770 train_time:9088ms step_avg:94.67ms
step:107/1770 train_time:9182ms step_avg:94.66ms
step:108/1770 train_time:9277ms step_avg:94.66ms
step:109/1770 train_time:9371ms step_avg:94.66ms
step:110/1770 train_time:9466ms step_avg:94.66ms
step:111/1770 train_time:9561ms step_avg:94.66ms
step:112/1770 train_time:9656ms step_avg:94.67ms
step:113/1770 train_time:9751ms step_avg:94.67ms
step:114/1770 train_time:9846ms step_avg:94.67ms
step:115/1770 train_time:9940ms step_avg:94.67ms
step:116/1770 train_time:10035ms step_avg:94.67ms
step:117/1770 train_time:10129ms step_avg:94.66ms
step:118/1770 train_time:10224ms step_avg:94.67ms
step:119/1770 train_time:10318ms step_avg:94.66ms
step:120/1770 train_time:10413ms step_avg:94.66ms
step:121/1770 train_time:10508ms step_avg:94.66ms
step:122/1770 train_time:10602ms step_avg:94.66ms
step:123/1770 train_time:10697ms step_avg:94.66ms
step:124/1770 train_time:10791ms step_avg:94.66ms
step:125/1770 train_time:10887ms step_avg:94.67ms
step:125/1770 val_loss:4.6537 train_time:10979ms step_avg:95.47ms
step:126/1770 train_time:11007ms step_avg:94.89ms
step:127/1770 train_time:11078ms step_avg:94.68ms
step:128/1770 train_time:11176ms step_avg:94.71ms
step:129/1770 train_time:11276ms step_avg:94.76ms
step:130/1770 train_time:11372ms step_avg:94.77ms
step:131/1770 train_time:11467ms step_avg:94.77ms
step:132/1770 train_time:11562ms step_avg:94.77ms
step:133/1770 train_time:11657ms step_avg:94.77ms
step:134/1770 train_time:11752ms step_avg:94.78ms
step:135/1770 train_time:11847ms step_avg:94.78ms
step:136/1770 train_time:11942ms step_avg:94.78ms
step:137/1770 train_time:12038ms step_avg:94.78ms
step:138/1770 train_time:12133ms step_avg:94.79ms
step:139/1770 train_time:12228ms step_avg:94.79ms
step:140/1770 train_time:12323ms step_avg:94.79ms
step:141/1770 train_time:12419ms step_avg:94.80ms
step:142/1770 train_time:12514ms step_avg:94.80ms
step:143/1770 train_time:12609ms step_avg:94.80ms
step:144/1770 train_time:12703ms step_avg:94.80ms
step:145/1770 train_time:12799ms step_avg:94.81ms
step:146/1770 train_time:12894ms step_avg:94.81ms
step:147/1770 train_time:12990ms step_avg:94.82ms
step:148/1770 train_time:13085ms step_avg:94.82ms
step:149/1770 train_time:13180ms step_avg:94.82ms
step:150/1770 train_time:13276ms step_avg:94.83ms
step:151/1770 train_time:13371ms step_avg:94.83ms
step:152/1770 train_time:13466ms step_avg:94.83ms
step:153/1770 train_time:13561ms step_avg:94.83ms
step:154/1770 train_time:13656ms step_avg:94.83ms
step:155/1770 train_time:13751ms step_avg:94.84ms
step:156/1770 train_time:13846ms step_avg:94.83ms
step:157/1770 train_time:13941ms step_avg:94.84ms
step:158/1770 train_time:14036ms step_avg:94.84ms
step:159/1770 train_time:14131ms step_avg:94.84ms
step:160/1770 train_time:14227ms step_avg:94.84ms
step:161/1770 train_time:14321ms step_avg:94.84ms
step:162/1770 train_time:14417ms step_avg:94.85ms
step:163/1770 train_time:14512ms step_avg:94.85ms
step:164/1770 train_time:14607ms step_avg:94.85ms
step:165/1770 train_time:14707ms step_avg:94.88ms
step:166/1770 train_time:14797ms step_avg:94.85ms
step:167/1770 train_time:14893ms step_avg:94.86ms
step:168/1770 train_time:14988ms step_avg:94.86ms
step:169/1770 train_time:15083ms step_avg:94.86ms
step:170/1770 train_time:15179ms step_avg:94.87ms
step:171/1770 train_time:15274ms step_avg:94.87ms
step:172/1770 train_time:15370ms step_avg:94.87ms
step:173/1770 train_time:15465ms step_avg:94.88ms
step:174/1770 train_time:15559ms step_avg:94.87ms
step:175/1770 train_time:15655ms step_avg:94.88ms
step:176/1770 train_time:15750ms step_avg:94.88ms
step:177/1770 train_time:15845ms step_avg:94.88ms
step:178/1770 train_time:15940ms step_avg:94.88ms
step:179/1770 train_time:16036ms step_avg:94.89ms
step:180/1770 train_time:16131ms step_avg:94.89ms
step:181/1770 train_time:16226ms step_avg:94.89ms
step:182/1770 train_time:16322ms step_avg:94.90ms
step:183/1770 train_time:16417ms step_avg:94.90ms
step:184/1770 train_time:16513ms step_avg:94.90ms
step:185/1770 train_time:16608ms step_avg:94.90ms
step:186/1770 train_time:16702ms step_avg:94.90ms
step:187/1770 train_time:16798ms step_avg:94.90ms
step:188/1770 train_time:16893ms step_avg:94.91ms
step:189/1770 train_time:16989ms step_avg:94.91ms
step:190/1770 train_time:17084ms step_avg:94.91ms
step:191/1770 train_time:17179ms step_avg:94.91ms
step:192/1770 train_time:17275ms step_avg:94.92ms
step:193/1770 train_time:17370ms step_avg:94.92ms
step:194/1770 train_time:17465ms step_avg:94.92ms
step:195/1770 train_time:17560ms step_avg:94.92ms
step:196/1770 train_time:17656ms step_avg:94.92ms
step:197/1770 train_time:17751ms step_avg:94.92ms
step:198/1770 train_time:17846ms step_avg:94.92ms
step:199/1770 train_time:17941ms step_avg:94.93ms
step:200/1770 train_time:18036ms step_avg:94.93ms
step:201/1770 train_time:18132ms step_avg:94.93ms
step:202/1770 train_time:18226ms step_avg:94.93ms
step:203/1770 train_time:18321ms step_avg:94.93ms
step:204/1770 train_time:18417ms step_avg:94.93ms
step:205/1770 train_time:18512ms step_avg:94.94ms
step:206/1770 train_time:18608ms step_avg:94.94ms
step:207/1770 train_time:18703ms step_avg:94.94ms
step:208/1770 train_time:18798ms step_avg:94.94ms
step:209/1770 train_time:18893ms step_avg:94.94ms
step:210/1770 train_time:18988ms step_avg:94.94ms
step:211/1770 train_time:19083ms step_avg:94.94ms
step:212/1770 train_time:19179ms step_avg:94.94ms
step:213/1770 train_time:19274ms step_avg:94.95ms
step:214/1770 train_time:19370ms step_avg:94.95ms
step:215/1770 train_time:19465ms step_avg:94.95ms
step:216/1770 train_time:19560ms step_avg:94.95ms
step:217/1770 train_time:19656ms step_avg:94.95ms
step:218/1770 train_time:19751ms step_avg:94.96ms
step:219/1770 train_time:19846ms step_avg:94.96ms
step:220/1770 train_time:19941ms step_avg:94.95ms
step:221/1770 train_time:20036ms step_avg:94.96ms
step:222/1770 train_time:20131ms step_avg:94.96ms
step:223/1770 train_time:20227ms step_avg:94.96ms
step:224/1770 train_time:20321ms step_avg:94.96ms
step:225/1770 train_time:20417ms step_avg:94.96ms
step:226/1770 train_time:20512ms step_avg:94.96ms
step:227/1770 train_time:20608ms step_avg:94.97ms
step:228/1770 train_time:20703ms step_avg:94.97ms
step:229/1770 train_time:20798ms step_avg:94.97ms
step:230/1770 train_time:20894ms step_avg:94.97ms
step:231/1770 train_time:20989ms step_avg:94.97ms
step:232/1770 train_time:21084ms step_avg:94.97ms
step:233/1770 train_time:21179ms step_avg:94.97ms
step:234/1770 train_time:21275ms step_avg:94.98ms
step:235/1770 train_time:21370ms step_avg:94.98ms
step:236/1770 train_time:21465ms step_avg:94.98ms
step:237/1770 train_time:21561ms step_avg:94.98ms
step:238/1770 train_time:21656ms step_avg:94.98ms
step:239/1770 train_time:21751ms step_avg:94.98ms
step:240/1770 train_time:21846ms step_avg:94.98ms
step:241/1770 train_time:21941ms step_avg:94.98ms
step:242/1770 train_time:22037ms step_avg:94.99ms
step:243/1770 train_time:22133ms step_avg:94.99ms
step:244/1770 train_time:22228ms step_avg:94.99ms
step:245/1770 train_time:22323ms step_avg:94.99ms
step:246/1770 train_time:22418ms step_avg:94.99ms
step:247/1770 train_time:22514ms step_avg:95.00ms
step:248/1770 train_time:22609ms step_avg:95.00ms
step:249/1770 train_time:22704ms step_avg:94.99ms
step:250/1770 train_time:22799ms step_avg:95.00ms
step:250/1770 val_loss:4.1153 train_time:22894ms step_avg:95.39ms
step:251/1770 train_time:22915ms step_avg:95.08ms
step:252/1770 train_time:23000ms step_avg:95.04ms
step:253/1770 train_time:23098ms step_avg:95.05ms
step:254/1770 train_time:23193ms step_avg:95.05ms
step:255/1770 train_time:23289ms step_avg:95.06ms
step:256/1770 train_time:23383ms step_avg:95.05ms
step:257/1770 train_time:23478ms step_avg:95.05ms
step:258/1770 train_time:23573ms step_avg:95.05ms
step:259/1770 train_time:23668ms step_avg:95.05ms
step:260/1770 train_time:23763ms step_avg:95.05ms
step:261/1770 train_time:23858ms step_avg:95.05ms
step:262/1770 train_time:23953ms step_avg:95.05ms
step:263/1770 train_time:24048ms step_avg:95.05ms
step:264/1770 train_time:24143ms step_avg:95.05ms
step:265/1770 train_time:24240ms step_avg:95.06ms
step:266/1770 train_time:24335ms step_avg:95.06ms
step:267/1770 train_time:24430ms step_avg:95.06ms
step:268/1770 train_time:24525ms step_avg:95.06ms
step:269/1770 train_time:24620ms step_avg:95.06ms
step:270/1770 train_time:24716ms step_avg:95.06ms
step:271/1770 train_time:24811ms step_avg:95.06ms
step:272/1770 train_time:24906ms step_avg:95.06ms
step:273/1770 train_time:25001ms step_avg:95.06ms
step:274/1770 train_time:25097ms step_avg:95.06ms
step:275/1770 train_time:25193ms step_avg:95.07ms
step:276/1770 train_time:25288ms step_avg:95.07ms
step:277/1770 train_time:25383ms step_avg:95.07ms
step:278/1770 train_time:25478ms step_avg:95.07ms
step:279/1770 train_time:25574ms step_avg:95.07ms
step:280/1770 train_time:25670ms step_avg:95.07ms
step:281/1770 train_time:25765ms step_avg:95.07ms
step:282/1770 train_time:25860ms step_avg:95.08ms
step:283/1770 train_time:25956ms step_avg:95.08ms
step:284/1770 train_time:26052ms step_avg:95.08ms
step:285/1770 train_time:26147ms step_avg:95.08ms
step:286/1770 train_time:26243ms step_avg:95.08ms
step:287/1770 train_time:26338ms step_avg:95.08ms
step:288/1770 train_time:26434ms step_avg:95.09ms
step:289/1770 train_time:26529ms step_avg:95.09ms
step:290/1770 train_time:26624ms step_avg:95.09ms
step:291/1770 train_time:26720ms step_avg:95.09ms
step:292/1770 train_time:26815ms step_avg:95.09ms
step:293/1770 train_time:26911ms step_avg:95.09ms
step:294/1770 train_time:27007ms step_avg:95.09ms
step:295/1770 train_time:27102ms step_avg:95.09ms
step:296/1770 train_time:27198ms step_avg:95.10ms
step:297/1770 train_time:27293ms step_avg:95.10ms
step:298/1770 train_time:27389ms step_avg:95.10ms
step:299/1770 train_time:27484ms step_avg:95.10ms
step:300/1770 train_time:27580ms step_avg:95.10ms
step:301/1770 train_time:27675ms step_avg:95.10ms
step:302/1770 train_time:27771ms step_avg:95.11ms
step:303/1770 train_time:27867ms step_avg:95.11ms
step:304/1770 train_time:27962ms step_avg:95.11ms
step:305/1770 train_time:28058ms step_avg:95.11ms
step:306/1770 train_time:28154ms step_avg:95.11ms
step:307/1770 train_time:28249ms step_avg:95.12ms
step:308/1770 train_time:28344ms step_avg:95.12ms
step:309/1770 train_time:28440ms step_avg:95.12ms
step:310/1770 train_time:28536ms step_avg:95.12ms
step:311/1770 train_time:28632ms step_avg:95.12ms
step:312/1770 train_time:28728ms step_avg:95.12ms
step:313/1770 train_time:28823ms step_avg:95.12ms
step:314/1770 train_time:28918ms step_avg:95.13ms
step:315/1770 train_time:29014ms step_avg:95.13ms
step:316/1770 train_time:29110ms step_avg:95.13ms
step:317/1770 train_time:29205ms step_avg:95.13ms
step:318/1770 train_time:29301ms step_avg:95.13ms
step:319/1770 train_time:29396ms step_avg:95.13ms
step:320/1770 train_time:29493ms step_avg:95.14ms
step:321/1770 train_time:29588ms step_avg:95.14ms
step:322/1770 train_time:29683ms step_avg:95.14ms
step:323/1770 train_time:29779ms step_avg:95.14ms
step:324/1770 train_time:29875ms step_avg:95.14ms
step:325/1770 train_time:29970ms step_avg:95.14ms
step:326/1770 train_time:30066ms step_avg:95.14ms
step:327/1770 train_time:30161ms step_avg:95.15ms
step:328/1770 train_time:30257ms step_avg:95.15ms
step:329/1770 train_time:30352ms step_avg:95.15ms
step:330/1770 train_time:30448ms step_avg:95.15ms
step:331/1770 train_time:30543ms step_avg:95.15ms
step:332/1770 train_time:30639ms step_avg:95.15ms
step:333/1770 train_time:30735ms step_avg:95.15ms
step:334/1770 train_time:30830ms step_avg:95.15ms
step:335/1770 train_time:30925ms step_avg:95.15ms
step:336/1770 train_time:31021ms step_avg:95.16ms
step:337/1770 train_time:31117ms step_avg:95.16ms
step:338/1770 train_time:31212ms step_avg:95.16ms
step:339/1770 train_time:31308ms step_avg:95.16ms
step:340/1770 train_time:31403ms step_avg:95.16ms
step:341/1770 train_time:31498ms step_avg:95.16ms
step:342/1770 train_time:31594ms step_avg:95.16ms
step:343/1770 train_time:31689ms step_avg:95.16ms
step:344/1770 train_time:31785ms step_avg:95.16ms
step:345/1770 train_time:31880ms step_avg:95.16ms
step:346/1770 train_time:31976ms step_avg:95.17ms
step:347/1770 train_time:32071ms step_avg:95.17ms
step:348/1770 train_time:32166ms step_avg:95.17ms
step:349/1770 train_time:32261ms step_avg:95.17ms
step:350/1770 train_time:32357ms step_avg:95.17ms
step:351/1770 train_time:32453ms step_avg:95.17ms
step:352/1770 train_time:32549ms step_avg:95.17ms
step:353/1770 train_time:32644ms step_avg:95.17ms
step:354/1770 train_time:32740ms step_avg:95.17ms
step:355/1770 train_time:32835ms step_avg:95.18ms
step:356/1770 train_time:32931ms step_avg:95.18ms
step:357/1770 train_time:33027ms step_avg:95.18ms
step:358/1770 train_time:33122ms step_avg:95.18ms
step:359/1770 train_time:33217ms step_avg:95.18ms
step:360/1770 train_time:33313ms step_avg:95.18ms
step:361/1770 train_time:33409ms step_avg:95.18ms
step:362/1770 train_time:33504ms step_avg:95.18ms
step:363/1770 train_time:33599ms step_avg:95.18ms
step:364/1770 train_time:33696ms step_avg:95.19ms
step:365/1770 train_time:33792ms step_avg:95.19ms
step:366/1770 train_time:33886ms step_avg:95.19ms
step:367/1770 train_time:33982ms step_avg:95.19ms
step:368/1770 train_time:34078ms step_avg:95.19ms
step:369/1770 train_time:34173ms step_avg:95.19ms
step:370/1770 train_time:34269ms step_avg:95.19ms
step:371/1770 train_time:34364ms step_avg:95.19ms
step:372/1770 train_time:34460ms step_avg:95.19ms
step:373/1770 train_time:34555ms step_avg:95.19ms
step:374/1770 train_time:34650ms step_avg:95.19ms
step:375/1770 train_time:34746ms step_avg:95.19ms
step:375/1770 val_loss:3.9056 train_time:34839ms step_avg:95.45ms
step:376/1770 train_time:34860ms step_avg:95.25ms
step:377/1770 train_time:34944ms step_avg:95.21ms
step:378/1770 train_time:35042ms step_avg:95.22ms
step:379/1770 train_time:35138ms step_avg:95.22ms
step:380/1770 train_time:35233ms step_avg:95.22ms
step:381/1770 train_time:35329ms step_avg:95.23ms
step:382/1770 train_time:35424ms step_avg:95.23ms
step:383/1770 train_time:35520ms step_avg:95.23ms
step:384/1770 train_time:35616ms step_avg:95.23ms
step:385/1770 train_time:35711ms step_avg:95.23ms
step:386/1770 train_time:35807ms step_avg:95.23ms
step:387/1770 train_time:35903ms step_avg:95.23ms
step:388/1770 train_time:35999ms step_avg:95.24ms
step:389/1770 train_time:36095ms step_avg:95.24ms
step:390/1770 train_time:36190ms step_avg:95.24ms
step:391/1770 train_time:36286ms step_avg:95.24ms
step:392/1770 train_time:36382ms step_avg:95.24ms
step:393/1770 train_time:36478ms step_avg:95.24ms
step:394/1770 train_time:36573ms step_avg:95.24ms
step:395/1770 train_time:36669ms step_avg:95.24ms
step:396/1770 train_time:36766ms step_avg:95.25ms
step:397/1770 train_time:36864ms step_avg:95.26ms
step:398/1770 train_time:36961ms step_avg:95.26ms
step:399/1770 train_time:37059ms step_avg:95.27ms
step:400/1770 train_time:37156ms step_avg:95.27ms
step:401/1770 train_time:37254ms step_avg:95.28ms
step:402/1770 train_time:37351ms step_avg:95.28ms
step:403/1770 train_time:37448ms step_avg:95.29ms
step:404/1770 train_time:37546ms step_avg:95.29ms
step:405/1770 train_time:37644ms step_avg:95.30ms
step:406/1770 train_time:37742ms step_avg:95.31ms
step:407/1770 train_time:37841ms step_avg:95.32ms
step:408/1770 train_time:37936ms step_avg:95.32ms
step:409/1770 train_time:38034ms step_avg:95.32ms
step:410/1770 train_time:38131ms step_avg:95.33ms
step:411/1770 train_time:38229ms step_avg:95.33ms
step:412/1770 train_time:38327ms step_avg:95.34ms
step:413/1770 train_time:38425ms step_avg:95.35ms
step:414/1770 train_time:38523ms step_avg:95.35ms
step:415/1770 train_time:38621ms step_avg:95.36ms
step:416/1770 train_time:38718ms step_avg:95.37ms
step:417/1770 train_time:38816ms step_avg:95.37ms
step:418/1770 train_time:38913ms step_avg:95.37ms
step:419/1770 train_time:39010ms step_avg:95.38ms
step:420/1770 train_time:39107ms step_avg:95.38ms
step:421/1770 train_time:39205ms step_avg:95.39ms
step:422/1770 train_time:39302ms step_avg:95.39ms
step:423/1770 train_time:39400ms step_avg:95.40ms
step:424/1770 train_time:39497ms step_avg:95.40ms
step:425/1770 train_time:39594ms step_avg:95.41ms
step:426/1770 train_time:39692ms step_avg:95.41ms
step:427/1770 train_time:39790ms step_avg:95.42ms
step:428/1770 train_time:39888ms step_avg:95.43ms
step:429/1770 train_time:39986ms step_avg:95.43ms
step:430/1770 train_time:40084ms step_avg:95.44ms
step:431/1770 train_time:40181ms step_avg:95.44ms
step:432/1770 train_time:40278ms step_avg:95.45ms
step:433/1770 train_time:40376ms step_avg:95.45ms
step:434/1770 train_time:40473ms step_avg:95.46ms
step:435/1770 train_time:40570ms step_avg:95.46ms
step:436/1770 train_time:40668ms step_avg:95.46ms
step:437/1770 train_time:40765ms step_avg:95.47ms
step:438/1770 train_time:40864ms step_avg:95.48ms
step:439/1770 train_time:40962ms step_avg:95.48ms
step:440/1770 train_time:41059ms step_avg:95.49ms
step:441/1770 train_time:41157ms step_avg:95.49ms
step:442/1770 train_time:41254ms step_avg:95.50ms
step:443/1770 train_time:41352ms step_avg:95.50ms
step:444/1770 train_time:41450ms step_avg:95.51ms
step:445/1770 train_time:41547ms step_avg:95.51ms
step:446/1770 train_time:41645ms step_avg:95.52ms
step:447/1770 train_time:41743ms step_avg:95.52ms
step:448/1770 train_time:41840ms step_avg:95.53ms
step:449/1770 train_time:41938ms step_avg:95.53ms
step:450/1770 train_time:42035ms step_avg:95.53ms
step:451/1770 train_time:42132ms step_avg:95.54ms
step:452/1770 train_time:42230ms step_avg:95.54ms
step:453/1770 train_time:42328ms step_avg:95.55ms
step:454/1770 train_time:42425ms step_avg:95.55ms
step:455/1770 train_time:42523ms step_avg:95.56ms
step:456/1770 train_time:42621ms step_avg:95.56ms
step:457/1770 train_time:42718ms step_avg:95.57ms
step:458/1770 train_time:42816ms step_avg:95.57ms
step:459/1770 train_time:42913ms step_avg:95.57ms
step:460/1770 train_time:43010ms step_avg:95.58ms
step:461/1770 train_time:43108ms step_avg:95.58ms
step:462/1770 train_time:43207ms step_avg:95.59ms
step:463/1770 train_time:43305ms step_avg:95.60ms
step:464/1770 train_time:43402ms step_avg:95.60ms
step:465/1770 train_time:43500ms step_avg:95.60ms
step:466/1770 train_time:43597ms step_avg:95.61ms
step:467/1770 train_time:43694ms step_avg:95.61ms
step:468/1770 train_time:43791ms step_avg:95.61ms
step:469/1770 train_time:43889ms step_avg:95.62ms
step:470/1770 train_time:43987ms step_avg:95.62ms
step:471/1770 train_time:44084ms step_avg:95.63ms
step:472/1770 train_time:44182ms step_avg:95.63ms
step:473/1770 train_time:44280ms step_avg:95.64ms
step:474/1770 train_time:44377ms step_avg:95.64ms
step:475/1770 train_time:44474ms step_avg:95.64ms
step:476/1770 train_time:44571ms step_avg:95.65ms
step:477/1770 train_time:44669ms step_avg:95.65ms
step:478/1770 train_time:44767ms step_avg:95.66ms
step:479/1770 train_time:44864ms step_avg:95.66ms
step:480/1770 train_time:44961ms step_avg:95.66ms
step:481/1770 train_time:45059ms step_avg:95.67ms
step:482/1770 train_time:45156ms step_avg:95.67ms
step:483/1770 train_time:45253ms step_avg:95.67ms
step:484/1770 train_time:45350ms step_avg:95.68ms
step:485/1770 train_time:45448ms step_avg:95.68ms
step:486/1770 train_time:45546ms step_avg:95.68ms
step:487/1770 train_time:45644ms step_avg:95.69ms
step:488/1770 train_time:45742ms step_avg:95.69ms
step:489/1770 train_time:45840ms step_avg:95.70ms
step:490/1770 train_time:45937ms step_avg:95.70ms
step:491/1770 train_time:46034ms step_avg:95.70ms
step:492/1770 train_time:46131ms step_avg:95.71ms
step:493/1770 train_time:46229ms step_avg:95.71ms
step:494/1770 train_time:46327ms step_avg:95.72ms
step:495/1770 train_time:46424ms step_avg:95.72ms
step:496/1770 train_time:46522ms step_avg:95.72ms
step:497/1770 train_time:46619ms step_avg:95.73ms
step:498/1770 train_time:46717ms step_avg:95.73ms
step:499/1770 train_time:46814ms step_avg:95.73ms
step:500/1770 train_time:46912ms step_avg:95.74ms
step:500/1770 val_loss:3.7516 train_time:47007ms step_avg:95.93ms
step:501/1770 train_time:47029ms step_avg:95.78ms
step:502/1770 train_time:47115ms step_avg:95.76ms
step:503/1770 train_time:47214ms step_avg:95.77ms
step:504/1770 train_time:47312ms step_avg:95.77ms
step:505/1770 train_time:47409ms step_avg:95.78ms
step:506/1770 train_time:47507ms step_avg:95.78ms
step:507/1770 train_time:47605ms step_avg:95.78ms
step:508/1770 train_time:47702ms step_avg:95.79ms
step:509/1770 train_time:47799ms step_avg:95.79ms
step:510/1770 train_time:47897ms step_avg:95.79ms
step:511/1770 train_time:47994ms step_avg:95.80ms
step:512/1770 train_time:48091ms step_avg:95.80ms
step:513/1770 train_time:48189ms step_avg:95.80ms
step:514/1770 train_time:48288ms step_avg:95.81ms
step:515/1770 train_time:48386ms step_avg:95.81ms
step:516/1770 train_time:48484ms step_avg:95.82ms
step:517/1770 train_time:48582ms step_avg:95.82ms
step:518/1770 train_time:48679ms step_avg:95.83ms
step:519/1770 train_time:48776ms step_avg:95.83ms
step:520/1770 train_time:48873ms step_avg:95.83ms
step:521/1770 train_time:48971ms step_avg:95.83ms
step:522/1770 train_time:49069ms step_avg:95.84ms
step:523/1770 train_time:49166ms step_avg:95.84ms
step:524/1770 train_time:49264ms step_avg:95.84ms
step:525/1770 train_time:49362ms step_avg:95.85ms
step:526/1770 train_time:49459ms step_avg:95.85ms
step:527/1770 train_time:49557ms step_avg:95.85ms
step:528/1770 train_time:49655ms step_avg:95.86ms
step:529/1770 train_time:49753ms step_avg:95.86ms
step:530/1770 train_time:49851ms step_avg:95.87ms
step:531/1770 train_time:49949ms step_avg:95.87ms
step:532/1770 train_time:50047ms step_avg:95.88ms
step:533/1770 train_time:50145ms step_avg:95.88ms
step:534/1770 train_time:50243ms step_avg:95.88ms
step:535/1770 train_time:50340ms step_avg:95.89ms
step:536/1770 train_time:50438ms step_avg:95.89ms
step:537/1770 train_time:50535ms step_avg:95.89ms
step:538/1770 train_time:50633ms step_avg:95.89ms
step:539/1770 train_time:50730ms step_avg:95.90ms
step:540/1770 train_time:50829ms step_avg:95.90ms
step:541/1770 train_time:50926ms step_avg:95.91ms
step:542/1770 train_time:51025ms step_avg:95.91ms
step:543/1770 train_time:51123ms step_avg:95.91ms
step:544/1770 train_time:51221ms step_avg:95.92ms
step:545/1770 train_time:51319ms step_avg:95.92ms
step:546/1770 train_time:51416ms step_avg:95.93ms
step:547/1770 train_time:51514ms step_avg:95.93ms
step:548/1770 train_time:51611ms step_avg:95.93ms
step:549/1770 train_time:51709ms step_avg:95.94ms
step:550/1770 train_time:51808ms step_avg:95.94ms
step:551/1770 train_time:51905ms step_avg:95.94ms
step:552/1770 train_time:52003ms step_avg:95.95ms
step:553/1770 train_time:52101ms step_avg:95.95ms
step:554/1770 train_time:52199ms step_avg:95.95ms
step:555/1770 train_time:52296ms step_avg:95.96ms
step:556/1770 train_time:52393ms step_avg:95.96ms
step:557/1770 train_time:52491ms step_avg:95.96ms
step:558/1770 train_time:52590ms step_avg:95.97ms
step:559/1770 train_time:52688ms step_avg:95.97ms
step:560/1770 train_time:52787ms step_avg:95.98ms
step:561/1770 train_time:52885ms step_avg:95.98ms
step:562/1770 train_time:52983ms step_avg:95.98ms
step:563/1770 train_time:53080ms step_avg:95.99ms
step:564/1770 train_time:53178ms step_avg:95.99ms
step:565/1770 train_time:53276ms step_avg:95.99ms
step:566/1770 train_time:53374ms step_avg:96.00ms
step:567/1770 train_time:53472ms step_avg:96.00ms
step:568/1770 train_time:53570ms step_avg:96.00ms
step:569/1770 train_time:53668ms step_avg:96.01ms
step:570/1770 train_time:53767ms step_avg:96.01ms
step:571/1770 train_time:53864ms step_avg:96.02ms
step:572/1770 train_time:53963ms step_avg:96.02ms
step:573/1770 train_time:54060ms step_avg:96.02ms
step:574/1770 train_time:54158ms step_avg:96.02ms
step:575/1770 train_time:54256ms step_avg:96.03ms
step:576/1770 train_time:54354ms step_avg:96.03ms
step:577/1770 train_time:54452ms step_avg:96.03ms
step:578/1770 train_time:54550ms step_avg:96.04ms
step:579/1770 train_time:54648ms step_avg:96.04ms
step:580/1770 train_time:54748ms step_avg:96.05ms
step:581/1770 train_time:54843ms step_avg:96.05ms
step:582/1770 train_time:54941ms step_avg:96.05ms
step:583/1770 train_time:55038ms step_avg:96.05ms
step:584/1770 train_time:55136ms step_avg:96.06ms
step:585/1770 train_time:55233ms step_avg:96.06ms
step:586/1770 train_time:55331ms step_avg:96.06ms
step:587/1770 train_time:55429ms step_avg:96.06ms
step:588/1770 train_time:55527ms step_avg:96.07ms
step:589/1770 train_time:55625ms step_avg:96.07ms
step:590/1770 train_time:55724ms step_avg:96.08ms
step:591/1770 train_time:55822ms step_avg:96.08ms
step:592/1770 train_time:55920ms step_avg:96.08ms
step:593/1770 train_time:56017ms step_avg:96.08ms
step:594/1770 train_time:56115ms step_avg:96.09ms
step:595/1770 train_time:56213ms step_avg:96.09ms
step:596/1770 train_time:56311ms step_avg:96.09ms
step:597/1770 train_time:56409ms step_avg:96.10ms
step:598/1770 train_time:56507ms step_avg:96.10ms
step:599/1770 train_time:56605ms step_avg:96.10ms
step:600/1770 train_time:56703ms step_avg:96.11ms
step:601/1770 train_time:56800ms step_avg:96.11ms
step:602/1770 train_time:56898ms step_avg:96.11ms
step:603/1770 train_time:56996ms step_avg:96.11ms
step:604/1770 train_time:57093ms step_avg:96.12ms
step:605/1770 train_time:57191ms step_avg:96.12ms
step:606/1770 train_time:57289ms step_avg:96.12ms
step:607/1770 train_time:57388ms step_avg:96.13ms
step:608/1770 train_time:57486ms step_avg:96.13ms
step:609/1770 train_time:57584ms step_avg:96.13ms
step:610/1770 train_time:57682ms step_avg:96.14ms
step:611/1770 train_time:57779ms step_avg:96.14ms
step:612/1770 train_time:57877ms step_avg:96.14ms
step:613/1770 train_time:57975ms step_avg:96.15ms
step:614/1770 train_time:58073ms step_avg:96.15ms
step:615/1770 train_time:58171ms step_avg:96.15ms
step:616/1770 train_time:58269ms step_avg:96.15ms
step:617/1770 train_time:58367ms step_avg:96.16ms
step:618/1770 train_time:58466ms step_avg:96.16ms
step:619/1770 train_time:58564ms step_avg:96.16ms
step:620/1770 train_time:58661ms step_avg:96.17ms
step:621/1770 train_time:58759ms step_avg:96.17ms
step:622/1770 train_time:58857ms step_avg:96.17ms
step:623/1770 train_time:58954ms step_avg:96.17ms
step:624/1770 train_time:59051ms step_avg:96.17ms
step:625/1770 train_time:59149ms step_avg:96.18ms
step:625/1770 val_loss:3.6677 train_time:59246ms step_avg:96.33ms
step:626/1770 train_time:59267ms step_avg:96.21ms
step:627/1770 train_time:59353ms step_avg:96.20ms
step:628/1770 train_time:59453ms step_avg:96.20ms
step:629/1770 train_time:59552ms step_avg:96.21ms
step:630/1770 train_time:59650ms step_avg:96.21ms
step:631/1770 train_time:59748ms step_avg:96.21ms
step:632/1770 train_time:59845ms step_avg:96.21ms
step:633/1770 train_time:59943ms step_avg:96.22ms
step:634/1770 train_time:60041ms step_avg:96.22ms
step:635/1770 train_time:60138ms step_avg:96.22ms
step:636/1770 train_time:60236ms step_avg:96.22ms
step:637/1770 train_time:60336ms step_avg:96.23ms
step:638/1770 train_time:60433ms step_avg:96.23ms
step:639/1770 train_time:60532ms step_avg:96.24ms
step:640/1770 train_time:60630ms step_avg:96.24ms
step:641/1770 train_time:60728ms step_avg:96.24ms
step:642/1770 train_time:60826ms step_avg:96.24ms
step:643/1770 train_time:60924ms step_avg:96.25ms
step:644/1770 train_time:61021ms step_avg:96.25ms
step:645/1770 train_time:61119ms step_avg:96.25ms
step:646/1770 train_time:61217ms step_avg:96.25ms
step:647/1770 train_time:61316ms step_avg:96.26ms
step:648/1770 train_time:61414ms step_avg:96.26ms
step:649/1770 train_time:61512ms step_avg:96.26ms
step:650/1770 train_time:61609ms step_avg:96.26ms
step:651/1770 train_time:61707ms step_avg:96.27ms
step:652/1770 train_time:61805ms step_avg:96.27ms
step:653/1770 train_time:61902ms step_avg:96.27ms
step:654/1770 train_time:62000ms step_avg:96.27ms
step:655/1770 train_time:62098ms step_avg:96.28ms
step:656/1770 train_time:62196ms step_avg:96.28ms
step:657/1770 train_time:62295ms step_avg:96.28ms
step:658/1770 train_time:62394ms step_avg:96.29ms
step:659/1770 train_time:62494ms step_avg:96.29ms
step:660/1770 train_time:62594ms step_avg:96.30ms
step:661/1770 train_time:62694ms step_avg:96.30ms
step:662/1770 train_time:62795ms step_avg:96.31ms
step:663/1770 train_time:62896ms step_avg:96.32ms
step:664/1770 train_time:62997ms step_avg:96.33ms
step:665/1770 train_time:63098ms step_avg:96.33ms
step:666/1770 train_time:63198ms step_avg:96.34ms
step:667/1770 train_time:63298ms step_avg:96.34ms
step:668/1770 train_time:63398ms step_avg:96.35ms
step:669/1770 train_time:63498ms step_avg:96.35ms
step:670/1770 train_time:63598ms step_avg:96.36ms
step:671/1770 train_time:63698ms step_avg:96.37ms
step:672/1770 train_time:63798ms step_avg:96.37ms
step:673/1770 train_time:63899ms step_avg:96.38ms
step:674/1770 train_time:63999ms step_avg:96.38ms
step:675/1770 train_time:64099ms step_avg:96.39ms
step:676/1770 train_time:64199ms step_avg:96.39ms
step:677/1770 train_time:64299ms step_avg:96.40ms
step:678/1770 train_time:64398ms step_avg:96.40ms
step:679/1770 train_time:64498ms step_avg:96.41ms
step:680/1770 train_time:64598ms step_avg:96.41ms
step:681/1770 train_time:64699ms step_avg:96.42ms
step:682/1770 train_time:64799ms step_avg:96.43ms
step:683/1770 train_time:64899ms step_avg:96.43ms
step:684/1770 train_time:64999ms step_avg:96.44ms
step:685/1770 train_time:65099ms step_avg:96.44ms
step:686/1770 train_time:65199ms step_avg:96.45ms
step:687/1770 train_time:65298ms step_avg:96.45ms
step:688/1770 train_time:65398ms step_avg:96.46ms
step:689/1770 train_time:65498ms step_avg:96.46ms
step:690/1770 train_time:65598ms step_avg:96.47ms
step:691/1770 train_time:65698ms step_avg:96.47ms
step:692/1770 train_time:65798ms step_avg:96.48ms
step:693/1770 train_time:65899ms step_avg:96.48ms
step:694/1770 train_time:65999ms step_avg:96.49ms
step:695/1770 train_time:66099ms step_avg:96.50ms
step:696/1770 train_time:66199ms step_avg:96.50ms
step:697/1770 train_time:66299ms step_avg:96.50ms
step:698/1770 train_time:66398ms step_avg:96.51ms
step:699/1770 train_time:66498ms step_avg:96.51ms
step:700/1770 train_time:66598ms step_avg:96.52ms
step:701/1770 train_time:66698ms step_avg:96.52ms
step:702/1770 train_time:66798ms step_avg:96.53ms
step:703/1770 train_time:66900ms step_avg:96.54ms
step:704/1770 train_time:67000ms step_avg:96.54ms
step:705/1770 train_time:67099ms step_avg:96.55ms
step:706/1770 train_time:67199ms step_avg:96.55ms
step:707/1770 train_time:67299ms step_avg:96.56ms
step:708/1770 train_time:67398ms step_avg:96.56ms
step:709/1770 train_time:67498ms step_avg:96.56ms
step:710/1770 train_time:67598ms step_avg:96.57ms
step:711/1770 train_time:67698ms step_avg:96.57ms
step:712/1770 train_time:67798ms step_avg:96.58ms
step:713/1770 train_time:67899ms step_avg:96.58ms
step:714/1770 train_time:67999ms step_avg:96.59ms
step:715/1770 train_time:68100ms step_avg:96.60ms
step:716/1770 train_time:68200ms step_avg:96.60ms
step:717/1770 train_time:68300ms step_avg:96.61ms
step:718/1770 train_time:68399ms step_avg:96.61ms
step:719/1770 train_time:68499ms step_avg:96.61ms
step:720/1770 train_time:68599ms step_avg:96.62ms
step:721/1770 train_time:68699ms step_avg:96.62ms
step:722/1770 train_time:68798ms step_avg:96.63ms
step:723/1770 train_time:68898ms step_avg:96.63ms
step:724/1770 train_time:68997ms step_avg:96.63ms
step:725/1770 train_time:69097ms step_avg:96.64ms
step:726/1770 train_time:69198ms step_avg:96.65ms
step:727/1770 train_time:69299ms step_avg:96.65ms
step:728/1770 train_time:69399ms step_avg:96.66ms
step:729/1770 train_time:69499ms step_avg:96.66ms
step:730/1770 train_time:69598ms step_avg:96.66ms
step:731/1770 train_time:69698ms step_avg:96.67ms
step:732/1770 train_time:69798ms step_avg:96.67ms
step:733/1770 train_time:69898ms step_avg:96.68ms
step:734/1770 train_time:69998ms step_avg:96.68ms
step:735/1770 train_time:70098ms step_avg:96.69ms
step:736/1770 train_time:70198ms step_avg:96.69ms
step:737/1770 train_time:70299ms step_avg:96.70ms
step:738/1770 train_time:70399ms step_avg:96.70ms
step:739/1770 train_time:70499ms step_avg:96.71ms
step:740/1770 train_time:70599ms step_avg:96.71ms
step:741/1770 train_time:70699ms step_avg:96.72ms
step:742/1770 train_time:70799ms step_avg:96.72ms
step:743/1770 train_time:70899ms step_avg:96.72ms
step:744/1770 train_time:70998ms step_avg:96.73ms
step:745/1770 train_time:71098ms step_avg:96.73ms
step:746/1770 train_time:71198ms step_avg:96.74ms
step:747/1770 train_time:71298ms step_avg:96.74ms
step:748/1770 train_time:71397ms step_avg:96.74ms
step:749/1770 train_time:71497ms step_avg:96.75ms
step:750/1770 train_time:71598ms step_avg:96.75ms
step:750/1770 val_loss:3.6024 train_time:71696ms step_avg:96.89ms
step:751/1770 train_time:71718ms step_avg:96.78ms
step:752/1770 train_time:71803ms step_avg:96.77ms
step:753/1770 train_time:71905ms step_avg:96.78ms
step:754/1770 train_time:72005ms step_avg:96.78ms
step:755/1770 train_time:72104ms step_avg:96.78ms
step:756/1770 train_time:72207ms step_avg:96.79ms
step:757/1770 train_time:72303ms step_avg:96.79ms
step:758/1770 train_time:72402ms step_avg:96.79ms
step:759/1770 train_time:72501ms step_avg:96.80ms
step:760/1770 train_time:72601ms step_avg:96.80ms
step:761/1770 train_time:72701ms step_avg:96.81ms
step:762/1770 train_time:72801ms step_avg:96.81ms
step:763/1770 train_time:72902ms step_avg:96.81ms
step:764/1770 train_time:73002ms step_avg:96.82ms
step:765/1770 train_time:73101ms step_avg:96.82ms
step:766/1770 train_time:73202ms step_avg:96.83ms
step:767/1770 train_time:73302ms step_avg:96.83ms
step:768/1770 train_time:73402ms step_avg:96.84ms
step:769/1770 train_time:73501ms step_avg:96.84ms
step:770/1770 train_time:73601ms step_avg:96.84ms
step:771/1770 train_time:73701ms step_avg:96.85ms
step:772/1770 train_time:73800ms step_avg:96.85ms
step:773/1770 train_time:73899ms step_avg:96.85ms
step:774/1770 train_time:73999ms step_avg:96.86ms
step:775/1770 train_time:74098ms step_avg:96.86ms
step:776/1770 train_time:74198ms step_avg:96.86ms
step:777/1770 train_time:74299ms step_avg:96.87ms
step:778/1770 train_time:74397ms step_avg:96.87ms
step:779/1770 train_time:74497ms step_avg:96.88ms
step:780/1770 train_time:74598ms step_avg:96.88ms
step:781/1770 train_time:74698ms step_avg:96.88ms
step:782/1770 train_time:74798ms step_avg:96.89ms
step:783/1770 train_time:74898ms step_avg:96.89ms
step:784/1770 train_time:74998ms step_avg:96.90ms
step:785/1770 train_time:75096ms step_avg:96.90ms
step:786/1770 train_time:75196ms step_avg:96.90ms
step:787/1770 train_time:75295ms step_avg:96.90ms
step:788/1770 train_time:75394ms step_avg:96.91ms
step:789/1770 train_time:75494ms step_avg:96.91ms
step:790/1770 train_time:75593ms step_avg:96.91ms
step:791/1770 train_time:75694ms step_avg:96.92ms
step:792/1770 train_time:75793ms step_avg:96.92ms
step:793/1770 train_time:75894ms step_avg:96.93ms
step:794/1770 train_time:75993ms step_avg:96.93ms
step:795/1770 train_time:76092ms step_avg:96.93ms
step:796/1770 train_time:76193ms step_avg:96.94ms
step:797/1770 train_time:76293ms step_avg:96.94ms
step:798/1770 train_time:76393ms step_avg:96.95ms
step:799/1770 train_time:76492ms step_avg:96.95ms
step:800/1770 train_time:76592ms step_avg:96.95ms
step:801/1770 train_time:76692ms step_avg:96.96ms
step:802/1770 train_time:76792ms step_avg:96.96ms
step:803/1770 train_time:76892ms step_avg:96.96ms
step:804/1770 train_time:76993ms step_avg:96.97ms
step:805/1770 train_time:77092ms step_avg:96.97ms
step:806/1770 train_time:77192ms step_avg:96.97ms
step:807/1770 train_time:77292ms step_avg:96.98ms
step:808/1770 train_time:77392ms step_avg:96.98ms
step:809/1770 train_time:77492ms step_avg:96.99ms
step:810/1770 train_time:77592ms step_avg:96.99ms
step:811/1770 train_time:77692ms step_avg:96.99ms
step:812/1770 train_time:77792ms step_avg:97.00ms
step:813/1770 train_time:77892ms step_avg:97.00ms
step:814/1770 train_time:77992ms step_avg:97.01ms
step:815/1770 train_time:78092ms step_avg:97.01ms
step:816/1770 train_time:78192ms step_avg:97.01ms
step:817/1770 train_time:78292ms step_avg:97.02ms
step:818/1770 train_time:78392ms step_avg:97.02ms
step:819/1770 train_time:78492ms step_avg:97.02ms
step:820/1770 train_time:78592ms step_avg:97.03ms
step:821/1770 train_time:78692ms step_avg:97.03ms
step:822/1770 train_time:78793ms step_avg:97.04ms
step:823/1770 train_time:78892ms step_avg:97.04ms
step:824/1770 train_time:78992ms step_avg:97.04ms
step:825/1770 train_time:79092ms step_avg:97.05ms
step:826/1770 train_time:79192ms step_avg:97.05ms
step:827/1770 train_time:79292ms step_avg:97.05ms
step:828/1770 train_time:79392ms step_avg:97.06ms
step:829/1770 train_time:79492ms step_avg:97.06ms
step:830/1770 train_time:79592ms step_avg:97.06ms
step:831/1770 train_time:79692ms step_avg:97.07ms
step:832/1770 train_time:79792ms step_avg:97.07ms
step:833/1770 train_time:79893ms step_avg:97.07ms
step:834/1770 train_time:79993ms step_avg:97.08ms
step:835/1770 train_time:80092ms step_avg:97.08ms
step:836/1770 train_time:80192ms step_avg:97.09ms
step:837/1770 train_time:80292ms step_avg:97.09ms
step:838/1770 train_time:80393ms step_avg:97.09ms
step:839/1770 train_time:80493ms step_avg:97.10ms
step:840/1770 train_time:80593ms step_avg:97.10ms
step:841/1770 train_time:80692ms step_avg:97.10ms
step:842/1770 train_time:80793ms step_avg:97.11ms
step:843/1770 train_time:80892ms step_avg:97.11ms
step:844/1770 train_time:80992ms step_avg:97.11ms
step:845/1770 train_time:81092ms step_avg:97.12ms
step:846/1770 train_time:81192ms step_avg:97.12ms
step:847/1770 train_time:81291ms step_avg:97.12ms
step:848/1770 train_time:81392ms step_avg:97.13ms
step:849/1770 train_time:81492ms step_avg:97.13ms
step:850/1770 train_time:81593ms step_avg:97.13ms
step:851/1770 train_time:81693ms step_avg:97.14ms
step:852/1770 train_time:81794ms step_avg:97.14ms
step:853/1770 train_time:81893ms step_avg:97.14ms
step:854/1770 train_time:81993ms step_avg:97.15ms
step:855/1770 train_time:82093ms step_avg:97.15ms
step:856/1770 train_time:82192ms step_avg:97.15ms
step:857/1770 train_time:82292ms step_avg:97.16ms
step:858/1770 train_time:82392ms step_avg:97.16ms
step:859/1770 train_time:82492ms step_avg:97.16ms
step:860/1770 train_time:82592ms step_avg:97.17ms
step:861/1770 train_time:82692ms step_avg:97.17ms
step:862/1770 train_time:82792ms step_avg:97.17ms
step:863/1770 train_time:82892ms step_avg:97.18ms
step:864/1770 train_time:82992ms step_avg:97.18ms
step:865/1770 train_time:83092ms step_avg:97.18ms
step:866/1770 train_time:83193ms step_avg:97.19ms
step:867/1770 train_time:83293ms step_avg:97.19ms
step:868/1770 train_time:83393ms step_avg:97.19ms
step:869/1770 train_time:83492ms step_avg:97.20ms
step:870/1770 train_time:83593ms step_avg:97.20ms
step:871/1770 train_time:83692ms step_avg:97.20ms
step:872/1770 train_time:83793ms step_avg:97.21ms
step:873/1770 train_time:83892ms step_avg:97.21ms
step:874/1770 train_time:83992ms step_avg:97.21ms
step:875/1770 train_time:84092ms step_avg:97.22ms
step:875/1770 val_loss:3.5552 train_time:84191ms step_avg:97.33ms
step:876/1770 train_time:84212ms step_avg:97.24ms
step:877/1770 train_time:84303ms step_avg:97.24ms
step:878/1770 train_time:84403ms step_avg:97.24ms
step:879/1770 train_time:84502ms step_avg:97.24ms
step:880/1770 train_time:84602ms step_avg:97.24ms
step:881/1770 train_time:84701ms step_avg:97.25ms
step:882/1770 train_time:84801ms step_avg:97.25ms
step:883/1770 train_time:84900ms step_avg:97.25ms
step:884/1770 train_time:84999ms step_avg:97.25ms
step:885/1770 train_time:85099ms step_avg:97.26ms
step:886/1770 train_time:85199ms step_avg:97.26ms
step:887/1770 train_time:85301ms step_avg:97.26ms
step:888/1770 train_time:85401ms step_avg:97.27ms
step:889/1770 train_time:85502ms step_avg:97.27ms
step:890/1770 train_time:85601ms step_avg:97.27ms
step:891/1770 train_time:85701ms step_avg:97.28ms
step:892/1770 train_time:85801ms step_avg:97.28ms
step:893/1770 train_time:85901ms step_avg:97.28ms
step:894/1770 train_time:86001ms step_avg:97.29ms
step:895/1770 train_time:86101ms step_avg:97.29ms
step:896/1770 train_time:86201ms step_avg:97.29ms
step:897/1770 train_time:86301ms step_avg:97.30ms
step:898/1770 train_time:86403ms step_avg:97.30ms
step:899/1770 train_time:86502ms step_avg:97.30ms
step:900/1770 train_time:86602ms step_avg:97.31ms
step:901/1770 train_time:86702ms step_avg:97.31ms
step:902/1770 train_time:86803ms step_avg:97.31ms
step:903/1770 train_time:86903ms step_avg:97.32ms
step:904/1770 train_time:87003ms step_avg:97.32ms
step:905/1770 train_time:87103ms step_avg:97.32ms
step:906/1770 train_time:87203ms step_avg:97.33ms
step:907/1770 train_time:87304ms step_avg:97.33ms
step:908/1770 train_time:87405ms step_avg:97.33ms
step:909/1770 train_time:87505ms step_avg:97.34ms
step:910/1770 train_time:87605ms step_avg:97.34ms
step:911/1770 train_time:87706ms step_avg:97.34ms
step:912/1770 train_time:87806ms step_avg:97.35ms
step:913/1770 train_time:87906ms step_avg:97.35ms
step:914/1770 train_time:88006ms step_avg:97.35ms
step:915/1770 train_time:88106ms step_avg:97.35ms
step:916/1770 train_time:88207ms step_avg:97.36ms
step:917/1770 train_time:88308ms step_avg:97.36ms
step:918/1770 train_time:88408ms step_avg:97.37ms
step:919/1770 train_time:88508ms step_avg:97.37ms
step:920/1770 train_time:88611ms step_avg:97.37ms
step:921/1770 train_time:88713ms step_avg:97.38ms
step:922/1770 train_time:88814ms step_avg:97.38ms
step:923/1770 train_time:88915ms step_avg:97.39ms
step:924/1770 train_time:89015ms step_avg:97.39ms
step:925/1770 train_time:89116ms step_avg:97.39ms
step:926/1770 train_time:89216ms step_avg:97.40ms
step:927/1770 train_time:89317ms step_avg:97.40ms
step:928/1770 train_time:89417ms step_avg:97.40ms
step:929/1770 train_time:89519ms step_avg:97.41ms
step:930/1770 train_time:89622ms step_avg:97.41ms
step:931/1770 train_time:89723ms step_avg:97.42ms
step:932/1770 train_time:89825ms step_avg:97.42ms
step:933/1770 train_time:89927ms step_avg:97.43ms
step:934/1770 train_time:90029ms step_avg:97.43ms
step:935/1770 train_time:90130ms step_avg:97.44ms
step:936/1770 train_time:90232ms step_avg:97.44ms
step:937/1770 train_time:90332ms step_avg:97.45ms
step:938/1770 train_time:90434ms step_avg:97.45ms
step:939/1770 train_time:90535ms step_avg:97.45ms
step:940/1770 train_time:90636ms step_avg:97.46ms
step:941/1770 train_time:90738ms step_avg:97.46ms
step:942/1770 train_time:90839ms step_avg:97.47ms
step:943/1770 train_time:90941ms step_avg:97.47ms
step:944/1770 train_time:91041ms step_avg:97.47ms
step:945/1770 train_time:91143ms step_avg:97.48ms
step:946/1770 train_time:91244ms step_avg:97.48ms
step:947/1770 train_time:91346ms step_avg:97.49ms
step:948/1770 train_time:91447ms step_avg:97.49ms
step:949/1770 train_time:91550ms step_avg:97.50ms
step:950/1770 train_time:91651ms step_avg:97.50ms
step:951/1770 train_time:91753ms step_avg:97.51ms
step:952/1770 train_time:91854ms step_avg:97.51ms
step:953/1770 train_time:91955ms step_avg:97.51ms
step:954/1770 train_time:92055ms step_avg:97.52ms
step:955/1770 train_time:92157ms step_avg:97.52ms
step:956/1770 train_time:92257ms step_avg:97.52ms
step:957/1770 train_time:92358ms step_avg:97.53ms
step:958/1770 train_time:92458ms step_avg:97.53ms
step:959/1770 train_time:92560ms step_avg:97.53ms
step:960/1770 train_time:92661ms step_avg:97.54ms
step:961/1770 train_time:92762ms step_avg:97.54ms
step:962/1770 train_time:92864ms step_avg:97.55ms
step:963/1770 train_time:92965ms step_avg:97.55ms
step:964/1770 train_time:93067ms step_avg:97.55ms
step:965/1770 train_time:93170ms step_avg:97.56ms
step:966/1770 train_time:93271ms step_avg:97.56ms
step:967/1770 train_time:93372ms step_avg:97.57ms
step:968/1770 train_time:93473ms step_avg:97.57ms
step:969/1770 train_time:93574ms step_avg:97.57ms
step:970/1770 train_time:93674ms step_avg:97.58ms
step:971/1770 train_time:93775ms step_avg:97.58ms
step:972/1770 train_time:93876ms step_avg:97.58ms
step:973/1770 train_time:93976ms step_avg:97.59ms
step:974/1770 train_time:94077ms step_avg:97.59ms
step:975/1770 train_time:94180ms step_avg:97.60ms
step:976/1770 train_time:94281ms step_avg:97.60ms
step:977/1770 train_time:94383ms step_avg:97.60ms
step:978/1770 train_time:94484ms step_avg:97.61ms
step:979/1770 train_time:94586ms step_avg:97.61ms
step:980/1770 train_time:94687ms step_avg:97.62ms
step:981/1770 train_time:94789ms step_avg:97.62ms
step:982/1770 train_time:94891ms step_avg:97.62ms
step:983/1770 train_time:94993ms step_avg:97.63ms
step:984/1770 train_time:95095ms step_avg:97.63ms
step:985/1770 train_time:95196ms step_avg:97.64ms
step:986/1770 train_time:95296ms step_avg:97.64ms
step:987/1770 train_time:95397ms step_avg:97.64ms
step:988/1770 train_time:95499ms step_avg:97.65ms
step:989/1770 train_time:95600ms step_avg:97.65ms
step:990/1770 train_time:95701ms step_avg:97.65ms
step:991/1770 train_time:95803ms step_avg:97.66ms
step:992/1770 train_time:95904ms step_avg:97.66ms
step:993/1770 train_time:96006ms step_avg:97.67ms
step:994/1770 train_time:96108ms step_avg:97.67ms
step:995/1770 train_time:96210ms step_avg:97.68ms
step:996/1770 train_time:96311ms step_avg:97.68ms
step:997/1770 train_time:96412ms step_avg:97.68ms
step:998/1770 train_time:96513ms step_avg:97.69ms
step:999/1770 train_time:96614ms step_avg:97.69ms
step:1000/1770 train_time:96715ms step_avg:97.69ms
step:1000/1770 val_loss:3.5168 train_time:96814ms step_avg:97.79ms
step:1001/1770 train_time:96835ms step_avg:97.71ms
step:1002/1770 train_time:96925ms step_avg:97.71ms
step:1003/1770 train_time:97028ms step_avg:97.71ms
step:1004/1770 train_time:97129ms step_avg:97.71ms
step:1005/1770 train_time:97230ms step_avg:97.72ms
step:1006/1770 train_time:97331ms step_avg:97.72ms
step:1007/1770 train_time:97432ms step_avg:97.73ms
step:1008/1770 train_time:97533ms step_avg:97.73ms
step:1009/1770 train_time:97634ms step_avg:97.73ms
step:1010/1770 train_time:97735ms step_avg:97.73ms
step:1011/1770 train_time:97837ms step_avg:97.74ms
step:1012/1770 train_time:97939ms step_avg:97.74ms
step:1013/1770 train_time:98041ms step_avg:97.75ms
step:1014/1770 train_time:98143ms step_avg:97.75ms
step:1015/1770 train_time:98243ms step_avg:97.75ms
step:1016/1770 train_time:98344ms step_avg:97.76ms
step:1017/1770 train_time:98445ms step_avg:97.76ms
step:1018/1770 train_time:98546ms step_avg:97.76ms
step:1019/1770 train_time:98646ms step_avg:97.77ms
step:1020/1770 train_time:98747ms step_avg:97.77ms
step:1021/1770 train_time:98849ms step_avg:97.77ms
step:1022/1770 train_time:98950ms step_avg:97.78ms
step:1023/1770 train_time:99051ms step_avg:97.78ms
step:1024/1770 train_time:99154ms step_avg:97.78ms
step:1025/1770 train_time:99257ms step_avg:97.79ms
step:1026/1770 train_time:99358ms step_avg:97.79ms
step:1027/1770 train_time:99464ms step_avg:97.80ms
step:1028/1770 train_time:99562ms step_avg:97.80ms
step:1029/1770 train_time:99662ms step_avg:97.80ms
step:1030/1770 train_time:99764ms step_avg:97.81ms
step:1031/1770 train_time:99865ms step_avg:97.81ms
step:1032/1770 train_time:99966ms step_avg:97.81ms
step:1033/1770 train_time:100068ms step_avg:97.82ms
step:1034/1770 train_time:100168ms step_avg:97.82ms
step:1035/1770 train_time:100269ms step_avg:97.82ms
step:1036/1770 train_time:100370ms step_avg:97.83ms
step:1037/1770 train_time:100471ms step_avg:97.83ms
step:1038/1770 train_time:100573ms step_avg:97.83ms
step:1039/1770 train_time:100675ms step_avg:97.84ms
step:1040/1770 train_time:100776ms step_avg:97.84ms
step:1041/1770 train_time:100878ms step_avg:97.84ms
step:1042/1770 train_time:100980ms step_avg:97.85ms
step:1043/1770 train_time:101082ms step_avg:97.85ms
step:1044/1770 train_time:101183ms step_avg:97.86ms
step:1045/1770 train_time:101284ms step_avg:97.86ms
step:1046/1770 train_time:101384ms step_avg:97.86ms
step:1047/1770 train_time:101485ms step_avg:97.86ms
step:1048/1770 train_time:101585ms step_avg:97.87ms
step:1049/1770 train_time:101686ms step_avg:97.87ms
step:1050/1770 train_time:101787ms step_avg:97.87ms
step:1051/1770 train_time:101888ms step_avg:97.88ms
step:1052/1770 train_time:101989ms step_avg:97.88ms
step:1053/1770 train_time:102091ms step_avg:97.88ms
step:1054/1770 train_time:102192ms step_avg:97.89ms
step:1055/1770 train_time:102294ms step_avg:97.89ms
step:1056/1770 train_time:102397ms step_avg:97.89ms
step:1057/1770 train_time:102498ms step_avg:97.90ms
step:1058/1770 train_time:102601ms step_avg:97.90ms
step:1059/1770 train_time:102702ms step_avg:97.90ms
step:1060/1770 train_time:102803ms step_avg:97.91ms
step:1061/1770 train_time:102905ms step_avg:97.91ms
step:1062/1770 train_time:103007ms step_avg:97.92ms
step:1063/1770 train_time:103110ms step_avg:97.92ms
step:1064/1770 train_time:103212ms step_avg:97.92ms
step:1065/1770 train_time:103314ms step_avg:97.93ms
step:1066/1770 train_time:103415ms step_avg:97.93ms
step:1067/1770 train_time:103518ms step_avg:97.94ms
step:1068/1770 train_time:103621ms step_avg:97.94ms
step:1069/1770 train_time:103723ms step_avg:97.94ms
step:1070/1770 train_time:103824ms step_avg:97.95ms
step:1071/1770 train_time:103926ms step_avg:97.95ms
step:1072/1770 train_time:104027ms step_avg:97.95ms
step:1073/1770 train_time:104127ms step_avg:97.96ms
step:1074/1770 train_time:104229ms step_avg:97.96ms
step:1075/1770 train_time:104331ms step_avg:97.96ms
step:1076/1770 train_time:104432ms step_avg:97.97ms
step:1077/1770 train_time:104534ms step_avg:97.97ms
step:1078/1770 train_time:104635ms step_avg:97.97ms
step:1079/1770 train_time:104737ms step_avg:97.98ms
step:1080/1770 train_time:104839ms step_avg:97.98ms
step:1081/1770 train_time:104941ms step_avg:97.98ms
step:1082/1770 train_time:105042ms step_avg:97.99ms
step:1083/1770 train_time:105143ms step_avg:97.99ms
step:1084/1770 train_time:105244ms step_avg:97.99ms
step:1085/1770 train_time:105345ms step_avg:98.00ms
step:1086/1770 train_time:105446ms step_avg:98.00ms
step:1087/1770 train_time:105547ms step_avg:98.00ms
step:1088/1770 train_time:105648ms step_avg:98.00ms
step:1089/1770 train_time:105750ms step_avg:98.01ms
step:1090/1770 train_time:105854ms step_avg:98.01ms
step:1091/1770 train_time:105955ms step_avg:98.02ms
step:1092/1770 train_time:106058ms step_avg:98.02ms
step:1093/1770 train_time:106159ms step_avg:98.02ms
step:1094/1770 train_time:106261ms step_avg:98.03ms
step:1095/1770 train_time:106362ms step_avg:98.03ms
step:1096/1770 train_time:106463ms step_avg:98.03ms
step:1097/1770 train_time:106564ms step_avg:98.03ms
step:1098/1770 train_time:106666ms step_avg:98.04ms
step:1099/1770 train_time:106767ms step_avg:98.04ms
step:1100/1770 train_time:106869ms step_avg:98.04ms
step:1101/1770 train_time:106970ms step_avg:98.05ms
step:1102/1770 train_time:107073ms step_avg:98.05ms
step:1103/1770 train_time:107174ms step_avg:98.06ms
step:1104/1770 train_time:107276ms step_avg:98.06ms
step:1105/1770 train_time:107377ms step_avg:98.06ms
step:1106/1770 train_time:107479ms step_avg:98.06ms
step:1107/1770 train_time:107580ms step_avg:98.07ms
step:1108/1770 train_time:107682ms step_avg:98.07ms
step:1109/1770 train_time:107783ms step_avg:98.07ms
step:1110/1770 train_time:107884ms step_avg:98.08ms
step:1111/1770 train_time:107986ms step_avg:98.08ms
step:1112/1770 train_time:108088ms step_avg:98.08ms
step:1113/1770 train_time:108188ms step_avg:98.09ms
step:1114/1770 train_time:108290ms step_avg:98.09ms
step:1115/1770 train_time:108392ms step_avg:98.09ms
step:1116/1770 train_time:108495ms step_avg:98.10ms
step:1117/1770 train_time:108596ms step_avg:98.10ms
step:1118/1770 train_time:108699ms step_avg:98.10ms
step:1119/1770 train_time:108800ms step_avg:98.11ms
step:1120/1770 train_time:108902ms step_avg:98.11ms
step:1121/1770 train_time:109002ms step_avg:98.11ms
step:1122/1770 train_time:109103ms step_avg:98.11ms
step:1123/1770 train_time:109204ms step_avg:98.12ms
step:1124/1770 train_time:109305ms step_avg:98.12ms
step:1125/1770 train_time:109407ms step_avg:98.12ms
step:1125/1770 val_loss:3.4771 train_time:109506ms step_avg:98.21ms
step:1126/1770 train_time:109529ms step_avg:98.14ms
step:1127/1770 train_time:109615ms step_avg:98.13ms
step:1128/1770 train_time:109718ms step_avg:98.14ms
step:1129/1770 train_time:109819ms step_avg:98.14ms
step:1130/1770 train_time:109922ms step_avg:98.14ms
step:1131/1770 train_time:110024ms step_avg:98.15ms
step:1132/1770 train_time:110125ms step_avg:98.15ms
step:1133/1770 train_time:110226ms step_avg:98.15ms
step:1134/1770 train_time:110328ms step_avg:98.16ms
step:1135/1770 train_time:110429ms step_avg:98.16ms
step:1136/1770 train_time:110531ms step_avg:98.16ms
step:1137/1770 train_time:110633ms step_avg:98.17ms
step:1138/1770 train_time:110734ms step_avg:98.17ms
step:1139/1770 train_time:110836ms step_avg:98.17ms
step:1140/1770 train_time:110937ms step_avg:98.17ms
step:1141/1770 train_time:111038ms step_avg:98.18ms
step:1142/1770 train_time:111140ms step_avg:98.18ms
step:1143/1770 train_time:111243ms step_avg:98.18ms
step:1144/1770 train_time:111344ms step_avg:98.19ms
step:1145/1770 train_time:111445ms step_avg:98.19ms
step:1146/1770 train_time:111547ms step_avg:98.19ms
step:1147/1770 train_time:111649ms step_avg:98.20ms
step:1148/1770 train_time:111750ms step_avg:98.20ms
step:1149/1770 train_time:111851ms step_avg:98.20ms
step:1150/1770 train_time:111952ms step_avg:98.20ms
step:1151/1770 train_time:112054ms step_avg:98.21ms
step:1152/1770 train_time:112156ms step_avg:98.21ms
step:1153/1770 train_time:112257ms step_avg:98.21ms
step:1154/1770 train_time:112360ms step_avg:98.22ms
step:1155/1770 train_time:112462ms step_avg:98.22ms
step:1156/1770 train_time:112563ms step_avg:98.22ms
step:1157/1770 train_time:112666ms step_avg:98.23ms
step:1158/1770 train_time:112767ms step_avg:98.23ms
step:1159/1770 train_time:112868ms step_avg:98.23ms
step:1160/1770 train_time:112969ms step_avg:98.23ms
step:1161/1770 train_time:113070ms step_avg:98.24ms
step:1162/1770 train_time:113172ms step_avg:98.24ms
step:1163/1770 train_time:113274ms step_avg:98.24ms
step:1164/1770 train_time:113375ms step_avg:98.25ms
step:1165/1770 train_time:113477ms step_avg:98.25ms
step:1166/1770 train_time:113578ms step_avg:98.25ms
step:1167/1770 train_time:113680ms step_avg:98.25ms
step:1168/1770 train_time:113782ms step_avg:98.26ms
step:1169/1770 train_time:113884ms step_avg:98.26ms
step:1170/1770 train_time:113985ms step_avg:98.26ms
step:1171/1770 train_time:114086ms step_avg:98.27ms
step:1172/1770 train_time:114187ms step_avg:98.27ms
step:1173/1770 train_time:114288ms step_avg:98.27ms
step:1174/1770 train_time:114389ms step_avg:98.27ms
step:1175/1770 train_time:114490ms step_avg:98.28ms
step:1176/1770 train_time:114592ms step_avg:98.28ms
step:1177/1770 train_time:114694ms step_avg:98.28ms
step:1178/1770 train_time:114796ms step_avg:98.28ms
step:1179/1770 train_time:114897ms step_avg:98.29ms
step:1180/1770 train_time:114999ms step_avg:98.29ms
step:1181/1770 train_time:115101ms step_avg:98.29ms
step:1182/1770 train_time:115203ms step_avg:98.30ms
step:1183/1770 train_time:115305ms step_avg:98.30ms
step:1184/1770 train_time:115409ms step_avg:98.30ms
step:1185/1770 train_time:115511ms step_avg:98.31ms
step:1186/1770 train_time:115614ms step_avg:98.31ms
step:1187/1770 train_time:115719ms step_avg:98.32ms
step:1188/1770 train_time:115821ms step_avg:98.32ms
step:1189/1770 train_time:115923ms step_avg:98.32ms
step:1190/1770 train_time:116025ms step_avg:98.33ms
step:1191/1770 train_time:116128ms step_avg:98.33ms
step:1192/1770 train_time:116230ms step_avg:98.33ms
step:1193/1770 train_time:116333ms step_avg:98.34ms
step:1194/1770 train_time:116436ms step_avg:98.34ms
step:1195/1770 train_time:116539ms step_avg:98.35ms
step:1196/1770 train_time:116644ms step_avg:98.35ms
step:1197/1770 train_time:116746ms step_avg:98.35ms
step:1198/1770 train_time:116848ms step_avg:98.36ms
step:1199/1770 train_time:116950ms step_avg:98.36ms
step:1200/1770 train_time:117054ms step_avg:98.36ms
step:1201/1770 train_time:117158ms step_avg:98.37ms
step:1202/1770 train_time:117260ms step_avg:98.37ms
step:1203/1770 train_time:117363ms step_avg:98.38ms
step:1204/1770 train_time:117465ms step_avg:98.38ms
step:1205/1770 train_time:117567ms step_avg:98.38ms
step:1206/1770 train_time:117669ms step_avg:98.39ms
step:1207/1770 train_time:117771ms step_avg:98.39ms
step:1208/1770 train_time:117873ms step_avg:98.39ms
step:1209/1770 train_time:117976ms step_avg:98.40ms
step:1210/1770 train_time:118078ms step_avg:98.40ms
step:1211/1770 train_time:118182ms step_avg:98.40ms
step:1212/1770 train_time:118287ms step_avg:98.41ms
step:1213/1770 train_time:118389ms step_avg:98.41ms
step:1214/1770 train_time:118490ms step_avg:98.41ms
step:1215/1770 train_time:118593ms step_avg:98.42ms
step:1216/1770 train_time:118699ms step_avg:98.42ms
step:1217/1770 train_time:118803ms step_avg:98.43ms
step:1218/1770 train_time:118905ms step_avg:98.43ms
step:1219/1770 train_time:119008ms step_avg:98.43ms
step:1220/1770 train_time:119110ms step_avg:98.44ms
step:1221/1770 train_time:119213ms step_avg:98.44ms
step:1222/1770 train_time:119317ms step_avg:98.45ms
step:1223/1770 train_time:119419ms step_avg:98.45ms
step:1224/1770 train_time:119524ms step_avg:98.45ms
step:1225/1770 train_time:119626ms step_avg:98.46ms
step:1226/1770 train_time:119729ms step_avg:98.46ms
step:1227/1770 train_time:119833ms step_avg:98.47ms
step:1228/1770 train_time:119938ms step_avg:98.47ms
step:1229/1770 train_time:120040ms step_avg:98.47ms
step:1230/1770 train_time:120144ms step_avg:98.48ms
step:1231/1770 train_time:120246ms step_avg:98.48ms
step:1232/1770 train_time:120348ms step_avg:98.48ms
step:1233/1770 train_time:120450ms step_avg:98.49ms
step:1234/1770 train_time:120553ms step_avg:98.49ms
step:1235/1770 train_time:120655ms step_avg:98.49ms
step:1236/1770 train_time:120758ms step_avg:98.50ms
step:1237/1770 train_time:120860ms step_avg:98.50ms
step:1238/1770 train_time:120964ms step_avg:98.51ms
step:1239/1770 train_time:121067ms step_avg:98.51ms
step:1240/1770 train_time:121169ms step_avg:98.51ms
step:1241/1770 train_time:121272ms step_avg:98.51ms
step:1242/1770 train_time:121374ms step_avg:98.52ms
step:1243/1770 train_time:121477ms step_avg:98.52ms
step:1244/1770 train_time:121579ms step_avg:98.52ms
step:1245/1770 train_time:121681ms step_avg:98.53ms
step:1246/1770 train_time:121784ms step_avg:98.53ms
step:1247/1770 train_time:121886ms step_avg:98.53ms
step:1248/1770 train_time:121989ms step_avg:98.54ms
step:1249/1770 train_time:122092ms step_avg:98.54ms
step:1250/1770 train_time:122194ms step_avg:98.54ms
step:1250/1770 val_loss:3.4295 train_time:122297ms step_avg:98.63ms
step:1251/1770 train_time:122319ms step_avg:98.56ms
step:1252/1770 train_time:122410ms step_avg:98.56ms
step:1253/1770 train_time:122514ms step_avg:98.56ms
step:1254/1770 train_time:122617ms step_avg:98.57ms
step:1255/1770 train_time:122722ms step_avg:98.57ms
step:1256/1770 train_time:122824ms step_avg:98.57ms
step:1257/1770 train_time:122927ms step_avg:98.58ms
step:1258/1770 train_time:123030ms step_avg:98.58ms
step:1259/1770 train_time:123133ms step_avg:98.59ms
step:1260/1770 train_time:123236ms step_avg:98.59ms
step:1261/1770 train_time:123341ms step_avg:98.59ms
step:1262/1770 train_time:123444ms step_avg:98.60ms
step:1263/1770 train_time:123546ms step_avg:98.60ms
step:1264/1770 train_time:123650ms step_avg:98.60ms
step:1265/1770 train_time:123752ms step_avg:98.61ms
step:1266/1770 train_time:123855ms step_avg:98.61ms
step:1267/1770 train_time:123958ms step_avg:98.61ms
step:1268/1770 train_time:124061ms step_avg:98.62ms
step:1269/1770 train_time:124163ms step_avg:98.62ms
step:1270/1770 train_time:124266ms step_avg:98.62ms
step:1271/1770 train_time:124368ms step_avg:98.63ms
step:1272/1770 train_time:124470ms step_avg:98.63ms
step:1273/1770 train_time:124573ms step_avg:98.63ms
step:1274/1770 train_time:124677ms step_avg:98.64ms
step:1275/1770 train_time:124783ms step_avg:98.64ms
step:1276/1770 train_time:124883ms step_avg:98.64ms
step:1277/1770 train_time:124984ms step_avg:98.65ms
step:1278/1770 train_time:125088ms step_avg:98.65ms
step:1279/1770 train_time:125191ms step_avg:98.65ms
step:1280/1770 train_time:125295ms step_avg:98.66ms
step:1281/1770 train_time:125397ms step_avg:98.66ms
step:1282/1770 train_time:125501ms step_avg:98.66ms
step:1283/1770 train_time:125603ms step_avg:98.67ms
step:1284/1770 train_time:125706ms step_avg:98.67ms
step:1285/1770 train_time:125808ms step_avg:98.67ms
step:1286/1770 train_time:125912ms step_avg:98.68ms
step:1287/1770 train_time:126017ms step_avg:98.68ms
step:1288/1770 train_time:126120ms step_avg:98.69ms
step:1289/1770 train_time:126223ms step_avg:98.69ms
step:1290/1770 train_time:126324ms step_avg:98.69ms
step:1291/1770 train_time:126427ms step_avg:98.69ms
step:1292/1770 train_time:126529ms step_avg:98.70ms
step:1293/1770 train_time:126632ms step_avg:98.70ms
step:1294/1770 train_time:126734ms step_avg:98.70ms
step:1295/1770 train_time:126837ms step_avg:98.71ms
step:1296/1770 train_time:126940ms step_avg:98.71ms
step:1297/1770 train_time:127042ms step_avg:98.71ms
step:1298/1770 train_time:127145ms step_avg:98.71ms
step:1299/1770 train_time:127247ms step_avg:98.72ms
step:1300/1770 train_time:127349ms step_avg:98.72ms
step:1301/1770 train_time:127452ms step_avg:98.72ms
step:1302/1770 train_time:127556ms step_avg:98.73ms
step:1303/1770 train_time:127658ms step_avg:98.73ms
step:1304/1770 train_time:127761ms step_avg:98.73ms
step:1305/1770 train_time:127863ms step_avg:98.74ms
step:1306/1770 train_time:127965ms step_avg:98.74ms
step:1307/1770 train_time:128067ms step_avg:98.74ms
step:1308/1770 train_time:128170ms step_avg:98.74ms
step:1309/1770 train_time:128273ms step_avg:98.75ms
step:1310/1770 train_time:128377ms step_avg:98.75ms
step:1311/1770 train_time:128480ms step_avg:98.75ms
step:1312/1770 train_time:128583ms step_avg:98.76ms
step:1313/1770 train_time:128684ms step_avg:98.76ms
step:1314/1770 train_time:128786ms step_avg:98.76ms
step:1315/1770 train_time:128890ms step_avg:98.77ms
step:1316/1770 train_time:128992ms step_avg:98.77ms
step:1317/1770 train_time:129094ms step_avg:98.77ms
step:1318/1770 train_time:129200ms step_avg:98.78ms
step:1319/1770 train_time:129303ms step_avg:98.78ms
step:1320/1770 train_time:129405ms step_avg:98.78ms
step:1321/1770 train_time:129507ms step_avg:98.79ms
step:1322/1770 train_time:129610ms step_avg:98.79ms
step:1323/1770 train_time:129714ms step_avg:98.79ms
step:1324/1770 train_time:129818ms step_avg:98.80ms
step:1325/1770 train_time:129922ms step_avg:98.80ms
step:1326/1770 train_time:130024ms step_avg:98.80ms
step:1327/1770 train_time:130130ms step_avg:98.81ms
step:1328/1770 train_time:130232ms step_avg:98.81ms
step:1329/1770 train_time:130335ms step_avg:98.81ms
step:1330/1770 train_time:130437ms step_avg:98.82ms
step:1331/1770 train_time:130540ms step_avg:98.82ms
step:1332/1770 train_time:130643ms step_avg:98.82ms
step:1333/1770 train_time:130745ms step_avg:98.82ms
step:1334/1770 train_time:130847ms step_avg:98.83ms
step:1335/1770 train_time:130949ms step_avg:98.83ms
step:1336/1770 train_time:131051ms step_avg:98.83ms
step:1337/1770 train_time:131154ms step_avg:98.84ms
step:1338/1770 train_time:131256ms step_avg:98.84ms
step:1339/1770 train_time:131360ms step_avg:98.84ms
step:1340/1770 train_time:131464ms step_avg:98.85ms
step:1341/1770 train_time:131566ms step_avg:98.85ms
step:1342/1770 train_time:131669ms step_avg:98.85ms
step:1343/1770 train_time:131772ms step_avg:98.85ms
step:1344/1770 train_time:131876ms step_avg:98.86ms
step:1345/1770 train_time:131978ms step_avg:98.86ms
step:1346/1770 train_time:132083ms step_avg:98.86ms
step:1347/1770 train_time:132185ms step_avg:98.87ms
step:1348/1770 train_time:132290ms step_avg:98.87ms
step:1349/1770 train_time:132392ms step_avg:98.87ms
step:1350/1770 train_time:132495ms step_avg:98.88ms
step:1351/1770 train_time:132598ms step_avg:98.88ms
step:1352/1770 train_time:132702ms step_avg:98.88ms
step:1353/1770 train_time:132804ms step_avg:98.89ms
step:1354/1770 train_time:132906ms step_avg:98.89ms
step:1355/1770 train_time:133009ms step_avg:98.89ms
step:1356/1770 train_time:133111ms step_avg:98.89ms
step:1357/1770 train_time:133214ms step_avg:98.90ms
step:1358/1770 train_time:133320ms step_avg:98.90ms
step:1359/1770 train_time:133422ms step_avg:98.90ms
step:1360/1770 train_time:133525ms step_avg:98.91ms
step:1361/1770 train_time:133629ms step_avg:98.91ms
step:1362/1770 train_time:133732ms step_avg:98.91ms
step:1363/1770 train_time:133836ms step_avg:98.92ms
step:1364/1770 train_time:133940ms step_avg:98.92ms
step:1365/1770 train_time:134041ms step_avg:98.92ms
step:1366/1770 train_time:134143ms step_avg:98.93ms
step:1367/1770 train_time:134246ms step_avg:98.93ms
step:1368/1770 train_time:134348ms step_avg:98.93ms
step:1369/1770 train_time:134451ms step_avg:98.93ms
step:1370/1770 train_time:134554ms step_avg:98.94ms
step:1371/1770 train_time:134657ms step_avg:98.94ms
step:1372/1770 train_time:134759ms step_avg:98.94ms
step:1373/1770 train_time:134862ms step_avg:98.94ms
step:1374/1770 train_time:134965ms step_avg:98.95ms
step:1375/1770 train_time:135068ms step_avg:98.95ms
step:1375/1770 val_loss:3.3884 train_time:135169ms step_avg:99.03ms
step:1376/1770 train_time:135191ms step_avg:98.97ms
step:1377/1770 train_time:135282ms step_avg:98.96ms
step:1378/1770 train_time:135384ms step_avg:98.97ms
step:1379/1770 train_time:135486ms step_avg:98.97ms
step:1380/1770 train_time:135589ms step_avg:98.97ms
step:1381/1770 train_time:135691ms step_avg:98.97ms
step:1382/1770 train_time:135794ms step_avg:98.98ms
step:1383/1770 train_time:135898ms step_avg:98.98ms
step:1384/1770 train_time:136000ms step_avg:98.98ms
step:1385/1770 train_time:136103ms step_avg:98.98ms
step:1386/1770 train_time:136206ms step_avg:98.99ms
step:1387/1770 train_time:136311ms step_avg:98.99ms
step:1388/1770 train_time:136413ms step_avg:98.99ms
step:1389/1770 train_time:136517ms step_avg:99.00ms
step:1390/1770 train_time:136619ms step_avg:99.00ms
step:1391/1770 train_time:136721ms step_avg:99.00ms
step:1392/1770 train_time:136824ms step_avg:99.00ms
step:1393/1770 train_time:136926ms step_avg:99.01ms
step:1394/1770 train_time:137029ms step_avg:99.01ms
step:1395/1770 train_time:137134ms step_avg:99.01ms
step:1396/1770 train_time:137238ms step_avg:99.02ms
step:1397/1770 train_time:137346ms step_avg:99.02ms
step:1398/1770 train_time:137443ms step_avg:99.02ms
step:1399/1770 train_time:137545ms step_avg:99.02ms
step:1400/1770 train_time:137649ms step_avg:99.03ms
step:1401/1770 train_time:137752ms step_avg:99.03ms
step:1402/1770 train_time:137855ms step_avg:99.03ms
step:1403/1770 train_time:137958ms step_avg:99.04ms
step:1404/1770 train_time:138061ms step_avg:99.04ms
step:1405/1770 train_time:138163ms step_avg:99.04ms
step:1406/1770 train_time:138267ms step_avg:99.04ms
step:1407/1770 train_time:138369ms step_avg:99.05ms
step:1408/1770 train_time:138472ms step_avg:99.05ms
step:1409/1770 train_time:138576ms step_avg:99.05ms
step:1410/1770 train_time:138678ms step_avg:99.06ms
step:1411/1770 train_time:138781ms step_avg:99.06ms
step:1412/1770 train_time:138883ms step_avg:99.06ms
step:1413/1770 train_time:138985ms step_avg:99.06ms
step:1414/1770 train_time:139089ms step_avg:99.07ms
step:1415/1770 train_time:139192ms step_avg:99.07ms
step:1416/1770 train_time:139295ms step_avg:99.07ms
step:1417/1770 train_time:139398ms step_avg:99.07ms
step:1418/1770 train_time:139501ms step_avg:99.08ms
step:1419/1770 train_time:139604ms step_avg:99.08ms
step:1420/1770 train_time:139706ms step_avg:99.08ms
step:1421/1770 train_time:139809ms step_avg:99.09ms
step:1422/1770 train_time:139912ms step_avg:99.09ms
step:1423/1770 train_time:140015ms step_avg:99.09ms
step:1424/1770 train_time:140118ms step_avg:99.09ms
step:1425/1770 train_time:140220ms step_avg:99.10ms
step:1426/1770 train_time:140323ms step_avg:99.10ms
step:1427/1770 train_time:140425ms step_avg:99.10ms
step:1428/1770 train_time:140529ms step_avg:99.10ms
step:1429/1770 train_time:140632ms step_avg:99.11ms
step:1430/1770 train_time:140734ms step_avg:99.11ms
step:1431/1770 train_time:140838ms step_avg:99.11ms
step:1432/1770 train_time:140939ms step_avg:99.11ms
step:1433/1770 train_time:141042ms step_avg:99.12ms
step:1434/1770 train_time:141144ms step_avg:99.12ms
step:1435/1770 train_time:141247ms step_avg:99.12ms
step:1436/1770 train_time:141354ms step_avg:99.13ms
step:1437/1770 train_time:141453ms step_avg:99.13ms
step:1438/1770 train_time:141555ms step_avg:99.13ms
step:1439/1770 train_time:141658ms step_avg:99.13ms
step:1440/1770 train_time:141760ms step_avg:99.13ms
step:1441/1770 train_time:141865ms step_avg:99.14ms
step:1442/1770 train_time:141968ms step_avg:99.14ms
step:1443/1770 train_time:142071ms step_avg:99.14ms
step:1444/1770 train_time:142175ms step_avg:99.15ms
step:1445/1770 train_time:142278ms step_avg:99.15ms
step:1446/1770 train_time:142381ms step_avg:99.15ms
step:1447/1770 train_time:142485ms step_avg:99.15ms
step:1448/1770 train_time:142589ms step_avg:99.16ms
step:1449/1770 train_time:142694ms step_avg:99.16ms
step:1450/1770 train_time:142797ms step_avg:99.16ms
step:1451/1770 train_time:142901ms step_avg:99.17ms
step:1452/1770 train_time:143005ms step_avg:99.17ms
step:1453/1770 train_time:143108ms step_avg:99.17ms
step:1454/1770 train_time:143211ms step_avg:99.18ms
step:1455/1770 train_time:143316ms step_avg:99.18ms
step:1456/1770 train_time:143421ms step_avg:99.18ms
step:1457/1770 train_time:143525ms step_avg:99.19ms
step:1458/1770 train_time:143630ms step_avg:99.19ms
step:1459/1770 train_time:143735ms step_avg:99.20ms
step:1460/1770 train_time:143838ms step_avg:99.20ms
step:1461/1770 train_time:143941ms step_avg:99.20ms
step:1462/1770 train_time:144045ms step_avg:99.20ms
step:1463/1770 train_time:144149ms step_avg:99.21ms
step:1464/1770 train_time:144255ms step_avg:99.21ms
step:1465/1770 train_time:144359ms step_avg:99.22ms
step:1466/1770 train_time:144463ms step_avg:99.22ms
step:1467/1770 train_time:144569ms step_avg:99.22ms
step:1468/1770 train_time:144673ms step_avg:99.23ms
step:1469/1770 train_time:144777ms step_avg:99.23ms
step:1470/1770 train_time:144879ms step_avg:99.23ms
step:1471/1770 train_time:144983ms step_avg:99.24ms
step:1472/1770 train_time:145087ms step_avg:99.24ms
step:1473/1770 train_time:145192ms step_avg:99.24ms
step:1474/1770 train_time:145298ms step_avg:99.25ms
step:1475/1770 train_time:145401ms step_avg:99.25ms
step:1476/1770 train_time:145505ms step_avg:99.25ms
step:1477/1770 train_time:145611ms step_avg:99.26ms
step:1478/1770 train_time:145716ms step_avg:99.26ms
step:1479/1770 train_time:145819ms step_avg:99.26ms
step:1480/1770 train_time:145923ms step_avg:99.27ms
step:1481/1770 train_time:146030ms step_avg:99.27ms
step:1482/1770 train_time:146134ms step_avg:99.28ms
step:1483/1770 train_time:146237ms step_avg:99.28ms
step:1484/1770 train_time:146340ms step_avg:99.28ms
step:1485/1770 train_time:146443ms step_avg:99.28ms
step:1486/1770 train_time:146547ms step_avg:99.29ms
step:1487/1770 train_time:146651ms step_avg:99.29ms
step:1488/1770 train_time:146756ms step_avg:99.29ms
step:1489/1770 train_time:146861ms step_avg:99.30ms
step:1490/1770 train_time:146965ms step_avg:99.30ms
step:1491/1770 train_time:147069ms step_avg:99.30ms
step:1492/1770 train_time:147174ms step_avg:99.31ms
step:1493/1770 train_time:147280ms step_avg:99.31ms
step:1494/1770 train_time:147388ms step_avg:99.32ms
step:1495/1770 train_time:147491ms step_avg:99.32ms
step:1496/1770 train_time:147594ms step_avg:99.32ms
step:1497/1770 train_time:147699ms step_avg:99.33ms
step:1498/1770 train_time:147802ms step_avg:99.33ms
step:1499/1770 train_time:147905ms step_avg:99.33ms
step:1500/1770 train_time:148008ms step_avg:99.33ms
step:1500/1770 val_loss:3.3523 train_time:148109ms step_avg:99.40ms
step:1501/1770 train_time:148130ms step_avg:99.35ms
step:1502/1770 train_time:148220ms step_avg:99.34ms
step:1503/1770 train_time:148324ms step_avg:99.35ms
step:1504/1770 train_time:148428ms step_avg:99.35ms
step:1505/1770 train_time:148534ms step_avg:99.35ms
step:1506/1770 train_time:148638ms step_avg:99.36ms
step:1507/1770 train_time:148743ms step_avg:99.36ms
step:1508/1770 train_time:148849ms step_avg:99.36ms
step:1509/1770 train_time:148952ms step_avg:99.37ms
step:1510/1770 train_time:149055ms step_avg:99.37ms
step:1511/1770 train_time:149160ms step_avg:99.37ms
step:1512/1770 train_time:149265ms step_avg:99.38ms
step:1513/1770 train_time:149369ms step_avg:99.38ms
step:1514/1770 train_time:149473ms step_avg:99.38ms
step:1515/1770 train_time:149576ms step_avg:99.39ms
step:1516/1770 train_time:149681ms step_avg:99.39ms
step:1517/1770 train_time:149785ms step_avg:99.39ms
step:1518/1770 train_time:149891ms step_avg:99.40ms
step:1519/1770 train_time:149993ms step_avg:99.40ms
step:1520/1770 train_time:150099ms step_avg:99.40ms
step:1521/1770 train_time:150204ms step_avg:99.41ms
step:1522/1770 train_time:150307ms step_avg:99.41ms
step:1523/1770 train_time:150412ms step_avg:99.41ms
step:1524/1770 train_time:150515ms step_avg:99.42ms
step:1525/1770 train_time:150618ms step_avg:99.42ms
step:1526/1770 train_time:150724ms step_avg:99.42ms
step:1527/1770 train_time:150827ms step_avg:99.42ms
step:1528/1770 train_time:150933ms step_avg:99.43ms
step:1529/1770 train_time:151036ms step_avg:99.43ms
step:1530/1770 train_time:151140ms step_avg:99.43ms
step:1531/1770 train_time:151244ms step_avg:99.44ms
step:1532/1770 train_time:151348ms step_avg:99.44ms
step:1533/1770 train_time:151453ms step_avg:99.44ms
step:1534/1770 train_time:151557ms step_avg:99.45ms
step:1535/1770 train_time:151662ms step_avg:99.45ms
step:1536/1770 train_time:151765ms step_avg:99.45ms
step:1537/1770 train_time:151869ms step_avg:99.46ms
step:1538/1770 train_time:151974ms step_avg:99.46ms
step:1539/1770 train_time:152077ms step_avg:99.46ms
step:1540/1770 train_time:152184ms step_avg:99.47ms
step:1541/1770 train_time:152288ms step_avg:99.47ms
step:1542/1770 train_time:152393ms step_avg:99.47ms
step:1543/1770 train_time:152496ms step_avg:99.48ms
step:1544/1770 train_time:152603ms step_avg:99.48ms
step:1545/1770 train_time:152707ms step_avg:99.48ms
step:1546/1770 train_time:152811ms step_avg:99.49ms
step:1547/1770 train_time:152915ms step_avg:99.49ms
step:1548/1770 train_time:153019ms step_avg:99.49ms
step:1549/1770 train_time:153124ms step_avg:99.50ms
step:1550/1770 train_time:153229ms step_avg:99.50ms
step:1551/1770 train_time:153332ms step_avg:99.50ms
step:1552/1770 train_time:153438ms step_avg:99.51ms
step:1553/1770 train_time:153542ms step_avg:99.51ms
step:1554/1770 train_time:153645ms step_avg:99.51ms
step:1555/1770 train_time:153749ms step_avg:99.51ms
step:1556/1770 train_time:153852ms step_avg:99.52ms
step:1557/1770 train_time:153955ms step_avg:99.52ms
step:1558/1770 train_time:154059ms step_avg:99.52ms
step:1559/1770 train_time:154164ms step_avg:99.52ms
step:1560/1770 train_time:154267ms step_avg:99.53ms
step:1561/1770 train_time:154373ms step_avg:99.53ms
step:1562/1770 train_time:154476ms step_avg:99.53ms
step:1563/1770 train_time:154581ms step_avg:99.54ms
step:1564/1770 train_time:154684ms step_avg:99.54ms
step:1565/1770 train_time:154788ms step_avg:99.54ms
step:1566/1770 train_time:154892ms step_avg:99.54ms
step:1567/1770 train_time:154996ms step_avg:99.55ms
step:1568/1770 train_time:155100ms step_avg:99.55ms
step:1569/1770 train_time:155207ms step_avg:99.56ms
step:1570/1770 train_time:155311ms step_avg:99.56ms
step:1571/1770 train_time:155415ms step_avg:99.56ms
step:1572/1770 train_time:155520ms step_avg:99.56ms
step:1573/1770 train_time:155626ms step_avg:99.57ms
step:1574/1770 train_time:155730ms step_avg:99.57ms
step:1575/1770 train_time:155832ms step_avg:99.57ms
step:1576/1770 train_time:155937ms step_avg:99.58ms
step:1577/1770 train_time:156043ms step_avg:99.58ms
step:1578/1770 train_time:156149ms step_avg:99.58ms
step:1579/1770 train_time:156252ms step_avg:99.59ms
step:1580/1770 train_time:156356ms step_avg:99.59ms
step:1581/1770 train_time:156463ms step_avg:99.59ms
step:1582/1770 train_time:156567ms step_avg:99.60ms
step:1583/1770 train_time:156671ms step_avg:99.60ms
step:1584/1770 train_time:156776ms step_avg:99.60ms
step:1585/1770 train_time:156880ms step_avg:99.61ms
step:1586/1770 train_time:156988ms step_avg:99.61ms
step:1587/1770 train_time:157093ms step_avg:99.61ms
step:1588/1770 train_time:157197ms step_avg:99.62ms
step:1589/1770 train_time:157303ms step_avg:99.62ms
step:1590/1770 train_time:157407ms step_avg:99.62ms
step:1591/1770 train_time:157511ms step_avg:99.63ms
step:1592/1770 train_time:157616ms step_avg:99.63ms
step:1593/1770 train_time:157720ms step_avg:99.63ms
step:1594/1770 train_time:157824ms step_avg:99.64ms
step:1595/1770 train_time:157927ms step_avg:99.64ms
step:1596/1770 train_time:158033ms step_avg:99.64ms
step:1597/1770 train_time:158137ms step_avg:99.65ms
step:1598/1770 train_time:158241ms step_avg:99.65ms
step:1599/1770 train_time:158346ms step_avg:99.65ms
step:1600/1770 train_time:158452ms step_avg:99.66ms
step:1601/1770 train_time:158557ms step_avg:99.66ms
step:1602/1770 train_time:158663ms step_avg:99.66ms
step:1603/1770 train_time:158767ms step_avg:99.67ms
step:1604/1770 train_time:158870ms step_avg:99.67ms
step:1605/1770 train_time:158973ms step_avg:99.67ms
step:1606/1770 train_time:159077ms step_avg:99.67ms
step:1607/1770 train_time:159184ms step_avg:99.68ms
step:1608/1770 train_time:159288ms step_avg:99.68ms
step:1609/1770 train_time:159392ms step_avg:99.68ms
step:1610/1770 train_time:159497ms step_avg:99.69ms
step:1611/1770 train_time:159604ms step_avg:99.69ms
step:1612/1770 train_time:159709ms step_avg:99.69ms
step:1613/1770 train_time:159813ms step_avg:99.70ms
step:1614/1770 train_time:159917ms step_avg:99.70ms
step:1615/1770 train_time:160021ms step_avg:99.70ms
step:1616/1770 train_time:160126ms step_avg:99.71ms
step:1617/1770 train_time:160232ms step_avg:99.71ms
step:1618/1770 train_time:160337ms step_avg:99.71ms
step:1619/1770 train_time:160441ms step_avg:99.71ms
step:1620/1770 train_time:160546ms step_avg:99.72ms
step:1621/1770 train_time:160649ms step_avg:99.72ms
step:1622/1770 train_time:160754ms step_avg:99.72ms
step:1623/1770 train_time:160860ms step_avg:99.73ms
step:1624/1770 train_time:160963ms step_avg:99.73ms
step:1625/1770 train_time:161066ms step_avg:99.73ms
step:1625/1770 val_loss:3.3214 train_time:161169ms step_avg:99.79ms
step:1626/1770 train_time:161190ms step_avg:99.75ms
step:1627/1770 train_time:161279ms step_avg:99.74ms
step:1628/1770 train_time:161382ms step_avg:99.74ms
step:1629/1770 train_time:161485ms step_avg:99.74ms
step:1630/1770 train_time:161590ms step_avg:99.75ms
step:1631/1770 train_time:161694ms step_avg:99.75ms
step:1632/1770 train_time:161797ms step_avg:99.75ms
step:1633/1770 train_time:161901ms step_avg:99.75ms
step:1634/1770 train_time:162005ms step_avg:99.76ms
step:1635/1770 train_time:162109ms step_avg:99.76ms
step:1636/1770 train_time:162213ms step_avg:99.76ms
step:1637/1770 train_time:162318ms step_avg:99.77ms
step:1638/1770 train_time:162421ms step_avg:99.77ms
step:1639/1770 train_time:162525ms step_avg:99.77ms
step:1640/1770 train_time:162630ms step_avg:99.77ms
step:1641/1770 train_time:162734ms step_avg:99.78ms
step:1642/1770 train_time:162838ms step_avg:99.78ms
step:1643/1770 train_time:162941ms step_avg:99.78ms
step:1644/1770 train_time:163046ms step_avg:99.78ms
step:1645/1770 train_time:163151ms step_avg:99.79ms
step:1646/1770 train_time:163256ms step_avg:99.79ms
step:1647/1770 train_time:163361ms step_avg:99.79ms
step:1648/1770 train_time:163464ms step_avg:99.79ms
step:1649/1770 train_time:163568ms step_avg:99.80ms
step:1650/1770 train_time:163672ms step_avg:99.80ms
step:1651/1770 train_time:163775ms step_avg:99.80ms
step:1652/1770 train_time:163879ms step_avg:99.80ms
step:1653/1770 train_time:163983ms step_avg:99.81ms
step:1654/1770 train_time:164091ms step_avg:99.81ms
step:1655/1770 train_time:164197ms step_avg:99.82ms
step:1656/1770 train_time:164301ms step_avg:99.82ms
step:1657/1770 train_time:164407ms step_avg:99.82ms
step:1658/1770 train_time:164511ms step_avg:99.82ms
step:1659/1770 train_time:164617ms step_avg:99.83ms
step:1660/1770 train_time:164721ms step_avg:99.83ms
step:1661/1770 train_time:164826ms step_avg:99.83ms
step:1662/1770 train_time:164931ms step_avg:99.84ms
step:1663/1770 train_time:165034ms step_avg:99.84ms
step:1664/1770 train_time:165142ms step_avg:99.84ms
step:1665/1770 train_time:165240ms step_avg:99.84ms
step:1666/1770 train_time:165344ms step_avg:99.85ms
step:1667/1770 train_time:165448ms step_avg:99.85ms
step:1668/1770 train_time:165552ms step_avg:99.85ms
step:1669/1770 train_time:165656ms step_avg:99.85ms
step:1670/1770 train_time:165759ms step_avg:99.85ms
step:1671/1770 train_time:165864ms step_avg:99.86ms
step:1672/1770 train_time:165968ms step_avg:99.86ms
step:1673/1770 train_time:166074ms step_avg:99.86ms
step:1674/1770 train_time:166177ms step_avg:99.87ms
step:1675/1770 train_time:166280ms step_avg:99.87ms
step:1676/1770 train_time:166386ms step_avg:99.87ms
step:1677/1770 train_time:166494ms step_avg:99.88ms
step:1678/1770 train_time:166597ms step_avg:99.88ms
step:1679/1770 train_time:166701ms step_avg:99.88ms
step:1680/1770 train_time:166804ms step_avg:99.88ms
step:1681/1770 train_time:166909ms step_avg:99.89ms
step:1682/1770 train_time:167016ms step_avg:99.89ms
step:1683/1770 train_time:167119ms step_avg:99.89ms
step:1684/1770 train_time:167222ms step_avg:99.89ms
step:1685/1770 train_time:167326ms step_avg:99.90ms
step:1686/1770 train_time:167431ms step_avg:99.90ms
step:1687/1770 train_time:167537ms step_avg:99.90ms
step:1688/1770 train_time:167640ms step_avg:99.90ms
step:1689/1770 train_time:167745ms step_avg:99.91ms
step:1690/1770 train_time:167848ms step_avg:99.91ms
step:1691/1770 train_time:167953ms step_avg:99.91ms
step:1692/1770 train_time:168056ms step_avg:99.91ms
step:1693/1770 train_time:168162ms step_avg:99.92ms
step:1694/1770 train_time:168265ms step_avg:99.92ms
step:1695/1770 train_time:168369ms step_avg:99.92ms
step:1696/1770 train_time:168474ms step_avg:99.93ms
step:1697/1770 train_time:168581ms step_avg:99.93ms
step:1698/1770 train_time:168685ms step_avg:99.93ms
step:1699/1770 train_time:168789ms step_avg:99.93ms
step:1700/1770 train_time:168893ms step_avg:99.94ms
step:1701/1770 train_time:168997ms step_avg:99.94ms
step:1702/1770 train_time:169101ms step_avg:99.94ms
step:1703/1770 train_time:169205ms step_avg:99.94ms
step:1704/1770 train_time:169309ms step_avg:99.95ms
step:1705/1770 train_time:169414ms step_avg:99.95ms
step:1706/1770 train_time:169516ms step_avg:99.95ms
step:1707/1770 train_time:169621ms step_avg:99.95ms
step:1708/1770 train_time:169725ms step_avg:99.96ms
step:1709/1770 train_time:169831ms step_avg:99.96ms
step:1710/1770 train_time:169939ms step_avg:99.96ms
step:1711/1770 train_time:170050ms step_avg:99.97ms
step:1712/1770 train_time:170151ms step_avg:99.97ms
step:1713/1770 train_time:170255ms step_avg:99.97ms
step:1714/1770 train_time:170359ms step_avg:99.98ms
step:1715/1770 train_time:170462ms step_avg:99.98ms
step:1716/1770 train_time:170567ms step_avg:99.98ms
step:1717/1770 train_time:170672ms step_avg:99.98ms
step:1718/1770 train_time:170777ms step_avg:99.99ms
step:1719/1770 train_time:170883ms step_avg:99.99ms
step:1720/1770 train_time:170989ms step_avg:99.99ms
step:1721/1770 train_time:171093ms step_avg:100.00ms
step:1722/1770 train_time:171200ms step_avg:100.00ms
step:1723/1770 train_time:171306ms step_avg:100.00ms
step:1724/1770 train_time:171414ms step_avg:100.01ms
step:1725/1770 train_time:171521ms step_avg:100.01ms
step:1726/1770 train_time:171628ms step_avg:100.02ms
step:1727/1770 train_time:171733ms step_avg:100.02ms
step:1728/1770 train_time:171839ms step_avg:100.02ms
step:1729/1770 train_time:171944ms step_avg:100.03ms
step:1730/1770 train_time:172053ms step_avg:100.03ms
step:1731/1770 train_time:172156ms step_avg:100.03ms
step:1732/1770 train_time:172260ms step_avg:100.04ms
step:1733/1770 train_time:172367ms step_avg:100.04ms
step:1734/1770 train_time:172472ms step_avg:100.04ms
step:1735/1770 train_time:172578ms step_avg:100.05ms
step:1736/1770 train_time:172682ms step_avg:100.05ms
step:1737/1770 train_time:172787ms step_avg:100.05ms
step:1738/1770 train_time:172892ms step_avg:100.05ms
step:1739/1770 train_time:172996ms step_avg:100.06ms
step:1740/1770 train_time:173101ms step_avg:100.06ms
step:1741/1770 train_time:173209ms step_avg:100.06ms
step:1742/1770 train_time:173316ms step_avg:100.07ms
step:1743/1770 train_time:173422ms step_avg:100.07ms
step:1744/1770 train_time:173527ms step_avg:100.07ms
step:1745/1770 train_time:173632ms step_avg:100.08ms
step:1746/1770 train_time:173740ms step_avg:100.08ms
step:1747/1770 train_time:173844ms step_avg:100.08ms
step:1748/1770 train_time:173951ms step_avg:100.09ms
step:1749/1770 train_time:174057ms step_avg:100.09ms
step:1750/1770 train_time:174161ms step_avg:100.09ms
step:1750/1770 val_loss:3.2972 train_time:174264ms step_avg:100.15ms
step:1751/1770 train_time:174285ms step_avg:100.11ms
step:1752/1770 train_time:174374ms step_avg:100.10ms
step:1753/1770 train_time:174478ms step_avg:100.10ms
step:1754/1770 train_time:174584ms step_avg:100.11ms
step:1755/1770 train_time:174689ms step_avg:100.11ms
step:1756/1770 train_time:174796ms step_avg:100.11ms
step:1757/1770 train_time:174900ms step_avg:100.11ms
step:1758/1770 train_time:175004ms step_avg:100.12ms
step:1759/1770 train_time:175110ms step_avg:100.12ms
step:1760/1770 train_time:175215ms step_avg:100.12ms
step:1761/1770 train_time:175323ms step_avg:100.13ms
step:1762/1770 train_time:175432ms step_avg:100.13ms
step:1763/1770 train_time:175535ms step_avg:100.13ms
step:1764/1770 train_time:175640ms step_avg:100.14ms
step:1765/1770 train_time:175745ms step_avg:100.14ms
step:1766/1770 train_time:175854ms step_avg:100.14ms
step:1767/1770 train_time:175958ms step_avg:100.15ms
step:1768/1770 train_time:176064ms step_avg:100.15ms
step:1769/1770 train_time:176169ms step_avg:100.15ms
step:1770/1770 train_time:176273ms step_avg:100.15ms
step:1770/1770 val_loss:3.2945 train_time:176378ms step_avg:100.21ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
