import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:03:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23785ms step_avg:nanms
step:2/1770 train_time:24205ms step_avg:nanms
step:3/1770 train_time:24301ms step_avg:nanms
step:4/1770 train_time:24395ms step_avg:nanms
step:5/1770 train_time:24489ms step_avg:nanms
step:6/1770 train_time:24583ms step_avg:nanms
step:7/1770 train_time:24678ms step_avg:nanms
step:8/1770 train_time:24772ms step_avg:nanms
step:9/1770 train_time:24866ms step_avg:nanms
step:10/1770 train_time:24961ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.42ms
step:14/1770 train_time:378ms step_avg:94.53ms
step:15/1770 train_time:473ms step_avg:94.58ms
step:16/1770 train_time:567ms step_avg:94.58ms
step:17/1770 train_time:662ms step_avg:94.53ms
step:18/1770 train_time:757ms step_avg:94.59ms
step:19/1770 train_time:851ms step_avg:94.59ms
step:20/1770 train_time:947ms step_avg:94.66ms
step:21/1770 train_time:1041ms step_avg:94.66ms
step:22/1770 train_time:1136ms step_avg:94.70ms
step:23/1770 train_time:1231ms step_avg:94.67ms
step:24/1770 train_time:1325ms step_avg:94.66ms
step:25/1770 train_time:1420ms step_avg:94.66ms
step:26/1770 train_time:1516ms step_avg:94.72ms
step:27/1770 train_time:1610ms step_avg:94.70ms
step:28/1770 train_time:1704ms step_avg:94.67ms
step:29/1770 train_time:1799ms step_avg:94.67ms
step:30/1770 train_time:1895ms step_avg:94.74ms
step:31/1770 train_time:1989ms step_avg:94.72ms
step:32/1770 train_time:2084ms step_avg:94.72ms
step:33/1770 train_time:2179ms step_avg:94.73ms
step:34/1770 train_time:2274ms step_avg:94.74ms
step:35/1770 train_time:2368ms step_avg:94.73ms
step:36/1770 train_time:2463ms step_avg:94.72ms
step:37/1770 train_time:2558ms step_avg:94.73ms
step:38/1770 train_time:2653ms step_avg:94.74ms
step:39/1770 train_time:2747ms step_avg:94.73ms
step:40/1770 train_time:2842ms step_avg:94.72ms
step:41/1770 train_time:2936ms step_avg:94.72ms
step:42/1770 train_time:3031ms step_avg:94.72ms
step:43/1770 train_time:3125ms step_avg:94.70ms
step:44/1770 train_time:3220ms step_avg:94.71ms
step:45/1770 train_time:3315ms step_avg:94.72ms
step:46/1770 train_time:3410ms step_avg:94.71ms
step:47/1770 train_time:3504ms step_avg:94.71ms
step:48/1770 train_time:3599ms step_avg:94.72ms
step:49/1770 train_time:3694ms step_avg:94.71ms
step:50/1770 train_time:3789ms step_avg:94.72ms
step:51/1770 train_time:3883ms step_avg:94.71ms
step:52/1770 train_time:3978ms step_avg:94.72ms
step:53/1770 train_time:4072ms step_avg:94.71ms
step:54/1770 train_time:4167ms step_avg:94.70ms
step:55/1770 train_time:4261ms step_avg:94.69ms
step:56/1770 train_time:4356ms step_avg:94.70ms
step:57/1770 train_time:4450ms step_avg:94.69ms
step:58/1770 train_time:4546ms step_avg:94.70ms
step:59/1770 train_time:4640ms step_avg:94.70ms
step:60/1770 train_time:4735ms step_avg:94.71ms
step:61/1770 train_time:4830ms step_avg:94.70ms
step:62/1770 train_time:4925ms step_avg:94.70ms
step:63/1770 train_time:5019ms step_avg:94.71ms
step:64/1770 train_time:5114ms step_avg:94.71ms
step:65/1770 train_time:5209ms step_avg:94.71ms
step:66/1770 train_time:5303ms step_avg:94.70ms
step:67/1770 train_time:5398ms step_avg:94.71ms
step:68/1770 train_time:5493ms step_avg:94.71ms
step:69/1770 train_time:5588ms step_avg:94.70ms
step:70/1770 train_time:5682ms step_avg:94.70ms
step:71/1770 train_time:5777ms step_avg:94.71ms
step:72/1770 train_time:5872ms step_avg:94.71ms
step:73/1770 train_time:5967ms step_avg:94.72ms
step:74/1770 train_time:6062ms step_avg:94.72ms
step:75/1770 train_time:6158ms step_avg:94.73ms
step:76/1770 train_time:6252ms step_avg:94.73ms
step:77/1770 train_time:6347ms step_avg:94.73ms
step:78/1770 train_time:6442ms step_avg:94.73ms
step:79/1770 train_time:6537ms step_avg:94.74ms
step:80/1770 train_time:6632ms step_avg:94.74ms
step:81/1770 train_time:6726ms step_avg:94.74ms
step:82/1770 train_time:6821ms step_avg:94.73ms
step:83/1770 train_time:6916ms step_avg:94.73ms
step:84/1770 train_time:7010ms step_avg:94.73ms
step:85/1770 train_time:7104ms step_avg:94.72ms
step:86/1770 train_time:7199ms step_avg:94.72ms
step:87/1770 train_time:7294ms step_avg:94.72ms
step:88/1770 train_time:7389ms step_avg:94.73ms
step:89/1770 train_time:7483ms step_avg:94.73ms
step:90/1770 train_time:7578ms step_avg:94.73ms
step:91/1770 train_time:7674ms step_avg:94.74ms
step:92/1770 train_time:7768ms step_avg:94.74ms
step:93/1770 train_time:7863ms step_avg:94.73ms
step:94/1770 train_time:7958ms step_avg:94.73ms
step:95/1770 train_time:8053ms step_avg:94.74ms
step:96/1770 train_time:8148ms step_avg:94.74ms
step:97/1770 train_time:8243ms step_avg:94.74ms
step:98/1770 train_time:8338ms step_avg:94.75ms
step:99/1770 train_time:8433ms step_avg:94.75ms
step:100/1770 train_time:8529ms step_avg:94.76ms
step:101/1770 train_time:8623ms step_avg:94.76ms
step:102/1770 train_time:8719ms step_avg:94.77ms
step:103/1770 train_time:8813ms step_avg:94.77ms
step:104/1770 train_time:8908ms step_avg:94.76ms
step:105/1770 train_time:9002ms step_avg:94.76ms
step:106/1770 train_time:9098ms step_avg:94.77ms
step:107/1770 train_time:9192ms step_avg:94.77ms
step:108/1770 train_time:9286ms step_avg:94.76ms
step:109/1770 train_time:9381ms step_avg:94.76ms
step:110/1770 train_time:9476ms step_avg:94.76ms
step:111/1770 train_time:9571ms step_avg:94.76ms
step:112/1770 train_time:9667ms step_avg:94.77ms
step:113/1770 train_time:9762ms step_avg:94.77ms
step:114/1770 train_time:9856ms step_avg:94.77ms
step:115/1770 train_time:9951ms step_avg:94.77ms
step:116/1770 train_time:10046ms step_avg:94.77ms
step:117/1770 train_time:10140ms step_avg:94.77ms
step:118/1770 train_time:10235ms step_avg:94.77ms
step:119/1770 train_time:10330ms step_avg:94.77ms
step:120/1770 train_time:10424ms step_avg:94.76ms
step:121/1770 train_time:10519ms step_avg:94.76ms
step:122/1770 train_time:10614ms step_avg:94.76ms
step:123/1770 train_time:10709ms step_avg:94.77ms
step:124/1770 train_time:10804ms step_avg:94.77ms
step:125/1770 train_time:10899ms step_avg:94.77ms
step:125/1770 val_loss:4.6470 train_time:10992ms step_avg:95.58ms
step:126/1770 train_time:11014ms step_avg:94.95ms
step:127/1770 train_time:11092ms step_avg:94.80ms
step:128/1770 train_time:11190ms step_avg:94.83ms
step:129/1770 train_time:11287ms step_avg:94.85ms
step:130/1770 train_time:11382ms step_avg:94.85ms
step:131/1770 train_time:11477ms step_avg:94.85ms
step:132/1770 train_time:11572ms step_avg:94.85ms
step:133/1770 train_time:11666ms step_avg:94.85ms
step:134/1770 train_time:11761ms step_avg:94.85ms
step:135/1770 train_time:11856ms step_avg:94.85ms
step:136/1770 train_time:11952ms step_avg:94.86ms
step:137/1770 train_time:12047ms step_avg:94.86ms
step:138/1770 train_time:12142ms step_avg:94.86ms
step:139/1770 train_time:12237ms step_avg:94.86ms
step:140/1770 train_time:12333ms step_avg:94.87ms
step:141/1770 train_time:12429ms step_avg:94.88ms
step:142/1770 train_time:12524ms step_avg:94.88ms
step:143/1770 train_time:12619ms step_avg:94.88ms
step:144/1770 train_time:12715ms step_avg:94.89ms
step:145/1770 train_time:12810ms step_avg:94.89ms
step:146/1770 train_time:12905ms step_avg:94.89ms
step:147/1770 train_time:13000ms step_avg:94.89ms
step:148/1770 train_time:13096ms step_avg:94.90ms
step:149/1770 train_time:13191ms step_avg:94.90ms
step:150/1770 train_time:13286ms step_avg:94.90ms
step:151/1770 train_time:13381ms step_avg:94.90ms
step:152/1770 train_time:13477ms step_avg:94.91ms
step:153/1770 train_time:13572ms step_avg:94.91ms
step:154/1770 train_time:13668ms step_avg:94.91ms
step:155/1770 train_time:13763ms step_avg:94.92ms
step:156/1770 train_time:13858ms step_avg:94.92ms
step:157/1770 train_time:13954ms step_avg:94.92ms
step:158/1770 train_time:14049ms step_avg:94.93ms
step:159/1770 train_time:14144ms step_avg:94.93ms
step:160/1770 train_time:14239ms step_avg:94.93ms
step:161/1770 train_time:14334ms step_avg:94.93ms
step:162/1770 train_time:14430ms step_avg:94.93ms
step:163/1770 train_time:14525ms step_avg:94.93ms
step:164/1770 train_time:14620ms step_avg:94.94ms
step:165/1770 train_time:14716ms step_avg:94.94ms
step:166/1770 train_time:14811ms step_avg:94.94ms
step:167/1770 train_time:14907ms step_avg:94.95ms
step:168/1770 train_time:15002ms step_avg:94.95ms
step:169/1770 train_time:15097ms step_avg:94.95ms
step:170/1770 train_time:15192ms step_avg:94.95ms
step:171/1770 train_time:15288ms step_avg:94.96ms
step:172/1770 train_time:15384ms step_avg:94.96ms
step:173/1770 train_time:15478ms step_avg:94.96ms
step:174/1770 train_time:15574ms step_avg:94.96ms
step:175/1770 train_time:15670ms step_avg:94.97ms
step:176/1770 train_time:15765ms step_avg:94.97ms
step:177/1770 train_time:15859ms step_avg:94.97ms
step:178/1770 train_time:15955ms step_avg:94.97ms
step:179/1770 train_time:16050ms step_avg:94.97ms
step:180/1770 train_time:16146ms step_avg:94.97ms
step:181/1770 train_time:16241ms step_avg:94.98ms
step:182/1770 train_time:16336ms step_avg:94.98ms
step:183/1770 train_time:16432ms step_avg:94.98ms
step:184/1770 train_time:16527ms step_avg:94.98ms
step:185/1770 train_time:16622ms step_avg:94.98ms
step:186/1770 train_time:16717ms step_avg:94.98ms
step:187/1770 train_time:16813ms step_avg:94.99ms
step:188/1770 train_time:16908ms step_avg:94.99ms
step:189/1770 train_time:17003ms step_avg:94.99ms
step:190/1770 train_time:17099ms step_avg:95.00ms
step:191/1770 train_time:17195ms step_avg:95.00ms
step:192/1770 train_time:17290ms step_avg:95.00ms
step:193/1770 train_time:17386ms step_avg:95.00ms
step:194/1770 train_time:17481ms step_avg:95.01ms
step:195/1770 train_time:17576ms step_avg:95.01ms
step:196/1770 train_time:17672ms step_avg:95.01ms
step:197/1770 train_time:17767ms step_avg:95.01ms
step:198/1770 train_time:17862ms step_avg:95.01ms
step:199/1770 train_time:17957ms step_avg:95.01ms
step:200/1770 train_time:18053ms step_avg:95.01ms
step:201/1770 train_time:18149ms step_avg:95.02ms
step:202/1770 train_time:18244ms step_avg:95.02ms
step:203/1770 train_time:18339ms step_avg:95.02ms
step:204/1770 train_time:18434ms step_avg:95.02ms
step:205/1770 train_time:18529ms step_avg:95.02ms
step:206/1770 train_time:18625ms step_avg:95.03ms
step:207/1770 train_time:18721ms step_avg:95.03ms
step:208/1770 train_time:18816ms step_avg:95.03ms
step:209/1770 train_time:18911ms step_avg:95.03ms
step:210/1770 train_time:19006ms step_avg:95.03ms
step:211/1770 train_time:19101ms step_avg:95.03ms
step:212/1770 train_time:19196ms step_avg:95.03ms
step:213/1770 train_time:19292ms step_avg:95.03ms
step:214/1770 train_time:19387ms step_avg:95.04ms
step:215/1770 train_time:19483ms step_avg:95.04ms
step:216/1770 train_time:19577ms step_avg:95.03ms
step:217/1770 train_time:19673ms step_avg:95.04ms
step:218/1770 train_time:19768ms step_avg:95.04ms
step:219/1770 train_time:19864ms step_avg:95.04ms
step:220/1770 train_time:19958ms step_avg:95.04ms
step:221/1770 train_time:20054ms step_avg:95.04ms
step:222/1770 train_time:20149ms step_avg:95.04ms
step:223/1770 train_time:20244ms step_avg:95.04ms
step:224/1770 train_time:20338ms step_avg:95.04ms
step:225/1770 train_time:20434ms step_avg:95.04ms
step:226/1770 train_time:20529ms step_avg:95.04ms
step:227/1770 train_time:20624ms step_avg:95.04ms
step:228/1770 train_time:20719ms step_avg:95.04ms
step:229/1770 train_time:20814ms step_avg:95.04ms
step:230/1770 train_time:20910ms step_avg:95.04ms
step:231/1770 train_time:21005ms step_avg:95.04ms
step:232/1770 train_time:21099ms step_avg:95.04ms
step:233/1770 train_time:21195ms step_avg:95.05ms
step:234/1770 train_time:21291ms step_avg:95.05ms
step:235/1770 train_time:21386ms step_avg:95.05ms
step:236/1770 train_time:21481ms step_avg:95.05ms
step:237/1770 train_time:21576ms step_avg:95.05ms
step:238/1770 train_time:21671ms step_avg:95.05ms
step:239/1770 train_time:21767ms step_avg:95.05ms
step:240/1770 train_time:21862ms step_avg:95.05ms
step:241/1770 train_time:21957ms step_avg:95.05ms
step:242/1770 train_time:22053ms step_avg:95.06ms
step:243/1770 train_time:22148ms step_avg:95.06ms
step:244/1770 train_time:22243ms step_avg:95.06ms
step:245/1770 train_time:22338ms step_avg:95.06ms
step:246/1770 train_time:22434ms step_avg:95.06ms
step:247/1770 train_time:22529ms step_avg:95.06ms
step:248/1770 train_time:22625ms step_avg:95.06ms
step:249/1770 train_time:22719ms step_avg:95.06ms
step:250/1770 train_time:22815ms step_avg:95.06ms
step:250/1770 val_loss:4.1066 train_time:22909ms step_avg:95.45ms
step:251/1770 train_time:22930ms step_avg:95.15ms
step:252/1770 train_time:23011ms step_avg:95.09ms
step:253/1770 train_time:23110ms step_avg:95.10ms
step:254/1770 train_time:23206ms step_avg:95.10ms
step:255/1770 train_time:23301ms step_avg:95.11ms
step:256/1770 train_time:23396ms step_avg:95.11ms
step:257/1770 train_time:23491ms step_avg:95.10ms
step:258/1770 train_time:23586ms step_avg:95.11ms
step:259/1770 train_time:23682ms step_avg:95.11ms
step:260/1770 train_time:23777ms step_avg:95.11ms
step:261/1770 train_time:23873ms step_avg:95.11ms
step:262/1770 train_time:23967ms step_avg:95.11ms
step:263/1770 train_time:24065ms step_avg:95.12ms
step:264/1770 train_time:24161ms step_avg:95.12ms
step:265/1770 train_time:24257ms step_avg:95.12ms
step:266/1770 train_time:24353ms step_avg:95.13ms
step:267/1770 train_time:24448ms step_avg:95.13ms
step:268/1770 train_time:24544ms step_avg:95.13ms
step:269/1770 train_time:24640ms step_avg:95.13ms
step:270/1770 train_time:24735ms step_avg:95.14ms
step:271/1770 train_time:24831ms step_avg:95.14ms
step:272/1770 train_time:24927ms step_avg:95.14ms
step:273/1770 train_time:25023ms step_avg:95.14ms
step:274/1770 train_time:25119ms step_avg:95.15ms
step:275/1770 train_time:25215ms step_avg:95.15ms
step:276/1770 train_time:25311ms step_avg:95.15ms
step:277/1770 train_time:25407ms step_avg:95.16ms
step:278/1770 train_time:25502ms step_avg:95.16ms
step:279/1770 train_time:25599ms step_avg:95.16ms
step:280/1770 train_time:25694ms step_avg:95.16ms
step:281/1770 train_time:25790ms step_avg:95.17ms
step:282/1770 train_time:25886ms step_avg:95.17ms
step:283/1770 train_time:25982ms step_avg:95.17ms
step:284/1770 train_time:26079ms step_avg:95.18ms
step:285/1770 train_time:26174ms step_avg:95.18ms
step:286/1770 train_time:26269ms step_avg:95.18ms
step:287/1770 train_time:26365ms step_avg:95.18ms
step:288/1770 train_time:26461ms step_avg:95.18ms
step:289/1770 train_time:26557ms step_avg:95.19ms
step:290/1770 train_time:26653ms step_avg:95.19ms
step:291/1770 train_time:26748ms step_avg:95.19ms
step:292/1770 train_time:26844ms step_avg:95.19ms
step:293/1770 train_time:26941ms step_avg:95.20ms
step:294/1770 train_time:27036ms step_avg:95.20ms
step:295/1770 train_time:27132ms step_avg:95.20ms
step:296/1770 train_time:27228ms step_avg:95.20ms
step:297/1770 train_time:27324ms step_avg:95.21ms
step:298/1770 train_time:27421ms step_avg:95.21ms
step:299/1770 train_time:27517ms step_avg:95.21ms
step:300/1770 train_time:27613ms step_avg:95.22ms
step:301/1770 train_time:27709ms step_avg:95.22ms
step:302/1770 train_time:27805ms step_avg:95.22ms
step:303/1770 train_time:27901ms step_avg:95.23ms
step:304/1770 train_time:27997ms step_avg:95.23ms
step:305/1770 train_time:28093ms step_avg:95.23ms
step:306/1770 train_time:28189ms step_avg:95.23ms
step:307/1770 train_time:28285ms step_avg:95.23ms
step:308/1770 train_time:28381ms step_avg:95.24ms
step:309/1770 train_time:28476ms step_avg:95.24ms
step:310/1770 train_time:28572ms step_avg:95.24ms
step:311/1770 train_time:28667ms step_avg:95.24ms
step:312/1770 train_time:28764ms step_avg:95.24ms
step:313/1770 train_time:28859ms step_avg:95.25ms
step:314/1770 train_time:28955ms step_avg:95.25ms
step:315/1770 train_time:29051ms step_avg:95.25ms
step:316/1770 train_time:29146ms step_avg:95.25ms
step:317/1770 train_time:29242ms step_avg:95.25ms
step:318/1770 train_time:29338ms step_avg:95.25ms
step:319/1770 train_time:29434ms step_avg:95.26ms
step:320/1770 train_time:29529ms step_avg:95.26ms
step:321/1770 train_time:29625ms step_avg:95.26ms
step:322/1770 train_time:29721ms step_avg:95.26ms
step:323/1770 train_time:29816ms step_avg:95.26ms
step:324/1770 train_time:29912ms step_avg:95.26ms
step:325/1770 train_time:30007ms step_avg:95.26ms
step:326/1770 train_time:30104ms step_avg:95.27ms
step:327/1770 train_time:30200ms step_avg:95.27ms
step:328/1770 train_time:30296ms step_avg:95.27ms
step:329/1770 train_time:30392ms step_avg:95.27ms
step:330/1770 train_time:30487ms step_avg:95.27ms
step:331/1770 train_time:30583ms step_avg:95.28ms
step:332/1770 train_time:30680ms step_avg:95.28ms
step:333/1770 train_time:30776ms step_avg:95.28ms
step:334/1770 train_time:30872ms step_avg:95.28ms
step:335/1770 train_time:30967ms step_avg:95.28ms
step:336/1770 train_time:31064ms step_avg:95.29ms
step:337/1770 train_time:31161ms step_avg:95.29ms
step:338/1770 train_time:31257ms step_avg:95.29ms
step:339/1770 train_time:31352ms step_avg:95.29ms
step:340/1770 train_time:31447ms step_avg:95.29ms
step:341/1770 train_time:31543ms step_avg:95.30ms
step:342/1770 train_time:31640ms step_avg:95.30ms
step:343/1770 train_time:31735ms step_avg:95.30ms
step:344/1770 train_time:31831ms step_avg:95.30ms
step:345/1770 train_time:31927ms step_avg:95.30ms
step:346/1770 train_time:32023ms step_avg:95.31ms
step:347/1770 train_time:32119ms step_avg:95.31ms
step:348/1770 train_time:32214ms step_avg:95.31ms
step:349/1770 train_time:32309ms step_avg:95.31ms
step:350/1770 train_time:32405ms step_avg:95.31ms
step:351/1770 train_time:32502ms step_avg:95.31ms
step:352/1770 train_time:32598ms step_avg:95.32ms
step:353/1770 train_time:32694ms step_avg:95.32ms
step:354/1770 train_time:32790ms step_avg:95.32ms
step:355/1770 train_time:32885ms step_avg:95.32ms
step:356/1770 train_time:32981ms step_avg:95.32ms
step:357/1770 train_time:33077ms step_avg:95.32ms
step:358/1770 train_time:33172ms step_avg:95.32ms
step:359/1770 train_time:33267ms step_avg:95.32ms
step:360/1770 train_time:33363ms step_avg:95.32ms
step:361/1770 train_time:33459ms step_avg:95.32ms
step:362/1770 train_time:33554ms step_avg:95.32ms
step:363/1770 train_time:33649ms step_avg:95.32ms
step:364/1770 train_time:33745ms step_avg:95.33ms
step:365/1770 train_time:33841ms step_avg:95.33ms
step:366/1770 train_time:33937ms step_avg:95.33ms
step:367/1770 train_time:34033ms step_avg:95.33ms
step:368/1770 train_time:34128ms step_avg:95.33ms
step:369/1770 train_time:34224ms step_avg:95.33ms
step:370/1770 train_time:34321ms step_avg:95.33ms
step:371/1770 train_time:34416ms step_avg:95.34ms
step:372/1770 train_time:34512ms step_avg:95.34ms
step:373/1770 train_time:34608ms step_avg:95.34ms
step:374/1770 train_time:34704ms step_avg:95.34ms
step:375/1770 train_time:34800ms step_avg:95.34ms
step:375/1770 val_loss:3.8980 train_time:34894ms step_avg:95.60ms
step:376/1770 train_time:34915ms step_avg:95.40ms
step:377/1770 train_time:34999ms step_avg:95.36ms
step:378/1770 train_time:35099ms step_avg:95.38ms
step:379/1770 train_time:35196ms step_avg:95.38ms
step:380/1770 train_time:35291ms step_avg:95.38ms
step:381/1770 train_time:35387ms step_avg:95.38ms
step:382/1770 train_time:35483ms step_avg:95.38ms
step:383/1770 train_time:35578ms step_avg:95.38ms
step:384/1770 train_time:35673ms step_avg:95.38ms
step:385/1770 train_time:35769ms step_avg:95.38ms
step:386/1770 train_time:35865ms step_avg:95.38ms
step:387/1770 train_time:35960ms step_avg:95.39ms
step:388/1770 train_time:36056ms step_avg:95.39ms
step:389/1770 train_time:36152ms step_avg:95.39ms
step:390/1770 train_time:36248ms step_avg:95.39ms
step:391/1770 train_time:36344ms step_avg:95.39ms
step:392/1770 train_time:36439ms step_avg:95.39ms
step:393/1770 train_time:36535ms step_avg:95.39ms
step:394/1770 train_time:36630ms step_avg:95.39ms
step:395/1770 train_time:36726ms step_avg:95.39ms
step:396/1770 train_time:36824ms step_avg:95.40ms
step:397/1770 train_time:36922ms step_avg:95.41ms
step:398/1770 train_time:37019ms step_avg:95.41ms
step:399/1770 train_time:37117ms step_avg:95.42ms
step:400/1770 train_time:37214ms step_avg:95.42ms
step:401/1770 train_time:37312ms step_avg:95.43ms
step:402/1770 train_time:37409ms step_avg:95.43ms
step:403/1770 train_time:37507ms step_avg:95.44ms
step:404/1770 train_time:37605ms step_avg:95.44ms
step:405/1770 train_time:37702ms step_avg:95.45ms
step:406/1770 train_time:37799ms step_avg:95.45ms
step:407/1770 train_time:37896ms step_avg:95.46ms
step:408/1770 train_time:37993ms step_avg:95.46ms
step:409/1770 train_time:38091ms step_avg:95.47ms
step:410/1770 train_time:38188ms step_avg:95.47ms
step:411/1770 train_time:38286ms step_avg:95.48ms
step:412/1770 train_time:38384ms step_avg:95.48ms
step:413/1770 train_time:38481ms step_avg:95.49ms
step:414/1770 train_time:38578ms step_avg:95.49ms
step:415/1770 train_time:38675ms step_avg:95.49ms
step:416/1770 train_time:38773ms step_avg:95.50ms
step:417/1770 train_time:38870ms step_avg:95.50ms
step:418/1770 train_time:38968ms step_avg:95.51ms
step:419/1770 train_time:39066ms step_avg:95.52ms
step:420/1770 train_time:39163ms step_avg:95.52ms
step:421/1770 train_time:39261ms step_avg:95.53ms
step:422/1770 train_time:39359ms step_avg:95.53ms
step:423/1770 train_time:39456ms step_avg:95.54ms
step:424/1770 train_time:39553ms step_avg:95.54ms
step:425/1770 train_time:39651ms step_avg:95.54ms
step:426/1770 train_time:39749ms step_avg:95.55ms
step:427/1770 train_time:39847ms step_avg:95.56ms
step:428/1770 train_time:39944ms step_avg:95.56ms
step:429/1770 train_time:40041ms step_avg:95.56ms
step:430/1770 train_time:40139ms step_avg:95.57ms
step:431/1770 train_time:40236ms step_avg:95.57ms
step:432/1770 train_time:40334ms step_avg:95.58ms
step:433/1770 train_time:40431ms step_avg:95.58ms
step:434/1770 train_time:40529ms step_avg:95.59ms
step:435/1770 train_time:40627ms step_avg:95.59ms
step:436/1770 train_time:40725ms step_avg:95.60ms
step:437/1770 train_time:40823ms step_avg:95.60ms
step:438/1770 train_time:40920ms step_avg:95.61ms
step:439/1770 train_time:41018ms step_avg:95.61ms
step:440/1770 train_time:41115ms step_avg:95.62ms
step:441/1770 train_time:41212ms step_avg:95.62ms
step:442/1770 train_time:41310ms step_avg:95.62ms
step:443/1770 train_time:41408ms step_avg:95.63ms
step:444/1770 train_time:41506ms step_avg:95.64ms
step:445/1770 train_time:41603ms step_avg:95.64ms
step:446/1770 train_time:41700ms step_avg:95.64ms
step:447/1770 train_time:41798ms step_avg:95.65ms
step:448/1770 train_time:41896ms step_avg:95.65ms
step:449/1770 train_time:41993ms step_avg:95.66ms
step:450/1770 train_time:42090ms step_avg:95.66ms
step:451/1770 train_time:42188ms step_avg:95.67ms
step:452/1770 train_time:42286ms step_avg:95.67ms
step:453/1770 train_time:42384ms step_avg:95.67ms
step:454/1770 train_time:42481ms step_avg:95.68ms
step:455/1770 train_time:42579ms step_avg:95.68ms
step:456/1770 train_time:42676ms step_avg:95.69ms
step:457/1770 train_time:42773ms step_avg:95.69ms
step:458/1770 train_time:42870ms step_avg:95.69ms
step:459/1770 train_time:42968ms step_avg:95.70ms
step:460/1770 train_time:43066ms step_avg:95.70ms
step:461/1770 train_time:43163ms step_avg:95.71ms
step:462/1770 train_time:43261ms step_avg:95.71ms
step:463/1770 train_time:43358ms step_avg:95.71ms
step:464/1770 train_time:43456ms step_avg:95.72ms
step:465/1770 train_time:43553ms step_avg:95.72ms
step:466/1770 train_time:43651ms step_avg:95.73ms
step:467/1770 train_time:43749ms step_avg:95.73ms
step:468/1770 train_time:43847ms step_avg:95.74ms
step:469/1770 train_time:43945ms step_avg:95.74ms
step:470/1770 train_time:44043ms step_avg:95.75ms
step:471/1770 train_time:44140ms step_avg:95.75ms
step:472/1770 train_time:44238ms step_avg:95.75ms
step:473/1770 train_time:44335ms step_avg:95.76ms
step:474/1770 train_time:44432ms step_avg:95.76ms
step:475/1770 train_time:44530ms step_avg:95.76ms
step:476/1770 train_time:44628ms step_avg:95.77ms
step:477/1770 train_time:44725ms step_avg:95.77ms
step:478/1770 train_time:44823ms step_avg:95.78ms
step:479/1770 train_time:44921ms step_avg:95.78ms
step:480/1770 train_time:45018ms step_avg:95.78ms
step:481/1770 train_time:45116ms step_avg:95.79ms
step:482/1770 train_time:45213ms step_avg:95.79ms
step:483/1770 train_time:45311ms step_avg:95.79ms
step:484/1770 train_time:45408ms step_avg:95.80ms
step:485/1770 train_time:45506ms step_avg:95.80ms
step:486/1770 train_time:45603ms step_avg:95.81ms
step:487/1770 train_time:45701ms step_avg:95.81ms
step:488/1770 train_time:45799ms step_avg:95.81ms
step:489/1770 train_time:45897ms step_avg:95.82ms
step:490/1770 train_time:45994ms step_avg:95.82ms
step:491/1770 train_time:46091ms step_avg:95.82ms
step:492/1770 train_time:46189ms step_avg:95.83ms
step:493/1770 train_time:46287ms step_avg:95.83ms
step:494/1770 train_time:46384ms step_avg:95.84ms
step:495/1770 train_time:46482ms step_avg:95.84ms
step:496/1770 train_time:46580ms step_avg:95.84ms
step:497/1770 train_time:46677ms step_avg:95.85ms
step:498/1770 train_time:46774ms step_avg:95.85ms
step:499/1770 train_time:46872ms step_avg:95.85ms
step:500/1770 train_time:46970ms step_avg:95.86ms
step:500/1770 val_loss:3.7475 train_time:47066ms step_avg:96.05ms
step:501/1770 train_time:47089ms step_avg:95.90ms
step:502/1770 train_time:47173ms step_avg:95.88ms
step:503/1770 train_time:47273ms step_avg:95.89ms
step:504/1770 train_time:47371ms step_avg:95.89ms
step:505/1770 train_time:47469ms step_avg:95.90ms
step:506/1770 train_time:47566ms step_avg:95.90ms
step:507/1770 train_time:47664ms step_avg:95.90ms
step:508/1770 train_time:47762ms step_avg:95.91ms
step:509/1770 train_time:47859ms step_avg:95.91ms
step:510/1770 train_time:47957ms step_avg:95.91ms
step:511/1770 train_time:48054ms step_avg:95.92ms
step:512/1770 train_time:48152ms step_avg:95.92ms
step:513/1770 train_time:48250ms step_avg:95.92ms
step:514/1770 train_time:48348ms step_avg:95.93ms
step:515/1770 train_time:48445ms step_avg:95.93ms
step:516/1770 train_time:48543ms step_avg:95.94ms
step:517/1770 train_time:48642ms step_avg:95.94ms
step:518/1770 train_time:48740ms step_avg:95.95ms
step:519/1770 train_time:48837ms step_avg:95.95ms
step:520/1770 train_time:48935ms step_avg:95.95ms
step:521/1770 train_time:49032ms step_avg:95.95ms
step:522/1770 train_time:49130ms step_avg:95.96ms
step:523/1770 train_time:49228ms step_avg:95.96ms
step:524/1770 train_time:49326ms step_avg:95.96ms
step:525/1770 train_time:49423ms step_avg:95.97ms
step:526/1770 train_time:49521ms step_avg:95.97ms
step:527/1770 train_time:49619ms step_avg:95.97ms
step:528/1770 train_time:49717ms step_avg:95.98ms
step:529/1770 train_time:49814ms step_avg:95.98ms
step:530/1770 train_time:49913ms step_avg:95.99ms
step:531/1770 train_time:50011ms step_avg:95.99ms
step:532/1770 train_time:50110ms step_avg:96.00ms
step:533/1770 train_time:50208ms step_avg:96.00ms
step:534/1770 train_time:50306ms step_avg:96.00ms
step:535/1770 train_time:50404ms step_avg:96.01ms
step:536/1770 train_time:50502ms step_avg:96.01ms
step:537/1770 train_time:50600ms step_avg:96.02ms
step:538/1770 train_time:50698ms step_avg:96.02ms
step:539/1770 train_time:50796ms step_avg:96.02ms
step:540/1770 train_time:50894ms step_avg:96.03ms
step:541/1770 train_time:50992ms step_avg:96.03ms
step:542/1770 train_time:51090ms step_avg:96.03ms
step:543/1770 train_time:51189ms step_avg:96.04ms
step:544/1770 train_time:51287ms step_avg:96.04ms
step:545/1770 train_time:51385ms step_avg:96.05ms
step:546/1770 train_time:51482ms step_avg:96.05ms
step:547/1770 train_time:51580ms step_avg:96.05ms
step:548/1770 train_time:51678ms step_avg:96.06ms
step:549/1770 train_time:51776ms step_avg:96.06ms
step:550/1770 train_time:51873ms step_avg:96.06ms
step:551/1770 train_time:51971ms step_avg:96.07ms
step:552/1770 train_time:52070ms step_avg:96.07ms
step:553/1770 train_time:52168ms step_avg:96.07ms
step:554/1770 train_time:52267ms step_avg:96.08ms
step:555/1770 train_time:52365ms step_avg:96.08ms
step:556/1770 train_time:52463ms step_avg:96.09ms
step:557/1770 train_time:52561ms step_avg:96.09ms
step:558/1770 train_time:52658ms step_avg:96.09ms
step:559/1770 train_time:52756ms step_avg:96.09ms
step:560/1770 train_time:52854ms step_avg:96.10ms
step:561/1770 train_time:52952ms step_avg:96.10ms
step:562/1770 train_time:53051ms step_avg:96.11ms
step:563/1770 train_time:53149ms step_avg:96.11ms
step:564/1770 train_time:53247ms step_avg:96.11ms
step:565/1770 train_time:53344ms step_avg:96.12ms
step:566/1770 train_time:53442ms step_avg:96.12ms
step:567/1770 train_time:53540ms step_avg:96.12ms
step:568/1770 train_time:53638ms step_avg:96.13ms
step:569/1770 train_time:53736ms step_avg:96.13ms
step:570/1770 train_time:53834ms step_avg:96.13ms
step:571/1770 train_time:53932ms step_avg:96.14ms
step:572/1770 train_time:54030ms step_avg:96.14ms
step:573/1770 train_time:54128ms step_avg:96.14ms
step:574/1770 train_time:54226ms step_avg:96.15ms
step:575/1770 train_time:54324ms step_avg:96.15ms
step:576/1770 train_time:54422ms step_avg:96.15ms
step:577/1770 train_time:54520ms step_avg:96.15ms
step:578/1770 train_time:54618ms step_avg:96.16ms
step:579/1770 train_time:54715ms step_avg:96.16ms
step:580/1770 train_time:54813ms step_avg:96.16ms
step:581/1770 train_time:54911ms step_avg:96.17ms
step:582/1770 train_time:55010ms step_avg:96.17ms
step:583/1770 train_time:55108ms step_avg:96.17ms
step:584/1770 train_time:55206ms step_avg:96.18ms
step:585/1770 train_time:55303ms step_avg:96.18ms
step:586/1770 train_time:55401ms step_avg:96.18ms
step:587/1770 train_time:55498ms step_avg:96.18ms
step:588/1770 train_time:55596ms step_avg:96.19ms
step:589/1770 train_time:55694ms step_avg:96.19ms
step:590/1770 train_time:55792ms step_avg:96.19ms
step:591/1770 train_time:55891ms step_avg:96.20ms
step:592/1770 train_time:55989ms step_avg:96.20ms
step:593/1770 train_time:56087ms step_avg:96.20ms
step:594/1770 train_time:56185ms step_avg:96.21ms
step:595/1770 train_time:56283ms step_avg:96.21ms
step:596/1770 train_time:56381ms step_avg:96.21ms
step:597/1770 train_time:56479ms step_avg:96.22ms
step:598/1770 train_time:56577ms step_avg:96.22ms
step:599/1770 train_time:56675ms step_avg:96.22ms
step:600/1770 train_time:56773ms step_avg:96.23ms
step:601/1770 train_time:56871ms step_avg:96.23ms
step:602/1770 train_time:56969ms step_avg:96.23ms
step:603/1770 train_time:57068ms step_avg:96.24ms
step:604/1770 train_time:57166ms step_avg:96.24ms
step:605/1770 train_time:57263ms step_avg:96.24ms
step:606/1770 train_time:57362ms step_avg:96.24ms
step:607/1770 train_time:57459ms step_avg:96.25ms
step:608/1770 train_time:57557ms step_avg:96.25ms
step:609/1770 train_time:57654ms step_avg:96.25ms
step:610/1770 train_time:57753ms step_avg:96.25ms
step:611/1770 train_time:57851ms step_avg:96.26ms
step:612/1770 train_time:57950ms step_avg:96.26ms
step:613/1770 train_time:58047ms step_avg:96.26ms
step:614/1770 train_time:58145ms step_avg:96.27ms
step:615/1770 train_time:58243ms step_avg:96.27ms
step:616/1770 train_time:58341ms step_avg:96.27ms
step:617/1770 train_time:58439ms step_avg:96.28ms
step:618/1770 train_time:58537ms step_avg:96.28ms
step:619/1770 train_time:58634ms step_avg:96.28ms
step:620/1770 train_time:58733ms step_avg:96.28ms
step:621/1770 train_time:58831ms step_avg:96.29ms
step:622/1770 train_time:58929ms step_avg:96.29ms
step:623/1770 train_time:59027ms step_avg:96.29ms
step:624/1770 train_time:59125ms step_avg:96.30ms
step:625/1770 train_time:59224ms step_avg:96.30ms
step:625/1770 val_loss:3.6607 train_time:59320ms step_avg:96.46ms
step:626/1770 train_time:59341ms step_avg:96.33ms
step:627/1770 train_time:59425ms step_avg:96.31ms
step:628/1770 train_time:59525ms step_avg:96.32ms
step:629/1770 train_time:59623ms step_avg:96.32ms
step:630/1770 train_time:59721ms step_avg:96.32ms
step:631/1770 train_time:59819ms step_avg:96.33ms
step:632/1770 train_time:59917ms step_avg:96.33ms
step:633/1770 train_time:60014ms step_avg:96.33ms
step:634/1770 train_time:60112ms step_avg:96.33ms
step:635/1770 train_time:60210ms step_avg:96.34ms
step:636/1770 train_time:60308ms step_avg:96.34ms
step:637/1770 train_time:60407ms step_avg:96.34ms
step:638/1770 train_time:60505ms step_avg:96.35ms
step:639/1770 train_time:60603ms step_avg:96.35ms
step:640/1770 train_time:60701ms step_avg:96.35ms
step:641/1770 train_time:60798ms step_avg:96.35ms
step:642/1770 train_time:60896ms step_avg:96.35ms
step:643/1770 train_time:60994ms step_avg:96.36ms
step:644/1770 train_time:61091ms step_avg:96.36ms
step:645/1770 train_time:61189ms step_avg:96.36ms
step:646/1770 train_time:61287ms step_avg:96.36ms
step:647/1770 train_time:61385ms step_avg:96.37ms
step:648/1770 train_time:61484ms step_avg:96.37ms
step:649/1770 train_time:61582ms step_avg:96.37ms
step:650/1770 train_time:61680ms step_avg:96.37ms
step:651/1770 train_time:61777ms step_avg:96.38ms
step:652/1770 train_time:61876ms step_avg:96.38ms
step:653/1770 train_time:61973ms step_avg:96.38ms
step:654/1770 train_time:62070ms step_avg:96.38ms
step:655/1770 train_time:62168ms step_avg:96.39ms
step:656/1770 train_time:62267ms step_avg:96.39ms
step:657/1770 train_time:62365ms step_avg:96.39ms
step:658/1770 train_time:62465ms step_avg:96.40ms
step:659/1770 train_time:62564ms step_avg:96.40ms
step:660/1770 train_time:62664ms step_avg:96.41ms
step:661/1770 train_time:62764ms step_avg:96.41ms
step:662/1770 train_time:62864ms step_avg:96.42ms
step:663/1770 train_time:62965ms step_avg:96.42ms
step:664/1770 train_time:63066ms step_avg:96.43ms
step:665/1770 train_time:63166ms step_avg:96.44ms
step:666/1770 train_time:63266ms step_avg:96.44ms
step:667/1770 train_time:63367ms step_avg:96.45ms
step:668/1770 train_time:63467ms step_avg:96.45ms
step:669/1770 train_time:63567ms step_avg:96.46ms
step:670/1770 train_time:63667ms step_avg:96.46ms
step:671/1770 train_time:63767ms step_avg:96.47ms
step:672/1770 train_time:63867ms step_avg:96.48ms
step:673/1770 train_time:63966ms step_avg:96.48ms
step:674/1770 train_time:64067ms step_avg:96.49ms
step:675/1770 train_time:64167ms step_avg:96.49ms
step:676/1770 train_time:64267ms step_avg:96.50ms
step:677/1770 train_time:64367ms step_avg:96.50ms
step:678/1770 train_time:64467ms step_avg:96.51ms
step:679/1770 train_time:64567ms step_avg:96.51ms
step:680/1770 train_time:64666ms step_avg:96.52ms
step:681/1770 train_time:64766ms step_avg:96.52ms
step:682/1770 train_time:64866ms step_avg:96.53ms
step:683/1770 train_time:64967ms step_avg:96.53ms
step:684/1770 train_time:65067ms step_avg:96.54ms
step:685/1770 train_time:65168ms step_avg:96.54ms
step:686/1770 train_time:65268ms step_avg:96.55ms
step:687/1770 train_time:65368ms step_avg:96.55ms
step:688/1770 train_time:65467ms step_avg:96.56ms
step:689/1770 train_time:65567ms step_avg:96.56ms
step:690/1770 train_time:65667ms step_avg:96.57ms
step:691/1770 train_time:65767ms step_avg:96.57ms
step:692/1770 train_time:65867ms step_avg:96.58ms
step:693/1770 train_time:65967ms step_avg:96.58ms
step:694/1770 train_time:66066ms step_avg:96.59ms
step:695/1770 train_time:66167ms step_avg:96.59ms
step:696/1770 train_time:66267ms step_avg:96.60ms
step:697/1770 train_time:66367ms step_avg:96.60ms
step:698/1770 train_time:66467ms step_avg:96.61ms
step:699/1770 train_time:66567ms step_avg:96.61ms
step:700/1770 train_time:66668ms step_avg:96.62ms
step:701/1770 train_time:66768ms step_avg:96.62ms
step:702/1770 train_time:66868ms step_avg:96.63ms
step:703/1770 train_time:66968ms step_avg:96.63ms
step:704/1770 train_time:67069ms step_avg:96.64ms
step:705/1770 train_time:67170ms step_avg:96.65ms
step:706/1770 train_time:67270ms step_avg:96.65ms
step:707/1770 train_time:67370ms step_avg:96.66ms
step:708/1770 train_time:67470ms step_avg:96.66ms
step:709/1770 train_time:67570ms step_avg:96.67ms
step:710/1770 train_time:67670ms step_avg:96.67ms
step:711/1770 train_time:67769ms step_avg:96.67ms
step:712/1770 train_time:67869ms step_avg:96.68ms
step:713/1770 train_time:67969ms step_avg:96.68ms
step:714/1770 train_time:68068ms step_avg:96.69ms
step:715/1770 train_time:68169ms step_avg:96.69ms
step:716/1770 train_time:68269ms step_avg:96.70ms
step:717/1770 train_time:68369ms step_avg:96.70ms
step:718/1770 train_time:68469ms step_avg:96.71ms
step:719/1770 train_time:68569ms step_avg:96.71ms
step:720/1770 train_time:68669ms step_avg:96.72ms
step:721/1770 train_time:68769ms step_avg:96.72ms
step:722/1770 train_time:68868ms step_avg:96.73ms
step:723/1770 train_time:68969ms step_avg:96.73ms
step:724/1770 train_time:69068ms step_avg:96.73ms
step:725/1770 train_time:69168ms step_avg:96.74ms
step:726/1770 train_time:69268ms step_avg:96.74ms
step:727/1770 train_time:69368ms step_avg:96.75ms
step:728/1770 train_time:69469ms step_avg:96.75ms
step:729/1770 train_time:69570ms step_avg:96.76ms
step:730/1770 train_time:69670ms step_avg:96.76ms
step:731/1770 train_time:69770ms step_avg:96.77ms
step:732/1770 train_time:69870ms step_avg:96.77ms
step:733/1770 train_time:69970ms step_avg:96.78ms
step:734/1770 train_time:70069ms step_avg:96.78ms
step:735/1770 train_time:70169ms step_avg:96.79ms
step:736/1770 train_time:70269ms step_avg:96.79ms
step:737/1770 train_time:70368ms step_avg:96.79ms
step:738/1770 train_time:70468ms step_avg:96.80ms
step:739/1770 train_time:70568ms step_avg:96.80ms
step:740/1770 train_time:70668ms step_avg:96.81ms
step:741/1770 train_time:70768ms step_avg:96.81ms
step:742/1770 train_time:70868ms step_avg:96.81ms
step:743/1770 train_time:70968ms step_avg:96.82ms
step:744/1770 train_time:71069ms step_avg:96.82ms
step:745/1770 train_time:71169ms step_avg:96.83ms
step:746/1770 train_time:71269ms step_avg:96.83ms
step:747/1770 train_time:71369ms step_avg:96.84ms
step:748/1770 train_time:71468ms step_avg:96.84ms
step:749/1770 train_time:71568ms step_avg:96.84ms
step:750/1770 train_time:71668ms step_avg:96.85ms
step:750/1770 val_loss:3.5996 train_time:71767ms step_avg:96.98ms
step:751/1770 train_time:71788ms step_avg:96.88ms
step:752/1770 train_time:71874ms step_avg:96.86ms
step:753/1770 train_time:71973ms step_avg:96.87ms
step:754/1770 train_time:72073ms step_avg:96.87ms
step:755/1770 train_time:72173ms step_avg:96.88ms
step:756/1770 train_time:72274ms step_avg:96.88ms
step:757/1770 train_time:72373ms step_avg:96.89ms
step:758/1770 train_time:72473ms step_avg:96.89ms
step:759/1770 train_time:72572ms step_avg:96.89ms
step:760/1770 train_time:72672ms step_avg:96.90ms
step:761/1770 train_time:72773ms step_avg:96.90ms
step:762/1770 train_time:72874ms step_avg:96.91ms
step:763/1770 train_time:72974ms step_avg:96.91ms
step:764/1770 train_time:73075ms step_avg:96.92ms
step:765/1770 train_time:73176ms step_avg:96.92ms
step:766/1770 train_time:73275ms step_avg:96.93ms
step:767/1770 train_time:73376ms step_avg:96.93ms
step:768/1770 train_time:73475ms step_avg:96.93ms
step:769/1770 train_time:73575ms step_avg:96.94ms
step:770/1770 train_time:73676ms step_avg:96.94ms
step:771/1770 train_time:73777ms step_avg:96.95ms
step:772/1770 train_time:73877ms step_avg:96.95ms
step:773/1770 train_time:73976ms step_avg:96.95ms
step:774/1770 train_time:74076ms step_avg:96.96ms
step:775/1770 train_time:74176ms step_avg:96.96ms
step:776/1770 train_time:74276ms step_avg:96.97ms
step:777/1770 train_time:74376ms step_avg:96.97ms
step:778/1770 train_time:74475ms step_avg:96.97ms
step:779/1770 train_time:74575ms step_avg:96.98ms
step:780/1770 train_time:74676ms step_avg:96.98ms
step:781/1770 train_time:74776ms step_avg:96.99ms
step:782/1770 train_time:74876ms step_avg:96.99ms
step:783/1770 train_time:74976ms step_avg:96.99ms
step:784/1770 train_time:75076ms step_avg:97.00ms
step:785/1770 train_time:75176ms step_avg:97.00ms
step:786/1770 train_time:75276ms step_avg:97.01ms
step:787/1770 train_time:75377ms step_avg:97.01ms
step:788/1770 train_time:75477ms step_avg:97.01ms
step:789/1770 train_time:75577ms step_avg:97.02ms
step:790/1770 train_time:75677ms step_avg:97.02ms
step:791/1770 train_time:75777ms step_avg:97.03ms
step:792/1770 train_time:75877ms step_avg:97.03ms
step:793/1770 train_time:75977ms step_avg:97.03ms
step:794/1770 train_time:76077ms step_avg:97.04ms
step:795/1770 train_time:76177ms step_avg:97.04ms
step:796/1770 train_time:76277ms step_avg:97.04ms
step:797/1770 train_time:76377ms step_avg:97.05ms
step:798/1770 train_time:76478ms step_avg:97.05ms
step:799/1770 train_time:76578ms step_avg:97.06ms
step:800/1770 train_time:76678ms step_avg:97.06ms
step:801/1770 train_time:76778ms step_avg:97.06ms
step:802/1770 train_time:76877ms step_avg:97.07ms
step:803/1770 train_time:76978ms step_avg:97.07ms
step:804/1770 train_time:77078ms step_avg:97.08ms
step:805/1770 train_time:77177ms step_avg:97.08ms
step:806/1770 train_time:77277ms step_avg:97.08ms
step:807/1770 train_time:77377ms step_avg:97.09ms
step:808/1770 train_time:77476ms step_avg:97.09ms
step:809/1770 train_time:77577ms step_avg:97.09ms
step:810/1770 train_time:77677ms step_avg:97.10ms
step:811/1770 train_time:77776ms step_avg:97.10ms
step:812/1770 train_time:77876ms step_avg:97.10ms
step:813/1770 train_time:77977ms step_avg:97.11ms
step:814/1770 train_time:78077ms step_avg:97.11ms
step:815/1770 train_time:78177ms step_avg:97.11ms
step:816/1770 train_time:78277ms step_avg:97.12ms
step:817/1770 train_time:78378ms step_avg:97.12ms
step:818/1770 train_time:78477ms step_avg:97.12ms
step:819/1770 train_time:78577ms step_avg:97.13ms
step:820/1770 train_time:78677ms step_avg:97.13ms
step:821/1770 train_time:78777ms step_avg:97.14ms
step:822/1770 train_time:78877ms step_avg:97.14ms
step:823/1770 train_time:78977ms step_avg:97.14ms
step:824/1770 train_time:79077ms step_avg:97.15ms
step:825/1770 train_time:79177ms step_avg:97.15ms
step:826/1770 train_time:79277ms step_avg:97.15ms
step:827/1770 train_time:79377ms step_avg:97.16ms
step:828/1770 train_time:79477ms step_avg:97.16ms
step:829/1770 train_time:79577ms step_avg:97.16ms
step:830/1770 train_time:79677ms step_avg:97.17ms
step:831/1770 train_time:79777ms step_avg:97.17ms
step:832/1770 train_time:79877ms step_avg:97.17ms
step:833/1770 train_time:79977ms step_avg:97.18ms
step:834/1770 train_time:80077ms step_avg:97.18ms
step:835/1770 train_time:80177ms step_avg:97.18ms
step:836/1770 train_time:80277ms step_avg:97.19ms
step:837/1770 train_time:80377ms step_avg:97.19ms
step:838/1770 train_time:80477ms step_avg:97.19ms
step:839/1770 train_time:80577ms step_avg:97.20ms
step:840/1770 train_time:80677ms step_avg:97.20ms
step:841/1770 train_time:80777ms step_avg:97.20ms
step:842/1770 train_time:80877ms step_avg:97.21ms
step:843/1770 train_time:80977ms step_avg:97.21ms
step:844/1770 train_time:81076ms step_avg:97.21ms
step:845/1770 train_time:81176ms step_avg:97.22ms
step:846/1770 train_time:81277ms step_avg:97.22ms
step:847/1770 train_time:81376ms step_avg:97.22ms
step:848/1770 train_time:81477ms step_avg:97.23ms
step:849/1770 train_time:81576ms step_avg:97.23ms
step:850/1770 train_time:81677ms step_avg:97.23ms
step:851/1770 train_time:81778ms step_avg:97.24ms
step:852/1770 train_time:81878ms step_avg:97.24ms
step:853/1770 train_time:81978ms step_avg:97.25ms
step:854/1770 train_time:82077ms step_avg:97.25ms
step:855/1770 train_time:82178ms step_avg:97.25ms
step:856/1770 train_time:82277ms step_avg:97.25ms
step:857/1770 train_time:82377ms step_avg:97.26ms
step:858/1770 train_time:82477ms step_avg:97.26ms
step:859/1770 train_time:82577ms step_avg:97.26ms
step:860/1770 train_time:82676ms step_avg:97.27ms
step:861/1770 train_time:82777ms step_avg:97.27ms
step:862/1770 train_time:82877ms step_avg:97.27ms
step:863/1770 train_time:82977ms step_avg:97.28ms
step:864/1770 train_time:83077ms step_avg:97.28ms
step:865/1770 train_time:83177ms step_avg:97.28ms
step:866/1770 train_time:83277ms step_avg:97.29ms
step:867/1770 train_time:83377ms step_avg:97.29ms
step:868/1770 train_time:83477ms step_avg:97.29ms
step:869/1770 train_time:83577ms step_avg:97.30ms
step:870/1770 train_time:83677ms step_avg:97.30ms
step:871/1770 train_time:83777ms step_avg:97.30ms
step:872/1770 train_time:83877ms step_avg:97.30ms
step:873/1770 train_time:83977ms step_avg:97.31ms
step:874/1770 train_time:84077ms step_avg:97.31ms
step:875/1770 train_time:84176ms step_avg:97.31ms
step:875/1770 val_loss:3.5496 train_time:84275ms step_avg:97.43ms
step:876/1770 train_time:84296ms step_avg:97.34ms
step:877/1770 train_time:84381ms step_avg:97.32ms
step:878/1770 train_time:84482ms step_avg:97.33ms
step:879/1770 train_time:84581ms step_avg:97.33ms
step:880/1770 train_time:84681ms step_avg:97.33ms
step:881/1770 train_time:84781ms step_avg:97.34ms
step:882/1770 train_time:84881ms step_avg:97.34ms
step:883/1770 train_time:84981ms step_avg:97.34ms
step:884/1770 train_time:85081ms step_avg:97.35ms
step:885/1770 train_time:85181ms step_avg:97.35ms
step:886/1770 train_time:85280ms step_avg:97.35ms
step:887/1770 train_time:85381ms step_avg:97.36ms
step:888/1770 train_time:85480ms step_avg:97.36ms
step:889/1770 train_time:85581ms step_avg:97.36ms
step:890/1770 train_time:85680ms step_avg:97.36ms
step:891/1770 train_time:85780ms step_avg:97.37ms
step:892/1770 train_time:85880ms step_avg:97.37ms
step:893/1770 train_time:85980ms step_avg:97.37ms
step:894/1770 train_time:86080ms step_avg:97.38ms
step:895/1770 train_time:86180ms step_avg:97.38ms
step:896/1770 train_time:86280ms step_avg:97.38ms
step:897/1770 train_time:86380ms step_avg:97.38ms
step:898/1770 train_time:86480ms step_avg:97.39ms
step:899/1770 train_time:86580ms step_avg:97.39ms
step:900/1770 train_time:86680ms step_avg:97.39ms
step:901/1770 train_time:86780ms step_avg:97.40ms
step:902/1770 train_time:86881ms step_avg:97.40ms
step:903/1770 train_time:86981ms step_avg:97.40ms
step:904/1770 train_time:87081ms step_avg:97.41ms
step:905/1770 train_time:87181ms step_avg:97.41ms
step:906/1770 train_time:87280ms step_avg:97.41ms
step:907/1770 train_time:87381ms step_avg:97.41ms
step:908/1770 train_time:87481ms step_avg:97.42ms
step:909/1770 train_time:87580ms step_avg:97.42ms
step:910/1770 train_time:87681ms step_avg:97.42ms
step:911/1770 train_time:87780ms step_avg:97.43ms
step:912/1770 train_time:87880ms step_avg:97.43ms
step:913/1770 train_time:87980ms step_avg:97.43ms
step:914/1770 train_time:88080ms step_avg:97.43ms
step:915/1770 train_time:88180ms step_avg:97.44ms
step:916/1770 train_time:88280ms step_avg:97.44ms
step:917/1770 train_time:88380ms step_avg:97.44ms
step:918/1770 train_time:88480ms step_avg:97.44ms
step:919/1770 train_time:88580ms step_avg:97.45ms
step:920/1770 train_time:88682ms step_avg:97.45ms
step:921/1770 train_time:88784ms step_avg:97.46ms
step:922/1770 train_time:88885ms step_avg:97.46ms
step:923/1770 train_time:88985ms step_avg:97.46ms
step:924/1770 train_time:89087ms step_avg:97.47ms
step:925/1770 train_time:89188ms step_avg:97.47ms
step:926/1770 train_time:89289ms step_avg:97.48ms
step:927/1770 train_time:89390ms step_avg:97.48ms
step:928/1770 train_time:89492ms step_avg:97.49ms
step:929/1770 train_time:89595ms step_avg:97.49ms
step:930/1770 train_time:89697ms step_avg:97.50ms
step:931/1770 train_time:89798ms step_avg:97.50ms
step:932/1770 train_time:89899ms step_avg:97.50ms
step:933/1770 train_time:90000ms step_avg:97.51ms
step:934/1770 train_time:90101ms step_avg:97.51ms
step:935/1770 train_time:90203ms step_avg:97.52ms
step:936/1770 train_time:90304ms step_avg:97.52ms
step:937/1770 train_time:90404ms step_avg:97.52ms
step:938/1770 train_time:90505ms step_avg:97.53ms
step:939/1770 train_time:90606ms step_avg:97.53ms
step:940/1770 train_time:90708ms step_avg:97.54ms
step:941/1770 train_time:90809ms step_avg:97.54ms
step:942/1770 train_time:90912ms step_avg:97.55ms
step:943/1770 train_time:91014ms step_avg:97.55ms
step:944/1770 train_time:91116ms step_avg:97.55ms
step:945/1770 train_time:91217ms step_avg:97.56ms
step:946/1770 train_time:91319ms step_avg:97.56ms
step:947/1770 train_time:91421ms step_avg:97.57ms
step:948/1770 train_time:91522ms step_avg:97.57ms
step:949/1770 train_time:91623ms step_avg:97.58ms
step:950/1770 train_time:91725ms step_avg:97.58ms
step:951/1770 train_time:91826ms step_avg:97.58ms
step:952/1770 train_time:91927ms step_avg:97.59ms
step:953/1770 train_time:92028ms step_avg:97.59ms
step:954/1770 train_time:92129ms step_avg:97.59ms
step:955/1770 train_time:92231ms step_avg:97.60ms
step:956/1770 train_time:92333ms step_avg:97.60ms
step:957/1770 train_time:92436ms step_avg:97.61ms
step:958/1770 train_time:92537ms step_avg:97.61ms
step:959/1770 train_time:92639ms step_avg:97.62ms
step:960/1770 train_time:92740ms step_avg:97.62ms
step:961/1770 train_time:92842ms step_avg:97.63ms
step:962/1770 train_time:92943ms step_avg:97.63ms
step:963/1770 train_time:93044ms step_avg:97.63ms
step:964/1770 train_time:93146ms step_avg:97.64ms
step:965/1770 train_time:93246ms step_avg:97.64ms
step:966/1770 train_time:93347ms step_avg:97.64ms
step:967/1770 train_time:93448ms step_avg:97.65ms
step:968/1770 train_time:93550ms step_avg:97.65ms
step:969/1770 train_time:93652ms step_avg:97.66ms
step:970/1770 train_time:93753ms step_avg:97.66ms
step:971/1770 train_time:93856ms step_avg:97.66ms
step:972/1770 train_time:93958ms step_avg:97.67ms
step:973/1770 train_time:94059ms step_avg:97.67ms
step:974/1770 train_time:94161ms step_avg:97.68ms
step:975/1770 train_time:94262ms step_avg:97.68ms
step:976/1770 train_time:94363ms step_avg:97.68ms
step:977/1770 train_time:94465ms step_avg:97.69ms
step:978/1770 train_time:94565ms step_avg:97.69ms
step:979/1770 train_time:94666ms step_avg:97.69ms
step:980/1770 train_time:94767ms step_avg:97.70ms
step:981/1770 train_time:94869ms step_avg:97.70ms
step:982/1770 train_time:94971ms step_avg:97.71ms
step:983/1770 train_time:95073ms step_avg:97.71ms
step:984/1770 train_time:95175ms step_avg:97.72ms
step:985/1770 train_time:95277ms step_avg:97.72ms
step:986/1770 train_time:95379ms step_avg:97.72ms
step:987/1770 train_time:95481ms step_avg:97.73ms
step:988/1770 train_time:95582ms step_avg:97.73ms
step:989/1770 train_time:95685ms step_avg:97.74ms
step:990/1770 train_time:95785ms step_avg:97.74ms
step:991/1770 train_time:95886ms step_avg:97.74ms
step:992/1770 train_time:95988ms step_avg:97.75ms
step:993/1770 train_time:96089ms step_avg:97.75ms
step:994/1770 train_time:96192ms step_avg:97.76ms
step:995/1770 train_time:96293ms step_avg:97.76ms
step:996/1770 train_time:96395ms step_avg:97.76ms
step:997/1770 train_time:96497ms step_avg:97.77ms
step:998/1770 train_time:96598ms step_avg:97.77ms
step:999/1770 train_time:96699ms step_avg:97.77ms
step:1000/1770 train_time:96801ms step_avg:97.78ms
step:1000/1770 val_loss:3.5113 train_time:96900ms step_avg:97.88ms
step:1001/1770 train_time:96922ms step_avg:97.80ms
step:1002/1770 train_time:97010ms step_avg:97.79ms
step:1003/1770 train_time:97113ms step_avg:97.80ms
step:1004/1770 train_time:97214ms step_avg:97.80ms
step:1005/1770 train_time:97315ms step_avg:97.80ms
step:1006/1770 train_time:97416ms step_avg:97.81ms
step:1007/1770 train_time:97517ms step_avg:97.81ms
step:1008/1770 train_time:97618ms step_avg:97.81ms
step:1009/1770 train_time:97719ms step_avg:97.82ms
step:1010/1770 train_time:97820ms step_avg:97.82ms
step:1011/1770 train_time:97921ms step_avg:97.82ms
step:1012/1770 train_time:98023ms step_avg:97.83ms
step:1013/1770 train_time:98124ms step_avg:97.83ms
step:1014/1770 train_time:98225ms step_avg:97.83ms
step:1015/1770 train_time:98327ms step_avg:97.84ms
step:1016/1770 train_time:98428ms step_avg:97.84ms
step:1017/1770 train_time:98530ms step_avg:97.85ms
step:1018/1770 train_time:98632ms step_avg:97.85ms
step:1019/1770 train_time:98733ms step_avg:97.85ms
step:1020/1770 train_time:98835ms step_avg:97.86ms
step:1021/1770 train_time:98937ms step_avg:97.86ms
step:1022/1770 train_time:99039ms step_avg:97.86ms
step:1023/1770 train_time:99140ms step_avg:97.87ms
step:1024/1770 train_time:99241ms step_avg:97.87ms
step:1025/1770 train_time:99342ms step_avg:97.87ms
step:1026/1770 train_time:99443ms step_avg:97.88ms
step:1027/1770 train_time:99545ms step_avg:97.88ms
step:1028/1770 train_time:99647ms step_avg:97.89ms
step:1029/1770 train_time:99748ms step_avg:97.89ms
step:1030/1770 train_time:99851ms step_avg:97.89ms
step:1031/1770 train_time:99952ms step_avg:97.90ms
step:1032/1770 train_time:100054ms step_avg:97.90ms
step:1033/1770 train_time:100156ms step_avg:97.90ms
step:1034/1770 train_time:100258ms step_avg:97.91ms
step:1035/1770 train_time:100359ms step_avg:97.91ms
step:1036/1770 train_time:100459ms step_avg:97.91ms
step:1037/1770 train_time:100560ms step_avg:97.92ms
step:1038/1770 train_time:100661ms step_avg:97.92ms
step:1039/1770 train_time:100762ms step_avg:97.92ms
step:1040/1770 train_time:100864ms step_avg:97.93ms
step:1041/1770 train_time:100965ms step_avg:97.93ms
step:1042/1770 train_time:101068ms step_avg:97.93ms
step:1043/1770 train_time:101171ms step_avg:97.94ms
step:1044/1770 train_time:101272ms step_avg:97.94ms
step:1045/1770 train_time:101373ms step_avg:97.94ms
step:1046/1770 train_time:101474ms step_avg:97.95ms
step:1047/1770 train_time:101576ms step_avg:97.95ms
step:1048/1770 train_time:101677ms step_avg:97.95ms
step:1049/1770 train_time:101778ms step_avg:97.96ms
step:1050/1770 train_time:101880ms step_avg:97.96ms
step:1051/1770 train_time:101982ms step_avg:97.97ms
step:1052/1770 train_time:102083ms step_avg:97.97ms
step:1053/1770 train_time:102184ms step_avg:97.97ms
step:1054/1770 train_time:102285ms step_avg:97.97ms
step:1055/1770 train_time:102388ms step_avg:97.98ms
step:1056/1770 train_time:102489ms step_avg:97.98ms
step:1057/1770 train_time:102590ms step_avg:97.99ms
step:1058/1770 train_time:102692ms step_avg:97.99ms
step:1059/1770 train_time:102794ms step_avg:97.99ms
step:1060/1770 train_time:102896ms step_avg:98.00ms
step:1061/1770 train_time:102998ms step_avg:98.00ms
step:1062/1770 train_time:103100ms step_avg:98.00ms
step:1063/1770 train_time:103202ms step_avg:98.01ms
step:1064/1770 train_time:103305ms step_avg:98.01ms
step:1065/1770 train_time:103406ms step_avg:98.02ms
step:1066/1770 train_time:103508ms step_avg:98.02ms
step:1067/1770 train_time:103610ms step_avg:98.02ms
step:1068/1770 train_time:103713ms step_avg:98.03ms
step:1069/1770 train_time:103815ms step_avg:98.03ms
step:1070/1770 train_time:103917ms step_avg:98.03ms
step:1071/1770 train_time:104019ms step_avg:98.04ms
step:1072/1770 train_time:104120ms step_avg:98.04ms
step:1073/1770 train_time:104220ms step_avg:98.04ms
step:1074/1770 train_time:104322ms step_avg:98.05ms
step:1075/1770 train_time:104424ms step_avg:98.05ms
step:1076/1770 train_time:104527ms step_avg:98.05ms
step:1077/1770 train_time:104628ms step_avg:98.06ms
step:1078/1770 train_time:104730ms step_avg:98.06ms
step:1079/1770 train_time:104832ms step_avg:98.07ms
step:1080/1770 train_time:104933ms step_avg:98.07ms
step:1081/1770 train_time:105034ms step_avg:98.07ms
step:1082/1770 train_time:105136ms step_avg:98.08ms
step:1083/1770 train_time:105238ms step_avg:98.08ms
step:1084/1770 train_time:105339ms step_avg:98.08ms
step:1085/1770 train_time:105440ms step_avg:98.08ms
step:1086/1770 train_time:105541ms step_avg:98.09ms
step:1087/1770 train_time:105642ms step_avg:98.09ms
step:1088/1770 train_time:105744ms step_avg:98.09ms
step:1089/1770 train_time:105846ms step_avg:98.10ms
step:1090/1770 train_time:105948ms step_avg:98.10ms
step:1091/1770 train_time:106050ms step_avg:98.10ms
step:1092/1770 train_time:106152ms step_avg:98.11ms
step:1093/1770 train_time:106253ms step_avg:98.11ms
step:1094/1770 train_time:106355ms step_avg:98.11ms
step:1095/1770 train_time:106456ms step_avg:98.12ms
step:1096/1770 train_time:106559ms step_avg:98.12ms
step:1097/1770 train_time:106660ms step_avg:98.12ms
step:1098/1770 train_time:106761ms step_avg:98.13ms
step:1099/1770 train_time:106862ms step_avg:98.13ms
step:1100/1770 train_time:106963ms step_avg:98.13ms
step:1101/1770 train_time:107065ms step_avg:98.13ms
step:1102/1770 train_time:107168ms step_avg:98.14ms
step:1103/1770 train_time:107270ms step_avg:98.14ms
step:1104/1770 train_time:107372ms step_avg:98.15ms
step:1105/1770 train_time:107473ms step_avg:98.15ms
step:1106/1770 train_time:107575ms step_avg:98.15ms
step:1107/1770 train_time:107677ms step_avg:98.16ms
step:1108/1770 train_time:107779ms step_avg:98.16ms
step:1109/1770 train_time:107879ms step_avg:98.16ms
step:1110/1770 train_time:107980ms step_avg:98.16ms
step:1111/1770 train_time:108082ms step_avg:98.17ms
step:1112/1770 train_time:108183ms step_avg:98.17ms
step:1113/1770 train_time:108285ms step_avg:98.17ms
step:1114/1770 train_time:108387ms step_avg:98.18ms
step:1115/1770 train_time:108489ms step_avg:98.18ms
step:1116/1770 train_time:108592ms step_avg:98.18ms
step:1117/1770 train_time:108693ms step_avg:98.19ms
step:1118/1770 train_time:108794ms step_avg:98.19ms
step:1119/1770 train_time:108896ms step_avg:98.19ms
step:1120/1770 train_time:108998ms step_avg:98.20ms
step:1121/1770 train_time:109099ms step_avg:98.20ms
step:1122/1770 train_time:109200ms step_avg:98.20ms
step:1123/1770 train_time:109301ms step_avg:98.20ms
step:1124/1770 train_time:109402ms step_avg:98.21ms
step:1125/1770 train_time:109504ms step_avg:98.21ms
step:1125/1770 val_loss:3.4714 train_time:109603ms step_avg:98.30ms
step:1126/1770 train_time:109625ms step_avg:98.23ms
step:1127/1770 train_time:109714ms step_avg:98.22ms
step:1128/1770 train_time:109816ms step_avg:98.23ms
step:1129/1770 train_time:109917ms step_avg:98.23ms
step:1130/1770 train_time:110018ms step_avg:98.23ms
step:1131/1770 train_time:110120ms step_avg:98.23ms
step:1132/1770 train_time:110221ms step_avg:98.24ms
step:1133/1770 train_time:110322ms step_avg:98.24ms
step:1134/1770 train_time:110424ms step_avg:98.24ms
step:1135/1770 train_time:110525ms step_avg:98.24ms
step:1136/1770 train_time:110628ms step_avg:98.25ms
step:1137/1770 train_time:110731ms step_avg:98.25ms
step:1138/1770 train_time:110832ms step_avg:98.26ms
step:1139/1770 train_time:110935ms step_avg:98.26ms
step:1140/1770 train_time:111036ms step_avg:98.26ms
step:1141/1770 train_time:111137ms step_avg:98.26ms
step:1142/1770 train_time:111238ms step_avg:98.27ms
step:1143/1770 train_time:111339ms step_avg:98.27ms
step:1144/1770 train_time:111440ms step_avg:98.27ms
step:1145/1770 train_time:111541ms step_avg:98.27ms
step:1146/1770 train_time:111643ms step_avg:98.28ms
step:1147/1770 train_time:111745ms step_avg:98.28ms
step:1148/1770 train_time:111847ms step_avg:98.28ms
step:1149/1770 train_time:111949ms step_avg:98.29ms
step:1150/1770 train_time:112050ms step_avg:98.29ms
step:1151/1770 train_time:112154ms step_avg:98.29ms
step:1152/1770 train_time:112257ms step_avg:98.30ms
step:1153/1770 train_time:112358ms step_avg:98.30ms
step:1154/1770 train_time:112459ms step_avg:98.30ms
step:1155/1770 train_time:112561ms step_avg:98.31ms
step:1156/1770 train_time:112663ms step_avg:98.31ms
step:1157/1770 train_time:112766ms step_avg:98.31ms
step:1158/1770 train_time:112868ms step_avg:98.32ms
step:1159/1770 train_time:112970ms step_avg:98.32ms
step:1160/1770 train_time:113071ms step_avg:98.32ms
step:1161/1770 train_time:113172ms step_avg:98.33ms
step:1162/1770 train_time:113274ms step_avg:98.33ms
step:1163/1770 train_time:113376ms step_avg:98.33ms
step:1164/1770 train_time:113477ms step_avg:98.33ms
step:1165/1770 train_time:113578ms step_avg:98.34ms
step:1166/1770 train_time:113681ms step_avg:98.34ms
step:1167/1770 train_time:113782ms step_avg:98.34ms
step:1168/1770 train_time:113884ms step_avg:98.35ms
step:1169/1770 train_time:113986ms step_avg:98.35ms
step:1170/1770 train_time:114087ms step_avg:98.35ms
step:1171/1770 train_time:114189ms step_avg:98.35ms
step:1172/1770 train_time:114291ms step_avg:98.36ms
step:1173/1770 train_time:114393ms step_avg:98.36ms
step:1174/1770 train_time:114495ms step_avg:98.36ms
step:1175/1770 train_time:114596ms step_avg:98.37ms
step:1176/1770 train_time:114698ms step_avg:98.37ms
step:1177/1770 train_time:114799ms step_avg:98.37ms
step:1178/1770 train_time:114901ms step_avg:98.37ms
step:1179/1770 train_time:115003ms step_avg:98.38ms
step:1180/1770 train_time:115105ms step_avg:98.38ms
step:1181/1770 train_time:115208ms step_avg:98.38ms
step:1182/1770 train_time:115309ms step_avg:98.39ms
step:1183/1770 train_time:115412ms step_avg:98.39ms
step:1184/1770 train_time:115515ms step_avg:98.39ms
step:1185/1770 train_time:115618ms step_avg:98.40ms
step:1186/1770 train_time:115720ms step_avg:98.40ms
step:1187/1770 train_time:115825ms step_avg:98.41ms
step:1188/1770 train_time:115928ms step_avg:98.41ms
step:1189/1770 train_time:116030ms step_avg:98.41ms
step:1190/1770 train_time:116132ms step_avg:98.42ms
step:1191/1770 train_time:116236ms step_avg:98.42ms
step:1192/1770 train_time:116338ms step_avg:98.42ms
step:1193/1770 train_time:116441ms step_avg:98.43ms
step:1194/1770 train_time:116543ms step_avg:98.43ms
step:1195/1770 train_time:116647ms step_avg:98.44ms
step:1196/1770 train_time:116750ms step_avg:98.44ms
step:1197/1770 train_time:116852ms step_avg:98.44ms
step:1198/1770 train_time:116954ms step_avg:98.45ms
step:1199/1770 train_time:117057ms step_avg:98.45ms
step:1200/1770 train_time:117159ms step_avg:98.45ms
step:1201/1770 train_time:117264ms step_avg:98.46ms
step:1202/1770 train_time:117366ms step_avg:98.46ms
step:1203/1770 train_time:117469ms step_avg:98.47ms
step:1204/1770 train_time:117571ms step_avg:98.47ms
step:1205/1770 train_time:117674ms step_avg:98.47ms
step:1206/1770 train_time:117777ms step_avg:98.48ms
step:1207/1770 train_time:117879ms step_avg:98.48ms
step:1208/1770 train_time:117981ms step_avg:98.48ms
step:1209/1770 train_time:118084ms step_avg:98.49ms
step:1210/1770 train_time:118186ms step_avg:98.49ms
step:1211/1770 train_time:118289ms step_avg:98.49ms
step:1212/1770 train_time:118394ms step_avg:98.50ms
step:1213/1770 train_time:118497ms step_avg:98.50ms
step:1214/1770 train_time:118599ms step_avg:98.50ms
step:1215/1770 train_time:118703ms step_avg:98.51ms
step:1216/1770 train_time:118808ms step_avg:98.51ms
step:1217/1770 train_time:118910ms step_avg:98.52ms
step:1218/1770 train_time:119013ms step_avg:98.52ms
step:1219/1770 train_time:119116ms step_avg:98.52ms
step:1220/1770 train_time:119218ms step_avg:98.53ms
step:1221/1770 train_time:119320ms step_avg:98.53ms
step:1222/1770 train_time:119426ms step_avg:98.54ms
step:1223/1770 train_time:119528ms step_avg:98.54ms
step:1224/1770 train_time:119632ms step_avg:98.54ms
step:1225/1770 train_time:119735ms step_avg:98.55ms
step:1226/1770 train_time:119837ms step_avg:98.55ms
step:1227/1770 train_time:119941ms step_avg:98.56ms
step:1228/1770 train_time:120046ms step_avg:98.56ms
step:1229/1770 train_time:120149ms step_avg:98.56ms
step:1230/1770 train_time:120253ms step_avg:98.57ms
step:1231/1770 train_time:120356ms step_avg:98.57ms
step:1232/1770 train_time:120458ms step_avg:98.57ms
step:1233/1770 train_time:120561ms step_avg:98.58ms
step:1234/1770 train_time:120663ms step_avg:98.58ms
step:1235/1770 train_time:120766ms step_avg:98.58ms
step:1236/1770 train_time:120869ms step_avg:98.59ms
step:1237/1770 train_time:120971ms step_avg:98.59ms
step:1238/1770 train_time:121075ms step_avg:98.60ms
step:1239/1770 train_time:121178ms step_avg:98.60ms
step:1240/1770 train_time:121282ms step_avg:98.60ms
step:1241/1770 train_time:121385ms step_avg:98.61ms
step:1242/1770 train_time:121487ms step_avg:98.61ms
step:1243/1770 train_time:121590ms step_avg:98.61ms
step:1244/1770 train_time:121692ms step_avg:98.62ms
step:1245/1770 train_time:121795ms step_avg:98.62ms
step:1246/1770 train_time:121897ms step_avg:98.62ms
step:1247/1770 train_time:122000ms step_avg:98.63ms
step:1248/1770 train_time:122104ms step_avg:98.63ms
step:1249/1770 train_time:122206ms step_avg:98.63ms
step:1250/1770 train_time:122309ms step_avg:98.64ms
step:1250/1770 val_loss:3.4248 train_time:122411ms step_avg:98.72ms
step:1251/1770 train_time:122432ms step_avg:98.66ms
step:1252/1770 train_time:122524ms step_avg:98.65ms
step:1253/1770 train_time:122627ms step_avg:98.65ms
step:1254/1770 train_time:122730ms step_avg:98.66ms
step:1255/1770 train_time:122836ms step_avg:98.66ms
step:1256/1770 train_time:122939ms step_avg:98.67ms
step:1257/1770 train_time:123042ms step_avg:98.67ms
step:1258/1770 train_time:123145ms step_avg:98.67ms
step:1259/1770 train_time:123247ms step_avg:98.68ms
step:1260/1770 train_time:123350ms step_avg:98.68ms
step:1261/1770 train_time:123455ms step_avg:98.68ms
step:1262/1770 train_time:123558ms step_avg:98.69ms
step:1263/1770 train_time:123660ms step_avg:98.69ms
step:1264/1770 train_time:123764ms step_avg:98.70ms
step:1265/1770 train_time:123866ms step_avg:98.70ms
step:1266/1770 train_time:123970ms step_avg:98.70ms
step:1267/1770 train_time:124073ms step_avg:98.71ms
step:1268/1770 train_time:124178ms step_avg:98.71ms
step:1269/1770 train_time:124281ms step_avg:98.71ms
step:1270/1770 train_time:124384ms step_avg:98.72ms
step:1271/1770 train_time:124486ms step_avg:98.72ms
step:1272/1770 train_time:124588ms step_avg:98.72ms
step:1273/1770 train_time:124691ms step_avg:98.73ms
step:1274/1770 train_time:124794ms step_avg:98.73ms
step:1275/1770 train_time:124897ms step_avg:98.73ms
step:1276/1770 train_time:125001ms step_avg:98.74ms
step:1277/1770 train_time:125103ms step_avg:98.74ms
step:1278/1770 train_time:125206ms step_avg:98.74ms
step:1279/1770 train_time:125310ms step_avg:98.75ms
step:1280/1770 train_time:125414ms step_avg:98.75ms
step:1281/1770 train_time:125517ms step_avg:98.75ms
step:1282/1770 train_time:125620ms step_avg:98.76ms
step:1283/1770 train_time:125723ms step_avg:98.76ms
step:1284/1770 train_time:125826ms step_avg:98.76ms
step:1285/1770 train_time:125929ms step_avg:98.77ms
step:1286/1770 train_time:126033ms step_avg:98.77ms
step:1287/1770 train_time:126138ms step_avg:98.78ms
step:1288/1770 train_time:126241ms step_avg:98.78ms
step:1289/1770 train_time:126344ms step_avg:98.78ms
step:1290/1770 train_time:126446ms step_avg:98.79ms
step:1291/1770 train_time:126550ms step_avg:98.79ms
step:1292/1770 train_time:126653ms step_avg:98.79ms
step:1293/1770 train_time:126757ms step_avg:98.80ms
step:1294/1770 train_time:126859ms step_avg:98.80ms
step:1295/1770 train_time:126961ms step_avg:98.80ms
step:1296/1770 train_time:127064ms step_avg:98.81ms
step:1297/1770 train_time:127166ms step_avg:98.81ms
step:1298/1770 train_time:127269ms step_avg:98.81ms
step:1299/1770 train_time:127371ms step_avg:98.81ms
step:1300/1770 train_time:127473ms step_avg:98.82ms
step:1301/1770 train_time:127577ms step_avg:98.82ms
step:1302/1770 train_time:127679ms step_avg:98.82ms
step:1303/1770 train_time:127783ms step_avg:98.83ms
step:1304/1770 train_time:127885ms step_avg:98.83ms
step:1305/1770 train_time:127988ms step_avg:98.83ms
step:1306/1770 train_time:128091ms step_avg:98.84ms
step:1307/1770 train_time:128193ms step_avg:98.84ms
step:1308/1770 train_time:128297ms step_avg:98.84ms
step:1309/1770 train_time:128400ms step_avg:98.85ms
step:1310/1770 train_time:128502ms step_avg:98.85ms
step:1311/1770 train_time:128604ms step_avg:98.85ms
step:1312/1770 train_time:128706ms step_avg:98.85ms
step:1313/1770 train_time:128808ms step_avg:98.86ms
step:1314/1770 train_time:128911ms step_avg:98.86ms
step:1315/1770 train_time:129014ms step_avg:98.86ms
step:1316/1770 train_time:129117ms step_avg:98.86ms
step:1317/1770 train_time:129221ms step_avg:98.87ms
step:1318/1770 train_time:129326ms step_avg:98.87ms
step:1319/1770 train_time:129431ms step_avg:98.88ms
step:1320/1770 train_time:129533ms step_avg:98.88ms
step:1321/1770 train_time:129636ms step_avg:98.88ms
step:1322/1770 train_time:129739ms step_avg:98.89ms
step:1323/1770 train_time:129843ms step_avg:98.89ms
step:1324/1770 train_time:129946ms step_avg:98.89ms
step:1325/1770 train_time:130051ms step_avg:98.90ms
step:1326/1770 train_time:130154ms step_avg:98.90ms
step:1327/1770 train_time:130259ms step_avg:98.91ms
step:1328/1770 train_time:130362ms step_avg:98.91ms
step:1329/1770 train_time:130464ms step_avg:98.91ms
step:1330/1770 train_time:130566ms step_avg:98.91ms
step:1331/1770 train_time:130669ms step_avg:98.92ms
step:1332/1770 train_time:130772ms step_avg:98.92ms
step:1333/1770 train_time:130874ms step_avg:98.92ms
step:1334/1770 train_time:130977ms step_avg:98.93ms
step:1335/1770 train_time:131080ms step_avg:98.93ms
step:1336/1770 train_time:131182ms step_avg:98.93ms
step:1337/1770 train_time:131285ms step_avg:98.93ms
step:1338/1770 train_time:131387ms step_avg:98.94ms
step:1339/1770 train_time:131491ms step_avg:98.94ms
step:1340/1770 train_time:131595ms step_avg:98.94ms
step:1341/1770 train_time:131697ms step_avg:98.95ms
step:1342/1770 train_time:131801ms step_avg:98.95ms
step:1343/1770 train_time:131904ms step_avg:98.95ms
step:1344/1770 train_time:132008ms step_avg:98.96ms
step:1345/1770 train_time:132110ms step_avg:98.96ms
step:1346/1770 train_time:132214ms step_avg:98.96ms
step:1347/1770 train_time:132317ms step_avg:98.97ms
step:1348/1770 train_time:132422ms step_avg:98.97ms
step:1349/1770 train_time:132524ms step_avg:98.97ms
step:1350/1770 train_time:132627ms step_avg:98.98ms
step:1351/1770 train_time:132730ms step_avg:98.98ms
step:1352/1770 train_time:132833ms step_avg:98.98ms
step:1353/1770 train_time:132936ms step_avg:98.98ms
step:1354/1770 train_time:133040ms step_avg:98.99ms
step:1355/1770 train_time:133142ms step_avg:98.99ms
step:1356/1770 train_time:133244ms step_avg:98.99ms
step:1357/1770 train_time:133347ms step_avg:99.00ms
step:1358/1770 train_time:133451ms step_avg:99.00ms
step:1359/1770 train_time:133554ms step_avg:99.00ms
step:1360/1770 train_time:133657ms step_avg:99.01ms
step:1361/1770 train_time:133761ms step_avg:99.01ms
step:1362/1770 train_time:133864ms step_avg:99.01ms
step:1363/1770 train_time:133968ms step_avg:99.02ms
step:1364/1770 train_time:134072ms step_avg:99.02ms
step:1365/1770 train_time:134175ms step_avg:99.02ms
step:1366/1770 train_time:134277ms step_avg:99.02ms
step:1367/1770 train_time:134381ms step_avg:99.03ms
step:1368/1770 train_time:134483ms step_avg:99.03ms
step:1369/1770 train_time:134586ms step_avg:99.03ms
step:1370/1770 train_time:134689ms step_avg:99.04ms
step:1371/1770 train_time:134792ms step_avg:99.04ms
step:1372/1770 train_time:134895ms step_avg:99.04ms
step:1373/1770 train_time:134998ms step_avg:99.04ms
step:1374/1770 train_time:135101ms step_avg:99.05ms
step:1375/1770 train_time:135204ms step_avg:99.05ms
step:1375/1770 val_loss:3.3828 train_time:135306ms step_avg:99.13ms
step:1376/1770 train_time:135328ms step_avg:99.07ms
step:1377/1770 train_time:135418ms step_avg:99.06ms
step:1378/1770 train_time:135520ms step_avg:99.06ms
step:1379/1770 train_time:135624ms step_avg:99.07ms
step:1380/1770 train_time:135726ms step_avg:99.07ms
step:1381/1770 train_time:135829ms step_avg:99.07ms
step:1382/1770 train_time:135931ms step_avg:99.08ms
step:1383/1770 train_time:136035ms step_avg:99.08ms
step:1384/1770 train_time:136139ms step_avg:99.08ms
step:1385/1770 train_time:136242ms step_avg:99.08ms
step:1386/1770 train_time:136345ms step_avg:99.09ms
step:1387/1770 train_time:136448ms step_avg:99.09ms
step:1388/1770 train_time:136551ms step_avg:99.09ms
step:1389/1770 train_time:136654ms step_avg:99.10ms
step:1390/1770 train_time:136756ms step_avg:99.10ms
step:1391/1770 train_time:136859ms step_avg:99.10ms
step:1392/1770 train_time:136962ms step_avg:99.10ms
step:1393/1770 train_time:137064ms step_avg:99.11ms
step:1394/1770 train_time:137167ms step_avg:99.11ms
step:1395/1770 train_time:137270ms step_avg:99.11ms
step:1396/1770 train_time:137374ms step_avg:99.12ms
step:1397/1770 train_time:137477ms step_avg:99.12ms
step:1398/1770 train_time:137581ms step_avg:99.12ms
step:1399/1770 train_time:137684ms step_avg:99.12ms
step:1400/1770 train_time:137787ms step_avg:99.13ms
step:1401/1770 train_time:137889ms step_avg:99.13ms
step:1402/1770 train_time:137992ms step_avg:99.13ms
step:1403/1770 train_time:138095ms step_avg:99.14ms
step:1404/1770 train_time:138198ms step_avg:99.14ms
step:1405/1770 train_time:138301ms step_avg:99.14ms
step:1406/1770 train_time:138405ms step_avg:99.14ms
step:1407/1770 train_time:138507ms step_avg:99.15ms
step:1408/1770 train_time:138611ms step_avg:99.15ms
step:1409/1770 train_time:138714ms step_avg:99.15ms
step:1410/1770 train_time:138817ms step_avg:99.16ms
step:1411/1770 train_time:138920ms step_avg:99.16ms
step:1412/1770 train_time:139023ms step_avg:99.16ms
step:1413/1770 train_time:139125ms step_avg:99.16ms
step:1414/1770 train_time:139228ms step_avg:99.17ms
step:1415/1770 train_time:139332ms step_avg:99.17ms
step:1416/1770 train_time:139436ms step_avg:99.17ms
step:1417/1770 train_time:139539ms step_avg:99.17ms
step:1418/1770 train_time:139642ms step_avg:99.18ms
step:1419/1770 train_time:139745ms step_avg:99.18ms
step:1420/1770 train_time:139848ms step_avg:99.18ms
step:1421/1770 train_time:139950ms step_avg:99.19ms
step:1422/1770 train_time:140053ms step_avg:99.19ms
step:1423/1770 train_time:140156ms step_avg:99.19ms
step:1424/1770 train_time:140259ms step_avg:99.19ms
step:1425/1770 train_time:140363ms step_avg:99.20ms
step:1426/1770 train_time:140465ms step_avg:99.20ms
step:1427/1770 train_time:140567ms step_avg:99.20ms
step:1428/1770 train_time:140672ms step_avg:99.20ms
step:1429/1770 train_time:140775ms step_avg:99.21ms
step:1430/1770 train_time:140877ms step_avg:99.21ms
step:1431/1770 train_time:140981ms step_avg:99.21ms
step:1432/1770 train_time:141083ms step_avg:99.21ms
step:1433/1770 train_time:141186ms step_avg:99.22ms
step:1434/1770 train_time:141288ms step_avg:99.22ms
step:1435/1770 train_time:141391ms step_avg:99.22ms
step:1436/1770 train_time:141495ms step_avg:99.23ms
step:1437/1770 train_time:141598ms step_avg:99.23ms
step:1438/1770 train_time:141700ms step_avg:99.23ms
step:1439/1770 train_time:141802ms step_avg:99.23ms
step:1440/1770 train_time:141904ms step_avg:99.23ms
step:1441/1770 train_time:142009ms step_avg:99.24ms
step:1442/1770 train_time:142112ms step_avg:99.24ms
step:1443/1770 train_time:142215ms step_avg:99.24ms
step:1444/1770 train_time:142318ms step_avg:99.25ms
step:1445/1770 train_time:142422ms step_avg:99.25ms
step:1446/1770 train_time:142526ms step_avg:99.25ms
step:1447/1770 train_time:142629ms step_avg:99.25ms
step:1448/1770 train_time:142734ms step_avg:99.26ms
step:1449/1770 train_time:142838ms step_avg:99.26ms
step:1450/1770 train_time:142942ms step_avg:99.26ms
step:1451/1770 train_time:143046ms step_avg:99.27ms
step:1452/1770 train_time:143151ms step_avg:99.27ms
step:1453/1770 train_time:143255ms step_avg:99.28ms
step:1454/1770 train_time:143359ms step_avg:99.28ms
step:1455/1770 train_time:143464ms step_avg:99.28ms
step:1456/1770 train_time:143568ms step_avg:99.29ms
step:1457/1770 train_time:143673ms step_avg:99.29ms
step:1458/1770 train_time:143778ms step_avg:99.29ms
step:1459/1770 train_time:143882ms step_avg:99.30ms
step:1460/1770 train_time:143986ms step_avg:99.30ms
step:1461/1770 train_time:144090ms step_avg:99.30ms
step:1462/1770 train_time:144194ms step_avg:99.31ms
step:1463/1770 train_time:144298ms step_avg:99.31ms
step:1464/1770 train_time:144403ms step_avg:99.31ms
step:1465/1770 train_time:144506ms step_avg:99.32ms
step:1466/1770 train_time:144611ms step_avg:99.32ms
step:1467/1770 train_time:144717ms step_avg:99.33ms
step:1468/1770 train_time:144821ms step_avg:99.33ms
step:1469/1770 train_time:144925ms step_avg:99.33ms
step:1470/1770 train_time:145028ms step_avg:99.33ms
step:1471/1770 train_time:145131ms step_avg:99.34ms
step:1472/1770 train_time:145235ms step_avg:99.34ms
step:1473/1770 train_time:145340ms step_avg:99.34ms
step:1474/1770 train_time:145446ms step_avg:99.35ms
step:1475/1770 train_time:145549ms step_avg:99.35ms
step:1476/1770 train_time:145652ms step_avg:99.35ms
step:1477/1770 train_time:145759ms step_avg:99.36ms
step:1478/1770 train_time:145863ms step_avg:99.36ms
step:1479/1770 train_time:145967ms step_avg:99.36ms
step:1480/1770 train_time:146071ms step_avg:99.37ms
step:1481/1770 train_time:146180ms step_avg:99.37ms
step:1482/1770 train_time:146284ms step_avg:99.38ms
step:1483/1770 train_time:146388ms step_avg:99.38ms
step:1484/1770 train_time:146491ms step_avg:99.38ms
step:1485/1770 train_time:146595ms step_avg:99.39ms
step:1486/1770 train_time:146700ms step_avg:99.39ms
step:1487/1770 train_time:146804ms step_avg:99.39ms
step:1488/1770 train_time:146908ms step_avg:99.40ms
step:1489/1770 train_time:147013ms step_avg:99.40ms
step:1490/1770 train_time:147117ms step_avg:99.40ms
step:1491/1770 train_time:147221ms step_avg:99.41ms
step:1492/1770 train_time:147326ms step_avg:99.41ms
step:1493/1770 train_time:147432ms step_avg:99.41ms
step:1494/1770 train_time:147540ms step_avg:99.42ms
step:1495/1770 train_time:147643ms step_avg:99.42ms
step:1496/1770 train_time:147747ms step_avg:99.43ms
step:1497/1770 train_time:147852ms step_avg:99.43ms
step:1498/1770 train_time:147955ms step_avg:99.43ms
step:1499/1770 train_time:148058ms step_avg:99.43ms
step:1500/1770 train_time:148162ms step_avg:99.44ms
step:1500/1770 val_loss:3.3457 train_time:148263ms step_avg:99.51ms
step:1501/1770 train_time:148285ms step_avg:99.45ms
step:1502/1770 train_time:148375ms step_avg:99.45ms
step:1503/1770 train_time:148479ms step_avg:99.45ms
step:1504/1770 train_time:148583ms step_avg:99.45ms
step:1505/1770 train_time:148689ms step_avg:99.46ms
step:1506/1770 train_time:148793ms step_avg:99.46ms
step:1507/1770 train_time:148897ms step_avg:99.46ms
step:1508/1770 train_time:149003ms step_avg:99.47ms
step:1509/1770 train_time:149107ms step_avg:99.47ms
step:1510/1770 train_time:149211ms step_avg:99.47ms
step:1511/1770 train_time:149315ms step_avg:99.48ms
step:1512/1770 train_time:149420ms step_avg:99.48ms
step:1513/1770 train_time:149525ms step_avg:99.48ms
step:1514/1770 train_time:149629ms step_avg:99.49ms
step:1515/1770 train_time:149733ms step_avg:99.49ms
step:1516/1770 train_time:149837ms step_avg:99.49ms
step:1517/1770 train_time:149941ms step_avg:99.50ms
step:1518/1770 train_time:150048ms step_avg:99.50ms
step:1519/1770 train_time:150151ms step_avg:99.50ms
step:1520/1770 train_time:150256ms step_avg:99.51ms
step:1521/1770 train_time:150360ms step_avg:99.51ms
step:1522/1770 train_time:150464ms step_avg:99.51ms
step:1523/1770 train_time:150569ms step_avg:99.52ms
step:1524/1770 train_time:150673ms step_avg:99.52ms
step:1525/1770 train_time:150776ms step_avg:99.52ms
step:1526/1770 train_time:150881ms step_avg:99.53ms
step:1527/1770 train_time:150984ms step_avg:99.53ms
step:1528/1770 train_time:151090ms step_avg:99.53ms
step:1529/1770 train_time:151194ms step_avg:99.53ms
step:1530/1770 train_time:151297ms step_avg:99.54ms
step:1531/1770 train_time:151402ms step_avg:99.54ms
step:1532/1770 train_time:151506ms step_avg:99.54ms
step:1533/1770 train_time:151611ms step_avg:99.55ms
step:1534/1770 train_time:151716ms step_avg:99.55ms
step:1535/1770 train_time:151820ms step_avg:99.55ms
step:1536/1770 train_time:151923ms step_avg:99.56ms
step:1537/1770 train_time:152028ms step_avg:99.56ms
step:1538/1770 train_time:152133ms step_avg:99.56ms
step:1539/1770 train_time:152237ms step_avg:99.57ms
step:1540/1770 train_time:152343ms step_avg:99.57ms
step:1541/1770 train_time:152448ms step_avg:99.57ms
step:1542/1770 train_time:152553ms step_avg:99.58ms
step:1543/1770 train_time:152656ms step_avg:99.58ms
step:1544/1770 train_time:152762ms step_avg:99.58ms
step:1545/1770 train_time:152865ms step_avg:99.59ms
step:1546/1770 train_time:152969ms step_avg:99.59ms
step:1547/1770 train_time:153074ms step_avg:99.59ms
step:1548/1770 train_time:153177ms step_avg:99.60ms
step:1549/1770 train_time:153282ms step_avg:99.60ms
step:1550/1770 train_time:153387ms step_avg:99.60ms
step:1551/1770 train_time:153491ms step_avg:99.60ms
step:1552/1770 train_time:153597ms step_avg:99.61ms
step:1553/1770 train_time:153701ms step_avg:99.61ms
step:1554/1770 train_time:153804ms step_avg:99.61ms
step:1555/1770 train_time:153909ms step_avg:99.62ms
step:1556/1770 train_time:154012ms step_avg:99.62ms
step:1557/1770 train_time:154116ms step_avg:99.62ms
step:1558/1770 train_time:154221ms step_avg:99.63ms
step:1559/1770 train_time:154325ms step_avg:99.63ms
step:1560/1770 train_time:154428ms step_avg:99.63ms
step:1561/1770 train_time:154534ms step_avg:99.64ms
step:1562/1770 train_time:154639ms step_avg:99.64ms
step:1563/1770 train_time:154742ms step_avg:99.64ms
step:1564/1770 train_time:154845ms step_avg:99.64ms
step:1565/1770 train_time:154949ms step_avg:99.65ms
step:1566/1770 train_time:155053ms step_avg:99.65ms
step:1567/1770 train_time:155158ms step_avg:99.65ms
step:1568/1770 train_time:155262ms step_avg:99.65ms
step:1569/1770 train_time:155369ms step_avg:99.66ms
step:1570/1770 train_time:155472ms step_avg:99.66ms
step:1571/1770 train_time:155576ms step_avg:99.66ms
step:1572/1770 train_time:155681ms step_avg:99.67ms
step:1573/1770 train_time:155788ms step_avg:99.67ms
step:1574/1770 train_time:155892ms step_avg:99.67ms
step:1575/1770 train_time:155995ms step_avg:99.68ms
step:1576/1770 train_time:156098ms step_avg:99.68ms
step:1577/1770 train_time:156203ms step_avg:99.68ms
step:1578/1770 train_time:156309ms step_avg:99.69ms
step:1579/1770 train_time:156412ms step_avg:99.69ms
step:1580/1770 train_time:156517ms step_avg:99.69ms
step:1581/1770 train_time:156623ms step_avg:99.70ms
step:1582/1770 train_time:156728ms step_avg:99.70ms
step:1583/1770 train_time:156832ms step_avg:99.70ms
step:1584/1770 train_time:156936ms step_avg:99.71ms
step:1585/1770 train_time:157042ms step_avg:99.71ms
step:1586/1770 train_time:157149ms step_avg:99.71ms
step:1587/1770 train_time:157253ms step_avg:99.72ms
step:1588/1770 train_time:157358ms step_avg:99.72ms
step:1589/1770 train_time:157464ms step_avg:99.72ms
step:1590/1770 train_time:157567ms step_avg:99.73ms
step:1591/1770 train_time:157670ms step_avg:99.73ms
step:1592/1770 train_time:157776ms step_avg:99.73ms
step:1593/1770 train_time:157880ms step_avg:99.73ms
step:1594/1770 train_time:157984ms step_avg:99.74ms
step:1595/1770 train_time:158088ms step_avg:99.74ms
step:1596/1770 train_time:158193ms step_avg:99.74ms
step:1597/1770 train_time:158296ms step_avg:99.75ms
step:1598/1770 train_time:158400ms step_avg:99.75ms
step:1599/1770 train_time:158505ms step_avg:99.75ms
step:1600/1770 train_time:158611ms step_avg:99.76ms
step:1601/1770 train_time:158716ms step_avg:99.76ms
step:1602/1770 train_time:158821ms step_avg:99.76ms
step:1603/1770 train_time:158926ms step_avg:99.77ms
step:1604/1770 train_time:159029ms step_avg:99.77ms
step:1605/1770 train_time:159132ms step_avg:99.77ms
step:1606/1770 train_time:159237ms step_avg:99.77ms
step:1607/1770 train_time:159345ms step_avg:99.78ms
step:1608/1770 train_time:159449ms step_avg:99.78ms
step:1609/1770 train_time:159553ms step_avg:99.78ms
step:1610/1770 train_time:159657ms step_avg:99.79ms
step:1611/1770 train_time:159764ms step_avg:99.79ms
step:1612/1770 train_time:159870ms step_avg:99.79ms
step:1613/1770 train_time:159974ms step_avg:99.80ms
step:1614/1770 train_time:160078ms step_avg:99.80ms
step:1615/1770 train_time:160183ms step_avg:99.80ms
step:1616/1770 train_time:160287ms step_avg:99.81ms
step:1617/1770 train_time:160393ms step_avg:99.81ms
step:1618/1770 train_time:160498ms step_avg:99.81ms
step:1619/1770 train_time:160603ms step_avg:99.82ms
step:1620/1770 train_time:160708ms step_avg:99.82ms
step:1621/1770 train_time:160812ms step_avg:99.82ms
step:1622/1770 train_time:160917ms step_avg:99.82ms
step:1623/1770 train_time:161023ms step_avg:99.83ms
step:1624/1770 train_time:161126ms step_avg:99.83ms
step:1625/1770 train_time:161229ms step_avg:99.83ms
step:1625/1770 val_loss:3.3132 train_time:161331ms step_avg:99.90ms
step:1626/1770 train_time:161352ms step_avg:99.85ms
step:1627/1770 train_time:161440ms step_avg:99.84ms
step:1628/1770 train_time:161544ms step_avg:99.84ms
step:1629/1770 train_time:161646ms step_avg:99.84ms
step:1630/1770 train_time:161751ms step_avg:99.85ms
step:1631/1770 train_time:161855ms step_avg:99.85ms
step:1632/1770 train_time:161959ms step_avg:99.85ms
step:1633/1770 train_time:162064ms step_avg:99.85ms
step:1634/1770 train_time:162166ms step_avg:99.86ms
step:1635/1770 train_time:162271ms step_avg:99.86ms
step:1636/1770 train_time:162376ms step_avg:99.86ms
step:1637/1770 train_time:162481ms step_avg:99.87ms
step:1638/1770 train_time:162584ms step_avg:99.87ms
step:1639/1770 train_time:162689ms step_avg:99.87ms
step:1640/1770 train_time:162793ms step_avg:99.87ms
step:1641/1770 train_time:162897ms step_avg:99.88ms
step:1642/1770 train_time:163001ms step_avg:99.88ms
step:1643/1770 train_time:163105ms step_avg:99.88ms
step:1644/1770 train_time:163211ms step_avg:99.88ms
step:1645/1770 train_time:163314ms step_avg:99.89ms
step:1646/1770 train_time:163420ms step_avg:99.89ms
step:1647/1770 train_time:163524ms step_avg:99.89ms
step:1648/1770 train_time:163628ms step_avg:99.89ms
step:1649/1770 train_time:163731ms step_avg:99.90ms
step:1650/1770 train_time:163836ms step_avg:99.90ms
step:1651/1770 train_time:163939ms step_avg:99.90ms
step:1652/1770 train_time:164043ms step_avg:99.90ms
step:1653/1770 train_time:164147ms step_avg:99.91ms
step:1654/1770 train_time:164255ms step_avg:99.91ms
step:1655/1770 train_time:164362ms step_avg:99.92ms
step:1656/1770 train_time:164466ms step_avg:99.92ms
step:1657/1770 train_time:164572ms step_avg:99.92ms
step:1658/1770 train_time:164676ms step_avg:99.92ms
step:1659/1770 train_time:164782ms step_avg:99.93ms
step:1660/1770 train_time:164885ms step_avg:99.93ms
step:1661/1770 train_time:164990ms step_avg:99.93ms
step:1662/1770 train_time:165095ms step_avg:99.94ms
step:1663/1770 train_time:165199ms step_avg:99.94ms
step:1664/1770 train_time:165303ms step_avg:99.94ms
step:1665/1770 train_time:165407ms step_avg:99.94ms
step:1666/1770 train_time:165511ms step_avg:99.95ms
step:1667/1770 train_time:165615ms step_avg:99.95ms
step:1668/1770 train_time:165719ms step_avg:99.95ms
step:1669/1770 train_time:165822ms step_avg:99.95ms
step:1670/1770 train_time:165926ms step_avg:99.96ms
step:1671/1770 train_time:166030ms step_avg:99.96ms
step:1672/1770 train_time:166134ms step_avg:99.96ms
step:1673/1770 train_time:166240ms step_avg:99.96ms
step:1674/1770 train_time:166343ms step_avg:99.97ms
step:1675/1770 train_time:166446ms step_avg:99.97ms
step:1676/1770 train_time:166552ms step_avg:99.97ms
step:1677/1770 train_time:166661ms step_avg:99.98ms
step:1678/1770 train_time:166764ms step_avg:99.98ms
step:1679/1770 train_time:166868ms step_avg:99.98ms
step:1680/1770 train_time:166972ms step_avg:99.98ms
step:1681/1770 train_time:167077ms step_avg:99.99ms
step:1682/1770 train_time:167183ms step_avg:99.99ms
step:1683/1770 train_time:167286ms step_avg:99.99ms
step:1684/1770 train_time:167389ms step_avg:99.99ms
step:1685/1770 train_time:167493ms step_avg:100.00ms
step:1686/1770 train_time:167598ms step_avg:100.00ms
step:1687/1770 train_time:167704ms step_avg:100.00ms
step:1688/1770 train_time:167808ms step_avg:100.00ms
step:1689/1770 train_time:167912ms step_avg:100.01ms
step:1690/1770 train_time:168016ms step_avg:100.01ms
step:1691/1770 train_time:168120ms step_avg:100.01ms
step:1692/1770 train_time:168225ms step_avg:100.01ms
step:1693/1770 train_time:168330ms step_avg:100.02ms
step:1694/1770 train_time:168434ms step_avg:100.02ms
step:1695/1770 train_time:168539ms step_avg:100.02ms
step:1696/1770 train_time:168645ms step_avg:100.03ms
step:1697/1770 train_time:168751ms step_avg:100.03ms
step:1698/1770 train_time:168856ms step_avg:100.03ms
step:1699/1770 train_time:168960ms step_avg:100.04ms
step:1700/1770 train_time:169064ms step_avg:100.04ms
step:1701/1770 train_time:169167ms step_avg:100.04ms
step:1702/1770 train_time:169272ms step_avg:100.04ms
step:1703/1770 train_time:169375ms step_avg:100.04ms
step:1704/1770 train_time:169479ms step_avg:100.05ms
step:1705/1770 train_time:169582ms step_avg:100.05ms
step:1706/1770 train_time:169685ms step_avg:100.05ms
step:1707/1770 train_time:169790ms step_avg:100.05ms
step:1708/1770 train_time:169895ms step_avg:100.06ms
step:1709/1770 train_time:170000ms step_avg:100.06ms
step:1710/1770 train_time:170108ms step_avg:100.06ms
step:1711/1770 train_time:170215ms step_avg:100.07ms
step:1712/1770 train_time:170321ms step_avg:100.07ms
step:1713/1770 train_time:170425ms step_avg:100.07ms
step:1714/1770 train_time:170529ms step_avg:100.08ms
step:1715/1770 train_time:170633ms step_avg:100.08ms
step:1716/1770 train_time:170738ms step_avg:100.08ms
step:1717/1770 train_time:170842ms step_avg:100.08ms
step:1718/1770 train_time:170947ms step_avg:100.09ms
step:1719/1770 train_time:171053ms step_avg:100.09ms
step:1720/1770 train_time:171158ms step_avg:100.09ms
step:1721/1770 train_time:171263ms step_avg:100.10ms
step:1722/1770 train_time:171370ms step_avg:100.10ms
step:1723/1770 train_time:171477ms step_avg:100.10ms
step:1724/1770 train_time:171584ms step_avg:100.11ms
step:1725/1770 train_time:171690ms step_avg:100.11ms
step:1726/1770 train_time:171797ms step_avg:100.11ms
step:1727/1770 train_time:171901ms step_avg:100.12ms
step:1728/1770 train_time:172008ms step_avg:100.12ms
step:1729/1770 train_time:172112ms step_avg:100.12ms
step:1730/1770 train_time:172218ms step_avg:100.13ms
step:1731/1770 train_time:172325ms step_avg:100.13ms
step:1732/1770 train_time:172428ms step_avg:100.13ms
step:1733/1770 train_time:172535ms step_avg:100.14ms
step:1734/1770 train_time:172639ms step_avg:100.14ms
step:1735/1770 train_time:172746ms step_avg:100.14ms
step:1736/1770 train_time:172850ms step_avg:100.14ms
step:1737/1770 train_time:172955ms step_avg:100.15ms
step:1738/1770 train_time:173060ms step_avg:100.15ms
step:1739/1770 train_time:173165ms step_avg:100.15ms
step:1740/1770 train_time:173269ms step_avg:100.16ms
step:1741/1770 train_time:173378ms step_avg:100.16ms
step:1742/1770 train_time:173485ms step_avg:100.16ms
step:1743/1770 train_time:173591ms step_avg:100.17ms
step:1744/1770 train_time:173696ms step_avg:100.17ms
step:1745/1770 train_time:173801ms step_avg:100.17ms
step:1746/1770 train_time:173909ms step_avg:100.18ms
step:1747/1770 train_time:174013ms step_avg:100.18ms
step:1748/1770 train_time:174120ms step_avg:100.18ms
step:1749/1770 train_time:174226ms step_avg:100.19ms
step:1750/1770 train_time:174330ms step_avg:100.19ms
step:1750/1770 val_loss:3.2895 train_time:174434ms step_avg:100.25ms
step:1751/1770 train_time:174455ms step_avg:100.20ms
step:1752/1770 train_time:174544ms step_avg:100.20ms
step:1753/1770 train_time:174648ms step_avg:100.20ms
step:1754/1770 train_time:174754ms step_avg:100.20ms
step:1755/1770 train_time:174859ms step_avg:100.21ms
step:1756/1770 train_time:174964ms step_avg:100.21ms
step:1757/1770 train_time:175070ms step_avg:100.21ms
step:1758/1770 train_time:175174ms step_avg:100.21ms
step:1759/1770 train_time:175280ms step_avg:100.22ms
step:1760/1770 train_time:175386ms step_avg:100.22ms
step:1761/1770 train_time:175493ms step_avg:100.22ms
step:1762/1770 train_time:175601ms step_avg:100.23ms
step:1763/1770 train_time:175705ms step_avg:100.23ms
step:1764/1770 train_time:175811ms step_avg:100.23ms
step:1765/1770 train_time:175915ms step_avg:100.24ms
step:1766/1770 train_time:176024ms step_avg:100.24ms
step:1767/1770 train_time:176127ms step_avg:100.24ms
step:1768/1770 train_time:176232ms step_avg:100.25ms
step:1769/1770 train_time:176336ms step_avg:100.25ms
step:1770/1770 train_time:176441ms step_avg:100.25ms
step:1770/1770 val_loss:3.2864 train_time:176545ms step_avg:100.31ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
