import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:08:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:25062ms step_avg:nanms
step:2/1770 train_time:25495ms step_avg:nanms
step:3/1770 train_time:25591ms step_avg:nanms
step:4/1770 train_time:25685ms step_avg:nanms
step:5/1770 train_time:25779ms step_avg:nanms
step:6/1770 train_time:25874ms step_avg:nanms
step:7/1770 train_time:25968ms step_avg:nanms
step:8/1770 train_time:26063ms step_avg:nanms
step:9/1770 train_time:26157ms step_avg:nanms
step:10/1770 train_time:26251ms step_avg:nanms
step:11/1770 train_time:95ms step_avg:nanms
step:12/1770 train_time:189ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.40ms
step:14/1770 train_time:378ms step_avg:94.48ms
step:15/1770 train_time:472ms step_avg:94.45ms
step:16/1770 train_time:567ms step_avg:94.44ms
step:17/1770 train_time:661ms step_avg:94.43ms
step:18/1770 train_time:756ms step_avg:94.49ms
step:19/1770 train_time:851ms step_avg:94.57ms
step:20/1770 train_time:946ms step_avg:94.59ms
step:21/1770 train_time:1040ms step_avg:94.59ms
step:22/1770 train_time:1135ms step_avg:94.62ms
step:23/1770 train_time:1230ms step_avg:94.60ms
step:24/1770 train_time:1325ms step_avg:94.61ms
step:25/1770 train_time:1419ms step_avg:94.62ms
step:26/1770 train_time:1514ms step_avg:94.64ms
step:27/1770 train_time:1609ms step_avg:94.65ms
step:28/1770 train_time:1704ms step_avg:94.64ms
step:29/1770 train_time:1801ms step_avg:94.78ms
step:30/1770 train_time:1893ms step_avg:94.67ms
step:31/1770 train_time:1987ms step_avg:94.64ms
step:32/1770 train_time:2082ms step_avg:94.64ms
step:33/1770 train_time:2177ms step_avg:94.67ms
step:34/1770 train_time:2272ms step_avg:94.69ms
step:35/1770 train_time:2367ms step_avg:94.68ms
step:36/1770 train_time:2462ms step_avg:94.68ms
step:37/1770 train_time:2556ms step_avg:94.68ms
step:38/1770 train_time:2651ms step_avg:94.67ms
step:39/1770 train_time:2745ms step_avg:94.67ms
step:40/1770 train_time:2840ms step_avg:94.67ms
step:41/1770 train_time:2935ms step_avg:94.66ms
step:42/1770 train_time:3029ms step_avg:94.66ms
step:43/1770 train_time:3124ms step_avg:94.66ms
step:44/1770 train_time:3219ms step_avg:94.69ms
step:45/1770 train_time:3313ms step_avg:94.67ms
step:46/1770 train_time:3408ms step_avg:94.67ms
step:47/1770 train_time:3503ms step_avg:94.67ms
step:48/1770 train_time:3598ms step_avg:94.67ms
step:49/1770 train_time:3692ms step_avg:94.68ms
step:50/1770 train_time:3787ms step_avg:94.67ms
step:51/1770 train_time:3882ms step_avg:94.68ms
step:52/1770 train_time:3976ms step_avg:94.68ms
step:53/1770 train_time:4071ms step_avg:94.67ms
step:54/1770 train_time:4165ms step_avg:94.67ms
step:55/1770 train_time:4260ms step_avg:94.67ms
step:56/1770 train_time:4355ms step_avg:94.67ms
step:57/1770 train_time:4449ms step_avg:94.67ms
step:58/1770 train_time:4545ms step_avg:94.68ms
step:59/1770 train_time:4640ms step_avg:94.69ms
step:60/1770 train_time:4734ms step_avg:94.69ms
step:61/1770 train_time:4829ms step_avg:94.68ms
step:62/1770 train_time:4923ms step_avg:94.67ms
step:63/1770 train_time:5017ms step_avg:94.67ms
step:64/1770 train_time:5112ms step_avg:94.66ms
step:65/1770 train_time:5206ms step_avg:94.66ms
step:66/1770 train_time:5301ms step_avg:94.67ms
step:67/1770 train_time:5396ms step_avg:94.67ms
step:68/1770 train_time:5490ms step_avg:94.66ms
step:69/1770 train_time:5586ms step_avg:94.68ms
step:70/1770 train_time:5680ms step_avg:94.67ms
step:71/1770 train_time:5775ms step_avg:94.67ms
step:72/1770 train_time:5870ms step_avg:94.68ms
step:73/1770 train_time:5964ms step_avg:94.67ms
step:74/1770 train_time:6059ms step_avg:94.68ms
step:75/1770 train_time:6154ms step_avg:94.68ms
step:76/1770 train_time:6249ms step_avg:94.68ms
step:77/1770 train_time:6344ms step_avg:94.68ms
step:78/1770 train_time:6439ms step_avg:94.69ms
step:79/1770 train_time:6534ms step_avg:94.70ms
step:80/1770 train_time:6628ms step_avg:94.69ms
step:81/1770 train_time:6723ms step_avg:94.69ms
step:82/1770 train_time:6819ms step_avg:94.70ms
step:83/1770 train_time:6914ms step_avg:94.71ms
step:84/1770 train_time:7008ms step_avg:94.70ms
step:85/1770 train_time:7102ms step_avg:94.70ms
step:86/1770 train_time:7198ms step_avg:94.71ms
step:87/1770 train_time:7292ms step_avg:94.70ms
step:88/1770 train_time:7386ms step_avg:94.70ms
step:89/1770 train_time:7481ms step_avg:94.70ms
step:90/1770 train_time:7576ms step_avg:94.70ms
step:91/1770 train_time:7670ms step_avg:94.69ms
step:92/1770 train_time:7764ms step_avg:94.69ms
step:93/1770 train_time:7859ms step_avg:94.69ms
step:94/1770 train_time:7955ms step_avg:94.70ms
step:95/1770 train_time:8049ms step_avg:94.70ms
step:96/1770 train_time:8144ms step_avg:94.69ms
step:97/1770 train_time:8239ms step_avg:94.70ms
step:98/1770 train_time:8333ms step_avg:94.70ms
step:99/1770 train_time:8428ms step_avg:94.70ms
step:100/1770 train_time:8523ms step_avg:94.70ms
step:101/1770 train_time:8617ms step_avg:94.69ms
step:102/1770 train_time:8712ms step_avg:94.69ms
step:103/1770 train_time:8806ms step_avg:94.69ms
step:104/1770 train_time:8901ms step_avg:94.69ms
step:105/1770 train_time:8996ms step_avg:94.69ms
step:106/1770 train_time:9091ms step_avg:94.70ms
step:107/1770 train_time:9186ms step_avg:94.70ms
step:108/1770 train_time:9281ms step_avg:94.70ms
step:109/1770 train_time:9375ms step_avg:94.70ms
step:110/1770 train_time:9470ms step_avg:94.70ms
step:111/1770 train_time:9564ms step_avg:94.70ms
step:112/1770 train_time:9660ms step_avg:94.70ms
step:113/1770 train_time:9754ms step_avg:94.70ms
step:114/1770 train_time:9849ms step_avg:94.70ms
step:115/1770 train_time:9943ms step_avg:94.69ms
step:116/1770 train_time:10037ms step_avg:94.69ms
step:117/1770 train_time:10132ms step_avg:94.69ms
step:118/1770 train_time:10226ms step_avg:94.69ms
step:119/1770 train_time:10321ms step_avg:94.69ms
step:120/1770 train_time:10416ms step_avg:94.69ms
step:121/1770 train_time:10510ms step_avg:94.69ms
step:122/1770 train_time:10604ms step_avg:94.68ms
step:123/1770 train_time:10699ms step_avg:94.68ms
step:124/1770 train_time:10794ms step_avg:94.69ms
step:125/1770 train_time:10889ms step_avg:94.68ms
step:125/1770 val_loss:4.6638 train_time:10981ms step_avg:95.49ms
step:126/1770 train_time:11003ms step_avg:94.85ms
step:127/1770 train_time:11080ms step_avg:94.70ms
step:128/1770 train_time:11180ms step_avg:94.75ms
step:129/1770 train_time:11280ms step_avg:94.79ms
step:130/1770 train_time:11375ms step_avg:94.79ms
step:131/1770 train_time:11470ms step_avg:94.79ms
step:132/1770 train_time:11565ms step_avg:94.79ms
step:133/1770 train_time:11659ms step_avg:94.79ms
step:134/1770 train_time:11755ms step_avg:94.80ms
step:135/1770 train_time:11850ms step_avg:94.80ms
step:136/1770 train_time:11945ms step_avg:94.80ms
step:137/1770 train_time:12040ms step_avg:94.80ms
step:138/1770 train_time:12135ms step_avg:94.80ms
step:139/1770 train_time:12230ms step_avg:94.81ms
step:140/1770 train_time:12325ms step_avg:94.81ms
step:141/1770 train_time:12420ms step_avg:94.81ms
step:142/1770 train_time:12515ms step_avg:94.81ms
step:143/1770 train_time:12611ms step_avg:94.82ms
step:144/1770 train_time:12706ms step_avg:94.82ms
step:145/1770 train_time:12801ms step_avg:94.82ms
step:146/1770 train_time:12896ms step_avg:94.83ms
step:147/1770 train_time:12992ms step_avg:94.83ms
step:148/1770 train_time:13088ms step_avg:94.84ms
step:149/1770 train_time:13183ms step_avg:94.84ms
step:150/1770 train_time:13277ms step_avg:94.84ms
step:151/1770 train_time:13373ms step_avg:94.84ms
step:152/1770 train_time:13469ms step_avg:94.85ms
step:153/1770 train_time:13564ms step_avg:94.85ms
step:154/1770 train_time:13659ms step_avg:94.86ms
step:155/1770 train_time:13754ms step_avg:94.86ms
step:156/1770 train_time:13850ms step_avg:94.86ms
step:157/1770 train_time:13945ms step_avg:94.86ms
step:158/1770 train_time:14041ms step_avg:94.87ms
step:159/1770 train_time:14136ms step_avg:94.87ms
step:160/1770 train_time:14231ms step_avg:94.87ms
step:161/1770 train_time:14326ms step_avg:94.88ms
step:162/1770 train_time:14421ms step_avg:94.88ms
step:163/1770 train_time:14516ms step_avg:94.88ms
step:164/1770 train_time:14612ms step_avg:94.88ms
step:165/1770 train_time:14707ms step_avg:94.88ms
step:166/1770 train_time:14802ms step_avg:94.88ms
step:167/1770 train_time:14897ms step_avg:94.89ms
step:168/1770 train_time:14993ms step_avg:94.89ms
step:169/1770 train_time:15088ms step_avg:94.89ms
step:170/1770 train_time:15183ms step_avg:94.89ms
step:171/1770 train_time:15278ms step_avg:94.90ms
step:172/1770 train_time:15373ms step_avg:94.90ms
step:173/1770 train_time:15468ms step_avg:94.90ms
step:174/1770 train_time:15564ms step_avg:94.90ms
step:175/1770 train_time:15659ms step_avg:94.90ms
step:176/1770 train_time:15755ms step_avg:94.91ms
step:177/1770 train_time:15850ms step_avg:94.91ms
step:178/1770 train_time:15945ms step_avg:94.91ms
step:179/1770 train_time:16041ms step_avg:94.91ms
step:180/1770 train_time:16135ms step_avg:94.91ms
step:181/1770 train_time:16231ms step_avg:94.92ms
step:182/1770 train_time:16327ms step_avg:94.92ms
step:183/1770 train_time:16423ms step_avg:94.93ms
step:184/1770 train_time:16518ms step_avg:94.93ms
step:185/1770 train_time:16613ms step_avg:94.93ms
step:186/1770 train_time:16709ms step_avg:94.94ms
step:187/1770 train_time:16804ms step_avg:94.94ms
step:188/1770 train_time:16900ms step_avg:94.94ms
step:189/1770 train_time:16995ms step_avg:94.95ms
step:190/1770 train_time:17091ms step_avg:94.95ms
step:191/1770 train_time:17187ms step_avg:94.95ms
step:192/1770 train_time:17282ms step_avg:94.95ms
step:193/1770 train_time:17376ms step_avg:94.95ms
step:194/1770 train_time:17472ms step_avg:94.95ms
step:195/1770 train_time:17567ms step_avg:94.96ms
step:196/1770 train_time:17662ms step_avg:94.96ms
step:197/1770 train_time:17757ms step_avg:94.96ms
step:198/1770 train_time:17852ms step_avg:94.96ms
step:199/1770 train_time:17947ms step_avg:94.96ms
step:200/1770 train_time:18043ms step_avg:94.96ms
step:201/1770 train_time:18138ms step_avg:94.96ms
step:202/1770 train_time:18234ms step_avg:94.97ms
step:203/1770 train_time:18329ms step_avg:94.97ms
step:204/1770 train_time:18425ms step_avg:94.97ms
step:205/1770 train_time:18521ms step_avg:94.98ms
step:206/1770 train_time:18615ms step_avg:94.98ms
step:207/1770 train_time:18711ms step_avg:94.98ms
step:208/1770 train_time:18805ms step_avg:94.98ms
step:209/1770 train_time:18900ms step_avg:94.98ms
step:210/1770 train_time:18995ms step_avg:94.97ms
step:211/1770 train_time:19090ms step_avg:94.98ms
step:212/1770 train_time:19186ms step_avg:94.98ms
step:213/1770 train_time:19281ms step_avg:94.98ms
step:214/1770 train_time:19376ms step_avg:94.98ms
step:215/1770 train_time:19471ms step_avg:94.98ms
step:216/1770 train_time:19567ms step_avg:94.98ms
step:217/1770 train_time:19661ms step_avg:94.98ms
step:218/1770 train_time:19756ms step_avg:94.98ms
step:219/1770 train_time:19852ms step_avg:94.99ms
step:220/1770 train_time:19948ms step_avg:94.99ms
step:221/1770 train_time:20043ms step_avg:94.99ms
step:222/1770 train_time:20138ms step_avg:94.99ms
step:223/1770 train_time:20234ms step_avg:94.99ms
step:224/1770 train_time:20329ms step_avg:95.00ms
step:225/1770 train_time:20425ms step_avg:95.00ms
step:226/1770 train_time:20521ms step_avg:95.00ms
step:227/1770 train_time:20616ms step_avg:95.00ms
step:228/1770 train_time:20711ms step_avg:95.00ms
step:229/1770 train_time:20806ms step_avg:95.01ms
step:230/1770 train_time:20901ms step_avg:95.00ms
step:231/1770 train_time:20996ms step_avg:95.00ms
step:232/1770 train_time:21091ms step_avg:95.00ms
step:233/1770 train_time:21186ms step_avg:95.00ms
step:234/1770 train_time:21281ms step_avg:95.00ms
step:235/1770 train_time:21377ms step_avg:95.01ms
step:236/1770 train_time:21472ms step_avg:95.01ms
step:237/1770 train_time:21567ms step_avg:95.01ms
step:238/1770 train_time:21662ms step_avg:95.01ms
step:239/1770 train_time:21757ms step_avg:95.01ms
step:240/1770 train_time:21852ms step_avg:95.01ms
step:241/1770 train_time:21947ms step_avg:95.01ms
step:242/1770 train_time:22042ms step_avg:95.01ms
step:243/1770 train_time:22138ms step_avg:95.01ms
step:244/1770 train_time:22234ms step_avg:95.02ms
step:245/1770 train_time:22329ms step_avg:95.02ms
step:246/1770 train_time:22424ms step_avg:95.02ms
step:247/1770 train_time:22519ms step_avg:95.02ms
step:248/1770 train_time:22614ms step_avg:95.02ms
step:249/1770 train_time:22710ms step_avg:95.02ms
step:250/1770 train_time:22805ms step_avg:95.02ms
step:250/1770 val_loss:4.1146 train_time:22899ms step_avg:95.41ms
step:251/1770 train_time:22920ms step_avg:95.10ms
step:252/1770 train_time:23003ms step_avg:95.06ms
step:253/1770 train_time:23100ms step_avg:95.06ms
step:254/1770 train_time:23196ms step_avg:95.06ms
step:255/1770 train_time:23290ms step_avg:95.06ms
step:256/1770 train_time:23385ms step_avg:95.06ms
step:257/1770 train_time:23480ms step_avg:95.06ms
step:258/1770 train_time:23575ms step_avg:95.06ms
step:259/1770 train_time:23670ms step_avg:95.06ms
step:260/1770 train_time:23766ms step_avg:95.06ms
step:261/1770 train_time:23860ms step_avg:95.06ms
step:262/1770 train_time:23957ms step_avg:95.07ms
step:263/1770 train_time:24052ms step_avg:95.07ms
step:264/1770 train_time:24147ms step_avg:95.07ms
step:265/1770 train_time:24243ms step_avg:95.07ms
step:266/1770 train_time:24339ms step_avg:95.08ms
step:267/1770 train_time:24435ms step_avg:95.08ms
step:268/1770 train_time:24531ms step_avg:95.08ms
step:269/1770 train_time:24627ms step_avg:95.08ms
step:270/1770 train_time:24722ms step_avg:95.08ms
step:271/1770 train_time:24818ms step_avg:95.09ms
step:272/1770 train_time:24913ms step_avg:95.09ms
step:273/1770 train_time:25009ms step_avg:95.09ms
step:274/1770 train_time:25104ms step_avg:95.09ms
step:275/1770 train_time:25200ms step_avg:95.09ms
step:276/1770 train_time:25296ms step_avg:95.10ms
step:277/1770 train_time:25391ms step_avg:95.10ms
step:278/1770 train_time:25486ms step_avg:95.10ms
step:279/1770 train_time:25582ms step_avg:95.10ms
step:280/1770 train_time:25678ms step_avg:95.10ms
step:281/1770 train_time:25773ms step_avg:95.11ms
step:282/1770 train_time:25869ms step_avg:95.11ms
step:283/1770 train_time:25964ms step_avg:95.11ms
step:284/1770 train_time:26060ms step_avg:95.11ms
step:285/1770 train_time:26156ms step_avg:95.11ms
step:286/1770 train_time:26253ms step_avg:95.12ms
step:287/1770 train_time:26348ms step_avg:95.12ms
step:288/1770 train_time:26443ms step_avg:95.12ms
step:289/1770 train_time:26539ms step_avg:95.12ms
step:290/1770 train_time:26635ms step_avg:95.12ms
step:291/1770 train_time:26731ms step_avg:95.13ms
step:292/1770 train_time:26826ms step_avg:95.13ms
step:293/1770 train_time:26921ms step_avg:95.13ms
step:294/1770 train_time:27017ms step_avg:95.13ms
step:295/1770 train_time:27112ms step_avg:95.13ms
step:296/1770 train_time:27208ms step_avg:95.13ms
step:297/1770 train_time:27303ms step_avg:95.13ms
step:298/1770 train_time:27399ms step_avg:95.13ms
step:299/1770 train_time:27495ms step_avg:95.14ms
step:300/1770 train_time:27590ms step_avg:95.14ms
step:301/1770 train_time:27686ms step_avg:95.14ms
step:302/1770 train_time:27781ms step_avg:95.14ms
step:303/1770 train_time:27877ms step_avg:95.14ms
step:304/1770 train_time:27973ms step_avg:95.15ms
step:305/1770 train_time:28069ms step_avg:95.15ms
step:306/1770 train_time:28164ms step_avg:95.15ms
step:307/1770 train_time:28260ms step_avg:95.15ms
step:308/1770 train_time:28355ms step_avg:95.15ms
step:309/1770 train_time:28451ms step_avg:95.15ms
step:310/1770 train_time:28547ms step_avg:95.16ms
step:311/1770 train_time:28642ms step_avg:95.16ms
step:312/1770 train_time:28738ms step_avg:95.16ms
step:313/1770 train_time:28834ms step_avg:95.16ms
step:314/1770 train_time:28929ms step_avg:95.16ms
step:315/1770 train_time:29024ms step_avg:95.16ms
step:316/1770 train_time:29121ms step_avg:95.17ms
step:317/1770 train_time:29216ms step_avg:95.17ms
step:318/1770 train_time:29312ms step_avg:95.17ms
step:319/1770 train_time:29407ms step_avg:95.17ms
step:320/1770 train_time:29502ms step_avg:95.17ms
step:321/1770 train_time:29598ms step_avg:95.17ms
step:322/1770 train_time:29693ms step_avg:95.17ms
step:323/1770 train_time:29789ms step_avg:95.17ms
step:324/1770 train_time:29884ms step_avg:95.17ms
step:325/1770 train_time:29980ms step_avg:95.17ms
step:326/1770 train_time:30076ms step_avg:95.18ms
step:327/1770 train_time:30171ms step_avg:95.18ms
step:328/1770 train_time:30266ms step_avg:95.18ms
step:329/1770 train_time:30361ms step_avg:95.18ms
step:330/1770 train_time:30457ms step_avg:95.18ms
step:331/1770 train_time:30553ms step_avg:95.18ms
step:332/1770 train_time:30648ms step_avg:95.18ms
step:333/1770 train_time:30744ms step_avg:95.18ms
step:334/1770 train_time:30840ms step_avg:95.18ms
step:335/1770 train_time:30936ms step_avg:95.19ms
step:336/1770 train_time:31032ms step_avg:95.19ms
step:337/1770 train_time:31126ms step_avg:95.19ms
step:338/1770 train_time:31222ms step_avg:95.19ms
step:339/1770 train_time:31318ms step_avg:95.19ms
step:340/1770 train_time:31413ms step_avg:95.19ms
step:341/1770 train_time:31509ms step_avg:95.19ms
step:342/1770 train_time:31604ms step_avg:95.19ms
step:343/1770 train_time:31700ms step_avg:95.20ms
step:344/1770 train_time:31796ms step_avg:95.20ms
step:345/1770 train_time:31891ms step_avg:95.20ms
step:346/1770 train_time:31987ms step_avg:95.20ms
step:347/1770 train_time:32082ms step_avg:95.20ms
step:348/1770 train_time:32179ms step_avg:95.20ms
step:349/1770 train_time:32275ms step_avg:95.21ms
step:350/1770 train_time:32370ms step_avg:95.21ms
step:351/1770 train_time:32465ms step_avg:95.21ms
step:352/1770 train_time:32561ms step_avg:95.21ms
step:353/1770 train_time:32657ms step_avg:95.21ms
step:354/1770 train_time:32752ms step_avg:95.21ms
step:355/1770 train_time:32848ms step_avg:95.21ms
step:356/1770 train_time:32944ms step_avg:95.21ms
step:357/1770 train_time:33040ms step_avg:95.22ms
step:358/1770 train_time:33135ms step_avg:95.22ms
step:359/1770 train_time:33230ms step_avg:95.22ms
step:360/1770 train_time:33326ms step_avg:95.22ms
step:361/1770 train_time:33421ms step_avg:95.22ms
step:362/1770 train_time:33517ms step_avg:95.22ms
step:363/1770 train_time:33613ms step_avg:95.22ms
step:364/1770 train_time:33708ms step_avg:95.22ms
step:365/1770 train_time:33804ms step_avg:95.22ms
step:366/1770 train_time:33900ms step_avg:95.23ms
step:367/1770 train_time:33996ms step_avg:95.23ms
step:368/1770 train_time:34091ms step_avg:95.23ms
step:369/1770 train_time:34187ms step_avg:95.23ms
step:370/1770 train_time:34282ms step_avg:95.23ms
step:371/1770 train_time:34378ms step_avg:95.23ms
step:372/1770 train_time:34474ms step_avg:95.23ms
step:373/1770 train_time:34569ms step_avg:95.23ms
step:374/1770 train_time:34665ms step_avg:95.23ms
step:375/1770 train_time:34760ms step_avg:95.23ms
step:375/1770 val_loss:3.9048 train_time:34854ms step_avg:95.49ms
step:376/1770 train_time:34879ms step_avg:95.30ms
step:377/1770 train_time:34961ms step_avg:95.26ms
step:378/1770 train_time:35059ms step_avg:95.27ms
step:379/1770 train_time:35156ms step_avg:95.27ms
step:380/1770 train_time:35251ms step_avg:95.27ms
step:381/1770 train_time:35346ms step_avg:95.27ms
step:382/1770 train_time:35442ms step_avg:95.27ms
step:383/1770 train_time:35537ms step_avg:95.27ms
step:384/1770 train_time:35632ms step_avg:95.27ms
step:385/1770 train_time:35728ms step_avg:95.27ms
step:386/1770 train_time:35823ms step_avg:95.27ms
step:387/1770 train_time:35919ms step_avg:95.28ms
step:388/1770 train_time:36015ms step_avg:95.28ms
step:389/1770 train_time:36110ms step_avg:95.28ms
step:390/1770 train_time:36205ms step_avg:95.28ms
step:391/1770 train_time:36301ms step_avg:95.28ms
step:392/1770 train_time:36397ms step_avg:95.28ms
step:393/1770 train_time:36492ms step_avg:95.28ms
step:394/1770 train_time:36588ms step_avg:95.28ms
step:395/1770 train_time:36684ms step_avg:95.28ms
step:396/1770 train_time:36781ms step_avg:95.29ms
step:397/1770 train_time:36879ms step_avg:95.29ms
step:398/1770 train_time:36976ms step_avg:95.30ms
step:399/1770 train_time:37074ms step_avg:95.31ms
step:400/1770 train_time:37171ms step_avg:95.31ms
step:401/1770 train_time:37269ms step_avg:95.32ms
step:402/1770 train_time:37367ms step_avg:95.32ms
step:403/1770 train_time:37464ms step_avg:95.33ms
step:404/1770 train_time:37561ms step_avg:95.33ms
step:405/1770 train_time:37659ms step_avg:95.34ms
step:406/1770 train_time:37756ms step_avg:95.34ms
step:407/1770 train_time:37853ms step_avg:95.35ms
step:408/1770 train_time:37950ms step_avg:95.35ms
step:409/1770 train_time:38049ms step_avg:95.36ms
step:410/1770 train_time:38146ms step_avg:95.37ms
step:411/1770 train_time:38244ms step_avg:95.37ms
step:412/1770 train_time:38341ms step_avg:95.38ms
step:413/1770 train_time:38439ms step_avg:95.38ms
step:414/1770 train_time:38536ms step_avg:95.39ms
step:415/1770 train_time:38634ms step_avg:95.39ms
step:416/1770 train_time:38731ms step_avg:95.40ms
step:417/1770 train_time:38829ms step_avg:95.40ms
step:418/1770 train_time:38926ms step_avg:95.41ms
step:419/1770 train_time:39024ms step_avg:95.41ms
step:420/1770 train_time:39121ms step_avg:95.42ms
step:421/1770 train_time:39219ms step_avg:95.42ms
step:422/1770 train_time:39316ms step_avg:95.43ms
step:423/1770 train_time:39413ms step_avg:95.43ms
step:424/1770 train_time:39511ms step_avg:95.44ms
step:425/1770 train_time:39609ms step_avg:95.44ms
step:426/1770 train_time:39706ms step_avg:95.45ms
step:427/1770 train_time:39804ms step_avg:95.45ms
step:428/1770 train_time:39901ms step_avg:95.46ms
step:429/1770 train_time:39999ms step_avg:95.46ms
step:430/1770 train_time:40096ms step_avg:95.47ms
step:431/1770 train_time:40194ms step_avg:95.47ms
step:432/1770 train_time:40291ms step_avg:95.48ms
step:433/1770 train_time:40388ms step_avg:95.48ms
step:434/1770 train_time:40486ms step_avg:95.49ms
step:435/1770 train_time:40584ms step_avg:95.49ms
step:436/1770 train_time:40682ms step_avg:95.50ms
step:437/1770 train_time:40779ms step_avg:95.50ms
step:438/1770 train_time:40877ms step_avg:95.51ms
step:439/1770 train_time:40974ms step_avg:95.51ms
step:440/1770 train_time:41072ms step_avg:95.52ms
step:441/1770 train_time:41169ms step_avg:95.52ms
step:442/1770 train_time:41267ms step_avg:95.53ms
step:443/1770 train_time:41365ms step_avg:95.53ms
step:444/1770 train_time:41462ms step_avg:95.54ms
step:445/1770 train_time:41560ms step_avg:95.54ms
step:446/1770 train_time:41657ms step_avg:95.54ms
step:447/1770 train_time:41754ms step_avg:95.55ms
step:448/1770 train_time:41852ms step_avg:95.55ms
step:449/1770 train_time:41949ms step_avg:95.56ms
step:450/1770 train_time:42047ms step_avg:95.56ms
step:451/1770 train_time:42145ms step_avg:95.57ms
step:452/1770 train_time:42243ms step_avg:95.57ms
step:453/1770 train_time:42341ms step_avg:95.58ms
step:454/1770 train_time:42438ms step_avg:95.58ms
step:455/1770 train_time:42536ms step_avg:95.59ms
step:456/1770 train_time:42633ms step_avg:95.59ms
step:457/1770 train_time:42730ms step_avg:95.59ms
step:458/1770 train_time:42827ms step_avg:95.60ms
step:459/1770 train_time:42925ms step_avg:95.60ms
step:460/1770 train_time:43023ms step_avg:95.61ms
step:461/1770 train_time:43120ms step_avg:95.61ms
step:462/1770 train_time:43217ms step_avg:95.61ms
step:463/1770 train_time:43315ms step_avg:95.62ms
step:464/1770 train_time:43413ms step_avg:95.62ms
step:465/1770 train_time:43510ms step_avg:95.63ms
step:466/1770 train_time:43608ms step_avg:95.63ms
step:467/1770 train_time:43705ms step_avg:95.64ms
step:468/1770 train_time:43803ms step_avg:95.64ms
step:469/1770 train_time:43901ms step_avg:95.64ms
step:470/1770 train_time:43999ms step_avg:95.65ms
step:471/1770 train_time:44096ms step_avg:95.65ms
step:472/1770 train_time:44193ms step_avg:95.66ms
step:473/1770 train_time:44291ms step_avg:95.66ms
step:474/1770 train_time:44388ms step_avg:95.66ms
step:475/1770 train_time:44487ms step_avg:95.67ms
step:476/1770 train_time:44584ms step_avg:95.67ms
step:477/1770 train_time:44682ms step_avg:95.68ms
step:478/1770 train_time:44779ms step_avg:95.68ms
step:479/1770 train_time:44877ms step_avg:95.69ms
step:480/1770 train_time:44974ms step_avg:95.69ms
step:481/1770 train_time:45071ms step_avg:95.69ms
step:482/1770 train_time:45169ms step_avg:95.70ms
step:483/1770 train_time:45267ms step_avg:95.70ms
step:484/1770 train_time:45365ms step_avg:95.71ms
step:485/1770 train_time:45462ms step_avg:95.71ms
step:486/1770 train_time:45560ms step_avg:95.71ms
step:487/1770 train_time:45658ms step_avg:95.72ms
step:488/1770 train_time:45755ms step_avg:95.72ms
step:489/1770 train_time:45852ms step_avg:95.73ms
step:490/1770 train_time:45950ms step_avg:95.73ms
step:491/1770 train_time:46047ms step_avg:95.73ms
step:492/1770 train_time:46145ms step_avg:95.74ms
step:493/1770 train_time:46243ms step_avg:95.74ms
step:494/1770 train_time:46341ms step_avg:95.74ms
step:495/1770 train_time:46439ms step_avg:95.75ms
step:496/1770 train_time:46536ms step_avg:95.75ms
step:497/1770 train_time:46633ms step_avg:95.76ms
step:498/1770 train_time:46730ms step_avg:95.76ms
step:499/1770 train_time:46828ms step_avg:95.76ms
step:500/1770 train_time:46926ms step_avg:95.77ms
step:500/1770 val_loss:3.7566 train_time:47022ms step_avg:95.96ms
step:501/1770 train_time:47047ms step_avg:95.82ms
step:502/1770 train_time:47127ms step_avg:95.79ms
step:503/1770 train_time:47225ms step_avg:95.79ms
step:504/1770 train_time:47323ms step_avg:95.79ms
step:505/1770 train_time:47420ms step_avg:95.80ms
step:506/1770 train_time:47517ms step_avg:95.80ms
step:507/1770 train_time:47615ms step_avg:95.80ms
step:508/1770 train_time:47713ms step_avg:95.81ms
step:509/1770 train_time:47810ms step_avg:95.81ms
step:510/1770 train_time:47907ms step_avg:95.81ms
step:511/1770 train_time:48005ms step_avg:95.82ms
step:512/1770 train_time:48102ms step_avg:95.82ms
step:513/1770 train_time:48200ms step_avg:95.82ms
step:514/1770 train_time:48297ms step_avg:95.83ms
step:515/1770 train_time:48395ms step_avg:95.83ms
step:516/1770 train_time:48492ms step_avg:95.83ms
step:517/1770 train_time:48590ms step_avg:95.84ms
step:518/1770 train_time:48688ms step_avg:95.84ms
step:519/1770 train_time:48785ms step_avg:95.84ms
step:520/1770 train_time:48882ms step_avg:95.85ms
step:521/1770 train_time:48980ms step_avg:95.85ms
step:522/1770 train_time:49077ms step_avg:95.85ms
step:523/1770 train_time:49174ms step_avg:95.86ms
step:524/1770 train_time:49272ms step_avg:95.86ms
step:525/1770 train_time:49370ms step_avg:95.86ms
step:526/1770 train_time:49467ms step_avg:95.87ms
step:527/1770 train_time:49566ms step_avg:95.87ms
step:528/1770 train_time:49663ms step_avg:95.88ms
step:529/1770 train_time:49761ms step_avg:95.88ms
step:530/1770 train_time:49859ms step_avg:95.88ms
step:531/1770 train_time:49957ms step_avg:95.89ms
step:532/1770 train_time:50055ms step_avg:95.89ms
step:533/1770 train_time:50153ms step_avg:95.89ms
step:534/1770 train_time:50251ms step_avg:95.90ms
step:535/1770 train_time:50349ms step_avg:95.90ms
step:536/1770 train_time:50447ms step_avg:95.91ms
step:537/1770 train_time:50545ms step_avg:95.91ms
step:538/1770 train_time:50643ms step_avg:95.91ms
step:539/1770 train_time:50740ms step_avg:95.92ms
step:540/1770 train_time:50838ms step_avg:95.92ms
step:541/1770 train_time:50936ms step_avg:95.92ms
step:542/1770 train_time:51034ms step_avg:95.93ms
step:543/1770 train_time:51132ms step_avg:95.93ms
step:544/1770 train_time:51230ms step_avg:95.94ms
step:545/1770 train_time:51328ms step_avg:95.94ms
step:546/1770 train_time:51426ms step_avg:95.94ms
step:547/1770 train_time:51524ms step_avg:95.95ms
step:548/1770 train_time:51622ms step_avg:95.95ms
step:549/1770 train_time:51719ms step_avg:95.95ms
step:550/1770 train_time:51817ms step_avg:95.96ms
step:551/1770 train_time:51915ms step_avg:95.96ms
step:552/1770 train_time:52013ms step_avg:95.97ms
step:553/1770 train_time:52112ms step_avg:95.97ms
step:554/1770 train_time:52210ms step_avg:95.97ms
step:555/1770 train_time:52308ms step_avg:95.98ms
step:556/1770 train_time:52406ms step_avg:95.98ms
step:557/1770 train_time:52504ms step_avg:95.99ms
step:558/1770 train_time:52602ms step_avg:95.99ms
step:559/1770 train_time:52700ms step_avg:95.99ms
step:560/1770 train_time:52797ms step_avg:95.99ms
step:561/1770 train_time:52895ms step_avg:96.00ms
step:562/1770 train_time:52993ms step_avg:96.00ms
step:563/1770 train_time:53091ms step_avg:96.01ms
step:564/1770 train_time:53189ms step_avg:96.01ms
step:565/1770 train_time:53287ms step_avg:96.01ms
step:566/1770 train_time:53385ms step_avg:96.02ms
step:567/1770 train_time:53483ms step_avg:96.02ms
step:568/1770 train_time:53581ms step_avg:96.02ms
step:569/1770 train_time:53678ms step_avg:96.02ms
step:570/1770 train_time:53776ms step_avg:96.03ms
step:571/1770 train_time:53874ms step_avg:96.03ms
step:572/1770 train_time:53973ms step_avg:96.04ms
step:573/1770 train_time:54071ms step_avg:96.04ms
step:574/1770 train_time:54169ms step_avg:96.04ms
step:575/1770 train_time:54267ms step_avg:96.05ms
step:576/1770 train_time:54365ms step_avg:96.05ms
step:577/1770 train_time:54463ms step_avg:96.05ms
step:578/1770 train_time:54561ms step_avg:96.06ms
step:579/1770 train_time:54659ms step_avg:96.06ms
step:580/1770 train_time:54756ms step_avg:96.06ms
step:581/1770 train_time:54854ms step_avg:96.07ms
step:582/1770 train_time:54952ms step_avg:96.07ms
step:583/1770 train_time:55051ms step_avg:96.07ms
step:584/1770 train_time:55149ms step_avg:96.08ms
step:585/1770 train_time:55246ms step_avg:96.08ms
step:586/1770 train_time:55344ms step_avg:96.08ms
step:587/1770 train_time:55442ms step_avg:96.09ms
step:588/1770 train_time:55539ms step_avg:96.09ms
step:589/1770 train_time:55637ms step_avg:96.09ms
step:590/1770 train_time:55735ms step_avg:96.09ms
step:591/1770 train_time:55833ms step_avg:96.10ms
step:592/1770 train_time:55932ms step_avg:96.10ms
step:593/1770 train_time:56030ms step_avg:96.11ms
step:594/1770 train_time:56128ms step_avg:96.11ms
step:595/1770 train_time:56226ms step_avg:96.11ms
step:596/1770 train_time:56323ms step_avg:96.11ms
step:597/1770 train_time:56421ms step_avg:96.12ms
step:598/1770 train_time:56519ms step_avg:96.12ms
step:599/1770 train_time:56616ms step_avg:96.12ms
step:600/1770 train_time:56714ms step_avg:96.13ms
step:601/1770 train_time:56812ms step_avg:96.13ms
step:602/1770 train_time:56910ms step_avg:96.13ms
step:603/1770 train_time:57009ms step_avg:96.14ms
step:604/1770 train_time:57107ms step_avg:96.14ms
step:605/1770 train_time:57205ms step_avg:96.14ms
step:606/1770 train_time:57303ms step_avg:96.15ms
step:607/1770 train_time:57401ms step_avg:96.15ms
step:608/1770 train_time:57499ms step_avg:96.15ms
step:609/1770 train_time:57597ms step_avg:96.16ms
step:610/1770 train_time:57695ms step_avg:96.16ms
step:611/1770 train_time:57793ms step_avg:96.16ms
step:612/1770 train_time:57891ms step_avg:96.16ms
step:613/1770 train_time:57989ms step_avg:96.17ms
step:614/1770 train_time:58087ms step_avg:96.17ms
step:615/1770 train_time:58185ms step_avg:96.17ms
step:616/1770 train_time:58283ms step_avg:96.18ms
step:617/1770 train_time:58380ms step_avg:96.18ms
step:618/1770 train_time:58478ms step_avg:96.18ms
step:619/1770 train_time:58576ms step_avg:96.18ms
step:620/1770 train_time:58674ms step_avg:96.19ms
step:621/1770 train_time:58772ms step_avg:96.19ms
step:622/1770 train_time:58871ms step_avg:96.19ms
step:623/1770 train_time:58969ms step_avg:96.20ms
step:624/1770 train_time:59067ms step_avg:96.20ms
step:625/1770 train_time:59165ms step_avg:96.20ms
step:625/1770 val_loss:3.6672 train_time:59261ms step_avg:96.36ms
step:626/1770 train_time:59282ms step_avg:96.24ms
step:627/1770 train_time:59367ms step_avg:96.22ms
step:628/1770 train_time:59466ms step_avg:96.22ms
step:629/1770 train_time:59563ms step_avg:96.23ms
step:630/1770 train_time:59662ms step_avg:96.23ms
step:631/1770 train_time:59759ms step_avg:96.23ms
step:632/1770 train_time:59857ms step_avg:96.23ms
step:633/1770 train_time:59955ms step_avg:96.24ms
step:634/1770 train_time:60054ms step_avg:96.24ms
step:635/1770 train_time:60152ms step_avg:96.24ms
step:636/1770 train_time:60250ms step_avg:96.25ms
step:637/1770 train_time:60348ms step_avg:96.25ms
step:638/1770 train_time:60445ms step_avg:96.25ms
step:639/1770 train_time:60543ms step_avg:96.25ms
step:640/1770 train_time:60641ms step_avg:96.26ms
step:641/1770 train_time:60739ms step_avg:96.26ms
step:642/1770 train_time:60837ms step_avg:96.26ms
step:643/1770 train_time:60935ms step_avg:96.26ms
step:644/1770 train_time:61033ms step_avg:96.27ms
step:645/1770 train_time:61131ms step_avg:96.27ms
step:646/1770 train_time:61229ms step_avg:96.27ms
step:647/1770 train_time:61327ms step_avg:96.27ms
step:648/1770 train_time:61425ms step_avg:96.28ms
step:649/1770 train_time:61523ms step_avg:96.28ms
step:650/1770 train_time:61620ms step_avg:96.28ms
step:651/1770 train_time:61718ms step_avg:96.28ms
step:652/1770 train_time:61816ms step_avg:96.29ms
step:653/1770 train_time:61914ms step_avg:96.29ms
step:654/1770 train_time:62012ms step_avg:96.29ms
step:655/1770 train_time:62110ms step_avg:96.30ms
step:656/1770 train_time:62208ms step_avg:96.30ms
step:657/1770 train_time:62306ms step_avg:96.30ms
step:658/1770 train_time:62406ms step_avg:96.31ms
step:659/1770 train_time:62505ms step_avg:96.31ms
step:660/1770 train_time:62604ms step_avg:96.31ms
step:661/1770 train_time:62704ms step_avg:96.32ms
step:662/1770 train_time:62805ms step_avg:96.33ms
step:663/1770 train_time:62905ms step_avg:96.33ms
step:664/1770 train_time:63005ms step_avg:96.34ms
step:665/1770 train_time:63106ms step_avg:96.34ms
step:666/1770 train_time:63206ms step_avg:96.35ms
step:667/1770 train_time:63306ms step_avg:96.36ms
step:668/1770 train_time:63406ms step_avg:96.36ms
step:669/1770 train_time:63505ms step_avg:96.37ms
step:670/1770 train_time:63605ms step_avg:96.37ms
step:671/1770 train_time:63705ms step_avg:96.38ms
step:672/1770 train_time:63804ms step_avg:96.38ms
step:673/1770 train_time:63904ms step_avg:96.39ms
step:674/1770 train_time:64003ms step_avg:96.39ms
step:675/1770 train_time:64103ms step_avg:96.40ms
step:676/1770 train_time:64203ms step_avg:96.40ms
step:677/1770 train_time:64303ms step_avg:96.41ms
step:678/1770 train_time:64403ms step_avg:96.41ms
step:679/1770 train_time:64503ms step_avg:96.42ms
step:680/1770 train_time:64603ms step_avg:96.42ms
step:681/1770 train_time:64704ms step_avg:96.43ms
step:682/1770 train_time:64804ms step_avg:96.43ms
step:683/1770 train_time:64904ms step_avg:96.44ms
step:684/1770 train_time:65003ms step_avg:96.44ms
step:685/1770 train_time:65103ms step_avg:96.45ms
step:686/1770 train_time:65202ms step_avg:96.45ms
step:687/1770 train_time:65302ms step_avg:96.46ms
step:688/1770 train_time:65401ms step_avg:96.46ms
step:689/1770 train_time:65502ms step_avg:96.47ms
step:690/1770 train_time:65601ms step_avg:96.47ms
step:691/1770 train_time:65701ms step_avg:96.48ms
step:692/1770 train_time:65800ms step_avg:96.48ms
step:693/1770 train_time:65900ms step_avg:96.49ms
step:694/1770 train_time:66000ms step_avg:96.49ms
step:695/1770 train_time:66101ms step_avg:96.50ms
step:696/1770 train_time:66200ms step_avg:96.50ms
step:697/1770 train_time:66300ms step_avg:96.51ms
step:698/1770 train_time:66399ms step_avg:96.51ms
step:699/1770 train_time:66499ms step_avg:96.51ms
step:700/1770 train_time:66599ms step_avg:96.52ms
step:701/1770 train_time:66698ms step_avg:96.52ms
step:702/1770 train_time:66797ms step_avg:96.53ms
step:703/1770 train_time:66896ms step_avg:96.53ms
step:704/1770 train_time:66996ms step_avg:96.54ms
step:705/1770 train_time:67096ms step_avg:96.54ms
step:706/1770 train_time:67196ms step_avg:96.55ms
step:707/1770 train_time:67296ms step_avg:96.55ms
step:708/1770 train_time:67396ms step_avg:96.56ms
step:709/1770 train_time:67496ms step_avg:96.56ms
step:710/1770 train_time:67596ms step_avg:96.57ms
step:711/1770 train_time:67696ms step_avg:96.57ms
step:712/1770 train_time:67796ms step_avg:96.58ms
step:713/1770 train_time:67896ms step_avg:96.58ms
step:714/1770 train_time:67996ms step_avg:96.58ms
step:715/1770 train_time:68096ms step_avg:96.59ms
step:716/1770 train_time:68195ms step_avg:96.59ms
step:717/1770 train_time:68295ms step_avg:96.60ms
step:718/1770 train_time:68395ms step_avg:96.60ms
step:719/1770 train_time:68495ms step_avg:96.61ms
step:720/1770 train_time:68595ms step_avg:96.61ms
step:721/1770 train_time:68695ms step_avg:96.62ms
step:722/1770 train_time:68795ms step_avg:96.62ms
step:723/1770 train_time:68895ms step_avg:96.63ms
step:724/1770 train_time:68995ms step_avg:96.63ms
step:725/1770 train_time:69095ms step_avg:96.64ms
step:726/1770 train_time:69195ms step_avg:96.64ms
step:727/1770 train_time:69295ms step_avg:96.65ms
step:728/1770 train_time:69395ms step_avg:96.65ms
step:729/1770 train_time:69496ms step_avg:96.66ms
step:730/1770 train_time:69596ms step_avg:96.66ms
step:731/1770 train_time:69695ms step_avg:96.66ms
step:732/1770 train_time:69795ms step_avg:96.67ms
step:733/1770 train_time:69896ms step_avg:96.67ms
step:734/1770 train_time:69995ms step_avg:96.68ms
step:735/1770 train_time:70096ms step_avg:96.68ms
step:736/1770 train_time:70196ms step_avg:96.69ms
step:737/1770 train_time:70296ms step_avg:96.69ms
step:738/1770 train_time:70395ms step_avg:96.70ms
step:739/1770 train_time:70497ms step_avg:96.70ms
step:740/1770 train_time:70595ms step_avg:96.71ms
step:741/1770 train_time:70695ms step_avg:96.71ms
step:742/1770 train_time:70795ms step_avg:96.71ms
step:743/1770 train_time:70895ms step_avg:96.72ms
step:744/1770 train_time:70995ms step_avg:96.72ms
step:745/1770 train_time:71095ms step_avg:96.73ms
step:746/1770 train_time:71195ms step_avg:96.73ms
step:747/1770 train_time:71295ms step_avg:96.74ms
step:748/1770 train_time:71395ms step_avg:96.74ms
step:749/1770 train_time:71495ms step_avg:96.75ms
step:750/1770 train_time:71595ms step_avg:96.75ms
step:750/1770 val_loss:3.6033 train_time:71693ms step_avg:96.88ms
step:751/1770 train_time:71714ms step_avg:96.78ms
step:752/1770 train_time:71802ms step_avg:96.77ms
step:753/1770 train_time:71903ms step_avg:96.77ms
step:754/1770 train_time:72003ms step_avg:96.78ms
step:755/1770 train_time:72103ms step_avg:96.78ms
step:756/1770 train_time:72203ms step_avg:96.79ms
step:757/1770 train_time:72302ms step_avg:96.79ms
step:758/1770 train_time:72402ms step_avg:96.79ms
step:759/1770 train_time:72501ms step_avg:96.80ms
step:760/1770 train_time:72601ms step_avg:96.80ms
step:761/1770 train_time:72702ms step_avg:96.81ms
step:762/1770 train_time:72805ms step_avg:96.82ms
step:763/1770 train_time:72906ms step_avg:96.82ms
step:764/1770 train_time:73005ms step_avg:96.82ms
step:765/1770 train_time:73105ms step_avg:96.83ms
step:766/1770 train_time:73205ms step_avg:96.83ms
step:767/1770 train_time:73306ms step_avg:96.84ms
step:768/1770 train_time:73406ms step_avg:96.84ms
step:769/1770 train_time:73505ms step_avg:96.85ms
step:770/1770 train_time:73605ms step_avg:96.85ms
step:771/1770 train_time:73705ms step_avg:96.85ms
step:772/1770 train_time:73805ms step_avg:96.86ms
step:773/1770 train_time:73905ms step_avg:96.86ms
step:774/1770 train_time:74005ms step_avg:96.87ms
step:775/1770 train_time:74105ms step_avg:96.87ms
step:776/1770 train_time:74207ms step_avg:96.88ms
step:777/1770 train_time:74307ms step_avg:96.88ms
step:778/1770 train_time:74407ms step_avg:96.88ms
step:779/1770 train_time:74506ms step_avg:96.89ms
step:780/1770 train_time:74606ms step_avg:96.89ms
step:781/1770 train_time:74706ms step_avg:96.89ms
step:782/1770 train_time:74805ms step_avg:96.90ms
step:783/1770 train_time:74905ms step_avg:96.90ms
step:784/1770 train_time:75005ms step_avg:96.91ms
step:785/1770 train_time:75104ms step_avg:96.91ms
step:786/1770 train_time:75204ms step_avg:96.91ms
step:787/1770 train_time:75305ms step_avg:96.92ms
step:788/1770 train_time:75405ms step_avg:96.92ms
step:789/1770 train_time:75506ms step_avg:96.93ms
step:790/1770 train_time:75607ms step_avg:96.93ms
step:791/1770 train_time:75708ms step_avg:96.94ms
step:792/1770 train_time:75808ms step_avg:96.94ms
step:793/1770 train_time:75908ms step_avg:96.95ms
step:794/1770 train_time:76008ms step_avg:96.95ms
step:795/1770 train_time:76108ms step_avg:96.95ms
step:796/1770 train_time:76208ms step_avg:96.96ms
step:797/1770 train_time:76308ms step_avg:96.96ms
step:798/1770 train_time:76408ms step_avg:96.96ms
step:799/1770 train_time:76507ms step_avg:96.97ms
step:800/1770 train_time:76607ms step_avg:96.97ms
step:801/1770 train_time:76707ms step_avg:96.97ms
step:802/1770 train_time:76806ms step_avg:96.98ms
step:803/1770 train_time:76907ms step_avg:96.98ms
step:804/1770 train_time:77007ms step_avg:96.99ms
step:805/1770 train_time:77106ms step_avg:96.99ms
step:806/1770 train_time:77207ms step_avg:96.99ms
step:807/1770 train_time:77306ms step_avg:97.00ms
step:808/1770 train_time:77407ms step_avg:97.00ms
step:809/1770 train_time:77507ms step_avg:97.00ms
step:810/1770 train_time:77606ms step_avg:97.01ms
step:811/1770 train_time:77706ms step_avg:97.01ms
step:812/1770 train_time:77806ms step_avg:97.02ms
step:813/1770 train_time:77906ms step_avg:97.02ms
step:814/1770 train_time:78007ms step_avg:97.02ms
step:815/1770 train_time:78106ms step_avg:97.03ms
step:816/1770 train_time:78206ms step_avg:97.03ms
step:817/1770 train_time:78306ms step_avg:97.03ms
step:818/1770 train_time:78406ms step_avg:97.04ms
step:819/1770 train_time:78506ms step_avg:97.04ms
step:820/1770 train_time:78606ms step_avg:97.04ms
step:821/1770 train_time:78706ms step_avg:97.05ms
step:822/1770 train_time:78807ms step_avg:97.05ms
step:823/1770 train_time:78906ms step_avg:97.06ms
step:824/1770 train_time:79006ms step_avg:97.06ms
step:825/1770 train_time:79106ms step_avg:97.06ms
step:826/1770 train_time:79205ms step_avg:97.07ms
step:827/1770 train_time:79306ms step_avg:97.07ms
step:828/1770 train_time:79407ms step_avg:97.07ms
step:829/1770 train_time:79505ms step_avg:97.08ms
step:830/1770 train_time:79605ms step_avg:97.08ms
step:831/1770 train_time:79705ms step_avg:97.08ms
step:832/1770 train_time:79805ms step_avg:97.09ms
step:833/1770 train_time:79906ms step_avg:97.09ms
step:834/1770 train_time:80007ms step_avg:97.10ms
step:835/1770 train_time:80106ms step_avg:97.10ms
step:836/1770 train_time:80206ms step_avg:97.10ms
step:837/1770 train_time:80306ms step_avg:97.11ms
step:838/1770 train_time:80406ms step_avg:97.11ms
step:839/1770 train_time:80506ms step_avg:97.11ms
step:840/1770 train_time:80606ms step_avg:97.12ms
step:841/1770 train_time:80706ms step_avg:97.12ms
step:842/1770 train_time:80806ms step_avg:97.12ms
step:843/1770 train_time:80906ms step_avg:97.13ms
step:844/1770 train_time:81005ms step_avg:97.13ms
step:845/1770 train_time:81105ms step_avg:97.13ms
step:846/1770 train_time:81207ms step_avg:97.14ms
step:847/1770 train_time:81306ms step_avg:97.14ms
step:848/1770 train_time:81406ms step_avg:97.14ms
step:849/1770 train_time:81506ms step_avg:97.15ms
step:850/1770 train_time:81606ms step_avg:97.15ms
step:851/1770 train_time:81706ms step_avg:97.15ms
step:852/1770 train_time:81806ms step_avg:97.16ms
step:853/1770 train_time:81906ms step_avg:97.16ms
step:854/1770 train_time:82005ms step_avg:97.16ms
step:855/1770 train_time:82105ms step_avg:97.17ms
step:856/1770 train_time:82205ms step_avg:97.17ms
step:857/1770 train_time:82305ms step_avg:97.17ms
step:858/1770 train_time:82405ms step_avg:97.18ms
step:859/1770 train_time:82505ms step_avg:97.18ms
step:860/1770 train_time:82606ms step_avg:97.18ms
step:861/1770 train_time:82706ms step_avg:97.19ms
step:862/1770 train_time:82806ms step_avg:97.19ms
step:863/1770 train_time:82905ms step_avg:97.19ms
step:864/1770 train_time:83005ms step_avg:97.20ms
step:865/1770 train_time:83105ms step_avg:97.20ms
step:866/1770 train_time:83206ms step_avg:97.20ms
step:867/1770 train_time:83306ms step_avg:97.21ms
step:868/1770 train_time:83407ms step_avg:97.21ms
step:869/1770 train_time:83506ms step_avg:97.21ms
step:870/1770 train_time:83607ms step_avg:97.22ms
step:871/1770 train_time:83706ms step_avg:97.22ms
step:872/1770 train_time:83805ms step_avg:97.22ms
step:873/1770 train_time:83906ms step_avg:97.23ms
step:874/1770 train_time:84005ms step_avg:97.23ms
step:875/1770 train_time:84106ms step_avg:97.23ms
step:875/1770 val_loss:3.5537 train_time:84204ms step_avg:97.35ms
step:876/1770 train_time:84226ms step_avg:97.26ms
step:877/1770 train_time:84311ms step_avg:97.24ms
step:878/1770 train_time:84413ms step_avg:97.25ms
step:879/1770 train_time:84513ms step_avg:97.25ms
step:880/1770 train_time:84613ms step_avg:97.26ms
step:881/1770 train_time:84713ms step_avg:97.26ms
step:882/1770 train_time:84813ms step_avg:97.26ms
step:883/1770 train_time:84913ms step_avg:97.27ms
step:884/1770 train_time:85013ms step_avg:97.27ms
step:885/1770 train_time:85113ms step_avg:97.27ms
step:886/1770 train_time:85213ms step_avg:97.27ms
step:887/1770 train_time:85314ms step_avg:97.28ms
step:888/1770 train_time:85415ms step_avg:97.28ms
step:889/1770 train_time:85515ms step_avg:97.29ms
step:890/1770 train_time:85615ms step_avg:97.29ms
step:891/1770 train_time:85714ms step_avg:97.29ms
step:892/1770 train_time:85814ms step_avg:97.30ms
step:893/1770 train_time:85914ms step_avg:97.30ms
step:894/1770 train_time:86014ms step_avg:97.30ms
step:895/1770 train_time:86113ms step_avg:97.30ms
step:896/1770 train_time:86213ms step_avg:97.31ms
step:897/1770 train_time:86313ms step_avg:97.31ms
step:898/1770 train_time:86414ms step_avg:97.31ms
step:899/1770 train_time:86514ms step_avg:97.32ms
step:900/1770 train_time:86614ms step_avg:97.32ms
step:901/1770 train_time:86714ms step_avg:97.32ms
step:902/1770 train_time:86815ms step_avg:97.33ms
step:903/1770 train_time:86915ms step_avg:97.33ms
step:904/1770 train_time:87015ms step_avg:97.33ms
step:905/1770 train_time:87115ms step_avg:97.34ms
step:906/1770 train_time:87215ms step_avg:97.34ms
step:907/1770 train_time:87315ms step_avg:97.34ms
step:908/1770 train_time:87415ms step_avg:97.34ms
step:909/1770 train_time:87515ms step_avg:97.35ms
step:910/1770 train_time:87615ms step_avg:97.35ms
step:911/1770 train_time:87715ms step_avg:97.35ms
step:912/1770 train_time:87815ms step_avg:97.36ms
step:913/1770 train_time:87915ms step_avg:97.36ms
step:914/1770 train_time:88015ms step_avg:97.36ms
step:915/1770 train_time:88115ms step_avg:97.36ms
step:916/1770 train_time:88214ms step_avg:97.37ms
step:917/1770 train_time:88314ms step_avg:97.37ms
step:918/1770 train_time:88414ms step_avg:97.37ms
step:919/1770 train_time:88514ms step_avg:97.38ms
step:920/1770 train_time:88616ms step_avg:97.38ms
step:921/1770 train_time:88718ms step_avg:97.39ms
step:922/1770 train_time:88820ms step_avg:97.39ms
step:923/1770 train_time:88920ms step_avg:97.39ms
step:924/1770 train_time:89021ms step_avg:97.40ms
step:925/1770 train_time:89122ms step_avg:97.40ms
step:926/1770 train_time:89223ms step_avg:97.40ms
step:927/1770 train_time:89325ms step_avg:97.41ms
step:928/1770 train_time:89428ms step_avg:97.42ms
step:929/1770 train_time:89529ms step_avg:97.42ms
step:930/1770 train_time:89630ms step_avg:97.42ms
step:931/1770 train_time:89731ms step_avg:97.43ms
step:932/1770 train_time:89833ms step_avg:97.43ms
step:933/1770 train_time:89934ms step_avg:97.44ms
step:934/1770 train_time:90036ms step_avg:97.44ms
step:935/1770 train_time:90137ms step_avg:97.45ms
step:936/1770 train_time:90238ms step_avg:97.45ms
step:937/1770 train_time:90338ms step_avg:97.45ms
step:938/1770 train_time:90439ms step_avg:97.46ms
step:939/1770 train_time:90540ms step_avg:97.46ms
step:940/1770 train_time:90642ms step_avg:97.46ms
step:941/1770 train_time:90743ms step_avg:97.47ms
step:942/1770 train_time:90845ms step_avg:97.47ms
step:943/1770 train_time:90947ms step_avg:97.48ms
step:944/1770 train_time:91048ms step_avg:97.48ms
step:945/1770 train_time:91150ms step_avg:97.49ms
step:946/1770 train_time:91252ms step_avg:97.49ms
step:947/1770 train_time:91354ms step_avg:97.50ms
step:948/1770 train_time:91456ms step_avg:97.50ms
step:949/1770 train_time:91557ms step_avg:97.50ms
step:950/1770 train_time:91659ms step_avg:97.51ms
step:951/1770 train_time:91760ms step_avg:97.51ms
step:952/1770 train_time:91861ms step_avg:97.52ms
step:953/1770 train_time:91962ms step_avg:97.52ms
step:954/1770 train_time:92064ms step_avg:97.53ms
step:955/1770 train_time:92167ms step_avg:97.53ms
step:956/1770 train_time:92269ms step_avg:97.54ms
step:957/1770 train_time:92371ms step_avg:97.54ms
step:958/1770 train_time:92472ms step_avg:97.54ms
step:959/1770 train_time:92573ms step_avg:97.55ms
step:960/1770 train_time:92674ms step_avg:97.55ms
step:961/1770 train_time:92776ms step_avg:97.56ms
step:962/1770 train_time:92877ms step_avg:97.56ms
step:963/1770 train_time:92978ms step_avg:97.56ms
step:964/1770 train_time:93079ms step_avg:97.57ms
step:965/1770 train_time:93180ms step_avg:97.57ms
step:966/1770 train_time:93281ms step_avg:97.57ms
step:967/1770 train_time:93383ms step_avg:97.58ms
step:968/1770 train_time:93485ms step_avg:97.58ms
step:969/1770 train_time:93587ms step_avg:97.59ms
step:970/1770 train_time:93689ms step_avg:97.59ms
step:971/1770 train_time:93791ms step_avg:97.60ms
step:972/1770 train_time:93893ms step_avg:97.60ms
step:973/1770 train_time:93994ms step_avg:97.61ms
step:974/1770 train_time:94096ms step_avg:97.61ms
step:975/1770 train_time:94197ms step_avg:97.61ms
step:976/1770 train_time:94298ms step_avg:97.62ms
step:977/1770 train_time:94399ms step_avg:97.62ms
step:978/1770 train_time:94500ms step_avg:97.62ms
step:979/1770 train_time:94601ms step_avg:97.63ms
step:980/1770 train_time:94701ms step_avg:97.63ms
step:981/1770 train_time:94803ms step_avg:97.63ms
step:982/1770 train_time:94906ms step_avg:97.64ms
step:983/1770 train_time:95006ms step_avg:97.64ms
step:984/1770 train_time:95108ms step_avg:97.65ms
step:985/1770 train_time:95210ms step_avg:97.65ms
step:986/1770 train_time:95311ms step_avg:97.66ms
step:987/1770 train_time:95414ms step_avg:97.66ms
step:988/1770 train_time:95515ms step_avg:97.66ms
step:989/1770 train_time:95619ms step_avg:97.67ms
step:990/1770 train_time:95720ms step_avg:97.67ms
step:991/1770 train_time:95821ms step_avg:97.68ms
step:992/1770 train_time:95921ms step_avg:97.68ms
step:993/1770 train_time:96023ms step_avg:97.68ms
step:994/1770 train_time:96125ms step_avg:97.69ms
step:995/1770 train_time:96227ms step_avg:97.69ms
step:996/1770 train_time:96330ms step_avg:97.70ms
step:997/1770 train_time:96432ms step_avg:97.70ms
step:998/1770 train_time:96533ms step_avg:97.71ms
step:999/1770 train_time:96634ms step_avg:97.71ms
step:1000/1770 train_time:96735ms step_avg:97.71ms
step:1000/1770 val_loss:3.5152 train_time:96835ms step_avg:97.81ms
step:1001/1770 train_time:96856ms step_avg:97.74ms
step:1002/1770 train_time:96947ms step_avg:97.73ms
step:1003/1770 train_time:97051ms step_avg:97.73ms
step:1004/1770 train_time:97152ms step_avg:97.74ms
step:1005/1770 train_time:97253ms step_avg:97.74ms
step:1006/1770 train_time:97354ms step_avg:97.75ms
step:1007/1770 train_time:97455ms step_avg:97.75ms
step:1008/1770 train_time:97556ms step_avg:97.75ms
step:1009/1770 train_time:97657ms step_avg:97.75ms
step:1010/1770 train_time:97758ms step_avg:97.76ms
step:1011/1770 train_time:97860ms step_avg:97.76ms
step:1012/1770 train_time:97962ms step_avg:97.77ms
step:1013/1770 train_time:98063ms step_avg:97.77ms
step:1014/1770 train_time:98165ms step_avg:97.77ms
step:1015/1770 train_time:98268ms step_avg:97.78ms
step:1016/1770 train_time:98369ms step_avg:97.78ms
step:1017/1770 train_time:98470ms step_avg:97.79ms
step:1018/1770 train_time:98572ms step_avg:97.79ms
step:1019/1770 train_time:98673ms step_avg:97.79ms
step:1020/1770 train_time:98775ms step_avg:97.80ms
step:1021/1770 train_time:98877ms step_avg:97.80ms
step:1022/1770 train_time:98977ms step_avg:97.80ms
step:1023/1770 train_time:99078ms step_avg:97.81ms
step:1024/1770 train_time:99180ms step_avg:97.81ms
step:1025/1770 train_time:99282ms step_avg:97.81ms
step:1026/1770 train_time:99383ms step_avg:97.82ms
step:1027/1770 train_time:99485ms step_avg:97.82ms
step:1028/1770 train_time:99588ms step_avg:97.83ms
step:1029/1770 train_time:99689ms step_avg:97.83ms
step:1030/1770 train_time:99791ms step_avg:97.83ms
step:1031/1770 train_time:99892ms step_avg:97.84ms
step:1032/1770 train_time:99994ms step_avg:97.84ms
step:1033/1770 train_time:100096ms step_avg:97.85ms
step:1034/1770 train_time:100196ms step_avg:97.85ms
step:1035/1770 train_time:100297ms step_avg:97.85ms
step:1036/1770 train_time:100398ms step_avg:97.85ms
step:1037/1770 train_time:100499ms step_avg:97.86ms
step:1038/1770 train_time:100600ms step_avg:97.86ms
step:1039/1770 train_time:100701ms step_avg:97.86ms
step:1040/1770 train_time:100803ms step_avg:97.87ms
step:1041/1770 train_time:100905ms step_avg:97.87ms
step:1042/1770 train_time:101007ms step_avg:97.88ms
step:1043/1770 train_time:101109ms step_avg:97.88ms
step:1044/1770 train_time:101210ms step_avg:97.88ms
step:1045/1770 train_time:101311ms step_avg:97.89ms
step:1046/1770 train_time:101413ms step_avg:97.89ms
step:1047/1770 train_time:101514ms step_avg:97.89ms
step:1048/1770 train_time:101614ms step_avg:97.89ms
step:1049/1770 train_time:101716ms step_avg:97.90ms
step:1050/1770 train_time:101817ms step_avg:97.90ms
step:1051/1770 train_time:101919ms step_avg:97.90ms
step:1052/1770 train_time:102020ms step_avg:97.91ms
step:1053/1770 train_time:102121ms step_avg:97.91ms
step:1054/1770 train_time:102223ms step_avg:97.91ms
step:1055/1770 train_time:102325ms step_avg:97.92ms
step:1056/1770 train_time:102426ms step_avg:97.92ms
step:1057/1770 train_time:102529ms step_avg:97.93ms
step:1058/1770 train_time:102631ms step_avg:97.93ms
step:1059/1770 train_time:102734ms step_avg:97.93ms
step:1060/1770 train_time:102836ms step_avg:97.94ms
step:1061/1770 train_time:102937ms step_avg:97.94ms
step:1062/1770 train_time:103039ms step_avg:97.95ms
step:1063/1770 train_time:103141ms step_avg:97.95ms
step:1064/1770 train_time:103243ms step_avg:97.95ms
step:1065/1770 train_time:103345ms step_avg:97.96ms
step:1066/1770 train_time:103446ms step_avg:97.96ms
step:1067/1770 train_time:103549ms step_avg:97.96ms
step:1068/1770 train_time:103651ms step_avg:97.97ms
step:1069/1770 train_time:103753ms step_avg:97.97ms
step:1070/1770 train_time:103855ms step_avg:97.98ms
step:1071/1770 train_time:103956ms step_avg:97.98ms
step:1072/1770 train_time:104057ms step_avg:97.98ms
step:1073/1770 train_time:104158ms step_avg:97.99ms
step:1074/1770 train_time:104259ms step_avg:97.99ms
step:1075/1770 train_time:104362ms step_avg:97.99ms
step:1076/1770 train_time:104463ms step_avg:98.00ms
step:1077/1770 train_time:104565ms step_avg:98.00ms
step:1078/1770 train_time:104668ms step_avg:98.00ms
step:1079/1770 train_time:104770ms step_avg:98.01ms
step:1080/1770 train_time:104872ms step_avg:98.01ms
step:1081/1770 train_time:104973ms step_avg:98.01ms
step:1082/1770 train_time:105075ms step_avg:98.02ms
step:1083/1770 train_time:105176ms step_avg:98.02ms
step:1084/1770 train_time:105278ms step_avg:98.02ms
step:1085/1770 train_time:105379ms step_avg:98.03ms
step:1086/1770 train_time:105480ms step_avg:98.03ms
step:1087/1770 train_time:105582ms step_avg:98.03ms
step:1088/1770 train_time:105683ms step_avg:98.04ms
step:1089/1770 train_time:105785ms step_avg:98.04ms
step:1090/1770 train_time:105888ms step_avg:98.04ms
step:1091/1770 train_time:105990ms step_avg:98.05ms
step:1092/1770 train_time:106090ms step_avg:98.05ms
step:1093/1770 train_time:106192ms step_avg:98.05ms
step:1094/1770 train_time:106294ms step_avg:98.06ms
step:1095/1770 train_time:106395ms step_avg:98.06ms
step:1096/1770 train_time:106496ms step_avg:98.06ms
step:1097/1770 train_time:106597ms step_avg:98.07ms
step:1098/1770 train_time:106698ms step_avg:98.07ms
step:1099/1770 train_time:106799ms step_avg:98.07ms
step:1100/1770 train_time:106900ms step_avg:98.07ms
step:1101/1770 train_time:107002ms step_avg:98.08ms
step:1102/1770 train_time:107103ms step_avg:98.08ms
step:1103/1770 train_time:107206ms step_avg:98.08ms
step:1104/1770 train_time:107308ms step_avg:98.09ms
step:1105/1770 train_time:107409ms step_avg:98.09ms
step:1106/1770 train_time:107511ms step_avg:98.09ms
step:1107/1770 train_time:107612ms step_avg:98.10ms
step:1108/1770 train_time:107715ms step_avg:98.10ms
step:1109/1770 train_time:107815ms step_avg:98.10ms
step:1110/1770 train_time:107917ms step_avg:98.11ms
step:1111/1770 train_time:108018ms step_avg:98.11ms
step:1112/1770 train_time:108120ms step_avg:98.11ms
step:1113/1770 train_time:108221ms step_avg:98.12ms
step:1114/1770 train_time:108323ms step_avg:98.12ms
step:1115/1770 train_time:108425ms step_avg:98.12ms
step:1116/1770 train_time:108528ms step_avg:98.13ms
step:1117/1770 train_time:108629ms step_avg:98.13ms
step:1118/1770 train_time:108731ms step_avg:98.13ms
step:1119/1770 train_time:108832ms step_avg:98.14ms
step:1120/1770 train_time:108933ms step_avg:98.14ms
step:1121/1770 train_time:109035ms step_avg:98.14ms
step:1122/1770 train_time:109136ms step_avg:98.14ms
step:1123/1770 train_time:109237ms step_avg:98.15ms
step:1124/1770 train_time:109338ms step_avg:98.15ms
step:1125/1770 train_time:109440ms step_avg:98.15ms
step:1125/1770 val_loss:3.4749 train_time:109539ms step_avg:98.24ms
step:1126/1770 train_time:109561ms step_avg:98.17ms
step:1127/1770 train_time:109650ms step_avg:98.16ms
step:1128/1770 train_time:109751ms step_avg:98.17ms
step:1129/1770 train_time:109852ms step_avg:98.17ms
step:1130/1770 train_time:109954ms step_avg:98.17ms
step:1131/1770 train_time:110056ms step_avg:98.18ms
step:1132/1770 train_time:110158ms step_avg:98.18ms
step:1133/1770 train_time:110259ms step_avg:98.18ms
step:1134/1770 train_time:110361ms step_avg:98.19ms
step:1135/1770 train_time:110462ms step_avg:98.19ms
step:1136/1770 train_time:110565ms step_avg:98.19ms
step:1137/1770 train_time:110667ms step_avg:98.20ms
step:1138/1770 train_time:110768ms step_avg:98.20ms
step:1139/1770 train_time:110869ms step_avg:98.20ms
step:1140/1770 train_time:110970ms step_avg:98.20ms
step:1141/1770 train_time:111071ms step_avg:98.21ms
step:1142/1770 train_time:111172ms step_avg:98.21ms
step:1143/1770 train_time:111274ms step_avg:98.21ms
step:1144/1770 train_time:111376ms step_avg:98.21ms
step:1145/1770 train_time:111477ms step_avg:98.22ms
step:1146/1770 train_time:111581ms step_avg:98.22ms
step:1147/1770 train_time:111683ms step_avg:98.23ms
step:1148/1770 train_time:111784ms step_avg:98.23ms
step:1149/1770 train_time:111886ms step_avg:98.23ms
step:1150/1770 train_time:111987ms step_avg:98.23ms
step:1151/1770 train_time:112090ms step_avg:98.24ms
step:1152/1770 train_time:112192ms step_avg:98.24ms
step:1153/1770 train_time:112293ms step_avg:98.24ms
step:1154/1770 train_time:112395ms step_avg:98.25ms
step:1155/1770 train_time:112496ms step_avg:98.25ms
step:1156/1770 train_time:112598ms step_avg:98.25ms
step:1157/1770 train_time:112701ms step_avg:98.26ms
step:1158/1770 train_time:112803ms step_avg:98.26ms
step:1159/1770 train_time:112905ms step_avg:98.26ms
step:1160/1770 train_time:113006ms step_avg:98.27ms
step:1161/1770 train_time:113108ms step_avg:98.27ms
step:1162/1770 train_time:113210ms step_avg:98.27ms
step:1163/1770 train_time:113311ms step_avg:98.27ms
step:1164/1770 train_time:113412ms step_avg:98.28ms
step:1165/1770 train_time:113514ms step_avg:98.28ms
step:1166/1770 train_time:113615ms step_avg:98.28ms
step:1167/1770 train_time:113718ms step_avg:98.29ms
step:1168/1770 train_time:113820ms step_avg:98.29ms
step:1169/1770 train_time:113922ms step_avg:98.29ms
step:1170/1770 train_time:114022ms step_avg:98.30ms
step:1171/1770 train_time:114124ms step_avg:98.30ms
step:1172/1770 train_time:114226ms step_avg:98.30ms
step:1173/1770 train_time:114328ms step_avg:98.30ms
step:1174/1770 train_time:114429ms step_avg:98.31ms
step:1175/1770 train_time:114530ms step_avg:98.31ms
step:1176/1770 train_time:114632ms step_avg:98.31ms
step:1177/1770 train_time:114733ms step_avg:98.31ms
step:1178/1770 train_time:114835ms step_avg:98.32ms
step:1179/1770 train_time:114937ms step_avg:98.32ms
step:1180/1770 train_time:115039ms step_avg:98.32ms
step:1181/1770 train_time:115141ms step_avg:98.33ms
step:1182/1770 train_time:115243ms step_avg:98.33ms
step:1183/1770 train_time:115345ms step_avg:98.33ms
step:1184/1770 train_time:115449ms step_avg:98.34ms
step:1185/1770 train_time:115551ms step_avg:98.34ms
step:1186/1770 train_time:115654ms step_avg:98.35ms
step:1187/1770 train_time:115760ms step_avg:98.35ms
step:1188/1770 train_time:115863ms step_avg:98.36ms
step:1189/1770 train_time:115966ms step_avg:98.36ms
step:1190/1770 train_time:116069ms step_avg:98.36ms
step:1191/1770 train_time:116172ms step_avg:98.37ms
step:1192/1770 train_time:116275ms step_avg:98.37ms
step:1193/1770 train_time:116378ms step_avg:98.38ms
step:1194/1770 train_time:116480ms step_avg:98.38ms
step:1195/1770 train_time:116584ms step_avg:98.38ms
step:1196/1770 train_time:116687ms step_avg:98.39ms
step:1197/1770 train_time:116789ms step_avg:98.39ms
step:1198/1770 train_time:116891ms step_avg:98.39ms
step:1199/1770 train_time:116994ms step_avg:98.40ms
step:1200/1770 train_time:117097ms step_avg:98.40ms
step:1201/1770 train_time:117200ms step_avg:98.40ms
step:1202/1770 train_time:117302ms step_avg:98.41ms
step:1203/1770 train_time:117404ms step_avg:98.41ms
step:1204/1770 train_time:117507ms step_avg:98.41ms
step:1205/1770 train_time:117609ms step_avg:98.42ms
step:1206/1770 train_time:117712ms step_avg:98.42ms
step:1207/1770 train_time:117815ms step_avg:98.43ms
step:1208/1770 train_time:117918ms step_avg:98.43ms
step:1209/1770 train_time:118020ms step_avg:98.43ms
step:1210/1770 train_time:118122ms step_avg:98.44ms
step:1211/1770 train_time:118226ms step_avg:98.44ms
step:1212/1770 train_time:118330ms step_avg:98.44ms
step:1213/1770 train_time:118433ms step_avg:98.45ms
step:1214/1770 train_time:118535ms step_avg:98.45ms
step:1215/1770 train_time:118638ms step_avg:98.45ms
step:1216/1770 train_time:118744ms step_avg:98.46ms
step:1217/1770 train_time:118846ms step_avg:98.46ms
step:1218/1770 train_time:118949ms step_avg:98.47ms
step:1219/1770 train_time:119051ms step_avg:98.47ms
step:1220/1770 train_time:119154ms step_avg:98.47ms
step:1221/1770 train_time:119256ms step_avg:98.48ms
step:1222/1770 train_time:119360ms step_avg:98.48ms
step:1223/1770 train_time:119463ms step_avg:98.49ms
step:1224/1770 train_time:119568ms step_avg:98.49ms
step:1225/1770 train_time:119670ms step_avg:98.49ms
step:1226/1770 train_time:119772ms step_avg:98.50ms
step:1227/1770 train_time:119877ms step_avg:98.50ms
step:1228/1770 train_time:119981ms step_avg:98.51ms
step:1229/1770 train_time:120084ms step_avg:98.51ms
step:1230/1770 train_time:120188ms step_avg:98.51ms
step:1231/1770 train_time:120290ms step_avg:98.52ms
step:1232/1770 train_time:120393ms step_avg:98.52ms
step:1233/1770 train_time:120496ms step_avg:98.52ms
step:1234/1770 train_time:120599ms step_avg:98.53ms
step:1235/1770 train_time:120702ms step_avg:98.53ms
step:1236/1770 train_time:120805ms step_avg:98.54ms
step:1237/1770 train_time:120908ms step_avg:98.54ms
step:1238/1770 train_time:121012ms step_avg:98.54ms
step:1239/1770 train_time:121114ms step_avg:98.55ms
step:1240/1770 train_time:121217ms step_avg:98.55ms
step:1241/1770 train_time:121320ms step_avg:98.55ms
step:1242/1770 train_time:121422ms step_avg:98.56ms
step:1243/1770 train_time:121526ms step_avg:98.56ms
step:1244/1770 train_time:121628ms step_avg:98.56ms
step:1245/1770 train_time:121730ms step_avg:98.57ms
step:1246/1770 train_time:121833ms step_avg:98.57ms
step:1247/1770 train_time:121936ms step_avg:98.57ms
step:1248/1770 train_time:122039ms step_avg:98.58ms
step:1249/1770 train_time:122142ms step_avg:98.58ms
step:1250/1770 train_time:122244ms step_avg:98.58ms
step:1250/1770 val_loss:3.4280 train_time:122346ms step_avg:98.67ms
step:1251/1770 train_time:122367ms step_avg:98.60ms
step:1252/1770 train_time:122454ms step_avg:98.59ms
step:1253/1770 train_time:122556ms step_avg:98.60ms
step:1254/1770 train_time:122658ms step_avg:98.60ms
step:1255/1770 train_time:122763ms step_avg:98.61ms
step:1256/1770 train_time:122865ms step_avg:98.61ms
step:1257/1770 train_time:122967ms step_avg:98.61ms
step:1258/1770 train_time:123071ms step_avg:98.61ms
step:1259/1770 train_time:123174ms step_avg:98.62ms
step:1260/1770 train_time:123276ms step_avg:98.62ms
step:1261/1770 train_time:123379ms step_avg:98.62ms
step:1262/1770 train_time:123484ms step_avg:98.63ms
step:1263/1770 train_time:123585ms step_avg:98.63ms
step:1264/1770 train_time:123689ms step_avg:98.64ms
step:1265/1770 train_time:123791ms step_avg:98.64ms
step:1266/1770 train_time:123895ms step_avg:98.64ms
step:1267/1770 train_time:123998ms step_avg:98.65ms
step:1268/1770 train_time:124100ms step_avg:98.65ms
step:1269/1770 train_time:124203ms step_avg:98.65ms
step:1270/1770 train_time:124307ms step_avg:98.66ms
step:1271/1770 train_time:124410ms step_avg:98.66ms
step:1272/1770 train_time:124512ms step_avg:98.66ms
step:1273/1770 train_time:124615ms step_avg:98.67ms
step:1274/1770 train_time:124718ms step_avg:98.67ms
step:1275/1770 train_time:124820ms step_avg:98.67ms
step:1276/1770 train_time:124923ms step_avg:98.68ms
step:1277/1770 train_time:125025ms step_avg:98.68ms
step:1278/1770 train_time:125129ms step_avg:98.68ms
step:1279/1770 train_time:125232ms step_avg:98.69ms
step:1280/1770 train_time:125336ms step_avg:98.69ms
step:1281/1770 train_time:125437ms step_avg:98.69ms
step:1282/1770 train_time:125541ms step_avg:98.70ms
step:1283/1770 train_time:125645ms step_avg:98.70ms
step:1284/1770 train_time:125748ms step_avg:98.70ms
step:1285/1770 train_time:125851ms step_avg:98.71ms
step:1286/1770 train_time:125954ms step_avg:98.71ms
step:1287/1770 train_time:126059ms step_avg:98.71ms
step:1288/1770 train_time:126162ms step_avg:98.72ms
step:1289/1770 train_time:126265ms step_avg:98.72ms
step:1290/1770 train_time:126367ms step_avg:98.72ms
step:1291/1770 train_time:126470ms step_avg:98.73ms
step:1292/1770 train_time:126573ms step_avg:98.73ms
step:1293/1770 train_time:126676ms step_avg:98.73ms
step:1294/1770 train_time:126778ms step_avg:98.74ms
step:1295/1770 train_time:126880ms step_avg:98.74ms
step:1296/1770 train_time:126983ms step_avg:98.74ms
step:1297/1770 train_time:127086ms step_avg:98.75ms
step:1298/1770 train_time:127189ms step_avg:98.75ms
step:1299/1770 train_time:127291ms step_avg:98.75ms
step:1300/1770 train_time:127394ms step_avg:98.76ms
step:1301/1770 train_time:127497ms step_avg:98.76ms
step:1302/1770 train_time:127600ms step_avg:98.76ms
step:1303/1770 train_time:127702ms step_avg:98.76ms
step:1304/1770 train_time:127806ms step_avg:98.77ms
step:1305/1770 train_time:127908ms step_avg:98.77ms
step:1306/1770 train_time:128012ms step_avg:98.77ms
step:1307/1770 train_time:128114ms step_avg:98.78ms
step:1308/1770 train_time:128217ms step_avg:98.78ms
step:1309/1770 train_time:128319ms step_avg:98.78ms
step:1310/1770 train_time:128422ms step_avg:98.79ms
step:1311/1770 train_time:128524ms step_avg:98.79ms
step:1312/1770 train_time:128627ms step_avg:98.79ms
step:1313/1770 train_time:128729ms step_avg:98.79ms
step:1314/1770 train_time:128832ms step_avg:98.80ms
step:1315/1770 train_time:128935ms step_avg:98.80ms
step:1316/1770 train_time:129038ms step_avg:98.80ms
step:1317/1770 train_time:129141ms step_avg:98.81ms
step:1318/1770 train_time:129247ms step_avg:98.81ms
step:1319/1770 train_time:129350ms step_avg:98.82ms
step:1320/1770 train_time:129453ms step_avg:98.82ms
step:1321/1770 train_time:129556ms step_avg:98.82ms
step:1322/1770 train_time:129658ms step_avg:98.82ms
step:1323/1770 train_time:129763ms step_avg:98.83ms
step:1324/1770 train_time:129868ms step_avg:98.83ms
step:1325/1770 train_time:129972ms step_avg:98.84ms
step:1326/1770 train_time:130074ms step_avg:98.84ms
step:1327/1770 train_time:130179ms step_avg:98.85ms
step:1328/1770 train_time:130281ms step_avg:98.85ms
step:1329/1770 train_time:130384ms step_avg:98.85ms
step:1330/1770 train_time:130486ms step_avg:98.85ms
step:1331/1770 train_time:130588ms step_avg:98.86ms
step:1332/1770 train_time:130691ms step_avg:98.86ms
step:1333/1770 train_time:130793ms step_avg:98.86ms
step:1334/1770 train_time:130896ms step_avg:98.86ms
step:1335/1770 train_time:130998ms step_avg:98.87ms
step:1336/1770 train_time:131099ms step_avg:98.87ms
step:1337/1770 train_time:131202ms step_avg:98.87ms
step:1338/1770 train_time:131304ms step_avg:98.87ms
step:1339/1770 train_time:131408ms step_avg:98.88ms
step:1340/1770 train_time:131512ms step_avg:98.88ms
step:1341/1770 train_time:131615ms step_avg:98.88ms
step:1342/1770 train_time:131718ms step_avg:98.89ms
step:1343/1770 train_time:131822ms step_avg:98.89ms
step:1344/1770 train_time:131925ms step_avg:98.89ms
step:1345/1770 train_time:132027ms step_avg:98.90ms
step:1346/1770 train_time:132130ms step_avg:98.90ms
step:1347/1770 train_time:132233ms step_avg:98.90ms
step:1348/1770 train_time:132338ms step_avg:98.91ms
step:1349/1770 train_time:132441ms step_avg:98.91ms
step:1350/1770 train_time:132544ms step_avg:98.91ms
step:1351/1770 train_time:132648ms step_avg:98.92ms
step:1352/1770 train_time:132751ms step_avg:98.92ms
step:1353/1770 train_time:132855ms step_avg:98.92ms
step:1354/1770 train_time:132957ms step_avg:98.93ms
step:1355/1770 train_time:133060ms step_avg:98.93ms
step:1356/1770 train_time:133163ms step_avg:98.93ms
step:1357/1770 train_time:133266ms step_avg:98.94ms
step:1358/1770 train_time:133369ms step_avg:98.94ms
step:1359/1770 train_time:133473ms step_avg:98.94ms
step:1360/1770 train_time:133575ms step_avg:98.94ms
step:1361/1770 train_time:133679ms step_avg:98.95ms
step:1362/1770 train_time:133782ms step_avg:98.95ms
step:1363/1770 train_time:133886ms step_avg:98.95ms
step:1364/1770 train_time:133989ms step_avg:98.96ms
step:1365/1770 train_time:134092ms step_avg:98.96ms
step:1366/1770 train_time:134194ms step_avg:98.96ms
step:1367/1770 train_time:134297ms step_avg:98.97ms
step:1368/1770 train_time:134400ms step_avg:98.97ms
step:1369/1770 train_time:134504ms step_avg:98.97ms
step:1370/1770 train_time:134607ms step_avg:98.98ms
step:1371/1770 train_time:134709ms step_avg:98.98ms
step:1372/1770 train_time:134811ms step_avg:98.98ms
step:1373/1770 train_time:134914ms step_avg:98.98ms
step:1374/1770 train_time:135018ms step_avg:98.99ms
step:1375/1770 train_time:135121ms step_avg:98.99ms
step:1375/1770 val_loss:3.3864 train_time:135224ms step_avg:99.07ms
step:1376/1770 train_time:135248ms step_avg:99.01ms
step:1377/1770 train_time:135334ms step_avg:99.00ms
step:1378/1770 train_time:135437ms step_avg:99.00ms
step:1379/1770 train_time:135540ms step_avg:99.01ms
step:1380/1770 train_time:135642ms step_avg:99.01ms
step:1381/1770 train_time:135745ms step_avg:99.01ms
step:1382/1770 train_time:135847ms step_avg:99.01ms
step:1383/1770 train_time:135950ms step_avg:99.02ms
step:1384/1770 train_time:136053ms step_avg:99.02ms
step:1385/1770 train_time:136155ms step_avg:99.02ms
step:1386/1770 train_time:136259ms step_avg:99.03ms
step:1387/1770 train_time:136362ms step_avg:99.03ms
step:1388/1770 train_time:136465ms step_avg:99.03ms
step:1389/1770 train_time:136567ms step_avg:99.03ms
step:1390/1770 train_time:136670ms step_avg:99.04ms
step:1391/1770 train_time:136773ms step_avg:99.04ms
step:1392/1770 train_time:136876ms step_avg:99.04ms
step:1393/1770 train_time:136978ms step_avg:99.04ms
step:1394/1770 train_time:137081ms step_avg:99.05ms
step:1395/1770 train_time:137184ms step_avg:99.05ms
step:1396/1770 train_time:137288ms step_avg:99.05ms
step:1397/1770 train_time:137390ms step_avg:99.06ms
step:1398/1770 train_time:137494ms step_avg:99.06ms
step:1399/1770 train_time:137597ms step_avg:99.06ms
step:1400/1770 train_time:137701ms step_avg:99.07ms
step:1401/1770 train_time:137804ms step_avg:99.07ms
step:1402/1770 train_time:137907ms step_avg:99.07ms
step:1403/1770 train_time:138009ms step_avg:99.07ms
step:1404/1770 train_time:138114ms step_avg:99.08ms
step:1405/1770 train_time:138216ms step_avg:99.08ms
step:1406/1770 train_time:138319ms step_avg:99.08ms
step:1407/1770 train_time:138421ms step_avg:99.08ms
step:1408/1770 train_time:138524ms step_avg:99.09ms
step:1409/1770 train_time:138626ms step_avg:99.09ms
step:1410/1770 train_time:138729ms step_avg:99.09ms
step:1411/1770 train_time:138831ms step_avg:99.09ms
step:1412/1770 train_time:138934ms step_avg:99.10ms
step:1413/1770 train_time:139036ms step_avg:99.10ms
step:1414/1770 train_time:139140ms step_avg:99.10ms
step:1415/1770 train_time:139243ms step_avg:99.11ms
step:1416/1770 train_time:139346ms step_avg:99.11ms
step:1417/1770 train_time:139449ms step_avg:99.11ms
step:1418/1770 train_time:139552ms step_avg:99.11ms
step:1419/1770 train_time:139656ms step_avg:99.12ms
step:1420/1770 train_time:139758ms step_avg:99.12ms
step:1421/1770 train_time:139860ms step_avg:99.12ms
step:1422/1770 train_time:139964ms step_avg:99.12ms
step:1423/1770 train_time:140067ms step_avg:99.13ms
step:1424/1770 train_time:140171ms step_avg:99.13ms
step:1425/1770 train_time:140274ms step_avg:99.13ms
step:1426/1770 train_time:140377ms step_avg:99.14ms
step:1427/1770 train_time:140480ms step_avg:99.14ms
step:1428/1770 train_time:140584ms step_avg:99.14ms
step:1429/1770 train_time:140687ms step_avg:99.15ms
step:1430/1770 train_time:140789ms step_avg:99.15ms
step:1431/1770 train_time:140894ms step_avg:99.15ms
step:1432/1770 train_time:140997ms step_avg:99.15ms
step:1433/1770 train_time:141099ms step_avg:99.16ms
step:1434/1770 train_time:141201ms step_avg:99.16ms
step:1435/1770 train_time:141303ms step_avg:99.16ms
step:1436/1770 train_time:141408ms step_avg:99.16ms
step:1437/1770 train_time:141511ms step_avg:99.17ms
step:1438/1770 train_time:141613ms step_avg:99.17ms
step:1439/1770 train_time:141716ms step_avg:99.17ms
step:1440/1770 train_time:141819ms step_avg:99.17ms
step:1441/1770 train_time:141925ms step_avg:99.18ms
step:1442/1770 train_time:142028ms step_avg:99.18ms
step:1443/1770 train_time:142131ms step_avg:99.18ms
step:1444/1770 train_time:142235ms step_avg:99.19ms
step:1445/1770 train_time:142338ms step_avg:99.19ms
step:1446/1770 train_time:142442ms step_avg:99.19ms
step:1447/1770 train_time:142546ms step_avg:99.20ms
step:1448/1770 train_time:142650ms step_avg:99.20ms
step:1449/1770 train_time:142755ms step_avg:99.20ms
step:1450/1770 train_time:142859ms step_avg:99.21ms
step:1451/1770 train_time:142963ms step_avg:99.21ms
step:1452/1770 train_time:143068ms step_avg:99.21ms
step:1453/1770 train_time:143172ms step_avg:99.22ms
step:1454/1770 train_time:143276ms step_avg:99.22ms
step:1455/1770 train_time:143380ms step_avg:99.23ms
step:1456/1770 train_time:143485ms step_avg:99.23ms
step:1457/1770 train_time:143589ms step_avg:99.23ms
step:1458/1770 train_time:143694ms step_avg:99.24ms
step:1459/1770 train_time:143798ms step_avg:99.24ms
step:1460/1770 train_time:143902ms step_avg:99.24ms
step:1461/1770 train_time:144006ms step_avg:99.25ms
step:1462/1770 train_time:144109ms step_avg:99.25ms
step:1463/1770 train_time:144213ms step_avg:99.25ms
step:1464/1770 train_time:144318ms step_avg:99.26ms
step:1465/1770 train_time:144422ms step_avg:99.26ms
step:1466/1770 train_time:144527ms step_avg:99.26ms
step:1467/1770 train_time:144632ms step_avg:99.27ms
step:1468/1770 train_time:144736ms step_avg:99.27ms
step:1469/1770 train_time:144840ms step_avg:99.27ms
step:1470/1770 train_time:144944ms step_avg:99.28ms
step:1471/1770 train_time:145047ms step_avg:99.28ms
step:1472/1770 train_time:145151ms step_avg:99.28ms
step:1473/1770 train_time:145256ms step_avg:99.29ms
step:1474/1770 train_time:145363ms step_avg:99.29ms
step:1475/1770 train_time:145466ms step_avg:99.29ms
step:1476/1770 train_time:145569ms step_avg:99.30ms
step:1477/1770 train_time:145675ms step_avg:99.30ms
step:1478/1770 train_time:145780ms step_avg:99.31ms
step:1479/1770 train_time:145884ms step_avg:99.31ms
step:1480/1770 train_time:145987ms step_avg:99.31ms
step:1481/1770 train_time:146095ms step_avg:99.32ms
step:1482/1770 train_time:146199ms step_avg:99.32ms
step:1483/1770 train_time:146303ms step_avg:99.32ms
step:1484/1770 train_time:146407ms step_avg:99.33ms
step:1485/1770 train_time:146510ms step_avg:99.33ms
step:1486/1770 train_time:146614ms step_avg:99.33ms
step:1487/1770 train_time:146718ms step_avg:99.33ms
step:1488/1770 train_time:146823ms step_avg:99.34ms
step:1489/1770 train_time:146928ms step_avg:99.34ms
step:1490/1770 train_time:147032ms step_avg:99.35ms
step:1491/1770 train_time:147136ms step_avg:99.35ms
step:1492/1770 train_time:147241ms step_avg:99.35ms
step:1493/1770 train_time:147347ms step_avg:99.36ms
step:1494/1770 train_time:147455ms step_avg:99.36ms
step:1495/1770 train_time:147558ms step_avg:99.37ms
step:1496/1770 train_time:147661ms step_avg:99.37ms
step:1497/1770 train_time:147766ms step_avg:99.37ms
step:1498/1770 train_time:147870ms step_avg:99.37ms
step:1499/1770 train_time:147973ms step_avg:99.38ms
step:1500/1770 train_time:148077ms step_avg:99.38ms
step:1500/1770 val_loss:3.3508 train_time:148179ms step_avg:99.45ms
step:1501/1770 train_time:148200ms step_avg:99.40ms
step:1502/1770 train_time:148290ms step_avg:99.39ms
step:1503/1770 train_time:148393ms step_avg:99.39ms
step:1504/1770 train_time:148497ms step_avg:99.40ms
step:1505/1770 train_time:148603ms step_avg:99.40ms
step:1506/1770 train_time:148707ms step_avg:99.40ms
step:1507/1770 train_time:148811ms step_avg:99.41ms
step:1508/1770 train_time:148916ms step_avg:99.41ms
step:1509/1770 train_time:149021ms step_avg:99.41ms
step:1510/1770 train_time:149124ms step_avg:99.42ms
step:1511/1770 train_time:149229ms step_avg:99.42ms
step:1512/1770 train_time:149334ms step_avg:99.42ms
step:1513/1770 train_time:149440ms step_avg:99.43ms
step:1514/1770 train_time:149544ms step_avg:99.43ms
step:1515/1770 train_time:149647ms step_avg:99.43ms
step:1516/1770 train_time:149752ms step_avg:99.44ms
step:1517/1770 train_time:149856ms step_avg:99.44ms
step:1518/1770 train_time:149963ms step_avg:99.44ms
step:1519/1770 train_time:150065ms step_avg:99.45ms
step:1520/1770 train_time:150171ms step_avg:99.45ms
step:1521/1770 train_time:150275ms step_avg:99.45ms
step:1522/1770 train_time:150380ms step_avg:99.46ms
step:1523/1770 train_time:150484ms step_avg:99.46ms
step:1524/1770 train_time:150588ms step_avg:99.46ms
step:1525/1770 train_time:150691ms step_avg:99.47ms
step:1526/1770 train_time:150795ms step_avg:99.47ms
step:1527/1770 train_time:150900ms step_avg:99.47ms
step:1528/1770 train_time:151006ms step_avg:99.48ms
step:1529/1770 train_time:151109ms step_avg:99.48ms
step:1530/1770 train_time:151213ms step_avg:99.48ms
step:1531/1770 train_time:151317ms step_avg:99.49ms
step:1532/1770 train_time:151422ms step_avg:99.49ms
step:1533/1770 train_time:151526ms step_avg:99.49ms
step:1534/1770 train_time:151631ms step_avg:99.50ms
step:1535/1770 train_time:151734ms step_avg:99.50ms
step:1536/1770 train_time:151838ms step_avg:99.50ms
step:1537/1770 train_time:151942ms step_avg:99.50ms
step:1538/1770 train_time:152047ms step_avg:99.51ms
step:1539/1770 train_time:152151ms step_avg:99.51ms
step:1540/1770 train_time:152258ms step_avg:99.51ms
step:1541/1770 train_time:152363ms step_avg:99.52ms
step:1542/1770 train_time:152467ms step_avg:99.52ms
step:1543/1770 train_time:152570ms step_avg:99.52ms
step:1544/1770 train_time:152676ms step_avg:99.53ms
step:1545/1770 train_time:152781ms step_avg:99.53ms
step:1546/1770 train_time:152885ms step_avg:99.53ms
step:1547/1770 train_time:152988ms step_avg:99.54ms
step:1548/1770 train_time:153092ms step_avg:99.54ms
step:1549/1770 train_time:153196ms step_avg:99.54ms
step:1550/1770 train_time:153300ms step_avg:99.55ms
step:1551/1770 train_time:153403ms step_avg:99.55ms
step:1552/1770 train_time:153510ms step_avg:99.55ms
step:1553/1770 train_time:153613ms step_avg:99.56ms
step:1554/1770 train_time:153717ms step_avg:99.56ms
step:1555/1770 train_time:153822ms step_avg:99.56ms
step:1556/1770 train_time:153926ms step_avg:99.56ms
step:1557/1770 train_time:154029ms step_avg:99.57ms
step:1558/1770 train_time:154134ms step_avg:99.57ms
step:1559/1770 train_time:154238ms step_avg:99.57ms
step:1560/1770 train_time:154342ms step_avg:99.58ms
step:1561/1770 train_time:154447ms step_avg:99.58ms
step:1562/1770 train_time:154551ms step_avg:99.58ms
step:1563/1770 train_time:154655ms step_avg:99.58ms
step:1564/1770 train_time:154759ms step_avg:99.59ms
step:1565/1770 train_time:154863ms step_avg:99.59ms
step:1566/1770 train_time:154966ms step_avg:99.59ms
step:1567/1770 train_time:155071ms step_avg:99.60ms
step:1568/1770 train_time:155175ms step_avg:99.60ms
step:1569/1770 train_time:155282ms step_avg:99.60ms
step:1570/1770 train_time:155385ms step_avg:99.61ms
step:1571/1770 train_time:155489ms step_avg:99.61ms
step:1572/1770 train_time:155594ms step_avg:99.61ms
step:1573/1770 train_time:155701ms step_avg:99.62ms
step:1574/1770 train_time:155804ms step_avg:99.62ms
step:1575/1770 train_time:155907ms step_avg:99.62ms
step:1576/1770 train_time:156011ms step_avg:99.62ms
step:1577/1770 train_time:156117ms step_avg:99.63ms
step:1578/1770 train_time:156222ms step_avg:99.63ms
step:1579/1770 train_time:156326ms step_avg:99.63ms
step:1580/1770 train_time:156430ms step_avg:99.64ms
step:1581/1770 train_time:156537ms step_avg:99.64ms
step:1582/1770 train_time:156642ms step_avg:99.64ms
step:1583/1770 train_time:156746ms step_avg:99.65ms
step:1584/1770 train_time:156850ms step_avg:99.65ms
step:1585/1770 train_time:156955ms step_avg:99.65ms
step:1586/1770 train_time:157062ms step_avg:99.66ms
step:1587/1770 train_time:157167ms step_avg:99.66ms
step:1588/1770 train_time:157271ms step_avg:99.66ms
step:1589/1770 train_time:157377ms step_avg:99.67ms
step:1590/1770 train_time:157481ms step_avg:99.67ms
step:1591/1770 train_time:157584ms step_avg:99.67ms
step:1592/1770 train_time:157689ms step_avg:99.68ms
step:1593/1770 train_time:157793ms step_avg:99.68ms
step:1594/1770 train_time:157897ms step_avg:99.68ms
step:1595/1770 train_time:158001ms step_avg:99.69ms
step:1596/1770 train_time:158107ms step_avg:99.69ms
step:1597/1770 train_time:158210ms step_avg:99.69ms
step:1598/1770 train_time:158314ms step_avg:99.69ms
step:1599/1770 train_time:158419ms step_avg:99.70ms
step:1600/1770 train_time:158526ms step_avg:99.70ms
step:1601/1770 train_time:158630ms step_avg:99.70ms
step:1602/1770 train_time:158736ms step_avg:99.71ms
step:1603/1770 train_time:158840ms step_avg:99.71ms
step:1604/1770 train_time:158942ms step_avg:99.71ms
step:1605/1770 train_time:159046ms step_avg:99.72ms
step:1606/1770 train_time:159150ms step_avg:99.72ms
step:1607/1770 train_time:159258ms step_avg:99.72ms
step:1608/1770 train_time:159362ms step_avg:99.73ms
step:1609/1770 train_time:159465ms step_avg:99.73ms
step:1610/1770 train_time:159571ms step_avg:99.73ms
step:1611/1770 train_time:159677ms step_avg:99.74ms
step:1612/1770 train_time:159783ms step_avg:99.74ms
step:1613/1770 train_time:159887ms step_avg:99.74ms
step:1614/1770 train_time:159991ms step_avg:99.75ms
step:1615/1770 train_time:160096ms step_avg:99.75ms
step:1616/1770 train_time:160199ms step_avg:99.75ms
step:1617/1770 train_time:160307ms step_avg:99.76ms
step:1618/1770 train_time:160411ms step_avg:99.76ms
step:1619/1770 train_time:160515ms step_avg:99.76ms
step:1620/1770 train_time:160619ms step_avg:99.76ms
step:1621/1770 train_time:160723ms step_avg:99.77ms
step:1622/1770 train_time:160828ms step_avg:99.77ms
step:1623/1770 train_time:160935ms step_avg:99.77ms
step:1624/1770 train_time:161038ms step_avg:99.78ms
step:1625/1770 train_time:161142ms step_avg:99.78ms
step:1625/1770 val_loss:3.3186 train_time:161245ms step_avg:99.84ms
step:1626/1770 train_time:161266ms step_avg:99.79ms
step:1627/1770 train_time:161355ms step_avg:99.79ms
step:1628/1770 train_time:161459ms step_avg:99.79ms
step:1629/1770 train_time:161563ms step_avg:99.79ms
step:1630/1770 train_time:161667ms step_avg:99.79ms
step:1631/1770 train_time:161770ms step_avg:99.80ms
step:1632/1770 train_time:161875ms step_avg:99.80ms
step:1633/1770 train_time:161979ms step_avg:99.80ms
step:1634/1770 train_time:162083ms step_avg:99.80ms
step:1635/1770 train_time:162186ms step_avg:99.81ms
step:1636/1770 train_time:162291ms step_avg:99.81ms
step:1637/1770 train_time:162396ms step_avg:99.81ms
step:1638/1770 train_time:162499ms step_avg:99.82ms
step:1639/1770 train_time:162604ms step_avg:99.82ms
step:1640/1770 train_time:162708ms step_avg:99.82ms
step:1641/1770 train_time:162811ms step_avg:99.82ms
step:1642/1770 train_time:162915ms step_avg:99.83ms
step:1643/1770 train_time:163020ms step_avg:99.83ms
step:1644/1770 train_time:163126ms step_avg:99.83ms
step:1645/1770 train_time:163229ms step_avg:99.83ms
step:1646/1770 train_time:163336ms step_avg:99.84ms
step:1647/1770 train_time:163440ms step_avg:99.84ms
step:1648/1770 train_time:163543ms step_avg:99.84ms
step:1649/1770 train_time:163647ms step_avg:99.85ms
step:1650/1770 train_time:163750ms step_avg:99.85ms
step:1651/1770 train_time:163855ms step_avg:99.85ms
step:1652/1770 train_time:163959ms step_avg:99.85ms
step:1653/1770 train_time:164063ms step_avg:99.86ms
step:1654/1770 train_time:164171ms step_avg:99.86ms
step:1655/1770 train_time:164278ms step_avg:99.87ms
step:1656/1770 train_time:164382ms step_avg:99.87ms
step:1657/1770 train_time:164487ms step_avg:99.87ms
step:1658/1770 train_time:164591ms step_avg:99.87ms
step:1659/1770 train_time:164697ms step_avg:99.88ms
step:1660/1770 train_time:164801ms step_avg:99.88ms
step:1661/1770 train_time:164906ms step_avg:99.88ms
step:1662/1770 train_time:165011ms step_avg:99.89ms
step:1663/1770 train_time:165114ms step_avg:99.89ms
step:1664/1770 train_time:165218ms step_avg:99.89ms
step:1665/1770 train_time:165322ms step_avg:99.89ms
step:1666/1770 train_time:165426ms step_avg:99.90ms
step:1667/1770 train_time:165530ms step_avg:99.90ms
step:1668/1770 train_time:165633ms step_avg:99.90ms
step:1669/1770 train_time:165737ms step_avg:99.90ms
step:1670/1770 train_time:165840ms step_avg:99.90ms
step:1671/1770 train_time:165945ms step_avg:99.91ms
step:1672/1770 train_time:166049ms step_avg:99.91ms
step:1673/1770 train_time:166155ms step_avg:99.91ms
step:1674/1770 train_time:166259ms step_avg:99.92ms
step:1675/1770 train_time:166363ms step_avg:99.92ms
step:1676/1770 train_time:166469ms step_avg:99.92ms
step:1677/1770 train_time:166578ms step_avg:99.93ms
step:1678/1770 train_time:166681ms step_avg:99.93ms
step:1679/1770 train_time:166785ms step_avg:99.93ms
step:1680/1770 train_time:166889ms step_avg:99.93ms
step:1681/1770 train_time:166995ms step_avg:99.94ms
step:1682/1770 train_time:167100ms step_avg:99.94ms
step:1683/1770 train_time:167204ms step_avg:99.94ms
step:1684/1770 train_time:167307ms step_avg:99.94ms
step:1685/1770 train_time:167412ms step_avg:99.95ms
step:1686/1770 train_time:167517ms step_avg:99.95ms
step:1687/1770 train_time:167622ms step_avg:99.95ms
step:1688/1770 train_time:167726ms step_avg:99.96ms
step:1689/1770 train_time:167830ms step_avg:99.96ms
step:1690/1770 train_time:167934ms step_avg:99.96ms
step:1691/1770 train_time:168038ms step_avg:99.96ms
step:1692/1770 train_time:168143ms step_avg:99.97ms
step:1693/1770 train_time:168248ms step_avg:99.97ms
step:1694/1770 train_time:168352ms step_avg:99.97ms
step:1695/1770 train_time:168457ms step_avg:99.97ms
step:1696/1770 train_time:168563ms step_avg:99.98ms
step:1697/1770 train_time:168669ms step_avg:99.98ms
step:1698/1770 train_time:168773ms step_avg:99.98ms
step:1699/1770 train_time:168878ms step_avg:99.99ms
step:1700/1770 train_time:168982ms step_avg:99.99ms
step:1701/1770 train_time:169085ms step_avg:99.99ms
step:1702/1770 train_time:169190ms step_avg:99.99ms
step:1703/1770 train_time:169293ms step_avg:100.00ms
step:1704/1770 train_time:169397ms step_avg:100.00ms
step:1705/1770 train_time:169501ms step_avg:100.00ms
step:1706/1770 train_time:169604ms step_avg:100.00ms
step:1707/1770 train_time:169709ms step_avg:100.01ms
step:1708/1770 train_time:169814ms step_avg:100.01ms
step:1709/1770 train_time:169919ms step_avg:100.01ms
step:1710/1770 train_time:170026ms step_avg:100.02ms
step:1711/1770 train_time:170133ms step_avg:100.02ms
step:1712/1770 train_time:170238ms step_avg:100.02ms
step:1713/1770 train_time:170343ms step_avg:100.03ms
step:1714/1770 train_time:170448ms step_avg:100.03ms
step:1715/1770 train_time:170552ms step_avg:100.03ms
step:1716/1770 train_time:170657ms step_avg:100.03ms
step:1717/1770 train_time:170761ms step_avg:100.04ms
step:1718/1770 train_time:170867ms step_avg:100.04ms
step:1719/1770 train_time:170973ms step_avg:100.04ms
step:1720/1770 train_time:171078ms step_avg:100.05ms
step:1721/1770 train_time:171183ms step_avg:100.05ms
step:1722/1770 train_time:171291ms step_avg:100.05ms
step:1723/1770 train_time:171397ms step_avg:100.06ms
step:1724/1770 train_time:171503ms step_avg:100.06ms
step:1725/1770 train_time:171610ms step_avg:100.06ms
step:1726/1770 train_time:171717ms step_avg:100.07ms
step:1727/1770 train_time:171822ms step_avg:100.07ms
step:1728/1770 train_time:171928ms step_avg:100.07ms
step:1729/1770 train_time:172033ms step_avg:100.08ms
step:1730/1770 train_time:172139ms step_avg:100.08ms
step:1731/1770 train_time:172246ms step_avg:100.09ms
step:1732/1770 train_time:172351ms step_avg:100.09ms
step:1733/1770 train_time:172458ms step_avg:100.09ms
step:1734/1770 train_time:172563ms step_avg:100.09ms
step:1735/1770 train_time:172668ms step_avg:100.10ms
step:1736/1770 train_time:172773ms step_avg:100.10ms
step:1737/1770 train_time:172879ms step_avg:100.10ms
step:1738/1770 train_time:172983ms step_avg:100.11ms
step:1739/1770 train_time:173089ms step_avg:100.11ms
step:1740/1770 train_time:173194ms step_avg:100.11ms
step:1741/1770 train_time:173301ms step_avg:100.12ms
step:1742/1770 train_time:173409ms step_avg:100.12ms
step:1743/1770 train_time:173516ms step_avg:100.12ms
step:1744/1770 train_time:173620ms step_avg:100.13ms
step:1745/1770 train_time:173725ms step_avg:100.13ms
step:1746/1770 train_time:173833ms step_avg:100.13ms
step:1747/1770 train_time:173936ms step_avg:100.14ms
step:1748/1770 train_time:174043ms step_avg:100.14ms
step:1749/1770 train_time:174150ms step_avg:100.14ms
step:1750/1770 train_time:174255ms step_avg:100.15ms
step:1750/1770 val_loss:3.2950 train_time:174358ms step_avg:100.21ms
step:1751/1770 train_time:174379ms step_avg:100.16ms
step:1752/1770 train_time:174467ms step_avg:100.15ms
step:1753/1770 train_time:174572ms step_avg:100.16ms
step:1754/1770 train_time:174678ms step_avg:100.16ms
step:1755/1770 train_time:174782ms step_avg:100.16ms
step:1756/1770 train_time:174888ms step_avg:100.16ms
step:1757/1770 train_time:174993ms step_avg:100.17ms
step:1758/1770 train_time:175098ms step_avg:100.17ms
step:1759/1770 train_time:175204ms step_avg:100.17ms
step:1760/1770 train_time:175309ms step_avg:100.18ms
step:1761/1770 train_time:175417ms step_avg:100.18ms
step:1762/1770 train_time:175525ms step_avg:100.19ms
step:1763/1770 train_time:175629ms step_avg:100.19ms
step:1764/1770 train_time:175736ms step_avg:100.19ms
step:1765/1770 train_time:175841ms step_avg:100.19ms
step:1766/1770 train_time:175950ms step_avg:100.20ms
step:1767/1770 train_time:176053ms step_avg:100.20ms
step:1768/1770 train_time:176158ms step_avg:100.20ms
step:1769/1770 train_time:176261ms step_avg:100.21ms
step:1770/1770 train_time:176366ms step_avg:100.21ms
step:1770/1770 val_loss:3.2919 train_time:176471ms step_avg:100.27ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
