import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:35:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:42510ms step_avg:nanms
step:2/1393 train_time:42928ms step_avg:nanms
step:3/1393 train_time:43050ms step_avg:nanms
step:4/1393 train_time:43170ms step_avg:nanms
step:5/1393 train_time:43291ms step_avg:nanms
step:6/1393 train_time:43412ms step_avg:nanms
step:7/1393 train_time:43533ms step_avg:nanms
step:8/1393 train_time:43654ms step_avg:nanms
step:9/1393 train_time:43775ms step_avg:nanms
step:10/1393 train_time:43897ms step_avg:nanms
step:11/1393 train_time:122ms step_avg:nanms
step:12/1393 train_time:244ms step_avg:nanms
step:13/1393 train_time:367ms step_avg:122.34ms
step:14/1393 train_time:489ms step_avg:122.21ms
step:15/1393 train_time:610ms step_avg:122.07ms
step:16/1393 train_time:731ms step_avg:121.86ms
step:17/1393 train_time:854ms step_avg:121.96ms
step:18/1393 train_time:975ms step_avg:121.88ms
step:19/1393 train_time:1097ms step_avg:121.86ms
step:20/1393 train_time:1219ms step_avg:121.87ms
step:21/1393 train_time:1340ms step_avg:121.77ms
step:22/1393 train_time:1463ms step_avg:121.88ms
step:23/1393 train_time:1585ms step_avg:121.92ms
step:24/1393 train_time:1707ms step_avg:121.94ms
step:25/1393 train_time:1829ms step_avg:121.95ms
step:26/1393 train_time:1951ms step_avg:121.93ms
step:27/1393 train_time:2073ms step_avg:121.96ms
step:28/1393 train_time:2196ms step_avg:121.99ms
step:29/1393 train_time:2318ms step_avg:121.98ms
step:30/1393 train_time:2441ms step_avg:122.03ms
step:31/1393 train_time:2562ms step_avg:122.00ms
step:32/1393 train_time:2684ms step_avg:122.01ms
step:33/1393 train_time:2805ms step_avg:121.98ms
step:34/1393 train_time:2927ms step_avg:121.95ms
step:35/1393 train_time:3048ms step_avg:121.93ms
step:36/1393 train_time:3171ms step_avg:121.96ms
step:37/1393 train_time:3293ms step_avg:121.97ms
step:38/1393 train_time:3415ms step_avg:121.97ms
step:39/1393 train_time:3537ms step_avg:121.98ms
step:40/1393 train_time:3659ms step_avg:121.97ms
step:41/1393 train_time:3781ms step_avg:121.98ms
step:42/1393 train_time:3904ms step_avg:122.00ms
step:43/1393 train_time:4026ms step_avg:122.00ms
step:44/1393 train_time:4149ms step_avg:122.02ms
step:45/1393 train_time:4272ms step_avg:122.05ms
step:46/1393 train_time:4393ms step_avg:122.04ms
step:47/1393 train_time:4517ms step_avg:122.08ms
step:48/1393 train_time:4638ms step_avg:122.06ms
step:49/1393 train_time:4760ms step_avg:122.04ms
step:50/1393 train_time:4881ms step_avg:122.03ms
step:51/1393 train_time:5003ms step_avg:122.03ms
step:52/1393 train_time:5127ms step_avg:122.07ms
step:53/1393 train_time:5250ms step_avg:122.10ms
step:54/1393 train_time:5373ms step_avg:122.11ms
step:55/1393 train_time:5496ms step_avg:122.13ms
step:56/1393 train_time:5616ms step_avg:122.09ms
step:57/1393 train_time:5738ms step_avg:122.08ms
step:58/1393 train_time:5860ms step_avg:122.08ms
step:59/1393 train_time:5981ms step_avg:122.06ms
step:60/1393 train_time:6105ms step_avg:122.09ms
step:61/1393 train_time:6228ms step_avg:122.12ms
step:62/1393 train_time:6350ms step_avg:122.11ms
step:63/1393 train_time:6470ms step_avg:122.08ms
step:64/1393 train_time:6594ms step_avg:122.10ms
step:65/1393 train_time:6716ms step_avg:122.10ms
step:66/1393 train_time:6837ms step_avg:122.10ms
step:67/1393 train_time:6959ms step_avg:122.09ms
step:68/1393 train_time:7081ms step_avg:122.08ms
step:69/1393 train_time:7203ms step_avg:122.09ms
step:70/1393 train_time:7325ms step_avg:122.09ms
step:71/1393 train_time:7447ms step_avg:122.08ms
step:72/1393 train_time:7569ms step_avg:122.07ms
step:73/1393 train_time:7691ms step_avg:122.07ms
step:74/1393 train_time:7812ms step_avg:122.07ms
step:75/1393 train_time:7934ms step_avg:122.06ms
step:76/1393 train_time:8056ms step_avg:122.06ms
step:77/1393 train_time:8178ms step_avg:122.05ms
step:78/1393 train_time:8300ms step_avg:122.05ms
step:79/1393 train_time:8422ms step_avg:122.06ms
step:80/1393 train_time:8543ms step_avg:122.04ms
step:81/1393 train_time:8665ms step_avg:122.04ms
step:82/1393 train_time:8787ms step_avg:122.04ms
step:83/1393 train_time:8910ms step_avg:122.06ms
step:84/1393 train_time:9032ms step_avg:122.06ms
step:85/1393 train_time:9154ms step_avg:122.05ms
step:86/1393 train_time:9276ms step_avg:122.06ms
step:87/1393 train_time:9399ms step_avg:122.06ms
step:88/1393 train_time:9520ms step_avg:122.06ms
step:89/1393 train_time:9641ms step_avg:122.04ms
step:90/1393 train_time:9764ms step_avg:122.05ms
step:91/1393 train_time:9886ms step_avg:122.05ms
step:92/1393 train_time:10008ms step_avg:122.05ms
step:93/1393 train_time:10131ms step_avg:122.06ms
step:94/1393 train_time:10257ms step_avg:122.10ms
step:95/1393 train_time:10378ms step_avg:122.10ms
step:96/1393 train_time:10500ms step_avg:122.09ms
step:97/1393 train_time:10621ms step_avg:122.08ms
step:98/1393 train_time:10743ms step_avg:122.08ms
step:99/1393 train_time:10865ms step_avg:122.08ms
step:100/1393 train_time:10987ms step_avg:122.08ms
step:101/1393 train_time:11110ms step_avg:122.08ms
step:102/1393 train_time:11232ms step_avg:122.09ms
step:103/1393 train_time:11352ms step_avg:122.07ms
step:104/1393 train_time:11474ms step_avg:122.07ms
step:105/1393 train_time:11597ms step_avg:122.07ms
step:106/1393 train_time:11719ms step_avg:122.07ms
step:107/1393 train_time:11842ms step_avg:122.08ms
step:108/1393 train_time:11965ms step_avg:122.09ms
step:109/1393 train_time:12088ms step_avg:122.11ms
step:110/1393 train_time:12211ms step_avg:122.11ms
step:111/1393 train_time:12333ms step_avg:122.11ms
step:112/1393 train_time:12457ms step_avg:122.12ms
step:113/1393 train_time:12580ms step_avg:122.13ms
step:114/1393 train_time:12702ms step_avg:122.13ms
step:115/1393 train_time:12825ms step_avg:122.14ms
step:116/1393 train_time:12948ms step_avg:122.15ms
step:117/1393 train_time:13071ms step_avg:122.16ms
step:118/1393 train_time:13194ms step_avg:122.17ms
step:119/1393 train_time:13316ms step_avg:122.17ms
step:120/1393 train_time:13439ms step_avg:122.17ms
step:121/1393 train_time:13561ms step_avg:122.17ms
step:122/1393 train_time:13683ms step_avg:122.17ms
step:123/1393 train_time:13805ms step_avg:122.17ms
step:124/1393 train_time:13928ms step_avg:122.18ms
step:125/1393 train_time:14051ms step_avg:122.18ms
step:125/1393 val_loss:4.3997 train_time:14173ms step_avg:123.25ms
step:126/1393 train_time:14199ms step_avg:122.40ms
step:127/1393 train_time:14302ms step_avg:122.24ms
step:128/1393 train_time:14432ms step_avg:122.31ms
step:129/1393 train_time:14558ms step_avg:122.34ms
step:130/1393 train_time:14680ms step_avg:122.34ms
step:131/1393 train_time:14802ms step_avg:122.33ms
step:132/1393 train_time:14924ms step_avg:122.33ms
step:133/1393 train_time:15047ms step_avg:122.33ms
step:134/1393 train_time:15168ms step_avg:122.32ms
step:135/1393 train_time:15291ms step_avg:122.33ms
step:136/1393 train_time:15413ms step_avg:122.33ms
step:137/1393 train_time:15537ms step_avg:122.34ms
step:138/1393 train_time:15660ms step_avg:122.34ms
step:139/1393 train_time:15782ms step_avg:122.34ms
step:140/1393 train_time:15904ms step_avg:122.34ms
step:141/1393 train_time:16027ms step_avg:122.34ms
step:142/1393 train_time:16150ms step_avg:122.35ms
step:143/1393 train_time:16275ms step_avg:122.37ms
step:144/1393 train_time:16397ms step_avg:122.37ms
step:145/1393 train_time:16520ms step_avg:122.37ms
step:146/1393 train_time:16643ms step_avg:122.37ms
step:147/1393 train_time:16765ms step_avg:122.37ms
step:148/1393 train_time:16888ms step_avg:122.38ms
step:149/1393 train_time:17011ms step_avg:122.38ms
step:150/1393 train_time:17133ms step_avg:122.38ms
step:151/1393 train_time:17256ms step_avg:122.38ms
step:152/1393 train_time:17378ms step_avg:122.38ms
step:153/1393 train_time:17502ms step_avg:122.39ms
step:154/1393 train_time:17625ms step_avg:122.39ms
step:155/1393 train_time:17748ms step_avg:122.40ms
step:156/1393 train_time:17870ms step_avg:122.40ms
step:157/1393 train_time:17994ms step_avg:122.41ms
step:158/1393 train_time:18117ms step_avg:122.41ms
step:159/1393 train_time:18239ms step_avg:122.41ms
step:160/1393 train_time:18362ms step_avg:122.41ms
step:161/1393 train_time:18485ms step_avg:122.42ms
step:162/1393 train_time:18609ms step_avg:122.43ms
step:163/1393 train_time:18731ms step_avg:122.43ms
step:164/1393 train_time:18854ms step_avg:122.43ms
step:165/1393 train_time:18977ms step_avg:122.43ms
step:166/1393 train_time:19100ms step_avg:122.43ms
step:167/1393 train_time:19223ms step_avg:122.44ms
step:168/1393 train_time:19346ms step_avg:122.44ms
step:169/1393 train_time:19468ms step_avg:122.44ms
step:170/1393 train_time:19590ms step_avg:122.44ms
step:171/1393 train_time:19714ms step_avg:122.45ms
step:172/1393 train_time:19838ms step_avg:122.45ms
step:173/1393 train_time:19960ms step_avg:122.46ms
step:174/1393 train_time:20083ms step_avg:122.46ms
step:175/1393 train_time:20206ms step_avg:122.46ms
step:176/1393 train_time:20329ms step_avg:122.47ms
step:177/1393 train_time:20453ms step_avg:122.47ms
step:178/1393 train_time:20576ms step_avg:122.48ms
step:179/1393 train_time:20699ms step_avg:122.48ms
step:180/1393 train_time:20822ms step_avg:122.48ms
step:181/1393 train_time:20945ms step_avg:122.49ms
step:182/1393 train_time:21068ms step_avg:122.49ms
step:183/1393 train_time:21192ms step_avg:122.49ms
step:184/1393 train_time:21314ms step_avg:122.50ms
step:185/1393 train_time:21437ms step_avg:122.50ms
step:186/1393 train_time:21560ms step_avg:122.50ms
step:187/1393 train_time:21683ms step_avg:122.50ms
step:188/1393 train_time:21805ms step_avg:122.50ms
step:189/1393 train_time:21928ms step_avg:122.50ms
step:190/1393 train_time:22052ms step_avg:122.51ms
step:191/1393 train_time:22176ms step_avg:122.52ms
step:192/1393 train_time:22299ms step_avg:122.52ms
step:193/1393 train_time:22422ms step_avg:122.53ms
step:194/1393 train_time:22546ms step_avg:122.53ms
step:195/1393 train_time:22669ms step_avg:122.54ms
step:196/1393 train_time:22791ms step_avg:122.53ms
step:197/1393 train_time:22915ms step_avg:122.54ms
step:198/1393 train_time:23038ms step_avg:122.54ms
step:199/1393 train_time:23161ms step_avg:122.54ms
step:200/1393 train_time:23283ms step_avg:122.54ms
step:201/1393 train_time:23406ms step_avg:122.54ms
step:202/1393 train_time:23529ms step_avg:122.55ms
step:203/1393 train_time:23652ms step_avg:122.55ms
step:204/1393 train_time:23775ms step_avg:122.55ms
step:205/1393 train_time:23899ms step_avg:122.56ms
step:206/1393 train_time:24021ms step_avg:122.56ms
step:207/1393 train_time:24146ms step_avg:122.57ms
step:208/1393 train_time:24270ms step_avg:122.58ms
step:209/1393 train_time:24394ms step_avg:122.58ms
step:210/1393 train_time:24517ms step_avg:122.58ms
step:211/1393 train_time:24641ms step_avg:122.59ms
step:212/1393 train_time:24765ms step_avg:122.60ms
step:213/1393 train_time:24888ms step_avg:122.60ms
step:214/1393 train_time:25011ms step_avg:122.60ms
step:215/1393 train_time:25135ms step_avg:122.61ms
step:216/1393 train_time:25259ms step_avg:122.62ms
step:217/1393 train_time:25384ms step_avg:122.63ms
step:218/1393 train_time:25507ms step_avg:122.63ms
step:219/1393 train_time:25632ms step_avg:122.64ms
step:220/1393 train_time:25755ms step_avg:122.64ms
step:221/1393 train_time:25878ms step_avg:122.65ms
step:222/1393 train_time:26001ms step_avg:122.65ms
step:223/1393 train_time:26124ms step_avg:122.65ms
step:224/1393 train_time:26248ms step_avg:122.65ms
step:225/1393 train_time:26372ms step_avg:122.66ms
step:226/1393 train_time:26496ms step_avg:122.67ms
step:227/1393 train_time:26619ms step_avg:122.67ms
step:228/1393 train_time:26742ms step_avg:122.67ms
step:229/1393 train_time:26866ms step_avg:122.68ms
step:230/1393 train_time:26990ms step_avg:122.68ms
step:231/1393 train_time:27115ms step_avg:122.69ms
step:232/1393 train_time:27238ms step_avg:122.69ms
step:233/1393 train_time:27360ms step_avg:122.69ms
step:234/1393 train_time:27485ms step_avg:122.70ms
step:235/1393 train_time:27608ms step_avg:122.70ms
step:236/1393 train_time:27732ms step_avg:122.71ms
step:237/1393 train_time:27856ms step_avg:122.72ms
step:238/1393 train_time:27979ms step_avg:122.72ms
step:239/1393 train_time:28102ms step_avg:122.72ms
step:240/1393 train_time:28226ms step_avg:122.72ms
step:241/1393 train_time:28349ms step_avg:122.72ms
step:242/1393 train_time:28473ms step_avg:122.73ms
step:243/1393 train_time:28598ms step_avg:122.74ms
step:244/1393 train_time:28720ms step_avg:122.74ms
step:245/1393 train_time:28843ms step_avg:122.74ms
step:246/1393 train_time:28967ms step_avg:122.74ms
step:247/1393 train_time:29090ms step_avg:122.74ms
step:248/1393 train_time:29214ms step_avg:122.75ms
step:249/1393 train_time:29338ms step_avg:122.75ms
step:250/1393 train_time:29460ms step_avg:122.75ms
step:250/1393 val_loss:3.9784 train_time:29583ms step_avg:123.26ms
step:251/1393 train_time:29605ms step_avg:122.84ms
step:252/1393 train_time:29720ms step_avg:122.81ms
step:253/1393 train_time:29845ms step_avg:122.82ms
step:254/1393 train_time:29968ms step_avg:122.82ms
step:255/1393 train_time:30090ms step_avg:122.82ms
step:256/1393 train_time:30214ms step_avg:122.82ms
step:257/1393 train_time:30337ms step_avg:122.82ms
step:258/1393 train_time:30459ms step_avg:122.82ms
step:259/1393 train_time:30583ms step_avg:122.82ms
step:260/1393 train_time:30707ms step_avg:122.83ms
step:261/1393 train_time:30831ms step_avg:122.83ms
step:262/1393 train_time:30956ms step_avg:122.84ms
step:263/1393 train_time:31079ms step_avg:122.84ms
step:264/1393 train_time:31202ms step_avg:122.84ms
step:265/1393 train_time:31325ms step_avg:122.84ms
step:266/1393 train_time:31448ms step_avg:122.84ms
step:267/1393 train_time:31571ms step_avg:122.85ms
step:268/1393 train_time:31696ms step_avg:122.85ms
step:269/1393 train_time:31820ms step_avg:122.86ms
step:270/1393 train_time:31944ms step_avg:122.86ms
step:271/1393 train_time:32067ms step_avg:122.86ms
step:272/1393 train_time:32190ms step_avg:122.86ms
step:273/1393 train_time:32314ms step_avg:122.87ms
step:274/1393 train_time:32437ms step_avg:122.87ms
step:275/1393 train_time:32560ms step_avg:122.87ms
step:276/1393 train_time:32683ms step_avg:122.87ms
step:277/1393 train_time:32806ms step_avg:122.87ms
step:278/1393 train_time:32930ms step_avg:122.87ms
step:279/1393 train_time:33054ms step_avg:122.88ms
step:280/1393 train_time:33177ms step_avg:122.88ms
step:281/1393 train_time:33299ms step_avg:122.87ms
step:282/1393 train_time:33421ms step_avg:122.87ms
step:283/1393 train_time:33545ms step_avg:122.87ms
step:284/1393 train_time:33668ms step_avg:122.88ms
step:285/1393 train_time:33792ms step_avg:122.88ms
step:286/1393 train_time:33915ms step_avg:122.88ms
step:287/1393 train_time:34039ms step_avg:122.89ms
step:288/1393 train_time:34163ms step_avg:122.89ms
step:289/1393 train_time:34287ms step_avg:122.89ms
step:290/1393 train_time:34410ms step_avg:122.89ms
step:291/1393 train_time:34534ms step_avg:122.90ms
step:292/1393 train_time:34659ms step_avg:122.90ms
step:293/1393 train_time:34781ms step_avg:122.90ms
step:294/1393 train_time:34905ms step_avg:122.90ms
step:295/1393 train_time:35028ms step_avg:122.90ms
step:296/1393 train_time:35151ms step_avg:122.91ms
step:297/1393 train_time:35275ms step_avg:122.91ms
step:298/1393 train_time:35398ms step_avg:122.91ms
step:299/1393 train_time:35522ms step_avg:122.91ms
step:300/1393 train_time:35646ms step_avg:122.92ms
step:301/1393 train_time:35770ms step_avg:122.92ms
step:302/1393 train_time:35893ms step_avg:122.92ms
step:303/1393 train_time:36016ms step_avg:122.92ms
step:304/1393 train_time:36140ms step_avg:122.93ms
step:305/1393 train_time:36264ms step_avg:122.93ms
step:306/1393 train_time:36387ms step_avg:122.93ms
step:307/1393 train_time:36512ms step_avg:122.94ms
step:308/1393 train_time:36636ms step_avg:122.94ms
step:309/1393 train_time:36759ms step_avg:122.94ms
step:310/1393 train_time:36883ms step_avg:122.94ms
step:311/1393 train_time:37006ms step_avg:122.94ms
step:312/1393 train_time:37132ms step_avg:122.95ms
step:313/1393 train_time:37259ms step_avg:122.97ms
step:314/1393 train_time:37386ms step_avg:122.98ms
step:315/1393 train_time:37511ms step_avg:122.99ms
step:316/1393 train_time:37638ms step_avg:123.00ms
step:317/1393 train_time:37764ms step_avg:123.01ms
step:318/1393 train_time:37890ms step_avg:123.02ms
step:319/1393 train_time:38015ms step_avg:123.03ms
step:320/1393 train_time:38140ms step_avg:123.03ms
step:321/1393 train_time:38266ms step_avg:123.04ms
step:322/1393 train_time:38391ms step_avg:123.05ms
step:323/1393 train_time:38518ms step_avg:123.06ms
step:324/1393 train_time:38644ms step_avg:123.07ms
step:325/1393 train_time:38770ms step_avg:123.08ms
step:326/1393 train_time:38897ms step_avg:123.09ms
step:327/1393 train_time:39023ms step_avg:123.10ms
step:328/1393 train_time:39149ms step_avg:123.11ms
step:329/1393 train_time:39275ms step_avg:123.12ms
step:330/1393 train_time:39400ms step_avg:123.13ms
step:331/1393 train_time:39526ms step_avg:123.13ms
step:332/1393 train_time:39652ms step_avg:123.14ms
step:333/1393 train_time:39778ms step_avg:123.15ms
step:334/1393 train_time:39905ms step_avg:123.16ms
step:335/1393 train_time:40031ms step_avg:123.17ms
step:336/1393 train_time:40156ms step_avg:123.18ms
step:337/1393 train_time:40283ms step_avg:123.19ms
step:338/1393 train_time:40408ms step_avg:123.20ms
step:339/1393 train_time:40534ms step_avg:123.20ms
step:340/1393 train_time:40660ms step_avg:123.21ms
step:341/1393 train_time:40786ms step_avg:123.22ms
step:342/1393 train_time:40911ms step_avg:123.23ms
step:343/1393 train_time:41038ms step_avg:123.24ms
step:344/1393 train_time:41164ms step_avg:123.24ms
step:345/1393 train_time:41290ms step_avg:123.25ms
step:346/1393 train_time:41415ms step_avg:123.26ms
step:347/1393 train_time:41540ms step_avg:123.26ms
step:348/1393 train_time:41665ms step_avg:123.27ms
step:349/1393 train_time:41791ms step_avg:123.28ms
step:350/1393 train_time:41917ms step_avg:123.29ms
step:351/1393 train_time:42044ms step_avg:123.30ms
step:352/1393 train_time:42170ms step_avg:123.30ms
step:353/1393 train_time:42296ms step_avg:123.31ms
step:354/1393 train_time:42422ms step_avg:123.32ms
step:355/1393 train_time:42548ms step_avg:123.33ms
step:356/1393 train_time:42674ms step_avg:123.33ms
step:357/1393 train_time:42798ms step_avg:123.34ms
step:358/1393 train_time:42924ms step_avg:123.35ms
step:359/1393 train_time:43050ms step_avg:123.35ms
step:360/1393 train_time:43177ms step_avg:123.36ms
step:361/1393 train_time:43302ms step_avg:123.37ms
step:362/1393 train_time:43429ms step_avg:123.38ms
step:363/1393 train_time:43554ms step_avg:123.38ms
step:364/1393 train_time:43681ms step_avg:123.39ms
step:365/1393 train_time:43807ms step_avg:123.40ms
step:366/1393 train_time:43933ms step_avg:123.41ms
step:367/1393 train_time:44058ms step_avg:123.41ms
step:368/1393 train_time:44185ms step_avg:123.42ms
step:369/1393 train_time:44311ms step_avg:123.43ms
step:370/1393 train_time:44437ms step_avg:123.44ms
step:371/1393 train_time:44563ms step_avg:123.44ms
step:372/1393 train_time:44689ms step_avg:123.45ms
step:373/1393 train_time:44815ms step_avg:123.46ms
step:374/1393 train_time:44941ms step_avg:123.46ms
step:375/1393 train_time:45067ms step_avg:123.47ms
step:375/1393 val_loss:3.7791 train_time:45192ms step_avg:123.81ms
step:376/1393 train_time:45213ms step_avg:123.53ms
step:377/1393 train_time:45329ms step_avg:123.51ms
step:378/1393 train_time:45456ms step_avg:123.52ms
step:379/1393 train_time:45581ms step_avg:123.53ms
step:380/1393 train_time:45707ms step_avg:123.53ms
step:381/1393 train_time:45832ms step_avg:123.54ms
step:382/1393 train_time:45957ms step_avg:123.54ms
step:383/1393 train_time:46083ms step_avg:123.55ms
step:384/1393 train_time:46208ms step_avg:123.55ms
step:385/1393 train_time:46335ms step_avg:123.56ms
step:386/1393 train_time:46463ms step_avg:123.57ms
step:387/1393 train_time:46589ms step_avg:123.58ms
step:388/1393 train_time:46715ms step_avg:123.59ms
step:389/1393 train_time:46841ms step_avg:123.59ms
step:390/1393 train_time:46966ms step_avg:123.59ms
step:391/1393 train_time:47092ms step_avg:123.60ms
step:392/1393 train_time:47216ms step_avg:123.60ms
step:393/1393 train_time:47342ms step_avg:123.61ms
step:394/1393 train_time:47467ms step_avg:123.61ms
step:395/1393 train_time:47595ms step_avg:123.62ms
step:396/1393 train_time:47720ms step_avg:123.63ms
step:397/1393 train_time:47847ms step_avg:123.64ms
step:398/1393 train_time:47973ms step_avg:123.64ms
step:399/1393 train_time:48099ms step_avg:123.65ms
step:400/1393 train_time:48224ms step_avg:123.65ms
step:401/1393 train_time:48350ms step_avg:123.66ms
step:402/1393 train_time:48476ms step_avg:123.66ms
step:403/1393 train_time:48601ms step_avg:123.67ms
step:404/1393 train_time:48728ms step_avg:123.67ms
step:405/1393 train_time:48854ms step_avg:123.68ms
step:406/1393 train_time:48979ms step_avg:123.68ms
step:407/1393 train_time:49104ms step_avg:123.69ms
step:408/1393 train_time:49230ms step_avg:123.69ms
step:409/1393 train_time:49356ms step_avg:123.70ms
step:410/1393 train_time:49482ms step_avg:123.70ms
step:411/1393 train_time:49608ms step_avg:123.71ms
step:412/1393 train_time:49734ms step_avg:123.72ms
step:413/1393 train_time:49860ms step_avg:123.72ms
step:414/1393 train_time:49987ms step_avg:123.73ms
step:415/1393 train_time:50113ms step_avg:123.74ms
step:416/1393 train_time:50240ms step_avg:123.74ms
step:417/1393 train_time:50366ms step_avg:123.75ms
step:418/1393 train_time:50492ms step_avg:123.75ms
step:419/1393 train_time:50618ms step_avg:123.76ms
step:420/1393 train_time:50744ms step_avg:123.77ms
step:421/1393 train_time:50871ms step_avg:123.77ms
step:422/1393 train_time:50997ms step_avg:123.78ms
step:423/1393 train_time:51123ms step_avg:123.78ms
step:424/1393 train_time:51250ms step_avg:123.79ms
step:425/1393 train_time:51377ms step_avg:123.80ms
step:426/1393 train_time:51503ms step_avg:123.80ms
step:427/1393 train_time:51629ms step_avg:123.81ms
step:428/1393 train_time:51755ms step_avg:123.81ms
step:429/1393 train_time:51881ms step_avg:123.82ms
step:430/1393 train_time:52007ms step_avg:123.83ms
step:431/1393 train_time:52134ms step_avg:123.83ms
step:432/1393 train_time:52260ms step_avg:123.84ms
step:433/1393 train_time:52385ms step_avg:123.84ms
step:434/1393 train_time:52511ms step_avg:123.85ms
step:435/1393 train_time:52638ms step_avg:123.85ms
step:436/1393 train_time:52764ms step_avg:123.86ms
step:437/1393 train_time:52892ms step_avg:123.87ms
step:438/1393 train_time:53018ms step_avg:123.87ms
step:439/1393 train_time:53144ms step_avg:123.88ms
step:440/1393 train_time:53270ms step_avg:123.88ms
step:441/1393 train_time:53396ms step_avg:123.89ms
step:442/1393 train_time:53522ms step_avg:123.89ms
step:443/1393 train_time:53649ms step_avg:123.90ms
step:444/1393 train_time:53775ms step_avg:123.90ms
step:445/1393 train_time:53902ms step_avg:123.91ms
step:446/1393 train_time:54030ms step_avg:123.92ms
step:447/1393 train_time:54157ms step_avg:123.93ms
step:448/1393 train_time:54284ms step_avg:123.94ms
step:449/1393 train_time:54410ms step_avg:123.94ms
step:450/1393 train_time:54537ms step_avg:123.95ms
step:451/1393 train_time:54663ms step_avg:123.95ms
step:452/1393 train_time:54789ms step_avg:123.96ms
step:453/1393 train_time:54915ms step_avg:123.96ms
step:454/1393 train_time:55042ms step_avg:123.97ms
step:455/1393 train_time:55169ms step_avg:123.98ms
step:456/1393 train_time:55296ms step_avg:123.98ms
step:457/1393 train_time:55422ms step_avg:123.99ms
step:458/1393 train_time:55548ms step_avg:123.99ms
step:459/1393 train_time:55674ms step_avg:124.00ms
step:460/1393 train_time:55801ms step_avg:124.00ms
step:461/1393 train_time:55927ms step_avg:124.01ms
step:462/1393 train_time:56053ms step_avg:124.01ms
step:463/1393 train_time:56180ms step_avg:124.02ms
step:464/1393 train_time:56307ms step_avg:124.03ms
step:465/1393 train_time:56434ms step_avg:124.03ms
step:466/1393 train_time:56561ms step_avg:124.04ms
step:467/1393 train_time:56686ms step_avg:124.04ms
step:468/1393 train_time:56812ms step_avg:124.04ms
step:469/1393 train_time:56938ms step_avg:124.05ms
step:470/1393 train_time:57065ms step_avg:124.05ms
step:471/1393 train_time:57192ms step_avg:124.06ms
step:472/1393 train_time:57318ms step_avg:124.06ms
step:473/1393 train_time:57443ms step_avg:124.07ms
step:474/1393 train_time:57570ms step_avg:124.07ms
step:475/1393 train_time:57697ms step_avg:124.08ms
step:476/1393 train_time:57823ms step_avg:124.08ms
step:477/1393 train_time:57950ms step_avg:124.09ms
step:478/1393 train_time:58076ms step_avg:124.09ms
step:479/1393 train_time:58201ms step_avg:124.10ms
step:480/1393 train_time:58327ms step_avg:124.10ms
step:481/1393 train_time:58454ms step_avg:124.11ms
step:482/1393 train_time:58579ms step_avg:124.11ms
step:483/1393 train_time:58706ms step_avg:124.11ms
step:484/1393 train_time:58832ms step_avg:124.12ms
step:485/1393 train_time:58958ms step_avg:124.12ms
step:486/1393 train_time:59085ms step_avg:124.13ms
step:487/1393 train_time:59211ms step_avg:124.13ms
step:488/1393 train_time:59337ms step_avg:124.14ms
step:489/1393 train_time:59464ms step_avg:124.14ms
step:490/1393 train_time:59591ms step_avg:124.15ms
step:491/1393 train_time:59717ms step_avg:124.15ms
step:492/1393 train_time:59842ms step_avg:124.15ms
step:493/1393 train_time:59968ms step_avg:124.16ms
step:494/1393 train_time:60094ms step_avg:124.16ms
step:495/1393 train_time:60221ms step_avg:124.17ms
step:496/1393 train_time:60348ms step_avg:124.17ms
step:497/1393 train_time:60474ms step_avg:124.18ms
step:498/1393 train_time:60601ms step_avg:124.18ms
step:499/1393 train_time:60727ms step_avg:124.19ms
step:500/1393 train_time:60854ms step_avg:124.19ms
step:500/1393 val_loss:3.6598 train_time:60979ms step_avg:124.45ms
step:501/1393 train_time:61001ms step_avg:124.24ms
step:502/1393 train_time:61116ms step_avg:124.22ms
step:503/1393 train_time:61244ms step_avg:124.23ms
step:504/1393 train_time:61370ms step_avg:124.23ms
step:505/1393 train_time:61496ms step_avg:124.23ms
step:506/1393 train_time:61621ms step_avg:124.24ms
step:507/1393 train_time:61748ms step_avg:124.24ms
step:508/1393 train_time:61873ms step_avg:124.24ms
step:509/1393 train_time:62000ms step_avg:124.25ms
step:510/1393 train_time:62128ms step_avg:124.26ms
step:511/1393 train_time:62254ms step_avg:124.26ms
step:512/1393 train_time:62381ms step_avg:124.26ms
step:513/1393 train_time:62507ms step_avg:124.27ms
step:514/1393 train_time:62633ms step_avg:124.27ms
step:515/1393 train_time:62760ms step_avg:124.28ms
step:516/1393 train_time:62886ms step_avg:124.28ms
step:517/1393 train_time:63012ms step_avg:124.28ms
step:518/1393 train_time:63141ms step_avg:124.29ms
step:519/1393 train_time:63270ms step_avg:124.30ms
step:520/1393 train_time:63398ms step_avg:124.31ms
step:521/1393 train_time:63527ms step_avg:124.32ms
step:522/1393 train_time:63655ms step_avg:124.33ms
step:523/1393 train_time:63782ms step_avg:124.33ms
step:524/1393 train_time:63912ms step_avg:124.34ms
step:525/1393 train_time:64040ms step_avg:124.35ms
step:526/1393 train_time:64168ms step_avg:124.36ms
step:527/1393 train_time:64296ms step_avg:124.36ms
step:528/1393 train_time:64425ms step_avg:124.37ms
step:529/1393 train_time:64553ms step_avg:124.38ms
step:530/1393 train_time:64682ms step_avg:124.39ms
step:531/1393 train_time:64811ms step_avg:124.40ms
step:532/1393 train_time:64941ms step_avg:124.41ms
step:533/1393 train_time:65069ms step_avg:124.42ms
step:534/1393 train_time:65198ms step_avg:124.42ms
step:535/1393 train_time:65326ms step_avg:124.43ms
step:536/1393 train_time:65455ms step_avg:124.44ms
step:537/1393 train_time:65583ms step_avg:124.45ms
step:538/1393 train_time:65713ms step_avg:124.46ms
step:539/1393 train_time:65842ms step_avg:124.46ms
step:540/1393 train_time:65971ms step_avg:124.47ms
step:541/1393 train_time:66100ms step_avg:124.48ms
step:542/1393 train_time:66229ms step_avg:124.49ms
step:543/1393 train_time:66357ms step_avg:124.50ms
step:544/1393 train_time:66485ms step_avg:124.50ms
step:545/1393 train_time:66614ms step_avg:124.51ms
step:546/1393 train_time:66742ms step_avg:124.52ms
step:547/1393 train_time:66872ms step_avg:124.53ms
step:548/1393 train_time:66999ms step_avg:124.53ms
step:549/1393 train_time:67128ms step_avg:124.54ms
step:550/1393 train_time:67256ms step_avg:124.55ms
step:551/1393 train_time:67385ms step_avg:124.56ms
step:552/1393 train_time:67513ms step_avg:124.56ms
step:553/1393 train_time:67642ms step_avg:124.57ms
step:554/1393 train_time:67770ms step_avg:124.58ms
step:555/1393 train_time:67899ms step_avg:124.59ms
step:556/1393 train_time:68028ms step_avg:124.59ms
step:557/1393 train_time:68156ms step_avg:124.60ms
step:558/1393 train_time:68284ms step_avg:124.61ms
step:559/1393 train_time:68412ms step_avg:124.61ms
step:560/1393 train_time:68541ms step_avg:124.62ms
step:561/1393 train_time:68669ms step_avg:124.63ms
step:562/1393 train_time:68798ms step_avg:124.63ms
step:563/1393 train_time:68926ms step_avg:124.64ms
step:564/1393 train_time:69054ms step_avg:124.65ms
step:565/1393 train_time:69183ms step_avg:124.65ms
step:566/1393 train_time:69312ms step_avg:124.66ms
step:567/1393 train_time:69440ms step_avg:124.67ms
step:568/1393 train_time:69569ms step_avg:124.68ms
step:569/1393 train_time:69697ms step_avg:124.68ms
step:570/1393 train_time:69825ms step_avg:124.69ms
step:571/1393 train_time:69954ms step_avg:124.70ms
step:572/1393 train_time:70083ms step_avg:124.70ms
step:573/1393 train_time:70212ms step_avg:124.71ms
step:574/1393 train_time:70341ms step_avg:124.72ms
step:575/1393 train_time:70469ms step_avg:124.72ms
step:576/1393 train_time:70597ms step_avg:124.73ms
step:577/1393 train_time:70726ms step_avg:124.74ms
step:578/1393 train_time:70855ms step_avg:124.74ms
step:579/1393 train_time:70983ms step_avg:124.75ms
step:580/1393 train_time:71111ms step_avg:124.76ms
step:581/1393 train_time:71239ms step_avg:124.76ms
step:582/1393 train_time:71368ms step_avg:124.77ms
step:583/1393 train_time:71496ms step_avg:124.78ms
step:584/1393 train_time:71624ms step_avg:124.78ms
step:585/1393 train_time:71752ms step_avg:124.79ms
step:586/1393 train_time:71881ms step_avg:124.79ms
step:587/1393 train_time:72010ms step_avg:124.80ms
step:588/1393 train_time:72139ms step_avg:124.81ms
step:589/1393 train_time:72267ms step_avg:124.81ms
step:590/1393 train_time:72396ms step_avg:124.82ms
step:591/1393 train_time:72524ms step_avg:124.83ms
step:592/1393 train_time:72652ms step_avg:124.83ms
step:593/1393 train_time:72781ms step_avg:124.84ms
step:594/1393 train_time:72909ms step_avg:124.85ms
step:595/1393 train_time:73039ms step_avg:124.85ms
step:596/1393 train_time:73167ms step_avg:124.86ms
step:597/1393 train_time:73296ms step_avg:124.86ms
step:598/1393 train_time:73424ms step_avg:124.87ms
step:599/1393 train_time:73552ms step_avg:124.88ms
step:600/1393 train_time:73681ms step_avg:124.88ms
step:601/1393 train_time:73810ms step_avg:124.89ms
step:602/1393 train_time:73937ms step_avg:124.89ms
step:603/1393 train_time:74066ms step_avg:124.90ms
step:604/1393 train_time:74194ms step_avg:124.91ms
step:605/1393 train_time:74322ms step_avg:124.91ms
step:606/1393 train_time:74452ms step_avg:124.92ms
step:607/1393 train_time:74580ms step_avg:124.92ms
step:608/1393 train_time:74709ms step_avg:124.93ms
step:609/1393 train_time:74838ms step_avg:124.94ms
step:610/1393 train_time:74966ms step_avg:124.94ms
step:611/1393 train_time:75096ms step_avg:124.95ms
step:612/1393 train_time:75224ms step_avg:124.96ms
step:613/1393 train_time:75353ms step_avg:124.96ms
step:614/1393 train_time:75481ms step_avg:124.97ms
step:615/1393 train_time:75610ms step_avg:124.97ms
step:616/1393 train_time:75738ms step_avg:124.98ms
step:617/1393 train_time:75866ms step_avg:124.99ms
step:618/1393 train_time:75995ms step_avg:124.99ms
step:619/1393 train_time:76123ms step_avg:125.00ms
step:620/1393 train_time:76252ms step_avg:125.00ms
step:621/1393 train_time:76381ms step_avg:125.01ms
step:622/1393 train_time:76510ms step_avg:125.02ms
step:623/1393 train_time:76640ms step_avg:125.02ms
step:624/1393 train_time:76769ms step_avg:125.03ms
step:625/1393 train_time:76898ms step_avg:125.04ms
step:625/1393 val_loss:3.5798 train_time:77025ms step_avg:125.24ms
step:626/1393 train_time:77047ms step_avg:125.08ms
step:627/1393 train_time:77164ms step_avg:125.06ms
step:628/1393 train_time:77294ms step_avg:125.07ms
step:629/1393 train_time:77423ms step_avg:125.08ms
step:630/1393 train_time:77551ms step_avg:125.08ms
step:631/1393 train_time:77679ms step_avg:125.09ms
step:632/1393 train_time:77808ms step_avg:125.09ms
step:633/1393 train_time:77936ms step_avg:125.10ms
step:634/1393 train_time:78065ms step_avg:125.10ms
step:635/1393 train_time:78194ms step_avg:125.11ms
step:636/1393 train_time:78323ms step_avg:125.12ms
step:637/1393 train_time:78452ms step_avg:125.12ms
step:638/1393 train_time:78580ms step_avg:125.13ms
step:639/1393 train_time:78708ms step_avg:125.13ms
step:640/1393 train_time:78837ms step_avg:125.14ms
step:641/1393 train_time:78966ms step_avg:125.14ms
step:642/1393 train_time:79094ms step_avg:125.15ms
step:643/1393 train_time:79223ms step_avg:125.15ms
step:644/1393 train_time:79352ms step_avg:125.16ms
step:645/1393 train_time:79481ms step_avg:125.17ms
step:646/1393 train_time:79610ms step_avg:125.17ms
step:647/1393 train_time:79738ms step_avg:125.18ms
step:648/1393 train_time:79867ms step_avg:125.18ms
step:649/1393 train_time:79996ms step_avg:125.19ms
step:650/1393 train_time:80125ms step_avg:125.19ms
step:651/1393 train_time:80253ms step_avg:125.20ms
step:652/1393 train_time:80382ms step_avg:125.21ms
step:653/1393 train_time:80512ms step_avg:125.21ms
step:654/1393 train_time:80641ms step_avg:125.22ms
step:655/1393 train_time:80770ms step_avg:125.22ms
step:656/1393 train_time:80898ms step_avg:125.23ms
step:657/1393 train_time:81028ms step_avg:125.24ms
step:658/1393 train_time:81156ms step_avg:125.24ms
step:659/1393 train_time:81284ms step_avg:125.25ms
step:660/1393 train_time:81413ms step_avg:125.25ms
step:661/1393 train_time:81543ms step_avg:125.26ms
step:662/1393 train_time:81672ms step_avg:125.26ms
step:663/1393 train_time:81800ms step_avg:125.27ms
step:664/1393 train_time:81930ms step_avg:125.27ms
step:665/1393 train_time:82058ms step_avg:125.28ms
step:666/1393 train_time:82188ms step_avg:125.29ms
step:667/1393 train_time:82316ms step_avg:125.29ms
step:668/1393 train_time:82445ms step_avg:125.30ms
step:669/1393 train_time:82575ms step_avg:125.30ms
step:670/1393 train_time:82703ms step_avg:125.31ms
step:671/1393 train_time:82833ms step_avg:125.31ms
step:672/1393 train_time:82961ms step_avg:125.32ms
step:673/1393 train_time:83091ms step_avg:125.33ms
step:674/1393 train_time:83219ms step_avg:125.33ms
step:675/1393 train_time:83347ms step_avg:125.33ms
step:676/1393 train_time:83476ms step_avg:125.34ms
step:677/1393 train_time:83606ms step_avg:125.35ms
step:678/1393 train_time:83734ms step_avg:125.35ms
step:679/1393 train_time:83863ms step_avg:125.36ms
step:680/1393 train_time:83993ms step_avg:125.36ms
step:681/1393 train_time:84122ms step_avg:125.37ms
step:682/1393 train_time:84252ms step_avg:125.37ms
step:683/1393 train_time:84380ms step_avg:125.38ms
step:684/1393 train_time:84510ms step_avg:125.39ms
step:685/1393 train_time:84638ms step_avg:125.39ms
step:686/1393 train_time:84768ms step_avg:125.40ms
step:687/1393 train_time:84897ms step_avg:125.40ms
step:688/1393 train_time:85026ms step_avg:125.41ms
step:689/1393 train_time:85155ms step_avg:125.41ms
step:690/1393 train_time:85284ms step_avg:125.42ms
step:691/1393 train_time:85413ms step_avg:125.42ms
step:692/1393 train_time:85544ms step_avg:125.43ms
step:693/1393 train_time:85672ms step_avg:125.44ms
step:694/1393 train_time:85801ms step_avg:125.44ms
step:695/1393 train_time:85930ms step_avg:125.44ms
step:696/1393 train_time:86058ms step_avg:125.45ms
step:697/1393 train_time:86187ms step_avg:125.45ms
step:698/1393 train_time:86315ms step_avg:125.46ms
step:699/1393 train_time:86444ms step_avg:125.46ms
step:700/1393 train_time:86572ms step_avg:125.47ms
step:701/1393 train_time:86701ms step_avg:125.47ms
step:702/1393 train_time:86831ms step_avg:125.48ms
step:703/1393 train_time:86960ms step_avg:125.48ms
step:704/1393 train_time:87089ms step_avg:125.49ms
step:705/1393 train_time:87220ms step_avg:125.50ms
step:706/1393 train_time:87349ms step_avg:125.50ms
step:707/1393 train_time:87478ms step_avg:125.51ms
step:708/1393 train_time:87607ms step_avg:125.51ms
step:709/1393 train_time:87736ms step_avg:125.52ms
step:710/1393 train_time:87866ms step_avg:125.52ms
step:711/1393 train_time:87994ms step_avg:125.53ms
step:712/1393 train_time:88122ms step_avg:125.53ms
step:713/1393 train_time:88251ms step_avg:125.54ms
step:714/1393 train_time:88380ms step_avg:125.54ms
step:715/1393 train_time:88508ms step_avg:125.54ms
step:716/1393 train_time:88637ms step_avg:125.55ms
step:717/1393 train_time:88767ms step_avg:125.55ms
step:718/1393 train_time:88897ms step_avg:125.56ms
step:719/1393 train_time:89026ms step_avg:125.57ms
step:720/1393 train_time:89156ms step_avg:125.57ms
step:721/1393 train_time:89284ms step_avg:125.58ms
step:722/1393 train_time:89413ms step_avg:125.58ms
step:723/1393 train_time:89542ms step_avg:125.59ms
step:724/1393 train_time:89671ms step_avg:125.59ms
step:725/1393 train_time:89802ms step_avg:125.60ms
step:726/1393 train_time:89932ms step_avg:125.60ms
step:727/1393 train_time:90063ms step_avg:125.61ms
step:728/1393 train_time:90193ms step_avg:125.62ms
step:729/1393 train_time:90325ms step_avg:125.63ms
step:730/1393 train_time:90456ms step_avg:125.63ms
step:731/1393 train_time:90587ms step_avg:125.64ms
step:732/1393 train_time:90717ms step_avg:125.65ms
step:733/1393 train_time:90848ms step_avg:125.65ms
step:734/1393 train_time:90978ms step_avg:125.66ms
step:735/1393 train_time:91109ms step_avg:125.67ms
step:736/1393 train_time:91239ms step_avg:125.67ms
step:737/1393 train_time:91371ms step_avg:125.68ms
step:738/1393 train_time:91502ms step_avg:125.69ms
step:739/1393 train_time:91633ms step_avg:125.70ms
step:740/1393 train_time:91763ms step_avg:125.70ms
step:741/1393 train_time:91895ms step_avg:125.71ms
step:742/1393 train_time:92024ms step_avg:125.72ms
step:743/1393 train_time:92156ms step_avg:125.72ms
step:744/1393 train_time:92287ms step_avg:125.73ms
step:745/1393 train_time:92418ms step_avg:125.74ms
step:746/1393 train_time:92549ms step_avg:125.75ms
step:747/1393 train_time:92680ms step_avg:125.75ms
step:748/1393 train_time:92810ms step_avg:125.76ms
step:749/1393 train_time:92941ms step_avg:125.77ms
step:750/1393 train_time:93071ms step_avg:125.77ms
step:750/1393 val_loss:3.5263 train_time:93201ms step_avg:125.95ms
step:751/1393 train_time:93223ms step_avg:125.81ms
step:752/1393 train_time:93342ms step_avg:125.80ms
step:753/1393 train_time:93472ms step_avg:125.80ms
step:754/1393 train_time:93602ms step_avg:125.81ms
step:755/1393 train_time:93733ms step_avg:125.82ms
step:756/1393 train_time:93863ms step_avg:125.82ms
step:757/1393 train_time:93994ms step_avg:125.83ms
step:758/1393 train_time:94125ms step_avg:125.84ms
step:759/1393 train_time:94257ms step_avg:125.84ms
step:760/1393 train_time:94389ms step_avg:125.85ms
step:761/1393 train_time:94520ms step_avg:125.86ms
step:762/1393 train_time:94652ms step_avg:125.87ms
step:763/1393 train_time:94782ms step_avg:125.87ms
step:764/1393 train_time:94912ms step_avg:125.88ms
step:765/1393 train_time:95042ms step_avg:125.88ms
step:766/1393 train_time:95174ms step_avg:125.89ms
step:767/1393 train_time:95304ms step_avg:125.90ms
step:768/1393 train_time:95436ms step_avg:125.91ms
step:769/1393 train_time:95567ms step_avg:125.91ms
step:770/1393 train_time:95698ms step_avg:125.92ms
step:771/1393 train_time:95828ms step_avg:125.92ms
step:772/1393 train_time:95959ms step_avg:125.93ms
step:773/1393 train_time:96089ms step_avg:125.94ms
step:774/1393 train_time:96220ms step_avg:125.94ms
step:775/1393 train_time:96352ms step_avg:125.95ms
step:776/1393 train_time:96483ms step_avg:125.96ms
step:777/1393 train_time:96613ms step_avg:125.96ms
step:778/1393 train_time:96744ms step_avg:125.97ms
step:779/1393 train_time:96874ms step_avg:125.97ms
step:780/1393 train_time:97005ms step_avg:125.98ms
step:781/1393 train_time:97136ms step_avg:125.99ms
step:782/1393 train_time:97267ms step_avg:125.99ms
step:783/1393 train_time:97397ms step_avg:126.00ms
step:784/1393 train_time:97528ms step_avg:126.01ms
step:785/1393 train_time:97660ms step_avg:126.01ms
step:786/1393 train_time:97791ms step_avg:126.02ms
step:787/1393 train_time:97921ms step_avg:126.02ms
step:788/1393 train_time:98051ms step_avg:126.03ms
step:789/1393 train_time:98183ms step_avg:126.04ms
step:790/1393 train_time:98313ms step_avg:126.04ms
step:791/1393 train_time:98445ms step_avg:126.05ms
step:792/1393 train_time:98575ms step_avg:126.05ms
step:793/1393 train_time:98705ms step_avg:126.06ms
step:794/1393 train_time:98836ms step_avg:126.07ms
step:795/1393 train_time:98967ms step_avg:126.07ms
step:796/1393 train_time:99098ms step_avg:126.08ms
step:797/1393 train_time:99230ms step_avg:126.09ms
step:798/1393 train_time:99361ms step_avg:126.09ms
step:799/1393 train_time:99492ms step_avg:126.10ms
step:800/1393 train_time:99623ms step_avg:126.10ms
step:801/1393 train_time:99753ms step_avg:126.11ms
step:802/1393 train_time:99884ms step_avg:126.12ms
step:803/1393 train_time:100015ms step_avg:126.12ms
step:804/1393 train_time:100145ms step_avg:126.13ms
step:805/1393 train_time:100277ms step_avg:126.13ms
step:806/1393 train_time:100407ms step_avg:126.14ms
step:807/1393 train_time:100538ms step_avg:126.15ms
step:808/1393 train_time:100669ms step_avg:126.15ms
step:809/1393 train_time:100799ms step_avg:126.16ms
step:810/1393 train_time:100930ms step_avg:126.16ms
step:811/1393 train_time:101060ms step_avg:126.17ms
step:812/1393 train_time:101191ms step_avg:126.17ms
step:813/1393 train_time:101322ms step_avg:126.18ms
step:814/1393 train_time:101452ms step_avg:126.18ms
step:815/1393 train_time:101582ms step_avg:126.19ms
step:816/1393 train_time:101712ms step_avg:126.19ms
step:817/1393 train_time:101842ms step_avg:126.20ms
step:818/1393 train_time:101972ms step_avg:126.20ms
step:819/1393 train_time:102104ms step_avg:126.21ms
step:820/1393 train_time:102234ms step_avg:126.22ms
step:821/1393 train_time:102364ms step_avg:126.22ms
step:822/1393 train_time:102494ms step_avg:126.22ms
step:823/1393 train_time:102624ms step_avg:126.23ms
step:824/1393 train_time:102756ms step_avg:126.24ms
step:825/1393 train_time:102886ms step_avg:126.24ms
step:826/1393 train_time:103017ms step_avg:126.25ms
step:827/1393 train_time:103148ms step_avg:126.25ms
step:828/1393 train_time:103280ms step_avg:126.26ms
step:829/1393 train_time:103410ms step_avg:126.26ms
step:830/1393 train_time:103541ms step_avg:126.27ms
step:831/1393 train_time:103671ms step_avg:126.27ms
step:832/1393 train_time:103803ms step_avg:126.28ms
step:833/1393 train_time:103933ms step_avg:126.29ms
step:834/1393 train_time:104064ms step_avg:126.29ms
step:835/1393 train_time:104196ms step_avg:126.30ms
step:836/1393 train_time:104327ms step_avg:126.30ms
step:837/1393 train_time:104458ms step_avg:126.31ms
step:838/1393 train_time:104589ms step_avg:126.32ms
step:839/1393 train_time:104719ms step_avg:126.32ms
step:840/1393 train_time:104850ms step_avg:126.33ms
step:841/1393 train_time:104980ms step_avg:126.33ms
step:842/1393 train_time:105112ms step_avg:126.34ms
step:843/1393 train_time:105244ms step_avg:126.34ms
step:844/1393 train_time:105375ms step_avg:126.35ms
step:845/1393 train_time:105505ms step_avg:126.35ms
step:846/1393 train_time:105636ms step_avg:126.36ms
step:847/1393 train_time:105768ms step_avg:126.37ms
step:848/1393 train_time:105899ms step_avg:126.37ms
step:849/1393 train_time:106030ms step_avg:126.38ms
step:850/1393 train_time:106160ms step_avg:126.38ms
step:851/1393 train_time:106292ms step_avg:126.39ms
step:852/1393 train_time:106423ms step_avg:126.39ms
step:853/1393 train_time:106554ms step_avg:126.40ms
step:854/1393 train_time:106684ms step_avg:126.40ms
step:855/1393 train_time:106815ms step_avg:126.41ms
step:856/1393 train_time:106946ms step_avg:126.41ms
step:857/1393 train_time:107077ms step_avg:126.42ms
step:858/1393 train_time:107208ms step_avg:126.42ms
step:859/1393 train_time:107339ms step_avg:126.43ms
step:860/1393 train_time:107469ms step_avg:126.43ms
step:861/1393 train_time:107600ms step_avg:126.44ms
step:862/1393 train_time:107732ms step_avg:126.45ms
step:863/1393 train_time:107864ms step_avg:126.45ms
step:864/1393 train_time:107996ms step_avg:126.46ms
step:865/1393 train_time:108127ms step_avg:126.46ms
step:866/1393 train_time:108261ms step_avg:126.47ms
step:867/1393 train_time:108392ms step_avg:126.48ms
step:868/1393 train_time:108521ms step_avg:126.48ms
step:869/1393 train_time:108652ms step_avg:126.49ms
step:870/1393 train_time:108784ms step_avg:126.49ms
step:871/1393 train_time:108916ms step_avg:126.50ms
step:872/1393 train_time:109046ms step_avg:126.50ms
step:873/1393 train_time:109177ms step_avg:126.51ms
step:874/1393 train_time:109308ms step_avg:126.51ms
step:875/1393 train_time:109439ms step_avg:126.52ms
step:875/1393 val_loss:3.4751 train_time:109569ms step_avg:126.67ms
step:876/1393 train_time:109590ms step_avg:126.55ms
step:877/1393 train_time:109712ms step_avg:126.54ms
step:878/1393 train_time:109842ms step_avg:126.55ms
step:879/1393 train_time:109973ms step_avg:126.55ms
step:880/1393 train_time:110104ms step_avg:126.56ms
step:881/1393 train_time:110234ms step_avg:126.56ms
step:882/1393 train_time:110364ms step_avg:126.56ms
step:883/1393 train_time:110495ms step_avg:126.57ms
step:884/1393 train_time:110627ms step_avg:126.58ms
step:885/1393 train_time:110760ms step_avg:126.58ms
step:886/1393 train_time:110893ms step_avg:126.59ms
step:887/1393 train_time:111022ms step_avg:126.59ms
step:888/1393 train_time:111153ms step_avg:126.60ms
step:889/1393 train_time:111285ms step_avg:126.60ms
step:890/1393 train_time:111415ms step_avg:126.61ms
step:891/1393 train_time:111545ms step_avg:126.61ms
step:892/1393 train_time:111677ms step_avg:126.62ms
step:893/1393 train_time:111807ms step_avg:126.62ms
step:894/1393 train_time:111938ms step_avg:126.63ms
step:895/1393 train_time:112069ms step_avg:126.63ms
step:896/1393 train_time:112200ms step_avg:126.64ms
step:897/1393 train_time:112330ms step_avg:126.64ms
step:898/1393 train_time:112462ms step_avg:126.65ms
step:899/1393 train_time:112594ms step_avg:126.65ms
step:900/1393 train_time:112725ms step_avg:126.66ms
step:901/1393 train_time:112856ms step_avg:126.66ms
step:902/1393 train_time:112987ms step_avg:126.67ms
step:903/1393 train_time:113118ms step_avg:126.67ms
step:904/1393 train_time:113249ms step_avg:126.68ms
step:905/1393 train_time:113380ms step_avg:126.68ms
step:906/1393 train_time:113511ms step_avg:126.69ms
step:907/1393 train_time:113642ms step_avg:126.69ms
step:908/1393 train_time:113773ms step_avg:126.70ms
step:909/1393 train_time:113904ms step_avg:126.70ms
step:910/1393 train_time:114035ms step_avg:126.71ms
step:911/1393 train_time:114167ms step_avg:126.71ms
step:912/1393 train_time:114297ms step_avg:126.71ms
step:913/1393 train_time:114428ms step_avg:126.72ms
step:914/1393 train_time:114559ms step_avg:126.72ms
step:915/1393 train_time:114690ms step_avg:126.73ms
step:916/1393 train_time:114821ms step_avg:126.73ms
step:917/1393 train_time:114953ms step_avg:126.74ms
step:918/1393 train_time:115083ms step_avg:126.74ms
step:919/1393 train_time:115216ms step_avg:126.75ms
step:920/1393 train_time:115349ms step_avg:126.76ms
step:921/1393 train_time:115480ms step_avg:126.76ms
step:922/1393 train_time:115611ms step_avg:126.77ms
step:923/1393 train_time:115741ms step_avg:126.77ms
step:924/1393 train_time:115872ms step_avg:126.77ms
step:925/1393 train_time:116003ms step_avg:126.78ms
step:926/1393 train_time:116135ms step_avg:126.79ms
step:927/1393 train_time:116266ms step_avg:126.79ms
step:928/1393 train_time:116398ms step_avg:126.79ms
step:929/1393 train_time:116529ms step_avg:126.80ms
step:930/1393 train_time:116660ms step_avg:126.80ms
step:931/1393 train_time:116792ms step_avg:126.81ms
step:932/1393 train_time:116925ms step_avg:126.82ms
step:933/1393 train_time:117057ms step_avg:126.82ms
step:934/1393 train_time:117189ms step_avg:126.83ms
step:935/1393 train_time:117321ms step_avg:126.83ms
step:936/1393 train_time:117453ms step_avg:126.84ms
step:937/1393 train_time:117588ms step_avg:126.85ms
step:938/1393 train_time:117721ms step_avg:126.85ms
step:939/1393 train_time:117853ms step_avg:126.86ms
step:940/1393 train_time:117986ms step_avg:126.87ms
step:941/1393 train_time:118120ms step_avg:126.87ms
step:942/1393 train_time:118252ms step_avg:126.88ms
step:943/1393 train_time:118386ms step_avg:126.89ms
step:944/1393 train_time:118521ms step_avg:126.90ms
step:945/1393 train_time:118654ms step_avg:126.90ms
step:946/1393 train_time:118786ms step_avg:126.91ms
step:947/1393 train_time:118920ms step_avg:126.92ms
step:948/1393 train_time:119052ms step_avg:126.92ms
step:949/1393 train_time:119184ms step_avg:126.93ms
step:950/1393 train_time:119317ms step_avg:126.93ms
step:951/1393 train_time:119450ms step_avg:126.94ms
step:952/1393 train_time:119584ms step_avg:126.95ms
step:953/1393 train_time:119717ms step_avg:126.95ms
step:954/1393 train_time:119849ms step_avg:126.96ms
step:955/1393 train_time:119982ms step_avg:126.97ms
step:956/1393 train_time:120116ms step_avg:126.97ms
step:957/1393 train_time:120248ms step_avg:126.98ms
step:958/1393 train_time:120381ms step_avg:126.98ms
step:959/1393 train_time:120514ms step_avg:126.99ms
step:960/1393 train_time:120647ms step_avg:127.00ms
step:961/1393 train_time:120780ms step_avg:127.00ms
step:962/1393 train_time:120912ms step_avg:127.01ms
step:963/1393 train_time:121046ms step_avg:127.02ms
step:964/1393 train_time:121178ms step_avg:127.02ms
step:965/1393 train_time:121311ms step_avg:127.03ms
step:966/1393 train_time:121444ms step_avg:127.03ms
step:967/1393 train_time:121578ms step_avg:127.04ms
step:968/1393 train_time:121710ms step_avg:127.05ms
step:969/1393 train_time:121843ms step_avg:127.05ms
step:970/1393 train_time:121975ms step_avg:127.06ms
step:971/1393 train_time:122109ms step_avg:127.06ms
step:972/1393 train_time:122241ms step_avg:127.07ms
step:973/1393 train_time:122374ms step_avg:127.08ms
step:974/1393 train_time:122506ms step_avg:127.08ms
step:975/1393 train_time:122639ms step_avg:127.09ms
step:976/1393 train_time:122771ms step_avg:127.09ms
step:977/1393 train_time:122905ms step_avg:127.10ms
step:978/1393 train_time:123037ms step_avg:127.10ms
step:979/1393 train_time:123169ms step_avg:127.11ms
step:980/1393 train_time:123301ms step_avg:127.11ms
step:981/1393 train_time:123433ms step_avg:127.12ms
step:982/1393 train_time:123565ms step_avg:127.12ms
step:983/1393 train_time:123697ms step_avg:127.13ms
step:984/1393 train_time:123829ms step_avg:127.13ms
step:985/1393 train_time:123962ms step_avg:127.14ms
step:986/1393 train_time:124097ms step_avg:127.15ms
step:987/1393 train_time:124230ms step_avg:127.15ms
step:988/1393 train_time:124362ms step_avg:127.16ms
step:989/1393 train_time:124495ms step_avg:127.17ms
step:990/1393 train_time:124628ms step_avg:127.17ms
step:991/1393 train_time:124760ms step_avg:127.18ms
step:992/1393 train_time:124893ms step_avg:127.18ms
step:993/1393 train_time:125029ms step_avg:127.19ms
step:994/1393 train_time:125162ms step_avg:127.20ms
step:995/1393 train_time:125294ms step_avg:127.20ms
step:996/1393 train_time:125426ms step_avg:127.21ms
step:997/1393 train_time:125557ms step_avg:127.21ms
step:998/1393 train_time:125689ms step_avg:127.22ms
step:999/1393 train_time:125822ms step_avg:127.22ms
step:1000/1393 train_time:125954ms step_avg:127.23ms
step:1000/1393 val_loss:3.4116 train_time:126084ms step_avg:127.36ms
step:1001/1393 train_time:126106ms step_avg:127.25ms
step:1002/1393 train_time:126226ms step_avg:127.24ms
step:1003/1393 train_time:126358ms step_avg:127.25ms
step:1004/1393 train_time:126490ms step_avg:127.25ms
step:1005/1393 train_time:126622ms step_avg:127.26ms
step:1006/1393 train_time:126753ms step_avg:127.26ms
step:1007/1393 train_time:126886ms step_avg:127.27ms
step:1008/1393 train_time:127018ms step_avg:127.27ms
step:1009/1393 train_time:127153ms step_avg:127.28ms
step:1010/1393 train_time:127287ms step_avg:127.29ms
step:1011/1393 train_time:127420ms step_avg:127.29ms
step:1012/1393 train_time:127552ms step_avg:127.30ms
step:1013/1393 train_time:127685ms step_avg:127.30ms
step:1014/1393 train_time:127817ms step_avg:127.31ms
step:1015/1393 train_time:127949ms step_avg:127.31ms
step:1016/1393 train_time:128081ms step_avg:127.32ms
step:1017/1393 train_time:128214ms step_avg:127.32ms
step:1018/1393 train_time:128346ms step_avg:127.33ms
step:1019/1393 train_time:128479ms step_avg:127.33ms
step:1020/1393 train_time:128612ms step_avg:127.34ms
step:1021/1393 train_time:128745ms step_avg:127.34ms
step:1022/1393 train_time:128877ms step_avg:127.35ms
step:1023/1393 train_time:129011ms step_avg:127.36ms
step:1024/1393 train_time:129144ms step_avg:127.36ms
step:1025/1393 train_time:129277ms step_avg:127.37ms
step:1026/1393 train_time:129411ms step_avg:127.37ms
step:1027/1393 train_time:129543ms step_avg:127.38ms
step:1028/1393 train_time:129676ms step_avg:127.38ms
step:1029/1393 train_time:129809ms step_avg:127.39ms
step:1030/1393 train_time:129942ms step_avg:127.39ms
step:1031/1393 train_time:130073ms step_avg:127.40ms
step:1032/1393 train_time:130205ms step_avg:127.40ms
step:1033/1393 train_time:130337ms step_avg:127.41ms
step:1034/1393 train_time:130471ms step_avg:127.41ms
step:1035/1393 train_time:130605ms step_avg:127.42ms
step:1036/1393 train_time:130738ms step_avg:127.43ms
step:1037/1393 train_time:130872ms step_avg:127.43ms
step:1038/1393 train_time:131005ms step_avg:127.44ms
step:1039/1393 train_time:131137ms step_avg:127.44ms
step:1040/1393 train_time:131270ms step_avg:127.45ms
step:1041/1393 train_time:131402ms step_avg:127.45ms
step:1042/1393 train_time:131534ms step_avg:127.46ms
step:1043/1393 train_time:131667ms step_avg:127.46ms
step:1044/1393 train_time:131801ms step_avg:127.47ms
step:1045/1393 train_time:131935ms step_avg:127.47ms
step:1046/1393 train_time:132067ms step_avg:127.48ms
step:1047/1393 train_time:132199ms step_avg:127.48ms
step:1048/1393 train_time:132332ms step_avg:127.49ms
step:1049/1393 train_time:132466ms step_avg:127.49ms
step:1050/1393 train_time:132599ms step_avg:127.50ms
step:1051/1393 train_time:132733ms step_avg:127.51ms
step:1052/1393 train_time:132866ms step_avg:127.51ms
step:1053/1393 train_time:132998ms step_avg:127.51ms
step:1054/1393 train_time:133130ms step_avg:127.52ms
step:1055/1393 train_time:133264ms step_avg:127.53ms
step:1056/1393 train_time:133395ms step_avg:127.53ms
step:1057/1393 train_time:133527ms step_avg:127.53ms
step:1058/1393 train_time:133660ms step_avg:127.54ms
step:1059/1393 train_time:133793ms step_avg:127.54ms
step:1060/1393 train_time:133927ms step_avg:127.55ms
step:1061/1393 train_time:134059ms step_avg:127.55ms
step:1062/1393 train_time:134192ms step_avg:127.56ms
step:1063/1393 train_time:134326ms step_avg:127.57ms
step:1064/1393 train_time:134460ms step_avg:127.57ms
step:1065/1393 train_time:134593ms step_avg:127.58ms
step:1066/1393 train_time:134728ms step_avg:127.58ms
step:1067/1393 train_time:134861ms step_avg:127.59ms
step:1068/1393 train_time:134993ms step_avg:127.59ms
step:1069/1393 train_time:135128ms step_avg:127.60ms
step:1070/1393 train_time:135260ms step_avg:127.60ms
step:1071/1393 train_time:135394ms step_avg:127.61ms
step:1072/1393 train_time:135526ms step_avg:127.61ms
step:1073/1393 train_time:135658ms step_avg:127.62ms
step:1074/1393 train_time:135791ms step_avg:127.62ms
step:1075/1393 train_time:135924ms step_avg:127.63ms
step:1076/1393 train_time:136057ms step_avg:127.63ms
step:1077/1393 train_time:136191ms step_avg:127.64ms
step:1078/1393 train_time:136324ms step_avg:127.64ms
step:1079/1393 train_time:136461ms step_avg:127.65ms
step:1080/1393 train_time:136594ms step_avg:127.66ms
step:1081/1393 train_time:136726ms step_avg:127.66ms
step:1082/1393 train_time:136858ms step_avg:127.67ms
step:1083/1393 train_time:136991ms step_avg:127.67ms
step:1084/1393 train_time:137124ms step_avg:127.68ms
step:1085/1393 train_time:137257ms step_avg:127.68ms
step:1086/1393 train_time:137390ms step_avg:127.69ms
step:1087/1393 train_time:137523ms step_avg:127.69ms
step:1088/1393 train_time:137656ms step_avg:127.70ms
step:1089/1393 train_time:137790ms step_avg:127.70ms
step:1090/1393 train_time:137923ms step_avg:127.71ms
step:1091/1393 train_time:138056ms step_avg:127.71ms
step:1092/1393 train_time:138189ms step_avg:127.72ms
step:1093/1393 train_time:138321ms step_avg:127.72ms
step:1094/1393 train_time:138454ms step_avg:127.73ms
step:1095/1393 train_time:138587ms step_avg:127.73ms
step:1096/1393 train_time:138721ms step_avg:127.74ms
step:1097/1393 train_time:138854ms step_avg:127.74ms
step:1098/1393 train_time:138988ms step_avg:127.75ms
step:1099/1393 train_time:139121ms step_avg:127.75ms
step:1100/1393 train_time:139254ms step_avg:127.76ms
step:1101/1393 train_time:139386ms step_avg:127.76ms
step:1102/1393 train_time:139519ms step_avg:127.76ms
step:1103/1393 train_time:139653ms step_avg:127.77ms
step:1104/1393 train_time:139785ms step_avg:127.77ms
step:1105/1393 train_time:139920ms step_avg:127.78ms
step:1106/1393 train_time:140053ms step_avg:127.79ms
step:1107/1393 train_time:140185ms step_avg:127.79ms
step:1108/1393 train_time:140321ms step_avg:127.80ms
step:1109/1393 train_time:140454ms step_avg:127.80ms
step:1110/1393 train_time:140588ms step_avg:127.81ms
step:1111/1393 train_time:140721ms step_avg:127.81ms
step:1112/1393 train_time:140853ms step_avg:127.82ms
step:1113/1393 train_time:140986ms step_avg:127.82ms
step:1114/1393 train_time:141119ms step_avg:127.83ms
step:1115/1393 train_time:141252ms step_avg:127.83ms
step:1116/1393 train_time:141385ms step_avg:127.83ms
step:1117/1393 train_time:141519ms step_avg:127.84ms
step:1118/1393 train_time:141654ms step_avg:127.85ms
step:1119/1393 train_time:141786ms step_avg:127.85ms
step:1120/1393 train_time:141918ms step_avg:127.85ms
step:1121/1393 train_time:142052ms step_avg:127.86ms
step:1122/1393 train_time:142184ms step_avg:127.86ms
step:1123/1393 train_time:142318ms step_avg:127.87ms
step:1124/1393 train_time:142450ms step_avg:127.87ms
step:1125/1393 train_time:142583ms step_avg:127.88ms
step:1125/1393 val_loss:3.3616 train_time:142716ms step_avg:128.00ms
step:1126/1393 train_time:142737ms step_avg:127.90ms
step:1127/1393 train_time:142858ms step_avg:127.89ms
step:1128/1393 train_time:142991ms step_avg:127.90ms
step:1129/1393 train_time:143125ms step_avg:127.90ms
step:1130/1393 train_time:143257ms step_avg:127.91ms
step:1131/1393 train_time:143391ms step_avg:127.91ms
step:1132/1393 train_time:143523ms step_avg:127.92ms
step:1133/1393 train_time:143654ms step_avg:127.92ms
step:1134/1393 train_time:143788ms step_avg:127.93ms
step:1135/1393 train_time:143921ms step_avg:127.93ms
step:1136/1393 train_time:144056ms step_avg:127.94ms
step:1137/1393 train_time:144189ms step_avg:127.94ms
step:1138/1393 train_time:144325ms step_avg:127.95ms
step:1139/1393 train_time:144459ms step_avg:127.95ms
step:1140/1393 train_time:144594ms step_avg:127.96ms
step:1141/1393 train_time:144728ms step_avg:127.96ms
step:1142/1393 train_time:144862ms step_avg:127.97ms
step:1143/1393 train_time:144999ms step_avg:127.98ms
step:1144/1393 train_time:145133ms step_avg:127.98ms
step:1145/1393 train_time:145268ms step_avg:127.99ms
step:1146/1393 train_time:145403ms step_avg:128.00ms
step:1147/1393 train_time:145539ms step_avg:128.00ms
step:1148/1393 train_time:145674ms step_avg:128.01ms
step:1149/1393 train_time:145808ms step_avg:128.01ms
step:1150/1393 train_time:145941ms step_avg:128.02ms
step:1151/1393 train_time:146077ms step_avg:128.03ms
step:1152/1393 train_time:146210ms step_avg:128.03ms
step:1153/1393 train_time:146346ms step_avg:128.04ms
step:1154/1393 train_time:146480ms step_avg:128.04ms
step:1155/1393 train_time:146614ms step_avg:128.05ms
step:1156/1393 train_time:146752ms step_avg:128.06ms
step:1157/1393 train_time:146885ms step_avg:128.06ms
step:1158/1393 train_time:147019ms step_avg:128.07ms
step:1159/1393 train_time:147153ms step_avg:128.07ms
step:1160/1393 train_time:147287ms step_avg:128.08ms
step:1161/1393 train_time:147421ms step_avg:128.08ms
step:1162/1393 train_time:147557ms step_avg:128.09ms
step:1163/1393 train_time:147690ms step_avg:128.09ms
step:1164/1393 train_time:147826ms step_avg:128.10ms
step:1165/1393 train_time:147960ms step_avg:128.10ms
step:1166/1393 train_time:148093ms step_avg:128.11ms
step:1167/1393 train_time:148227ms step_avg:128.11ms
step:1168/1393 train_time:148362ms step_avg:128.12ms
step:1169/1393 train_time:148497ms step_avg:128.12ms
step:1170/1393 train_time:148630ms step_avg:128.13ms
step:1171/1393 train_time:148765ms step_avg:128.14ms
step:1172/1393 train_time:148900ms step_avg:128.14ms
step:1173/1393 train_time:149033ms step_avg:128.15ms
step:1174/1393 train_time:149172ms step_avg:128.15ms
step:1175/1393 train_time:149306ms step_avg:128.16ms
step:1176/1393 train_time:149441ms step_avg:128.17ms
step:1177/1393 train_time:149577ms step_avg:128.17ms
step:1178/1393 train_time:149710ms step_avg:128.18ms
step:1179/1393 train_time:149844ms step_avg:128.18ms
step:1180/1393 train_time:149981ms step_avg:128.19ms
step:1181/1393 train_time:150117ms step_avg:128.20ms
step:1182/1393 train_time:150251ms step_avg:128.20ms
step:1183/1393 train_time:150385ms step_avg:128.21ms
step:1184/1393 train_time:150521ms step_avg:128.21ms
step:1185/1393 train_time:150655ms step_avg:128.22ms
step:1186/1393 train_time:150789ms step_avg:128.22ms
step:1187/1393 train_time:150927ms step_avg:128.23ms
step:1188/1393 train_time:151062ms step_avg:128.24ms
step:1189/1393 train_time:151197ms step_avg:128.24ms
step:1190/1393 train_time:151331ms step_avg:128.25ms
step:1191/1393 train_time:151465ms step_avg:128.25ms
step:1192/1393 train_time:151599ms step_avg:128.26ms
step:1193/1393 train_time:151733ms step_avg:128.26ms
step:1194/1393 train_time:151866ms step_avg:128.27ms
step:1195/1393 train_time:152001ms step_avg:128.27ms
step:1196/1393 train_time:152136ms step_avg:128.28ms
step:1197/1393 train_time:152270ms step_avg:128.28ms
step:1198/1393 train_time:152407ms step_avg:128.29ms
step:1199/1393 train_time:152540ms step_avg:128.29ms
step:1200/1393 train_time:152675ms step_avg:128.30ms
step:1201/1393 train_time:152808ms step_avg:128.30ms
step:1202/1393 train_time:152946ms step_avg:128.31ms
step:1203/1393 train_time:153085ms step_avg:128.32ms
step:1204/1393 train_time:153219ms step_avg:128.32ms
step:1205/1393 train_time:153354ms step_avg:128.33ms
step:1206/1393 train_time:153490ms step_avg:128.34ms
step:1207/1393 train_time:153625ms step_avg:128.34ms
step:1208/1393 train_time:153760ms step_avg:128.35ms
step:1209/1393 train_time:153894ms step_avg:128.35ms
step:1210/1393 train_time:154030ms step_avg:128.36ms
step:1211/1393 train_time:154165ms step_avg:128.36ms
step:1212/1393 train_time:154299ms step_avg:128.37ms
step:1213/1393 train_time:154432ms step_avg:128.37ms
step:1214/1393 train_time:154568ms step_avg:128.38ms
step:1215/1393 train_time:154703ms step_avg:128.38ms
step:1216/1393 train_time:154837ms step_avg:128.39ms
step:1217/1393 train_time:154972ms step_avg:128.39ms
step:1218/1393 train_time:155105ms step_avg:128.40ms
step:1219/1393 train_time:155237ms step_avg:128.40ms
step:1220/1393 train_time:155372ms step_avg:128.41ms
step:1221/1393 train_time:155505ms step_avg:128.41ms
step:1222/1393 train_time:155639ms step_avg:128.42ms
step:1223/1393 train_time:155773ms step_avg:128.42ms
step:1224/1393 train_time:155908ms step_avg:128.43ms
step:1225/1393 train_time:156044ms step_avg:128.43ms
step:1226/1393 train_time:156178ms step_avg:128.44ms
step:1227/1393 train_time:156312ms step_avg:128.44ms
step:1228/1393 train_time:156447ms step_avg:128.45ms
step:1229/1393 train_time:156580ms step_avg:128.45ms
step:1230/1393 train_time:156716ms step_avg:128.46ms
step:1231/1393 train_time:156853ms step_avg:128.46ms
step:1232/1393 train_time:156990ms step_avg:128.47ms
step:1233/1393 train_time:157124ms step_avg:128.47ms
step:1234/1393 train_time:157258ms step_avg:128.48ms
step:1235/1393 train_time:157392ms step_avg:128.48ms
step:1236/1393 train_time:157526ms step_avg:128.49ms
step:1237/1393 train_time:157659ms step_avg:128.49ms
step:1238/1393 train_time:157800ms step_avg:128.50ms
step:1239/1393 train_time:157935ms step_avg:128.51ms
step:1240/1393 train_time:158069ms step_avg:128.51ms
step:1241/1393 train_time:158206ms step_avg:128.52ms
step:1242/1393 train_time:158340ms step_avg:128.52ms
step:1243/1393 train_time:158474ms step_avg:128.53ms
step:1244/1393 train_time:158608ms step_avg:128.53ms
step:1245/1393 train_time:158743ms step_avg:128.54ms
step:1246/1393 train_time:158878ms step_avg:128.54ms
step:1247/1393 train_time:159013ms step_avg:128.55ms
step:1248/1393 train_time:159148ms step_avg:128.55ms
step:1249/1393 train_time:159280ms step_avg:128.56ms
step:1250/1393 train_time:159416ms step_avg:128.56ms
step:1250/1393 val_loss:3.3144 train_time:159549ms step_avg:128.67ms
step:1251/1393 train_time:159570ms step_avg:128.58ms
step:1252/1393 train_time:159694ms step_avg:128.58ms
step:1253/1393 train_time:159827ms step_avg:128.58ms
step:1254/1393 train_time:159960ms step_avg:128.58ms
step:1255/1393 train_time:160098ms step_avg:128.59ms
step:1256/1393 train_time:160232ms step_avg:128.60ms
step:1257/1393 train_time:160366ms step_avg:128.60ms
step:1258/1393 train_time:160501ms step_avg:128.61ms
step:1259/1393 train_time:160638ms step_avg:128.61ms
step:1260/1393 train_time:160772ms step_avg:128.62ms
step:1261/1393 train_time:160905ms step_avg:128.62ms
step:1262/1393 train_time:161041ms step_avg:128.63ms
step:1263/1393 train_time:161176ms step_avg:128.63ms
step:1264/1393 train_time:161311ms step_avg:128.64ms
step:1265/1393 train_time:161445ms step_avg:128.64ms
step:1266/1393 train_time:161581ms step_avg:128.65ms
step:1267/1393 train_time:161715ms step_avg:128.65ms
step:1268/1393 train_time:161849ms step_avg:128.66ms
step:1269/1393 train_time:161985ms step_avg:128.66ms
step:1270/1393 train_time:162120ms step_avg:128.67ms
step:1271/1393 train_time:162254ms step_avg:128.67ms
step:1272/1393 train_time:162388ms step_avg:128.68ms
step:1273/1393 train_time:162521ms step_avg:128.68ms
step:1274/1393 train_time:162656ms step_avg:128.68ms
step:1275/1393 train_time:162792ms step_avg:128.69ms
step:1276/1393 train_time:162925ms step_avg:128.69ms
step:1277/1393 train_time:163059ms step_avg:128.70ms
step:1278/1393 train_time:163193ms step_avg:128.70ms
step:1279/1393 train_time:163327ms step_avg:128.71ms
step:1280/1393 train_time:163464ms step_avg:128.71ms
step:1281/1393 train_time:163598ms step_avg:128.72ms
step:1282/1393 train_time:163731ms step_avg:128.72ms
step:1283/1393 train_time:163866ms step_avg:128.72ms
step:1284/1393 train_time:164002ms step_avg:128.73ms
step:1285/1393 train_time:164136ms step_avg:128.73ms
step:1286/1393 train_time:164270ms step_avg:128.74ms
step:1287/1393 train_time:164404ms step_avg:128.74ms
step:1288/1393 train_time:164539ms step_avg:128.75ms
step:1289/1393 train_time:164674ms step_avg:128.75ms
step:1290/1393 train_time:164809ms step_avg:128.76ms
step:1291/1393 train_time:164946ms step_avg:128.76ms
step:1292/1393 train_time:165081ms step_avg:128.77ms
step:1293/1393 train_time:165218ms step_avg:128.77ms
step:1294/1393 train_time:165352ms step_avg:128.78ms
step:1295/1393 train_time:165486ms step_avg:128.78ms
step:1296/1393 train_time:165622ms step_avg:128.79ms
step:1297/1393 train_time:165757ms step_avg:128.79ms
step:1298/1393 train_time:165890ms step_avg:128.80ms
step:1299/1393 train_time:166024ms step_avg:128.80ms
step:1300/1393 train_time:166159ms step_avg:128.81ms
step:1301/1393 train_time:166293ms step_avg:128.81ms
step:1302/1393 train_time:166427ms step_avg:128.81ms
step:1303/1393 train_time:166563ms step_avg:128.82ms
step:1304/1393 train_time:166699ms step_avg:128.82ms
step:1305/1393 train_time:166833ms step_avg:128.83ms
step:1306/1393 train_time:166968ms step_avg:128.83ms
step:1307/1393 train_time:167102ms step_avg:128.84ms
step:1308/1393 train_time:167236ms step_avg:128.84ms
step:1309/1393 train_time:167371ms step_avg:128.85ms
step:1310/1393 train_time:167507ms step_avg:128.85ms
step:1311/1393 train_time:167641ms step_avg:128.86ms
step:1312/1393 train_time:167775ms step_avg:128.86ms
step:1313/1393 train_time:167910ms step_avg:128.86ms
step:1314/1393 train_time:168043ms step_avg:128.87ms
step:1315/1393 train_time:168180ms step_avg:128.87ms
step:1316/1393 train_time:168314ms step_avg:128.88ms
step:1317/1393 train_time:168448ms step_avg:128.88ms
step:1318/1393 train_time:168583ms step_avg:128.89ms
step:1319/1393 train_time:168720ms step_avg:128.89ms
step:1320/1393 train_time:168855ms step_avg:128.90ms
step:1321/1393 train_time:168990ms step_avg:128.90ms
step:1322/1393 train_time:169127ms step_avg:128.91ms
step:1323/1393 train_time:169262ms step_avg:128.91ms
step:1324/1393 train_time:169396ms step_avg:128.92ms
step:1325/1393 train_time:169531ms step_avg:128.92ms
step:1326/1393 train_time:169667ms step_avg:128.93ms
step:1327/1393 train_time:169801ms step_avg:128.93ms
step:1328/1393 train_time:169935ms step_avg:128.93ms
step:1329/1393 train_time:170074ms step_avg:128.94ms
step:1330/1393 train_time:170208ms step_avg:128.95ms
step:1331/1393 train_time:170347ms step_avg:128.95ms
step:1332/1393 train_time:170485ms step_avg:128.96ms
step:1333/1393 train_time:170619ms step_avg:128.96ms
step:1334/1393 train_time:170752ms step_avg:128.97ms
step:1335/1393 train_time:170886ms step_avg:128.97ms
step:1336/1393 train_time:171024ms step_avg:128.98ms
step:1337/1393 train_time:171158ms step_avg:128.98ms
step:1338/1393 train_time:171293ms step_avg:128.99ms
step:1339/1393 train_time:171428ms step_avg:128.99ms
step:1340/1393 train_time:171564ms step_avg:129.00ms
step:1341/1393 train_time:171697ms step_avg:129.00ms
step:1342/1393 train_time:171832ms step_avg:129.00ms
step:1343/1393 train_time:171967ms step_avg:129.01ms
step:1344/1393 train_time:172102ms step_avg:129.01ms
step:1345/1393 train_time:172239ms step_avg:129.02ms
step:1346/1393 train_time:172373ms step_avg:129.02ms
step:1347/1393 train_time:172510ms step_avg:129.03ms
step:1348/1393 train_time:172645ms step_avg:129.03ms
step:1349/1393 train_time:172782ms step_avg:129.04ms
step:1350/1393 train_time:172916ms step_avg:129.04ms
step:1351/1393 train_time:173051ms step_avg:129.05ms
step:1352/1393 train_time:173190ms step_avg:129.05ms
step:1353/1393 train_time:173328ms step_avg:129.06ms
step:1354/1393 train_time:173463ms step_avg:129.07ms
step:1355/1393 train_time:173598ms step_avg:129.07ms
step:1356/1393 train_time:173733ms step_avg:129.07ms
step:1357/1393 train_time:173869ms step_avg:129.08ms
step:1358/1393 train_time:174006ms step_avg:129.08ms
step:1359/1393 train_time:174141ms step_avg:129.09ms
step:1360/1393 train_time:174277ms step_avg:129.09ms
step:1361/1393 train_time:174412ms step_avg:129.10ms
step:1362/1393 train_time:174550ms step_avg:129.11ms
step:1363/1393 train_time:174689ms step_avg:129.11ms
step:1364/1393 train_time:174824ms step_avg:129.12ms
step:1365/1393 train_time:174959ms step_avg:129.12ms
step:1366/1393 train_time:175093ms step_avg:129.12ms
step:1367/1393 train_time:175229ms step_avg:129.13ms
step:1368/1393 train_time:175365ms step_avg:129.13ms
step:1369/1393 train_time:175503ms step_avg:129.14ms
step:1370/1393 train_time:175642ms step_avg:129.15ms
step:1371/1393 train_time:175779ms step_avg:129.15ms
step:1372/1393 train_time:175916ms step_avg:129.16ms
step:1373/1393 train_time:176052ms step_avg:129.16ms
step:1374/1393 train_time:176188ms step_avg:129.17ms
step:1375/1393 train_time:176322ms step_avg:129.17ms
step:1375/1393 val_loss:3.2798 train_time:176456ms step_avg:129.27ms
step:1376/1393 train_time:176477ms step_avg:129.19ms
step:1377/1393 train_time:176602ms step_avg:129.19ms
step:1378/1393 train_time:176739ms step_avg:129.19ms
step:1379/1393 train_time:176874ms step_avg:129.20ms
step:1380/1393 train_time:177010ms step_avg:129.20ms
step:1381/1393 train_time:177147ms step_avg:129.21ms
step:1382/1393 train_time:177283ms step_avg:129.22ms
step:1383/1393 train_time:177418ms step_avg:129.22ms
step:1384/1393 train_time:177556ms step_avg:129.23ms
step:1385/1393 train_time:177692ms step_avg:129.23ms
step:1386/1393 train_time:177827ms step_avg:129.23ms
step:1387/1393 train_time:177962ms step_avg:129.24ms
step:1388/1393 train_time:178098ms step_avg:129.24ms
step:1389/1393 train_time:178234ms step_avg:129.25ms
step:1390/1393 train_time:178370ms step_avg:129.25ms
step:1391/1393 train_time:178505ms step_avg:129.26ms
step:1392/1393 train_time:178643ms step_avg:129.26ms
step:1393/1393 train_time:178777ms step_avg:129.27ms
step:1393/1393 val_loss:3.2762 train_time:178912ms step_avg:129.36ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
