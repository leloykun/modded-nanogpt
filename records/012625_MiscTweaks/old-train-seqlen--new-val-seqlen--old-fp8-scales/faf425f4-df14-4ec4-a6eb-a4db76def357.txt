import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:58:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23392ms step_avg:nanms
step:2/1393 train_time:23920ms step_avg:nanms
step:3/1393 train_time:24041ms step_avg:nanms
step:4/1393 train_time:24161ms step_avg:nanms
step:5/1393 train_time:24282ms step_avg:nanms
step:6/1393 train_time:24403ms step_avg:nanms
step:7/1393 train_time:24525ms step_avg:nanms
step:8/1393 train_time:24646ms step_avg:nanms
step:9/1393 train_time:24768ms step_avg:nanms
step:10/1393 train_time:24892ms step_avg:nanms
step:11/1393 train_time:123ms step_avg:nanms
step:12/1393 train_time:247ms step_avg:nanms
step:13/1393 train_time:370ms step_avg:123.24ms
step:14/1393 train_time:491ms step_avg:122.81ms
step:15/1393 train_time:613ms step_avg:122.63ms
step:16/1393 train_time:735ms step_avg:122.44ms
step:17/1393 train_time:856ms step_avg:122.24ms
step:18/1393 train_time:978ms step_avg:122.26ms
step:19/1393 train_time:1100ms step_avg:122.26ms
step:20/1393 train_time:1222ms step_avg:122.23ms
step:21/1393 train_time:1346ms step_avg:122.32ms
step:22/1393 train_time:1467ms step_avg:122.25ms
step:23/1393 train_time:1588ms step_avg:122.19ms
step:24/1393 train_time:1712ms step_avg:122.27ms
step:25/1393 train_time:1833ms step_avg:122.20ms
step:26/1393 train_time:1956ms step_avg:122.25ms
step:27/1393 train_time:2077ms step_avg:122.20ms
step:28/1393 train_time:2199ms step_avg:122.18ms
step:29/1393 train_time:2322ms step_avg:122.21ms
step:30/1393 train_time:2445ms step_avg:122.23ms
step:31/1393 train_time:2567ms step_avg:122.25ms
step:32/1393 train_time:2689ms step_avg:122.24ms
step:33/1393 train_time:2813ms step_avg:122.30ms
step:34/1393 train_time:2935ms step_avg:122.29ms
step:35/1393 train_time:3057ms step_avg:122.27ms
step:36/1393 train_time:3179ms step_avg:122.28ms
step:37/1393 train_time:3301ms step_avg:122.26ms
step:38/1393 train_time:3424ms step_avg:122.30ms
step:39/1393 train_time:3546ms step_avg:122.29ms
step:40/1393 train_time:3670ms step_avg:122.32ms
step:41/1393 train_time:3791ms step_avg:122.30ms
step:42/1393 train_time:3913ms step_avg:122.27ms
step:43/1393 train_time:4036ms step_avg:122.29ms
step:44/1393 train_time:4157ms step_avg:122.26ms
step:45/1393 train_time:4279ms step_avg:122.25ms
step:46/1393 train_time:4400ms step_avg:122.22ms
step:47/1393 train_time:4523ms step_avg:122.24ms
step:48/1393 train_time:4645ms step_avg:122.25ms
step:49/1393 train_time:4769ms step_avg:122.28ms
step:50/1393 train_time:4891ms step_avg:122.26ms
step:51/1393 train_time:5012ms step_avg:122.24ms
step:52/1393 train_time:5135ms step_avg:122.26ms
step:53/1393 train_time:5258ms step_avg:122.27ms
step:54/1393 train_time:5381ms step_avg:122.30ms
step:55/1393 train_time:5503ms step_avg:122.28ms
step:56/1393 train_time:5625ms step_avg:122.28ms
step:57/1393 train_time:5750ms step_avg:122.35ms
step:58/1393 train_time:5874ms step_avg:122.37ms
step:59/1393 train_time:5995ms step_avg:122.35ms
step:60/1393 train_time:6117ms step_avg:122.35ms
step:61/1393 train_time:6241ms step_avg:122.38ms
step:62/1393 train_time:6364ms step_avg:122.38ms
step:63/1393 train_time:6486ms step_avg:122.37ms
step:64/1393 train_time:6607ms step_avg:122.36ms
step:65/1393 train_time:6729ms step_avg:122.34ms
step:66/1393 train_time:6851ms step_avg:122.33ms
step:67/1393 train_time:6973ms step_avg:122.33ms
step:68/1393 train_time:7095ms step_avg:122.33ms
step:69/1393 train_time:7217ms step_avg:122.32ms
step:70/1393 train_time:7340ms step_avg:122.34ms
step:71/1393 train_time:7464ms step_avg:122.36ms
step:72/1393 train_time:7587ms step_avg:122.38ms
step:73/1393 train_time:7709ms step_avg:122.37ms
step:74/1393 train_time:7832ms step_avg:122.38ms
step:75/1393 train_time:7955ms step_avg:122.38ms
step:76/1393 train_time:8077ms step_avg:122.38ms
step:77/1393 train_time:8201ms step_avg:122.40ms
step:78/1393 train_time:8325ms step_avg:122.43ms
step:79/1393 train_time:8448ms step_avg:122.44ms
step:80/1393 train_time:8571ms step_avg:122.44ms
step:81/1393 train_time:8692ms step_avg:122.43ms
step:82/1393 train_time:8813ms step_avg:122.41ms
step:83/1393 train_time:8935ms step_avg:122.40ms
step:84/1393 train_time:9058ms step_avg:122.41ms
step:85/1393 train_time:9180ms step_avg:122.40ms
step:86/1393 train_time:9303ms step_avg:122.41ms
step:87/1393 train_time:9426ms step_avg:122.41ms
step:88/1393 train_time:9549ms step_avg:122.43ms
step:89/1393 train_time:9672ms step_avg:122.43ms
step:90/1393 train_time:9795ms step_avg:122.44ms
step:91/1393 train_time:9916ms step_avg:122.42ms
step:92/1393 train_time:10038ms step_avg:122.41ms
step:93/1393 train_time:10161ms step_avg:122.42ms
step:94/1393 train_time:10284ms step_avg:122.43ms
step:95/1393 train_time:10407ms step_avg:122.43ms
step:96/1393 train_time:10529ms step_avg:122.43ms
step:97/1393 train_time:10653ms step_avg:122.44ms
step:98/1393 train_time:10774ms step_avg:122.43ms
step:99/1393 train_time:10895ms step_avg:122.41ms
step:100/1393 train_time:11017ms step_avg:122.41ms
step:101/1393 train_time:11140ms step_avg:122.41ms
step:102/1393 train_time:11262ms step_avg:122.42ms
step:103/1393 train_time:11385ms step_avg:122.42ms
step:104/1393 train_time:11508ms step_avg:122.42ms
step:105/1393 train_time:11632ms step_avg:122.44ms
step:106/1393 train_time:11755ms step_avg:122.44ms
step:107/1393 train_time:11876ms step_avg:122.44ms
step:108/1393 train_time:11999ms step_avg:122.44ms
step:109/1393 train_time:12121ms step_avg:122.44ms
step:110/1393 train_time:12244ms step_avg:122.44ms
step:111/1393 train_time:12367ms step_avg:122.45ms
step:112/1393 train_time:12490ms step_avg:122.45ms
step:113/1393 train_time:12614ms step_avg:122.47ms
step:114/1393 train_time:12736ms step_avg:122.47ms
step:115/1393 train_time:12860ms step_avg:122.48ms
step:116/1393 train_time:12985ms step_avg:122.50ms
step:117/1393 train_time:13109ms step_avg:122.51ms
step:118/1393 train_time:13231ms step_avg:122.51ms
step:119/1393 train_time:13353ms step_avg:122.51ms
step:120/1393 train_time:13475ms step_avg:122.50ms
step:121/1393 train_time:13598ms step_avg:122.50ms
step:122/1393 train_time:13721ms step_avg:122.50ms
step:123/1393 train_time:13843ms step_avg:122.51ms
step:124/1393 train_time:13967ms step_avg:122.52ms
step:125/1393 train_time:14090ms step_avg:122.52ms
step:125/1393 val_loss:4.4087 train_time:14212ms step_avg:123.58ms
step:126/1393 train_time:14233ms step_avg:122.70ms
step:127/1393 train_time:14338ms step_avg:122.55ms
step:128/1393 train_time:14472ms step_avg:122.64ms
step:129/1393 train_time:14598ms step_avg:122.67ms
step:130/1393 train_time:14720ms step_avg:122.67ms
step:131/1393 train_time:14843ms step_avg:122.67ms
step:132/1393 train_time:14966ms step_avg:122.67ms
step:133/1393 train_time:15088ms step_avg:122.66ms
step:134/1393 train_time:15211ms step_avg:122.67ms
step:135/1393 train_time:15333ms step_avg:122.66ms
step:136/1393 train_time:15458ms step_avg:122.68ms
step:137/1393 train_time:15581ms step_avg:122.69ms
step:138/1393 train_time:15704ms step_avg:122.69ms
step:139/1393 train_time:15827ms step_avg:122.69ms
step:140/1393 train_time:15950ms step_avg:122.70ms
step:141/1393 train_time:16072ms step_avg:122.69ms
step:142/1393 train_time:16196ms step_avg:122.70ms
step:143/1393 train_time:16319ms step_avg:122.70ms
step:144/1393 train_time:16443ms step_avg:122.71ms
step:145/1393 train_time:16565ms step_avg:122.71ms
step:146/1393 train_time:16688ms step_avg:122.71ms
step:147/1393 train_time:16810ms step_avg:122.70ms
step:148/1393 train_time:16933ms step_avg:122.71ms
step:149/1393 train_time:17057ms step_avg:122.71ms
step:150/1393 train_time:17180ms step_avg:122.71ms
step:151/1393 train_time:17302ms step_avg:122.71ms
step:152/1393 train_time:17426ms step_avg:122.72ms
step:153/1393 train_time:17549ms step_avg:122.72ms
step:154/1393 train_time:17672ms step_avg:122.72ms
step:155/1393 train_time:17795ms step_avg:122.72ms
step:156/1393 train_time:17918ms step_avg:122.73ms
step:157/1393 train_time:18041ms step_avg:122.73ms
step:158/1393 train_time:18165ms step_avg:122.73ms
step:159/1393 train_time:18287ms step_avg:122.73ms
step:160/1393 train_time:18410ms step_avg:122.73ms
step:161/1393 train_time:18534ms step_avg:122.74ms
step:162/1393 train_time:18657ms step_avg:122.74ms
step:163/1393 train_time:18779ms step_avg:122.74ms
step:164/1393 train_time:18902ms step_avg:122.74ms
step:165/1393 train_time:19025ms step_avg:122.74ms
step:166/1393 train_time:19147ms step_avg:122.74ms
step:167/1393 train_time:19270ms step_avg:122.74ms
step:168/1393 train_time:19393ms step_avg:122.74ms
step:169/1393 train_time:19517ms step_avg:122.75ms
step:170/1393 train_time:19639ms step_avg:122.74ms
step:171/1393 train_time:19762ms step_avg:122.75ms
step:172/1393 train_time:19885ms step_avg:122.75ms
step:173/1393 train_time:20007ms step_avg:122.74ms
step:174/1393 train_time:20130ms step_avg:122.75ms
step:175/1393 train_time:20254ms step_avg:122.75ms
step:176/1393 train_time:20376ms step_avg:122.75ms
step:177/1393 train_time:20499ms step_avg:122.75ms
step:178/1393 train_time:20622ms step_avg:122.75ms
step:179/1393 train_time:20745ms step_avg:122.75ms
step:180/1393 train_time:20868ms step_avg:122.75ms
step:181/1393 train_time:20991ms step_avg:122.76ms
step:182/1393 train_time:21114ms step_avg:122.76ms
step:183/1393 train_time:21236ms step_avg:122.75ms
step:184/1393 train_time:21359ms step_avg:122.75ms
step:185/1393 train_time:21482ms step_avg:122.76ms
step:186/1393 train_time:21606ms step_avg:122.76ms
step:187/1393 train_time:21729ms step_avg:122.76ms
step:188/1393 train_time:21852ms step_avg:122.76ms
step:189/1393 train_time:21975ms step_avg:122.77ms
step:190/1393 train_time:22098ms step_avg:122.77ms
step:191/1393 train_time:22221ms step_avg:122.77ms
step:192/1393 train_time:22344ms step_avg:122.77ms
step:193/1393 train_time:22467ms step_avg:122.77ms
step:194/1393 train_time:22590ms step_avg:122.77ms
step:195/1393 train_time:22713ms step_avg:122.77ms
step:196/1393 train_time:22835ms step_avg:122.77ms
step:197/1393 train_time:22959ms step_avg:122.77ms
step:198/1393 train_time:23083ms step_avg:122.78ms
step:199/1393 train_time:23207ms step_avg:122.79ms
step:200/1393 train_time:23329ms step_avg:122.79ms
step:201/1393 train_time:23452ms step_avg:122.79ms
step:202/1393 train_time:23575ms step_avg:122.78ms
step:203/1393 train_time:23698ms step_avg:122.79ms
step:204/1393 train_time:23822ms step_avg:122.79ms
step:205/1393 train_time:23944ms step_avg:122.79ms
step:206/1393 train_time:24067ms step_avg:122.79ms
step:207/1393 train_time:24189ms step_avg:122.79ms
step:208/1393 train_time:24314ms step_avg:122.80ms
step:209/1393 train_time:24437ms step_avg:122.80ms
step:210/1393 train_time:24560ms step_avg:122.80ms
step:211/1393 train_time:24683ms step_avg:122.80ms
step:212/1393 train_time:24806ms step_avg:122.80ms
step:213/1393 train_time:24929ms step_avg:122.80ms
step:214/1393 train_time:25052ms step_avg:122.80ms
step:215/1393 train_time:25175ms step_avg:122.81ms
step:216/1393 train_time:25299ms step_avg:122.81ms
step:217/1393 train_time:25423ms step_avg:122.82ms
step:218/1393 train_time:25546ms step_avg:122.82ms
step:219/1393 train_time:25669ms step_avg:122.82ms
step:220/1393 train_time:25793ms step_avg:122.82ms
step:221/1393 train_time:25915ms step_avg:122.82ms
step:222/1393 train_time:26039ms step_avg:122.82ms
step:223/1393 train_time:26162ms step_avg:122.83ms
step:224/1393 train_time:26286ms step_avg:122.83ms
step:225/1393 train_time:26409ms step_avg:122.83ms
step:226/1393 train_time:26534ms step_avg:122.84ms
step:227/1393 train_time:26657ms step_avg:122.85ms
step:228/1393 train_time:26781ms step_avg:122.85ms
step:229/1393 train_time:26904ms step_avg:122.85ms
step:230/1393 train_time:27027ms step_avg:122.85ms
step:231/1393 train_time:27150ms step_avg:122.85ms
step:232/1393 train_time:27274ms step_avg:122.86ms
step:233/1393 train_time:27397ms step_avg:122.86ms
step:234/1393 train_time:27521ms step_avg:122.86ms
step:235/1393 train_time:27644ms step_avg:122.86ms
step:236/1393 train_time:27767ms step_avg:122.86ms
step:237/1393 train_time:27890ms step_avg:122.86ms
step:238/1393 train_time:28014ms step_avg:122.87ms
step:239/1393 train_time:28137ms step_avg:122.87ms
step:240/1393 train_time:28261ms step_avg:122.87ms
step:241/1393 train_time:28384ms step_avg:122.88ms
step:242/1393 train_time:28508ms step_avg:122.88ms
step:243/1393 train_time:28633ms step_avg:122.89ms
step:244/1393 train_time:28757ms step_avg:122.89ms
step:245/1393 train_time:28881ms step_avg:122.90ms
step:246/1393 train_time:29004ms step_avg:122.90ms
step:247/1393 train_time:29127ms step_avg:122.90ms
step:248/1393 train_time:29251ms step_avg:122.91ms
step:249/1393 train_time:29375ms step_avg:122.91ms
step:250/1393 train_time:29499ms step_avg:122.91ms
step:250/1393 val_loss:3.9852 train_time:29621ms step_avg:123.42ms
step:251/1393 train_time:29643ms step_avg:123.00ms
step:252/1393 train_time:29759ms step_avg:122.97ms
step:253/1393 train_time:29885ms step_avg:122.98ms
step:254/1393 train_time:30008ms step_avg:122.98ms
step:255/1393 train_time:30131ms step_avg:122.98ms
step:256/1393 train_time:30254ms step_avg:122.98ms
step:257/1393 train_time:30377ms step_avg:122.98ms
step:258/1393 train_time:30499ms step_avg:122.98ms
step:259/1393 train_time:30623ms step_avg:122.98ms
step:260/1393 train_time:30748ms step_avg:122.99ms
step:261/1393 train_time:30872ms step_avg:123.00ms
step:262/1393 train_time:30996ms step_avg:123.00ms
step:263/1393 train_time:31119ms step_avg:123.00ms
step:264/1393 train_time:31243ms step_avg:123.00ms
step:265/1393 train_time:31366ms step_avg:123.00ms
step:266/1393 train_time:31489ms step_avg:123.00ms
step:267/1393 train_time:31611ms step_avg:123.00ms
step:268/1393 train_time:31736ms step_avg:123.01ms
step:269/1393 train_time:31860ms step_avg:123.01ms
step:270/1393 train_time:31983ms step_avg:123.01ms
step:271/1393 train_time:32106ms step_avg:123.01ms
step:272/1393 train_time:32229ms step_avg:123.01ms
step:273/1393 train_time:32352ms step_avg:123.01ms
step:274/1393 train_time:32476ms step_avg:123.01ms
step:275/1393 train_time:32600ms step_avg:123.02ms
step:276/1393 train_time:32724ms step_avg:123.02ms
step:277/1393 train_time:32848ms step_avg:123.03ms
step:278/1393 train_time:32973ms step_avg:123.03ms
step:279/1393 train_time:33095ms step_avg:123.03ms
step:280/1393 train_time:33218ms step_avg:123.03ms
step:281/1393 train_time:33341ms step_avg:123.03ms
step:282/1393 train_time:33464ms step_avg:123.03ms
step:283/1393 train_time:33587ms step_avg:123.03ms
step:284/1393 train_time:33709ms step_avg:123.03ms
step:285/1393 train_time:33833ms step_avg:123.03ms
step:286/1393 train_time:33957ms step_avg:123.03ms
step:287/1393 train_time:34081ms step_avg:123.03ms
step:288/1393 train_time:34204ms step_avg:123.03ms
step:289/1393 train_time:34326ms step_avg:123.03ms
step:290/1393 train_time:34449ms step_avg:123.03ms
step:291/1393 train_time:34573ms step_avg:123.03ms
step:292/1393 train_time:34696ms step_avg:123.04ms
step:293/1393 train_time:34819ms step_avg:123.04ms
step:294/1393 train_time:34943ms step_avg:123.04ms
step:295/1393 train_time:35068ms step_avg:123.04ms
step:296/1393 train_time:35190ms step_avg:123.04ms
step:297/1393 train_time:35314ms step_avg:123.05ms
step:298/1393 train_time:35438ms step_avg:123.05ms
step:299/1393 train_time:35561ms step_avg:123.05ms
step:300/1393 train_time:35685ms step_avg:123.05ms
step:301/1393 train_time:35809ms step_avg:123.06ms
step:302/1393 train_time:35932ms step_avg:123.05ms
step:303/1393 train_time:36056ms step_avg:123.06ms
step:304/1393 train_time:36179ms step_avg:123.06ms
step:305/1393 train_time:36304ms step_avg:123.06ms
step:306/1393 train_time:36428ms step_avg:123.07ms
step:307/1393 train_time:36553ms step_avg:123.07ms
step:308/1393 train_time:36677ms step_avg:123.08ms
step:309/1393 train_time:36800ms step_avg:123.08ms
step:310/1393 train_time:36923ms step_avg:123.08ms
step:311/1393 train_time:37047ms step_avg:123.08ms
step:312/1393 train_time:37173ms step_avg:123.09ms
step:313/1393 train_time:37299ms step_avg:123.10ms
step:314/1393 train_time:37424ms step_avg:123.11ms
step:315/1393 train_time:37550ms step_avg:123.12ms
step:316/1393 train_time:37677ms step_avg:123.13ms
step:317/1393 train_time:37803ms step_avg:123.14ms
step:318/1393 train_time:37929ms step_avg:123.15ms
step:319/1393 train_time:38056ms step_avg:123.16ms
step:320/1393 train_time:38182ms step_avg:123.17ms
step:321/1393 train_time:38309ms step_avg:123.18ms
step:322/1393 train_time:38436ms step_avg:123.19ms
step:323/1393 train_time:38562ms step_avg:123.20ms
step:324/1393 train_time:38690ms step_avg:123.22ms
step:325/1393 train_time:38817ms step_avg:123.23ms
step:326/1393 train_time:38943ms step_avg:123.24ms
step:327/1393 train_time:39070ms step_avg:123.25ms
step:328/1393 train_time:39195ms step_avg:123.26ms
step:329/1393 train_time:39321ms step_avg:123.26ms
step:330/1393 train_time:39447ms step_avg:123.27ms
step:331/1393 train_time:39573ms step_avg:123.28ms
step:332/1393 train_time:39699ms step_avg:123.29ms
step:333/1393 train_time:39826ms step_avg:123.30ms
step:334/1393 train_time:39953ms step_avg:123.31ms
step:335/1393 train_time:40078ms step_avg:123.32ms
step:336/1393 train_time:40204ms step_avg:123.33ms
step:337/1393 train_time:40331ms step_avg:123.34ms
step:338/1393 train_time:40458ms step_avg:123.35ms
step:339/1393 train_time:40585ms step_avg:123.36ms
step:340/1393 train_time:40711ms step_avg:123.37ms
step:341/1393 train_time:40837ms step_avg:123.38ms
step:342/1393 train_time:40963ms step_avg:123.38ms
step:343/1393 train_time:41090ms step_avg:123.39ms
step:344/1393 train_time:41216ms step_avg:123.40ms
step:345/1393 train_time:41342ms step_avg:123.41ms
step:346/1393 train_time:41470ms step_avg:123.42ms
step:347/1393 train_time:41595ms step_avg:123.43ms
step:348/1393 train_time:41721ms step_avg:123.43ms
step:349/1393 train_time:41849ms step_avg:123.45ms
step:350/1393 train_time:41975ms step_avg:123.45ms
step:351/1393 train_time:42101ms step_avg:123.46ms
step:352/1393 train_time:42227ms step_avg:123.47ms
step:353/1393 train_time:42353ms step_avg:123.48ms
step:354/1393 train_time:42479ms step_avg:123.48ms
step:355/1393 train_time:42605ms step_avg:123.49ms
step:356/1393 train_time:42731ms step_avg:123.50ms
step:357/1393 train_time:42858ms step_avg:123.51ms
step:358/1393 train_time:42983ms step_avg:123.52ms
step:359/1393 train_time:43111ms step_avg:123.53ms
step:360/1393 train_time:43236ms step_avg:123.53ms
step:361/1393 train_time:43363ms step_avg:123.54ms
step:362/1393 train_time:43490ms step_avg:123.55ms
step:363/1393 train_time:43615ms step_avg:123.56ms
step:364/1393 train_time:43741ms step_avg:123.56ms
step:365/1393 train_time:43867ms step_avg:123.57ms
step:366/1393 train_time:43993ms step_avg:123.57ms
step:367/1393 train_time:44119ms step_avg:123.58ms
step:368/1393 train_time:44246ms step_avg:123.59ms
step:369/1393 train_time:44372ms step_avg:123.60ms
step:370/1393 train_time:44498ms step_avg:123.60ms
step:371/1393 train_time:44623ms step_avg:123.61ms
step:372/1393 train_time:44749ms step_avg:123.62ms
step:373/1393 train_time:44876ms step_avg:123.62ms
step:374/1393 train_time:45001ms step_avg:123.63ms
step:375/1393 train_time:45127ms step_avg:123.64ms
step:375/1393 val_loss:3.7829 train_time:45251ms step_avg:123.98ms
step:376/1393 train_time:45273ms step_avg:123.70ms
step:377/1393 train_time:45390ms step_avg:123.68ms
step:378/1393 train_time:45516ms step_avg:123.69ms
step:379/1393 train_time:45642ms step_avg:123.69ms
step:380/1393 train_time:45768ms step_avg:123.70ms
step:381/1393 train_time:45894ms step_avg:123.70ms
step:382/1393 train_time:46019ms step_avg:123.71ms
step:383/1393 train_time:46144ms step_avg:123.71ms
step:384/1393 train_time:46270ms step_avg:123.72ms
step:385/1393 train_time:46397ms step_avg:123.73ms
step:386/1393 train_time:46525ms step_avg:123.74ms
step:387/1393 train_time:46650ms step_avg:123.74ms
step:388/1393 train_time:46776ms step_avg:123.75ms
step:389/1393 train_time:46902ms step_avg:123.75ms
step:390/1393 train_time:47029ms step_avg:123.76ms
step:391/1393 train_time:47155ms step_avg:123.77ms
step:392/1393 train_time:47281ms step_avg:123.77ms
step:393/1393 train_time:47408ms step_avg:123.78ms
step:394/1393 train_time:47535ms step_avg:123.79ms
step:395/1393 train_time:47661ms step_avg:123.79ms
step:396/1393 train_time:47786ms step_avg:123.80ms
step:397/1393 train_time:47911ms step_avg:123.80ms
step:398/1393 train_time:48037ms step_avg:123.81ms
step:399/1393 train_time:48163ms step_avg:123.81ms
step:400/1393 train_time:48290ms step_avg:123.82ms
step:401/1393 train_time:48416ms step_avg:123.83ms
step:402/1393 train_time:48542ms step_avg:123.83ms
step:403/1393 train_time:48669ms step_avg:123.84ms
step:404/1393 train_time:48796ms step_avg:123.85ms
step:405/1393 train_time:48921ms step_avg:123.85ms
step:406/1393 train_time:49048ms step_avg:123.86ms
step:407/1393 train_time:49174ms step_avg:123.86ms
step:408/1393 train_time:49300ms step_avg:123.87ms
step:409/1393 train_time:49426ms step_avg:123.87ms
step:410/1393 train_time:49551ms step_avg:123.88ms
step:411/1393 train_time:49677ms step_avg:123.88ms
step:412/1393 train_time:49804ms step_avg:123.89ms
step:413/1393 train_time:49930ms step_avg:123.90ms
step:414/1393 train_time:50056ms step_avg:123.90ms
step:415/1393 train_time:50182ms step_avg:123.91ms
step:416/1393 train_time:50309ms step_avg:123.91ms
step:417/1393 train_time:50436ms step_avg:123.92ms
step:418/1393 train_time:50561ms step_avg:123.92ms
step:419/1393 train_time:50688ms step_avg:123.93ms
step:420/1393 train_time:50815ms step_avg:123.94ms
step:421/1393 train_time:50941ms step_avg:123.95ms
step:422/1393 train_time:51070ms step_avg:123.96ms
step:423/1393 train_time:51196ms step_avg:123.96ms
step:424/1393 train_time:51322ms step_avg:123.97ms
step:425/1393 train_time:51448ms step_avg:123.97ms
step:426/1393 train_time:51575ms step_avg:123.98ms
step:427/1393 train_time:51701ms step_avg:123.98ms
step:428/1393 train_time:51827ms step_avg:123.99ms
step:429/1393 train_time:51953ms step_avg:123.99ms
step:430/1393 train_time:52080ms step_avg:124.00ms
step:431/1393 train_time:52208ms step_avg:124.01ms
step:432/1393 train_time:52336ms step_avg:124.02ms
step:433/1393 train_time:52462ms step_avg:124.02ms
step:434/1393 train_time:52588ms step_avg:124.03ms
step:435/1393 train_time:52715ms step_avg:124.04ms
step:436/1393 train_time:52842ms step_avg:124.04ms
step:437/1393 train_time:52968ms step_avg:124.05ms
step:438/1393 train_time:53094ms step_avg:124.05ms
step:439/1393 train_time:53220ms step_avg:124.06ms
step:440/1393 train_time:53346ms step_avg:124.06ms
step:441/1393 train_time:53473ms step_avg:124.07ms
step:442/1393 train_time:53600ms step_avg:124.07ms
step:443/1393 train_time:53727ms step_avg:124.08ms
step:444/1393 train_time:53854ms step_avg:124.09ms
step:445/1393 train_time:53979ms step_avg:124.09ms
step:446/1393 train_time:54106ms step_avg:124.10ms
step:447/1393 train_time:54232ms step_avg:124.10ms
step:448/1393 train_time:54358ms step_avg:124.11ms
step:449/1393 train_time:54484ms step_avg:124.11ms
step:450/1393 train_time:54611ms step_avg:124.12ms
step:451/1393 train_time:54738ms step_avg:124.12ms
step:452/1393 train_time:54865ms step_avg:124.13ms
step:453/1393 train_time:54991ms step_avg:124.13ms
step:454/1393 train_time:55117ms step_avg:124.14ms
step:455/1393 train_time:55244ms step_avg:124.14ms
step:456/1393 train_time:55370ms step_avg:124.15ms
step:457/1393 train_time:55497ms step_avg:124.15ms
step:458/1393 train_time:55622ms step_avg:124.16ms
step:459/1393 train_time:55748ms step_avg:124.16ms
step:460/1393 train_time:55875ms step_avg:124.17ms
step:461/1393 train_time:56001ms step_avg:124.17ms
step:462/1393 train_time:56128ms step_avg:124.18ms
step:463/1393 train_time:56254ms step_avg:124.18ms
step:464/1393 train_time:56380ms step_avg:124.19ms
step:465/1393 train_time:56507ms step_avg:124.19ms
step:466/1393 train_time:56634ms step_avg:124.20ms
step:467/1393 train_time:56760ms step_avg:124.20ms
step:468/1393 train_time:56886ms step_avg:124.20ms
step:469/1393 train_time:57013ms step_avg:124.21ms
step:470/1393 train_time:57140ms step_avg:124.22ms
step:471/1393 train_time:57266ms step_avg:124.22ms
step:472/1393 train_time:57393ms step_avg:124.23ms
step:473/1393 train_time:57519ms step_avg:124.23ms
step:474/1393 train_time:57644ms step_avg:124.23ms
step:475/1393 train_time:57770ms step_avg:124.24ms
step:476/1393 train_time:57896ms step_avg:124.24ms
step:477/1393 train_time:58023ms step_avg:124.25ms
step:478/1393 train_time:58150ms step_avg:124.25ms
step:479/1393 train_time:58276ms step_avg:124.26ms
step:480/1393 train_time:58402ms step_avg:124.26ms
step:481/1393 train_time:58529ms step_avg:124.27ms
step:482/1393 train_time:58657ms step_avg:124.27ms
step:483/1393 train_time:58782ms step_avg:124.28ms
step:484/1393 train_time:58909ms step_avg:124.28ms
step:485/1393 train_time:59035ms step_avg:124.28ms
step:486/1393 train_time:59162ms step_avg:124.29ms
step:487/1393 train_time:59289ms step_avg:124.29ms
step:488/1393 train_time:59415ms step_avg:124.30ms
step:489/1393 train_time:59541ms step_avg:124.30ms
step:490/1393 train_time:59668ms step_avg:124.31ms
step:491/1393 train_time:59794ms step_avg:124.31ms
step:492/1393 train_time:59920ms step_avg:124.32ms
step:493/1393 train_time:60046ms step_avg:124.32ms
step:494/1393 train_time:60172ms step_avg:124.32ms
step:495/1393 train_time:60299ms step_avg:124.33ms
step:496/1393 train_time:60426ms step_avg:124.33ms
step:497/1393 train_time:60553ms step_avg:124.34ms
step:498/1393 train_time:60680ms step_avg:124.34ms
step:499/1393 train_time:60806ms step_avg:124.35ms
step:500/1393 train_time:60932ms step_avg:124.35ms
step:500/1393 val_loss:3.6633 train_time:61056ms step_avg:124.60ms
step:501/1393 train_time:61078ms step_avg:124.39ms
step:502/1393 train_time:61195ms step_avg:124.38ms
step:503/1393 train_time:61323ms step_avg:124.39ms
step:504/1393 train_time:61449ms step_avg:124.39ms
step:505/1393 train_time:61575ms step_avg:124.39ms
step:506/1393 train_time:61701ms step_avg:124.40ms
step:507/1393 train_time:61827ms step_avg:124.40ms
step:508/1393 train_time:61953ms step_avg:124.40ms
step:509/1393 train_time:62079ms step_avg:124.41ms
step:510/1393 train_time:62207ms step_avg:124.41ms
step:511/1393 train_time:62334ms step_avg:124.42ms
step:512/1393 train_time:62461ms step_avg:124.42ms
step:513/1393 train_time:62587ms step_avg:124.43ms
step:514/1393 train_time:62714ms step_avg:124.43ms
step:515/1393 train_time:62840ms step_avg:124.44ms
step:516/1393 train_time:62966ms step_avg:124.44ms
step:517/1393 train_time:63093ms step_avg:124.44ms
step:518/1393 train_time:63222ms step_avg:124.45ms
step:519/1393 train_time:63350ms step_avg:124.46ms
step:520/1393 train_time:63478ms step_avg:124.47ms
step:521/1393 train_time:63607ms step_avg:124.48ms
step:522/1393 train_time:63735ms step_avg:124.48ms
step:523/1393 train_time:63864ms step_avg:124.49ms
step:524/1393 train_time:63993ms step_avg:124.50ms
step:525/1393 train_time:64122ms step_avg:124.51ms
step:526/1393 train_time:64250ms step_avg:124.52ms
step:527/1393 train_time:64380ms step_avg:124.53ms
step:528/1393 train_time:64508ms step_avg:124.53ms
step:529/1393 train_time:64637ms step_avg:124.54ms
step:530/1393 train_time:64766ms step_avg:124.55ms
step:531/1393 train_time:64895ms step_avg:124.56ms
step:532/1393 train_time:65023ms step_avg:124.56ms
step:533/1393 train_time:65151ms step_avg:124.57ms
step:534/1393 train_time:65281ms step_avg:124.58ms
step:535/1393 train_time:65410ms step_avg:124.59ms
step:536/1393 train_time:65538ms step_avg:124.60ms
step:537/1393 train_time:65667ms step_avg:124.61ms
step:538/1393 train_time:65796ms step_avg:124.61ms
step:539/1393 train_time:65925ms step_avg:124.62ms
step:540/1393 train_time:66053ms step_avg:124.63ms
step:541/1393 train_time:66182ms step_avg:124.64ms
step:542/1393 train_time:66310ms step_avg:124.64ms
step:543/1393 train_time:66440ms step_avg:124.65ms
step:544/1393 train_time:66568ms step_avg:124.66ms
step:545/1393 train_time:66696ms step_avg:124.67ms
step:546/1393 train_time:66825ms step_avg:124.67ms
step:547/1393 train_time:66953ms step_avg:124.68ms
step:548/1393 train_time:67083ms step_avg:124.69ms
step:549/1393 train_time:67211ms step_avg:124.70ms
step:550/1393 train_time:67340ms step_avg:124.70ms
step:551/1393 train_time:67470ms step_avg:124.71ms
step:552/1393 train_time:67598ms step_avg:124.72ms
step:553/1393 train_time:67726ms step_avg:124.73ms
step:554/1393 train_time:67855ms step_avg:124.73ms
step:555/1393 train_time:67984ms step_avg:124.74ms
step:556/1393 train_time:68112ms step_avg:124.75ms
step:557/1393 train_time:68241ms step_avg:124.76ms
step:558/1393 train_time:68370ms step_avg:124.76ms
step:559/1393 train_time:68499ms step_avg:124.77ms
step:560/1393 train_time:68628ms step_avg:124.78ms
step:561/1393 train_time:68756ms step_avg:124.78ms
step:562/1393 train_time:68885ms step_avg:124.79ms
step:563/1393 train_time:69014ms step_avg:124.80ms
step:564/1393 train_time:69142ms step_avg:124.80ms
step:565/1393 train_time:69270ms step_avg:124.81ms
step:566/1393 train_time:69398ms step_avg:124.82ms
step:567/1393 train_time:69527ms step_avg:124.82ms
step:568/1393 train_time:69656ms step_avg:124.83ms
step:569/1393 train_time:69786ms step_avg:124.84ms
step:570/1393 train_time:69914ms step_avg:124.85ms
step:571/1393 train_time:70044ms step_avg:124.85ms
step:572/1393 train_time:70171ms step_avg:124.86ms
step:573/1393 train_time:70299ms step_avg:124.87ms
step:574/1393 train_time:70428ms step_avg:124.87ms
step:575/1393 train_time:70557ms step_avg:124.88ms
step:576/1393 train_time:70686ms step_avg:124.89ms
step:577/1393 train_time:70815ms step_avg:124.89ms
step:578/1393 train_time:70943ms step_avg:124.90ms
step:579/1393 train_time:71072ms step_avg:124.91ms
step:580/1393 train_time:71201ms step_avg:124.91ms
step:581/1393 train_time:71329ms step_avg:124.92ms
step:582/1393 train_time:71458ms step_avg:124.93ms
step:583/1393 train_time:71587ms step_avg:124.93ms
step:584/1393 train_time:71715ms step_avg:124.94ms
step:585/1393 train_time:71843ms step_avg:124.95ms
step:586/1393 train_time:71972ms step_avg:124.95ms
step:587/1393 train_time:72101ms step_avg:124.96ms
step:588/1393 train_time:72229ms step_avg:124.96ms
step:589/1393 train_time:72358ms step_avg:124.97ms
step:590/1393 train_time:72487ms step_avg:124.98ms
step:591/1393 train_time:72615ms step_avg:124.98ms
step:592/1393 train_time:72744ms step_avg:124.99ms
step:593/1393 train_time:72873ms step_avg:125.00ms
step:594/1393 train_time:73001ms step_avg:125.00ms
step:595/1393 train_time:73131ms step_avg:125.01ms
step:596/1393 train_time:73260ms step_avg:125.02ms
step:597/1393 train_time:73389ms step_avg:125.02ms
step:598/1393 train_time:73517ms step_avg:125.03ms
step:599/1393 train_time:73647ms step_avg:125.04ms
step:600/1393 train_time:73776ms step_avg:125.04ms
step:601/1393 train_time:73904ms step_avg:125.05ms
step:602/1393 train_time:74033ms step_avg:125.06ms
step:603/1393 train_time:74161ms step_avg:125.06ms
step:604/1393 train_time:74290ms step_avg:125.07ms
step:605/1393 train_time:74418ms step_avg:125.07ms
step:606/1393 train_time:74546ms step_avg:125.08ms
step:607/1393 train_time:74675ms step_avg:125.08ms
step:608/1393 train_time:74804ms step_avg:125.09ms
step:609/1393 train_time:74932ms step_avg:125.09ms
step:610/1393 train_time:75060ms step_avg:125.10ms
step:611/1393 train_time:75189ms step_avg:125.11ms
step:612/1393 train_time:75317ms step_avg:125.11ms
step:613/1393 train_time:75446ms step_avg:125.12ms
step:614/1393 train_time:75575ms step_avg:125.12ms
step:615/1393 train_time:75703ms step_avg:125.13ms
step:616/1393 train_time:75832ms step_avg:125.14ms
step:617/1393 train_time:75961ms step_avg:125.14ms
step:618/1393 train_time:76091ms step_avg:125.15ms
step:619/1393 train_time:76220ms step_avg:125.16ms
step:620/1393 train_time:76348ms step_avg:125.16ms
step:621/1393 train_time:76476ms step_avg:125.17ms
step:622/1393 train_time:76604ms step_avg:125.17ms
step:623/1393 train_time:76734ms step_avg:125.18ms
step:624/1393 train_time:76863ms step_avg:125.18ms
step:625/1393 train_time:76991ms step_avg:125.19ms
step:625/1393 val_loss:3.5807 train_time:77120ms step_avg:125.40ms
step:626/1393 train_time:77141ms step_avg:125.23ms
step:627/1393 train_time:77260ms step_avg:125.22ms
step:628/1393 train_time:77389ms step_avg:125.23ms
step:629/1393 train_time:77518ms step_avg:125.23ms
step:630/1393 train_time:77646ms step_avg:125.24ms
step:631/1393 train_time:77774ms step_avg:125.24ms
step:632/1393 train_time:77902ms step_avg:125.24ms
step:633/1393 train_time:78030ms step_avg:125.25ms
step:634/1393 train_time:78160ms step_avg:125.26ms
step:635/1393 train_time:78291ms step_avg:125.26ms
step:636/1393 train_time:78420ms step_avg:125.27ms
step:637/1393 train_time:78550ms step_avg:125.28ms
step:638/1393 train_time:78678ms step_avg:125.28ms
step:639/1393 train_time:78806ms step_avg:125.29ms
step:640/1393 train_time:78934ms step_avg:125.29ms
step:641/1393 train_time:79062ms step_avg:125.30ms
step:642/1393 train_time:79191ms step_avg:125.30ms
step:643/1393 train_time:79320ms step_avg:125.31ms
step:644/1393 train_time:79450ms step_avg:125.32ms
step:645/1393 train_time:79579ms step_avg:125.32ms
step:646/1393 train_time:79709ms step_avg:125.33ms
step:647/1393 train_time:79838ms step_avg:125.33ms
step:648/1393 train_time:79967ms step_avg:125.34ms
step:649/1393 train_time:80095ms step_avg:125.34ms
step:650/1393 train_time:80226ms step_avg:125.35ms
step:651/1393 train_time:80355ms step_avg:125.36ms
step:652/1393 train_time:80485ms step_avg:125.37ms
step:653/1393 train_time:80614ms step_avg:125.37ms
step:654/1393 train_time:80743ms step_avg:125.38ms
step:655/1393 train_time:80872ms step_avg:125.38ms
step:656/1393 train_time:81001ms step_avg:125.39ms
step:657/1393 train_time:81129ms step_avg:125.39ms
step:658/1393 train_time:81257ms step_avg:125.40ms
step:659/1393 train_time:81386ms step_avg:125.40ms
step:660/1393 train_time:81515ms step_avg:125.41ms
step:661/1393 train_time:81644ms step_avg:125.41ms
step:662/1393 train_time:81774ms step_avg:125.42ms
step:663/1393 train_time:81902ms step_avg:125.42ms
step:664/1393 train_time:82031ms step_avg:125.43ms
step:665/1393 train_time:82160ms step_avg:125.44ms
step:666/1393 train_time:82288ms step_avg:125.44ms
step:667/1393 train_time:82418ms step_avg:125.45ms
step:668/1393 train_time:82548ms step_avg:125.45ms
step:669/1393 train_time:82676ms step_avg:125.46ms
step:670/1393 train_time:82805ms step_avg:125.46ms
step:671/1393 train_time:82934ms step_avg:125.47ms
step:672/1393 train_time:83064ms step_avg:125.47ms
step:673/1393 train_time:83193ms step_avg:125.48ms
step:674/1393 train_time:83322ms step_avg:125.48ms
step:675/1393 train_time:83452ms step_avg:125.49ms
step:676/1393 train_time:83580ms step_avg:125.50ms
step:677/1393 train_time:83709ms step_avg:125.50ms
step:678/1393 train_time:83837ms step_avg:125.50ms
step:679/1393 train_time:83966ms step_avg:125.51ms
step:680/1393 train_time:84095ms step_avg:125.52ms
step:681/1393 train_time:84224ms step_avg:125.52ms
step:682/1393 train_time:84354ms step_avg:125.53ms
step:683/1393 train_time:84482ms step_avg:125.53ms
step:684/1393 train_time:84611ms step_avg:125.54ms
step:685/1393 train_time:84740ms step_avg:125.54ms
step:686/1393 train_time:84869ms step_avg:125.55ms
step:687/1393 train_time:85000ms step_avg:125.55ms
step:688/1393 train_time:85128ms step_avg:125.56ms
step:689/1393 train_time:85257ms step_avg:125.56ms
step:690/1393 train_time:85387ms step_avg:125.57ms
step:691/1393 train_time:85515ms step_avg:125.57ms
step:692/1393 train_time:85644ms step_avg:125.58ms
step:693/1393 train_time:85773ms step_avg:125.58ms
step:694/1393 train_time:85901ms step_avg:125.59ms
step:695/1393 train_time:86029ms step_avg:125.59ms
step:696/1393 train_time:86158ms step_avg:125.59ms
step:697/1393 train_time:86288ms step_avg:125.60ms
step:698/1393 train_time:86417ms step_avg:125.61ms
step:699/1393 train_time:86545ms step_avg:125.61ms
step:700/1393 train_time:86675ms step_avg:125.62ms
step:701/1393 train_time:86804ms step_avg:125.62ms
step:702/1393 train_time:86932ms step_avg:125.62ms
step:703/1393 train_time:87061ms step_avg:125.63ms
step:704/1393 train_time:87190ms step_avg:125.63ms
step:705/1393 train_time:87318ms step_avg:125.64ms
step:706/1393 train_time:87448ms step_avg:125.64ms
step:707/1393 train_time:87577ms step_avg:125.65ms
step:708/1393 train_time:87706ms step_avg:125.65ms
step:709/1393 train_time:87836ms step_avg:125.66ms
step:710/1393 train_time:87965ms step_avg:125.66ms
step:711/1393 train_time:88094ms step_avg:125.67ms
step:712/1393 train_time:88223ms step_avg:125.67ms
step:713/1393 train_time:88351ms step_avg:125.68ms
step:714/1393 train_time:88481ms step_avg:125.68ms
step:715/1393 train_time:88611ms step_avg:125.69ms
step:716/1393 train_time:88740ms step_avg:125.69ms
step:717/1393 train_time:88869ms step_avg:125.70ms
step:718/1393 train_time:88998ms step_avg:125.70ms
step:719/1393 train_time:89128ms step_avg:125.71ms
step:720/1393 train_time:89257ms step_avg:125.71ms
step:721/1393 train_time:89385ms step_avg:125.72ms
step:722/1393 train_time:89516ms step_avg:125.72ms
step:723/1393 train_time:89644ms step_avg:125.73ms
step:724/1393 train_time:89773ms step_avg:125.73ms
step:725/1393 train_time:89904ms step_avg:125.74ms
step:726/1393 train_time:90035ms step_avg:125.75ms
step:727/1393 train_time:90165ms step_avg:125.75ms
step:728/1393 train_time:90296ms step_avg:125.76ms
step:729/1393 train_time:90427ms step_avg:125.77ms
step:730/1393 train_time:90558ms step_avg:125.78ms
step:731/1393 train_time:90690ms step_avg:125.78ms
step:732/1393 train_time:90820ms step_avg:125.79ms
step:733/1393 train_time:90951ms step_avg:125.80ms
step:734/1393 train_time:91081ms step_avg:125.80ms
step:735/1393 train_time:91212ms step_avg:125.81ms
step:736/1393 train_time:91342ms step_avg:125.82ms
step:737/1393 train_time:91474ms step_avg:125.82ms
step:738/1393 train_time:91604ms step_avg:125.83ms
step:739/1393 train_time:91735ms step_avg:125.84ms
step:740/1393 train_time:91865ms step_avg:125.84ms
step:741/1393 train_time:92000ms step_avg:125.85ms
step:742/1393 train_time:92130ms step_avg:125.86ms
step:743/1393 train_time:92260ms step_avg:125.87ms
step:744/1393 train_time:92391ms step_avg:125.87ms
step:745/1393 train_time:92522ms step_avg:125.88ms
step:746/1393 train_time:92654ms step_avg:125.89ms
step:747/1393 train_time:92785ms step_avg:125.90ms
step:748/1393 train_time:92916ms step_avg:125.90ms
step:749/1393 train_time:93047ms step_avg:125.91ms
step:750/1393 train_time:93178ms step_avg:125.92ms
step:750/1393 val_loss:3.5238 train_time:93308ms step_avg:126.09ms
step:751/1393 train_time:93329ms step_avg:125.95ms
step:752/1393 train_time:93448ms step_avg:125.94ms
step:753/1393 train_time:93579ms step_avg:125.95ms
step:754/1393 train_time:93710ms step_avg:125.95ms
step:755/1393 train_time:93840ms step_avg:125.96ms
step:756/1393 train_time:93970ms step_avg:125.96ms
step:757/1393 train_time:94101ms step_avg:125.97ms
step:758/1393 train_time:94232ms step_avg:125.98ms
step:759/1393 train_time:94363ms step_avg:125.99ms
step:760/1393 train_time:94495ms step_avg:125.99ms
step:761/1393 train_time:94626ms step_avg:126.00ms
step:762/1393 train_time:94756ms step_avg:126.01ms
step:763/1393 train_time:94887ms step_avg:126.01ms
step:764/1393 train_time:95018ms step_avg:126.02ms
step:765/1393 train_time:95149ms step_avg:126.03ms
step:766/1393 train_time:95280ms step_avg:126.03ms
step:767/1393 train_time:95413ms step_avg:126.04ms
step:768/1393 train_time:95544ms step_avg:126.05ms
step:769/1393 train_time:95675ms step_avg:126.05ms
step:770/1393 train_time:95805ms step_avg:126.06ms
step:771/1393 train_time:95937ms step_avg:126.07ms
step:772/1393 train_time:96067ms step_avg:126.07ms
step:773/1393 train_time:96198ms step_avg:126.08ms
step:774/1393 train_time:96328ms step_avg:126.08ms
step:775/1393 train_time:96460ms step_avg:126.09ms
step:776/1393 train_time:96590ms step_avg:126.10ms
step:777/1393 train_time:96722ms step_avg:126.10ms
step:778/1393 train_time:96853ms step_avg:126.11ms
step:779/1393 train_time:96984ms step_avg:126.12ms
step:780/1393 train_time:97114ms step_avg:126.12ms
step:781/1393 train_time:97245ms step_avg:126.13ms
step:782/1393 train_time:97375ms step_avg:126.13ms
step:783/1393 train_time:97506ms step_avg:126.14ms
step:784/1393 train_time:97637ms step_avg:126.15ms
step:785/1393 train_time:97768ms step_avg:126.15ms
step:786/1393 train_time:97899ms step_avg:126.16ms
step:787/1393 train_time:98030ms step_avg:126.16ms
step:788/1393 train_time:98160ms step_avg:126.17ms
step:789/1393 train_time:98291ms step_avg:126.18ms
step:790/1393 train_time:98421ms step_avg:126.18ms
step:791/1393 train_time:98551ms step_avg:126.19ms
step:792/1393 train_time:98681ms step_avg:126.19ms
step:793/1393 train_time:98812ms step_avg:126.20ms
step:794/1393 train_time:98944ms step_avg:126.20ms
step:795/1393 train_time:99076ms step_avg:126.21ms
step:796/1393 train_time:99207ms step_avg:126.22ms
step:797/1393 train_time:99339ms step_avg:126.23ms
step:798/1393 train_time:99471ms step_avg:126.23ms
step:799/1393 train_time:99603ms step_avg:126.24ms
step:800/1393 train_time:99735ms step_avg:126.25ms
step:801/1393 train_time:99865ms step_avg:126.25ms
step:802/1393 train_time:99996ms step_avg:126.26ms
step:803/1393 train_time:100126ms step_avg:126.26ms
step:804/1393 train_time:100256ms step_avg:126.27ms
step:805/1393 train_time:100387ms step_avg:126.27ms
step:806/1393 train_time:100519ms step_avg:126.28ms
step:807/1393 train_time:100650ms step_avg:126.29ms
step:808/1393 train_time:100781ms step_avg:126.29ms
step:809/1393 train_time:100911ms step_avg:126.30ms
step:810/1393 train_time:101043ms step_avg:126.30ms
step:811/1393 train_time:101174ms step_avg:126.31ms
step:812/1393 train_time:101305ms step_avg:126.32ms
step:813/1393 train_time:101437ms step_avg:126.32ms
step:814/1393 train_time:101568ms step_avg:126.33ms
step:815/1393 train_time:101699ms step_avg:126.33ms
step:816/1393 train_time:101830ms step_avg:126.34ms
step:817/1393 train_time:101961ms step_avg:126.35ms
step:818/1393 train_time:102092ms step_avg:126.35ms
step:819/1393 train_time:102223ms step_avg:126.36ms
step:820/1393 train_time:102354ms step_avg:126.36ms
step:821/1393 train_time:102485ms step_avg:126.37ms
step:822/1393 train_time:102615ms step_avg:126.37ms
step:823/1393 train_time:102746ms step_avg:126.38ms
step:824/1393 train_time:102876ms step_avg:126.38ms
step:825/1393 train_time:103006ms step_avg:126.39ms
step:826/1393 train_time:103137ms step_avg:126.39ms
step:827/1393 train_time:103269ms step_avg:126.40ms
step:828/1393 train_time:103401ms step_avg:126.41ms
step:829/1393 train_time:103533ms step_avg:126.41ms
step:830/1393 train_time:103664ms step_avg:126.42ms
step:831/1393 train_time:103794ms step_avg:126.42ms
step:832/1393 train_time:103925ms step_avg:126.43ms
step:833/1393 train_time:104055ms step_avg:126.43ms
step:834/1393 train_time:104186ms step_avg:126.44ms
step:835/1393 train_time:104317ms step_avg:126.45ms
step:836/1393 train_time:104448ms step_avg:126.45ms
step:837/1393 train_time:104580ms step_avg:126.46ms
step:838/1393 train_time:104711ms step_avg:126.46ms
step:839/1393 train_time:104842ms step_avg:126.47ms
step:840/1393 train_time:104973ms step_avg:126.47ms
step:841/1393 train_time:105105ms step_avg:126.48ms
step:842/1393 train_time:105235ms step_avg:126.48ms
step:843/1393 train_time:105366ms step_avg:126.49ms
step:844/1393 train_time:105497ms step_avg:126.49ms
step:845/1393 train_time:105627ms step_avg:126.50ms
step:846/1393 train_time:105758ms step_avg:126.51ms
step:847/1393 train_time:105890ms step_avg:126.51ms
step:848/1393 train_time:106021ms step_avg:126.52ms
step:849/1393 train_time:106152ms step_avg:126.52ms
step:850/1393 train_time:106282ms step_avg:126.53ms
step:851/1393 train_time:106414ms step_avg:126.53ms
step:852/1393 train_time:106544ms step_avg:126.54ms
step:853/1393 train_time:106675ms step_avg:126.54ms
step:854/1393 train_time:106805ms step_avg:126.55ms
step:855/1393 train_time:106936ms step_avg:126.55ms
step:856/1393 train_time:107066ms step_avg:126.56ms
step:857/1393 train_time:107198ms step_avg:126.56ms
step:858/1393 train_time:107328ms step_avg:126.57ms
step:859/1393 train_time:107461ms step_avg:126.57ms
step:860/1393 train_time:107593ms step_avg:126.58ms
step:861/1393 train_time:107723ms step_avg:126.58ms
step:862/1393 train_time:107855ms step_avg:126.59ms
step:863/1393 train_time:107987ms step_avg:126.60ms
step:864/1393 train_time:108119ms step_avg:126.60ms
step:865/1393 train_time:108249ms step_avg:126.61ms
step:866/1393 train_time:108382ms step_avg:126.61ms
step:867/1393 train_time:108514ms step_avg:126.62ms
step:868/1393 train_time:108644ms step_avg:126.62ms
step:869/1393 train_time:108775ms step_avg:126.63ms
step:870/1393 train_time:108907ms step_avg:126.64ms
step:871/1393 train_time:109038ms step_avg:126.64ms
step:872/1393 train_time:109168ms step_avg:126.65ms
step:873/1393 train_time:109299ms step_avg:126.65ms
step:874/1393 train_time:109429ms step_avg:126.65ms
step:875/1393 train_time:109561ms step_avg:126.66ms
step:875/1393 val_loss:3.4750 train_time:109689ms step_avg:126.81ms
step:876/1393 train_time:109711ms step_avg:126.69ms
step:877/1393 train_time:109829ms step_avg:126.68ms
step:878/1393 train_time:109960ms step_avg:126.68ms
step:879/1393 train_time:110091ms step_avg:126.69ms
step:880/1393 train_time:110221ms step_avg:126.69ms
step:881/1393 train_time:110353ms step_avg:126.70ms
step:882/1393 train_time:110484ms step_avg:126.70ms
step:883/1393 train_time:110614ms step_avg:126.71ms
step:884/1393 train_time:110747ms step_avg:126.71ms
step:885/1393 train_time:110879ms step_avg:126.72ms
step:886/1393 train_time:111010ms step_avg:126.72ms
step:887/1393 train_time:111141ms step_avg:126.73ms
step:888/1393 train_time:111272ms step_avg:126.73ms
step:889/1393 train_time:111405ms step_avg:126.74ms
step:890/1393 train_time:111535ms step_avg:126.74ms
step:891/1393 train_time:111666ms step_avg:126.75ms
step:892/1393 train_time:111797ms step_avg:126.75ms
step:893/1393 train_time:111928ms step_avg:126.76ms
step:894/1393 train_time:112059ms step_avg:126.76ms
step:895/1393 train_time:112189ms step_avg:126.77ms
step:896/1393 train_time:112320ms step_avg:126.77ms
step:897/1393 train_time:112452ms step_avg:126.78ms
step:898/1393 train_time:112584ms step_avg:126.78ms
step:899/1393 train_time:112715ms step_avg:126.79ms
step:900/1393 train_time:112848ms step_avg:126.80ms
step:901/1393 train_time:112979ms step_avg:126.80ms
step:902/1393 train_time:113110ms step_avg:126.80ms
step:903/1393 train_time:113240ms step_avg:126.81ms
step:904/1393 train_time:113371ms step_avg:126.81ms
step:905/1393 train_time:113501ms step_avg:126.82ms
step:906/1393 train_time:113632ms step_avg:126.82ms
step:907/1393 train_time:113764ms step_avg:126.83ms
step:908/1393 train_time:113896ms step_avg:126.83ms
step:909/1393 train_time:114028ms step_avg:126.84ms
step:910/1393 train_time:114160ms step_avg:126.84ms
step:911/1393 train_time:114291ms step_avg:126.85ms
step:912/1393 train_time:114422ms step_avg:126.85ms
step:913/1393 train_time:114552ms step_avg:126.86ms
step:914/1393 train_time:114683ms step_avg:126.86ms
step:915/1393 train_time:114814ms step_avg:126.87ms
step:916/1393 train_time:114946ms step_avg:126.87ms
step:917/1393 train_time:115077ms step_avg:126.88ms
step:918/1393 train_time:115207ms step_avg:126.88ms
step:919/1393 train_time:115340ms step_avg:126.89ms
step:920/1393 train_time:115472ms step_avg:126.89ms
step:921/1393 train_time:115603ms step_avg:126.90ms
step:922/1393 train_time:115735ms step_avg:126.90ms
step:923/1393 train_time:115866ms step_avg:126.91ms
step:924/1393 train_time:115998ms step_avg:126.91ms
step:925/1393 train_time:116129ms step_avg:126.92ms
step:926/1393 train_time:116259ms step_avg:126.92ms
step:927/1393 train_time:116390ms step_avg:126.93ms
step:928/1393 train_time:116521ms step_avg:126.93ms
step:929/1393 train_time:116653ms step_avg:126.93ms
step:930/1393 train_time:116784ms step_avg:126.94ms
step:931/1393 train_time:116918ms step_avg:126.95ms
step:932/1393 train_time:117050ms step_avg:126.95ms
step:933/1393 train_time:117185ms step_avg:126.96ms
step:934/1393 train_time:117317ms step_avg:126.97ms
step:935/1393 train_time:117451ms step_avg:126.97ms
step:936/1393 train_time:117583ms step_avg:126.98ms
step:937/1393 train_time:117717ms step_avg:126.99ms
step:938/1393 train_time:117851ms step_avg:126.99ms
step:939/1393 train_time:117984ms step_avg:127.00ms
step:940/1393 train_time:118118ms step_avg:127.01ms
step:941/1393 train_time:118251ms step_avg:127.02ms
step:942/1393 train_time:118384ms step_avg:127.02ms
step:943/1393 train_time:118517ms step_avg:127.03ms
step:944/1393 train_time:118652ms step_avg:127.04ms
step:945/1393 train_time:118786ms step_avg:127.04ms
step:946/1393 train_time:118917ms step_avg:127.05ms
step:947/1393 train_time:119051ms step_avg:127.06ms
step:948/1393 train_time:119184ms step_avg:127.06ms
step:949/1393 train_time:119316ms step_avg:127.07ms
step:950/1393 train_time:119449ms step_avg:127.07ms
step:951/1393 train_time:119583ms step_avg:127.08ms
step:952/1393 train_time:119716ms step_avg:127.09ms
step:953/1393 train_time:119849ms step_avg:127.09ms
step:954/1393 train_time:119983ms step_avg:127.10ms
step:955/1393 train_time:120115ms step_avg:127.11ms
step:956/1393 train_time:120249ms step_avg:127.11ms
step:957/1393 train_time:120382ms step_avg:127.12ms
step:958/1393 train_time:120515ms step_avg:127.13ms
step:959/1393 train_time:120648ms step_avg:127.13ms
step:960/1393 train_time:120780ms step_avg:127.14ms
step:961/1393 train_time:120913ms step_avg:127.14ms
step:962/1393 train_time:121047ms step_avg:127.15ms
step:963/1393 train_time:121182ms step_avg:127.16ms
step:964/1393 train_time:121315ms step_avg:127.16ms
step:965/1393 train_time:121447ms step_avg:127.17ms
step:966/1393 train_time:121579ms step_avg:127.17ms
step:967/1393 train_time:121712ms step_avg:127.18ms
step:968/1393 train_time:121844ms step_avg:127.19ms
step:969/1393 train_time:121977ms step_avg:127.19ms
step:970/1393 train_time:122109ms step_avg:127.20ms
step:971/1393 train_time:122242ms step_avg:127.20ms
step:972/1393 train_time:122375ms step_avg:127.21ms
step:973/1393 train_time:122507ms step_avg:127.21ms
step:974/1393 train_time:122640ms step_avg:127.22ms
step:975/1393 train_time:122772ms step_avg:127.22ms
step:976/1393 train_time:122904ms step_avg:127.23ms
step:977/1393 train_time:123037ms step_avg:127.24ms
step:978/1393 train_time:123169ms step_avg:127.24ms
step:979/1393 train_time:123302ms step_avg:127.25ms
step:980/1393 train_time:123434ms step_avg:127.25ms
step:981/1393 train_time:123565ms step_avg:127.26ms
step:982/1393 train_time:123697ms step_avg:127.26ms
step:983/1393 train_time:123829ms step_avg:127.27ms
step:984/1393 train_time:123961ms step_avg:127.27ms
step:985/1393 train_time:124095ms step_avg:127.28ms
step:986/1393 train_time:124230ms step_avg:127.28ms
step:987/1393 train_time:124363ms step_avg:127.29ms
step:988/1393 train_time:124496ms step_avg:127.30ms
step:989/1393 train_time:124628ms step_avg:127.30ms
step:990/1393 train_time:124761ms step_avg:127.31ms
step:991/1393 train_time:124894ms step_avg:127.31ms
step:992/1393 train_time:125028ms step_avg:127.32ms
step:993/1393 train_time:125163ms step_avg:127.33ms
step:994/1393 train_time:125295ms step_avg:127.33ms
step:995/1393 train_time:125429ms step_avg:127.34ms
step:996/1393 train_time:125561ms step_avg:127.34ms
step:997/1393 train_time:125693ms step_avg:127.35ms
step:998/1393 train_time:125825ms step_avg:127.35ms
step:999/1393 train_time:125958ms step_avg:127.36ms
step:1000/1393 train_time:126092ms step_avg:127.37ms
step:1000/1393 val_loss:3.4129 train_time:126222ms step_avg:127.50ms
step:1001/1393 train_time:126243ms step_avg:127.39ms
step:1002/1393 train_time:126363ms step_avg:127.38ms
step:1003/1393 train_time:126496ms step_avg:127.39ms
step:1004/1393 train_time:126629ms step_avg:127.39ms
step:1005/1393 train_time:126762ms step_avg:127.40ms
step:1006/1393 train_time:126893ms step_avg:127.40ms
step:1007/1393 train_time:127025ms step_avg:127.41ms
step:1008/1393 train_time:127158ms step_avg:127.41ms
step:1009/1393 train_time:127294ms step_avg:127.42ms
step:1010/1393 train_time:127427ms step_avg:127.43ms
step:1011/1393 train_time:127560ms step_avg:127.43ms
step:1012/1393 train_time:127693ms step_avg:127.44ms
step:1013/1393 train_time:127826ms step_avg:127.44ms
step:1014/1393 train_time:127958ms step_avg:127.45ms
step:1015/1393 train_time:128090ms step_avg:127.45ms
step:1016/1393 train_time:128223ms step_avg:127.46ms
step:1017/1393 train_time:128355ms step_avg:127.46ms
step:1018/1393 train_time:128487ms step_avg:127.47ms
step:1019/1393 train_time:128621ms step_avg:127.47ms
step:1020/1393 train_time:128754ms step_avg:127.48ms
step:1021/1393 train_time:128886ms step_avg:127.48ms
step:1022/1393 train_time:129019ms step_avg:127.49ms
step:1023/1393 train_time:129153ms step_avg:127.50ms
step:1024/1393 train_time:129284ms step_avg:127.50ms
step:1025/1393 train_time:129418ms step_avg:127.51ms
step:1026/1393 train_time:129551ms step_avg:127.51ms
step:1027/1393 train_time:129684ms step_avg:127.52ms
step:1028/1393 train_time:129816ms step_avg:127.52ms
step:1029/1393 train_time:129951ms step_avg:127.53ms
step:1030/1393 train_time:130083ms step_avg:127.53ms
step:1031/1393 train_time:130215ms step_avg:127.54ms
step:1032/1393 train_time:130348ms step_avg:127.54ms
step:1033/1393 train_time:130479ms step_avg:127.55ms
step:1034/1393 train_time:130613ms step_avg:127.55ms
step:1035/1393 train_time:130747ms step_avg:127.56ms
step:1036/1393 train_time:130879ms step_avg:127.56ms
step:1037/1393 train_time:131014ms step_avg:127.57ms
step:1038/1393 train_time:131147ms step_avg:127.58ms
step:1039/1393 train_time:131279ms step_avg:127.58ms
step:1040/1393 train_time:131412ms step_avg:127.58ms
step:1041/1393 train_time:131544ms step_avg:127.59ms
step:1042/1393 train_time:131677ms step_avg:127.59ms
step:1043/1393 train_time:131810ms step_avg:127.60ms
step:1044/1393 train_time:131942ms step_avg:127.60ms
step:1045/1393 train_time:132076ms step_avg:127.61ms
step:1046/1393 train_time:132208ms step_avg:127.61ms
step:1047/1393 train_time:132340ms step_avg:127.62ms
step:1048/1393 train_time:132473ms step_avg:127.62ms
step:1049/1393 train_time:132606ms step_avg:127.63ms
step:1050/1393 train_time:132739ms step_avg:127.63ms
step:1051/1393 train_time:132873ms step_avg:127.64ms
step:1052/1393 train_time:133005ms step_avg:127.64ms
step:1053/1393 train_time:133138ms step_avg:127.65ms
step:1054/1393 train_time:133271ms step_avg:127.65ms
step:1055/1393 train_time:133405ms step_avg:127.66ms
step:1056/1393 train_time:133537ms step_avg:127.66ms
step:1057/1393 train_time:133669ms step_avg:127.67ms
step:1058/1393 train_time:133802ms step_avg:127.67ms
step:1059/1393 train_time:133937ms step_avg:127.68ms
step:1060/1393 train_time:134070ms step_avg:127.69ms
step:1061/1393 train_time:134203ms step_avg:127.69ms
step:1062/1393 train_time:134338ms step_avg:127.70ms
step:1063/1393 train_time:134471ms step_avg:127.70ms
step:1064/1393 train_time:134604ms step_avg:127.71ms
step:1065/1393 train_time:134737ms step_avg:127.71ms
step:1066/1393 train_time:134871ms step_avg:127.72ms
step:1067/1393 train_time:135003ms step_avg:127.72ms
step:1068/1393 train_time:135135ms step_avg:127.73ms
step:1069/1393 train_time:135271ms step_avg:127.73ms
step:1070/1393 train_time:135404ms step_avg:127.74ms
step:1071/1393 train_time:135538ms step_avg:127.75ms
step:1072/1393 train_time:135670ms step_avg:127.75ms
step:1073/1393 train_time:135803ms step_avg:127.75ms
step:1074/1393 train_time:135937ms step_avg:127.76ms
step:1075/1393 train_time:136070ms step_avg:127.77ms
step:1076/1393 train_time:136203ms step_avg:127.77ms
step:1077/1393 train_time:136336ms step_avg:127.77ms
step:1078/1393 train_time:136469ms step_avg:127.78ms
step:1079/1393 train_time:136606ms step_avg:127.79ms
step:1080/1393 train_time:136739ms step_avg:127.79ms
step:1081/1393 train_time:136872ms step_avg:127.80ms
step:1082/1393 train_time:137004ms step_avg:127.80ms
step:1083/1393 train_time:137138ms step_avg:127.81ms
step:1084/1393 train_time:137272ms step_avg:127.81ms
step:1085/1393 train_time:137405ms step_avg:127.82ms
step:1086/1393 train_time:137537ms step_avg:127.82ms
step:1087/1393 train_time:137672ms step_avg:127.83ms
step:1088/1393 train_time:137804ms step_avg:127.83ms
step:1089/1393 train_time:137941ms step_avg:127.84ms
step:1090/1393 train_time:138076ms step_avg:127.85ms
step:1091/1393 train_time:138209ms step_avg:127.85ms
step:1092/1393 train_time:138342ms step_avg:127.86ms
step:1093/1393 train_time:138475ms step_avg:127.86ms
step:1094/1393 train_time:138608ms step_avg:127.87ms
step:1095/1393 train_time:138740ms step_avg:127.87ms
step:1096/1393 train_time:138874ms step_avg:127.88ms
step:1097/1393 train_time:139007ms step_avg:127.88ms
step:1098/1393 train_time:139140ms step_avg:127.89ms
step:1099/1393 train_time:139273ms step_avg:127.89ms
step:1100/1393 train_time:139405ms step_avg:127.89ms
step:1101/1393 train_time:139538ms step_avg:127.90ms
step:1102/1393 train_time:139671ms step_avg:127.90ms
step:1103/1393 train_time:139804ms step_avg:127.91ms
step:1104/1393 train_time:139938ms step_avg:127.91ms
step:1105/1393 train_time:140073ms step_avg:127.92ms
step:1106/1393 train_time:140207ms step_avg:127.93ms
step:1107/1393 train_time:140339ms step_avg:127.93ms
step:1108/1393 train_time:140475ms step_avg:127.94ms
step:1109/1393 train_time:140608ms step_avg:127.94ms
step:1110/1393 train_time:140741ms step_avg:127.95ms
step:1111/1393 train_time:140874ms step_avg:127.95ms
step:1112/1393 train_time:141007ms step_avg:127.96ms
step:1113/1393 train_time:141138ms step_avg:127.96ms
step:1114/1393 train_time:141272ms step_avg:127.96ms
step:1115/1393 train_time:141405ms step_avg:127.97ms
step:1116/1393 train_time:141538ms step_avg:127.97ms
step:1117/1393 train_time:141672ms step_avg:127.98ms
step:1118/1393 train_time:141807ms step_avg:127.98ms
step:1119/1393 train_time:141939ms step_avg:127.99ms
step:1120/1393 train_time:142071ms step_avg:127.99ms
step:1121/1393 train_time:142204ms step_avg:128.00ms
step:1122/1393 train_time:142337ms step_avg:128.00ms
step:1123/1393 train_time:142469ms step_avg:128.00ms
step:1124/1393 train_time:142602ms step_avg:128.01ms
step:1125/1393 train_time:142734ms step_avg:128.01ms
step:1125/1393 val_loss:3.3616 train_time:142867ms step_avg:128.13ms
step:1126/1393 train_time:142888ms step_avg:128.04ms
step:1127/1393 train_time:143009ms step_avg:128.03ms
step:1128/1393 train_time:143143ms step_avg:128.04ms
step:1129/1393 train_time:143276ms step_avg:128.04ms
step:1130/1393 train_time:143408ms step_avg:128.04ms
step:1131/1393 train_time:143541ms step_avg:128.05ms
step:1132/1393 train_time:143674ms step_avg:128.05ms
step:1133/1393 train_time:143805ms step_avg:128.05ms
step:1134/1393 train_time:143940ms step_avg:128.06ms
step:1135/1393 train_time:144073ms step_avg:128.06ms
step:1136/1393 train_time:144207ms step_avg:128.07ms
step:1137/1393 train_time:144340ms step_avg:128.07ms
step:1138/1393 train_time:144477ms step_avg:128.08ms
step:1139/1393 train_time:144610ms step_avg:128.09ms
step:1140/1393 train_time:144745ms step_avg:128.09ms
step:1141/1393 train_time:144879ms step_avg:128.10ms
step:1142/1393 train_time:145013ms step_avg:128.10ms
step:1143/1393 train_time:145149ms step_avg:128.11ms
step:1144/1393 train_time:145283ms step_avg:128.12ms
step:1145/1393 train_time:145418ms step_avg:128.12ms
step:1146/1393 train_time:145552ms step_avg:128.13ms
step:1147/1393 train_time:145687ms step_avg:128.13ms
step:1148/1393 train_time:145822ms step_avg:128.14ms
step:1149/1393 train_time:145955ms step_avg:128.14ms
step:1150/1393 train_time:146089ms step_avg:128.15ms
step:1151/1393 train_time:146224ms step_avg:128.15ms
step:1152/1393 train_time:146358ms step_avg:128.16ms
step:1153/1393 train_time:146495ms step_avg:128.17ms
step:1154/1393 train_time:146629ms step_avg:128.17ms
step:1155/1393 train_time:146764ms step_avg:128.18ms
step:1156/1393 train_time:146902ms step_avg:128.19ms
step:1157/1393 train_time:147037ms step_avg:128.19ms
step:1158/1393 train_time:147171ms step_avg:128.20ms
step:1159/1393 train_time:147305ms step_avg:128.20ms
step:1160/1393 train_time:147439ms step_avg:128.21ms
step:1161/1393 train_time:147573ms step_avg:128.21ms
step:1162/1393 train_time:147708ms step_avg:128.22ms
step:1163/1393 train_time:147843ms step_avg:128.22ms
step:1164/1393 train_time:147979ms step_avg:128.23ms
step:1165/1393 train_time:148114ms step_avg:128.24ms
step:1166/1393 train_time:148248ms step_avg:128.24ms
step:1167/1393 train_time:148382ms step_avg:128.25ms
step:1168/1393 train_time:148516ms step_avg:128.25ms
step:1169/1393 train_time:148650ms step_avg:128.26ms
step:1170/1393 train_time:148785ms step_avg:128.26ms
step:1171/1393 train_time:148919ms step_avg:128.27ms
step:1172/1393 train_time:149055ms step_avg:128.27ms
step:1173/1393 train_time:149188ms step_avg:128.28ms
step:1174/1393 train_time:149325ms step_avg:128.29ms
step:1175/1393 train_time:149459ms step_avg:128.29ms
step:1176/1393 train_time:149594ms step_avg:128.30ms
step:1177/1393 train_time:149730ms step_avg:128.30ms
step:1178/1393 train_time:149864ms step_avg:128.31ms
step:1179/1393 train_time:149998ms step_avg:128.31ms
step:1180/1393 train_time:150136ms step_avg:128.32ms
step:1181/1393 train_time:150272ms step_avg:128.33ms
step:1182/1393 train_time:150406ms step_avg:128.33ms
step:1183/1393 train_time:150541ms step_avg:128.34ms
step:1184/1393 train_time:150676ms step_avg:128.34ms
step:1185/1393 train_time:150810ms step_avg:128.35ms
step:1186/1393 train_time:150944ms step_avg:128.35ms
step:1187/1393 train_time:151083ms step_avg:128.36ms
step:1188/1393 train_time:151219ms step_avg:128.37ms
step:1189/1393 train_time:151355ms step_avg:128.38ms
step:1190/1393 train_time:151490ms step_avg:128.38ms
step:1191/1393 train_time:151623ms step_avg:128.39ms
step:1192/1393 train_time:151758ms step_avg:128.39ms
step:1193/1393 train_time:151892ms step_avg:128.40ms
step:1194/1393 train_time:152026ms step_avg:128.40ms
step:1195/1393 train_time:152160ms step_avg:128.40ms
step:1196/1393 train_time:152294ms step_avg:128.41ms
step:1197/1393 train_time:152429ms step_avg:128.42ms
step:1198/1393 train_time:152566ms step_avg:128.42ms
step:1199/1393 train_time:152700ms step_avg:128.43ms
step:1200/1393 train_time:152834ms step_avg:128.43ms
step:1201/1393 train_time:152967ms step_avg:128.44ms
step:1202/1393 train_time:153104ms step_avg:128.44ms
step:1203/1393 train_time:153240ms step_avg:128.45ms
step:1204/1393 train_time:153374ms step_avg:128.45ms
step:1205/1393 train_time:153510ms step_avg:128.46ms
step:1206/1393 train_time:153645ms step_avg:128.47ms
step:1207/1393 train_time:153779ms step_avg:128.47ms
step:1208/1393 train_time:153914ms step_avg:128.48ms
step:1209/1393 train_time:154050ms step_avg:128.48ms
step:1210/1393 train_time:154186ms step_avg:128.49ms
step:1211/1393 train_time:154321ms step_avg:128.49ms
step:1212/1393 train_time:154454ms step_avg:128.50ms
step:1213/1393 train_time:154588ms step_avg:128.50ms
step:1214/1393 train_time:154725ms step_avg:128.51ms
step:1215/1393 train_time:154860ms step_avg:128.51ms
step:1216/1393 train_time:154993ms step_avg:128.52ms
step:1217/1393 train_time:155128ms step_avg:128.52ms
step:1218/1393 train_time:155261ms step_avg:128.53ms
step:1219/1393 train_time:155394ms step_avg:128.53ms
step:1220/1393 train_time:155528ms step_avg:128.54ms
step:1221/1393 train_time:155662ms step_avg:128.54ms
step:1222/1393 train_time:155796ms step_avg:128.54ms
step:1223/1393 train_time:155929ms step_avg:128.55ms
step:1224/1393 train_time:156066ms step_avg:128.55ms
step:1225/1393 train_time:156205ms step_avg:128.56ms
step:1226/1393 train_time:156338ms step_avg:128.57ms
step:1227/1393 train_time:156473ms step_avg:128.57ms
step:1228/1393 train_time:156608ms step_avg:128.58ms
step:1229/1393 train_time:156740ms step_avg:128.58ms
step:1230/1393 train_time:156876ms step_avg:128.59ms
step:1231/1393 train_time:157012ms step_avg:128.59ms
step:1232/1393 train_time:157148ms step_avg:128.60ms
step:1233/1393 train_time:157283ms step_avg:128.60ms
step:1234/1393 train_time:157417ms step_avg:128.61ms
step:1235/1393 train_time:157552ms step_avg:128.61ms
step:1236/1393 train_time:157685ms step_avg:128.62ms
step:1237/1393 train_time:157819ms step_avg:128.62ms
step:1238/1393 train_time:157958ms step_avg:128.63ms
step:1239/1393 train_time:158092ms step_avg:128.63ms
step:1240/1393 train_time:158228ms step_avg:128.64ms
step:1241/1393 train_time:158366ms step_avg:128.65ms
step:1242/1393 train_time:158501ms step_avg:128.65ms
step:1243/1393 train_time:158636ms step_avg:128.66ms
step:1244/1393 train_time:158770ms step_avg:128.66ms
step:1245/1393 train_time:158904ms step_avg:128.67ms
step:1246/1393 train_time:159038ms step_avg:128.67ms
step:1247/1393 train_time:159173ms step_avg:128.68ms
step:1248/1393 train_time:159307ms step_avg:128.68ms
step:1249/1393 train_time:159441ms step_avg:128.68ms
step:1250/1393 train_time:159576ms step_avg:128.69ms
step:1250/1393 val_loss:3.3143 train_time:159709ms step_avg:128.80ms
step:1251/1393 train_time:159731ms step_avg:128.71ms
step:1252/1393 train_time:159853ms step_avg:128.71ms
step:1253/1393 train_time:159986ms step_avg:128.71ms
step:1254/1393 train_time:160119ms step_avg:128.71ms
step:1255/1393 train_time:160259ms step_avg:128.72ms
step:1256/1393 train_time:160392ms step_avg:128.73ms
step:1257/1393 train_time:160526ms step_avg:128.73ms
step:1258/1393 train_time:160660ms step_avg:128.73ms
step:1259/1393 train_time:160796ms step_avg:128.74ms
step:1260/1393 train_time:160931ms step_avg:128.74ms
step:1261/1393 train_time:161064ms step_avg:128.75ms
step:1262/1393 train_time:161201ms step_avg:128.75ms
step:1263/1393 train_time:161335ms step_avg:128.76ms
step:1264/1393 train_time:161469ms step_avg:128.76ms
step:1265/1393 train_time:161602ms step_avg:128.77ms
step:1266/1393 train_time:161737ms step_avg:128.77ms
step:1267/1393 train_time:161872ms step_avg:128.78ms
step:1268/1393 train_time:162007ms step_avg:128.78ms
step:1269/1393 train_time:162143ms step_avg:128.79ms
step:1270/1393 train_time:162278ms step_avg:128.79ms
step:1271/1393 train_time:162413ms step_avg:128.80ms
step:1272/1393 train_time:162547ms step_avg:128.80ms
step:1273/1393 train_time:162680ms step_avg:128.80ms
step:1274/1393 train_time:162814ms step_avg:128.81ms
step:1275/1393 train_time:162949ms step_avg:128.81ms
step:1276/1393 train_time:163084ms step_avg:128.82ms
step:1277/1393 train_time:163219ms step_avg:128.82ms
step:1278/1393 train_time:163352ms step_avg:128.83ms
step:1279/1393 train_time:163486ms step_avg:128.83ms
step:1280/1393 train_time:163624ms step_avg:128.84ms
step:1281/1393 train_time:163759ms step_avg:128.84ms
step:1282/1393 train_time:163892ms step_avg:128.85ms
step:1283/1393 train_time:164026ms step_avg:128.85ms
step:1284/1393 train_time:164161ms step_avg:128.85ms
step:1285/1393 train_time:164295ms step_avg:128.86ms
step:1286/1393 train_time:164430ms step_avg:128.86ms
step:1287/1393 train_time:164565ms step_avg:128.87ms
step:1288/1393 train_time:164699ms step_avg:128.87ms
step:1289/1393 train_time:164835ms step_avg:128.88ms
step:1290/1393 train_time:164972ms step_avg:128.88ms
step:1291/1393 train_time:165108ms step_avg:128.89ms
step:1292/1393 train_time:165243ms step_avg:128.89ms
step:1293/1393 train_time:165379ms step_avg:128.90ms
step:1294/1393 train_time:165513ms step_avg:128.90ms
step:1295/1393 train_time:165647ms step_avg:128.91ms
step:1296/1393 train_time:165783ms step_avg:128.91ms
step:1297/1393 train_time:165918ms step_avg:128.92ms
step:1298/1393 train_time:166051ms step_avg:128.92ms
step:1299/1393 train_time:166186ms step_avg:128.93ms
step:1300/1393 train_time:166321ms step_avg:128.93ms
step:1301/1393 train_time:166455ms step_avg:128.93ms
step:1302/1393 train_time:166589ms step_avg:128.94ms
step:1303/1393 train_time:166724ms step_avg:128.94ms
step:1304/1393 train_time:166859ms step_avg:128.95ms
step:1305/1393 train_time:166994ms step_avg:128.95ms
step:1306/1393 train_time:167128ms step_avg:128.96ms
step:1307/1393 train_time:167264ms step_avg:128.96ms
step:1308/1393 train_time:167400ms step_avg:128.97ms
step:1309/1393 train_time:167535ms step_avg:128.97ms
step:1310/1393 train_time:167671ms step_avg:128.98ms
step:1311/1393 train_time:167805ms step_avg:128.98ms
step:1312/1393 train_time:167939ms step_avg:128.99ms
step:1313/1393 train_time:168073ms step_avg:128.99ms
step:1314/1393 train_time:168207ms step_avg:128.99ms
step:1315/1393 train_time:168343ms step_avg:129.00ms
step:1316/1393 train_time:168476ms step_avg:129.00ms
step:1317/1393 train_time:168610ms step_avg:129.01ms
step:1318/1393 train_time:168746ms step_avg:129.01ms
step:1319/1393 train_time:168883ms step_avg:129.02ms
step:1320/1393 train_time:169017ms step_avg:129.02ms
step:1321/1393 train_time:169151ms step_avg:129.02ms
step:1322/1393 train_time:169288ms step_avg:129.03ms
step:1323/1393 train_time:169422ms step_avg:129.03ms
step:1324/1393 train_time:169555ms step_avg:129.04ms
step:1325/1393 train_time:169690ms step_avg:129.04ms
step:1326/1393 train_time:169826ms step_avg:129.05ms
step:1327/1393 train_time:169961ms step_avg:129.05ms
step:1328/1393 train_time:170095ms step_avg:129.06ms
step:1329/1393 train_time:170234ms step_avg:129.06ms
step:1330/1393 train_time:170369ms step_avg:129.07ms
step:1331/1393 train_time:170507ms step_avg:129.07ms
step:1332/1393 train_time:170643ms step_avg:129.08ms
step:1333/1393 train_time:170777ms step_avg:129.08ms
step:1334/1393 train_time:170912ms step_avg:129.09ms
step:1335/1393 train_time:171045ms step_avg:129.09ms
step:1336/1393 train_time:171183ms step_avg:129.10ms
step:1337/1393 train_time:171318ms step_avg:129.10ms
step:1338/1393 train_time:171452ms step_avg:129.11ms
step:1339/1393 train_time:171586ms step_avg:129.11ms
step:1340/1393 train_time:171722ms step_avg:129.11ms
step:1341/1393 train_time:171855ms step_avg:129.12ms
step:1342/1393 train_time:171989ms step_avg:129.12ms
step:1343/1393 train_time:172125ms step_avg:129.13ms
step:1344/1393 train_time:172261ms step_avg:129.13ms
step:1345/1393 train_time:172398ms step_avg:129.14ms
step:1346/1393 train_time:172533ms step_avg:129.14ms
step:1347/1393 train_time:172669ms step_avg:129.15ms
step:1348/1393 train_time:172803ms step_avg:129.15ms
step:1349/1393 train_time:172938ms step_avg:129.15ms
step:1350/1393 train_time:173072ms step_avg:129.16ms
step:1351/1393 train_time:173208ms step_avg:129.16ms
step:1352/1393 train_time:173347ms step_avg:129.17ms
step:1353/1393 train_time:173484ms step_avg:129.18ms
step:1354/1393 train_time:173619ms step_avg:129.18ms
step:1355/1393 train_time:173754ms step_avg:129.18ms
step:1356/1393 train_time:173887ms step_avg:129.19ms
step:1357/1393 train_time:174023ms step_avg:129.19ms
step:1358/1393 train_time:174161ms step_avg:129.20ms
step:1359/1393 train_time:174297ms step_avg:129.20ms
step:1360/1393 train_time:174434ms step_avg:129.21ms
step:1361/1393 train_time:174570ms step_avg:129.22ms
step:1362/1393 train_time:174709ms step_avg:129.22ms
step:1363/1393 train_time:174847ms step_avg:129.23ms
step:1364/1393 train_time:174983ms step_avg:129.23ms
step:1365/1393 train_time:175118ms step_avg:129.24ms
step:1366/1393 train_time:175253ms step_avg:129.24ms
step:1367/1393 train_time:175390ms step_avg:129.25ms
step:1368/1393 train_time:175526ms step_avg:129.25ms
step:1369/1393 train_time:175664ms step_avg:129.26ms
step:1370/1393 train_time:175802ms step_avg:129.27ms
step:1371/1393 train_time:175939ms step_avg:129.27ms
step:1372/1393 train_time:176077ms step_avg:129.28ms
step:1373/1393 train_time:176212ms step_avg:129.28ms
step:1374/1393 train_time:176350ms step_avg:129.29ms
step:1375/1393 train_time:176484ms step_avg:129.29ms
step:1375/1393 val_loss:3.2794 train_time:176617ms step_avg:129.39ms
step:1376/1393 train_time:176638ms step_avg:129.31ms
step:1377/1393 train_time:176761ms step_avg:129.31ms
step:1378/1393 train_time:176896ms step_avg:129.31ms
step:1379/1393 train_time:177031ms step_avg:129.31ms
step:1380/1393 train_time:177167ms step_avg:129.32ms
step:1381/1393 train_time:177304ms step_avg:129.32ms
step:1382/1393 train_time:177441ms step_avg:129.33ms
step:1383/1393 train_time:177576ms step_avg:129.33ms
step:1384/1393 train_time:177714ms step_avg:129.34ms
step:1385/1393 train_time:177850ms step_avg:129.35ms
step:1386/1393 train_time:177984ms step_avg:129.35ms
step:1387/1393 train_time:178122ms step_avg:129.36ms
step:1388/1393 train_time:178257ms step_avg:129.36ms
step:1389/1393 train_time:178392ms step_avg:129.36ms
step:1390/1393 train_time:178529ms step_avg:129.37ms
step:1391/1393 train_time:178664ms step_avg:129.37ms
step:1392/1393 train_time:178802ms step_avg:129.38ms
step:1393/1393 train_time:178936ms step_avg:129.38ms
step:1393/1393 val_loss:3.2758 train_time:179071ms step_avg:129.48ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
