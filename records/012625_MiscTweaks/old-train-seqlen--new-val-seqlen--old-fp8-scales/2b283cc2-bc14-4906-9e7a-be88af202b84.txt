import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:40:29 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23530ms step_avg:nanms
step:2/1393 train_time:23945ms step_avg:nanms
step:3/1393 train_time:24065ms step_avg:nanms
step:4/1393 train_time:24185ms step_avg:nanms
step:5/1393 train_time:24306ms step_avg:nanms
step:6/1393 train_time:24427ms step_avg:nanms
step:7/1393 train_time:24549ms step_avg:nanms
step:8/1393 train_time:24670ms step_avg:nanms
step:9/1393 train_time:24792ms step_avg:nanms
step:10/1393 train_time:24915ms step_avg:nanms
step:11/1393 train_time:125ms step_avg:nanms
step:12/1393 train_time:250ms step_avg:nanms
step:13/1393 train_time:372ms step_avg:124.01ms
step:14/1393 train_time:495ms step_avg:123.83ms
step:15/1393 train_time:617ms step_avg:123.47ms
step:16/1393 train_time:739ms step_avg:123.23ms
step:17/1393 train_time:862ms step_avg:123.18ms
step:18/1393 train_time:983ms step_avg:122.90ms
step:19/1393 train_time:1105ms step_avg:122.74ms
step:20/1393 train_time:1227ms step_avg:122.70ms
step:21/1393 train_time:1349ms step_avg:122.65ms
step:22/1393 train_time:1472ms step_avg:122.68ms
step:23/1393 train_time:1593ms step_avg:122.57ms
step:24/1393 train_time:1716ms step_avg:122.59ms
step:25/1393 train_time:1838ms step_avg:122.55ms
step:26/1393 train_time:1961ms step_avg:122.53ms
step:27/1393 train_time:2083ms step_avg:122.53ms
step:28/1393 train_time:2206ms step_avg:122.58ms
step:29/1393 train_time:2329ms step_avg:122.59ms
step:30/1393 train_time:2450ms step_avg:122.52ms
step:31/1393 train_time:2573ms step_avg:122.54ms
step:32/1393 train_time:2695ms step_avg:122.48ms
step:33/1393 train_time:2816ms step_avg:122.45ms
step:34/1393 train_time:2940ms step_avg:122.48ms
step:35/1393 train_time:3061ms step_avg:122.45ms
step:36/1393 train_time:3183ms step_avg:122.41ms
step:37/1393 train_time:3306ms step_avg:122.46ms
step:38/1393 train_time:3429ms step_avg:122.48ms
step:39/1393 train_time:3551ms step_avg:122.47ms
step:40/1393 train_time:3674ms step_avg:122.47ms
step:41/1393 train_time:3796ms step_avg:122.45ms
step:42/1393 train_time:3918ms step_avg:122.45ms
step:43/1393 train_time:4043ms step_avg:122.50ms
step:44/1393 train_time:4164ms step_avg:122.47ms
step:45/1393 train_time:4287ms step_avg:122.47ms
step:46/1393 train_time:4410ms step_avg:122.51ms
step:47/1393 train_time:4533ms step_avg:122.50ms
step:48/1393 train_time:4655ms step_avg:122.50ms
step:49/1393 train_time:4777ms step_avg:122.48ms
step:50/1393 train_time:4899ms step_avg:122.47ms
step:51/1393 train_time:5021ms step_avg:122.45ms
step:52/1393 train_time:5143ms step_avg:122.44ms
step:53/1393 train_time:5264ms step_avg:122.42ms
step:54/1393 train_time:5386ms step_avg:122.42ms
step:55/1393 train_time:5510ms step_avg:122.44ms
step:56/1393 train_time:5633ms step_avg:122.45ms
step:57/1393 train_time:5755ms step_avg:122.46ms
step:58/1393 train_time:5878ms step_avg:122.46ms
step:59/1393 train_time:6002ms step_avg:122.50ms
step:60/1393 train_time:6126ms step_avg:122.51ms
step:61/1393 train_time:6249ms step_avg:122.53ms
step:62/1393 train_time:6371ms step_avg:122.52ms
step:63/1393 train_time:6493ms step_avg:122.52ms
step:64/1393 train_time:6616ms step_avg:122.51ms
step:65/1393 train_time:6738ms step_avg:122.52ms
step:66/1393 train_time:6860ms step_avg:122.51ms
step:67/1393 train_time:6983ms step_avg:122.51ms
step:68/1393 train_time:7107ms step_avg:122.53ms
step:69/1393 train_time:7228ms step_avg:122.51ms
step:70/1393 train_time:7350ms step_avg:122.49ms
step:71/1393 train_time:7472ms step_avg:122.48ms
step:72/1393 train_time:7593ms step_avg:122.47ms
step:73/1393 train_time:7716ms step_avg:122.48ms
step:74/1393 train_time:7838ms step_avg:122.47ms
step:75/1393 train_time:7960ms step_avg:122.47ms
step:76/1393 train_time:8083ms step_avg:122.47ms
step:77/1393 train_time:8205ms step_avg:122.46ms
step:78/1393 train_time:8327ms step_avg:122.46ms
step:79/1393 train_time:8449ms step_avg:122.46ms
step:80/1393 train_time:8572ms step_avg:122.46ms
step:81/1393 train_time:8693ms step_avg:122.44ms
step:82/1393 train_time:8817ms step_avg:122.46ms
step:83/1393 train_time:8938ms step_avg:122.44ms
step:84/1393 train_time:9062ms step_avg:122.46ms
step:85/1393 train_time:9184ms step_avg:122.45ms
step:86/1393 train_time:9306ms step_avg:122.45ms
step:87/1393 train_time:9428ms step_avg:122.44ms
step:88/1393 train_time:9550ms step_avg:122.43ms
step:89/1393 train_time:9672ms step_avg:122.42ms
step:90/1393 train_time:9794ms step_avg:122.42ms
step:91/1393 train_time:9917ms step_avg:122.43ms
step:92/1393 train_time:10039ms step_avg:122.43ms
step:93/1393 train_time:10161ms step_avg:122.42ms
step:94/1393 train_time:10283ms step_avg:122.41ms
step:95/1393 train_time:10405ms step_avg:122.41ms
step:96/1393 train_time:10527ms step_avg:122.41ms
step:97/1393 train_time:10651ms step_avg:122.42ms
step:98/1393 train_time:10773ms step_avg:122.42ms
step:99/1393 train_time:10896ms step_avg:122.43ms
step:100/1393 train_time:11019ms step_avg:122.44ms
step:101/1393 train_time:11141ms step_avg:122.43ms
step:102/1393 train_time:11263ms step_avg:122.42ms
step:103/1393 train_time:11384ms step_avg:122.41ms
step:104/1393 train_time:11507ms step_avg:122.41ms
step:105/1393 train_time:11629ms step_avg:122.42ms
step:106/1393 train_time:11752ms step_avg:122.42ms
step:107/1393 train_time:11874ms step_avg:122.42ms
step:108/1393 train_time:11997ms step_avg:122.42ms
step:109/1393 train_time:12120ms step_avg:122.43ms
step:110/1393 train_time:12244ms step_avg:122.44ms
step:111/1393 train_time:12367ms step_avg:122.44ms
step:112/1393 train_time:12490ms step_avg:122.45ms
step:113/1393 train_time:12613ms step_avg:122.46ms
step:114/1393 train_time:12736ms step_avg:122.46ms
step:115/1393 train_time:12859ms step_avg:122.47ms
step:116/1393 train_time:12982ms step_avg:122.47ms
step:117/1393 train_time:13105ms step_avg:122.47ms
step:118/1393 train_time:13227ms step_avg:122.48ms
step:119/1393 train_time:13351ms step_avg:122.48ms
step:120/1393 train_time:13473ms step_avg:122.49ms
step:121/1393 train_time:13596ms step_avg:122.49ms
step:122/1393 train_time:13719ms step_avg:122.49ms
step:123/1393 train_time:13842ms step_avg:122.49ms
step:124/1393 train_time:13965ms step_avg:122.50ms
step:125/1393 train_time:14086ms step_avg:122.49ms
step:125/1393 val_loss:4.4043 train_time:14208ms step_avg:123.55ms
step:126/1393 train_time:14231ms step_avg:122.68ms
step:127/1393 train_time:14337ms step_avg:122.53ms
step:128/1393 train_time:14472ms step_avg:122.64ms
step:129/1393 train_time:14596ms step_avg:122.66ms
step:130/1393 train_time:14719ms step_avg:122.65ms
step:131/1393 train_time:14841ms step_avg:122.65ms
step:132/1393 train_time:14963ms step_avg:122.65ms
step:133/1393 train_time:15085ms step_avg:122.64ms
step:134/1393 train_time:15208ms step_avg:122.65ms
step:135/1393 train_time:15331ms step_avg:122.65ms
step:136/1393 train_time:15455ms step_avg:122.66ms
step:137/1393 train_time:15579ms step_avg:122.67ms
step:138/1393 train_time:15701ms step_avg:122.66ms
step:139/1393 train_time:15824ms step_avg:122.66ms
step:140/1393 train_time:15947ms step_avg:122.67ms
step:141/1393 train_time:16070ms step_avg:122.67ms
step:142/1393 train_time:16192ms step_avg:122.67ms
step:143/1393 train_time:16316ms step_avg:122.68ms
step:144/1393 train_time:16439ms step_avg:122.68ms
step:145/1393 train_time:16562ms step_avg:122.68ms
step:146/1393 train_time:16685ms step_avg:122.69ms
step:147/1393 train_time:16809ms step_avg:122.69ms
step:148/1393 train_time:16932ms step_avg:122.70ms
step:149/1393 train_time:17055ms step_avg:122.70ms
step:150/1393 train_time:17177ms step_avg:122.69ms
step:151/1393 train_time:17300ms step_avg:122.69ms
step:152/1393 train_time:17422ms step_avg:122.69ms
step:153/1393 train_time:17545ms step_avg:122.69ms
step:154/1393 train_time:17668ms step_avg:122.69ms
step:155/1393 train_time:17791ms step_avg:122.69ms
step:156/1393 train_time:17915ms step_avg:122.71ms
step:157/1393 train_time:18040ms step_avg:122.72ms
step:158/1393 train_time:18163ms step_avg:122.72ms
step:159/1393 train_time:18285ms step_avg:122.72ms
step:160/1393 train_time:18408ms step_avg:122.72ms
step:161/1393 train_time:18531ms step_avg:122.72ms
step:162/1393 train_time:18654ms step_avg:122.72ms
step:163/1393 train_time:18778ms step_avg:122.73ms
step:164/1393 train_time:18901ms step_avg:122.73ms
step:165/1393 train_time:19025ms step_avg:122.74ms
step:166/1393 train_time:19148ms step_avg:122.74ms
step:167/1393 train_time:19271ms step_avg:122.74ms
step:168/1393 train_time:19393ms step_avg:122.74ms
step:169/1393 train_time:19515ms step_avg:122.73ms
step:170/1393 train_time:19639ms step_avg:122.74ms
step:171/1393 train_time:19761ms step_avg:122.74ms
step:172/1393 train_time:19884ms step_avg:122.74ms
step:173/1393 train_time:20008ms step_avg:122.75ms
step:174/1393 train_time:20132ms step_avg:122.76ms
step:175/1393 train_time:20255ms step_avg:122.76ms
step:176/1393 train_time:20379ms step_avg:122.77ms
step:177/1393 train_time:20501ms step_avg:122.76ms
step:178/1393 train_time:20623ms step_avg:122.76ms
step:179/1393 train_time:20746ms step_avg:122.76ms
step:180/1393 train_time:20869ms step_avg:122.76ms
step:181/1393 train_time:20991ms step_avg:122.75ms
step:182/1393 train_time:21115ms step_avg:122.76ms
step:183/1393 train_time:21237ms step_avg:122.76ms
step:184/1393 train_time:21360ms step_avg:122.76ms
step:185/1393 train_time:21483ms step_avg:122.76ms
step:186/1393 train_time:21606ms step_avg:122.76ms
step:187/1393 train_time:21729ms step_avg:122.76ms
step:188/1393 train_time:21851ms step_avg:122.76ms
step:189/1393 train_time:21974ms step_avg:122.76ms
step:190/1393 train_time:22098ms step_avg:122.77ms
step:191/1393 train_time:22221ms step_avg:122.77ms
step:192/1393 train_time:22343ms step_avg:122.77ms
step:193/1393 train_time:22466ms step_avg:122.77ms
step:194/1393 train_time:22589ms step_avg:122.77ms
step:195/1393 train_time:22712ms step_avg:122.77ms
step:196/1393 train_time:22834ms step_avg:122.77ms
step:197/1393 train_time:22958ms step_avg:122.77ms
step:198/1393 train_time:23081ms step_avg:122.77ms
step:199/1393 train_time:23204ms step_avg:122.77ms
step:200/1393 train_time:23327ms step_avg:122.78ms
step:201/1393 train_time:23451ms step_avg:122.78ms
step:202/1393 train_time:23575ms step_avg:122.79ms
step:203/1393 train_time:23697ms step_avg:122.78ms
step:204/1393 train_time:23819ms step_avg:122.78ms
step:205/1393 train_time:23942ms step_avg:122.78ms
step:206/1393 train_time:24065ms step_avg:122.78ms
step:207/1393 train_time:24189ms step_avg:122.79ms
step:208/1393 train_time:24313ms step_avg:122.79ms
step:209/1393 train_time:24437ms step_avg:122.80ms
step:210/1393 train_time:24560ms step_avg:122.80ms
step:211/1393 train_time:24684ms step_avg:122.81ms
step:212/1393 train_time:24808ms step_avg:122.81ms
step:213/1393 train_time:24931ms step_avg:122.81ms
step:214/1393 train_time:25055ms step_avg:122.82ms
step:215/1393 train_time:25179ms step_avg:122.82ms
step:216/1393 train_time:25302ms step_avg:122.82ms
step:217/1393 train_time:25424ms step_avg:122.82ms
step:218/1393 train_time:25549ms step_avg:122.83ms
step:219/1393 train_time:25672ms step_avg:122.83ms
step:220/1393 train_time:25795ms step_avg:122.83ms
step:221/1393 train_time:25919ms step_avg:122.84ms
step:222/1393 train_time:26042ms step_avg:122.84ms
step:223/1393 train_time:26166ms step_avg:122.84ms
step:224/1393 train_time:26289ms step_avg:122.85ms
step:225/1393 train_time:26412ms step_avg:122.85ms
step:226/1393 train_time:26535ms step_avg:122.85ms
step:227/1393 train_time:26659ms step_avg:122.85ms
step:228/1393 train_time:26782ms step_avg:122.85ms
step:229/1393 train_time:26906ms step_avg:122.86ms
step:230/1393 train_time:27029ms step_avg:122.86ms
step:231/1393 train_time:27152ms step_avg:122.86ms
step:232/1393 train_time:27277ms step_avg:122.87ms
step:233/1393 train_time:27401ms step_avg:122.87ms
step:234/1393 train_time:27523ms step_avg:122.87ms
step:235/1393 train_time:27646ms step_avg:122.87ms
step:236/1393 train_time:27769ms step_avg:122.87ms
step:237/1393 train_time:27893ms step_avg:122.87ms
step:238/1393 train_time:28016ms step_avg:122.88ms
step:239/1393 train_time:28139ms step_avg:122.88ms
step:240/1393 train_time:28263ms step_avg:122.88ms
step:241/1393 train_time:28387ms step_avg:122.89ms
step:242/1393 train_time:28510ms step_avg:122.89ms
step:243/1393 train_time:28633ms step_avg:122.89ms
step:244/1393 train_time:28757ms step_avg:122.90ms
step:245/1393 train_time:28881ms step_avg:122.90ms
step:246/1393 train_time:29004ms step_avg:122.90ms
step:247/1393 train_time:29128ms step_avg:122.90ms
step:248/1393 train_time:29251ms step_avg:122.90ms
step:249/1393 train_time:29375ms step_avg:122.91ms
step:250/1393 train_time:29499ms step_avg:122.91ms
step:250/1393 val_loss:3.9833 train_time:29619ms step_avg:123.41ms
step:251/1393 train_time:29643ms step_avg:123.00ms
step:252/1393 train_time:29757ms step_avg:122.96ms
step:253/1393 train_time:29884ms step_avg:122.98ms
step:254/1393 train_time:30007ms step_avg:122.98ms
step:255/1393 train_time:30131ms step_avg:122.98ms
step:256/1393 train_time:30253ms step_avg:122.98ms
step:257/1393 train_time:30377ms step_avg:122.98ms
step:258/1393 train_time:30500ms step_avg:122.99ms
step:259/1393 train_time:30623ms step_avg:122.98ms
step:260/1393 train_time:30748ms step_avg:122.99ms
step:261/1393 train_time:30872ms step_avg:123.00ms
step:262/1393 train_time:30995ms step_avg:123.00ms
step:263/1393 train_time:31119ms step_avg:123.00ms
step:264/1393 train_time:31243ms step_avg:123.00ms
step:265/1393 train_time:31366ms step_avg:123.00ms
step:266/1393 train_time:31488ms step_avg:123.00ms
step:267/1393 train_time:31612ms step_avg:123.00ms
step:268/1393 train_time:31735ms step_avg:123.00ms
step:269/1393 train_time:31859ms step_avg:123.01ms
step:270/1393 train_time:31983ms step_avg:123.01ms
step:271/1393 train_time:32106ms step_avg:123.01ms
step:272/1393 train_time:32230ms step_avg:123.01ms
step:273/1393 train_time:32352ms step_avg:123.01ms
step:274/1393 train_time:32476ms step_avg:123.02ms
step:275/1393 train_time:32600ms step_avg:123.02ms
step:276/1393 train_time:32722ms step_avg:123.02ms
step:277/1393 train_time:32846ms step_avg:123.02ms
step:278/1393 train_time:32969ms step_avg:123.02ms
step:279/1393 train_time:33092ms step_avg:123.02ms
step:280/1393 train_time:33216ms step_avg:123.02ms
step:281/1393 train_time:33339ms step_avg:123.02ms
step:282/1393 train_time:33463ms step_avg:123.03ms
step:283/1393 train_time:33587ms step_avg:123.03ms
step:284/1393 train_time:33711ms step_avg:123.03ms
step:285/1393 train_time:33836ms step_avg:123.04ms
step:286/1393 train_time:33960ms step_avg:123.04ms
step:287/1393 train_time:34083ms step_avg:123.04ms
step:288/1393 train_time:34206ms step_avg:123.04ms
step:289/1393 train_time:34329ms step_avg:123.04ms
step:290/1393 train_time:34452ms step_avg:123.04ms
step:291/1393 train_time:34576ms step_avg:123.04ms
step:292/1393 train_time:34700ms step_avg:123.05ms
step:293/1393 train_time:34823ms step_avg:123.05ms
step:294/1393 train_time:34946ms step_avg:123.05ms
step:295/1393 train_time:35070ms step_avg:123.05ms
step:296/1393 train_time:35193ms step_avg:123.05ms
step:297/1393 train_time:35316ms step_avg:123.05ms
step:298/1393 train_time:35439ms step_avg:123.05ms
step:299/1393 train_time:35562ms step_avg:123.05ms
step:300/1393 train_time:35685ms step_avg:123.05ms
step:301/1393 train_time:35809ms step_avg:123.06ms
step:302/1393 train_time:35933ms step_avg:123.06ms
step:303/1393 train_time:36056ms step_avg:123.06ms
step:304/1393 train_time:36180ms step_avg:123.06ms
step:305/1393 train_time:36304ms step_avg:123.07ms
step:306/1393 train_time:36427ms step_avg:123.07ms
step:307/1393 train_time:36551ms step_avg:123.07ms
step:308/1393 train_time:36674ms step_avg:123.07ms
step:309/1393 train_time:36797ms step_avg:123.07ms
step:310/1393 train_time:36922ms step_avg:123.07ms
step:311/1393 train_time:37046ms step_avg:123.07ms
step:312/1393 train_time:37171ms step_avg:123.08ms
step:313/1393 train_time:37299ms step_avg:123.10ms
step:314/1393 train_time:37424ms step_avg:123.11ms
step:315/1393 train_time:37551ms step_avg:123.12ms
step:316/1393 train_time:37677ms step_avg:123.13ms
step:317/1393 train_time:37803ms step_avg:123.14ms
step:318/1393 train_time:37929ms step_avg:123.15ms
step:319/1393 train_time:38055ms step_avg:123.16ms
step:320/1393 train_time:38182ms step_avg:123.17ms
step:321/1393 train_time:38308ms step_avg:123.18ms
step:322/1393 train_time:38433ms step_avg:123.18ms
step:323/1393 train_time:38559ms step_avg:123.19ms
step:324/1393 train_time:38686ms step_avg:123.20ms
step:325/1393 train_time:38812ms step_avg:123.21ms
step:326/1393 train_time:38938ms step_avg:123.22ms
step:327/1393 train_time:39064ms step_avg:123.23ms
step:328/1393 train_time:39190ms step_avg:123.24ms
step:329/1393 train_time:39317ms step_avg:123.25ms
step:330/1393 train_time:39443ms step_avg:123.26ms
step:331/1393 train_time:39569ms step_avg:123.27ms
step:332/1393 train_time:39695ms step_avg:123.28ms
step:333/1393 train_time:39821ms step_avg:123.29ms
step:334/1393 train_time:39947ms step_avg:123.29ms
step:335/1393 train_time:40074ms step_avg:123.30ms
step:336/1393 train_time:40200ms step_avg:123.31ms
step:337/1393 train_time:40326ms step_avg:123.32ms
step:338/1393 train_time:40452ms step_avg:123.33ms
step:339/1393 train_time:40579ms step_avg:123.34ms
step:340/1393 train_time:40705ms step_avg:123.35ms
step:341/1393 train_time:40831ms step_avg:123.36ms
step:342/1393 train_time:40957ms step_avg:123.36ms
step:343/1393 train_time:41083ms step_avg:123.37ms
step:344/1393 train_time:41209ms step_avg:123.38ms
step:345/1393 train_time:41336ms step_avg:123.39ms
step:346/1393 train_time:41462ms step_avg:123.40ms
step:347/1393 train_time:41588ms step_avg:123.41ms
step:348/1393 train_time:41714ms step_avg:123.41ms
step:349/1393 train_time:41840ms step_avg:123.42ms
step:350/1393 train_time:41967ms step_avg:123.43ms
step:351/1393 train_time:42094ms step_avg:123.44ms
step:352/1393 train_time:42220ms step_avg:123.45ms
step:353/1393 train_time:42345ms step_avg:123.46ms
step:354/1393 train_time:42473ms step_avg:123.47ms
step:355/1393 train_time:42600ms step_avg:123.48ms
step:356/1393 train_time:42725ms step_avg:123.48ms
step:357/1393 train_time:42851ms step_avg:123.49ms
step:358/1393 train_time:42978ms step_avg:123.50ms
step:359/1393 train_time:43104ms step_avg:123.51ms
step:360/1393 train_time:43231ms step_avg:123.52ms
step:361/1393 train_time:43357ms step_avg:123.52ms
step:362/1393 train_time:43483ms step_avg:123.53ms
step:363/1393 train_time:43611ms step_avg:123.55ms
step:364/1393 train_time:43738ms step_avg:123.55ms
step:365/1393 train_time:43864ms step_avg:123.56ms
step:366/1393 train_time:43990ms step_avg:123.57ms
step:367/1393 train_time:44116ms step_avg:123.57ms
step:368/1393 train_time:44242ms step_avg:123.58ms
step:369/1393 train_time:44368ms step_avg:123.59ms
step:370/1393 train_time:44494ms step_avg:123.60ms
step:371/1393 train_time:44621ms step_avg:123.60ms
step:372/1393 train_time:44747ms step_avg:123.61ms
step:373/1393 train_time:44873ms step_avg:123.62ms
step:374/1393 train_time:44998ms step_avg:123.62ms
step:375/1393 train_time:45124ms step_avg:123.63ms
step:375/1393 val_loss:3.7834 train_time:45249ms step_avg:123.97ms
step:376/1393 train_time:45270ms step_avg:123.69ms
step:377/1393 train_time:45388ms step_avg:123.67ms
step:378/1393 train_time:45516ms step_avg:123.68ms
step:379/1393 train_time:45641ms step_avg:123.69ms
step:380/1393 train_time:45766ms step_avg:123.69ms
step:381/1393 train_time:45892ms step_avg:123.70ms
step:382/1393 train_time:46018ms step_avg:123.71ms
step:383/1393 train_time:46143ms step_avg:123.71ms
step:384/1393 train_time:46270ms step_avg:123.72ms
step:385/1393 train_time:46399ms step_avg:123.73ms
step:386/1393 train_time:46525ms step_avg:123.74ms
step:387/1393 train_time:46651ms step_avg:123.74ms
step:388/1393 train_time:46778ms step_avg:123.75ms
step:389/1393 train_time:46904ms step_avg:123.76ms
step:390/1393 train_time:47030ms step_avg:123.76ms
step:391/1393 train_time:47155ms step_avg:123.77ms
step:392/1393 train_time:47281ms step_avg:123.77ms
step:393/1393 train_time:47406ms step_avg:123.78ms
step:394/1393 train_time:47533ms step_avg:123.78ms
step:395/1393 train_time:47659ms step_avg:123.79ms
step:396/1393 train_time:47786ms step_avg:123.80ms
step:397/1393 train_time:47912ms step_avg:123.80ms
step:398/1393 train_time:48038ms step_avg:123.81ms
step:399/1393 train_time:48165ms step_avg:123.82ms
step:400/1393 train_time:48290ms step_avg:123.82ms
step:401/1393 train_time:48415ms step_avg:123.82ms
step:402/1393 train_time:48542ms step_avg:123.83ms
step:403/1393 train_time:48668ms step_avg:123.84ms
step:404/1393 train_time:48793ms step_avg:123.84ms
step:405/1393 train_time:48920ms step_avg:123.85ms
step:406/1393 train_time:49046ms step_avg:123.85ms
step:407/1393 train_time:49173ms step_avg:123.86ms
step:408/1393 train_time:49299ms step_avg:123.87ms
step:409/1393 train_time:49425ms step_avg:123.87ms
step:410/1393 train_time:49551ms step_avg:123.88ms
step:411/1393 train_time:49677ms step_avg:123.88ms
step:412/1393 train_time:49803ms step_avg:123.89ms
step:413/1393 train_time:49930ms step_avg:123.89ms
step:414/1393 train_time:50056ms step_avg:123.90ms
step:415/1393 train_time:50182ms step_avg:123.91ms
step:416/1393 train_time:50308ms step_avg:123.91ms
step:417/1393 train_time:50435ms step_avg:123.92ms
step:418/1393 train_time:50560ms step_avg:123.92ms
step:419/1393 train_time:50686ms step_avg:123.93ms
step:420/1393 train_time:50813ms step_avg:123.93ms
step:421/1393 train_time:50940ms step_avg:123.94ms
step:422/1393 train_time:51066ms step_avg:123.95ms
step:423/1393 train_time:51193ms step_avg:123.95ms
step:424/1393 train_time:51319ms step_avg:123.96ms
step:425/1393 train_time:51445ms step_avg:123.96ms
step:426/1393 train_time:51572ms step_avg:123.97ms
step:427/1393 train_time:51698ms step_avg:123.98ms
step:428/1393 train_time:51824ms step_avg:123.98ms
step:429/1393 train_time:51950ms step_avg:123.99ms
step:430/1393 train_time:52077ms step_avg:123.99ms
step:431/1393 train_time:52204ms step_avg:124.00ms
step:432/1393 train_time:52330ms step_avg:124.00ms
step:433/1393 train_time:52456ms step_avg:124.01ms
step:434/1393 train_time:52582ms step_avg:124.01ms
step:435/1393 train_time:52709ms step_avg:124.02ms
step:436/1393 train_time:52835ms step_avg:124.03ms
step:437/1393 train_time:52961ms step_avg:124.03ms
step:438/1393 train_time:53087ms step_avg:124.04ms
step:439/1393 train_time:53215ms step_avg:124.04ms
step:440/1393 train_time:53341ms step_avg:124.05ms
step:441/1393 train_time:53468ms step_avg:124.06ms
step:442/1393 train_time:53596ms step_avg:124.06ms
step:443/1393 train_time:53722ms step_avg:124.07ms
step:444/1393 train_time:53848ms step_avg:124.07ms
step:445/1393 train_time:53974ms step_avg:124.08ms
step:446/1393 train_time:54100ms step_avg:124.08ms
step:447/1393 train_time:54226ms step_avg:124.09ms
step:448/1393 train_time:54353ms step_avg:124.09ms
step:449/1393 train_time:54479ms step_avg:124.10ms
step:450/1393 train_time:54605ms step_avg:124.10ms
step:451/1393 train_time:54733ms step_avg:124.11ms
step:452/1393 train_time:54860ms step_avg:124.12ms
step:453/1393 train_time:54985ms step_avg:124.12ms
step:454/1393 train_time:55112ms step_avg:124.13ms
step:455/1393 train_time:55238ms step_avg:124.13ms
step:456/1393 train_time:55364ms step_avg:124.13ms
step:457/1393 train_time:55491ms step_avg:124.14ms
step:458/1393 train_time:55618ms step_avg:124.15ms
step:459/1393 train_time:55744ms step_avg:124.15ms
step:460/1393 train_time:55870ms step_avg:124.16ms
step:461/1393 train_time:55997ms step_avg:124.16ms
step:462/1393 train_time:56124ms step_avg:124.17ms
step:463/1393 train_time:56250ms step_avg:124.17ms
step:464/1393 train_time:56375ms step_avg:124.17ms
step:465/1393 train_time:56502ms step_avg:124.18ms
step:466/1393 train_time:56628ms step_avg:124.18ms
step:467/1393 train_time:56754ms step_avg:124.19ms
step:468/1393 train_time:56881ms step_avg:124.19ms
step:469/1393 train_time:57007ms step_avg:124.20ms
step:470/1393 train_time:57133ms step_avg:124.20ms
step:471/1393 train_time:57260ms step_avg:124.21ms
step:472/1393 train_time:57386ms step_avg:124.21ms
step:473/1393 train_time:57512ms step_avg:124.22ms
step:474/1393 train_time:57639ms step_avg:124.22ms
step:475/1393 train_time:57765ms step_avg:124.23ms
step:476/1393 train_time:57891ms step_avg:124.23ms
step:477/1393 train_time:58017ms step_avg:124.23ms
step:478/1393 train_time:58143ms step_avg:124.24ms
step:479/1393 train_time:58270ms step_avg:124.24ms
step:480/1393 train_time:58397ms step_avg:124.25ms
step:481/1393 train_time:58523ms step_avg:124.25ms
step:482/1393 train_time:58650ms step_avg:124.26ms
step:483/1393 train_time:58776ms step_avg:124.26ms
step:484/1393 train_time:58902ms step_avg:124.27ms
step:485/1393 train_time:59030ms step_avg:124.27ms
step:486/1393 train_time:59156ms step_avg:124.28ms
step:487/1393 train_time:59283ms step_avg:124.28ms
step:488/1393 train_time:59409ms step_avg:124.29ms
step:489/1393 train_time:59535ms step_avg:124.29ms
step:490/1393 train_time:59661ms step_avg:124.29ms
step:491/1393 train_time:59787ms step_avg:124.30ms
step:492/1393 train_time:59914ms step_avg:124.30ms
step:493/1393 train_time:60041ms step_avg:124.31ms
step:494/1393 train_time:60166ms step_avg:124.31ms
step:495/1393 train_time:60292ms step_avg:124.31ms
step:496/1393 train_time:60419ms step_avg:124.32ms
step:497/1393 train_time:60546ms step_avg:124.32ms
step:498/1393 train_time:60674ms step_avg:124.33ms
step:499/1393 train_time:60799ms step_avg:124.33ms
step:500/1393 train_time:60926ms step_avg:124.34ms
step:500/1393 val_loss:3.6631 train_time:61050ms step_avg:124.59ms
step:501/1393 train_time:61071ms step_avg:124.38ms
step:502/1393 train_time:61191ms step_avg:124.37ms
step:503/1393 train_time:61319ms step_avg:124.38ms
step:504/1393 train_time:61446ms step_avg:124.38ms
step:505/1393 train_time:61572ms step_avg:124.39ms
step:506/1393 train_time:61697ms step_avg:124.39ms
step:507/1393 train_time:61823ms step_avg:124.39ms
step:508/1393 train_time:61949ms step_avg:124.40ms
step:509/1393 train_time:62075ms step_avg:124.40ms
step:510/1393 train_time:62203ms step_avg:124.41ms
step:511/1393 train_time:62329ms step_avg:124.41ms
step:512/1393 train_time:62457ms step_avg:124.42ms
step:513/1393 train_time:62583ms step_avg:124.42ms
step:514/1393 train_time:62709ms step_avg:124.42ms
step:515/1393 train_time:62836ms step_avg:124.43ms
step:516/1393 train_time:62963ms step_avg:124.43ms
step:517/1393 train_time:63089ms step_avg:124.44ms
step:518/1393 train_time:63218ms step_avg:124.44ms
step:519/1393 train_time:63346ms step_avg:124.45ms
step:520/1393 train_time:63475ms step_avg:124.46ms
step:521/1393 train_time:63603ms step_avg:124.47ms
step:522/1393 train_time:63731ms step_avg:124.48ms
step:523/1393 train_time:63860ms step_avg:124.48ms
step:524/1393 train_time:63988ms step_avg:124.49ms
step:525/1393 train_time:64116ms step_avg:124.50ms
step:526/1393 train_time:64246ms step_avg:124.51ms
step:527/1393 train_time:64375ms step_avg:124.52ms
step:528/1393 train_time:64503ms step_avg:124.52ms
step:529/1393 train_time:64632ms step_avg:124.53ms
step:530/1393 train_time:64759ms step_avg:124.54ms
step:531/1393 train_time:64888ms step_avg:124.54ms
step:532/1393 train_time:65016ms step_avg:124.55ms
step:533/1393 train_time:65145ms step_avg:124.56ms
step:534/1393 train_time:65275ms step_avg:124.57ms
step:535/1393 train_time:65404ms step_avg:124.58ms
step:536/1393 train_time:65533ms step_avg:124.59ms
step:537/1393 train_time:65661ms step_avg:124.59ms
step:538/1393 train_time:65789ms step_avg:124.60ms
step:539/1393 train_time:65918ms step_avg:124.61ms
step:540/1393 train_time:66046ms step_avg:124.61ms
step:541/1393 train_time:66175ms step_avg:124.62ms
step:542/1393 train_time:66304ms step_avg:124.63ms
step:543/1393 train_time:66433ms step_avg:124.64ms
step:544/1393 train_time:66561ms step_avg:124.65ms
step:545/1393 train_time:66689ms step_avg:124.65ms
step:546/1393 train_time:66818ms step_avg:124.66ms
step:547/1393 train_time:66947ms step_avg:124.67ms
step:548/1393 train_time:67076ms step_avg:124.68ms
step:549/1393 train_time:67204ms step_avg:124.68ms
step:550/1393 train_time:67332ms step_avg:124.69ms
step:551/1393 train_time:67460ms step_avg:124.70ms
step:552/1393 train_time:67589ms step_avg:124.70ms
step:553/1393 train_time:67717ms step_avg:124.71ms
step:554/1393 train_time:67846ms step_avg:124.72ms
step:555/1393 train_time:67975ms step_avg:124.72ms
step:556/1393 train_time:68104ms step_avg:124.73ms
step:557/1393 train_time:68233ms step_avg:124.74ms
step:558/1393 train_time:68362ms step_avg:124.75ms
step:559/1393 train_time:68489ms step_avg:124.75ms
step:560/1393 train_time:68617ms step_avg:124.76ms
step:561/1393 train_time:68746ms step_avg:124.77ms
step:562/1393 train_time:68875ms step_avg:124.77ms
step:563/1393 train_time:69004ms step_avg:124.78ms
step:564/1393 train_time:69132ms step_avg:124.79ms
step:565/1393 train_time:69261ms step_avg:124.79ms
step:566/1393 train_time:69391ms step_avg:124.80ms
step:567/1393 train_time:69518ms step_avg:124.81ms
step:568/1393 train_time:69647ms step_avg:124.82ms
step:569/1393 train_time:69776ms step_avg:124.82ms
step:570/1393 train_time:69905ms step_avg:124.83ms
step:571/1393 train_time:70033ms step_avg:124.84ms
step:572/1393 train_time:70161ms step_avg:124.84ms
step:573/1393 train_time:70290ms step_avg:124.85ms
step:574/1393 train_time:70418ms step_avg:124.85ms
step:575/1393 train_time:70547ms step_avg:124.86ms
step:576/1393 train_time:70675ms step_avg:124.87ms
step:577/1393 train_time:70804ms step_avg:124.87ms
step:578/1393 train_time:70932ms step_avg:124.88ms
step:579/1393 train_time:71061ms step_avg:124.89ms
step:580/1393 train_time:71189ms step_avg:124.89ms
step:581/1393 train_time:71317ms step_avg:124.90ms
step:582/1393 train_time:71446ms step_avg:124.90ms
step:583/1393 train_time:71574ms step_avg:124.91ms
step:584/1393 train_time:71703ms step_avg:124.92ms
step:585/1393 train_time:71831ms step_avg:124.92ms
step:586/1393 train_time:71959ms step_avg:124.93ms
step:587/1393 train_time:72087ms step_avg:124.94ms
step:588/1393 train_time:72216ms step_avg:124.94ms
step:589/1393 train_time:72344ms step_avg:124.95ms
step:590/1393 train_time:72473ms step_avg:124.95ms
step:591/1393 train_time:72602ms step_avg:124.96ms
step:592/1393 train_time:72730ms step_avg:124.97ms
step:593/1393 train_time:72859ms step_avg:124.97ms
step:594/1393 train_time:72989ms step_avg:124.98ms
step:595/1393 train_time:73118ms step_avg:124.99ms
step:596/1393 train_time:73246ms step_avg:124.99ms
step:597/1393 train_time:73375ms step_avg:125.00ms
step:598/1393 train_time:73505ms step_avg:125.01ms
step:599/1393 train_time:73633ms step_avg:125.01ms
step:600/1393 train_time:73761ms step_avg:125.02ms
step:601/1393 train_time:73889ms step_avg:125.02ms
step:602/1393 train_time:74018ms step_avg:125.03ms
step:603/1393 train_time:74147ms step_avg:125.04ms
step:604/1393 train_time:74275ms step_avg:125.04ms
step:605/1393 train_time:74404ms step_avg:125.05ms
step:606/1393 train_time:74532ms step_avg:125.05ms
step:607/1393 train_time:74661ms step_avg:125.06ms
step:608/1393 train_time:74789ms step_avg:125.06ms
step:609/1393 train_time:74917ms step_avg:125.07ms
step:610/1393 train_time:75046ms step_avg:125.08ms
step:611/1393 train_time:75175ms step_avg:125.08ms
step:612/1393 train_time:75304ms step_avg:125.09ms
step:613/1393 train_time:75432ms step_avg:125.10ms
step:614/1393 train_time:75560ms step_avg:125.10ms
step:615/1393 train_time:75688ms step_avg:125.10ms
step:616/1393 train_time:75817ms step_avg:125.11ms
step:617/1393 train_time:75946ms step_avg:125.12ms
step:618/1393 train_time:76074ms step_avg:125.12ms
step:619/1393 train_time:76203ms step_avg:125.13ms
step:620/1393 train_time:76332ms step_avg:125.13ms
step:621/1393 train_time:76460ms step_avg:125.14ms
step:622/1393 train_time:76589ms step_avg:125.15ms
step:623/1393 train_time:76718ms step_avg:125.15ms
step:624/1393 train_time:76847ms step_avg:125.16ms
step:625/1393 train_time:76975ms step_avg:125.16ms
step:625/1393 val_loss:3.5809 train_time:77103ms step_avg:125.37ms
step:626/1393 train_time:77124ms step_avg:125.20ms
step:627/1393 train_time:77242ms step_avg:125.19ms
step:628/1393 train_time:77371ms step_avg:125.20ms
step:629/1393 train_time:77500ms step_avg:125.20ms
step:630/1393 train_time:77628ms step_avg:125.21ms
step:631/1393 train_time:77756ms step_avg:125.21ms
step:632/1393 train_time:77884ms step_avg:125.22ms
step:633/1393 train_time:78012ms step_avg:125.22ms
step:634/1393 train_time:78141ms step_avg:125.23ms
step:635/1393 train_time:78272ms step_avg:125.23ms
step:636/1393 train_time:78401ms step_avg:125.24ms
step:637/1393 train_time:78531ms step_avg:125.25ms
step:638/1393 train_time:78660ms step_avg:125.25ms
step:639/1393 train_time:78788ms step_avg:125.26ms
step:640/1393 train_time:78916ms step_avg:125.26ms
step:641/1393 train_time:79044ms step_avg:125.27ms
step:642/1393 train_time:79173ms step_avg:125.27ms
step:643/1393 train_time:79302ms step_avg:125.28ms
step:644/1393 train_time:79431ms step_avg:125.29ms
step:645/1393 train_time:79561ms step_avg:125.29ms
step:646/1393 train_time:79690ms step_avg:125.30ms
step:647/1393 train_time:79819ms step_avg:125.30ms
step:648/1393 train_time:79947ms step_avg:125.31ms
step:649/1393 train_time:80076ms step_avg:125.31ms
step:650/1393 train_time:80205ms step_avg:125.32ms
step:651/1393 train_time:80334ms step_avg:125.33ms
step:652/1393 train_time:80464ms step_avg:125.33ms
step:653/1393 train_time:80594ms step_avg:125.34ms
step:654/1393 train_time:80723ms step_avg:125.35ms
step:655/1393 train_time:80852ms step_avg:125.35ms
step:656/1393 train_time:80981ms step_avg:125.36ms
step:657/1393 train_time:81110ms step_avg:125.36ms
step:658/1393 train_time:81238ms step_avg:125.37ms
step:659/1393 train_time:81367ms step_avg:125.37ms
step:660/1393 train_time:81496ms step_avg:125.38ms
step:661/1393 train_time:81626ms step_avg:125.39ms
step:662/1393 train_time:81754ms step_avg:125.39ms
step:663/1393 train_time:81884ms step_avg:125.40ms
step:664/1393 train_time:82013ms step_avg:125.40ms
step:665/1393 train_time:82143ms step_avg:125.41ms
step:666/1393 train_time:82271ms step_avg:125.41ms
step:667/1393 train_time:82400ms step_avg:125.42ms
step:668/1393 train_time:82529ms step_avg:125.42ms
step:669/1393 train_time:82658ms step_avg:125.43ms
step:670/1393 train_time:82787ms step_avg:125.43ms
step:671/1393 train_time:82915ms step_avg:125.44ms
step:672/1393 train_time:83044ms step_avg:125.44ms
step:673/1393 train_time:83173ms step_avg:125.45ms
step:674/1393 train_time:83303ms step_avg:125.46ms
step:675/1393 train_time:83431ms step_avg:125.46ms
step:676/1393 train_time:83560ms step_avg:125.47ms
step:677/1393 train_time:83689ms step_avg:125.47ms
step:678/1393 train_time:83817ms step_avg:125.48ms
step:679/1393 train_time:83947ms step_avg:125.48ms
step:680/1393 train_time:84076ms step_avg:125.49ms
step:681/1393 train_time:84206ms step_avg:125.49ms
step:682/1393 train_time:84335ms step_avg:125.50ms
step:683/1393 train_time:84464ms step_avg:125.50ms
step:684/1393 train_time:84593ms step_avg:125.51ms
step:685/1393 train_time:84722ms step_avg:125.51ms
step:686/1393 train_time:84851ms step_avg:125.52ms
step:687/1393 train_time:84979ms step_avg:125.52ms
step:688/1393 train_time:85108ms step_avg:125.53ms
step:689/1393 train_time:85237ms step_avg:125.53ms
step:690/1393 train_time:85366ms step_avg:125.54ms
step:691/1393 train_time:85496ms step_avg:125.54ms
step:692/1393 train_time:85625ms step_avg:125.55ms
step:693/1393 train_time:85754ms step_avg:125.56ms
step:694/1393 train_time:85883ms step_avg:125.56ms
step:695/1393 train_time:86012ms step_avg:125.56ms
step:696/1393 train_time:86140ms step_avg:125.57ms
step:697/1393 train_time:86269ms step_avg:125.57ms
step:698/1393 train_time:86398ms step_avg:125.58ms
step:699/1393 train_time:86527ms step_avg:125.58ms
step:700/1393 train_time:86656ms step_avg:125.59ms
step:701/1393 train_time:86785ms step_avg:125.59ms
step:702/1393 train_time:86914ms step_avg:125.60ms
step:703/1393 train_time:87044ms step_avg:125.60ms
step:704/1393 train_time:87172ms step_avg:125.61ms
step:705/1393 train_time:87301ms step_avg:125.61ms
step:706/1393 train_time:87431ms step_avg:125.62ms
step:707/1393 train_time:87560ms step_avg:125.62ms
step:708/1393 train_time:87689ms step_avg:125.63ms
step:709/1393 train_time:87818ms step_avg:125.63ms
step:710/1393 train_time:87947ms step_avg:125.64ms
step:711/1393 train_time:88077ms step_avg:125.64ms
step:712/1393 train_time:88206ms step_avg:125.65ms
step:713/1393 train_time:88337ms step_avg:125.66ms
step:714/1393 train_time:88465ms step_avg:125.66ms
step:715/1393 train_time:88594ms step_avg:125.67ms
step:716/1393 train_time:88724ms step_avg:125.67ms
step:717/1393 train_time:88852ms step_avg:125.67ms
step:718/1393 train_time:88982ms step_avg:125.68ms
step:719/1393 train_time:89110ms step_avg:125.68ms
step:720/1393 train_time:89238ms step_avg:125.69ms
step:721/1393 train_time:89367ms step_avg:125.69ms
step:722/1393 train_time:89496ms step_avg:125.70ms
step:723/1393 train_time:89626ms step_avg:125.70ms
step:724/1393 train_time:89755ms step_avg:125.71ms
step:725/1393 train_time:89885ms step_avg:125.71ms
step:726/1393 train_time:90016ms step_avg:125.72ms
step:727/1393 train_time:90147ms step_avg:125.73ms
step:728/1393 train_time:90277ms step_avg:125.73ms
step:729/1393 train_time:90408ms step_avg:125.74ms
step:730/1393 train_time:90539ms step_avg:125.75ms
step:731/1393 train_time:90670ms step_avg:125.76ms
step:732/1393 train_time:90800ms step_avg:125.76ms
step:733/1393 train_time:90932ms step_avg:125.77ms
step:734/1393 train_time:91063ms step_avg:125.78ms
step:735/1393 train_time:91195ms step_avg:125.79ms
step:736/1393 train_time:91325ms step_avg:125.79ms
step:737/1393 train_time:91456ms step_avg:125.80ms
step:738/1393 train_time:91586ms step_avg:125.80ms
step:739/1393 train_time:91716ms step_avg:125.81ms
step:740/1393 train_time:91846ms step_avg:125.82ms
step:741/1393 train_time:91979ms step_avg:125.83ms
step:742/1393 train_time:92110ms step_avg:125.83ms
step:743/1393 train_time:92242ms step_avg:125.84ms
step:744/1393 train_time:92372ms step_avg:125.85ms
step:745/1393 train_time:92503ms step_avg:125.85ms
step:746/1393 train_time:92635ms step_avg:125.86ms
step:747/1393 train_time:92766ms step_avg:125.87ms
step:748/1393 train_time:92897ms step_avg:125.88ms
step:749/1393 train_time:93028ms step_avg:125.88ms
step:750/1393 train_time:93160ms step_avg:125.89ms
step:750/1393 val_loss:3.5264 train_time:93291ms step_avg:126.07ms
step:751/1393 train_time:93312ms step_avg:125.93ms
step:752/1393 train_time:93432ms step_avg:125.92ms
step:753/1393 train_time:93562ms step_avg:125.93ms
step:754/1393 train_time:93693ms step_avg:125.93ms
step:755/1393 train_time:93822ms step_avg:125.94ms
step:756/1393 train_time:93952ms step_avg:125.94ms
step:757/1393 train_time:94083ms step_avg:125.95ms
step:758/1393 train_time:94213ms step_avg:125.95ms
step:759/1393 train_time:94344ms step_avg:125.96ms
step:760/1393 train_time:94476ms step_avg:125.97ms
step:761/1393 train_time:94606ms step_avg:125.97ms
step:762/1393 train_time:94736ms step_avg:125.98ms
step:763/1393 train_time:94867ms step_avg:125.99ms
step:764/1393 train_time:94999ms step_avg:125.99ms
step:765/1393 train_time:95130ms step_avg:126.00ms
step:766/1393 train_time:95260ms step_avg:126.01ms
step:767/1393 train_time:95391ms step_avg:126.01ms
step:768/1393 train_time:95523ms step_avg:126.02ms
step:769/1393 train_time:95654ms step_avg:126.03ms
step:770/1393 train_time:95785ms step_avg:126.03ms
step:771/1393 train_time:95916ms step_avg:126.04ms
step:772/1393 train_time:96047ms step_avg:126.05ms
step:773/1393 train_time:96178ms step_avg:126.05ms
step:774/1393 train_time:96308ms step_avg:126.06ms
step:775/1393 train_time:96438ms step_avg:126.06ms
step:776/1393 train_time:96569ms step_avg:126.07ms
step:777/1393 train_time:96699ms step_avg:126.07ms
step:778/1393 train_time:96830ms step_avg:126.08ms
step:779/1393 train_time:96959ms step_avg:126.09ms
step:780/1393 train_time:97090ms step_avg:126.09ms
step:781/1393 train_time:97221ms step_avg:126.10ms
step:782/1393 train_time:97352ms step_avg:126.10ms
step:783/1393 train_time:97483ms step_avg:126.11ms
step:784/1393 train_time:97615ms step_avg:126.12ms
step:785/1393 train_time:97745ms step_avg:126.12ms
step:786/1393 train_time:97876ms step_avg:126.13ms
step:787/1393 train_time:98007ms step_avg:126.13ms
step:788/1393 train_time:98136ms step_avg:126.14ms
step:789/1393 train_time:98267ms step_avg:126.14ms
step:790/1393 train_time:98397ms step_avg:126.15ms
step:791/1393 train_time:98527ms step_avg:126.16ms
step:792/1393 train_time:98659ms step_avg:126.16ms
step:793/1393 train_time:98790ms step_avg:126.17ms
step:794/1393 train_time:98921ms step_avg:126.17ms
step:795/1393 train_time:99053ms step_avg:126.18ms
step:796/1393 train_time:99185ms step_avg:126.19ms
step:797/1393 train_time:99315ms step_avg:126.19ms
step:798/1393 train_time:99446ms step_avg:126.20ms
step:799/1393 train_time:99577ms step_avg:126.21ms
step:800/1393 train_time:99708ms step_avg:126.21ms
step:801/1393 train_time:99838ms step_avg:126.22ms
step:802/1393 train_time:99969ms step_avg:126.22ms
step:803/1393 train_time:100099ms step_avg:126.23ms
step:804/1393 train_time:100230ms step_avg:126.23ms
step:805/1393 train_time:100362ms step_avg:126.24ms
step:806/1393 train_time:100492ms step_avg:126.25ms
step:807/1393 train_time:100624ms step_avg:126.25ms
step:808/1393 train_time:100755ms step_avg:126.26ms
step:809/1393 train_time:100886ms step_avg:126.27ms
step:810/1393 train_time:101016ms step_avg:126.27ms
step:811/1393 train_time:101147ms step_avg:126.28ms
step:812/1393 train_time:101277ms step_avg:126.28ms
step:813/1393 train_time:101408ms step_avg:126.29ms
step:814/1393 train_time:101538ms step_avg:126.29ms
step:815/1393 train_time:101669ms step_avg:126.30ms
step:816/1393 train_time:101800ms step_avg:126.30ms
step:817/1393 train_time:101930ms step_avg:126.31ms
step:818/1393 train_time:102061ms step_avg:126.31ms
step:819/1393 train_time:102193ms step_avg:126.32ms
step:820/1393 train_time:102323ms step_avg:126.33ms
step:821/1393 train_time:102453ms step_avg:126.33ms
step:822/1393 train_time:102585ms step_avg:126.34ms
step:823/1393 train_time:102716ms step_avg:126.34ms
step:824/1393 train_time:102848ms step_avg:126.35ms
step:825/1393 train_time:102979ms step_avg:126.35ms
step:826/1393 train_time:103110ms step_avg:126.36ms
step:827/1393 train_time:103240ms step_avg:126.37ms
step:828/1393 train_time:103371ms step_avg:126.37ms
step:829/1393 train_time:103502ms step_avg:126.38ms
step:830/1393 train_time:103634ms step_avg:126.38ms
step:831/1393 train_time:103765ms step_avg:126.39ms
step:832/1393 train_time:103896ms step_avg:126.39ms
step:833/1393 train_time:104027ms step_avg:126.40ms
step:834/1393 train_time:104158ms step_avg:126.41ms
step:835/1393 train_time:104289ms step_avg:126.41ms
step:836/1393 train_time:104421ms step_avg:126.42ms
step:837/1393 train_time:104552ms step_avg:126.42ms
step:838/1393 train_time:104685ms step_avg:126.43ms
step:839/1393 train_time:104815ms step_avg:126.43ms
step:840/1393 train_time:104946ms step_avg:126.44ms
step:841/1393 train_time:105076ms step_avg:126.45ms
step:842/1393 train_time:105207ms step_avg:126.45ms
step:843/1393 train_time:105338ms step_avg:126.46ms
step:844/1393 train_time:105468ms step_avg:126.46ms
step:845/1393 train_time:105599ms step_avg:126.47ms
step:846/1393 train_time:105730ms step_avg:126.47ms
step:847/1393 train_time:105861ms step_avg:126.48ms
step:848/1393 train_time:105992ms step_avg:126.48ms
step:849/1393 train_time:106124ms step_avg:126.49ms
step:850/1393 train_time:106255ms step_avg:126.49ms
step:851/1393 train_time:106387ms step_avg:126.50ms
step:852/1393 train_time:106518ms step_avg:126.51ms
step:853/1393 train_time:106650ms step_avg:126.51ms
step:854/1393 train_time:106780ms step_avg:126.52ms
step:855/1393 train_time:106911ms step_avg:126.52ms
step:856/1393 train_time:107042ms step_avg:126.53ms
step:857/1393 train_time:107172ms step_avg:126.53ms
step:858/1393 train_time:107303ms step_avg:126.54ms
step:859/1393 train_time:107435ms step_avg:126.54ms
step:860/1393 train_time:107567ms step_avg:126.55ms
step:861/1393 train_time:107698ms step_avg:126.56ms
step:862/1393 train_time:107831ms step_avg:126.56ms
step:863/1393 train_time:107962ms step_avg:126.57ms
step:864/1393 train_time:108093ms step_avg:126.57ms
step:865/1393 train_time:108224ms step_avg:126.58ms
step:866/1393 train_time:108356ms step_avg:126.58ms
step:867/1393 train_time:108488ms step_avg:126.59ms
step:868/1393 train_time:108617ms step_avg:126.59ms
step:869/1393 train_time:108748ms step_avg:126.60ms
step:870/1393 train_time:108878ms step_avg:126.60ms
step:871/1393 train_time:109009ms step_avg:126.61ms
step:872/1393 train_time:109140ms step_avg:126.61ms
step:873/1393 train_time:109272ms step_avg:126.62ms
step:874/1393 train_time:109403ms step_avg:126.62ms
step:875/1393 train_time:109536ms step_avg:126.63ms
step:875/1393 val_loss:3.4758 train_time:109666ms step_avg:126.78ms
step:876/1393 train_time:109687ms step_avg:126.66ms
step:877/1393 train_time:109811ms step_avg:126.66ms
step:878/1393 train_time:109943ms step_avg:126.66ms
step:879/1393 train_time:110074ms step_avg:126.67ms
step:880/1393 train_time:110204ms step_avg:126.67ms
step:881/1393 train_time:110334ms step_avg:126.68ms
step:882/1393 train_time:110465ms step_avg:126.68ms
step:883/1393 train_time:110595ms step_avg:126.68ms
step:884/1393 train_time:110728ms step_avg:126.69ms
step:885/1393 train_time:110861ms step_avg:126.70ms
step:886/1393 train_time:110992ms step_avg:126.70ms
step:887/1393 train_time:111122ms step_avg:126.71ms
step:888/1393 train_time:111253ms step_avg:126.71ms
step:889/1393 train_time:111385ms step_avg:126.72ms
step:890/1393 train_time:111515ms step_avg:126.72ms
step:891/1393 train_time:111645ms step_avg:126.73ms
step:892/1393 train_time:111776ms step_avg:126.73ms
step:893/1393 train_time:111908ms step_avg:126.74ms
step:894/1393 train_time:112040ms step_avg:126.74ms
step:895/1393 train_time:112172ms step_avg:126.75ms
step:896/1393 train_time:112303ms step_avg:126.75ms
step:897/1393 train_time:112433ms step_avg:126.76ms
step:898/1393 train_time:112564ms step_avg:126.76ms
step:899/1393 train_time:112695ms step_avg:126.77ms
step:900/1393 train_time:112826ms step_avg:126.77ms
step:901/1393 train_time:112956ms step_avg:126.77ms
step:902/1393 train_time:113086ms step_avg:126.78ms
step:903/1393 train_time:113217ms step_avg:126.78ms
step:904/1393 train_time:113349ms step_avg:126.79ms
step:905/1393 train_time:113479ms step_avg:126.79ms
step:906/1393 train_time:113611ms step_avg:126.80ms
step:907/1393 train_time:113742ms step_avg:126.80ms
step:908/1393 train_time:113873ms step_avg:126.81ms
step:909/1393 train_time:114004ms step_avg:126.81ms
step:910/1393 train_time:114135ms step_avg:126.82ms
step:911/1393 train_time:114266ms step_avg:126.82ms
step:912/1393 train_time:114397ms step_avg:126.83ms
step:913/1393 train_time:114529ms step_avg:126.83ms
step:914/1393 train_time:114659ms step_avg:126.84ms
step:915/1393 train_time:114791ms step_avg:126.84ms
step:916/1393 train_time:114924ms step_avg:126.85ms
step:917/1393 train_time:115055ms step_avg:126.85ms
step:918/1393 train_time:115186ms step_avg:126.86ms
step:919/1393 train_time:115320ms step_avg:126.86ms
step:920/1393 train_time:115452ms step_avg:126.87ms
step:921/1393 train_time:115582ms step_avg:126.87ms
step:922/1393 train_time:115713ms step_avg:126.88ms
step:923/1393 train_time:115844ms step_avg:126.88ms
step:924/1393 train_time:115974ms step_avg:126.89ms
step:925/1393 train_time:116105ms step_avg:126.89ms
step:926/1393 train_time:116236ms step_avg:126.89ms
step:927/1393 train_time:116366ms step_avg:126.90ms
step:928/1393 train_time:116497ms step_avg:126.90ms
step:929/1393 train_time:116629ms step_avg:126.91ms
step:930/1393 train_time:116759ms step_avg:126.91ms
step:931/1393 train_time:116892ms step_avg:126.92ms
step:932/1393 train_time:117024ms step_avg:126.92ms
step:933/1393 train_time:117157ms step_avg:126.93ms
step:934/1393 train_time:117289ms step_avg:126.94ms
step:935/1393 train_time:117422ms step_avg:126.94ms
step:936/1393 train_time:117555ms step_avg:126.95ms
step:937/1393 train_time:117690ms step_avg:126.96ms
step:938/1393 train_time:117824ms step_avg:126.97ms
step:939/1393 train_time:117956ms step_avg:126.97ms
step:940/1393 train_time:118092ms step_avg:126.98ms
step:941/1393 train_time:118224ms step_avg:126.99ms
step:942/1393 train_time:118358ms step_avg:126.99ms
step:943/1393 train_time:118493ms step_avg:127.00ms
step:944/1393 train_time:118628ms step_avg:127.01ms
step:945/1393 train_time:118762ms step_avg:127.02ms
step:946/1393 train_time:118894ms step_avg:127.02ms
step:947/1393 train_time:119028ms step_avg:127.03ms
step:948/1393 train_time:119160ms step_avg:127.04ms
step:949/1393 train_time:119294ms step_avg:127.04ms
step:950/1393 train_time:119426ms step_avg:127.05ms
step:951/1393 train_time:119560ms step_avg:127.06ms
step:952/1393 train_time:119692ms step_avg:127.06ms
step:953/1393 train_time:119826ms step_avg:127.07ms
step:954/1393 train_time:119958ms step_avg:127.07ms
step:955/1393 train_time:120091ms step_avg:127.08ms
step:956/1393 train_time:120226ms step_avg:127.09ms
step:957/1393 train_time:120358ms step_avg:127.09ms
step:958/1393 train_time:120491ms step_avg:127.10ms
step:959/1393 train_time:120625ms step_avg:127.11ms
step:960/1393 train_time:120757ms step_avg:127.11ms
step:961/1393 train_time:120890ms step_avg:127.12ms
step:962/1393 train_time:121023ms step_avg:127.12ms
step:963/1393 train_time:121156ms step_avg:127.13ms
step:964/1393 train_time:121290ms step_avg:127.14ms
step:965/1393 train_time:121423ms step_avg:127.14ms
step:966/1393 train_time:121556ms step_avg:127.15ms
step:967/1393 train_time:121688ms step_avg:127.16ms
step:968/1393 train_time:121819ms step_avg:127.16ms
step:969/1393 train_time:121952ms step_avg:127.17ms
step:970/1393 train_time:122085ms step_avg:127.17ms
step:971/1393 train_time:122219ms step_avg:127.18ms
step:972/1393 train_time:122351ms step_avg:127.18ms
step:973/1393 train_time:122486ms step_avg:127.19ms
step:974/1393 train_time:122617ms step_avg:127.20ms
step:975/1393 train_time:122750ms step_avg:127.20ms
step:976/1393 train_time:122883ms step_avg:127.21ms
step:977/1393 train_time:123015ms step_avg:127.21ms
step:978/1393 train_time:123146ms step_avg:127.22ms
step:979/1393 train_time:123279ms step_avg:127.22ms
step:980/1393 train_time:123414ms step_avg:127.23ms
step:981/1393 train_time:123545ms step_avg:127.24ms
step:982/1393 train_time:123677ms step_avg:127.24ms
step:983/1393 train_time:123809ms step_avg:127.24ms
step:984/1393 train_time:123941ms step_avg:127.25ms
step:985/1393 train_time:124074ms step_avg:127.26ms
step:986/1393 train_time:124209ms step_avg:127.26ms
step:987/1393 train_time:124342ms step_avg:127.27ms
step:988/1393 train_time:124474ms step_avg:127.27ms
step:989/1393 train_time:124606ms step_avg:127.28ms
step:990/1393 train_time:124740ms step_avg:127.29ms
step:991/1393 train_time:124874ms step_avg:127.29ms
step:992/1393 train_time:125007ms step_avg:127.30ms
step:993/1393 train_time:125144ms step_avg:127.31ms
step:994/1393 train_time:125276ms step_avg:127.31ms
step:995/1393 train_time:125408ms step_avg:127.32ms
step:996/1393 train_time:125541ms step_avg:127.32ms
step:997/1393 train_time:125673ms step_avg:127.33ms
step:998/1393 train_time:125805ms step_avg:127.33ms
step:999/1393 train_time:125938ms step_avg:127.34ms
step:1000/1393 train_time:126070ms step_avg:127.34ms
step:1000/1393 val_loss:3.4127 train_time:126201ms step_avg:127.48ms
step:1001/1393 train_time:126222ms step_avg:127.37ms
step:1002/1393 train_time:126343ms step_avg:127.36ms
step:1003/1393 train_time:126477ms step_avg:127.37ms
step:1004/1393 train_time:126609ms step_avg:127.37ms
step:1005/1393 train_time:126741ms step_avg:127.38ms
step:1006/1393 train_time:126874ms step_avg:127.38ms
step:1007/1393 train_time:127006ms step_avg:127.39ms
step:1008/1393 train_time:127138ms step_avg:127.39ms
step:1009/1393 train_time:127273ms step_avg:127.40ms
step:1010/1393 train_time:127406ms step_avg:127.41ms
step:1011/1393 train_time:127540ms step_avg:127.41ms
step:1012/1393 train_time:127672ms step_avg:127.42ms
step:1013/1393 train_time:127805ms step_avg:127.42ms
step:1014/1393 train_time:127938ms step_avg:127.43ms
step:1015/1393 train_time:128069ms step_avg:127.43ms
step:1016/1393 train_time:128202ms step_avg:127.44ms
step:1017/1393 train_time:128334ms step_avg:127.44ms
step:1018/1393 train_time:128467ms step_avg:127.45ms
step:1019/1393 train_time:128601ms step_avg:127.45ms
step:1020/1393 train_time:128734ms step_avg:127.46ms
step:1021/1393 train_time:128867ms step_avg:127.46ms
step:1022/1393 train_time:128999ms step_avg:127.47ms
step:1023/1393 train_time:129132ms step_avg:127.47ms
step:1024/1393 train_time:129265ms step_avg:127.48ms
step:1025/1393 train_time:129399ms step_avg:127.49ms
step:1026/1393 train_time:129531ms step_avg:127.49ms
step:1027/1393 train_time:129665ms step_avg:127.50ms
step:1028/1393 train_time:129799ms step_avg:127.50ms
step:1029/1393 train_time:129932ms step_avg:127.51ms
step:1030/1393 train_time:130067ms step_avg:127.52ms
step:1031/1393 train_time:130199ms step_avg:127.52ms
step:1032/1393 train_time:130331ms step_avg:127.53ms
step:1033/1393 train_time:130463ms step_avg:127.53ms
step:1034/1393 train_time:130596ms step_avg:127.54ms
step:1035/1393 train_time:130730ms step_avg:127.54ms
step:1036/1393 train_time:130862ms step_avg:127.55ms
step:1037/1393 train_time:130995ms step_avg:127.55ms
step:1038/1393 train_time:131128ms step_avg:127.56ms
step:1039/1393 train_time:131260ms step_avg:127.56ms
step:1040/1393 train_time:131392ms step_avg:127.57ms
step:1041/1393 train_time:131526ms step_avg:127.57ms
step:1042/1393 train_time:131660ms step_avg:127.58ms
step:1043/1393 train_time:131793ms step_avg:127.58ms
step:1044/1393 train_time:131930ms step_avg:127.59ms
step:1045/1393 train_time:132063ms step_avg:127.60ms
step:1046/1393 train_time:132196ms step_avg:127.60ms
step:1047/1393 train_time:132329ms step_avg:127.61ms
step:1048/1393 train_time:132461ms step_avg:127.61ms
step:1049/1393 train_time:132594ms step_avg:127.62ms
step:1050/1393 train_time:132728ms step_avg:127.62ms
step:1051/1393 train_time:132862ms step_avg:127.63ms
step:1052/1393 train_time:132994ms step_avg:127.63ms
step:1053/1393 train_time:133127ms step_avg:127.64ms
step:1054/1393 train_time:133260ms step_avg:127.64ms
step:1055/1393 train_time:133394ms step_avg:127.65ms
step:1056/1393 train_time:133525ms step_avg:127.65ms
step:1057/1393 train_time:133658ms step_avg:127.66ms
step:1058/1393 train_time:133791ms step_avg:127.66ms
step:1059/1393 train_time:133924ms step_avg:127.67ms
step:1060/1393 train_time:134059ms step_avg:127.68ms
step:1061/1393 train_time:134190ms step_avg:127.68ms
step:1062/1393 train_time:134324ms step_avg:127.68ms
step:1063/1393 train_time:134456ms step_avg:127.69ms
step:1064/1393 train_time:134589ms step_avg:127.69ms
step:1065/1393 train_time:134722ms step_avg:127.70ms
step:1066/1393 train_time:134855ms step_avg:127.70ms
step:1067/1393 train_time:134988ms step_avg:127.71ms
step:1068/1393 train_time:135120ms step_avg:127.71ms
step:1069/1393 train_time:135254ms step_avg:127.72ms
step:1070/1393 train_time:135386ms step_avg:127.72ms
step:1071/1393 train_time:135521ms step_avg:127.73ms
step:1072/1393 train_time:135654ms step_avg:127.73ms
step:1073/1393 train_time:135787ms step_avg:127.74ms
step:1074/1393 train_time:135920ms step_avg:127.74ms
step:1075/1393 train_time:136052ms step_avg:127.75ms
step:1076/1393 train_time:136184ms step_avg:127.75ms
step:1077/1393 train_time:136317ms step_avg:127.76ms
step:1078/1393 train_time:136449ms step_avg:127.76ms
step:1079/1393 train_time:136586ms step_avg:127.77ms
step:1080/1393 train_time:136719ms step_avg:127.77ms
step:1081/1393 train_time:136854ms step_avg:127.78ms
step:1082/1393 train_time:136986ms step_avg:127.79ms
step:1083/1393 train_time:137119ms step_avg:127.79ms
step:1084/1393 train_time:137252ms step_avg:127.80ms
step:1085/1393 train_time:137384ms step_avg:127.80ms
step:1086/1393 train_time:137517ms step_avg:127.80ms
step:1087/1393 train_time:137650ms step_avg:127.81ms
step:1088/1393 train_time:137783ms step_avg:127.81ms
step:1089/1393 train_time:137917ms step_avg:127.82ms
step:1090/1393 train_time:138052ms step_avg:127.83ms
step:1091/1393 train_time:138184ms step_avg:127.83ms
step:1092/1393 train_time:138317ms step_avg:127.83ms
step:1093/1393 train_time:138451ms step_avg:127.84ms
step:1094/1393 train_time:138583ms step_avg:127.84ms
step:1095/1393 train_time:138716ms step_avg:127.85ms
step:1096/1393 train_time:138849ms step_avg:127.85ms
step:1097/1393 train_time:138982ms step_avg:127.86ms
step:1098/1393 train_time:139115ms step_avg:127.86ms
step:1099/1393 train_time:139249ms step_avg:127.87ms
step:1100/1393 train_time:139382ms step_avg:127.87ms
step:1101/1393 train_time:139515ms step_avg:127.88ms
step:1102/1393 train_time:139648ms step_avg:127.88ms
step:1103/1393 train_time:139780ms step_avg:127.89ms
step:1104/1393 train_time:139913ms step_avg:127.89ms
step:1105/1393 train_time:140049ms step_avg:127.90ms
step:1106/1393 train_time:140182ms step_avg:127.90ms
step:1107/1393 train_time:140313ms step_avg:127.91ms
step:1108/1393 train_time:140449ms step_avg:127.91ms
step:1109/1393 train_time:140581ms step_avg:127.92ms
step:1110/1393 train_time:140714ms step_avg:127.92ms
step:1111/1393 train_time:140847ms step_avg:127.93ms
step:1112/1393 train_time:140981ms step_avg:127.93ms
step:1113/1393 train_time:141113ms step_avg:127.94ms
step:1114/1393 train_time:141247ms step_avg:127.94ms
step:1115/1393 train_time:141379ms step_avg:127.95ms
step:1116/1393 train_time:141513ms step_avg:127.95ms
step:1117/1393 train_time:141646ms step_avg:127.96ms
step:1118/1393 train_time:141781ms step_avg:127.96ms
step:1119/1393 train_time:141913ms step_avg:127.97ms
step:1120/1393 train_time:142045ms step_avg:127.97ms
step:1121/1393 train_time:142178ms step_avg:127.97ms
step:1122/1393 train_time:142310ms step_avg:127.98ms
step:1123/1393 train_time:142444ms step_avg:127.98ms
step:1124/1393 train_time:142577ms step_avg:127.99ms
step:1125/1393 train_time:142709ms step_avg:127.99ms
step:1125/1393 val_loss:3.3613 train_time:142843ms step_avg:128.11ms
step:1126/1393 train_time:142864ms step_avg:128.01ms
step:1127/1393 train_time:142985ms step_avg:128.01ms
step:1128/1393 train_time:143119ms step_avg:128.01ms
step:1129/1393 train_time:143252ms step_avg:128.02ms
step:1130/1393 train_time:143384ms step_avg:128.02ms
step:1131/1393 train_time:143517ms step_avg:128.03ms
step:1132/1393 train_time:143650ms step_avg:128.03ms
step:1133/1393 train_time:143781ms step_avg:128.03ms
step:1134/1393 train_time:143916ms step_avg:128.04ms
step:1135/1393 train_time:144050ms step_avg:128.04ms
step:1136/1393 train_time:144187ms step_avg:128.05ms
step:1137/1393 train_time:144318ms step_avg:128.06ms
step:1138/1393 train_time:144454ms step_avg:128.06ms
step:1139/1393 train_time:144588ms step_avg:128.07ms
step:1140/1393 train_time:144722ms step_avg:128.07ms
step:1141/1393 train_time:144856ms step_avg:128.08ms
step:1142/1393 train_time:144990ms step_avg:128.08ms
step:1143/1393 train_time:145125ms step_avg:128.09ms
step:1144/1393 train_time:145259ms step_avg:128.09ms
step:1145/1393 train_time:145394ms step_avg:128.10ms
step:1146/1393 train_time:145529ms step_avg:128.11ms
step:1147/1393 train_time:145664ms step_avg:128.11ms
step:1148/1393 train_time:145799ms step_avg:128.12ms
step:1149/1393 train_time:145933ms step_avg:128.12ms
step:1150/1393 train_time:146067ms step_avg:128.13ms
step:1151/1393 train_time:146202ms step_avg:128.13ms
step:1152/1393 train_time:146336ms step_avg:128.14ms
step:1153/1393 train_time:146473ms step_avg:128.15ms
step:1154/1393 train_time:146607ms step_avg:128.15ms
step:1155/1393 train_time:146742ms step_avg:128.16ms
step:1156/1393 train_time:146878ms step_avg:128.17ms
step:1157/1393 train_time:147013ms step_avg:128.17ms
step:1158/1393 train_time:147147ms step_avg:128.18ms
step:1159/1393 train_time:147280ms step_avg:128.18ms
step:1160/1393 train_time:147415ms step_avg:128.19ms
step:1161/1393 train_time:147550ms step_avg:128.19ms
step:1162/1393 train_time:147683ms step_avg:128.20ms
step:1163/1393 train_time:147818ms step_avg:128.20ms
step:1164/1393 train_time:147954ms step_avg:128.21ms
step:1165/1393 train_time:148087ms step_avg:128.21ms
step:1166/1393 train_time:148221ms step_avg:128.22ms
step:1167/1393 train_time:148356ms step_avg:128.22ms
step:1168/1393 train_time:148490ms step_avg:128.23ms
step:1169/1393 train_time:148624ms step_avg:128.23ms
step:1170/1393 train_time:148758ms step_avg:128.24ms
step:1171/1393 train_time:148892ms step_avg:128.24ms
step:1172/1393 train_time:149029ms step_avg:128.25ms
step:1173/1393 train_time:149162ms step_avg:128.26ms
step:1174/1393 train_time:149301ms step_avg:128.27ms
step:1175/1393 train_time:149436ms step_avg:128.27ms
step:1176/1393 train_time:149572ms step_avg:128.28ms
step:1177/1393 train_time:149709ms step_avg:128.29ms
step:1178/1393 train_time:149844ms step_avg:128.29ms
step:1179/1393 train_time:149977ms step_avg:128.30ms
step:1180/1393 train_time:150113ms step_avg:128.30ms
step:1181/1393 train_time:150248ms step_avg:128.31ms
step:1182/1393 train_time:150381ms step_avg:128.31ms
step:1183/1393 train_time:150516ms step_avg:128.32ms
step:1184/1393 train_time:150651ms step_avg:128.32ms
step:1185/1393 train_time:150786ms step_avg:128.33ms
step:1186/1393 train_time:150921ms step_avg:128.33ms
step:1187/1393 train_time:151060ms step_avg:128.34ms
step:1188/1393 train_time:151194ms step_avg:128.35ms
step:1189/1393 train_time:151330ms step_avg:128.35ms
step:1190/1393 train_time:151463ms step_avg:128.36ms
step:1191/1393 train_time:151598ms step_avg:128.36ms
step:1192/1393 train_time:151732ms step_avg:128.37ms
step:1193/1393 train_time:151866ms step_avg:128.37ms
step:1194/1393 train_time:152000ms step_avg:128.38ms
step:1195/1393 train_time:152134ms step_avg:128.38ms
step:1196/1393 train_time:152269ms step_avg:128.39ms
step:1197/1393 train_time:152404ms step_avg:128.39ms
step:1198/1393 train_time:152541ms step_avg:128.40ms
step:1199/1393 train_time:152675ms step_avg:128.41ms
step:1200/1393 train_time:152809ms step_avg:128.41ms
step:1201/1393 train_time:152943ms step_avg:128.42ms
step:1202/1393 train_time:153081ms step_avg:128.42ms
step:1203/1393 train_time:153219ms step_avg:128.43ms
step:1204/1393 train_time:153354ms step_avg:128.44ms
step:1205/1393 train_time:153490ms step_avg:128.44ms
step:1206/1393 train_time:153625ms step_avg:128.45ms
step:1207/1393 train_time:153760ms step_avg:128.45ms
step:1208/1393 train_time:153893ms step_avg:128.46ms
step:1209/1393 train_time:154029ms step_avg:128.46ms
step:1210/1393 train_time:154164ms step_avg:128.47ms
step:1211/1393 train_time:154300ms step_avg:128.48ms
step:1212/1393 train_time:154434ms step_avg:128.48ms
step:1213/1393 train_time:154568ms step_avg:128.49ms
step:1214/1393 train_time:154701ms step_avg:128.49ms
step:1215/1393 train_time:154837ms step_avg:128.50ms
step:1216/1393 train_time:154972ms step_avg:128.50ms
step:1217/1393 train_time:155106ms step_avg:128.51ms
step:1218/1393 train_time:155239ms step_avg:128.51ms
step:1219/1393 train_time:155372ms step_avg:128.51ms
step:1220/1393 train_time:155506ms step_avg:128.52ms
step:1221/1393 train_time:155639ms step_avg:128.52ms
step:1222/1393 train_time:155774ms step_avg:128.53ms
step:1223/1393 train_time:155908ms step_avg:128.53ms
step:1224/1393 train_time:156043ms step_avg:128.54ms
step:1225/1393 train_time:156181ms step_avg:128.54ms
step:1226/1393 train_time:156314ms step_avg:128.55ms
step:1227/1393 train_time:156447ms step_avg:128.55ms
step:1228/1393 train_time:156583ms step_avg:128.56ms
step:1229/1393 train_time:156715ms step_avg:128.56ms
step:1230/1393 train_time:156851ms step_avg:128.57ms
step:1231/1393 train_time:156986ms step_avg:128.57ms
step:1232/1393 train_time:157123ms step_avg:128.58ms
step:1233/1393 train_time:157257ms step_avg:128.58ms
step:1234/1393 train_time:157390ms step_avg:128.59ms
step:1235/1393 train_time:157525ms step_avg:128.59ms
step:1236/1393 train_time:157659ms step_avg:128.60ms
step:1237/1393 train_time:157792ms step_avg:128.60ms
step:1238/1393 train_time:157930ms step_avg:128.61ms
step:1239/1393 train_time:158064ms step_avg:128.61ms
step:1240/1393 train_time:158199ms step_avg:128.62ms
step:1241/1393 train_time:158334ms step_avg:128.62ms
step:1242/1393 train_time:158468ms step_avg:128.63ms
step:1243/1393 train_time:158603ms step_avg:128.63ms
step:1244/1393 train_time:158738ms step_avg:128.64ms
step:1245/1393 train_time:158872ms step_avg:128.64ms
step:1246/1393 train_time:159005ms step_avg:128.65ms
step:1247/1393 train_time:159140ms step_avg:128.65ms
step:1248/1393 train_time:159273ms step_avg:128.65ms
step:1249/1393 train_time:159408ms step_avg:128.66ms
step:1250/1393 train_time:159542ms step_avg:128.66ms
step:1250/1393 val_loss:3.3145 train_time:159676ms step_avg:128.77ms
step:1251/1393 train_time:159697ms step_avg:128.68ms
step:1252/1393 train_time:159820ms step_avg:128.68ms
step:1253/1393 train_time:159953ms step_avg:128.68ms
step:1254/1393 train_time:160086ms step_avg:128.69ms
step:1255/1393 train_time:160226ms step_avg:128.70ms
step:1256/1393 train_time:160360ms step_avg:128.70ms
step:1257/1393 train_time:160493ms step_avg:128.70ms
step:1258/1393 train_time:160627ms step_avg:128.71ms
step:1259/1393 train_time:160764ms step_avg:128.71ms
step:1260/1393 train_time:160898ms step_avg:128.72ms
step:1261/1393 train_time:161032ms step_avg:128.72ms
step:1262/1393 train_time:161167ms step_avg:128.73ms
step:1263/1393 train_time:161302ms step_avg:128.73ms
step:1264/1393 train_time:161436ms step_avg:128.74ms
step:1265/1393 train_time:161570ms step_avg:128.74ms
step:1266/1393 train_time:161705ms step_avg:128.75ms
step:1267/1393 train_time:161841ms step_avg:128.75ms
step:1268/1393 train_time:161974ms step_avg:128.76ms
step:1269/1393 train_time:162110ms step_avg:128.76ms
step:1270/1393 train_time:162246ms step_avg:128.77ms
step:1271/1393 train_time:162381ms step_avg:128.77ms
step:1272/1393 train_time:162515ms step_avg:128.78ms
step:1273/1393 train_time:162649ms step_avg:128.78ms
step:1274/1393 train_time:162784ms step_avg:128.78ms
step:1275/1393 train_time:162919ms step_avg:128.79ms
step:1276/1393 train_time:163053ms step_avg:128.79ms
step:1277/1393 train_time:163186ms step_avg:128.80ms
step:1278/1393 train_time:163321ms step_avg:128.80ms
step:1279/1393 train_time:163455ms step_avg:128.81ms
step:1280/1393 train_time:163593ms step_avg:128.81ms
step:1281/1393 train_time:163727ms step_avg:128.82ms
step:1282/1393 train_time:163860ms step_avg:128.82ms
step:1283/1393 train_time:163995ms step_avg:128.83ms
step:1284/1393 train_time:164130ms step_avg:128.83ms
step:1285/1393 train_time:164263ms step_avg:128.83ms
step:1286/1393 train_time:164399ms step_avg:128.84ms
step:1287/1393 train_time:164533ms step_avg:128.84ms
step:1288/1393 train_time:164667ms step_avg:128.85ms
step:1289/1393 train_time:164803ms step_avg:128.85ms
step:1290/1393 train_time:164939ms step_avg:128.86ms
step:1291/1393 train_time:165076ms step_avg:128.86ms
step:1292/1393 train_time:165211ms step_avg:128.87ms
step:1293/1393 train_time:165348ms step_avg:128.88ms
step:1294/1393 train_time:165481ms step_avg:128.88ms
step:1295/1393 train_time:165616ms step_avg:128.88ms
step:1296/1393 train_time:165752ms step_avg:128.89ms
step:1297/1393 train_time:165887ms step_avg:128.89ms
step:1298/1393 train_time:166022ms step_avg:128.90ms
step:1299/1393 train_time:166155ms step_avg:128.90ms
step:1300/1393 train_time:166291ms step_avg:128.91ms
step:1301/1393 train_time:166425ms step_avg:128.91ms
step:1302/1393 train_time:166559ms step_avg:128.92ms
step:1303/1393 train_time:166695ms step_avg:128.92ms
step:1304/1393 train_time:166830ms step_avg:128.93ms
step:1305/1393 train_time:166965ms step_avg:128.93ms
step:1306/1393 train_time:167100ms step_avg:128.93ms
step:1307/1393 train_time:167234ms step_avg:128.94ms
step:1308/1393 train_time:167369ms step_avg:128.94ms
step:1309/1393 train_time:167505ms step_avg:128.95ms
step:1310/1393 train_time:167642ms step_avg:128.96ms
step:1311/1393 train_time:167776ms step_avg:128.96ms
step:1312/1393 train_time:167910ms step_avg:128.96ms
step:1313/1393 train_time:168046ms step_avg:128.97ms
step:1314/1393 train_time:168179ms step_avg:128.97ms
step:1315/1393 train_time:168313ms step_avg:128.98ms
step:1316/1393 train_time:168448ms step_avg:128.98ms
step:1317/1393 train_time:168582ms step_avg:128.98ms
step:1318/1393 train_time:168717ms step_avg:128.99ms
step:1319/1393 train_time:168854ms step_avg:128.99ms
step:1320/1393 train_time:168988ms step_avg:129.00ms
step:1321/1393 train_time:169122ms step_avg:129.00ms
step:1322/1393 train_time:169259ms step_avg:129.01ms
step:1323/1393 train_time:169393ms step_avg:129.01ms
step:1324/1393 train_time:169527ms step_avg:129.02ms
step:1325/1393 train_time:169663ms step_avg:129.02ms
step:1326/1393 train_time:169799ms step_avg:129.03ms
step:1327/1393 train_time:169933ms step_avg:129.03ms
step:1328/1393 train_time:170067ms step_avg:129.03ms
step:1329/1393 train_time:170206ms step_avg:129.04ms
step:1330/1393 train_time:170341ms step_avg:129.05ms
step:1331/1393 train_time:170478ms step_avg:129.05ms
step:1332/1393 train_time:170616ms step_avg:129.06ms
step:1333/1393 train_time:170751ms step_avg:129.06ms
step:1334/1393 train_time:170885ms step_avg:129.07ms
step:1335/1393 train_time:171020ms step_avg:129.07ms
step:1336/1393 train_time:171158ms step_avg:129.08ms
step:1337/1393 train_time:171295ms step_avg:129.08ms
step:1338/1393 train_time:171428ms step_avg:129.09ms
step:1339/1393 train_time:171563ms step_avg:129.09ms
step:1340/1393 train_time:171700ms step_avg:129.10ms
step:1341/1393 train_time:171834ms step_avg:129.10ms
step:1342/1393 train_time:171969ms step_avg:129.11ms
step:1343/1393 train_time:172104ms step_avg:129.11ms
step:1344/1393 train_time:172239ms step_avg:129.12ms
step:1345/1393 train_time:172376ms step_avg:129.12ms
step:1346/1393 train_time:172510ms step_avg:129.12ms
step:1347/1393 train_time:172647ms step_avg:129.13ms
step:1348/1393 train_time:172782ms step_avg:129.13ms
step:1349/1393 train_time:172916ms step_avg:129.14ms
step:1350/1393 train_time:173051ms step_avg:129.14ms
step:1351/1393 train_time:173187ms step_avg:129.15ms
step:1352/1393 train_time:173326ms step_avg:129.15ms
step:1353/1393 train_time:173463ms step_avg:129.16ms
step:1354/1393 train_time:173598ms step_avg:129.17ms
step:1355/1393 train_time:173732ms step_avg:129.17ms
step:1356/1393 train_time:173867ms step_avg:129.17ms
step:1357/1393 train_time:174002ms step_avg:129.18ms
step:1358/1393 train_time:174140ms step_avg:129.18ms
step:1359/1393 train_time:174277ms step_avg:129.19ms
step:1360/1393 train_time:174414ms step_avg:129.20ms
step:1361/1393 train_time:174551ms step_avg:129.20ms
step:1362/1393 train_time:174689ms step_avg:129.21ms
step:1363/1393 train_time:174828ms step_avg:129.21ms
step:1364/1393 train_time:174964ms step_avg:129.22ms
step:1365/1393 train_time:175097ms step_avg:129.22ms
step:1366/1393 train_time:175233ms step_avg:129.23ms
step:1367/1393 train_time:175370ms step_avg:129.23ms
step:1368/1393 train_time:175505ms step_avg:129.24ms
step:1369/1393 train_time:175644ms step_avg:129.24ms
step:1370/1393 train_time:175783ms step_avg:129.25ms
step:1371/1393 train_time:175920ms step_avg:129.26ms
step:1372/1393 train_time:176059ms step_avg:129.27ms
step:1373/1393 train_time:176194ms step_avg:129.27ms
step:1374/1393 train_time:176333ms step_avg:129.28ms
step:1375/1393 train_time:176466ms step_avg:129.28ms
step:1375/1393 val_loss:3.2800 train_time:176600ms step_avg:129.38ms
step:1376/1393 train_time:176621ms step_avg:129.30ms
step:1377/1393 train_time:176747ms step_avg:129.30ms
step:1378/1393 train_time:176881ms step_avg:129.30ms
step:1379/1393 train_time:177017ms step_avg:129.30ms
step:1380/1393 train_time:177153ms step_avg:129.31ms
step:1381/1393 train_time:177289ms step_avg:129.31ms
step:1382/1393 train_time:177425ms step_avg:129.32ms
step:1383/1393 train_time:177560ms step_avg:129.32ms
step:1384/1393 train_time:177698ms step_avg:129.33ms
step:1385/1393 train_time:177834ms step_avg:129.33ms
step:1386/1393 train_time:177969ms step_avg:129.34ms
step:1387/1393 train_time:178107ms step_avg:129.34ms
step:1388/1393 train_time:178241ms step_avg:129.35ms
step:1389/1393 train_time:178376ms step_avg:129.35ms
step:1390/1393 train_time:178511ms step_avg:129.36ms
step:1391/1393 train_time:178647ms step_avg:129.36ms
step:1392/1393 train_time:178783ms step_avg:129.37ms
step:1393/1393 train_time:178918ms step_avg:129.37ms
step:1393/1393 val_loss:3.2762 train_time:179053ms step_avg:129.47ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
