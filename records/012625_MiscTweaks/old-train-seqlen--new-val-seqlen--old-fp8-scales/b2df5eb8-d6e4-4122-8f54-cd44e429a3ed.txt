import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:44:54 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:24585ms step_avg:nanms
step:2/1393 train_time:25050ms step_avg:nanms
step:3/1393 train_time:25232ms step_avg:nanms
step:4/1393 train_time:25389ms step_avg:nanms
step:5/1393 train_time:25509ms step_avg:nanms
step:6/1393 train_time:25630ms step_avg:nanms
step:7/1393 train_time:25753ms step_avg:nanms
step:8/1393 train_time:25875ms step_avg:nanms
step:9/1393 train_time:25996ms step_avg:nanms
step:10/1393 train_time:26118ms step_avg:nanms
step:11/1393 train_time:121ms step_avg:nanms
step:12/1393 train_time:244ms step_avg:nanms
step:13/1393 train_time:366ms step_avg:121.95ms
step:14/1393 train_time:488ms step_avg:122.09ms
step:15/1393 train_time:610ms step_avg:122.01ms
step:16/1393 train_time:732ms step_avg:122.06ms
step:17/1393 train_time:854ms step_avg:122.02ms
step:18/1393 train_time:977ms step_avg:122.19ms
step:19/1393 train_time:1100ms step_avg:122.21ms
step:20/1393 train_time:1222ms step_avg:122.17ms
step:21/1393 train_time:1344ms step_avg:122.18ms
step:22/1393 train_time:1467ms step_avg:122.22ms
step:23/1393 train_time:1589ms step_avg:122.22ms
step:24/1393 train_time:1712ms step_avg:122.27ms
step:25/1393 train_time:1834ms step_avg:122.26ms
step:26/1393 train_time:1956ms step_avg:122.24ms
step:27/1393 train_time:2078ms step_avg:122.26ms
step:28/1393 train_time:2200ms step_avg:122.23ms
step:29/1393 train_time:2323ms step_avg:122.24ms
step:30/1393 train_time:2445ms step_avg:122.25ms
step:31/1393 train_time:2569ms step_avg:122.32ms
step:32/1393 train_time:2690ms step_avg:122.29ms
step:33/1393 train_time:2814ms step_avg:122.36ms
step:34/1393 train_time:2937ms step_avg:122.38ms
step:35/1393 train_time:3059ms step_avg:122.36ms
step:36/1393 train_time:3181ms step_avg:122.35ms
step:37/1393 train_time:3303ms step_avg:122.32ms
step:38/1393 train_time:3425ms step_avg:122.33ms
step:39/1393 train_time:3548ms step_avg:122.36ms
step:40/1393 train_time:3671ms step_avg:122.36ms
step:41/1393 train_time:3794ms step_avg:122.37ms
step:42/1393 train_time:3918ms step_avg:122.42ms
step:43/1393 train_time:4039ms step_avg:122.40ms
step:44/1393 train_time:4161ms step_avg:122.37ms
step:45/1393 train_time:4283ms step_avg:122.37ms
step:46/1393 train_time:4407ms step_avg:122.41ms
step:47/1393 train_time:4530ms step_avg:122.43ms
step:48/1393 train_time:4655ms step_avg:122.50ms
step:49/1393 train_time:4779ms step_avg:122.54ms
step:50/1393 train_time:4903ms step_avg:122.58ms
step:51/1393 train_time:5026ms step_avg:122.58ms
step:52/1393 train_time:5148ms step_avg:122.57ms
step:53/1393 train_time:5270ms step_avg:122.56ms
step:54/1393 train_time:5391ms step_avg:122.53ms
step:55/1393 train_time:5513ms step_avg:122.52ms
step:56/1393 train_time:5635ms step_avg:122.50ms
step:57/1393 train_time:5757ms step_avg:122.49ms
step:58/1393 train_time:5881ms step_avg:122.51ms
step:59/1393 train_time:6004ms step_avg:122.53ms
step:60/1393 train_time:6127ms step_avg:122.55ms
step:61/1393 train_time:6249ms step_avg:122.53ms
step:62/1393 train_time:6372ms step_avg:122.53ms
step:63/1393 train_time:6493ms step_avg:122.52ms
step:64/1393 train_time:6616ms step_avg:122.51ms
step:65/1393 train_time:6738ms step_avg:122.51ms
step:66/1393 train_time:6861ms step_avg:122.51ms
step:67/1393 train_time:6983ms step_avg:122.51ms
step:68/1393 train_time:7105ms step_avg:122.51ms
step:69/1393 train_time:7229ms step_avg:122.53ms
step:70/1393 train_time:7351ms step_avg:122.52ms
step:71/1393 train_time:7473ms step_avg:122.52ms
step:72/1393 train_time:7596ms step_avg:122.51ms
step:73/1393 train_time:7718ms step_avg:122.51ms
step:74/1393 train_time:7840ms step_avg:122.49ms
step:75/1393 train_time:7961ms step_avg:122.48ms
step:76/1393 train_time:8083ms step_avg:122.47ms
step:77/1393 train_time:8205ms step_avg:122.47ms
step:78/1393 train_time:8328ms step_avg:122.47ms
step:79/1393 train_time:8451ms step_avg:122.48ms
step:80/1393 train_time:8574ms step_avg:122.49ms
step:81/1393 train_time:8696ms step_avg:122.48ms
step:82/1393 train_time:8819ms step_avg:122.49ms
step:83/1393 train_time:8941ms step_avg:122.47ms
step:84/1393 train_time:9062ms step_avg:122.46ms
step:85/1393 train_time:9185ms step_avg:122.46ms
step:86/1393 train_time:9307ms step_avg:122.46ms
step:87/1393 train_time:9430ms step_avg:122.46ms
step:88/1393 train_time:9553ms step_avg:122.47ms
step:89/1393 train_time:9676ms step_avg:122.48ms
step:90/1393 train_time:9798ms step_avg:122.47ms
step:91/1393 train_time:9920ms step_avg:122.47ms
step:92/1393 train_time:10041ms step_avg:122.46ms
step:93/1393 train_time:10163ms step_avg:122.44ms
step:94/1393 train_time:10286ms step_avg:122.46ms
step:95/1393 train_time:10409ms step_avg:122.46ms
step:96/1393 train_time:10531ms step_avg:122.45ms
step:97/1393 train_time:10654ms step_avg:122.46ms
step:98/1393 train_time:10777ms step_avg:122.46ms
step:99/1393 train_time:10900ms step_avg:122.47ms
step:100/1393 train_time:11022ms step_avg:122.46ms
step:101/1393 train_time:11145ms step_avg:122.47ms
step:102/1393 train_time:11266ms step_avg:122.46ms
step:103/1393 train_time:11388ms step_avg:122.45ms
step:104/1393 train_time:11510ms step_avg:122.45ms
step:105/1393 train_time:11632ms step_avg:122.45ms
step:106/1393 train_time:11756ms step_avg:122.46ms
step:107/1393 train_time:11880ms step_avg:122.47ms
step:108/1393 train_time:12002ms step_avg:122.47ms
step:109/1393 train_time:12126ms step_avg:122.48ms
step:110/1393 train_time:12249ms step_avg:122.49ms
step:111/1393 train_time:12372ms step_avg:122.50ms
step:112/1393 train_time:12495ms step_avg:122.50ms
step:113/1393 train_time:12618ms step_avg:122.50ms
step:114/1393 train_time:12740ms step_avg:122.50ms
step:115/1393 train_time:12862ms step_avg:122.49ms
step:116/1393 train_time:12984ms step_avg:122.49ms
step:117/1393 train_time:13107ms step_avg:122.50ms
step:118/1393 train_time:13230ms step_avg:122.50ms
step:119/1393 train_time:13354ms step_avg:122.52ms
step:120/1393 train_time:13477ms step_avg:122.52ms
step:121/1393 train_time:13600ms step_avg:122.52ms
step:122/1393 train_time:13721ms step_avg:122.51ms
step:123/1393 train_time:13843ms step_avg:122.51ms
step:124/1393 train_time:13967ms step_avg:122.52ms
step:125/1393 train_time:14091ms step_avg:122.53ms
step:125/1393 val_loss:4.4029 train_time:14212ms step_avg:123.59ms
step:126/1393 train_time:14234ms step_avg:122.71ms
step:127/1393 train_time:14339ms step_avg:122.56ms
step:128/1393 train_time:14471ms step_avg:122.63ms
step:129/1393 train_time:14596ms step_avg:122.66ms
step:130/1393 train_time:14719ms step_avg:122.66ms
step:131/1393 train_time:14842ms step_avg:122.66ms
step:132/1393 train_time:14964ms step_avg:122.66ms
step:133/1393 train_time:15087ms step_avg:122.66ms
step:134/1393 train_time:15209ms step_avg:122.65ms
step:135/1393 train_time:15331ms step_avg:122.65ms
step:136/1393 train_time:15456ms step_avg:122.66ms
step:137/1393 train_time:15581ms step_avg:122.69ms
step:138/1393 train_time:15702ms step_avg:122.67ms
step:139/1393 train_time:15826ms step_avg:122.68ms
step:140/1393 train_time:15950ms step_avg:122.69ms
step:141/1393 train_time:16073ms step_avg:122.69ms
step:142/1393 train_time:16195ms step_avg:122.69ms
step:143/1393 train_time:16317ms step_avg:122.69ms
step:144/1393 train_time:16441ms step_avg:122.69ms
step:145/1393 train_time:16564ms step_avg:122.70ms
step:146/1393 train_time:16688ms step_avg:122.71ms
step:147/1393 train_time:16812ms step_avg:122.72ms
step:148/1393 train_time:16935ms step_avg:122.72ms
step:149/1393 train_time:17058ms step_avg:122.72ms
step:150/1393 train_time:17180ms step_avg:122.72ms
step:151/1393 train_time:17304ms step_avg:122.72ms
step:152/1393 train_time:17426ms step_avg:122.72ms
step:153/1393 train_time:17549ms step_avg:122.72ms
step:154/1393 train_time:17672ms step_avg:122.72ms
step:155/1393 train_time:17796ms step_avg:122.73ms
step:156/1393 train_time:17919ms step_avg:122.74ms
step:157/1393 train_time:18043ms step_avg:122.74ms
step:158/1393 train_time:18166ms step_avg:122.74ms
step:159/1393 train_time:18289ms step_avg:122.75ms
step:160/1393 train_time:18411ms step_avg:122.74ms
step:161/1393 train_time:18533ms step_avg:122.74ms
step:162/1393 train_time:18655ms step_avg:122.73ms
step:163/1393 train_time:18779ms step_avg:122.74ms
step:164/1393 train_time:18901ms step_avg:122.73ms
step:165/1393 train_time:19024ms step_avg:122.74ms
step:166/1393 train_time:19147ms step_avg:122.74ms
step:167/1393 train_time:19270ms step_avg:122.74ms
step:168/1393 train_time:19393ms step_avg:122.74ms
step:169/1393 train_time:19516ms step_avg:122.74ms
step:170/1393 train_time:19640ms step_avg:122.75ms
step:171/1393 train_time:19762ms step_avg:122.74ms
step:172/1393 train_time:19886ms step_avg:122.75ms
step:173/1393 train_time:20010ms step_avg:122.76ms
step:174/1393 train_time:20133ms step_avg:122.76ms
step:175/1393 train_time:20258ms step_avg:122.77ms
step:176/1393 train_time:20380ms step_avg:122.77ms
step:177/1393 train_time:20503ms step_avg:122.77ms
step:178/1393 train_time:20626ms step_avg:122.78ms
step:179/1393 train_time:20750ms step_avg:122.78ms
step:180/1393 train_time:20874ms step_avg:122.79ms
step:181/1393 train_time:20997ms step_avg:122.79ms
step:182/1393 train_time:21120ms step_avg:122.79ms
step:183/1393 train_time:21243ms step_avg:122.79ms
step:184/1393 train_time:21366ms step_avg:122.79ms
step:185/1393 train_time:21488ms step_avg:122.79ms
step:186/1393 train_time:21610ms step_avg:122.78ms
step:187/1393 train_time:21733ms step_avg:122.78ms
step:188/1393 train_time:21856ms step_avg:122.79ms
step:189/1393 train_time:21979ms step_avg:122.79ms
step:190/1393 train_time:22102ms step_avg:122.79ms
step:191/1393 train_time:22225ms step_avg:122.79ms
step:192/1393 train_time:22348ms step_avg:122.79ms
step:193/1393 train_time:22471ms step_avg:122.79ms
step:194/1393 train_time:22595ms step_avg:122.80ms
step:195/1393 train_time:22719ms step_avg:122.80ms
step:196/1393 train_time:22841ms step_avg:122.80ms
step:197/1393 train_time:22964ms step_avg:122.80ms
step:198/1393 train_time:23088ms step_avg:122.81ms
step:199/1393 train_time:23212ms step_avg:122.81ms
step:200/1393 train_time:23336ms step_avg:122.82ms
step:201/1393 train_time:23460ms step_avg:122.83ms
step:202/1393 train_time:23583ms step_avg:122.83ms
step:203/1393 train_time:23705ms step_avg:122.83ms
step:204/1393 train_time:23829ms step_avg:122.83ms
step:205/1393 train_time:23953ms step_avg:122.84ms
step:206/1393 train_time:24078ms step_avg:122.85ms
step:207/1393 train_time:24203ms step_avg:122.86ms
step:208/1393 train_time:24325ms step_avg:122.85ms
step:209/1393 train_time:24449ms step_avg:122.86ms
step:210/1393 train_time:24572ms step_avg:122.86ms
step:211/1393 train_time:24695ms step_avg:122.86ms
step:212/1393 train_time:24818ms step_avg:122.86ms
step:213/1393 train_time:24941ms step_avg:122.86ms
step:214/1393 train_time:25065ms step_avg:122.87ms
step:215/1393 train_time:25189ms step_avg:122.87ms
step:216/1393 train_time:25313ms step_avg:122.88ms
step:217/1393 train_time:25438ms step_avg:122.89ms
step:218/1393 train_time:25562ms step_avg:122.89ms
step:219/1393 train_time:25685ms step_avg:122.89ms
step:220/1393 train_time:25808ms step_avg:122.90ms
step:221/1393 train_time:25931ms step_avg:122.90ms
step:222/1393 train_time:26056ms step_avg:122.90ms
step:223/1393 train_time:26180ms step_avg:122.91ms
step:224/1393 train_time:26305ms step_avg:122.92ms
step:225/1393 train_time:26428ms step_avg:122.92ms
step:226/1393 train_time:26551ms step_avg:122.92ms
step:227/1393 train_time:26675ms step_avg:122.93ms
step:228/1393 train_time:26799ms step_avg:122.93ms
step:229/1393 train_time:26923ms step_avg:122.94ms
step:230/1393 train_time:27047ms step_avg:122.94ms
step:231/1393 train_time:27171ms step_avg:122.95ms
step:232/1393 train_time:27294ms step_avg:122.95ms
step:233/1393 train_time:27418ms step_avg:122.95ms
step:234/1393 train_time:27541ms step_avg:122.95ms
step:235/1393 train_time:27664ms step_avg:122.95ms
step:236/1393 train_time:27789ms step_avg:122.96ms
step:237/1393 train_time:27913ms step_avg:122.97ms
step:238/1393 train_time:28037ms step_avg:122.97ms
step:239/1393 train_time:28161ms step_avg:122.97ms
step:240/1393 train_time:28284ms step_avg:122.97ms
step:241/1393 train_time:28408ms step_avg:122.98ms
step:242/1393 train_time:28531ms step_avg:122.98ms
step:243/1393 train_time:28655ms step_avg:122.98ms
step:244/1393 train_time:28778ms step_avg:122.98ms
step:245/1393 train_time:28902ms step_avg:122.99ms
step:246/1393 train_time:29026ms step_avg:122.99ms
step:247/1393 train_time:29150ms step_avg:123.00ms
step:248/1393 train_time:29274ms step_avg:123.00ms
step:249/1393 train_time:29398ms step_avg:123.01ms
step:250/1393 train_time:29522ms step_avg:123.01ms
step:250/1393 val_loss:3.9859 train_time:29643ms step_avg:123.51ms
step:251/1393 train_time:29666ms step_avg:123.10ms
step:252/1393 train_time:29782ms step_avg:123.06ms
step:253/1393 train_time:29908ms step_avg:123.08ms
step:254/1393 train_time:30031ms step_avg:123.08ms
step:255/1393 train_time:30154ms step_avg:123.08ms
step:256/1393 train_time:30277ms step_avg:123.08ms
step:257/1393 train_time:30399ms step_avg:123.07ms
step:258/1393 train_time:30522ms step_avg:123.07ms
step:259/1393 train_time:30645ms step_avg:123.07ms
step:260/1393 train_time:30769ms step_avg:123.08ms
step:261/1393 train_time:30894ms step_avg:123.08ms
step:262/1393 train_time:31017ms step_avg:123.08ms
step:263/1393 train_time:31139ms step_avg:123.08ms
step:264/1393 train_time:31263ms step_avg:123.08ms
step:265/1393 train_time:31386ms step_avg:123.08ms
step:266/1393 train_time:31510ms step_avg:123.09ms
step:267/1393 train_time:31634ms step_avg:123.09ms
step:268/1393 train_time:31756ms step_avg:123.09ms
step:269/1393 train_time:31881ms step_avg:123.09ms
step:270/1393 train_time:32004ms step_avg:123.09ms
step:271/1393 train_time:32126ms step_avg:123.09ms
step:272/1393 train_time:32250ms step_avg:123.09ms
step:273/1393 train_time:32373ms step_avg:123.09ms
step:274/1393 train_time:32496ms step_avg:123.09ms
step:275/1393 train_time:32620ms step_avg:123.09ms
step:276/1393 train_time:32744ms step_avg:123.10ms
step:277/1393 train_time:32868ms step_avg:123.10ms
step:278/1393 train_time:32992ms step_avg:123.10ms
step:279/1393 train_time:33116ms step_avg:123.11ms
step:280/1393 train_time:33240ms step_avg:123.11ms
step:281/1393 train_time:33363ms step_avg:123.11ms
step:282/1393 train_time:33486ms step_avg:123.11ms
step:283/1393 train_time:33608ms step_avg:123.11ms
step:284/1393 train_time:33732ms step_avg:123.11ms
step:285/1393 train_time:33856ms step_avg:123.11ms
step:286/1393 train_time:33980ms step_avg:123.12ms
step:287/1393 train_time:34104ms step_avg:123.12ms
step:288/1393 train_time:34228ms step_avg:123.12ms
step:289/1393 train_time:34351ms step_avg:123.12ms
step:290/1393 train_time:34474ms step_avg:123.12ms
step:291/1393 train_time:34597ms step_avg:123.12ms
step:292/1393 train_time:34721ms step_avg:123.12ms
step:293/1393 train_time:34845ms step_avg:123.13ms
step:294/1393 train_time:34968ms step_avg:123.13ms
step:295/1393 train_time:35093ms step_avg:123.13ms
step:296/1393 train_time:35217ms step_avg:123.14ms
step:297/1393 train_time:35340ms step_avg:123.14ms
step:298/1393 train_time:35464ms step_avg:123.14ms
step:299/1393 train_time:35587ms step_avg:123.14ms
step:300/1393 train_time:35711ms step_avg:123.14ms
step:301/1393 train_time:35835ms step_avg:123.15ms
step:302/1393 train_time:35958ms step_avg:123.14ms
step:303/1393 train_time:36082ms step_avg:123.15ms
step:304/1393 train_time:36205ms step_avg:123.15ms
step:305/1393 train_time:36329ms step_avg:123.15ms
step:306/1393 train_time:36452ms step_avg:123.15ms
step:307/1393 train_time:36576ms step_avg:123.15ms
step:308/1393 train_time:36700ms step_avg:123.16ms
step:309/1393 train_time:36823ms step_avg:123.16ms
step:310/1393 train_time:36947ms step_avg:123.16ms
step:311/1393 train_time:37070ms step_avg:123.16ms
step:312/1393 train_time:37197ms step_avg:123.17ms
step:313/1393 train_time:37323ms step_avg:123.18ms
step:314/1393 train_time:37451ms step_avg:123.19ms
step:315/1393 train_time:37578ms step_avg:123.21ms
step:316/1393 train_time:37704ms step_avg:123.22ms
step:317/1393 train_time:37829ms step_avg:123.22ms
step:318/1393 train_time:37955ms step_avg:123.23ms
step:319/1393 train_time:38081ms step_avg:123.24ms
step:320/1393 train_time:38207ms step_avg:123.25ms
step:321/1393 train_time:38334ms step_avg:123.26ms
step:322/1393 train_time:38461ms step_avg:123.27ms
step:323/1393 train_time:38587ms step_avg:123.28ms
step:324/1393 train_time:38715ms step_avg:123.30ms
step:325/1393 train_time:38841ms step_avg:123.30ms
step:326/1393 train_time:38966ms step_avg:123.31ms
step:327/1393 train_time:39091ms step_avg:123.32ms
step:328/1393 train_time:39217ms step_avg:123.32ms
step:329/1393 train_time:39343ms step_avg:123.33ms
step:330/1393 train_time:39470ms step_avg:123.34ms
step:331/1393 train_time:39600ms step_avg:123.36ms
step:332/1393 train_time:39723ms step_avg:123.36ms
step:333/1393 train_time:39850ms step_avg:123.37ms
step:334/1393 train_time:39976ms step_avg:123.38ms
step:335/1393 train_time:40102ms step_avg:123.39ms
step:336/1393 train_time:40229ms step_avg:123.40ms
step:337/1393 train_time:40356ms step_avg:123.41ms
step:338/1393 train_time:40482ms step_avg:123.42ms
step:339/1393 train_time:40608ms step_avg:123.43ms
step:340/1393 train_time:40735ms step_avg:123.44ms
step:341/1393 train_time:40861ms step_avg:123.45ms
step:342/1393 train_time:40987ms step_avg:123.45ms
step:343/1393 train_time:41114ms step_avg:123.46ms
step:344/1393 train_time:41240ms step_avg:123.47ms
step:345/1393 train_time:41366ms step_avg:123.48ms
step:346/1393 train_time:41492ms step_avg:123.49ms
step:347/1393 train_time:41618ms step_avg:123.50ms
step:348/1393 train_time:41744ms step_avg:123.50ms
step:349/1393 train_time:41870ms step_avg:123.51ms
step:350/1393 train_time:41995ms step_avg:123.52ms
step:351/1393 train_time:42121ms step_avg:123.52ms
step:352/1393 train_time:42247ms step_avg:123.53ms
step:353/1393 train_time:42374ms step_avg:123.54ms
step:354/1393 train_time:42500ms step_avg:123.55ms
step:355/1393 train_time:42625ms step_avg:123.55ms
step:356/1393 train_time:42751ms step_avg:123.56ms
step:357/1393 train_time:42876ms step_avg:123.56ms
step:358/1393 train_time:43002ms step_avg:123.57ms
step:359/1393 train_time:43128ms step_avg:123.58ms
step:360/1393 train_time:43254ms step_avg:123.58ms
step:361/1393 train_time:43380ms step_avg:123.59ms
step:362/1393 train_time:43507ms step_avg:123.60ms
step:363/1393 train_time:43633ms step_avg:123.61ms
step:364/1393 train_time:43758ms step_avg:123.61ms
step:365/1393 train_time:43883ms step_avg:123.62ms
step:366/1393 train_time:44008ms step_avg:123.62ms
step:367/1393 train_time:44134ms step_avg:123.62ms
step:368/1393 train_time:44260ms step_avg:123.63ms
step:369/1393 train_time:44386ms step_avg:123.64ms
step:370/1393 train_time:44511ms step_avg:123.64ms
step:371/1393 train_time:44637ms step_avg:123.65ms
step:372/1393 train_time:44763ms step_avg:123.66ms
step:373/1393 train_time:44892ms step_avg:123.67ms
step:374/1393 train_time:45017ms step_avg:123.67ms
step:375/1393 train_time:45143ms step_avg:123.68ms
step:375/1393 val_loss:3.7883 train_time:45267ms step_avg:124.02ms
step:376/1393 train_time:45289ms step_avg:123.74ms
step:377/1393 train_time:45407ms step_avg:123.72ms
step:378/1393 train_time:45534ms step_avg:123.73ms
step:379/1393 train_time:45660ms step_avg:123.74ms
step:380/1393 train_time:45786ms step_avg:123.75ms
step:381/1393 train_time:45912ms step_avg:123.75ms
step:382/1393 train_time:46037ms step_avg:123.76ms
step:383/1393 train_time:46162ms step_avg:123.76ms
step:384/1393 train_time:46287ms step_avg:123.76ms
step:385/1393 train_time:46415ms step_avg:123.77ms
step:386/1393 train_time:46542ms step_avg:123.78ms
step:387/1393 train_time:46668ms step_avg:123.79ms
step:388/1393 train_time:46793ms step_avg:123.79ms
step:389/1393 train_time:46919ms step_avg:123.80ms
step:390/1393 train_time:47045ms step_avg:123.80ms
step:391/1393 train_time:47170ms step_avg:123.81ms
step:392/1393 train_time:47296ms step_avg:123.81ms
step:393/1393 train_time:47424ms step_avg:123.82ms
step:394/1393 train_time:47551ms step_avg:123.83ms
step:395/1393 train_time:47677ms step_avg:123.84ms
step:396/1393 train_time:47804ms step_avg:123.84ms
step:397/1393 train_time:47930ms step_avg:123.85ms
step:398/1393 train_time:48055ms step_avg:123.85ms
step:399/1393 train_time:48181ms step_avg:123.86ms
step:400/1393 train_time:48307ms step_avg:123.86ms
step:401/1393 train_time:48433ms step_avg:123.87ms
step:402/1393 train_time:48559ms step_avg:123.87ms
step:403/1393 train_time:48686ms step_avg:123.88ms
step:404/1393 train_time:48812ms step_avg:123.89ms
step:405/1393 train_time:48937ms step_avg:123.89ms
step:406/1393 train_time:49064ms step_avg:123.90ms
step:407/1393 train_time:49190ms step_avg:123.90ms
step:408/1393 train_time:49315ms step_avg:123.91ms
step:409/1393 train_time:49441ms step_avg:123.91ms
step:410/1393 train_time:49568ms step_avg:123.92ms
step:411/1393 train_time:49694ms step_avg:123.92ms
step:412/1393 train_time:49820ms step_avg:123.93ms
step:413/1393 train_time:49946ms step_avg:123.94ms
step:414/1393 train_time:50072ms step_avg:123.94ms
step:415/1393 train_time:50198ms step_avg:123.95ms
step:416/1393 train_time:50325ms step_avg:123.95ms
step:417/1393 train_time:50452ms step_avg:123.96ms
step:418/1393 train_time:50578ms step_avg:123.96ms
step:419/1393 train_time:50704ms step_avg:123.97ms
step:420/1393 train_time:50831ms step_avg:123.98ms
step:421/1393 train_time:50957ms step_avg:123.98ms
step:422/1393 train_time:51084ms step_avg:123.99ms
step:423/1393 train_time:51210ms step_avg:124.00ms
step:424/1393 train_time:51336ms step_avg:124.00ms
step:425/1393 train_time:51465ms step_avg:124.01ms
step:426/1393 train_time:51591ms step_avg:124.02ms
step:427/1393 train_time:51719ms step_avg:124.03ms
step:428/1393 train_time:51846ms step_avg:124.03ms
step:429/1393 train_time:51972ms step_avg:124.04ms
step:430/1393 train_time:52098ms step_avg:124.04ms
step:431/1393 train_time:52225ms step_avg:124.05ms
step:432/1393 train_time:52351ms step_avg:124.05ms
step:433/1393 train_time:52477ms step_avg:124.06ms
step:434/1393 train_time:52604ms step_avg:124.07ms
step:435/1393 train_time:52730ms step_avg:124.07ms
step:436/1393 train_time:52856ms step_avg:124.08ms
step:437/1393 train_time:52985ms step_avg:124.09ms
step:438/1393 train_time:53111ms step_avg:124.09ms
step:439/1393 train_time:53236ms step_avg:124.09ms
step:440/1393 train_time:53362ms step_avg:124.10ms
step:441/1393 train_time:53488ms step_avg:124.10ms
step:442/1393 train_time:53615ms step_avg:124.11ms
step:443/1393 train_time:53741ms step_avg:124.11ms
step:444/1393 train_time:53867ms step_avg:124.12ms
step:445/1393 train_time:53995ms step_avg:124.13ms
step:446/1393 train_time:54120ms step_avg:124.13ms
step:447/1393 train_time:54246ms step_avg:124.13ms
step:448/1393 train_time:54372ms step_avg:124.14ms
step:449/1393 train_time:54499ms step_avg:124.14ms
step:450/1393 train_time:54625ms step_avg:124.15ms
step:451/1393 train_time:54752ms step_avg:124.15ms
step:452/1393 train_time:54878ms step_avg:124.16ms
step:453/1393 train_time:55005ms step_avg:124.17ms
step:454/1393 train_time:55132ms step_avg:124.17ms
step:455/1393 train_time:55258ms step_avg:124.18ms
step:456/1393 train_time:55386ms step_avg:124.18ms
step:457/1393 train_time:55512ms step_avg:124.19ms
step:458/1393 train_time:55637ms step_avg:124.19ms
step:459/1393 train_time:55764ms step_avg:124.20ms
step:460/1393 train_time:55889ms step_avg:124.20ms
step:461/1393 train_time:56015ms step_avg:124.20ms
step:462/1393 train_time:56142ms step_avg:124.21ms
step:463/1393 train_time:56267ms step_avg:124.21ms
step:464/1393 train_time:56394ms step_avg:124.22ms
step:465/1393 train_time:56521ms step_avg:124.22ms
step:466/1393 train_time:56646ms step_avg:124.22ms
step:467/1393 train_time:56773ms step_avg:124.23ms
step:468/1393 train_time:56898ms step_avg:124.23ms
step:469/1393 train_time:57026ms step_avg:124.24ms
step:470/1393 train_time:57152ms step_avg:124.24ms
step:471/1393 train_time:57279ms step_avg:124.25ms
step:472/1393 train_time:57405ms step_avg:124.25ms
step:473/1393 train_time:57532ms step_avg:124.26ms
step:474/1393 train_time:57657ms step_avg:124.26ms
step:475/1393 train_time:57789ms step_avg:124.28ms
step:476/1393 train_time:57911ms step_avg:124.27ms
step:477/1393 train_time:58037ms step_avg:124.28ms
step:478/1393 train_time:58164ms step_avg:124.28ms
step:479/1393 train_time:58290ms step_avg:124.29ms
step:480/1393 train_time:58417ms step_avg:124.29ms
step:481/1393 train_time:58543ms step_avg:124.30ms
step:482/1393 train_time:58669ms step_avg:124.30ms
step:483/1393 train_time:58796ms step_avg:124.31ms
step:484/1393 train_time:58923ms step_avg:124.31ms
step:485/1393 train_time:59051ms step_avg:124.32ms
step:486/1393 train_time:59177ms step_avg:124.32ms
step:487/1393 train_time:59303ms step_avg:124.32ms
step:488/1393 train_time:59430ms step_avg:124.33ms
step:489/1393 train_time:59557ms step_avg:124.34ms
step:490/1393 train_time:59683ms step_avg:124.34ms
step:491/1393 train_time:59809ms step_avg:124.34ms
step:492/1393 train_time:59936ms step_avg:124.35ms
step:493/1393 train_time:60063ms step_avg:124.35ms
step:494/1393 train_time:60189ms step_avg:124.36ms
step:495/1393 train_time:60315ms step_avg:124.36ms
step:496/1393 train_time:60442ms step_avg:124.37ms
step:497/1393 train_time:60568ms step_avg:124.37ms
step:498/1393 train_time:60695ms step_avg:124.38ms
step:499/1393 train_time:60822ms step_avg:124.38ms
step:500/1393 train_time:60948ms step_avg:124.38ms
step:500/1393 val_loss:3.6699 train_time:61073ms step_avg:124.64ms
step:501/1393 train_time:61095ms step_avg:124.43ms
step:502/1393 train_time:61211ms step_avg:124.41ms
step:503/1393 train_time:61340ms step_avg:124.42ms
step:504/1393 train_time:61466ms step_avg:124.42ms
step:505/1393 train_time:61592ms step_avg:124.43ms
step:506/1393 train_time:61718ms step_avg:124.43ms
step:507/1393 train_time:61844ms step_avg:124.43ms
step:508/1393 train_time:61969ms step_avg:124.44ms
step:509/1393 train_time:62096ms step_avg:124.44ms
step:510/1393 train_time:62223ms step_avg:124.45ms
step:511/1393 train_time:62350ms step_avg:124.45ms
step:512/1393 train_time:62477ms step_avg:124.46ms
step:513/1393 train_time:62603ms step_avg:124.46ms
step:514/1393 train_time:62729ms step_avg:124.46ms
step:515/1393 train_time:62855ms step_avg:124.46ms
step:516/1393 train_time:62980ms step_avg:124.47ms
step:517/1393 train_time:63107ms step_avg:124.47ms
step:518/1393 train_time:63236ms step_avg:124.48ms
step:519/1393 train_time:63365ms step_avg:124.49ms
step:520/1393 train_time:63494ms step_avg:124.50ms
step:521/1393 train_time:63623ms step_avg:124.51ms
step:522/1393 train_time:63752ms step_avg:124.52ms
step:523/1393 train_time:63881ms step_avg:124.52ms
step:524/1393 train_time:64010ms step_avg:124.53ms
step:525/1393 train_time:64137ms step_avg:124.54ms
step:526/1393 train_time:64267ms step_avg:124.55ms
step:527/1393 train_time:64396ms step_avg:124.56ms
step:528/1393 train_time:64523ms step_avg:124.56ms
step:529/1393 train_time:64652ms step_avg:124.57ms
step:530/1393 train_time:64780ms step_avg:124.58ms
step:531/1393 train_time:64909ms step_avg:124.59ms
step:532/1393 train_time:65038ms step_avg:124.59ms
step:533/1393 train_time:65166ms step_avg:124.60ms
step:534/1393 train_time:65295ms step_avg:124.61ms
step:535/1393 train_time:65423ms step_avg:124.61ms
step:536/1393 train_time:65552ms step_avg:124.62ms
step:537/1393 train_time:65680ms step_avg:124.63ms
step:538/1393 train_time:65809ms step_avg:124.64ms
step:539/1393 train_time:65938ms step_avg:124.65ms
step:540/1393 train_time:66068ms step_avg:124.66ms
step:541/1393 train_time:66197ms step_avg:124.66ms
step:542/1393 train_time:66324ms step_avg:124.67ms
step:543/1393 train_time:66453ms step_avg:124.68ms
step:544/1393 train_time:66581ms step_avg:124.68ms
step:545/1393 train_time:66710ms step_avg:124.69ms
step:546/1393 train_time:66838ms step_avg:124.70ms
step:547/1393 train_time:66966ms step_avg:124.70ms
step:548/1393 train_time:67095ms step_avg:124.71ms
step:549/1393 train_time:67224ms step_avg:124.72ms
step:550/1393 train_time:67353ms step_avg:124.73ms
step:551/1393 train_time:67483ms step_avg:124.74ms
step:552/1393 train_time:67610ms step_avg:124.74ms
step:553/1393 train_time:67739ms step_avg:124.75ms
step:554/1393 train_time:67868ms step_avg:124.76ms
step:555/1393 train_time:67996ms step_avg:124.76ms
step:556/1393 train_time:68125ms step_avg:124.77ms
step:557/1393 train_time:68253ms step_avg:124.78ms
step:558/1393 train_time:68381ms step_avg:124.78ms
step:559/1393 train_time:68509ms step_avg:124.79ms
step:560/1393 train_time:68637ms step_avg:124.79ms
step:561/1393 train_time:68765ms step_avg:124.80ms
step:562/1393 train_time:68894ms step_avg:124.81ms
step:563/1393 train_time:69023ms step_avg:124.81ms
step:564/1393 train_time:69151ms step_avg:124.82ms
step:565/1393 train_time:69282ms step_avg:124.83ms
step:566/1393 train_time:69409ms step_avg:124.84ms
step:567/1393 train_time:69538ms step_avg:124.84ms
step:568/1393 train_time:69667ms step_avg:124.85ms
step:569/1393 train_time:69795ms step_avg:124.86ms
step:570/1393 train_time:69923ms step_avg:124.86ms
step:571/1393 train_time:70052ms step_avg:124.87ms
step:572/1393 train_time:70181ms step_avg:124.88ms
step:573/1393 train_time:70309ms step_avg:124.88ms
step:574/1393 train_time:70438ms step_avg:124.89ms
step:575/1393 train_time:70567ms step_avg:124.90ms
step:576/1393 train_time:70696ms step_avg:124.90ms
step:577/1393 train_time:70823ms step_avg:124.91ms
step:578/1393 train_time:70952ms step_avg:124.92ms
step:579/1393 train_time:71080ms step_avg:124.92ms
step:580/1393 train_time:71210ms step_avg:124.93ms
step:581/1393 train_time:71338ms step_avg:124.94ms
step:582/1393 train_time:71467ms step_avg:124.94ms
step:583/1393 train_time:71595ms step_avg:124.95ms
step:584/1393 train_time:71723ms step_avg:124.95ms
step:585/1393 train_time:71852ms step_avg:124.96ms
step:586/1393 train_time:71980ms step_avg:124.97ms
step:587/1393 train_time:72109ms step_avg:124.97ms
step:588/1393 train_time:72237ms step_avg:124.98ms
step:589/1393 train_time:72365ms step_avg:124.98ms
step:590/1393 train_time:72493ms step_avg:124.99ms
step:591/1393 train_time:72622ms step_avg:124.99ms
step:592/1393 train_time:72751ms step_avg:125.00ms
step:593/1393 train_time:72880ms step_avg:125.01ms
step:594/1393 train_time:73009ms step_avg:125.01ms
step:595/1393 train_time:73137ms step_avg:125.02ms
step:596/1393 train_time:73267ms step_avg:125.03ms
step:597/1393 train_time:73395ms step_avg:125.03ms
step:598/1393 train_time:73524ms step_avg:125.04ms
step:599/1393 train_time:73652ms step_avg:125.05ms
step:600/1393 train_time:73781ms step_avg:125.05ms
step:601/1393 train_time:73910ms step_avg:125.06ms
step:602/1393 train_time:74039ms step_avg:125.07ms
step:603/1393 train_time:74167ms step_avg:125.07ms
step:604/1393 train_time:74296ms step_avg:125.08ms
step:605/1393 train_time:74425ms step_avg:125.08ms
step:606/1393 train_time:74553ms step_avg:125.09ms
step:607/1393 train_time:74681ms step_avg:125.09ms
step:608/1393 train_time:74810ms step_avg:125.10ms
step:609/1393 train_time:74938ms step_avg:125.11ms
step:610/1393 train_time:75068ms step_avg:125.11ms
step:611/1393 train_time:75195ms step_avg:125.12ms
step:612/1393 train_time:75323ms step_avg:125.12ms
step:613/1393 train_time:75452ms step_avg:125.13ms
step:614/1393 train_time:75580ms step_avg:125.13ms
step:615/1393 train_time:75709ms step_avg:125.14ms
step:616/1393 train_time:75838ms step_avg:125.15ms
step:617/1393 train_time:75967ms step_avg:125.15ms
step:618/1393 train_time:76096ms step_avg:125.16ms
step:619/1393 train_time:76224ms step_avg:125.16ms
step:620/1393 train_time:76352ms step_avg:125.17ms
step:621/1393 train_time:76480ms step_avg:125.17ms
step:622/1393 train_time:76609ms step_avg:125.18ms
step:623/1393 train_time:76737ms step_avg:125.18ms
step:624/1393 train_time:76866ms step_avg:125.19ms
step:625/1393 train_time:76994ms step_avg:125.19ms
step:625/1393 val_loss:3.5867 train_time:77122ms step_avg:125.40ms
step:626/1393 train_time:77152ms step_avg:125.25ms
step:627/1393 train_time:77261ms step_avg:125.22ms
step:628/1393 train_time:77391ms step_avg:125.23ms
step:629/1393 train_time:77520ms step_avg:125.23ms
step:630/1393 train_time:77648ms step_avg:125.24ms
step:631/1393 train_time:77776ms step_avg:125.24ms
step:632/1393 train_time:77904ms step_avg:125.25ms
step:633/1393 train_time:78032ms step_avg:125.25ms
step:634/1393 train_time:78160ms step_avg:125.26ms
step:635/1393 train_time:78291ms step_avg:125.26ms
step:636/1393 train_time:78420ms step_avg:125.27ms
step:637/1393 train_time:78550ms step_avg:125.28ms
step:638/1393 train_time:78678ms step_avg:125.28ms
step:639/1393 train_time:78807ms step_avg:125.29ms
step:640/1393 train_time:78935ms step_avg:125.29ms
step:641/1393 train_time:79064ms step_avg:125.30ms
step:642/1393 train_time:79193ms step_avg:125.31ms
step:643/1393 train_time:79322ms step_avg:125.31ms
step:644/1393 train_time:79452ms step_avg:125.32ms
step:645/1393 train_time:79580ms step_avg:125.32ms
step:646/1393 train_time:79708ms step_avg:125.33ms
step:647/1393 train_time:79837ms step_avg:125.33ms
step:648/1393 train_time:79967ms step_avg:125.34ms
step:649/1393 train_time:80095ms step_avg:125.34ms
step:650/1393 train_time:80224ms step_avg:125.35ms
step:651/1393 train_time:80353ms step_avg:125.36ms
step:652/1393 train_time:80480ms step_avg:125.36ms
step:653/1393 train_time:80611ms step_avg:125.37ms
step:654/1393 train_time:80739ms step_avg:125.37ms
step:655/1393 train_time:80869ms step_avg:125.38ms
step:656/1393 train_time:80998ms step_avg:125.38ms
step:657/1393 train_time:81127ms step_avg:125.39ms
step:658/1393 train_time:81257ms step_avg:125.40ms
step:659/1393 train_time:81385ms step_avg:125.40ms
step:660/1393 train_time:81515ms step_avg:125.41ms
step:661/1393 train_time:81644ms step_avg:125.41ms
step:662/1393 train_time:81773ms step_avg:125.42ms
step:663/1393 train_time:81902ms step_avg:125.42ms
step:664/1393 train_time:82031ms step_avg:125.43ms
step:665/1393 train_time:82160ms step_avg:125.43ms
step:666/1393 train_time:82289ms step_avg:125.44ms
step:667/1393 train_time:82417ms step_avg:125.44ms
step:668/1393 train_time:82546ms step_avg:125.45ms
step:669/1393 train_time:82676ms step_avg:125.46ms
step:670/1393 train_time:82804ms step_avg:125.46ms
step:671/1393 train_time:82934ms step_avg:125.47ms
step:672/1393 train_time:83064ms step_avg:125.47ms
step:673/1393 train_time:83193ms step_avg:125.48ms
step:674/1393 train_time:83321ms step_avg:125.48ms
step:675/1393 train_time:83451ms step_avg:125.49ms
step:676/1393 train_time:83579ms step_avg:125.49ms
step:677/1393 train_time:83708ms step_avg:125.50ms
step:678/1393 train_time:83837ms step_avg:125.51ms
step:679/1393 train_time:83967ms step_avg:125.51ms
step:680/1393 train_time:84096ms step_avg:125.52ms
step:681/1393 train_time:84225ms step_avg:125.52ms
step:682/1393 train_time:84355ms step_avg:125.53ms
step:683/1393 train_time:84483ms step_avg:125.53ms
step:684/1393 train_time:84611ms step_avg:125.54ms
step:685/1393 train_time:84740ms step_avg:125.54ms
step:686/1393 train_time:84870ms step_avg:125.55ms
step:687/1393 train_time:85000ms step_avg:125.55ms
step:688/1393 train_time:85129ms step_avg:125.56ms
step:689/1393 train_time:85258ms step_avg:125.56ms
step:690/1393 train_time:85388ms step_avg:125.57ms
step:691/1393 train_time:85516ms step_avg:125.57ms
step:692/1393 train_time:85645ms step_avg:125.58ms
step:693/1393 train_time:85774ms step_avg:125.58ms
step:694/1393 train_time:85902ms step_avg:125.59ms
step:695/1393 train_time:86034ms step_avg:125.60ms
step:696/1393 train_time:86160ms step_avg:125.60ms
step:697/1393 train_time:86290ms step_avg:125.60ms
step:698/1393 train_time:86418ms step_avg:125.61ms
step:699/1393 train_time:86547ms step_avg:125.61ms
step:700/1393 train_time:86676ms step_avg:125.62ms
step:701/1393 train_time:86804ms step_avg:125.62ms
step:702/1393 train_time:86932ms step_avg:125.62ms
step:703/1393 train_time:87061ms step_avg:125.63ms
step:704/1393 train_time:87190ms step_avg:125.63ms
step:705/1393 train_time:87320ms step_avg:125.64ms
step:706/1393 train_time:87449ms step_avg:125.64ms
step:707/1393 train_time:87578ms step_avg:125.65ms
step:708/1393 train_time:87708ms step_avg:125.66ms
step:709/1393 train_time:87837ms step_avg:125.66ms
step:710/1393 train_time:87966ms step_avg:125.67ms
step:711/1393 train_time:88095ms step_avg:125.67ms
step:712/1393 train_time:88224ms step_avg:125.67ms
step:713/1393 train_time:88352ms step_avg:125.68ms
step:714/1393 train_time:88480ms step_avg:125.68ms
step:715/1393 train_time:88609ms step_avg:125.69ms
step:716/1393 train_time:88740ms step_avg:125.69ms
step:717/1393 train_time:88869ms step_avg:125.70ms
step:718/1393 train_time:88998ms step_avg:125.70ms
step:719/1393 train_time:89127ms step_avg:125.71ms
step:720/1393 train_time:89257ms step_avg:125.71ms
step:721/1393 train_time:89385ms step_avg:125.72ms
step:722/1393 train_time:89513ms step_avg:125.72ms
step:723/1393 train_time:89642ms step_avg:125.73ms
step:724/1393 train_time:89772ms step_avg:125.73ms
step:725/1393 train_time:89903ms step_avg:125.74ms
step:726/1393 train_time:90034ms step_avg:125.75ms
step:727/1393 train_time:90165ms step_avg:125.75ms
step:728/1393 train_time:90296ms step_avg:125.76ms
step:729/1393 train_time:90426ms step_avg:125.77ms
step:730/1393 train_time:90556ms step_avg:125.77ms
step:731/1393 train_time:90688ms step_avg:125.78ms
step:732/1393 train_time:90819ms step_avg:125.79ms
step:733/1393 train_time:90949ms step_avg:125.79ms
step:734/1393 train_time:91080ms step_avg:125.80ms
step:735/1393 train_time:91213ms step_avg:125.81ms
step:736/1393 train_time:91343ms step_avg:125.82ms
step:737/1393 train_time:91474ms step_avg:125.82ms
step:738/1393 train_time:91604ms step_avg:125.83ms
step:739/1393 train_time:91735ms step_avg:125.84ms
step:740/1393 train_time:91865ms step_avg:125.84ms
step:741/1393 train_time:91998ms step_avg:125.85ms
step:742/1393 train_time:92128ms step_avg:125.86ms
step:743/1393 train_time:92258ms step_avg:125.86ms
step:744/1393 train_time:92389ms step_avg:125.87ms
step:745/1393 train_time:92521ms step_avg:125.88ms
step:746/1393 train_time:92651ms step_avg:125.88ms
step:747/1393 train_time:92781ms step_avg:125.89ms
step:748/1393 train_time:92911ms step_avg:125.90ms
step:749/1393 train_time:93042ms step_avg:125.90ms
step:750/1393 train_time:93173ms step_avg:125.91ms
step:750/1393 val_loss:3.5328 train_time:93303ms step_avg:126.09ms
step:751/1393 train_time:93331ms step_avg:125.95ms
step:752/1393 train_time:93445ms step_avg:125.94ms
step:753/1393 train_time:93576ms step_avg:125.94ms
step:754/1393 train_time:93707ms step_avg:125.95ms
step:755/1393 train_time:93837ms step_avg:125.96ms
step:756/1393 train_time:93967ms step_avg:125.96ms
step:757/1393 train_time:94098ms step_avg:125.97ms
step:758/1393 train_time:94230ms step_avg:125.98ms
step:759/1393 train_time:94361ms step_avg:125.98ms
step:760/1393 train_time:94491ms step_avg:125.99ms
step:761/1393 train_time:94623ms step_avg:126.00ms
step:762/1393 train_time:94754ms step_avg:126.00ms
step:763/1393 train_time:94884ms step_avg:126.01ms
step:764/1393 train_time:95015ms step_avg:126.02ms
step:765/1393 train_time:95146ms step_avg:126.02ms
step:766/1393 train_time:95277ms step_avg:126.03ms
step:767/1393 train_time:95407ms step_avg:126.03ms
step:768/1393 train_time:95538ms step_avg:126.04ms
step:769/1393 train_time:95668ms step_avg:126.05ms
step:770/1393 train_time:95799ms step_avg:126.05ms
step:771/1393 train_time:95929ms step_avg:126.06ms
step:772/1393 train_time:96060ms step_avg:126.06ms
step:773/1393 train_time:96190ms step_avg:126.07ms
step:774/1393 train_time:96321ms step_avg:126.07ms
step:775/1393 train_time:96451ms step_avg:126.08ms
step:776/1393 train_time:96582ms step_avg:126.09ms
step:777/1393 train_time:96715ms step_avg:126.09ms
step:778/1393 train_time:96843ms step_avg:126.10ms
step:779/1393 train_time:96973ms step_avg:126.10ms
step:780/1393 train_time:97104ms step_avg:126.11ms
step:781/1393 train_time:97235ms step_avg:126.11ms
step:782/1393 train_time:97365ms step_avg:126.12ms
step:783/1393 train_time:97496ms step_avg:126.13ms
step:784/1393 train_time:97627ms step_avg:126.13ms
step:785/1393 train_time:97758ms step_avg:126.14ms
step:786/1393 train_time:97888ms step_avg:126.14ms
step:787/1393 train_time:98019ms step_avg:126.15ms
step:788/1393 train_time:98149ms step_avg:126.16ms
step:789/1393 train_time:98278ms step_avg:126.16ms
step:790/1393 train_time:98413ms step_avg:126.17ms
step:791/1393 train_time:98539ms step_avg:126.17ms
step:792/1393 train_time:98671ms step_avg:126.18ms
step:793/1393 train_time:98801ms step_avg:126.18ms
step:794/1393 train_time:98931ms step_avg:126.19ms
step:795/1393 train_time:99064ms step_avg:126.20ms
step:796/1393 train_time:99196ms step_avg:126.20ms
step:797/1393 train_time:99326ms step_avg:126.21ms
step:798/1393 train_time:99457ms step_avg:126.21ms
step:799/1393 train_time:99589ms step_avg:126.22ms
step:800/1393 train_time:99720ms step_avg:126.23ms
step:801/1393 train_time:99850ms step_avg:126.23ms
step:802/1393 train_time:99981ms step_avg:126.24ms
step:803/1393 train_time:100112ms step_avg:126.24ms
step:804/1393 train_time:100243ms step_avg:126.25ms
step:805/1393 train_time:100374ms step_avg:126.26ms
step:806/1393 train_time:100504ms step_avg:126.26ms
step:807/1393 train_time:100635ms step_avg:126.27ms
step:808/1393 train_time:100766ms step_avg:126.27ms
step:809/1393 train_time:100897ms step_avg:126.28ms
step:810/1393 train_time:101027ms step_avg:126.28ms
step:811/1393 train_time:101157ms step_avg:126.29ms
step:812/1393 train_time:101289ms step_avg:126.30ms
step:813/1393 train_time:101420ms step_avg:126.30ms
step:814/1393 train_time:101551ms step_avg:126.31ms
step:815/1393 train_time:101681ms step_avg:126.31ms
step:816/1393 train_time:101812ms step_avg:126.32ms
step:817/1393 train_time:101943ms step_avg:126.32ms
step:818/1393 train_time:102074ms step_avg:126.33ms
step:819/1393 train_time:102205ms step_avg:126.34ms
step:820/1393 train_time:102335ms step_avg:126.34ms
step:821/1393 train_time:102466ms step_avg:126.34ms
step:822/1393 train_time:102595ms step_avg:126.35ms
step:823/1393 train_time:102726ms step_avg:126.35ms
step:824/1393 train_time:102857ms step_avg:126.36ms
step:825/1393 train_time:102988ms step_avg:126.37ms
step:826/1393 train_time:103119ms step_avg:126.37ms
step:827/1393 train_time:103250ms step_avg:126.38ms
step:828/1393 train_time:103381ms step_avg:126.38ms
step:829/1393 train_time:103515ms step_avg:126.39ms
step:830/1393 train_time:103644ms step_avg:126.39ms
step:831/1393 train_time:103775ms step_avg:126.40ms
step:832/1393 train_time:103906ms step_avg:126.41ms
step:833/1393 train_time:104036ms step_avg:126.41ms
step:834/1393 train_time:104168ms step_avg:126.42ms
step:835/1393 train_time:104298ms step_avg:126.42ms
step:836/1393 train_time:104431ms step_avg:126.43ms
step:837/1393 train_time:104561ms step_avg:126.43ms
step:838/1393 train_time:104692ms step_avg:126.44ms
step:839/1393 train_time:104824ms step_avg:126.45ms
step:840/1393 train_time:104954ms step_avg:126.45ms
step:841/1393 train_time:105086ms step_avg:126.46ms
step:842/1393 train_time:105216ms step_avg:126.46ms
step:843/1393 train_time:105347ms step_avg:126.47ms
step:844/1393 train_time:105478ms step_avg:126.47ms
step:845/1393 train_time:105609ms step_avg:126.48ms
step:846/1393 train_time:105741ms step_avg:126.48ms
step:847/1393 train_time:105874ms step_avg:126.49ms
step:848/1393 train_time:106004ms step_avg:126.50ms
step:849/1393 train_time:106135ms step_avg:126.50ms
step:850/1393 train_time:106267ms step_avg:126.51ms
step:851/1393 train_time:106398ms step_avg:126.51ms
step:852/1393 train_time:106529ms step_avg:126.52ms
step:853/1393 train_time:106660ms step_avg:126.52ms
step:854/1393 train_time:106791ms step_avg:126.53ms
step:855/1393 train_time:106922ms step_avg:126.53ms
step:856/1393 train_time:107053ms step_avg:126.54ms
step:857/1393 train_time:107183ms step_avg:126.54ms
step:858/1393 train_time:107315ms step_avg:126.55ms
step:859/1393 train_time:107446ms step_avg:126.56ms
step:860/1393 train_time:107578ms step_avg:126.56ms
step:861/1393 train_time:107708ms step_avg:126.57ms
step:862/1393 train_time:107840ms step_avg:126.57ms
step:863/1393 train_time:107970ms step_avg:126.58ms
step:864/1393 train_time:108102ms step_avg:126.58ms
step:865/1393 train_time:108232ms step_avg:126.59ms
step:866/1393 train_time:108368ms step_avg:126.60ms
step:867/1393 train_time:108499ms step_avg:126.60ms
step:868/1393 train_time:108629ms step_avg:126.61ms
step:869/1393 train_time:108760ms step_avg:126.61ms
step:870/1393 train_time:108892ms step_avg:126.62ms
step:871/1393 train_time:109023ms step_avg:126.62ms
step:872/1393 train_time:109154ms step_avg:126.63ms
step:873/1393 train_time:109285ms step_avg:126.63ms
step:874/1393 train_time:109416ms step_avg:126.64ms
step:875/1393 train_time:109547ms step_avg:126.64ms
step:875/1393 val_loss:3.4825 train_time:109677ms step_avg:126.79ms
step:876/1393 train_time:109698ms step_avg:126.67ms
step:877/1393 train_time:109819ms step_avg:126.67ms
step:878/1393 train_time:109950ms step_avg:126.67ms
step:879/1393 train_time:110081ms step_avg:126.68ms
step:880/1393 train_time:110211ms step_avg:126.68ms
step:881/1393 train_time:110341ms step_avg:126.68ms
step:882/1393 train_time:110471ms step_avg:126.69ms
step:883/1393 train_time:110602ms step_avg:126.69ms
step:884/1393 train_time:110733ms step_avg:126.70ms
step:885/1393 train_time:110866ms step_avg:126.70ms
step:886/1393 train_time:110997ms step_avg:126.71ms
step:887/1393 train_time:111127ms step_avg:126.71ms
step:888/1393 train_time:111258ms step_avg:126.72ms
step:889/1393 train_time:111391ms step_avg:126.72ms
step:890/1393 train_time:111521ms step_avg:126.73ms
step:891/1393 train_time:111652ms step_avg:126.73ms
step:892/1393 train_time:111783ms step_avg:126.74ms
step:893/1393 train_time:111914ms step_avg:126.74ms
step:894/1393 train_time:112045ms step_avg:126.75ms
step:895/1393 train_time:112176ms step_avg:126.75ms
step:896/1393 train_time:112307ms step_avg:126.76ms
step:897/1393 train_time:112437ms step_avg:126.76ms
step:898/1393 train_time:112567ms step_avg:126.77ms
step:899/1393 train_time:112699ms step_avg:126.77ms
step:900/1393 train_time:112829ms step_avg:126.77ms
step:901/1393 train_time:112960ms step_avg:126.78ms
step:902/1393 train_time:113091ms step_avg:126.78ms
step:903/1393 train_time:113223ms step_avg:126.79ms
step:904/1393 train_time:113353ms step_avg:126.79ms
step:905/1393 train_time:113485ms step_avg:126.80ms
step:906/1393 train_time:113615ms step_avg:126.80ms
step:907/1393 train_time:113747ms step_avg:126.81ms
step:908/1393 train_time:113877ms step_avg:126.81ms
step:909/1393 train_time:114008ms step_avg:126.82ms
step:910/1393 train_time:114141ms step_avg:126.82ms
step:911/1393 train_time:114272ms step_avg:126.83ms
step:912/1393 train_time:114403ms step_avg:126.83ms
step:913/1393 train_time:114534ms step_avg:126.84ms
step:914/1393 train_time:114665ms step_avg:126.84ms
step:915/1393 train_time:114796ms step_avg:126.85ms
step:916/1393 train_time:114927ms step_avg:126.85ms
step:917/1393 train_time:115058ms step_avg:126.86ms
step:918/1393 train_time:115189ms step_avg:126.86ms
step:919/1393 train_time:115323ms step_avg:126.87ms
step:920/1393 train_time:115455ms step_avg:126.87ms
step:921/1393 train_time:115585ms step_avg:126.88ms
step:922/1393 train_time:115717ms step_avg:126.88ms
step:923/1393 train_time:115848ms step_avg:126.89ms
step:924/1393 train_time:115978ms step_avg:126.89ms
step:925/1393 train_time:116109ms step_avg:126.90ms
step:926/1393 train_time:116241ms step_avg:126.90ms
step:927/1393 train_time:116372ms step_avg:126.90ms
step:928/1393 train_time:116503ms step_avg:126.91ms
step:929/1393 train_time:116633ms step_avg:126.91ms
step:930/1393 train_time:116763ms step_avg:126.92ms
step:931/1393 train_time:116896ms step_avg:126.92ms
step:932/1393 train_time:117028ms step_avg:126.93ms
step:933/1393 train_time:117161ms step_avg:126.94ms
step:934/1393 train_time:117294ms step_avg:126.94ms
step:935/1393 train_time:117427ms step_avg:126.95ms
step:936/1393 train_time:117564ms step_avg:126.96ms
step:937/1393 train_time:117695ms step_avg:126.96ms
step:938/1393 train_time:117828ms step_avg:126.97ms
step:939/1393 train_time:117961ms step_avg:126.98ms
step:940/1393 train_time:118093ms step_avg:126.98ms
step:941/1393 train_time:118226ms step_avg:126.99ms
step:942/1393 train_time:118358ms step_avg:126.99ms
step:943/1393 train_time:118492ms step_avg:127.00ms
step:944/1393 train_time:118626ms step_avg:127.01ms
step:945/1393 train_time:118760ms step_avg:127.02ms
step:946/1393 train_time:118892ms step_avg:127.02ms
step:947/1393 train_time:119026ms step_avg:127.03ms
step:948/1393 train_time:119159ms step_avg:127.03ms
step:949/1393 train_time:119291ms step_avg:127.04ms
step:950/1393 train_time:119424ms step_avg:127.05ms
step:951/1393 train_time:119558ms step_avg:127.05ms
step:952/1393 train_time:119689ms step_avg:127.06ms
step:953/1393 train_time:119823ms step_avg:127.07ms
step:954/1393 train_time:119955ms step_avg:127.07ms
step:955/1393 train_time:120087ms step_avg:127.08ms
step:956/1393 train_time:120223ms step_avg:127.09ms
step:957/1393 train_time:120356ms step_avg:127.09ms
step:958/1393 train_time:120488ms step_avg:127.10ms
step:959/1393 train_time:120622ms step_avg:127.10ms
step:960/1393 train_time:120755ms step_avg:127.11ms
step:961/1393 train_time:120887ms step_avg:127.12ms
step:962/1393 train_time:121020ms step_avg:127.12ms
step:963/1393 train_time:121153ms step_avg:127.13ms
step:964/1393 train_time:121286ms step_avg:127.13ms
step:965/1393 train_time:121420ms step_avg:127.14ms
step:966/1393 train_time:121552ms step_avg:127.15ms
step:967/1393 train_time:121685ms step_avg:127.15ms
step:968/1393 train_time:121818ms step_avg:127.16ms
step:969/1393 train_time:121951ms step_avg:127.17ms
step:970/1393 train_time:122085ms step_avg:127.17ms
step:971/1393 train_time:122216ms step_avg:127.18ms
step:972/1393 train_time:122349ms step_avg:127.18ms
step:973/1393 train_time:122481ms step_avg:127.19ms
step:974/1393 train_time:122614ms step_avg:127.19ms
step:975/1393 train_time:122746ms step_avg:127.20ms
step:976/1393 train_time:122879ms step_avg:127.20ms
step:977/1393 train_time:123011ms step_avg:127.21ms
step:978/1393 train_time:123144ms step_avg:127.22ms
step:979/1393 train_time:123276ms step_avg:127.22ms
step:980/1393 train_time:123410ms step_avg:127.23ms
step:981/1393 train_time:123542ms step_avg:127.23ms
step:982/1393 train_time:123675ms step_avg:127.24ms
step:983/1393 train_time:123806ms step_avg:127.24ms
step:984/1393 train_time:123939ms step_avg:127.25ms
step:985/1393 train_time:124072ms step_avg:127.25ms
step:986/1393 train_time:124207ms step_avg:127.26ms
step:987/1393 train_time:124340ms step_avg:127.27ms
step:988/1393 train_time:124472ms step_avg:127.27ms
step:989/1393 train_time:124605ms step_avg:127.28ms
step:990/1393 train_time:124738ms step_avg:127.28ms
step:991/1393 train_time:124871ms step_avg:127.29ms
step:992/1393 train_time:125005ms step_avg:127.30ms
step:993/1393 train_time:125139ms step_avg:127.30ms
step:994/1393 train_time:125272ms step_avg:127.31ms
step:995/1393 train_time:125405ms step_avg:127.32ms
step:996/1393 train_time:125537ms step_avg:127.32ms
step:997/1393 train_time:125669ms step_avg:127.32ms
step:998/1393 train_time:125802ms step_avg:127.33ms
step:999/1393 train_time:125935ms step_avg:127.34ms
step:1000/1393 train_time:126070ms step_avg:127.34ms
step:1000/1393 val_loss:3.4174 train_time:126197ms step_avg:127.47ms
step:1001/1393 train_time:126219ms step_avg:127.37ms
step:1002/1393 train_time:126339ms step_avg:127.36ms
step:1003/1393 train_time:126471ms step_avg:127.36ms
step:1004/1393 train_time:126604ms step_avg:127.37ms
step:1005/1393 train_time:126738ms step_avg:127.37ms
step:1006/1393 train_time:126869ms step_avg:127.38ms
step:1007/1393 train_time:127001ms step_avg:127.38ms
step:1008/1393 train_time:127133ms step_avg:127.39ms
step:1009/1393 train_time:127268ms step_avg:127.40ms
step:1010/1393 train_time:127401ms step_avg:127.40ms
step:1011/1393 train_time:127535ms step_avg:127.41ms
step:1012/1393 train_time:127669ms step_avg:127.41ms
step:1013/1393 train_time:127801ms step_avg:127.42ms
step:1014/1393 train_time:127934ms step_avg:127.42ms
step:1015/1393 train_time:128066ms step_avg:127.43ms
step:1016/1393 train_time:128197ms step_avg:127.43ms
step:1017/1393 train_time:128330ms step_avg:127.44ms
step:1018/1393 train_time:128463ms step_avg:127.44ms
step:1019/1393 train_time:128596ms step_avg:127.45ms
step:1020/1393 train_time:128729ms step_avg:127.45ms
step:1021/1393 train_time:128862ms step_avg:127.46ms
step:1022/1393 train_time:128993ms step_avg:127.46ms
step:1023/1393 train_time:129127ms step_avg:127.47ms
step:1024/1393 train_time:129260ms step_avg:127.48ms
step:1025/1393 train_time:129393ms step_avg:127.48ms
step:1026/1393 train_time:129526ms step_avg:127.49ms
step:1027/1393 train_time:129659ms step_avg:127.49ms
step:1028/1393 train_time:129791ms step_avg:127.50ms
step:1029/1393 train_time:129925ms step_avg:127.50ms
step:1030/1393 train_time:130057ms step_avg:127.51ms
step:1031/1393 train_time:130189ms step_avg:127.51ms
step:1032/1393 train_time:130320ms step_avg:127.51ms
step:1033/1393 train_time:130452ms step_avg:127.52ms
step:1034/1393 train_time:130586ms step_avg:127.53ms
step:1035/1393 train_time:130719ms step_avg:127.53ms
step:1036/1393 train_time:130851ms step_avg:127.53ms
step:1037/1393 train_time:130984ms step_avg:127.54ms
step:1038/1393 train_time:131117ms step_avg:127.55ms
step:1039/1393 train_time:131248ms step_avg:127.55ms
step:1040/1393 train_time:131380ms step_avg:127.55ms
step:1041/1393 train_time:131513ms step_avg:127.56ms
step:1042/1393 train_time:131645ms step_avg:127.56ms
step:1043/1393 train_time:131778ms step_avg:127.57ms
step:1044/1393 train_time:131912ms step_avg:127.57ms
step:1045/1393 train_time:132045ms step_avg:127.58ms
step:1046/1393 train_time:132177ms step_avg:127.58ms
step:1047/1393 train_time:132310ms step_avg:127.59ms
step:1048/1393 train_time:132442ms step_avg:127.59ms
step:1049/1393 train_time:132576ms step_avg:127.60ms
step:1050/1393 train_time:132711ms step_avg:127.61ms
step:1051/1393 train_time:132844ms step_avg:127.61ms
step:1052/1393 train_time:132976ms step_avg:127.62ms
step:1053/1393 train_time:133115ms step_avg:127.63ms
step:1054/1393 train_time:133243ms step_avg:127.63ms
step:1055/1393 train_time:133375ms step_avg:127.63ms
step:1056/1393 train_time:133507ms step_avg:127.64ms
step:1057/1393 train_time:133641ms step_avg:127.64ms
step:1058/1393 train_time:133774ms step_avg:127.65ms
step:1059/1393 train_time:133908ms step_avg:127.65ms
step:1060/1393 train_time:134044ms step_avg:127.66ms
step:1061/1393 train_time:134176ms step_avg:127.67ms
step:1062/1393 train_time:134308ms step_avg:127.67ms
step:1063/1393 train_time:134440ms step_avg:127.67ms
step:1064/1393 train_time:134574ms step_avg:127.68ms
step:1065/1393 train_time:134706ms step_avg:127.68ms
step:1066/1393 train_time:134840ms step_avg:127.69ms
step:1067/1393 train_time:134973ms step_avg:127.69ms
step:1068/1393 train_time:135106ms step_avg:127.70ms
step:1069/1393 train_time:135240ms step_avg:127.71ms
step:1070/1393 train_time:135372ms step_avg:127.71ms
step:1071/1393 train_time:135507ms step_avg:127.72ms
step:1072/1393 train_time:135640ms step_avg:127.72ms
step:1073/1393 train_time:135773ms step_avg:127.73ms
step:1074/1393 train_time:135905ms step_avg:127.73ms
step:1075/1393 train_time:136037ms step_avg:127.73ms
step:1076/1393 train_time:136169ms step_avg:127.74ms
step:1077/1393 train_time:136301ms step_avg:127.74ms
step:1078/1393 train_time:136435ms step_avg:127.75ms
step:1079/1393 train_time:136571ms step_avg:127.76ms
step:1080/1393 train_time:136704ms step_avg:127.76ms
step:1081/1393 train_time:136837ms step_avg:127.77ms
step:1082/1393 train_time:136970ms step_avg:127.77ms
step:1083/1393 train_time:137103ms step_avg:127.78ms
step:1084/1393 train_time:137236ms step_avg:127.78ms
step:1085/1393 train_time:137369ms step_avg:127.78ms
step:1086/1393 train_time:137500ms step_avg:127.79ms
step:1087/1393 train_time:137635ms step_avg:127.79ms
step:1088/1393 train_time:137768ms step_avg:127.80ms
step:1089/1393 train_time:137901ms step_avg:127.80ms
step:1090/1393 train_time:138036ms step_avg:127.81ms
step:1091/1393 train_time:138167ms step_avg:127.81ms
step:1092/1393 train_time:138300ms step_avg:127.82ms
step:1093/1393 train_time:138434ms step_avg:127.82ms
step:1094/1393 train_time:138566ms step_avg:127.83ms
step:1095/1393 train_time:138698ms step_avg:127.83ms
step:1096/1393 train_time:138832ms step_avg:127.84ms
step:1097/1393 train_time:138965ms step_avg:127.84ms
step:1098/1393 train_time:139099ms step_avg:127.85ms
step:1099/1393 train_time:139232ms step_avg:127.85ms
step:1100/1393 train_time:139365ms step_avg:127.86ms
step:1101/1393 train_time:139497ms step_avg:127.86ms
step:1102/1393 train_time:139631ms step_avg:127.87ms
step:1103/1393 train_time:139764ms step_avg:127.87ms
step:1104/1393 train_time:139897ms step_avg:127.88ms
step:1105/1393 train_time:140032ms step_avg:127.88ms
step:1106/1393 train_time:140164ms step_avg:127.89ms
step:1107/1393 train_time:140296ms step_avg:127.89ms
step:1108/1393 train_time:140432ms step_avg:127.90ms
step:1109/1393 train_time:140565ms step_avg:127.90ms
step:1110/1393 train_time:140698ms step_avg:127.91ms
step:1111/1393 train_time:140831ms step_avg:127.91ms
step:1112/1393 train_time:140964ms step_avg:127.92ms
step:1113/1393 train_time:141095ms step_avg:127.92ms
step:1114/1393 train_time:141229ms step_avg:127.92ms
step:1115/1393 train_time:141362ms step_avg:127.93ms
step:1116/1393 train_time:141495ms step_avg:127.93ms
step:1117/1393 train_time:141631ms step_avg:127.94ms
step:1118/1393 train_time:141765ms step_avg:127.95ms
step:1119/1393 train_time:141896ms step_avg:127.95ms
step:1120/1393 train_time:142029ms step_avg:127.95ms
step:1121/1393 train_time:142162ms step_avg:127.96ms
step:1122/1393 train_time:142295ms step_avg:127.96ms
step:1123/1393 train_time:142427ms step_avg:127.97ms
step:1124/1393 train_time:142560ms step_avg:127.97ms
step:1125/1393 train_time:142692ms step_avg:127.97ms
step:1125/1393 val_loss:3.3666 train_time:142824ms step_avg:128.09ms
step:1126/1393 train_time:142846ms step_avg:128.00ms
step:1127/1393 train_time:142965ms step_avg:127.99ms
step:1128/1393 train_time:143099ms step_avg:128.00ms
step:1129/1393 train_time:143233ms step_avg:128.00ms
step:1130/1393 train_time:143366ms step_avg:128.00ms
step:1131/1393 train_time:143499ms step_avg:128.01ms
step:1132/1393 train_time:143631ms step_avg:128.01ms
step:1133/1393 train_time:143762ms step_avg:128.02ms
step:1134/1393 train_time:143897ms step_avg:128.02ms
step:1135/1393 train_time:144031ms step_avg:128.03ms
step:1136/1393 train_time:144166ms step_avg:128.03ms
step:1137/1393 train_time:144298ms step_avg:128.04ms
step:1138/1393 train_time:144435ms step_avg:128.04ms
step:1139/1393 train_time:144568ms step_avg:128.05ms
step:1140/1393 train_time:144703ms step_avg:128.06ms
step:1141/1393 train_time:144837ms step_avg:128.06ms
step:1142/1393 train_time:144971ms step_avg:128.07ms
step:1143/1393 train_time:145108ms step_avg:128.07ms
step:1144/1393 train_time:145243ms step_avg:128.08ms
step:1145/1393 train_time:145377ms step_avg:128.09ms
step:1146/1393 train_time:145511ms step_avg:128.09ms
step:1147/1393 train_time:145645ms step_avg:128.10ms
step:1148/1393 train_time:145779ms step_avg:128.10ms
step:1149/1393 train_time:145912ms step_avg:128.11ms
step:1150/1393 train_time:146047ms step_avg:128.11ms
step:1151/1393 train_time:146182ms step_avg:128.12ms
step:1152/1393 train_time:146316ms step_avg:128.12ms
step:1153/1393 train_time:146453ms step_avg:128.13ms
step:1154/1393 train_time:146588ms step_avg:128.14ms
step:1155/1393 train_time:146722ms step_avg:128.14ms
step:1156/1393 train_time:146859ms step_avg:128.15ms
step:1157/1393 train_time:146993ms step_avg:128.15ms
step:1158/1393 train_time:147127ms step_avg:128.16ms
step:1159/1393 train_time:147261ms step_avg:128.16ms
step:1160/1393 train_time:147395ms step_avg:128.17ms
step:1161/1393 train_time:147528ms step_avg:128.17ms
step:1162/1393 train_time:147662ms step_avg:128.18ms
step:1163/1393 train_time:147796ms step_avg:128.18ms
step:1164/1393 train_time:147931ms step_avg:128.19ms
step:1165/1393 train_time:148065ms step_avg:128.19ms
step:1166/1393 train_time:148199ms step_avg:128.20ms
step:1167/1393 train_time:148333ms step_avg:128.20ms
step:1168/1393 train_time:148468ms step_avg:128.21ms
step:1169/1393 train_time:148602ms step_avg:128.22ms
step:1170/1393 train_time:148736ms step_avg:128.22ms
step:1171/1393 train_time:148871ms step_avg:128.23ms
step:1172/1393 train_time:149005ms step_avg:128.23ms
step:1173/1393 train_time:149139ms step_avg:128.24ms
step:1174/1393 train_time:149277ms step_avg:128.25ms
step:1175/1393 train_time:149411ms step_avg:128.25ms
step:1176/1393 train_time:149546ms step_avg:128.26ms
step:1177/1393 train_time:149683ms step_avg:128.26ms
step:1178/1393 train_time:149817ms step_avg:128.27ms
step:1179/1393 train_time:149951ms step_avg:128.27ms
step:1180/1393 train_time:150086ms step_avg:128.28ms
step:1181/1393 train_time:150222ms step_avg:128.29ms
step:1182/1393 train_time:150356ms step_avg:128.29ms
step:1183/1393 train_time:150490ms step_avg:128.30ms
step:1184/1393 train_time:150625ms step_avg:128.30ms
step:1185/1393 train_time:150760ms step_avg:128.31ms
step:1186/1393 train_time:150895ms step_avg:128.31ms
step:1187/1393 train_time:151034ms step_avg:128.32ms
step:1188/1393 train_time:151169ms step_avg:128.33ms
step:1189/1393 train_time:151304ms step_avg:128.33ms
step:1190/1393 train_time:151438ms step_avg:128.34ms
step:1191/1393 train_time:151573ms step_avg:128.34ms
step:1192/1393 train_time:151707ms step_avg:128.35ms
step:1193/1393 train_time:151841ms step_avg:128.35ms
step:1194/1393 train_time:151975ms step_avg:128.36ms
step:1195/1393 train_time:152108ms step_avg:128.36ms
step:1196/1393 train_time:152242ms step_avg:128.37ms
step:1197/1393 train_time:152378ms step_avg:128.37ms
step:1198/1393 train_time:152514ms step_avg:128.38ms
step:1199/1393 train_time:152649ms step_avg:128.38ms
step:1200/1393 train_time:152783ms step_avg:128.39ms
step:1201/1393 train_time:152916ms step_avg:128.39ms
step:1202/1393 train_time:153054ms step_avg:128.40ms
step:1203/1393 train_time:153192ms step_avg:128.41ms
step:1204/1393 train_time:153326ms step_avg:128.41ms
step:1205/1393 train_time:153461ms step_avg:128.42ms
step:1206/1393 train_time:153596ms step_avg:128.42ms
step:1207/1393 train_time:153732ms step_avg:128.43ms
step:1208/1393 train_time:153865ms step_avg:128.43ms
step:1209/1393 train_time:154000ms step_avg:128.44ms
step:1210/1393 train_time:154135ms step_avg:128.45ms
step:1211/1393 train_time:154270ms step_avg:128.45ms
step:1212/1393 train_time:154403ms step_avg:128.45ms
step:1213/1393 train_time:154537ms step_avg:128.46ms
step:1214/1393 train_time:154672ms step_avg:128.47ms
step:1215/1393 train_time:154809ms step_avg:128.47ms
step:1216/1393 train_time:154941ms step_avg:128.48ms
step:1217/1393 train_time:155078ms step_avg:128.48ms
step:1218/1393 train_time:155210ms step_avg:128.49ms
step:1219/1393 train_time:155343ms step_avg:128.49ms
step:1220/1393 train_time:155477ms step_avg:128.49ms
step:1221/1393 train_time:155610ms step_avg:128.50ms
step:1222/1393 train_time:155744ms step_avg:128.50ms
step:1223/1393 train_time:155877ms step_avg:128.51ms
step:1224/1393 train_time:156011ms step_avg:128.51ms
step:1225/1393 train_time:156148ms step_avg:128.52ms
step:1226/1393 train_time:156282ms step_avg:128.52ms
step:1227/1393 train_time:156417ms step_avg:128.53ms
step:1228/1393 train_time:156552ms step_avg:128.53ms
step:1229/1393 train_time:156685ms step_avg:128.54ms
step:1230/1393 train_time:156821ms step_avg:128.54ms
step:1231/1393 train_time:156955ms step_avg:128.55ms
step:1232/1393 train_time:157090ms step_avg:128.55ms
step:1233/1393 train_time:157224ms step_avg:128.56ms
step:1234/1393 train_time:157358ms step_avg:128.56ms
step:1235/1393 train_time:157492ms step_avg:128.57ms
step:1236/1393 train_time:157626ms step_avg:128.57ms
step:1237/1393 train_time:157759ms step_avg:128.57ms
step:1238/1393 train_time:157898ms step_avg:128.58ms
step:1239/1393 train_time:158033ms step_avg:128.59ms
step:1240/1393 train_time:158168ms step_avg:128.59ms
step:1241/1393 train_time:158304ms step_avg:128.60ms
step:1242/1393 train_time:158437ms step_avg:128.60ms
step:1243/1393 train_time:158574ms step_avg:128.61ms
step:1244/1393 train_time:158709ms step_avg:128.61ms
step:1245/1393 train_time:158843ms step_avg:128.62ms
step:1246/1393 train_time:158976ms step_avg:128.62ms
step:1247/1393 train_time:159111ms step_avg:128.63ms
step:1248/1393 train_time:159243ms step_avg:128.63ms
step:1249/1393 train_time:159376ms step_avg:128.63ms
step:1250/1393 train_time:159511ms step_avg:128.64ms
step:1250/1393 val_loss:3.3195 train_time:159644ms step_avg:128.75ms
step:1251/1393 train_time:159665ms step_avg:128.66ms
step:1252/1393 train_time:159788ms step_avg:128.65ms
step:1253/1393 train_time:159921ms step_avg:128.66ms
step:1254/1393 train_time:160053ms step_avg:128.66ms
step:1255/1393 train_time:160193ms step_avg:128.67ms
step:1256/1393 train_time:160326ms step_avg:128.67ms
step:1257/1393 train_time:160459ms step_avg:128.68ms
step:1258/1393 train_time:160594ms step_avg:128.68ms
step:1259/1393 train_time:160729ms step_avg:128.69ms
step:1260/1393 train_time:160864ms step_avg:128.69ms
step:1261/1393 train_time:160998ms step_avg:128.70ms
step:1262/1393 train_time:161136ms step_avg:128.70ms
step:1263/1393 train_time:161272ms step_avg:128.71ms
step:1264/1393 train_time:161407ms step_avg:128.71ms
step:1265/1393 train_time:161541ms step_avg:128.72ms
step:1266/1393 train_time:161676ms step_avg:128.72ms
step:1267/1393 train_time:161809ms step_avg:128.73ms
step:1268/1393 train_time:161943ms step_avg:128.73ms
step:1269/1393 train_time:162078ms step_avg:128.74ms
step:1270/1393 train_time:162212ms step_avg:128.74ms
step:1271/1393 train_time:162347ms step_avg:128.75ms
step:1272/1393 train_time:162482ms step_avg:128.75ms
step:1273/1393 train_time:162616ms step_avg:128.75ms
step:1274/1393 train_time:162750ms step_avg:128.76ms
step:1275/1393 train_time:162884ms step_avg:128.76ms
step:1276/1393 train_time:163017ms step_avg:128.77ms
step:1277/1393 train_time:163154ms step_avg:128.77ms
step:1278/1393 train_time:163289ms step_avg:128.78ms
step:1279/1393 train_time:163425ms step_avg:128.78ms
step:1280/1393 train_time:163562ms step_avg:128.79ms
step:1281/1393 train_time:163696ms step_avg:128.79ms
step:1282/1393 train_time:163829ms step_avg:128.80ms
step:1283/1393 train_time:163963ms step_avg:128.80ms
step:1284/1393 train_time:164098ms step_avg:128.81ms
step:1285/1393 train_time:164232ms step_avg:128.81ms
step:1286/1393 train_time:164368ms step_avg:128.82ms
step:1287/1393 train_time:164503ms step_avg:128.82ms
step:1288/1393 train_time:164637ms step_avg:128.82ms
step:1289/1393 train_time:164774ms step_avg:128.83ms
step:1290/1393 train_time:164911ms step_avg:128.84ms
step:1291/1393 train_time:165047ms step_avg:128.84ms
step:1292/1393 train_time:165182ms step_avg:128.85ms
step:1293/1393 train_time:165319ms step_avg:128.85ms
step:1294/1393 train_time:165453ms step_avg:128.86ms
step:1295/1393 train_time:165587ms step_avg:128.86ms
step:1296/1393 train_time:165723ms step_avg:128.87ms
step:1297/1393 train_time:165858ms step_avg:128.87ms
step:1298/1393 train_time:165992ms step_avg:128.88ms
step:1299/1393 train_time:166126ms step_avg:128.88ms
step:1300/1393 train_time:166262ms step_avg:128.88ms
step:1301/1393 train_time:166395ms step_avg:128.89ms
step:1302/1393 train_time:166530ms step_avg:128.89ms
step:1303/1393 train_time:166666ms step_avg:128.90ms
step:1304/1393 train_time:166802ms step_avg:128.90ms
step:1305/1393 train_time:166938ms step_avg:128.91ms
step:1306/1393 train_time:167073ms step_avg:128.91ms
step:1307/1393 train_time:167208ms step_avg:128.92ms
step:1308/1393 train_time:167342ms step_avg:128.92ms
step:1309/1393 train_time:167478ms step_avg:128.93ms
step:1310/1393 train_time:167612ms step_avg:128.93ms
step:1311/1393 train_time:167746ms step_avg:128.94ms
step:1312/1393 train_time:167880ms step_avg:128.94ms
step:1313/1393 train_time:168015ms step_avg:128.94ms
step:1314/1393 train_time:168149ms step_avg:128.95ms
step:1315/1393 train_time:168284ms step_avg:128.95ms
step:1316/1393 train_time:168417ms step_avg:128.96ms
step:1317/1393 train_time:168551ms step_avg:128.96ms
step:1318/1393 train_time:168688ms step_avg:128.97ms
step:1319/1393 train_time:168824ms step_avg:128.97ms
step:1320/1393 train_time:168958ms step_avg:128.98ms
step:1321/1393 train_time:169091ms step_avg:128.98ms
step:1322/1393 train_time:169229ms step_avg:128.99ms
step:1323/1393 train_time:169364ms step_avg:128.99ms
step:1324/1393 train_time:169497ms step_avg:128.99ms
step:1325/1393 train_time:169633ms step_avg:129.00ms
step:1326/1393 train_time:169769ms step_avg:129.00ms
step:1327/1393 train_time:169903ms step_avg:129.01ms
step:1328/1393 train_time:170037ms step_avg:129.01ms
step:1329/1393 train_time:170175ms step_avg:129.02ms
step:1330/1393 train_time:170310ms step_avg:129.02ms
step:1331/1393 train_time:170448ms step_avg:129.03ms
step:1332/1393 train_time:170585ms step_avg:129.04ms
step:1333/1393 train_time:170721ms step_avg:129.04ms
step:1334/1393 train_time:170855ms step_avg:129.04ms
step:1335/1393 train_time:170988ms step_avg:129.05ms
step:1336/1393 train_time:171127ms step_avg:129.06ms
step:1337/1393 train_time:171262ms step_avg:129.06ms
step:1338/1393 train_time:171396ms step_avg:129.06ms
step:1339/1393 train_time:171532ms step_avg:129.07ms
step:1340/1393 train_time:171667ms step_avg:129.07ms
step:1341/1393 train_time:171800ms step_avg:129.08ms
step:1342/1393 train_time:171935ms step_avg:129.08ms
step:1343/1393 train_time:172069ms step_avg:129.08ms
step:1344/1393 train_time:172204ms step_avg:129.09ms
step:1345/1393 train_time:172339ms step_avg:129.09ms
step:1346/1393 train_time:172474ms step_avg:129.10ms
step:1347/1393 train_time:172612ms step_avg:129.10ms
step:1348/1393 train_time:172746ms step_avg:129.11ms
step:1349/1393 train_time:172882ms step_avg:129.11ms
step:1350/1393 train_time:173016ms step_avg:129.12ms
step:1351/1393 train_time:173151ms step_avg:129.12ms
step:1352/1393 train_time:173291ms step_avg:129.13ms
step:1353/1393 train_time:173428ms step_avg:129.13ms
step:1354/1393 train_time:173563ms step_avg:129.14ms
step:1355/1393 train_time:173698ms step_avg:129.14ms
step:1356/1393 train_time:173831ms step_avg:129.15ms
step:1357/1393 train_time:173967ms step_avg:129.15ms
step:1358/1393 train_time:174103ms step_avg:129.16ms
step:1359/1393 train_time:174239ms step_avg:129.16ms
step:1360/1393 train_time:174375ms step_avg:129.17ms
step:1361/1393 train_time:174510ms step_avg:129.17ms
step:1362/1393 train_time:174647ms step_avg:129.18ms
step:1363/1393 train_time:174786ms step_avg:129.18ms
step:1364/1393 train_time:174922ms step_avg:129.19ms
step:1365/1393 train_time:175056ms step_avg:129.19ms
step:1366/1393 train_time:175191ms step_avg:129.20ms
step:1367/1393 train_time:175327ms step_avg:129.20ms
step:1368/1393 train_time:175463ms step_avg:129.21ms
step:1369/1393 train_time:175601ms step_avg:129.21ms
step:1370/1393 train_time:175739ms step_avg:129.22ms
step:1371/1393 train_time:175876ms step_avg:129.23ms
step:1372/1393 train_time:176015ms step_avg:129.23ms
step:1373/1393 train_time:176150ms step_avg:129.24ms
step:1374/1393 train_time:176288ms step_avg:129.24ms
step:1375/1393 train_time:176423ms step_avg:129.25ms
step:1375/1393 val_loss:3.2842 train_time:176556ms step_avg:129.35ms
step:1376/1393 train_time:176577ms step_avg:129.27ms
step:1377/1393 train_time:176701ms step_avg:129.26ms
step:1378/1393 train_time:176837ms step_avg:129.27ms
step:1379/1393 train_time:176973ms step_avg:129.27ms
step:1380/1393 train_time:177108ms step_avg:129.28ms
step:1381/1393 train_time:177246ms step_avg:129.28ms
step:1382/1393 train_time:177382ms step_avg:129.29ms
step:1383/1393 train_time:177517ms step_avg:129.29ms
step:1384/1393 train_time:177655ms step_avg:129.30ms
step:1385/1393 train_time:177790ms step_avg:129.30ms
step:1386/1393 train_time:177924ms step_avg:129.31ms
step:1387/1393 train_time:178063ms step_avg:129.31ms
step:1388/1393 train_time:178198ms step_avg:129.32ms
step:1389/1393 train_time:178334ms step_avg:129.32ms
step:1390/1393 train_time:178470ms step_avg:129.33ms
step:1391/1393 train_time:178606ms step_avg:129.33ms
step:1392/1393 train_time:178743ms step_avg:129.34ms
step:1393/1393 train_time:178878ms step_avg:129.34ms
step:1393/1393 val_loss:3.2804 train_time:179011ms step_avg:129.44ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
