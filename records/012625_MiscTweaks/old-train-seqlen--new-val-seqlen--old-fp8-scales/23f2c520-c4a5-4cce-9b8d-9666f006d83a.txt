import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:53:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:1ms step_avg:nanms
step:1/1393 train_time:25151ms step_avg:nanms
step:2/1393 train_time:25672ms step_avg:nanms
step:3/1393 train_time:25794ms step_avg:nanms
step:4/1393 train_time:25915ms step_avg:nanms
step:5/1393 train_time:26036ms step_avg:nanms
step:6/1393 train_time:26157ms step_avg:nanms
step:7/1393 train_time:26279ms step_avg:nanms
step:8/1393 train_time:26401ms step_avg:nanms
step:9/1393 train_time:26522ms step_avg:nanms
step:10/1393 train_time:26644ms step_avg:nanms
step:11/1393 train_time:123ms step_avg:nanms
step:12/1393 train_time:246ms step_avg:nanms
step:13/1393 train_time:368ms step_avg:122.75ms
step:14/1393 train_time:491ms step_avg:122.67ms
step:15/1393 train_time:613ms step_avg:122.62ms
step:16/1393 train_time:737ms step_avg:122.79ms
step:17/1393 train_time:861ms step_avg:123.01ms
step:18/1393 train_time:982ms step_avg:122.76ms
step:19/1393 train_time:1105ms step_avg:122.74ms
step:20/1393 train_time:1227ms step_avg:122.68ms
step:21/1393 train_time:1350ms step_avg:122.74ms
step:22/1393 train_time:1471ms step_avg:122.59ms
step:23/1393 train_time:1594ms step_avg:122.63ms
step:24/1393 train_time:1718ms step_avg:122.73ms
step:25/1393 train_time:1842ms step_avg:122.77ms
step:26/1393 train_time:1966ms step_avg:122.86ms
step:27/1393 train_time:2090ms step_avg:122.94ms
step:28/1393 train_time:2211ms step_avg:122.86ms
step:29/1393 train_time:2334ms step_avg:122.86ms
step:30/1393 train_time:2456ms step_avg:122.80ms
step:31/1393 train_time:2577ms step_avg:122.73ms
step:32/1393 train_time:2701ms step_avg:122.75ms
step:33/1393 train_time:2822ms step_avg:122.69ms
step:34/1393 train_time:2944ms step_avg:122.65ms
step:35/1393 train_time:3067ms step_avg:122.69ms
step:36/1393 train_time:3188ms step_avg:122.62ms
step:37/1393 train_time:3310ms step_avg:122.61ms
step:38/1393 train_time:3432ms step_avg:122.57ms
step:39/1393 train_time:3555ms step_avg:122.59ms
step:40/1393 train_time:3677ms step_avg:122.57ms
step:41/1393 train_time:3800ms step_avg:122.58ms
step:42/1393 train_time:3923ms step_avg:122.59ms
step:43/1393 train_time:4048ms step_avg:122.66ms
step:44/1393 train_time:4168ms step_avg:122.60ms
step:45/1393 train_time:4291ms step_avg:122.60ms
step:46/1393 train_time:4414ms step_avg:122.61ms
step:47/1393 train_time:4534ms step_avg:122.55ms
step:48/1393 train_time:4656ms step_avg:122.52ms
step:49/1393 train_time:4778ms step_avg:122.52ms
step:50/1393 train_time:4900ms step_avg:122.50ms
step:51/1393 train_time:5022ms step_avg:122.49ms
step:52/1393 train_time:5144ms step_avg:122.48ms
step:53/1393 train_time:5267ms step_avg:122.48ms
step:54/1393 train_time:5389ms step_avg:122.49ms
step:55/1393 train_time:5511ms step_avg:122.48ms
step:56/1393 train_time:5633ms step_avg:122.46ms
step:57/1393 train_time:5757ms step_avg:122.48ms
step:58/1393 train_time:5879ms step_avg:122.49ms
step:59/1393 train_time:6002ms step_avg:122.49ms
step:60/1393 train_time:6126ms step_avg:122.51ms
step:61/1393 train_time:6248ms step_avg:122.51ms
step:62/1393 train_time:6372ms step_avg:122.53ms
step:63/1393 train_time:6494ms step_avg:122.53ms
step:64/1393 train_time:6617ms step_avg:122.53ms
step:65/1393 train_time:6738ms step_avg:122.51ms
step:66/1393 train_time:6862ms step_avg:122.53ms
step:67/1393 train_time:6982ms step_avg:122.49ms
step:68/1393 train_time:7105ms step_avg:122.50ms
step:69/1393 train_time:7227ms step_avg:122.50ms
step:70/1393 train_time:7350ms step_avg:122.51ms
step:71/1393 train_time:7474ms step_avg:122.52ms
step:72/1393 train_time:7597ms step_avg:122.53ms
step:73/1393 train_time:7719ms step_avg:122.53ms
step:74/1393 train_time:7841ms step_avg:122.52ms
step:75/1393 train_time:7963ms step_avg:122.51ms
step:76/1393 train_time:8085ms step_avg:122.51ms
step:77/1393 train_time:8207ms step_avg:122.49ms
step:78/1393 train_time:8330ms step_avg:122.49ms
step:79/1393 train_time:8452ms step_avg:122.49ms
step:80/1393 train_time:8575ms step_avg:122.50ms
step:81/1393 train_time:8699ms step_avg:122.51ms
step:82/1393 train_time:8820ms step_avg:122.50ms
step:83/1393 train_time:8942ms step_avg:122.49ms
step:84/1393 train_time:9063ms step_avg:122.48ms
step:85/1393 train_time:9186ms step_avg:122.47ms
step:86/1393 train_time:9307ms step_avg:122.46ms
step:87/1393 train_time:9429ms step_avg:122.45ms
step:88/1393 train_time:9551ms step_avg:122.45ms
step:89/1393 train_time:9673ms step_avg:122.45ms
step:90/1393 train_time:9795ms step_avg:122.44ms
step:91/1393 train_time:9917ms step_avg:122.43ms
step:92/1393 train_time:10040ms step_avg:122.43ms
step:93/1393 train_time:10162ms step_avg:122.43ms
step:94/1393 train_time:10284ms step_avg:122.43ms
step:95/1393 train_time:10408ms step_avg:122.45ms
step:96/1393 train_time:10530ms step_avg:122.44ms
step:97/1393 train_time:10653ms step_avg:122.44ms
step:98/1393 train_time:10775ms step_avg:122.45ms
step:99/1393 train_time:10898ms step_avg:122.45ms
step:100/1393 train_time:11021ms step_avg:122.45ms
step:101/1393 train_time:11144ms step_avg:122.46ms
step:102/1393 train_time:11267ms step_avg:122.47ms
step:103/1393 train_time:11391ms step_avg:122.48ms
step:104/1393 train_time:11512ms step_avg:122.47ms
step:105/1393 train_time:11634ms step_avg:122.46ms
step:106/1393 train_time:11756ms step_avg:122.46ms
step:107/1393 train_time:11879ms step_avg:122.47ms
step:108/1393 train_time:12003ms step_avg:122.48ms
step:109/1393 train_time:12127ms step_avg:122.49ms
step:110/1393 train_time:12251ms step_avg:122.51ms
step:111/1393 train_time:12376ms step_avg:122.53ms
step:112/1393 train_time:12498ms step_avg:122.53ms
step:113/1393 train_time:12621ms step_avg:122.53ms
step:114/1393 train_time:12743ms step_avg:122.53ms
step:115/1393 train_time:12866ms step_avg:122.53ms
step:116/1393 train_time:12989ms step_avg:122.54ms
step:117/1393 train_time:13112ms step_avg:122.54ms
step:118/1393 train_time:13235ms step_avg:122.55ms
step:119/1393 train_time:13358ms step_avg:122.55ms
step:120/1393 train_time:13482ms step_avg:122.57ms
step:121/1393 train_time:13606ms step_avg:122.58ms
step:122/1393 train_time:13730ms step_avg:122.59ms
step:123/1393 train_time:13853ms step_avg:122.59ms
step:124/1393 train_time:13975ms step_avg:122.59ms
step:125/1393 train_time:14099ms step_avg:122.60ms
step:125/1393 val_loss:4.3994 train_time:14220ms step_avg:123.65ms
step:126/1393 train_time:14245ms step_avg:122.81ms
step:127/1393 train_time:14349ms step_avg:122.64ms
step:128/1393 train_time:14484ms step_avg:122.74ms
step:129/1393 train_time:14608ms step_avg:122.75ms
step:130/1393 train_time:14730ms step_avg:122.75ms
step:131/1393 train_time:14853ms step_avg:122.75ms
step:132/1393 train_time:14977ms step_avg:122.76ms
step:133/1393 train_time:15100ms step_avg:122.76ms
step:134/1393 train_time:15222ms step_avg:122.76ms
step:135/1393 train_time:15344ms step_avg:122.75ms
step:136/1393 train_time:15468ms step_avg:122.76ms
step:137/1393 train_time:15591ms step_avg:122.76ms
step:138/1393 train_time:15714ms step_avg:122.77ms
step:139/1393 train_time:15837ms step_avg:122.77ms
step:140/1393 train_time:15960ms step_avg:122.77ms
step:141/1393 train_time:16083ms step_avg:122.77ms
step:142/1393 train_time:16206ms step_avg:122.77ms
step:143/1393 train_time:16329ms step_avg:122.78ms
step:144/1393 train_time:16454ms step_avg:122.79ms
step:145/1393 train_time:16578ms step_avg:122.80ms
step:146/1393 train_time:16701ms step_avg:122.80ms
step:147/1393 train_time:16825ms step_avg:122.81ms
step:148/1393 train_time:16947ms step_avg:122.80ms
step:149/1393 train_time:17070ms step_avg:122.80ms
step:150/1393 train_time:17193ms step_avg:122.81ms
step:151/1393 train_time:17317ms step_avg:122.82ms
step:152/1393 train_time:17440ms step_avg:122.82ms
step:153/1393 train_time:17563ms step_avg:122.82ms
step:154/1393 train_time:17686ms step_avg:122.82ms
step:155/1393 train_time:17809ms step_avg:122.82ms
step:156/1393 train_time:17931ms step_avg:122.82ms
step:157/1393 train_time:18054ms step_avg:122.82ms
step:158/1393 train_time:18177ms step_avg:122.82ms
step:159/1393 train_time:18300ms step_avg:122.82ms
step:160/1393 train_time:18423ms step_avg:122.82ms
step:161/1393 train_time:18546ms step_avg:122.82ms
step:162/1393 train_time:18670ms step_avg:122.83ms
step:163/1393 train_time:18795ms step_avg:122.84ms
step:164/1393 train_time:18918ms step_avg:122.84ms
step:165/1393 train_time:19041ms step_avg:122.84ms
step:166/1393 train_time:19164ms step_avg:122.85ms
step:167/1393 train_time:19287ms step_avg:122.85ms
step:168/1393 train_time:19412ms step_avg:122.86ms
step:169/1393 train_time:19535ms step_avg:122.86ms
step:170/1393 train_time:19659ms step_avg:122.87ms
step:171/1393 train_time:19782ms step_avg:122.87ms
step:172/1393 train_time:19906ms step_avg:122.87ms
step:173/1393 train_time:20029ms step_avg:122.88ms
step:174/1393 train_time:20152ms step_avg:122.88ms
step:175/1393 train_time:20275ms step_avg:122.88ms
step:176/1393 train_time:20398ms step_avg:122.88ms
step:177/1393 train_time:20521ms step_avg:122.88ms
step:178/1393 train_time:20644ms step_avg:122.88ms
step:179/1393 train_time:20769ms step_avg:122.89ms
step:180/1393 train_time:20892ms step_avg:122.90ms
step:181/1393 train_time:21016ms step_avg:122.90ms
step:182/1393 train_time:21139ms step_avg:122.90ms
step:183/1393 train_time:21262ms step_avg:122.90ms
step:184/1393 train_time:21386ms step_avg:122.91ms
step:185/1393 train_time:21509ms step_avg:122.91ms
step:186/1393 train_time:21632ms step_avg:122.91ms
step:187/1393 train_time:21755ms step_avg:122.91ms
step:188/1393 train_time:21878ms step_avg:122.91ms
step:189/1393 train_time:22001ms step_avg:122.91ms
step:190/1393 train_time:22124ms step_avg:122.91ms
step:191/1393 train_time:22247ms step_avg:122.91ms
step:192/1393 train_time:22371ms step_avg:122.92ms
step:193/1393 train_time:22493ms step_avg:122.91ms
step:194/1393 train_time:22615ms step_avg:122.91ms
step:195/1393 train_time:22738ms step_avg:122.91ms
step:196/1393 train_time:22862ms step_avg:122.91ms
step:197/1393 train_time:22984ms step_avg:122.91ms
step:198/1393 train_time:23107ms step_avg:122.91ms
step:199/1393 train_time:23230ms step_avg:122.91ms
step:200/1393 train_time:23354ms step_avg:122.91ms
step:201/1393 train_time:23477ms step_avg:122.91ms
step:202/1393 train_time:23599ms step_avg:122.91ms
step:203/1393 train_time:23722ms step_avg:122.91ms
step:204/1393 train_time:23844ms step_avg:122.91ms
step:205/1393 train_time:23967ms step_avg:122.91ms
step:206/1393 train_time:24089ms step_avg:122.91ms
step:207/1393 train_time:24213ms step_avg:122.91ms
step:208/1393 train_time:24336ms step_avg:122.91ms
step:209/1393 train_time:24461ms step_avg:122.92ms
step:210/1393 train_time:24584ms step_avg:122.92ms
step:211/1393 train_time:24707ms step_avg:122.92ms
step:212/1393 train_time:24833ms step_avg:122.93ms
step:213/1393 train_time:24956ms step_avg:122.94ms
step:214/1393 train_time:25080ms step_avg:122.94ms
step:215/1393 train_time:25203ms step_avg:122.94ms
step:216/1393 train_time:25326ms step_avg:122.94ms
step:217/1393 train_time:25449ms step_avg:122.94ms
step:218/1393 train_time:25573ms step_avg:122.95ms
step:219/1393 train_time:25697ms step_avg:122.95ms
step:220/1393 train_time:25820ms step_avg:122.95ms
step:221/1393 train_time:25944ms step_avg:122.96ms
step:222/1393 train_time:26068ms step_avg:122.96ms
step:223/1393 train_time:26191ms step_avg:122.96ms
step:224/1393 train_time:26314ms step_avg:122.96ms
step:225/1393 train_time:26437ms step_avg:122.96ms
step:226/1393 train_time:26560ms step_avg:122.96ms
step:227/1393 train_time:26684ms step_avg:122.97ms
step:228/1393 train_time:26808ms step_avg:122.97ms
step:229/1393 train_time:26932ms step_avg:122.98ms
step:230/1393 train_time:27057ms step_avg:122.99ms
step:231/1393 train_time:27181ms step_avg:122.99ms
step:232/1393 train_time:27304ms step_avg:122.99ms
step:233/1393 train_time:27427ms step_avg:122.99ms
step:234/1393 train_time:27550ms step_avg:122.99ms
step:235/1393 train_time:27673ms step_avg:122.99ms
step:236/1393 train_time:27797ms step_avg:122.99ms
step:237/1393 train_time:27920ms step_avg:123.00ms
step:238/1393 train_time:28045ms step_avg:123.00ms
step:239/1393 train_time:28168ms step_avg:123.00ms
step:240/1393 train_time:28293ms step_avg:123.01ms
step:241/1393 train_time:28416ms step_avg:123.01ms
step:242/1393 train_time:28539ms step_avg:123.01ms
step:243/1393 train_time:28661ms step_avg:123.01ms
step:244/1393 train_time:28784ms step_avg:123.01ms
step:245/1393 train_time:28908ms step_avg:123.01ms
step:246/1393 train_time:29032ms step_avg:123.02ms
step:247/1393 train_time:29156ms step_avg:123.02ms
step:248/1393 train_time:29279ms step_avg:123.02ms
step:249/1393 train_time:29403ms step_avg:123.03ms
step:250/1393 train_time:29526ms step_avg:123.02ms
step:250/1393 val_loss:3.9900 train_time:29647ms step_avg:123.53ms
step:251/1393 train_time:29671ms step_avg:123.12ms
step:252/1393 train_time:29783ms step_avg:123.07ms
step:253/1393 train_time:29910ms step_avg:123.09ms
step:254/1393 train_time:30033ms step_avg:123.09ms
step:255/1393 train_time:30156ms step_avg:123.08ms
step:256/1393 train_time:30279ms step_avg:123.09ms
step:257/1393 train_time:30402ms step_avg:123.08ms
step:258/1393 train_time:30524ms step_avg:123.08ms
step:259/1393 train_time:30647ms step_avg:123.08ms
step:260/1393 train_time:30774ms step_avg:123.10ms
step:261/1393 train_time:30898ms step_avg:123.10ms
step:262/1393 train_time:31021ms step_avg:123.10ms
step:263/1393 train_time:31146ms step_avg:123.11ms
step:264/1393 train_time:31268ms step_avg:123.10ms
step:265/1393 train_time:31393ms step_avg:123.11ms
step:266/1393 train_time:31514ms step_avg:123.10ms
step:267/1393 train_time:31638ms step_avg:123.10ms
step:268/1393 train_time:31761ms step_avg:123.10ms
step:269/1393 train_time:31885ms step_avg:123.11ms
step:270/1393 train_time:32008ms step_avg:123.11ms
step:271/1393 train_time:32132ms step_avg:123.11ms
step:272/1393 train_time:32255ms step_avg:123.11ms
step:273/1393 train_time:32378ms step_avg:123.11ms
step:274/1393 train_time:32501ms step_avg:123.11ms
step:275/1393 train_time:32624ms step_avg:123.11ms
step:276/1393 train_time:32749ms step_avg:123.12ms
step:277/1393 train_time:32873ms step_avg:123.12ms
step:278/1393 train_time:32996ms step_avg:123.12ms
step:279/1393 train_time:33119ms step_avg:123.12ms
step:280/1393 train_time:33243ms step_avg:123.12ms
step:281/1393 train_time:33366ms step_avg:123.12ms
step:282/1393 train_time:33489ms step_avg:123.12ms
step:283/1393 train_time:33612ms step_avg:123.12ms
step:284/1393 train_time:33736ms step_avg:123.12ms
step:285/1393 train_time:33859ms step_avg:123.12ms
step:286/1393 train_time:33984ms step_avg:123.13ms
step:287/1393 train_time:34108ms step_avg:123.14ms
step:288/1393 train_time:34232ms step_avg:123.14ms
step:289/1393 train_time:34355ms step_avg:123.14ms
step:290/1393 train_time:34478ms step_avg:123.14ms
step:291/1393 train_time:34601ms step_avg:123.14ms
step:292/1393 train_time:34724ms step_avg:123.14ms
step:293/1393 train_time:34848ms step_avg:123.14ms
step:294/1393 train_time:34973ms step_avg:123.14ms
step:295/1393 train_time:35096ms step_avg:123.14ms
step:296/1393 train_time:35220ms step_avg:123.15ms
step:297/1393 train_time:35343ms step_avg:123.15ms
step:298/1393 train_time:35467ms step_avg:123.15ms
step:299/1393 train_time:35590ms step_avg:123.15ms
step:300/1393 train_time:35713ms step_avg:123.15ms
step:301/1393 train_time:35837ms step_avg:123.15ms
step:302/1393 train_time:35960ms step_avg:123.15ms
step:303/1393 train_time:36085ms step_avg:123.16ms
step:304/1393 train_time:36208ms step_avg:123.16ms
step:305/1393 train_time:36331ms step_avg:123.16ms
step:306/1393 train_time:36455ms step_avg:123.16ms
step:307/1393 train_time:36579ms step_avg:123.16ms
step:308/1393 train_time:36703ms step_avg:123.16ms
step:309/1393 train_time:36826ms step_avg:123.16ms
step:310/1393 train_time:36950ms step_avg:123.17ms
step:311/1393 train_time:37073ms step_avg:123.17ms
step:312/1393 train_time:37200ms step_avg:123.18ms
step:313/1393 train_time:37326ms step_avg:123.19ms
step:314/1393 train_time:37452ms step_avg:123.20ms
step:315/1393 train_time:37578ms step_avg:123.21ms
step:316/1393 train_time:37705ms step_avg:123.22ms
step:317/1393 train_time:37834ms step_avg:123.24ms
step:318/1393 train_time:37959ms step_avg:123.24ms
step:319/1393 train_time:38085ms step_avg:123.25ms
step:320/1393 train_time:38211ms step_avg:123.26ms
step:321/1393 train_time:38336ms step_avg:123.27ms
step:322/1393 train_time:38463ms step_avg:123.28ms
step:323/1393 train_time:38589ms step_avg:123.29ms
step:324/1393 train_time:38715ms step_avg:123.29ms
step:325/1393 train_time:38841ms step_avg:123.30ms
step:326/1393 train_time:38967ms step_avg:123.31ms
step:327/1393 train_time:39092ms step_avg:123.32ms
step:328/1393 train_time:39219ms step_avg:123.33ms
step:329/1393 train_time:39345ms step_avg:123.34ms
step:330/1393 train_time:39471ms step_avg:123.35ms
step:331/1393 train_time:39597ms step_avg:123.35ms
step:332/1393 train_time:39725ms step_avg:123.37ms
step:333/1393 train_time:39850ms step_avg:123.38ms
step:334/1393 train_time:39975ms step_avg:123.38ms
step:335/1393 train_time:40102ms step_avg:123.39ms
step:336/1393 train_time:40228ms step_avg:123.40ms
step:337/1393 train_time:40354ms step_avg:123.41ms
step:338/1393 train_time:40479ms step_avg:123.41ms
step:339/1393 train_time:40605ms step_avg:123.42ms
step:340/1393 train_time:40731ms step_avg:123.43ms
step:341/1393 train_time:40857ms step_avg:123.44ms
step:342/1393 train_time:40983ms step_avg:123.44ms
step:343/1393 train_time:41109ms step_avg:123.45ms
step:344/1393 train_time:41234ms step_avg:123.46ms
step:345/1393 train_time:41360ms step_avg:123.46ms
step:346/1393 train_time:41485ms step_avg:123.47ms
step:347/1393 train_time:41611ms step_avg:123.48ms
step:348/1393 train_time:41737ms step_avg:123.48ms
step:349/1393 train_time:41862ms step_avg:123.49ms
step:350/1393 train_time:41989ms step_avg:123.50ms
step:351/1393 train_time:42115ms step_avg:123.50ms
step:352/1393 train_time:42240ms step_avg:123.51ms
step:353/1393 train_time:42366ms step_avg:123.52ms
step:354/1393 train_time:42491ms step_avg:123.52ms
step:355/1393 train_time:42619ms step_avg:123.53ms
step:356/1393 train_time:42744ms step_avg:123.54ms
step:357/1393 train_time:42870ms step_avg:123.55ms
step:358/1393 train_time:42995ms step_avg:123.55ms
step:359/1393 train_time:43121ms step_avg:123.56ms
step:360/1393 train_time:43247ms step_avg:123.56ms
step:361/1393 train_time:43373ms step_avg:123.57ms
step:362/1393 train_time:43499ms step_avg:123.58ms
step:363/1393 train_time:43626ms step_avg:123.59ms
step:364/1393 train_time:43752ms step_avg:123.59ms
step:365/1393 train_time:43878ms step_avg:123.60ms
step:366/1393 train_time:44005ms step_avg:123.61ms
step:367/1393 train_time:44131ms step_avg:123.62ms
step:368/1393 train_time:44257ms step_avg:123.62ms
step:369/1393 train_time:44382ms step_avg:123.63ms
step:370/1393 train_time:44508ms step_avg:123.63ms
step:371/1393 train_time:44634ms step_avg:123.64ms
step:372/1393 train_time:44760ms step_avg:123.65ms
step:373/1393 train_time:44886ms step_avg:123.65ms
step:374/1393 train_time:45012ms step_avg:123.66ms
step:375/1393 train_time:45138ms step_avg:123.67ms
step:375/1393 val_loss:3.7890 train_time:45262ms step_avg:124.01ms
step:376/1393 train_time:45287ms step_avg:123.73ms
step:377/1393 train_time:45400ms step_avg:123.71ms
step:378/1393 train_time:45528ms step_avg:123.72ms
step:379/1393 train_time:45654ms step_avg:123.72ms
step:380/1393 train_time:45779ms step_avg:123.73ms
step:381/1393 train_time:45904ms step_avg:123.73ms
step:382/1393 train_time:46029ms step_avg:123.73ms
step:383/1393 train_time:46155ms step_avg:123.74ms
step:384/1393 train_time:46281ms step_avg:123.75ms
step:385/1393 train_time:46408ms step_avg:123.75ms
step:386/1393 train_time:46536ms step_avg:123.76ms
step:387/1393 train_time:46662ms step_avg:123.77ms
step:388/1393 train_time:46789ms step_avg:123.78ms
step:389/1393 train_time:46915ms step_avg:123.79ms
step:390/1393 train_time:47041ms step_avg:123.79ms
step:391/1393 train_time:47166ms step_avg:123.80ms
step:392/1393 train_time:47291ms step_avg:123.80ms
step:393/1393 train_time:47417ms step_avg:123.81ms
step:394/1393 train_time:47544ms step_avg:123.81ms
step:395/1393 train_time:47670ms step_avg:123.82ms
step:396/1393 train_time:47797ms step_avg:123.83ms
step:397/1393 train_time:47924ms step_avg:123.84ms
step:398/1393 train_time:48049ms step_avg:123.84ms
step:399/1393 train_time:48175ms step_avg:123.84ms
step:400/1393 train_time:48301ms step_avg:123.85ms
step:401/1393 train_time:48428ms step_avg:123.86ms
step:402/1393 train_time:48553ms step_avg:123.86ms
step:403/1393 train_time:48679ms step_avg:123.86ms
step:404/1393 train_time:48806ms step_avg:123.87ms
step:405/1393 train_time:48931ms step_avg:123.88ms
step:406/1393 train_time:49058ms step_avg:123.88ms
step:407/1393 train_time:49184ms step_avg:123.89ms
step:408/1393 train_time:49309ms step_avg:123.89ms
step:409/1393 train_time:49435ms step_avg:123.90ms
step:410/1393 train_time:49561ms step_avg:123.90ms
step:411/1393 train_time:49687ms step_avg:123.91ms
step:412/1393 train_time:49813ms step_avg:123.91ms
step:413/1393 train_time:49939ms step_avg:123.92ms
step:414/1393 train_time:50068ms step_avg:123.93ms
step:415/1393 train_time:50195ms step_avg:123.94ms
step:416/1393 train_time:50320ms step_avg:123.94ms
step:417/1393 train_time:50447ms step_avg:123.95ms
step:418/1393 train_time:50573ms step_avg:123.95ms
step:419/1393 train_time:50699ms step_avg:123.96ms
step:420/1393 train_time:50826ms step_avg:123.97ms
step:421/1393 train_time:50953ms step_avg:123.97ms
step:422/1393 train_time:51080ms step_avg:123.98ms
step:423/1393 train_time:51207ms step_avg:123.99ms
step:424/1393 train_time:51334ms step_avg:124.00ms
step:425/1393 train_time:51460ms step_avg:124.00ms
step:426/1393 train_time:51587ms step_avg:124.01ms
step:427/1393 train_time:51713ms step_avg:124.01ms
step:428/1393 train_time:51839ms step_avg:124.02ms
step:429/1393 train_time:51965ms step_avg:124.02ms
step:430/1393 train_time:52092ms step_avg:124.03ms
step:431/1393 train_time:52218ms step_avg:124.03ms
step:432/1393 train_time:52345ms step_avg:124.04ms
step:433/1393 train_time:52472ms step_avg:124.05ms
step:434/1393 train_time:52600ms step_avg:124.06ms
step:435/1393 train_time:52726ms step_avg:124.06ms
step:436/1393 train_time:52852ms step_avg:124.07ms
step:437/1393 train_time:52978ms step_avg:124.07ms
step:438/1393 train_time:53106ms step_avg:124.08ms
step:439/1393 train_time:53232ms step_avg:124.08ms
step:440/1393 train_time:53358ms step_avg:124.09ms
step:441/1393 train_time:53486ms step_avg:124.10ms
step:442/1393 train_time:53613ms step_avg:124.10ms
step:443/1393 train_time:53740ms step_avg:124.11ms
step:444/1393 train_time:53867ms step_avg:124.12ms
step:445/1393 train_time:53992ms step_avg:124.12ms
step:446/1393 train_time:54119ms step_avg:124.13ms
step:447/1393 train_time:54245ms step_avg:124.13ms
step:448/1393 train_time:54371ms step_avg:124.13ms
step:449/1393 train_time:54497ms step_avg:124.14ms
step:450/1393 train_time:54624ms step_avg:124.15ms
step:451/1393 train_time:54751ms step_avg:124.15ms
step:452/1393 train_time:54878ms step_avg:124.16ms
step:453/1393 train_time:55005ms step_avg:124.17ms
step:454/1393 train_time:55131ms step_avg:124.17ms
step:455/1393 train_time:55258ms step_avg:124.17ms
step:456/1393 train_time:55384ms step_avg:124.18ms
step:457/1393 train_time:55511ms step_avg:124.19ms
step:458/1393 train_time:55638ms step_avg:124.19ms
step:459/1393 train_time:55767ms step_avg:124.20ms
step:460/1393 train_time:55894ms step_avg:124.21ms
step:461/1393 train_time:56021ms step_avg:124.22ms
step:462/1393 train_time:56149ms step_avg:124.22ms
step:463/1393 train_time:56275ms step_avg:124.23ms
step:464/1393 train_time:56401ms step_avg:124.23ms
step:465/1393 train_time:56528ms step_avg:124.24ms
step:466/1393 train_time:56654ms step_avg:124.24ms
step:467/1393 train_time:56782ms step_avg:124.25ms
step:468/1393 train_time:56909ms step_avg:124.26ms
step:469/1393 train_time:57035ms step_avg:124.26ms
step:470/1393 train_time:57162ms step_avg:124.27ms
step:471/1393 train_time:57289ms step_avg:124.27ms
step:472/1393 train_time:57416ms step_avg:124.28ms
step:473/1393 train_time:57542ms step_avg:124.28ms
step:474/1393 train_time:57668ms step_avg:124.29ms
step:475/1393 train_time:57795ms step_avg:124.29ms
step:476/1393 train_time:57921ms step_avg:124.29ms
step:477/1393 train_time:58048ms step_avg:124.30ms
step:478/1393 train_time:58175ms step_avg:124.31ms
step:479/1393 train_time:58301ms step_avg:124.31ms
step:480/1393 train_time:58427ms step_avg:124.31ms
step:481/1393 train_time:58554ms step_avg:124.32ms
step:482/1393 train_time:58680ms step_avg:124.32ms
step:483/1393 train_time:58806ms step_avg:124.33ms
step:484/1393 train_time:58933ms step_avg:124.33ms
step:485/1393 train_time:59060ms step_avg:124.34ms
step:486/1393 train_time:59186ms step_avg:124.34ms
step:487/1393 train_time:59312ms step_avg:124.34ms
step:488/1393 train_time:59439ms step_avg:124.35ms
step:489/1393 train_time:59565ms step_avg:124.35ms
step:490/1393 train_time:59692ms step_avg:124.36ms
step:491/1393 train_time:59818ms step_avg:124.36ms
step:492/1393 train_time:59944ms step_avg:124.36ms
step:493/1393 train_time:60070ms step_avg:124.37ms
step:494/1393 train_time:60197ms step_avg:124.37ms
step:495/1393 train_time:60324ms step_avg:124.38ms
step:496/1393 train_time:60450ms step_avg:124.38ms
step:497/1393 train_time:60577ms step_avg:124.39ms
step:498/1393 train_time:60706ms step_avg:124.40ms
step:499/1393 train_time:60831ms step_avg:124.40ms
step:500/1393 train_time:60957ms step_avg:124.40ms
step:500/1393 val_loss:3.6719 train_time:61082ms step_avg:124.66ms
step:501/1393 train_time:61108ms step_avg:124.46ms
step:502/1393 train_time:61219ms step_avg:124.43ms
step:503/1393 train_time:61347ms step_avg:124.44ms
step:504/1393 train_time:61473ms step_avg:124.44ms
step:505/1393 train_time:61600ms step_avg:124.44ms
step:506/1393 train_time:61725ms step_avg:124.45ms
step:507/1393 train_time:61851ms step_avg:124.45ms
step:508/1393 train_time:61977ms step_avg:124.45ms
step:509/1393 train_time:62104ms step_avg:124.46ms
step:510/1393 train_time:62231ms step_avg:124.46ms
step:511/1393 train_time:62358ms step_avg:124.47ms
step:512/1393 train_time:62485ms step_avg:124.47ms
step:513/1393 train_time:62612ms step_avg:124.48ms
step:514/1393 train_time:62739ms step_avg:124.48ms
step:515/1393 train_time:62865ms step_avg:124.48ms
step:516/1393 train_time:62990ms step_avg:124.49ms
step:517/1393 train_time:63116ms step_avg:124.49ms
step:518/1393 train_time:63245ms step_avg:124.50ms
step:519/1393 train_time:63374ms step_avg:124.51ms
step:520/1393 train_time:63502ms step_avg:124.51ms
step:521/1393 train_time:63632ms step_avg:124.52ms
step:522/1393 train_time:63762ms step_avg:124.53ms
step:523/1393 train_time:63891ms step_avg:124.54ms
step:524/1393 train_time:64019ms step_avg:124.55ms
step:525/1393 train_time:64148ms step_avg:124.56ms
step:526/1393 train_time:64276ms step_avg:124.57ms
step:527/1393 train_time:64405ms step_avg:124.57ms
step:528/1393 train_time:64533ms step_avg:124.58ms
step:529/1393 train_time:64662ms step_avg:124.59ms
step:530/1393 train_time:64791ms step_avg:124.60ms
step:531/1393 train_time:64919ms step_avg:124.61ms
step:532/1393 train_time:65048ms step_avg:124.61ms
step:533/1393 train_time:65177ms step_avg:124.62ms
step:534/1393 train_time:65306ms step_avg:124.63ms
step:535/1393 train_time:65434ms step_avg:124.64ms
step:536/1393 train_time:65563ms step_avg:124.64ms
step:537/1393 train_time:65691ms step_avg:124.65ms
step:538/1393 train_time:65821ms step_avg:124.66ms
step:539/1393 train_time:65949ms step_avg:124.67ms
step:540/1393 train_time:66078ms step_avg:124.68ms
step:541/1393 train_time:66207ms step_avg:124.68ms
step:542/1393 train_time:66335ms step_avg:124.69ms
step:543/1393 train_time:66463ms step_avg:124.70ms
step:544/1393 train_time:66592ms step_avg:124.70ms
step:545/1393 train_time:66721ms step_avg:124.71ms
step:546/1393 train_time:66849ms step_avg:124.72ms
step:547/1393 train_time:66978ms step_avg:124.73ms
step:548/1393 train_time:67107ms step_avg:124.73ms
step:549/1393 train_time:67235ms step_avg:124.74ms
step:550/1393 train_time:67363ms step_avg:124.75ms
step:551/1393 train_time:67492ms step_avg:124.75ms
step:552/1393 train_time:67621ms step_avg:124.76ms
step:553/1393 train_time:67749ms step_avg:124.77ms
step:554/1393 train_time:67879ms step_avg:124.78ms
step:555/1393 train_time:68007ms step_avg:124.78ms
step:556/1393 train_time:68136ms step_avg:124.79ms
step:557/1393 train_time:68264ms step_avg:124.80ms
step:558/1393 train_time:68392ms step_avg:124.80ms
step:559/1393 train_time:68521ms step_avg:124.81ms
step:560/1393 train_time:68649ms step_avg:124.82ms
step:561/1393 train_time:68779ms step_avg:124.83ms
step:562/1393 train_time:68908ms step_avg:124.83ms
step:563/1393 train_time:69037ms step_avg:124.84ms
step:564/1393 train_time:69164ms step_avg:124.85ms
step:565/1393 train_time:69293ms step_avg:124.85ms
step:566/1393 train_time:69421ms step_avg:124.86ms
step:567/1393 train_time:69550ms step_avg:124.86ms
step:568/1393 train_time:69679ms step_avg:124.87ms
step:569/1393 train_time:69807ms step_avg:124.88ms
step:570/1393 train_time:69935ms step_avg:124.88ms
step:571/1393 train_time:70062ms step_avg:124.89ms
step:572/1393 train_time:70190ms step_avg:124.89ms
step:573/1393 train_time:70318ms step_avg:124.90ms
step:574/1393 train_time:70448ms step_avg:124.91ms
step:575/1393 train_time:70577ms step_avg:124.91ms
step:576/1393 train_time:70705ms step_avg:124.92ms
step:577/1393 train_time:70836ms step_avg:124.93ms
step:578/1393 train_time:70965ms step_avg:124.94ms
step:579/1393 train_time:71093ms step_avg:124.94ms
step:580/1393 train_time:71221ms step_avg:124.95ms
step:581/1393 train_time:71350ms step_avg:124.96ms
step:582/1393 train_time:71478ms step_avg:124.96ms
step:583/1393 train_time:71608ms step_avg:124.97ms
step:584/1393 train_time:71737ms step_avg:124.98ms
step:585/1393 train_time:71865ms step_avg:124.98ms
step:586/1393 train_time:71995ms step_avg:124.99ms
step:587/1393 train_time:72123ms step_avg:125.00ms
step:588/1393 train_time:72251ms step_avg:125.00ms
step:589/1393 train_time:72380ms step_avg:125.01ms
step:590/1393 train_time:72508ms step_avg:125.01ms
step:591/1393 train_time:72637ms step_avg:125.02ms
step:592/1393 train_time:72766ms step_avg:125.03ms
step:593/1393 train_time:72895ms step_avg:125.03ms
step:594/1393 train_time:73024ms step_avg:125.04ms
step:595/1393 train_time:73152ms step_avg:125.05ms
step:596/1393 train_time:73282ms step_avg:125.05ms
step:597/1393 train_time:73409ms step_avg:125.06ms
step:598/1393 train_time:73539ms step_avg:125.07ms
step:599/1393 train_time:73668ms step_avg:125.07ms
step:600/1393 train_time:73796ms step_avg:125.08ms
step:601/1393 train_time:73924ms step_avg:125.08ms
step:602/1393 train_time:74052ms step_avg:125.09ms
step:603/1393 train_time:74181ms step_avg:125.09ms
step:604/1393 train_time:74309ms step_avg:125.10ms
step:605/1393 train_time:74439ms step_avg:125.11ms
step:606/1393 train_time:74567ms step_avg:125.11ms
step:607/1393 train_time:74696ms step_avg:125.12ms
step:608/1393 train_time:74823ms step_avg:125.12ms
step:609/1393 train_time:74951ms step_avg:125.13ms
step:610/1393 train_time:75080ms step_avg:125.13ms
step:611/1393 train_time:75209ms step_avg:125.14ms
step:612/1393 train_time:75338ms step_avg:125.15ms
step:613/1393 train_time:75467ms step_avg:125.15ms
step:614/1393 train_time:75595ms step_avg:125.16ms
step:615/1393 train_time:75724ms step_avg:125.16ms
step:616/1393 train_time:75853ms step_avg:125.17ms
step:617/1393 train_time:75980ms step_avg:125.17ms
step:618/1393 train_time:76110ms step_avg:125.18ms
step:619/1393 train_time:76239ms step_avg:125.19ms
step:620/1393 train_time:76367ms step_avg:125.19ms
step:621/1393 train_time:76496ms step_avg:125.20ms
step:622/1393 train_time:76625ms step_avg:125.20ms
step:623/1393 train_time:76754ms step_avg:125.21ms
step:624/1393 train_time:76883ms step_avg:125.22ms
step:625/1393 train_time:77012ms step_avg:125.22ms
step:625/1393 val_loss:3.5877 train_time:77139ms step_avg:125.43ms
step:626/1393 train_time:77162ms step_avg:125.26ms
step:627/1393 train_time:77282ms step_avg:125.25ms
step:628/1393 train_time:77411ms step_avg:125.26ms
step:629/1393 train_time:77541ms step_avg:125.27ms
step:630/1393 train_time:77669ms step_avg:125.27ms
step:631/1393 train_time:77797ms step_avg:125.28ms
step:632/1393 train_time:77927ms step_avg:125.28ms
step:633/1393 train_time:78055ms step_avg:125.29ms
step:634/1393 train_time:78184ms step_avg:125.29ms
step:635/1393 train_time:78315ms step_avg:125.30ms
step:636/1393 train_time:78444ms step_avg:125.31ms
step:637/1393 train_time:78573ms step_avg:125.32ms
step:638/1393 train_time:78702ms step_avg:125.32ms
step:639/1393 train_time:78830ms step_avg:125.33ms
step:640/1393 train_time:78959ms step_avg:125.33ms
step:641/1393 train_time:79089ms step_avg:125.34ms
step:642/1393 train_time:79218ms step_avg:125.34ms
step:643/1393 train_time:79348ms step_avg:125.35ms
step:644/1393 train_time:79477ms step_avg:125.36ms
step:645/1393 train_time:79606ms step_avg:125.36ms
step:646/1393 train_time:79735ms step_avg:125.37ms
step:647/1393 train_time:79863ms step_avg:125.37ms
step:648/1393 train_time:79992ms step_avg:125.38ms
step:649/1393 train_time:80121ms step_avg:125.39ms
step:650/1393 train_time:80250ms step_avg:125.39ms
step:651/1393 train_time:80381ms step_avg:125.40ms
step:652/1393 train_time:80509ms step_avg:125.40ms
step:653/1393 train_time:80637ms step_avg:125.41ms
step:654/1393 train_time:80766ms step_avg:125.41ms
step:655/1393 train_time:80895ms step_avg:125.42ms
step:656/1393 train_time:81024ms step_avg:125.42ms
step:657/1393 train_time:81153ms step_avg:125.43ms
step:658/1393 train_time:81282ms step_avg:125.44ms
step:659/1393 train_time:81411ms step_avg:125.44ms
step:660/1393 train_time:81540ms step_avg:125.45ms
step:661/1393 train_time:81668ms step_avg:125.45ms
step:662/1393 train_time:81797ms step_avg:125.46ms
step:663/1393 train_time:81926ms step_avg:125.46ms
step:664/1393 train_time:82055ms step_avg:125.47ms
step:665/1393 train_time:82184ms step_avg:125.47ms
step:666/1393 train_time:82314ms step_avg:125.48ms
step:667/1393 train_time:82443ms step_avg:125.48ms
step:668/1393 train_time:82572ms step_avg:125.49ms
step:669/1393 train_time:82701ms step_avg:125.50ms
step:670/1393 train_time:82830ms step_avg:125.50ms
step:671/1393 train_time:82959ms step_avg:125.50ms
step:672/1393 train_time:83089ms step_avg:125.51ms
step:673/1393 train_time:83217ms step_avg:125.52ms
step:674/1393 train_time:83346ms step_avg:125.52ms
step:675/1393 train_time:83476ms step_avg:125.53ms
step:676/1393 train_time:83605ms step_avg:125.53ms
step:677/1393 train_time:83734ms step_avg:125.54ms
step:678/1393 train_time:83862ms step_avg:125.54ms
step:679/1393 train_time:83991ms step_avg:125.55ms
step:680/1393 train_time:84121ms step_avg:125.55ms
step:681/1393 train_time:84251ms step_avg:125.56ms
step:682/1393 train_time:84379ms step_avg:125.56ms
step:683/1393 train_time:84507ms step_avg:125.57ms
step:684/1393 train_time:84636ms step_avg:125.57ms
step:685/1393 train_time:84766ms step_avg:125.58ms
step:686/1393 train_time:84895ms step_avg:125.58ms
step:687/1393 train_time:85024ms step_avg:125.59ms
step:688/1393 train_time:85153ms step_avg:125.59ms
step:689/1393 train_time:85283ms step_avg:125.60ms
step:690/1393 train_time:85412ms step_avg:125.61ms
step:691/1393 train_time:85541ms step_avg:125.61ms
step:692/1393 train_time:85669ms step_avg:125.61ms
step:693/1393 train_time:85797ms step_avg:125.62ms
step:694/1393 train_time:85926ms step_avg:125.62ms
step:695/1393 train_time:86055ms step_avg:125.63ms
step:696/1393 train_time:86184ms step_avg:125.63ms
step:697/1393 train_time:86312ms step_avg:125.64ms
step:698/1393 train_time:86441ms step_avg:125.64ms
step:699/1393 train_time:86570ms step_avg:125.65ms
step:700/1393 train_time:86700ms step_avg:125.65ms
step:701/1393 train_time:86829ms step_avg:125.66ms
step:702/1393 train_time:86957ms step_avg:125.66ms
step:703/1393 train_time:87087ms step_avg:125.67ms
step:704/1393 train_time:87216ms step_avg:125.67ms
step:705/1393 train_time:87344ms step_avg:125.67ms
step:706/1393 train_time:87473ms step_avg:125.68ms
step:707/1393 train_time:87602ms step_avg:125.68ms
step:708/1393 train_time:87731ms step_avg:125.69ms
step:709/1393 train_time:87859ms step_avg:125.69ms
step:710/1393 train_time:87990ms step_avg:125.70ms
step:711/1393 train_time:88119ms step_avg:125.70ms
step:712/1393 train_time:88247ms step_avg:125.71ms
step:713/1393 train_time:88376ms step_avg:125.71ms
step:714/1393 train_time:88505ms step_avg:125.72ms
step:715/1393 train_time:88634ms step_avg:125.72ms
step:716/1393 train_time:88763ms step_avg:125.73ms
step:717/1393 train_time:88893ms step_avg:125.73ms
step:718/1393 train_time:89022ms step_avg:125.74ms
step:719/1393 train_time:89151ms step_avg:125.74ms
step:720/1393 train_time:89280ms step_avg:125.75ms
step:721/1393 train_time:89409ms step_avg:125.75ms
step:722/1393 train_time:89537ms step_avg:125.75ms
step:723/1393 train_time:89666ms step_avg:125.76ms
step:724/1393 train_time:89796ms step_avg:125.76ms
step:725/1393 train_time:89926ms step_avg:125.77ms
step:726/1393 train_time:90057ms step_avg:125.78ms
step:727/1393 train_time:90188ms step_avg:125.78ms
step:728/1393 train_time:90318ms step_avg:125.79ms
step:729/1393 train_time:90450ms step_avg:125.80ms
step:730/1393 train_time:90580ms step_avg:125.81ms
step:731/1393 train_time:90711ms step_avg:125.81ms
step:732/1393 train_time:90842ms step_avg:125.82ms
step:733/1393 train_time:90973ms step_avg:125.83ms
step:734/1393 train_time:91104ms step_avg:125.83ms
step:735/1393 train_time:91235ms step_avg:125.84ms
step:736/1393 train_time:91366ms step_avg:125.85ms
step:737/1393 train_time:91497ms step_avg:125.86ms
step:738/1393 train_time:91628ms step_avg:125.86ms
step:739/1393 train_time:91758ms step_avg:125.87ms
step:740/1393 train_time:91889ms step_avg:125.88ms
step:741/1393 train_time:92022ms step_avg:125.89ms
step:742/1393 train_time:92152ms step_avg:125.89ms
step:743/1393 train_time:92283ms step_avg:125.90ms
step:744/1393 train_time:92414ms step_avg:125.90ms
step:745/1393 train_time:92546ms step_avg:125.91ms
step:746/1393 train_time:92678ms step_avg:125.92ms
step:747/1393 train_time:92808ms step_avg:125.93ms
step:748/1393 train_time:92938ms step_avg:125.93ms
step:749/1393 train_time:93070ms step_avg:125.94ms
step:750/1393 train_time:93200ms step_avg:125.95ms
step:750/1393 val_loss:3.5328 train_time:93329ms step_avg:126.12ms
step:751/1393 train_time:93352ms step_avg:125.98ms
step:752/1393 train_time:93475ms step_avg:125.98ms
step:753/1393 train_time:93604ms step_avg:125.98ms
step:754/1393 train_time:93734ms step_avg:125.99ms
step:755/1393 train_time:93865ms step_avg:125.99ms
step:756/1393 train_time:93995ms step_avg:126.00ms
step:757/1393 train_time:94127ms step_avg:126.01ms
step:758/1393 train_time:94258ms step_avg:126.01ms
step:759/1393 train_time:94388ms step_avg:126.02ms
step:760/1393 train_time:94521ms step_avg:126.03ms
step:761/1393 train_time:94652ms step_avg:126.03ms
step:762/1393 train_time:94781ms step_avg:126.04ms
step:763/1393 train_time:94912ms step_avg:126.04ms
step:764/1393 train_time:95042ms step_avg:126.05ms
step:765/1393 train_time:95174ms step_avg:126.06ms
step:766/1393 train_time:95305ms step_avg:126.06ms
step:767/1393 train_time:95438ms step_avg:126.07ms
step:768/1393 train_time:95569ms step_avg:126.08ms
step:769/1393 train_time:95701ms step_avg:126.09ms
step:770/1393 train_time:95834ms step_avg:126.10ms
step:771/1393 train_time:95964ms step_avg:126.10ms
step:772/1393 train_time:96095ms step_avg:126.11ms
step:773/1393 train_time:96225ms step_avg:126.11ms
step:774/1393 train_time:96357ms step_avg:126.12ms
step:775/1393 train_time:96488ms step_avg:126.13ms
step:776/1393 train_time:96618ms step_avg:126.13ms
step:777/1393 train_time:96749ms step_avg:126.14ms
step:778/1393 train_time:96881ms step_avg:126.15ms
step:779/1393 train_time:97011ms step_avg:126.15ms
step:780/1393 train_time:97142ms step_avg:126.16ms
step:781/1393 train_time:97273ms step_avg:126.17ms
step:782/1393 train_time:97403ms step_avg:126.17ms
step:783/1393 train_time:97535ms step_avg:126.18ms
step:784/1393 train_time:97666ms step_avg:126.18ms
step:785/1393 train_time:97796ms step_avg:126.19ms
step:786/1393 train_time:97927ms step_avg:126.19ms
step:787/1393 train_time:98058ms step_avg:126.20ms
step:788/1393 train_time:98189ms step_avg:126.21ms
step:789/1393 train_time:98319ms step_avg:126.21ms
step:790/1393 train_time:98449ms step_avg:126.22ms
step:791/1393 train_time:98580ms step_avg:126.22ms
step:792/1393 train_time:98710ms step_avg:126.23ms
step:793/1393 train_time:98840ms step_avg:126.23ms
step:794/1393 train_time:98971ms step_avg:126.24ms
step:795/1393 train_time:99103ms step_avg:126.25ms
step:796/1393 train_time:99235ms step_avg:126.25ms
step:797/1393 train_time:99366ms step_avg:126.26ms
step:798/1393 train_time:99497ms step_avg:126.27ms
step:799/1393 train_time:99630ms step_avg:126.27ms
step:800/1393 train_time:99761ms step_avg:126.28ms
step:801/1393 train_time:99891ms step_avg:126.28ms
step:802/1393 train_time:100021ms step_avg:126.29ms
step:803/1393 train_time:100152ms step_avg:126.30ms
step:804/1393 train_time:100282ms step_avg:126.30ms
step:805/1393 train_time:100413ms step_avg:126.31ms
step:806/1393 train_time:100544ms step_avg:126.31ms
step:807/1393 train_time:100675ms step_avg:126.32ms
step:808/1393 train_time:100805ms step_avg:126.32ms
step:809/1393 train_time:100935ms step_avg:126.33ms
step:810/1393 train_time:101066ms step_avg:126.33ms
step:811/1393 train_time:101198ms step_avg:126.34ms
step:812/1393 train_time:101329ms step_avg:126.34ms
step:813/1393 train_time:101459ms step_avg:126.35ms
step:814/1393 train_time:101589ms step_avg:126.35ms
step:815/1393 train_time:101721ms step_avg:126.36ms
step:816/1393 train_time:101850ms step_avg:126.36ms
step:817/1393 train_time:101980ms step_avg:126.37ms
step:818/1393 train_time:102111ms step_avg:126.38ms
step:819/1393 train_time:102242ms step_avg:126.38ms
step:820/1393 train_time:102373ms step_avg:126.39ms
step:821/1393 train_time:102504ms step_avg:126.39ms
step:822/1393 train_time:102634ms step_avg:126.40ms
step:823/1393 train_time:102764ms step_avg:126.40ms
step:824/1393 train_time:102895ms step_avg:126.41ms
step:825/1393 train_time:103026ms step_avg:126.41ms
step:826/1393 train_time:103157ms step_avg:126.42ms
step:827/1393 train_time:103288ms step_avg:126.42ms
step:828/1393 train_time:103419ms step_avg:126.43ms
step:829/1393 train_time:103550ms step_avg:126.43ms
step:830/1393 train_time:103681ms step_avg:126.44ms
step:831/1393 train_time:103812ms step_avg:126.45ms
step:832/1393 train_time:103943ms step_avg:126.45ms
step:833/1393 train_time:104073ms step_avg:126.46ms
step:834/1393 train_time:104204ms step_avg:126.46ms
step:835/1393 train_time:104334ms step_avg:126.47ms
step:836/1393 train_time:104466ms step_avg:126.47ms
step:837/1393 train_time:104597ms step_avg:126.48ms
step:838/1393 train_time:104727ms step_avg:126.48ms
step:839/1393 train_time:104858ms step_avg:126.49ms
step:840/1393 train_time:104989ms step_avg:126.49ms
step:841/1393 train_time:105120ms step_avg:126.50ms
step:842/1393 train_time:105251ms step_avg:126.50ms
step:843/1393 train_time:105382ms step_avg:126.51ms
step:844/1393 train_time:105514ms step_avg:126.52ms
step:845/1393 train_time:105645ms step_avg:126.52ms
step:846/1393 train_time:105775ms step_avg:126.52ms
step:847/1393 train_time:105906ms step_avg:126.53ms
step:848/1393 train_time:106037ms step_avg:126.54ms
step:849/1393 train_time:106168ms step_avg:126.54ms
step:850/1393 train_time:106300ms step_avg:126.55ms
step:851/1393 train_time:106432ms step_avg:126.55ms
step:852/1393 train_time:106562ms step_avg:126.56ms
step:853/1393 train_time:106692ms step_avg:126.56ms
step:854/1393 train_time:106823ms step_avg:126.57ms
step:855/1393 train_time:106954ms step_avg:126.57ms
step:856/1393 train_time:107084ms step_avg:126.58ms
step:857/1393 train_time:107215ms step_avg:126.58ms
step:858/1393 train_time:107346ms step_avg:126.59ms
step:859/1393 train_time:107477ms step_avg:126.59ms
step:860/1393 train_time:107609ms step_avg:126.60ms
step:861/1393 train_time:107740ms step_avg:126.60ms
step:862/1393 train_time:107871ms step_avg:126.61ms
step:863/1393 train_time:108002ms step_avg:126.61ms
step:864/1393 train_time:108134ms step_avg:126.62ms
step:865/1393 train_time:108264ms step_avg:126.62ms
step:866/1393 train_time:108397ms step_avg:126.63ms
step:867/1393 train_time:108528ms step_avg:126.64ms
step:868/1393 train_time:108658ms step_avg:126.64ms
step:869/1393 train_time:108788ms step_avg:126.65ms
step:870/1393 train_time:108919ms step_avg:126.65ms
step:871/1393 train_time:109050ms step_avg:126.65ms
step:872/1393 train_time:109180ms step_avg:126.66ms
step:873/1393 train_time:109311ms step_avg:126.66ms
step:874/1393 train_time:109442ms step_avg:126.67ms
step:875/1393 train_time:109574ms step_avg:126.68ms
step:875/1393 val_loss:3.4834 train_time:109703ms step_avg:126.82ms
step:876/1393 train_time:109725ms step_avg:126.70ms
step:877/1393 train_time:109849ms step_avg:126.70ms
step:878/1393 train_time:109981ms step_avg:126.71ms
step:879/1393 train_time:110112ms step_avg:126.71ms
step:880/1393 train_time:110243ms step_avg:126.72ms
step:881/1393 train_time:110373ms step_avg:126.72ms
step:882/1393 train_time:110504ms step_avg:126.73ms
step:883/1393 train_time:110635ms step_avg:126.73ms
step:884/1393 train_time:110766ms step_avg:126.73ms
step:885/1393 train_time:110900ms step_avg:126.74ms
step:886/1393 train_time:111031ms step_avg:126.75ms
step:887/1393 train_time:111161ms step_avg:126.75ms
step:888/1393 train_time:111292ms step_avg:126.76ms
step:889/1393 train_time:111425ms step_avg:126.76ms
step:890/1393 train_time:111555ms step_avg:126.77ms
step:891/1393 train_time:111686ms step_avg:126.77ms
step:892/1393 train_time:111816ms step_avg:126.78ms
step:893/1393 train_time:111948ms step_avg:126.78ms
step:894/1393 train_time:112079ms step_avg:126.79ms
step:895/1393 train_time:112211ms step_avg:126.79ms
step:896/1393 train_time:112341ms step_avg:126.80ms
step:897/1393 train_time:112471ms step_avg:126.80ms
step:898/1393 train_time:112603ms step_avg:126.80ms
step:899/1393 train_time:112734ms step_avg:126.81ms
step:900/1393 train_time:112864ms step_avg:126.81ms
step:901/1393 train_time:112995ms step_avg:126.82ms
step:902/1393 train_time:113125ms step_avg:126.82ms
step:903/1393 train_time:113256ms step_avg:126.83ms
step:904/1393 train_time:113388ms step_avg:126.83ms
step:905/1393 train_time:113518ms step_avg:126.84ms
step:906/1393 train_time:113649ms step_avg:126.84ms
step:907/1393 train_time:113782ms step_avg:126.85ms
step:908/1393 train_time:113912ms step_avg:126.85ms
step:909/1393 train_time:114042ms step_avg:126.85ms
step:910/1393 train_time:114174ms step_avg:126.86ms
step:911/1393 train_time:114305ms step_avg:126.86ms
step:912/1393 train_time:114436ms step_avg:126.87ms
step:913/1393 train_time:114566ms step_avg:126.87ms
step:914/1393 train_time:114698ms step_avg:126.88ms
step:915/1393 train_time:114828ms step_avg:126.88ms
step:916/1393 train_time:114959ms step_avg:126.89ms
step:917/1393 train_time:115092ms step_avg:126.89ms
step:918/1393 train_time:115222ms step_avg:126.90ms
step:919/1393 train_time:115356ms step_avg:126.90ms
step:920/1393 train_time:115488ms step_avg:126.91ms
step:921/1393 train_time:115618ms step_avg:126.91ms
step:922/1393 train_time:115751ms step_avg:126.92ms
step:923/1393 train_time:115881ms step_avg:126.92ms
step:924/1393 train_time:116011ms step_avg:126.93ms
step:925/1393 train_time:116142ms step_avg:126.93ms
step:926/1393 train_time:116274ms step_avg:126.94ms
step:927/1393 train_time:116405ms step_avg:126.94ms
step:928/1393 train_time:116536ms step_avg:126.95ms
step:929/1393 train_time:116667ms step_avg:126.95ms
step:930/1393 train_time:116798ms step_avg:126.95ms
step:931/1393 train_time:116930ms step_avg:126.96ms
step:932/1393 train_time:117063ms step_avg:126.97ms
step:933/1393 train_time:117195ms step_avg:126.97ms
step:934/1393 train_time:117328ms step_avg:126.98ms
step:935/1393 train_time:117461ms step_avg:126.98ms
step:936/1393 train_time:117594ms step_avg:126.99ms
step:937/1393 train_time:117728ms step_avg:127.00ms
step:938/1393 train_time:117860ms step_avg:127.00ms
step:939/1393 train_time:117993ms step_avg:127.01ms
step:940/1393 train_time:118127ms step_avg:127.02ms
step:941/1393 train_time:118259ms step_avg:127.02ms
step:942/1393 train_time:118390ms step_avg:127.03ms
step:943/1393 train_time:118523ms step_avg:127.03ms
step:944/1393 train_time:118658ms step_avg:127.04ms
step:945/1393 train_time:118791ms step_avg:127.05ms
step:946/1393 train_time:118923ms step_avg:127.05ms
step:947/1393 train_time:119057ms step_avg:127.06ms
step:948/1393 train_time:119190ms step_avg:127.07ms
step:949/1393 train_time:119322ms step_avg:127.07ms
step:950/1393 train_time:119454ms step_avg:127.08ms
step:951/1393 train_time:119589ms step_avg:127.09ms
step:952/1393 train_time:119721ms step_avg:127.09ms
step:953/1393 train_time:119853ms step_avg:127.10ms
step:954/1393 train_time:119986ms step_avg:127.10ms
step:955/1393 train_time:120119ms step_avg:127.11ms
step:956/1393 train_time:120253ms step_avg:127.12ms
step:957/1393 train_time:120385ms step_avg:127.12ms
step:958/1393 train_time:120518ms step_avg:127.13ms
step:959/1393 train_time:120652ms step_avg:127.14ms
step:960/1393 train_time:120785ms step_avg:127.14ms
step:961/1393 train_time:120917ms step_avg:127.15ms
step:962/1393 train_time:121050ms step_avg:127.15ms
step:963/1393 train_time:121184ms step_avg:127.16ms
step:964/1393 train_time:121316ms step_avg:127.17ms
step:965/1393 train_time:121449ms step_avg:127.17ms
step:966/1393 train_time:121581ms step_avg:127.18ms
step:967/1393 train_time:121714ms step_avg:127.18ms
step:968/1393 train_time:121845ms step_avg:127.19ms
step:969/1393 train_time:121978ms step_avg:127.19ms
step:970/1393 train_time:122111ms step_avg:127.20ms
step:971/1393 train_time:122243ms step_avg:127.20ms
step:972/1393 train_time:122375ms step_avg:127.21ms
step:973/1393 train_time:122508ms step_avg:127.21ms
step:974/1393 train_time:122639ms step_avg:127.22ms
step:975/1393 train_time:122772ms step_avg:127.22ms
step:976/1393 train_time:122904ms step_avg:127.23ms
step:977/1393 train_time:123036ms step_avg:127.24ms
step:978/1393 train_time:123168ms step_avg:127.24ms
step:979/1393 train_time:123301ms step_avg:127.25ms
step:980/1393 train_time:123434ms step_avg:127.25ms
step:981/1393 train_time:123566ms step_avg:127.26ms
step:982/1393 train_time:123698ms step_avg:127.26ms
step:983/1393 train_time:123832ms step_avg:127.27ms
step:984/1393 train_time:123963ms step_avg:127.27ms
step:985/1393 train_time:124095ms step_avg:127.28ms
step:986/1393 train_time:124231ms step_avg:127.29ms
step:987/1393 train_time:124364ms step_avg:127.29ms
step:988/1393 train_time:124497ms step_avg:127.30ms
step:989/1393 train_time:124630ms step_avg:127.30ms
step:990/1393 train_time:124763ms step_avg:127.31ms
step:991/1393 train_time:124895ms step_avg:127.31ms
step:992/1393 train_time:125029ms step_avg:127.32ms
step:993/1393 train_time:125166ms step_avg:127.33ms
step:994/1393 train_time:125298ms step_avg:127.34ms
step:995/1393 train_time:125431ms step_avg:127.34ms
step:996/1393 train_time:125563ms step_avg:127.35ms
step:997/1393 train_time:125695ms step_avg:127.35ms
step:998/1393 train_time:125828ms step_avg:127.36ms
step:999/1393 train_time:125959ms step_avg:127.36ms
step:1000/1393 train_time:126093ms step_avg:127.37ms
step:1000/1393 val_loss:3.4190 train_time:126224ms step_avg:127.50ms
step:1001/1393 train_time:126247ms step_avg:127.39ms
step:1002/1393 train_time:126365ms step_avg:127.38ms
step:1003/1393 train_time:126498ms step_avg:127.39ms
step:1004/1393 train_time:126630ms step_avg:127.39ms
step:1005/1393 train_time:126763ms step_avg:127.40ms
step:1006/1393 train_time:126894ms step_avg:127.40ms
step:1007/1393 train_time:127026ms step_avg:127.41ms
step:1008/1393 train_time:127158ms step_avg:127.41ms
step:1009/1393 train_time:127294ms step_avg:127.42ms
step:1010/1393 train_time:127427ms step_avg:127.43ms
step:1011/1393 train_time:127563ms step_avg:127.44ms
step:1012/1393 train_time:127695ms step_avg:127.44ms
step:1013/1393 train_time:127828ms step_avg:127.45ms
step:1014/1393 train_time:127960ms step_avg:127.45ms
step:1015/1393 train_time:128092ms step_avg:127.45ms
step:1016/1393 train_time:128225ms step_avg:127.46ms
step:1017/1393 train_time:128358ms step_avg:127.47ms
step:1018/1393 train_time:128492ms step_avg:127.47ms
step:1019/1393 train_time:128626ms step_avg:127.48ms
step:1020/1393 train_time:128759ms step_avg:127.48ms
step:1021/1393 train_time:128892ms step_avg:127.49ms
step:1022/1393 train_time:129023ms step_avg:127.49ms
step:1023/1393 train_time:129156ms step_avg:127.50ms
step:1024/1393 train_time:129288ms step_avg:127.50ms
step:1025/1393 train_time:129421ms step_avg:127.51ms
step:1026/1393 train_time:129553ms step_avg:127.51ms
step:1027/1393 train_time:129686ms step_avg:127.52ms
step:1028/1393 train_time:129820ms step_avg:127.52ms
step:1029/1393 train_time:129954ms step_avg:127.53ms
step:1030/1393 train_time:130086ms step_avg:127.54ms
step:1031/1393 train_time:130219ms step_avg:127.54ms
step:1032/1393 train_time:130351ms step_avg:127.54ms
step:1033/1393 train_time:130483ms step_avg:127.55ms
step:1034/1393 train_time:130616ms step_avg:127.55ms
step:1035/1393 train_time:130749ms step_avg:127.56ms
step:1036/1393 train_time:130882ms step_avg:127.57ms
step:1037/1393 train_time:131016ms step_avg:127.57ms
step:1038/1393 train_time:131149ms step_avg:127.58ms
step:1039/1393 train_time:131282ms step_avg:127.58ms
step:1040/1393 train_time:131415ms step_avg:127.59ms
step:1041/1393 train_time:131548ms step_avg:127.59ms
step:1042/1393 train_time:131681ms step_avg:127.60ms
step:1043/1393 train_time:131814ms step_avg:127.60ms
step:1044/1393 train_time:131949ms step_avg:127.61ms
step:1045/1393 train_time:132084ms step_avg:127.62ms
step:1046/1393 train_time:132216ms step_avg:127.62ms
step:1047/1393 train_time:132348ms step_avg:127.63ms
step:1048/1393 train_time:132480ms step_avg:127.63ms
step:1049/1393 train_time:132612ms step_avg:127.63ms
step:1050/1393 train_time:132746ms step_avg:127.64ms
step:1051/1393 train_time:132881ms step_avg:127.65ms
step:1052/1393 train_time:133013ms step_avg:127.65ms
step:1053/1393 train_time:133146ms step_avg:127.66ms
step:1054/1393 train_time:133279ms step_avg:127.66ms
step:1055/1393 train_time:133412ms step_avg:127.67ms
step:1056/1393 train_time:133544ms step_avg:127.67ms
step:1057/1393 train_time:133677ms step_avg:127.68ms
step:1058/1393 train_time:133811ms step_avg:127.68ms
step:1059/1393 train_time:133944ms step_avg:127.69ms
step:1060/1393 train_time:134078ms step_avg:127.69ms
step:1061/1393 train_time:134211ms step_avg:127.70ms
step:1062/1393 train_time:134344ms step_avg:127.70ms
step:1063/1393 train_time:134476ms step_avg:127.71ms
step:1064/1393 train_time:134609ms step_avg:127.71ms
step:1065/1393 train_time:134741ms step_avg:127.72ms
step:1066/1393 train_time:134875ms step_avg:127.72ms
step:1067/1393 train_time:135007ms step_avg:127.73ms
step:1068/1393 train_time:135139ms step_avg:127.73ms
step:1069/1393 train_time:135274ms step_avg:127.74ms
step:1070/1393 train_time:135406ms step_avg:127.74ms
step:1071/1393 train_time:135541ms step_avg:127.75ms
step:1072/1393 train_time:135673ms step_avg:127.75ms
step:1073/1393 train_time:135806ms step_avg:127.76ms
step:1074/1393 train_time:135939ms step_avg:127.76ms
step:1075/1393 train_time:136072ms step_avg:127.77ms
step:1076/1393 train_time:136203ms step_avg:127.77ms
step:1077/1393 train_time:136336ms step_avg:127.77ms
step:1078/1393 train_time:136468ms step_avg:127.78ms
step:1079/1393 train_time:136603ms step_avg:127.79ms
step:1080/1393 train_time:136736ms step_avg:127.79ms
step:1081/1393 train_time:136869ms step_avg:127.80ms
step:1082/1393 train_time:137001ms step_avg:127.80ms
step:1083/1393 train_time:137134ms step_avg:127.80ms
step:1084/1393 train_time:137268ms step_avg:127.81ms
step:1085/1393 train_time:137401ms step_avg:127.81ms
step:1086/1393 train_time:137534ms step_avg:127.82ms
step:1087/1393 train_time:137667ms step_avg:127.82ms
step:1088/1393 train_time:137801ms step_avg:127.83ms
step:1089/1393 train_time:137936ms step_avg:127.84ms
step:1090/1393 train_time:138070ms step_avg:127.84ms
step:1091/1393 train_time:138201ms step_avg:127.85ms
step:1092/1393 train_time:138335ms step_avg:127.85ms
step:1093/1393 train_time:138468ms step_avg:127.86ms
step:1094/1393 train_time:138600ms step_avg:127.86ms
step:1095/1393 train_time:138733ms step_avg:127.86ms
step:1096/1393 train_time:138866ms step_avg:127.87ms
step:1097/1393 train_time:138999ms step_avg:127.87ms
step:1098/1393 train_time:139132ms step_avg:127.88ms
step:1099/1393 train_time:139264ms step_avg:127.88ms
step:1100/1393 train_time:139397ms step_avg:127.89ms
step:1101/1393 train_time:139529ms step_avg:127.89ms
step:1102/1393 train_time:139663ms step_avg:127.90ms
step:1103/1393 train_time:139796ms step_avg:127.90ms
step:1104/1393 train_time:139930ms step_avg:127.91ms
step:1105/1393 train_time:140066ms step_avg:127.91ms
step:1106/1393 train_time:140200ms step_avg:127.92ms
step:1107/1393 train_time:140333ms step_avg:127.92ms
step:1108/1393 train_time:140468ms step_avg:127.93ms
step:1109/1393 train_time:140601ms step_avg:127.94ms
step:1110/1393 train_time:140734ms step_avg:127.94ms
step:1111/1393 train_time:140866ms step_avg:127.94ms
step:1112/1393 train_time:140999ms step_avg:127.95ms
step:1113/1393 train_time:141131ms step_avg:127.95ms
step:1114/1393 train_time:141265ms step_avg:127.96ms
step:1115/1393 train_time:141397ms step_avg:127.96ms
step:1116/1393 train_time:141533ms step_avg:127.97ms
step:1117/1393 train_time:141667ms step_avg:127.97ms
step:1118/1393 train_time:141802ms step_avg:127.98ms
step:1119/1393 train_time:141935ms step_avg:127.98ms
step:1120/1393 train_time:142068ms step_avg:127.99ms
step:1121/1393 train_time:142200ms step_avg:127.99ms
step:1122/1393 train_time:142333ms step_avg:128.00ms
step:1123/1393 train_time:142467ms step_avg:128.00ms
step:1124/1393 train_time:142600ms step_avg:128.01ms
step:1125/1393 train_time:142732ms step_avg:128.01ms
step:1125/1393 val_loss:3.3682 train_time:142864ms step_avg:128.13ms
step:1126/1393 train_time:142887ms step_avg:128.03ms
step:1127/1393 train_time:143004ms step_avg:128.03ms
step:1128/1393 train_time:143139ms step_avg:128.03ms
step:1129/1393 train_time:143272ms step_avg:128.04ms
step:1130/1393 train_time:143404ms step_avg:128.04ms
step:1131/1393 train_time:143537ms step_avg:128.04ms
step:1132/1393 train_time:143669ms step_avg:128.05ms
step:1133/1393 train_time:143801ms step_avg:128.05ms
step:1134/1393 train_time:143935ms step_avg:128.06ms
step:1135/1393 train_time:144070ms step_avg:128.06ms
step:1136/1393 train_time:144204ms step_avg:128.07ms
step:1137/1393 train_time:144336ms step_avg:128.07ms
step:1138/1393 train_time:144473ms step_avg:128.08ms
step:1139/1393 train_time:144607ms step_avg:128.08ms
step:1140/1393 train_time:144741ms step_avg:128.09ms
step:1141/1393 train_time:144876ms step_avg:128.10ms
step:1142/1393 train_time:145011ms step_avg:128.10ms
step:1143/1393 train_time:145148ms step_avg:128.11ms
step:1144/1393 train_time:145281ms step_avg:128.11ms
step:1145/1393 train_time:145416ms step_avg:128.12ms
step:1146/1393 train_time:145551ms step_avg:128.13ms
step:1147/1393 train_time:145686ms step_avg:128.13ms
step:1148/1393 train_time:145819ms step_avg:128.14ms
step:1149/1393 train_time:145952ms step_avg:128.14ms
step:1150/1393 train_time:146087ms step_avg:128.15ms
step:1151/1393 train_time:146224ms step_avg:128.15ms
step:1152/1393 train_time:146358ms step_avg:128.16ms
step:1153/1393 train_time:146495ms step_avg:128.17ms
step:1154/1393 train_time:146629ms step_avg:128.17ms
step:1155/1393 train_time:146763ms step_avg:128.18ms
step:1156/1393 train_time:146900ms step_avg:128.19ms
step:1157/1393 train_time:147034ms step_avg:128.19ms
step:1158/1393 train_time:147168ms step_avg:128.19ms
step:1159/1393 train_time:147302ms step_avg:128.20ms
step:1160/1393 train_time:147435ms step_avg:128.20ms
step:1161/1393 train_time:147569ms step_avg:128.21ms
step:1162/1393 train_time:147704ms step_avg:128.22ms
step:1163/1393 train_time:147837ms step_avg:128.22ms
step:1164/1393 train_time:147971ms step_avg:128.22ms
step:1165/1393 train_time:148105ms step_avg:128.23ms
step:1166/1393 train_time:148239ms step_avg:128.23ms
step:1167/1393 train_time:148372ms step_avg:128.24ms
step:1168/1393 train_time:148507ms step_avg:128.24ms
step:1169/1393 train_time:148641ms step_avg:128.25ms
step:1170/1393 train_time:148776ms step_avg:128.26ms
step:1171/1393 train_time:148910ms step_avg:128.26ms
step:1172/1393 train_time:149044ms step_avg:128.26ms
step:1173/1393 train_time:149178ms step_avg:128.27ms
step:1174/1393 train_time:149317ms step_avg:128.28ms
step:1175/1393 train_time:149451ms step_avg:128.28ms
step:1176/1393 train_time:149586ms step_avg:128.29ms
step:1177/1393 train_time:149721ms step_avg:128.30ms
step:1178/1393 train_time:149855ms step_avg:128.30ms
step:1179/1393 train_time:149988ms step_avg:128.30ms
step:1180/1393 train_time:150124ms step_avg:128.31ms
step:1181/1393 train_time:150261ms step_avg:128.32ms
step:1182/1393 train_time:150394ms step_avg:128.32ms
step:1183/1393 train_time:150529ms step_avg:128.33ms
step:1184/1393 train_time:150663ms step_avg:128.33ms
step:1185/1393 train_time:150798ms step_avg:128.34ms
step:1186/1393 train_time:150932ms step_avg:128.34ms
step:1187/1393 train_time:151071ms step_avg:128.35ms
step:1188/1393 train_time:151205ms step_avg:128.36ms
step:1189/1393 train_time:151340ms step_avg:128.36ms
step:1190/1393 train_time:151474ms step_avg:128.37ms
step:1191/1393 train_time:151608ms step_avg:128.37ms
step:1192/1393 train_time:151744ms step_avg:128.38ms
step:1193/1393 train_time:151878ms step_avg:128.38ms
step:1194/1393 train_time:152012ms step_avg:128.39ms
step:1195/1393 train_time:152146ms step_avg:128.39ms
step:1196/1393 train_time:152280ms step_avg:128.40ms
step:1197/1393 train_time:152415ms step_avg:128.40ms
step:1198/1393 train_time:152554ms step_avg:128.41ms
step:1199/1393 train_time:152688ms step_avg:128.42ms
step:1200/1393 train_time:152823ms step_avg:128.42ms
step:1201/1393 train_time:152956ms step_avg:128.43ms
step:1202/1393 train_time:153095ms step_avg:128.44ms
step:1203/1393 train_time:153232ms step_avg:128.44ms
step:1204/1393 train_time:153368ms step_avg:128.45ms
step:1205/1393 train_time:153503ms step_avg:128.45ms
step:1206/1393 train_time:153639ms step_avg:128.46ms
step:1207/1393 train_time:153773ms step_avg:128.47ms
step:1208/1393 train_time:153908ms step_avg:128.47ms
step:1209/1393 train_time:154042ms step_avg:128.48ms
step:1210/1393 train_time:154177ms step_avg:128.48ms
step:1211/1393 train_time:154312ms step_avg:128.49ms
step:1212/1393 train_time:154446ms step_avg:128.49ms
step:1213/1393 train_time:154579ms step_avg:128.49ms
step:1214/1393 train_time:154714ms step_avg:128.50ms
step:1215/1393 train_time:154850ms step_avg:128.51ms
step:1216/1393 train_time:154984ms step_avg:128.51ms
step:1217/1393 train_time:155118ms step_avg:128.52ms
step:1218/1393 train_time:155251ms step_avg:128.52ms
step:1219/1393 train_time:155384ms step_avg:128.52ms
step:1220/1393 train_time:155517ms step_avg:128.53ms
step:1221/1393 train_time:155651ms step_avg:128.53ms
step:1222/1393 train_time:155785ms step_avg:128.54ms
step:1223/1393 train_time:155918ms step_avg:128.54ms
step:1224/1393 train_time:156054ms step_avg:128.55ms
step:1225/1393 train_time:156190ms step_avg:128.55ms
step:1226/1393 train_time:156324ms step_avg:128.56ms
step:1227/1393 train_time:156458ms step_avg:128.56ms
step:1228/1393 train_time:156592ms step_avg:128.56ms
step:1229/1393 train_time:156725ms step_avg:128.57ms
step:1230/1393 train_time:156860ms step_avg:128.57ms
step:1231/1393 train_time:156996ms step_avg:128.58ms
step:1232/1393 train_time:157132ms step_avg:128.59ms
step:1233/1393 train_time:157267ms step_avg:128.59ms
step:1234/1393 train_time:157402ms step_avg:128.60ms
step:1235/1393 train_time:157535ms step_avg:128.60ms
step:1236/1393 train_time:157670ms step_avg:128.61ms
step:1237/1393 train_time:157804ms step_avg:128.61ms
step:1238/1393 train_time:157942ms step_avg:128.62ms
step:1239/1393 train_time:158075ms step_avg:128.62ms
step:1240/1393 train_time:158210ms step_avg:128.63ms
step:1241/1393 train_time:158346ms step_avg:128.63ms
step:1242/1393 train_time:158480ms step_avg:128.64ms
step:1243/1393 train_time:158615ms step_avg:128.64ms
step:1244/1393 train_time:158750ms step_avg:128.65ms
step:1245/1393 train_time:158885ms step_avg:128.65ms
step:1246/1393 train_time:159018ms step_avg:128.66ms
step:1247/1393 train_time:159153ms step_avg:128.66ms
step:1248/1393 train_time:159286ms step_avg:128.66ms
step:1249/1393 train_time:159420ms step_avg:128.67ms
step:1250/1393 train_time:159554ms step_avg:128.67ms
step:1250/1393 val_loss:3.3203 train_time:159687ms step_avg:128.78ms
step:1251/1393 train_time:159710ms step_avg:128.69ms
step:1252/1393 train_time:159831ms step_avg:128.69ms
step:1253/1393 train_time:159963ms step_avg:128.69ms
step:1254/1393 train_time:160096ms step_avg:128.69ms
step:1255/1393 train_time:160236ms step_avg:128.70ms
step:1256/1393 train_time:160369ms step_avg:128.71ms
step:1257/1393 train_time:160502ms step_avg:128.71ms
step:1258/1393 train_time:160636ms step_avg:128.71ms
step:1259/1393 train_time:160773ms step_avg:128.72ms
step:1260/1393 train_time:160907ms step_avg:128.73ms
step:1261/1393 train_time:161040ms step_avg:128.73ms
step:1262/1393 train_time:161177ms step_avg:128.74ms
step:1263/1393 train_time:161312ms step_avg:128.74ms
step:1264/1393 train_time:161447ms step_avg:128.75ms
step:1265/1393 train_time:161580ms step_avg:128.75ms
step:1266/1393 train_time:161716ms step_avg:128.75ms
step:1267/1393 train_time:161850ms step_avg:128.76ms
step:1268/1393 train_time:161985ms step_avg:128.76ms
step:1269/1393 train_time:162120ms step_avg:128.77ms
step:1270/1393 train_time:162256ms step_avg:128.77ms
step:1271/1393 train_time:162390ms step_avg:128.78ms
step:1272/1393 train_time:162524ms step_avg:128.78ms
step:1273/1393 train_time:162657ms step_avg:128.79ms
step:1274/1393 train_time:162792ms step_avg:128.79ms
step:1275/1393 train_time:162927ms step_avg:128.80ms
step:1276/1393 train_time:163060ms step_avg:128.80ms
step:1277/1393 train_time:163193ms step_avg:128.80ms
step:1278/1393 train_time:163329ms step_avg:128.81ms
step:1279/1393 train_time:163463ms step_avg:128.81ms
step:1280/1393 train_time:163599ms step_avg:128.82ms
step:1281/1393 train_time:163733ms step_avg:128.82ms
step:1282/1393 train_time:163866ms step_avg:128.83ms
step:1283/1393 train_time:164002ms step_avg:128.83ms
step:1284/1393 train_time:164136ms step_avg:128.84ms
step:1285/1393 train_time:164271ms step_avg:128.84ms
step:1286/1393 train_time:164405ms step_avg:128.84ms
step:1287/1393 train_time:164541ms step_avg:128.85ms
step:1288/1393 train_time:164675ms step_avg:128.85ms
step:1289/1393 train_time:164812ms step_avg:128.86ms
step:1290/1393 train_time:164947ms step_avg:128.87ms
step:1291/1393 train_time:165084ms step_avg:128.87ms
step:1292/1393 train_time:165218ms step_avg:128.88ms
step:1293/1393 train_time:165355ms step_avg:128.88ms
step:1294/1393 train_time:165488ms step_avg:128.88ms
step:1295/1393 train_time:165624ms step_avg:128.89ms
step:1296/1393 train_time:165760ms step_avg:128.90ms
step:1297/1393 train_time:165895ms step_avg:128.90ms
step:1298/1393 train_time:166028ms step_avg:128.90ms
step:1299/1393 train_time:166162ms step_avg:128.91ms
step:1300/1393 train_time:166296ms step_avg:128.91ms
step:1301/1393 train_time:166430ms step_avg:128.92ms
step:1302/1393 train_time:166564ms step_avg:128.92ms
step:1303/1393 train_time:166700ms step_avg:128.93ms
step:1304/1393 train_time:166836ms step_avg:128.93ms
step:1305/1393 train_time:166970ms step_avg:128.93ms
step:1306/1393 train_time:167105ms step_avg:128.94ms
step:1307/1393 train_time:167239ms step_avg:128.94ms
step:1308/1393 train_time:167374ms step_avg:128.95ms
step:1309/1393 train_time:167510ms step_avg:128.95ms
step:1310/1393 train_time:167645ms step_avg:128.96ms
step:1311/1393 train_time:167778ms step_avg:128.96ms
step:1312/1393 train_time:167912ms step_avg:128.96ms
step:1313/1393 train_time:168047ms step_avg:128.97ms
step:1314/1393 train_time:168181ms step_avg:128.97ms
step:1315/1393 train_time:168315ms step_avg:128.98ms
step:1316/1393 train_time:168450ms step_avg:128.98ms
step:1317/1393 train_time:168584ms step_avg:128.99ms
step:1318/1393 train_time:168720ms step_avg:128.99ms
step:1319/1393 train_time:168855ms step_avg:129.00ms
step:1320/1393 train_time:168990ms step_avg:129.00ms
step:1321/1393 train_time:169124ms step_avg:129.00ms
step:1322/1393 train_time:169261ms step_avg:129.01ms
step:1323/1393 train_time:169395ms step_avg:129.01ms
step:1324/1393 train_time:169528ms step_avg:129.02ms
step:1325/1393 train_time:169664ms step_avg:129.02ms
step:1326/1393 train_time:169800ms step_avg:129.03ms
step:1327/1393 train_time:169934ms step_avg:129.03ms
step:1328/1393 train_time:170068ms step_avg:129.03ms
step:1329/1393 train_time:170207ms step_avg:129.04ms
step:1330/1393 train_time:170342ms step_avg:129.05ms
step:1331/1393 train_time:170480ms step_avg:129.05ms
step:1332/1393 train_time:170617ms step_avg:129.06ms
step:1333/1393 train_time:170753ms step_avg:129.07ms
step:1334/1393 train_time:170887ms step_avg:129.07ms
step:1335/1393 train_time:171020ms step_avg:129.07ms
step:1336/1393 train_time:171159ms step_avg:129.08ms
step:1337/1393 train_time:171293ms step_avg:129.08ms
step:1338/1393 train_time:171428ms step_avg:129.09ms
step:1339/1393 train_time:171563ms step_avg:129.09ms
step:1340/1393 train_time:171698ms step_avg:129.10ms
step:1341/1393 train_time:171832ms step_avg:129.10ms
step:1342/1393 train_time:171967ms step_avg:129.10ms
step:1343/1393 train_time:172101ms step_avg:129.11ms
step:1344/1393 train_time:172236ms step_avg:129.11ms
step:1345/1393 train_time:172371ms step_avg:129.12ms
step:1346/1393 train_time:172506ms step_avg:129.12ms
step:1347/1393 train_time:172643ms step_avg:129.13ms
step:1348/1393 train_time:172778ms step_avg:129.13ms
step:1349/1393 train_time:172914ms step_avg:129.14ms
step:1350/1393 train_time:173048ms step_avg:129.14ms
step:1351/1393 train_time:173183ms step_avg:129.14ms
step:1352/1393 train_time:173321ms step_avg:129.15ms
step:1353/1393 train_time:173458ms step_avg:129.16ms
step:1354/1393 train_time:173594ms step_avg:129.16ms
step:1355/1393 train_time:173728ms step_avg:129.17ms
step:1356/1393 train_time:173861ms step_avg:129.17ms
step:1357/1393 train_time:173997ms step_avg:129.17ms
step:1358/1393 train_time:174134ms step_avg:129.18ms
step:1359/1393 train_time:174269ms step_avg:129.18ms
step:1360/1393 train_time:174406ms step_avg:129.19ms
step:1361/1393 train_time:174541ms step_avg:129.19ms
step:1362/1393 train_time:174678ms step_avg:129.20ms
step:1363/1393 train_time:174817ms step_avg:129.21ms
step:1364/1393 train_time:174952ms step_avg:129.21ms
step:1365/1393 train_time:175085ms step_avg:129.21ms
step:1366/1393 train_time:175221ms step_avg:129.22ms
step:1367/1393 train_time:175356ms step_avg:129.22ms
step:1368/1393 train_time:175493ms step_avg:129.23ms
step:1369/1393 train_time:175631ms step_avg:129.24ms
step:1370/1393 train_time:175769ms step_avg:129.24ms
step:1371/1393 train_time:175906ms step_avg:129.25ms
step:1372/1393 train_time:176044ms step_avg:129.25ms
step:1373/1393 train_time:176180ms step_avg:129.26ms
step:1374/1393 train_time:176319ms step_avg:129.27ms
step:1375/1393 train_time:176453ms step_avg:129.27ms
step:1375/1393 val_loss:3.2857 train_time:176586ms step_avg:129.37ms
step:1376/1393 train_time:176611ms step_avg:129.29ms
step:1377/1393 train_time:176731ms step_avg:129.28ms
step:1378/1393 train_time:176868ms step_avg:129.29ms
step:1379/1393 train_time:177003ms step_avg:129.29ms
step:1380/1393 train_time:177138ms step_avg:129.30ms
step:1381/1393 train_time:177275ms step_avg:129.30ms
step:1382/1393 train_time:177411ms step_avg:129.31ms
step:1383/1393 train_time:177546ms step_avg:129.31ms
step:1384/1393 train_time:177684ms step_avg:129.32ms
step:1385/1393 train_time:177819ms step_avg:129.32ms
step:1386/1393 train_time:177954ms step_avg:129.33ms
step:1387/1393 train_time:178092ms step_avg:129.33ms
step:1388/1393 train_time:178227ms step_avg:129.34ms
step:1389/1393 train_time:178362ms step_avg:129.34ms
step:1390/1393 train_time:178498ms step_avg:129.35ms
step:1391/1393 train_time:178635ms step_avg:129.35ms
step:1392/1393 train_time:178771ms step_avg:129.36ms
step:1393/1393 train_time:178906ms step_avg:129.36ms
step:1393/1393 val_loss:3.2818 train_time:179039ms step_avg:129.46ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
