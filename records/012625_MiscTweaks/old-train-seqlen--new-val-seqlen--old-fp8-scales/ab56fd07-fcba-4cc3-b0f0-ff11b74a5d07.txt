import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**29)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 04:49:23 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23110ms step_avg:nanms
step:2/1393 train_time:23577ms step_avg:nanms
step:3/1393 train_time:23697ms step_avg:nanms
step:4/1393 train_time:23817ms step_avg:nanms
step:5/1393 train_time:23939ms step_avg:nanms
step:6/1393 train_time:24059ms step_avg:nanms
step:7/1393 train_time:24181ms step_avg:nanms
step:8/1393 train_time:24303ms step_avg:nanms
step:9/1393 train_time:24424ms step_avg:nanms
step:10/1393 train_time:24546ms step_avg:nanms
step:11/1393 train_time:123ms step_avg:nanms
step:12/1393 train_time:245ms step_avg:nanms
step:13/1393 train_time:367ms step_avg:122.38ms
step:14/1393 train_time:489ms step_avg:122.27ms
step:15/1393 train_time:612ms step_avg:122.47ms
step:16/1393 train_time:734ms step_avg:122.25ms
step:17/1393 train_time:856ms step_avg:122.30ms
step:18/1393 train_time:978ms step_avg:122.26ms
step:19/1393 train_time:1100ms step_avg:122.27ms
step:20/1393 train_time:1222ms step_avg:122.20ms
step:21/1393 train_time:1344ms step_avg:122.19ms
step:22/1393 train_time:1466ms step_avg:122.16ms
step:23/1393 train_time:1588ms step_avg:122.17ms
step:24/1393 train_time:1712ms step_avg:122.27ms
step:25/1393 train_time:1833ms step_avg:122.20ms
step:26/1393 train_time:1955ms step_avg:122.20ms
step:27/1393 train_time:2078ms step_avg:122.23ms
step:28/1393 train_time:2201ms step_avg:122.27ms
step:29/1393 train_time:2323ms step_avg:122.25ms
step:30/1393 train_time:2444ms step_avg:122.20ms
step:31/1393 train_time:2565ms step_avg:122.15ms
step:32/1393 train_time:2687ms step_avg:122.14ms
step:33/1393 train_time:2810ms step_avg:122.17ms
step:34/1393 train_time:2931ms step_avg:122.13ms
step:35/1393 train_time:3055ms step_avg:122.19ms
step:36/1393 train_time:3178ms step_avg:122.22ms
step:37/1393 train_time:3300ms step_avg:122.23ms
step:38/1393 train_time:3421ms step_avg:122.19ms
step:39/1393 train_time:3544ms step_avg:122.20ms
step:40/1393 train_time:3665ms step_avg:122.17ms
step:41/1393 train_time:3786ms step_avg:122.14ms
step:42/1393 train_time:3909ms step_avg:122.16ms
step:43/1393 train_time:4032ms step_avg:122.17ms
step:44/1393 train_time:4155ms step_avg:122.21ms
step:45/1393 train_time:4278ms step_avg:122.23ms
step:46/1393 train_time:4400ms step_avg:122.21ms
step:47/1393 train_time:4523ms step_avg:122.25ms
step:48/1393 train_time:4646ms step_avg:122.26ms
step:49/1393 train_time:4767ms step_avg:122.24ms
step:50/1393 train_time:4891ms step_avg:122.28ms
step:51/1393 train_time:5011ms step_avg:122.22ms
step:52/1393 train_time:5133ms step_avg:122.22ms
step:53/1393 train_time:5255ms step_avg:122.22ms
step:54/1393 train_time:5378ms step_avg:122.23ms
step:55/1393 train_time:5502ms step_avg:122.26ms
step:56/1393 train_time:5624ms step_avg:122.26ms
step:57/1393 train_time:5746ms step_avg:122.25ms
step:58/1393 train_time:5867ms step_avg:122.24ms
step:59/1393 train_time:5989ms step_avg:122.23ms
step:60/1393 train_time:6112ms step_avg:122.25ms
step:61/1393 train_time:6235ms step_avg:122.25ms
step:62/1393 train_time:6357ms step_avg:122.26ms
step:63/1393 train_time:6480ms step_avg:122.26ms
step:64/1393 train_time:6602ms step_avg:122.26ms
step:65/1393 train_time:6724ms step_avg:122.26ms
step:66/1393 train_time:6845ms step_avg:122.24ms
step:67/1393 train_time:6967ms step_avg:122.22ms
step:68/1393 train_time:7089ms step_avg:122.23ms
step:69/1393 train_time:7212ms step_avg:122.23ms
step:70/1393 train_time:7335ms step_avg:122.25ms
step:71/1393 train_time:7457ms step_avg:122.25ms
step:72/1393 train_time:7580ms step_avg:122.26ms
step:73/1393 train_time:7702ms step_avg:122.25ms
step:74/1393 train_time:7824ms step_avg:122.25ms
step:75/1393 train_time:7946ms step_avg:122.25ms
step:76/1393 train_time:8069ms step_avg:122.25ms
step:77/1393 train_time:8191ms step_avg:122.25ms
step:78/1393 train_time:8314ms step_avg:122.27ms
step:79/1393 train_time:8436ms step_avg:122.26ms
step:80/1393 train_time:8559ms step_avg:122.27ms
step:81/1393 train_time:8682ms step_avg:122.27ms
step:82/1393 train_time:8804ms step_avg:122.28ms
step:83/1393 train_time:8926ms step_avg:122.27ms
step:84/1393 train_time:9048ms step_avg:122.27ms
step:85/1393 train_time:9170ms step_avg:122.27ms
step:86/1393 train_time:9294ms step_avg:122.29ms
step:87/1393 train_time:9417ms step_avg:122.30ms
step:88/1393 train_time:9538ms step_avg:122.29ms
step:89/1393 train_time:9662ms step_avg:122.30ms
step:90/1393 train_time:9783ms step_avg:122.29ms
step:91/1393 train_time:9905ms step_avg:122.28ms
step:92/1393 train_time:10026ms step_avg:122.27ms
step:93/1393 train_time:10149ms step_avg:122.27ms
step:94/1393 train_time:10270ms step_avg:122.26ms
step:95/1393 train_time:10392ms step_avg:122.26ms
step:96/1393 train_time:10514ms step_avg:122.26ms
step:97/1393 train_time:10636ms step_avg:122.26ms
step:98/1393 train_time:10758ms step_avg:122.25ms
step:99/1393 train_time:10880ms step_avg:122.25ms
step:100/1393 train_time:11001ms step_avg:122.24ms
step:101/1393 train_time:11124ms step_avg:122.24ms
step:102/1393 train_time:11245ms step_avg:122.23ms
step:103/1393 train_time:11368ms step_avg:122.24ms
step:104/1393 train_time:11491ms step_avg:122.24ms
step:105/1393 train_time:11614ms step_avg:122.25ms
step:106/1393 train_time:11736ms step_avg:122.25ms
step:107/1393 train_time:11859ms step_avg:122.26ms
step:108/1393 train_time:11981ms step_avg:122.26ms
step:109/1393 train_time:12105ms step_avg:122.27ms
step:110/1393 train_time:12227ms step_avg:122.27ms
step:111/1393 train_time:12349ms step_avg:122.27ms
step:112/1393 train_time:12473ms step_avg:122.28ms
step:113/1393 train_time:12596ms step_avg:122.29ms
step:114/1393 train_time:12719ms step_avg:122.30ms
step:115/1393 train_time:12842ms step_avg:122.30ms
step:116/1393 train_time:12964ms step_avg:122.30ms
step:117/1393 train_time:13086ms step_avg:122.30ms
step:118/1393 train_time:13209ms step_avg:122.30ms
step:119/1393 train_time:13331ms step_avg:122.30ms
step:120/1393 train_time:13454ms step_avg:122.31ms
step:121/1393 train_time:13577ms step_avg:122.31ms
step:122/1393 train_time:13701ms step_avg:122.33ms
step:123/1393 train_time:13823ms step_avg:122.33ms
step:124/1393 train_time:13946ms step_avg:122.33ms
step:125/1393 train_time:14068ms step_avg:122.33ms
step:125/1393 val_loss:4.4284 train_time:14189ms step_avg:123.38ms
step:126/1393 train_time:14211ms step_avg:122.51ms
step:127/1393 train_time:14315ms step_avg:122.35ms
step:128/1393 train_time:14447ms step_avg:122.43ms
step:129/1393 train_time:14571ms step_avg:122.44ms
step:130/1393 train_time:14693ms step_avg:122.44ms
step:131/1393 train_time:14816ms step_avg:122.45ms
step:132/1393 train_time:14939ms step_avg:122.45ms
step:133/1393 train_time:15061ms step_avg:122.45ms
step:134/1393 train_time:15183ms step_avg:122.44ms
step:135/1393 train_time:15305ms step_avg:122.44ms
step:136/1393 train_time:15430ms step_avg:122.46ms
step:137/1393 train_time:15554ms step_avg:122.47ms
step:138/1393 train_time:15677ms step_avg:122.48ms
step:139/1393 train_time:15800ms step_avg:122.48ms
step:140/1393 train_time:15922ms step_avg:122.48ms
step:141/1393 train_time:16045ms step_avg:122.48ms
step:142/1393 train_time:16167ms step_avg:122.48ms
step:143/1393 train_time:16289ms step_avg:122.48ms
step:144/1393 train_time:16413ms step_avg:122.49ms
step:145/1393 train_time:16537ms step_avg:122.50ms
step:146/1393 train_time:16661ms step_avg:122.51ms
step:147/1393 train_time:16785ms step_avg:122.52ms
step:148/1393 train_time:16908ms step_avg:122.52ms
step:149/1393 train_time:17030ms step_avg:122.52ms
step:150/1393 train_time:17152ms step_avg:122.52ms
step:151/1393 train_time:17275ms step_avg:122.52ms
step:152/1393 train_time:17398ms step_avg:122.52ms
step:153/1393 train_time:17520ms step_avg:122.52ms
step:154/1393 train_time:17644ms step_avg:122.53ms
step:155/1393 train_time:17767ms step_avg:122.53ms
step:156/1393 train_time:17890ms step_avg:122.53ms
step:157/1393 train_time:18013ms step_avg:122.54ms
step:158/1393 train_time:18135ms step_avg:122.53ms
step:159/1393 train_time:18259ms step_avg:122.54ms
step:160/1393 train_time:18382ms step_avg:122.54ms
step:161/1393 train_time:18506ms step_avg:122.55ms
step:162/1393 train_time:18629ms step_avg:122.56ms
step:163/1393 train_time:18752ms step_avg:122.56ms
step:164/1393 train_time:18875ms step_avg:122.57ms
step:165/1393 train_time:18998ms step_avg:122.57ms
step:166/1393 train_time:19120ms step_avg:122.57ms
step:167/1393 train_time:19243ms step_avg:122.56ms
step:168/1393 train_time:19365ms step_avg:122.56ms
step:169/1393 train_time:19489ms step_avg:122.57ms
step:170/1393 train_time:19612ms step_avg:122.57ms
step:171/1393 train_time:19734ms step_avg:122.57ms
step:172/1393 train_time:19858ms step_avg:122.58ms
step:173/1393 train_time:19982ms step_avg:122.59ms
step:174/1393 train_time:20105ms step_avg:122.59ms
step:175/1393 train_time:20228ms step_avg:122.60ms
step:176/1393 train_time:20352ms step_avg:122.60ms
step:177/1393 train_time:20473ms step_avg:122.59ms
step:178/1393 train_time:20596ms step_avg:122.60ms
step:179/1393 train_time:20719ms step_avg:122.60ms
step:180/1393 train_time:20842ms step_avg:122.60ms
step:181/1393 train_time:20966ms step_avg:122.61ms
step:182/1393 train_time:21090ms step_avg:122.62ms
step:183/1393 train_time:21212ms step_avg:122.61ms
step:184/1393 train_time:21337ms step_avg:122.62ms
step:185/1393 train_time:21460ms step_avg:122.63ms
step:186/1393 train_time:21582ms step_avg:122.63ms
step:187/1393 train_time:21704ms step_avg:122.62ms
step:188/1393 train_time:21828ms step_avg:122.63ms
step:189/1393 train_time:21951ms step_avg:122.63ms
step:190/1393 train_time:22073ms step_avg:122.63ms
step:191/1393 train_time:22197ms step_avg:122.64ms
step:192/1393 train_time:22319ms step_avg:122.63ms
step:193/1393 train_time:22442ms step_avg:122.63ms
step:194/1393 train_time:22565ms step_avg:122.64ms
step:195/1393 train_time:22687ms step_avg:122.63ms
step:196/1393 train_time:22811ms step_avg:122.64ms
step:197/1393 train_time:22934ms step_avg:122.64ms
step:198/1393 train_time:23057ms step_avg:122.64ms
step:199/1393 train_time:23180ms step_avg:122.64ms
step:200/1393 train_time:23303ms step_avg:122.65ms
step:201/1393 train_time:23424ms step_avg:122.64ms
step:202/1393 train_time:23548ms step_avg:122.64ms
step:203/1393 train_time:23670ms step_avg:122.64ms
step:204/1393 train_time:23794ms step_avg:122.65ms
step:205/1393 train_time:23918ms step_avg:122.66ms
step:206/1393 train_time:24041ms step_avg:122.66ms
step:207/1393 train_time:24163ms step_avg:122.65ms
step:208/1393 train_time:24286ms step_avg:122.65ms
step:209/1393 train_time:24409ms step_avg:122.66ms
step:210/1393 train_time:24532ms step_avg:122.66ms
step:211/1393 train_time:24655ms step_avg:122.66ms
step:212/1393 train_time:24778ms step_avg:122.67ms
step:213/1393 train_time:24901ms step_avg:122.67ms
step:214/1393 train_time:25025ms step_avg:122.67ms
step:215/1393 train_time:25149ms step_avg:122.68ms
step:216/1393 train_time:25273ms step_avg:122.68ms
step:217/1393 train_time:25396ms step_avg:122.68ms
step:218/1393 train_time:25519ms step_avg:122.69ms
step:219/1393 train_time:25642ms step_avg:122.69ms
step:220/1393 train_time:25764ms step_avg:122.69ms
step:221/1393 train_time:25889ms step_avg:122.70ms
step:222/1393 train_time:26015ms step_avg:122.71ms
step:223/1393 train_time:26140ms step_avg:122.72ms
step:224/1393 train_time:26263ms step_avg:122.73ms
step:225/1393 train_time:26387ms step_avg:122.73ms
step:226/1393 train_time:26511ms step_avg:122.74ms
step:227/1393 train_time:26635ms step_avg:122.74ms
step:228/1393 train_time:26758ms step_avg:122.74ms
step:229/1393 train_time:26881ms step_avg:122.74ms
step:230/1393 train_time:27003ms step_avg:122.74ms
step:231/1393 train_time:27127ms step_avg:122.75ms
step:232/1393 train_time:27252ms step_avg:122.76ms
step:233/1393 train_time:27377ms step_avg:122.77ms
step:234/1393 train_time:27500ms step_avg:122.77ms
step:235/1393 train_time:27624ms step_avg:122.77ms
step:236/1393 train_time:27747ms step_avg:122.78ms
step:237/1393 train_time:27871ms step_avg:122.78ms
step:238/1393 train_time:27993ms step_avg:122.78ms
step:239/1393 train_time:28117ms step_avg:122.78ms
step:240/1393 train_time:28240ms step_avg:122.78ms
step:241/1393 train_time:28363ms step_avg:122.78ms
step:242/1393 train_time:28487ms step_avg:122.79ms
step:243/1393 train_time:28610ms step_avg:122.79ms
step:244/1393 train_time:28735ms step_avg:122.80ms
step:245/1393 train_time:28858ms step_avg:122.80ms
step:246/1393 train_time:28982ms step_avg:122.80ms
step:247/1393 train_time:29106ms step_avg:122.81ms
step:248/1393 train_time:29229ms step_avg:122.81ms
step:249/1393 train_time:29352ms step_avg:122.81ms
step:250/1393 train_time:29476ms step_avg:122.82ms
step:250/1393 val_loss:3.9821 train_time:29597ms step_avg:123.32ms
step:251/1393 train_time:29619ms step_avg:122.90ms
step:252/1393 train_time:29735ms step_avg:122.87ms
step:253/1393 train_time:29863ms step_avg:122.89ms
step:254/1393 train_time:29987ms step_avg:122.90ms
step:255/1393 train_time:30110ms step_avg:122.90ms
step:256/1393 train_time:30233ms step_avg:122.90ms
step:257/1393 train_time:30356ms step_avg:122.90ms
step:258/1393 train_time:30479ms step_avg:122.90ms
step:259/1393 train_time:30601ms step_avg:122.90ms
step:260/1393 train_time:30725ms step_avg:122.90ms
step:261/1393 train_time:30850ms step_avg:122.91ms
step:262/1393 train_time:30974ms step_avg:122.91ms
step:263/1393 train_time:31097ms step_avg:122.91ms
step:264/1393 train_time:31220ms step_avg:122.91ms
step:265/1393 train_time:31342ms step_avg:122.91ms
step:266/1393 train_time:31467ms step_avg:122.92ms
step:267/1393 train_time:31589ms step_avg:122.91ms
step:268/1393 train_time:31712ms step_avg:122.91ms
step:269/1393 train_time:31836ms step_avg:122.92ms
step:270/1393 train_time:31960ms step_avg:122.92ms
step:271/1393 train_time:32084ms step_avg:122.93ms
step:272/1393 train_time:32207ms step_avg:122.93ms
step:273/1393 train_time:32330ms step_avg:122.93ms
step:274/1393 train_time:32453ms step_avg:122.93ms
step:275/1393 train_time:32577ms step_avg:122.93ms
step:276/1393 train_time:32700ms step_avg:122.93ms
step:277/1393 train_time:32823ms step_avg:122.93ms
step:278/1393 train_time:32946ms step_avg:122.93ms
step:279/1393 train_time:33069ms step_avg:122.93ms
step:280/1393 train_time:33192ms step_avg:122.93ms
step:281/1393 train_time:33315ms step_avg:122.93ms
step:282/1393 train_time:33438ms step_avg:122.94ms
step:283/1393 train_time:33562ms step_avg:122.94ms
step:284/1393 train_time:33685ms step_avg:122.94ms
step:285/1393 train_time:33810ms step_avg:122.94ms
step:286/1393 train_time:33933ms step_avg:122.95ms
step:287/1393 train_time:34056ms step_avg:122.95ms
step:288/1393 train_time:34180ms step_avg:122.95ms
step:289/1393 train_time:34303ms step_avg:122.95ms
step:290/1393 train_time:34425ms step_avg:122.95ms
step:291/1393 train_time:34549ms step_avg:122.95ms
step:292/1393 train_time:34672ms step_avg:122.95ms
step:293/1393 train_time:34796ms step_avg:122.95ms
step:294/1393 train_time:34919ms step_avg:122.95ms
step:295/1393 train_time:35043ms step_avg:122.96ms
step:296/1393 train_time:35166ms step_avg:122.96ms
step:297/1393 train_time:35289ms step_avg:122.96ms
step:298/1393 train_time:35412ms step_avg:122.96ms
step:299/1393 train_time:35535ms step_avg:122.96ms
step:300/1393 train_time:35659ms step_avg:122.96ms
step:301/1393 train_time:35782ms step_avg:122.96ms
step:302/1393 train_time:35907ms step_avg:122.97ms
step:303/1393 train_time:36031ms step_avg:122.97ms
step:304/1393 train_time:36154ms step_avg:122.97ms
step:305/1393 train_time:36277ms step_avg:122.97ms
step:306/1393 train_time:36401ms step_avg:122.98ms
step:307/1393 train_time:36524ms step_avg:122.98ms
step:308/1393 train_time:36647ms step_avg:122.98ms
step:309/1393 train_time:36770ms step_avg:122.98ms
step:310/1393 train_time:36894ms step_avg:122.98ms
step:311/1393 train_time:37018ms step_avg:122.98ms
step:312/1393 train_time:37144ms step_avg:122.99ms
step:313/1393 train_time:37271ms step_avg:123.00ms
step:314/1393 train_time:37397ms step_avg:123.02ms
step:315/1393 train_time:37523ms step_avg:123.03ms
step:316/1393 train_time:37649ms step_avg:123.04ms
step:317/1393 train_time:37775ms step_avg:123.05ms
step:318/1393 train_time:37901ms step_avg:123.05ms
step:319/1393 train_time:38026ms step_avg:123.06ms
step:320/1393 train_time:38151ms step_avg:123.07ms
step:321/1393 train_time:38278ms step_avg:123.08ms
step:322/1393 train_time:38403ms step_avg:123.09ms
step:323/1393 train_time:38529ms step_avg:123.10ms
step:324/1393 train_time:38655ms step_avg:123.10ms
step:325/1393 train_time:38782ms step_avg:123.12ms
step:326/1393 train_time:38907ms step_avg:123.12ms
step:327/1393 train_time:39034ms step_avg:123.13ms
step:328/1393 train_time:39159ms step_avg:123.14ms
step:329/1393 train_time:39286ms step_avg:123.15ms
step:330/1393 train_time:39412ms step_avg:123.16ms
step:331/1393 train_time:39539ms step_avg:123.17ms
step:332/1393 train_time:39666ms step_avg:123.19ms
step:333/1393 train_time:39791ms step_avg:123.19ms
step:334/1393 train_time:39916ms step_avg:123.20ms
step:335/1393 train_time:40042ms step_avg:123.21ms
step:336/1393 train_time:40168ms step_avg:123.21ms
step:337/1393 train_time:40295ms step_avg:123.22ms
step:338/1393 train_time:40420ms step_avg:123.23ms
step:339/1393 train_time:40545ms step_avg:123.24ms
step:340/1393 train_time:40672ms step_avg:123.25ms
step:341/1393 train_time:40797ms step_avg:123.25ms
step:342/1393 train_time:40922ms step_avg:123.26ms
step:343/1393 train_time:41049ms step_avg:123.27ms
step:344/1393 train_time:41175ms step_avg:123.28ms
step:345/1393 train_time:41300ms step_avg:123.28ms
step:346/1393 train_time:41426ms step_avg:123.29ms
step:347/1393 train_time:41553ms step_avg:123.30ms
step:348/1393 train_time:41680ms step_avg:123.31ms
step:349/1393 train_time:41805ms step_avg:123.32ms
step:350/1393 train_time:41930ms step_avg:123.32ms
step:351/1393 train_time:42056ms step_avg:123.33ms
step:352/1393 train_time:42182ms step_avg:123.34ms
step:353/1393 train_time:42309ms step_avg:123.35ms
step:354/1393 train_time:42435ms step_avg:123.36ms
step:355/1393 train_time:42560ms step_avg:123.36ms
step:356/1393 train_time:42686ms step_avg:123.37ms
step:357/1393 train_time:42812ms step_avg:123.38ms
step:358/1393 train_time:42937ms step_avg:123.38ms
step:359/1393 train_time:43063ms step_avg:123.39ms
step:360/1393 train_time:43188ms step_avg:123.40ms
step:361/1393 train_time:43315ms step_avg:123.40ms
step:362/1393 train_time:43441ms step_avg:123.41ms
step:363/1393 train_time:43567ms step_avg:123.42ms
step:364/1393 train_time:43692ms step_avg:123.42ms
step:365/1393 train_time:43818ms step_avg:123.43ms
step:366/1393 train_time:43943ms step_avg:123.43ms
step:367/1393 train_time:44069ms step_avg:123.44ms
step:368/1393 train_time:44195ms step_avg:123.45ms
step:369/1393 train_time:44322ms step_avg:123.46ms
step:370/1393 train_time:44447ms step_avg:123.46ms
step:371/1393 train_time:44574ms step_avg:123.47ms
step:372/1393 train_time:44699ms step_avg:123.48ms
step:373/1393 train_time:44825ms step_avg:123.48ms
step:374/1393 train_time:44950ms step_avg:123.49ms
step:375/1393 train_time:45076ms step_avg:123.50ms
step:375/1393 val_loss:3.7898 train_time:45202ms step_avg:123.84ms
step:376/1393 train_time:45223ms step_avg:123.56ms
step:377/1393 train_time:45344ms step_avg:123.55ms
step:378/1393 train_time:45474ms step_avg:123.57ms
step:379/1393 train_time:45601ms step_avg:123.58ms
step:380/1393 train_time:45726ms step_avg:123.58ms
step:381/1393 train_time:45851ms step_avg:123.59ms
step:382/1393 train_time:45976ms step_avg:123.59ms
step:383/1393 train_time:46101ms step_avg:123.59ms
step:384/1393 train_time:46226ms step_avg:123.60ms
step:385/1393 train_time:46353ms step_avg:123.61ms
step:386/1393 train_time:46480ms step_avg:123.62ms
step:387/1393 train_time:46607ms step_avg:123.63ms
step:388/1393 train_time:46732ms step_avg:123.63ms
step:389/1393 train_time:46858ms step_avg:123.63ms
step:390/1393 train_time:46984ms step_avg:123.64ms
step:391/1393 train_time:47109ms step_avg:123.64ms
step:392/1393 train_time:47234ms step_avg:123.65ms
step:393/1393 train_time:47360ms step_avg:123.65ms
step:394/1393 train_time:47486ms step_avg:123.66ms
step:395/1393 train_time:47613ms step_avg:123.67ms
step:396/1393 train_time:47740ms step_avg:123.68ms
step:397/1393 train_time:47867ms step_avg:123.69ms
step:398/1393 train_time:47992ms step_avg:123.69ms
step:399/1393 train_time:48117ms step_avg:123.69ms
step:400/1393 train_time:48243ms step_avg:123.70ms
step:401/1393 train_time:48369ms step_avg:123.71ms
step:402/1393 train_time:48496ms step_avg:123.71ms
step:403/1393 train_time:48621ms step_avg:123.72ms
step:404/1393 train_time:48747ms step_avg:123.72ms
step:405/1393 train_time:48872ms step_avg:123.73ms
step:406/1393 train_time:48998ms step_avg:123.73ms
step:407/1393 train_time:49124ms step_avg:123.74ms
step:408/1393 train_time:49250ms step_avg:123.74ms
step:409/1393 train_time:49375ms step_avg:123.75ms
step:410/1393 train_time:49502ms step_avg:123.76ms
step:411/1393 train_time:49629ms step_avg:123.76ms
step:412/1393 train_time:49755ms step_avg:123.77ms
step:413/1393 train_time:49882ms step_avg:123.78ms
step:414/1393 train_time:50008ms step_avg:123.78ms
step:415/1393 train_time:50134ms step_avg:123.79ms
step:416/1393 train_time:50260ms step_avg:123.79ms
step:417/1393 train_time:50386ms step_avg:123.80ms
step:418/1393 train_time:50512ms step_avg:123.81ms
step:419/1393 train_time:50638ms step_avg:123.81ms
step:420/1393 train_time:50765ms step_avg:123.82ms
step:421/1393 train_time:50892ms step_avg:123.82ms
step:422/1393 train_time:51019ms step_avg:123.83ms
step:423/1393 train_time:51145ms step_avg:123.84ms
step:424/1393 train_time:51272ms step_avg:123.85ms
step:425/1393 train_time:51398ms step_avg:123.85ms
step:426/1393 train_time:51524ms step_avg:123.85ms
step:427/1393 train_time:51649ms step_avg:123.86ms
step:428/1393 train_time:51775ms step_avg:123.86ms
step:429/1393 train_time:51902ms step_avg:123.87ms
step:430/1393 train_time:52028ms step_avg:123.88ms
step:431/1393 train_time:52154ms step_avg:123.88ms
step:432/1393 train_time:52280ms step_avg:123.89ms
step:433/1393 train_time:52407ms step_avg:123.89ms
step:434/1393 train_time:52533ms step_avg:123.90ms
step:435/1393 train_time:52659ms step_avg:123.90ms
step:436/1393 train_time:52785ms step_avg:123.91ms
step:437/1393 train_time:52912ms step_avg:123.91ms
step:438/1393 train_time:53039ms step_avg:123.92ms
step:439/1393 train_time:53165ms step_avg:123.93ms
step:440/1393 train_time:53290ms step_avg:123.93ms
step:441/1393 train_time:53417ms step_avg:123.94ms
step:442/1393 train_time:53543ms step_avg:123.94ms
step:443/1393 train_time:53670ms step_avg:123.95ms
step:444/1393 train_time:53796ms step_avg:123.95ms
step:445/1393 train_time:53922ms step_avg:123.96ms
step:446/1393 train_time:54048ms step_avg:123.96ms
step:447/1393 train_time:54175ms step_avg:123.97ms
step:448/1393 train_time:54301ms step_avg:123.98ms
step:449/1393 train_time:54427ms step_avg:123.98ms
step:450/1393 train_time:54553ms step_avg:123.98ms
step:451/1393 train_time:54679ms step_avg:123.99ms
step:452/1393 train_time:54805ms step_avg:123.99ms
step:453/1393 train_time:54931ms step_avg:124.00ms
step:454/1393 train_time:55057ms step_avg:124.00ms
step:455/1393 train_time:55184ms step_avg:124.01ms
step:456/1393 train_time:55312ms step_avg:124.02ms
step:457/1393 train_time:55438ms step_avg:124.02ms
step:458/1393 train_time:55565ms step_avg:124.03ms
step:459/1393 train_time:55691ms step_avg:124.03ms
step:460/1393 train_time:55818ms step_avg:124.04ms
step:461/1393 train_time:55944ms step_avg:124.05ms
step:462/1393 train_time:56071ms step_avg:124.05ms
step:463/1393 train_time:56197ms step_avg:124.06ms
step:464/1393 train_time:56324ms step_avg:124.06ms
step:465/1393 train_time:56450ms step_avg:124.07ms
step:466/1393 train_time:56576ms step_avg:124.07ms
step:467/1393 train_time:56702ms step_avg:124.08ms
step:468/1393 train_time:56829ms step_avg:124.08ms
step:469/1393 train_time:56956ms step_avg:124.09ms
step:470/1393 train_time:57082ms step_avg:124.09ms
step:471/1393 train_time:57208ms step_avg:124.10ms
step:472/1393 train_time:57334ms step_avg:124.10ms
step:473/1393 train_time:57461ms step_avg:124.11ms
step:474/1393 train_time:57587ms step_avg:124.11ms
step:475/1393 train_time:57713ms step_avg:124.11ms
step:476/1393 train_time:57840ms step_avg:124.12ms
step:477/1393 train_time:57966ms step_avg:124.12ms
step:478/1393 train_time:58094ms step_avg:124.13ms
step:479/1393 train_time:58220ms step_avg:124.14ms
step:480/1393 train_time:58345ms step_avg:124.14ms
step:481/1393 train_time:58471ms step_avg:124.14ms
step:482/1393 train_time:58598ms step_avg:124.15ms
step:483/1393 train_time:58724ms step_avg:124.15ms
step:484/1393 train_time:58851ms step_avg:124.16ms
step:485/1393 train_time:58978ms step_avg:124.16ms
step:486/1393 train_time:59104ms step_avg:124.17ms
step:487/1393 train_time:59230ms step_avg:124.17ms
step:488/1393 train_time:59357ms step_avg:124.18ms
step:489/1393 train_time:59484ms step_avg:124.18ms
step:490/1393 train_time:59611ms step_avg:124.19ms
step:491/1393 train_time:59737ms step_avg:124.19ms
step:492/1393 train_time:59864ms step_avg:124.20ms
step:493/1393 train_time:59990ms step_avg:124.20ms
step:494/1393 train_time:60116ms step_avg:124.21ms
step:495/1393 train_time:60242ms step_avg:124.21ms
step:496/1393 train_time:60369ms step_avg:124.22ms
step:497/1393 train_time:60494ms step_avg:124.22ms
step:498/1393 train_time:60621ms step_avg:124.22ms
step:499/1393 train_time:60746ms step_avg:124.23ms
step:500/1393 train_time:60874ms step_avg:124.23ms
step:500/1393 val_loss:3.6707 train_time:60999ms step_avg:124.49ms
step:501/1393 train_time:61020ms step_avg:124.28ms
step:502/1393 train_time:61135ms step_avg:124.26ms
step:503/1393 train_time:61263ms step_avg:124.27ms
step:504/1393 train_time:61389ms step_avg:124.27ms
step:505/1393 train_time:61514ms step_avg:124.27ms
step:506/1393 train_time:61640ms step_avg:124.27ms
step:507/1393 train_time:61766ms step_avg:124.28ms
step:508/1393 train_time:61892ms step_avg:124.28ms
step:509/1393 train_time:62019ms step_avg:124.29ms
step:510/1393 train_time:62146ms step_avg:124.29ms
step:511/1393 train_time:62273ms step_avg:124.30ms
step:512/1393 train_time:62399ms step_avg:124.30ms
step:513/1393 train_time:62525ms step_avg:124.30ms
step:514/1393 train_time:62652ms step_avg:124.31ms
step:515/1393 train_time:62778ms step_avg:124.31ms
step:516/1393 train_time:62903ms step_avg:124.32ms
step:517/1393 train_time:63030ms step_avg:124.32ms
step:518/1393 train_time:63159ms step_avg:124.33ms
step:519/1393 train_time:63287ms step_avg:124.34ms
step:520/1393 train_time:63416ms step_avg:124.35ms
step:521/1393 train_time:63544ms step_avg:124.35ms
step:522/1393 train_time:63672ms step_avg:124.36ms
step:523/1393 train_time:63800ms step_avg:124.37ms
step:524/1393 train_time:63929ms step_avg:124.38ms
step:525/1393 train_time:64058ms step_avg:124.38ms
step:526/1393 train_time:64186ms step_avg:124.39ms
step:527/1393 train_time:64316ms step_avg:124.40ms
step:528/1393 train_time:64445ms step_avg:124.41ms
step:529/1393 train_time:64574ms step_avg:124.42ms
step:530/1393 train_time:64702ms step_avg:124.43ms
step:531/1393 train_time:64830ms step_avg:124.43ms
step:532/1393 train_time:64958ms step_avg:124.44ms
step:533/1393 train_time:65086ms step_avg:124.45ms
step:534/1393 train_time:65214ms step_avg:124.45ms
step:535/1393 train_time:65342ms step_avg:124.46ms
step:536/1393 train_time:65472ms step_avg:124.47ms
step:537/1393 train_time:65601ms step_avg:124.48ms
step:538/1393 train_time:65729ms step_avg:124.49ms
step:539/1393 train_time:65858ms step_avg:124.50ms
step:540/1393 train_time:65986ms step_avg:124.50ms
step:541/1393 train_time:66115ms step_avg:124.51ms
step:542/1393 train_time:66244ms step_avg:124.52ms
step:543/1393 train_time:66373ms step_avg:124.53ms
step:544/1393 train_time:66501ms step_avg:124.53ms
step:545/1393 train_time:66630ms step_avg:124.54ms
step:546/1393 train_time:66758ms step_avg:124.55ms
step:547/1393 train_time:66887ms step_avg:124.56ms
step:548/1393 train_time:67015ms step_avg:124.56ms
step:549/1393 train_time:67143ms step_avg:124.57ms
step:550/1393 train_time:67272ms step_avg:124.58ms
step:551/1393 train_time:67401ms step_avg:124.59ms
step:552/1393 train_time:67529ms step_avg:124.59ms
step:553/1393 train_time:67657ms step_avg:124.60ms
step:554/1393 train_time:67786ms step_avg:124.61ms
step:555/1393 train_time:67914ms step_avg:124.61ms
step:556/1393 train_time:68043ms step_avg:124.62ms
step:557/1393 train_time:68172ms step_avg:124.63ms
step:558/1393 train_time:68300ms step_avg:124.63ms
step:559/1393 train_time:68428ms step_avg:124.64ms
step:560/1393 train_time:68557ms step_avg:124.65ms
step:561/1393 train_time:68685ms step_avg:124.66ms
step:562/1393 train_time:68813ms step_avg:124.66ms
step:563/1393 train_time:68943ms step_avg:124.67ms
step:564/1393 train_time:69072ms step_avg:124.68ms
step:565/1393 train_time:69201ms step_avg:124.69ms
step:566/1393 train_time:69330ms step_avg:124.69ms
step:567/1393 train_time:69459ms step_avg:124.70ms
step:568/1393 train_time:69587ms step_avg:124.71ms
step:569/1393 train_time:69715ms step_avg:124.71ms
step:570/1393 train_time:69844ms step_avg:124.72ms
step:571/1393 train_time:69973ms step_avg:124.73ms
step:572/1393 train_time:70102ms step_avg:124.74ms
step:573/1393 train_time:70230ms step_avg:124.74ms
step:574/1393 train_time:70359ms step_avg:124.75ms
step:575/1393 train_time:70488ms step_avg:124.76ms
step:576/1393 train_time:70616ms step_avg:124.76ms
step:577/1393 train_time:70744ms step_avg:124.77ms
step:578/1393 train_time:70872ms step_avg:124.77ms
step:579/1393 train_time:71001ms step_avg:124.78ms
step:580/1393 train_time:71130ms step_avg:124.79ms
step:581/1393 train_time:71259ms step_avg:124.80ms
step:582/1393 train_time:71387ms step_avg:124.80ms
step:583/1393 train_time:71514ms step_avg:124.81ms
step:584/1393 train_time:71642ms step_avg:124.81ms
step:585/1393 train_time:71771ms step_avg:124.82ms
step:586/1393 train_time:71899ms step_avg:124.83ms
step:587/1393 train_time:72028ms step_avg:124.83ms
step:588/1393 train_time:72156ms step_avg:124.84ms
step:589/1393 train_time:72285ms step_avg:124.84ms
step:590/1393 train_time:72413ms step_avg:124.85ms
step:591/1393 train_time:72542ms step_avg:124.86ms
step:592/1393 train_time:72671ms step_avg:124.86ms
step:593/1393 train_time:72800ms step_avg:124.87ms
step:594/1393 train_time:72928ms step_avg:124.88ms
step:595/1393 train_time:73057ms step_avg:124.88ms
step:596/1393 train_time:73184ms step_avg:124.89ms
step:597/1393 train_time:73313ms step_avg:124.89ms
step:598/1393 train_time:73442ms step_avg:124.90ms
step:599/1393 train_time:73571ms step_avg:124.91ms
step:600/1393 train_time:73700ms step_avg:124.92ms
step:601/1393 train_time:73828ms step_avg:124.92ms
step:602/1393 train_time:73956ms step_avg:124.93ms
step:603/1393 train_time:74084ms step_avg:124.93ms
step:604/1393 train_time:74212ms step_avg:124.94ms
step:605/1393 train_time:74341ms step_avg:124.94ms
step:606/1393 train_time:74469ms step_avg:124.95ms
step:607/1393 train_time:74598ms step_avg:124.95ms
step:608/1393 train_time:74727ms step_avg:124.96ms
step:609/1393 train_time:74856ms step_avg:124.97ms
step:610/1393 train_time:74985ms step_avg:124.97ms
step:611/1393 train_time:75114ms step_avg:124.98ms
step:612/1393 train_time:75242ms step_avg:124.99ms
step:613/1393 train_time:75372ms step_avg:124.99ms
step:614/1393 train_time:75499ms step_avg:125.00ms
step:615/1393 train_time:75628ms step_avg:125.00ms
step:616/1393 train_time:75756ms step_avg:125.01ms
step:617/1393 train_time:75885ms step_avg:125.02ms
step:618/1393 train_time:76014ms step_avg:125.02ms
step:619/1393 train_time:76143ms step_avg:125.03ms
step:620/1393 train_time:76271ms step_avg:125.03ms
step:621/1393 train_time:76400ms step_avg:125.04ms
step:622/1393 train_time:76529ms step_avg:125.05ms
step:623/1393 train_time:76659ms step_avg:125.06ms
step:624/1393 train_time:76787ms step_avg:125.06ms
step:625/1393 train_time:76917ms step_avg:125.07ms
step:625/1393 val_loss:3.5836 train_time:77044ms step_avg:125.28ms
step:626/1393 train_time:77066ms step_avg:125.11ms
step:627/1393 train_time:77183ms step_avg:125.09ms
step:628/1393 train_time:77312ms step_avg:125.10ms
step:629/1393 train_time:77441ms step_avg:125.11ms
step:630/1393 train_time:77569ms step_avg:125.11ms
step:631/1393 train_time:77697ms step_avg:125.12ms
step:632/1393 train_time:77825ms step_avg:125.12ms
step:633/1393 train_time:77953ms step_avg:125.12ms
step:634/1393 train_time:78083ms step_avg:125.13ms
step:635/1393 train_time:78213ms step_avg:125.14ms
step:636/1393 train_time:78343ms step_avg:125.15ms
step:637/1393 train_time:78471ms step_avg:125.15ms
step:638/1393 train_time:78599ms step_avg:125.16ms
step:639/1393 train_time:78727ms step_avg:125.16ms
step:640/1393 train_time:78856ms step_avg:125.17ms
step:641/1393 train_time:78984ms step_avg:125.17ms
step:642/1393 train_time:79113ms step_avg:125.18ms
step:643/1393 train_time:79241ms step_avg:125.18ms
step:644/1393 train_time:79370ms step_avg:125.19ms
step:645/1393 train_time:79500ms step_avg:125.20ms
step:646/1393 train_time:79628ms step_avg:125.20ms
step:647/1393 train_time:79756ms step_avg:125.21ms
step:648/1393 train_time:79885ms step_avg:125.21ms
step:649/1393 train_time:80013ms step_avg:125.22ms
step:650/1393 train_time:80142ms step_avg:125.22ms
step:651/1393 train_time:80271ms step_avg:125.23ms
step:652/1393 train_time:80400ms step_avg:125.23ms
step:653/1393 train_time:80529ms step_avg:125.24ms
step:654/1393 train_time:80658ms step_avg:125.25ms
step:655/1393 train_time:80787ms step_avg:125.25ms
step:656/1393 train_time:80915ms step_avg:125.26ms
step:657/1393 train_time:81044ms step_avg:125.26ms
step:658/1393 train_time:81173ms step_avg:125.27ms
step:659/1393 train_time:81302ms step_avg:125.27ms
step:660/1393 train_time:81431ms step_avg:125.28ms
step:661/1393 train_time:81560ms step_avg:125.28ms
step:662/1393 train_time:81688ms step_avg:125.29ms
step:663/1393 train_time:81817ms step_avg:125.29ms
step:664/1393 train_time:81946ms step_avg:125.30ms
step:665/1393 train_time:82074ms step_avg:125.30ms
step:666/1393 train_time:82202ms step_avg:125.31ms
step:667/1393 train_time:82332ms step_avg:125.31ms
step:668/1393 train_time:82461ms step_avg:125.32ms
step:669/1393 train_time:82589ms step_avg:125.33ms
step:670/1393 train_time:82719ms step_avg:125.33ms
step:671/1393 train_time:82847ms step_avg:125.34ms
step:672/1393 train_time:82976ms step_avg:125.34ms
step:673/1393 train_time:83105ms step_avg:125.35ms
step:674/1393 train_time:83233ms step_avg:125.35ms
step:675/1393 train_time:83361ms step_avg:125.36ms
step:676/1393 train_time:83490ms step_avg:125.36ms
step:677/1393 train_time:83620ms step_avg:125.37ms
step:678/1393 train_time:83748ms step_avg:125.37ms
step:679/1393 train_time:83877ms step_avg:125.38ms
step:680/1393 train_time:84006ms step_avg:125.38ms
step:681/1393 train_time:84135ms step_avg:125.39ms
step:682/1393 train_time:84264ms step_avg:125.39ms
step:683/1393 train_time:84392ms step_avg:125.40ms
step:684/1393 train_time:84521ms step_avg:125.40ms
step:685/1393 train_time:84650ms step_avg:125.41ms
step:686/1393 train_time:84778ms step_avg:125.41ms
step:687/1393 train_time:84907ms step_avg:125.42ms
step:688/1393 train_time:85036ms step_avg:125.42ms
step:689/1393 train_time:85165ms step_avg:125.43ms
step:690/1393 train_time:85294ms step_avg:125.43ms
step:691/1393 train_time:85422ms step_avg:125.44ms
step:692/1393 train_time:85551ms step_avg:125.44ms
step:693/1393 train_time:85679ms step_avg:125.45ms
step:694/1393 train_time:85809ms step_avg:125.45ms
step:695/1393 train_time:85938ms step_avg:125.46ms
step:696/1393 train_time:86067ms step_avg:125.46ms
step:697/1393 train_time:86196ms step_avg:125.47ms
step:698/1393 train_time:86324ms step_avg:125.47ms
step:699/1393 train_time:86453ms step_avg:125.48ms
step:700/1393 train_time:86582ms step_avg:125.48ms
step:701/1393 train_time:86711ms step_avg:125.49ms
step:702/1393 train_time:86840ms step_avg:125.49ms
step:703/1393 train_time:86968ms step_avg:125.50ms
step:704/1393 train_time:87097ms step_avg:125.50ms
step:705/1393 train_time:87226ms step_avg:125.50ms
step:706/1393 train_time:87355ms step_avg:125.51ms
step:707/1393 train_time:87484ms step_avg:125.52ms
step:708/1393 train_time:87614ms step_avg:125.52ms
step:709/1393 train_time:87743ms step_avg:125.53ms
step:710/1393 train_time:87872ms step_avg:125.53ms
step:711/1393 train_time:88001ms step_avg:125.54ms
step:712/1393 train_time:88130ms step_avg:125.54ms
step:713/1393 train_time:88258ms step_avg:125.54ms
step:714/1393 train_time:88388ms step_avg:125.55ms
step:715/1393 train_time:88515ms step_avg:125.55ms
step:716/1393 train_time:88644ms step_avg:125.56ms
step:717/1393 train_time:88772ms step_avg:125.56ms
step:718/1393 train_time:88901ms step_avg:125.57ms
step:719/1393 train_time:89030ms step_avg:125.57ms
step:720/1393 train_time:89160ms step_avg:125.58ms
step:721/1393 train_time:89289ms step_avg:125.58ms
step:722/1393 train_time:89418ms step_avg:125.59ms
step:723/1393 train_time:89546ms step_avg:125.59ms
step:724/1393 train_time:89675ms step_avg:125.60ms
step:725/1393 train_time:89805ms step_avg:125.60ms
step:726/1393 train_time:89937ms step_avg:125.61ms
step:727/1393 train_time:90067ms step_avg:125.62ms
step:728/1393 train_time:90197ms step_avg:125.62ms
step:729/1393 train_time:90327ms step_avg:125.63ms
step:730/1393 train_time:90459ms step_avg:125.64ms
step:731/1393 train_time:90590ms step_avg:125.64ms
step:732/1393 train_time:90722ms step_avg:125.65ms
step:733/1393 train_time:90852ms step_avg:125.66ms
step:734/1393 train_time:90983ms step_avg:125.67ms
step:735/1393 train_time:91114ms step_avg:125.67ms
step:736/1393 train_time:91244ms step_avg:125.68ms
step:737/1393 train_time:91376ms step_avg:125.69ms
step:738/1393 train_time:91506ms step_avg:125.69ms
step:739/1393 train_time:91635ms step_avg:125.70ms
step:740/1393 train_time:91766ms step_avg:125.71ms
step:741/1393 train_time:91897ms step_avg:125.71ms
step:742/1393 train_time:92028ms step_avg:125.72ms
step:743/1393 train_time:92159ms step_avg:125.73ms
step:744/1393 train_time:92289ms step_avg:125.73ms
step:745/1393 train_time:92421ms step_avg:125.74ms
step:746/1393 train_time:92552ms step_avg:125.75ms
step:747/1393 train_time:92683ms step_avg:125.76ms
step:748/1393 train_time:92814ms step_avg:125.76ms
step:749/1393 train_time:92945ms step_avg:125.77ms
step:750/1393 train_time:93077ms step_avg:125.78ms
step:750/1393 val_loss:3.5301 train_time:93206ms step_avg:125.95ms
step:751/1393 train_time:93228ms step_avg:125.81ms
step:752/1393 train_time:93348ms step_avg:125.81ms
step:753/1393 train_time:93479ms step_avg:125.81ms
step:754/1393 train_time:93610ms step_avg:125.82ms
step:755/1393 train_time:93741ms step_avg:125.83ms
step:756/1393 train_time:93871ms step_avg:125.83ms
step:757/1393 train_time:94002ms step_avg:125.84ms
step:758/1393 train_time:94132ms step_avg:125.84ms
step:759/1393 train_time:94262ms step_avg:125.85ms
step:760/1393 train_time:94392ms step_avg:125.86ms
step:761/1393 train_time:94523ms step_avg:125.86ms
step:762/1393 train_time:94654ms step_avg:125.87ms
step:763/1393 train_time:94785ms step_avg:125.88ms
step:764/1393 train_time:94915ms step_avg:125.88ms
step:765/1393 train_time:95046ms step_avg:125.89ms
step:766/1393 train_time:95177ms step_avg:125.90ms
step:767/1393 train_time:95308ms step_avg:125.90ms
step:768/1393 train_time:95439ms step_avg:125.91ms
step:769/1393 train_time:95571ms step_avg:125.92ms
step:770/1393 train_time:95701ms step_avg:125.92ms
step:771/1393 train_time:95832ms step_avg:125.93ms
step:772/1393 train_time:95961ms step_avg:125.93ms
step:773/1393 train_time:96092ms step_avg:125.94ms
step:774/1393 train_time:96223ms step_avg:125.95ms
step:775/1393 train_time:96353ms step_avg:125.95ms
step:776/1393 train_time:96484ms step_avg:125.96ms
step:777/1393 train_time:96616ms step_avg:125.97ms
step:778/1393 train_time:96747ms step_avg:125.97ms
step:779/1393 train_time:96878ms step_avg:125.98ms
step:780/1393 train_time:97009ms step_avg:125.99ms
step:781/1393 train_time:97139ms step_avg:125.99ms
step:782/1393 train_time:97270ms step_avg:126.00ms
step:783/1393 train_time:97401ms step_avg:126.00ms
step:784/1393 train_time:97531ms step_avg:126.01ms
step:785/1393 train_time:97662ms step_avg:126.02ms
step:786/1393 train_time:97792ms step_avg:126.02ms
step:787/1393 train_time:97922ms step_avg:126.03ms
step:788/1393 train_time:98053ms step_avg:126.03ms
step:789/1393 train_time:98182ms step_avg:126.04ms
step:790/1393 train_time:98312ms step_avg:126.04ms
step:791/1393 train_time:98442ms step_avg:126.05ms
step:792/1393 train_time:98573ms step_avg:126.05ms
step:793/1393 train_time:98702ms step_avg:126.06ms
step:794/1393 train_time:98832ms step_avg:126.06ms
step:795/1393 train_time:98964ms step_avg:126.07ms
step:796/1393 train_time:99095ms step_avg:126.08ms
step:797/1393 train_time:99226ms step_avg:126.08ms
step:798/1393 train_time:99357ms step_avg:126.09ms
step:799/1393 train_time:99489ms step_avg:126.10ms
step:800/1393 train_time:99620ms step_avg:126.10ms
step:801/1393 train_time:99750ms step_avg:126.11ms
step:802/1393 train_time:99880ms step_avg:126.11ms
step:803/1393 train_time:100010ms step_avg:126.12ms
step:804/1393 train_time:100141ms step_avg:126.12ms
step:805/1393 train_time:100273ms step_avg:126.13ms
step:806/1393 train_time:100404ms step_avg:126.14ms
step:807/1393 train_time:100536ms step_avg:126.14ms
step:808/1393 train_time:100666ms step_avg:126.15ms
step:809/1393 train_time:100796ms step_avg:126.15ms
step:810/1393 train_time:100927ms step_avg:126.16ms
step:811/1393 train_time:101058ms step_avg:126.16ms
step:812/1393 train_time:101189ms step_avg:126.17ms
step:813/1393 train_time:101320ms step_avg:126.18ms
step:814/1393 train_time:101449ms step_avg:126.18ms
step:815/1393 train_time:101580ms step_avg:126.19ms
step:816/1393 train_time:101711ms step_avg:126.19ms
step:817/1393 train_time:101841ms step_avg:126.20ms
step:818/1393 train_time:101972ms step_avg:126.20ms
step:819/1393 train_time:102103ms step_avg:126.21ms
step:820/1393 train_time:102233ms step_avg:126.21ms
step:821/1393 train_time:102365ms step_avg:126.22ms
step:822/1393 train_time:102495ms step_avg:126.23ms
step:823/1393 train_time:102625ms step_avg:126.23ms
step:824/1393 train_time:102756ms step_avg:126.24ms
step:825/1393 train_time:102887ms step_avg:126.24ms
step:826/1393 train_time:103018ms step_avg:126.25ms
step:827/1393 train_time:103149ms step_avg:126.25ms
step:828/1393 train_time:103280ms step_avg:126.26ms
step:829/1393 train_time:103412ms step_avg:126.27ms
step:830/1393 train_time:103544ms step_avg:126.27ms
step:831/1393 train_time:103674ms step_avg:126.28ms
step:832/1393 train_time:103806ms step_avg:126.28ms
step:833/1393 train_time:103937ms step_avg:126.29ms
step:834/1393 train_time:104069ms step_avg:126.30ms
step:835/1393 train_time:104200ms step_avg:126.30ms
step:836/1393 train_time:104331ms step_avg:126.31ms
step:837/1393 train_time:104462ms step_avg:126.31ms
step:838/1393 train_time:104592ms step_avg:126.32ms
step:839/1393 train_time:104722ms step_avg:126.32ms
step:840/1393 train_time:104853ms step_avg:126.33ms
step:841/1393 train_time:104983ms step_avg:126.33ms
step:842/1393 train_time:105114ms step_avg:126.34ms
step:843/1393 train_time:105245ms step_avg:126.34ms
step:844/1393 train_time:105377ms step_avg:126.35ms
step:845/1393 train_time:105507ms step_avg:126.36ms
step:846/1393 train_time:105638ms step_avg:126.36ms
step:847/1393 train_time:105771ms step_avg:126.37ms
step:848/1393 train_time:105902ms step_avg:126.37ms
step:849/1393 train_time:106033ms step_avg:126.38ms
step:850/1393 train_time:106164ms step_avg:126.39ms
step:851/1393 train_time:106295ms step_avg:126.39ms
step:852/1393 train_time:106426ms step_avg:126.40ms
step:853/1393 train_time:106557ms step_avg:126.40ms
step:854/1393 train_time:106687ms step_avg:126.41ms
step:855/1393 train_time:106818ms step_avg:126.41ms
step:856/1393 train_time:106949ms step_avg:126.42ms
step:857/1393 train_time:107079ms step_avg:126.42ms
step:858/1393 train_time:107212ms step_avg:126.43ms
step:859/1393 train_time:107343ms step_avg:126.44ms
step:860/1393 train_time:107474ms step_avg:126.44ms
step:861/1393 train_time:107605ms step_avg:126.45ms
step:862/1393 train_time:107737ms step_avg:126.45ms
step:863/1393 train_time:107869ms step_avg:126.46ms
step:864/1393 train_time:108000ms step_avg:126.46ms
step:865/1393 train_time:108132ms step_avg:126.47ms
step:866/1393 train_time:108264ms step_avg:126.48ms
step:867/1393 train_time:108395ms step_avg:126.48ms
step:868/1393 train_time:108525ms step_avg:126.49ms
step:869/1393 train_time:108656ms step_avg:126.49ms
step:870/1393 train_time:108787ms step_avg:126.50ms
step:871/1393 train_time:108918ms step_avg:126.50ms
step:872/1393 train_time:109048ms step_avg:126.51ms
step:873/1393 train_time:109179ms step_avg:126.51ms
step:874/1393 train_time:109311ms step_avg:126.52ms
step:875/1393 train_time:109445ms step_avg:126.53ms
step:875/1393 val_loss:3.4786 train_time:109575ms step_avg:126.68ms
step:876/1393 train_time:109597ms step_avg:126.55ms
step:877/1393 train_time:109718ms step_avg:126.55ms
step:878/1393 train_time:109850ms step_avg:126.55ms
step:879/1393 train_time:109980ms step_avg:126.56ms
step:880/1393 train_time:110110ms step_avg:126.56ms
step:881/1393 train_time:110241ms step_avg:126.57ms
step:882/1393 train_time:110371ms step_avg:126.57ms
step:883/1393 train_time:110502ms step_avg:126.58ms
step:884/1393 train_time:110634ms step_avg:126.58ms
step:885/1393 train_time:110766ms step_avg:126.59ms
step:886/1393 train_time:110898ms step_avg:126.60ms
step:887/1393 train_time:111028ms step_avg:126.60ms
step:888/1393 train_time:111159ms step_avg:126.61ms
step:889/1393 train_time:111293ms step_avg:126.61ms
step:890/1393 train_time:111423ms step_avg:126.62ms
step:891/1393 train_time:111554ms step_avg:126.62ms
step:892/1393 train_time:111685ms step_avg:126.63ms
step:893/1393 train_time:111818ms step_avg:126.63ms
step:894/1393 train_time:111948ms step_avg:126.64ms
step:895/1393 train_time:112080ms step_avg:126.64ms
step:896/1393 train_time:112210ms step_avg:126.65ms
step:897/1393 train_time:112342ms step_avg:126.65ms
step:898/1393 train_time:112473ms step_avg:126.66ms
step:899/1393 train_time:112605ms step_avg:126.66ms
step:900/1393 train_time:112736ms step_avg:126.67ms
step:901/1393 train_time:112866ms step_avg:126.67ms
step:902/1393 train_time:112998ms step_avg:126.68ms
step:903/1393 train_time:113129ms step_avg:126.68ms
step:904/1393 train_time:113260ms step_avg:126.69ms
step:905/1393 train_time:113391ms step_avg:126.69ms
step:906/1393 train_time:113521ms step_avg:126.70ms
step:907/1393 train_time:113652ms step_avg:126.70ms
step:908/1393 train_time:113783ms step_avg:126.71ms
step:909/1393 train_time:113914ms step_avg:126.71ms
step:910/1393 train_time:114047ms step_avg:126.72ms
step:911/1393 train_time:114178ms step_avg:126.72ms
step:912/1393 train_time:114308ms step_avg:126.73ms
step:913/1393 train_time:114439ms step_avg:126.73ms
step:914/1393 train_time:114570ms step_avg:126.74ms
step:915/1393 train_time:114701ms step_avg:126.74ms
step:916/1393 train_time:114833ms step_avg:126.75ms
step:917/1393 train_time:114964ms step_avg:126.75ms
step:918/1393 train_time:115095ms step_avg:126.76ms
step:919/1393 train_time:115228ms step_avg:126.76ms
step:920/1393 train_time:115359ms step_avg:126.77ms
step:921/1393 train_time:115490ms step_avg:126.77ms
step:922/1393 train_time:115622ms step_avg:126.78ms
step:923/1393 train_time:115752ms step_avg:126.78ms
step:924/1393 train_time:115882ms step_avg:126.79ms
step:925/1393 train_time:116013ms step_avg:126.79ms
step:926/1393 train_time:116144ms step_avg:126.79ms
step:927/1393 train_time:116275ms step_avg:126.80ms
step:928/1393 train_time:116406ms step_avg:126.80ms
step:929/1393 train_time:116537ms step_avg:126.81ms
step:930/1393 train_time:116668ms step_avg:126.81ms
step:931/1393 train_time:116800ms step_avg:126.82ms
step:932/1393 train_time:116933ms step_avg:126.83ms
step:933/1393 train_time:117066ms step_avg:126.83ms
step:934/1393 train_time:117199ms step_avg:126.84ms
step:935/1393 train_time:117331ms step_avg:126.84ms
step:936/1393 train_time:117465ms step_avg:126.85ms
step:937/1393 train_time:117600ms step_avg:126.86ms
step:938/1393 train_time:117733ms step_avg:126.87ms
step:939/1393 train_time:117866ms step_avg:126.87ms
step:940/1393 train_time:118000ms step_avg:126.88ms
step:941/1393 train_time:118132ms step_avg:126.89ms
step:942/1393 train_time:118264ms step_avg:126.89ms
step:943/1393 train_time:118398ms step_avg:126.90ms
step:944/1393 train_time:118532ms step_avg:126.91ms
step:945/1393 train_time:118665ms step_avg:126.91ms
step:946/1393 train_time:118798ms step_avg:126.92ms
step:947/1393 train_time:118932ms step_avg:126.93ms
step:948/1393 train_time:119065ms step_avg:126.94ms
step:949/1393 train_time:119199ms step_avg:126.94ms
step:950/1393 train_time:119331ms step_avg:126.95ms
step:951/1393 train_time:119465ms step_avg:126.96ms
step:952/1393 train_time:119598ms step_avg:126.96ms
step:953/1393 train_time:119731ms step_avg:126.97ms
step:954/1393 train_time:119863ms step_avg:126.97ms
step:955/1393 train_time:119996ms step_avg:126.98ms
step:956/1393 train_time:120131ms step_avg:126.99ms
step:957/1393 train_time:120264ms step_avg:126.99ms
step:958/1393 train_time:120397ms step_avg:127.00ms
step:959/1393 train_time:120529ms step_avg:127.01ms
step:960/1393 train_time:120662ms step_avg:127.01ms
step:961/1393 train_time:120795ms step_avg:127.02ms
step:962/1393 train_time:120928ms step_avg:127.02ms
step:963/1393 train_time:121060ms step_avg:127.03ms
step:964/1393 train_time:121194ms step_avg:127.04ms
step:965/1393 train_time:121327ms step_avg:127.04ms
step:966/1393 train_time:121459ms step_avg:127.05ms
step:967/1393 train_time:121592ms step_avg:127.05ms
step:968/1393 train_time:121724ms step_avg:127.06ms
step:969/1393 train_time:121859ms step_avg:127.07ms
step:970/1393 train_time:121992ms step_avg:127.07ms
step:971/1393 train_time:122125ms step_avg:127.08ms
step:972/1393 train_time:122258ms step_avg:127.09ms
step:973/1393 train_time:122392ms step_avg:127.09ms
step:974/1393 train_time:122524ms step_avg:127.10ms
step:975/1393 train_time:122656ms step_avg:127.10ms
step:976/1393 train_time:122788ms step_avg:127.11ms
step:977/1393 train_time:122921ms step_avg:127.12ms
step:978/1393 train_time:123053ms step_avg:127.12ms
step:979/1393 train_time:123186ms step_avg:127.13ms
step:980/1393 train_time:123319ms step_avg:127.13ms
step:981/1393 train_time:123451ms step_avg:127.14ms
step:982/1393 train_time:123583ms step_avg:127.14ms
step:983/1393 train_time:123715ms step_avg:127.15ms
step:984/1393 train_time:123848ms step_avg:127.15ms
step:985/1393 train_time:123980ms step_avg:127.16ms
step:986/1393 train_time:124116ms step_avg:127.17ms
step:987/1393 train_time:124249ms step_avg:127.17ms
step:988/1393 train_time:124381ms step_avg:127.18ms
step:989/1393 train_time:124513ms step_avg:127.18ms
step:990/1393 train_time:124647ms step_avg:127.19ms
step:991/1393 train_time:124779ms step_avg:127.20ms
step:992/1393 train_time:124912ms step_avg:127.20ms
step:993/1393 train_time:125046ms step_avg:127.21ms
step:994/1393 train_time:125178ms step_avg:127.21ms
step:995/1393 train_time:125310ms step_avg:127.22ms
step:996/1393 train_time:125443ms step_avg:127.22ms
step:997/1393 train_time:125574ms step_avg:127.23ms
step:998/1393 train_time:125705ms step_avg:127.23ms
step:999/1393 train_time:125837ms step_avg:127.24ms
step:1000/1393 train_time:125970ms step_avg:127.24ms
step:1000/1393 val_loss:3.4157 train_time:126101ms step_avg:127.38ms
step:1001/1393 train_time:126123ms step_avg:127.27ms
step:1002/1393 train_time:126243ms step_avg:127.26ms
step:1003/1393 train_time:126375ms step_avg:127.27ms
step:1004/1393 train_time:126507ms step_avg:127.27ms
step:1005/1393 train_time:126640ms step_avg:127.28ms
step:1006/1393 train_time:126772ms step_avg:127.28ms
step:1007/1393 train_time:126905ms step_avg:127.29ms
step:1008/1393 train_time:127037ms step_avg:127.29ms
step:1009/1393 train_time:127172ms step_avg:127.30ms
step:1010/1393 train_time:127305ms step_avg:127.31ms
step:1011/1393 train_time:127438ms step_avg:127.31ms
step:1012/1393 train_time:127571ms step_avg:127.32ms
step:1013/1393 train_time:127704ms step_avg:127.32ms
step:1014/1393 train_time:127836ms step_avg:127.33ms
step:1015/1393 train_time:127968ms step_avg:127.33ms
step:1016/1393 train_time:128101ms step_avg:127.34ms
step:1017/1393 train_time:128233ms step_avg:127.34ms
step:1018/1393 train_time:128366ms step_avg:127.35ms
step:1019/1393 train_time:128498ms step_avg:127.35ms
step:1020/1393 train_time:128631ms step_avg:127.36ms
step:1021/1393 train_time:128763ms step_avg:127.36ms
step:1022/1393 train_time:128894ms step_avg:127.37ms
step:1023/1393 train_time:129028ms step_avg:127.37ms
step:1024/1393 train_time:129161ms step_avg:127.38ms
step:1025/1393 train_time:129295ms step_avg:127.38ms
step:1026/1393 train_time:129427ms step_avg:127.39ms
step:1027/1393 train_time:129559ms step_avg:127.39ms
step:1028/1393 train_time:129692ms step_avg:127.40ms
step:1029/1393 train_time:129825ms step_avg:127.40ms
step:1030/1393 train_time:129958ms step_avg:127.41ms
step:1031/1393 train_time:130090ms step_avg:127.41ms
step:1032/1393 train_time:130222ms step_avg:127.42ms
step:1033/1393 train_time:130354ms step_avg:127.42ms
step:1034/1393 train_time:130488ms step_avg:127.43ms
step:1035/1393 train_time:130621ms step_avg:127.44ms
step:1036/1393 train_time:130754ms step_avg:127.44ms
step:1037/1393 train_time:130888ms step_avg:127.45ms
step:1038/1393 train_time:131021ms step_avg:127.45ms
step:1039/1393 train_time:131153ms step_avg:127.46ms
step:1040/1393 train_time:131285ms step_avg:127.46ms
step:1041/1393 train_time:131418ms step_avg:127.47ms
step:1042/1393 train_time:131550ms step_avg:127.47ms
step:1043/1393 train_time:131683ms step_avg:127.48ms
step:1044/1393 train_time:131818ms step_avg:127.48ms
step:1045/1393 train_time:131951ms step_avg:127.49ms
step:1046/1393 train_time:132084ms step_avg:127.49ms
step:1047/1393 train_time:132216ms step_avg:127.50ms
step:1048/1393 train_time:132348ms step_avg:127.50ms
step:1049/1393 train_time:132481ms step_avg:127.51ms
step:1050/1393 train_time:132614ms step_avg:127.51ms
step:1051/1393 train_time:132747ms step_avg:127.52ms
step:1052/1393 train_time:132880ms step_avg:127.52ms
step:1053/1393 train_time:133013ms step_avg:127.53ms
step:1054/1393 train_time:133145ms step_avg:127.53ms
step:1055/1393 train_time:133279ms step_avg:127.54ms
step:1056/1393 train_time:133411ms step_avg:127.54ms
step:1057/1393 train_time:133544ms step_avg:127.55ms
step:1058/1393 train_time:133678ms step_avg:127.56ms
step:1059/1393 train_time:133812ms step_avg:127.56ms
step:1060/1393 train_time:133946ms step_avg:127.57ms
step:1061/1393 train_time:134078ms step_avg:127.57ms
step:1062/1393 train_time:134212ms step_avg:127.58ms
step:1063/1393 train_time:134345ms step_avg:127.58ms
step:1064/1393 train_time:134478ms step_avg:127.59ms
step:1065/1393 train_time:134612ms step_avg:127.59ms
step:1066/1393 train_time:134746ms step_avg:127.60ms
step:1067/1393 train_time:134878ms step_avg:127.60ms
step:1068/1393 train_time:135010ms step_avg:127.61ms
step:1069/1393 train_time:135144ms step_avg:127.61ms
step:1070/1393 train_time:135277ms step_avg:127.62ms
step:1071/1393 train_time:135412ms step_avg:127.63ms
step:1072/1393 train_time:135544ms step_avg:127.63ms
step:1073/1393 train_time:135677ms step_avg:127.64ms
step:1074/1393 train_time:135809ms step_avg:127.64ms
step:1075/1393 train_time:135942ms step_avg:127.65ms
step:1076/1393 train_time:136074ms step_avg:127.65ms
step:1077/1393 train_time:136206ms step_avg:127.65ms
step:1078/1393 train_time:136339ms step_avg:127.66ms
step:1079/1393 train_time:136475ms step_avg:127.67ms
step:1080/1393 train_time:136608ms step_avg:127.67ms
step:1081/1393 train_time:136741ms step_avg:127.68ms
step:1082/1393 train_time:136873ms step_avg:127.68ms
step:1083/1393 train_time:137005ms step_avg:127.68ms
step:1084/1393 train_time:137138ms step_avg:127.69ms
step:1085/1393 train_time:137271ms step_avg:127.69ms
step:1086/1393 train_time:137404ms step_avg:127.70ms
step:1087/1393 train_time:137538ms step_avg:127.70ms
step:1088/1393 train_time:137671ms step_avg:127.71ms
step:1089/1393 train_time:137806ms step_avg:127.72ms
step:1090/1393 train_time:137939ms step_avg:127.72ms
step:1091/1393 train_time:138071ms step_avg:127.73ms
step:1092/1393 train_time:138204ms step_avg:127.73ms
step:1093/1393 train_time:138337ms step_avg:127.74ms
step:1094/1393 train_time:138470ms step_avg:127.74ms
step:1095/1393 train_time:138602ms step_avg:127.74ms
step:1096/1393 train_time:138736ms step_avg:127.75ms
step:1097/1393 train_time:138869ms step_avg:127.75ms
step:1098/1393 train_time:139004ms step_avg:127.76ms
step:1099/1393 train_time:139136ms step_avg:127.77ms
step:1100/1393 train_time:139270ms step_avg:127.77ms
step:1101/1393 train_time:139402ms step_avg:127.77ms
step:1102/1393 train_time:139535ms step_avg:127.78ms
step:1103/1393 train_time:139669ms step_avg:127.79ms
step:1104/1393 train_time:139803ms step_avg:127.79ms
step:1105/1393 train_time:139937ms step_avg:127.80ms
step:1106/1393 train_time:140070ms step_avg:127.80ms
step:1107/1393 train_time:140203ms step_avg:127.81ms
step:1108/1393 train_time:140337ms step_avg:127.81ms
step:1109/1393 train_time:140469ms step_avg:127.82ms
step:1110/1393 train_time:140602ms step_avg:127.82ms
step:1111/1393 train_time:140735ms step_avg:127.83ms
step:1112/1393 train_time:140869ms step_avg:127.83ms
step:1113/1393 train_time:141001ms step_avg:127.83ms
step:1114/1393 train_time:141135ms step_avg:127.84ms
step:1115/1393 train_time:141268ms step_avg:127.84ms
step:1116/1393 train_time:141401ms step_avg:127.85ms
step:1117/1393 train_time:141533ms step_avg:127.85ms
step:1118/1393 train_time:141668ms step_avg:127.86ms
step:1119/1393 train_time:141801ms step_avg:127.86ms
step:1120/1393 train_time:141933ms step_avg:127.87ms
step:1121/1393 train_time:142066ms step_avg:127.87ms
step:1122/1393 train_time:142198ms step_avg:127.88ms
step:1123/1393 train_time:142331ms step_avg:127.88ms
step:1124/1393 train_time:142466ms step_avg:127.89ms
step:1125/1393 train_time:142599ms step_avg:127.89ms
step:1125/1393 val_loss:3.3632 train_time:142732ms step_avg:128.01ms
step:1126/1393 train_time:142753ms step_avg:127.92ms
step:1127/1393 train_time:142872ms step_avg:127.91ms
step:1128/1393 train_time:143006ms step_avg:127.91ms
step:1129/1393 train_time:143139ms step_avg:127.92ms
step:1130/1393 train_time:143271ms step_avg:127.92ms
step:1131/1393 train_time:143404ms step_avg:127.92ms
step:1132/1393 train_time:143537ms step_avg:127.93ms
step:1133/1393 train_time:143668ms step_avg:127.93ms
step:1134/1393 train_time:143804ms step_avg:127.94ms
step:1135/1393 train_time:143938ms step_avg:127.94ms
step:1136/1393 train_time:144073ms step_avg:127.95ms
step:1137/1393 train_time:144205ms step_avg:127.95ms
step:1138/1393 train_time:144340ms step_avg:127.96ms
step:1139/1393 train_time:144474ms step_avg:127.97ms
step:1140/1393 train_time:144608ms step_avg:127.97ms
step:1141/1393 train_time:144743ms step_avg:127.98ms
step:1142/1393 train_time:144878ms step_avg:127.98ms
step:1143/1393 train_time:145013ms step_avg:127.99ms
step:1144/1393 train_time:145148ms step_avg:128.00ms
step:1145/1393 train_time:145281ms step_avg:128.00ms
step:1146/1393 train_time:145415ms step_avg:128.01ms
step:1147/1393 train_time:145550ms step_avg:128.01ms
step:1148/1393 train_time:145684ms step_avg:128.02ms
step:1149/1393 train_time:145817ms step_avg:128.02ms
step:1150/1393 train_time:145951ms step_avg:128.03ms
step:1151/1393 train_time:146087ms step_avg:128.03ms
step:1152/1393 train_time:146221ms step_avg:128.04ms
step:1153/1393 train_time:146357ms step_avg:128.05ms
step:1154/1393 train_time:146491ms step_avg:128.05ms
step:1155/1393 train_time:146626ms step_avg:128.06ms
step:1156/1393 train_time:146763ms step_avg:128.07ms
step:1157/1393 train_time:146897ms step_avg:128.07ms
step:1158/1393 train_time:147031ms step_avg:128.08ms
step:1159/1393 train_time:147164ms step_avg:128.08ms
step:1160/1393 train_time:147297ms step_avg:128.08ms
step:1161/1393 train_time:147431ms step_avg:128.09ms
step:1162/1393 train_time:147567ms step_avg:128.10ms
step:1163/1393 train_time:147702ms step_avg:128.10ms
step:1164/1393 train_time:147836ms step_avg:128.11ms
step:1165/1393 train_time:147972ms step_avg:128.11ms
step:1166/1393 train_time:148105ms step_avg:128.12ms
step:1167/1393 train_time:148238ms step_avg:128.12ms
step:1168/1393 train_time:148373ms step_avg:128.13ms
step:1169/1393 train_time:148507ms step_avg:128.13ms
step:1170/1393 train_time:148641ms step_avg:128.14ms
step:1171/1393 train_time:148773ms step_avg:128.14ms
step:1172/1393 train_time:148908ms step_avg:128.15ms
step:1173/1393 train_time:149043ms step_avg:128.15ms
step:1174/1393 train_time:149182ms step_avg:128.16ms
step:1175/1393 train_time:149316ms step_avg:128.17ms
step:1176/1393 train_time:149452ms step_avg:128.17ms
step:1177/1393 train_time:149589ms step_avg:128.18ms
step:1178/1393 train_time:149722ms step_avg:128.19ms
step:1179/1393 train_time:149855ms step_avg:128.19ms
step:1180/1393 train_time:149991ms step_avg:128.20ms
step:1181/1393 train_time:150127ms step_avg:128.20ms
step:1182/1393 train_time:150260ms step_avg:128.21ms
step:1183/1393 train_time:150395ms step_avg:128.21ms
step:1184/1393 train_time:150530ms step_avg:128.22ms
step:1185/1393 train_time:150665ms step_avg:128.23ms
step:1186/1393 train_time:150799ms step_avg:128.23ms
step:1187/1393 train_time:150939ms step_avg:128.24ms
step:1188/1393 train_time:151073ms step_avg:128.25ms
step:1189/1393 train_time:151209ms step_avg:128.25ms
step:1190/1393 train_time:151342ms step_avg:128.26ms
step:1191/1393 train_time:151476ms step_avg:128.26ms
step:1192/1393 train_time:151611ms step_avg:128.27ms
step:1193/1393 train_time:151745ms step_avg:128.27ms
step:1194/1393 train_time:151879ms step_avg:128.28ms
step:1195/1393 train_time:152013ms step_avg:128.28ms
step:1196/1393 train_time:152147ms step_avg:128.29ms
step:1197/1393 train_time:152282ms step_avg:128.29ms
step:1198/1393 train_time:152418ms step_avg:128.30ms
step:1199/1393 train_time:152551ms step_avg:128.30ms
step:1200/1393 train_time:152685ms step_avg:128.31ms
step:1201/1393 train_time:152819ms step_avg:128.31ms
step:1202/1393 train_time:152958ms step_avg:128.32ms
step:1203/1393 train_time:153095ms step_avg:128.33ms
step:1204/1393 train_time:153229ms step_avg:128.33ms
step:1205/1393 train_time:153364ms step_avg:128.34ms
step:1206/1393 train_time:153500ms step_avg:128.34ms
step:1207/1393 train_time:153634ms step_avg:128.35ms
step:1208/1393 train_time:153769ms step_avg:128.36ms
step:1209/1393 train_time:153903ms step_avg:128.36ms
step:1210/1393 train_time:154038ms step_avg:128.37ms
step:1211/1393 train_time:154173ms step_avg:128.37ms
step:1212/1393 train_time:154306ms step_avg:128.37ms
step:1213/1393 train_time:154441ms step_avg:128.38ms
step:1214/1393 train_time:154576ms step_avg:128.39ms
step:1215/1393 train_time:154712ms step_avg:128.39ms
step:1216/1393 train_time:154844ms step_avg:128.39ms
step:1217/1393 train_time:154978ms step_avg:128.40ms
step:1218/1393 train_time:155111ms step_avg:128.40ms
step:1219/1393 train_time:155244ms step_avg:128.41ms
step:1220/1393 train_time:155378ms step_avg:128.41ms
step:1221/1393 train_time:155512ms step_avg:128.42ms
step:1222/1393 train_time:155646ms step_avg:128.42ms
step:1223/1393 train_time:155780ms step_avg:128.43ms
step:1224/1393 train_time:155914ms step_avg:128.43ms
step:1225/1393 train_time:156050ms step_avg:128.44ms
step:1226/1393 train_time:156184ms step_avg:128.44ms
step:1227/1393 train_time:156318ms step_avg:128.45ms
step:1228/1393 train_time:156453ms step_avg:128.45ms
step:1229/1393 train_time:156586ms step_avg:128.45ms
step:1230/1393 train_time:156722ms step_avg:128.46ms
step:1231/1393 train_time:156857ms step_avg:128.47ms
step:1232/1393 train_time:156993ms step_avg:128.47ms
step:1233/1393 train_time:157128ms step_avg:128.48ms
step:1234/1393 train_time:157262ms step_avg:128.48ms
step:1235/1393 train_time:157395ms step_avg:128.49ms
step:1236/1393 train_time:157531ms step_avg:128.49ms
step:1237/1393 train_time:157665ms step_avg:128.50ms
step:1238/1393 train_time:157802ms step_avg:128.50ms
step:1239/1393 train_time:157936ms step_avg:128.51ms
step:1240/1393 train_time:158071ms step_avg:128.51ms
step:1241/1393 train_time:158208ms step_avg:128.52ms
step:1242/1393 train_time:158342ms step_avg:128.52ms
step:1243/1393 train_time:158476ms step_avg:128.53ms
step:1244/1393 train_time:158612ms step_avg:128.53ms
step:1245/1393 train_time:158746ms step_avg:128.54ms
step:1246/1393 train_time:158880ms step_avg:128.54ms
step:1247/1393 train_time:159015ms step_avg:128.55ms
step:1248/1393 train_time:159148ms step_avg:128.55ms
step:1249/1393 train_time:159280ms step_avg:128.56ms
step:1250/1393 train_time:159415ms step_avg:128.56ms
step:1250/1393 val_loss:3.3169 train_time:159549ms step_avg:128.67ms
step:1251/1393 train_time:159570ms step_avg:128.58ms
step:1252/1393 train_time:159699ms step_avg:128.58ms
step:1253/1393 train_time:159832ms step_avg:128.59ms
step:1254/1393 train_time:159964ms step_avg:128.59ms
step:1255/1393 train_time:160105ms step_avg:128.60ms
step:1256/1393 train_time:160237ms step_avg:128.60ms
step:1257/1393 train_time:160371ms step_avg:128.61ms
step:1258/1393 train_time:160505ms step_avg:128.61ms
step:1259/1393 train_time:160643ms step_avg:128.62ms
step:1260/1393 train_time:160776ms step_avg:128.62ms
step:1261/1393 train_time:160911ms step_avg:128.63ms
step:1262/1393 train_time:161047ms step_avg:128.63ms
step:1263/1393 train_time:161183ms step_avg:128.64ms
step:1264/1393 train_time:161317ms step_avg:128.64ms
step:1265/1393 train_time:161451ms step_avg:128.65ms
step:1266/1393 train_time:161586ms step_avg:128.65ms
step:1267/1393 train_time:161723ms step_avg:128.66ms
step:1268/1393 train_time:161858ms step_avg:128.66ms
step:1269/1393 train_time:161994ms step_avg:128.67ms
step:1270/1393 train_time:162129ms step_avg:128.67ms
step:1271/1393 train_time:162263ms step_avg:128.68ms
step:1272/1393 train_time:162396ms step_avg:128.68ms
step:1273/1393 train_time:162529ms step_avg:128.68ms
step:1274/1393 train_time:162663ms step_avg:128.69ms
step:1275/1393 train_time:162798ms step_avg:128.69ms
step:1276/1393 train_time:162932ms step_avg:128.70ms
step:1277/1393 train_time:163066ms step_avg:128.70ms
step:1278/1393 train_time:163201ms step_avg:128.71ms
step:1279/1393 train_time:163336ms step_avg:128.71ms
step:1280/1393 train_time:163474ms step_avg:128.72ms
step:1281/1393 train_time:163609ms step_avg:128.72ms
step:1282/1393 train_time:163742ms step_avg:128.73ms
step:1283/1393 train_time:163877ms step_avg:128.73ms
step:1284/1393 train_time:164011ms step_avg:128.74ms
step:1285/1393 train_time:164145ms step_avg:128.74ms
step:1286/1393 train_time:164281ms step_avg:128.75ms
step:1287/1393 train_time:164415ms step_avg:128.75ms
step:1288/1393 train_time:164550ms step_avg:128.76ms
step:1289/1393 train_time:164686ms step_avg:128.76ms
step:1290/1393 train_time:164823ms step_avg:128.77ms
step:1291/1393 train_time:164958ms step_avg:128.77ms
step:1292/1393 train_time:165094ms step_avg:128.78ms
step:1293/1393 train_time:165233ms step_avg:128.79ms
step:1294/1393 train_time:165367ms step_avg:128.79ms
step:1295/1393 train_time:165502ms step_avg:128.80ms
step:1296/1393 train_time:165638ms step_avg:128.80ms
step:1297/1393 train_time:165773ms step_avg:128.81ms
step:1298/1393 train_time:165906ms step_avg:128.81ms
step:1299/1393 train_time:166040ms step_avg:128.81ms
step:1300/1393 train_time:166174ms step_avg:128.82ms
step:1301/1393 train_time:166308ms step_avg:128.82ms
step:1302/1393 train_time:166442ms step_avg:128.82ms
step:1303/1393 train_time:166577ms step_avg:128.83ms
step:1304/1393 train_time:166712ms step_avg:128.83ms
step:1305/1393 train_time:166847ms step_avg:128.84ms
step:1306/1393 train_time:166982ms step_avg:128.84ms
step:1307/1393 train_time:167117ms step_avg:128.85ms
step:1308/1393 train_time:167254ms step_avg:128.86ms
step:1309/1393 train_time:167390ms step_avg:128.86ms
step:1310/1393 train_time:167524ms step_avg:128.86ms
step:1311/1393 train_time:167658ms step_avg:128.87ms
step:1312/1393 train_time:167792ms step_avg:128.87ms
step:1313/1393 train_time:167927ms step_avg:128.88ms
step:1314/1393 train_time:168061ms step_avg:128.88ms
step:1315/1393 train_time:168196ms step_avg:128.89ms
step:1316/1393 train_time:168330ms step_avg:128.89ms
step:1317/1393 train_time:168465ms step_avg:128.89ms
step:1318/1393 train_time:168600ms step_avg:128.90ms
step:1319/1393 train_time:168737ms step_avg:128.91ms
step:1320/1393 train_time:168870ms step_avg:128.91ms
step:1321/1393 train_time:169004ms step_avg:128.91ms
step:1322/1393 train_time:169143ms step_avg:128.92ms
step:1323/1393 train_time:169277ms step_avg:128.92ms
step:1324/1393 train_time:169411ms step_avg:128.93ms
step:1325/1393 train_time:169547ms step_avg:128.93ms
step:1326/1393 train_time:169681ms step_avg:128.94ms
step:1327/1393 train_time:169816ms step_avg:128.94ms
step:1328/1393 train_time:169949ms step_avg:128.94ms
step:1329/1393 train_time:170089ms step_avg:128.95ms
step:1330/1393 train_time:170224ms step_avg:128.96ms
step:1331/1393 train_time:170362ms step_avg:128.96ms
step:1332/1393 train_time:170498ms step_avg:128.97ms
step:1333/1393 train_time:170634ms step_avg:128.97ms
step:1334/1393 train_time:170768ms step_avg:128.98ms
step:1335/1393 train_time:170902ms step_avg:128.98ms
step:1336/1393 train_time:171040ms step_avg:128.99ms
step:1337/1393 train_time:171175ms step_avg:128.99ms
step:1338/1393 train_time:171310ms step_avg:129.00ms
step:1339/1393 train_time:171445ms step_avg:129.00ms
step:1340/1393 train_time:171580ms step_avg:129.01ms
step:1341/1393 train_time:171714ms step_avg:129.01ms
step:1342/1393 train_time:171848ms step_avg:129.01ms
step:1343/1393 train_time:171983ms step_avg:129.02ms
step:1344/1393 train_time:172118ms step_avg:129.02ms
step:1345/1393 train_time:172254ms step_avg:129.03ms
step:1346/1393 train_time:172389ms step_avg:129.03ms
step:1347/1393 train_time:172525ms step_avg:129.04ms
step:1348/1393 train_time:172660ms step_avg:129.04ms
step:1349/1393 train_time:172795ms step_avg:129.05ms
step:1350/1393 train_time:172929ms step_avg:129.05ms
step:1351/1393 train_time:173065ms step_avg:129.06ms
step:1352/1393 train_time:173203ms step_avg:129.06ms
step:1353/1393 train_time:173341ms step_avg:129.07ms
step:1354/1393 train_time:173476ms step_avg:129.07ms
step:1355/1393 train_time:173611ms step_avg:129.08ms
step:1356/1393 train_time:173745ms step_avg:129.08ms
step:1357/1393 train_time:173881ms step_avg:129.09ms
step:1358/1393 train_time:174018ms step_avg:129.09ms
step:1359/1393 train_time:174155ms step_avg:129.10ms
step:1360/1393 train_time:174292ms step_avg:129.11ms
step:1361/1393 train_time:174428ms step_avg:129.11ms
step:1362/1393 train_time:174565ms step_avg:129.12ms
step:1363/1393 train_time:174704ms step_avg:129.12ms
step:1364/1393 train_time:174839ms step_avg:129.13ms
step:1365/1393 train_time:174973ms step_avg:129.13ms
step:1366/1393 train_time:175109ms step_avg:129.14ms
step:1367/1393 train_time:175246ms step_avg:129.14ms
step:1368/1393 train_time:175382ms step_avg:129.15ms
step:1369/1393 train_time:175520ms step_avg:129.15ms
step:1370/1393 train_time:175657ms step_avg:129.16ms
step:1371/1393 train_time:175796ms step_avg:129.17ms
step:1372/1393 train_time:175934ms step_avg:129.17ms
step:1373/1393 train_time:176070ms step_avg:129.18ms
step:1374/1393 train_time:176208ms step_avg:129.18ms
step:1375/1393 train_time:176343ms step_avg:129.19ms
step:1375/1393 val_loss:3.2821 train_time:176476ms step_avg:129.29ms
step:1376/1393 train_time:176497ms step_avg:129.21ms
step:1377/1393 train_time:176620ms step_avg:129.20ms
step:1378/1393 train_time:176756ms step_avg:129.21ms
step:1379/1393 train_time:176891ms step_avg:129.21ms
step:1380/1393 train_time:177027ms step_avg:129.22ms
step:1381/1393 train_time:177163ms step_avg:129.22ms
step:1382/1393 train_time:177300ms step_avg:129.23ms
step:1383/1393 train_time:177436ms step_avg:129.23ms
step:1384/1393 train_time:177574ms step_avg:129.24ms
step:1385/1393 train_time:177710ms step_avg:129.24ms
step:1386/1393 train_time:177844ms step_avg:129.25ms
step:1387/1393 train_time:177981ms step_avg:129.25ms
step:1388/1393 train_time:178117ms step_avg:129.26ms
step:1389/1393 train_time:178252ms step_avg:129.26ms
step:1390/1393 train_time:178388ms step_avg:129.27ms
step:1391/1393 train_time:178523ms step_avg:129.27ms
step:1392/1393 train_time:178660ms step_avg:129.28ms
step:1393/1393 train_time:178794ms step_avg:129.28ms
step:1393/1393 val_loss:3.2782 train_time:178928ms step_avg:129.38ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
