import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:40:27 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24276ms step_avg:nanms
step:2/1770 train_time:24698ms step_avg:nanms
step:3/1770 train_time:24792ms step_avg:nanms
step:4/1770 train_time:24885ms step_avg:nanms
step:5/1770 train_time:24979ms step_avg:nanms
step:6/1770 train_time:25072ms step_avg:nanms
step:7/1770 train_time:25166ms step_avg:nanms
step:8/1770 train_time:25260ms step_avg:nanms
step:9/1770 train_time:25354ms step_avg:nanms
step:10/1770 train_time:25447ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.75ms
step:14/1770 train_time:376ms step_avg:93.97ms
step:15/1770 train_time:470ms step_avg:93.99ms
step:16/1770 train_time:566ms step_avg:94.27ms
step:17/1770 train_time:657ms step_avg:93.89ms
step:18/1770 train_time:751ms step_avg:93.83ms
step:19/1770 train_time:845ms step_avg:93.86ms
step:20/1770 train_time:938ms step_avg:93.83ms
step:21/1770 train_time:1032ms step_avg:93.80ms
step:22/1770 train_time:1125ms step_avg:93.77ms
step:23/1770 train_time:1219ms step_avg:93.80ms
step:24/1770 train_time:1313ms step_avg:93.81ms
step:25/1770 train_time:1407ms step_avg:93.78ms
step:26/1770 train_time:1500ms step_avg:93.78ms
step:27/1770 train_time:1595ms step_avg:93.82ms
step:28/1770 train_time:1689ms step_avg:93.82ms
step:29/1770 train_time:1782ms step_avg:93.82ms
step:30/1770 train_time:1877ms step_avg:93.85ms
step:31/1770 train_time:1971ms step_avg:93.86ms
step:32/1770 train_time:2065ms step_avg:93.88ms
step:33/1770 train_time:2159ms step_avg:93.88ms
step:34/1770 train_time:2253ms step_avg:93.89ms
step:35/1770 train_time:2347ms step_avg:93.90ms
step:36/1770 train_time:2441ms step_avg:93.90ms
step:37/1770 train_time:2536ms step_avg:93.92ms
step:38/1770 train_time:2630ms step_avg:93.92ms
step:39/1770 train_time:2724ms step_avg:93.93ms
step:40/1770 train_time:2818ms step_avg:93.93ms
step:41/1770 train_time:2912ms step_avg:93.92ms
step:42/1770 train_time:3006ms step_avg:93.93ms
step:43/1770 train_time:3100ms step_avg:93.93ms
step:44/1770 train_time:3194ms step_avg:93.94ms
step:45/1770 train_time:3287ms step_avg:93.92ms
step:46/1770 train_time:3381ms step_avg:93.91ms
step:47/1770 train_time:3475ms step_avg:93.91ms
step:48/1770 train_time:3569ms step_avg:93.91ms
step:49/1770 train_time:3663ms step_avg:93.91ms
step:50/1770 train_time:3756ms step_avg:93.90ms
step:51/1770 train_time:3850ms step_avg:93.90ms
step:52/1770 train_time:3943ms step_avg:93.89ms
step:53/1770 train_time:4038ms step_avg:93.90ms
step:54/1770 train_time:4132ms step_avg:93.91ms
step:55/1770 train_time:4226ms step_avg:93.91ms
step:56/1770 train_time:4320ms step_avg:93.92ms
step:57/1770 train_time:4415ms step_avg:93.93ms
step:58/1770 train_time:4508ms step_avg:93.92ms
step:59/1770 train_time:4602ms step_avg:93.92ms
step:60/1770 train_time:4697ms step_avg:93.94ms
step:61/1770 train_time:4791ms step_avg:93.95ms
step:62/1770 train_time:4885ms step_avg:93.94ms
step:63/1770 train_time:4979ms step_avg:93.93ms
step:64/1770 train_time:5072ms step_avg:93.93ms
step:65/1770 train_time:5166ms step_avg:93.93ms
step:66/1770 train_time:5260ms step_avg:93.93ms
step:67/1770 train_time:5354ms step_avg:93.94ms
step:68/1770 train_time:5448ms step_avg:93.94ms
step:69/1770 train_time:5543ms step_avg:93.95ms
step:70/1770 train_time:5638ms step_avg:93.96ms
step:71/1770 train_time:5731ms step_avg:93.95ms
step:72/1770 train_time:5825ms step_avg:93.95ms
step:73/1770 train_time:5919ms step_avg:93.95ms
step:74/1770 train_time:6012ms step_avg:93.94ms
step:75/1770 train_time:6106ms step_avg:93.94ms
step:76/1770 train_time:6200ms step_avg:93.93ms
step:77/1770 train_time:6294ms step_avg:93.94ms
step:78/1770 train_time:6388ms step_avg:93.94ms
step:79/1770 train_time:6481ms step_avg:93.93ms
step:80/1770 train_time:6575ms step_avg:93.93ms
step:81/1770 train_time:6669ms step_avg:93.94ms
step:82/1770 train_time:6764ms step_avg:93.94ms
step:83/1770 train_time:6858ms step_avg:93.95ms
step:84/1770 train_time:6953ms step_avg:93.96ms
step:85/1770 train_time:7047ms step_avg:93.96ms
step:86/1770 train_time:7141ms step_avg:93.96ms
step:87/1770 train_time:7234ms step_avg:93.95ms
step:88/1770 train_time:7328ms step_avg:93.95ms
step:89/1770 train_time:7422ms step_avg:93.96ms
step:90/1770 train_time:7516ms step_avg:93.95ms
step:91/1770 train_time:7610ms step_avg:93.95ms
step:92/1770 train_time:7703ms step_avg:93.94ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8079ms step_avg:93.94ms
step:97/1770 train_time:8173ms step_avg:93.94ms
step:98/1770 train_time:8267ms step_avg:93.94ms
step:99/1770 train_time:8361ms step_avg:93.94ms
step:100/1770 train_time:8454ms step_avg:93.94ms
step:101/1770 train_time:8549ms step_avg:93.94ms
step:102/1770 train_time:8642ms step_avg:93.93ms
step:103/1770 train_time:8736ms step_avg:93.93ms
step:104/1770 train_time:8830ms step_avg:93.93ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.93ms
step:107/1770 train_time:9112ms step_avg:93.93ms
step:108/1770 train_time:9206ms step_avg:93.93ms
step:109/1770 train_time:9299ms step_avg:93.93ms
step:110/1770 train_time:9393ms step_avg:93.93ms
step:111/1770 train_time:9487ms step_avg:93.93ms
step:112/1770 train_time:9581ms step_avg:93.93ms
step:113/1770 train_time:9675ms step_avg:93.93ms
step:114/1770 train_time:9769ms step_avg:93.93ms
step:115/1770 train_time:9863ms step_avg:93.93ms
step:116/1770 train_time:9957ms step_avg:93.93ms
step:117/1770 train_time:10050ms step_avg:93.93ms
step:118/1770 train_time:10144ms step_avg:93.93ms
step:119/1770 train_time:10238ms step_avg:93.93ms
step:120/1770 train_time:10332ms step_avg:93.92ms
step:121/1770 train_time:10425ms step_avg:93.92ms
step:122/1770 train_time:10518ms step_avg:93.91ms
step:123/1770 train_time:10612ms step_avg:93.91ms
step:124/1770 train_time:10706ms step_avg:93.91ms
step:125/1770 train_time:10800ms step_avg:93.91ms
step:125/1770 val_loss:4.6526 train_time:10892ms step_avg:94.72ms
step:126/1770 train_time:10914ms step_avg:94.09ms
step:127/1770 train_time:10989ms step_avg:93.92ms
step:128/1770 train_time:11083ms step_avg:93.92ms
step:129/1770 train_time:11182ms step_avg:93.97ms
step:130/1770 train_time:11279ms step_avg:93.99ms
step:131/1770 train_time:11373ms step_avg:93.99ms
step:132/1770 train_time:11467ms step_avg:93.99ms
step:133/1770 train_time:11561ms step_avg:93.99ms
step:134/1770 train_time:11655ms step_avg:93.99ms
step:135/1770 train_time:11749ms step_avg:93.99ms
step:136/1770 train_time:11843ms step_avg:94.00ms
step:137/1770 train_time:11937ms step_avg:94.00ms
step:138/1770 train_time:12032ms step_avg:94.00ms
step:139/1770 train_time:12126ms step_avg:94.00ms
step:140/1770 train_time:12221ms step_avg:94.01ms
step:141/1770 train_time:12316ms step_avg:94.02ms
step:142/1770 train_time:12410ms step_avg:94.02ms
step:143/1770 train_time:12505ms step_avg:94.02ms
step:144/1770 train_time:12599ms step_avg:94.02ms
step:145/1770 train_time:12693ms step_avg:94.02ms
step:146/1770 train_time:12787ms step_avg:94.03ms
step:147/1770 train_time:12882ms step_avg:94.03ms
step:148/1770 train_time:12977ms step_avg:94.03ms
step:149/1770 train_time:13071ms step_avg:94.04ms
step:150/1770 train_time:13166ms step_avg:94.04ms
step:151/1770 train_time:13261ms step_avg:94.05ms
step:152/1770 train_time:13356ms step_avg:94.06ms
step:153/1770 train_time:13450ms step_avg:94.06ms
step:154/1770 train_time:13545ms step_avg:94.06ms
step:155/1770 train_time:13639ms step_avg:94.06ms
step:156/1770 train_time:13733ms step_avg:94.06ms
step:157/1770 train_time:13828ms step_avg:94.07ms
step:158/1770 train_time:13922ms step_avg:94.07ms
step:159/1770 train_time:14016ms step_avg:94.07ms
step:160/1770 train_time:14111ms step_avg:94.07ms
step:161/1770 train_time:14206ms step_avg:94.08ms
step:162/1770 train_time:14301ms step_avg:94.08ms
step:163/1770 train_time:14395ms step_avg:94.09ms
step:164/1770 train_time:14490ms step_avg:94.09ms
step:165/1770 train_time:14586ms step_avg:94.10ms
step:166/1770 train_time:14679ms step_avg:94.10ms
step:167/1770 train_time:14773ms step_avg:94.10ms
step:168/1770 train_time:14868ms step_avg:94.10ms
step:169/1770 train_time:14962ms step_avg:94.10ms
step:170/1770 train_time:15057ms step_avg:94.11ms
step:171/1770 train_time:15151ms step_avg:94.11ms
step:172/1770 train_time:15245ms step_avg:94.11ms
step:173/1770 train_time:15339ms step_avg:94.11ms
step:174/1770 train_time:15434ms step_avg:94.11ms
step:175/1770 train_time:15528ms step_avg:94.11ms
step:176/1770 train_time:15623ms step_avg:94.11ms
step:177/1770 train_time:15718ms step_avg:94.12ms
step:178/1770 train_time:15813ms step_avg:94.12ms
step:179/1770 train_time:15907ms step_avg:94.13ms
step:180/1770 train_time:16002ms step_avg:94.13ms
step:181/1770 train_time:16097ms step_avg:94.13ms
step:182/1770 train_time:16191ms step_avg:94.13ms
step:183/1770 train_time:16286ms step_avg:94.14ms
step:184/1770 train_time:16381ms step_avg:94.14ms
step:185/1770 train_time:16475ms step_avg:94.14ms
step:186/1770 train_time:16570ms step_avg:94.15ms
step:187/1770 train_time:16665ms step_avg:94.15ms
step:188/1770 train_time:16760ms step_avg:94.16ms
step:189/1770 train_time:16854ms step_avg:94.16ms
step:190/1770 train_time:16949ms step_avg:94.16ms
step:191/1770 train_time:17043ms step_avg:94.16ms
step:192/1770 train_time:17137ms step_avg:94.16ms
step:193/1770 train_time:17232ms step_avg:94.16ms
step:194/1770 train_time:17326ms step_avg:94.17ms
step:195/1770 train_time:17421ms step_avg:94.17ms
step:196/1770 train_time:17516ms step_avg:94.17ms
step:197/1770 train_time:17610ms step_avg:94.17ms
step:198/1770 train_time:17705ms step_avg:94.18ms
step:199/1770 train_time:17800ms step_avg:94.18ms
step:200/1770 train_time:17894ms step_avg:94.18ms
step:201/1770 train_time:17988ms step_avg:94.18ms
step:202/1770 train_time:18082ms step_avg:94.18ms
step:203/1770 train_time:18177ms step_avg:94.18ms
step:204/1770 train_time:18272ms step_avg:94.18ms
step:205/1770 train_time:18366ms step_avg:94.18ms
step:206/1770 train_time:18460ms step_avg:94.18ms
step:207/1770 train_time:18554ms step_avg:94.18ms
step:208/1770 train_time:18649ms step_avg:94.19ms
step:209/1770 train_time:18744ms step_avg:94.19ms
step:210/1770 train_time:18838ms step_avg:94.19ms
step:211/1770 train_time:18933ms step_avg:94.20ms
step:212/1770 train_time:19028ms step_avg:94.20ms
step:213/1770 train_time:19123ms step_avg:94.20ms
step:214/1770 train_time:19217ms step_avg:94.20ms
step:215/1770 train_time:19312ms step_avg:94.20ms
step:216/1770 train_time:19406ms step_avg:94.20ms
step:217/1770 train_time:19501ms step_avg:94.21ms
step:218/1770 train_time:19595ms step_avg:94.21ms
step:219/1770 train_time:19689ms step_avg:94.21ms
step:220/1770 train_time:19783ms step_avg:94.21ms
step:221/1770 train_time:19878ms step_avg:94.21ms
step:222/1770 train_time:19973ms step_avg:94.21ms
step:223/1770 train_time:20068ms step_avg:94.22ms
step:224/1770 train_time:20163ms step_avg:94.22ms
step:225/1770 train_time:20257ms step_avg:94.22ms
step:226/1770 train_time:20352ms step_avg:94.22ms
step:227/1770 train_time:20446ms step_avg:94.22ms
step:228/1770 train_time:20541ms step_avg:94.22ms
step:229/1770 train_time:20635ms step_avg:94.23ms
step:230/1770 train_time:20730ms step_avg:94.23ms
step:231/1770 train_time:20825ms step_avg:94.23ms
step:232/1770 train_time:20920ms step_avg:94.23ms
step:233/1770 train_time:21014ms step_avg:94.23ms
step:234/1770 train_time:21108ms step_avg:94.23ms
step:235/1770 train_time:21202ms step_avg:94.23ms
step:236/1770 train_time:21297ms step_avg:94.23ms
step:237/1770 train_time:21392ms step_avg:94.24ms
step:238/1770 train_time:21486ms step_avg:94.24ms
step:239/1770 train_time:21580ms step_avg:94.24ms
step:240/1770 train_time:21675ms step_avg:94.24ms
step:241/1770 train_time:21769ms step_avg:94.24ms
step:242/1770 train_time:21864ms step_avg:94.24ms
step:243/1770 train_time:21959ms step_avg:94.24ms
step:244/1770 train_time:22053ms step_avg:94.25ms
step:245/1770 train_time:22148ms step_avg:94.25ms
step:246/1770 train_time:22242ms step_avg:94.25ms
step:247/1770 train_time:22337ms step_avg:94.25ms
step:248/1770 train_time:22431ms step_avg:94.25ms
step:249/1770 train_time:22526ms step_avg:94.25ms
step:250/1770 train_time:22620ms step_avg:94.25ms
step:250/1770 val_loss:4.1192 train_time:22713ms step_avg:94.64ms
step:251/1770 train_time:22734ms step_avg:94.33ms
step:252/1770 train_time:22821ms step_avg:94.30ms
step:253/1770 train_time:22921ms step_avg:94.33ms
step:254/1770 train_time:23018ms step_avg:94.33ms
step:255/1770 train_time:23113ms step_avg:94.34ms
step:256/1770 train_time:23206ms step_avg:94.33ms
step:257/1770 train_time:23300ms step_avg:94.33ms
step:258/1770 train_time:23394ms step_avg:94.33ms
step:259/1770 train_time:23488ms step_avg:94.33ms
step:260/1770 train_time:23582ms step_avg:94.33ms
step:261/1770 train_time:23676ms step_avg:94.33ms
step:262/1770 train_time:23770ms step_avg:94.33ms
step:263/1770 train_time:23866ms step_avg:94.33ms
step:264/1770 train_time:23962ms step_avg:94.34ms
step:265/1770 train_time:24058ms step_avg:94.34ms
step:266/1770 train_time:24153ms step_avg:94.35ms
step:267/1770 train_time:24248ms step_avg:94.35ms
step:268/1770 train_time:24343ms step_avg:94.35ms
step:269/1770 train_time:24438ms step_avg:94.35ms
step:270/1770 train_time:24532ms step_avg:94.36ms
step:271/1770 train_time:24627ms step_avg:94.36ms
step:272/1770 train_time:24721ms step_avg:94.36ms
step:273/1770 train_time:24816ms step_avg:94.36ms
step:274/1770 train_time:24913ms step_avg:94.37ms
step:275/1770 train_time:25008ms step_avg:94.37ms
step:276/1770 train_time:25104ms step_avg:94.38ms
step:277/1770 train_time:25199ms step_avg:94.38ms
step:278/1770 train_time:25294ms step_avg:94.38ms
step:279/1770 train_time:25390ms step_avg:94.39ms
step:280/1770 train_time:25485ms step_avg:94.39ms
step:281/1770 train_time:25579ms step_avg:94.39ms
step:282/1770 train_time:25674ms step_avg:94.39ms
step:283/1770 train_time:25770ms step_avg:94.39ms
step:284/1770 train_time:25865ms step_avg:94.40ms
step:285/1770 train_time:25960ms step_avg:94.40ms
step:286/1770 train_time:26055ms step_avg:94.40ms
step:287/1770 train_time:26150ms step_avg:94.41ms
step:288/1770 train_time:26246ms step_avg:94.41ms
step:289/1770 train_time:26342ms step_avg:94.42ms
step:290/1770 train_time:26437ms step_avg:94.42ms
step:291/1770 train_time:26532ms step_avg:94.42ms
step:292/1770 train_time:26627ms step_avg:94.42ms
step:293/1770 train_time:26721ms step_avg:94.42ms
step:294/1770 train_time:26816ms step_avg:94.42ms
step:295/1770 train_time:26910ms step_avg:94.42ms
step:296/1770 train_time:27006ms step_avg:94.43ms
step:297/1770 train_time:27102ms step_avg:94.43ms
step:298/1770 train_time:27197ms step_avg:94.44ms
step:299/1770 train_time:27293ms step_avg:94.44ms
step:300/1770 train_time:27388ms step_avg:94.44ms
step:301/1770 train_time:27483ms step_avg:94.44ms
step:302/1770 train_time:27578ms step_avg:94.45ms
step:303/1770 train_time:27674ms step_avg:94.45ms
step:304/1770 train_time:27768ms step_avg:94.45ms
step:305/1770 train_time:27863ms step_avg:94.45ms
step:306/1770 train_time:27958ms step_avg:94.45ms
step:307/1770 train_time:28053ms step_avg:94.46ms
step:308/1770 train_time:28149ms step_avg:94.46ms
step:309/1770 train_time:28246ms step_avg:94.47ms
step:310/1770 train_time:28341ms step_avg:94.47ms
step:311/1770 train_time:28436ms step_avg:94.47ms
step:312/1770 train_time:28531ms step_avg:94.47ms
step:313/1770 train_time:28625ms step_avg:94.47ms
step:314/1770 train_time:28719ms step_avg:94.47ms
step:315/1770 train_time:28815ms step_avg:94.48ms
step:316/1770 train_time:28910ms step_avg:94.48ms
step:317/1770 train_time:29005ms step_avg:94.48ms
step:318/1770 train_time:29100ms step_avg:94.48ms
step:319/1770 train_time:29195ms step_avg:94.48ms
step:320/1770 train_time:29291ms step_avg:94.49ms
step:321/1770 train_time:29386ms step_avg:94.49ms
step:322/1770 train_time:29481ms step_avg:94.49ms
step:323/1770 train_time:29576ms step_avg:94.49ms
step:324/1770 train_time:29672ms step_avg:94.50ms
step:325/1770 train_time:29766ms step_avg:94.50ms
step:326/1770 train_time:29861ms step_avg:94.50ms
step:327/1770 train_time:29955ms step_avg:94.50ms
step:328/1770 train_time:30050ms step_avg:94.50ms
step:329/1770 train_time:30145ms step_avg:94.50ms
step:330/1770 train_time:30240ms step_avg:94.50ms
step:331/1770 train_time:30336ms step_avg:94.50ms
step:332/1770 train_time:30431ms step_avg:94.51ms
step:333/1770 train_time:30525ms step_avg:94.51ms
step:334/1770 train_time:30620ms step_avg:94.51ms
step:335/1770 train_time:30716ms step_avg:94.51ms
step:336/1770 train_time:30811ms step_avg:94.51ms
step:337/1770 train_time:30906ms step_avg:94.51ms
step:338/1770 train_time:31000ms step_avg:94.51ms
step:339/1770 train_time:31096ms step_avg:94.52ms
step:340/1770 train_time:31192ms step_avg:94.52ms
step:341/1770 train_time:31286ms step_avg:94.52ms
step:342/1770 train_time:31381ms step_avg:94.52ms
step:343/1770 train_time:31476ms step_avg:94.52ms
step:344/1770 train_time:31571ms step_avg:94.53ms
step:345/1770 train_time:31666ms step_avg:94.53ms
step:346/1770 train_time:31761ms step_avg:94.53ms
step:347/1770 train_time:31856ms step_avg:94.53ms
step:348/1770 train_time:31951ms step_avg:94.53ms
step:349/1770 train_time:32046ms step_avg:94.53ms
step:350/1770 train_time:32141ms step_avg:94.53ms
step:351/1770 train_time:32237ms step_avg:94.54ms
step:352/1770 train_time:32333ms step_avg:94.54ms
step:353/1770 train_time:32428ms step_avg:94.54ms
step:354/1770 train_time:32523ms step_avg:94.54ms
step:355/1770 train_time:32618ms step_avg:94.54ms
step:356/1770 train_time:32713ms step_avg:94.55ms
step:357/1770 train_time:32808ms step_avg:94.55ms
step:358/1770 train_time:32903ms step_avg:94.55ms
step:359/1770 train_time:32998ms step_avg:94.55ms
step:360/1770 train_time:33093ms step_avg:94.55ms
step:361/1770 train_time:33188ms step_avg:94.55ms
step:362/1770 train_time:33283ms step_avg:94.55ms
step:363/1770 train_time:33378ms step_avg:94.56ms
step:364/1770 train_time:33474ms step_avg:94.56ms
step:365/1770 train_time:33570ms step_avg:94.56ms
step:366/1770 train_time:33665ms step_avg:94.56ms
step:367/1770 train_time:33760ms step_avg:94.57ms
step:368/1770 train_time:33855ms step_avg:94.57ms
step:369/1770 train_time:33950ms step_avg:94.57ms
step:370/1770 train_time:34045ms step_avg:94.57ms
step:371/1770 train_time:34140ms step_avg:94.57ms
step:372/1770 train_time:34235ms step_avg:94.57ms
step:373/1770 train_time:34330ms step_avg:94.57ms
step:374/1770 train_time:34425ms step_avg:94.57ms
step:375/1770 train_time:34519ms step_avg:94.57ms
step:375/1770 val_loss:3.9148 train_time:34613ms step_avg:94.83ms
step:376/1770 train_time:34634ms step_avg:94.63ms
step:377/1770 train_time:34719ms step_avg:94.60ms
step:378/1770 train_time:34819ms step_avg:94.62ms
step:379/1770 train_time:34916ms step_avg:94.62ms
step:380/1770 train_time:35011ms step_avg:94.62ms
step:381/1770 train_time:35105ms step_avg:94.62ms
step:382/1770 train_time:35199ms step_avg:94.62ms
step:383/1770 train_time:35294ms step_avg:94.62ms
step:384/1770 train_time:35389ms step_avg:94.62ms
step:385/1770 train_time:35483ms step_avg:94.62ms
step:386/1770 train_time:35578ms step_avg:94.62ms
step:387/1770 train_time:35673ms step_avg:94.62ms
step:388/1770 train_time:35769ms step_avg:94.63ms
step:389/1770 train_time:35865ms step_avg:94.63ms
step:390/1770 train_time:35960ms step_avg:94.63ms
step:391/1770 train_time:36055ms step_avg:94.63ms
step:392/1770 train_time:36150ms step_avg:94.63ms
step:393/1770 train_time:36245ms step_avg:94.63ms
step:394/1770 train_time:36340ms step_avg:94.64ms
step:395/1770 train_time:36435ms step_avg:94.64ms
step:396/1770 train_time:36531ms step_avg:94.64ms
step:397/1770 train_time:36629ms step_avg:94.65ms
step:398/1770 train_time:36725ms step_avg:94.65ms
step:399/1770 train_time:36823ms step_avg:94.66ms
step:400/1770 train_time:36920ms step_avg:94.67ms
step:401/1770 train_time:37017ms step_avg:94.67ms
step:402/1770 train_time:37113ms step_avg:94.68ms
step:403/1770 train_time:37210ms step_avg:94.68ms
step:404/1770 train_time:37306ms step_avg:94.69ms
step:405/1770 train_time:37403ms step_avg:94.69ms
step:406/1770 train_time:37500ms step_avg:94.70ms
step:407/1770 train_time:37597ms step_avg:94.70ms
step:408/1770 train_time:37695ms step_avg:94.71ms
step:409/1770 train_time:37792ms step_avg:94.72ms
step:410/1770 train_time:37889ms step_avg:94.72ms
step:411/1770 train_time:37987ms step_avg:94.73ms
step:412/1770 train_time:38083ms step_avg:94.73ms
step:413/1770 train_time:38180ms step_avg:94.74ms
step:414/1770 train_time:38278ms step_avg:94.75ms
step:415/1770 train_time:38375ms step_avg:94.75ms
step:416/1770 train_time:38472ms step_avg:94.76ms
step:417/1770 train_time:38569ms step_avg:94.76ms
step:418/1770 train_time:38666ms step_avg:94.77ms
step:419/1770 train_time:38763ms step_avg:94.77ms
step:420/1770 train_time:38860ms step_avg:94.78ms
step:421/1770 train_time:38956ms step_avg:94.78ms
step:422/1770 train_time:39053ms step_avg:94.79ms
step:423/1770 train_time:39150ms step_avg:94.79ms
step:424/1770 train_time:39246ms step_avg:94.80ms
step:425/1770 train_time:39343ms step_avg:94.80ms
step:426/1770 train_time:39439ms step_avg:94.81ms
step:427/1770 train_time:39536ms step_avg:94.81ms
step:428/1770 train_time:39634ms step_avg:94.82ms
step:429/1770 train_time:39731ms step_avg:94.82ms
step:430/1770 train_time:39829ms step_avg:94.83ms
step:431/1770 train_time:39926ms step_avg:94.84ms
step:432/1770 train_time:40023ms step_avg:94.84ms
step:433/1770 train_time:40120ms step_avg:94.85ms
step:434/1770 train_time:40217ms step_avg:94.85ms
step:435/1770 train_time:40314ms step_avg:94.86ms
step:436/1770 train_time:40411ms step_avg:94.86ms
step:437/1770 train_time:40507ms step_avg:94.86ms
step:438/1770 train_time:40604ms step_avg:94.87ms
step:439/1770 train_time:40701ms step_avg:94.87ms
step:440/1770 train_time:40797ms step_avg:94.88ms
step:441/1770 train_time:40895ms step_avg:94.88ms
step:442/1770 train_time:40992ms step_avg:94.89ms
step:443/1770 train_time:41089ms step_avg:94.89ms
step:444/1770 train_time:41186ms step_avg:94.90ms
step:445/1770 train_time:41283ms step_avg:94.90ms
step:446/1770 train_time:41380ms step_avg:94.91ms
step:447/1770 train_time:41477ms step_avg:94.91ms
step:448/1770 train_time:41573ms step_avg:94.92ms
step:449/1770 train_time:41670ms step_avg:94.92ms
step:450/1770 train_time:41767ms step_avg:94.93ms
step:451/1770 train_time:41864ms step_avg:94.93ms
step:452/1770 train_time:41961ms step_avg:94.93ms
step:453/1770 train_time:42058ms step_avg:94.94ms
step:454/1770 train_time:42155ms step_avg:94.94ms
step:455/1770 train_time:42252ms step_avg:94.95ms
step:456/1770 train_time:42349ms step_avg:94.95ms
step:457/1770 train_time:42447ms step_avg:94.96ms
step:458/1770 train_time:42544ms step_avg:94.96ms
step:459/1770 train_time:42641ms step_avg:94.97ms
step:460/1770 train_time:42738ms step_avg:94.97ms
step:461/1770 train_time:42836ms step_avg:94.98ms
step:462/1770 train_time:42933ms step_avg:94.98ms
step:463/1770 train_time:43030ms step_avg:94.99ms
step:464/1770 train_time:43127ms step_avg:94.99ms
step:465/1770 train_time:43224ms step_avg:95.00ms
step:466/1770 train_time:43321ms step_avg:95.00ms
step:467/1770 train_time:43418ms step_avg:95.01ms
step:468/1770 train_time:43516ms step_avg:95.01ms
step:469/1770 train_time:43613ms step_avg:95.02ms
step:470/1770 train_time:43710ms step_avg:95.02ms
step:471/1770 train_time:43807ms step_avg:95.03ms
step:472/1770 train_time:43904ms step_avg:95.03ms
step:473/1770 train_time:44000ms step_avg:95.03ms
step:474/1770 train_time:44097ms step_avg:95.04ms
step:475/1770 train_time:44195ms step_avg:95.04ms
step:476/1770 train_time:44292ms step_avg:95.05ms
step:477/1770 train_time:44389ms step_avg:95.05ms
step:478/1770 train_time:44486ms step_avg:95.06ms
step:479/1770 train_time:44583ms step_avg:95.06ms
step:480/1770 train_time:44680ms step_avg:95.06ms
step:481/1770 train_time:44778ms step_avg:95.07ms
step:482/1770 train_time:44875ms step_avg:95.07ms
step:483/1770 train_time:44972ms step_avg:95.08ms
step:484/1770 train_time:45069ms step_avg:95.08ms
step:485/1770 train_time:45166ms step_avg:95.09ms
step:486/1770 train_time:45262ms step_avg:95.09ms
step:487/1770 train_time:45359ms step_avg:95.09ms
step:488/1770 train_time:45456ms step_avg:95.10ms
step:489/1770 train_time:45554ms step_avg:95.10ms
step:490/1770 train_time:45651ms step_avg:95.11ms
step:491/1770 train_time:45748ms step_avg:95.11ms
step:492/1770 train_time:45845ms step_avg:95.11ms
step:493/1770 train_time:45942ms step_avg:95.12ms
step:494/1770 train_time:46038ms step_avg:95.12ms
step:495/1770 train_time:46135ms step_avg:95.12ms
step:496/1770 train_time:46232ms step_avg:95.13ms
step:497/1770 train_time:46330ms step_avg:95.13ms
step:498/1770 train_time:46427ms step_avg:95.14ms
step:499/1770 train_time:46523ms step_avg:95.14ms
step:500/1770 train_time:46620ms step_avg:95.14ms
step:500/1770 val_loss:3.7595 train_time:46716ms step_avg:95.34ms
step:501/1770 train_time:46737ms step_avg:95.19ms
step:502/1770 train_time:46826ms step_avg:95.18ms
step:503/1770 train_time:46926ms step_avg:95.18ms
step:504/1770 train_time:47023ms step_avg:95.19ms
step:505/1770 train_time:47120ms step_avg:95.19ms
step:506/1770 train_time:47216ms step_avg:95.19ms
step:507/1770 train_time:47312ms step_avg:95.20ms
step:508/1770 train_time:47409ms step_avg:95.20ms
step:509/1770 train_time:47505ms step_avg:95.20ms
step:510/1770 train_time:47602ms step_avg:95.20ms
step:511/1770 train_time:47699ms step_avg:95.21ms
step:512/1770 train_time:47796ms step_avg:95.21ms
step:513/1770 train_time:47893ms step_avg:95.22ms
step:514/1770 train_time:47990ms step_avg:95.22ms
step:515/1770 train_time:48087ms step_avg:95.22ms
step:516/1770 train_time:48184ms step_avg:95.23ms
step:517/1770 train_time:48282ms step_avg:95.23ms
step:518/1770 train_time:48379ms step_avg:95.23ms
step:519/1770 train_time:48476ms step_avg:95.24ms
step:520/1770 train_time:48572ms step_avg:95.24ms
step:521/1770 train_time:48669ms step_avg:95.24ms
step:522/1770 train_time:48766ms step_avg:95.25ms
step:523/1770 train_time:48863ms step_avg:95.25ms
step:524/1770 train_time:48960ms step_avg:95.25ms
step:525/1770 train_time:49058ms step_avg:95.26ms
step:526/1770 train_time:49156ms step_avg:95.26ms
step:527/1770 train_time:49254ms step_avg:95.27ms
step:528/1770 train_time:49352ms step_avg:95.27ms
step:529/1770 train_time:49449ms step_avg:95.28ms
step:530/1770 train_time:49546ms step_avg:95.28ms
step:531/1770 train_time:49644ms step_avg:95.29ms
step:532/1770 train_time:49741ms step_avg:95.29ms
step:533/1770 train_time:49840ms step_avg:95.30ms
step:534/1770 train_time:49937ms step_avg:95.30ms
step:535/1770 train_time:50034ms step_avg:95.30ms
step:536/1770 train_time:50131ms step_avg:95.31ms
step:537/1770 train_time:50229ms step_avg:95.31ms
step:538/1770 train_time:50327ms step_avg:95.32ms
step:539/1770 train_time:50424ms step_avg:95.32ms
step:540/1770 train_time:50521ms step_avg:95.32ms
step:541/1770 train_time:50618ms step_avg:95.33ms
step:542/1770 train_time:50716ms step_avg:95.33ms
step:543/1770 train_time:50814ms step_avg:95.34ms
step:544/1770 train_time:50911ms step_avg:95.34ms
step:545/1770 train_time:51009ms step_avg:95.34ms
step:546/1770 train_time:51106ms step_avg:95.35ms
step:547/1770 train_time:51204ms step_avg:95.35ms
step:548/1770 train_time:51301ms step_avg:95.36ms
step:549/1770 train_time:51399ms step_avg:95.36ms
step:550/1770 train_time:51496ms step_avg:95.36ms
step:551/1770 train_time:51593ms step_avg:95.37ms
step:552/1770 train_time:51690ms step_avg:95.37ms
step:553/1770 train_time:51787ms step_avg:95.37ms
step:554/1770 train_time:51884ms step_avg:95.38ms
step:555/1770 train_time:51981ms step_avg:95.38ms
step:556/1770 train_time:52079ms step_avg:95.38ms
step:557/1770 train_time:52176ms step_avg:95.39ms
step:558/1770 train_time:52274ms step_avg:95.39ms
step:559/1770 train_time:52371ms step_avg:95.39ms
step:560/1770 train_time:52468ms step_avg:95.40ms
step:561/1770 train_time:52566ms step_avg:95.40ms
step:562/1770 train_time:52663ms step_avg:95.40ms
step:563/1770 train_time:52761ms step_avg:95.41ms
step:564/1770 train_time:52858ms step_avg:95.41ms
step:565/1770 train_time:52955ms step_avg:95.41ms
step:566/1770 train_time:53053ms step_avg:95.42ms
step:567/1770 train_time:53150ms step_avg:95.42ms
step:568/1770 train_time:53247ms step_avg:95.43ms
step:569/1770 train_time:53345ms step_avg:95.43ms
step:570/1770 train_time:53442ms step_avg:95.43ms
step:571/1770 train_time:53540ms step_avg:95.44ms
step:572/1770 train_time:53637ms step_avg:95.44ms
step:573/1770 train_time:53735ms step_avg:95.44ms
step:574/1770 train_time:53832ms step_avg:95.45ms
step:575/1770 train_time:53929ms step_avg:95.45ms
step:576/1770 train_time:54027ms step_avg:95.45ms
step:577/1770 train_time:54125ms step_avg:95.46ms
step:578/1770 train_time:54223ms step_avg:95.46ms
step:579/1770 train_time:54320ms step_avg:95.47ms
step:580/1770 train_time:54417ms step_avg:95.47ms
step:581/1770 train_time:54514ms step_avg:95.47ms
step:582/1770 train_time:54612ms step_avg:95.48ms
step:583/1770 train_time:54710ms step_avg:95.48ms
step:584/1770 train_time:54807ms step_avg:95.48ms
step:585/1770 train_time:54904ms step_avg:95.49ms
step:586/1770 train_time:55001ms step_avg:95.49ms
step:587/1770 train_time:55099ms step_avg:95.49ms
step:588/1770 train_time:55196ms step_avg:95.50ms
step:589/1770 train_time:55294ms step_avg:95.50ms
step:590/1770 train_time:55392ms step_avg:95.50ms
step:591/1770 train_time:55489ms step_avg:95.51ms
step:592/1770 train_time:55587ms step_avg:95.51ms
step:593/1770 train_time:55685ms step_avg:95.51ms
step:594/1770 train_time:55782ms step_avg:95.52ms
step:595/1770 train_time:55880ms step_avg:95.52ms
step:596/1770 train_time:55977ms step_avg:95.52ms
step:597/1770 train_time:56075ms step_avg:95.53ms
step:598/1770 train_time:56171ms step_avg:95.53ms
step:599/1770 train_time:56269ms step_avg:95.53ms
step:600/1770 train_time:56366ms step_avg:95.54ms
step:601/1770 train_time:56464ms step_avg:95.54ms
step:602/1770 train_time:56561ms step_avg:95.54ms
step:603/1770 train_time:56658ms step_avg:95.55ms
step:604/1770 train_time:56756ms step_avg:95.55ms
step:605/1770 train_time:56853ms step_avg:95.55ms
step:606/1770 train_time:56950ms step_avg:95.55ms
step:607/1770 train_time:57047ms step_avg:95.56ms
step:608/1770 train_time:57145ms step_avg:95.56ms
step:609/1770 train_time:57242ms step_avg:95.56ms
step:610/1770 train_time:57340ms step_avg:95.57ms
step:611/1770 train_time:57437ms step_avg:95.57ms
step:612/1770 train_time:57535ms step_avg:95.57ms
step:613/1770 train_time:57632ms step_avg:95.58ms
step:614/1770 train_time:57729ms step_avg:95.58ms
step:615/1770 train_time:57827ms step_avg:95.58ms
step:616/1770 train_time:57925ms step_avg:95.59ms
step:617/1770 train_time:58022ms step_avg:95.59ms
step:618/1770 train_time:58120ms step_avg:95.59ms
step:619/1770 train_time:58217ms step_avg:95.59ms
step:620/1770 train_time:58315ms step_avg:95.60ms
step:621/1770 train_time:58413ms step_avg:95.60ms
step:622/1770 train_time:58510ms step_avg:95.60ms
step:623/1770 train_time:58608ms step_avg:95.61ms
step:624/1770 train_time:58706ms step_avg:95.61ms
step:625/1770 train_time:58803ms step_avg:95.61ms
step:625/1770 val_loss:3.6712 train_time:58899ms step_avg:95.77ms
step:626/1770 train_time:58920ms step_avg:95.65ms
step:627/1770 train_time:59010ms step_avg:95.64ms
step:628/1770 train_time:59111ms step_avg:95.65ms
step:629/1770 train_time:59209ms step_avg:95.65ms
step:630/1770 train_time:59306ms step_avg:95.66ms
step:631/1770 train_time:59404ms step_avg:95.66ms
step:632/1770 train_time:59500ms step_avg:95.66ms
step:633/1770 train_time:59597ms step_avg:95.66ms
step:634/1770 train_time:59695ms step_avg:95.66ms
step:635/1770 train_time:59792ms step_avg:95.67ms
step:636/1770 train_time:59888ms step_avg:95.67ms
step:637/1770 train_time:59986ms step_avg:95.67ms
step:638/1770 train_time:60084ms step_avg:95.67ms
step:639/1770 train_time:60181ms step_avg:95.68ms
step:640/1770 train_time:60279ms step_avg:95.68ms
step:641/1770 train_time:60377ms step_avg:95.68ms
step:642/1770 train_time:60474ms step_avg:95.69ms
step:643/1770 train_time:60571ms step_avg:95.69ms
step:644/1770 train_time:60668ms step_avg:95.69ms
step:645/1770 train_time:60766ms step_avg:95.69ms
step:646/1770 train_time:60863ms step_avg:95.70ms
step:647/1770 train_time:60960ms step_avg:95.70ms
step:648/1770 train_time:61058ms step_avg:95.70ms
step:649/1770 train_time:61155ms step_avg:95.70ms
step:650/1770 train_time:61253ms step_avg:95.71ms
step:651/1770 train_time:61350ms step_avg:95.71ms
step:652/1770 train_time:61447ms step_avg:95.71ms
step:653/1770 train_time:61544ms step_avg:95.71ms
step:654/1770 train_time:61641ms step_avg:95.72ms
step:655/1770 train_time:61738ms step_avg:95.72ms
step:656/1770 train_time:61836ms step_avg:95.72ms
step:657/1770 train_time:61933ms step_avg:95.72ms
step:658/1770 train_time:62033ms step_avg:95.73ms
step:659/1770 train_time:62132ms step_avg:95.74ms
step:660/1770 train_time:62232ms step_avg:95.74ms
step:661/1770 train_time:62330ms step_avg:95.75ms
step:662/1770 train_time:62430ms step_avg:95.75ms
step:663/1770 train_time:62528ms step_avg:95.76ms
step:664/1770 train_time:62627ms step_avg:95.76ms
step:665/1770 train_time:62726ms step_avg:95.77ms
step:666/1770 train_time:62825ms step_avg:95.77ms
step:667/1770 train_time:62924ms step_avg:95.77ms
step:668/1770 train_time:63024ms step_avg:95.78ms
step:669/1770 train_time:63122ms step_avg:95.79ms
step:670/1770 train_time:63222ms step_avg:95.79ms
step:671/1770 train_time:63322ms step_avg:95.80ms
step:672/1770 train_time:63420ms step_avg:95.80ms
step:673/1770 train_time:63519ms step_avg:95.81ms
step:674/1770 train_time:63619ms step_avg:95.81ms
step:675/1770 train_time:63718ms step_avg:95.82ms
step:676/1770 train_time:63818ms step_avg:95.82ms
step:677/1770 train_time:63917ms step_avg:95.83ms
step:678/1770 train_time:64017ms step_avg:95.83ms
step:679/1770 train_time:64117ms step_avg:95.84ms
step:680/1770 train_time:64216ms step_avg:95.85ms
step:681/1770 train_time:64316ms step_avg:95.85ms
step:682/1770 train_time:64415ms step_avg:95.86ms
step:683/1770 train_time:64514ms step_avg:95.86ms
step:684/1770 train_time:64613ms step_avg:95.86ms
step:685/1770 train_time:64712ms step_avg:95.87ms
step:686/1770 train_time:64810ms step_avg:95.87ms
step:687/1770 train_time:64909ms step_avg:95.88ms
step:688/1770 train_time:65009ms step_avg:95.88ms
step:689/1770 train_time:65108ms step_avg:95.89ms
step:690/1770 train_time:65207ms step_avg:95.89ms
step:691/1770 train_time:65307ms step_avg:95.90ms
step:692/1770 train_time:65407ms step_avg:95.90ms
step:693/1770 train_time:65506ms step_avg:95.91ms
step:694/1770 train_time:65605ms step_avg:95.91ms
step:695/1770 train_time:65705ms step_avg:95.92ms
step:696/1770 train_time:65804ms step_avg:95.92ms
step:697/1770 train_time:65903ms step_avg:95.93ms
step:698/1770 train_time:66001ms step_avg:95.93ms
step:699/1770 train_time:66100ms step_avg:95.94ms
step:700/1770 train_time:66199ms step_avg:95.94ms
step:701/1770 train_time:66298ms step_avg:95.95ms
step:702/1770 train_time:66398ms step_avg:95.95ms
step:703/1770 train_time:66497ms step_avg:95.95ms
step:704/1770 train_time:66597ms step_avg:95.96ms
step:705/1770 train_time:66697ms step_avg:95.97ms
step:706/1770 train_time:66797ms step_avg:95.97ms
step:707/1770 train_time:66897ms step_avg:95.98ms
step:708/1770 train_time:66996ms step_avg:95.98ms
step:709/1770 train_time:67095ms step_avg:95.99ms
step:710/1770 train_time:67194ms step_avg:95.99ms
step:711/1770 train_time:67293ms step_avg:96.00ms
step:712/1770 train_time:67392ms step_avg:96.00ms
step:713/1770 train_time:67491ms step_avg:96.00ms
step:714/1770 train_time:67590ms step_avg:96.01ms
step:715/1770 train_time:67690ms step_avg:96.01ms
step:716/1770 train_time:67789ms step_avg:96.02ms
step:717/1770 train_time:67888ms step_avg:96.02ms
step:718/1770 train_time:67986ms step_avg:96.03ms
step:719/1770 train_time:68085ms step_avg:96.03ms
step:720/1770 train_time:68185ms step_avg:96.03ms
step:721/1770 train_time:68284ms step_avg:96.04ms
step:722/1770 train_time:68383ms step_avg:96.04ms
step:723/1770 train_time:68482ms step_avg:96.05ms
step:724/1770 train_time:68581ms step_avg:96.05ms
step:725/1770 train_time:68679ms step_avg:96.05ms
step:726/1770 train_time:68778ms step_avg:96.06ms
step:727/1770 train_time:68878ms step_avg:96.06ms
step:728/1770 train_time:68977ms step_avg:96.07ms
step:729/1770 train_time:69076ms step_avg:96.07ms
step:730/1770 train_time:69176ms step_avg:96.08ms
step:731/1770 train_time:69276ms step_avg:96.08ms
step:732/1770 train_time:69376ms step_avg:96.09ms
step:733/1770 train_time:69476ms step_avg:96.09ms
step:734/1770 train_time:69575ms step_avg:96.10ms
step:735/1770 train_time:69674ms step_avg:96.10ms
step:736/1770 train_time:69773ms step_avg:96.11ms
step:737/1770 train_time:69872ms step_avg:96.11ms
step:738/1770 train_time:69971ms step_avg:96.11ms
step:739/1770 train_time:70070ms step_avg:96.12ms
step:740/1770 train_time:70169ms step_avg:96.12ms
step:741/1770 train_time:70268ms step_avg:96.13ms
step:742/1770 train_time:70367ms step_avg:96.13ms
step:743/1770 train_time:70466ms step_avg:96.13ms
step:744/1770 train_time:70566ms step_avg:96.14ms
step:745/1770 train_time:70665ms step_avg:96.14ms
step:746/1770 train_time:70764ms step_avg:96.15ms
step:747/1770 train_time:70863ms step_avg:96.15ms
step:748/1770 train_time:70962ms step_avg:96.15ms
step:749/1770 train_time:71061ms step_avg:96.16ms
step:750/1770 train_time:71159ms step_avg:96.16ms
step:750/1770 val_loss:3.6077 train_time:71257ms step_avg:96.29ms
step:751/1770 train_time:71278ms step_avg:96.19ms
step:752/1770 train_time:71371ms step_avg:96.19ms
step:753/1770 train_time:71473ms step_avg:96.20ms
step:754/1770 train_time:71573ms step_avg:96.20ms
step:755/1770 train_time:71672ms step_avg:96.20ms
step:756/1770 train_time:71770ms step_avg:96.21ms
step:757/1770 train_time:71868ms step_avg:96.21ms
step:758/1770 train_time:71967ms step_avg:96.21ms
step:759/1770 train_time:72065ms step_avg:96.21ms
step:760/1770 train_time:72163ms step_avg:96.22ms
step:761/1770 train_time:72261ms step_avg:96.22ms
step:762/1770 train_time:72361ms step_avg:96.22ms
step:763/1770 train_time:72461ms step_avg:96.23ms
step:764/1770 train_time:72561ms step_avg:96.23ms
step:765/1770 train_time:72661ms step_avg:96.24ms
step:766/1770 train_time:72760ms step_avg:96.24ms
step:767/1770 train_time:72860ms step_avg:96.25ms
step:768/1770 train_time:72959ms step_avg:96.25ms
step:769/1770 train_time:73058ms step_avg:96.26ms
step:770/1770 train_time:73157ms step_avg:96.26ms
step:771/1770 train_time:73256ms step_avg:96.26ms
step:772/1770 train_time:73354ms step_avg:96.27ms
step:773/1770 train_time:73453ms step_avg:96.27ms
step:774/1770 train_time:73552ms step_avg:96.27ms
step:775/1770 train_time:73650ms step_avg:96.28ms
step:776/1770 train_time:73750ms step_avg:96.28ms
step:777/1770 train_time:73849ms step_avg:96.28ms
step:778/1770 train_time:73948ms step_avg:96.29ms
step:779/1770 train_time:74048ms step_avg:96.29ms
step:780/1770 train_time:74147ms step_avg:96.29ms
step:781/1770 train_time:74246ms step_avg:96.30ms
step:782/1770 train_time:74344ms step_avg:96.30ms
step:783/1770 train_time:74443ms step_avg:96.30ms
step:784/1770 train_time:74542ms step_avg:96.31ms
step:785/1770 train_time:74641ms step_avg:96.31ms
step:786/1770 train_time:74740ms step_avg:96.31ms
step:787/1770 train_time:74839ms step_avg:96.32ms
step:788/1770 train_time:74939ms step_avg:96.32ms
step:789/1770 train_time:75040ms step_avg:96.33ms
step:790/1770 train_time:75139ms step_avg:96.33ms
step:791/1770 train_time:75239ms step_avg:96.34ms
step:792/1770 train_time:75338ms step_avg:96.34ms
step:793/1770 train_time:75437ms step_avg:96.34ms
step:794/1770 train_time:75536ms step_avg:96.35ms
step:795/1770 train_time:75635ms step_avg:96.35ms
step:796/1770 train_time:75734ms step_avg:96.35ms
step:797/1770 train_time:75834ms step_avg:96.36ms
step:798/1770 train_time:75933ms step_avg:96.36ms
step:799/1770 train_time:76033ms step_avg:96.37ms
step:800/1770 train_time:76133ms step_avg:96.37ms
step:801/1770 train_time:76232ms step_avg:96.37ms
step:802/1770 train_time:76331ms step_avg:96.38ms
step:803/1770 train_time:76431ms step_avg:96.38ms
step:804/1770 train_time:76530ms step_avg:96.39ms
step:805/1770 train_time:76629ms step_avg:96.39ms
step:806/1770 train_time:76729ms step_avg:96.39ms
step:807/1770 train_time:76829ms step_avg:96.40ms
step:808/1770 train_time:76929ms step_avg:96.40ms
step:809/1770 train_time:77028ms step_avg:96.41ms
step:810/1770 train_time:77129ms step_avg:96.41ms
step:811/1770 train_time:77228ms step_avg:96.41ms
step:812/1770 train_time:77327ms step_avg:96.42ms
step:813/1770 train_time:77427ms step_avg:96.42ms
step:814/1770 train_time:77525ms step_avg:96.42ms
step:815/1770 train_time:77624ms step_avg:96.43ms
step:816/1770 train_time:77724ms step_avg:96.43ms
step:817/1770 train_time:77823ms step_avg:96.43ms
step:818/1770 train_time:77922ms step_avg:96.44ms
step:819/1770 train_time:78021ms step_avg:96.44ms
step:820/1770 train_time:78121ms step_avg:96.45ms
step:821/1770 train_time:78221ms step_avg:96.45ms
step:822/1770 train_time:78320ms step_avg:96.45ms
step:823/1770 train_time:78420ms step_avg:96.46ms
step:824/1770 train_time:78519ms step_avg:96.46ms
step:825/1770 train_time:78618ms step_avg:96.46ms
step:826/1770 train_time:78718ms step_avg:96.47ms
step:827/1770 train_time:78817ms step_avg:96.47ms
step:828/1770 train_time:78916ms step_avg:96.47ms
step:829/1770 train_time:79015ms step_avg:96.48ms
step:830/1770 train_time:79114ms step_avg:96.48ms
step:831/1770 train_time:79213ms step_avg:96.48ms
step:832/1770 train_time:79313ms step_avg:96.49ms
step:833/1770 train_time:79412ms step_avg:96.49ms
step:834/1770 train_time:79511ms step_avg:96.49ms
step:835/1770 train_time:79610ms step_avg:96.50ms
step:836/1770 train_time:79711ms step_avg:96.50ms
step:837/1770 train_time:79811ms step_avg:96.51ms
step:838/1770 train_time:79911ms step_avg:96.51ms
step:839/1770 train_time:80011ms step_avg:96.52ms
step:840/1770 train_time:80111ms step_avg:96.52ms
step:841/1770 train_time:80210ms step_avg:96.52ms
step:842/1770 train_time:80309ms step_avg:96.53ms
step:843/1770 train_time:80408ms step_avg:96.53ms
step:844/1770 train_time:80507ms step_avg:96.53ms
step:845/1770 train_time:80607ms step_avg:96.54ms
step:846/1770 train_time:80706ms step_avg:96.54ms
step:847/1770 train_time:80805ms step_avg:96.54ms
step:848/1770 train_time:80904ms step_avg:96.54ms
step:849/1770 train_time:81004ms step_avg:96.55ms
step:850/1770 train_time:81103ms step_avg:96.55ms
step:851/1770 train_time:81204ms step_avg:96.56ms
step:852/1770 train_time:81303ms step_avg:96.56ms
step:853/1770 train_time:81402ms step_avg:96.56ms
step:854/1770 train_time:81501ms step_avg:96.56ms
step:855/1770 train_time:81601ms step_avg:96.57ms
step:856/1770 train_time:81700ms step_avg:96.57ms
step:857/1770 train_time:81799ms step_avg:96.58ms
step:858/1770 train_time:81899ms step_avg:96.58ms
step:859/1770 train_time:81999ms step_avg:96.58ms
step:860/1770 train_time:82098ms step_avg:96.59ms
step:861/1770 train_time:82198ms step_avg:96.59ms
step:862/1770 train_time:82298ms step_avg:96.59ms
step:863/1770 train_time:82397ms step_avg:96.60ms
step:864/1770 train_time:82495ms step_avg:96.60ms
step:865/1770 train_time:82594ms step_avg:96.60ms
step:866/1770 train_time:82694ms step_avg:96.60ms
step:867/1770 train_time:82793ms step_avg:96.61ms
step:868/1770 train_time:82892ms step_avg:96.61ms
step:869/1770 train_time:82992ms step_avg:96.61ms
step:870/1770 train_time:83092ms step_avg:96.62ms
step:871/1770 train_time:83192ms step_avg:96.62ms
step:872/1770 train_time:83292ms step_avg:96.63ms
step:873/1770 train_time:83391ms step_avg:96.63ms
step:874/1770 train_time:83491ms step_avg:96.63ms
step:875/1770 train_time:83590ms step_avg:96.64ms
step:875/1770 val_loss:3.5587 train_time:83688ms step_avg:96.75ms
step:876/1770 train_time:83717ms step_avg:96.67ms
step:877/1770 train_time:83803ms step_avg:96.66ms
step:878/1770 train_time:83903ms step_avg:96.66ms
step:879/1770 train_time:84004ms step_avg:96.67ms
step:880/1770 train_time:84103ms step_avg:96.67ms
step:881/1770 train_time:84202ms step_avg:96.67ms
step:882/1770 train_time:84300ms step_avg:96.67ms
step:883/1770 train_time:84399ms step_avg:96.68ms
step:884/1770 train_time:84498ms step_avg:96.68ms
step:885/1770 train_time:84596ms step_avg:96.68ms
step:886/1770 train_time:84695ms step_avg:96.68ms
step:887/1770 train_time:84796ms step_avg:96.69ms
step:888/1770 train_time:84895ms step_avg:96.69ms
step:889/1770 train_time:84995ms step_avg:96.70ms
step:890/1770 train_time:85094ms step_avg:96.70ms
step:891/1770 train_time:85194ms step_avg:96.70ms
step:892/1770 train_time:85293ms step_avg:96.70ms
step:893/1770 train_time:85393ms step_avg:96.71ms
step:894/1770 train_time:85493ms step_avg:96.71ms
step:895/1770 train_time:85592ms step_avg:96.71ms
step:896/1770 train_time:85691ms step_avg:96.72ms
step:897/1770 train_time:85791ms step_avg:96.72ms
step:898/1770 train_time:85890ms step_avg:96.72ms
step:899/1770 train_time:85990ms step_avg:96.73ms
step:900/1770 train_time:86090ms step_avg:96.73ms
step:901/1770 train_time:86190ms step_avg:96.73ms
step:902/1770 train_time:86288ms step_avg:96.74ms
step:903/1770 train_time:86387ms step_avg:96.74ms
step:904/1770 train_time:86487ms step_avg:96.74ms
step:905/1770 train_time:86586ms step_avg:96.74ms
step:906/1770 train_time:86685ms step_avg:96.75ms
step:907/1770 train_time:86784ms step_avg:96.75ms
step:908/1770 train_time:86883ms step_avg:96.75ms
step:909/1770 train_time:86983ms step_avg:96.76ms
step:910/1770 train_time:87083ms step_avg:96.76ms
step:911/1770 train_time:87183ms step_avg:96.76ms
step:912/1770 train_time:87283ms step_avg:96.77ms
step:913/1770 train_time:87383ms step_avg:96.77ms
step:914/1770 train_time:87483ms step_avg:96.77ms
step:915/1770 train_time:87583ms step_avg:96.78ms
step:916/1770 train_time:87682ms step_avg:96.78ms
step:917/1770 train_time:87781ms step_avg:96.78ms
step:918/1770 train_time:87880ms step_avg:96.78ms
step:919/1770 train_time:87979ms step_avg:96.79ms
step:920/1770 train_time:88080ms step_avg:96.79ms
step:921/1770 train_time:88181ms step_avg:96.80ms
step:922/1770 train_time:88283ms step_avg:96.80ms
step:923/1770 train_time:88383ms step_avg:96.80ms
step:924/1770 train_time:88484ms step_avg:96.81ms
step:925/1770 train_time:88584ms step_avg:96.81ms
step:926/1770 train_time:88685ms step_avg:96.82ms
step:927/1770 train_time:88786ms step_avg:96.82ms
step:928/1770 train_time:88887ms step_avg:96.83ms
step:929/1770 train_time:88987ms step_avg:96.83ms
step:930/1770 train_time:89089ms step_avg:96.84ms
step:931/1770 train_time:89190ms step_avg:96.84ms
step:932/1770 train_time:89290ms step_avg:96.84ms
step:933/1770 train_time:89391ms step_avg:96.85ms
step:934/1770 train_time:89492ms step_avg:96.85ms
step:935/1770 train_time:89594ms step_avg:96.86ms
step:936/1770 train_time:89694ms step_avg:96.86ms
step:937/1770 train_time:89795ms step_avg:96.87ms
step:938/1770 train_time:89896ms step_avg:96.87ms
step:939/1770 train_time:89997ms step_avg:96.87ms
step:940/1770 train_time:90098ms step_avg:96.88ms
step:941/1770 train_time:90199ms step_avg:96.88ms
step:942/1770 train_time:90299ms step_avg:96.89ms
step:943/1770 train_time:90399ms step_avg:96.89ms
step:944/1770 train_time:90500ms step_avg:96.90ms
step:945/1770 train_time:90601ms step_avg:96.90ms
step:946/1770 train_time:90702ms step_avg:96.90ms
step:947/1770 train_time:90804ms step_avg:96.91ms
step:948/1770 train_time:90906ms step_avg:96.91ms
step:949/1770 train_time:91008ms step_avg:96.92ms
step:950/1770 train_time:91109ms step_avg:96.92ms
step:951/1770 train_time:91210ms step_avg:96.93ms
step:952/1770 train_time:91310ms step_avg:96.93ms
step:953/1770 train_time:91410ms step_avg:96.94ms
step:954/1770 train_time:91511ms step_avg:96.94ms
step:955/1770 train_time:91612ms step_avg:96.94ms
step:956/1770 train_time:91714ms step_avg:96.95ms
step:957/1770 train_time:91815ms step_avg:96.95ms
step:958/1770 train_time:91916ms step_avg:96.96ms
step:959/1770 train_time:92018ms step_avg:96.96ms
step:960/1770 train_time:92119ms step_avg:96.97ms
step:961/1770 train_time:92219ms step_avg:96.97ms
step:962/1770 train_time:92320ms step_avg:96.98ms
step:963/1770 train_time:92420ms step_avg:96.98ms
step:964/1770 train_time:92521ms step_avg:96.98ms
step:965/1770 train_time:92623ms step_avg:96.99ms
step:966/1770 train_time:92724ms step_avg:96.99ms
step:967/1770 train_time:92825ms step_avg:97.00ms
step:968/1770 train_time:92926ms step_avg:97.00ms
step:969/1770 train_time:93026ms step_avg:97.00ms
step:970/1770 train_time:93127ms step_avg:97.01ms
step:971/1770 train_time:93228ms step_avg:97.01ms
step:972/1770 train_time:93329ms step_avg:97.02ms
step:973/1770 train_time:93429ms step_avg:97.02ms
step:974/1770 train_time:93529ms step_avg:97.02ms
step:975/1770 train_time:93630ms step_avg:97.03ms
step:976/1770 train_time:93731ms step_avg:97.03ms
step:977/1770 train_time:93832ms step_avg:97.03ms
step:978/1770 train_time:93934ms step_avg:97.04ms
step:979/1770 train_time:94035ms step_avg:97.04ms
step:980/1770 train_time:94135ms step_avg:97.05ms
step:981/1770 train_time:94236ms step_avg:97.05ms
step:982/1770 train_time:94336ms step_avg:97.05ms
step:983/1770 train_time:94436ms step_avg:97.06ms
step:984/1770 train_time:94537ms step_avg:97.06ms
step:985/1770 train_time:94637ms step_avg:97.06ms
step:986/1770 train_time:94738ms step_avg:97.07ms
step:987/1770 train_time:94838ms step_avg:97.07ms
step:988/1770 train_time:94939ms step_avg:97.07ms
step:989/1770 train_time:95040ms step_avg:97.08ms
step:990/1770 train_time:95141ms step_avg:97.08ms
step:991/1770 train_time:95243ms step_avg:97.09ms
step:992/1770 train_time:95343ms step_avg:97.09ms
step:993/1770 train_time:95444ms step_avg:97.09ms
step:994/1770 train_time:95545ms step_avg:97.10ms
step:995/1770 train_time:95646ms step_avg:97.10ms
step:996/1770 train_time:95747ms step_avg:97.11ms
step:997/1770 train_time:95848ms step_avg:97.11ms
step:998/1770 train_time:95949ms step_avg:97.11ms
step:999/1770 train_time:96049ms step_avg:97.12ms
step:1000/1770 train_time:96150ms step_avg:97.12ms
step:1000/1770 val_loss:3.5194 train_time:96248ms step_avg:97.22ms
step:1001/1770 train_time:96270ms step_avg:97.14ms
step:1002/1770 train_time:96360ms step_avg:97.14ms
step:1003/1770 train_time:96464ms step_avg:97.14ms
step:1004/1770 train_time:96565ms step_avg:97.15ms
step:1005/1770 train_time:96664ms step_avg:97.15ms
step:1006/1770 train_time:96764ms step_avg:97.15ms
step:1007/1770 train_time:96864ms step_avg:97.16ms
step:1008/1770 train_time:96964ms step_avg:97.16ms
step:1009/1770 train_time:97064ms step_avg:97.16ms
step:1010/1770 train_time:97164ms step_avg:97.16ms
step:1011/1770 train_time:97267ms step_avg:97.17ms
step:1012/1770 train_time:97368ms step_avg:97.17ms
step:1013/1770 train_time:97471ms step_avg:97.18ms
step:1014/1770 train_time:97572ms step_avg:97.18ms
step:1015/1770 train_time:97672ms step_avg:97.19ms
step:1016/1770 train_time:97772ms step_avg:97.19ms
step:1017/1770 train_time:97874ms step_avg:97.19ms
step:1018/1770 train_time:97974ms step_avg:97.20ms
step:1019/1770 train_time:98076ms step_avg:97.20ms
step:1020/1770 train_time:98176ms step_avg:97.20ms
step:1021/1770 train_time:98277ms step_avg:97.21ms
step:1022/1770 train_time:98378ms step_avg:97.21ms
step:1023/1770 train_time:98479ms step_avg:97.22ms
step:1024/1770 train_time:98581ms step_avg:97.22ms
step:1025/1770 train_time:98681ms step_avg:97.22ms
step:1026/1770 train_time:98782ms step_avg:97.23ms
step:1027/1770 train_time:98884ms step_avg:97.23ms
step:1028/1770 train_time:98985ms step_avg:97.23ms
step:1029/1770 train_time:99087ms step_avg:97.24ms
step:1030/1770 train_time:99186ms step_avg:97.24ms
step:1031/1770 train_time:99287ms step_avg:97.24ms
step:1032/1770 train_time:99387ms step_avg:97.25ms
step:1033/1770 train_time:99488ms step_avg:97.25ms
step:1034/1770 train_time:99589ms step_avg:97.26ms
step:1035/1770 train_time:99690ms step_avg:97.26ms
step:1036/1770 train_time:99791ms step_avg:97.26ms
step:1037/1770 train_time:99894ms step_avg:97.27ms
step:1038/1770 train_time:99995ms step_avg:97.27ms
step:1039/1770 train_time:100096ms step_avg:97.27ms
step:1040/1770 train_time:100196ms step_avg:97.28ms
step:1041/1770 train_time:100296ms step_avg:97.28ms
step:1042/1770 train_time:100396ms step_avg:97.28ms
step:1043/1770 train_time:100496ms step_avg:97.29ms
step:1044/1770 train_time:100596ms step_avg:97.29ms
step:1045/1770 train_time:100696ms step_avg:97.29ms
step:1046/1770 train_time:100797ms step_avg:97.29ms
step:1047/1770 train_time:100897ms step_avg:97.30ms
step:1048/1770 train_time:100998ms step_avg:97.30ms
step:1049/1770 train_time:101100ms step_avg:97.30ms
step:1050/1770 train_time:101200ms step_avg:97.31ms
step:1051/1770 train_time:101302ms step_avg:97.31ms
step:1052/1770 train_time:101402ms step_avg:97.32ms
step:1053/1770 train_time:101503ms step_avg:97.32ms
step:1054/1770 train_time:101603ms step_avg:97.32ms
step:1055/1770 train_time:101704ms step_avg:97.32ms
step:1056/1770 train_time:101804ms step_avg:97.33ms
step:1057/1770 train_time:101905ms step_avg:97.33ms
step:1058/1770 train_time:102006ms step_avg:97.33ms
step:1059/1770 train_time:102106ms step_avg:97.34ms
step:1060/1770 train_time:102207ms step_avg:97.34ms
step:1061/1770 train_time:102309ms step_avg:97.34ms
step:1062/1770 train_time:102410ms step_avg:97.35ms
step:1063/1770 train_time:102512ms step_avg:97.35ms
step:1064/1770 train_time:102614ms step_avg:97.36ms
step:1065/1770 train_time:102716ms step_avg:97.36ms
step:1066/1770 train_time:102817ms step_avg:97.36ms
step:1067/1770 train_time:102917ms step_avg:97.37ms
step:1068/1770 train_time:103018ms step_avg:97.37ms
step:1069/1770 train_time:103120ms step_avg:97.37ms
step:1070/1770 train_time:103221ms step_avg:97.38ms
step:1071/1770 train_time:103321ms step_avg:97.38ms
step:1072/1770 train_time:103422ms step_avg:97.38ms
step:1073/1770 train_time:103523ms step_avg:97.39ms
step:1074/1770 train_time:103624ms step_avg:97.39ms
step:1075/1770 train_time:103725ms step_avg:97.39ms
step:1076/1770 train_time:103826ms step_avg:97.40ms
step:1077/1770 train_time:103928ms step_avg:97.40ms
step:1078/1770 train_time:104029ms step_avg:97.41ms
step:1079/1770 train_time:104130ms step_avg:97.41ms
step:1080/1770 train_time:104232ms step_avg:97.41ms
step:1081/1770 train_time:104333ms step_avg:97.42ms
step:1082/1770 train_time:104435ms step_avg:97.42ms
step:1083/1770 train_time:104536ms step_avg:97.42ms
step:1084/1770 train_time:104636ms step_avg:97.43ms
step:1085/1770 train_time:104737ms step_avg:97.43ms
step:1086/1770 train_time:104838ms step_avg:97.43ms
step:1087/1770 train_time:104939ms step_avg:97.44ms
step:1088/1770 train_time:105041ms step_avg:97.44ms
step:1089/1770 train_time:105142ms step_avg:97.44ms
step:1090/1770 train_time:105245ms step_avg:97.45ms
step:1091/1770 train_time:105346ms step_avg:97.45ms
step:1092/1770 train_time:105447ms step_avg:97.46ms
step:1093/1770 train_time:105547ms step_avg:97.46ms
step:1094/1770 train_time:105648ms step_avg:97.46ms
step:1095/1770 train_time:105749ms step_avg:97.46ms
step:1096/1770 train_time:105850ms step_avg:97.47ms
step:1097/1770 train_time:105951ms step_avg:97.47ms
step:1098/1770 train_time:106052ms step_avg:97.47ms
step:1099/1770 train_time:106154ms step_avg:97.48ms
step:1100/1770 train_time:106256ms step_avg:97.48ms
step:1101/1770 train_time:106357ms step_avg:97.49ms
step:1102/1770 train_time:106457ms step_avg:97.49ms
step:1103/1770 train_time:106558ms step_avg:97.49ms
step:1104/1770 train_time:106660ms step_avg:97.50ms
step:1105/1770 train_time:106761ms step_avg:97.50ms
step:1106/1770 train_time:106863ms step_avg:97.50ms
step:1107/1770 train_time:106964ms step_avg:97.51ms
step:1108/1770 train_time:107065ms step_avg:97.51ms
step:1109/1770 train_time:107165ms step_avg:97.51ms
step:1110/1770 train_time:107267ms step_avg:97.52ms
step:1111/1770 train_time:107368ms step_avg:97.52ms
step:1112/1770 train_time:107468ms step_avg:97.52ms
step:1113/1770 train_time:107569ms step_avg:97.52ms
step:1114/1770 train_time:107670ms step_avg:97.53ms
step:1115/1770 train_time:107772ms step_avg:97.53ms
step:1116/1770 train_time:107875ms step_avg:97.54ms
step:1117/1770 train_time:107976ms step_avg:97.54ms
step:1118/1770 train_time:108078ms step_avg:97.54ms
step:1119/1770 train_time:108177ms step_avg:97.54ms
step:1120/1770 train_time:108278ms step_avg:97.55ms
step:1121/1770 train_time:108379ms step_avg:97.55ms
step:1122/1770 train_time:108480ms step_avg:97.55ms
step:1123/1770 train_time:108582ms step_avg:97.56ms
step:1124/1770 train_time:108685ms step_avg:97.56ms
step:1125/1770 train_time:108785ms step_avg:97.57ms
step:1125/1770 val_loss:3.4776 train_time:108884ms step_avg:97.65ms
step:1126/1770 train_time:108908ms step_avg:97.59ms
step:1127/1770 train_time:108997ms step_avg:97.58ms
step:1128/1770 train_time:109099ms step_avg:97.58ms
step:1129/1770 train_time:109200ms step_avg:97.59ms
step:1130/1770 train_time:109301ms step_avg:97.59ms
step:1131/1770 train_time:109401ms step_avg:97.59ms
step:1132/1770 train_time:109502ms step_avg:97.60ms
step:1133/1770 train_time:109603ms step_avg:97.60ms
step:1134/1770 train_time:109704ms step_avg:97.60ms
step:1135/1770 train_time:109804ms step_avg:97.60ms
step:1136/1770 train_time:109906ms step_avg:97.61ms
step:1137/1770 train_time:110008ms step_avg:97.61ms
step:1138/1770 train_time:110109ms step_avg:97.61ms
step:1139/1770 train_time:110210ms step_avg:97.62ms
step:1140/1770 train_time:110312ms step_avg:97.62ms
step:1141/1770 train_time:110413ms step_avg:97.62ms
step:1142/1770 train_time:110513ms step_avg:97.63ms
step:1143/1770 train_time:110613ms step_avg:97.63ms
step:1144/1770 train_time:110715ms step_avg:97.63ms
step:1145/1770 train_time:110815ms step_avg:97.63ms
step:1146/1770 train_time:110917ms step_avg:97.64ms
step:1147/1770 train_time:111018ms step_avg:97.64ms
step:1148/1770 train_time:111119ms step_avg:97.64ms
step:1149/1770 train_time:111221ms step_avg:97.65ms
step:1150/1770 train_time:111322ms step_avg:97.65ms
step:1151/1770 train_time:111424ms step_avg:97.65ms
step:1152/1770 train_time:111526ms step_avg:97.66ms
step:1153/1770 train_time:111626ms step_avg:97.66ms
step:1154/1770 train_time:111727ms step_avg:97.66ms
step:1155/1770 train_time:111828ms step_avg:97.67ms
step:1156/1770 train_time:111929ms step_avg:97.67ms
step:1157/1770 train_time:112033ms step_avg:97.67ms
step:1158/1770 train_time:112134ms step_avg:97.68ms
step:1159/1770 train_time:112234ms step_avg:97.68ms
step:1160/1770 train_time:112336ms step_avg:97.68ms
step:1161/1770 train_time:112436ms step_avg:97.69ms
step:1162/1770 train_time:112538ms step_avg:97.69ms
step:1163/1770 train_time:112640ms step_avg:97.69ms
step:1164/1770 train_time:112741ms step_avg:97.70ms
step:1165/1770 train_time:112843ms step_avg:97.70ms
step:1166/1770 train_time:112945ms step_avg:97.70ms
step:1167/1770 train_time:113046ms step_avg:97.71ms
step:1168/1770 train_time:113146ms step_avg:97.71ms
step:1169/1770 train_time:113247ms step_avg:97.71ms
step:1170/1770 train_time:113349ms step_avg:97.71ms
step:1171/1770 train_time:113451ms step_avg:97.72ms
step:1172/1770 train_time:113551ms step_avg:97.72ms
step:1173/1770 train_time:113652ms step_avg:97.72ms
step:1174/1770 train_time:113753ms step_avg:97.73ms
step:1175/1770 train_time:113854ms step_avg:97.73ms
step:1176/1770 train_time:113956ms step_avg:97.73ms
step:1177/1770 train_time:114057ms step_avg:97.73ms
step:1178/1770 train_time:114158ms step_avg:97.74ms
step:1179/1770 train_time:114260ms step_avg:97.74ms
step:1180/1770 train_time:114362ms step_avg:97.75ms
step:1181/1770 train_time:114463ms step_avg:97.75ms
step:1182/1770 train_time:114564ms step_avg:97.75ms
step:1183/1770 train_time:114666ms step_avg:97.75ms
step:1184/1770 train_time:114769ms step_avg:97.76ms
step:1185/1770 train_time:114871ms step_avg:97.76ms
step:1186/1770 train_time:114973ms step_avg:97.77ms
step:1187/1770 train_time:115077ms step_avg:97.77ms
step:1188/1770 train_time:115178ms step_avg:97.77ms
step:1189/1770 train_time:115280ms step_avg:97.78ms
step:1190/1770 train_time:115382ms step_avg:97.78ms
step:1191/1770 train_time:115484ms step_avg:97.79ms
step:1192/1770 train_time:115587ms step_avg:97.79ms
step:1193/1770 train_time:115689ms step_avg:97.79ms
step:1194/1770 train_time:115791ms step_avg:97.80ms
step:1195/1770 train_time:115893ms step_avg:97.80ms
step:1196/1770 train_time:115996ms step_avg:97.80ms
step:1197/1770 train_time:116097ms step_avg:97.81ms
step:1198/1770 train_time:116200ms step_avg:97.81ms
step:1199/1770 train_time:116301ms step_avg:97.81ms
step:1200/1770 train_time:116403ms step_avg:97.82ms
step:1201/1770 train_time:116506ms step_avg:97.82ms
step:1202/1770 train_time:116607ms step_avg:97.82ms
step:1203/1770 train_time:116710ms step_avg:97.83ms
step:1204/1770 train_time:116812ms step_avg:97.83ms
step:1205/1770 train_time:116913ms step_avg:97.83ms
step:1206/1770 train_time:117015ms step_avg:97.84ms
step:1207/1770 train_time:117117ms step_avg:97.84ms
step:1208/1770 train_time:117218ms step_avg:97.84ms
step:1209/1770 train_time:117319ms step_avg:97.85ms
step:1210/1770 train_time:117421ms step_avg:97.85ms
step:1211/1770 train_time:117523ms step_avg:97.85ms
step:1212/1770 train_time:117627ms step_avg:97.86ms
step:1213/1770 train_time:117729ms step_avg:97.86ms
step:1214/1770 train_time:117831ms step_avg:97.87ms
step:1215/1770 train_time:117933ms step_avg:97.87ms
step:1216/1770 train_time:118038ms step_avg:97.88ms
step:1217/1770 train_time:118140ms step_avg:97.88ms
step:1218/1770 train_time:118241ms step_avg:97.88ms
step:1219/1770 train_time:118343ms step_avg:97.89ms
step:1220/1770 train_time:118446ms step_avg:97.89ms
step:1221/1770 train_time:118547ms step_avg:97.89ms
step:1222/1770 train_time:118652ms step_avg:97.90ms
step:1223/1770 train_time:118754ms step_avg:97.90ms
step:1224/1770 train_time:118856ms step_avg:97.90ms
step:1225/1770 train_time:118959ms step_avg:97.91ms
step:1226/1770 train_time:119062ms step_avg:97.91ms
step:1227/1770 train_time:119167ms step_avg:97.92ms
step:1228/1770 train_time:119270ms step_avg:97.92ms
step:1229/1770 train_time:119374ms step_avg:97.93ms
step:1230/1770 train_time:119476ms step_avg:97.93ms
step:1231/1770 train_time:119578ms step_avg:97.93ms
step:1232/1770 train_time:119681ms step_avg:97.94ms
step:1233/1770 train_time:119783ms step_avg:97.94ms
step:1234/1770 train_time:119884ms step_avg:97.94ms
step:1235/1770 train_time:119986ms step_avg:97.95ms
step:1236/1770 train_time:120088ms step_avg:97.95ms
step:1237/1770 train_time:120191ms step_avg:97.96ms
step:1238/1770 train_time:120293ms step_avg:97.96ms
step:1239/1770 train_time:120396ms step_avg:97.96ms
step:1240/1770 train_time:120498ms step_avg:97.97ms
step:1241/1770 train_time:120601ms step_avg:97.97ms
step:1242/1770 train_time:120703ms step_avg:97.97ms
step:1243/1770 train_time:120806ms step_avg:97.98ms
step:1244/1770 train_time:120907ms step_avg:97.98ms
step:1245/1770 train_time:121009ms step_avg:97.98ms
step:1246/1770 train_time:121111ms step_avg:97.99ms
step:1247/1770 train_time:121214ms step_avg:97.99ms
step:1248/1770 train_time:121316ms step_avg:97.99ms
step:1249/1770 train_time:121418ms step_avg:98.00ms
step:1250/1770 train_time:121520ms step_avg:98.00ms
step:1250/1770 val_loss:3.4294 train_time:121622ms step_avg:98.08ms
step:1251/1770 train_time:121645ms step_avg:98.02ms
step:1252/1770 train_time:121737ms step_avg:98.02ms
step:1253/1770 train_time:121839ms step_avg:98.02ms
step:1254/1770 train_time:121942ms step_avg:98.02ms
step:1255/1770 train_time:122046ms step_avg:98.03ms
step:1256/1770 train_time:122147ms step_avg:98.03ms
step:1257/1770 train_time:122249ms step_avg:98.03ms
step:1258/1770 train_time:122351ms step_avg:98.04ms
step:1259/1770 train_time:122453ms step_avg:98.04ms
step:1260/1770 train_time:122555ms step_avg:98.04ms
step:1261/1770 train_time:122658ms step_avg:98.05ms
step:1262/1770 train_time:122762ms step_avg:98.05ms
step:1263/1770 train_time:122863ms step_avg:98.06ms
step:1264/1770 train_time:122967ms step_avg:98.06ms
step:1265/1770 train_time:123068ms step_avg:98.06ms
step:1266/1770 train_time:123170ms step_avg:98.07ms
step:1267/1770 train_time:123272ms step_avg:98.07ms
step:1268/1770 train_time:123374ms step_avg:98.07ms
step:1269/1770 train_time:123475ms step_avg:98.07ms
step:1270/1770 train_time:123578ms step_avg:98.08ms
step:1271/1770 train_time:123680ms step_avg:98.08ms
step:1272/1770 train_time:123782ms step_avg:98.08ms
step:1273/1770 train_time:123884ms step_avg:98.09ms
step:1274/1770 train_time:123986ms step_avg:98.09ms
step:1275/1770 train_time:124088ms step_avg:98.09ms
step:1276/1770 train_time:124190ms step_avg:98.10ms
step:1277/1770 train_time:124292ms step_avg:98.10ms
step:1278/1770 train_time:124395ms step_avg:98.10ms
step:1279/1770 train_time:124497ms step_avg:98.11ms
step:1280/1770 train_time:124600ms step_avg:98.11ms
step:1281/1770 train_time:124703ms step_avg:98.11ms
step:1282/1770 train_time:124806ms step_avg:98.12ms
step:1283/1770 train_time:124910ms step_avg:98.12ms
step:1284/1770 train_time:125011ms step_avg:98.13ms
step:1285/1770 train_time:125114ms step_avg:98.13ms
step:1286/1770 train_time:125217ms step_avg:98.13ms
step:1287/1770 train_time:125321ms step_avg:98.14ms
step:1288/1770 train_time:125423ms step_avg:98.14ms
step:1289/1770 train_time:125525ms step_avg:98.14ms
step:1290/1770 train_time:125626ms step_avg:98.15ms
step:1291/1770 train_time:125728ms step_avg:98.15ms
step:1292/1770 train_time:125829ms step_avg:98.15ms
step:1293/1770 train_time:125931ms step_avg:98.15ms
step:1294/1770 train_time:126033ms step_avg:98.16ms
step:1295/1770 train_time:126136ms step_avg:98.16ms
step:1296/1770 train_time:126239ms step_avg:98.16ms
step:1297/1770 train_time:126341ms step_avg:98.17ms
step:1298/1770 train_time:126444ms step_avg:98.17ms
step:1299/1770 train_time:126545ms step_avg:98.17ms
step:1300/1770 train_time:126647ms step_avg:98.18ms
step:1301/1770 train_time:126749ms step_avg:98.18ms
step:1302/1770 train_time:126852ms step_avg:98.18ms
step:1303/1770 train_time:126954ms step_avg:98.19ms
step:1304/1770 train_time:127056ms step_avg:98.19ms
step:1305/1770 train_time:127157ms step_avg:98.19ms
step:1306/1770 train_time:127259ms step_avg:98.19ms
step:1307/1770 train_time:127361ms step_avg:98.20ms
step:1308/1770 train_time:127464ms step_avg:98.20ms
step:1309/1770 train_time:127566ms step_avg:98.20ms
step:1310/1770 train_time:127668ms step_avg:98.21ms
step:1311/1770 train_time:127770ms step_avg:98.21ms
step:1312/1770 train_time:127872ms step_avg:98.21ms
step:1313/1770 train_time:127974ms step_avg:98.21ms
step:1314/1770 train_time:128077ms step_avg:98.22ms
step:1315/1770 train_time:128179ms step_avg:98.22ms
step:1316/1770 train_time:128281ms step_avg:98.22ms
step:1317/1770 train_time:128384ms step_avg:98.23ms
step:1318/1770 train_time:128489ms step_avg:98.23ms
step:1319/1770 train_time:128591ms step_avg:98.24ms
step:1320/1770 train_time:128693ms step_avg:98.24ms
step:1321/1770 train_time:128795ms step_avg:98.24ms
step:1322/1770 train_time:128897ms step_avg:98.24ms
step:1323/1770 train_time:129000ms step_avg:98.25ms
step:1324/1770 train_time:129103ms step_avg:98.25ms
step:1325/1770 train_time:129207ms step_avg:98.26ms
step:1326/1770 train_time:129309ms step_avg:98.26ms
step:1327/1770 train_time:129414ms step_avg:98.26ms
step:1328/1770 train_time:129516ms step_avg:98.27ms
step:1329/1770 train_time:129618ms step_avg:98.27ms
step:1330/1770 train_time:129721ms step_avg:98.27ms
step:1331/1770 train_time:129822ms step_avg:98.28ms
step:1332/1770 train_time:129924ms step_avg:98.28ms
step:1333/1770 train_time:130025ms step_avg:98.28ms
step:1334/1770 train_time:130126ms step_avg:98.28ms
step:1335/1770 train_time:130229ms step_avg:98.29ms
step:1336/1770 train_time:130331ms step_avg:98.29ms
step:1337/1770 train_time:130434ms step_avg:98.29ms
step:1338/1770 train_time:130536ms step_avg:98.29ms
step:1339/1770 train_time:130638ms step_avg:98.30ms
step:1340/1770 train_time:130741ms step_avg:98.30ms
step:1341/1770 train_time:130843ms step_avg:98.30ms
step:1342/1770 train_time:130946ms step_avg:98.31ms
step:1343/1770 train_time:131049ms step_avg:98.31ms
step:1344/1770 train_time:131151ms step_avg:98.31ms
step:1345/1770 train_time:131253ms step_avg:98.32ms
step:1346/1770 train_time:131354ms step_avg:98.32ms
step:1347/1770 train_time:131457ms step_avg:98.32ms
step:1348/1770 train_time:131561ms step_avg:98.33ms
step:1349/1770 train_time:131664ms step_avg:98.33ms
step:1350/1770 train_time:131766ms step_avg:98.33ms
step:1351/1770 train_time:131869ms step_avg:98.34ms
step:1352/1770 train_time:131971ms step_avg:98.34ms
step:1353/1770 train_time:132074ms step_avg:98.34ms
step:1354/1770 train_time:132176ms step_avg:98.35ms
step:1355/1770 train_time:132278ms step_avg:98.35ms
step:1356/1770 train_time:132380ms step_avg:98.35ms
step:1357/1770 train_time:132482ms step_avg:98.35ms
step:1358/1770 train_time:132586ms step_avg:98.36ms
step:1359/1770 train_time:132688ms step_avg:98.36ms
step:1360/1770 train_time:132790ms step_avg:98.36ms
step:1361/1770 train_time:132893ms step_avg:98.37ms
step:1362/1770 train_time:132995ms step_avg:98.37ms
step:1363/1770 train_time:133099ms step_avg:98.37ms
step:1364/1770 train_time:133202ms step_avg:98.38ms
step:1365/1770 train_time:133304ms step_avg:98.38ms
step:1366/1770 train_time:133406ms step_avg:98.38ms
step:1367/1770 train_time:133510ms step_avg:98.39ms
step:1368/1770 train_time:133611ms step_avg:98.39ms
step:1369/1770 train_time:133715ms step_avg:98.39ms
step:1370/1770 train_time:133817ms step_avg:98.39ms
step:1371/1770 train_time:133920ms step_avg:98.40ms
step:1372/1770 train_time:134022ms step_avg:98.40ms
step:1373/1770 train_time:134124ms step_avg:98.40ms
step:1374/1770 train_time:134227ms step_avg:98.41ms
step:1375/1770 train_time:134330ms step_avg:98.41ms
step:1375/1770 val_loss:3.3860 train_time:134431ms step_avg:98.48ms
step:1376/1770 train_time:134459ms step_avg:98.43ms
step:1377/1770 train_time:134545ms step_avg:98.42ms
step:1378/1770 train_time:134647ms step_avg:98.43ms
step:1379/1770 train_time:134749ms step_avg:98.43ms
step:1380/1770 train_time:134850ms step_avg:98.43ms
step:1381/1770 train_time:134953ms step_avg:98.43ms
step:1382/1770 train_time:135054ms step_avg:98.44ms
step:1383/1770 train_time:135157ms step_avg:98.44ms
step:1384/1770 train_time:135259ms step_avg:98.44ms
step:1385/1770 train_time:135362ms step_avg:98.44ms
step:1386/1770 train_time:135465ms step_avg:98.45ms
step:1387/1770 train_time:135568ms step_avg:98.45ms
step:1388/1770 train_time:135670ms step_avg:98.45ms
step:1389/1770 train_time:135772ms step_avg:98.46ms
step:1390/1770 train_time:135873ms step_avg:98.46ms
step:1391/1770 train_time:135975ms step_avg:98.46ms
step:1392/1770 train_time:136078ms step_avg:98.46ms
step:1393/1770 train_time:136179ms step_avg:98.47ms
step:1394/1770 train_time:136281ms step_avg:98.47ms
step:1395/1770 train_time:136384ms step_avg:98.47ms
step:1396/1770 train_time:136488ms step_avg:98.48ms
step:1397/1770 train_time:136590ms step_avg:98.48ms
step:1398/1770 train_time:136692ms step_avg:98.48ms
step:1399/1770 train_time:136793ms step_avg:98.48ms
step:1400/1770 train_time:136897ms step_avg:98.49ms
step:1401/1770 train_time:136999ms step_avg:98.49ms
step:1402/1770 train_time:137101ms step_avg:98.49ms
step:1403/1770 train_time:137204ms step_avg:98.50ms
step:1404/1770 train_time:137306ms step_avg:98.50ms
step:1405/1770 train_time:137408ms step_avg:98.50ms
step:1406/1770 train_time:137509ms step_avg:98.50ms
step:1407/1770 train_time:137610ms step_avg:98.50ms
step:1408/1770 train_time:137713ms step_avg:98.51ms
step:1409/1770 train_time:137816ms step_avg:98.51ms
step:1410/1770 train_time:137918ms step_avg:98.51ms
step:1411/1770 train_time:138020ms step_avg:98.52ms
step:1412/1770 train_time:138123ms step_avg:98.52ms
step:1413/1770 train_time:138224ms step_avg:98.52ms
step:1414/1770 train_time:138327ms step_avg:98.52ms
step:1415/1770 train_time:138430ms step_avg:98.53ms
step:1416/1770 train_time:138533ms step_avg:98.53ms
step:1417/1770 train_time:138635ms step_avg:98.53ms
step:1418/1770 train_time:138738ms step_avg:98.54ms
step:1419/1770 train_time:138840ms step_avg:98.54ms
step:1420/1770 train_time:138943ms step_avg:98.54ms
step:1421/1770 train_time:139045ms step_avg:98.54ms
step:1422/1770 train_time:139147ms step_avg:98.55ms
step:1423/1770 train_time:139249ms step_avg:98.55ms
step:1424/1770 train_time:139352ms step_avg:98.55ms
step:1425/1770 train_time:139454ms step_avg:98.55ms
step:1426/1770 train_time:139556ms step_avg:98.56ms
step:1427/1770 train_time:139659ms step_avg:98.56ms
step:1428/1770 train_time:139763ms step_avg:98.56ms
step:1429/1770 train_time:139866ms step_avg:98.57ms
step:1430/1770 train_time:139967ms step_avg:98.57ms
step:1431/1770 train_time:140071ms step_avg:98.57ms
step:1432/1770 train_time:140173ms step_avg:98.57ms
step:1433/1770 train_time:140275ms step_avg:98.58ms
step:1434/1770 train_time:140376ms step_avg:98.58ms
step:1435/1770 train_time:140478ms step_avg:98.58ms
step:1436/1770 train_time:140582ms step_avg:98.59ms
step:1437/1770 train_time:140686ms step_avg:98.59ms
step:1438/1770 train_time:140787ms step_avg:98.59ms
step:1439/1770 train_time:140890ms step_avg:98.59ms
step:1440/1770 train_time:140992ms step_avg:98.60ms
step:1441/1770 train_time:141097ms step_avg:98.60ms
step:1442/1770 train_time:141198ms step_avg:98.60ms
step:1443/1770 train_time:141300ms step_avg:98.60ms
step:1444/1770 train_time:141402ms step_avg:98.61ms
step:1445/1770 train_time:141505ms step_avg:98.61ms
step:1446/1770 train_time:141608ms step_avg:98.61ms
step:1447/1770 train_time:141712ms step_avg:98.62ms
step:1448/1770 train_time:141815ms step_avg:98.62ms
step:1449/1770 train_time:141919ms step_avg:98.62ms
step:1450/1770 train_time:142021ms step_avg:98.63ms
step:1451/1770 train_time:142125ms step_avg:98.63ms
step:1452/1770 train_time:142229ms step_avg:98.63ms
step:1453/1770 train_time:142331ms step_avg:98.64ms
step:1454/1770 train_time:142435ms step_avg:98.64ms
step:1455/1770 train_time:142538ms step_avg:98.64ms
step:1456/1770 train_time:142643ms step_avg:98.65ms
step:1457/1770 train_time:142746ms step_avg:98.65ms
step:1458/1770 train_time:142850ms step_avg:98.65ms
step:1459/1770 train_time:142954ms step_avg:98.66ms
step:1460/1770 train_time:143056ms step_avg:98.66ms
step:1461/1770 train_time:143159ms step_avg:98.66ms
step:1462/1770 train_time:143263ms step_avg:98.67ms
step:1463/1770 train_time:143367ms step_avg:98.67ms
step:1464/1770 train_time:143474ms step_avg:98.68ms
step:1465/1770 train_time:143577ms step_avg:98.68ms
step:1466/1770 train_time:143681ms step_avg:98.68ms
step:1467/1770 train_time:143785ms step_avg:98.69ms
step:1468/1770 train_time:143888ms step_avg:98.69ms
step:1469/1770 train_time:143991ms step_avg:98.69ms
step:1470/1770 train_time:144093ms step_avg:98.69ms
step:1471/1770 train_time:144196ms step_avg:98.70ms
step:1472/1770 train_time:144300ms step_avg:98.70ms
step:1473/1770 train_time:144404ms step_avg:98.70ms
step:1474/1770 train_time:144508ms step_avg:98.71ms
step:1475/1770 train_time:144610ms step_avg:98.71ms
step:1476/1770 train_time:144714ms step_avg:98.71ms
step:1477/1770 train_time:144820ms step_avg:98.72ms
step:1478/1770 train_time:144924ms step_avg:98.72ms
step:1479/1770 train_time:145027ms step_avg:98.73ms
step:1480/1770 train_time:145130ms step_avg:98.73ms
step:1481/1770 train_time:145237ms step_avg:98.73ms
step:1482/1770 train_time:145341ms step_avg:98.74ms
step:1483/1770 train_time:145444ms step_avg:98.74ms
step:1484/1770 train_time:145547ms step_avg:98.74ms
step:1485/1770 train_time:145651ms step_avg:98.75ms
step:1486/1770 train_time:145754ms step_avg:98.75ms
step:1487/1770 train_time:145857ms step_avg:98.75ms
step:1488/1770 train_time:145961ms step_avg:98.76ms
step:1489/1770 train_time:146066ms step_avg:98.76ms
step:1490/1770 train_time:146170ms step_avg:98.76ms
step:1491/1770 train_time:146273ms step_avg:98.77ms
step:1492/1770 train_time:146376ms step_avg:98.77ms
step:1493/1770 train_time:146483ms step_avg:98.77ms
step:1494/1770 train_time:146589ms step_avg:98.78ms
step:1495/1770 train_time:146691ms step_avg:98.78ms
step:1496/1770 train_time:146795ms step_avg:98.79ms
step:1497/1770 train_time:146898ms step_avg:98.79ms
step:1498/1770 train_time:147001ms step_avg:98.79ms
step:1499/1770 train_time:147103ms step_avg:98.79ms
step:1500/1770 train_time:147206ms step_avg:98.80ms
step:1500/1770 val_loss:3.3477 train_time:147308ms step_avg:98.86ms
step:1501/1770 train_time:147331ms step_avg:98.81ms
step:1502/1770 train_time:147421ms step_avg:98.81ms
step:1503/1770 train_time:147524ms step_avg:98.81ms
step:1504/1770 train_time:147628ms step_avg:98.81ms
step:1505/1770 train_time:147734ms step_avg:98.82ms
step:1506/1770 train_time:147837ms step_avg:98.82ms
step:1507/1770 train_time:147940ms step_avg:98.82ms
step:1508/1770 train_time:148045ms step_avg:98.83ms
step:1509/1770 train_time:148147ms step_avg:98.83ms
step:1510/1770 train_time:148249ms step_avg:98.83ms
step:1511/1770 train_time:148354ms step_avg:98.84ms
step:1512/1770 train_time:148457ms step_avg:98.84ms
step:1513/1770 train_time:148562ms step_avg:98.84ms
step:1514/1770 train_time:148665ms step_avg:98.85ms
step:1515/1770 train_time:148768ms step_avg:98.85ms
step:1516/1770 train_time:148872ms step_avg:98.85ms
step:1517/1770 train_time:148975ms step_avg:98.86ms
step:1518/1770 train_time:149080ms step_avg:98.86ms
step:1519/1770 train_time:149182ms step_avg:98.86ms
step:1520/1770 train_time:149287ms step_avg:98.87ms
step:1521/1770 train_time:149390ms step_avg:98.87ms
step:1522/1770 train_time:149494ms step_avg:98.87ms
step:1523/1770 train_time:149598ms step_avg:98.88ms
step:1524/1770 train_time:149701ms step_avg:98.88ms
step:1525/1770 train_time:149803ms step_avg:98.88ms
step:1526/1770 train_time:149906ms step_avg:98.88ms
step:1527/1770 train_time:150009ms step_avg:98.89ms
step:1528/1770 train_time:150115ms step_avg:98.89ms
step:1529/1770 train_time:150219ms step_avg:98.89ms
step:1530/1770 train_time:150322ms step_avg:98.90ms
step:1531/1770 train_time:150425ms step_avg:98.90ms
step:1532/1770 train_time:150528ms step_avg:98.90ms
step:1533/1770 train_time:150632ms step_avg:98.90ms
step:1534/1770 train_time:150735ms step_avg:98.91ms
step:1535/1770 train_time:150838ms step_avg:98.91ms
step:1536/1770 train_time:150941ms step_avg:98.91ms
step:1537/1770 train_time:151044ms step_avg:98.92ms
step:1538/1770 train_time:151150ms step_avg:98.92ms
step:1539/1770 train_time:151254ms step_avg:98.92ms
step:1540/1770 train_time:151360ms step_avg:98.93ms
step:1541/1770 train_time:151464ms step_avg:98.93ms
step:1542/1770 train_time:151567ms step_avg:98.93ms
step:1543/1770 train_time:151670ms step_avg:98.94ms
step:1544/1770 train_time:151775ms step_avg:98.94ms
step:1545/1770 train_time:151878ms step_avg:98.94ms
step:1546/1770 train_time:151981ms step_avg:98.95ms
step:1547/1770 train_time:152083ms step_avg:98.95ms
step:1548/1770 train_time:152187ms step_avg:98.95ms
step:1549/1770 train_time:152290ms step_avg:98.95ms
step:1550/1770 train_time:152395ms step_avg:98.96ms
step:1551/1770 train_time:152498ms step_avg:98.96ms
step:1552/1770 train_time:152603ms step_avg:98.96ms
step:1553/1770 train_time:152706ms step_avg:98.97ms
step:1554/1770 train_time:152810ms step_avg:98.97ms
step:1555/1770 train_time:152914ms step_avg:98.97ms
step:1556/1770 train_time:153017ms step_avg:98.98ms
step:1557/1770 train_time:153120ms step_avg:98.98ms
step:1558/1770 train_time:153223ms step_avg:98.98ms
step:1559/1770 train_time:153327ms step_avg:98.98ms
step:1560/1770 train_time:153429ms step_avg:98.99ms
step:1561/1770 train_time:153535ms step_avg:98.99ms
step:1562/1770 train_time:153638ms step_avg:98.99ms
step:1563/1770 train_time:153741ms step_avg:99.00ms
step:1564/1770 train_time:153843ms step_avg:99.00ms
step:1565/1770 train_time:153947ms step_avg:99.00ms
step:1566/1770 train_time:154049ms step_avg:99.00ms
step:1567/1770 train_time:154153ms step_avg:99.01ms
step:1568/1770 train_time:154256ms step_avg:99.01ms
step:1569/1770 train_time:154362ms step_avg:99.01ms
step:1570/1770 train_time:154465ms step_avg:99.02ms
step:1571/1770 train_time:154568ms step_avg:99.02ms
step:1572/1770 train_time:154673ms step_avg:99.02ms
step:1573/1770 train_time:154778ms step_avg:99.03ms
step:1574/1770 train_time:154882ms step_avg:99.03ms
step:1575/1770 train_time:154984ms step_avg:99.03ms
step:1576/1770 train_time:155087ms step_avg:99.03ms
step:1577/1770 train_time:155191ms step_avg:99.04ms
step:1578/1770 train_time:155296ms step_avg:99.04ms
step:1579/1770 train_time:155399ms step_avg:99.04ms
step:1580/1770 train_time:155502ms step_avg:99.05ms
step:1581/1770 train_time:155607ms step_avg:99.05ms
step:1582/1770 train_time:155712ms step_avg:99.05ms
step:1583/1770 train_time:155816ms step_avg:99.06ms
step:1584/1770 train_time:155921ms step_avg:99.06ms
step:1585/1770 train_time:156026ms step_avg:99.06ms
step:1586/1770 train_time:156132ms step_avg:99.07ms
step:1587/1770 train_time:156235ms step_avg:99.07ms
step:1588/1770 train_time:156338ms step_avg:99.07ms
step:1589/1770 train_time:156444ms step_avg:99.08ms
step:1590/1770 train_time:156547ms step_avg:99.08ms
step:1591/1770 train_time:156649ms step_avg:99.08ms
step:1592/1770 train_time:156753ms step_avg:99.09ms
step:1593/1770 train_time:156856ms step_avg:99.09ms
step:1594/1770 train_time:156959ms step_avg:99.09ms
step:1595/1770 train_time:157062ms step_avg:99.09ms
step:1596/1770 train_time:157168ms step_avg:99.10ms
step:1597/1770 train_time:157271ms step_avg:99.10ms
step:1598/1770 train_time:157375ms step_avg:99.10ms
step:1599/1770 train_time:157481ms step_avg:99.11ms
step:1600/1770 train_time:157586ms step_avg:99.11ms
step:1601/1770 train_time:157690ms step_avg:99.11ms
step:1602/1770 train_time:157794ms step_avg:99.12ms
step:1603/1770 train_time:157898ms step_avg:99.12ms
step:1604/1770 train_time:158000ms step_avg:99.12ms
step:1605/1770 train_time:158102ms step_avg:99.12ms
step:1606/1770 train_time:158207ms step_avg:99.13ms
step:1607/1770 train_time:158314ms step_avg:99.13ms
step:1608/1770 train_time:158417ms step_avg:99.13ms
step:1609/1770 train_time:158519ms step_avg:99.14ms
step:1610/1770 train_time:158625ms step_avg:99.14ms
step:1611/1770 train_time:158729ms step_avg:99.14ms
step:1612/1770 train_time:158833ms step_avg:99.15ms
step:1613/1770 train_time:158937ms step_avg:99.15ms
step:1614/1770 train_time:159039ms step_avg:99.15ms
step:1615/1770 train_time:159143ms step_avg:99.15ms
step:1616/1770 train_time:159246ms step_avg:99.16ms
step:1617/1770 train_time:159351ms step_avg:99.16ms
step:1618/1770 train_time:159456ms step_avg:99.16ms
step:1619/1770 train_time:159561ms step_avg:99.17ms
step:1620/1770 train_time:159664ms step_avg:99.17ms
step:1621/1770 train_time:159767ms step_avg:99.17ms
step:1622/1770 train_time:159872ms step_avg:99.18ms
step:1623/1770 train_time:159979ms step_avg:99.18ms
step:1624/1770 train_time:160081ms step_avg:99.18ms
step:1625/1770 train_time:160184ms step_avg:99.19ms
step:1625/1770 val_loss:3.3133 train_time:160286ms step_avg:99.25ms
step:1626/1770 train_time:160312ms step_avg:99.20ms
step:1627/1770 train_time:160399ms step_avg:99.20ms
step:1628/1770 train_time:160502ms step_avg:99.20ms
step:1629/1770 train_time:160605ms step_avg:99.20ms
step:1630/1770 train_time:160708ms step_avg:99.20ms
step:1631/1770 train_time:160810ms step_avg:99.20ms
step:1632/1770 train_time:160913ms step_avg:99.21ms
step:1633/1770 train_time:161017ms step_avg:99.21ms
step:1634/1770 train_time:161120ms step_avg:99.21ms
step:1635/1770 train_time:161223ms step_avg:99.21ms
step:1636/1770 train_time:161327ms step_avg:99.22ms
step:1637/1770 train_time:161433ms step_avg:99.22ms
step:1638/1770 train_time:161535ms step_avg:99.22ms
step:1639/1770 train_time:161638ms step_avg:99.23ms
step:1640/1770 train_time:161743ms step_avg:99.23ms
step:1641/1770 train_time:161845ms step_avg:99.23ms
step:1642/1770 train_time:161948ms step_avg:99.23ms
step:1643/1770 train_time:162051ms step_avg:99.24ms
step:1644/1770 train_time:162157ms step_avg:99.24ms
step:1645/1770 train_time:162260ms step_avg:99.24ms
step:1646/1770 train_time:162366ms step_avg:99.25ms
step:1647/1770 train_time:162471ms step_avg:99.25ms
step:1648/1770 train_time:162574ms step_avg:99.25ms
step:1649/1770 train_time:162677ms step_avg:99.25ms
step:1650/1770 train_time:162780ms step_avg:99.26ms
step:1651/1770 train_time:162883ms step_avg:99.26ms
step:1652/1770 train_time:162986ms step_avg:99.26ms
step:1653/1770 train_time:163089ms step_avg:99.26ms
step:1654/1770 train_time:163197ms step_avg:99.27ms
step:1655/1770 train_time:163303ms step_avg:99.27ms
step:1656/1770 train_time:163406ms step_avg:99.27ms
step:1657/1770 train_time:163511ms step_avg:99.28ms
step:1658/1770 train_time:163615ms step_avg:99.28ms
step:1659/1770 train_time:163719ms step_avg:99.28ms
step:1660/1770 train_time:163822ms step_avg:99.29ms
step:1661/1770 train_time:163926ms step_avg:99.29ms
step:1662/1770 train_time:164030ms step_avg:99.29ms
step:1663/1770 train_time:164133ms step_avg:99.29ms
step:1664/1770 train_time:164236ms step_avg:99.30ms
step:1665/1770 train_time:164338ms step_avg:99.30ms
step:1666/1770 train_time:164442ms step_avg:99.30ms
step:1667/1770 train_time:164545ms step_avg:99.30ms
step:1668/1770 train_time:164649ms step_avg:99.31ms
step:1669/1770 train_time:164751ms step_avg:99.31ms
step:1670/1770 train_time:164854ms step_avg:99.31ms
step:1671/1770 train_time:164959ms step_avg:99.31ms
step:1672/1770 train_time:165063ms step_avg:99.32ms
step:1673/1770 train_time:165169ms step_avg:99.32ms
step:1674/1770 train_time:165272ms step_avg:99.32ms
step:1675/1770 train_time:165374ms step_avg:99.32ms
step:1676/1770 train_time:165478ms step_avg:99.33ms
step:1677/1770 train_time:165586ms step_avg:99.33ms
step:1678/1770 train_time:165688ms step_avg:99.33ms
step:1679/1770 train_time:165792ms step_avg:99.34ms
step:1680/1770 train_time:165895ms step_avg:99.34ms
step:1681/1770 train_time:165999ms step_avg:99.34ms
step:1682/1770 train_time:166104ms step_avg:99.34ms
step:1683/1770 train_time:166207ms step_avg:99.35ms
step:1684/1770 train_time:166312ms step_avg:99.35ms
step:1685/1770 train_time:166415ms step_avg:99.35ms
step:1686/1770 train_time:166519ms step_avg:99.36ms
step:1687/1770 train_time:166625ms step_avg:99.36ms
step:1688/1770 train_time:166728ms step_avg:99.36ms
step:1689/1770 train_time:166832ms step_avg:99.36ms
step:1690/1770 train_time:166934ms step_avg:99.37ms
step:1691/1770 train_time:167038ms step_avg:99.37ms
step:1692/1770 train_time:167142ms step_avg:99.37ms
step:1693/1770 train_time:167246ms step_avg:99.37ms
step:1694/1770 train_time:167349ms step_avg:99.38ms
step:1695/1770 train_time:167454ms step_avg:99.38ms
step:1696/1770 train_time:167559ms step_avg:99.38ms
step:1697/1770 train_time:167664ms step_avg:99.39ms
step:1698/1770 train_time:167768ms step_avg:99.39ms
step:1699/1770 train_time:167871ms step_avg:99.39ms
step:1700/1770 train_time:167975ms step_avg:99.39ms
step:1701/1770 train_time:168078ms step_avg:99.40ms
step:1702/1770 train_time:168181ms step_avg:99.40ms
step:1703/1770 train_time:168283ms step_avg:99.40ms
step:1704/1770 train_time:168387ms step_avg:99.40ms
step:1705/1770 train_time:168491ms step_avg:99.40ms
step:1706/1770 train_time:168594ms step_avg:99.41ms
step:1707/1770 train_time:168698ms step_avg:99.41ms
step:1708/1770 train_time:168801ms step_avg:99.41ms
step:1709/1770 train_time:168905ms step_avg:99.41ms
step:1710/1770 train_time:169013ms step_avg:99.42ms
step:1711/1770 train_time:169120ms step_avg:99.42ms
step:1712/1770 train_time:169224ms step_avg:99.43ms
step:1713/1770 train_time:169327ms step_avg:99.43ms
step:1714/1770 train_time:169431ms step_avg:99.43ms
step:1715/1770 train_time:169535ms step_avg:99.43ms
step:1716/1770 train_time:169639ms step_avg:99.44ms
step:1717/1770 train_time:169743ms step_avg:99.44ms
step:1718/1770 train_time:169848ms step_avg:99.44ms
step:1719/1770 train_time:169953ms step_avg:99.45ms
step:1720/1770 train_time:170059ms step_avg:99.45ms
step:1721/1770 train_time:170163ms step_avg:99.45ms
step:1722/1770 train_time:170269ms step_avg:99.46ms
step:1723/1770 train_time:170375ms step_avg:99.46ms
step:1724/1770 train_time:170483ms step_avg:99.46ms
step:1725/1770 train_time:170589ms step_avg:99.47ms
step:1726/1770 train_time:170695ms step_avg:99.47ms
step:1727/1770 train_time:170798ms step_avg:99.47ms
step:1728/1770 train_time:170904ms step_avg:99.48ms
step:1729/1770 train_time:171007ms step_avg:99.48ms
step:1730/1770 train_time:171112ms step_avg:99.48ms
step:1731/1770 train_time:171218ms step_avg:99.49ms
step:1732/1770 train_time:171321ms step_avg:99.49ms
step:1733/1770 train_time:171427ms step_avg:99.49ms
step:1734/1770 train_time:171531ms step_avg:99.50ms
step:1735/1770 train_time:171637ms step_avg:99.50ms
step:1736/1770 train_time:171741ms step_avg:99.50ms
step:1737/1770 train_time:171844ms step_avg:99.50ms
step:1738/1770 train_time:171948ms step_avg:99.51ms
step:1739/1770 train_time:172052ms step_avg:99.51ms
step:1740/1770 train_time:172156ms step_avg:99.51ms
step:1741/1770 train_time:172262ms step_avg:99.52ms
step:1742/1770 train_time:172371ms step_avg:99.52ms
step:1743/1770 train_time:172477ms step_avg:99.52ms
step:1744/1770 train_time:172581ms step_avg:99.53ms
step:1745/1770 train_time:172685ms step_avg:99.53ms
step:1746/1770 train_time:172791ms step_avg:99.53ms
step:1747/1770 train_time:172894ms step_avg:99.54ms
step:1748/1770 train_time:173000ms step_avg:99.54ms
step:1749/1770 train_time:173106ms step_avg:99.54ms
step:1750/1770 train_time:173211ms step_avg:99.55ms
step:1750/1770 val_loss:3.2861 train_time:173313ms step_avg:99.61ms
step:1751/1770 train_time:173336ms step_avg:99.56ms
step:1752/1770 train_time:173429ms step_avg:99.56ms
step:1753/1770 train_time:173533ms step_avg:99.56ms
step:1754/1770 train_time:173637ms step_avg:99.56ms
step:1755/1770 train_time:173741ms step_avg:99.57ms
step:1756/1770 train_time:173846ms step_avg:99.57ms
step:1757/1770 train_time:173950ms step_avg:99.57ms
step:1758/1770 train_time:174053ms step_avg:99.57ms
step:1759/1770 train_time:174158ms step_avg:99.58ms
step:1760/1770 train_time:174262ms step_avg:99.58ms
step:1761/1770 train_time:174369ms step_avg:99.58ms
step:1762/1770 train_time:174477ms step_avg:99.59ms
step:1763/1770 train_time:174580ms step_avg:99.59ms
step:1764/1770 train_time:174685ms step_avg:99.59ms
step:1765/1770 train_time:174789ms step_avg:99.60ms
step:1766/1770 train_time:174898ms step_avg:99.60ms
step:1767/1770 train_time:175001ms step_avg:99.60ms
step:1768/1770 train_time:175104ms step_avg:99.60ms
step:1769/1770 train_time:175208ms step_avg:99.61ms
step:1770/1770 train_time:175312ms step_avg:99.61ms
step:1770/1770 val_loss:3.2829 train_time:175416ms step_avg:99.67ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
