import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:41:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23566ms step_avg:nanms
step:2/1770 train_time:23979ms step_avg:nanms
step:3/1770 train_time:24074ms step_avg:nanms
step:4/1770 train_time:24166ms step_avg:nanms
step:5/1770 train_time:24260ms step_avg:nanms
step:6/1770 train_time:24354ms step_avg:nanms
step:7/1770 train_time:24448ms step_avg:nanms
step:8/1770 train_time:24541ms step_avg:nanms
step:9/1770 train_time:24636ms step_avg:nanms
step:10/1770 train_time:24728ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.72ms
step:14/1770 train_time:376ms step_avg:93.97ms
step:15/1770 train_time:470ms step_avg:94.01ms
step:16/1770 train_time:564ms step_avg:94.05ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:94.03ms
step:19/1770 train_time:845ms step_avg:93.94ms
step:20/1770 train_time:939ms step_avg:93.90ms
step:21/1770 train_time:1033ms step_avg:93.89ms
step:22/1770 train_time:1126ms step_avg:93.87ms
step:23/1770 train_time:1220ms step_avg:93.88ms
step:24/1770 train_time:1314ms step_avg:93.87ms
step:25/1770 train_time:1408ms step_avg:93.87ms
step:26/1770 train_time:1502ms step_avg:93.89ms
step:27/1770 train_time:1595ms step_avg:93.84ms
step:28/1770 train_time:1690ms step_avg:93.87ms
step:29/1770 train_time:1784ms step_avg:93.90ms
step:30/1770 train_time:1878ms step_avg:93.91ms
step:31/1770 train_time:1972ms step_avg:93.90ms
step:32/1770 train_time:2066ms step_avg:93.91ms
step:33/1770 train_time:2160ms step_avg:93.89ms
step:34/1770 train_time:2253ms step_avg:93.89ms
step:35/1770 train_time:2347ms step_avg:93.87ms
step:36/1770 train_time:2440ms step_avg:93.86ms
step:37/1770 train_time:2534ms step_avg:93.85ms
step:38/1770 train_time:2628ms step_avg:93.87ms
step:39/1770 train_time:2722ms step_avg:93.86ms
step:40/1770 train_time:2816ms step_avg:93.87ms
step:41/1770 train_time:2911ms step_avg:93.89ms
step:42/1770 train_time:3005ms step_avg:93.91ms
step:43/1770 train_time:3099ms step_avg:93.90ms
step:44/1770 train_time:3193ms step_avg:93.90ms
step:45/1770 train_time:3287ms step_avg:93.90ms
step:46/1770 train_time:3381ms step_avg:93.91ms
step:47/1770 train_time:3474ms step_avg:93.90ms
step:48/1770 train_time:3568ms step_avg:93.90ms
step:49/1770 train_time:3662ms step_avg:93.90ms
step:50/1770 train_time:3756ms step_avg:93.90ms
step:51/1770 train_time:3850ms step_avg:93.89ms
step:52/1770 train_time:3944ms step_avg:93.90ms
step:53/1770 train_time:4037ms step_avg:93.89ms
step:54/1770 train_time:4131ms step_avg:93.89ms
step:55/1770 train_time:4225ms step_avg:93.88ms
step:56/1770 train_time:4319ms step_avg:93.88ms
step:57/1770 train_time:4413ms step_avg:93.89ms
step:58/1770 train_time:4506ms step_avg:93.88ms
step:59/1770 train_time:4600ms step_avg:93.88ms
step:60/1770 train_time:4694ms step_avg:93.88ms
step:61/1770 train_time:4788ms step_avg:93.88ms
step:62/1770 train_time:4883ms step_avg:93.90ms
step:63/1770 train_time:4975ms step_avg:93.88ms
step:64/1770 train_time:5069ms step_avg:93.87ms
step:65/1770 train_time:5163ms step_avg:93.87ms
step:66/1770 train_time:5256ms step_avg:93.86ms
step:67/1770 train_time:5350ms step_avg:93.87ms
step:68/1770 train_time:5445ms step_avg:93.87ms
step:69/1770 train_time:5539ms step_avg:93.88ms
step:70/1770 train_time:5632ms step_avg:93.87ms
step:71/1770 train_time:5726ms step_avg:93.87ms
step:72/1770 train_time:5821ms step_avg:93.88ms
step:73/1770 train_time:5915ms step_avg:93.89ms
step:74/1770 train_time:6008ms step_avg:93.88ms
step:75/1770 train_time:6102ms step_avg:93.88ms
step:76/1770 train_time:6196ms step_avg:93.87ms
step:77/1770 train_time:6289ms step_avg:93.87ms
step:78/1770 train_time:6383ms step_avg:93.87ms
step:79/1770 train_time:6477ms step_avg:93.87ms
step:80/1770 train_time:6571ms step_avg:93.87ms
step:81/1770 train_time:6664ms step_avg:93.86ms
step:82/1770 train_time:6758ms step_avg:93.86ms
step:83/1770 train_time:6852ms step_avg:93.86ms
step:84/1770 train_time:6946ms step_avg:93.86ms
step:85/1770 train_time:7040ms step_avg:93.86ms
step:86/1770 train_time:7133ms step_avg:93.85ms
step:87/1770 train_time:7227ms step_avg:93.86ms
step:88/1770 train_time:7321ms step_avg:93.85ms
step:89/1770 train_time:7414ms step_avg:93.85ms
step:90/1770 train_time:7509ms step_avg:93.86ms
step:91/1770 train_time:7603ms step_avg:93.87ms
step:92/1770 train_time:7697ms step_avg:93.87ms
step:93/1770 train_time:7791ms step_avg:93.87ms
step:94/1770 train_time:7885ms step_avg:93.87ms
step:95/1770 train_time:7979ms step_avg:93.87ms
step:96/1770 train_time:8073ms step_avg:93.87ms
step:97/1770 train_time:8168ms step_avg:93.89ms
step:98/1770 train_time:8260ms step_avg:93.87ms
step:99/1770 train_time:8354ms step_avg:93.86ms
step:100/1770 train_time:8448ms step_avg:93.86ms
step:101/1770 train_time:8541ms step_avg:93.86ms
step:102/1770 train_time:8635ms step_avg:93.86ms
step:103/1770 train_time:8729ms step_avg:93.86ms
step:104/1770 train_time:8823ms step_avg:93.86ms
step:105/1770 train_time:8916ms step_avg:93.86ms
step:106/1770 train_time:9010ms step_avg:93.86ms
step:107/1770 train_time:9104ms step_avg:93.86ms
step:108/1770 train_time:9198ms step_avg:93.86ms
step:109/1770 train_time:9292ms step_avg:93.86ms
step:110/1770 train_time:9385ms step_avg:93.85ms
step:111/1770 train_time:9479ms step_avg:93.85ms
step:112/1770 train_time:9573ms step_avg:93.85ms
step:113/1770 train_time:9666ms step_avg:93.85ms
step:114/1770 train_time:9760ms step_avg:93.85ms
step:115/1770 train_time:9854ms step_avg:93.84ms
step:116/1770 train_time:9947ms step_avg:93.84ms
step:117/1770 train_time:10041ms step_avg:93.84ms
step:118/1770 train_time:10134ms step_avg:93.84ms
step:119/1770 train_time:10229ms step_avg:93.84ms
step:120/1770 train_time:10322ms step_avg:93.84ms
step:121/1770 train_time:10417ms step_avg:93.84ms
step:122/1770 train_time:10510ms step_avg:93.84ms
step:123/1770 train_time:10604ms step_avg:93.84ms
step:124/1770 train_time:10698ms step_avg:93.84ms
step:125/1770 train_time:10791ms step_avg:93.84ms
step:125/1770 val_loss:4.6556 train_time:10884ms step_avg:94.64ms
step:126/1770 train_time:10908ms step_avg:94.03ms
step:127/1770 train_time:10980ms step_avg:93.85ms
step:128/1770 train_time:11080ms step_avg:93.90ms
step:129/1770 train_time:11178ms step_avg:93.94ms
step:130/1770 train_time:11272ms step_avg:93.94ms
step:131/1770 train_time:11366ms step_avg:93.93ms
step:132/1770 train_time:11459ms step_avg:93.93ms
step:133/1770 train_time:11553ms step_avg:93.93ms
step:134/1770 train_time:11646ms step_avg:93.92ms
step:135/1770 train_time:11741ms step_avg:93.92ms
step:136/1770 train_time:11835ms step_avg:93.93ms
step:137/1770 train_time:11929ms step_avg:93.93ms
step:138/1770 train_time:12024ms step_avg:93.94ms
step:139/1770 train_time:12120ms step_avg:93.95ms
step:140/1770 train_time:12216ms step_avg:93.97ms
step:141/1770 train_time:12311ms step_avg:93.98ms
step:142/1770 train_time:12405ms step_avg:93.98ms
step:143/1770 train_time:12499ms step_avg:93.98ms
step:144/1770 train_time:12594ms step_avg:93.98ms
step:145/1770 train_time:12688ms step_avg:93.98ms
step:146/1770 train_time:12782ms step_avg:93.98ms
step:147/1770 train_time:12876ms step_avg:93.99ms
step:148/1770 train_time:12971ms step_avg:93.99ms
step:149/1770 train_time:13065ms step_avg:93.99ms
step:150/1770 train_time:13159ms step_avg:93.99ms
step:151/1770 train_time:13254ms step_avg:94.00ms
step:152/1770 train_time:13348ms step_avg:94.00ms
step:153/1770 train_time:13443ms step_avg:94.00ms
step:154/1770 train_time:13537ms step_avg:94.01ms
step:155/1770 train_time:13632ms step_avg:94.01ms
step:156/1770 train_time:13728ms step_avg:94.02ms
step:157/1770 train_time:13821ms step_avg:94.02ms
step:158/1770 train_time:13916ms step_avg:94.03ms
step:159/1770 train_time:14011ms step_avg:94.03ms
step:160/1770 train_time:14105ms step_avg:94.03ms
step:161/1770 train_time:14199ms step_avg:94.04ms
step:162/1770 train_time:14294ms step_avg:94.04ms
step:163/1770 train_time:14388ms step_avg:94.04ms
step:164/1770 train_time:14482ms step_avg:94.04ms
step:165/1770 train_time:14577ms step_avg:94.04ms
step:166/1770 train_time:14672ms step_avg:94.05ms
step:167/1770 train_time:14766ms step_avg:94.05ms
step:168/1770 train_time:14860ms step_avg:94.05ms
step:169/1770 train_time:14955ms step_avg:94.06ms
step:170/1770 train_time:15050ms step_avg:94.06ms
step:171/1770 train_time:15144ms step_avg:94.06ms
step:172/1770 train_time:15239ms step_avg:94.07ms
step:173/1770 train_time:15334ms step_avg:94.07ms
step:174/1770 train_time:15429ms step_avg:94.08ms
step:175/1770 train_time:15523ms step_avg:94.08ms
step:176/1770 train_time:15617ms step_avg:94.08ms
step:177/1770 train_time:15712ms step_avg:94.08ms
step:178/1770 train_time:15806ms step_avg:94.08ms
step:179/1770 train_time:15901ms step_avg:94.09ms
step:180/1770 train_time:15996ms step_avg:94.09ms
step:181/1770 train_time:16091ms step_avg:94.10ms
step:182/1770 train_time:16185ms step_avg:94.10ms
step:183/1770 train_time:16280ms step_avg:94.10ms
step:184/1770 train_time:16374ms step_avg:94.10ms
step:185/1770 train_time:16469ms step_avg:94.11ms
step:186/1770 train_time:16563ms step_avg:94.11ms
step:187/1770 train_time:16657ms step_avg:94.11ms
step:188/1770 train_time:16751ms step_avg:94.11ms
step:189/1770 train_time:16846ms step_avg:94.11ms
step:190/1770 train_time:16940ms step_avg:94.11ms
step:191/1770 train_time:17034ms step_avg:94.11ms
step:192/1770 train_time:17129ms step_avg:94.11ms
step:193/1770 train_time:17223ms step_avg:94.11ms
step:194/1770 train_time:17318ms step_avg:94.12ms
step:195/1770 train_time:17413ms step_avg:94.12ms
step:196/1770 train_time:17507ms step_avg:94.12ms
step:197/1770 train_time:17602ms step_avg:94.13ms
step:198/1770 train_time:17697ms step_avg:94.13ms
step:199/1770 train_time:17792ms step_avg:94.14ms
step:200/1770 train_time:17886ms step_avg:94.14ms
step:201/1770 train_time:17980ms step_avg:94.13ms
step:202/1770 train_time:18075ms step_avg:94.14ms
step:203/1770 train_time:18168ms step_avg:94.14ms
step:204/1770 train_time:18263ms step_avg:94.14ms
step:205/1770 train_time:18358ms step_avg:94.14ms
step:206/1770 train_time:18452ms step_avg:94.14ms
step:207/1770 train_time:18547ms step_avg:94.15ms
step:208/1770 train_time:18641ms step_avg:94.15ms
step:209/1770 train_time:18736ms step_avg:94.15ms
step:210/1770 train_time:18831ms step_avg:94.16ms
step:211/1770 train_time:18926ms step_avg:94.16ms
step:212/1770 train_time:19020ms step_avg:94.16ms
step:213/1770 train_time:19115ms step_avg:94.16ms
step:214/1770 train_time:19208ms step_avg:94.16ms
step:215/1770 train_time:19302ms step_avg:94.16ms
step:216/1770 train_time:19397ms step_avg:94.16ms
step:217/1770 train_time:19492ms step_avg:94.16ms
step:218/1770 train_time:19586ms step_avg:94.16ms
step:219/1770 train_time:19680ms step_avg:94.16ms
step:220/1770 train_time:19775ms step_avg:94.17ms
step:221/1770 train_time:19869ms step_avg:94.17ms
step:222/1770 train_time:19963ms step_avg:94.17ms
step:223/1770 train_time:20058ms step_avg:94.17ms
step:224/1770 train_time:20153ms step_avg:94.17ms
step:225/1770 train_time:20248ms step_avg:94.18ms
step:226/1770 train_time:20342ms step_avg:94.18ms
step:227/1770 train_time:20436ms step_avg:94.18ms
step:228/1770 train_time:20531ms step_avg:94.18ms
step:229/1770 train_time:20625ms step_avg:94.18ms
step:230/1770 train_time:20720ms step_avg:94.18ms
step:231/1770 train_time:20815ms step_avg:94.19ms
step:232/1770 train_time:20910ms step_avg:94.19ms
step:233/1770 train_time:21004ms step_avg:94.19ms
step:234/1770 train_time:21099ms step_avg:94.19ms
step:235/1770 train_time:21193ms step_avg:94.19ms
step:236/1770 train_time:21287ms step_avg:94.19ms
step:237/1770 train_time:21381ms step_avg:94.19ms
step:238/1770 train_time:21476ms step_avg:94.19ms
step:239/1770 train_time:21570ms step_avg:94.19ms
step:240/1770 train_time:21664ms step_avg:94.19ms
step:241/1770 train_time:21758ms step_avg:94.19ms
step:242/1770 train_time:21853ms step_avg:94.19ms
step:243/1770 train_time:21948ms step_avg:94.20ms
step:244/1770 train_time:22042ms step_avg:94.20ms
step:245/1770 train_time:22137ms step_avg:94.20ms
step:246/1770 train_time:22232ms step_avg:94.20ms
step:247/1770 train_time:22327ms step_avg:94.21ms
step:248/1770 train_time:22422ms step_avg:94.21ms
step:249/1770 train_time:22516ms step_avg:94.21ms
step:250/1770 train_time:22611ms step_avg:94.21ms
step:250/1770 val_loss:4.1057 train_time:22703ms step_avg:94.60ms
step:251/1770 train_time:22728ms step_avg:94.31ms
step:252/1770 train_time:22807ms step_avg:94.24ms
step:253/1770 train_time:22902ms step_avg:94.25ms
step:254/1770 train_time:22996ms step_avg:94.24ms
step:255/1770 train_time:23091ms step_avg:94.25ms
step:256/1770 train_time:23184ms step_avg:94.24ms
step:257/1770 train_time:23278ms step_avg:94.24ms
step:258/1770 train_time:23372ms step_avg:94.24ms
step:259/1770 train_time:23466ms step_avg:94.24ms
step:260/1770 train_time:23560ms step_avg:94.24ms
step:261/1770 train_time:23654ms step_avg:94.24ms
step:262/1770 train_time:23750ms step_avg:94.24ms
step:263/1770 train_time:23845ms step_avg:94.25ms
step:264/1770 train_time:23940ms step_avg:94.25ms
step:265/1770 train_time:24035ms step_avg:94.25ms
step:266/1770 train_time:24130ms step_avg:94.26ms
step:267/1770 train_time:24225ms step_avg:94.26ms
step:268/1770 train_time:24320ms step_avg:94.26ms
step:269/1770 train_time:24415ms step_avg:94.26ms
step:270/1770 train_time:24510ms step_avg:94.27ms
step:271/1770 train_time:24604ms step_avg:94.27ms
step:272/1770 train_time:24699ms step_avg:94.27ms
step:273/1770 train_time:24794ms step_avg:94.27ms
step:274/1770 train_time:24889ms step_avg:94.28ms
step:275/1770 train_time:24984ms step_avg:94.28ms
step:276/1770 train_time:25079ms step_avg:94.28ms
step:277/1770 train_time:25173ms step_avg:94.28ms
step:278/1770 train_time:25268ms step_avg:94.28ms
step:279/1770 train_time:25363ms step_avg:94.29ms
step:280/1770 train_time:25458ms step_avg:94.29ms
step:281/1770 train_time:25554ms step_avg:94.29ms
step:282/1770 train_time:25650ms step_avg:94.30ms
step:283/1770 train_time:25744ms step_avg:94.30ms
step:284/1770 train_time:25840ms step_avg:94.31ms
step:285/1770 train_time:25935ms step_avg:94.31ms
step:286/1770 train_time:26031ms step_avg:94.31ms
step:287/1770 train_time:26125ms step_avg:94.31ms
step:288/1770 train_time:26220ms step_avg:94.32ms
step:289/1770 train_time:26315ms step_avg:94.32ms
step:290/1770 train_time:26410ms step_avg:94.32ms
step:291/1770 train_time:26505ms step_avg:94.32ms
step:292/1770 train_time:26600ms step_avg:94.33ms
step:293/1770 train_time:26695ms step_avg:94.33ms
step:294/1770 train_time:26790ms step_avg:94.33ms
step:295/1770 train_time:26885ms step_avg:94.33ms
step:296/1770 train_time:26980ms step_avg:94.33ms
step:297/1770 train_time:27075ms step_avg:94.34ms
step:298/1770 train_time:27170ms step_avg:94.34ms
step:299/1770 train_time:27266ms step_avg:94.35ms
step:300/1770 train_time:27361ms step_avg:94.35ms
step:301/1770 train_time:27455ms step_avg:94.35ms
step:302/1770 train_time:27551ms step_avg:94.35ms
step:303/1770 train_time:27646ms step_avg:94.36ms
step:304/1770 train_time:27741ms step_avg:94.36ms
step:305/1770 train_time:27835ms step_avg:94.36ms
step:306/1770 train_time:27930ms step_avg:94.36ms
step:307/1770 train_time:28026ms step_avg:94.36ms
step:308/1770 train_time:28121ms step_avg:94.37ms
step:309/1770 train_time:28216ms step_avg:94.37ms
step:310/1770 train_time:28311ms step_avg:94.37ms
step:311/1770 train_time:28406ms step_avg:94.37ms
step:312/1770 train_time:28500ms step_avg:94.37ms
step:313/1770 train_time:28595ms step_avg:94.37ms
step:314/1770 train_time:28690ms step_avg:94.38ms
step:315/1770 train_time:28785ms step_avg:94.38ms
step:316/1770 train_time:28881ms step_avg:94.38ms
step:317/1770 train_time:28976ms step_avg:94.38ms
step:318/1770 train_time:29071ms step_avg:94.39ms
step:319/1770 train_time:29166ms step_avg:94.39ms
step:320/1770 train_time:29261ms step_avg:94.39ms
step:321/1770 train_time:29356ms step_avg:94.39ms
step:322/1770 train_time:29451ms step_avg:94.39ms
step:323/1770 train_time:29545ms step_avg:94.39ms
step:324/1770 train_time:29640ms step_avg:94.40ms
step:325/1770 train_time:29735ms step_avg:94.40ms
step:326/1770 train_time:29830ms step_avg:94.40ms
step:327/1770 train_time:29925ms step_avg:94.40ms
step:328/1770 train_time:30020ms step_avg:94.40ms
step:329/1770 train_time:30115ms step_avg:94.40ms
step:330/1770 train_time:30210ms step_avg:94.41ms
step:331/1770 train_time:30305ms step_avg:94.41ms
step:332/1770 train_time:30401ms step_avg:94.41ms
step:333/1770 train_time:30496ms step_avg:94.42ms
step:334/1770 train_time:30591ms step_avg:94.42ms
step:335/1770 train_time:30686ms step_avg:94.42ms
step:336/1770 train_time:30782ms step_avg:94.42ms
step:337/1770 train_time:30878ms step_avg:94.43ms
step:338/1770 train_time:30973ms step_avg:94.43ms
step:339/1770 train_time:31068ms step_avg:94.43ms
step:340/1770 train_time:31162ms step_avg:94.43ms
step:341/1770 train_time:31257ms step_avg:94.43ms
step:342/1770 train_time:31352ms step_avg:94.43ms
step:343/1770 train_time:31448ms step_avg:94.44ms
step:344/1770 train_time:31543ms step_avg:94.44ms
step:345/1770 train_time:31638ms step_avg:94.44ms
step:346/1770 train_time:31733ms step_avg:94.44ms
step:347/1770 train_time:31829ms step_avg:94.45ms
step:348/1770 train_time:31924ms step_avg:94.45ms
step:349/1770 train_time:32018ms step_avg:94.45ms
step:350/1770 train_time:32114ms step_avg:94.45ms
step:351/1770 train_time:32209ms step_avg:94.45ms
step:352/1770 train_time:32303ms step_avg:94.45ms
step:353/1770 train_time:32398ms step_avg:94.46ms
step:354/1770 train_time:32494ms step_avg:94.46ms
step:355/1770 train_time:32589ms step_avg:94.46ms
step:356/1770 train_time:32684ms step_avg:94.46ms
step:357/1770 train_time:32780ms step_avg:94.47ms
step:358/1770 train_time:32875ms step_avg:94.47ms
step:359/1770 train_time:32969ms step_avg:94.47ms
step:360/1770 train_time:33064ms step_avg:94.47ms
step:361/1770 train_time:33159ms step_avg:94.47ms
step:362/1770 train_time:33254ms step_avg:94.47ms
step:363/1770 train_time:33349ms step_avg:94.47ms
step:364/1770 train_time:33444ms step_avg:94.48ms
step:365/1770 train_time:33540ms step_avg:94.48ms
step:366/1770 train_time:33635ms step_avg:94.48ms
step:367/1770 train_time:33730ms step_avg:94.48ms
step:368/1770 train_time:33825ms step_avg:94.48ms
step:369/1770 train_time:33920ms step_avg:94.48ms
step:370/1770 train_time:34015ms step_avg:94.49ms
step:371/1770 train_time:34111ms step_avg:94.49ms
step:372/1770 train_time:34205ms step_avg:94.49ms
step:373/1770 train_time:34300ms step_avg:94.49ms
step:374/1770 train_time:34395ms step_avg:94.49ms
step:375/1770 train_time:34490ms step_avg:94.49ms
step:375/1770 val_loss:3.8965 train_time:34583ms step_avg:94.75ms
step:376/1770 train_time:34607ms step_avg:94.55ms
step:377/1770 train_time:34688ms step_avg:94.52ms
step:378/1770 train_time:34786ms step_avg:94.53ms
step:379/1770 train_time:34882ms step_avg:94.53ms
step:380/1770 train_time:34977ms step_avg:94.53ms
step:381/1770 train_time:35071ms step_avg:94.53ms
step:382/1770 train_time:35166ms step_avg:94.53ms
step:383/1770 train_time:35260ms step_avg:94.53ms
step:384/1770 train_time:35355ms step_avg:94.53ms
step:385/1770 train_time:35449ms step_avg:94.53ms
step:386/1770 train_time:35544ms step_avg:94.53ms
step:387/1770 train_time:35640ms step_avg:94.54ms
step:388/1770 train_time:35736ms step_avg:94.54ms
step:389/1770 train_time:35833ms step_avg:94.55ms
step:390/1770 train_time:35928ms step_avg:94.55ms
step:391/1770 train_time:36022ms step_avg:94.55ms
step:392/1770 train_time:36117ms step_avg:94.55ms
step:393/1770 train_time:36213ms step_avg:94.55ms
step:394/1770 train_time:36307ms step_avg:94.55ms
step:395/1770 train_time:36402ms step_avg:94.55ms
step:396/1770 train_time:36498ms step_avg:94.56ms
step:397/1770 train_time:36595ms step_avg:94.56ms
step:398/1770 train_time:36693ms step_avg:94.57ms
step:399/1770 train_time:36790ms step_avg:94.58ms
step:400/1770 train_time:36888ms step_avg:94.58ms
step:401/1770 train_time:36985ms step_avg:94.59ms
step:402/1770 train_time:37081ms step_avg:94.60ms
step:403/1770 train_time:37178ms step_avg:94.60ms
step:404/1770 train_time:37275ms step_avg:94.61ms
step:405/1770 train_time:37372ms step_avg:94.61ms
step:406/1770 train_time:37469ms step_avg:94.62ms
step:407/1770 train_time:37566ms step_avg:94.62ms
step:408/1770 train_time:37663ms step_avg:94.63ms
step:409/1770 train_time:37760ms step_avg:94.64ms
step:410/1770 train_time:37857ms step_avg:94.64ms
step:411/1770 train_time:37955ms step_avg:94.65ms
step:412/1770 train_time:38052ms step_avg:94.66ms
step:413/1770 train_time:38150ms step_avg:94.66ms
step:414/1770 train_time:38246ms step_avg:94.67ms
step:415/1770 train_time:38343ms step_avg:94.67ms
step:416/1770 train_time:38439ms step_avg:94.68ms
step:417/1770 train_time:38537ms step_avg:94.69ms
step:418/1770 train_time:38633ms step_avg:94.69ms
step:419/1770 train_time:38730ms step_avg:94.70ms
step:420/1770 train_time:38828ms step_avg:94.70ms
step:421/1770 train_time:38925ms step_avg:94.71ms
step:422/1770 train_time:39022ms step_avg:94.71ms
step:423/1770 train_time:39119ms step_avg:94.72ms
step:424/1770 train_time:39217ms step_avg:94.73ms
step:425/1770 train_time:39314ms step_avg:94.73ms
step:426/1770 train_time:39411ms step_avg:94.74ms
step:427/1770 train_time:39508ms step_avg:94.74ms
step:428/1770 train_time:39604ms step_avg:94.75ms
step:429/1770 train_time:39701ms step_avg:94.75ms
step:430/1770 train_time:39799ms step_avg:94.76ms
step:431/1770 train_time:39895ms step_avg:94.76ms
step:432/1770 train_time:39993ms step_avg:94.77ms
step:433/1770 train_time:40090ms step_avg:94.78ms
step:434/1770 train_time:40187ms step_avg:94.78ms
step:435/1770 train_time:40284ms step_avg:94.79ms
step:436/1770 train_time:40380ms step_avg:94.79ms
step:437/1770 train_time:40477ms step_avg:94.79ms
step:438/1770 train_time:40574ms step_avg:94.80ms
step:439/1770 train_time:40672ms step_avg:94.81ms
step:440/1770 train_time:40769ms step_avg:94.81ms
step:441/1770 train_time:40866ms step_avg:94.82ms
step:442/1770 train_time:40964ms step_avg:94.82ms
step:443/1770 train_time:41060ms step_avg:94.83ms
step:444/1770 train_time:41157ms step_avg:94.83ms
step:445/1770 train_time:41254ms step_avg:94.84ms
step:446/1770 train_time:41351ms step_avg:94.84ms
step:447/1770 train_time:41448ms step_avg:94.85ms
step:448/1770 train_time:41545ms step_avg:94.85ms
step:449/1770 train_time:41642ms step_avg:94.86ms
step:450/1770 train_time:41739ms step_avg:94.86ms
step:451/1770 train_time:41837ms step_avg:94.87ms
step:452/1770 train_time:41934ms step_avg:94.87ms
step:453/1770 train_time:42030ms step_avg:94.88ms
step:454/1770 train_time:42127ms step_avg:94.88ms
step:455/1770 train_time:42224ms step_avg:94.88ms
step:456/1770 train_time:42320ms step_avg:94.89ms
step:457/1770 train_time:42417ms step_avg:94.89ms
step:458/1770 train_time:42514ms step_avg:94.90ms
step:459/1770 train_time:42612ms step_avg:94.90ms
step:460/1770 train_time:42708ms step_avg:94.91ms
step:461/1770 train_time:42805ms step_avg:94.91ms
step:462/1770 train_time:42901ms step_avg:94.91ms
step:463/1770 train_time:42998ms step_avg:94.92ms
step:464/1770 train_time:43095ms step_avg:94.92ms
step:465/1770 train_time:43192ms step_avg:94.93ms
step:466/1770 train_time:43289ms step_avg:94.93ms
step:467/1770 train_time:43386ms step_avg:94.94ms
step:468/1770 train_time:43483ms step_avg:94.94ms
step:469/1770 train_time:43580ms step_avg:94.94ms
step:470/1770 train_time:43677ms step_avg:94.95ms
step:471/1770 train_time:43774ms step_avg:94.95ms
step:472/1770 train_time:43871ms step_avg:94.96ms
step:473/1770 train_time:43968ms step_avg:94.96ms
step:474/1770 train_time:44064ms step_avg:94.97ms
step:475/1770 train_time:44161ms step_avg:94.97ms
step:476/1770 train_time:44258ms step_avg:94.97ms
step:477/1770 train_time:44355ms step_avg:94.98ms
step:478/1770 train_time:44452ms step_avg:94.98ms
step:479/1770 train_time:44550ms step_avg:94.99ms
step:480/1770 train_time:44647ms step_avg:94.99ms
step:481/1770 train_time:44744ms step_avg:95.00ms
step:482/1770 train_time:44840ms step_avg:95.00ms
step:483/1770 train_time:44939ms step_avg:95.01ms
step:484/1770 train_time:45035ms step_avg:95.01ms
step:485/1770 train_time:45132ms step_avg:95.02ms
step:486/1770 train_time:45229ms step_avg:95.02ms
step:487/1770 train_time:45326ms step_avg:95.02ms
step:488/1770 train_time:45423ms step_avg:95.03ms
step:489/1770 train_time:45520ms step_avg:95.03ms
step:490/1770 train_time:45617ms step_avg:95.03ms
step:491/1770 train_time:45714ms step_avg:95.04ms
step:492/1770 train_time:45812ms step_avg:95.04ms
step:493/1770 train_time:45909ms step_avg:95.05ms
step:494/1770 train_time:46005ms step_avg:95.05ms
step:495/1770 train_time:46102ms step_avg:95.05ms
step:496/1770 train_time:46199ms step_avg:95.06ms
step:497/1770 train_time:46296ms step_avg:95.06ms
step:498/1770 train_time:46393ms step_avg:95.07ms
step:499/1770 train_time:46490ms step_avg:95.07ms
step:500/1770 train_time:46588ms step_avg:95.08ms
step:500/1770 val_loss:3.7486 train_time:46684ms step_avg:95.27ms
step:501/1770 train_time:46709ms step_avg:95.13ms
step:502/1770 train_time:46792ms step_avg:95.11ms
step:503/1770 train_time:46892ms step_avg:95.11ms
step:504/1770 train_time:46990ms step_avg:95.12ms
step:505/1770 train_time:47087ms step_avg:95.13ms
step:506/1770 train_time:47184ms step_avg:95.13ms
step:507/1770 train_time:47281ms step_avg:95.13ms
step:508/1770 train_time:47377ms step_avg:95.13ms
step:509/1770 train_time:47473ms step_avg:95.14ms
step:510/1770 train_time:47570ms step_avg:95.14ms
step:511/1770 train_time:47667ms step_avg:95.14ms
step:512/1770 train_time:47765ms step_avg:95.15ms
step:513/1770 train_time:47863ms step_avg:95.16ms
step:514/1770 train_time:47961ms step_avg:95.16ms
step:515/1770 train_time:48058ms step_avg:95.17ms
step:516/1770 train_time:48155ms step_avg:95.17ms
step:517/1770 train_time:48251ms step_avg:95.17ms
step:518/1770 train_time:48348ms step_avg:95.17ms
step:519/1770 train_time:48445ms step_avg:95.18ms
step:520/1770 train_time:48541ms step_avg:95.18ms
step:521/1770 train_time:48638ms step_avg:95.18ms
step:522/1770 train_time:48735ms step_avg:95.18ms
step:523/1770 train_time:48832ms step_avg:95.19ms
step:524/1770 train_time:48930ms step_avg:95.19ms
step:525/1770 train_time:49028ms step_avg:95.20ms
step:526/1770 train_time:49125ms step_avg:95.20ms
step:527/1770 train_time:49222ms step_avg:95.21ms
step:528/1770 train_time:49320ms step_avg:95.21ms
step:529/1770 train_time:49417ms step_avg:95.22ms
step:530/1770 train_time:49514ms step_avg:95.22ms
step:531/1770 train_time:49611ms step_avg:95.22ms
step:532/1770 train_time:49708ms step_avg:95.23ms
step:533/1770 train_time:49805ms step_avg:95.23ms
step:534/1770 train_time:49903ms step_avg:95.23ms
step:535/1770 train_time:50000ms step_avg:95.24ms
step:536/1770 train_time:50097ms step_avg:95.24ms
step:537/1770 train_time:50194ms step_avg:95.25ms
step:538/1770 train_time:50291ms step_avg:95.25ms
step:539/1770 train_time:50390ms step_avg:95.25ms
step:540/1770 train_time:50487ms step_avg:95.26ms
step:541/1770 train_time:50585ms step_avg:95.26ms
step:542/1770 train_time:50682ms step_avg:95.27ms
step:543/1770 train_time:50779ms step_avg:95.27ms
step:544/1770 train_time:50876ms step_avg:95.27ms
step:545/1770 train_time:50973ms step_avg:95.28ms
step:546/1770 train_time:51070ms step_avg:95.28ms
step:547/1770 train_time:51167ms step_avg:95.28ms
step:548/1770 train_time:51265ms step_avg:95.29ms
step:549/1770 train_time:51362ms step_avg:95.29ms
step:550/1770 train_time:51460ms step_avg:95.30ms
step:551/1770 train_time:51557ms step_avg:95.30ms
step:552/1770 train_time:51654ms step_avg:95.30ms
step:553/1770 train_time:51751ms step_avg:95.31ms
step:554/1770 train_time:51850ms step_avg:95.31ms
step:555/1770 train_time:51947ms step_avg:95.32ms
step:556/1770 train_time:52044ms step_avg:95.32ms
step:557/1770 train_time:52142ms step_avg:95.32ms
step:558/1770 train_time:52239ms step_avg:95.33ms
step:559/1770 train_time:52336ms step_avg:95.33ms
step:560/1770 train_time:52433ms step_avg:95.33ms
step:561/1770 train_time:52531ms step_avg:95.34ms
step:562/1770 train_time:52628ms step_avg:95.34ms
step:563/1770 train_time:52726ms step_avg:95.34ms
step:564/1770 train_time:52823ms step_avg:95.35ms
step:565/1770 train_time:52920ms step_avg:95.35ms
step:566/1770 train_time:53017ms step_avg:95.36ms
step:567/1770 train_time:53115ms step_avg:95.36ms
step:568/1770 train_time:53212ms step_avg:95.36ms
step:569/1770 train_time:53309ms step_avg:95.37ms
step:570/1770 train_time:53407ms step_avg:95.37ms
step:571/1770 train_time:53504ms step_avg:95.37ms
step:572/1770 train_time:53602ms step_avg:95.38ms
step:573/1770 train_time:53699ms step_avg:95.38ms
step:574/1770 train_time:53795ms step_avg:95.38ms
step:575/1770 train_time:53893ms step_avg:95.39ms
step:576/1770 train_time:53990ms step_avg:95.39ms
step:577/1770 train_time:54088ms step_avg:95.39ms
step:578/1770 train_time:54186ms step_avg:95.40ms
step:579/1770 train_time:54284ms step_avg:95.40ms
step:580/1770 train_time:54382ms step_avg:95.41ms
step:581/1770 train_time:54478ms step_avg:95.41ms
step:582/1770 train_time:54576ms step_avg:95.41ms
step:583/1770 train_time:54673ms step_avg:95.41ms
step:584/1770 train_time:54770ms step_avg:95.42ms
step:585/1770 train_time:54868ms step_avg:95.42ms
step:586/1770 train_time:54966ms step_avg:95.43ms
step:587/1770 train_time:55063ms step_avg:95.43ms
step:588/1770 train_time:55160ms step_avg:95.43ms
step:589/1770 train_time:55258ms step_avg:95.44ms
step:590/1770 train_time:55354ms step_avg:95.44ms
step:591/1770 train_time:55452ms step_avg:95.44ms
step:592/1770 train_time:55550ms step_avg:95.45ms
step:593/1770 train_time:55647ms step_avg:95.45ms
step:594/1770 train_time:55744ms step_avg:95.45ms
step:595/1770 train_time:55842ms step_avg:95.46ms
step:596/1770 train_time:55939ms step_avg:95.46ms
step:597/1770 train_time:56036ms step_avg:95.46ms
step:598/1770 train_time:56133ms step_avg:95.46ms
step:599/1770 train_time:56231ms step_avg:95.47ms
step:600/1770 train_time:56329ms step_avg:95.47ms
step:601/1770 train_time:56426ms step_avg:95.48ms
step:602/1770 train_time:56524ms step_avg:95.48ms
step:603/1770 train_time:56621ms step_avg:95.48ms
step:604/1770 train_time:56718ms step_avg:95.48ms
step:605/1770 train_time:56814ms step_avg:95.49ms
step:606/1770 train_time:56912ms step_avg:95.49ms
step:607/1770 train_time:57010ms step_avg:95.49ms
step:608/1770 train_time:57108ms step_avg:95.50ms
step:609/1770 train_time:57206ms step_avg:95.50ms
step:610/1770 train_time:57304ms step_avg:95.51ms
step:611/1770 train_time:57401ms step_avg:95.51ms
step:612/1770 train_time:57498ms step_avg:95.51ms
step:613/1770 train_time:57595ms step_avg:95.51ms
step:614/1770 train_time:57692ms step_avg:95.52ms
step:615/1770 train_time:57790ms step_avg:95.52ms
step:616/1770 train_time:57887ms step_avg:95.52ms
step:617/1770 train_time:57985ms step_avg:95.53ms
step:618/1770 train_time:58083ms step_avg:95.53ms
step:619/1770 train_time:58180ms step_avg:95.53ms
step:620/1770 train_time:58278ms step_avg:95.54ms
step:621/1770 train_time:58374ms step_avg:95.54ms
step:622/1770 train_time:58472ms step_avg:95.54ms
step:623/1770 train_time:58569ms step_avg:95.55ms
step:624/1770 train_time:58667ms step_avg:95.55ms
step:625/1770 train_time:58765ms step_avg:95.55ms
step:625/1770 val_loss:3.6604 train_time:58860ms step_avg:95.71ms
step:626/1770 train_time:58882ms step_avg:95.59ms
step:627/1770 train_time:58967ms step_avg:95.57ms
step:628/1770 train_time:59067ms step_avg:95.58ms
step:629/1770 train_time:59164ms step_avg:95.58ms
step:630/1770 train_time:59261ms step_avg:95.58ms
step:631/1770 train_time:59358ms step_avg:95.59ms
step:632/1770 train_time:59455ms step_avg:95.59ms
step:633/1770 train_time:59551ms step_avg:95.59ms
step:634/1770 train_time:59648ms step_avg:95.59ms
step:635/1770 train_time:59745ms step_avg:95.59ms
step:636/1770 train_time:59842ms step_avg:95.59ms
step:637/1770 train_time:59941ms step_avg:95.60ms
step:638/1770 train_time:60039ms step_avg:95.60ms
step:639/1770 train_time:60137ms step_avg:95.61ms
step:640/1770 train_time:60234ms step_avg:95.61ms
step:641/1770 train_time:60332ms step_avg:95.61ms
step:642/1770 train_time:60429ms step_avg:95.61ms
step:643/1770 train_time:60526ms step_avg:95.62ms
step:644/1770 train_time:60623ms step_avg:95.62ms
step:645/1770 train_time:60720ms step_avg:95.62ms
step:646/1770 train_time:60817ms step_avg:95.62ms
step:647/1770 train_time:60914ms step_avg:95.63ms
step:648/1770 train_time:61012ms step_avg:95.63ms
step:649/1770 train_time:61109ms step_avg:95.63ms
step:650/1770 train_time:61206ms step_avg:95.64ms
step:651/1770 train_time:61304ms step_avg:95.64ms
step:652/1770 train_time:61402ms step_avg:95.64ms
step:653/1770 train_time:61500ms step_avg:95.64ms
step:654/1770 train_time:61597ms step_avg:95.65ms
step:655/1770 train_time:61694ms step_avg:95.65ms
step:656/1770 train_time:61791ms step_avg:95.65ms
step:657/1770 train_time:61888ms step_avg:95.65ms
step:658/1770 train_time:61987ms step_avg:95.66ms
step:659/1770 train_time:62086ms step_avg:95.66ms
step:660/1770 train_time:62186ms step_avg:95.67ms
step:661/1770 train_time:62286ms step_avg:95.68ms
step:662/1770 train_time:62386ms step_avg:95.68ms
step:663/1770 train_time:62485ms step_avg:95.69ms
step:664/1770 train_time:62585ms step_avg:95.70ms
step:665/1770 train_time:62684ms step_avg:95.70ms
step:666/1770 train_time:62783ms step_avg:95.71ms
step:667/1770 train_time:62883ms step_avg:95.71ms
step:668/1770 train_time:62982ms step_avg:95.72ms
step:669/1770 train_time:63081ms step_avg:95.72ms
step:670/1770 train_time:63180ms step_avg:95.73ms
step:671/1770 train_time:63279ms step_avg:95.73ms
step:672/1770 train_time:63378ms step_avg:95.74ms
step:673/1770 train_time:63478ms step_avg:95.74ms
step:674/1770 train_time:63578ms step_avg:95.75ms
step:675/1770 train_time:63677ms step_avg:95.76ms
step:676/1770 train_time:63776ms step_avg:95.76ms
step:677/1770 train_time:63876ms step_avg:95.77ms
step:678/1770 train_time:63975ms step_avg:95.77ms
step:679/1770 train_time:64073ms step_avg:95.77ms
step:680/1770 train_time:64172ms step_avg:95.78ms
step:681/1770 train_time:64270ms step_avg:95.78ms
step:682/1770 train_time:64369ms step_avg:95.79ms
step:683/1770 train_time:64469ms step_avg:95.79ms
step:684/1770 train_time:64568ms step_avg:95.80ms
step:685/1770 train_time:64667ms step_avg:95.80ms
step:686/1770 train_time:64767ms step_avg:95.81ms
step:687/1770 train_time:64866ms step_avg:95.81ms
step:688/1770 train_time:64965ms step_avg:95.82ms
step:689/1770 train_time:65065ms step_avg:95.82ms
step:690/1770 train_time:65164ms step_avg:95.83ms
step:691/1770 train_time:65262ms step_avg:95.83ms
step:692/1770 train_time:65362ms step_avg:95.84ms
step:693/1770 train_time:65461ms step_avg:95.84ms
step:694/1770 train_time:65561ms step_avg:95.85ms
step:695/1770 train_time:65660ms step_avg:95.85ms
step:696/1770 train_time:65760ms step_avg:95.86ms
step:697/1770 train_time:65859ms step_avg:95.86ms
step:698/1770 train_time:65959ms step_avg:95.87ms
step:699/1770 train_time:66058ms step_avg:95.88ms
step:700/1770 train_time:66157ms step_avg:95.88ms
step:701/1770 train_time:66257ms step_avg:95.88ms
step:702/1770 train_time:66355ms step_avg:95.89ms
step:703/1770 train_time:66455ms step_avg:95.89ms
step:704/1770 train_time:66554ms step_avg:95.90ms
step:705/1770 train_time:66653ms step_avg:95.90ms
step:706/1770 train_time:66751ms step_avg:95.91ms
step:707/1770 train_time:66850ms step_avg:95.91ms
step:708/1770 train_time:66949ms step_avg:95.92ms
step:709/1770 train_time:67048ms step_avg:95.92ms
step:710/1770 train_time:67147ms step_avg:95.92ms
step:711/1770 train_time:67246ms step_avg:95.93ms
step:712/1770 train_time:67345ms step_avg:95.93ms
step:713/1770 train_time:67445ms step_avg:95.94ms
step:714/1770 train_time:67544ms step_avg:95.94ms
step:715/1770 train_time:67643ms step_avg:95.95ms
step:716/1770 train_time:67742ms step_avg:95.95ms
step:717/1770 train_time:67842ms step_avg:95.96ms
step:718/1770 train_time:67941ms step_avg:95.96ms
step:719/1770 train_time:68040ms step_avg:95.97ms
step:720/1770 train_time:68140ms step_avg:95.97ms
step:721/1770 train_time:68239ms step_avg:95.98ms
step:722/1770 train_time:68339ms step_avg:95.98ms
step:723/1770 train_time:68438ms step_avg:95.99ms
step:724/1770 train_time:68537ms step_avg:95.99ms
step:725/1770 train_time:68637ms step_avg:96.00ms
step:726/1770 train_time:68737ms step_avg:96.00ms
step:727/1770 train_time:68835ms step_avg:96.00ms
step:728/1770 train_time:68934ms step_avg:96.01ms
step:729/1770 train_time:69034ms step_avg:96.01ms
step:730/1770 train_time:69132ms step_avg:96.02ms
step:731/1770 train_time:69231ms step_avg:96.02ms
step:732/1770 train_time:69330ms step_avg:96.02ms
step:733/1770 train_time:69429ms step_avg:96.03ms
step:734/1770 train_time:69527ms step_avg:96.03ms
step:735/1770 train_time:69626ms step_avg:96.04ms
step:736/1770 train_time:69726ms step_avg:96.04ms
step:737/1770 train_time:69824ms step_avg:96.04ms
step:738/1770 train_time:69923ms step_avg:96.05ms
step:739/1770 train_time:70023ms step_avg:96.05ms
step:740/1770 train_time:70122ms step_avg:96.06ms
step:741/1770 train_time:70222ms step_avg:96.06ms
step:742/1770 train_time:70322ms step_avg:96.07ms
step:743/1770 train_time:70421ms step_avg:96.07ms
step:744/1770 train_time:70520ms step_avg:96.08ms
step:745/1770 train_time:70620ms step_avg:96.08ms
step:746/1770 train_time:70720ms step_avg:96.09ms
step:747/1770 train_time:70819ms step_avg:96.09ms
step:748/1770 train_time:70919ms step_avg:96.10ms
step:749/1770 train_time:71017ms step_avg:96.10ms
step:750/1770 train_time:71116ms step_avg:96.10ms
step:750/1770 val_loss:3.5976 train_time:71213ms step_avg:96.23ms
step:751/1770 train_time:71236ms step_avg:96.13ms
step:752/1770 train_time:71318ms step_avg:96.12ms
step:753/1770 train_time:71418ms step_avg:96.12ms
step:754/1770 train_time:71517ms step_avg:96.13ms
step:755/1770 train_time:71616ms step_avg:96.13ms
step:756/1770 train_time:71715ms step_avg:96.13ms
step:757/1770 train_time:71813ms step_avg:96.14ms
step:758/1770 train_time:71911ms step_avg:96.14ms
step:759/1770 train_time:72009ms step_avg:96.14ms
step:760/1770 train_time:72108ms step_avg:96.14ms
step:761/1770 train_time:72207ms step_avg:96.15ms
step:762/1770 train_time:72307ms step_avg:96.15ms
step:763/1770 train_time:72406ms step_avg:96.16ms
step:764/1770 train_time:72505ms step_avg:96.16ms
step:765/1770 train_time:72604ms step_avg:96.16ms
step:766/1770 train_time:72704ms step_avg:96.17ms
step:767/1770 train_time:72803ms step_avg:96.17ms
step:768/1770 train_time:72903ms step_avg:96.18ms
step:769/1770 train_time:73002ms step_avg:96.18ms
step:770/1770 train_time:73101ms step_avg:96.19ms
step:771/1770 train_time:73200ms step_avg:96.19ms
step:772/1770 train_time:73299ms step_avg:96.19ms
step:773/1770 train_time:73399ms step_avg:96.20ms
step:774/1770 train_time:73499ms step_avg:96.20ms
step:775/1770 train_time:73598ms step_avg:96.21ms
step:776/1770 train_time:73698ms step_avg:96.21ms
step:777/1770 train_time:73798ms step_avg:96.22ms
step:778/1770 train_time:73897ms step_avg:96.22ms
step:779/1770 train_time:73996ms step_avg:96.22ms
step:780/1770 train_time:74095ms step_avg:96.23ms
step:781/1770 train_time:74194ms step_avg:96.23ms
step:782/1770 train_time:74293ms step_avg:96.23ms
step:783/1770 train_time:74392ms step_avg:96.24ms
step:784/1770 train_time:74491ms step_avg:96.24ms
step:785/1770 train_time:74590ms step_avg:96.25ms
step:786/1770 train_time:74689ms step_avg:96.25ms
step:787/1770 train_time:74787ms step_avg:96.25ms
step:788/1770 train_time:74886ms step_avg:96.25ms
step:789/1770 train_time:74985ms step_avg:96.26ms
step:790/1770 train_time:75084ms step_avg:96.26ms
step:791/1770 train_time:75184ms step_avg:96.27ms
step:792/1770 train_time:75284ms step_avg:96.27ms
step:793/1770 train_time:75384ms step_avg:96.28ms
step:794/1770 train_time:75485ms step_avg:96.28ms
step:795/1770 train_time:75584ms step_avg:96.29ms
step:796/1770 train_time:75684ms step_avg:96.29ms
step:797/1770 train_time:75784ms step_avg:96.29ms
step:798/1770 train_time:75883ms step_avg:96.30ms
step:799/1770 train_time:75982ms step_avg:96.30ms
step:800/1770 train_time:76081ms step_avg:96.30ms
step:801/1770 train_time:76180ms step_avg:96.31ms
step:802/1770 train_time:76280ms step_avg:96.31ms
step:803/1770 train_time:76379ms step_avg:96.32ms
step:804/1770 train_time:76480ms step_avg:96.32ms
step:805/1770 train_time:76580ms step_avg:96.33ms
step:806/1770 train_time:76679ms step_avg:96.33ms
step:807/1770 train_time:76779ms step_avg:96.34ms
step:808/1770 train_time:76880ms step_avg:96.34ms
step:809/1770 train_time:76979ms step_avg:96.34ms
step:810/1770 train_time:77079ms step_avg:96.35ms
step:811/1770 train_time:77178ms step_avg:96.35ms
step:812/1770 train_time:77277ms step_avg:96.36ms
step:813/1770 train_time:77377ms step_avg:96.36ms
step:814/1770 train_time:77477ms step_avg:96.36ms
step:815/1770 train_time:77576ms step_avg:96.37ms
step:816/1770 train_time:77676ms step_avg:96.37ms
step:817/1770 train_time:77777ms step_avg:96.38ms
step:818/1770 train_time:77877ms step_avg:96.38ms
step:819/1770 train_time:77977ms step_avg:96.39ms
step:820/1770 train_time:78078ms step_avg:96.39ms
step:821/1770 train_time:78178ms step_avg:96.40ms
step:822/1770 train_time:78278ms step_avg:96.40ms
step:823/1770 train_time:78378ms step_avg:96.41ms
step:824/1770 train_time:78478ms step_avg:96.41ms
step:825/1770 train_time:78577ms step_avg:96.41ms
step:826/1770 train_time:78677ms step_avg:96.42ms
step:827/1770 train_time:78778ms step_avg:96.42ms
step:828/1770 train_time:78878ms step_avg:96.43ms
step:829/1770 train_time:78977ms step_avg:96.43ms
step:830/1770 train_time:79077ms step_avg:96.44ms
step:831/1770 train_time:79177ms step_avg:96.44ms
step:832/1770 train_time:79277ms step_avg:96.44ms
step:833/1770 train_time:79377ms step_avg:96.45ms
step:834/1770 train_time:79477ms step_avg:96.45ms
step:835/1770 train_time:79577ms step_avg:96.46ms
step:836/1770 train_time:79676ms step_avg:96.46ms
step:837/1770 train_time:79776ms step_avg:96.46ms
step:838/1770 train_time:79877ms step_avg:96.47ms
step:839/1770 train_time:79977ms step_avg:96.47ms
step:840/1770 train_time:80077ms step_avg:96.48ms
step:841/1770 train_time:80177ms step_avg:96.48ms
step:842/1770 train_time:80277ms step_avg:96.49ms
step:843/1770 train_time:80377ms step_avg:96.49ms
step:844/1770 train_time:80478ms step_avg:96.50ms
step:845/1770 train_time:80578ms step_avg:96.50ms
step:846/1770 train_time:80677ms step_avg:96.50ms
step:847/1770 train_time:80776ms step_avg:96.51ms
step:848/1770 train_time:80875ms step_avg:96.51ms
step:849/1770 train_time:80975ms step_avg:96.51ms
step:850/1770 train_time:81074ms step_avg:96.52ms
step:851/1770 train_time:81173ms step_avg:96.52ms
step:852/1770 train_time:81273ms step_avg:96.52ms
step:853/1770 train_time:81372ms step_avg:96.53ms
step:854/1770 train_time:81471ms step_avg:96.53ms
step:855/1770 train_time:81570ms step_avg:96.53ms
step:856/1770 train_time:81669ms step_avg:96.54ms
step:857/1770 train_time:81768ms step_avg:96.54ms
step:858/1770 train_time:81867ms step_avg:96.54ms
step:859/1770 train_time:81966ms step_avg:96.54ms
step:860/1770 train_time:82065ms step_avg:96.55ms
step:861/1770 train_time:82164ms step_avg:96.55ms
step:862/1770 train_time:82264ms step_avg:96.55ms
step:863/1770 train_time:82364ms step_avg:96.56ms
step:864/1770 train_time:82463ms step_avg:96.56ms
step:865/1770 train_time:82562ms step_avg:96.56ms
step:866/1770 train_time:82662ms step_avg:96.57ms
step:867/1770 train_time:82761ms step_avg:96.57ms
step:868/1770 train_time:82861ms step_avg:96.57ms
step:869/1770 train_time:82961ms step_avg:96.58ms
step:870/1770 train_time:83060ms step_avg:96.58ms
step:871/1770 train_time:83160ms step_avg:96.58ms
step:872/1770 train_time:83259ms step_avg:96.59ms
step:873/1770 train_time:83359ms step_avg:96.59ms
step:874/1770 train_time:83459ms step_avg:96.60ms
step:875/1770 train_time:83558ms step_avg:96.60ms
step:875/1770 val_loss:3.5482 train_time:83656ms step_avg:96.71ms
step:876/1770 train_time:83679ms step_avg:96.63ms
step:877/1770 train_time:83762ms step_avg:96.61ms
step:878/1770 train_time:83862ms step_avg:96.62ms
step:879/1770 train_time:83962ms step_avg:96.62ms
step:880/1770 train_time:84061ms step_avg:96.62ms
step:881/1770 train_time:84159ms step_avg:96.62ms
step:882/1770 train_time:84258ms step_avg:96.63ms
step:883/1770 train_time:84356ms step_avg:96.63ms
step:884/1770 train_time:84455ms step_avg:96.63ms
step:885/1770 train_time:84553ms step_avg:96.63ms
step:886/1770 train_time:84653ms step_avg:96.64ms
step:887/1770 train_time:84754ms step_avg:96.64ms
step:888/1770 train_time:84855ms step_avg:96.65ms
step:889/1770 train_time:84954ms step_avg:96.65ms
step:890/1770 train_time:85054ms step_avg:96.65ms
step:891/1770 train_time:85154ms step_avg:96.66ms
step:892/1770 train_time:85254ms step_avg:96.66ms
step:893/1770 train_time:85354ms step_avg:96.66ms
step:894/1770 train_time:85454ms step_avg:96.67ms
step:895/1770 train_time:85554ms step_avg:96.67ms
step:896/1770 train_time:85654ms step_avg:96.67ms
step:897/1770 train_time:85753ms step_avg:96.68ms
step:898/1770 train_time:85853ms step_avg:96.68ms
step:899/1770 train_time:85952ms step_avg:96.68ms
step:900/1770 train_time:86052ms step_avg:96.69ms
step:901/1770 train_time:86152ms step_avg:96.69ms
step:902/1770 train_time:86254ms step_avg:96.70ms
step:903/1770 train_time:86353ms step_avg:96.70ms
step:904/1770 train_time:86453ms step_avg:96.70ms
step:905/1770 train_time:86552ms step_avg:96.71ms
step:906/1770 train_time:86652ms step_avg:96.71ms
step:907/1770 train_time:86752ms step_avg:96.71ms
step:908/1770 train_time:86852ms step_avg:96.72ms
step:909/1770 train_time:86951ms step_avg:96.72ms
step:910/1770 train_time:87052ms step_avg:96.72ms
step:911/1770 train_time:87153ms step_avg:96.73ms
step:912/1770 train_time:87253ms step_avg:96.73ms
step:913/1770 train_time:87352ms step_avg:96.74ms
step:914/1770 train_time:87451ms step_avg:96.74ms
step:915/1770 train_time:87551ms step_avg:96.74ms
step:916/1770 train_time:87651ms step_avg:96.74ms
step:917/1770 train_time:87750ms step_avg:96.75ms
step:918/1770 train_time:87849ms step_avg:96.75ms
step:919/1770 train_time:87949ms step_avg:96.75ms
step:920/1770 train_time:88050ms step_avg:96.76ms
step:921/1770 train_time:88151ms step_avg:96.76ms
step:922/1770 train_time:88252ms step_avg:96.77ms
step:923/1770 train_time:88353ms step_avg:96.77ms
step:924/1770 train_time:88454ms step_avg:96.78ms
step:925/1770 train_time:88555ms step_avg:96.78ms
step:926/1770 train_time:88655ms step_avg:96.79ms
step:927/1770 train_time:88756ms step_avg:96.79ms
step:928/1770 train_time:88858ms step_avg:96.79ms
step:929/1770 train_time:88958ms step_avg:96.80ms
step:930/1770 train_time:89059ms step_avg:96.80ms
step:931/1770 train_time:89160ms step_avg:96.81ms
step:932/1770 train_time:89261ms step_avg:96.81ms
step:933/1770 train_time:89363ms step_avg:96.82ms
step:934/1770 train_time:89462ms step_avg:96.82ms
step:935/1770 train_time:89563ms step_avg:96.82ms
step:936/1770 train_time:89664ms step_avg:96.83ms
step:937/1770 train_time:89765ms step_avg:96.83ms
step:938/1770 train_time:89866ms step_avg:96.84ms
step:939/1770 train_time:89967ms step_avg:96.84ms
step:940/1770 train_time:90067ms step_avg:96.85ms
step:941/1770 train_time:90168ms step_avg:96.85ms
step:942/1770 train_time:90268ms step_avg:96.85ms
step:943/1770 train_time:90370ms step_avg:96.86ms
step:944/1770 train_time:90470ms step_avg:96.86ms
step:945/1770 train_time:90571ms step_avg:96.87ms
step:946/1770 train_time:90673ms step_avg:96.87ms
step:947/1770 train_time:90775ms step_avg:96.88ms
step:948/1770 train_time:90876ms step_avg:96.88ms
step:949/1770 train_time:90978ms step_avg:96.89ms
step:950/1770 train_time:91080ms step_avg:96.89ms
step:951/1770 train_time:91182ms step_avg:96.90ms
step:952/1770 train_time:91283ms step_avg:96.90ms
step:953/1770 train_time:91383ms step_avg:96.91ms
step:954/1770 train_time:91483ms step_avg:96.91ms
step:955/1770 train_time:91583ms step_avg:96.91ms
step:956/1770 train_time:91684ms step_avg:96.92ms
step:957/1770 train_time:91785ms step_avg:96.92ms
step:958/1770 train_time:91887ms step_avg:96.93ms
step:959/1770 train_time:91987ms step_avg:96.93ms
step:960/1770 train_time:92088ms step_avg:96.93ms
step:961/1770 train_time:92189ms step_avg:96.94ms
step:962/1770 train_time:92290ms step_avg:96.94ms
step:963/1770 train_time:92391ms step_avg:96.95ms
step:964/1770 train_time:92492ms step_avg:96.95ms
step:965/1770 train_time:92593ms step_avg:96.96ms
step:966/1770 train_time:92694ms step_avg:96.96ms
step:967/1770 train_time:92796ms step_avg:96.97ms
step:968/1770 train_time:92896ms step_avg:96.97ms
step:969/1770 train_time:92997ms step_avg:96.97ms
step:970/1770 train_time:93098ms step_avg:96.98ms
step:971/1770 train_time:93200ms step_avg:96.98ms
step:972/1770 train_time:93300ms step_avg:96.99ms
step:973/1770 train_time:93400ms step_avg:96.99ms
step:974/1770 train_time:93501ms step_avg:96.99ms
step:975/1770 train_time:93603ms step_avg:97.00ms
step:976/1770 train_time:93704ms step_avg:97.00ms
step:977/1770 train_time:93804ms step_avg:97.01ms
step:978/1770 train_time:93905ms step_avg:97.01ms
step:979/1770 train_time:94007ms step_avg:97.01ms
step:980/1770 train_time:94108ms step_avg:97.02ms
step:981/1770 train_time:94209ms step_avg:97.02ms
step:982/1770 train_time:94309ms step_avg:97.03ms
step:983/1770 train_time:94409ms step_avg:97.03ms
step:984/1770 train_time:94511ms step_avg:97.03ms
step:985/1770 train_time:94612ms step_avg:97.04ms
step:986/1770 train_time:94714ms step_avg:97.04ms
step:987/1770 train_time:94815ms step_avg:97.05ms
step:988/1770 train_time:94916ms step_avg:97.05ms
step:989/1770 train_time:95018ms step_avg:97.06ms
step:990/1770 train_time:95119ms step_avg:97.06ms
step:991/1770 train_time:95220ms step_avg:97.06ms
step:992/1770 train_time:95322ms step_avg:97.07ms
step:993/1770 train_time:95422ms step_avg:97.07ms
step:994/1770 train_time:95523ms step_avg:97.08ms
step:995/1770 train_time:95624ms step_avg:97.08ms
step:996/1770 train_time:95725ms step_avg:97.08ms
step:997/1770 train_time:95825ms step_avg:97.09ms
step:998/1770 train_time:95925ms step_avg:97.09ms
step:999/1770 train_time:96026ms step_avg:97.09ms
step:1000/1770 train_time:96128ms step_avg:97.10ms
step:1000/1770 val_loss:3.5106 train_time:96228ms step_avg:97.20ms
step:1001/1770 train_time:96251ms step_avg:97.12ms
step:1002/1770 train_time:96339ms step_avg:97.12ms
step:1003/1770 train_time:96441ms step_avg:97.12ms
step:1004/1770 train_time:96543ms step_avg:97.13ms
step:1005/1770 train_time:96643ms step_avg:97.13ms
step:1006/1770 train_time:96743ms step_avg:97.13ms
step:1007/1770 train_time:96843ms step_avg:97.13ms
step:1008/1770 train_time:96943ms step_avg:97.14ms
step:1009/1770 train_time:97043ms step_avg:97.14ms
step:1010/1770 train_time:97143ms step_avg:97.14ms
step:1011/1770 train_time:97244ms step_avg:97.15ms
step:1012/1770 train_time:97346ms step_avg:97.15ms
step:1013/1770 train_time:97447ms step_avg:97.16ms
step:1014/1770 train_time:97549ms step_avg:97.16ms
step:1015/1770 train_time:97649ms step_avg:97.16ms
step:1016/1770 train_time:97750ms step_avg:97.17ms
step:1017/1770 train_time:97851ms step_avg:97.17ms
step:1018/1770 train_time:97952ms step_avg:97.17ms
step:1019/1770 train_time:98053ms step_avg:97.18ms
step:1020/1770 train_time:98153ms step_avg:97.18ms
step:1021/1770 train_time:98254ms step_avg:97.18ms
step:1022/1770 train_time:98356ms step_avg:97.19ms
step:1023/1770 train_time:98457ms step_avg:97.19ms
step:1024/1770 train_time:98559ms step_avg:97.20ms
step:1025/1770 train_time:98659ms step_avg:97.20ms
step:1026/1770 train_time:98759ms step_avg:97.20ms
step:1027/1770 train_time:98860ms step_avg:97.21ms
step:1028/1770 train_time:98962ms step_avg:97.21ms
step:1029/1770 train_time:99062ms step_avg:97.21ms
step:1030/1770 train_time:99162ms step_avg:97.22ms
step:1031/1770 train_time:99263ms step_avg:97.22ms
step:1032/1770 train_time:99364ms step_avg:97.22ms
step:1033/1770 train_time:99465ms step_avg:97.23ms
step:1034/1770 train_time:99565ms step_avg:97.23ms
step:1035/1770 train_time:99666ms step_avg:97.23ms
step:1036/1770 train_time:99767ms step_avg:97.24ms
step:1037/1770 train_time:99869ms step_avg:97.24ms
step:1038/1770 train_time:99970ms step_avg:97.25ms
step:1039/1770 train_time:100071ms step_avg:97.25ms
step:1040/1770 train_time:100171ms step_avg:97.25ms
step:1041/1770 train_time:100272ms step_avg:97.26ms
step:1042/1770 train_time:100374ms step_avg:97.26ms
step:1043/1770 train_time:100475ms step_avg:97.27ms
step:1044/1770 train_time:100576ms step_avg:97.27ms
step:1045/1770 train_time:100676ms step_avg:97.27ms
step:1046/1770 train_time:100777ms step_avg:97.27ms
step:1047/1770 train_time:100877ms step_avg:97.28ms
step:1048/1770 train_time:100978ms step_avg:97.28ms
step:1049/1770 train_time:101079ms step_avg:97.28ms
step:1050/1770 train_time:101180ms step_avg:97.29ms
step:1051/1770 train_time:101281ms step_avg:97.29ms
step:1052/1770 train_time:101382ms step_avg:97.30ms
step:1053/1770 train_time:101482ms step_avg:97.30ms
step:1054/1770 train_time:101583ms step_avg:97.30ms
step:1055/1770 train_time:101683ms step_avg:97.30ms
step:1056/1770 train_time:101784ms step_avg:97.31ms
step:1057/1770 train_time:101885ms step_avg:97.31ms
step:1058/1770 train_time:101987ms step_avg:97.32ms
step:1059/1770 train_time:102090ms step_avg:97.32ms
step:1060/1770 train_time:102191ms step_avg:97.33ms
step:1061/1770 train_time:102293ms step_avg:97.33ms
step:1062/1770 train_time:102395ms step_avg:97.33ms
step:1063/1770 train_time:102498ms step_avg:97.34ms
step:1064/1770 train_time:102599ms step_avg:97.34ms
step:1065/1770 train_time:102700ms step_avg:97.35ms
step:1066/1770 train_time:102801ms step_avg:97.35ms
step:1067/1770 train_time:102902ms step_avg:97.35ms
step:1068/1770 train_time:103004ms step_avg:97.36ms
step:1069/1770 train_time:103106ms step_avg:97.36ms
step:1070/1770 train_time:103207ms step_avg:97.36ms
step:1071/1770 train_time:103307ms step_avg:97.37ms
step:1072/1770 train_time:103408ms step_avg:97.37ms
step:1073/1770 train_time:103509ms step_avg:97.37ms
step:1074/1770 train_time:103611ms step_avg:97.38ms
step:1075/1770 train_time:103712ms step_avg:97.38ms
step:1076/1770 train_time:103815ms step_avg:97.39ms
step:1077/1770 train_time:103916ms step_avg:97.39ms
step:1078/1770 train_time:104018ms step_avg:97.40ms
step:1079/1770 train_time:104119ms step_avg:97.40ms
step:1080/1770 train_time:104219ms step_avg:97.40ms
step:1081/1770 train_time:104319ms step_avg:97.40ms
step:1082/1770 train_time:104420ms step_avg:97.41ms
step:1083/1770 train_time:104521ms step_avg:97.41ms
step:1084/1770 train_time:104621ms step_avg:97.41ms
step:1085/1770 train_time:104723ms step_avg:97.42ms
step:1086/1770 train_time:104824ms step_avg:97.42ms
step:1087/1770 train_time:104924ms step_avg:97.42ms
step:1088/1770 train_time:105026ms step_avg:97.43ms
step:1089/1770 train_time:105127ms step_avg:97.43ms
step:1090/1770 train_time:105229ms step_avg:97.43ms
step:1091/1770 train_time:105331ms step_avg:97.44ms
step:1092/1770 train_time:105432ms step_avg:97.44ms
step:1093/1770 train_time:105533ms step_avg:97.44ms
step:1094/1770 train_time:105635ms step_avg:97.45ms
step:1095/1770 train_time:105735ms step_avg:97.45ms
step:1096/1770 train_time:105836ms step_avg:97.46ms
step:1097/1770 train_time:105938ms step_avg:97.46ms
step:1098/1770 train_time:106039ms step_avg:97.46ms
step:1099/1770 train_time:106139ms step_avg:97.46ms
step:1100/1770 train_time:106240ms step_avg:97.47ms
step:1101/1770 train_time:106340ms step_avg:97.47ms
step:1102/1770 train_time:106441ms step_avg:97.47ms
step:1103/1770 train_time:106543ms step_avg:97.48ms
step:1104/1770 train_time:106645ms step_avg:97.48ms
step:1105/1770 train_time:106746ms step_avg:97.48ms
step:1106/1770 train_time:106847ms step_avg:97.49ms
step:1107/1770 train_time:106948ms step_avg:97.49ms
step:1108/1770 train_time:107050ms step_avg:97.50ms
step:1109/1770 train_time:107150ms step_avg:97.50ms
step:1110/1770 train_time:107251ms step_avg:97.50ms
step:1111/1770 train_time:107352ms step_avg:97.50ms
step:1112/1770 train_time:107454ms step_avg:97.51ms
step:1113/1770 train_time:107555ms step_avg:97.51ms
step:1114/1770 train_time:107656ms step_avg:97.51ms
step:1115/1770 train_time:107758ms step_avg:97.52ms
step:1116/1770 train_time:107859ms step_avg:97.52ms
step:1117/1770 train_time:107960ms step_avg:97.53ms
step:1118/1770 train_time:108060ms step_avg:97.53ms
step:1119/1770 train_time:108162ms step_avg:97.53ms
step:1120/1770 train_time:108262ms step_avg:97.53ms
step:1121/1770 train_time:108363ms step_avg:97.54ms
step:1122/1770 train_time:108464ms step_avg:97.54ms
step:1123/1770 train_time:108565ms step_avg:97.54ms
step:1124/1770 train_time:108667ms step_avg:97.55ms
step:1125/1770 train_time:108768ms step_avg:97.55ms
step:1125/1770 val_loss:3.4710 train_time:108868ms step_avg:97.64ms
step:1126/1770 train_time:108890ms step_avg:97.57ms
step:1127/1770 train_time:108978ms step_avg:97.56ms
step:1128/1770 train_time:109079ms step_avg:97.57ms
step:1129/1770 train_time:109179ms step_avg:97.57ms
step:1130/1770 train_time:109281ms step_avg:97.57ms
step:1131/1770 train_time:109381ms step_avg:97.57ms
step:1132/1770 train_time:109482ms step_avg:97.58ms
step:1133/1770 train_time:109582ms step_avg:97.58ms
step:1134/1770 train_time:109683ms step_avg:97.58ms
step:1135/1770 train_time:109784ms step_avg:97.59ms
step:1136/1770 train_time:109887ms step_avg:97.59ms
step:1137/1770 train_time:109990ms step_avg:97.60ms
step:1138/1770 train_time:110090ms step_avg:97.60ms
step:1139/1770 train_time:110190ms step_avg:97.60ms
step:1140/1770 train_time:110291ms step_avg:97.60ms
step:1141/1770 train_time:110391ms step_avg:97.60ms
step:1142/1770 train_time:110493ms step_avg:97.61ms
step:1143/1770 train_time:110593ms step_avg:97.61ms
step:1144/1770 train_time:110694ms step_avg:97.61ms
step:1145/1770 train_time:110795ms step_avg:97.62ms
step:1146/1770 train_time:110897ms step_avg:97.62ms
step:1147/1770 train_time:111001ms step_avg:97.63ms
step:1148/1770 train_time:111102ms step_avg:97.63ms
step:1149/1770 train_time:111202ms step_avg:97.63ms
step:1150/1770 train_time:111303ms step_avg:97.63ms
step:1151/1770 train_time:111405ms step_avg:97.64ms
step:1152/1770 train_time:111505ms step_avg:97.64ms
step:1153/1770 train_time:111606ms step_avg:97.64ms
step:1154/1770 train_time:111707ms step_avg:97.65ms
step:1155/1770 train_time:111808ms step_avg:97.65ms
step:1156/1770 train_time:111909ms step_avg:97.65ms
step:1157/1770 train_time:112012ms step_avg:97.66ms
step:1158/1770 train_time:112113ms step_avg:97.66ms
step:1159/1770 train_time:112214ms step_avg:97.66ms
step:1160/1770 train_time:112316ms step_avg:97.67ms
step:1161/1770 train_time:112417ms step_avg:97.67ms
step:1162/1770 train_time:112520ms step_avg:97.67ms
step:1163/1770 train_time:112620ms step_avg:97.68ms
step:1164/1770 train_time:112721ms step_avg:97.68ms
step:1165/1770 train_time:112823ms step_avg:97.68ms
step:1166/1770 train_time:112925ms step_avg:97.69ms
step:1167/1770 train_time:113026ms step_avg:97.69ms
step:1168/1770 train_time:113127ms step_avg:97.69ms
step:1169/1770 train_time:113228ms step_avg:97.69ms
step:1170/1770 train_time:113328ms step_avg:97.70ms
step:1171/1770 train_time:113430ms step_avg:97.70ms
step:1172/1770 train_time:113530ms step_avg:97.70ms
step:1173/1770 train_time:113631ms step_avg:97.71ms
step:1174/1770 train_time:113732ms step_avg:97.71ms
step:1175/1770 train_time:113834ms step_avg:97.71ms
step:1176/1770 train_time:113935ms step_avg:97.71ms
step:1177/1770 train_time:114035ms step_avg:97.72ms
step:1178/1770 train_time:114137ms step_avg:97.72ms
step:1179/1770 train_time:114239ms step_avg:97.72ms
step:1180/1770 train_time:114341ms step_avg:97.73ms
step:1181/1770 train_time:114444ms step_avg:97.73ms
step:1182/1770 train_time:114545ms step_avg:97.73ms
step:1183/1770 train_time:114646ms step_avg:97.74ms
step:1184/1770 train_time:114749ms step_avg:97.74ms
step:1185/1770 train_time:114851ms step_avg:97.75ms
step:1186/1770 train_time:114953ms step_avg:97.75ms
step:1187/1770 train_time:115058ms step_avg:97.76ms
step:1188/1770 train_time:115160ms step_avg:97.76ms
step:1189/1770 train_time:115263ms step_avg:97.76ms
step:1190/1770 train_time:115364ms step_avg:97.77ms
step:1191/1770 train_time:115467ms step_avg:97.77ms
step:1192/1770 train_time:115570ms step_avg:97.77ms
step:1193/1770 train_time:115672ms step_avg:97.78ms
step:1194/1770 train_time:115774ms step_avg:97.78ms
step:1195/1770 train_time:115877ms step_avg:97.79ms
step:1196/1770 train_time:115980ms step_avg:97.79ms
step:1197/1770 train_time:116081ms step_avg:97.79ms
step:1198/1770 train_time:116183ms step_avg:97.80ms
step:1199/1770 train_time:116284ms step_avg:97.80ms
step:1200/1770 train_time:116387ms step_avg:97.80ms
step:1201/1770 train_time:116489ms step_avg:97.81ms
step:1202/1770 train_time:116590ms step_avg:97.81ms
step:1203/1770 train_time:116692ms step_avg:97.81ms
step:1204/1770 train_time:116794ms step_avg:97.82ms
step:1205/1770 train_time:116895ms step_avg:97.82ms
step:1206/1770 train_time:116999ms step_avg:97.83ms
step:1207/1770 train_time:117101ms step_avg:97.83ms
step:1208/1770 train_time:117202ms step_avg:97.83ms
step:1209/1770 train_time:117304ms step_avg:97.84ms
step:1210/1770 train_time:117406ms step_avg:97.84ms
step:1211/1770 train_time:117508ms step_avg:97.84ms
step:1212/1770 train_time:117612ms step_avg:97.85ms
step:1213/1770 train_time:117714ms step_avg:97.85ms
step:1214/1770 train_time:117815ms step_avg:97.85ms
step:1215/1770 train_time:117917ms step_avg:97.86ms
step:1216/1770 train_time:118021ms step_avg:97.86ms
step:1217/1770 train_time:118122ms step_avg:97.86ms
step:1218/1770 train_time:118224ms step_avg:97.87ms
step:1219/1770 train_time:118325ms step_avg:97.87ms
step:1220/1770 train_time:118428ms step_avg:97.87ms
step:1221/1770 train_time:118530ms step_avg:97.88ms
step:1222/1770 train_time:118633ms step_avg:97.88ms
step:1223/1770 train_time:118734ms step_avg:97.88ms
step:1224/1770 train_time:118838ms step_avg:97.89ms
step:1225/1770 train_time:118940ms step_avg:97.89ms
step:1226/1770 train_time:119042ms step_avg:97.90ms
step:1227/1770 train_time:119146ms step_avg:97.90ms
step:1228/1770 train_time:119249ms step_avg:97.91ms
step:1229/1770 train_time:119352ms step_avg:97.91ms
step:1230/1770 train_time:119454ms step_avg:97.91ms
step:1231/1770 train_time:119556ms step_avg:97.92ms
step:1232/1770 train_time:119658ms step_avg:97.92ms
step:1233/1770 train_time:119761ms step_avg:97.92ms
step:1234/1770 train_time:119863ms step_avg:97.93ms
step:1235/1770 train_time:119964ms step_avg:97.93ms
step:1236/1770 train_time:120066ms step_avg:97.93ms
step:1237/1770 train_time:120168ms step_avg:97.94ms
step:1238/1770 train_time:120271ms step_avg:97.94ms
step:1239/1770 train_time:120374ms step_avg:97.94ms
step:1240/1770 train_time:120477ms step_avg:97.95ms
step:1241/1770 train_time:120579ms step_avg:97.95ms
step:1242/1770 train_time:120681ms step_avg:97.96ms
step:1243/1770 train_time:120784ms step_avg:97.96ms
step:1244/1770 train_time:120886ms step_avg:97.96ms
step:1245/1770 train_time:120988ms step_avg:97.97ms
step:1246/1770 train_time:121090ms step_avg:97.97ms
step:1247/1770 train_time:121192ms step_avg:97.97ms
step:1248/1770 train_time:121295ms step_avg:97.98ms
step:1249/1770 train_time:121396ms step_avg:97.98ms
step:1250/1770 train_time:121498ms step_avg:97.98ms
step:1250/1770 val_loss:3.4228 train_time:121599ms step_avg:98.06ms
step:1251/1770 train_time:121621ms step_avg:98.00ms
step:1252/1770 train_time:121709ms step_avg:97.99ms
step:1253/1770 train_time:121811ms step_avg:98.00ms
step:1254/1770 train_time:121914ms step_avg:98.00ms
step:1255/1770 train_time:122018ms step_avg:98.01ms
step:1256/1770 train_time:122119ms step_avg:98.01ms
step:1257/1770 train_time:122220ms step_avg:98.01ms
step:1258/1770 train_time:122323ms step_avg:98.02ms
step:1259/1770 train_time:122425ms step_avg:98.02ms
step:1260/1770 train_time:122526ms step_avg:98.02ms
step:1261/1770 train_time:122630ms step_avg:98.03ms
step:1262/1770 train_time:122733ms step_avg:98.03ms
step:1263/1770 train_time:122835ms step_avg:98.03ms
step:1264/1770 train_time:122939ms step_avg:98.04ms
step:1265/1770 train_time:123040ms step_avg:98.04ms
step:1266/1770 train_time:123143ms step_avg:98.04ms
step:1267/1770 train_time:123245ms step_avg:98.05ms
step:1268/1770 train_time:123347ms step_avg:98.05ms
step:1269/1770 train_time:123449ms step_avg:98.05ms
step:1270/1770 train_time:123552ms step_avg:98.06ms
step:1271/1770 train_time:123653ms step_avg:98.06ms
step:1272/1770 train_time:123755ms step_avg:98.06ms
step:1273/1770 train_time:123857ms step_avg:98.07ms
step:1274/1770 train_time:123960ms step_avg:98.07ms
step:1275/1770 train_time:124062ms step_avg:98.07ms
step:1276/1770 train_time:124164ms step_avg:98.08ms
step:1277/1770 train_time:124266ms step_avg:98.08ms
step:1278/1770 train_time:124370ms step_avg:98.08ms
step:1279/1770 train_time:124472ms step_avg:98.09ms
step:1280/1770 train_time:124576ms step_avg:98.09ms
step:1281/1770 train_time:124677ms step_avg:98.09ms
step:1282/1770 train_time:124780ms step_avg:98.10ms
step:1283/1770 train_time:124883ms step_avg:98.10ms
step:1284/1770 train_time:124986ms step_avg:98.10ms
step:1285/1770 train_time:125087ms step_avg:98.11ms
step:1286/1770 train_time:125189ms step_avg:98.11ms
step:1287/1770 train_time:125294ms step_avg:98.12ms
step:1288/1770 train_time:125396ms step_avg:98.12ms
step:1289/1770 train_time:125499ms step_avg:98.12ms
step:1290/1770 train_time:125601ms step_avg:98.13ms
step:1291/1770 train_time:125704ms step_avg:98.13ms
step:1292/1770 train_time:125806ms step_avg:98.13ms
step:1293/1770 train_time:125909ms step_avg:98.14ms
step:1294/1770 train_time:126010ms step_avg:98.14ms
step:1295/1770 train_time:126112ms step_avg:98.14ms
step:1296/1770 train_time:126214ms step_avg:98.14ms
step:1297/1770 train_time:126315ms step_avg:98.15ms
step:1298/1770 train_time:126417ms step_avg:98.15ms
step:1299/1770 train_time:126519ms step_avg:98.15ms
step:1300/1770 train_time:126621ms step_avg:98.16ms
step:1301/1770 train_time:126724ms step_avg:98.16ms
step:1302/1770 train_time:126826ms step_avg:98.16ms
step:1303/1770 train_time:126928ms step_avg:98.17ms
step:1304/1770 train_time:127030ms step_avg:98.17ms
step:1305/1770 train_time:127132ms step_avg:98.17ms
step:1306/1770 train_time:127234ms step_avg:98.17ms
step:1307/1770 train_time:127336ms step_avg:98.18ms
step:1308/1770 train_time:127438ms step_avg:98.18ms
step:1309/1770 train_time:127540ms step_avg:98.18ms
step:1310/1770 train_time:127642ms step_avg:98.19ms
step:1311/1770 train_time:127743ms step_avg:98.19ms
step:1312/1770 train_time:127845ms step_avg:98.19ms
step:1313/1770 train_time:127946ms step_avg:98.19ms
step:1314/1770 train_time:128049ms step_avg:98.20ms
step:1315/1770 train_time:128151ms step_avg:98.20ms
step:1316/1770 train_time:128253ms step_avg:98.20ms
step:1317/1770 train_time:128355ms step_avg:98.21ms
step:1318/1770 train_time:128460ms step_avg:98.21ms
step:1319/1770 train_time:128562ms step_avg:98.21ms
step:1320/1770 train_time:128665ms step_avg:98.22ms
step:1321/1770 train_time:128766ms step_avg:98.22ms
step:1322/1770 train_time:128868ms step_avg:98.22ms
step:1323/1770 train_time:128972ms step_avg:98.23ms
step:1324/1770 train_time:129075ms step_avg:98.23ms
step:1325/1770 train_time:129178ms step_avg:98.23ms
step:1326/1770 train_time:129280ms step_avg:98.24ms
step:1327/1770 train_time:129385ms step_avg:98.24ms
step:1328/1770 train_time:129487ms step_avg:98.24ms
step:1329/1770 train_time:129588ms step_avg:98.25ms
step:1330/1770 train_time:129690ms step_avg:98.25ms
step:1331/1770 train_time:129791ms step_avg:98.25ms
step:1332/1770 train_time:129893ms step_avg:98.26ms
step:1333/1770 train_time:129994ms step_avg:98.26ms
step:1334/1770 train_time:130096ms step_avg:98.26ms
step:1335/1770 train_time:130199ms step_avg:98.26ms
step:1336/1770 train_time:130301ms step_avg:98.27ms
step:1337/1770 train_time:130403ms step_avg:98.27ms
step:1338/1770 train_time:130505ms step_avg:98.27ms
step:1339/1770 train_time:130607ms step_avg:98.27ms
step:1340/1770 train_time:130711ms step_avg:98.28ms
step:1341/1770 train_time:130813ms step_avg:98.28ms
step:1342/1770 train_time:130915ms step_avg:98.28ms
step:1343/1770 train_time:131017ms step_avg:98.29ms
step:1344/1770 train_time:131120ms step_avg:98.29ms
step:1345/1770 train_time:131222ms step_avg:98.29ms
step:1346/1770 train_time:131323ms step_avg:98.30ms
step:1347/1770 train_time:131427ms step_avg:98.30ms
step:1348/1770 train_time:131533ms step_avg:98.31ms
step:1349/1770 train_time:131635ms step_avg:98.31ms
step:1350/1770 train_time:131737ms step_avg:98.31ms
step:1351/1770 train_time:131839ms step_avg:98.31ms
step:1352/1770 train_time:131942ms step_avg:98.32ms
step:1353/1770 train_time:132045ms step_avg:98.32ms
step:1354/1770 train_time:132147ms step_avg:98.32ms
step:1355/1770 train_time:132249ms step_avg:98.33ms
step:1356/1770 train_time:132351ms step_avg:98.33ms
step:1357/1770 train_time:132452ms step_avg:98.33ms
step:1358/1770 train_time:132554ms step_avg:98.33ms
step:1359/1770 train_time:132657ms step_avg:98.34ms
step:1360/1770 train_time:132760ms step_avg:98.34ms
step:1361/1770 train_time:132862ms step_avg:98.34ms
step:1362/1770 train_time:132964ms step_avg:98.35ms
step:1363/1770 train_time:133068ms step_avg:98.35ms
step:1364/1770 train_time:133171ms step_avg:98.35ms
step:1365/1770 train_time:133272ms step_avg:98.36ms
step:1366/1770 train_time:133374ms step_avg:98.36ms
step:1367/1770 train_time:133477ms step_avg:98.36ms
step:1368/1770 train_time:133578ms step_avg:98.36ms
step:1369/1770 train_time:133681ms step_avg:98.37ms
step:1370/1770 train_time:133784ms step_avg:98.37ms
step:1371/1770 train_time:133886ms step_avg:98.37ms
step:1372/1770 train_time:133988ms step_avg:98.38ms
step:1373/1770 train_time:134090ms step_avg:98.38ms
step:1374/1770 train_time:134193ms step_avg:98.38ms
step:1375/1770 train_time:134295ms step_avg:98.38ms
step:1375/1770 val_loss:3.3782 train_time:134397ms step_avg:98.46ms
step:1376/1770 train_time:134419ms step_avg:98.40ms
step:1377/1770 train_time:134506ms step_avg:98.39ms
step:1378/1770 train_time:134608ms step_avg:98.40ms
step:1379/1770 train_time:134710ms step_avg:98.40ms
step:1380/1770 train_time:134811ms step_avg:98.40ms
step:1381/1770 train_time:134914ms step_avg:98.41ms
step:1382/1770 train_time:135015ms step_avg:98.41ms
step:1383/1770 train_time:135118ms step_avg:98.41ms
step:1384/1770 train_time:135220ms step_avg:98.41ms
step:1385/1770 train_time:135322ms step_avg:98.42ms
step:1386/1770 train_time:135425ms step_avg:98.42ms
step:1387/1770 train_time:135529ms step_avg:98.42ms
step:1388/1770 train_time:135631ms step_avg:98.43ms
step:1389/1770 train_time:135733ms step_avg:98.43ms
step:1390/1770 train_time:135836ms step_avg:98.43ms
step:1391/1770 train_time:135937ms step_avg:98.43ms
step:1392/1770 train_time:136040ms step_avg:98.44ms
step:1393/1770 train_time:136141ms step_avg:98.44ms
step:1394/1770 train_time:136243ms step_avg:98.44ms
step:1395/1770 train_time:136346ms step_avg:98.45ms
step:1396/1770 train_time:136450ms step_avg:98.45ms
step:1397/1770 train_time:136553ms step_avg:98.45ms
step:1398/1770 train_time:136656ms step_avg:98.46ms
step:1399/1770 train_time:136758ms step_avg:98.46ms
step:1400/1770 train_time:136860ms step_avg:98.46ms
step:1401/1770 train_time:136961ms step_avg:98.46ms
step:1402/1770 train_time:137064ms step_avg:98.47ms
step:1403/1770 train_time:137166ms step_avg:98.47ms
step:1404/1770 train_time:137268ms step_avg:98.47ms
step:1405/1770 train_time:137369ms step_avg:98.47ms
step:1406/1770 train_time:137471ms step_avg:98.48ms
step:1407/1770 train_time:137574ms step_avg:98.48ms
step:1408/1770 train_time:137677ms step_avg:98.48ms
step:1409/1770 train_time:137780ms step_avg:98.48ms
step:1410/1770 train_time:137882ms step_avg:98.49ms
step:1411/1770 train_time:137984ms step_avg:98.49ms
step:1412/1770 train_time:138086ms step_avg:98.49ms
step:1413/1770 train_time:138187ms step_avg:98.49ms
step:1414/1770 train_time:138290ms step_avg:98.50ms
step:1415/1770 train_time:138392ms step_avg:98.50ms
step:1416/1770 train_time:138495ms step_avg:98.50ms
step:1417/1770 train_time:138597ms step_avg:98.51ms
step:1418/1770 train_time:138699ms step_avg:98.51ms
step:1419/1770 train_time:138802ms step_avg:98.51ms
step:1420/1770 train_time:138903ms step_avg:98.51ms
step:1421/1770 train_time:139005ms step_avg:98.52ms
step:1422/1770 train_time:139107ms step_avg:98.52ms
step:1423/1770 train_time:139210ms step_avg:98.52ms
step:1424/1770 train_time:139312ms step_avg:98.52ms
step:1425/1770 train_time:139414ms step_avg:98.53ms
step:1426/1770 train_time:139516ms step_avg:98.53ms
step:1427/1770 train_time:139618ms step_avg:98.53ms
step:1428/1770 train_time:139721ms step_avg:98.53ms
step:1429/1770 train_time:139823ms step_avg:98.54ms
step:1430/1770 train_time:139925ms step_avg:98.54ms
step:1431/1770 train_time:140028ms step_avg:98.54ms
step:1432/1770 train_time:140129ms step_avg:98.54ms
step:1433/1770 train_time:140232ms step_avg:98.55ms
step:1434/1770 train_time:140334ms step_avg:98.55ms
step:1435/1770 train_time:140437ms step_avg:98.55ms
step:1436/1770 train_time:140540ms step_avg:98.56ms
step:1437/1770 train_time:140642ms step_avg:98.56ms
step:1438/1770 train_time:140744ms step_avg:98.56ms
step:1439/1770 train_time:140845ms step_avg:98.56ms
step:1440/1770 train_time:140947ms step_avg:98.56ms
step:1441/1770 train_time:141052ms step_avg:98.57ms
step:1442/1770 train_time:141153ms step_avg:98.57ms
step:1443/1770 train_time:141256ms step_avg:98.57ms
step:1444/1770 train_time:141358ms step_avg:98.58ms
step:1445/1770 train_time:141462ms step_avg:98.58ms
step:1446/1770 train_time:141565ms step_avg:98.58ms
step:1447/1770 train_time:141668ms step_avg:98.59ms
step:1448/1770 train_time:141772ms step_avg:98.59ms
step:1449/1770 train_time:141875ms step_avg:98.59ms
step:1450/1770 train_time:141977ms step_avg:98.60ms
step:1451/1770 train_time:142081ms step_avg:98.60ms
step:1452/1770 train_time:142185ms step_avg:98.60ms
step:1453/1770 train_time:142288ms step_avg:98.61ms
step:1454/1770 train_time:142391ms step_avg:98.61ms
step:1455/1770 train_time:142496ms step_avg:98.61ms
step:1456/1770 train_time:142599ms step_avg:98.62ms
step:1457/1770 train_time:142703ms step_avg:98.62ms
step:1458/1770 train_time:142806ms step_avg:98.62ms
step:1459/1770 train_time:142911ms step_avg:98.63ms
step:1460/1770 train_time:143014ms step_avg:98.63ms
step:1461/1770 train_time:143117ms step_avg:98.63ms
step:1462/1770 train_time:143220ms step_avg:98.64ms
step:1463/1770 train_time:143323ms step_avg:98.64ms
step:1464/1770 train_time:143428ms step_avg:98.64ms
step:1465/1770 train_time:143531ms step_avg:98.65ms
step:1466/1770 train_time:143635ms step_avg:98.65ms
step:1467/1770 train_time:143739ms step_avg:98.65ms
step:1468/1770 train_time:143843ms step_avg:98.66ms
step:1469/1770 train_time:143946ms step_avg:98.66ms
step:1470/1770 train_time:144048ms step_avg:98.66ms
step:1471/1770 train_time:144151ms step_avg:98.67ms
step:1472/1770 train_time:144254ms step_avg:98.67ms
step:1473/1770 train_time:144358ms step_avg:98.67ms
step:1474/1770 train_time:144462ms step_avg:98.68ms
step:1475/1770 train_time:144565ms step_avg:98.68ms
step:1476/1770 train_time:144668ms step_avg:98.68ms
step:1477/1770 train_time:144773ms step_avg:98.69ms
step:1478/1770 train_time:144877ms step_avg:98.69ms
step:1479/1770 train_time:144980ms step_avg:98.69ms
step:1480/1770 train_time:145083ms step_avg:98.70ms
step:1481/1770 train_time:145190ms step_avg:98.70ms
step:1482/1770 train_time:145293ms step_avg:98.70ms
step:1483/1770 train_time:145396ms step_avg:98.71ms
step:1484/1770 train_time:145499ms step_avg:98.71ms
step:1485/1770 train_time:145602ms step_avg:98.71ms
step:1486/1770 train_time:145705ms step_avg:98.72ms
step:1487/1770 train_time:145808ms step_avg:98.72ms
step:1488/1770 train_time:145912ms step_avg:98.72ms
step:1489/1770 train_time:146015ms step_avg:98.73ms
step:1490/1770 train_time:146118ms step_avg:98.73ms
step:1491/1770 train_time:146221ms step_avg:98.73ms
step:1492/1770 train_time:146325ms step_avg:98.73ms
step:1493/1770 train_time:146431ms step_avg:98.74ms
step:1494/1770 train_time:146538ms step_avg:98.75ms
step:1495/1770 train_time:146640ms step_avg:98.75ms
step:1496/1770 train_time:146743ms step_avg:98.75ms
step:1497/1770 train_time:146846ms step_avg:98.75ms
step:1498/1770 train_time:146949ms step_avg:98.76ms
step:1499/1770 train_time:147051ms step_avg:98.76ms
step:1500/1770 train_time:147154ms step_avg:98.76ms
step:1500/1770 val_loss:3.3411 train_time:147255ms step_avg:98.83ms
step:1501/1770 train_time:147277ms step_avg:98.78ms
step:1502/1770 train_time:147365ms step_avg:98.77ms
step:1503/1770 train_time:147468ms step_avg:98.77ms
step:1504/1770 train_time:147571ms step_avg:98.78ms
step:1505/1770 train_time:147676ms step_avg:98.78ms
step:1506/1770 train_time:147779ms step_avg:98.78ms
step:1507/1770 train_time:147883ms step_avg:98.79ms
step:1508/1770 train_time:147988ms step_avg:98.79ms
step:1509/1770 train_time:148091ms step_avg:98.79ms
step:1510/1770 train_time:148193ms step_avg:98.80ms
step:1511/1770 train_time:148298ms step_avg:98.80ms
step:1512/1770 train_time:148404ms step_avg:98.80ms
step:1513/1770 train_time:148509ms step_avg:98.81ms
step:1514/1770 train_time:148612ms step_avg:98.81ms
step:1515/1770 train_time:148716ms step_avg:98.81ms
step:1516/1770 train_time:148819ms step_avg:98.82ms
step:1517/1770 train_time:148923ms step_avg:98.82ms
step:1518/1770 train_time:149027ms step_avg:98.82ms
step:1519/1770 train_time:149129ms step_avg:98.83ms
step:1520/1770 train_time:149234ms step_avg:98.83ms
step:1521/1770 train_time:149338ms step_avg:98.83ms
step:1522/1770 train_time:149442ms step_avg:98.84ms
step:1523/1770 train_time:149545ms step_avg:98.84ms
step:1524/1770 train_time:149648ms step_avg:98.84ms
step:1525/1770 train_time:149751ms step_avg:98.85ms
step:1526/1770 train_time:149854ms step_avg:98.85ms
step:1527/1770 train_time:149957ms step_avg:98.85ms
step:1528/1770 train_time:150063ms step_avg:98.86ms
step:1529/1770 train_time:150165ms step_avg:98.86ms
step:1530/1770 train_time:150268ms step_avg:98.86ms
step:1531/1770 train_time:150370ms step_avg:98.86ms
step:1532/1770 train_time:150474ms step_avg:98.87ms
step:1533/1770 train_time:150577ms step_avg:98.87ms
step:1534/1770 train_time:150683ms step_avg:98.87ms
step:1535/1770 train_time:150786ms step_avg:98.88ms
step:1536/1770 train_time:150889ms step_avg:98.88ms
step:1537/1770 train_time:150993ms step_avg:98.88ms
step:1538/1770 train_time:151098ms step_avg:98.89ms
step:1539/1770 train_time:151201ms step_avg:98.89ms
step:1540/1770 train_time:151307ms step_avg:98.89ms
step:1541/1770 train_time:151412ms step_avg:98.90ms
step:1542/1770 train_time:151515ms step_avg:98.90ms
step:1543/1770 train_time:151619ms step_avg:98.90ms
step:1544/1770 train_time:151724ms step_avg:98.91ms
step:1545/1770 train_time:151828ms step_avg:98.91ms
step:1546/1770 train_time:151932ms step_avg:98.91ms
step:1547/1770 train_time:152035ms step_avg:98.92ms
step:1548/1770 train_time:152138ms step_avg:98.92ms
step:1549/1770 train_time:152242ms step_avg:98.92ms
step:1550/1770 train_time:152346ms step_avg:98.93ms
step:1551/1770 train_time:152449ms step_avg:98.93ms
step:1552/1770 train_time:152554ms step_avg:98.93ms
step:1553/1770 train_time:152658ms step_avg:98.94ms
step:1554/1770 train_time:152760ms step_avg:98.94ms
step:1555/1770 train_time:152864ms step_avg:98.94ms
step:1556/1770 train_time:152967ms step_avg:98.94ms
step:1557/1770 train_time:153070ms step_avg:98.95ms
step:1558/1770 train_time:153173ms step_avg:98.95ms
step:1559/1770 train_time:153277ms step_avg:98.95ms
step:1560/1770 train_time:153379ms step_avg:98.95ms
step:1561/1770 train_time:153485ms step_avg:98.96ms
step:1562/1770 train_time:153587ms step_avg:98.96ms
step:1563/1770 train_time:153690ms step_avg:98.96ms
step:1564/1770 train_time:153793ms step_avg:98.97ms
step:1565/1770 train_time:153897ms step_avg:98.97ms
step:1566/1770 train_time:154000ms step_avg:98.97ms
step:1567/1770 train_time:154104ms step_avg:98.98ms
step:1568/1770 train_time:154207ms step_avg:98.98ms
step:1569/1770 train_time:154313ms step_avg:98.98ms
step:1570/1770 train_time:154416ms step_avg:98.98ms
step:1571/1770 train_time:154520ms step_avg:98.99ms
step:1572/1770 train_time:154625ms step_avg:98.99ms
step:1573/1770 train_time:154730ms step_avg:99.00ms
step:1574/1770 train_time:154833ms step_avg:99.00ms
step:1575/1770 train_time:154935ms step_avg:99.00ms
step:1576/1770 train_time:155037ms step_avg:99.00ms
step:1577/1770 train_time:155142ms step_avg:99.01ms
step:1578/1770 train_time:155249ms step_avg:99.01ms
step:1579/1770 train_time:155352ms step_avg:99.01ms
step:1580/1770 train_time:155456ms step_avg:99.02ms
step:1581/1770 train_time:155562ms step_avg:99.02ms
step:1582/1770 train_time:155666ms step_avg:99.02ms
step:1583/1770 train_time:155769ms step_avg:99.03ms
step:1584/1770 train_time:155873ms step_avg:99.03ms
step:1585/1770 train_time:155976ms step_avg:99.03ms
step:1586/1770 train_time:156083ms step_avg:99.04ms
step:1587/1770 train_time:156187ms step_avg:99.04ms
step:1588/1770 train_time:156290ms step_avg:99.04ms
step:1589/1770 train_time:156394ms step_avg:99.05ms
step:1590/1770 train_time:156498ms step_avg:99.05ms
step:1591/1770 train_time:156601ms step_avg:99.05ms
step:1592/1770 train_time:156705ms step_avg:99.06ms
step:1593/1770 train_time:156809ms step_avg:99.06ms
step:1594/1770 train_time:156912ms step_avg:99.06ms
step:1595/1770 train_time:157015ms step_avg:99.06ms
step:1596/1770 train_time:157118ms step_avg:99.07ms
step:1597/1770 train_time:157222ms step_avg:99.07ms
step:1598/1770 train_time:157326ms step_avg:99.07ms
step:1599/1770 train_time:157431ms step_avg:99.08ms
step:1600/1770 train_time:157537ms step_avg:99.08ms
step:1601/1770 train_time:157641ms step_avg:99.08ms
step:1602/1770 train_time:157746ms step_avg:99.09ms
step:1603/1770 train_time:157850ms step_avg:99.09ms
step:1604/1770 train_time:157951ms step_avg:99.09ms
step:1605/1770 train_time:158053ms step_avg:99.09ms
step:1606/1770 train_time:158156ms step_avg:99.10ms
step:1607/1770 train_time:158264ms step_avg:99.10ms
step:1608/1770 train_time:158368ms step_avg:99.10ms
step:1609/1770 train_time:158472ms step_avg:99.11ms
step:1610/1770 train_time:158576ms step_avg:99.11ms
step:1611/1770 train_time:158681ms step_avg:99.11ms
step:1612/1770 train_time:158785ms step_avg:99.12ms
step:1613/1770 train_time:158889ms step_avg:99.12ms
step:1614/1770 train_time:158992ms step_avg:99.12ms
step:1615/1770 train_time:159096ms step_avg:99.12ms
step:1616/1770 train_time:159200ms step_avg:99.13ms
step:1617/1770 train_time:159305ms step_avg:99.13ms
step:1618/1770 train_time:159409ms step_avg:99.13ms
step:1619/1770 train_time:159513ms step_avg:99.14ms
step:1620/1770 train_time:159617ms step_avg:99.14ms
step:1621/1770 train_time:159721ms step_avg:99.14ms
step:1622/1770 train_time:159826ms step_avg:99.15ms
step:1623/1770 train_time:159931ms step_avg:99.15ms
step:1624/1770 train_time:160034ms step_avg:99.15ms
step:1625/1770 train_time:160136ms step_avg:99.16ms
step:1625/1770 val_loss:3.3064 train_time:160238ms step_avg:99.22ms
step:1626/1770 train_time:160260ms step_avg:99.17ms
step:1627/1770 train_time:160348ms step_avg:99.16ms
step:1628/1770 train_time:160451ms step_avg:99.17ms
step:1629/1770 train_time:160553ms step_avg:99.17ms
step:1630/1770 train_time:160657ms step_avg:99.17ms
step:1631/1770 train_time:160759ms step_avg:99.17ms
step:1632/1770 train_time:160862ms step_avg:99.18ms
step:1633/1770 train_time:160965ms step_avg:99.18ms
step:1634/1770 train_time:161067ms step_avg:99.18ms
step:1635/1770 train_time:161171ms step_avg:99.18ms
step:1636/1770 train_time:161276ms step_avg:99.19ms
step:1637/1770 train_time:161382ms step_avg:99.19ms
step:1638/1770 train_time:161485ms step_avg:99.19ms
step:1639/1770 train_time:161590ms step_avg:99.20ms
step:1640/1770 train_time:161694ms step_avg:99.20ms
step:1641/1770 train_time:161797ms step_avg:99.20ms
step:1642/1770 train_time:161900ms step_avg:99.20ms
step:1643/1770 train_time:162003ms step_avg:99.21ms
step:1644/1770 train_time:162109ms step_avg:99.21ms
step:1645/1770 train_time:162212ms step_avg:99.21ms
step:1646/1770 train_time:162317ms step_avg:99.22ms
step:1647/1770 train_time:162420ms step_avg:99.22ms
step:1648/1770 train_time:162523ms step_avg:99.22ms
step:1649/1770 train_time:162626ms step_avg:99.22ms
step:1650/1770 train_time:162730ms step_avg:99.23ms
step:1651/1770 train_time:162833ms step_avg:99.23ms
step:1652/1770 train_time:162936ms step_avg:99.23ms
step:1653/1770 train_time:163039ms step_avg:99.23ms
step:1654/1770 train_time:163146ms step_avg:99.24ms
step:1655/1770 train_time:163252ms step_avg:99.24ms
step:1656/1770 train_time:163355ms step_avg:99.24ms
step:1657/1770 train_time:163461ms step_avg:99.25ms
step:1658/1770 train_time:163564ms step_avg:99.25ms
step:1659/1770 train_time:163668ms step_avg:99.25ms
step:1660/1770 train_time:163771ms step_avg:99.26ms
step:1661/1770 train_time:163876ms step_avg:99.26ms
step:1662/1770 train_time:163980ms step_avg:99.26ms
step:1663/1770 train_time:164082ms step_avg:99.26ms
step:1664/1770 train_time:164187ms step_avg:99.27ms
step:1665/1770 train_time:164291ms step_avg:99.27ms
step:1666/1770 train_time:164394ms step_avg:99.27ms
step:1667/1770 train_time:164497ms step_avg:99.27ms
step:1668/1770 train_time:164600ms step_avg:99.28ms
step:1669/1770 train_time:164702ms step_avg:99.28ms
step:1670/1770 train_time:164805ms step_avg:99.28ms
step:1671/1770 train_time:164910ms step_avg:99.28ms
step:1672/1770 train_time:165014ms step_avg:99.29ms
step:1673/1770 train_time:165118ms step_avg:99.29ms
step:1674/1770 train_time:165223ms step_avg:99.29ms
step:1675/1770 train_time:165327ms step_avg:99.30ms
step:1676/1770 train_time:165431ms step_avg:99.30ms
step:1677/1770 train_time:165539ms step_avg:99.30ms
step:1678/1770 train_time:165642ms step_avg:99.31ms
step:1679/1770 train_time:165745ms step_avg:99.31ms
step:1680/1770 train_time:165848ms step_avg:99.31ms
step:1681/1770 train_time:165952ms step_avg:99.31ms
step:1682/1770 train_time:166057ms step_avg:99.32ms
step:1683/1770 train_time:166160ms step_avg:99.32ms
step:1684/1770 train_time:166263ms step_avg:99.32ms
step:1685/1770 train_time:166368ms step_avg:99.32ms
step:1686/1770 train_time:166473ms step_avg:99.33ms
step:1687/1770 train_time:166578ms step_avg:99.33ms
step:1688/1770 train_time:166681ms step_avg:99.33ms
step:1689/1770 train_time:166784ms step_avg:99.34ms
step:1690/1770 train_time:166887ms step_avg:99.34ms
step:1691/1770 train_time:166991ms step_avg:99.34ms
step:1692/1770 train_time:167096ms step_avg:99.34ms
step:1693/1770 train_time:167201ms step_avg:99.35ms
step:1694/1770 train_time:167304ms step_avg:99.35ms
step:1695/1770 train_time:167408ms step_avg:99.35ms
step:1696/1770 train_time:167512ms step_avg:99.35ms
step:1697/1770 train_time:167617ms step_avg:99.36ms
step:1698/1770 train_time:167720ms step_avg:99.36ms
step:1699/1770 train_time:167824ms step_avg:99.36ms
step:1700/1770 train_time:167927ms step_avg:99.37ms
step:1701/1770 train_time:168030ms step_avg:99.37ms
step:1702/1770 train_time:168134ms step_avg:99.37ms
step:1703/1770 train_time:168237ms step_avg:99.37ms
step:1704/1770 train_time:168340ms step_avg:99.37ms
step:1705/1770 train_time:168443ms step_avg:99.38ms
step:1706/1770 train_time:168546ms step_avg:99.38ms
step:1707/1770 train_time:168651ms step_avg:99.38ms
step:1708/1770 train_time:168756ms step_avg:99.38ms
step:1709/1770 train_time:168860ms step_avg:99.39ms
step:1710/1770 train_time:168966ms step_avg:99.39ms
step:1711/1770 train_time:169072ms step_avg:99.40ms
step:1712/1770 train_time:169177ms step_avg:99.40ms
step:1713/1770 train_time:169280ms step_avg:99.40ms
step:1714/1770 train_time:169384ms step_avg:99.40ms
step:1715/1770 train_time:169487ms step_avg:99.41ms
step:1716/1770 train_time:169592ms step_avg:99.41ms
step:1717/1770 train_time:169697ms step_avg:99.41ms
step:1718/1770 train_time:169802ms step_avg:99.42ms
step:1719/1770 train_time:169907ms step_avg:99.42ms
step:1720/1770 train_time:170011ms step_avg:99.42ms
step:1721/1770 train_time:170115ms step_avg:99.42ms
step:1722/1770 train_time:170221ms step_avg:99.43ms
step:1723/1770 train_time:170327ms step_avg:99.43ms
step:1724/1770 train_time:170432ms step_avg:99.44ms
step:1725/1770 train_time:170539ms step_avg:99.44ms
step:1726/1770 train_time:170644ms step_avg:99.44ms
step:1727/1770 train_time:170748ms step_avg:99.45ms
step:1728/1770 train_time:170853ms step_avg:99.45ms
step:1729/1770 train_time:170957ms step_avg:99.45ms
step:1730/1770 train_time:171062ms step_avg:99.45ms
step:1731/1770 train_time:171169ms step_avg:99.46ms
step:1732/1770 train_time:171273ms step_avg:99.46ms
step:1733/1770 train_time:171379ms step_avg:99.47ms
step:1734/1770 train_time:171482ms step_avg:99.47ms
step:1735/1770 train_time:171588ms step_avg:99.47ms
step:1736/1770 train_time:171691ms step_avg:99.47ms
step:1737/1770 train_time:171795ms step_avg:99.48ms
step:1738/1770 train_time:171899ms step_avg:99.48ms
step:1739/1770 train_time:172003ms step_avg:99.48ms
step:1740/1770 train_time:172107ms step_avg:99.48ms
step:1741/1770 train_time:172213ms step_avg:99.49ms
step:1742/1770 train_time:172320ms step_avg:99.49ms
step:1743/1770 train_time:172426ms step_avg:99.50ms
step:1744/1770 train_time:172531ms step_avg:99.50ms
step:1745/1770 train_time:172634ms step_avg:99.50ms
step:1746/1770 train_time:172741ms step_avg:99.51ms
step:1747/1770 train_time:172844ms step_avg:99.51ms
step:1748/1770 train_time:172950ms step_avg:99.51ms
step:1749/1770 train_time:173055ms step_avg:99.51ms
step:1750/1770 train_time:173158ms step_avg:99.52ms
step:1750/1770 val_loss:3.2799 train_time:173261ms step_avg:99.58ms
step:1751/1770 train_time:173282ms step_avg:99.53ms
step:1752/1770 train_time:173371ms step_avg:99.52ms
step:1753/1770 train_time:173474ms step_avg:99.53ms
step:1754/1770 train_time:173579ms step_avg:99.53ms
step:1755/1770 train_time:173683ms step_avg:99.53ms
step:1756/1770 train_time:173787ms step_avg:99.53ms
step:1757/1770 train_time:173892ms step_avg:99.54ms
step:1758/1770 train_time:173995ms step_avg:99.54ms
step:1759/1770 train_time:174099ms step_avg:99.54ms
step:1760/1770 train_time:174204ms step_avg:99.55ms
step:1761/1770 train_time:174311ms step_avg:99.55ms
step:1762/1770 train_time:174418ms step_avg:99.55ms
step:1763/1770 train_time:174521ms step_avg:99.56ms
step:1764/1770 train_time:174627ms step_avg:99.56ms
step:1765/1770 train_time:174730ms step_avg:99.56ms
step:1766/1770 train_time:174839ms step_avg:99.57ms
step:1767/1770 train_time:174941ms step_avg:99.57ms
step:1768/1770 train_time:175045ms step_avg:99.57ms
step:1769/1770 train_time:175149ms step_avg:99.57ms
step:1770/1770 train_time:175253ms step_avg:99.58ms
step:1770/1770 val_loss:3.2770 train_time:175358ms step_avg:99.63ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
