import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:58:40 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23383ms step_avg:nanms
step:2/1393 train_time:23804ms step_avg:nanms
step:3/1393 train_time:23924ms step_avg:nanms
step:4/1393 train_time:24160ms step_avg:nanms
step:5/1393 train_time:24280ms step_avg:nanms
step:6/1393 train_time:24401ms step_avg:nanms
step:7/1393 train_time:24521ms step_avg:nanms
step:8/1393 train_time:24642ms step_avg:nanms
step:9/1393 train_time:24763ms step_avg:nanms
step:10/1393 train_time:24884ms step_avg:nanms
step:11/1393 train_time:121ms step_avg:nanms
step:12/1393 train_time:246ms step_avg:nanms
step:13/1393 train_time:368ms step_avg:122.75ms
step:14/1393 train_time:490ms step_avg:122.54ms
step:15/1393 train_time:611ms step_avg:122.12ms
step:16/1393 train_time:732ms step_avg:121.95ms
step:17/1393 train_time:853ms step_avg:121.90ms
step:18/1393 train_time:976ms step_avg:122.03ms
step:19/1393 train_time:1098ms step_avg:122.03ms
step:20/1393 train_time:1221ms step_avg:122.05ms
step:21/1393 train_time:1343ms step_avg:122.07ms
step:22/1393 train_time:1465ms step_avg:122.07ms
step:23/1393 train_time:1588ms step_avg:122.12ms
step:24/1393 train_time:1709ms step_avg:122.04ms
step:25/1393 train_time:1830ms step_avg:121.97ms
step:26/1393 train_time:1951ms step_avg:121.94ms
step:27/1393 train_time:2072ms step_avg:121.91ms
step:28/1393 train_time:2194ms step_avg:121.91ms
step:29/1393 train_time:2317ms step_avg:121.97ms
step:30/1393 train_time:2439ms step_avg:121.96ms
step:31/1393 train_time:2561ms step_avg:121.94ms
step:32/1393 train_time:2682ms step_avg:121.92ms
step:33/1393 train_time:2804ms step_avg:121.92ms
step:34/1393 train_time:2925ms step_avg:121.87ms
step:35/1393 train_time:3047ms step_avg:121.88ms
step:36/1393 train_time:3168ms step_avg:121.85ms
step:37/1393 train_time:3291ms step_avg:121.88ms
step:38/1393 train_time:3413ms step_avg:121.88ms
step:39/1393 train_time:3534ms step_avg:121.87ms
step:40/1393 train_time:3656ms step_avg:121.86ms
step:41/1393 train_time:3778ms step_avg:121.87ms
step:42/1393 train_time:3898ms step_avg:121.82ms
step:43/1393 train_time:4020ms step_avg:121.83ms
step:44/1393 train_time:4141ms step_avg:121.79ms
step:45/1393 train_time:4263ms step_avg:121.80ms
step:46/1393 train_time:4384ms step_avg:121.79ms
step:47/1393 train_time:4506ms step_avg:121.79ms
step:48/1393 train_time:4628ms step_avg:121.79ms
step:49/1393 train_time:4751ms step_avg:121.81ms
step:50/1393 train_time:4872ms step_avg:121.80ms
step:51/1393 train_time:4994ms step_avg:121.81ms
step:52/1393 train_time:5115ms step_avg:121.80ms
step:53/1393 train_time:5237ms step_avg:121.79ms
step:54/1393 train_time:5358ms step_avg:121.78ms
step:55/1393 train_time:5481ms step_avg:121.79ms
step:56/1393 train_time:5602ms step_avg:121.78ms
step:57/1393 train_time:5723ms step_avg:121.77ms
step:58/1393 train_time:5845ms step_avg:121.77ms
step:59/1393 train_time:5966ms step_avg:121.75ms
step:60/1393 train_time:6088ms step_avg:121.75ms
step:61/1393 train_time:6209ms step_avg:121.75ms
step:62/1393 train_time:6331ms step_avg:121.74ms
step:63/1393 train_time:6451ms step_avg:121.72ms
step:64/1393 train_time:6572ms step_avg:121.71ms
step:65/1393 train_time:6694ms step_avg:121.71ms
step:66/1393 train_time:6817ms step_avg:121.73ms
step:67/1393 train_time:6938ms step_avg:121.72ms
step:68/1393 train_time:7059ms step_avg:121.72ms
step:69/1393 train_time:7181ms step_avg:121.70ms
step:70/1393 train_time:7302ms step_avg:121.70ms
step:71/1393 train_time:7423ms step_avg:121.70ms
step:72/1393 train_time:7545ms step_avg:121.69ms
step:73/1393 train_time:7666ms step_avg:121.69ms
step:74/1393 train_time:7789ms step_avg:121.70ms
step:75/1393 train_time:7910ms step_avg:121.70ms
step:76/1393 train_time:8031ms step_avg:121.68ms
step:77/1393 train_time:8152ms step_avg:121.67ms
step:78/1393 train_time:8275ms step_avg:121.68ms
step:79/1393 train_time:8396ms step_avg:121.69ms
step:80/1393 train_time:8518ms step_avg:121.68ms
step:81/1393 train_time:8639ms step_avg:121.67ms
step:82/1393 train_time:8761ms step_avg:121.68ms
step:83/1393 train_time:8884ms step_avg:121.69ms
step:84/1393 train_time:9005ms step_avg:121.69ms
step:85/1393 train_time:9126ms step_avg:121.68ms
step:86/1393 train_time:9248ms step_avg:121.69ms
step:87/1393 train_time:9370ms step_avg:121.68ms
step:88/1393 train_time:9490ms step_avg:121.67ms
step:89/1393 train_time:9612ms step_avg:121.67ms
step:90/1393 train_time:9733ms step_avg:121.66ms
step:91/1393 train_time:9855ms step_avg:121.66ms
step:92/1393 train_time:9976ms step_avg:121.66ms
step:93/1393 train_time:10097ms step_avg:121.65ms
step:94/1393 train_time:10220ms step_avg:121.66ms
step:95/1393 train_time:10342ms step_avg:121.68ms
step:96/1393 train_time:10464ms step_avg:121.68ms
step:97/1393 train_time:10587ms step_avg:121.69ms
step:98/1393 train_time:10708ms step_avg:121.68ms
step:99/1393 train_time:10829ms step_avg:121.67ms
step:100/1393 train_time:10950ms step_avg:121.67ms
step:101/1393 train_time:11071ms step_avg:121.66ms
step:102/1393 train_time:11193ms step_avg:121.66ms
step:103/1393 train_time:11315ms step_avg:121.66ms
step:104/1393 train_time:11436ms step_avg:121.66ms
step:105/1393 train_time:11558ms step_avg:121.66ms
step:106/1393 train_time:11682ms step_avg:121.68ms
step:107/1393 train_time:11804ms step_avg:121.69ms
step:108/1393 train_time:11925ms step_avg:121.69ms
step:109/1393 train_time:12047ms step_avg:121.69ms
step:110/1393 train_time:12169ms step_avg:121.69ms
step:111/1393 train_time:12291ms step_avg:121.69ms
step:112/1393 train_time:12413ms step_avg:121.70ms
step:113/1393 train_time:12535ms step_avg:121.70ms
step:114/1393 train_time:12658ms step_avg:121.71ms
step:115/1393 train_time:12779ms step_avg:121.71ms
step:116/1393 train_time:12903ms step_avg:121.72ms
step:117/1393 train_time:13025ms step_avg:121.73ms
step:118/1393 train_time:13147ms step_avg:121.74ms
step:119/1393 train_time:13270ms step_avg:121.74ms
step:120/1393 train_time:13392ms step_avg:121.75ms
step:121/1393 train_time:13514ms step_avg:121.75ms
step:122/1393 train_time:13637ms step_avg:121.76ms
step:123/1393 train_time:13758ms step_avg:121.75ms
step:124/1393 train_time:13881ms step_avg:121.76ms
step:125/1393 train_time:14004ms step_avg:121.78ms
step:125/1393 val_loss:4.4050 train_time:14125ms step_avg:122.82ms
step:126/1393 train_time:14147ms step_avg:121.96ms
step:127/1393 train_time:14251ms step_avg:121.80ms
step:128/1393 train_time:14383ms step_avg:121.89ms
step:129/1393 train_time:14506ms step_avg:121.90ms
step:130/1393 train_time:14628ms step_avg:121.90ms
step:131/1393 train_time:14750ms step_avg:121.90ms
step:132/1393 train_time:14871ms step_avg:121.90ms
step:133/1393 train_time:14992ms step_avg:121.89ms
step:134/1393 train_time:15114ms step_avg:121.89ms
step:135/1393 train_time:15236ms step_avg:121.89ms
step:136/1393 train_time:15359ms step_avg:121.90ms
step:137/1393 train_time:15482ms step_avg:121.91ms
step:138/1393 train_time:15604ms step_avg:121.91ms
step:139/1393 train_time:15728ms step_avg:121.92ms
step:140/1393 train_time:15850ms step_avg:121.92ms
step:141/1393 train_time:15971ms step_avg:121.92ms
step:142/1393 train_time:16093ms step_avg:121.92ms
step:143/1393 train_time:16215ms step_avg:121.92ms
step:144/1393 train_time:16338ms step_avg:121.92ms
step:145/1393 train_time:16461ms step_avg:121.93ms
step:146/1393 train_time:16583ms step_avg:121.94ms
step:147/1393 train_time:16705ms step_avg:121.94ms
step:148/1393 train_time:16828ms step_avg:121.94ms
step:149/1393 train_time:16950ms step_avg:121.94ms
step:150/1393 train_time:17072ms step_avg:121.94ms
step:151/1393 train_time:17193ms step_avg:121.94ms
step:152/1393 train_time:17316ms step_avg:121.94ms
step:153/1393 train_time:17440ms step_avg:121.96ms
step:154/1393 train_time:17562ms step_avg:121.96ms
step:155/1393 train_time:17684ms step_avg:121.96ms
step:156/1393 train_time:17807ms step_avg:121.96ms
step:157/1393 train_time:17929ms step_avg:121.97ms
step:158/1393 train_time:18051ms step_avg:121.96ms
step:159/1393 train_time:18173ms step_avg:121.97ms
step:160/1393 train_time:18296ms step_avg:121.97ms
step:161/1393 train_time:18418ms step_avg:121.97ms
step:162/1393 train_time:18540ms step_avg:121.98ms
step:163/1393 train_time:18662ms step_avg:121.98ms
step:164/1393 train_time:18785ms step_avg:121.98ms
step:165/1393 train_time:18908ms step_avg:121.98ms
step:166/1393 train_time:19030ms step_avg:121.98ms
step:167/1393 train_time:19151ms step_avg:121.98ms
step:168/1393 train_time:19274ms step_avg:121.98ms
step:169/1393 train_time:19395ms step_avg:121.98ms
step:170/1393 train_time:19517ms step_avg:121.98ms
step:171/1393 train_time:19640ms step_avg:121.99ms
step:172/1393 train_time:19762ms step_avg:121.99ms
step:173/1393 train_time:19885ms step_avg:121.99ms
step:174/1393 train_time:20007ms step_avg:121.99ms
step:175/1393 train_time:20129ms step_avg:121.99ms
step:176/1393 train_time:20250ms step_avg:121.99ms
step:177/1393 train_time:20373ms step_avg:121.99ms
step:178/1393 train_time:20495ms step_avg:122.00ms
step:179/1393 train_time:20617ms step_avg:121.99ms
step:180/1393 train_time:20742ms step_avg:122.01ms
step:181/1393 train_time:20865ms step_avg:122.02ms
step:182/1393 train_time:20987ms step_avg:122.02ms
step:183/1393 train_time:21109ms step_avg:122.02ms
step:184/1393 train_time:21231ms step_avg:122.02ms
step:185/1393 train_time:21354ms step_avg:122.02ms
step:186/1393 train_time:21477ms step_avg:122.03ms
step:187/1393 train_time:21600ms step_avg:122.03ms
step:188/1393 train_time:21722ms step_avg:122.03ms
step:189/1393 train_time:21845ms step_avg:122.04ms
step:190/1393 train_time:21968ms step_avg:122.04ms
step:191/1393 train_time:22090ms step_avg:122.04ms
step:192/1393 train_time:22212ms step_avg:122.04ms
step:193/1393 train_time:22335ms step_avg:122.05ms
step:194/1393 train_time:22457ms step_avg:122.05ms
step:195/1393 train_time:22579ms step_avg:122.05ms
step:196/1393 train_time:22701ms step_avg:122.05ms
step:197/1393 train_time:22823ms step_avg:122.05ms
step:198/1393 train_time:22945ms step_avg:122.05ms
step:199/1393 train_time:23067ms step_avg:122.05ms
step:200/1393 train_time:23190ms step_avg:122.05ms
step:201/1393 train_time:23311ms step_avg:122.05ms
step:202/1393 train_time:23433ms step_avg:122.05ms
step:203/1393 train_time:23556ms step_avg:122.05ms
step:204/1393 train_time:23679ms step_avg:122.06ms
step:205/1393 train_time:23802ms step_avg:122.06ms
step:206/1393 train_time:23925ms step_avg:122.07ms
step:207/1393 train_time:24047ms step_avg:122.07ms
step:208/1393 train_time:24170ms step_avg:122.07ms
step:209/1393 train_time:24292ms step_avg:122.07ms
step:210/1393 train_time:24415ms step_avg:122.08ms
step:211/1393 train_time:24537ms step_avg:122.08ms
step:212/1393 train_time:24660ms step_avg:122.08ms
step:213/1393 train_time:24783ms step_avg:122.08ms
step:214/1393 train_time:24906ms step_avg:122.09ms
step:215/1393 train_time:25029ms step_avg:122.09ms
step:216/1393 train_time:25153ms step_avg:122.10ms
step:217/1393 train_time:25275ms step_avg:122.10ms
step:218/1393 train_time:25399ms step_avg:122.11ms
step:219/1393 train_time:25521ms step_avg:122.11ms
step:220/1393 train_time:25644ms step_avg:122.11ms
step:221/1393 train_time:25767ms step_avg:122.12ms
step:222/1393 train_time:25890ms step_avg:122.12ms
step:223/1393 train_time:26013ms step_avg:122.12ms
step:224/1393 train_time:26135ms step_avg:122.13ms
step:225/1393 train_time:26258ms step_avg:122.13ms
step:226/1393 train_time:26380ms step_avg:122.13ms
step:227/1393 train_time:26503ms step_avg:122.13ms
step:228/1393 train_time:26626ms step_avg:122.14ms
step:229/1393 train_time:26748ms step_avg:122.14ms
step:230/1393 train_time:26871ms step_avg:122.14ms
step:231/1393 train_time:26994ms step_avg:122.15ms
step:232/1393 train_time:27118ms step_avg:122.15ms
step:233/1393 train_time:27240ms step_avg:122.15ms
step:234/1393 train_time:27363ms step_avg:122.15ms
step:235/1393 train_time:27485ms step_avg:122.16ms
step:236/1393 train_time:27607ms step_avg:122.16ms
step:237/1393 train_time:27730ms step_avg:122.16ms
step:238/1393 train_time:27853ms step_avg:122.16ms
step:239/1393 train_time:27975ms step_avg:122.16ms
step:240/1393 train_time:28098ms step_avg:122.17ms
step:241/1393 train_time:28221ms step_avg:122.17ms
step:242/1393 train_time:28344ms step_avg:122.17ms
step:243/1393 train_time:28467ms step_avg:122.18ms
step:244/1393 train_time:28590ms step_avg:122.18ms
step:245/1393 train_time:28713ms step_avg:122.18ms
step:246/1393 train_time:28836ms step_avg:122.19ms
step:247/1393 train_time:28958ms step_avg:122.19ms
step:248/1393 train_time:29081ms step_avg:122.19ms
step:249/1393 train_time:29204ms step_avg:122.19ms
step:250/1393 train_time:29327ms step_avg:122.20ms
step:250/1393 val_loss:3.9852 train_time:29448ms step_avg:122.70ms
step:251/1393 train_time:29470ms step_avg:122.28ms
step:252/1393 train_time:29585ms step_avg:122.25ms
step:253/1393 train_time:29710ms step_avg:122.26ms
step:254/1393 train_time:29832ms step_avg:122.26ms
step:255/1393 train_time:29954ms step_avg:122.26ms
step:256/1393 train_time:30075ms step_avg:122.26ms
step:257/1393 train_time:30197ms step_avg:122.26ms
step:258/1393 train_time:30319ms step_avg:122.25ms
step:259/1393 train_time:30442ms step_avg:122.26ms
step:260/1393 train_time:30567ms step_avg:122.27ms
step:261/1393 train_time:30692ms step_avg:122.28ms
step:262/1393 train_time:30815ms step_avg:122.28ms
step:263/1393 train_time:30937ms step_avg:122.28ms
step:264/1393 train_time:31060ms step_avg:122.28ms
step:265/1393 train_time:31182ms step_avg:122.28ms
step:266/1393 train_time:31304ms step_avg:122.28ms
step:267/1393 train_time:31427ms step_avg:122.28ms
step:268/1393 train_time:31551ms step_avg:122.29ms
step:269/1393 train_time:31674ms step_avg:122.29ms
step:270/1393 train_time:31798ms step_avg:122.30ms
step:271/1393 train_time:31920ms step_avg:122.30ms
step:272/1393 train_time:32043ms step_avg:122.30ms
step:273/1393 train_time:32165ms step_avg:122.30ms
step:274/1393 train_time:32287ms step_avg:122.30ms
step:275/1393 train_time:32410ms step_avg:122.30ms
step:276/1393 train_time:32533ms step_avg:122.30ms
step:277/1393 train_time:32656ms step_avg:122.31ms
step:278/1393 train_time:32780ms step_avg:122.31ms
step:279/1393 train_time:32902ms step_avg:122.31ms
step:280/1393 train_time:33025ms step_avg:122.31ms
step:281/1393 train_time:33147ms step_avg:122.31ms
step:282/1393 train_time:33269ms step_avg:122.31ms
step:283/1393 train_time:33392ms step_avg:122.31ms
step:284/1393 train_time:33514ms step_avg:122.32ms
step:285/1393 train_time:33637ms step_avg:122.32ms
step:286/1393 train_time:33760ms step_avg:122.32ms
step:287/1393 train_time:33882ms step_avg:122.32ms
step:288/1393 train_time:34006ms step_avg:122.32ms
step:289/1393 train_time:34129ms step_avg:122.33ms
step:290/1393 train_time:34252ms step_avg:122.33ms
step:291/1393 train_time:34375ms step_avg:122.33ms
step:292/1393 train_time:34498ms step_avg:122.33ms
step:293/1393 train_time:34621ms step_avg:122.33ms
step:294/1393 train_time:34743ms step_avg:122.34ms
step:295/1393 train_time:34865ms step_avg:122.33ms
step:296/1393 train_time:34988ms step_avg:122.33ms
step:297/1393 train_time:35110ms step_avg:122.34ms
step:298/1393 train_time:35233ms step_avg:122.34ms
step:299/1393 train_time:35356ms step_avg:122.34ms
step:300/1393 train_time:35480ms step_avg:122.34ms
step:301/1393 train_time:35602ms step_avg:122.35ms
step:302/1393 train_time:35725ms step_avg:122.35ms
step:303/1393 train_time:35848ms step_avg:122.35ms
step:304/1393 train_time:35971ms step_avg:122.35ms
step:305/1393 train_time:36093ms step_avg:122.35ms
step:306/1393 train_time:36216ms step_avg:122.35ms
step:307/1393 train_time:36338ms step_avg:122.35ms
step:308/1393 train_time:36461ms step_avg:122.35ms
step:309/1393 train_time:36583ms step_avg:122.35ms
step:310/1393 train_time:36706ms step_avg:122.35ms
step:311/1393 train_time:36828ms step_avg:122.35ms
step:312/1393 train_time:36954ms step_avg:122.36ms
step:313/1393 train_time:37079ms step_avg:122.37ms
step:314/1393 train_time:37204ms step_avg:122.38ms
step:315/1393 train_time:37331ms step_avg:122.40ms
step:316/1393 train_time:37456ms step_avg:122.41ms
step:317/1393 train_time:37582ms step_avg:122.42ms
step:318/1393 train_time:37707ms step_avg:122.42ms
step:319/1393 train_time:37832ms step_avg:122.43ms
step:320/1393 train_time:37957ms step_avg:122.44ms
step:321/1393 train_time:38083ms step_avg:122.45ms
step:322/1393 train_time:38208ms step_avg:122.46ms
step:323/1393 train_time:38334ms step_avg:122.47ms
step:324/1393 train_time:38460ms step_avg:122.48ms
step:325/1393 train_time:38584ms step_avg:122.49ms
step:326/1393 train_time:38709ms step_avg:122.50ms
step:327/1393 train_time:38834ms step_avg:122.51ms
step:328/1393 train_time:38960ms step_avg:122.52ms
step:329/1393 train_time:39085ms step_avg:122.52ms
step:330/1393 train_time:39210ms step_avg:122.53ms
step:331/1393 train_time:39336ms step_avg:122.54ms
step:332/1393 train_time:39462ms step_avg:122.55ms
step:333/1393 train_time:39588ms step_avg:122.56ms
step:334/1393 train_time:39713ms step_avg:122.57ms
step:335/1393 train_time:39839ms step_avg:122.58ms
step:336/1393 train_time:39965ms step_avg:122.59ms
step:337/1393 train_time:40091ms step_avg:122.60ms
step:338/1393 train_time:40216ms step_avg:122.61ms
step:339/1393 train_time:40342ms step_avg:122.62ms
step:340/1393 train_time:40468ms step_avg:122.63ms
step:341/1393 train_time:40594ms step_avg:122.64ms
step:342/1393 train_time:40720ms step_avg:122.65ms
step:343/1393 train_time:40845ms step_avg:122.66ms
step:344/1393 train_time:40970ms step_avg:122.67ms
step:345/1393 train_time:41095ms step_avg:122.67ms
step:346/1393 train_time:41219ms step_avg:122.68ms
step:347/1393 train_time:41345ms step_avg:122.69ms
step:348/1393 train_time:41471ms step_avg:122.70ms
step:349/1393 train_time:41597ms step_avg:122.70ms
step:350/1393 train_time:41723ms step_avg:122.71ms
step:351/1393 train_time:41849ms step_avg:122.73ms
step:352/1393 train_time:41975ms step_avg:122.73ms
step:353/1393 train_time:42100ms step_avg:122.74ms
step:354/1393 train_time:42225ms step_avg:122.75ms
step:355/1393 train_time:42351ms step_avg:122.76ms
step:356/1393 train_time:42476ms step_avg:122.76ms
step:357/1393 train_time:42601ms step_avg:122.77ms
step:358/1393 train_time:42727ms step_avg:122.78ms
step:359/1393 train_time:42854ms step_avg:122.79ms
step:360/1393 train_time:42980ms step_avg:122.80ms
step:361/1393 train_time:43106ms step_avg:122.81ms
step:362/1393 train_time:43230ms step_avg:122.81ms
step:363/1393 train_time:43356ms step_avg:122.82ms
step:364/1393 train_time:43482ms step_avg:122.83ms
step:365/1393 train_time:43607ms step_avg:122.84ms
step:366/1393 train_time:43733ms step_avg:122.85ms
step:367/1393 train_time:43860ms step_avg:122.86ms
step:368/1393 train_time:43986ms step_avg:122.87ms
step:369/1393 train_time:44112ms step_avg:122.87ms
step:370/1393 train_time:44237ms step_avg:122.88ms
step:371/1393 train_time:44363ms step_avg:122.89ms
step:372/1393 train_time:44488ms step_avg:122.89ms
step:373/1393 train_time:44614ms step_avg:122.90ms
step:374/1393 train_time:44739ms step_avg:122.91ms
step:375/1393 train_time:44864ms step_avg:122.92ms
step:375/1393 val_loss:3.7845 train_time:44987ms step_avg:123.25ms
step:376/1393 train_time:45009ms step_avg:122.97ms
step:377/1393 train_time:45124ms step_avg:122.95ms
step:378/1393 train_time:45249ms step_avg:122.96ms
step:379/1393 train_time:45374ms step_avg:122.96ms
step:380/1393 train_time:45498ms step_avg:122.97ms
step:381/1393 train_time:45623ms step_avg:122.97ms
step:382/1393 train_time:45747ms step_avg:122.98ms
step:383/1393 train_time:45872ms step_avg:122.98ms
step:384/1393 train_time:45998ms step_avg:122.99ms
step:385/1393 train_time:46125ms step_avg:123.00ms
step:386/1393 train_time:46251ms step_avg:123.01ms
step:387/1393 train_time:46376ms step_avg:123.01ms
step:388/1393 train_time:46500ms step_avg:123.02ms
step:389/1393 train_time:46627ms step_avg:123.03ms
step:390/1393 train_time:46753ms step_avg:123.03ms
step:391/1393 train_time:46878ms step_avg:123.04ms
step:392/1393 train_time:47005ms step_avg:123.05ms
step:393/1393 train_time:47131ms step_avg:123.06ms
step:394/1393 train_time:47256ms step_avg:123.06ms
step:395/1393 train_time:47381ms step_avg:123.07ms
step:396/1393 train_time:47506ms step_avg:123.07ms
step:397/1393 train_time:47631ms step_avg:123.08ms
step:398/1393 train_time:47756ms step_avg:123.08ms
step:399/1393 train_time:47881ms step_avg:123.09ms
step:400/1393 train_time:48007ms step_avg:123.09ms
step:401/1393 train_time:48133ms step_avg:123.10ms
step:402/1393 train_time:48259ms step_avg:123.11ms
step:403/1393 train_time:48385ms step_avg:123.12ms
step:404/1393 train_time:48510ms step_avg:123.12ms
step:405/1393 train_time:48635ms step_avg:123.13ms
step:406/1393 train_time:48759ms step_avg:123.13ms
step:407/1393 train_time:48884ms step_avg:123.13ms
step:408/1393 train_time:49009ms step_avg:123.14ms
step:409/1393 train_time:49134ms step_avg:123.14ms
step:410/1393 train_time:49260ms step_avg:123.15ms
step:411/1393 train_time:49387ms step_avg:123.16ms
step:412/1393 train_time:49512ms step_avg:123.16ms
step:413/1393 train_time:49637ms step_avg:123.17ms
step:414/1393 train_time:49762ms step_avg:123.17ms
step:415/1393 train_time:49888ms step_avg:123.18ms
step:416/1393 train_time:50014ms step_avg:123.19ms
step:417/1393 train_time:50140ms step_avg:123.19ms
step:418/1393 train_time:50266ms step_avg:123.20ms
step:419/1393 train_time:50392ms step_avg:123.21ms
step:420/1393 train_time:50517ms step_avg:123.21ms
step:421/1393 train_time:50643ms step_avg:123.22ms
step:422/1393 train_time:50768ms step_avg:123.22ms
step:423/1393 train_time:50895ms step_avg:123.23ms
step:424/1393 train_time:51021ms step_avg:123.24ms
step:425/1393 train_time:51146ms step_avg:123.24ms
step:426/1393 train_time:51271ms step_avg:123.25ms
step:427/1393 train_time:51396ms step_avg:123.25ms
step:428/1393 train_time:51522ms step_avg:123.26ms
step:429/1393 train_time:51648ms step_avg:123.26ms
step:430/1393 train_time:51774ms step_avg:123.27ms
step:431/1393 train_time:51900ms step_avg:123.28ms
step:432/1393 train_time:52026ms step_avg:123.28ms
step:433/1393 train_time:52152ms step_avg:123.29ms
step:434/1393 train_time:52278ms step_avg:123.30ms
step:435/1393 train_time:52403ms step_avg:123.30ms
step:436/1393 train_time:52529ms step_avg:123.31ms
step:437/1393 train_time:52655ms step_avg:123.31ms
step:438/1393 train_time:52780ms step_avg:123.32ms
step:439/1393 train_time:52906ms step_avg:123.33ms
step:440/1393 train_time:53032ms step_avg:123.33ms
step:441/1393 train_time:53158ms step_avg:123.34ms
step:442/1393 train_time:53284ms step_avg:123.34ms
step:443/1393 train_time:53410ms step_avg:123.35ms
step:444/1393 train_time:53536ms step_avg:123.35ms
step:445/1393 train_time:53662ms step_avg:123.36ms
step:446/1393 train_time:53788ms step_avg:123.37ms
step:447/1393 train_time:53914ms step_avg:123.37ms
step:448/1393 train_time:54039ms step_avg:123.38ms
step:449/1393 train_time:54165ms step_avg:123.38ms
step:450/1393 train_time:54292ms step_avg:123.39ms
step:451/1393 train_time:54418ms step_avg:123.40ms
step:452/1393 train_time:54543ms step_avg:123.40ms
step:453/1393 train_time:54669ms step_avg:123.41ms
step:454/1393 train_time:54795ms step_avg:123.41ms
step:455/1393 train_time:54921ms step_avg:123.42ms
step:456/1393 train_time:55047ms step_avg:123.42ms
step:457/1393 train_time:55173ms step_avg:123.43ms
step:458/1393 train_time:55298ms step_avg:123.43ms
step:459/1393 train_time:55424ms step_avg:123.44ms
step:460/1393 train_time:55550ms step_avg:123.45ms
step:461/1393 train_time:55676ms step_avg:123.45ms
step:462/1393 train_time:55802ms step_avg:123.46ms
step:463/1393 train_time:55928ms step_avg:123.46ms
step:464/1393 train_time:56053ms step_avg:123.47ms
step:465/1393 train_time:56178ms step_avg:123.47ms
step:466/1393 train_time:56304ms step_avg:123.47ms
step:467/1393 train_time:56430ms step_avg:123.48ms
step:468/1393 train_time:56556ms step_avg:123.48ms
step:469/1393 train_time:56681ms step_avg:123.49ms
step:470/1393 train_time:56807ms step_avg:123.49ms
step:471/1393 train_time:56933ms step_avg:123.50ms
step:472/1393 train_time:57059ms step_avg:123.50ms
step:473/1393 train_time:57183ms step_avg:123.51ms
step:474/1393 train_time:57309ms step_avg:123.51ms
step:475/1393 train_time:57435ms step_avg:123.52ms
step:476/1393 train_time:57561ms step_avg:123.52ms
step:477/1393 train_time:57687ms step_avg:123.53ms
step:478/1393 train_time:57813ms step_avg:123.53ms
step:479/1393 train_time:57938ms step_avg:123.54ms
step:480/1393 train_time:58064ms step_avg:123.54ms
step:481/1393 train_time:58190ms step_avg:123.54ms
step:482/1393 train_time:58315ms step_avg:123.55ms
step:483/1393 train_time:58440ms step_avg:123.55ms
step:484/1393 train_time:58567ms step_avg:123.56ms
step:485/1393 train_time:58694ms step_avg:123.57ms
step:486/1393 train_time:58819ms step_avg:123.57ms
step:487/1393 train_time:58945ms step_avg:123.58ms
step:488/1393 train_time:59071ms step_avg:123.58ms
step:489/1393 train_time:59197ms step_avg:123.58ms
step:490/1393 train_time:59322ms step_avg:123.59ms
step:491/1393 train_time:59448ms step_avg:123.59ms
step:492/1393 train_time:59574ms step_avg:123.60ms
step:493/1393 train_time:59701ms step_avg:123.60ms
step:494/1393 train_time:59826ms step_avg:123.61ms
step:495/1393 train_time:59952ms step_avg:123.61ms
step:496/1393 train_time:60078ms step_avg:123.62ms
step:497/1393 train_time:60203ms step_avg:123.62ms
step:498/1393 train_time:60329ms step_avg:123.63ms
step:499/1393 train_time:60455ms step_avg:123.63ms
step:500/1393 train_time:60581ms step_avg:123.63ms
step:500/1393 val_loss:3.6639 train_time:60705ms step_avg:123.89ms
step:501/1393 train_time:60727ms step_avg:123.68ms
step:502/1393 train_time:60840ms step_avg:123.66ms
step:503/1393 train_time:60968ms step_avg:123.67ms
step:504/1393 train_time:61093ms step_avg:123.67ms
step:505/1393 train_time:61219ms step_avg:123.67ms
step:506/1393 train_time:61344ms step_avg:123.68ms
step:507/1393 train_time:61469ms step_avg:123.68ms
step:508/1393 train_time:61594ms step_avg:123.68ms
step:509/1393 train_time:61721ms step_avg:123.69ms
step:510/1393 train_time:61849ms step_avg:123.70ms
step:511/1393 train_time:61974ms step_avg:123.70ms
step:512/1393 train_time:62100ms step_avg:123.71ms
step:513/1393 train_time:62225ms step_avg:123.71ms
step:514/1393 train_time:62350ms step_avg:123.71ms
step:515/1393 train_time:62476ms step_avg:123.71ms
step:516/1393 train_time:62601ms step_avg:123.72ms
step:517/1393 train_time:62726ms step_avg:123.72ms
step:518/1393 train_time:62855ms step_avg:123.73ms
step:519/1393 train_time:62982ms step_avg:123.74ms
step:520/1393 train_time:63109ms step_avg:123.74ms
step:521/1393 train_time:63237ms step_avg:123.75ms
step:522/1393 train_time:63364ms step_avg:123.76ms
step:523/1393 train_time:63491ms step_avg:123.76ms
step:524/1393 train_time:63619ms step_avg:123.77ms
step:525/1393 train_time:63747ms step_avg:123.78ms
step:526/1393 train_time:63876ms step_avg:123.79ms
step:527/1393 train_time:64003ms step_avg:123.80ms
step:528/1393 train_time:64131ms step_avg:123.80ms
step:529/1393 train_time:64258ms step_avg:123.81ms
step:530/1393 train_time:64385ms step_avg:123.82ms
step:531/1393 train_time:64513ms step_avg:123.83ms
step:532/1393 train_time:64640ms step_avg:123.83ms
step:533/1393 train_time:64769ms step_avg:123.84ms
step:534/1393 train_time:64897ms step_avg:123.85ms
step:535/1393 train_time:65025ms step_avg:123.86ms
step:536/1393 train_time:65153ms step_avg:123.87ms
step:537/1393 train_time:65281ms step_avg:123.87ms
step:538/1393 train_time:65408ms step_avg:123.88ms
step:539/1393 train_time:65536ms step_avg:123.89ms
step:540/1393 train_time:65664ms step_avg:123.90ms
step:541/1393 train_time:65792ms step_avg:123.90ms
step:542/1393 train_time:65919ms step_avg:123.91ms
step:543/1393 train_time:66047ms step_avg:123.91ms
step:544/1393 train_time:66174ms step_avg:123.92ms
step:545/1393 train_time:66301ms step_avg:123.93ms
step:546/1393 train_time:66429ms step_avg:123.93ms
step:547/1393 train_time:66556ms step_avg:123.94ms
step:548/1393 train_time:66684ms step_avg:123.95ms
step:549/1393 train_time:66812ms step_avg:123.95ms
step:550/1393 train_time:66939ms step_avg:123.96ms
step:551/1393 train_time:67067ms step_avg:123.97ms
step:552/1393 train_time:67196ms step_avg:123.98ms
step:553/1393 train_time:67323ms step_avg:123.98ms
step:554/1393 train_time:67452ms step_avg:123.99ms
step:555/1393 train_time:67579ms step_avg:124.00ms
step:556/1393 train_time:67706ms step_avg:124.00ms
step:557/1393 train_time:67833ms step_avg:124.01ms
step:558/1393 train_time:67961ms step_avg:124.02ms
step:559/1393 train_time:68089ms step_avg:124.02ms
step:560/1393 train_time:68217ms step_avg:124.03ms
step:561/1393 train_time:68345ms step_avg:124.04ms
step:562/1393 train_time:68473ms step_avg:124.05ms
step:563/1393 train_time:68600ms step_avg:124.05ms
step:564/1393 train_time:68728ms step_avg:124.06ms
step:565/1393 train_time:68855ms step_avg:124.06ms
step:566/1393 train_time:68984ms step_avg:124.07ms
step:567/1393 train_time:69112ms step_avg:124.08ms
step:568/1393 train_time:69240ms step_avg:124.09ms
step:569/1393 train_time:69367ms step_avg:124.09ms
step:570/1393 train_time:69495ms step_avg:124.10ms
step:571/1393 train_time:69624ms step_avg:124.11ms
step:572/1393 train_time:69752ms step_avg:124.11ms
step:573/1393 train_time:69880ms step_avg:124.12ms
step:574/1393 train_time:70008ms step_avg:124.13ms
step:575/1393 train_time:70137ms step_avg:124.14ms
step:576/1393 train_time:70264ms step_avg:124.14ms
step:577/1393 train_time:70392ms step_avg:124.15ms
step:578/1393 train_time:70519ms step_avg:124.15ms
step:579/1393 train_time:70647ms step_avg:124.16ms
step:580/1393 train_time:70774ms step_avg:124.17ms
step:581/1393 train_time:70902ms step_avg:124.17ms
step:582/1393 train_time:71031ms step_avg:124.18ms
step:583/1393 train_time:71159ms step_avg:124.19ms
step:584/1393 train_time:71287ms step_avg:124.19ms
step:585/1393 train_time:71414ms step_avg:124.20ms
step:586/1393 train_time:71542ms step_avg:124.20ms
step:587/1393 train_time:71670ms step_avg:124.21ms
step:588/1393 train_time:71798ms step_avg:124.22ms
step:589/1393 train_time:71925ms step_avg:124.22ms
step:590/1393 train_time:72053ms step_avg:124.23ms
step:591/1393 train_time:72181ms step_avg:124.24ms
step:592/1393 train_time:72309ms step_avg:124.24ms
step:593/1393 train_time:72437ms step_avg:124.25ms
step:594/1393 train_time:72564ms step_avg:124.25ms
step:595/1393 train_time:72692ms step_avg:124.26ms
step:596/1393 train_time:72820ms step_avg:124.27ms
step:597/1393 train_time:72947ms step_avg:124.27ms
step:598/1393 train_time:73076ms step_avg:124.28ms
step:599/1393 train_time:73203ms step_avg:124.28ms
step:600/1393 train_time:73331ms step_avg:124.29ms
step:601/1393 train_time:73459ms step_avg:124.30ms
step:602/1393 train_time:73586ms step_avg:124.30ms
step:603/1393 train_time:73714ms step_avg:124.31ms
step:604/1393 train_time:73842ms step_avg:124.31ms
step:605/1393 train_time:73970ms step_avg:124.32ms
step:606/1393 train_time:74097ms step_avg:124.32ms
step:607/1393 train_time:74224ms step_avg:124.33ms
step:608/1393 train_time:74352ms step_avg:124.33ms
step:609/1393 train_time:74480ms step_avg:124.34ms
step:610/1393 train_time:74608ms step_avg:124.35ms
step:611/1393 train_time:74735ms step_avg:124.35ms
step:612/1393 train_time:74863ms step_avg:124.36ms
step:613/1393 train_time:74991ms step_avg:124.36ms
step:614/1393 train_time:75119ms step_avg:124.37ms
step:615/1393 train_time:75246ms step_avg:124.37ms
step:616/1393 train_time:75374ms step_avg:124.38ms
step:617/1393 train_time:75501ms step_avg:124.38ms
step:618/1393 train_time:75629ms step_avg:124.39ms
step:619/1393 train_time:75758ms step_avg:124.40ms
step:620/1393 train_time:75885ms step_avg:124.40ms
step:621/1393 train_time:76013ms step_avg:124.41ms
step:622/1393 train_time:76141ms step_avg:124.41ms
step:623/1393 train_time:76269ms step_avg:124.42ms
step:624/1393 train_time:76396ms step_avg:124.42ms
step:625/1393 train_time:76524ms step_avg:124.43ms
step:625/1393 val_loss:3.5828 train_time:76651ms step_avg:124.64ms
step:626/1393 train_time:76673ms step_avg:124.47ms
step:627/1393 train_time:76790ms step_avg:124.46ms
step:628/1393 train_time:76920ms step_avg:124.47ms
step:629/1393 train_time:77047ms step_avg:124.47ms
step:630/1393 train_time:77175ms step_avg:124.48ms
step:631/1393 train_time:77302ms step_avg:124.48ms
step:632/1393 train_time:77429ms step_avg:124.48ms
step:633/1393 train_time:77556ms step_avg:124.49ms
step:634/1393 train_time:77686ms step_avg:124.50ms
step:635/1393 train_time:77815ms step_avg:124.50ms
step:636/1393 train_time:77943ms step_avg:124.51ms
step:637/1393 train_time:78070ms step_avg:124.51ms
step:638/1393 train_time:78198ms step_avg:124.52ms
step:639/1393 train_time:78326ms step_avg:124.52ms
step:640/1393 train_time:78453ms step_avg:124.53ms
step:641/1393 train_time:78581ms step_avg:124.53ms
step:642/1393 train_time:78709ms step_avg:124.54ms
step:643/1393 train_time:78837ms step_avg:124.55ms
step:644/1393 train_time:78966ms step_avg:124.55ms
step:645/1393 train_time:79094ms step_avg:124.56ms
step:646/1393 train_time:79222ms step_avg:124.56ms
step:647/1393 train_time:79350ms step_avg:124.57ms
step:648/1393 train_time:79477ms step_avg:124.57ms
step:649/1393 train_time:79605ms step_avg:124.58ms
step:650/1393 train_time:79733ms step_avg:124.58ms
step:651/1393 train_time:79862ms step_avg:124.59ms
step:652/1393 train_time:79991ms step_avg:124.60ms
step:653/1393 train_time:80119ms step_avg:124.60ms
step:654/1393 train_time:80246ms step_avg:124.61ms
step:655/1393 train_time:80374ms step_avg:124.61ms
step:656/1393 train_time:80502ms step_avg:124.62ms
step:657/1393 train_time:80630ms step_avg:124.62ms
step:658/1393 train_time:80759ms step_avg:124.63ms
step:659/1393 train_time:80887ms step_avg:124.63ms
step:660/1393 train_time:81015ms step_avg:124.64ms
step:661/1393 train_time:81143ms step_avg:124.64ms
step:662/1393 train_time:81272ms step_avg:124.65ms
step:663/1393 train_time:81400ms step_avg:124.66ms
step:664/1393 train_time:81528ms step_avg:124.66ms
step:665/1393 train_time:81656ms step_avg:124.67ms
step:666/1393 train_time:81784ms step_avg:124.67ms
step:667/1393 train_time:81913ms step_avg:124.68ms
step:668/1393 train_time:82041ms step_avg:124.68ms
step:669/1393 train_time:82169ms step_avg:124.69ms
step:670/1393 train_time:82297ms step_avg:124.69ms
step:671/1393 train_time:82425ms step_avg:124.70ms
step:672/1393 train_time:82553ms step_avg:124.70ms
step:673/1393 train_time:82681ms step_avg:124.71ms
step:674/1393 train_time:82810ms step_avg:124.71ms
step:675/1393 train_time:82938ms step_avg:124.72ms
step:676/1393 train_time:83066ms step_avg:124.72ms
step:677/1393 train_time:83194ms step_avg:124.73ms
step:678/1393 train_time:83321ms step_avg:124.73ms
step:679/1393 train_time:83450ms step_avg:124.74ms
step:680/1393 train_time:83578ms step_avg:124.74ms
step:681/1393 train_time:83707ms step_avg:124.75ms
step:682/1393 train_time:83834ms step_avg:124.75ms
step:683/1393 train_time:83962ms step_avg:124.76ms
step:684/1393 train_time:84091ms step_avg:124.76ms
step:685/1393 train_time:84219ms step_avg:124.77ms
step:686/1393 train_time:84348ms step_avg:124.77ms
step:687/1393 train_time:84475ms step_avg:124.78ms
step:688/1393 train_time:84603ms step_avg:124.78ms
step:689/1393 train_time:84732ms step_avg:124.79ms
step:690/1393 train_time:84860ms step_avg:124.79ms
step:691/1393 train_time:84988ms step_avg:124.80ms
step:692/1393 train_time:85115ms step_avg:124.80ms
step:693/1393 train_time:85243ms step_avg:124.81ms
step:694/1393 train_time:85371ms step_avg:124.81ms
step:695/1393 train_time:85499ms step_avg:124.82ms
step:696/1393 train_time:85627ms step_avg:124.82ms
step:697/1393 train_time:85756ms step_avg:124.83ms
step:698/1393 train_time:85884ms step_avg:124.83ms
step:699/1393 train_time:86012ms step_avg:124.84ms
step:700/1393 train_time:86140ms step_avg:124.84ms
step:701/1393 train_time:86268ms step_avg:124.85ms
step:702/1393 train_time:86396ms step_avg:124.85ms
step:703/1393 train_time:86524ms step_avg:124.85ms
step:704/1393 train_time:86652ms step_avg:124.86ms
step:705/1393 train_time:86780ms step_avg:124.86ms
step:706/1393 train_time:86908ms step_avg:124.87ms
step:707/1393 train_time:87036ms step_avg:124.87ms
step:708/1393 train_time:87164ms step_avg:124.88ms
step:709/1393 train_time:87292ms step_avg:124.88ms
step:710/1393 train_time:87420ms step_avg:124.89ms
step:711/1393 train_time:87549ms step_avg:124.89ms
step:712/1393 train_time:87677ms step_avg:124.90ms
step:713/1393 train_time:87805ms step_avg:124.90ms
step:714/1393 train_time:87934ms step_avg:124.91ms
step:715/1393 train_time:88062ms step_avg:124.91ms
step:716/1393 train_time:88191ms step_avg:124.92ms
step:717/1393 train_time:88319ms step_avg:124.92ms
step:718/1393 train_time:88447ms step_avg:124.93ms
step:719/1393 train_time:88575ms step_avg:124.93ms
step:720/1393 train_time:88704ms step_avg:124.94ms
step:721/1393 train_time:88832ms step_avg:124.94ms
step:722/1393 train_time:88960ms step_avg:124.94ms
step:723/1393 train_time:89088ms step_avg:124.95ms
step:724/1393 train_time:89216ms step_avg:124.95ms
step:725/1393 train_time:89347ms step_avg:124.96ms
step:726/1393 train_time:89477ms step_avg:124.97ms
step:727/1393 train_time:89607ms step_avg:124.97ms
step:728/1393 train_time:89737ms step_avg:124.98ms
step:729/1393 train_time:89867ms step_avg:124.99ms
step:730/1393 train_time:89997ms step_avg:125.00ms
step:731/1393 train_time:90126ms step_avg:125.00ms
step:732/1393 train_time:90256ms step_avg:125.01ms
step:733/1393 train_time:90388ms step_avg:125.02ms
step:734/1393 train_time:90517ms step_avg:125.02ms
step:735/1393 train_time:90648ms step_avg:125.03ms
step:736/1393 train_time:90778ms step_avg:125.04ms
step:737/1393 train_time:90909ms step_avg:125.05ms
step:738/1393 train_time:91038ms step_avg:125.05ms
step:739/1393 train_time:91168ms step_avg:125.06ms
step:740/1393 train_time:91297ms step_avg:125.06ms
step:741/1393 train_time:91430ms step_avg:125.08ms
step:742/1393 train_time:91559ms step_avg:125.08ms
step:743/1393 train_time:91689ms step_avg:125.09ms
step:744/1393 train_time:91819ms step_avg:125.09ms
step:745/1393 train_time:91950ms step_avg:125.10ms
step:746/1393 train_time:92079ms step_avg:125.11ms
step:747/1393 train_time:92209ms step_avg:125.11ms
step:748/1393 train_time:92339ms step_avg:125.12ms
step:749/1393 train_time:92469ms step_avg:125.13ms
step:750/1393 train_time:92599ms step_avg:125.13ms
step:750/1393 val_loss:3.5272 train_time:92728ms step_avg:125.31ms
step:751/1393 train_time:92751ms step_avg:125.17ms
step:752/1393 train_time:92868ms step_avg:125.16ms
step:753/1393 train_time:92998ms step_avg:125.17ms
step:754/1393 train_time:93128ms step_avg:125.17ms
step:755/1393 train_time:93257ms step_avg:125.18ms
step:756/1393 train_time:93386ms step_avg:125.18ms
step:757/1393 train_time:93516ms step_avg:125.19ms
step:758/1393 train_time:93645ms step_avg:125.19ms
step:759/1393 train_time:93776ms step_avg:125.20ms
step:760/1393 train_time:93908ms step_avg:125.21ms
step:761/1393 train_time:94038ms step_avg:125.22ms
step:762/1393 train_time:94168ms step_avg:125.22ms
step:763/1393 train_time:94298ms step_avg:125.23ms
step:764/1393 train_time:94427ms step_avg:125.24ms
step:765/1393 train_time:94557ms step_avg:125.24ms
step:766/1393 train_time:94688ms step_avg:125.25ms
step:767/1393 train_time:94819ms step_avg:125.26ms
step:768/1393 train_time:94949ms step_avg:125.26ms
step:769/1393 train_time:95080ms step_avg:125.27ms
step:770/1393 train_time:95210ms step_avg:125.28ms
step:771/1393 train_time:95340ms step_avg:125.28ms
step:772/1393 train_time:95470ms step_avg:125.29ms
step:773/1393 train_time:95600ms step_avg:125.29ms
step:774/1393 train_time:95731ms step_avg:125.30ms
step:775/1393 train_time:95860ms step_avg:125.31ms
step:776/1393 train_time:95992ms step_avg:125.32ms
step:777/1393 train_time:96122ms step_avg:125.32ms
step:778/1393 train_time:96252ms step_avg:125.33ms
step:779/1393 train_time:96381ms step_avg:125.33ms
step:780/1393 train_time:96511ms step_avg:125.34ms
step:781/1393 train_time:96640ms step_avg:125.34ms
step:782/1393 train_time:96770ms step_avg:125.35ms
step:783/1393 train_time:96899ms step_avg:125.35ms
step:784/1393 train_time:97030ms step_avg:125.36ms
step:785/1393 train_time:97160ms step_avg:125.37ms
step:786/1393 train_time:97290ms step_avg:125.37ms
step:787/1393 train_time:97420ms step_avg:125.38ms
step:788/1393 train_time:97550ms step_avg:125.39ms
step:789/1393 train_time:97680ms step_avg:125.39ms
step:790/1393 train_time:97810ms step_avg:125.40ms
step:791/1393 train_time:97940ms step_avg:125.40ms
step:792/1393 train_time:98070ms step_avg:125.41ms
step:793/1393 train_time:98200ms step_avg:125.42ms
step:794/1393 train_time:98331ms step_avg:125.42ms
step:795/1393 train_time:98461ms step_avg:125.43ms
step:796/1393 train_time:98592ms step_avg:125.44ms
step:797/1393 train_time:98723ms step_avg:125.44ms
step:798/1393 train_time:98853ms step_avg:125.45ms
step:799/1393 train_time:98983ms step_avg:125.45ms
step:800/1393 train_time:99113ms step_avg:125.46ms
step:801/1393 train_time:99242ms step_avg:125.46ms
step:802/1393 train_time:99373ms step_avg:125.47ms
step:803/1393 train_time:99502ms step_avg:125.48ms
step:804/1393 train_time:99632ms step_avg:125.48ms
step:805/1393 train_time:99763ms step_avg:125.49ms
step:806/1393 train_time:99893ms step_avg:125.49ms
step:807/1393 train_time:100023ms step_avg:125.50ms
step:808/1393 train_time:100153ms step_avg:125.51ms
step:809/1393 train_time:100283ms step_avg:125.51ms
step:810/1393 train_time:100412ms step_avg:125.52ms
step:811/1393 train_time:100541ms step_avg:125.52ms
step:812/1393 train_time:100672ms step_avg:125.53ms
step:813/1393 train_time:100803ms step_avg:125.53ms
step:814/1393 train_time:100932ms step_avg:125.54ms
step:815/1393 train_time:101062ms step_avg:125.54ms
step:816/1393 train_time:101193ms step_avg:125.55ms
step:817/1393 train_time:101323ms step_avg:125.56ms
step:818/1393 train_time:101454ms step_avg:125.56ms
step:819/1393 train_time:101585ms step_avg:125.57ms
step:820/1393 train_time:101716ms step_avg:125.57ms
step:821/1393 train_time:101845ms step_avg:125.58ms
step:822/1393 train_time:101975ms step_avg:125.58ms
step:823/1393 train_time:102105ms step_avg:125.59ms
step:824/1393 train_time:102235ms step_avg:125.60ms
step:825/1393 train_time:102365ms step_avg:125.60ms
step:826/1393 train_time:102495ms step_avg:125.61ms
step:827/1393 train_time:102625ms step_avg:125.61ms
step:828/1393 train_time:102756ms step_avg:125.62ms
step:829/1393 train_time:102886ms step_avg:125.62ms
step:830/1393 train_time:103017ms step_avg:125.63ms
step:831/1393 train_time:103147ms step_avg:125.64ms
step:832/1393 train_time:103278ms step_avg:125.64ms
step:833/1393 train_time:103408ms step_avg:125.65ms
step:834/1393 train_time:103538ms step_avg:125.65ms
step:835/1393 train_time:103668ms step_avg:125.66ms
step:836/1393 train_time:103798ms step_avg:125.66ms
step:837/1393 train_time:103929ms step_avg:125.67ms
step:838/1393 train_time:104058ms step_avg:125.67ms
step:839/1393 train_time:104187ms step_avg:125.68ms
step:840/1393 train_time:104318ms step_avg:125.68ms
step:841/1393 train_time:104448ms step_avg:125.69ms
step:842/1393 train_time:104578ms step_avg:125.70ms
step:843/1393 train_time:104709ms step_avg:125.70ms
step:844/1393 train_time:104839ms step_avg:125.71ms
step:845/1393 train_time:104970ms step_avg:125.71ms
step:846/1393 train_time:105100ms step_avg:125.72ms
step:847/1393 train_time:105230ms step_avg:125.72ms
step:848/1393 train_time:105360ms step_avg:125.73ms
step:849/1393 train_time:105491ms step_avg:125.73ms
step:850/1393 train_time:105621ms step_avg:125.74ms
step:851/1393 train_time:105752ms step_avg:125.75ms
step:852/1393 train_time:105882ms step_avg:125.75ms
step:853/1393 train_time:106012ms step_avg:125.76ms
step:854/1393 train_time:106142ms step_avg:125.76ms
step:855/1393 train_time:106272ms step_avg:125.77ms
step:856/1393 train_time:106402ms step_avg:125.77ms
step:857/1393 train_time:106532ms step_avg:125.78ms
step:858/1393 train_time:106662ms step_avg:125.78ms
step:859/1393 train_time:106793ms step_avg:125.79ms
step:860/1393 train_time:106924ms step_avg:125.79ms
step:861/1393 train_time:107053ms step_avg:125.80ms
step:862/1393 train_time:107184ms step_avg:125.80ms
step:863/1393 train_time:107314ms step_avg:125.81ms
step:864/1393 train_time:107445ms step_avg:125.81ms
step:865/1393 train_time:107575ms step_avg:125.82ms
step:866/1393 train_time:107707ms step_avg:125.83ms
step:867/1393 train_time:107837ms step_avg:125.83ms
step:868/1393 train_time:107966ms step_avg:125.83ms
step:869/1393 train_time:108096ms step_avg:125.84ms
step:870/1393 train_time:108227ms step_avg:125.85ms
step:871/1393 train_time:108358ms step_avg:125.85ms
step:872/1393 train_time:108487ms step_avg:125.86ms
step:873/1393 train_time:108618ms step_avg:125.86ms
step:874/1393 train_time:108748ms step_avg:125.87ms
step:875/1393 train_time:108878ms step_avg:125.87ms
step:875/1393 val_loss:3.4772 train_time:109008ms step_avg:126.02ms
step:876/1393 train_time:109030ms step_avg:125.90ms
step:877/1393 train_time:109147ms step_avg:125.89ms
step:878/1393 train_time:109277ms step_avg:125.89ms
step:879/1393 train_time:109407ms step_avg:125.90ms
step:880/1393 train_time:109536ms step_avg:125.90ms
step:881/1393 train_time:109665ms step_avg:125.91ms
step:882/1393 train_time:109795ms step_avg:125.91ms
step:883/1393 train_time:109924ms step_avg:125.92ms
step:884/1393 train_time:110056ms step_avg:125.92ms
step:885/1393 train_time:110189ms step_avg:125.93ms
step:886/1393 train_time:110319ms step_avg:125.93ms
step:887/1393 train_time:110449ms step_avg:125.94ms
step:888/1393 train_time:110579ms step_avg:125.94ms
step:889/1393 train_time:110711ms step_avg:125.95ms
step:890/1393 train_time:110839ms step_avg:125.95ms
step:891/1393 train_time:110970ms step_avg:125.96ms
step:892/1393 train_time:111100ms step_avg:125.96ms
step:893/1393 train_time:111231ms step_avg:125.97ms
step:894/1393 train_time:111360ms step_avg:125.97ms
step:895/1393 train_time:111492ms step_avg:125.98ms
step:896/1393 train_time:111622ms step_avg:125.98ms
step:897/1393 train_time:111751ms step_avg:125.99ms
step:898/1393 train_time:111881ms step_avg:125.99ms
step:899/1393 train_time:112012ms step_avg:126.00ms
step:900/1393 train_time:112143ms step_avg:126.00ms
step:901/1393 train_time:112274ms step_avg:126.01ms
step:902/1393 train_time:112405ms step_avg:126.01ms
step:903/1393 train_time:112536ms step_avg:126.02ms
step:904/1393 train_time:112666ms step_avg:126.02ms
step:905/1393 train_time:112797ms step_avg:126.03ms
step:906/1393 train_time:112927ms step_avg:126.03ms
step:907/1393 train_time:113058ms step_avg:126.04ms
step:908/1393 train_time:113189ms step_avg:126.05ms
step:909/1393 train_time:113319ms step_avg:126.05ms
step:910/1393 train_time:113451ms step_avg:126.06ms
step:911/1393 train_time:113581ms step_avg:126.06ms
step:912/1393 train_time:113711ms step_avg:126.07ms
step:913/1393 train_time:113842ms step_avg:126.07ms
step:914/1393 train_time:113972ms step_avg:126.08ms
step:915/1393 train_time:114103ms step_avg:126.08ms
step:916/1393 train_time:114234ms step_avg:126.09ms
step:917/1393 train_time:114366ms step_avg:126.09ms
step:918/1393 train_time:114497ms step_avg:126.10ms
step:919/1393 train_time:114629ms step_avg:126.10ms
step:920/1393 train_time:114760ms step_avg:126.11ms
step:921/1393 train_time:114891ms step_avg:126.11ms
step:922/1393 train_time:115021ms step_avg:126.12ms
step:923/1393 train_time:115150ms step_avg:126.12ms
step:924/1393 train_time:115280ms step_avg:126.13ms
step:925/1393 train_time:115411ms step_avg:126.13ms
step:926/1393 train_time:115541ms step_avg:126.14ms
step:927/1393 train_time:115672ms step_avg:126.14ms
step:928/1393 train_time:115804ms step_avg:126.15ms
step:929/1393 train_time:115934ms step_avg:126.15ms
step:930/1393 train_time:116064ms step_avg:126.16ms
step:931/1393 train_time:116196ms step_avg:126.16ms
step:932/1393 train_time:116327ms step_avg:126.17ms
step:933/1393 train_time:116460ms step_avg:126.18ms
step:934/1393 train_time:116592ms step_avg:126.18ms
step:935/1393 train_time:116724ms step_avg:126.19ms
step:936/1393 train_time:116857ms step_avg:126.19ms
step:937/1393 train_time:116992ms step_avg:126.20ms
step:938/1393 train_time:117124ms step_avg:126.21ms
step:939/1393 train_time:117256ms step_avg:126.22ms
step:940/1393 train_time:117390ms step_avg:126.23ms
step:941/1393 train_time:117520ms step_avg:126.23ms
step:942/1393 train_time:117652ms step_avg:126.24ms
step:943/1393 train_time:117786ms step_avg:126.24ms
step:944/1393 train_time:117920ms step_avg:126.25ms
step:945/1393 train_time:118052ms step_avg:126.26ms
step:946/1393 train_time:118184ms step_avg:126.27ms
step:947/1393 train_time:118317ms step_avg:126.27ms
step:948/1393 train_time:118448ms step_avg:126.28ms
step:949/1393 train_time:118581ms step_avg:126.28ms
step:950/1393 train_time:118713ms step_avg:126.29ms
step:951/1393 train_time:118847ms step_avg:126.30ms
step:952/1393 train_time:118979ms step_avg:126.30ms
step:953/1393 train_time:119111ms step_avg:126.31ms
step:954/1393 train_time:119243ms step_avg:126.32ms
step:955/1393 train_time:119374ms step_avg:126.32ms
step:956/1393 train_time:119508ms step_avg:126.33ms
step:957/1393 train_time:119640ms step_avg:126.34ms
step:958/1393 train_time:119773ms step_avg:126.34ms
step:959/1393 train_time:119906ms step_avg:126.35ms
step:960/1393 train_time:120038ms step_avg:126.36ms
step:961/1393 train_time:120170ms step_avg:126.36ms
step:962/1393 train_time:120301ms step_avg:126.37ms
step:963/1393 train_time:120434ms step_avg:126.37ms
step:964/1393 train_time:120566ms step_avg:126.38ms
step:965/1393 train_time:120700ms step_avg:126.39ms
step:966/1393 train_time:120832ms step_avg:126.39ms
step:967/1393 train_time:120964ms step_avg:126.40ms
step:968/1393 train_time:121095ms step_avg:126.40ms
step:969/1393 train_time:121228ms step_avg:126.41ms
step:970/1393 train_time:121360ms step_avg:126.42ms
step:971/1393 train_time:121492ms step_avg:126.42ms
step:972/1393 train_time:121624ms step_avg:126.43ms
step:973/1393 train_time:121756ms step_avg:126.43ms
step:974/1393 train_time:121888ms step_avg:126.44ms
step:975/1393 train_time:122019ms step_avg:126.44ms
step:976/1393 train_time:122150ms step_avg:126.45ms
step:977/1393 train_time:122282ms step_avg:126.45ms
step:978/1393 train_time:122414ms step_avg:126.46ms
step:979/1393 train_time:122545ms step_avg:126.47ms
step:980/1393 train_time:122679ms step_avg:126.47ms
step:981/1393 train_time:122810ms step_avg:126.48ms
step:982/1393 train_time:122941ms step_avg:126.48ms
step:983/1393 train_time:123073ms step_avg:126.49ms
step:984/1393 train_time:123204ms step_avg:126.49ms
step:985/1393 train_time:123337ms step_avg:126.50ms
step:986/1393 train_time:123471ms step_avg:126.51ms
step:987/1393 train_time:123603ms step_avg:126.51ms
step:988/1393 train_time:123735ms step_avg:126.52ms
step:989/1393 train_time:123867ms step_avg:126.52ms
step:990/1393 train_time:123999ms step_avg:126.53ms
step:991/1393 train_time:124131ms step_avg:126.54ms
step:992/1393 train_time:124265ms step_avg:126.54ms
step:993/1393 train_time:124400ms step_avg:126.55ms
step:994/1393 train_time:124532ms step_avg:126.56ms
step:995/1393 train_time:124664ms step_avg:126.56ms
step:996/1393 train_time:124796ms step_avg:126.57ms
step:997/1393 train_time:124927ms step_avg:126.57ms
step:998/1393 train_time:125058ms step_avg:126.58ms
step:999/1393 train_time:125191ms step_avg:126.58ms
step:1000/1393 train_time:125322ms step_avg:126.59ms
step:1000/1393 val_loss:3.4139 train_time:125452ms step_avg:126.72ms
step:1001/1393 train_time:125474ms step_avg:126.61ms
step:1002/1393 train_time:125591ms step_avg:126.60ms
step:1003/1393 train_time:125723ms step_avg:126.61ms
step:1004/1393 train_time:125856ms step_avg:126.62ms
step:1005/1393 train_time:125987ms step_avg:126.62ms
step:1006/1393 train_time:126118ms step_avg:126.62ms
step:1007/1393 train_time:126248ms step_avg:126.63ms
step:1008/1393 train_time:126381ms step_avg:126.63ms
step:1009/1393 train_time:126516ms step_avg:126.64ms
step:1010/1393 train_time:126649ms step_avg:126.65ms
step:1011/1393 train_time:126782ms step_avg:126.66ms
step:1012/1393 train_time:126913ms step_avg:126.66ms
step:1013/1393 train_time:127045ms step_avg:126.66ms
step:1014/1393 train_time:127176ms step_avg:126.67ms
step:1015/1393 train_time:127307ms step_avg:126.67ms
step:1016/1393 train_time:127439ms step_avg:126.68ms
step:1017/1393 train_time:127572ms step_avg:126.68ms
step:1018/1393 train_time:127704ms step_avg:126.69ms
step:1019/1393 train_time:127837ms step_avg:126.70ms
step:1020/1393 train_time:127968ms step_avg:126.70ms
step:1021/1393 train_time:128100ms step_avg:126.71ms
step:1022/1393 train_time:128230ms step_avg:126.71ms
step:1023/1393 train_time:128362ms step_avg:126.71ms
step:1024/1393 train_time:128493ms step_avg:126.72ms
step:1025/1393 train_time:128626ms step_avg:126.73ms
step:1026/1393 train_time:128758ms step_avg:126.73ms
step:1027/1393 train_time:128892ms step_avg:126.74ms
step:1028/1393 train_time:129024ms step_avg:126.74ms
step:1029/1393 train_time:129156ms step_avg:126.75ms
step:1030/1393 train_time:129288ms step_avg:126.75ms
step:1031/1393 train_time:129419ms step_avg:126.76ms
step:1032/1393 train_time:129550ms step_avg:126.76ms
step:1033/1393 train_time:129683ms step_avg:126.77ms
step:1034/1393 train_time:129816ms step_avg:126.77ms
step:1035/1393 train_time:129949ms step_avg:126.78ms
step:1036/1393 train_time:130081ms step_avg:126.78ms
step:1037/1393 train_time:130215ms step_avg:126.79ms
step:1038/1393 train_time:130347ms step_avg:126.80ms
step:1039/1393 train_time:130479ms step_avg:126.80ms
step:1040/1393 train_time:130612ms step_avg:126.81ms
step:1041/1393 train_time:130743ms step_avg:126.81ms
step:1042/1393 train_time:130876ms step_avg:126.82ms
step:1043/1393 train_time:131009ms step_avg:126.82ms
step:1044/1393 train_time:131142ms step_avg:126.83ms
step:1045/1393 train_time:131275ms step_avg:126.84ms
step:1046/1393 train_time:131406ms step_avg:126.84ms
step:1047/1393 train_time:131537ms step_avg:126.84ms
step:1048/1393 train_time:131670ms step_avg:126.85ms
step:1049/1393 train_time:131802ms step_avg:126.85ms
step:1050/1393 train_time:131935ms step_avg:126.86ms
step:1051/1393 train_time:132068ms step_avg:126.87ms
step:1052/1393 train_time:132201ms step_avg:126.87ms
step:1053/1393 train_time:132334ms step_avg:126.88ms
step:1054/1393 train_time:132465ms step_avg:126.88ms
step:1055/1393 train_time:132598ms step_avg:126.89ms
step:1056/1393 train_time:132729ms step_avg:126.89ms
step:1057/1393 train_time:132860ms step_avg:126.90ms
step:1058/1393 train_time:132994ms step_avg:126.90ms
step:1059/1393 train_time:133126ms step_avg:126.91ms
step:1060/1393 train_time:133261ms step_avg:126.92ms
step:1061/1393 train_time:133392ms step_avg:126.92ms
step:1062/1393 train_time:133524ms step_avg:126.92ms
step:1063/1393 train_time:133656ms step_avg:126.93ms
step:1064/1393 train_time:133788ms step_avg:126.93ms
step:1065/1393 train_time:133921ms step_avg:126.94ms
step:1066/1393 train_time:134053ms step_avg:126.94ms
step:1067/1393 train_time:134188ms step_avg:126.95ms
step:1068/1393 train_time:134319ms step_avg:126.96ms
step:1069/1393 train_time:134452ms step_avg:126.96ms
step:1070/1393 train_time:134584ms step_avg:126.97ms
step:1071/1393 train_time:134716ms step_avg:126.97ms
step:1072/1393 train_time:134848ms step_avg:126.98ms
step:1073/1393 train_time:134979ms step_avg:126.98ms
step:1074/1393 train_time:135110ms step_avg:126.98ms
step:1075/1393 train_time:135243ms step_avg:126.99ms
step:1076/1393 train_time:135376ms step_avg:126.99ms
step:1077/1393 train_time:135508ms step_avg:127.00ms
step:1078/1393 train_time:135640ms step_avg:127.00ms
step:1079/1393 train_time:135776ms step_avg:127.01ms
step:1080/1393 train_time:135907ms step_avg:127.02ms
step:1081/1393 train_time:136038ms step_avg:127.02ms
step:1082/1393 train_time:136169ms step_avg:127.02ms
step:1083/1393 train_time:136302ms step_avg:127.03ms
step:1084/1393 train_time:136434ms step_avg:127.03ms
step:1085/1393 train_time:136567ms step_avg:127.04ms
step:1086/1393 train_time:136700ms step_avg:127.04ms
step:1087/1393 train_time:136833ms step_avg:127.05ms
step:1088/1393 train_time:136965ms step_avg:127.05ms
step:1089/1393 train_time:137100ms step_avg:127.06ms
step:1090/1393 train_time:137234ms step_avg:127.07ms
step:1091/1393 train_time:137367ms step_avg:127.07ms
step:1092/1393 train_time:137499ms step_avg:127.08ms
step:1093/1393 train_time:137630ms step_avg:127.08ms
step:1094/1393 train_time:137762ms step_avg:127.09ms
step:1095/1393 train_time:137895ms step_avg:127.09ms
step:1096/1393 train_time:138028ms step_avg:127.10ms
step:1097/1393 train_time:138160ms step_avg:127.10ms
step:1098/1393 train_time:138293ms step_avg:127.11ms
step:1099/1393 train_time:138424ms step_avg:127.11ms
step:1100/1393 train_time:138556ms step_avg:127.12ms
step:1101/1393 train_time:138689ms step_avg:127.12ms
step:1102/1393 train_time:138820ms step_avg:127.12ms
step:1103/1393 train_time:138953ms step_avg:127.13ms
step:1104/1393 train_time:139086ms step_avg:127.13ms
step:1105/1393 train_time:139220ms step_avg:127.14ms
step:1106/1393 train_time:139353ms step_avg:127.15ms
step:1107/1393 train_time:139484ms step_avg:127.15ms
step:1108/1393 train_time:139619ms step_avg:127.16ms
step:1109/1393 train_time:139750ms step_avg:127.16ms
step:1110/1393 train_time:139882ms step_avg:127.17ms
step:1111/1393 train_time:140014ms step_avg:127.17ms
step:1112/1393 train_time:140146ms step_avg:127.17ms
step:1113/1393 train_time:140278ms step_avg:127.18ms
step:1114/1393 train_time:140411ms step_avg:127.18ms
step:1115/1393 train_time:140543ms step_avg:127.19ms
step:1116/1393 train_time:140677ms step_avg:127.19ms
step:1117/1393 train_time:140809ms step_avg:127.20ms
step:1118/1393 train_time:140943ms step_avg:127.21ms
step:1119/1393 train_time:141074ms step_avg:127.21ms
step:1120/1393 train_time:141206ms step_avg:127.21ms
step:1121/1393 train_time:141338ms step_avg:127.22ms
step:1122/1393 train_time:141470ms step_avg:127.22ms
step:1123/1393 train_time:141601ms step_avg:127.22ms
step:1124/1393 train_time:141734ms step_avg:127.23ms
step:1125/1393 train_time:141866ms step_avg:127.23ms
step:1125/1393 val_loss:3.3634 train_time:141998ms step_avg:127.35ms
step:1126/1393 train_time:142020ms step_avg:127.26ms
step:1127/1393 train_time:142138ms step_avg:127.25ms
step:1128/1393 train_time:142271ms step_avg:127.25ms
step:1129/1393 train_time:142404ms step_avg:127.26ms
step:1130/1393 train_time:142534ms step_avg:127.26ms
step:1131/1393 train_time:142667ms step_avg:127.27ms
step:1132/1393 train_time:142798ms step_avg:127.27ms
step:1133/1393 train_time:142929ms step_avg:127.27ms
step:1134/1393 train_time:143064ms step_avg:127.28ms
step:1135/1393 train_time:143199ms step_avg:127.29ms
step:1136/1393 train_time:143335ms step_avg:127.30ms
step:1137/1393 train_time:143466ms step_avg:127.30ms
step:1138/1393 train_time:143600ms step_avg:127.31ms
step:1139/1393 train_time:143733ms step_avg:127.31ms
step:1140/1393 train_time:143867ms step_avg:127.32ms
step:1141/1393 train_time:143999ms step_avg:127.32ms
step:1142/1393 train_time:144133ms step_avg:127.33ms
step:1143/1393 train_time:144268ms step_avg:127.33ms
step:1144/1393 train_time:144402ms step_avg:127.34ms
step:1145/1393 train_time:144535ms step_avg:127.34ms
step:1146/1393 train_time:144669ms step_avg:127.35ms
step:1147/1393 train_time:144803ms step_avg:127.36ms
step:1148/1393 train_time:144937ms step_avg:127.36ms
step:1149/1393 train_time:145071ms step_avg:127.37ms
step:1150/1393 train_time:145205ms step_avg:127.37ms
step:1151/1393 train_time:145341ms step_avg:127.38ms
step:1152/1393 train_time:145473ms step_avg:127.38ms
step:1153/1393 train_time:145609ms step_avg:127.39ms
step:1154/1393 train_time:145743ms step_avg:127.40ms
step:1155/1393 train_time:145875ms step_avg:127.40ms
step:1156/1393 train_time:146012ms step_avg:127.41ms
step:1157/1393 train_time:146147ms step_avg:127.42ms
step:1158/1393 train_time:146280ms step_avg:127.42ms
step:1159/1393 train_time:146414ms step_avg:127.43ms
step:1160/1393 train_time:146548ms step_avg:127.43ms
step:1161/1393 train_time:146681ms step_avg:127.44ms
step:1162/1393 train_time:146816ms step_avg:127.44ms
step:1163/1393 train_time:146949ms step_avg:127.45ms
step:1164/1393 train_time:147084ms step_avg:127.46ms
step:1165/1393 train_time:147217ms step_avg:127.46ms
step:1166/1393 train_time:147350ms step_avg:127.47ms
step:1167/1393 train_time:147484ms step_avg:127.47ms
step:1168/1393 train_time:147618ms step_avg:127.48ms
step:1169/1393 train_time:147752ms step_avg:127.48ms
step:1170/1393 train_time:147885ms step_avg:127.49ms
step:1171/1393 train_time:148019ms step_avg:127.49ms
step:1172/1393 train_time:148152ms step_avg:127.50ms
step:1173/1393 train_time:148285ms step_avg:127.50ms
step:1174/1393 train_time:148423ms step_avg:127.51ms
step:1175/1393 train_time:148556ms step_avg:127.52ms
step:1176/1393 train_time:148691ms step_avg:127.52ms
step:1177/1393 train_time:148828ms step_avg:127.53ms
step:1178/1393 train_time:148960ms step_avg:127.53ms
step:1179/1393 train_time:149094ms step_avg:127.54ms
step:1180/1393 train_time:149231ms step_avg:127.55ms
step:1181/1393 train_time:149366ms step_avg:127.55ms
step:1182/1393 train_time:149500ms step_avg:127.56ms
step:1183/1393 train_time:149633ms step_avg:127.56ms
step:1184/1393 train_time:149767ms step_avg:127.57ms
step:1185/1393 train_time:149901ms step_avg:127.58ms
step:1186/1393 train_time:150035ms step_avg:127.58ms
step:1187/1393 train_time:150173ms step_avg:127.59ms
step:1188/1393 train_time:150306ms step_avg:127.59ms
step:1189/1393 train_time:150441ms step_avg:127.60ms
step:1190/1393 train_time:150575ms step_avg:127.61ms
step:1191/1393 train_time:150709ms step_avg:127.61ms
step:1192/1393 train_time:150842ms step_avg:127.62ms
step:1193/1393 train_time:150976ms step_avg:127.62ms
step:1194/1393 train_time:151109ms step_avg:127.63ms
step:1195/1393 train_time:151243ms step_avg:127.63ms
step:1196/1393 train_time:151377ms step_avg:127.64ms
step:1197/1393 train_time:151510ms step_avg:127.64ms
step:1198/1393 train_time:151647ms step_avg:127.65ms
step:1199/1393 train_time:151780ms step_avg:127.65ms
step:1200/1393 train_time:151913ms step_avg:127.66ms
step:1201/1393 train_time:152046ms step_avg:127.66ms
step:1202/1393 train_time:152183ms step_avg:127.67ms
step:1203/1393 train_time:152320ms step_avg:127.68ms
step:1204/1393 train_time:152454ms step_avg:127.68ms
step:1205/1393 train_time:152590ms step_avg:127.69ms
step:1206/1393 train_time:152725ms step_avg:127.70ms
step:1207/1393 train_time:152859ms step_avg:127.70ms
step:1208/1393 train_time:152993ms step_avg:127.71ms
step:1209/1393 train_time:153128ms step_avg:127.71ms
step:1210/1393 train_time:153263ms step_avg:127.72ms
step:1211/1393 train_time:153397ms step_avg:127.72ms
step:1212/1393 train_time:153530ms step_avg:127.73ms
step:1213/1393 train_time:153664ms step_avg:127.73ms
step:1214/1393 train_time:153799ms step_avg:127.74ms
step:1215/1393 train_time:153934ms step_avg:127.75ms
step:1216/1393 train_time:154066ms step_avg:127.75ms
step:1217/1393 train_time:154199ms step_avg:127.75ms
step:1218/1393 train_time:154331ms step_avg:127.76ms
step:1219/1393 train_time:154464ms step_avg:127.76ms
step:1220/1393 train_time:154598ms step_avg:127.77ms
step:1221/1393 train_time:154731ms step_avg:127.77ms
step:1222/1393 train_time:154865ms step_avg:127.78ms
step:1223/1393 train_time:154998ms step_avg:127.78ms
step:1224/1393 train_time:155132ms step_avg:127.79ms
step:1225/1393 train_time:155268ms step_avg:127.79ms
step:1226/1393 train_time:155401ms step_avg:127.80ms
step:1227/1393 train_time:155535ms step_avg:127.80ms
step:1228/1393 train_time:155670ms step_avg:127.81ms
step:1229/1393 train_time:155802ms step_avg:127.81ms
step:1230/1393 train_time:155938ms step_avg:127.82ms
step:1231/1393 train_time:156073ms step_avg:127.82ms
step:1232/1393 train_time:156207ms step_avg:127.83ms
step:1233/1393 train_time:156341ms step_avg:127.83ms
step:1234/1393 train_time:156474ms step_avg:127.84ms
step:1235/1393 train_time:156608ms step_avg:127.84ms
step:1236/1393 train_time:156740ms step_avg:127.85ms
step:1237/1393 train_time:156875ms step_avg:127.85ms
step:1238/1393 train_time:157010ms step_avg:127.86ms
step:1239/1393 train_time:157145ms step_avg:127.86ms
step:1240/1393 train_time:157280ms step_avg:127.87ms
step:1241/1393 train_time:157415ms step_avg:127.88ms
step:1242/1393 train_time:157548ms step_avg:127.88ms
step:1243/1393 train_time:157684ms step_avg:127.89ms
step:1244/1393 train_time:157817ms step_avg:127.89ms
step:1245/1393 train_time:157950ms step_avg:127.89ms
step:1246/1393 train_time:158083ms step_avg:127.90ms
step:1247/1393 train_time:158217ms step_avg:127.90ms
step:1248/1393 train_time:158351ms step_avg:127.91ms
step:1249/1393 train_time:158484ms step_avg:127.91ms
step:1250/1393 train_time:158617ms step_avg:127.92ms
step:1250/1393 val_loss:3.3169 train_time:158750ms step_avg:128.02ms
step:1251/1393 train_time:158771ms step_avg:127.94ms
step:1252/1393 train_time:158893ms step_avg:127.93ms
step:1253/1393 train_time:159027ms step_avg:127.94ms
step:1254/1393 train_time:159159ms step_avg:127.94ms
step:1255/1393 train_time:159297ms step_avg:127.95ms
step:1256/1393 train_time:159430ms step_avg:127.95ms
step:1257/1393 train_time:159563ms step_avg:127.96ms
step:1258/1393 train_time:159696ms step_avg:127.96ms
step:1259/1393 train_time:159832ms step_avg:127.97ms
step:1260/1393 train_time:159965ms step_avg:127.97ms
step:1261/1393 train_time:160098ms step_avg:127.98ms
step:1262/1393 train_time:160234ms step_avg:127.98ms
step:1263/1393 train_time:160369ms step_avg:127.99ms
step:1264/1393 train_time:160503ms step_avg:127.99ms
step:1265/1393 train_time:160636ms step_avg:128.00ms
step:1266/1393 train_time:160770ms step_avg:128.00ms
step:1267/1393 train_time:160905ms step_avg:128.01ms
step:1268/1393 train_time:161040ms step_avg:128.01ms
step:1269/1393 train_time:161175ms step_avg:128.02ms
step:1270/1393 train_time:161310ms step_avg:128.02ms
step:1271/1393 train_time:161445ms step_avg:128.03ms
step:1272/1393 train_time:161577ms step_avg:128.03ms
step:1273/1393 train_time:161710ms step_avg:128.04ms
step:1274/1393 train_time:161843ms step_avg:128.04ms
step:1275/1393 train_time:161979ms step_avg:128.05ms
step:1276/1393 train_time:162112ms step_avg:128.05ms
step:1277/1393 train_time:162245ms step_avg:128.05ms
step:1278/1393 train_time:162379ms step_avg:128.06ms
step:1279/1393 train_time:162512ms step_avg:128.06ms
step:1280/1393 train_time:162649ms step_avg:128.07ms
step:1281/1393 train_time:162782ms step_avg:128.07ms
step:1282/1393 train_time:162915ms step_avg:128.08ms
step:1283/1393 train_time:163049ms step_avg:128.08ms
step:1284/1393 train_time:163183ms step_avg:128.09ms
step:1285/1393 train_time:163316ms step_avg:128.09ms
step:1286/1393 train_time:163450ms step_avg:128.10ms
step:1287/1393 train_time:163585ms step_avg:128.10ms
step:1288/1393 train_time:163717ms step_avg:128.10ms
step:1289/1393 train_time:163853ms step_avg:128.11ms
step:1290/1393 train_time:163989ms step_avg:128.12ms
step:1291/1393 train_time:164124ms step_avg:128.12ms
step:1292/1393 train_time:164258ms step_avg:128.13ms
step:1293/1393 train_time:164393ms step_avg:128.13ms
step:1294/1393 train_time:164527ms step_avg:128.14ms
step:1295/1393 train_time:164661ms step_avg:128.14ms
step:1296/1393 train_time:164796ms step_avg:128.15ms
step:1297/1393 train_time:164930ms step_avg:128.15ms
step:1298/1393 train_time:165064ms step_avg:128.15ms
step:1299/1393 train_time:165197ms step_avg:128.16ms
step:1300/1393 train_time:165331ms step_avg:128.16ms
step:1301/1393 train_time:165465ms step_avg:128.17ms
step:1302/1393 train_time:165599ms step_avg:128.17ms
step:1303/1393 train_time:165735ms step_avg:128.18ms
step:1304/1393 train_time:165870ms step_avg:128.18ms
step:1305/1393 train_time:166004ms step_avg:128.19ms
step:1306/1393 train_time:166138ms step_avg:128.19ms
step:1307/1393 train_time:166272ms step_avg:128.20ms
step:1308/1393 train_time:166406ms step_avg:128.20ms
step:1309/1393 train_time:166541ms step_avg:128.21ms
step:1310/1393 train_time:166674ms step_avg:128.21ms
step:1311/1393 train_time:166807ms step_avg:128.21ms
step:1312/1393 train_time:166940ms step_avg:128.22ms
step:1313/1393 train_time:167074ms step_avg:128.22ms
step:1314/1393 train_time:167208ms step_avg:128.23ms
step:1315/1393 train_time:167342ms step_avg:128.23ms
step:1316/1393 train_time:167475ms step_avg:128.23ms
step:1317/1393 train_time:167608ms step_avg:128.24ms
step:1318/1393 train_time:167743ms step_avg:128.24ms
step:1319/1393 train_time:167879ms step_avg:128.25ms
step:1320/1393 train_time:168012ms step_avg:128.25ms
step:1321/1393 train_time:168146ms step_avg:128.26ms
step:1322/1393 train_time:168283ms step_avg:128.26ms
step:1323/1393 train_time:168417ms step_avg:128.27ms
step:1324/1393 train_time:168550ms step_avg:128.27ms
step:1325/1393 train_time:168686ms step_avg:128.28ms
step:1326/1393 train_time:168820ms step_avg:128.28ms
step:1327/1393 train_time:168954ms step_avg:128.29ms
step:1328/1393 train_time:169086ms step_avg:128.29ms
step:1329/1393 train_time:169224ms step_avg:128.30ms
step:1330/1393 train_time:169359ms step_avg:128.30ms
step:1331/1393 train_time:169496ms step_avg:128.31ms
step:1332/1393 train_time:169633ms step_avg:128.32ms
step:1333/1393 train_time:169767ms step_avg:128.32ms
step:1334/1393 train_time:169901ms step_avg:128.32ms
step:1335/1393 train_time:170033ms step_avg:128.33ms
step:1336/1393 train_time:170171ms step_avg:128.33ms
step:1337/1393 train_time:170305ms step_avg:128.34ms
step:1338/1393 train_time:170439ms step_avg:128.34ms
step:1339/1393 train_time:170573ms step_avg:128.35ms
step:1340/1393 train_time:170710ms step_avg:128.35ms
step:1341/1393 train_time:170842ms step_avg:128.36ms
step:1342/1393 train_time:170977ms step_avg:128.36ms
step:1343/1393 train_time:171110ms step_avg:128.36ms
step:1344/1393 train_time:171246ms step_avg:128.37ms
step:1345/1393 train_time:171381ms step_avg:128.38ms
step:1346/1393 train_time:171515ms step_avg:128.38ms
step:1347/1393 train_time:171651ms step_avg:128.39ms
step:1348/1393 train_time:171785ms step_avg:128.39ms
step:1349/1393 train_time:171920ms step_avg:128.39ms
step:1350/1393 train_time:172054ms step_avg:128.40ms
step:1351/1393 train_time:172188ms step_avg:128.40ms
step:1352/1393 train_time:172327ms step_avg:128.41ms
step:1353/1393 train_time:172463ms step_avg:128.42ms
step:1354/1393 train_time:172598ms step_avg:128.42ms
step:1355/1393 train_time:172733ms step_avg:128.43ms
step:1356/1393 train_time:172865ms step_avg:128.43ms
step:1357/1393 train_time:173000ms step_avg:128.43ms
step:1358/1393 train_time:173137ms step_avg:128.44ms
step:1359/1393 train_time:173273ms step_avg:128.45ms
step:1360/1393 train_time:173409ms step_avg:128.45ms
step:1361/1393 train_time:173544ms step_avg:128.46ms
step:1362/1393 train_time:173682ms step_avg:128.46ms
step:1363/1393 train_time:173820ms step_avg:128.47ms
step:1364/1393 train_time:173955ms step_avg:128.48ms
step:1365/1393 train_time:174088ms step_avg:128.48ms
step:1366/1393 train_time:174224ms step_avg:128.48ms
step:1367/1393 train_time:174359ms step_avg:128.49ms
step:1368/1393 train_time:174495ms step_avg:128.49ms
step:1369/1393 train_time:174633ms step_avg:128.50ms
step:1370/1393 train_time:174770ms step_avg:128.51ms
step:1371/1393 train_time:174906ms step_avg:128.51ms
step:1372/1393 train_time:175043ms step_avg:128.52ms
step:1373/1393 train_time:175177ms step_avg:128.52ms
step:1374/1393 train_time:175313ms step_avg:128.53ms
step:1375/1393 train_time:175447ms step_avg:128.53ms
step:1375/1393 val_loss:3.2830 train_time:175581ms step_avg:128.63ms
step:1376/1393 train_time:175602ms step_avg:128.55ms
step:1377/1393 train_time:175726ms step_avg:128.55ms
step:1378/1393 train_time:175859ms step_avg:128.55ms
step:1379/1393 train_time:175994ms step_avg:128.56ms
step:1380/1393 train_time:176129ms step_avg:128.56ms
step:1381/1393 train_time:176265ms step_avg:128.57ms
step:1382/1393 train_time:176400ms step_avg:128.57ms
step:1383/1393 train_time:176534ms step_avg:128.58ms
step:1384/1393 train_time:176672ms step_avg:128.58ms
step:1385/1393 train_time:176806ms step_avg:128.59ms
step:1386/1393 train_time:176940ms step_avg:128.59ms
step:1387/1393 train_time:177077ms step_avg:128.60ms
step:1388/1393 train_time:177211ms step_avg:128.60ms
step:1389/1393 train_time:177346ms step_avg:128.60ms
step:1390/1393 train_time:177482ms step_avg:128.61ms
step:1391/1393 train_time:177616ms step_avg:128.61ms
step:1392/1393 train_time:177751ms step_avg:128.62ms
step:1393/1393 train_time:177885ms step_avg:128.62ms
step:1393/1393 val_loss:3.2795 train_time:178019ms step_avg:128.72ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
