import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 07:07:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:23819ms step_avg:nanms
step:2/1393 train_time:24228ms step_avg:nanms
step:3/1393 train_time:24368ms step_avg:nanms
step:4/1393 train_time:24487ms step_avg:nanms
step:5/1393 train_time:24608ms step_avg:nanms
step:6/1393 train_time:24728ms step_avg:nanms
step:7/1393 train_time:24848ms step_avg:nanms
step:8/1393 train_time:24969ms step_avg:nanms
step:9/1393 train_time:25090ms step_avg:nanms
step:10/1393 train_time:25212ms step_avg:nanms
step:11/1393 train_time:124ms step_avg:nanms
step:12/1393 train_time:245ms step_avg:nanms
step:13/1393 train_time:367ms step_avg:122.27ms
step:14/1393 train_time:489ms step_avg:122.20ms
step:15/1393 train_time:609ms step_avg:121.86ms
step:16/1393 train_time:732ms step_avg:121.95ms
step:17/1393 train_time:852ms step_avg:121.74ms
step:18/1393 train_time:974ms step_avg:121.74ms
step:19/1393 train_time:1096ms step_avg:121.76ms
step:20/1393 train_time:1218ms step_avg:121.83ms
step:21/1393 train_time:1340ms step_avg:121.81ms
step:22/1393 train_time:1461ms step_avg:121.77ms
step:23/1393 train_time:1582ms step_avg:121.72ms
step:24/1393 train_time:1705ms step_avg:121.75ms
step:25/1393 train_time:1827ms step_avg:121.80ms
step:26/1393 train_time:1950ms step_avg:121.86ms
step:27/1393 train_time:2072ms step_avg:121.88ms
step:28/1393 train_time:2195ms step_avg:121.96ms
step:29/1393 train_time:2316ms step_avg:121.91ms
step:30/1393 train_time:2438ms step_avg:121.88ms
step:31/1393 train_time:2559ms step_avg:121.85ms
step:32/1393 train_time:2681ms step_avg:121.85ms
step:33/1393 train_time:2802ms step_avg:121.84ms
step:34/1393 train_time:2925ms step_avg:121.86ms
step:35/1393 train_time:3045ms step_avg:121.81ms
step:36/1393 train_time:3167ms step_avg:121.80ms
step:37/1393 train_time:3290ms step_avg:121.86ms
step:38/1393 train_time:3412ms step_avg:121.85ms
step:39/1393 train_time:3533ms step_avg:121.83ms
step:40/1393 train_time:3655ms step_avg:121.82ms
step:41/1393 train_time:3776ms step_avg:121.82ms
step:42/1393 train_time:3898ms step_avg:121.80ms
step:43/1393 train_time:4019ms step_avg:121.79ms
step:44/1393 train_time:4141ms step_avg:121.78ms
step:45/1393 train_time:4262ms step_avg:121.78ms
step:46/1393 train_time:4385ms step_avg:121.80ms
step:47/1393 train_time:4506ms step_avg:121.79ms
step:48/1393 train_time:4628ms step_avg:121.78ms
step:49/1393 train_time:4749ms step_avg:121.78ms
step:50/1393 train_time:4871ms step_avg:121.78ms
step:51/1393 train_time:4993ms step_avg:121.78ms
step:52/1393 train_time:5114ms step_avg:121.77ms
step:53/1393 train_time:5235ms step_avg:121.75ms
step:54/1393 train_time:5356ms step_avg:121.74ms
step:55/1393 train_time:5478ms step_avg:121.74ms
step:56/1393 train_time:5599ms step_avg:121.72ms
step:57/1393 train_time:5720ms step_avg:121.70ms
step:58/1393 train_time:5841ms step_avg:121.69ms
step:59/1393 train_time:5962ms step_avg:121.67ms
step:60/1393 train_time:6085ms step_avg:121.69ms
step:61/1393 train_time:6205ms step_avg:121.67ms
step:62/1393 train_time:6326ms step_avg:121.65ms
step:63/1393 train_time:6447ms step_avg:121.64ms
step:64/1393 train_time:6569ms step_avg:121.64ms
step:65/1393 train_time:6689ms step_avg:121.63ms
step:66/1393 train_time:6811ms step_avg:121.63ms
step:67/1393 train_time:6932ms step_avg:121.62ms
step:68/1393 train_time:7055ms step_avg:121.64ms
step:69/1393 train_time:7177ms step_avg:121.64ms
step:70/1393 train_time:7298ms step_avg:121.64ms
step:71/1393 train_time:7420ms step_avg:121.64ms
step:72/1393 train_time:7541ms step_avg:121.63ms
step:73/1393 train_time:7664ms step_avg:121.65ms
step:74/1393 train_time:7785ms step_avg:121.65ms
step:75/1393 train_time:7907ms step_avg:121.65ms
step:76/1393 train_time:8028ms step_avg:121.63ms
step:77/1393 train_time:8150ms step_avg:121.64ms
step:78/1393 train_time:8272ms step_avg:121.65ms
step:79/1393 train_time:8394ms step_avg:121.65ms
step:80/1393 train_time:8514ms step_avg:121.63ms
step:81/1393 train_time:8635ms step_avg:121.63ms
step:82/1393 train_time:8757ms step_avg:121.63ms
step:83/1393 train_time:8880ms step_avg:121.64ms
step:84/1393 train_time:9001ms step_avg:121.63ms
step:85/1393 train_time:9123ms step_avg:121.64ms
step:86/1393 train_time:9244ms step_avg:121.63ms
step:87/1393 train_time:9366ms step_avg:121.64ms
step:88/1393 train_time:9488ms step_avg:121.64ms
step:89/1393 train_time:9609ms step_avg:121.64ms
step:90/1393 train_time:9731ms step_avg:121.63ms
step:91/1393 train_time:9852ms step_avg:121.63ms
step:92/1393 train_time:9973ms step_avg:121.62ms
step:93/1393 train_time:10094ms step_avg:121.62ms
step:94/1393 train_time:10215ms step_avg:121.61ms
step:95/1393 train_time:10337ms step_avg:121.61ms
step:96/1393 train_time:10458ms step_avg:121.60ms
step:97/1393 train_time:10579ms step_avg:121.60ms
step:98/1393 train_time:10700ms step_avg:121.59ms
step:99/1393 train_time:10821ms step_avg:121.58ms
step:100/1393 train_time:10944ms step_avg:121.59ms
step:101/1393 train_time:11065ms step_avg:121.59ms
step:102/1393 train_time:11188ms step_avg:121.61ms
step:103/1393 train_time:11309ms step_avg:121.60ms
step:104/1393 train_time:11430ms step_avg:121.60ms
step:105/1393 train_time:11551ms step_avg:121.59ms
step:106/1393 train_time:11673ms step_avg:121.59ms
step:107/1393 train_time:11795ms step_avg:121.60ms
step:108/1393 train_time:11917ms step_avg:121.60ms
step:109/1393 train_time:12038ms step_avg:121.60ms
step:110/1393 train_time:12161ms step_avg:121.61ms
step:111/1393 train_time:12282ms step_avg:121.61ms
step:112/1393 train_time:12403ms step_avg:121.60ms
step:113/1393 train_time:12526ms step_avg:121.61ms
step:114/1393 train_time:12647ms step_avg:121.61ms
step:115/1393 train_time:12770ms step_avg:121.62ms
step:116/1393 train_time:12891ms step_avg:121.61ms
step:117/1393 train_time:13014ms step_avg:121.63ms
step:118/1393 train_time:13136ms step_avg:121.63ms
step:119/1393 train_time:13259ms step_avg:121.64ms
step:120/1393 train_time:13380ms step_avg:121.64ms
step:121/1393 train_time:13502ms step_avg:121.64ms
step:122/1393 train_time:13624ms step_avg:121.64ms
step:123/1393 train_time:13746ms step_avg:121.65ms
step:124/1393 train_time:13870ms step_avg:121.67ms
step:125/1393 train_time:13992ms step_avg:121.67ms
step:125/1393 val_loss:4.4093 train_time:14112ms step_avg:122.71ms
step:126/1393 train_time:14136ms step_avg:121.86ms
step:127/1393 train_time:14237ms step_avg:121.69ms
step:128/1393 train_time:14365ms step_avg:121.74ms
step:129/1393 train_time:14490ms step_avg:121.76ms
step:130/1393 train_time:14612ms step_avg:121.76ms
step:131/1393 train_time:14733ms step_avg:121.76ms
step:132/1393 train_time:14854ms step_avg:121.75ms
step:133/1393 train_time:14975ms step_avg:121.75ms
step:134/1393 train_time:15096ms step_avg:121.75ms
step:135/1393 train_time:15218ms step_avg:121.75ms
step:136/1393 train_time:15341ms step_avg:121.76ms
step:137/1393 train_time:15465ms step_avg:121.77ms
step:138/1393 train_time:15587ms step_avg:121.77ms
step:139/1393 train_time:15709ms step_avg:121.77ms
step:140/1393 train_time:15831ms step_avg:121.78ms
step:141/1393 train_time:15952ms step_avg:121.77ms
step:142/1393 train_time:16073ms step_avg:121.77ms
step:143/1393 train_time:16196ms step_avg:121.77ms
step:144/1393 train_time:16318ms step_avg:121.78ms
step:145/1393 train_time:16441ms step_avg:121.78ms
step:146/1393 train_time:16563ms step_avg:121.79ms
step:147/1393 train_time:16685ms step_avg:121.79ms
step:148/1393 train_time:16808ms step_avg:121.80ms
step:149/1393 train_time:16930ms step_avg:121.80ms
step:150/1393 train_time:17052ms step_avg:121.80ms
step:151/1393 train_time:17174ms step_avg:121.80ms
step:152/1393 train_time:17296ms step_avg:121.80ms
step:153/1393 train_time:17418ms step_avg:121.80ms
step:154/1393 train_time:17540ms step_avg:121.81ms
step:155/1393 train_time:17663ms step_avg:121.82ms
step:156/1393 train_time:17785ms step_avg:121.82ms
step:157/1393 train_time:17907ms step_avg:121.82ms
step:158/1393 train_time:18029ms step_avg:121.82ms
step:159/1393 train_time:18150ms step_avg:121.81ms
step:160/1393 train_time:18273ms step_avg:121.82ms
step:161/1393 train_time:18394ms step_avg:121.82ms
step:162/1393 train_time:18516ms step_avg:121.82ms
step:163/1393 train_time:18639ms step_avg:121.82ms
step:164/1393 train_time:18761ms step_avg:121.82ms
step:165/1393 train_time:18883ms step_avg:121.83ms
step:166/1393 train_time:19006ms step_avg:121.83ms
step:167/1393 train_time:19127ms step_avg:121.83ms
step:168/1393 train_time:19249ms step_avg:121.83ms
step:169/1393 train_time:19371ms step_avg:121.83ms
step:170/1393 train_time:19494ms step_avg:121.84ms
step:171/1393 train_time:19616ms step_avg:121.84ms
step:172/1393 train_time:19739ms step_avg:121.84ms
step:173/1393 train_time:19861ms step_avg:121.85ms
step:174/1393 train_time:19983ms step_avg:121.85ms
step:175/1393 train_time:20105ms step_avg:121.85ms
step:176/1393 train_time:20228ms step_avg:121.85ms
step:177/1393 train_time:20349ms step_avg:121.85ms
step:178/1393 train_time:20471ms step_avg:121.85ms
step:179/1393 train_time:20592ms step_avg:121.85ms
step:180/1393 train_time:20715ms step_avg:121.85ms
step:181/1393 train_time:20836ms step_avg:121.85ms
step:182/1393 train_time:20959ms step_avg:121.86ms
step:183/1393 train_time:21081ms step_avg:121.86ms
step:184/1393 train_time:21204ms step_avg:121.86ms
step:185/1393 train_time:21326ms step_avg:121.86ms
step:186/1393 train_time:21447ms step_avg:121.86ms
step:187/1393 train_time:21568ms step_avg:121.86ms
step:188/1393 train_time:21690ms step_avg:121.85ms
step:189/1393 train_time:21812ms step_avg:121.85ms
step:190/1393 train_time:21933ms step_avg:121.85ms
step:191/1393 train_time:22056ms step_avg:121.86ms
step:192/1393 train_time:22178ms step_avg:121.86ms
step:193/1393 train_time:22300ms step_avg:121.86ms
step:194/1393 train_time:22422ms step_avg:121.86ms
step:195/1393 train_time:22543ms step_avg:121.86ms
step:196/1393 train_time:22666ms step_avg:121.86ms
step:197/1393 train_time:22789ms step_avg:121.87ms
step:198/1393 train_time:22911ms step_avg:121.87ms
step:199/1393 train_time:23033ms step_avg:121.87ms
step:200/1393 train_time:23155ms step_avg:121.87ms
step:201/1393 train_time:23278ms step_avg:121.87ms
step:202/1393 train_time:23399ms step_avg:121.87ms
step:203/1393 train_time:23520ms step_avg:121.87ms
step:204/1393 train_time:23644ms step_avg:121.87ms
step:205/1393 train_time:23766ms step_avg:121.88ms
step:206/1393 train_time:23887ms step_avg:121.87ms
step:207/1393 train_time:24010ms step_avg:121.88ms
step:208/1393 train_time:24132ms step_avg:121.88ms
step:209/1393 train_time:24253ms step_avg:121.88ms
step:210/1393 train_time:24376ms step_avg:121.88ms
step:211/1393 train_time:24498ms step_avg:121.88ms
step:212/1393 train_time:24621ms step_avg:121.88ms
step:213/1393 train_time:24743ms step_avg:121.89ms
step:214/1393 train_time:24866ms step_avg:121.89ms
step:215/1393 train_time:24990ms step_avg:121.90ms
step:216/1393 train_time:25113ms step_avg:121.91ms
step:217/1393 train_time:25235ms step_avg:121.91ms
step:218/1393 train_time:25357ms step_avg:121.91ms
step:219/1393 train_time:25481ms step_avg:121.92ms
step:220/1393 train_time:25604ms step_avg:121.92ms
step:221/1393 train_time:25727ms step_avg:121.93ms
step:222/1393 train_time:25850ms step_avg:121.94ms
step:223/1393 train_time:25974ms step_avg:121.94ms
step:224/1393 train_time:26096ms step_avg:121.94ms
step:225/1393 train_time:26218ms step_avg:121.94ms
step:226/1393 train_time:26340ms step_avg:121.94ms
step:227/1393 train_time:26462ms step_avg:121.95ms
step:228/1393 train_time:26585ms step_avg:121.95ms
step:229/1393 train_time:26708ms step_avg:121.95ms
step:230/1393 train_time:26831ms step_avg:121.96ms
step:231/1393 train_time:26955ms step_avg:121.97ms
step:232/1393 train_time:27077ms step_avg:121.97ms
step:233/1393 train_time:27200ms step_avg:121.97ms
step:234/1393 train_time:27323ms step_avg:121.98ms
step:235/1393 train_time:27445ms step_avg:121.98ms
step:236/1393 train_time:27567ms step_avg:121.98ms
step:237/1393 train_time:27690ms step_avg:121.98ms
step:238/1393 train_time:27813ms step_avg:121.99ms
step:239/1393 train_time:27937ms step_avg:121.99ms
step:240/1393 train_time:28060ms step_avg:122.00ms
step:241/1393 train_time:28182ms step_avg:122.00ms
step:242/1393 train_time:28306ms step_avg:122.01ms
step:243/1393 train_time:28428ms step_avg:122.01ms
step:244/1393 train_time:28551ms step_avg:122.01ms
step:245/1393 train_time:28674ms step_avg:122.02ms
step:246/1393 train_time:28796ms step_avg:122.02ms
step:247/1393 train_time:28919ms step_avg:122.02ms
step:248/1393 train_time:29042ms step_avg:122.02ms
step:249/1393 train_time:29165ms step_avg:122.03ms
step:250/1393 train_time:29288ms step_avg:122.03ms
step:250/1393 val_loss:3.9803 train_time:29409ms step_avg:122.54ms
step:251/1393 train_time:29431ms step_avg:122.12ms
step:252/1393 train_time:29544ms step_avg:122.08ms
step:253/1393 train_time:29668ms step_avg:122.09ms
step:254/1393 train_time:29789ms step_avg:122.09ms
step:255/1393 train_time:29911ms step_avg:122.09ms
step:256/1393 train_time:30032ms step_avg:122.08ms
step:257/1393 train_time:30154ms step_avg:122.08ms
step:258/1393 train_time:30276ms step_avg:122.08ms
step:259/1393 train_time:30399ms step_avg:122.08ms
step:260/1393 train_time:30525ms step_avg:122.10ms
step:261/1393 train_time:30649ms step_avg:122.11ms
step:262/1393 train_time:30772ms step_avg:122.11ms
step:263/1393 train_time:30894ms step_avg:122.11ms
step:264/1393 train_time:31015ms step_avg:122.11ms
step:265/1393 train_time:31137ms step_avg:122.11ms
step:266/1393 train_time:31258ms step_avg:122.10ms
step:267/1393 train_time:31381ms step_avg:122.10ms
step:268/1393 train_time:31506ms step_avg:122.12ms
step:269/1393 train_time:31630ms step_avg:122.12ms
step:270/1393 train_time:31752ms step_avg:122.12ms
step:271/1393 train_time:31875ms step_avg:122.13ms
step:272/1393 train_time:31996ms step_avg:122.12ms
step:273/1393 train_time:32118ms step_avg:122.12ms
step:274/1393 train_time:32240ms step_avg:122.12ms
step:275/1393 train_time:32361ms step_avg:122.12ms
step:276/1393 train_time:32485ms step_avg:122.12ms
step:277/1393 train_time:32608ms step_avg:122.13ms
step:278/1393 train_time:32731ms step_avg:122.13ms
step:279/1393 train_time:32853ms step_avg:122.13ms
step:280/1393 train_time:32976ms step_avg:122.13ms
step:281/1393 train_time:33098ms step_avg:122.13ms
step:282/1393 train_time:33221ms step_avg:122.14ms
step:283/1393 train_time:33343ms step_avg:122.14ms
step:284/1393 train_time:33466ms step_avg:122.14ms
step:285/1393 train_time:33588ms step_avg:122.14ms
step:286/1393 train_time:33712ms step_avg:122.14ms
step:287/1393 train_time:33834ms step_avg:122.15ms
step:288/1393 train_time:33958ms step_avg:122.15ms
step:289/1393 train_time:34081ms step_avg:122.15ms
step:290/1393 train_time:34203ms step_avg:122.15ms
step:291/1393 train_time:34325ms step_avg:122.15ms
step:292/1393 train_time:34447ms step_avg:122.15ms
step:293/1393 train_time:34570ms step_avg:122.16ms
step:294/1393 train_time:34692ms step_avg:122.16ms
step:295/1393 train_time:34815ms step_avg:122.16ms
step:296/1393 train_time:34939ms step_avg:122.16ms
step:297/1393 train_time:35061ms step_avg:122.16ms
step:298/1393 train_time:35183ms step_avg:122.16ms
step:299/1393 train_time:35305ms step_avg:122.16ms
step:300/1393 train_time:35429ms step_avg:122.17ms
step:301/1393 train_time:35552ms step_avg:122.17ms
step:302/1393 train_time:35674ms step_avg:122.17ms
step:303/1393 train_time:35797ms step_avg:122.17ms
step:304/1393 train_time:35919ms step_avg:122.17ms
step:305/1393 train_time:36041ms step_avg:122.17ms
step:306/1393 train_time:36165ms step_avg:122.18ms
step:307/1393 train_time:36288ms step_avg:122.18ms
step:308/1393 train_time:36410ms step_avg:122.18ms
step:309/1393 train_time:36534ms step_avg:122.19ms
step:310/1393 train_time:36656ms step_avg:122.19ms
step:311/1393 train_time:36779ms step_avg:122.19ms
step:312/1393 train_time:36904ms step_avg:122.20ms
step:313/1393 train_time:37029ms step_avg:122.21ms
step:314/1393 train_time:37153ms step_avg:122.22ms
step:315/1393 train_time:37280ms step_avg:122.23ms
step:316/1393 train_time:37404ms step_avg:122.24ms
step:317/1393 train_time:37530ms step_avg:122.25ms
step:318/1393 train_time:37654ms step_avg:122.25ms
step:319/1393 train_time:37779ms step_avg:122.26ms
step:320/1393 train_time:37904ms step_avg:122.27ms
step:321/1393 train_time:38029ms step_avg:122.28ms
step:322/1393 train_time:38154ms step_avg:122.29ms
step:323/1393 train_time:38279ms step_avg:122.30ms
step:324/1393 train_time:38404ms step_avg:122.31ms
step:325/1393 train_time:38530ms step_avg:122.32ms
step:326/1393 train_time:38655ms step_avg:122.33ms
step:327/1393 train_time:38780ms step_avg:122.33ms
step:328/1393 train_time:38906ms step_avg:122.35ms
step:329/1393 train_time:39031ms step_avg:122.35ms
step:330/1393 train_time:39156ms step_avg:122.36ms
step:331/1393 train_time:39281ms step_avg:122.37ms
step:332/1393 train_time:39407ms step_avg:122.38ms
step:333/1393 train_time:39532ms step_avg:122.39ms
step:334/1393 train_time:39657ms step_avg:122.40ms
step:335/1393 train_time:39782ms step_avg:122.41ms
step:336/1393 train_time:39908ms step_avg:122.42ms
step:337/1393 train_time:40033ms step_avg:122.43ms
step:338/1393 train_time:40158ms step_avg:122.43ms
step:339/1393 train_time:40283ms step_avg:122.44ms
step:340/1393 train_time:40408ms step_avg:122.45ms
step:341/1393 train_time:40534ms step_avg:122.46ms
step:342/1393 train_time:40659ms step_avg:122.47ms
step:343/1393 train_time:40784ms step_avg:122.47ms
step:344/1393 train_time:40910ms step_avg:122.48ms
step:345/1393 train_time:41034ms step_avg:122.49ms
step:346/1393 train_time:41159ms step_avg:122.50ms
step:347/1393 train_time:41285ms step_avg:122.51ms
step:348/1393 train_time:41411ms step_avg:122.52ms
step:349/1393 train_time:41536ms step_avg:122.53ms
step:350/1393 train_time:41661ms step_avg:122.53ms
step:351/1393 train_time:41787ms step_avg:122.54ms
step:352/1393 train_time:41912ms step_avg:122.55ms
step:353/1393 train_time:42036ms step_avg:122.55ms
step:354/1393 train_time:42161ms step_avg:122.56ms
step:355/1393 train_time:42287ms step_avg:122.57ms
step:356/1393 train_time:42412ms step_avg:122.58ms
step:357/1393 train_time:42538ms step_avg:122.59ms
step:358/1393 train_time:42663ms step_avg:122.59ms
step:359/1393 train_time:42789ms step_avg:122.60ms
step:360/1393 train_time:42914ms step_avg:122.61ms
step:361/1393 train_time:43039ms step_avg:122.62ms
step:362/1393 train_time:43165ms step_avg:122.63ms
step:363/1393 train_time:43291ms step_avg:122.64ms
step:364/1393 train_time:43416ms step_avg:122.64ms
step:365/1393 train_time:43541ms step_avg:122.65ms
step:366/1393 train_time:43667ms step_avg:122.66ms
step:367/1393 train_time:43792ms step_avg:122.67ms
step:368/1393 train_time:43917ms step_avg:122.67ms
step:369/1393 train_time:44042ms step_avg:122.68ms
step:370/1393 train_time:44168ms step_avg:122.69ms
step:371/1393 train_time:44294ms step_avg:122.70ms
step:372/1393 train_time:44421ms step_avg:122.71ms
step:373/1393 train_time:44547ms step_avg:122.72ms
step:374/1393 train_time:44672ms step_avg:122.73ms
step:375/1393 train_time:44797ms step_avg:122.73ms
step:375/1393 val_loss:3.7800 train_time:44920ms step_avg:123.07ms
step:376/1393 train_time:44944ms step_avg:122.80ms
step:377/1393 train_time:45058ms step_avg:122.77ms
step:378/1393 train_time:45183ms step_avg:122.78ms
step:379/1393 train_time:45309ms step_avg:122.79ms
step:380/1393 train_time:45433ms step_avg:122.79ms
step:381/1393 train_time:45558ms step_avg:122.80ms
step:382/1393 train_time:45682ms step_avg:122.80ms
step:383/1393 train_time:45807ms step_avg:122.81ms
step:384/1393 train_time:45933ms step_avg:122.81ms
step:385/1393 train_time:46059ms step_avg:122.82ms
step:386/1393 train_time:46186ms step_avg:122.84ms
step:387/1393 train_time:46311ms step_avg:122.84ms
step:388/1393 train_time:46436ms step_avg:122.85ms
step:389/1393 train_time:46561ms step_avg:122.85ms
step:390/1393 train_time:46686ms step_avg:122.86ms
step:391/1393 train_time:46811ms step_avg:122.86ms
step:392/1393 train_time:46936ms step_avg:122.87ms
step:393/1393 train_time:47063ms step_avg:122.88ms
step:394/1393 train_time:47189ms step_avg:122.89ms
step:395/1393 train_time:47315ms step_avg:122.90ms
step:396/1393 train_time:47440ms step_avg:122.90ms
step:397/1393 train_time:47564ms step_avg:122.91ms
step:398/1393 train_time:47689ms step_avg:122.91ms
step:399/1393 train_time:47813ms step_avg:122.91ms
step:400/1393 train_time:47938ms step_avg:122.92ms
step:401/1393 train_time:48064ms step_avg:122.92ms
step:402/1393 train_time:48189ms step_avg:122.93ms
step:403/1393 train_time:48315ms step_avg:122.94ms
step:404/1393 train_time:48440ms step_avg:122.94ms
step:405/1393 train_time:48565ms step_avg:122.95ms
step:406/1393 train_time:48690ms step_avg:122.96ms
step:407/1393 train_time:48815ms step_avg:122.96ms
step:408/1393 train_time:48941ms step_avg:122.97ms
step:409/1393 train_time:49066ms step_avg:122.97ms
step:410/1393 train_time:49192ms step_avg:122.98ms
step:411/1393 train_time:49316ms step_avg:122.98ms
step:412/1393 train_time:49442ms step_avg:122.99ms
step:413/1393 train_time:49567ms step_avg:123.00ms
step:414/1393 train_time:49693ms step_avg:123.00ms
step:415/1393 train_time:49819ms step_avg:123.01ms
step:416/1393 train_time:49945ms step_avg:123.02ms
step:417/1393 train_time:50071ms step_avg:123.02ms
step:418/1393 train_time:50196ms step_avg:123.03ms
step:419/1393 train_time:50323ms step_avg:123.04ms
step:420/1393 train_time:50449ms step_avg:123.05ms
step:421/1393 train_time:50575ms step_avg:123.05ms
step:422/1393 train_time:50699ms step_avg:123.06ms
step:423/1393 train_time:50825ms step_avg:123.06ms
step:424/1393 train_time:50951ms step_avg:123.07ms
step:425/1393 train_time:51076ms step_avg:123.08ms
step:426/1393 train_time:51203ms step_avg:123.08ms
step:427/1393 train_time:51328ms step_avg:123.09ms
step:428/1393 train_time:51454ms step_avg:123.10ms
step:429/1393 train_time:51580ms step_avg:123.10ms
step:430/1393 train_time:51706ms step_avg:123.11ms
step:431/1393 train_time:51831ms step_avg:123.11ms
step:432/1393 train_time:51957ms step_avg:123.12ms
step:433/1393 train_time:52083ms step_avg:123.13ms
step:434/1393 train_time:52208ms step_avg:123.13ms
step:435/1393 train_time:52335ms step_avg:123.14ms
step:436/1393 train_time:52461ms step_avg:123.15ms
step:437/1393 train_time:52588ms step_avg:123.16ms
step:438/1393 train_time:52714ms step_avg:123.16ms
step:439/1393 train_time:52839ms step_avg:123.17ms
step:440/1393 train_time:52964ms step_avg:123.17ms
step:441/1393 train_time:53090ms step_avg:123.18ms
step:442/1393 train_time:53216ms step_avg:123.19ms
step:443/1393 train_time:53342ms step_avg:123.19ms
step:444/1393 train_time:53469ms step_avg:123.20ms
step:445/1393 train_time:53596ms step_avg:123.21ms
step:446/1393 train_time:53722ms step_avg:123.22ms
step:447/1393 train_time:53848ms step_avg:123.22ms
step:448/1393 train_time:53974ms step_avg:123.23ms
step:449/1393 train_time:54099ms step_avg:123.23ms
step:450/1393 train_time:54224ms step_avg:123.24ms
step:451/1393 train_time:54351ms step_avg:123.25ms
step:452/1393 train_time:54477ms step_avg:123.25ms
step:453/1393 train_time:54602ms step_avg:123.25ms
step:454/1393 train_time:54727ms step_avg:123.26ms
step:455/1393 train_time:54852ms step_avg:123.26ms
step:456/1393 train_time:54978ms step_avg:123.27ms
step:457/1393 train_time:55104ms step_avg:123.28ms
step:458/1393 train_time:55230ms step_avg:123.28ms
step:459/1393 train_time:55356ms step_avg:123.29ms
step:460/1393 train_time:55481ms step_avg:123.29ms
step:461/1393 train_time:55607ms step_avg:123.30ms
step:462/1393 train_time:55732ms step_avg:123.30ms
step:463/1393 train_time:55858ms step_avg:123.31ms
step:464/1393 train_time:55983ms step_avg:123.31ms
step:465/1393 train_time:56109ms step_avg:123.32ms
step:466/1393 train_time:56236ms step_avg:123.32ms
step:467/1393 train_time:56360ms step_avg:123.33ms
step:468/1393 train_time:56486ms step_avg:123.33ms
step:469/1393 train_time:56612ms step_avg:123.34ms
step:470/1393 train_time:56737ms step_avg:123.34ms
step:471/1393 train_time:56862ms step_avg:123.35ms
step:472/1393 train_time:56988ms step_avg:123.35ms
step:473/1393 train_time:57114ms step_avg:123.36ms
step:474/1393 train_time:57239ms step_avg:123.36ms
step:475/1393 train_time:57365ms step_avg:123.37ms
step:476/1393 train_time:57492ms step_avg:123.37ms
step:477/1393 train_time:57617ms step_avg:123.38ms
step:478/1393 train_time:57743ms step_avg:123.38ms
step:479/1393 train_time:57869ms step_avg:123.39ms
step:480/1393 train_time:57995ms step_avg:123.39ms
step:481/1393 train_time:58120ms step_avg:123.40ms
step:482/1393 train_time:58246ms step_avg:123.40ms
step:483/1393 train_time:58371ms step_avg:123.41ms
step:484/1393 train_time:58497ms step_avg:123.41ms
step:485/1393 train_time:58622ms step_avg:123.42ms
step:486/1393 train_time:58748ms step_avg:123.42ms
step:487/1393 train_time:58874ms step_avg:123.43ms
step:488/1393 train_time:58999ms step_avg:123.43ms
step:489/1393 train_time:59124ms step_avg:123.43ms
step:490/1393 train_time:59250ms step_avg:123.44ms
step:491/1393 train_time:59376ms step_avg:123.44ms
step:492/1393 train_time:59502ms step_avg:123.45ms
step:493/1393 train_time:59628ms step_avg:123.45ms
step:494/1393 train_time:59753ms step_avg:123.46ms
step:495/1393 train_time:59879ms step_avg:123.46ms
step:496/1393 train_time:60005ms step_avg:123.47ms
step:497/1393 train_time:60130ms step_avg:123.47ms
step:498/1393 train_time:60256ms step_avg:123.47ms
step:499/1393 train_time:60381ms step_avg:123.48ms
step:500/1393 train_time:60508ms step_avg:123.49ms
step:500/1393 val_loss:3.6658 train_time:60632ms step_avg:123.74ms
step:501/1393 train_time:60654ms step_avg:123.53ms
step:502/1393 train_time:60771ms step_avg:123.52ms
step:503/1393 train_time:60897ms step_avg:123.52ms
step:504/1393 train_time:61021ms step_avg:123.53ms
step:505/1393 train_time:61147ms step_avg:123.53ms
step:506/1393 train_time:61271ms step_avg:123.53ms
step:507/1393 train_time:61396ms step_avg:123.53ms
step:508/1393 train_time:61521ms step_avg:123.54ms
step:509/1393 train_time:61647ms step_avg:123.54ms
step:510/1393 train_time:61775ms step_avg:123.55ms
step:511/1393 train_time:61901ms step_avg:123.56ms
step:512/1393 train_time:62027ms step_avg:123.56ms
step:513/1393 train_time:62153ms step_avg:123.56ms
step:514/1393 train_time:62278ms step_avg:123.57ms
step:515/1393 train_time:62403ms step_avg:123.57ms
step:516/1393 train_time:62528ms step_avg:123.57ms
step:517/1393 train_time:62653ms step_avg:123.58ms
step:518/1393 train_time:62782ms step_avg:123.59ms
step:519/1393 train_time:62910ms step_avg:123.60ms
step:520/1393 train_time:63038ms step_avg:123.60ms
step:521/1393 train_time:63165ms step_avg:123.61ms
step:522/1393 train_time:63293ms step_avg:123.62ms
step:523/1393 train_time:63420ms step_avg:123.63ms
step:524/1393 train_time:63547ms step_avg:123.63ms
step:525/1393 train_time:63674ms step_avg:123.64ms
step:526/1393 train_time:63803ms step_avg:123.65ms
step:527/1393 train_time:63932ms step_avg:123.66ms
step:528/1393 train_time:64060ms step_avg:123.67ms
step:529/1393 train_time:64187ms step_avg:123.67ms
step:530/1393 train_time:64315ms step_avg:123.68ms
step:531/1393 train_time:64443ms step_avg:123.69ms
step:532/1393 train_time:64570ms step_avg:123.70ms
step:533/1393 train_time:64698ms step_avg:123.71ms
step:534/1393 train_time:64825ms step_avg:123.71ms
step:535/1393 train_time:64953ms step_avg:123.72ms
step:536/1393 train_time:65081ms step_avg:123.73ms
step:537/1393 train_time:65210ms step_avg:123.74ms
step:538/1393 train_time:65337ms step_avg:123.74ms
step:539/1393 train_time:65465ms step_avg:123.75ms
step:540/1393 train_time:65592ms step_avg:123.76ms
step:541/1393 train_time:65719ms step_avg:123.77ms
step:542/1393 train_time:65847ms step_avg:123.77ms
step:543/1393 train_time:65975ms step_avg:123.78ms
step:544/1393 train_time:66103ms step_avg:123.79ms
step:545/1393 train_time:66231ms step_avg:123.80ms
step:546/1393 train_time:66359ms step_avg:123.80ms
step:547/1393 train_time:66486ms step_avg:123.81ms
step:548/1393 train_time:66615ms step_avg:123.82ms
step:549/1393 train_time:66742ms step_avg:123.83ms
step:550/1393 train_time:66870ms step_avg:123.83ms
step:551/1393 train_time:66998ms step_avg:123.84ms
step:552/1393 train_time:67125ms step_avg:123.85ms
step:553/1393 train_time:67253ms step_avg:123.85ms
step:554/1393 train_time:67381ms step_avg:123.86ms
step:555/1393 train_time:67509ms step_avg:123.87ms
step:556/1393 train_time:67636ms step_avg:123.87ms
step:557/1393 train_time:67763ms step_avg:123.88ms
step:558/1393 train_time:67891ms step_avg:123.89ms
step:559/1393 train_time:68019ms step_avg:123.90ms
step:560/1393 train_time:68146ms step_avg:123.90ms
step:561/1393 train_time:68274ms step_avg:123.91ms
step:562/1393 train_time:68402ms step_avg:123.92ms
step:563/1393 train_time:68530ms step_avg:123.92ms
step:564/1393 train_time:68657ms step_avg:123.93ms
step:565/1393 train_time:68785ms step_avg:123.94ms
step:566/1393 train_time:68914ms step_avg:123.95ms
step:567/1393 train_time:69041ms step_avg:123.95ms
step:568/1393 train_time:69168ms step_avg:123.96ms
step:569/1393 train_time:69297ms step_avg:123.97ms
step:570/1393 train_time:69424ms step_avg:123.97ms
step:571/1393 train_time:69553ms step_avg:123.98ms
step:572/1393 train_time:69679ms step_avg:123.98ms
step:573/1393 train_time:69808ms step_avg:123.99ms
step:574/1393 train_time:69936ms step_avg:124.00ms
step:575/1393 train_time:70064ms step_avg:124.01ms
step:576/1393 train_time:70192ms step_avg:124.01ms
step:577/1393 train_time:70319ms step_avg:124.02ms
step:578/1393 train_time:70447ms step_avg:124.03ms
step:579/1393 train_time:70574ms step_avg:124.03ms
step:580/1393 train_time:70702ms step_avg:124.04ms
step:581/1393 train_time:70830ms step_avg:124.05ms
step:582/1393 train_time:70958ms step_avg:124.05ms
step:583/1393 train_time:71086ms step_avg:124.06ms
step:584/1393 train_time:71214ms step_avg:124.07ms
step:585/1393 train_time:71342ms step_avg:124.07ms
step:586/1393 train_time:71469ms step_avg:124.08ms
step:587/1393 train_time:71597ms step_avg:124.08ms
step:588/1393 train_time:71724ms step_avg:124.09ms
step:589/1393 train_time:71852ms step_avg:124.10ms
step:590/1393 train_time:71980ms step_avg:124.10ms
step:591/1393 train_time:72108ms step_avg:124.11ms
step:592/1393 train_time:72235ms step_avg:124.12ms
step:593/1393 train_time:72362ms step_avg:124.12ms
step:594/1393 train_time:72490ms step_avg:124.13ms
step:595/1393 train_time:72618ms step_avg:124.13ms
step:596/1393 train_time:72745ms step_avg:124.14ms
step:597/1393 train_time:72873ms step_avg:124.14ms
step:598/1393 train_time:73000ms step_avg:124.15ms
step:599/1393 train_time:73128ms step_avg:124.16ms
step:600/1393 train_time:73256ms step_avg:124.16ms
step:601/1393 train_time:73384ms step_avg:124.17ms
step:602/1393 train_time:73512ms step_avg:124.18ms
step:603/1393 train_time:73640ms step_avg:124.18ms
step:604/1393 train_time:73768ms step_avg:124.19ms
step:605/1393 train_time:73896ms step_avg:124.20ms
step:606/1393 train_time:74024ms step_avg:124.20ms
step:607/1393 train_time:74152ms step_avg:124.21ms
step:608/1393 train_time:74280ms step_avg:124.21ms
step:609/1393 train_time:74408ms step_avg:124.22ms
step:610/1393 train_time:74536ms step_avg:124.23ms
step:611/1393 train_time:74664ms step_avg:124.23ms
step:612/1393 train_time:74792ms step_avg:124.24ms
step:613/1393 train_time:74919ms step_avg:124.24ms
step:614/1393 train_time:75047ms step_avg:124.25ms
step:615/1393 train_time:75174ms step_avg:124.25ms
step:616/1393 train_time:75303ms step_avg:124.26ms
step:617/1393 train_time:75431ms step_avg:124.27ms
step:618/1393 train_time:75558ms step_avg:124.27ms
step:619/1393 train_time:75687ms step_avg:124.28ms
step:620/1393 train_time:75815ms step_avg:124.29ms
step:621/1393 train_time:75943ms step_avg:124.29ms
step:622/1393 train_time:76071ms step_avg:124.30ms
step:623/1393 train_time:76199ms step_avg:124.30ms
step:624/1393 train_time:76327ms step_avg:124.31ms
step:625/1393 train_time:76456ms step_avg:124.32ms
step:625/1393 val_loss:3.5824 train_time:76582ms step_avg:124.52ms
step:626/1393 train_time:76604ms step_avg:124.36ms
step:627/1393 train_time:76720ms step_avg:124.34ms
step:628/1393 train_time:76848ms step_avg:124.35ms
step:629/1393 train_time:76975ms step_avg:124.35ms
step:630/1393 train_time:77102ms step_avg:124.36ms
step:631/1393 train_time:77229ms step_avg:124.36ms
step:632/1393 train_time:77357ms step_avg:124.37ms
step:633/1393 train_time:77485ms step_avg:124.37ms
step:634/1393 train_time:77614ms step_avg:124.38ms
step:635/1393 train_time:77744ms step_avg:124.39ms
step:636/1393 train_time:77873ms step_avg:124.40ms
step:637/1393 train_time:78001ms step_avg:124.40ms
step:638/1393 train_time:78129ms step_avg:124.41ms
step:639/1393 train_time:78256ms step_avg:124.41ms
step:640/1393 train_time:78384ms step_avg:124.42ms
step:641/1393 train_time:78512ms step_avg:124.42ms
step:642/1393 train_time:78640ms step_avg:124.43ms
step:643/1393 train_time:78768ms step_avg:124.44ms
step:644/1393 train_time:78896ms step_avg:124.44ms
step:645/1393 train_time:79026ms step_avg:124.45ms
step:646/1393 train_time:79154ms step_avg:124.46ms
step:647/1393 train_time:79281ms step_avg:124.46ms
step:648/1393 train_time:79410ms step_avg:124.47ms
step:649/1393 train_time:79538ms step_avg:124.47ms
step:650/1393 train_time:79666ms step_avg:124.48ms
step:651/1393 train_time:79794ms step_avg:124.48ms
step:652/1393 train_time:79922ms step_avg:124.49ms
step:653/1393 train_time:80050ms step_avg:124.49ms
step:654/1393 train_time:80178ms step_avg:124.50ms
step:655/1393 train_time:80305ms step_avg:124.50ms
step:656/1393 train_time:80434ms step_avg:124.51ms
step:657/1393 train_time:80562ms step_avg:124.52ms
step:658/1393 train_time:80691ms step_avg:124.52ms
step:659/1393 train_time:80819ms step_avg:124.53ms
step:660/1393 train_time:80948ms step_avg:124.53ms
step:661/1393 train_time:81076ms step_avg:124.54ms
step:662/1393 train_time:81204ms step_avg:124.55ms
step:663/1393 train_time:81331ms step_avg:124.55ms
step:664/1393 train_time:81459ms step_avg:124.56ms
step:665/1393 train_time:81587ms step_avg:124.56ms
step:666/1393 train_time:81715ms step_avg:124.57ms
step:667/1393 train_time:81844ms step_avg:124.57ms
step:668/1393 train_time:81971ms step_avg:124.58ms
step:669/1393 train_time:82099ms step_avg:124.58ms
step:670/1393 train_time:82228ms step_avg:124.59ms
step:671/1393 train_time:82356ms step_avg:124.59ms
step:672/1393 train_time:82483ms step_avg:124.60ms
step:673/1393 train_time:82611ms step_avg:124.60ms
step:674/1393 train_time:82739ms step_avg:124.61ms
step:675/1393 train_time:82867ms step_avg:124.61ms
step:676/1393 train_time:82995ms step_avg:124.62ms
step:677/1393 train_time:83124ms step_avg:124.62ms
step:678/1393 train_time:83253ms step_avg:124.63ms
step:679/1393 train_time:83380ms step_avg:124.63ms
step:680/1393 train_time:83509ms step_avg:124.64ms
step:681/1393 train_time:83637ms step_avg:124.65ms
step:682/1393 train_time:83765ms step_avg:124.65ms
step:683/1393 train_time:83894ms step_avg:124.66ms
step:684/1393 train_time:84022ms step_avg:124.66ms
step:685/1393 train_time:84151ms step_avg:124.67ms
step:686/1393 train_time:84278ms step_avg:124.67ms
step:687/1393 train_time:84407ms step_avg:124.68ms
step:688/1393 train_time:84535ms step_avg:124.68ms
step:689/1393 train_time:84663ms step_avg:124.69ms
step:690/1393 train_time:84791ms step_avg:124.69ms
step:691/1393 train_time:84919ms step_avg:124.70ms
step:692/1393 train_time:85048ms step_avg:124.70ms
step:693/1393 train_time:85177ms step_avg:124.71ms
step:694/1393 train_time:85305ms step_avg:124.72ms
step:695/1393 train_time:85434ms step_avg:124.72ms
step:696/1393 train_time:85561ms step_avg:124.72ms
step:697/1393 train_time:85689ms step_avg:124.73ms
step:698/1393 train_time:85817ms step_avg:124.73ms
step:699/1393 train_time:85944ms step_avg:124.74ms
step:700/1393 train_time:86073ms step_avg:124.74ms
step:701/1393 train_time:86200ms step_avg:124.75ms
step:702/1393 train_time:86329ms step_avg:124.75ms
step:703/1393 train_time:86458ms step_avg:124.76ms
step:704/1393 train_time:86586ms step_avg:124.76ms
step:705/1393 train_time:86714ms step_avg:124.77ms
step:706/1393 train_time:86842ms step_avg:124.77ms
step:707/1393 train_time:86970ms step_avg:124.78ms
step:708/1393 train_time:87098ms step_avg:124.78ms
step:709/1393 train_time:87227ms step_avg:124.79ms
step:710/1393 train_time:87355ms step_avg:124.79ms
step:711/1393 train_time:87483ms step_avg:124.80ms
step:712/1393 train_time:87611ms step_avg:124.80ms
step:713/1393 train_time:87738ms step_avg:124.81ms
step:714/1393 train_time:87867ms step_avg:124.81ms
step:715/1393 train_time:87995ms step_avg:124.82ms
step:716/1393 train_time:88124ms step_avg:124.82ms
step:717/1393 train_time:88252ms step_avg:124.83ms
step:718/1393 train_time:88380ms step_avg:124.83ms
step:719/1393 train_time:88508ms step_avg:124.83ms
step:720/1393 train_time:88636ms step_avg:124.84ms
step:721/1393 train_time:88764ms step_avg:124.84ms
step:722/1393 train_time:88893ms step_avg:124.85ms
step:723/1393 train_time:89021ms step_avg:124.85ms
step:724/1393 train_time:89149ms step_avg:124.86ms
step:725/1393 train_time:89280ms step_avg:124.87ms
step:726/1393 train_time:89411ms step_avg:124.88ms
step:727/1393 train_time:89541ms step_avg:124.88ms
step:728/1393 train_time:89670ms step_avg:124.89ms
step:729/1393 train_time:89800ms step_avg:124.90ms
step:730/1393 train_time:89930ms step_avg:124.90ms
step:731/1393 train_time:90060ms step_avg:124.91ms
step:732/1393 train_time:90190ms step_avg:124.92ms
step:733/1393 train_time:90320ms step_avg:124.92ms
step:734/1393 train_time:90450ms step_avg:124.93ms
step:735/1393 train_time:90582ms step_avg:124.94ms
step:736/1393 train_time:90712ms step_avg:124.95ms
step:737/1393 train_time:90842ms step_avg:124.95ms
step:738/1393 train_time:90971ms step_avg:124.96ms
step:739/1393 train_time:91100ms step_avg:124.97ms
step:740/1393 train_time:91231ms step_avg:124.97ms
step:741/1393 train_time:91363ms step_avg:124.98ms
step:742/1393 train_time:91493ms step_avg:124.99ms
step:743/1393 train_time:91622ms step_avg:125.00ms
step:744/1393 train_time:91752ms step_avg:125.00ms
step:745/1393 train_time:91883ms step_avg:125.01ms
step:746/1393 train_time:92013ms step_avg:125.02ms
step:747/1393 train_time:92144ms step_avg:125.03ms
step:748/1393 train_time:92274ms step_avg:125.03ms
step:749/1393 train_time:92404ms step_avg:125.04ms
step:750/1393 train_time:92534ms step_avg:125.05ms
step:750/1393 val_loss:3.5286 train_time:92663ms step_avg:125.22ms
step:751/1393 train_time:92685ms step_avg:125.08ms
step:752/1393 train_time:92802ms step_avg:125.07ms
step:753/1393 train_time:92934ms step_avg:125.08ms
step:754/1393 train_time:93064ms step_avg:125.09ms
step:755/1393 train_time:93193ms step_avg:125.09ms
step:756/1393 train_time:93322ms step_avg:125.10ms
step:757/1393 train_time:93451ms step_avg:125.10ms
step:758/1393 train_time:93581ms step_avg:125.11ms
step:759/1393 train_time:93713ms step_avg:125.12ms
step:760/1393 train_time:93844ms step_avg:125.13ms
step:761/1393 train_time:93975ms step_avg:125.13ms
step:762/1393 train_time:94105ms step_avg:125.14ms
step:763/1393 train_time:94235ms step_avg:125.15ms
step:764/1393 train_time:94366ms step_avg:125.15ms
step:765/1393 train_time:94496ms step_avg:125.16ms
step:766/1393 train_time:94627ms step_avg:125.17ms
step:767/1393 train_time:94756ms step_avg:125.17ms
step:768/1393 train_time:94887ms step_avg:125.18ms
step:769/1393 train_time:95017ms step_avg:125.19ms
step:770/1393 train_time:95147ms step_avg:125.19ms
step:771/1393 train_time:95277ms step_avg:125.20ms
step:772/1393 train_time:95407ms step_avg:125.21ms
step:773/1393 train_time:95538ms step_avg:125.21ms
step:774/1393 train_time:95667ms step_avg:125.22ms
step:775/1393 train_time:95797ms step_avg:125.22ms
step:776/1393 train_time:95926ms step_avg:125.23ms
step:777/1393 train_time:96056ms step_avg:125.24ms
step:778/1393 train_time:96187ms step_avg:125.24ms
step:779/1393 train_time:96316ms step_avg:125.25ms
step:780/1393 train_time:96446ms step_avg:125.25ms
step:781/1393 train_time:96576ms step_avg:125.26ms
step:782/1393 train_time:96706ms step_avg:125.27ms
step:783/1393 train_time:96835ms step_avg:125.27ms
step:784/1393 train_time:96965ms step_avg:125.28ms
step:785/1393 train_time:97095ms step_avg:125.28ms
step:786/1393 train_time:97226ms step_avg:125.29ms
step:787/1393 train_time:97357ms step_avg:125.30ms
step:788/1393 train_time:97487ms step_avg:125.31ms
step:789/1393 train_time:97617ms step_avg:125.31ms
step:790/1393 train_time:97746ms step_avg:125.32ms
step:791/1393 train_time:97876ms step_avg:125.32ms
step:792/1393 train_time:98006ms step_avg:125.33ms
step:793/1393 train_time:98136ms step_avg:125.33ms
step:794/1393 train_time:98267ms step_avg:125.34ms
step:795/1393 train_time:98398ms step_avg:125.35ms
step:796/1393 train_time:98529ms step_avg:125.35ms
step:797/1393 train_time:98659ms step_avg:125.36ms
step:798/1393 train_time:98789ms step_avg:125.37ms
step:799/1393 train_time:98920ms step_avg:125.37ms
step:800/1393 train_time:99050ms step_avg:125.38ms
step:801/1393 train_time:99181ms step_avg:125.39ms
step:802/1393 train_time:99311ms step_avg:125.39ms
step:803/1393 train_time:99441ms step_avg:125.40ms
step:804/1393 train_time:99571ms step_avg:125.40ms
step:805/1393 train_time:99702ms step_avg:125.41ms
step:806/1393 train_time:99831ms step_avg:125.42ms
step:807/1393 train_time:99961ms step_avg:125.42ms
step:808/1393 train_time:100091ms step_avg:125.43ms
step:809/1393 train_time:100220ms step_avg:125.43ms
step:810/1393 train_time:100351ms step_avg:125.44ms
step:811/1393 train_time:100480ms step_avg:125.44ms
step:812/1393 train_time:100612ms step_avg:125.45ms
step:813/1393 train_time:100742ms step_avg:125.46ms
step:814/1393 train_time:100872ms step_avg:125.46ms
step:815/1393 train_time:101002ms step_avg:125.47ms
step:816/1393 train_time:101131ms step_avg:125.47ms
step:817/1393 train_time:101260ms step_avg:125.48ms
step:818/1393 train_time:101390ms step_avg:125.48ms
step:819/1393 train_time:101520ms step_avg:125.49ms
step:820/1393 train_time:101651ms step_avg:125.49ms
step:821/1393 train_time:101781ms step_avg:125.50ms
step:822/1393 train_time:101911ms step_avg:125.51ms
step:823/1393 train_time:102041ms step_avg:125.51ms
step:824/1393 train_time:102171ms step_avg:125.52ms
step:825/1393 train_time:102302ms step_avg:125.52ms
step:826/1393 train_time:102432ms step_avg:125.53ms
step:827/1393 train_time:102563ms step_avg:125.54ms
step:828/1393 train_time:102693ms step_avg:125.54ms
step:829/1393 train_time:102822ms step_avg:125.55ms
step:830/1393 train_time:102952ms step_avg:125.55ms
step:831/1393 train_time:103082ms step_avg:125.56ms
step:832/1393 train_time:103213ms step_avg:125.56ms
step:833/1393 train_time:103344ms step_avg:125.57ms
step:834/1393 train_time:103474ms step_avg:125.58ms
step:835/1393 train_time:103605ms step_avg:125.58ms
step:836/1393 train_time:103735ms step_avg:125.59ms
step:837/1393 train_time:103866ms step_avg:125.59ms
step:838/1393 train_time:103996ms step_avg:125.60ms
step:839/1393 train_time:104125ms step_avg:125.60ms
step:840/1393 train_time:104256ms step_avg:125.61ms
step:841/1393 train_time:104386ms step_avg:125.62ms
step:842/1393 train_time:104517ms step_avg:125.62ms
step:843/1393 train_time:104647ms step_avg:125.63ms
step:844/1393 train_time:104778ms step_avg:125.63ms
step:845/1393 train_time:104906ms step_avg:125.64ms
step:846/1393 train_time:105036ms step_avg:125.64ms
step:847/1393 train_time:105167ms step_avg:125.65ms
step:848/1393 train_time:105298ms step_avg:125.65ms
step:849/1393 train_time:105428ms step_avg:125.66ms
step:850/1393 train_time:105558ms step_avg:125.66ms
step:851/1393 train_time:105690ms step_avg:125.67ms
step:852/1393 train_time:105820ms step_avg:125.68ms
step:853/1393 train_time:105949ms step_avg:125.68ms
step:854/1393 train_time:106079ms step_avg:125.69ms
step:855/1393 train_time:106209ms step_avg:125.69ms
step:856/1393 train_time:106340ms step_avg:125.70ms
step:857/1393 train_time:106471ms step_avg:125.70ms
step:858/1393 train_time:106601ms step_avg:125.71ms
step:859/1393 train_time:106732ms step_avg:125.71ms
step:860/1393 train_time:106862ms step_avg:125.72ms
step:861/1393 train_time:106992ms step_avg:125.72ms
step:862/1393 train_time:107123ms step_avg:125.73ms
step:863/1393 train_time:107253ms step_avg:125.74ms
step:864/1393 train_time:107383ms step_avg:125.74ms
step:865/1393 train_time:107513ms step_avg:125.75ms
step:866/1393 train_time:107644ms step_avg:125.75ms
step:867/1393 train_time:107774ms step_avg:125.76ms
step:868/1393 train_time:107904ms step_avg:125.76ms
step:869/1393 train_time:108035ms step_avg:125.77ms
step:870/1393 train_time:108165ms step_avg:125.77ms
step:871/1393 train_time:108295ms step_avg:125.78ms
step:872/1393 train_time:108426ms step_avg:125.78ms
step:873/1393 train_time:108556ms step_avg:125.79ms
step:874/1393 train_time:108686ms step_avg:125.79ms
step:875/1393 train_time:108816ms step_avg:125.80ms
step:875/1393 val_loss:3.4784 train_time:108945ms step_avg:125.95ms
step:876/1393 train_time:108967ms step_avg:125.83ms
step:877/1393 train_time:109086ms step_avg:125.82ms
step:878/1393 train_time:109217ms step_avg:125.83ms
step:879/1393 train_time:109346ms step_avg:125.83ms
step:880/1393 train_time:109476ms step_avg:125.83ms
step:881/1393 train_time:109605ms step_avg:125.84ms
step:882/1393 train_time:109735ms step_avg:125.84ms
step:883/1393 train_time:109865ms step_avg:125.85ms
step:884/1393 train_time:109996ms step_avg:125.85ms
step:885/1393 train_time:110128ms step_avg:125.86ms
step:886/1393 train_time:110260ms step_avg:125.87ms
step:887/1393 train_time:110390ms step_avg:125.87ms
step:888/1393 train_time:110521ms step_avg:125.88ms
step:889/1393 train_time:110652ms step_avg:125.88ms
step:890/1393 train_time:110782ms step_avg:125.89ms
step:891/1393 train_time:110911ms step_avg:125.89ms
step:892/1393 train_time:111042ms step_avg:125.90ms
step:893/1393 train_time:111173ms step_avg:125.90ms
step:894/1393 train_time:111302ms step_avg:125.91ms
step:895/1393 train_time:111434ms step_avg:125.91ms
step:896/1393 train_time:111565ms step_avg:125.92ms
step:897/1393 train_time:111694ms step_avg:125.92ms
step:898/1393 train_time:111824ms step_avg:125.93ms
step:899/1393 train_time:111955ms step_avg:125.93ms
step:900/1393 train_time:112087ms step_avg:125.94ms
step:901/1393 train_time:112218ms step_avg:125.95ms
step:902/1393 train_time:112348ms step_avg:125.95ms
step:903/1393 train_time:112478ms step_avg:125.95ms
step:904/1393 train_time:112608ms step_avg:125.96ms
step:905/1393 train_time:112737ms step_avg:125.96ms
step:906/1393 train_time:112867ms step_avg:125.97ms
step:907/1393 train_time:112999ms step_avg:125.97ms
step:908/1393 train_time:113130ms step_avg:125.98ms
step:909/1393 train_time:113260ms step_avg:125.98ms
step:910/1393 train_time:113393ms step_avg:125.99ms
step:911/1393 train_time:113523ms step_avg:126.00ms
step:912/1393 train_time:113653ms step_avg:126.00ms
step:913/1393 train_time:113782ms step_avg:126.00ms
step:914/1393 train_time:113912ms step_avg:126.01ms
step:915/1393 train_time:114042ms step_avg:126.01ms
step:916/1393 train_time:114174ms step_avg:126.02ms
step:917/1393 train_time:114306ms step_avg:126.03ms
step:918/1393 train_time:114436ms step_avg:126.03ms
step:919/1393 train_time:114569ms step_avg:126.04ms
step:920/1393 train_time:114698ms step_avg:126.04ms
step:921/1393 train_time:114829ms step_avg:126.05ms
step:922/1393 train_time:114960ms step_avg:126.05ms
step:923/1393 train_time:115090ms step_avg:126.06ms
step:924/1393 train_time:115219ms step_avg:126.06ms
step:925/1393 train_time:115349ms step_avg:126.06ms
step:926/1393 train_time:115479ms step_avg:126.07ms
step:927/1393 train_time:115611ms step_avg:126.07ms
step:928/1393 train_time:115741ms step_avg:126.08ms
step:929/1393 train_time:115871ms step_avg:126.08ms
step:930/1393 train_time:116000ms step_avg:126.09ms
step:931/1393 train_time:116132ms step_avg:126.09ms
step:932/1393 train_time:116265ms step_avg:126.10ms
step:933/1393 train_time:116397ms step_avg:126.11ms
step:934/1393 train_time:116529ms step_avg:126.11ms
step:935/1393 train_time:116661ms step_avg:126.12ms
step:936/1393 train_time:116794ms step_avg:126.13ms
step:937/1393 train_time:116927ms step_avg:126.13ms
step:938/1393 train_time:117059ms step_avg:126.14ms
step:939/1393 train_time:117191ms step_avg:126.15ms
step:940/1393 train_time:117325ms step_avg:126.16ms
step:941/1393 train_time:117456ms step_avg:126.16ms
step:942/1393 train_time:117588ms step_avg:126.17ms
step:943/1393 train_time:117721ms step_avg:126.17ms
step:944/1393 train_time:117854ms step_avg:126.18ms
step:945/1393 train_time:117988ms step_avg:126.19ms
step:946/1393 train_time:118119ms step_avg:126.20ms
step:947/1393 train_time:118251ms step_avg:126.20ms
step:948/1393 train_time:118384ms step_avg:126.21ms
step:949/1393 train_time:118517ms step_avg:126.22ms
step:950/1393 train_time:118649ms step_avg:126.22ms
step:951/1393 train_time:118782ms step_avg:126.23ms
step:952/1393 train_time:118915ms step_avg:126.24ms
step:953/1393 train_time:119047ms step_avg:126.24ms
step:954/1393 train_time:119179ms step_avg:126.25ms
step:955/1393 train_time:119312ms step_avg:126.26ms
step:956/1393 train_time:119447ms step_avg:126.27ms
step:957/1393 train_time:119579ms step_avg:126.27ms
step:958/1393 train_time:119712ms step_avg:126.28ms
step:959/1393 train_time:119843ms step_avg:126.28ms
step:960/1393 train_time:119975ms step_avg:126.29ms
step:961/1393 train_time:120107ms step_avg:126.30ms
step:962/1393 train_time:120239ms step_avg:126.30ms
step:963/1393 train_time:120372ms step_avg:126.31ms
step:964/1393 train_time:120505ms step_avg:126.32ms
step:965/1393 train_time:120638ms step_avg:126.32ms
step:966/1393 train_time:120769ms step_avg:126.33ms
step:967/1393 train_time:120902ms step_avg:126.33ms
step:968/1393 train_time:121033ms step_avg:126.34ms
step:969/1393 train_time:121165ms step_avg:126.35ms
step:970/1393 train_time:121296ms step_avg:126.35ms
step:971/1393 train_time:121429ms step_avg:126.36ms
step:972/1393 train_time:121560ms step_avg:126.36ms
step:973/1393 train_time:121692ms step_avg:126.37ms
step:974/1393 train_time:121823ms step_avg:126.37ms
step:975/1393 train_time:121956ms step_avg:126.38ms
step:976/1393 train_time:122089ms step_avg:126.39ms
step:977/1393 train_time:122220ms step_avg:126.39ms
step:978/1393 train_time:122351ms step_avg:126.40ms
step:979/1393 train_time:122484ms step_avg:126.40ms
step:980/1393 train_time:122615ms step_avg:126.41ms
step:981/1393 train_time:122746ms step_avg:126.41ms
step:982/1393 train_time:122877ms step_avg:126.42ms
step:983/1393 train_time:123010ms step_avg:126.42ms
step:984/1393 train_time:123142ms step_avg:126.43ms
step:985/1393 train_time:123274ms step_avg:126.44ms
step:986/1393 train_time:123409ms step_avg:126.44ms
step:987/1393 train_time:123541ms step_avg:126.45ms
step:988/1393 train_time:123672ms step_avg:126.45ms
step:989/1393 train_time:123805ms step_avg:126.46ms
step:990/1393 train_time:123936ms step_avg:126.47ms
step:991/1393 train_time:124068ms step_avg:126.47ms
step:992/1393 train_time:124202ms step_avg:126.48ms
step:993/1393 train_time:124338ms step_avg:126.49ms
step:994/1393 train_time:124469ms step_avg:126.49ms
step:995/1393 train_time:124600ms step_avg:126.50ms
step:996/1393 train_time:124732ms step_avg:126.50ms
step:997/1393 train_time:124863ms step_avg:126.51ms
step:998/1393 train_time:124994ms step_avg:126.51ms
step:999/1393 train_time:125126ms step_avg:126.52ms
step:1000/1393 train_time:125258ms step_avg:126.52ms
step:1000/1393 val_loss:3.4162 train_time:125388ms step_avg:126.65ms
step:1001/1393 train_time:125411ms step_avg:126.55ms
step:1002/1393 train_time:125530ms step_avg:126.54ms
step:1003/1393 train_time:125663ms step_avg:126.55ms
step:1004/1393 train_time:125794ms step_avg:126.55ms
step:1005/1393 train_time:125927ms step_avg:126.56ms
step:1006/1393 train_time:126058ms step_avg:126.56ms
step:1007/1393 train_time:126189ms step_avg:126.57ms
step:1008/1393 train_time:126321ms step_avg:126.57ms
step:1009/1393 train_time:126456ms step_avg:126.58ms
step:1010/1393 train_time:126589ms step_avg:126.59ms
step:1011/1393 train_time:126721ms step_avg:126.59ms
step:1012/1393 train_time:126852ms step_avg:126.60ms
step:1013/1393 train_time:126985ms step_avg:126.61ms
step:1014/1393 train_time:127115ms step_avg:126.61ms
step:1015/1393 train_time:127246ms step_avg:126.61ms
step:1016/1393 train_time:127377ms step_avg:126.62ms
step:1017/1393 train_time:127509ms step_avg:126.62ms
step:1018/1393 train_time:127642ms step_avg:126.63ms
step:1019/1393 train_time:127775ms step_avg:126.64ms
step:1020/1393 train_time:127907ms step_avg:126.64ms
step:1021/1393 train_time:128038ms step_avg:126.65ms
step:1022/1393 train_time:128169ms step_avg:126.65ms
step:1023/1393 train_time:128301ms step_avg:126.65ms
step:1024/1393 train_time:128433ms step_avg:126.66ms
step:1025/1393 train_time:128565ms step_avg:126.67ms
step:1026/1393 train_time:128697ms step_avg:126.67ms
step:1027/1393 train_time:128829ms step_avg:126.68ms
step:1028/1393 train_time:128962ms step_avg:126.68ms
step:1029/1393 train_time:129095ms step_avg:126.69ms
step:1030/1393 train_time:129226ms step_avg:126.69ms
step:1031/1393 train_time:129357ms step_avg:126.70ms
step:1032/1393 train_time:129489ms step_avg:126.70ms
step:1033/1393 train_time:129621ms step_avg:126.71ms
step:1034/1393 train_time:129754ms step_avg:126.71ms
step:1035/1393 train_time:129890ms step_avg:126.72ms
step:1036/1393 train_time:130024ms step_avg:126.73ms
step:1037/1393 train_time:130156ms step_avg:126.73ms
step:1038/1393 train_time:130289ms step_avg:126.74ms
step:1039/1393 train_time:130420ms step_avg:126.74ms
step:1040/1393 train_time:130552ms step_avg:126.75ms
step:1041/1393 train_time:130685ms step_avg:126.76ms
step:1042/1393 train_time:130817ms step_avg:126.76ms
step:1043/1393 train_time:130950ms step_avg:126.77ms
step:1044/1393 train_time:131084ms step_avg:126.77ms
step:1045/1393 train_time:131217ms step_avg:126.78ms
step:1046/1393 train_time:131349ms step_avg:126.78ms
step:1047/1393 train_time:131481ms step_avg:126.79ms
step:1048/1393 train_time:131613ms step_avg:126.79ms
step:1049/1393 train_time:131744ms step_avg:126.80ms
step:1050/1393 train_time:131877ms step_avg:126.80ms
step:1051/1393 train_time:132010ms step_avg:126.81ms
step:1052/1393 train_time:132142ms step_avg:126.82ms
step:1053/1393 train_time:132273ms step_avg:126.82ms
step:1054/1393 train_time:132406ms step_avg:126.83ms
step:1055/1393 train_time:132538ms step_avg:126.83ms
step:1056/1393 train_time:132669ms step_avg:126.83ms
step:1057/1393 train_time:132801ms step_avg:126.84ms
step:1058/1393 train_time:132934ms step_avg:126.85ms
step:1059/1393 train_time:133067ms step_avg:126.85ms
step:1060/1393 train_time:133201ms step_avg:126.86ms
step:1061/1393 train_time:133333ms step_avg:126.86ms
step:1062/1393 train_time:133465ms step_avg:126.87ms
step:1063/1393 train_time:133596ms step_avg:126.87ms
step:1064/1393 train_time:133729ms step_avg:126.88ms
step:1065/1393 train_time:133862ms step_avg:126.88ms
step:1066/1393 train_time:133994ms step_avg:126.89ms
step:1067/1393 train_time:134126ms step_avg:126.89ms
step:1068/1393 train_time:134258ms step_avg:126.90ms
step:1069/1393 train_time:134391ms step_avg:126.90ms
step:1070/1393 train_time:134523ms step_avg:126.91ms
step:1071/1393 train_time:134655ms step_avg:126.91ms
step:1072/1393 train_time:134788ms step_avg:126.92ms
step:1073/1393 train_time:134919ms step_avg:126.92ms
step:1074/1393 train_time:135051ms step_avg:126.93ms
step:1075/1393 train_time:135183ms step_avg:126.93ms
step:1076/1393 train_time:135315ms step_avg:126.94ms
step:1077/1393 train_time:135447ms step_avg:126.94ms
step:1078/1393 train_time:135579ms step_avg:126.95ms
step:1079/1393 train_time:135713ms step_avg:126.95ms
step:1080/1393 train_time:135845ms step_avg:126.96ms
step:1081/1393 train_time:135976ms step_avg:126.96ms
step:1082/1393 train_time:136107ms step_avg:126.97ms
step:1083/1393 train_time:136238ms step_avg:126.97ms
step:1084/1393 train_time:136372ms step_avg:126.98ms
step:1085/1393 train_time:136504ms step_avg:126.98ms
step:1086/1393 train_time:136636ms step_avg:126.98ms
step:1087/1393 train_time:136768ms step_avg:126.99ms
step:1088/1393 train_time:136899ms step_avg:126.99ms
step:1089/1393 train_time:137032ms step_avg:127.00ms
step:1090/1393 train_time:137165ms step_avg:127.00ms
step:1091/1393 train_time:137297ms step_avg:127.01ms
step:1092/1393 train_time:137429ms step_avg:127.01ms
step:1093/1393 train_time:137562ms step_avg:127.02ms
step:1094/1393 train_time:137694ms step_avg:127.02ms
step:1095/1393 train_time:137827ms step_avg:127.03ms
step:1096/1393 train_time:137961ms step_avg:127.04ms
step:1097/1393 train_time:138093ms step_avg:127.04ms
step:1098/1393 train_time:138226ms step_avg:127.05ms
step:1099/1393 train_time:138358ms step_avg:127.05ms
step:1100/1393 train_time:138489ms step_avg:127.05ms
step:1101/1393 train_time:138621ms step_avg:127.06ms
step:1102/1393 train_time:138752ms step_avg:127.06ms
step:1103/1393 train_time:138885ms step_avg:127.07ms
step:1104/1393 train_time:139019ms step_avg:127.07ms
step:1105/1393 train_time:139153ms step_avg:127.08ms
step:1106/1393 train_time:139286ms step_avg:127.09ms
step:1107/1393 train_time:139418ms step_avg:127.09ms
step:1108/1393 train_time:139553ms step_avg:127.10ms
step:1109/1393 train_time:139684ms step_avg:127.10ms
step:1110/1393 train_time:139816ms step_avg:127.11ms
step:1111/1393 train_time:139949ms step_avg:127.11ms
step:1112/1393 train_time:140081ms step_avg:127.12ms
step:1113/1393 train_time:140213ms step_avg:127.12ms
step:1114/1393 train_time:140348ms step_avg:127.13ms
step:1115/1393 train_time:140482ms step_avg:127.13ms
step:1116/1393 train_time:140614ms step_avg:127.14ms
step:1117/1393 train_time:140746ms step_avg:127.14ms
step:1118/1393 train_time:140880ms step_avg:127.15ms
step:1119/1393 train_time:141012ms step_avg:127.15ms
step:1120/1393 train_time:141143ms step_avg:127.16ms
step:1121/1393 train_time:141275ms step_avg:127.16ms
step:1122/1393 train_time:141407ms step_avg:127.16ms
step:1123/1393 train_time:141539ms step_avg:127.17ms
step:1124/1393 train_time:141671ms step_avg:127.17ms
step:1125/1393 train_time:141803ms step_avg:127.18ms
step:1125/1393 val_loss:3.3658 train_time:141934ms step_avg:127.29ms
step:1126/1393 train_time:141956ms step_avg:127.20ms
step:1127/1393 train_time:142074ms step_avg:127.19ms
step:1128/1393 train_time:142207ms step_avg:127.20ms
step:1129/1393 train_time:142340ms step_avg:127.20ms
step:1130/1393 train_time:142473ms step_avg:127.21ms
step:1131/1393 train_time:142609ms step_avg:127.22ms
step:1132/1393 train_time:142740ms step_avg:127.22ms
step:1133/1393 train_time:142871ms step_avg:127.22ms
step:1134/1393 train_time:143004ms step_avg:127.23ms
step:1135/1393 train_time:143136ms step_avg:127.23ms
step:1136/1393 train_time:143272ms step_avg:127.24ms
step:1137/1393 train_time:143404ms step_avg:127.24ms
step:1138/1393 train_time:143538ms step_avg:127.25ms
step:1139/1393 train_time:143671ms step_avg:127.26ms
step:1140/1393 train_time:143805ms step_avg:127.26ms
step:1141/1393 train_time:143939ms step_avg:127.27ms
step:1142/1393 train_time:144073ms step_avg:127.27ms
step:1143/1393 train_time:144209ms step_avg:127.28ms
step:1144/1393 train_time:144342ms step_avg:127.29ms
step:1145/1393 train_time:144476ms step_avg:127.29ms
step:1146/1393 train_time:144610ms step_avg:127.30ms
step:1147/1393 train_time:144744ms step_avg:127.30ms
step:1148/1393 train_time:144878ms step_avg:127.31ms
step:1149/1393 train_time:145011ms step_avg:127.31ms
step:1150/1393 train_time:145144ms step_avg:127.32ms
step:1151/1393 train_time:145279ms step_avg:127.33ms
step:1152/1393 train_time:145411ms step_avg:127.33ms
step:1153/1393 train_time:145548ms step_avg:127.34ms
step:1154/1393 train_time:145682ms step_avg:127.34ms
step:1155/1393 train_time:145816ms step_avg:127.35ms
step:1156/1393 train_time:145952ms step_avg:127.36ms
step:1157/1393 train_time:146084ms step_avg:127.36ms
step:1158/1393 train_time:146218ms step_avg:127.37ms
step:1159/1393 train_time:146351ms step_avg:127.37ms
step:1160/1393 train_time:146485ms step_avg:127.38ms
step:1161/1393 train_time:146618ms step_avg:127.38ms
step:1162/1393 train_time:146753ms step_avg:127.39ms
step:1163/1393 train_time:146887ms step_avg:127.40ms
step:1164/1393 train_time:147022ms step_avg:127.40ms
step:1165/1393 train_time:147155ms step_avg:127.41ms
step:1166/1393 train_time:147288ms step_avg:127.41ms
step:1167/1393 train_time:147420ms step_avg:127.42ms
step:1168/1393 train_time:147554ms step_avg:127.42ms
step:1169/1393 train_time:147689ms step_avg:127.43ms
step:1170/1393 train_time:147823ms step_avg:127.43ms
step:1171/1393 train_time:147957ms step_avg:127.44ms
step:1172/1393 train_time:148090ms step_avg:127.44ms
step:1173/1393 train_time:148225ms step_avg:127.45ms
step:1174/1393 train_time:148363ms step_avg:127.46ms
step:1175/1393 train_time:148496ms step_avg:127.46ms
step:1176/1393 train_time:148631ms step_avg:127.47ms
step:1177/1393 train_time:148767ms step_avg:127.48ms
step:1178/1393 train_time:148900ms step_avg:127.48ms
step:1179/1393 train_time:149033ms step_avg:127.49ms
step:1180/1393 train_time:149168ms step_avg:127.49ms
step:1181/1393 train_time:149302ms step_avg:127.50ms
step:1182/1393 train_time:149435ms step_avg:127.50ms
step:1183/1393 train_time:149569ms step_avg:127.51ms
step:1184/1393 train_time:149702ms step_avg:127.51ms
step:1185/1393 train_time:149836ms step_avg:127.52ms
step:1186/1393 train_time:149970ms step_avg:127.53ms
step:1187/1393 train_time:150108ms step_avg:127.53ms
step:1188/1393 train_time:150242ms step_avg:127.54ms
step:1189/1393 train_time:150376ms step_avg:127.55ms
step:1190/1393 train_time:150509ms step_avg:127.55ms
step:1191/1393 train_time:150642ms step_avg:127.55ms
step:1192/1393 train_time:150775ms step_avg:127.56ms
step:1193/1393 train_time:150908ms step_avg:127.56ms
step:1194/1393 train_time:151041ms step_avg:127.57ms
step:1195/1393 train_time:151175ms step_avg:127.57ms
step:1196/1393 train_time:151309ms step_avg:127.58ms
step:1197/1393 train_time:151443ms step_avg:127.58ms
step:1198/1393 train_time:151578ms step_avg:127.59ms
step:1199/1393 train_time:151711ms step_avg:127.60ms
step:1200/1393 train_time:151844ms step_avg:127.60ms
step:1201/1393 train_time:151976ms step_avg:127.60ms
step:1202/1393 train_time:152114ms step_avg:127.61ms
step:1203/1393 train_time:152251ms step_avg:127.62ms
step:1204/1393 train_time:152384ms step_avg:127.63ms
step:1205/1393 train_time:152520ms step_avg:127.63ms
step:1206/1393 train_time:152654ms step_avg:127.64ms
step:1207/1393 train_time:152787ms step_avg:127.64ms
step:1208/1393 train_time:152921ms step_avg:127.65ms
step:1209/1393 train_time:153055ms step_avg:127.65ms
step:1210/1393 train_time:153190ms step_avg:127.66ms
step:1211/1393 train_time:153324ms step_avg:127.66ms
step:1212/1393 train_time:153457ms step_avg:127.67ms
step:1213/1393 train_time:153590ms step_avg:127.67ms
step:1214/1393 train_time:153724ms step_avg:127.68ms
step:1215/1393 train_time:153859ms step_avg:127.68ms
step:1216/1393 train_time:153992ms step_avg:127.69ms
step:1217/1393 train_time:154126ms step_avg:127.69ms
step:1218/1393 train_time:154259ms step_avg:127.70ms
step:1219/1393 train_time:154393ms step_avg:127.70ms
step:1220/1393 train_time:154527ms step_avg:127.71ms
step:1221/1393 train_time:154660ms step_avg:127.71ms
step:1222/1393 train_time:154793ms step_avg:127.72ms
step:1223/1393 train_time:154926ms step_avg:127.72ms
step:1224/1393 train_time:155060ms step_avg:127.73ms
step:1225/1393 train_time:155197ms step_avg:127.73ms
step:1226/1393 train_time:155330ms step_avg:127.74ms
step:1227/1393 train_time:155464ms step_avg:127.74ms
step:1228/1393 train_time:155598ms step_avg:127.75ms
step:1229/1393 train_time:155731ms step_avg:127.75ms
step:1230/1393 train_time:155867ms step_avg:127.76ms
step:1231/1393 train_time:156003ms step_avg:127.77ms
step:1232/1393 train_time:156138ms step_avg:127.77ms
step:1233/1393 train_time:156272ms step_avg:127.78ms
step:1234/1393 train_time:156406ms step_avg:127.78ms
step:1235/1393 train_time:156539ms step_avg:127.79ms
step:1236/1393 train_time:156674ms step_avg:127.79ms
step:1237/1393 train_time:156807ms step_avg:127.80ms
step:1238/1393 train_time:156944ms step_avg:127.80ms
step:1239/1393 train_time:157077ms step_avg:127.81ms
step:1240/1393 train_time:157211ms step_avg:127.81ms
step:1241/1393 train_time:157347ms step_avg:127.82ms
step:1242/1393 train_time:157481ms step_avg:127.83ms
step:1243/1393 train_time:157617ms step_avg:127.83ms
step:1244/1393 train_time:157751ms step_avg:127.84ms
step:1245/1393 train_time:157884ms step_avg:127.84ms
step:1246/1393 train_time:158018ms step_avg:127.85ms
step:1247/1393 train_time:158152ms step_avg:127.85ms
step:1248/1393 train_time:158285ms step_avg:127.86ms
step:1249/1393 train_time:158418ms step_avg:127.86ms
step:1250/1393 train_time:158551ms step_avg:127.86ms
step:1250/1393 val_loss:3.3183 train_time:158684ms step_avg:127.97ms
step:1251/1393 train_time:158706ms step_avg:127.89ms
step:1252/1393 train_time:158828ms step_avg:127.88ms
step:1253/1393 train_time:158960ms step_avg:127.88ms
step:1254/1393 train_time:159091ms step_avg:127.89ms
step:1255/1393 train_time:159231ms step_avg:127.90ms
step:1256/1393 train_time:159364ms step_avg:127.90ms
step:1257/1393 train_time:159497ms step_avg:127.90ms
step:1258/1393 train_time:159630ms step_avg:127.91ms
step:1259/1393 train_time:159767ms step_avg:127.92ms
step:1260/1393 train_time:159902ms step_avg:127.92ms
step:1261/1393 train_time:160035ms step_avg:127.93ms
step:1262/1393 train_time:160170ms step_avg:127.93ms
step:1263/1393 train_time:160305ms step_avg:127.94ms
step:1264/1393 train_time:160438ms step_avg:127.94ms
step:1265/1393 train_time:160572ms step_avg:127.95ms
step:1266/1393 train_time:160706ms step_avg:127.95ms
step:1267/1393 train_time:160841ms step_avg:127.96ms
step:1268/1393 train_time:160976ms step_avg:127.96ms
step:1269/1393 train_time:161111ms step_avg:127.97ms
step:1270/1393 train_time:161244ms step_avg:127.97ms
step:1271/1393 train_time:161378ms step_avg:127.98ms
step:1272/1393 train_time:161511ms step_avg:127.98ms
step:1273/1393 train_time:161644ms step_avg:127.98ms
step:1274/1393 train_time:161777ms step_avg:127.99ms
step:1275/1393 train_time:161914ms step_avg:128.00ms
step:1276/1393 train_time:162047ms step_avg:128.00ms
step:1277/1393 train_time:162180ms step_avg:128.00ms
step:1278/1393 train_time:162313ms step_avg:128.01ms
step:1279/1393 train_time:162447ms step_avg:128.01ms
step:1280/1393 train_time:162583ms step_avg:128.02ms
step:1281/1393 train_time:162717ms step_avg:128.02ms
step:1282/1393 train_time:162851ms step_avg:128.03ms
step:1283/1393 train_time:162984ms step_avg:128.03ms
step:1284/1393 train_time:163118ms step_avg:128.04ms
step:1285/1393 train_time:163252ms step_avg:128.04ms
step:1286/1393 train_time:163386ms step_avg:128.05ms
step:1287/1393 train_time:163519ms step_avg:128.05ms
step:1288/1393 train_time:163653ms step_avg:128.05ms
step:1289/1393 train_time:163789ms step_avg:128.06ms
step:1290/1393 train_time:163924ms step_avg:128.07ms
step:1291/1393 train_time:164059ms step_avg:128.07ms
step:1292/1393 train_time:164194ms step_avg:128.08ms
step:1293/1393 train_time:164331ms step_avg:128.08ms
step:1294/1393 train_time:164464ms step_avg:128.09ms
step:1295/1393 train_time:164598ms step_avg:128.09ms
step:1296/1393 train_time:164733ms step_avg:128.10ms
step:1297/1393 train_time:164868ms step_avg:128.10ms
step:1298/1393 train_time:165001ms step_avg:128.11ms
step:1299/1393 train_time:165135ms step_avg:128.11ms
step:1300/1393 train_time:165269ms step_avg:128.12ms
step:1301/1393 train_time:165402ms step_avg:128.12ms
step:1302/1393 train_time:165535ms step_avg:128.12ms
step:1303/1393 train_time:165670ms step_avg:128.13ms
step:1304/1393 train_time:165805ms step_avg:128.13ms
step:1305/1393 train_time:165939ms step_avg:128.14ms
step:1306/1393 train_time:166074ms step_avg:128.14ms
step:1307/1393 train_time:166208ms step_avg:128.15ms
step:1308/1393 train_time:166342ms step_avg:128.15ms
step:1309/1393 train_time:166476ms step_avg:128.16ms
step:1310/1393 train_time:166611ms step_avg:128.16ms
step:1311/1393 train_time:166743ms step_avg:128.17ms
step:1312/1393 train_time:166876ms step_avg:128.17ms
step:1313/1393 train_time:167011ms step_avg:128.17ms
step:1314/1393 train_time:167145ms step_avg:128.18ms
step:1315/1393 train_time:167280ms step_avg:128.18ms
step:1316/1393 train_time:167413ms step_avg:128.19ms
step:1317/1393 train_time:167547ms step_avg:128.19ms
step:1318/1393 train_time:167681ms step_avg:128.20ms
step:1319/1393 train_time:167817ms step_avg:128.20ms
step:1320/1393 train_time:167950ms step_avg:128.21ms
step:1321/1393 train_time:168085ms step_avg:128.21ms
step:1322/1393 train_time:168223ms step_avg:128.22ms
step:1323/1393 train_time:168356ms step_avg:128.22ms
step:1324/1393 train_time:168489ms step_avg:128.23ms
step:1325/1393 train_time:168623ms step_avg:128.23ms
step:1326/1393 train_time:168757ms step_avg:128.24ms
step:1327/1393 train_time:168891ms step_avg:128.24ms
step:1328/1393 train_time:169025ms step_avg:128.24ms
step:1329/1393 train_time:169162ms step_avg:128.25ms
step:1330/1393 train_time:169297ms step_avg:128.26ms
step:1331/1393 train_time:169434ms step_avg:128.26ms
step:1332/1393 train_time:169571ms step_avg:128.27ms
step:1333/1393 train_time:169705ms step_avg:128.27ms
step:1334/1393 train_time:169838ms step_avg:128.28ms
step:1335/1393 train_time:169971ms step_avg:128.28ms
step:1336/1393 train_time:170109ms step_avg:128.29ms
step:1337/1393 train_time:170244ms step_avg:128.29ms
step:1338/1393 train_time:170377ms step_avg:128.30ms
step:1339/1393 train_time:170511ms step_avg:128.30ms
step:1340/1393 train_time:170646ms step_avg:128.31ms
step:1341/1393 train_time:170779ms step_avg:128.31ms
step:1342/1393 train_time:170913ms step_avg:128.31ms
step:1343/1393 train_time:171048ms step_avg:128.32ms
step:1344/1393 train_time:171182ms step_avg:128.32ms
step:1345/1393 train_time:171318ms step_avg:128.33ms
step:1346/1393 train_time:171452ms step_avg:128.33ms
step:1347/1393 train_time:171589ms step_avg:128.34ms
step:1348/1393 train_time:171722ms step_avg:128.34ms
step:1349/1393 train_time:171858ms step_avg:128.35ms
step:1350/1393 train_time:171991ms step_avg:128.35ms
step:1351/1393 train_time:172125ms step_avg:128.36ms
step:1352/1393 train_time:172264ms step_avg:128.36ms
step:1353/1393 train_time:172401ms step_avg:128.37ms
step:1354/1393 train_time:172536ms step_avg:128.37ms
step:1355/1393 train_time:172670ms step_avg:128.38ms
step:1356/1393 train_time:172803ms step_avg:128.38ms
step:1357/1393 train_time:172938ms step_avg:128.39ms
step:1358/1393 train_time:173075ms step_avg:128.39ms
step:1359/1393 train_time:173208ms step_avg:128.40ms
step:1360/1393 train_time:173344ms step_avg:128.40ms
step:1361/1393 train_time:173479ms step_avg:128.41ms
step:1362/1393 train_time:173616ms step_avg:128.41ms
step:1363/1393 train_time:173753ms step_avg:128.42ms
step:1364/1393 train_time:173889ms step_avg:128.43ms
step:1365/1393 train_time:174022ms step_avg:128.43ms
step:1366/1393 train_time:174157ms step_avg:128.43ms
step:1367/1393 train_time:174292ms step_avg:128.44ms
step:1368/1393 train_time:174427ms step_avg:128.44ms
step:1369/1393 train_time:174565ms step_avg:128.45ms
step:1370/1393 train_time:174703ms step_avg:128.46ms
step:1371/1393 train_time:174838ms step_avg:128.46ms
step:1372/1393 train_time:174976ms step_avg:128.47ms
step:1373/1393 train_time:175109ms step_avg:128.47ms
step:1374/1393 train_time:175247ms step_avg:128.48ms
step:1375/1393 train_time:175381ms step_avg:128.48ms
step:1375/1393 val_loss:3.2845 train_time:175514ms step_avg:128.58ms
step:1376/1393 train_time:175536ms step_avg:128.50ms
step:1377/1393 train_time:175661ms step_avg:128.50ms
step:1378/1393 train_time:175797ms step_avg:128.51ms
step:1379/1393 train_time:175931ms step_avg:128.51ms
step:1380/1393 train_time:176065ms step_avg:128.51ms
step:1381/1393 train_time:176201ms step_avg:128.52ms
step:1382/1393 train_time:176336ms step_avg:128.52ms
step:1383/1393 train_time:176471ms step_avg:128.53ms
step:1384/1393 train_time:176610ms step_avg:128.54ms
step:1385/1393 train_time:176744ms step_avg:128.54ms
step:1386/1393 train_time:176879ms step_avg:128.55ms
step:1387/1393 train_time:177014ms step_avg:128.55ms
step:1388/1393 train_time:177149ms step_avg:128.55ms
step:1389/1393 train_time:177284ms step_avg:128.56ms
step:1390/1393 train_time:177419ms step_avg:128.56ms
step:1391/1393 train_time:177553ms step_avg:128.57ms
step:1392/1393 train_time:177689ms step_avg:128.57ms
step:1393/1393 train_time:177823ms step_avg:128.58ms
step:1393/1393 val_loss:3.2808 train_time:177956ms step_avg:128.67ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
