import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:54:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:24034ms step_avg:nanms
step:2/1393 train_time:24433ms step_avg:nanms
step:3/1393 train_time:24554ms step_avg:nanms
step:4/1393 train_time:24673ms step_avg:nanms
step:5/1393 train_time:24794ms step_avg:nanms
step:6/1393 train_time:24914ms step_avg:nanms
step:7/1393 train_time:25035ms step_avg:nanms
step:8/1393 train_time:25156ms step_avg:nanms
step:9/1393 train_time:25279ms step_avg:nanms
step:10/1393 train_time:25400ms step_avg:nanms
step:11/1393 train_time:122ms step_avg:nanms
step:12/1393 train_time:243ms step_avg:nanms
step:13/1393 train_time:365ms step_avg:121.72ms
step:14/1393 train_time:486ms step_avg:121.44ms
step:15/1393 train_time:607ms step_avg:121.45ms
step:16/1393 train_time:728ms step_avg:121.37ms
step:17/1393 train_time:849ms step_avg:121.32ms
step:18/1393 train_time:971ms step_avg:121.41ms
step:19/1393 train_time:1093ms step_avg:121.42ms
step:20/1393 train_time:1214ms step_avg:121.42ms
step:21/1393 train_time:1335ms step_avg:121.38ms
step:22/1393 train_time:1457ms step_avg:121.38ms
step:23/1393 train_time:1578ms step_avg:121.39ms
step:24/1393 train_time:1699ms step_avg:121.36ms
step:25/1393 train_time:1821ms step_avg:121.38ms
step:26/1393 train_time:1942ms step_avg:121.39ms
step:27/1393 train_time:2065ms step_avg:121.45ms
step:28/1393 train_time:2185ms step_avg:121.41ms
step:29/1393 train_time:2307ms step_avg:121.43ms
step:30/1393 train_time:2428ms step_avg:121.42ms
step:31/1393 train_time:2550ms step_avg:121.42ms
step:32/1393 train_time:2671ms step_avg:121.42ms
step:33/1393 train_time:2793ms step_avg:121.45ms
step:34/1393 train_time:2916ms step_avg:121.49ms
step:35/1393 train_time:3037ms step_avg:121.48ms
step:36/1393 train_time:3159ms step_avg:121.50ms
step:37/1393 train_time:3282ms step_avg:121.55ms
step:38/1393 train_time:3404ms step_avg:121.57ms
step:39/1393 train_time:3524ms step_avg:121.53ms
step:40/1393 train_time:3646ms step_avg:121.52ms
step:41/1393 train_time:3767ms step_avg:121.52ms
step:42/1393 train_time:3890ms step_avg:121.56ms
step:43/1393 train_time:4011ms step_avg:121.55ms
step:44/1393 train_time:4133ms step_avg:121.54ms
step:45/1393 train_time:4254ms step_avg:121.54ms
step:46/1393 train_time:4375ms step_avg:121.52ms
step:47/1393 train_time:4496ms step_avg:121.52ms
step:48/1393 train_time:4617ms step_avg:121.51ms
step:49/1393 train_time:4740ms step_avg:121.53ms
step:50/1393 train_time:4860ms step_avg:121.51ms
step:51/1393 train_time:4983ms step_avg:121.53ms
step:52/1393 train_time:5103ms step_avg:121.51ms
step:53/1393 train_time:5226ms step_avg:121.54ms
step:54/1393 train_time:5347ms step_avg:121.52ms
step:55/1393 train_time:5468ms step_avg:121.52ms
step:56/1393 train_time:5589ms step_avg:121.50ms
step:57/1393 train_time:5712ms step_avg:121.53ms
step:58/1393 train_time:5834ms step_avg:121.54ms
step:59/1393 train_time:5955ms step_avg:121.52ms
step:60/1393 train_time:6076ms step_avg:121.52ms
step:61/1393 train_time:6199ms step_avg:121.55ms
step:62/1393 train_time:6321ms step_avg:121.57ms
step:63/1393 train_time:6442ms step_avg:121.55ms
step:64/1393 train_time:6563ms step_avg:121.53ms
step:65/1393 train_time:6684ms step_avg:121.53ms
step:66/1393 train_time:6806ms step_avg:121.53ms
step:67/1393 train_time:6927ms step_avg:121.52ms
step:68/1393 train_time:7049ms step_avg:121.53ms
step:69/1393 train_time:7172ms step_avg:121.56ms
step:70/1393 train_time:7293ms step_avg:121.54ms
step:71/1393 train_time:7415ms step_avg:121.55ms
step:72/1393 train_time:7537ms step_avg:121.56ms
step:73/1393 train_time:7658ms step_avg:121.55ms
step:74/1393 train_time:7780ms step_avg:121.56ms
step:75/1393 train_time:7902ms step_avg:121.56ms
step:76/1393 train_time:8023ms step_avg:121.56ms
step:77/1393 train_time:8146ms step_avg:121.58ms
step:78/1393 train_time:8268ms step_avg:121.58ms
step:79/1393 train_time:8390ms step_avg:121.59ms
step:80/1393 train_time:8512ms step_avg:121.60ms
step:81/1393 train_time:8634ms step_avg:121.61ms
step:82/1393 train_time:8755ms step_avg:121.60ms
step:83/1393 train_time:8877ms step_avg:121.60ms
step:84/1393 train_time:8998ms step_avg:121.60ms
step:85/1393 train_time:9120ms step_avg:121.60ms
step:86/1393 train_time:9241ms step_avg:121.59ms
step:87/1393 train_time:9363ms step_avg:121.60ms
step:88/1393 train_time:9485ms step_avg:121.60ms
step:89/1393 train_time:9608ms step_avg:121.62ms
step:90/1393 train_time:9729ms step_avg:121.62ms
step:91/1393 train_time:9851ms step_avg:121.62ms
step:92/1393 train_time:9971ms step_avg:121.60ms
step:93/1393 train_time:10092ms step_avg:121.59ms
step:94/1393 train_time:10214ms step_avg:121.59ms
step:95/1393 train_time:10335ms step_avg:121.59ms
step:96/1393 train_time:10456ms step_avg:121.58ms
step:97/1393 train_time:10577ms step_avg:121.58ms
step:98/1393 train_time:10698ms step_avg:121.57ms
step:99/1393 train_time:10820ms step_avg:121.57ms
step:100/1393 train_time:10941ms step_avg:121.56ms
step:101/1393 train_time:11063ms step_avg:121.57ms
step:102/1393 train_time:11184ms step_avg:121.57ms
step:103/1393 train_time:11306ms step_avg:121.57ms
step:104/1393 train_time:11427ms step_avg:121.57ms
step:105/1393 train_time:11551ms step_avg:121.59ms
step:106/1393 train_time:11673ms step_avg:121.60ms
step:107/1393 train_time:11795ms step_avg:121.60ms
step:108/1393 train_time:11917ms step_avg:121.60ms
step:109/1393 train_time:12040ms step_avg:121.62ms
step:110/1393 train_time:12163ms step_avg:121.63ms
step:111/1393 train_time:12284ms step_avg:121.63ms
step:112/1393 train_time:12407ms step_avg:121.63ms
step:113/1393 train_time:12528ms step_avg:121.63ms
step:114/1393 train_time:12651ms step_avg:121.64ms
step:115/1393 train_time:12774ms step_avg:121.66ms
step:116/1393 train_time:12895ms step_avg:121.65ms
step:117/1393 train_time:13017ms step_avg:121.66ms
step:118/1393 train_time:13139ms step_avg:121.66ms
step:119/1393 train_time:13261ms step_avg:121.66ms
step:120/1393 train_time:13384ms step_avg:121.67ms
step:121/1393 train_time:13507ms step_avg:121.68ms
step:122/1393 train_time:13629ms step_avg:121.69ms
step:123/1393 train_time:13751ms step_avg:121.69ms
step:124/1393 train_time:13873ms step_avg:121.69ms
step:125/1393 train_time:13995ms step_avg:121.69ms
step:125/1393 val_loss:4.4012 train_time:14115ms step_avg:122.74ms
step:126/1393 train_time:14137ms step_avg:121.87ms
step:127/1393 train_time:14241ms step_avg:121.72ms
step:128/1393 train_time:14374ms step_avg:121.82ms
step:129/1393 train_time:14499ms step_avg:121.84ms
step:130/1393 train_time:14621ms step_avg:121.84ms
step:131/1393 train_time:14742ms step_avg:121.84ms
step:132/1393 train_time:14864ms step_avg:121.83ms
step:133/1393 train_time:14985ms step_avg:121.83ms
step:134/1393 train_time:15106ms step_avg:121.82ms
step:135/1393 train_time:15227ms step_avg:121.82ms
step:136/1393 train_time:15351ms step_avg:121.84ms
step:137/1393 train_time:15474ms step_avg:121.84ms
step:138/1393 train_time:15596ms step_avg:121.85ms
step:139/1393 train_time:15718ms step_avg:121.85ms
step:140/1393 train_time:15840ms step_avg:121.85ms
step:141/1393 train_time:15962ms step_avg:121.85ms
step:142/1393 train_time:16084ms step_avg:121.85ms
step:143/1393 train_time:16206ms step_avg:121.85ms
step:144/1393 train_time:16330ms step_avg:121.86ms
step:145/1393 train_time:16452ms step_avg:121.87ms
step:146/1393 train_time:16575ms step_avg:121.87ms
step:147/1393 train_time:16697ms step_avg:121.87ms
step:148/1393 train_time:16819ms step_avg:121.88ms
step:149/1393 train_time:16941ms step_avg:121.88ms
step:150/1393 train_time:17062ms step_avg:121.87ms
step:151/1393 train_time:17184ms step_avg:121.88ms
step:152/1393 train_time:17307ms step_avg:121.88ms
step:153/1393 train_time:17430ms step_avg:121.89ms
step:154/1393 train_time:17552ms step_avg:121.89ms
step:155/1393 train_time:17673ms step_avg:121.89ms
step:156/1393 train_time:17796ms step_avg:121.89ms
step:157/1393 train_time:17918ms step_avg:121.89ms
step:158/1393 train_time:18040ms step_avg:121.89ms
step:159/1393 train_time:18162ms step_avg:121.89ms
step:160/1393 train_time:18285ms step_avg:121.90ms
step:161/1393 train_time:18408ms step_avg:121.91ms
step:162/1393 train_time:18531ms step_avg:121.91ms
step:163/1393 train_time:18654ms step_avg:121.92ms
step:164/1393 train_time:18776ms step_avg:121.92ms
step:165/1393 train_time:18898ms step_avg:121.92ms
step:166/1393 train_time:19020ms step_avg:121.92ms
step:167/1393 train_time:19141ms step_avg:121.92ms
step:168/1393 train_time:19264ms step_avg:121.92ms
step:169/1393 train_time:19385ms step_avg:121.92ms
step:170/1393 train_time:19507ms step_avg:121.92ms
step:171/1393 train_time:19629ms step_avg:121.92ms
step:172/1393 train_time:19751ms step_avg:121.92ms
step:173/1393 train_time:19874ms step_avg:121.93ms
step:174/1393 train_time:19997ms step_avg:121.93ms
step:175/1393 train_time:20119ms step_avg:121.93ms
step:176/1393 train_time:20242ms step_avg:121.94ms
step:177/1393 train_time:20365ms step_avg:121.94ms
step:178/1393 train_time:20487ms step_avg:121.95ms
step:179/1393 train_time:20608ms step_avg:121.94ms
step:180/1393 train_time:20731ms step_avg:121.95ms
step:181/1393 train_time:20852ms step_avg:121.94ms
step:182/1393 train_time:20974ms step_avg:121.94ms
step:183/1393 train_time:21097ms step_avg:121.95ms
step:184/1393 train_time:21220ms step_avg:121.95ms
step:185/1393 train_time:21343ms step_avg:121.96ms
step:186/1393 train_time:21465ms step_avg:121.96ms
step:187/1393 train_time:21587ms step_avg:121.96ms
step:188/1393 train_time:21710ms step_avg:121.96ms
step:189/1393 train_time:21831ms step_avg:121.96ms
step:190/1393 train_time:21954ms step_avg:121.96ms
step:191/1393 train_time:22076ms step_avg:121.97ms
step:192/1393 train_time:22197ms step_avg:121.96ms
step:193/1393 train_time:22320ms step_avg:121.97ms
step:194/1393 train_time:22442ms step_avg:121.97ms
step:195/1393 train_time:22564ms step_avg:121.97ms
step:196/1393 train_time:22686ms step_avg:121.97ms
step:197/1393 train_time:22809ms step_avg:121.97ms
step:198/1393 train_time:22931ms step_avg:121.97ms
step:199/1393 train_time:23053ms step_avg:121.97ms
step:200/1393 train_time:23174ms step_avg:121.97ms
step:201/1393 train_time:23296ms step_avg:121.97ms
step:202/1393 train_time:23418ms step_avg:121.97ms
step:203/1393 train_time:23541ms step_avg:121.97ms
step:204/1393 train_time:23663ms step_avg:121.97ms
step:205/1393 train_time:23785ms step_avg:121.98ms
step:206/1393 train_time:23908ms step_avg:121.98ms
step:207/1393 train_time:24029ms step_avg:121.98ms
step:208/1393 train_time:24152ms step_avg:121.98ms
step:209/1393 train_time:24275ms step_avg:121.99ms
step:210/1393 train_time:24397ms step_avg:121.99ms
step:211/1393 train_time:24520ms step_avg:121.99ms
step:212/1393 train_time:24642ms step_avg:121.99ms
step:213/1393 train_time:24766ms step_avg:122.00ms
step:214/1393 train_time:24888ms step_avg:122.00ms
step:215/1393 train_time:25010ms step_avg:122.00ms
step:216/1393 train_time:25133ms step_avg:122.00ms
step:217/1393 train_time:25255ms step_avg:122.01ms
step:218/1393 train_time:25379ms step_avg:122.01ms
step:219/1393 train_time:25501ms step_avg:122.01ms
step:220/1393 train_time:25623ms step_avg:122.01ms
step:221/1393 train_time:25745ms step_avg:122.02ms
step:222/1393 train_time:25868ms step_avg:122.02ms
step:223/1393 train_time:25991ms step_avg:122.02ms
step:224/1393 train_time:26114ms step_avg:122.03ms
step:225/1393 train_time:26236ms step_avg:122.03ms
step:226/1393 train_time:26360ms step_avg:122.04ms
step:227/1393 train_time:26484ms step_avg:122.05ms
step:228/1393 train_time:26608ms step_avg:122.05ms
step:229/1393 train_time:26731ms step_avg:122.06ms
step:230/1393 train_time:26855ms step_avg:122.07ms
step:231/1393 train_time:26977ms step_avg:122.07ms
step:232/1393 train_time:27099ms step_avg:122.07ms
step:233/1393 train_time:27222ms step_avg:122.07ms
step:234/1393 train_time:27344ms step_avg:122.07ms
step:235/1393 train_time:27466ms step_avg:122.07ms
step:236/1393 train_time:27589ms step_avg:122.08ms
step:237/1393 train_time:27712ms step_avg:122.08ms
step:238/1393 train_time:27835ms step_avg:122.08ms
step:239/1393 train_time:27957ms step_avg:122.08ms
step:240/1393 train_time:28080ms step_avg:122.09ms
step:241/1393 train_time:28202ms step_avg:122.09ms
step:242/1393 train_time:28325ms step_avg:122.09ms
step:243/1393 train_time:28448ms step_avg:122.09ms
step:244/1393 train_time:28571ms step_avg:122.10ms
step:245/1393 train_time:28693ms step_avg:122.10ms
step:246/1393 train_time:28816ms step_avg:122.10ms
step:247/1393 train_time:28939ms step_avg:122.11ms
step:248/1393 train_time:29063ms step_avg:122.11ms
step:249/1393 train_time:29185ms step_avg:122.11ms
step:250/1393 train_time:29308ms step_avg:122.12ms
step:250/1393 val_loss:3.9817 train_time:29428ms step_avg:122.62ms
step:251/1393 train_time:29449ms step_avg:122.20ms
step:252/1393 train_time:29560ms step_avg:122.15ms
step:253/1393 train_time:29685ms step_avg:122.16ms
step:254/1393 train_time:29807ms step_avg:122.16ms
step:255/1393 train_time:29931ms step_avg:122.17ms
step:256/1393 train_time:30052ms step_avg:122.16ms
step:257/1393 train_time:30175ms step_avg:122.17ms
step:258/1393 train_time:30297ms step_avg:122.16ms
step:259/1393 train_time:30420ms step_avg:122.17ms
step:260/1393 train_time:30544ms step_avg:122.17ms
step:261/1393 train_time:30668ms step_avg:122.18ms
step:262/1393 train_time:30790ms step_avg:122.18ms
step:263/1393 train_time:30913ms step_avg:122.19ms
step:264/1393 train_time:31035ms step_avg:122.19ms
step:265/1393 train_time:31158ms step_avg:122.19ms
step:266/1393 train_time:31279ms step_avg:122.18ms
step:267/1393 train_time:31402ms step_avg:122.19ms
step:268/1393 train_time:31525ms step_avg:122.19ms
step:269/1393 train_time:31649ms step_avg:122.20ms
step:270/1393 train_time:31771ms step_avg:122.20ms
step:271/1393 train_time:31894ms step_avg:122.20ms
step:272/1393 train_time:32016ms step_avg:122.20ms
step:273/1393 train_time:32138ms step_avg:122.20ms
step:274/1393 train_time:32260ms step_avg:122.20ms
step:275/1393 train_time:32383ms step_avg:122.20ms
step:276/1393 train_time:32507ms step_avg:122.21ms
step:277/1393 train_time:32630ms step_avg:122.21ms
step:278/1393 train_time:32753ms step_avg:122.21ms
step:279/1393 train_time:32875ms step_avg:122.21ms
step:280/1393 train_time:32997ms step_avg:122.21ms
step:281/1393 train_time:33120ms step_avg:122.21ms
step:282/1393 train_time:33242ms step_avg:122.21ms
step:283/1393 train_time:33366ms step_avg:122.22ms
step:284/1393 train_time:33488ms step_avg:122.22ms
step:285/1393 train_time:33611ms step_avg:122.22ms
step:286/1393 train_time:33733ms step_avg:122.22ms
step:287/1393 train_time:33855ms step_avg:122.22ms
step:288/1393 train_time:33978ms step_avg:122.22ms
step:289/1393 train_time:34100ms step_avg:122.22ms
step:290/1393 train_time:34222ms step_avg:122.22ms
step:291/1393 train_time:34345ms step_avg:122.23ms
step:292/1393 train_time:34469ms step_avg:122.23ms
step:293/1393 train_time:34592ms step_avg:122.23ms
step:294/1393 train_time:34714ms step_avg:122.23ms
step:295/1393 train_time:34837ms step_avg:122.23ms
step:296/1393 train_time:34959ms step_avg:122.24ms
step:297/1393 train_time:35083ms step_avg:122.24ms
step:298/1393 train_time:35205ms step_avg:122.24ms
step:299/1393 train_time:35328ms step_avg:122.24ms
step:300/1393 train_time:35451ms step_avg:122.24ms
step:301/1393 train_time:35573ms step_avg:122.24ms
step:302/1393 train_time:35695ms step_avg:122.24ms
step:303/1393 train_time:35818ms step_avg:122.25ms
step:304/1393 train_time:35941ms step_avg:122.25ms
step:305/1393 train_time:36065ms step_avg:122.25ms
step:306/1393 train_time:36187ms step_avg:122.25ms
step:307/1393 train_time:36310ms step_avg:122.26ms
step:308/1393 train_time:36432ms step_avg:122.26ms
step:309/1393 train_time:36555ms step_avg:122.26ms
step:310/1393 train_time:36678ms step_avg:122.26ms
step:311/1393 train_time:36800ms step_avg:122.26ms
step:312/1393 train_time:36925ms step_avg:122.27ms
step:313/1393 train_time:37051ms step_avg:122.28ms
step:314/1393 train_time:37175ms step_avg:122.29ms
step:315/1393 train_time:37300ms step_avg:122.30ms
step:316/1393 train_time:37426ms step_avg:122.31ms
step:317/1393 train_time:37551ms step_avg:122.31ms
step:318/1393 train_time:37675ms step_avg:122.32ms
step:319/1393 train_time:37800ms step_avg:122.33ms
step:320/1393 train_time:37925ms step_avg:122.34ms
step:321/1393 train_time:38051ms step_avg:122.35ms
step:322/1393 train_time:38176ms step_avg:122.36ms
step:323/1393 train_time:38301ms step_avg:122.37ms
step:324/1393 train_time:38426ms step_avg:122.38ms
step:325/1393 train_time:38552ms step_avg:122.39ms
step:326/1393 train_time:38677ms step_avg:122.40ms
step:327/1393 train_time:38803ms step_avg:122.41ms
step:328/1393 train_time:38929ms step_avg:122.42ms
step:329/1393 train_time:39054ms step_avg:122.43ms
step:330/1393 train_time:39179ms step_avg:122.43ms
step:331/1393 train_time:39304ms step_avg:122.44ms
step:332/1393 train_time:39430ms step_avg:122.45ms
step:333/1393 train_time:39556ms step_avg:122.46ms
step:334/1393 train_time:39681ms step_avg:122.47ms
step:335/1393 train_time:39806ms step_avg:122.48ms
step:336/1393 train_time:39931ms step_avg:122.49ms
step:337/1393 train_time:40057ms step_avg:122.50ms
step:338/1393 train_time:40182ms step_avg:122.51ms
step:339/1393 train_time:40308ms step_avg:122.52ms
step:340/1393 train_time:40434ms step_avg:122.53ms
step:341/1393 train_time:40559ms step_avg:122.53ms
step:342/1393 train_time:40683ms step_avg:122.54ms
step:343/1393 train_time:40808ms step_avg:122.55ms
step:344/1393 train_time:40934ms step_avg:122.56ms
step:345/1393 train_time:41059ms step_avg:122.56ms
step:346/1393 train_time:41184ms step_avg:122.57ms
step:347/1393 train_time:41309ms step_avg:122.58ms
step:348/1393 train_time:41435ms step_avg:122.59ms
step:349/1393 train_time:41560ms step_avg:122.60ms
step:350/1393 train_time:41685ms step_avg:122.60ms
step:351/1393 train_time:41811ms step_avg:122.61ms
step:352/1393 train_time:41936ms step_avg:122.62ms
step:353/1393 train_time:42061ms step_avg:122.63ms
step:354/1393 train_time:42186ms step_avg:122.63ms
step:355/1393 train_time:42312ms step_avg:122.64ms
step:356/1393 train_time:42438ms step_avg:122.65ms
step:357/1393 train_time:42563ms step_avg:122.66ms
step:358/1393 train_time:42688ms step_avg:122.67ms
step:359/1393 train_time:42813ms step_avg:122.67ms
step:360/1393 train_time:42938ms step_avg:122.68ms
step:361/1393 train_time:43063ms step_avg:122.69ms
step:362/1393 train_time:43188ms step_avg:122.69ms
step:363/1393 train_time:43314ms step_avg:122.70ms
step:364/1393 train_time:43440ms step_avg:122.71ms
step:365/1393 train_time:43565ms step_avg:122.72ms
step:366/1393 train_time:43691ms step_avg:122.73ms
step:367/1393 train_time:43816ms step_avg:122.73ms
step:368/1393 train_time:43940ms step_avg:122.74ms
step:369/1393 train_time:44066ms step_avg:122.75ms
step:370/1393 train_time:44192ms step_avg:122.75ms
step:371/1393 train_time:44317ms step_avg:122.76ms
step:372/1393 train_time:44442ms step_avg:122.77ms
step:373/1393 train_time:44568ms step_avg:122.78ms
step:374/1393 train_time:44693ms step_avg:122.78ms
step:375/1393 train_time:44819ms step_avg:122.79ms
step:375/1393 val_loss:3.7827 train_time:44942ms step_avg:123.13ms
step:376/1393 train_time:44963ms step_avg:122.85ms
step:377/1393 train_time:45079ms step_avg:122.83ms
step:378/1393 train_time:45205ms step_avg:122.84ms
step:379/1393 train_time:45329ms step_avg:122.84ms
step:380/1393 train_time:45453ms step_avg:122.85ms
step:381/1393 train_time:45578ms step_avg:122.85ms
step:382/1393 train_time:45703ms step_avg:122.86ms
step:383/1393 train_time:45826ms step_avg:122.86ms
step:384/1393 train_time:45952ms step_avg:122.87ms
step:385/1393 train_time:46081ms step_avg:122.88ms
step:386/1393 train_time:46208ms step_avg:122.89ms
step:387/1393 train_time:46332ms step_avg:122.90ms
step:388/1393 train_time:46457ms step_avg:122.90ms
step:389/1393 train_time:46581ms step_avg:122.91ms
step:390/1393 train_time:46706ms step_avg:122.91ms
step:391/1393 train_time:46831ms step_avg:122.91ms
step:392/1393 train_time:46956ms step_avg:122.92ms
step:393/1393 train_time:47082ms step_avg:122.93ms
step:394/1393 train_time:47208ms step_avg:122.94ms
step:395/1393 train_time:47333ms step_avg:122.94ms
step:396/1393 train_time:47459ms step_avg:122.95ms
step:397/1393 train_time:47583ms step_avg:122.95ms
step:398/1393 train_time:47708ms step_avg:122.96ms
step:399/1393 train_time:47834ms step_avg:122.97ms
step:400/1393 train_time:47959ms step_avg:122.97ms
step:401/1393 train_time:48086ms step_avg:122.98ms
step:402/1393 train_time:48211ms step_avg:122.99ms
step:403/1393 train_time:48336ms step_avg:122.99ms
step:404/1393 train_time:48462ms step_avg:123.00ms
step:405/1393 train_time:48587ms step_avg:123.00ms
step:406/1393 train_time:48712ms step_avg:123.01ms
step:407/1393 train_time:48836ms step_avg:123.01ms
step:408/1393 train_time:48962ms step_avg:123.02ms
step:409/1393 train_time:49087ms step_avg:123.03ms
step:410/1393 train_time:49212ms step_avg:123.03ms
step:411/1393 train_time:49338ms step_avg:123.04ms
step:412/1393 train_time:49463ms step_avg:123.04ms
step:413/1393 train_time:49588ms step_avg:123.05ms
step:414/1393 train_time:49714ms step_avg:123.05ms
step:415/1393 train_time:49839ms step_avg:123.06ms
step:416/1393 train_time:49965ms step_avg:123.07ms
step:417/1393 train_time:50090ms step_avg:123.07ms
step:418/1393 train_time:50216ms step_avg:123.08ms
step:419/1393 train_time:50341ms step_avg:123.08ms
step:420/1393 train_time:50467ms step_avg:123.09ms
step:421/1393 train_time:50593ms step_avg:123.10ms
step:422/1393 train_time:50720ms step_avg:123.11ms
step:423/1393 train_time:50846ms step_avg:123.11ms
step:424/1393 train_time:50972ms step_avg:123.12ms
step:425/1393 train_time:51097ms step_avg:123.12ms
step:426/1393 train_time:51222ms step_avg:123.13ms
step:427/1393 train_time:51349ms step_avg:123.14ms
step:428/1393 train_time:51475ms step_avg:123.15ms
step:429/1393 train_time:51601ms step_avg:123.15ms
step:430/1393 train_time:51727ms step_avg:123.16ms
step:431/1393 train_time:51852ms step_avg:123.16ms
step:432/1393 train_time:51978ms step_avg:123.17ms
step:433/1393 train_time:52103ms step_avg:123.18ms
step:434/1393 train_time:52229ms step_avg:123.18ms
step:435/1393 train_time:52355ms step_avg:123.19ms
step:436/1393 train_time:52481ms step_avg:123.19ms
step:437/1393 train_time:52606ms step_avg:123.20ms
step:438/1393 train_time:52732ms step_avg:123.21ms
step:439/1393 train_time:52858ms step_avg:123.21ms
step:440/1393 train_time:52985ms step_avg:123.22ms
step:441/1393 train_time:53111ms step_avg:123.23ms
step:442/1393 train_time:53237ms step_avg:123.23ms
step:443/1393 train_time:53362ms step_avg:123.24ms
step:444/1393 train_time:53487ms step_avg:123.24ms
step:445/1393 train_time:53613ms step_avg:123.25ms
step:446/1393 train_time:53739ms step_avg:123.25ms
step:447/1393 train_time:53864ms step_avg:123.26ms
step:448/1393 train_time:53991ms step_avg:123.27ms
step:449/1393 train_time:54117ms step_avg:123.27ms
step:450/1393 train_time:54243ms step_avg:123.28ms
step:451/1393 train_time:54367ms step_avg:123.28ms
step:452/1393 train_time:54494ms step_avg:123.29ms
step:453/1393 train_time:54620ms step_avg:123.30ms
step:454/1393 train_time:54747ms step_avg:123.30ms
step:455/1393 train_time:54872ms step_avg:123.31ms
step:456/1393 train_time:54998ms step_avg:123.31ms
step:457/1393 train_time:55123ms step_avg:123.32ms
step:458/1393 train_time:55249ms step_avg:123.32ms
step:459/1393 train_time:55373ms step_avg:123.33ms
step:460/1393 train_time:55499ms step_avg:123.33ms
step:461/1393 train_time:55624ms step_avg:123.34ms
step:462/1393 train_time:55751ms step_avg:123.34ms
step:463/1393 train_time:55876ms step_avg:123.35ms
step:464/1393 train_time:56002ms step_avg:123.35ms
step:465/1393 train_time:56128ms step_avg:123.36ms
step:466/1393 train_time:56255ms step_avg:123.37ms
step:467/1393 train_time:56380ms step_avg:123.37ms
step:468/1393 train_time:56506ms step_avg:123.38ms
step:469/1393 train_time:56632ms step_avg:123.38ms
step:470/1393 train_time:56758ms step_avg:123.39ms
step:471/1393 train_time:56883ms step_avg:123.39ms
step:472/1393 train_time:57010ms step_avg:123.40ms
step:473/1393 train_time:57135ms step_avg:123.40ms
step:474/1393 train_time:57261ms step_avg:123.41ms
step:475/1393 train_time:57386ms step_avg:123.41ms
step:476/1393 train_time:57512ms step_avg:123.42ms
step:477/1393 train_time:57637ms step_avg:123.42ms
step:478/1393 train_time:57763ms step_avg:123.43ms
step:479/1393 train_time:57889ms step_avg:123.43ms
step:480/1393 train_time:58014ms step_avg:123.43ms
step:481/1393 train_time:58139ms step_avg:123.44ms
step:482/1393 train_time:58265ms step_avg:123.44ms
step:483/1393 train_time:58390ms step_avg:123.45ms
step:484/1393 train_time:58516ms step_avg:123.45ms
step:485/1393 train_time:58641ms step_avg:123.46ms
step:486/1393 train_time:58768ms step_avg:123.46ms
step:487/1393 train_time:58895ms step_avg:123.47ms
step:488/1393 train_time:59020ms step_avg:123.47ms
step:489/1393 train_time:59146ms step_avg:123.48ms
step:490/1393 train_time:59272ms step_avg:123.48ms
step:491/1393 train_time:59397ms step_avg:123.49ms
step:492/1393 train_time:59523ms step_avg:123.49ms
step:493/1393 train_time:59649ms step_avg:123.50ms
step:494/1393 train_time:59775ms step_avg:123.50ms
step:495/1393 train_time:59901ms step_avg:123.51ms
step:496/1393 train_time:60028ms step_avg:123.51ms
step:497/1393 train_time:60154ms step_avg:123.52ms
step:498/1393 train_time:60280ms step_avg:123.52ms
step:499/1393 train_time:60406ms step_avg:123.53ms
step:500/1393 train_time:60531ms step_avg:123.53ms
step:500/1393 val_loss:3.6638 train_time:60655ms step_avg:123.79ms
step:501/1393 train_time:60677ms step_avg:123.58ms
step:502/1393 train_time:60792ms step_avg:123.56ms
step:503/1393 train_time:60919ms step_avg:123.57ms
step:504/1393 train_time:61045ms step_avg:123.57ms
step:505/1393 train_time:61169ms step_avg:123.57ms
step:506/1393 train_time:61295ms step_avg:123.58ms
step:507/1393 train_time:61420ms step_avg:123.58ms
step:508/1393 train_time:61545ms step_avg:123.58ms
step:509/1393 train_time:61671ms step_avg:123.59ms
step:510/1393 train_time:61799ms step_avg:123.60ms
step:511/1393 train_time:61926ms step_avg:123.60ms
step:512/1393 train_time:62051ms step_avg:123.61ms
step:513/1393 train_time:62177ms step_avg:123.61ms
step:514/1393 train_time:62302ms step_avg:123.61ms
step:515/1393 train_time:62427ms step_avg:123.62ms
step:516/1393 train_time:62552ms step_avg:123.62ms
step:517/1393 train_time:62678ms step_avg:123.63ms
step:518/1393 train_time:62806ms step_avg:123.63ms
step:519/1393 train_time:62935ms step_avg:123.64ms
step:520/1393 train_time:63063ms step_avg:123.65ms
step:521/1393 train_time:63190ms step_avg:123.66ms
step:522/1393 train_time:63317ms step_avg:123.67ms
step:523/1393 train_time:63445ms step_avg:123.67ms
step:524/1393 train_time:63572ms step_avg:123.68ms
step:525/1393 train_time:63700ms step_avg:123.69ms
step:526/1393 train_time:63828ms step_avg:123.70ms
step:527/1393 train_time:63956ms step_avg:123.71ms
step:528/1393 train_time:64084ms step_avg:123.71ms
step:529/1393 train_time:64212ms step_avg:123.72ms
step:530/1393 train_time:64340ms step_avg:123.73ms
step:531/1393 train_time:64467ms step_avg:123.74ms
step:532/1393 train_time:64594ms step_avg:123.74ms
step:533/1393 train_time:64722ms step_avg:123.75ms
step:534/1393 train_time:64851ms step_avg:123.76ms
step:535/1393 train_time:64979ms step_avg:123.77ms
step:536/1393 train_time:65107ms step_avg:123.78ms
step:537/1393 train_time:65236ms step_avg:123.79ms
step:538/1393 train_time:65363ms step_avg:123.79ms
step:539/1393 train_time:65491ms step_avg:123.80ms
step:540/1393 train_time:65619ms step_avg:123.81ms
step:541/1393 train_time:65746ms step_avg:123.82ms
step:542/1393 train_time:65874ms step_avg:123.82ms
step:543/1393 train_time:66003ms step_avg:123.83ms
step:544/1393 train_time:66130ms step_avg:123.84ms
step:545/1393 train_time:66258ms step_avg:123.85ms
step:546/1393 train_time:66386ms step_avg:123.85ms
step:547/1393 train_time:66513ms step_avg:123.86ms
step:548/1393 train_time:66641ms step_avg:123.87ms
step:549/1393 train_time:66769ms step_avg:123.88ms
step:550/1393 train_time:66897ms step_avg:123.88ms
step:551/1393 train_time:67024ms step_avg:123.89ms
step:552/1393 train_time:67151ms step_avg:123.90ms
step:553/1393 train_time:67279ms step_avg:123.90ms
step:554/1393 train_time:67407ms step_avg:123.91ms
step:555/1393 train_time:67535ms step_avg:123.92ms
step:556/1393 train_time:67663ms step_avg:123.93ms
step:557/1393 train_time:67792ms step_avg:123.93ms
step:558/1393 train_time:67919ms step_avg:123.94ms
step:559/1393 train_time:68047ms step_avg:123.95ms
step:560/1393 train_time:68175ms step_avg:123.95ms
step:561/1393 train_time:68303ms step_avg:123.96ms
step:562/1393 train_time:68431ms step_avg:123.97ms
step:563/1393 train_time:68559ms step_avg:123.98ms
step:564/1393 train_time:68687ms step_avg:123.98ms
step:565/1393 train_time:68815ms step_avg:123.99ms
step:566/1393 train_time:68943ms step_avg:124.00ms
step:567/1393 train_time:69070ms step_avg:124.00ms
step:568/1393 train_time:69198ms step_avg:124.01ms
step:569/1393 train_time:69326ms step_avg:124.02ms
step:570/1393 train_time:69454ms step_avg:124.03ms
step:571/1393 train_time:69582ms step_avg:124.03ms
step:572/1393 train_time:69710ms step_avg:124.04ms
step:573/1393 train_time:69837ms step_avg:124.04ms
step:574/1393 train_time:69964ms step_avg:124.05ms
step:575/1393 train_time:70092ms step_avg:124.06ms
step:576/1393 train_time:70219ms step_avg:124.06ms
step:577/1393 train_time:70347ms step_avg:124.07ms
step:578/1393 train_time:70475ms step_avg:124.08ms
step:579/1393 train_time:70603ms step_avg:124.08ms
step:580/1393 train_time:70731ms step_avg:124.09ms
step:581/1393 train_time:70860ms step_avg:124.10ms
step:582/1393 train_time:70987ms step_avg:124.10ms
step:583/1393 train_time:71115ms step_avg:124.11ms
step:584/1393 train_time:71243ms step_avg:124.12ms
step:585/1393 train_time:71370ms step_avg:124.12ms
step:586/1393 train_time:71498ms step_avg:124.13ms
step:587/1393 train_time:71626ms step_avg:124.14ms
step:588/1393 train_time:71754ms step_avg:124.14ms
step:589/1393 train_time:71882ms step_avg:124.15ms
step:590/1393 train_time:72008ms step_avg:124.15ms
step:591/1393 train_time:72136ms step_avg:124.16ms
step:592/1393 train_time:72264ms step_avg:124.16ms
step:593/1393 train_time:72391ms step_avg:124.17ms
step:594/1393 train_time:72518ms step_avg:124.18ms
step:595/1393 train_time:72647ms step_avg:124.18ms
step:596/1393 train_time:72776ms step_avg:124.19ms
step:597/1393 train_time:72904ms step_avg:124.20ms
step:598/1393 train_time:73031ms step_avg:124.20ms
step:599/1393 train_time:73159ms step_avg:124.21ms
step:600/1393 train_time:73288ms step_avg:124.22ms
step:601/1393 train_time:73415ms step_avg:124.22ms
step:602/1393 train_time:73543ms step_avg:124.23ms
step:603/1393 train_time:73672ms step_avg:124.24ms
step:604/1393 train_time:73800ms step_avg:124.24ms
step:605/1393 train_time:73927ms step_avg:124.25ms
step:606/1393 train_time:74055ms step_avg:124.25ms
step:607/1393 train_time:74183ms step_avg:124.26ms
step:608/1393 train_time:74311ms step_avg:124.27ms
step:609/1393 train_time:74439ms step_avg:124.27ms
step:610/1393 train_time:74566ms step_avg:124.28ms
step:611/1393 train_time:74694ms step_avg:124.28ms
step:612/1393 train_time:74822ms step_avg:124.29ms
step:613/1393 train_time:74950ms step_avg:124.29ms
step:614/1393 train_time:75077ms step_avg:124.30ms
step:615/1393 train_time:75205ms step_avg:124.31ms
step:616/1393 train_time:75332ms step_avg:124.31ms
step:617/1393 train_time:75460ms step_avg:124.32ms
step:618/1393 train_time:75587ms step_avg:124.32ms
step:619/1393 train_time:75715ms step_avg:124.33ms
step:620/1393 train_time:75843ms step_avg:124.33ms
step:621/1393 train_time:75971ms step_avg:124.34ms
step:622/1393 train_time:76099ms step_avg:124.35ms
step:623/1393 train_time:76228ms step_avg:124.35ms
step:624/1393 train_time:76356ms step_avg:124.36ms
step:625/1393 train_time:76484ms step_avg:124.36ms
step:625/1393 val_loss:3.5813 train_time:76611ms step_avg:124.57ms
step:626/1393 train_time:76632ms step_avg:124.40ms
step:627/1393 train_time:76747ms step_avg:124.39ms
step:628/1393 train_time:76876ms step_avg:124.39ms
step:629/1393 train_time:77003ms step_avg:124.40ms
step:630/1393 train_time:77131ms step_avg:124.40ms
step:631/1393 train_time:77257ms step_avg:124.41ms
step:632/1393 train_time:77385ms step_avg:124.41ms
step:633/1393 train_time:77513ms step_avg:124.42ms
step:634/1393 train_time:77643ms step_avg:124.43ms
step:635/1393 train_time:77773ms step_avg:124.44ms
step:636/1393 train_time:77902ms step_avg:124.44ms
step:637/1393 train_time:78031ms step_avg:124.45ms
step:638/1393 train_time:78159ms step_avg:124.46ms
step:639/1393 train_time:78286ms step_avg:124.46ms
step:640/1393 train_time:78413ms step_avg:124.47ms
step:641/1393 train_time:78542ms step_avg:124.47ms
step:642/1393 train_time:78671ms step_avg:124.48ms
step:643/1393 train_time:78799ms step_avg:124.48ms
step:644/1393 train_time:78927ms step_avg:124.49ms
step:645/1393 train_time:79055ms step_avg:124.50ms
step:646/1393 train_time:79183ms step_avg:124.50ms
step:647/1393 train_time:79310ms step_avg:124.51ms
step:648/1393 train_time:79439ms step_avg:124.51ms
step:649/1393 train_time:79566ms step_avg:124.52ms
step:650/1393 train_time:79695ms step_avg:124.52ms
step:651/1393 train_time:79823ms step_avg:124.53ms
step:652/1393 train_time:79951ms step_avg:124.53ms
step:653/1393 train_time:80079ms step_avg:124.54ms
step:654/1393 train_time:80208ms step_avg:124.55ms
step:655/1393 train_time:80335ms step_avg:124.55ms
step:656/1393 train_time:80463ms step_avg:124.56ms
step:657/1393 train_time:80591ms step_avg:124.56ms
step:658/1393 train_time:80720ms step_avg:124.57ms
step:659/1393 train_time:80848ms step_avg:124.57ms
step:660/1393 train_time:80976ms step_avg:124.58ms
step:661/1393 train_time:81104ms step_avg:124.58ms
step:662/1393 train_time:81231ms step_avg:124.59ms
step:663/1393 train_time:81359ms step_avg:124.59ms
step:664/1393 train_time:81487ms step_avg:124.60ms
step:665/1393 train_time:81615ms step_avg:124.60ms
step:666/1393 train_time:81744ms step_avg:124.61ms
step:667/1393 train_time:81872ms step_avg:124.62ms
step:668/1393 train_time:82001ms step_avg:124.62ms
step:669/1393 train_time:82128ms step_avg:124.63ms
step:670/1393 train_time:82257ms step_avg:124.63ms
step:671/1393 train_time:82385ms step_avg:124.64ms
step:672/1393 train_time:82512ms step_avg:124.64ms
step:673/1393 train_time:82640ms step_avg:124.65ms
step:674/1393 train_time:82769ms step_avg:124.65ms
step:675/1393 train_time:82898ms step_avg:124.66ms
step:676/1393 train_time:83027ms step_avg:124.66ms
step:677/1393 train_time:83154ms step_avg:124.67ms
step:678/1393 train_time:83282ms step_avg:124.67ms
step:679/1393 train_time:83409ms step_avg:124.68ms
step:680/1393 train_time:83537ms step_avg:124.68ms
step:681/1393 train_time:83665ms step_avg:124.69ms
step:682/1393 train_time:83793ms step_avg:124.69ms
step:683/1393 train_time:83922ms step_avg:124.70ms
step:684/1393 train_time:84051ms step_avg:124.70ms
step:685/1393 train_time:84179ms step_avg:124.71ms
step:686/1393 train_time:84307ms step_avg:124.71ms
step:687/1393 train_time:84434ms step_avg:124.72ms
step:688/1393 train_time:84563ms step_avg:124.72ms
step:689/1393 train_time:84691ms step_avg:124.73ms
step:690/1393 train_time:84820ms step_avg:124.73ms
step:691/1393 train_time:84948ms step_avg:124.74ms
step:692/1393 train_time:85075ms step_avg:124.74ms
step:693/1393 train_time:85204ms step_avg:124.75ms
step:694/1393 train_time:85332ms step_avg:124.75ms
step:695/1393 train_time:85460ms step_avg:124.76ms
step:696/1393 train_time:85588ms step_avg:124.76ms
step:697/1393 train_time:85716ms step_avg:124.77ms
step:698/1393 train_time:85844ms step_avg:124.77ms
step:699/1393 train_time:85971ms step_avg:124.78ms
step:700/1393 train_time:86100ms step_avg:124.78ms
step:701/1393 train_time:86228ms step_avg:124.79ms
step:702/1393 train_time:86356ms step_avg:124.79ms
step:703/1393 train_time:86485ms step_avg:124.80ms
step:704/1393 train_time:86613ms step_avg:124.80ms
step:705/1393 train_time:86741ms step_avg:124.81ms
step:706/1393 train_time:86869ms step_avg:124.81ms
step:707/1393 train_time:86997ms step_avg:124.82ms
step:708/1393 train_time:87125ms step_avg:124.82ms
step:709/1393 train_time:87255ms step_avg:124.83ms
step:710/1393 train_time:87384ms step_avg:124.83ms
step:711/1393 train_time:87511ms step_avg:124.84ms
step:712/1393 train_time:87639ms step_avg:124.84ms
step:713/1393 train_time:87767ms step_avg:124.85ms
step:714/1393 train_time:87894ms step_avg:124.85ms
step:715/1393 train_time:88023ms step_avg:124.86ms
step:716/1393 train_time:88150ms step_avg:124.86ms
step:717/1393 train_time:88279ms step_avg:124.86ms
step:718/1393 train_time:88407ms step_avg:124.87ms
step:719/1393 train_time:88535ms step_avg:124.87ms
step:720/1393 train_time:88664ms step_avg:124.88ms
step:721/1393 train_time:88792ms step_avg:124.88ms
step:722/1393 train_time:88920ms step_avg:124.89ms
step:723/1393 train_time:89047ms step_avg:124.89ms
step:724/1393 train_time:89176ms step_avg:124.90ms
step:725/1393 train_time:89306ms step_avg:124.90ms
step:726/1393 train_time:89436ms step_avg:124.91ms
step:727/1393 train_time:89566ms step_avg:124.92ms
step:728/1393 train_time:89696ms step_avg:124.93ms
step:729/1393 train_time:89826ms step_avg:124.93ms
step:730/1393 train_time:89957ms step_avg:124.94ms
step:731/1393 train_time:90087ms step_avg:124.95ms
step:732/1393 train_time:90217ms step_avg:124.95ms
step:733/1393 train_time:90347ms step_avg:124.96ms
step:734/1393 train_time:90477ms step_avg:124.97ms
step:735/1393 train_time:90608ms step_avg:124.98ms
step:736/1393 train_time:90738ms step_avg:124.98ms
step:737/1393 train_time:90868ms step_avg:124.99ms
step:738/1393 train_time:90998ms step_avg:125.00ms
step:739/1393 train_time:91127ms step_avg:125.00ms
step:740/1393 train_time:91257ms step_avg:125.01ms
step:741/1393 train_time:91389ms step_avg:125.02ms
step:742/1393 train_time:91519ms step_avg:125.03ms
step:743/1393 train_time:91649ms step_avg:125.03ms
step:744/1393 train_time:91778ms step_avg:125.04ms
step:745/1393 train_time:91909ms step_avg:125.05ms
step:746/1393 train_time:92039ms step_avg:125.05ms
step:747/1393 train_time:92169ms step_avg:125.06ms
step:748/1393 train_time:92299ms step_avg:125.07ms
step:749/1393 train_time:92429ms step_avg:125.07ms
step:750/1393 train_time:92560ms step_avg:125.08ms
step:750/1393 val_loss:3.5245 train_time:92689ms step_avg:125.26ms
step:751/1393 train_time:92710ms step_avg:125.11ms
step:752/1393 train_time:92827ms step_avg:125.10ms
step:753/1393 train_time:92956ms step_avg:125.11ms
step:754/1393 train_time:93085ms step_avg:125.11ms
step:755/1393 train_time:93215ms step_avg:125.12ms
step:756/1393 train_time:93343ms step_avg:125.12ms
step:757/1393 train_time:93473ms step_avg:125.13ms
step:758/1393 train_time:93604ms step_avg:125.14ms
step:759/1393 train_time:93734ms step_avg:125.15ms
step:760/1393 train_time:93864ms step_avg:125.15ms
step:761/1393 train_time:93995ms step_avg:125.16ms
step:762/1393 train_time:94123ms step_avg:125.16ms
step:763/1393 train_time:94253ms step_avg:125.17ms
step:764/1393 train_time:94383ms step_avg:125.18ms
step:765/1393 train_time:94513ms step_avg:125.18ms
step:766/1393 train_time:94643ms step_avg:125.19ms
step:767/1393 train_time:94775ms step_avg:125.20ms
step:768/1393 train_time:94905ms step_avg:125.20ms
step:769/1393 train_time:95035ms step_avg:125.21ms
step:770/1393 train_time:95166ms step_avg:125.22ms
step:771/1393 train_time:95296ms step_avg:125.22ms
step:772/1393 train_time:95425ms step_avg:125.23ms
step:773/1393 train_time:95556ms step_avg:125.24ms
step:774/1393 train_time:95686ms step_avg:125.24ms
step:775/1393 train_time:95816ms step_avg:125.25ms
step:776/1393 train_time:95946ms step_avg:125.26ms
step:777/1393 train_time:96076ms step_avg:125.26ms
step:778/1393 train_time:96207ms step_avg:125.27ms
step:779/1393 train_time:96337ms step_avg:125.28ms
step:780/1393 train_time:96467ms step_avg:125.28ms
step:781/1393 train_time:96598ms step_avg:125.29ms
step:782/1393 train_time:96728ms step_avg:125.30ms
step:783/1393 train_time:96858ms step_avg:125.30ms
step:784/1393 train_time:96989ms step_avg:125.31ms
step:785/1393 train_time:97119ms step_avg:125.31ms
step:786/1393 train_time:97250ms step_avg:125.32ms
step:787/1393 train_time:97380ms step_avg:125.33ms
step:788/1393 train_time:97510ms step_avg:125.33ms
step:789/1393 train_time:97640ms step_avg:125.34ms
step:790/1393 train_time:97770ms step_avg:125.35ms
step:791/1393 train_time:97900ms step_avg:125.35ms
step:792/1393 train_time:98031ms step_avg:125.36ms
step:793/1393 train_time:98160ms step_avg:125.36ms
step:794/1393 train_time:98289ms step_avg:125.37ms
step:795/1393 train_time:98420ms step_avg:125.38ms
step:796/1393 train_time:98551ms step_avg:125.38ms
step:797/1393 train_time:98681ms step_avg:125.39ms
step:798/1393 train_time:98811ms step_avg:125.40ms
step:799/1393 train_time:98943ms step_avg:125.40ms
step:800/1393 train_time:99073ms step_avg:125.41ms
step:801/1393 train_time:99202ms step_avg:125.41ms
step:802/1393 train_time:99332ms step_avg:125.42ms
step:803/1393 train_time:99461ms step_avg:125.42ms
step:804/1393 train_time:99591ms step_avg:125.43ms
step:805/1393 train_time:99722ms step_avg:125.44ms
step:806/1393 train_time:99852ms step_avg:125.44ms
step:807/1393 train_time:99982ms step_avg:125.45ms
step:808/1393 train_time:100112ms step_avg:125.45ms
step:809/1393 train_time:100242ms step_avg:125.46ms
step:810/1393 train_time:100372ms step_avg:125.47ms
step:811/1393 train_time:100503ms step_avg:125.47ms
step:812/1393 train_time:100633ms step_avg:125.48ms
step:813/1393 train_time:100763ms step_avg:125.48ms
step:814/1393 train_time:100893ms step_avg:125.49ms
step:815/1393 train_time:101024ms step_avg:125.50ms
step:816/1393 train_time:101155ms step_avg:125.50ms
step:817/1393 train_time:101284ms step_avg:125.51ms
step:818/1393 train_time:101414ms step_avg:125.51ms
step:819/1393 train_time:101544ms step_avg:125.52ms
step:820/1393 train_time:101674ms step_avg:125.52ms
step:821/1393 train_time:101805ms step_avg:125.53ms
step:822/1393 train_time:101934ms step_avg:125.53ms
step:823/1393 train_time:102064ms step_avg:125.54ms
step:824/1393 train_time:102193ms step_avg:125.54ms
step:825/1393 train_time:102324ms step_avg:125.55ms
step:826/1393 train_time:102454ms step_avg:125.56ms
step:827/1393 train_time:102584ms step_avg:125.56ms
step:828/1393 train_time:102715ms step_avg:125.57ms
step:829/1393 train_time:102845ms step_avg:125.57ms
step:830/1393 train_time:102976ms step_avg:125.58ms
step:831/1393 train_time:103106ms step_avg:125.59ms
step:832/1393 train_time:103237ms step_avg:125.59ms
step:833/1393 train_time:103368ms step_avg:125.60ms
step:834/1393 train_time:103499ms step_avg:125.61ms
step:835/1393 train_time:103629ms step_avg:125.61ms
step:836/1393 train_time:103760ms step_avg:125.62ms
step:837/1393 train_time:103891ms step_avg:125.62ms
step:838/1393 train_time:104021ms step_avg:125.63ms
step:839/1393 train_time:104151ms step_avg:125.63ms
step:840/1393 train_time:104281ms step_avg:125.64ms
step:841/1393 train_time:104412ms step_avg:125.65ms
step:842/1393 train_time:104542ms step_avg:125.65ms
step:843/1393 train_time:104672ms step_avg:125.66ms
step:844/1393 train_time:104802ms step_avg:125.66ms
step:845/1393 train_time:104932ms step_avg:125.67ms
step:846/1393 train_time:105062ms step_avg:125.67ms
step:847/1393 train_time:105192ms step_avg:125.68ms
step:848/1393 train_time:105322ms step_avg:125.68ms
step:849/1393 train_time:105452ms step_avg:125.69ms
step:850/1393 train_time:105582ms step_avg:125.69ms
step:851/1393 train_time:105713ms step_avg:125.70ms
step:852/1393 train_time:105843ms step_avg:125.70ms
step:853/1393 train_time:105973ms step_avg:125.71ms
step:854/1393 train_time:106103ms step_avg:125.71ms
step:855/1393 train_time:106234ms step_avg:125.72ms
step:856/1393 train_time:106363ms step_avg:125.72ms
step:857/1393 train_time:106494ms step_avg:125.73ms
step:858/1393 train_time:106624ms step_avg:125.74ms
step:859/1393 train_time:106754ms step_avg:125.74ms
step:860/1393 train_time:106884ms step_avg:125.75ms
step:861/1393 train_time:107014ms step_avg:125.75ms
step:862/1393 train_time:107146ms step_avg:125.76ms
step:863/1393 train_time:107275ms step_avg:125.76ms
step:864/1393 train_time:107405ms step_avg:125.77ms
step:865/1393 train_time:107536ms step_avg:125.77ms
step:866/1393 train_time:107668ms step_avg:125.78ms
step:867/1393 train_time:107799ms step_avg:125.79ms
step:868/1393 train_time:107928ms step_avg:125.79ms
step:869/1393 train_time:108059ms step_avg:125.80ms
step:870/1393 train_time:108190ms step_avg:125.80ms
step:871/1393 train_time:108321ms step_avg:125.81ms
step:872/1393 train_time:108451ms step_avg:125.81ms
step:873/1393 train_time:108581ms step_avg:125.82ms
step:874/1393 train_time:108712ms step_avg:125.82ms
step:875/1393 train_time:108842ms step_avg:125.83ms
step:875/1393 val_loss:3.4762 train_time:108971ms step_avg:125.98ms
step:876/1393 train_time:108993ms step_avg:125.86ms
step:877/1393 train_time:109112ms step_avg:125.85ms
step:878/1393 train_time:109242ms step_avg:125.86ms
step:879/1393 train_time:109372ms step_avg:125.86ms
step:880/1393 train_time:109501ms step_avg:125.86ms
step:881/1393 train_time:109631ms step_avg:125.87ms
step:882/1393 train_time:109760ms step_avg:125.87ms
step:883/1393 train_time:109890ms step_avg:125.88ms
step:884/1393 train_time:110021ms step_avg:125.88ms
step:885/1393 train_time:110153ms step_avg:125.89ms
step:886/1393 train_time:110284ms step_avg:125.89ms
step:887/1393 train_time:110413ms step_avg:125.90ms
step:888/1393 train_time:110543ms step_avg:125.90ms
step:889/1393 train_time:110673ms step_avg:125.91ms
step:890/1393 train_time:110802ms step_avg:125.91ms
step:891/1393 train_time:110932ms step_avg:125.92ms
step:892/1393 train_time:111062ms step_avg:125.92ms
step:893/1393 train_time:111192ms step_avg:125.93ms
step:894/1393 train_time:111323ms step_avg:125.93ms
step:895/1393 train_time:111454ms step_avg:125.94ms
step:896/1393 train_time:111585ms step_avg:125.94ms
step:897/1393 train_time:111714ms step_avg:125.95ms
step:898/1393 train_time:111844ms step_avg:125.95ms
step:899/1393 train_time:111976ms step_avg:125.96ms
step:900/1393 train_time:112106ms step_avg:125.96ms
step:901/1393 train_time:112235ms step_avg:125.97ms
step:902/1393 train_time:112365ms step_avg:125.97ms
step:903/1393 train_time:112495ms step_avg:125.97ms
step:904/1393 train_time:112626ms step_avg:125.98ms
step:905/1393 train_time:112756ms step_avg:125.98ms
step:906/1393 train_time:112886ms step_avg:125.99ms
step:907/1393 train_time:113018ms step_avg:126.00ms
step:908/1393 train_time:113148ms step_avg:126.00ms
step:909/1393 train_time:113278ms step_avg:126.00ms
step:910/1393 train_time:113411ms step_avg:126.01ms
step:911/1393 train_time:113541ms step_avg:126.02ms
step:912/1393 train_time:113671ms step_avg:126.02ms
step:913/1393 train_time:113802ms step_avg:126.03ms
step:914/1393 train_time:113933ms step_avg:126.03ms
step:915/1393 train_time:114064ms step_avg:126.04ms
step:916/1393 train_time:114195ms step_avg:126.04ms
step:917/1393 train_time:114326ms step_avg:126.05ms
step:918/1393 train_time:114457ms step_avg:126.05ms
step:919/1393 train_time:114589ms step_avg:126.06ms
step:920/1393 train_time:114721ms step_avg:126.07ms
step:921/1393 train_time:114851ms step_avg:126.07ms
step:922/1393 train_time:114982ms step_avg:126.08ms
step:923/1393 train_time:115111ms step_avg:126.08ms
step:924/1393 train_time:115241ms step_avg:126.08ms
step:925/1393 train_time:115372ms step_avg:126.09ms
step:926/1393 train_time:115503ms step_avg:126.09ms
step:927/1393 train_time:115634ms step_avg:126.10ms
step:928/1393 train_time:115765ms step_avg:126.11ms
step:929/1393 train_time:115894ms step_avg:126.11ms
step:930/1393 train_time:116025ms step_avg:126.11ms
step:931/1393 train_time:116158ms step_avg:126.12ms
step:932/1393 train_time:116290ms step_avg:126.13ms
step:933/1393 train_time:116423ms step_avg:126.14ms
step:934/1393 train_time:116555ms step_avg:126.14ms
step:935/1393 train_time:116686ms step_avg:126.15ms
step:936/1393 train_time:116818ms step_avg:126.15ms
step:937/1393 train_time:116951ms step_avg:126.16ms
step:938/1393 train_time:117085ms step_avg:126.17ms
step:939/1393 train_time:117217ms step_avg:126.18ms
step:940/1393 train_time:117351ms step_avg:126.18ms
step:941/1393 train_time:117483ms step_avg:126.19ms
step:942/1393 train_time:117615ms step_avg:126.20ms
step:943/1393 train_time:117748ms step_avg:126.20ms
step:944/1393 train_time:117880ms step_avg:126.21ms
step:945/1393 train_time:118013ms step_avg:126.22ms
step:946/1393 train_time:118145ms step_avg:126.22ms
step:947/1393 train_time:118277ms step_avg:126.23ms
step:948/1393 train_time:118410ms step_avg:126.24ms
step:949/1393 train_time:118543ms step_avg:126.24ms
step:950/1393 train_time:118675ms step_avg:126.25ms
step:951/1393 train_time:118807ms step_avg:126.26ms
step:952/1393 train_time:118939ms step_avg:126.26ms
step:953/1393 train_time:119071ms step_avg:126.27ms
step:954/1393 train_time:119202ms step_avg:126.27ms
step:955/1393 train_time:119334ms step_avg:126.28ms
step:956/1393 train_time:119469ms step_avg:126.29ms
step:957/1393 train_time:119601ms step_avg:126.29ms
step:958/1393 train_time:119733ms step_avg:126.30ms
step:959/1393 train_time:119864ms step_avg:126.31ms
step:960/1393 train_time:119996ms step_avg:126.31ms
step:961/1393 train_time:120129ms step_avg:126.32ms
step:962/1393 train_time:120261ms step_avg:126.32ms
step:963/1393 train_time:120393ms step_avg:126.33ms
step:964/1393 train_time:120526ms step_avg:126.34ms
step:965/1393 train_time:120659ms step_avg:126.34ms
step:966/1393 train_time:120790ms step_avg:126.35ms
step:967/1393 train_time:120921ms step_avg:126.35ms
step:968/1393 train_time:121053ms step_avg:126.36ms
step:969/1393 train_time:121187ms step_avg:126.37ms
step:970/1393 train_time:121319ms step_avg:126.37ms
step:971/1393 train_time:121452ms step_avg:126.38ms
step:972/1393 train_time:121583ms step_avg:126.39ms
step:973/1393 train_time:121715ms step_avg:126.39ms
step:974/1393 train_time:121846ms step_avg:126.40ms
step:975/1393 train_time:121977ms step_avg:126.40ms
step:976/1393 train_time:122110ms step_avg:126.41ms
step:977/1393 train_time:122241ms step_avg:126.41ms
step:978/1393 train_time:122374ms step_avg:126.42ms
step:979/1393 train_time:122507ms step_avg:126.43ms
step:980/1393 train_time:122639ms step_avg:126.43ms
step:981/1393 train_time:122770ms step_avg:126.44ms
step:982/1393 train_time:122901ms step_avg:126.44ms
step:983/1393 train_time:123033ms step_avg:126.45ms
step:984/1393 train_time:123164ms step_avg:126.45ms
step:985/1393 train_time:123296ms step_avg:126.46ms
step:986/1393 train_time:123432ms step_avg:126.47ms
step:987/1393 train_time:123564ms step_avg:126.47ms
step:988/1393 train_time:123696ms step_avg:126.48ms
step:989/1393 train_time:123829ms step_avg:126.49ms
step:990/1393 train_time:123961ms step_avg:126.49ms
step:991/1393 train_time:124094ms step_avg:126.50ms
step:992/1393 train_time:124226ms step_avg:126.50ms
step:993/1393 train_time:124360ms step_avg:126.51ms
step:994/1393 train_time:124493ms step_avg:126.52ms
step:995/1393 train_time:124623ms step_avg:126.52ms
step:996/1393 train_time:124754ms step_avg:126.53ms
step:997/1393 train_time:124887ms step_avg:126.53ms
step:998/1393 train_time:125018ms step_avg:126.54ms
step:999/1393 train_time:125151ms step_avg:126.54ms
step:1000/1393 train_time:125283ms step_avg:126.55ms
step:1000/1393 val_loss:3.4137 train_time:125413ms step_avg:126.68ms
step:1001/1393 train_time:125434ms step_avg:126.57ms
step:1002/1393 train_time:125554ms step_avg:126.57ms
step:1003/1393 train_time:125686ms step_avg:126.57ms
step:1004/1393 train_time:125818ms step_avg:126.58ms
step:1005/1393 train_time:125950ms step_avg:126.58ms
step:1006/1393 train_time:126080ms step_avg:126.59ms
step:1007/1393 train_time:126211ms step_avg:126.59ms
step:1008/1393 train_time:126342ms step_avg:126.60ms
step:1009/1393 train_time:126478ms step_avg:126.60ms
step:1010/1393 train_time:126611ms step_avg:126.61ms
step:1011/1393 train_time:126744ms step_avg:126.62ms
step:1012/1393 train_time:126876ms step_avg:126.62ms
step:1013/1393 train_time:127009ms step_avg:126.63ms
step:1014/1393 train_time:127140ms step_avg:126.63ms
step:1015/1393 train_time:127271ms step_avg:126.64ms
step:1016/1393 train_time:127402ms step_avg:126.64ms
step:1017/1393 train_time:127535ms step_avg:126.65ms
step:1018/1393 train_time:127667ms step_avg:126.65ms
step:1019/1393 train_time:127800ms step_avg:126.66ms
step:1020/1393 train_time:127933ms step_avg:126.67ms
step:1021/1393 train_time:128064ms step_avg:126.67ms
step:1022/1393 train_time:128195ms step_avg:126.67ms
step:1023/1393 train_time:128327ms step_avg:126.68ms
step:1024/1393 train_time:128458ms step_avg:126.68ms
step:1025/1393 train_time:128590ms step_avg:126.69ms
step:1026/1393 train_time:128722ms step_avg:126.70ms
step:1027/1393 train_time:128854ms step_avg:126.70ms
step:1028/1393 train_time:128987ms step_avg:126.71ms
step:1029/1393 train_time:129120ms step_avg:126.71ms
step:1030/1393 train_time:129252ms step_avg:126.72ms
step:1031/1393 train_time:129383ms step_avg:126.72ms
step:1032/1393 train_time:129514ms step_avg:126.73ms
step:1033/1393 train_time:129645ms step_avg:126.73ms
step:1034/1393 train_time:129778ms step_avg:126.74ms
step:1035/1393 train_time:129913ms step_avg:126.74ms
step:1036/1393 train_time:130045ms step_avg:126.75ms
step:1037/1393 train_time:130178ms step_avg:126.76ms
step:1038/1393 train_time:130311ms step_avg:126.76ms
step:1039/1393 train_time:130443ms step_avg:126.77ms
step:1040/1393 train_time:130575ms step_avg:126.77ms
step:1041/1393 train_time:130707ms step_avg:126.78ms
step:1042/1393 train_time:130839ms step_avg:126.78ms
step:1043/1393 train_time:130974ms step_avg:126.79ms
step:1044/1393 train_time:131108ms step_avg:126.80ms
step:1045/1393 train_time:131240ms step_avg:126.80ms
step:1046/1393 train_time:131372ms step_avg:126.81ms
step:1047/1393 train_time:131504ms step_avg:126.81ms
step:1048/1393 train_time:131636ms step_avg:126.82ms
step:1049/1393 train_time:131769ms step_avg:126.82ms
step:1050/1393 train_time:131901ms step_avg:126.83ms
step:1051/1393 train_time:132036ms step_avg:126.84ms
step:1052/1393 train_time:132168ms step_avg:126.84ms
step:1053/1393 train_time:132300ms step_avg:126.85ms
step:1054/1393 train_time:132432ms step_avg:126.85ms
step:1055/1393 train_time:132565ms step_avg:126.86ms
step:1056/1393 train_time:132696ms step_avg:126.86ms
step:1057/1393 train_time:132829ms step_avg:126.87ms
step:1058/1393 train_time:132962ms step_avg:126.87ms
step:1059/1393 train_time:133095ms step_avg:126.88ms
step:1060/1393 train_time:133228ms step_avg:126.88ms
step:1061/1393 train_time:133360ms step_avg:126.89ms
step:1062/1393 train_time:133494ms step_avg:126.90ms
step:1063/1393 train_time:133625ms step_avg:126.90ms
step:1064/1393 train_time:133758ms step_avg:126.90ms
step:1065/1393 train_time:133890ms step_avg:126.91ms
step:1066/1393 train_time:134022ms step_avg:126.92ms
step:1067/1393 train_time:134155ms step_avg:126.92ms
step:1068/1393 train_time:134286ms step_avg:126.92ms
step:1069/1393 train_time:134419ms step_avg:126.93ms
step:1070/1393 train_time:134552ms step_avg:126.94ms
step:1071/1393 train_time:134687ms step_avg:126.94ms
step:1072/1393 train_time:134819ms step_avg:126.95ms
step:1073/1393 train_time:134951ms step_avg:126.95ms
step:1074/1393 train_time:135083ms step_avg:126.96ms
step:1075/1393 train_time:135215ms step_avg:126.96ms
step:1076/1393 train_time:135346ms step_avg:126.97ms
step:1077/1393 train_time:135478ms step_avg:126.97ms
step:1078/1393 train_time:135611ms step_avg:126.98ms
step:1079/1393 train_time:135747ms step_avg:126.99ms
step:1080/1393 train_time:135879ms step_avg:126.99ms
step:1081/1393 train_time:136011ms step_avg:126.99ms
step:1082/1393 train_time:136142ms step_avg:127.00ms
step:1083/1393 train_time:136274ms step_avg:127.00ms
step:1084/1393 train_time:136408ms step_avg:127.01ms
step:1085/1393 train_time:136540ms step_avg:127.01ms
step:1086/1393 train_time:136673ms step_avg:127.02ms
step:1087/1393 train_time:136806ms step_avg:127.02ms
step:1088/1393 train_time:136938ms step_avg:127.03ms
step:1089/1393 train_time:137072ms step_avg:127.04ms
step:1090/1393 train_time:137205ms step_avg:127.04ms
step:1091/1393 train_time:137337ms step_avg:127.05ms
step:1092/1393 train_time:137468ms step_avg:127.05ms
step:1093/1393 train_time:137601ms step_avg:127.05ms
step:1094/1393 train_time:137733ms step_avg:127.06ms
step:1095/1393 train_time:137864ms step_avg:127.06ms
step:1096/1393 train_time:137998ms step_avg:127.07ms
step:1097/1393 train_time:138130ms step_avg:127.07ms
step:1098/1393 train_time:138262ms step_avg:127.08ms
step:1099/1393 train_time:138394ms step_avg:127.08ms
step:1100/1393 train_time:138525ms step_avg:127.09ms
step:1101/1393 train_time:138657ms step_avg:127.09ms
step:1102/1393 train_time:138790ms step_avg:127.10ms
step:1103/1393 train_time:138923ms step_avg:127.10ms
step:1104/1393 train_time:139055ms step_avg:127.11ms
step:1105/1393 train_time:139190ms step_avg:127.11ms
step:1106/1393 train_time:139323ms step_avg:127.12ms
step:1107/1393 train_time:139455ms step_avg:127.12ms
step:1108/1393 train_time:139589ms step_avg:127.13ms
step:1109/1393 train_time:139720ms step_avg:127.13ms
step:1110/1393 train_time:139853ms step_avg:127.14ms
step:1111/1393 train_time:139986ms step_avg:127.14ms
step:1112/1393 train_time:140118ms step_avg:127.15ms
step:1113/1393 train_time:140250ms step_avg:127.15ms
step:1114/1393 train_time:140383ms step_avg:127.16ms
step:1115/1393 train_time:140516ms step_avg:127.16ms
step:1116/1393 train_time:140648ms step_avg:127.17ms
step:1117/1393 train_time:140781ms step_avg:127.17ms
step:1118/1393 train_time:140914ms step_avg:127.18ms
step:1119/1393 train_time:141046ms step_avg:127.18ms
step:1120/1393 train_time:141178ms step_avg:127.19ms
step:1121/1393 train_time:141310ms step_avg:127.19ms
step:1122/1393 train_time:141442ms step_avg:127.20ms
step:1123/1393 train_time:141574ms step_avg:127.20ms
step:1124/1393 train_time:141707ms step_avg:127.21ms
step:1125/1393 train_time:141839ms step_avg:127.21ms
step:1125/1393 val_loss:3.3633 train_time:141972ms step_avg:127.33ms
step:1126/1393 train_time:141995ms step_avg:127.24ms
step:1127/1393 train_time:142114ms step_avg:127.23ms
step:1128/1393 train_time:142247ms step_avg:127.23ms
step:1129/1393 train_time:142380ms step_avg:127.24ms
step:1130/1393 train_time:142511ms step_avg:127.24ms
step:1131/1393 train_time:142644ms step_avg:127.25ms
step:1132/1393 train_time:142776ms step_avg:127.25ms
step:1133/1393 train_time:142906ms step_avg:127.25ms
step:1134/1393 train_time:143040ms step_avg:127.26ms
step:1135/1393 train_time:143173ms step_avg:127.26ms
step:1136/1393 train_time:143309ms step_avg:127.27ms
step:1137/1393 train_time:143439ms step_avg:127.28ms
step:1138/1393 train_time:143575ms step_avg:127.28ms
step:1139/1393 train_time:143709ms step_avg:127.29ms
step:1140/1393 train_time:143842ms step_avg:127.29ms
step:1141/1393 train_time:143976ms step_avg:127.30ms
step:1142/1393 train_time:144112ms step_avg:127.31ms
step:1143/1393 train_time:144247ms step_avg:127.31ms
step:1144/1393 train_time:144380ms step_avg:127.32ms
step:1145/1393 train_time:144514ms step_avg:127.32ms
step:1146/1393 train_time:144648ms step_avg:127.33ms
step:1147/1393 train_time:144783ms step_avg:127.34ms
step:1148/1393 train_time:144916ms step_avg:127.34ms
step:1149/1393 train_time:145050ms step_avg:127.35ms
step:1150/1393 train_time:145183ms step_avg:127.35ms
step:1151/1393 train_time:145317ms step_avg:127.36ms
step:1152/1393 train_time:145450ms step_avg:127.36ms
step:1153/1393 train_time:145586ms step_avg:127.37ms
step:1154/1393 train_time:145719ms step_avg:127.38ms
step:1155/1393 train_time:145852ms step_avg:127.38ms
step:1156/1393 train_time:145987ms step_avg:127.39ms
step:1157/1393 train_time:146121ms step_avg:127.39ms
step:1158/1393 train_time:146255ms step_avg:127.40ms
step:1159/1393 train_time:146387ms step_avg:127.40ms
step:1160/1393 train_time:146520ms step_avg:127.41ms
step:1161/1393 train_time:146653ms step_avg:127.41ms
step:1162/1393 train_time:146788ms step_avg:127.42ms
step:1163/1393 train_time:146922ms step_avg:127.43ms
step:1164/1393 train_time:147056ms step_avg:127.43ms
step:1165/1393 train_time:147190ms step_avg:127.44ms
step:1166/1393 train_time:147323ms step_avg:127.44ms
step:1167/1393 train_time:147456ms step_avg:127.45ms
step:1168/1393 train_time:147591ms step_avg:127.45ms
step:1169/1393 train_time:147724ms step_avg:127.46ms
step:1170/1393 train_time:147857ms step_avg:127.46ms
step:1171/1393 train_time:147991ms step_avg:127.47ms
step:1172/1393 train_time:148125ms step_avg:127.47ms
step:1173/1393 train_time:148259ms step_avg:127.48ms
step:1174/1393 train_time:148397ms step_avg:127.49ms
step:1175/1393 train_time:148531ms step_avg:127.49ms
step:1176/1393 train_time:148665ms step_avg:127.50ms
step:1177/1393 train_time:148802ms step_avg:127.51ms
step:1178/1393 train_time:148935ms step_avg:127.51ms
step:1179/1393 train_time:149068ms step_avg:127.52ms
step:1180/1393 train_time:149202ms step_avg:127.52ms
step:1181/1393 train_time:149338ms step_avg:127.53ms
step:1182/1393 train_time:149470ms step_avg:127.53ms
step:1183/1393 train_time:149604ms step_avg:127.54ms
step:1184/1393 train_time:149738ms step_avg:127.55ms
step:1185/1393 train_time:149873ms step_avg:127.55ms
step:1186/1393 train_time:150006ms step_avg:127.56ms
step:1187/1393 train_time:150145ms step_avg:127.57ms
step:1188/1393 train_time:150278ms step_avg:127.57ms
step:1189/1393 train_time:150414ms step_avg:127.58ms
step:1190/1393 train_time:150548ms step_avg:127.58ms
step:1191/1393 train_time:150681ms step_avg:127.59ms
step:1192/1393 train_time:150815ms step_avg:127.59ms
step:1193/1393 train_time:150948ms step_avg:127.60ms
step:1194/1393 train_time:151081ms step_avg:127.60ms
step:1195/1393 train_time:151215ms step_avg:127.61ms
step:1196/1393 train_time:151349ms step_avg:127.61ms
step:1197/1393 train_time:151484ms step_avg:127.62ms
step:1198/1393 train_time:151619ms step_avg:127.63ms
step:1199/1393 train_time:151753ms step_avg:127.63ms
step:1200/1393 train_time:151886ms step_avg:127.64ms
step:1201/1393 train_time:152019ms step_avg:127.64ms
step:1202/1393 train_time:152157ms step_avg:127.65ms
step:1203/1393 train_time:152294ms step_avg:127.66ms
step:1204/1393 train_time:152427ms step_avg:127.66ms
step:1205/1393 train_time:152562ms step_avg:127.67ms
step:1206/1393 train_time:152697ms step_avg:127.67ms
step:1207/1393 train_time:152830ms step_avg:127.68ms
step:1208/1393 train_time:152965ms step_avg:127.68ms
step:1209/1393 train_time:153098ms step_avg:127.69ms
step:1210/1393 train_time:153234ms step_avg:127.69ms
step:1211/1393 train_time:153368ms step_avg:127.70ms
step:1212/1393 train_time:153502ms step_avg:127.71ms
step:1213/1393 train_time:153635ms step_avg:127.71ms
step:1214/1393 train_time:153770ms step_avg:127.72ms
step:1215/1393 train_time:153905ms step_avg:127.72ms
step:1216/1393 train_time:154038ms step_avg:127.73ms
step:1217/1393 train_time:154172ms step_avg:127.73ms
step:1218/1393 train_time:154305ms step_avg:127.74ms
step:1219/1393 train_time:154438ms step_avg:127.74ms
step:1220/1393 train_time:154573ms step_avg:127.75ms
step:1221/1393 train_time:154707ms step_avg:127.75ms
step:1222/1393 train_time:154841ms step_avg:127.76ms
step:1223/1393 train_time:154975ms step_avg:127.76ms
step:1224/1393 train_time:155108ms step_avg:127.77ms
step:1225/1393 train_time:155243ms step_avg:127.77ms
step:1226/1393 train_time:155377ms step_avg:127.78ms
step:1227/1393 train_time:155510ms step_avg:127.78ms
step:1228/1393 train_time:155645ms step_avg:127.79ms
step:1229/1393 train_time:155777ms step_avg:127.79ms
step:1230/1393 train_time:155913ms step_avg:127.80ms
step:1231/1393 train_time:156048ms step_avg:127.80ms
step:1232/1393 train_time:156184ms step_avg:127.81ms
step:1233/1393 train_time:156317ms step_avg:127.81ms
step:1234/1393 train_time:156451ms step_avg:127.82ms
step:1235/1393 train_time:156585ms step_avg:127.82ms
step:1236/1393 train_time:156719ms step_avg:127.83ms
step:1237/1393 train_time:156852ms step_avg:127.83ms
step:1238/1393 train_time:156990ms step_avg:127.84ms
step:1239/1393 train_time:157123ms step_avg:127.85ms
step:1240/1393 train_time:157257ms step_avg:127.85ms
step:1241/1393 train_time:157392ms step_avg:127.86ms
step:1242/1393 train_time:157526ms step_avg:127.86ms
step:1243/1393 train_time:157661ms step_avg:127.87ms
step:1244/1393 train_time:157794ms step_avg:127.87ms
step:1245/1393 train_time:157928ms step_avg:127.88ms
step:1246/1393 train_time:158062ms step_avg:127.88ms
step:1247/1393 train_time:158196ms step_avg:127.89ms
step:1248/1393 train_time:158329ms step_avg:127.89ms
step:1249/1393 train_time:158461ms step_avg:127.89ms
step:1250/1393 train_time:158596ms step_avg:127.90ms
step:1250/1393 val_loss:3.3163 train_time:158728ms step_avg:128.01ms
step:1251/1393 train_time:158749ms step_avg:127.92ms
step:1252/1393 train_time:158872ms step_avg:127.92ms
step:1253/1393 train_time:159004ms step_avg:127.92ms
step:1254/1393 train_time:159136ms step_avg:127.92ms
step:1255/1393 train_time:159275ms step_avg:127.93ms
step:1256/1393 train_time:159408ms step_avg:127.94ms
step:1257/1393 train_time:159540ms step_avg:127.94ms
step:1258/1393 train_time:159674ms step_avg:127.94ms
step:1259/1393 train_time:159812ms step_avg:127.95ms
step:1260/1393 train_time:159945ms step_avg:127.96ms
step:1261/1393 train_time:160079ms step_avg:127.96ms
step:1262/1393 train_time:160214ms step_avg:127.97ms
step:1263/1393 train_time:160349ms step_avg:127.97ms
step:1264/1393 train_time:160482ms step_avg:127.98ms
step:1265/1393 train_time:160615ms step_avg:127.98ms
step:1266/1393 train_time:160750ms step_avg:127.99ms
step:1267/1393 train_time:160884ms step_avg:127.99ms
step:1268/1393 train_time:161017ms step_avg:127.99ms
step:1269/1393 train_time:161152ms step_avg:128.00ms
step:1270/1393 train_time:161287ms step_avg:128.01ms
step:1271/1393 train_time:161420ms step_avg:128.01ms
step:1272/1393 train_time:161554ms step_avg:128.01ms
step:1273/1393 train_time:161687ms step_avg:128.02ms
step:1274/1393 train_time:161820ms step_avg:128.02ms
step:1275/1393 train_time:161955ms step_avg:128.03ms
step:1276/1393 train_time:162089ms step_avg:128.03ms
step:1277/1393 train_time:162222ms step_avg:128.04ms
step:1278/1393 train_time:162355ms step_avg:128.04ms
step:1279/1393 train_time:162488ms step_avg:128.04ms
step:1280/1393 train_time:162625ms step_avg:128.05ms
step:1281/1393 train_time:162758ms step_avg:128.06ms
step:1282/1393 train_time:162892ms step_avg:128.06ms
step:1283/1393 train_time:163025ms step_avg:128.06ms
step:1284/1393 train_time:163159ms step_avg:128.07ms
step:1285/1393 train_time:163293ms step_avg:128.07ms
step:1286/1393 train_time:163427ms step_avg:128.08ms
step:1287/1393 train_time:163561ms step_avg:128.08ms
step:1288/1393 train_time:163694ms step_avg:128.09ms
step:1289/1393 train_time:163831ms step_avg:128.09ms
step:1290/1393 train_time:163966ms step_avg:128.10ms
step:1291/1393 train_time:164102ms step_avg:128.10ms
step:1292/1393 train_time:164237ms step_avg:128.11ms
step:1293/1393 train_time:164373ms step_avg:128.12ms
step:1294/1393 train_time:164507ms step_avg:128.12ms
step:1295/1393 train_time:164642ms step_avg:128.13ms
step:1296/1393 train_time:164777ms step_avg:128.13ms
step:1297/1393 train_time:164913ms step_avg:128.14ms
step:1298/1393 train_time:165045ms step_avg:128.14ms
step:1299/1393 train_time:165179ms step_avg:128.15ms
step:1300/1393 train_time:165312ms step_avg:128.15ms
step:1301/1393 train_time:165445ms step_avg:128.15ms
step:1302/1393 train_time:165579ms step_avg:128.16ms
step:1303/1393 train_time:165714ms step_avg:128.16ms
step:1304/1393 train_time:165849ms step_avg:128.17ms
step:1305/1393 train_time:165983ms step_avg:128.17ms
step:1306/1393 train_time:166117ms step_avg:128.18ms
step:1307/1393 train_time:166251ms step_avg:128.18ms
step:1308/1393 train_time:166387ms step_avg:128.19ms
step:1309/1393 train_time:166523ms step_avg:128.19ms
step:1310/1393 train_time:166655ms step_avg:128.20ms
step:1311/1393 train_time:166788ms step_avg:128.20ms
step:1312/1393 train_time:166923ms step_avg:128.20ms
step:1313/1393 train_time:167059ms step_avg:128.21ms
step:1314/1393 train_time:167191ms step_avg:128.21ms
step:1315/1393 train_time:167326ms step_avg:128.22ms
step:1316/1393 train_time:167460ms step_avg:128.22ms
step:1317/1393 train_time:167592ms step_avg:128.23ms
step:1318/1393 train_time:167726ms step_avg:128.23ms
step:1319/1393 train_time:167862ms step_avg:128.24ms
step:1320/1393 train_time:167996ms step_avg:128.24ms
step:1321/1393 train_time:168130ms step_avg:128.25ms
step:1322/1393 train_time:168267ms step_avg:128.25ms
step:1323/1393 train_time:168399ms step_avg:128.26ms
step:1324/1393 train_time:168532ms step_avg:128.26ms
step:1325/1393 train_time:168666ms step_avg:128.26ms
step:1326/1393 train_time:168800ms step_avg:128.27ms
step:1327/1393 train_time:168934ms step_avg:128.27ms
step:1328/1393 train_time:169068ms step_avg:128.28ms
step:1329/1393 train_time:169207ms step_avg:128.28ms
step:1330/1393 train_time:169342ms step_avg:128.29ms
step:1331/1393 train_time:169479ms step_avg:128.30ms
step:1332/1393 train_time:169616ms step_avg:128.30ms
step:1333/1393 train_time:169750ms step_avg:128.31ms
step:1334/1393 train_time:169883ms step_avg:128.31ms
step:1335/1393 train_time:170016ms step_avg:128.31ms
step:1336/1393 train_time:170154ms step_avg:128.32ms
step:1337/1393 train_time:170289ms step_avg:128.33ms
step:1338/1393 train_time:170423ms step_avg:128.33ms
step:1339/1393 train_time:170559ms step_avg:128.34ms
step:1340/1393 train_time:170695ms step_avg:128.34ms
step:1341/1393 train_time:170827ms step_avg:128.35ms
step:1342/1393 train_time:170961ms step_avg:128.35ms
step:1343/1393 train_time:171095ms step_avg:128.35ms
step:1344/1393 train_time:171229ms step_avg:128.36ms
step:1345/1393 train_time:171364ms step_avg:128.36ms
step:1346/1393 train_time:171498ms step_avg:128.37ms
step:1347/1393 train_time:171635ms step_avg:128.37ms
step:1348/1393 train_time:171768ms step_avg:128.38ms
step:1349/1393 train_time:171904ms step_avg:128.38ms
step:1350/1393 train_time:172037ms step_avg:128.39ms
step:1351/1393 train_time:172172ms step_avg:128.39ms
step:1352/1393 train_time:172310ms step_avg:128.40ms
step:1353/1393 train_time:172447ms step_avg:128.40ms
step:1354/1393 train_time:172581ms step_avg:128.41ms
step:1355/1393 train_time:172715ms step_avg:128.41ms
step:1356/1393 train_time:172849ms step_avg:128.42ms
step:1357/1393 train_time:172986ms step_avg:128.42ms
step:1358/1393 train_time:173120ms step_avg:128.43ms
step:1359/1393 train_time:173255ms step_avg:128.43ms
step:1360/1393 train_time:173392ms step_avg:128.44ms
step:1361/1393 train_time:173528ms step_avg:128.44ms
step:1362/1393 train_time:173665ms step_avg:128.45ms
step:1363/1393 train_time:173802ms step_avg:128.46ms
step:1364/1393 train_time:173938ms step_avg:128.46ms
step:1365/1393 train_time:174072ms step_avg:128.47ms
step:1366/1393 train_time:174207ms step_avg:128.47ms
step:1367/1393 train_time:174342ms step_avg:128.48ms
step:1368/1393 train_time:174478ms step_avg:128.48ms
step:1369/1393 train_time:174616ms step_avg:128.49ms
step:1370/1393 train_time:174753ms step_avg:128.50ms
step:1371/1393 train_time:174889ms step_avg:128.50ms
step:1372/1393 train_time:175024ms step_avg:128.51ms
step:1373/1393 train_time:175158ms step_avg:128.51ms
step:1374/1393 train_time:175297ms step_avg:128.52ms
step:1375/1393 train_time:175430ms step_avg:128.52ms
step:1375/1393 val_loss:3.2825 train_time:175563ms step_avg:128.62ms
step:1376/1393 train_time:175584ms step_avg:128.54ms
step:1377/1393 train_time:175709ms step_avg:128.54ms
step:1378/1393 train_time:175844ms step_avg:128.54ms
step:1379/1393 train_time:175978ms step_avg:128.55ms
step:1380/1393 train_time:176114ms step_avg:128.55ms
step:1381/1393 train_time:176250ms step_avg:128.56ms
step:1382/1393 train_time:176384ms step_avg:128.56ms
step:1383/1393 train_time:176519ms step_avg:128.56ms
step:1384/1393 train_time:176657ms step_avg:128.57ms
step:1385/1393 train_time:176791ms step_avg:128.58ms
step:1386/1393 train_time:176926ms step_avg:128.58ms
step:1387/1393 train_time:177061ms step_avg:128.58ms
step:1388/1393 train_time:177195ms step_avg:128.59ms
step:1389/1393 train_time:177330ms step_avg:128.59ms
step:1390/1393 train_time:177466ms step_avg:128.60ms
step:1391/1393 train_time:177600ms step_avg:128.60ms
step:1392/1393 train_time:177736ms step_avg:128.61ms
step:1393/1393 train_time:177869ms step_avg:128.61ms
step:1393/1393 val_loss:3.2789 train_time:178002ms step_avg:128.71ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
