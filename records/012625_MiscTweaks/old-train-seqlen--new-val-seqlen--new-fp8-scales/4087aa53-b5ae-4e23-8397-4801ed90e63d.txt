import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 07:03:03 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:24359ms step_avg:nanms
step:2/1393 train_time:24786ms step_avg:nanms
step:3/1393 train_time:24929ms step_avg:nanms
step:4/1393 train_time:25049ms step_avg:nanms
step:5/1393 train_time:25169ms step_avg:nanms
step:6/1393 train_time:25290ms step_avg:nanms
step:7/1393 train_time:25410ms step_avg:nanms
step:8/1393 train_time:25531ms step_avg:nanms
step:9/1393 train_time:25653ms step_avg:nanms
step:10/1393 train_time:25773ms step_avg:nanms
step:11/1393 train_time:127ms step_avg:nanms
step:12/1393 train_time:250ms step_avg:nanms
step:13/1393 train_time:371ms step_avg:123.68ms
step:14/1393 train_time:492ms step_avg:123.09ms
step:15/1393 train_time:613ms step_avg:122.55ms
step:16/1393 train_time:733ms step_avg:122.21ms
step:17/1393 train_time:854ms step_avg:122.02ms
step:18/1393 train_time:976ms step_avg:121.94ms
step:19/1393 train_time:1098ms step_avg:122.05ms
step:20/1393 train_time:1221ms step_avg:122.09ms
step:21/1393 train_time:1343ms step_avg:122.10ms
step:22/1393 train_time:1464ms step_avg:122.03ms
step:23/1393 train_time:1587ms step_avg:122.04ms
step:24/1393 train_time:1707ms step_avg:121.91ms
step:25/1393 train_time:1829ms step_avg:121.95ms
step:26/1393 train_time:1950ms step_avg:121.86ms
step:27/1393 train_time:2071ms step_avg:121.85ms
step:28/1393 train_time:2194ms step_avg:121.91ms
step:29/1393 train_time:2315ms step_avg:121.87ms
step:30/1393 train_time:2437ms step_avg:121.84ms
step:31/1393 train_time:2558ms step_avg:121.80ms
step:32/1393 train_time:2679ms step_avg:121.79ms
step:33/1393 train_time:2801ms step_avg:121.79ms
step:34/1393 train_time:2923ms step_avg:121.78ms
step:35/1393 train_time:3044ms step_avg:121.76ms
step:36/1393 train_time:3167ms step_avg:121.81ms
step:37/1393 train_time:3290ms step_avg:121.85ms
step:38/1393 train_time:3411ms step_avg:121.83ms
step:39/1393 train_time:3533ms step_avg:121.83ms
step:40/1393 train_time:3655ms step_avg:121.84ms
step:41/1393 train_time:3777ms step_avg:121.84ms
step:42/1393 train_time:3899ms step_avg:121.85ms
step:43/1393 train_time:4021ms step_avg:121.84ms
step:44/1393 train_time:4143ms step_avg:121.84ms
step:45/1393 train_time:4265ms step_avg:121.84ms
step:46/1393 train_time:4387ms step_avg:121.85ms
step:47/1393 train_time:4508ms step_avg:121.85ms
step:48/1393 train_time:4630ms step_avg:121.85ms
step:49/1393 train_time:4751ms step_avg:121.83ms
step:50/1393 train_time:4872ms step_avg:121.81ms
step:51/1393 train_time:4995ms step_avg:121.82ms
step:52/1393 train_time:5116ms step_avg:121.81ms
step:53/1393 train_time:5237ms step_avg:121.79ms
step:54/1393 train_time:5358ms step_avg:121.78ms
step:55/1393 train_time:5480ms step_avg:121.78ms
step:56/1393 train_time:5602ms step_avg:121.78ms
step:57/1393 train_time:5724ms step_avg:121.78ms
step:58/1393 train_time:5845ms step_avg:121.77ms
step:59/1393 train_time:5966ms step_avg:121.75ms
step:60/1393 train_time:6089ms step_avg:121.77ms
step:61/1393 train_time:6211ms step_avg:121.78ms
step:62/1393 train_time:6332ms step_avg:121.77ms
step:63/1393 train_time:6453ms step_avg:121.76ms
step:64/1393 train_time:6574ms step_avg:121.74ms
step:65/1393 train_time:6696ms step_avg:121.74ms
step:66/1393 train_time:6817ms step_avg:121.73ms
step:67/1393 train_time:6938ms step_avg:121.72ms
step:68/1393 train_time:7059ms step_avg:121.71ms
step:69/1393 train_time:7182ms step_avg:121.73ms
step:70/1393 train_time:7304ms step_avg:121.73ms
step:71/1393 train_time:7425ms step_avg:121.73ms
step:72/1393 train_time:7547ms step_avg:121.72ms
step:73/1393 train_time:7668ms step_avg:121.72ms
step:74/1393 train_time:7790ms step_avg:121.71ms
step:75/1393 train_time:7911ms step_avg:121.71ms
step:76/1393 train_time:8032ms step_avg:121.70ms
step:77/1393 train_time:8154ms step_avg:121.69ms
step:78/1393 train_time:8275ms step_avg:121.68ms
step:79/1393 train_time:8397ms step_avg:121.69ms
step:80/1393 train_time:8519ms step_avg:121.70ms
step:81/1393 train_time:8641ms step_avg:121.70ms
step:82/1393 train_time:8762ms step_avg:121.69ms
step:83/1393 train_time:8883ms step_avg:121.68ms
step:84/1393 train_time:9005ms step_avg:121.69ms
step:85/1393 train_time:9127ms step_avg:121.69ms
step:86/1393 train_time:9249ms step_avg:121.70ms
step:87/1393 train_time:9372ms step_avg:121.71ms
step:88/1393 train_time:9493ms step_avg:121.71ms
step:89/1393 train_time:9615ms step_avg:121.70ms
step:90/1393 train_time:9736ms step_avg:121.70ms
step:91/1393 train_time:9857ms step_avg:121.69ms
step:92/1393 train_time:9978ms step_avg:121.68ms
step:93/1393 train_time:10099ms step_avg:121.67ms
step:94/1393 train_time:10221ms step_avg:121.68ms
step:95/1393 train_time:10342ms step_avg:121.67ms
step:96/1393 train_time:10464ms step_avg:121.68ms
step:97/1393 train_time:10586ms step_avg:121.67ms
step:98/1393 train_time:10708ms step_avg:121.68ms
step:99/1393 train_time:10829ms step_avg:121.68ms
step:100/1393 train_time:10950ms step_avg:121.67ms
step:101/1393 train_time:11071ms step_avg:121.66ms
step:102/1393 train_time:11193ms step_avg:121.66ms
step:103/1393 train_time:11314ms step_avg:121.66ms
step:104/1393 train_time:11436ms step_avg:121.66ms
step:105/1393 train_time:11559ms step_avg:121.67ms
step:106/1393 train_time:11681ms step_avg:121.68ms
step:107/1393 train_time:11803ms step_avg:121.68ms
step:108/1393 train_time:11925ms step_avg:121.68ms
step:109/1393 train_time:12048ms step_avg:121.70ms
step:110/1393 train_time:12169ms step_avg:121.69ms
step:111/1393 train_time:12292ms step_avg:121.70ms
step:112/1393 train_time:12415ms step_avg:121.71ms
step:113/1393 train_time:12537ms step_avg:121.72ms
step:114/1393 train_time:12658ms step_avg:121.72ms
step:115/1393 train_time:12781ms step_avg:121.73ms
step:116/1393 train_time:12903ms step_avg:121.73ms
step:117/1393 train_time:13025ms step_avg:121.73ms
step:118/1393 train_time:13147ms step_avg:121.73ms
step:119/1393 train_time:13270ms step_avg:121.74ms
step:120/1393 train_time:13393ms step_avg:121.75ms
step:121/1393 train_time:13515ms step_avg:121.76ms
step:122/1393 train_time:13637ms step_avg:121.76ms
step:123/1393 train_time:13760ms step_avg:121.77ms
step:124/1393 train_time:13883ms step_avg:121.78ms
step:125/1393 train_time:14006ms step_avg:121.79ms
step:125/1393 val_loss:4.3947 train_time:14126ms step_avg:122.83ms
step:126/1393 train_time:14151ms step_avg:122.00ms
step:127/1393 train_time:14257ms step_avg:121.85ms
step:128/1393 train_time:14387ms step_avg:121.92ms
step:129/1393 train_time:14511ms step_avg:121.94ms
step:130/1393 train_time:14634ms step_avg:121.95ms
step:131/1393 train_time:14755ms step_avg:121.94ms
step:132/1393 train_time:14877ms step_avg:121.94ms
step:133/1393 train_time:14999ms step_avg:121.95ms
step:134/1393 train_time:15120ms step_avg:121.94ms
step:135/1393 train_time:15241ms step_avg:121.93ms
step:136/1393 train_time:15364ms step_avg:121.94ms
step:137/1393 train_time:15487ms step_avg:121.95ms
step:138/1393 train_time:15610ms step_avg:121.95ms
step:139/1393 train_time:15732ms step_avg:121.95ms
step:140/1393 train_time:15854ms step_avg:121.95ms
step:141/1393 train_time:15977ms step_avg:121.96ms
step:142/1393 train_time:16099ms step_avg:121.96ms
step:143/1393 train_time:16221ms step_avg:121.97ms
step:144/1393 train_time:16345ms step_avg:121.98ms
step:145/1393 train_time:16467ms step_avg:121.97ms
step:146/1393 train_time:16590ms step_avg:121.98ms
step:147/1393 train_time:16712ms step_avg:121.99ms
step:148/1393 train_time:16834ms step_avg:121.98ms
step:149/1393 train_time:16956ms step_avg:121.99ms
step:150/1393 train_time:17078ms step_avg:121.99ms
step:151/1393 train_time:17200ms step_avg:121.99ms
step:152/1393 train_time:17323ms step_avg:121.99ms
step:153/1393 train_time:17445ms step_avg:121.99ms
step:154/1393 train_time:17569ms step_avg:122.00ms
step:155/1393 train_time:17690ms step_avg:122.00ms
step:156/1393 train_time:17812ms step_avg:122.00ms
step:157/1393 train_time:17934ms step_avg:122.00ms
step:158/1393 train_time:18056ms step_avg:122.00ms
step:159/1393 train_time:18178ms step_avg:122.00ms
step:160/1393 train_time:18300ms step_avg:122.00ms
step:161/1393 train_time:18422ms step_avg:122.00ms
step:162/1393 train_time:18544ms step_avg:122.00ms
step:163/1393 train_time:18667ms step_avg:122.00ms
step:164/1393 train_time:18789ms step_avg:122.01ms
step:165/1393 train_time:18911ms step_avg:122.01ms
step:166/1393 train_time:19034ms step_avg:122.01ms
step:167/1393 train_time:19156ms step_avg:122.02ms
step:168/1393 train_time:19278ms step_avg:122.02ms
step:169/1393 train_time:19400ms step_avg:122.01ms
step:170/1393 train_time:19522ms step_avg:122.01ms
step:171/1393 train_time:19644ms step_avg:122.01ms
step:172/1393 train_time:19767ms step_avg:122.02ms
step:173/1393 train_time:19890ms step_avg:122.02ms
step:174/1393 train_time:20011ms step_avg:122.02ms
step:175/1393 train_time:20134ms step_avg:122.02ms
step:176/1393 train_time:20257ms step_avg:122.03ms
step:177/1393 train_time:20379ms step_avg:122.03ms
step:178/1393 train_time:20501ms step_avg:122.03ms
step:179/1393 train_time:20623ms step_avg:122.03ms
step:180/1393 train_time:20746ms step_avg:122.03ms
step:181/1393 train_time:20868ms step_avg:122.04ms
step:182/1393 train_time:20991ms step_avg:122.04ms
step:183/1393 train_time:21113ms step_avg:122.04ms
step:184/1393 train_time:21235ms step_avg:122.04ms
step:185/1393 train_time:21358ms step_avg:122.04ms
step:186/1393 train_time:21480ms step_avg:122.04ms
step:187/1393 train_time:21601ms step_avg:122.04ms
step:188/1393 train_time:21723ms step_avg:122.04ms
step:189/1393 train_time:21845ms step_avg:122.04ms
step:190/1393 train_time:21967ms step_avg:122.04ms
step:191/1393 train_time:22090ms step_avg:122.05ms
step:192/1393 train_time:22213ms step_avg:122.05ms
step:193/1393 train_time:22335ms step_avg:122.05ms
step:194/1393 train_time:22457ms step_avg:122.05ms
step:195/1393 train_time:22579ms step_avg:122.05ms
step:196/1393 train_time:22700ms step_avg:122.05ms
step:197/1393 train_time:22822ms step_avg:122.04ms
step:198/1393 train_time:22944ms step_avg:122.04ms
step:199/1393 train_time:23066ms step_avg:122.04ms
step:200/1393 train_time:23189ms step_avg:122.05ms
step:201/1393 train_time:23312ms step_avg:122.05ms
step:202/1393 train_time:23435ms step_avg:122.06ms
step:203/1393 train_time:23558ms step_avg:122.06ms
step:204/1393 train_time:23681ms step_avg:122.07ms
step:205/1393 train_time:23802ms step_avg:122.06ms
step:206/1393 train_time:23924ms step_avg:122.06ms
step:207/1393 train_time:24046ms step_avg:122.06ms
step:208/1393 train_time:24168ms step_avg:122.06ms
step:209/1393 train_time:24291ms step_avg:122.07ms
step:210/1393 train_time:24414ms step_avg:122.07ms
step:211/1393 train_time:24538ms step_avg:122.08ms
step:212/1393 train_time:24660ms step_avg:122.08ms
step:213/1393 train_time:24783ms step_avg:122.08ms
step:214/1393 train_time:24904ms step_avg:122.08ms
step:215/1393 train_time:25026ms step_avg:122.08ms
step:216/1393 train_time:25149ms step_avg:122.08ms
step:217/1393 train_time:25272ms step_avg:122.09ms
step:218/1393 train_time:25396ms step_avg:122.09ms
step:219/1393 train_time:25519ms step_avg:122.10ms
step:220/1393 train_time:25642ms step_avg:122.10ms
step:221/1393 train_time:25765ms step_avg:122.11ms
step:222/1393 train_time:25886ms step_avg:122.11ms
step:223/1393 train_time:26009ms step_avg:122.11ms
step:224/1393 train_time:26132ms step_avg:122.11ms
step:225/1393 train_time:26256ms step_avg:122.12ms
step:226/1393 train_time:26378ms step_avg:122.12ms
step:227/1393 train_time:26501ms step_avg:122.13ms
step:228/1393 train_time:26624ms step_avg:122.13ms
step:229/1393 train_time:26747ms step_avg:122.13ms
step:230/1393 train_time:26870ms step_avg:122.14ms
step:231/1393 train_time:26993ms step_avg:122.14ms
step:232/1393 train_time:27115ms step_avg:122.14ms
step:233/1393 train_time:27238ms step_avg:122.15ms
step:234/1393 train_time:27362ms step_avg:122.15ms
step:235/1393 train_time:27485ms step_avg:122.16ms
step:236/1393 train_time:27608ms step_avg:122.16ms
step:237/1393 train_time:27731ms step_avg:122.16ms
step:238/1393 train_time:27853ms step_avg:122.16ms
step:239/1393 train_time:27976ms step_avg:122.17ms
step:240/1393 train_time:28099ms step_avg:122.17ms
step:241/1393 train_time:28221ms step_avg:122.17ms
step:242/1393 train_time:28344ms step_avg:122.17ms
step:243/1393 train_time:28467ms step_avg:122.18ms
step:244/1393 train_time:28590ms step_avg:122.18ms
step:245/1393 train_time:28714ms step_avg:122.19ms
step:246/1393 train_time:28836ms step_avg:122.19ms
step:247/1393 train_time:28959ms step_avg:122.19ms
step:248/1393 train_time:29082ms step_avg:122.19ms
step:249/1393 train_time:29204ms step_avg:122.19ms
step:250/1393 train_time:29327ms step_avg:122.20ms
step:250/1393 val_loss:3.9798 train_time:29448ms step_avg:122.70ms
step:251/1393 train_time:29473ms step_avg:122.30ms
step:252/1393 train_time:29579ms step_avg:122.23ms
step:253/1393 train_time:29703ms step_avg:122.23ms
step:254/1393 train_time:29825ms step_avg:122.24ms
step:255/1393 train_time:29947ms step_avg:122.23ms
step:256/1393 train_time:30069ms step_avg:122.23ms
step:257/1393 train_time:30192ms step_avg:122.23ms
step:258/1393 train_time:30314ms step_avg:122.23ms
step:259/1393 train_time:30438ms step_avg:122.24ms
step:260/1393 train_time:30563ms step_avg:122.25ms
step:261/1393 train_time:30689ms step_avg:122.27ms
step:262/1393 train_time:30812ms step_avg:122.27ms
step:263/1393 train_time:30934ms step_avg:122.27ms
step:264/1393 train_time:31057ms step_avg:122.27ms
step:265/1393 train_time:31180ms step_avg:122.27ms
step:266/1393 train_time:31302ms step_avg:122.27ms
step:267/1393 train_time:31425ms step_avg:122.28ms
step:268/1393 train_time:31549ms step_avg:122.28ms
step:269/1393 train_time:31673ms step_avg:122.29ms
step:270/1393 train_time:31795ms step_avg:122.29ms
step:271/1393 train_time:31918ms step_avg:122.29ms
step:272/1393 train_time:32040ms step_avg:122.29ms
step:273/1393 train_time:32164ms step_avg:122.29ms
step:274/1393 train_time:32286ms step_avg:122.30ms
step:275/1393 train_time:32409ms step_avg:122.30ms
step:276/1393 train_time:32531ms step_avg:122.30ms
step:277/1393 train_time:32654ms step_avg:122.30ms
step:278/1393 train_time:32777ms step_avg:122.30ms
step:279/1393 train_time:32899ms step_avg:122.30ms
step:280/1393 train_time:33023ms step_avg:122.31ms
step:281/1393 train_time:33147ms step_avg:122.31ms
step:282/1393 train_time:33269ms step_avg:122.31ms
step:283/1393 train_time:33393ms step_avg:122.32ms
step:284/1393 train_time:33518ms step_avg:122.33ms
step:285/1393 train_time:33641ms step_avg:122.33ms
step:286/1393 train_time:33764ms step_avg:122.33ms
step:287/1393 train_time:33887ms step_avg:122.34ms
step:288/1393 train_time:34010ms step_avg:122.34ms
step:289/1393 train_time:34132ms step_avg:122.34ms
step:290/1393 train_time:34254ms step_avg:122.34ms
step:291/1393 train_time:34378ms step_avg:122.34ms
step:292/1393 train_time:34502ms step_avg:122.35ms
step:293/1393 train_time:34625ms step_avg:122.35ms
step:294/1393 train_time:34748ms step_avg:122.35ms
step:295/1393 train_time:34870ms step_avg:122.35ms
step:296/1393 train_time:34992ms step_avg:122.35ms
step:297/1393 train_time:35115ms step_avg:122.35ms
step:298/1393 train_time:35238ms step_avg:122.35ms
step:299/1393 train_time:35361ms step_avg:122.36ms
step:300/1393 train_time:35484ms step_avg:122.36ms
step:301/1393 train_time:35607ms step_avg:122.36ms
step:302/1393 train_time:35730ms step_avg:122.36ms
step:303/1393 train_time:35853ms step_avg:122.37ms
step:304/1393 train_time:35976ms step_avg:122.37ms
step:305/1393 train_time:36099ms step_avg:122.37ms
step:306/1393 train_time:36223ms step_avg:122.38ms
step:307/1393 train_time:36346ms step_avg:122.38ms
step:308/1393 train_time:36468ms step_avg:122.38ms
step:309/1393 train_time:36592ms step_avg:122.38ms
step:310/1393 train_time:36715ms step_avg:122.38ms
step:311/1393 train_time:36838ms step_avg:122.38ms
step:312/1393 train_time:36964ms step_avg:122.40ms
step:313/1393 train_time:37089ms step_avg:122.41ms
step:314/1393 train_time:37214ms step_avg:122.42ms
step:315/1393 train_time:37340ms step_avg:122.43ms
step:316/1393 train_time:37466ms step_avg:122.44ms
step:317/1393 train_time:37591ms step_avg:122.45ms
step:318/1393 train_time:37716ms step_avg:122.45ms
step:319/1393 train_time:37842ms step_avg:122.46ms
step:320/1393 train_time:37968ms step_avg:122.48ms
step:321/1393 train_time:38093ms step_avg:122.49ms
step:322/1393 train_time:38219ms step_avg:122.50ms
step:323/1393 train_time:38346ms step_avg:122.51ms
step:324/1393 train_time:38471ms step_avg:122.52ms
step:325/1393 train_time:38596ms step_avg:122.53ms
step:326/1393 train_time:38721ms step_avg:122.54ms
step:327/1393 train_time:38847ms step_avg:122.55ms
step:328/1393 train_time:38972ms step_avg:122.55ms
step:329/1393 train_time:39097ms step_avg:122.56ms
step:330/1393 train_time:39223ms step_avg:122.57ms
step:331/1393 train_time:39348ms step_avg:122.58ms
step:332/1393 train_time:39473ms step_avg:122.59ms
step:333/1393 train_time:39598ms step_avg:122.59ms
step:334/1393 train_time:39724ms step_avg:122.60ms
step:335/1393 train_time:39851ms step_avg:122.62ms
step:336/1393 train_time:39975ms step_avg:122.62ms
step:337/1393 train_time:40100ms step_avg:122.63ms
step:338/1393 train_time:40227ms step_avg:122.64ms
step:339/1393 train_time:40353ms step_avg:122.65ms
step:340/1393 train_time:40478ms step_avg:122.66ms
step:341/1393 train_time:40603ms step_avg:122.67ms
step:342/1393 train_time:40728ms step_avg:122.67ms
step:343/1393 train_time:40854ms step_avg:122.68ms
step:344/1393 train_time:40979ms step_avg:122.69ms
step:345/1393 train_time:41104ms step_avg:122.70ms
step:346/1393 train_time:41231ms step_avg:122.71ms
step:347/1393 train_time:41356ms step_avg:122.72ms
step:348/1393 train_time:41481ms step_avg:122.73ms
step:349/1393 train_time:41607ms step_avg:122.73ms
step:350/1393 train_time:41732ms step_avg:122.74ms
step:351/1393 train_time:41858ms step_avg:122.75ms
step:352/1393 train_time:41983ms step_avg:122.76ms
step:353/1393 train_time:42108ms step_avg:122.77ms
step:354/1393 train_time:42234ms step_avg:122.77ms
step:355/1393 train_time:42359ms step_avg:122.78ms
step:356/1393 train_time:42484ms step_avg:122.79ms
step:357/1393 train_time:42611ms step_avg:122.80ms
step:358/1393 train_time:42735ms step_avg:122.80ms
step:359/1393 train_time:42860ms step_avg:122.81ms
step:360/1393 train_time:42985ms step_avg:122.82ms
step:361/1393 train_time:43110ms step_avg:122.82ms
step:362/1393 train_time:43237ms step_avg:122.83ms
step:363/1393 train_time:43363ms step_avg:122.84ms
step:364/1393 train_time:43489ms step_avg:122.85ms
step:365/1393 train_time:43614ms step_avg:122.86ms
step:366/1393 train_time:43740ms step_avg:122.87ms
step:367/1393 train_time:43866ms step_avg:122.87ms
step:368/1393 train_time:43990ms step_avg:122.88ms
step:369/1393 train_time:44115ms step_avg:122.88ms
step:370/1393 train_time:44240ms step_avg:122.89ms
step:371/1393 train_time:44365ms step_avg:122.89ms
step:372/1393 train_time:44491ms step_avg:122.90ms
step:373/1393 train_time:44616ms step_avg:122.91ms
step:374/1393 train_time:44742ms step_avg:122.92ms
step:375/1393 train_time:44868ms step_avg:122.93ms
step:375/1393 val_loss:3.7819 train_time:44991ms step_avg:123.26ms
step:376/1393 train_time:45013ms step_avg:122.99ms
step:377/1393 train_time:45126ms step_avg:122.96ms
step:378/1393 train_time:45251ms step_avg:122.96ms
step:379/1393 train_time:45375ms step_avg:122.97ms
step:380/1393 train_time:45500ms step_avg:122.97ms
step:381/1393 train_time:45625ms step_avg:122.98ms
step:382/1393 train_time:45750ms step_avg:122.98ms
step:383/1393 train_time:45874ms step_avg:122.99ms
step:384/1393 train_time:46001ms step_avg:123.00ms
step:385/1393 train_time:46129ms step_avg:123.01ms
step:386/1393 train_time:46255ms step_avg:123.02ms
step:387/1393 train_time:46380ms step_avg:123.02ms
step:388/1393 train_time:46505ms step_avg:123.03ms
step:389/1393 train_time:46630ms step_avg:123.03ms
step:390/1393 train_time:46755ms step_avg:123.04ms
step:391/1393 train_time:46880ms step_avg:123.04ms
step:392/1393 train_time:47005ms step_avg:123.05ms
step:393/1393 train_time:47131ms step_avg:123.06ms
step:394/1393 train_time:47257ms step_avg:123.07ms
step:395/1393 train_time:47383ms step_avg:123.07ms
step:396/1393 train_time:47508ms step_avg:123.08ms
step:397/1393 train_time:47633ms step_avg:123.08ms
step:398/1393 train_time:47758ms step_avg:123.09ms
step:399/1393 train_time:47882ms step_avg:123.09ms
step:400/1393 train_time:48008ms step_avg:123.10ms
step:401/1393 train_time:48134ms step_avg:123.11ms
step:402/1393 train_time:48261ms step_avg:123.11ms
step:403/1393 train_time:48385ms step_avg:123.12ms
step:404/1393 train_time:48511ms step_avg:123.12ms
step:405/1393 train_time:48636ms step_avg:123.13ms
step:406/1393 train_time:48762ms step_avg:123.14ms
step:407/1393 train_time:48887ms step_avg:123.14ms
step:408/1393 train_time:49013ms step_avg:123.15ms
step:409/1393 train_time:49138ms step_avg:123.15ms
step:410/1393 train_time:49263ms step_avg:123.16ms
step:411/1393 train_time:49389ms step_avg:123.16ms
step:412/1393 train_time:49514ms step_avg:123.17ms
step:413/1393 train_time:49640ms step_avg:123.18ms
step:414/1393 train_time:49764ms step_avg:123.18ms
step:415/1393 train_time:49890ms step_avg:123.18ms
step:416/1393 train_time:50015ms step_avg:123.19ms
step:417/1393 train_time:50141ms step_avg:123.20ms
step:418/1393 train_time:50266ms step_avg:123.20ms
step:419/1393 train_time:50392ms step_avg:123.21ms
step:420/1393 train_time:50517ms step_avg:123.21ms
step:421/1393 train_time:50644ms step_avg:123.22ms
step:422/1393 train_time:50769ms step_avg:123.23ms
step:423/1393 train_time:50894ms step_avg:123.23ms
step:424/1393 train_time:51021ms step_avg:123.24ms
step:425/1393 train_time:51146ms step_avg:123.24ms
step:426/1393 train_time:51272ms step_avg:123.25ms
step:427/1393 train_time:51398ms step_avg:123.26ms
step:428/1393 train_time:51525ms step_avg:123.26ms
step:429/1393 train_time:51652ms step_avg:123.27ms
step:430/1393 train_time:51778ms step_avg:123.28ms
step:431/1393 train_time:51903ms step_avg:123.28ms
step:432/1393 train_time:52028ms step_avg:123.29ms
step:433/1393 train_time:52154ms step_avg:123.30ms
step:434/1393 train_time:52280ms step_avg:123.30ms
step:435/1393 train_time:52406ms step_avg:123.31ms
step:436/1393 train_time:52532ms step_avg:123.31ms
step:437/1393 train_time:52659ms step_avg:123.32ms
step:438/1393 train_time:52785ms step_avg:123.33ms
step:439/1393 train_time:52910ms step_avg:123.33ms
step:440/1393 train_time:53035ms step_avg:123.34ms
step:441/1393 train_time:53161ms step_avg:123.34ms
step:442/1393 train_time:53287ms step_avg:123.35ms
step:443/1393 train_time:53413ms step_avg:123.36ms
step:444/1393 train_time:53539ms step_avg:123.36ms
step:445/1393 train_time:53664ms step_avg:123.37ms
step:446/1393 train_time:53790ms step_avg:123.37ms
step:447/1393 train_time:53916ms step_avg:123.38ms
step:448/1393 train_time:54042ms step_avg:123.38ms
step:449/1393 train_time:54168ms step_avg:123.39ms
step:450/1393 train_time:54294ms step_avg:123.39ms
step:451/1393 train_time:54420ms step_avg:123.40ms
step:452/1393 train_time:54545ms step_avg:123.41ms
step:453/1393 train_time:54670ms step_avg:123.41ms
step:454/1393 train_time:54796ms step_avg:123.41ms
step:455/1393 train_time:54921ms step_avg:123.42ms
step:456/1393 train_time:55046ms step_avg:123.42ms
step:457/1393 train_time:55171ms step_avg:123.43ms
step:458/1393 train_time:55298ms step_avg:123.43ms
step:459/1393 train_time:55423ms step_avg:123.44ms
step:460/1393 train_time:55549ms step_avg:123.44ms
step:461/1393 train_time:55675ms step_avg:123.45ms
step:462/1393 train_time:55800ms step_avg:123.45ms
step:463/1393 train_time:55926ms step_avg:123.46ms
step:464/1393 train_time:56052ms step_avg:123.46ms
step:465/1393 train_time:56177ms step_avg:123.47ms
step:466/1393 train_time:56303ms step_avg:123.47ms
step:467/1393 train_time:56429ms step_avg:123.48ms
step:468/1393 train_time:56555ms step_avg:123.48ms
step:469/1393 train_time:56682ms step_avg:123.49ms
step:470/1393 train_time:56807ms step_avg:123.49ms
step:471/1393 train_time:56933ms step_avg:123.50ms
step:472/1393 train_time:57058ms step_avg:123.50ms
step:473/1393 train_time:57184ms step_avg:123.51ms
step:474/1393 train_time:57309ms step_avg:123.51ms
step:475/1393 train_time:57435ms step_avg:123.52ms
step:476/1393 train_time:57560ms step_avg:123.52ms
step:477/1393 train_time:57685ms step_avg:123.52ms
step:478/1393 train_time:57812ms step_avg:123.53ms
step:479/1393 train_time:57938ms step_avg:123.53ms
step:480/1393 train_time:58063ms step_avg:123.54ms
step:481/1393 train_time:58190ms step_avg:123.55ms
step:482/1393 train_time:58315ms step_avg:123.55ms
step:483/1393 train_time:58441ms step_avg:123.55ms
step:484/1393 train_time:58566ms step_avg:123.56ms
step:485/1393 train_time:58692ms step_avg:123.56ms
step:486/1393 train_time:58817ms step_avg:123.57ms
step:487/1393 train_time:58943ms step_avg:123.57ms
step:488/1393 train_time:59069ms step_avg:123.57ms
step:489/1393 train_time:59195ms step_avg:123.58ms
step:490/1393 train_time:59321ms step_avg:123.59ms
step:491/1393 train_time:59448ms step_avg:123.59ms
step:492/1393 train_time:59574ms step_avg:123.60ms
step:493/1393 train_time:59700ms step_avg:123.60ms
step:494/1393 train_time:59825ms step_avg:123.61ms
step:495/1393 train_time:59951ms step_avg:123.61ms
step:496/1393 train_time:60077ms step_avg:123.61ms
step:497/1393 train_time:60202ms step_avg:123.62ms
step:498/1393 train_time:60328ms step_avg:123.62ms
step:499/1393 train_time:60453ms step_avg:123.63ms
step:500/1393 train_time:60579ms step_avg:123.63ms
step:500/1393 val_loss:3.6627 train_time:60703ms step_avg:123.88ms
step:501/1393 train_time:60724ms step_avg:123.67ms
step:502/1393 train_time:60838ms step_avg:123.65ms
step:503/1393 train_time:60964ms step_avg:123.66ms
step:504/1393 train_time:61089ms step_avg:123.66ms
step:505/1393 train_time:61214ms step_avg:123.66ms
step:506/1393 train_time:61339ms step_avg:123.67ms
step:507/1393 train_time:61465ms step_avg:123.67ms
step:508/1393 train_time:61590ms step_avg:123.67ms
step:509/1393 train_time:61718ms step_avg:123.68ms
step:510/1393 train_time:61846ms step_avg:123.69ms
step:511/1393 train_time:61973ms step_avg:123.70ms
step:512/1393 train_time:62099ms step_avg:123.70ms
step:513/1393 train_time:62223ms step_avg:123.70ms
step:514/1393 train_time:62349ms step_avg:123.71ms
step:515/1393 train_time:62473ms step_avg:123.71ms
step:516/1393 train_time:62599ms step_avg:123.71ms
step:517/1393 train_time:62725ms step_avg:123.72ms
step:518/1393 train_time:62853ms step_avg:123.73ms
step:519/1393 train_time:62981ms step_avg:123.74ms
step:520/1393 train_time:63109ms step_avg:123.74ms
step:521/1393 train_time:63236ms step_avg:123.75ms
step:522/1393 train_time:63364ms step_avg:123.76ms
step:523/1393 train_time:63491ms step_avg:123.76ms
step:524/1393 train_time:63619ms step_avg:123.77ms
step:525/1393 train_time:63746ms step_avg:123.78ms
step:526/1393 train_time:63874ms step_avg:123.79ms
step:527/1393 train_time:64003ms step_avg:123.80ms
step:528/1393 train_time:64131ms step_avg:123.80ms
step:529/1393 train_time:64259ms step_avg:123.81ms
step:530/1393 train_time:64387ms step_avg:123.82ms
step:531/1393 train_time:64515ms step_avg:123.83ms
step:532/1393 train_time:64643ms step_avg:123.84ms
step:533/1393 train_time:64771ms step_avg:123.84ms
step:534/1393 train_time:64899ms step_avg:123.85ms
step:535/1393 train_time:65027ms step_avg:123.86ms
step:536/1393 train_time:65155ms step_avg:123.87ms
step:537/1393 train_time:65282ms step_avg:123.88ms
step:538/1393 train_time:65410ms step_avg:123.88ms
step:539/1393 train_time:65538ms step_avg:123.89ms
step:540/1393 train_time:65666ms step_avg:123.90ms
step:541/1393 train_time:65793ms step_avg:123.90ms
step:542/1393 train_time:65921ms step_avg:123.91ms
step:543/1393 train_time:66049ms step_avg:123.92ms
step:544/1393 train_time:66176ms step_avg:123.92ms
step:545/1393 train_time:66304ms step_avg:123.93ms
step:546/1393 train_time:66431ms step_avg:123.94ms
step:547/1393 train_time:66559ms step_avg:123.95ms
step:548/1393 train_time:66687ms step_avg:123.95ms
step:549/1393 train_time:66815ms step_avg:123.96ms
step:550/1393 train_time:66942ms step_avg:123.97ms
step:551/1393 train_time:67070ms step_avg:123.97ms
step:552/1393 train_time:67197ms step_avg:123.98ms
step:553/1393 train_time:67324ms step_avg:123.99ms
step:554/1393 train_time:67452ms step_avg:123.99ms
step:555/1393 train_time:67580ms step_avg:124.00ms
step:556/1393 train_time:67708ms step_avg:124.01ms
step:557/1393 train_time:67835ms step_avg:124.01ms
step:558/1393 train_time:67963ms step_avg:124.02ms
step:559/1393 train_time:68090ms step_avg:124.03ms
step:560/1393 train_time:68218ms step_avg:124.03ms
step:561/1393 train_time:68347ms step_avg:124.04ms
step:562/1393 train_time:68474ms step_avg:124.05ms
step:563/1393 train_time:68602ms step_avg:124.05ms
step:564/1393 train_time:68730ms step_avg:124.06ms
step:565/1393 train_time:68858ms step_avg:124.07ms
step:566/1393 train_time:68986ms step_avg:124.07ms
step:567/1393 train_time:69114ms step_avg:124.08ms
step:568/1393 train_time:69242ms step_avg:124.09ms
step:569/1393 train_time:69370ms step_avg:124.10ms
step:570/1393 train_time:69499ms step_avg:124.11ms
step:571/1393 train_time:69627ms step_avg:124.11ms
step:572/1393 train_time:69755ms step_avg:124.12ms
step:573/1393 train_time:69883ms step_avg:124.13ms
step:574/1393 train_time:70011ms step_avg:124.13ms
step:575/1393 train_time:70138ms step_avg:124.14ms
step:576/1393 train_time:70266ms step_avg:124.15ms
step:577/1393 train_time:70394ms step_avg:124.15ms
step:578/1393 train_time:70522ms step_avg:124.16ms
step:579/1393 train_time:70650ms step_avg:124.17ms
step:580/1393 train_time:70778ms step_avg:124.17ms
step:581/1393 train_time:70907ms step_avg:124.18ms
step:582/1393 train_time:71034ms step_avg:124.19ms
step:583/1393 train_time:71162ms step_avg:124.19ms
step:584/1393 train_time:71290ms step_avg:124.20ms
step:585/1393 train_time:71418ms step_avg:124.21ms
step:586/1393 train_time:71547ms step_avg:124.21ms
step:587/1393 train_time:71674ms step_avg:124.22ms
step:588/1393 train_time:71803ms step_avg:124.23ms
step:589/1393 train_time:71929ms step_avg:124.23ms
step:590/1393 train_time:72058ms step_avg:124.24ms
step:591/1393 train_time:72186ms step_avg:124.24ms
step:592/1393 train_time:72313ms step_avg:124.25ms
step:593/1393 train_time:72440ms step_avg:124.25ms
step:594/1393 train_time:72568ms step_avg:124.26ms
step:595/1393 train_time:72696ms step_avg:124.27ms
step:596/1393 train_time:72826ms step_avg:124.28ms
step:597/1393 train_time:72953ms step_avg:124.28ms
step:598/1393 train_time:73080ms step_avg:124.29ms
step:599/1393 train_time:73208ms step_avg:124.29ms
step:600/1393 train_time:73335ms step_avg:124.30ms
step:601/1393 train_time:73463ms step_avg:124.30ms
step:602/1393 train_time:73590ms step_avg:124.31ms
step:603/1393 train_time:73718ms step_avg:124.31ms
step:604/1393 train_time:73846ms step_avg:124.32ms
step:605/1393 train_time:73974ms step_avg:124.33ms
step:606/1393 train_time:74102ms step_avg:124.33ms
step:607/1393 train_time:74229ms step_avg:124.34ms
step:608/1393 train_time:74358ms step_avg:124.34ms
step:609/1393 train_time:74485ms step_avg:124.35ms
step:610/1393 train_time:74612ms step_avg:124.35ms
step:611/1393 train_time:74739ms step_avg:124.36ms
step:612/1393 train_time:74867ms step_avg:124.36ms
step:613/1393 train_time:74994ms step_avg:124.37ms
step:614/1393 train_time:75122ms step_avg:124.37ms
step:615/1393 train_time:75250ms step_avg:124.38ms
step:616/1393 train_time:75377ms step_avg:124.38ms
step:617/1393 train_time:75505ms step_avg:124.39ms
step:618/1393 train_time:75634ms step_avg:124.40ms
step:619/1393 train_time:75764ms step_avg:124.41ms
step:620/1393 train_time:75891ms step_avg:124.41ms
step:621/1393 train_time:76019ms step_avg:124.42ms
step:622/1393 train_time:76148ms step_avg:124.42ms
step:623/1393 train_time:76277ms step_avg:124.43ms
step:624/1393 train_time:76406ms step_avg:124.44ms
step:625/1393 train_time:76534ms step_avg:124.44ms
step:625/1393 val_loss:3.5797 train_time:76660ms step_avg:124.65ms
step:626/1393 train_time:76685ms step_avg:124.49ms
step:627/1393 train_time:76798ms step_avg:124.47ms
step:628/1393 train_time:76926ms step_avg:124.48ms
step:629/1393 train_time:77053ms step_avg:124.48ms
step:630/1393 train_time:77181ms step_avg:124.49ms
step:631/1393 train_time:77308ms step_avg:124.49ms
step:632/1393 train_time:77435ms step_avg:124.49ms
step:633/1393 train_time:77564ms step_avg:124.50ms
step:634/1393 train_time:77693ms step_avg:124.51ms
step:635/1393 train_time:77824ms step_avg:124.52ms
step:636/1393 train_time:77952ms step_avg:124.52ms
step:637/1393 train_time:78080ms step_avg:124.53ms
step:638/1393 train_time:78208ms step_avg:124.54ms
step:639/1393 train_time:78336ms step_avg:124.54ms
step:640/1393 train_time:78464ms step_avg:124.55ms
step:641/1393 train_time:78592ms step_avg:124.55ms
step:642/1393 train_time:78721ms step_avg:124.56ms
step:643/1393 train_time:78849ms step_avg:124.56ms
step:644/1393 train_time:78977ms step_avg:124.57ms
step:645/1393 train_time:79106ms step_avg:124.58ms
step:646/1393 train_time:79235ms step_avg:124.58ms
step:647/1393 train_time:79363ms step_avg:124.59ms
step:648/1393 train_time:79490ms step_avg:124.59ms
step:649/1393 train_time:79618ms step_avg:124.60ms
step:650/1393 train_time:79747ms step_avg:124.60ms
step:651/1393 train_time:79876ms step_avg:124.61ms
step:652/1393 train_time:80004ms step_avg:124.62ms
step:653/1393 train_time:80132ms step_avg:124.62ms
step:654/1393 train_time:80261ms step_avg:124.63ms
step:655/1393 train_time:80388ms step_avg:124.63ms
step:656/1393 train_time:80516ms step_avg:124.64ms
step:657/1393 train_time:80644ms step_avg:124.64ms
step:658/1393 train_time:80772ms step_avg:124.65ms
step:659/1393 train_time:80900ms step_avg:124.65ms
step:660/1393 train_time:81028ms step_avg:124.66ms
step:661/1393 train_time:81156ms step_avg:124.66ms
step:662/1393 train_time:81286ms step_avg:124.67ms
step:663/1393 train_time:81413ms step_avg:124.68ms
step:664/1393 train_time:81541ms step_avg:124.68ms
step:665/1393 train_time:81669ms step_avg:124.69ms
step:666/1393 train_time:81797ms step_avg:124.69ms
step:667/1393 train_time:81925ms step_avg:124.70ms
step:668/1393 train_time:82053ms step_avg:124.70ms
step:669/1393 train_time:82181ms step_avg:124.71ms
step:670/1393 train_time:82309ms step_avg:124.71ms
step:671/1393 train_time:82437ms step_avg:124.72ms
step:672/1393 train_time:82566ms step_avg:124.72ms
step:673/1393 train_time:82694ms step_avg:124.73ms
step:674/1393 train_time:82822ms step_avg:124.73ms
step:675/1393 train_time:82951ms step_avg:124.74ms
step:676/1393 train_time:83078ms step_avg:124.74ms
step:677/1393 train_time:83207ms step_avg:124.75ms
step:678/1393 train_time:83335ms step_avg:124.75ms
step:679/1393 train_time:83463ms step_avg:124.76ms
step:680/1393 train_time:83591ms step_avg:124.76ms
step:681/1393 train_time:83720ms step_avg:124.77ms
step:682/1393 train_time:83848ms step_avg:124.77ms
step:683/1393 train_time:83976ms step_avg:124.78ms
step:684/1393 train_time:84105ms step_avg:124.78ms
step:685/1393 train_time:84233ms step_avg:124.79ms
step:686/1393 train_time:84361ms step_avg:124.79ms
step:687/1393 train_time:84489ms step_avg:124.80ms
step:688/1393 train_time:84617ms step_avg:124.80ms
step:689/1393 train_time:84747ms step_avg:124.81ms
step:690/1393 train_time:84875ms step_avg:124.82ms
step:691/1393 train_time:85003ms step_avg:124.82ms
step:692/1393 train_time:85132ms step_avg:124.83ms
step:693/1393 train_time:85260ms step_avg:124.83ms
step:694/1393 train_time:85388ms step_avg:124.84ms
step:695/1393 train_time:85516ms step_avg:124.84ms
step:696/1393 train_time:85644ms step_avg:124.85ms
step:697/1393 train_time:85771ms step_avg:124.85ms
step:698/1393 train_time:85900ms step_avg:124.85ms
step:699/1393 train_time:86028ms step_avg:124.86ms
step:700/1393 train_time:86156ms step_avg:124.86ms
step:701/1393 train_time:86285ms step_avg:124.87ms
step:702/1393 train_time:86413ms step_avg:124.87ms
step:703/1393 train_time:86541ms step_avg:124.88ms
step:704/1393 train_time:86669ms step_avg:124.88ms
step:705/1393 train_time:86797ms step_avg:124.89ms
step:706/1393 train_time:86927ms step_avg:124.89ms
step:707/1393 train_time:87054ms step_avg:124.90ms
step:708/1393 train_time:87183ms step_avg:124.90ms
step:709/1393 train_time:87311ms step_avg:124.91ms
step:710/1393 train_time:87440ms step_avg:124.91ms
step:711/1393 train_time:87568ms step_avg:124.92ms
step:712/1393 train_time:87696ms step_avg:124.92ms
step:713/1393 train_time:87825ms step_avg:124.93ms
step:714/1393 train_time:87952ms step_avg:124.93ms
step:715/1393 train_time:88080ms step_avg:124.94ms
step:716/1393 train_time:88208ms step_avg:124.94ms
step:717/1393 train_time:88336ms step_avg:124.95ms
step:718/1393 train_time:88465ms step_avg:124.95ms
step:719/1393 train_time:88593ms step_avg:124.96ms
step:720/1393 train_time:88722ms step_avg:124.96ms
step:721/1393 train_time:88849ms step_avg:124.96ms
step:722/1393 train_time:88978ms step_avg:124.97ms
step:723/1393 train_time:89106ms step_avg:124.97ms
step:724/1393 train_time:89234ms step_avg:124.98ms
step:725/1393 train_time:89365ms step_avg:124.99ms
step:726/1393 train_time:89495ms step_avg:124.99ms
step:727/1393 train_time:89625ms step_avg:125.00ms
step:728/1393 train_time:89754ms step_avg:125.01ms
step:729/1393 train_time:89885ms step_avg:125.01ms
step:730/1393 train_time:90014ms step_avg:125.02ms
step:731/1393 train_time:90145ms step_avg:125.03ms
step:732/1393 train_time:90274ms step_avg:125.03ms
step:733/1393 train_time:90405ms step_avg:125.04ms
step:734/1393 train_time:90535ms step_avg:125.05ms
step:735/1393 train_time:90665ms step_avg:125.05ms
step:736/1393 train_time:90794ms step_avg:125.06ms
step:737/1393 train_time:90924ms step_avg:125.07ms
step:738/1393 train_time:91054ms step_avg:125.07ms
step:739/1393 train_time:91185ms step_avg:125.08ms
step:740/1393 train_time:91314ms step_avg:125.09ms
step:741/1393 train_time:91446ms step_avg:125.10ms
step:742/1393 train_time:91576ms step_avg:125.10ms
step:743/1393 train_time:91706ms step_avg:125.11ms
step:744/1393 train_time:91836ms step_avg:125.12ms
step:745/1393 train_time:91965ms step_avg:125.12ms
step:746/1393 train_time:92095ms step_avg:125.13ms
step:747/1393 train_time:92224ms step_avg:125.13ms
step:748/1393 train_time:92353ms step_avg:125.14ms
step:749/1393 train_time:92484ms step_avg:125.15ms
step:750/1393 train_time:92613ms step_avg:125.15ms
step:750/1393 val_loss:3.5268 train_time:92742ms step_avg:125.33ms
step:751/1393 train_time:92763ms step_avg:125.19ms
step:752/1393 train_time:92883ms step_avg:125.18ms
step:753/1393 train_time:93012ms step_avg:125.18ms
step:754/1393 train_time:93142ms step_avg:125.19ms
step:755/1393 train_time:93272ms step_avg:125.20ms
step:756/1393 train_time:93402ms step_avg:125.20ms
step:757/1393 train_time:93532ms step_avg:125.21ms
step:758/1393 train_time:93661ms step_avg:125.22ms
step:759/1393 train_time:93792ms step_avg:125.22ms
step:760/1393 train_time:93923ms step_avg:125.23ms
step:761/1393 train_time:94053ms step_avg:125.24ms
step:762/1393 train_time:94184ms step_avg:125.24ms
step:763/1393 train_time:94314ms step_avg:125.25ms
step:764/1393 train_time:94443ms step_avg:125.26ms
step:765/1393 train_time:94573ms step_avg:125.26ms
step:766/1393 train_time:94703ms step_avg:125.27ms
step:767/1393 train_time:94834ms step_avg:125.28ms
step:768/1393 train_time:94965ms step_avg:125.28ms
step:769/1393 train_time:95095ms step_avg:125.29ms
step:770/1393 train_time:95225ms step_avg:125.30ms
step:771/1393 train_time:95355ms step_avg:125.30ms
step:772/1393 train_time:95485ms step_avg:125.31ms
step:773/1393 train_time:95616ms step_avg:125.32ms
step:774/1393 train_time:95745ms step_avg:125.32ms
step:775/1393 train_time:95875ms step_avg:125.33ms
step:776/1393 train_time:96006ms step_avg:125.33ms
step:777/1393 train_time:96136ms step_avg:125.34ms
step:778/1393 train_time:96267ms step_avg:125.35ms
step:779/1393 train_time:96397ms step_avg:125.35ms
step:780/1393 train_time:96527ms step_avg:125.36ms
step:781/1393 train_time:96656ms step_avg:125.36ms
step:782/1393 train_time:96787ms step_avg:125.37ms
step:783/1393 train_time:96917ms step_avg:125.38ms
step:784/1393 train_time:97046ms step_avg:125.38ms
step:785/1393 train_time:97176ms step_avg:125.39ms
step:786/1393 train_time:97307ms step_avg:125.40ms
step:787/1393 train_time:97437ms step_avg:125.40ms
step:788/1393 train_time:97567ms step_avg:125.41ms
step:789/1393 train_time:97696ms step_avg:125.41ms
step:790/1393 train_time:97826ms step_avg:125.42ms
step:791/1393 train_time:97956ms step_avg:125.42ms
step:792/1393 train_time:98086ms step_avg:125.43ms
step:793/1393 train_time:98216ms step_avg:125.43ms
step:794/1393 train_time:98346ms step_avg:125.44ms
step:795/1393 train_time:98478ms step_avg:125.45ms
step:796/1393 train_time:98608ms step_avg:125.46ms
step:797/1393 train_time:98738ms step_avg:125.46ms
step:798/1393 train_time:98868ms step_avg:125.47ms
step:799/1393 train_time:98998ms step_avg:125.47ms
step:800/1393 train_time:99128ms step_avg:125.48ms
step:801/1393 train_time:99258ms step_avg:125.48ms
step:802/1393 train_time:99389ms step_avg:125.49ms
step:803/1393 train_time:99519ms step_avg:125.50ms
step:804/1393 train_time:99649ms step_avg:125.50ms
step:805/1393 train_time:99779ms step_avg:125.51ms
step:806/1393 train_time:99910ms step_avg:125.51ms
step:807/1393 train_time:100039ms step_avg:125.52ms
step:808/1393 train_time:100169ms step_avg:125.52ms
step:809/1393 train_time:100298ms step_avg:125.53ms
step:810/1393 train_time:100428ms step_avg:125.54ms
step:811/1393 train_time:100559ms step_avg:125.54ms
step:812/1393 train_time:100689ms step_avg:125.55ms
step:813/1393 train_time:100818ms step_avg:125.55ms
step:814/1393 train_time:100947ms step_avg:125.56ms
step:815/1393 train_time:101077ms step_avg:125.56ms
step:816/1393 train_time:101207ms step_avg:125.57ms
step:817/1393 train_time:101337ms step_avg:125.57ms
step:818/1393 train_time:101467ms step_avg:125.58ms
step:819/1393 train_time:101599ms step_avg:125.59ms
step:820/1393 train_time:101730ms step_avg:125.59ms
step:821/1393 train_time:101859ms step_avg:125.60ms
step:822/1393 train_time:101989ms step_avg:125.60ms
step:823/1393 train_time:102118ms step_avg:125.61ms
step:824/1393 train_time:102248ms step_avg:125.61ms
step:825/1393 train_time:102379ms step_avg:125.62ms
step:826/1393 train_time:102509ms step_avg:125.62ms
step:827/1393 train_time:102639ms step_avg:125.63ms
step:828/1393 train_time:102769ms step_avg:125.63ms
step:829/1393 train_time:102899ms step_avg:125.64ms
step:830/1393 train_time:103030ms step_avg:125.65ms
step:831/1393 train_time:103160ms step_avg:125.65ms
step:832/1393 train_time:103291ms step_avg:125.66ms
step:833/1393 train_time:103421ms step_avg:125.66ms
step:834/1393 train_time:103552ms step_avg:125.67ms
step:835/1393 train_time:103682ms step_avg:125.68ms
step:836/1393 train_time:103813ms step_avg:125.68ms
step:837/1393 train_time:103943ms step_avg:125.69ms
step:838/1393 train_time:104072ms step_avg:125.69ms
step:839/1393 train_time:104202ms step_avg:125.70ms
step:840/1393 train_time:104332ms step_avg:125.70ms
step:841/1393 train_time:104461ms step_avg:125.71ms
step:842/1393 train_time:104591ms step_avg:125.71ms
step:843/1393 train_time:104724ms step_avg:125.72ms
step:844/1393 train_time:104854ms step_avg:125.72ms
step:845/1393 train_time:104984ms step_avg:125.73ms
step:846/1393 train_time:105115ms step_avg:125.74ms
step:847/1393 train_time:105245ms step_avg:125.74ms
step:848/1393 train_time:105376ms step_avg:125.75ms
step:849/1393 train_time:105507ms step_avg:125.75ms
step:850/1393 train_time:105637ms step_avg:125.76ms
step:851/1393 train_time:105768ms step_avg:125.76ms
step:852/1393 train_time:105899ms step_avg:125.77ms
step:853/1393 train_time:106030ms step_avg:125.78ms
step:854/1393 train_time:106159ms step_avg:125.78ms
step:855/1393 train_time:106289ms step_avg:125.79ms
step:856/1393 train_time:106419ms step_avg:125.79ms
step:857/1393 train_time:106550ms step_avg:125.80ms
step:858/1393 train_time:106681ms step_avg:125.80ms
step:859/1393 train_time:106812ms step_avg:125.81ms
step:860/1393 train_time:106944ms step_avg:125.82ms
step:861/1393 train_time:107074ms step_avg:125.82ms
step:862/1393 train_time:107205ms step_avg:125.83ms
step:863/1393 train_time:107335ms step_avg:125.83ms
step:864/1393 train_time:107466ms step_avg:125.84ms
step:865/1393 train_time:107596ms step_avg:125.84ms
step:866/1393 train_time:107728ms step_avg:125.85ms
step:867/1393 train_time:107859ms step_avg:125.86ms
step:868/1393 train_time:107988ms step_avg:125.86ms
step:869/1393 train_time:108118ms step_avg:125.87ms
step:870/1393 train_time:108249ms step_avg:125.87ms
step:871/1393 train_time:108380ms step_avg:125.88ms
step:872/1393 train_time:108510ms step_avg:125.88ms
step:873/1393 train_time:108639ms step_avg:125.89ms
step:874/1393 train_time:108770ms step_avg:125.89ms
step:875/1393 train_time:108901ms step_avg:125.90ms
step:875/1393 val_loss:3.4752 train_time:109030ms step_avg:126.05ms
step:876/1393 train_time:109051ms step_avg:125.93ms
step:877/1393 train_time:109171ms step_avg:125.92ms
step:878/1393 train_time:109302ms step_avg:125.92ms
step:879/1393 train_time:109431ms step_avg:125.93ms
step:880/1393 train_time:109560ms step_avg:125.93ms
step:881/1393 train_time:109690ms step_avg:125.94ms
step:882/1393 train_time:109818ms step_avg:125.94ms
step:883/1393 train_time:109950ms step_avg:125.94ms
step:884/1393 train_time:110083ms step_avg:125.95ms
step:885/1393 train_time:110215ms step_avg:125.96ms
step:886/1393 train_time:110346ms step_avg:125.97ms
step:887/1393 train_time:110475ms step_avg:125.97ms
step:888/1393 train_time:110605ms step_avg:125.97ms
step:889/1393 train_time:110735ms step_avg:125.98ms
step:890/1393 train_time:110865ms step_avg:125.98ms
step:891/1393 train_time:110995ms step_avg:125.99ms
step:892/1393 train_time:111127ms step_avg:125.99ms
step:893/1393 train_time:111258ms step_avg:126.00ms
step:894/1393 train_time:111388ms step_avg:126.00ms
step:895/1393 train_time:111519ms step_avg:126.01ms
step:896/1393 train_time:111650ms step_avg:126.02ms
step:897/1393 train_time:111779ms step_avg:126.02ms
step:898/1393 train_time:111911ms step_avg:126.03ms
step:899/1393 train_time:112042ms step_avg:126.03ms
step:900/1393 train_time:112172ms step_avg:126.04ms
step:901/1393 train_time:112302ms step_avg:126.04ms
step:902/1393 train_time:112433ms step_avg:126.05ms
step:903/1393 train_time:112564ms step_avg:126.05ms
step:904/1393 train_time:112694ms step_avg:126.06ms
step:905/1393 train_time:112823ms step_avg:126.06ms
step:906/1393 train_time:112953ms step_avg:126.06ms
step:907/1393 train_time:113085ms step_avg:126.07ms
step:908/1393 train_time:113215ms step_avg:126.07ms
step:909/1393 train_time:113346ms step_avg:126.08ms
step:910/1393 train_time:113479ms step_avg:126.09ms
step:911/1393 train_time:113611ms step_avg:126.09ms
step:912/1393 train_time:113740ms step_avg:126.10ms
step:913/1393 train_time:113870ms step_avg:126.10ms
step:914/1393 train_time:114000ms step_avg:126.11ms
step:915/1393 train_time:114131ms step_avg:126.11ms
step:916/1393 train_time:114262ms step_avg:126.12ms
step:917/1393 train_time:114394ms step_avg:126.12ms
step:918/1393 train_time:114525ms step_avg:126.13ms
step:919/1393 train_time:114656ms step_avg:126.13ms
step:920/1393 train_time:114787ms step_avg:126.14ms
step:921/1393 train_time:114917ms step_avg:126.14ms
step:922/1393 train_time:115048ms step_avg:126.15ms
step:923/1393 train_time:115178ms step_avg:126.15ms
step:924/1393 train_time:115308ms step_avg:126.16ms
step:925/1393 train_time:115439ms step_avg:126.16ms
step:926/1393 train_time:115570ms step_avg:126.17ms
step:927/1393 train_time:115700ms step_avg:126.17ms
step:928/1393 train_time:115831ms step_avg:126.18ms
step:929/1393 train_time:115961ms step_avg:126.18ms
step:930/1393 train_time:116091ms step_avg:126.19ms
step:931/1393 train_time:116223ms step_avg:126.19ms
step:932/1393 train_time:116355ms step_avg:126.20ms
step:933/1393 train_time:116488ms step_avg:126.21ms
step:934/1393 train_time:116620ms step_avg:126.21ms
step:935/1393 train_time:116753ms step_avg:126.22ms
step:936/1393 train_time:116885ms step_avg:126.23ms
step:937/1393 train_time:117019ms step_avg:126.23ms
step:938/1393 train_time:117152ms step_avg:126.24ms
step:939/1393 train_time:117283ms step_avg:126.25ms
step:940/1393 train_time:117415ms step_avg:126.25ms
step:941/1393 train_time:117547ms step_avg:126.26ms
step:942/1393 train_time:117678ms step_avg:126.26ms
step:943/1393 train_time:117811ms step_avg:126.27ms
step:944/1393 train_time:117945ms step_avg:126.28ms
step:945/1393 train_time:118078ms step_avg:126.29ms
step:946/1393 train_time:118211ms step_avg:126.29ms
step:947/1393 train_time:118342ms step_avg:126.30ms
step:948/1393 train_time:118474ms step_avg:126.31ms
step:949/1393 train_time:118606ms step_avg:126.31ms
step:950/1393 train_time:118739ms step_avg:126.32ms
step:951/1393 train_time:118872ms step_avg:126.33ms
step:952/1393 train_time:119003ms step_avg:126.33ms
step:953/1393 train_time:119136ms step_avg:126.34ms
step:954/1393 train_time:119268ms step_avg:126.34ms
step:955/1393 train_time:119400ms step_avg:126.35ms
step:956/1393 train_time:119534ms step_avg:126.36ms
step:957/1393 train_time:119667ms step_avg:126.36ms
step:958/1393 train_time:119799ms step_avg:126.37ms
step:959/1393 train_time:119932ms step_avg:126.38ms
step:960/1393 train_time:120063ms step_avg:126.38ms
step:961/1393 train_time:120195ms step_avg:126.39ms
step:962/1393 train_time:120327ms step_avg:126.39ms
step:963/1393 train_time:120460ms step_avg:126.40ms
step:964/1393 train_time:120592ms step_avg:126.41ms
step:965/1393 train_time:120726ms step_avg:126.41ms
step:966/1393 train_time:120857ms step_avg:126.42ms
step:967/1393 train_time:120989ms step_avg:126.43ms
step:968/1393 train_time:121120ms step_avg:126.43ms
step:969/1393 train_time:121252ms step_avg:126.44ms
step:970/1393 train_time:121384ms step_avg:126.44ms
step:971/1393 train_time:121517ms step_avg:126.45ms
step:972/1393 train_time:121649ms step_avg:126.45ms
step:973/1393 train_time:121781ms step_avg:126.46ms
step:974/1393 train_time:121913ms step_avg:126.47ms
step:975/1393 train_time:122046ms step_avg:126.47ms
step:976/1393 train_time:122177ms step_avg:126.48ms
step:977/1393 train_time:122308ms step_avg:126.48ms
step:978/1393 train_time:122440ms step_avg:126.49ms
step:979/1393 train_time:122572ms step_avg:126.49ms
step:980/1393 train_time:122704ms step_avg:126.50ms
step:981/1393 train_time:122835ms step_avg:126.50ms
step:982/1393 train_time:122968ms step_avg:126.51ms
step:983/1393 train_time:123099ms step_avg:126.51ms
step:984/1393 train_time:123231ms step_avg:126.52ms
step:985/1393 train_time:123363ms step_avg:126.53ms
step:986/1393 train_time:123496ms step_avg:126.53ms
step:987/1393 train_time:123628ms step_avg:126.54ms
step:988/1393 train_time:123760ms step_avg:126.54ms
step:989/1393 train_time:123893ms step_avg:126.55ms
step:990/1393 train_time:124025ms step_avg:126.56ms
step:991/1393 train_time:124158ms step_avg:126.56ms
step:992/1393 train_time:124292ms step_avg:126.57ms
step:993/1393 train_time:124425ms step_avg:126.58ms
step:994/1393 train_time:124557ms step_avg:126.58ms
step:995/1393 train_time:124689ms step_avg:126.59ms
step:996/1393 train_time:124820ms step_avg:126.59ms
step:997/1393 train_time:124953ms step_avg:126.60ms
step:998/1393 train_time:125084ms step_avg:126.60ms
step:999/1393 train_time:125216ms step_avg:126.61ms
step:1000/1393 train_time:125348ms step_avg:126.61ms
step:1000/1393 val_loss:3.4135 train_time:125478ms step_avg:126.75ms
step:1001/1393 train_time:125499ms step_avg:126.64ms
step:1002/1393 train_time:125618ms step_avg:126.63ms
step:1003/1393 train_time:125751ms step_avg:126.64ms
step:1004/1393 train_time:125883ms step_avg:126.64ms
step:1005/1393 train_time:126014ms step_avg:126.65ms
step:1006/1393 train_time:126146ms step_avg:126.65ms
step:1007/1393 train_time:126276ms step_avg:126.66ms
step:1008/1393 train_time:126409ms step_avg:126.66ms
step:1009/1393 train_time:126544ms step_avg:126.67ms
step:1010/1393 train_time:126677ms step_avg:126.68ms
step:1011/1393 train_time:126810ms step_avg:126.68ms
step:1012/1393 train_time:126943ms step_avg:126.69ms
step:1013/1393 train_time:127074ms step_avg:126.69ms
step:1014/1393 train_time:127205ms step_avg:126.70ms
step:1015/1393 train_time:127336ms step_avg:126.70ms
step:1016/1393 train_time:127467ms step_avg:126.71ms
step:1017/1393 train_time:127601ms step_avg:126.71ms
step:1018/1393 train_time:127734ms step_avg:126.72ms
step:1019/1393 train_time:127867ms step_avg:126.73ms
step:1020/1393 train_time:128000ms step_avg:126.73ms
step:1021/1393 train_time:128132ms step_avg:126.74ms
step:1022/1393 train_time:128264ms step_avg:126.74ms
step:1023/1393 train_time:128397ms step_avg:126.75ms
step:1024/1393 train_time:128529ms step_avg:126.75ms
step:1025/1393 train_time:128662ms step_avg:126.76ms
step:1026/1393 train_time:128794ms step_avg:126.77ms
step:1027/1393 train_time:128926ms step_avg:126.77ms
step:1028/1393 train_time:129060ms step_avg:126.78ms
step:1029/1393 train_time:129193ms step_avg:126.78ms
step:1030/1393 train_time:129325ms step_avg:126.79ms
step:1031/1393 train_time:129457ms step_avg:126.79ms
step:1032/1393 train_time:129589ms step_avg:126.80ms
step:1033/1393 train_time:129721ms step_avg:126.80ms
step:1034/1393 train_time:129853ms step_avg:126.81ms
step:1035/1393 train_time:129987ms step_avg:126.82ms
step:1036/1393 train_time:130120ms step_avg:126.82ms
step:1037/1393 train_time:130252ms step_avg:126.83ms
step:1038/1393 train_time:130386ms step_avg:126.83ms
step:1039/1393 train_time:130518ms step_avg:126.84ms
step:1040/1393 train_time:130650ms step_avg:126.84ms
step:1041/1393 train_time:130782ms step_avg:126.85ms
step:1042/1393 train_time:130915ms step_avg:126.86ms
step:1043/1393 train_time:131046ms step_avg:126.86ms
step:1044/1393 train_time:131180ms step_avg:126.87ms
step:1045/1393 train_time:131313ms step_avg:126.87ms
step:1046/1393 train_time:131445ms step_avg:126.88ms
step:1047/1393 train_time:131576ms step_avg:126.88ms
step:1048/1393 train_time:131709ms step_avg:126.89ms
step:1049/1393 train_time:131842ms step_avg:126.89ms
step:1050/1393 train_time:131974ms step_avg:126.90ms
step:1051/1393 train_time:132108ms step_avg:126.91ms
step:1052/1393 train_time:132240ms step_avg:126.91ms
step:1053/1393 train_time:132371ms step_avg:126.91ms
step:1054/1393 train_time:132504ms step_avg:126.92ms
step:1055/1393 train_time:132636ms step_avg:126.92ms
step:1056/1393 train_time:132768ms step_avg:126.93ms
step:1057/1393 train_time:132900ms step_avg:126.93ms
step:1058/1393 train_time:133034ms step_avg:126.94ms
step:1059/1393 train_time:133167ms step_avg:126.95ms
step:1060/1393 train_time:133300ms step_avg:126.95ms
step:1061/1393 train_time:133432ms step_avg:126.96ms
step:1062/1393 train_time:133566ms step_avg:126.96ms
step:1063/1393 train_time:133698ms step_avg:126.97ms
step:1064/1393 train_time:133830ms step_avg:126.97ms
step:1065/1393 train_time:133961ms step_avg:126.98ms
step:1066/1393 train_time:134094ms step_avg:126.98ms
step:1067/1393 train_time:134226ms step_avg:126.99ms
step:1068/1393 train_time:134359ms step_avg:126.99ms
step:1069/1393 train_time:134493ms step_avg:127.00ms
step:1070/1393 train_time:134625ms step_avg:127.00ms
step:1071/1393 train_time:134758ms step_avg:127.01ms
step:1072/1393 train_time:134889ms step_avg:127.01ms
step:1073/1393 train_time:135021ms step_avg:127.02ms
step:1074/1393 train_time:135153ms step_avg:127.02ms
step:1075/1393 train_time:135285ms step_avg:127.03ms
step:1076/1393 train_time:135417ms step_avg:127.03ms
step:1077/1393 train_time:135548ms step_avg:127.04ms
step:1078/1393 train_time:135680ms step_avg:127.04ms
step:1079/1393 train_time:135817ms step_avg:127.05ms
step:1080/1393 train_time:135950ms step_avg:127.06ms
step:1081/1393 train_time:136081ms step_avg:127.06ms
step:1082/1393 train_time:136214ms step_avg:127.06ms
step:1083/1393 train_time:136346ms step_avg:127.07ms
step:1084/1393 train_time:136479ms step_avg:127.08ms
step:1085/1393 train_time:136610ms step_avg:127.08ms
step:1086/1393 train_time:136743ms step_avg:127.08ms
step:1087/1393 train_time:136875ms step_avg:127.09ms
step:1088/1393 train_time:137009ms step_avg:127.10ms
step:1089/1393 train_time:137144ms step_avg:127.10ms
step:1090/1393 train_time:137277ms step_avg:127.11ms
step:1091/1393 train_time:137408ms step_avg:127.11ms
step:1092/1393 train_time:137541ms step_avg:127.12ms
step:1093/1393 train_time:137672ms step_avg:127.12ms
step:1094/1393 train_time:137804ms step_avg:127.13ms
step:1095/1393 train_time:137938ms step_avg:127.13ms
step:1096/1393 train_time:138071ms step_avg:127.14ms
step:1097/1393 train_time:138204ms step_avg:127.14ms
step:1098/1393 train_time:138337ms step_avg:127.15ms
step:1099/1393 train_time:138469ms step_avg:127.15ms
step:1100/1393 train_time:138601ms step_avg:127.16ms
step:1101/1393 train_time:138732ms step_avg:127.16ms
step:1102/1393 train_time:138864ms step_avg:127.16ms
step:1103/1393 train_time:138997ms step_avg:127.17ms
step:1104/1393 train_time:139129ms step_avg:127.17ms
step:1105/1393 train_time:139263ms step_avg:127.18ms
step:1106/1393 train_time:139395ms step_avg:127.19ms
step:1107/1393 train_time:139527ms step_avg:127.19ms
step:1108/1393 train_time:139662ms step_avg:127.20ms
step:1109/1393 train_time:139793ms step_avg:127.20ms
step:1110/1393 train_time:139925ms step_avg:127.20ms
step:1111/1393 train_time:140058ms step_avg:127.21ms
step:1112/1393 train_time:140190ms step_avg:127.21ms
step:1113/1393 train_time:140322ms step_avg:127.22ms
step:1114/1393 train_time:140455ms step_avg:127.22ms
step:1115/1393 train_time:140587ms step_avg:127.23ms
step:1116/1393 train_time:140719ms step_avg:127.23ms
step:1117/1393 train_time:140851ms step_avg:127.24ms
step:1118/1393 train_time:140986ms step_avg:127.24ms
step:1119/1393 train_time:141118ms step_avg:127.25ms
step:1120/1393 train_time:141249ms step_avg:127.25ms
step:1121/1393 train_time:141382ms step_avg:127.26ms
step:1122/1393 train_time:141513ms step_avg:127.26ms
step:1123/1393 train_time:141645ms step_avg:127.26ms
step:1124/1393 train_time:141777ms step_avg:127.27ms
step:1125/1393 train_time:141907ms step_avg:127.27ms
step:1125/1393 val_loss:3.3626 train_time:142040ms step_avg:127.39ms
step:1126/1393 train_time:142063ms step_avg:127.30ms
step:1127/1393 train_time:142182ms step_avg:127.29ms
step:1128/1393 train_time:142316ms step_avg:127.30ms
step:1129/1393 train_time:142449ms step_avg:127.30ms
step:1130/1393 train_time:142580ms step_avg:127.30ms
step:1131/1393 train_time:142714ms step_avg:127.31ms
step:1132/1393 train_time:142845ms step_avg:127.31ms
step:1133/1393 train_time:142976ms step_avg:127.32ms
step:1134/1393 train_time:143111ms step_avg:127.32ms
step:1135/1393 train_time:143244ms step_avg:127.33ms
step:1136/1393 train_time:143380ms step_avg:127.34ms
step:1137/1393 train_time:143511ms step_avg:127.34ms
step:1138/1393 train_time:143646ms step_avg:127.35ms
step:1139/1393 train_time:143779ms step_avg:127.35ms
step:1140/1393 train_time:143914ms step_avg:127.36ms
step:1141/1393 train_time:144047ms step_avg:127.36ms
step:1142/1393 train_time:144180ms step_avg:127.37ms
step:1143/1393 train_time:144316ms step_avg:127.38ms
step:1144/1393 train_time:144449ms step_avg:127.38ms
step:1145/1393 train_time:144583ms step_avg:127.39ms
step:1146/1393 train_time:144718ms step_avg:127.39ms
step:1147/1393 train_time:144851ms step_avg:127.40ms
step:1148/1393 train_time:144986ms step_avg:127.40ms
step:1149/1393 train_time:145118ms step_avg:127.41ms
step:1150/1393 train_time:145252ms step_avg:127.41ms
step:1151/1393 train_time:145387ms step_avg:127.42ms
step:1152/1393 train_time:145520ms step_avg:127.43ms
step:1153/1393 train_time:145656ms step_avg:127.43ms
step:1154/1393 train_time:145789ms step_avg:127.44ms
step:1155/1393 train_time:145922ms step_avg:127.44ms
step:1156/1393 train_time:146059ms step_avg:127.45ms
step:1157/1393 train_time:146193ms step_avg:127.46ms
step:1158/1393 train_time:146327ms step_avg:127.46ms
step:1159/1393 train_time:146460ms step_avg:127.47ms
step:1160/1393 train_time:146594ms step_avg:127.47ms
step:1161/1393 train_time:146726ms step_avg:127.48ms
step:1162/1393 train_time:146860ms step_avg:127.48ms
step:1163/1393 train_time:146993ms step_avg:127.49ms
step:1164/1393 train_time:147127ms step_avg:127.49ms
step:1165/1393 train_time:147260ms step_avg:127.50ms
step:1166/1393 train_time:147394ms step_avg:127.50ms
step:1167/1393 train_time:147527ms step_avg:127.51ms
step:1168/1393 train_time:147662ms step_avg:127.51ms
step:1169/1393 train_time:147795ms step_avg:127.52ms
step:1170/1393 train_time:147929ms step_avg:127.53ms
step:1171/1393 train_time:148062ms step_avg:127.53ms
step:1172/1393 train_time:148197ms step_avg:127.54ms
step:1173/1393 train_time:148330ms step_avg:127.54ms
step:1174/1393 train_time:148469ms step_avg:127.55ms
step:1175/1393 train_time:148602ms step_avg:127.56ms
step:1176/1393 train_time:148736ms step_avg:127.56ms
step:1177/1393 train_time:148871ms step_avg:127.57ms
step:1178/1393 train_time:149004ms step_avg:127.57ms
step:1179/1393 train_time:149137ms step_avg:127.58ms
step:1180/1393 train_time:149273ms step_avg:127.58ms
step:1181/1393 train_time:149407ms step_avg:127.59ms
step:1182/1393 train_time:149540ms step_avg:127.59ms
step:1183/1393 train_time:149675ms step_avg:127.60ms
step:1184/1393 train_time:149808ms step_avg:127.61ms
step:1185/1393 train_time:149943ms step_avg:127.61ms
step:1186/1393 train_time:150077ms step_avg:127.62ms
step:1187/1393 train_time:150217ms step_avg:127.63ms
step:1188/1393 train_time:150350ms step_avg:127.63ms
step:1189/1393 train_time:150484ms step_avg:127.64ms
step:1190/1393 train_time:150618ms step_avg:127.64ms
step:1191/1393 train_time:150751ms step_avg:127.65ms
step:1192/1393 train_time:150885ms step_avg:127.65ms
step:1193/1393 train_time:151020ms step_avg:127.66ms
step:1194/1393 train_time:151154ms step_avg:127.66ms
step:1195/1393 train_time:151290ms step_avg:127.67ms
step:1196/1393 train_time:151423ms step_avg:127.68ms
step:1197/1393 train_time:151557ms step_avg:127.68ms
step:1198/1393 train_time:151693ms step_avg:127.69ms
step:1199/1393 train_time:151826ms step_avg:127.69ms
step:1200/1393 train_time:151959ms step_avg:127.70ms
step:1201/1393 train_time:152092ms step_avg:127.70ms
step:1202/1393 train_time:152230ms step_avg:127.71ms
step:1203/1393 train_time:152368ms step_avg:127.72ms
step:1204/1393 train_time:152501ms step_avg:127.72ms
step:1205/1393 train_time:152636ms step_avg:127.73ms
step:1206/1393 train_time:152771ms step_avg:127.74ms
step:1207/1393 train_time:152905ms step_avg:127.74ms
step:1208/1393 train_time:153038ms step_avg:127.74ms
step:1209/1393 train_time:153173ms step_avg:127.75ms
step:1210/1393 train_time:153307ms step_avg:127.76ms
step:1211/1393 train_time:153441ms step_avg:127.76ms
step:1212/1393 train_time:153575ms step_avg:127.77ms
step:1213/1393 train_time:153708ms step_avg:127.77ms
step:1214/1393 train_time:153842ms step_avg:127.78ms
step:1215/1393 train_time:153977ms step_avg:127.78ms
step:1216/1393 train_time:154110ms step_avg:127.79ms
step:1217/1393 train_time:154243ms step_avg:127.79ms
step:1218/1393 train_time:154377ms step_avg:127.80ms
step:1219/1393 train_time:154512ms step_avg:127.80ms
step:1220/1393 train_time:154646ms step_avg:127.81ms
step:1221/1393 train_time:154778ms step_avg:127.81ms
step:1222/1393 train_time:154913ms step_avg:127.82ms
step:1223/1393 train_time:155045ms step_avg:127.82ms
step:1224/1393 train_time:155179ms step_avg:127.82ms
step:1225/1393 train_time:155315ms step_avg:127.83ms
step:1226/1393 train_time:155450ms step_avg:127.84ms
step:1227/1393 train_time:155584ms step_avg:127.84ms
step:1228/1393 train_time:155718ms step_avg:127.85ms
step:1229/1393 train_time:155851ms step_avg:127.85ms
step:1230/1393 train_time:155986ms step_avg:127.86ms
step:1231/1393 train_time:156120ms step_avg:127.86ms
step:1232/1393 train_time:156255ms step_avg:127.87ms
step:1233/1393 train_time:156388ms step_avg:127.87ms
step:1234/1393 train_time:156521ms step_avg:127.88ms
step:1235/1393 train_time:156655ms step_avg:127.88ms
step:1236/1393 train_time:156791ms step_avg:127.89ms
step:1237/1393 train_time:156923ms step_avg:127.89ms
step:1238/1393 train_time:157061ms step_avg:127.90ms
step:1239/1393 train_time:157194ms step_avg:127.90ms
step:1240/1393 train_time:157328ms step_avg:127.91ms
step:1241/1393 train_time:157465ms step_avg:127.92ms
step:1242/1393 train_time:157597ms step_avg:127.92ms
step:1243/1393 train_time:157732ms step_avg:127.93ms
step:1244/1393 train_time:157867ms step_avg:127.93ms
step:1245/1393 train_time:158001ms step_avg:127.94ms
step:1246/1393 train_time:158135ms step_avg:127.94ms
step:1247/1393 train_time:158269ms step_avg:127.95ms
step:1248/1393 train_time:158403ms step_avg:127.95ms
step:1249/1393 train_time:158536ms step_avg:127.95ms
step:1250/1393 train_time:158670ms step_avg:127.96ms
step:1250/1393 val_loss:3.3160 train_time:158802ms step_avg:128.07ms
step:1251/1393 train_time:158823ms step_avg:127.98ms
step:1252/1393 train_time:158943ms step_avg:127.97ms
step:1253/1393 train_time:159075ms step_avg:127.98ms
step:1254/1393 train_time:159207ms step_avg:127.98ms
step:1255/1393 train_time:159346ms step_avg:127.99ms
step:1256/1393 train_time:159480ms step_avg:127.99ms
step:1257/1393 train_time:159613ms step_avg:128.00ms
step:1258/1393 train_time:159747ms step_avg:128.00ms
step:1259/1393 train_time:159885ms step_avg:128.01ms
step:1260/1393 train_time:160018ms step_avg:128.01ms
step:1261/1393 train_time:160151ms step_avg:128.02ms
step:1262/1393 train_time:160286ms step_avg:128.02ms
step:1263/1393 train_time:160421ms step_avg:128.03ms
step:1264/1393 train_time:160554ms step_avg:128.03ms
step:1265/1393 train_time:160688ms step_avg:128.04ms
step:1266/1393 train_time:160824ms step_avg:128.04ms
step:1267/1393 train_time:160958ms step_avg:128.05ms
step:1268/1393 train_time:161092ms step_avg:128.05ms
step:1269/1393 train_time:161227ms step_avg:128.06ms
step:1270/1393 train_time:161362ms step_avg:128.06ms
step:1271/1393 train_time:161496ms step_avg:128.07ms
step:1272/1393 train_time:161629ms step_avg:128.07ms
step:1273/1393 train_time:161762ms step_avg:128.08ms
step:1274/1393 train_time:161897ms step_avg:128.08ms
step:1275/1393 train_time:162032ms step_avg:128.09ms
step:1276/1393 train_time:162165ms step_avg:128.09ms
step:1277/1393 train_time:162299ms step_avg:128.10ms
step:1278/1393 train_time:162433ms step_avg:128.10ms
step:1279/1393 train_time:162566ms step_avg:128.11ms
step:1280/1393 train_time:162703ms step_avg:128.11ms
step:1281/1393 train_time:162837ms step_avg:128.12ms
step:1282/1393 train_time:162969ms step_avg:128.12ms
step:1283/1393 train_time:163104ms step_avg:128.13ms
step:1284/1393 train_time:163239ms step_avg:128.13ms
step:1285/1393 train_time:163371ms step_avg:128.13ms
step:1286/1393 train_time:163506ms step_avg:128.14ms
step:1287/1393 train_time:163640ms step_avg:128.14ms
step:1288/1393 train_time:163773ms step_avg:128.15ms
step:1289/1393 train_time:163908ms step_avg:128.15ms
step:1290/1393 train_time:164044ms step_avg:128.16ms
step:1291/1393 train_time:164180ms step_avg:128.17ms
step:1292/1393 train_time:164315ms step_avg:128.17ms
step:1293/1393 train_time:164452ms step_avg:128.18ms
step:1294/1393 train_time:164586ms step_avg:128.18ms
step:1295/1393 train_time:164721ms step_avg:128.19ms
step:1296/1393 train_time:164855ms step_avg:128.19ms
step:1297/1393 train_time:164991ms step_avg:128.20ms
step:1298/1393 train_time:165123ms step_avg:128.20ms
step:1299/1393 train_time:165257ms step_avg:128.21ms
step:1300/1393 train_time:165390ms step_avg:128.21ms
step:1301/1393 train_time:165525ms step_avg:128.21ms
step:1302/1393 train_time:165660ms step_avg:128.22ms
step:1303/1393 train_time:165795ms step_avg:128.22ms
step:1304/1393 train_time:165930ms step_avg:128.23ms
step:1305/1393 train_time:166063ms step_avg:128.23ms
step:1306/1393 train_time:166198ms step_avg:128.24ms
step:1307/1393 train_time:166332ms step_avg:128.24ms
step:1308/1393 train_time:166466ms step_avg:128.25ms
step:1309/1393 train_time:166601ms step_avg:128.25ms
step:1310/1393 train_time:166735ms step_avg:128.26ms
step:1311/1393 train_time:166867ms step_avg:128.26ms
step:1312/1393 train_time:167000ms step_avg:128.26ms
step:1313/1393 train_time:167135ms step_avg:128.27ms
step:1314/1393 train_time:167268ms step_avg:128.27ms
step:1315/1393 train_time:167403ms step_avg:128.28ms
step:1316/1393 train_time:167537ms step_avg:128.28ms
step:1317/1393 train_time:167670ms step_avg:128.29ms
step:1318/1393 train_time:167804ms step_avg:128.29ms
step:1319/1393 train_time:167940ms step_avg:128.30ms
step:1320/1393 train_time:168074ms step_avg:128.30ms
step:1321/1393 train_time:168207ms step_avg:128.30ms
step:1322/1393 train_time:168344ms step_avg:128.31ms
step:1323/1393 train_time:168477ms step_avg:128.31ms
step:1324/1393 train_time:168611ms step_avg:128.32ms
step:1325/1393 train_time:168744ms step_avg:128.32ms
step:1326/1393 train_time:168879ms step_avg:128.33ms
step:1327/1393 train_time:169013ms step_avg:128.33ms
step:1328/1393 train_time:169147ms step_avg:128.34ms
step:1329/1393 train_time:169286ms step_avg:128.34ms
step:1330/1393 train_time:169420ms step_avg:128.35ms
step:1331/1393 train_time:169556ms step_avg:128.35ms
step:1332/1393 train_time:169693ms step_avg:128.36ms
step:1333/1393 train_time:169826ms step_avg:128.36ms
step:1334/1393 train_time:169961ms step_avg:128.37ms
step:1335/1393 train_time:170092ms step_avg:128.37ms
step:1336/1393 train_time:170231ms step_avg:128.38ms
step:1337/1393 train_time:170365ms step_avg:128.38ms
step:1338/1393 train_time:170500ms step_avg:128.39ms
step:1339/1393 train_time:170636ms step_avg:128.39ms
step:1340/1393 train_time:170771ms step_avg:128.40ms
step:1341/1393 train_time:170903ms step_avg:128.40ms
step:1342/1393 train_time:171036ms step_avg:128.41ms
step:1343/1393 train_time:171170ms step_avg:128.41ms
step:1344/1393 train_time:171304ms step_avg:128.41ms
step:1345/1393 train_time:171439ms step_avg:128.42ms
step:1346/1393 train_time:171573ms step_avg:128.42ms
step:1347/1393 train_time:171708ms step_avg:128.43ms
step:1348/1393 train_time:171842ms step_avg:128.43ms
step:1349/1393 train_time:171977ms step_avg:128.44ms
step:1350/1393 train_time:172111ms step_avg:128.44ms
step:1351/1393 train_time:172245ms step_avg:128.44ms
step:1352/1393 train_time:172383ms step_avg:128.45ms
step:1353/1393 train_time:172520ms step_avg:128.46ms
step:1354/1393 train_time:172657ms step_avg:128.46ms
step:1355/1393 train_time:172792ms step_avg:128.47ms
step:1356/1393 train_time:172926ms step_avg:128.47ms
step:1357/1393 train_time:173061ms step_avg:128.48ms
step:1358/1393 train_time:173197ms step_avg:128.48ms
step:1359/1393 train_time:173333ms step_avg:128.49ms
step:1360/1393 train_time:173469ms step_avg:128.50ms
step:1361/1393 train_time:173604ms step_avg:128.50ms
step:1362/1393 train_time:173740ms step_avg:128.51ms
step:1363/1393 train_time:173878ms step_avg:128.51ms
step:1364/1393 train_time:174015ms step_avg:128.52ms
step:1365/1393 train_time:174148ms step_avg:128.52ms
step:1366/1393 train_time:174283ms step_avg:128.53ms
step:1367/1393 train_time:174419ms step_avg:128.53ms
step:1368/1393 train_time:174554ms step_avg:128.54ms
step:1369/1393 train_time:174692ms step_avg:128.54ms
step:1370/1393 train_time:174829ms step_avg:128.55ms
step:1371/1393 train_time:174964ms step_avg:128.56ms
step:1372/1393 train_time:175102ms step_avg:128.56ms
step:1373/1393 train_time:175236ms step_avg:128.57ms
step:1374/1393 train_time:175374ms step_avg:128.57ms
step:1375/1393 train_time:175509ms step_avg:128.58ms
step:1375/1393 val_loss:3.2821 train_time:175641ms step_avg:128.67ms
step:1376/1393 train_time:175662ms step_avg:128.60ms
step:1377/1393 train_time:175786ms step_avg:128.59ms
step:1378/1393 train_time:175920ms step_avg:128.60ms
step:1379/1393 train_time:176054ms step_avg:128.60ms
step:1380/1393 train_time:176190ms step_avg:128.61ms
step:1381/1393 train_time:176324ms step_avg:128.61ms
step:1382/1393 train_time:176460ms step_avg:128.61ms
step:1383/1393 train_time:176595ms step_avg:128.62ms
step:1384/1393 train_time:176732ms step_avg:128.63ms
step:1385/1393 train_time:176867ms step_avg:128.63ms
step:1386/1393 train_time:177001ms step_avg:128.63ms
step:1387/1393 train_time:177138ms step_avg:128.64ms
step:1388/1393 train_time:177272ms step_avg:128.64ms
step:1389/1393 train_time:177407ms step_avg:128.65ms
step:1390/1393 train_time:177542ms step_avg:128.65ms
step:1391/1393 train_time:177676ms step_avg:128.66ms
step:1392/1393 train_time:177812ms step_avg:128.66ms
step:1393/1393 train_time:177946ms step_avg:128.67ms
step:1393/1393 val_loss:3.2786 train_time:178081ms step_avg:128.76ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
